On the Sampling Problem for Kernel Quadrature

Franc oisXavier Briol     Chris    Oates     Jon Cockayne   Wilson Ye Chen   Mark Girolami    

Abstract

The standard Kernel Quadrature method for numerical integration with random point sets  also
called Bayesian Monte Carlo  is known to converge in root mean square error at   rate determined by the ratio      where   and   encode the smoothness and dimension of the integrand  However  an empirical investigation
reveals that the rate constant   is highly sensitive to the distribution of the random points 
In contrast to standard Monte Carlo integration 
for which optimal importance sampling is wellunderstood  the sampling distribution that minimises   for Kernel Quadrature does not admit
  closed form  This paper argues that the practical choice of sampling distribution is an important open problem  One solution is considered   
novel automatic approach based on adaptive tempering and sequential Monte Carlo  Empirical results demonstrate   dramatic reduction in integration error of up to   orders of magnitude can be
achieved with the proposed method 

 cid 

 

  INTRODUCTION
Consider approximation of the Lebesgue integral

      

    

 

where   is   Borel measure de ned over     Rd and
  is Borel measurable  De ne       to be the set of
Borel measures  cid  such that       cid  meaning that
       cid      and assume           In
 cid   cid 
situations where      does not admit   closedform  Monte

  cid   cid 

 University of Warwick  Department of Statistics   Imperial
College London  Department of Mathematics   Newcastle University  School of Mathematics and Statistics  The Alan Turing Institute for Data Science  University of Technology Sydney 
School of Mathematical and Physical Sciences  Correspondence
to  Franc oisXavier Briol  fx briol warwick ac uk 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Carlo  MC  methods can be used to estimate the numerical
value of Eqn      classical research problem in computational statistics is to reduce the MC estimation error in this
context  where the integral can  for example  represent an
expectation or marginalisation over   random variable of
interest 
The default MC estimator comprises of

  cid 

  

 MC      

 
 

   xj 

where xj are sampled identically and independently        
from   Then we have   root mean square error  RMSE 

bound  cid 

    MC              CMC      

 

 

 

where CMC         Std       and the expectation is with
respect to the joint distribution of the  xj  
   For settings where the Lebesgue density of   is only known up to
normalising constant  Markov chain Monte Carlo  MCMC 
methods can be used  the rateconstant CMC       is then
related to the asymptotic variance of   under the Markov
chain sample path 
Considerations of computational cost place emphasis on
methods to reduce the rate constant CMC       For the
MC estimator  this rate constant can be made smaller via
importance sampling  IS     cid          cid  where an
optimal choice  cid              cid 
that minimises
Std         cid   cid  is available in explicit closedform
 see Robert and Casella    Thm    However  the
RMSE remains asymptotically gated at     
The default Kernel Quadrature  KQ  estimate comprises of

      

wjf  xj 

 

  

where the xj    cid  are independent  or arise from
  Markov chain  and supp    supp cid 
In contrast to MC  the weights  wj  
   in KQ are in general
nonuniform  realvalued and depend on  xj  
   The
KQ nomenclature derives from the  symmetric  positivede nite  kernel               that is used to conj   jk    xj  such that

struct an interpolant           cid  

  cid 

On the Sampling Problem for Kernel Quadrature

  are implicitly de ned via the equation       cid 

    xj       xj  for                  The weights wj in Eqn 
       
The KQ estimator is identical to the posterior mean in
Bayesian Monte Carlo    Hagan    Rasmussen and
Ghahramani    and its relationship with classical numerical quadrature rules has been studied  Diaconis   
  arkk   et al   
Under regularity conditions  Briol et al      established the following RMSE bound for KQ 

                        cid 
ns   

 

       

 cid 

where both the integrand   and each argument of the kernel
  admit continuous mixed weak derivatives of order   and
      can be arbitrarily small  An informationtheoretic
lower bound on the RMSE is           Bakhvalov 
  The faster convergence of the RMSE  relative to
MC  can lead to improved precision in applications  Akin
to IS  the samples  xj  
   need not be draws from   in order for KQ to provide consistent estimation  since   is encoded in the weights wj  Importantly  KQ can be viewed
as postprocessing of MC samples  the kernel   can be
reverseengineered       via crossvalidation  and does not
need to be speci ed upfront 
One notable disadvantage of KQ methods is that little is
known about how the rate constant        cid  depends on
the choice of sampling distribution  cid  In contrast to IS 
no general closedform expression has been established for
an optimal distribution  cid  for KQ  the technical meaning
of  optimal  is de ned below  Moreover  limited practical
guidance is available on the selection of the sampling distribution  an exception is Bach    as explained in Sec 
  and in applications it is usual to take  cid     
This choice is convenient but leads to estimators that are
not ef cient  as we demonstrate in dramatic empirical examples in Sec   
The main contributions of this paper are twofold  First  we
formalise the problem of optimal sampling for KQ as an
important and open challenge in computational statistics 
To be precise  our target is an optimal sampling distribution
for KQ  de ned as

    arg min

 cid 

sup
   

              

 

for some functional class   to be speci ed  In general  
 possibly nonunique  optimal   will depend on   and 
unlike for IS  also on the kernel   and the number of samples   
Second  we propose   novel and automatic method for selection of  cid  that is rooted in approximation of the unavailable   In brief  our method considers candidate sampling

 cid 

     for         and
distributions of the form  cid      
    reference distribution on     The exponent   is chosen
such that  cid  minimises an empirical upper bound on the
RMSE  The overall approach is facilitated with an ef cient
sequential MC  SMC  sampler and called SMCKQ  In particular  the approach     provides practical guidance for selection of  cid  for KQ   ii  offers robustness to kernel misspeci cation  and  iii  extends recent work on computing
posterior expectations with kernels obtained using Stein  
method  Oates et al   
The paper proceeds as follows  Empirical results in Sec 
  reveal that the RMSE for KQ is highly sensitive to the
choice of  cid  The proposed approach to selection of  cid  is
contained in Sec    Numerical experiments  presented in
Sec    demonstrate that dramatic reductions in integration
error  up to   orders of magnitude  can be achieved with
SMCKQ  Lastly    discussion is provided in Sec   

  BACKGROUND
This section presents an overview of KQ  Sec    and  
empirical  Secs    and theoretical  Sec    results on
the choice of sampling distribution  and discusses kernel
learning for KQ  Sec   

  Overview of Kernel Quadrature

We now proceed to describe KQ  Recall the approximation
   to    an explicit form for the coef cients    is given as
         where Ki       xi  xj  and fj      xj  It is
assumed that    exists almost surely  for nondegenerate
kernels  this corresponds to   having no atoms  From the
above de nition of KQ 

  cid 

  

 cid 

 

  

     xj dx 

      

De ning zj  cid 

     xj    leads to the estimate in Eqn 
  with weights          Pairs      for which the
zj have closed form are reported in Table   of Briol et al 
    Computation of these weights incurs   computational cost of at most      and can be justi ed when
either     evaluation of   forms the computational bottleneck  or  ii  the gain in estimator precision  as   function in
   dominates this cost       whenever            
Notable contributions on KQ include Diaconis  
  Hagan   Rasmussen and Ghahramani   who
introduced the method and Huszar and Duvenaud  
Osborne et al        Gunter et al    Bach  
Briol et al          arkk   et al    Kanagawa
et al    Liu and Lee   who provided consequent methodological extensions  KQ has been applied
to   wide range of problems including probabilistic ODE

On the Sampling Problem for Kernel Quadrature

 cid cid cid cid cid    cid 

  

 cid cid cid cid cid 

solvers  Kersting and Hennig    reinforcement learning  Paul et al     ltering  Pr uher and  Simandl   
and design of experiments  Ma et al   
Several characterisations of the KQ estimator are known
and detailed below  Let   denote the Hilbert space characterised by the reproducing kernel    and denote its norm as
 cid     cid    Berlinet and ThomasAgnan    Then we have
the following      The function    is the minimiser of  cid   cid  
over       subject to   xj       xj  for all                 
    The function    is the posterior mean for   under the
Gaussian process prior     GP     conditioned on data
  and      is the mean of the implied posterior marginal
over          The weights   are characterised as the minimiser over     Rn of

en xj  

     sup

 cid   cid   

 jf  xj        

the maximal error in the unit ball of    These characterisations connect KQ to     nonparametric regression     
probabilistic integration and     quasiMonte Carlo  QMC 
methods  Dick and Pillichshammer    The scattered
data approximation literature  Sommariva and Vianello 
  and the numerical analysis literature  where KQ is
known as the  empirical interpolation method  Eftang and
Stamm    Kristoffersen    can also be connected
to KQ  However  our search of all of these literatures did
not yield guidance on the optimal selection of the sampling
distribution  cid   with the exception of Bach   reported
in Sec   

  OverReliance on the Kernel

In Osborne et al      Huszar and Duvenaud  
Gunter et al    Briol et al      the selection of xn
was approached as   greedy optimisation problem  wherein
the maximal integration error en   xj  
   was minimised  given the location of the previous  xj   
     This
approach has demonstrated considerable success in applications  However  the error criterion en is strongly dependant on the choice of kernel   and the sequential optimisation approach is vulnerable to kernel misspeci cation  In particular  if the intrinsic length scale of   is  too
small  then the  xj  
   all cluster around the mode of
  leading to poor integral estimation  see Fig    in the
Appendix  Related work on subsample selection  such
as leverage scores  Bach    can also be nonrobust to
misspeci ed kernels  The partial solution of online kernel
learning requires   suf cient number   of data and is not
always practicable in smalln regimes that motivate KQ 
This paper considers sampling methods as   robust alternative to optimisation methods  Although our method also
makes use of   to select  cid  it reverts to  cid      in the

limit as the length scale of   is made small  In this sense 
sampling offers more robustness to kernel misspeci cation
than optimisation methods  at the expense of   possible
 nonasymptotic  decrease in precision in the case of  
wellspeci ed kernel  This line of research is thus complementary to existing work  However  we emphasise that
robustness is an important consideration for general applications of KQ in which kernel speci cation may be   nontrivial task 

  Sensitivity to the Sampling Distribution

To date  we are not aware of   clear demonstration of the
acute dependence of the performance of the KQ estimator
on the choice of distribution  cid  It is therefore important to
illustrate this phenomenon in order to build intuition 
Consider the toy problem with state space        target
distribution            single test function        
    sin    and kernel        cid    exp       cid  For
this problem  consider   range of sampling distributions of
the form  cid         for       Fig    plots

 Rn   

                 

 cid cid cid cid   

 

  cid 

  

an empirical estimate for the RMSE where          is
the mth of   independent KQ estimates for      based
on   samples drawn from the distribution  cid  with standard
deviation          In this case          is available in closedform  It is seen that the  obvious  choice of
            cid      is suboptimal  The intuition here
is that  extreme  samples xi from the tails of   are rather
informative for building the interpolant    underlying KQ 
we should therefore oversample these values via   heaviertailed  cid  The same intuition is used for column sampling
and to construct leverage scores  Mahoney    Drineas
et al   

  Established Results

Here we recall the main convergence results todate on KQ
and discuss how these relate to choices of sampling distribution  To reduce the level of detail below  we make several
assumptions at the outset 
Assumption on the domain  The domain   will either be
Rd itself or   compact subset of Rd that satis es an  interior
cone condition  meaning that there exists an angle    
    and   radius       such that for every      
there exists  cid cid      such that the cone              
Rd   cid   cid      yT     cos            is contained in
   see Wendland    for background 
Assumption on the kernel  Consider the integral operator             with        de ned as the

On the Sampling Problem for Kernel Quadrature

Figure   The performance of kernel quadrature is sensitive to
the choice of sampling distribution  Here the test function was
            sin    the target measure was      while  
samples were generated from      The kernel        cid   
exp       cid  was used  Notice that the values of   that minimise the root mean square error  RMSE  are uniformly greater
than        dashed line  and depend on the number   of samples
in general 

Bochner integral  cid 
 cid 
composition        cid     cid 

       cid        cid dx cid  Assume that
         dx      so that   is selfadjoint  positive
semide nite and traceclass  Simon    Then  from an
extension of Mercer   theorem    onig    we have   dem   mem   em   cid  where
   and em    are the eigenvalues and eigenfunctions of
  Further assume that   is dense in   
The  rst result is adapted and extended from Thm    in
Oates et al   
Theorem   Assume that  cid  admits   density  cid  de ned
on   compact domain     Assume that  cid      for some
      Let            xm be  xed and de ne the Euclidean
 ll distance

hm   sup
   

min

   

 cid     xj cid 

Let xm          xn be independent draws from  cid  Assume
  gives rise to   Sobolev space Hs  Then there exists
       such that  for hm     

 cid 

                            

for all       Here         ck cid cid   cid   for some constant
    ck cid      independent of   and   

All proofs are reserved for the Appendix  The main contribution of Thm    is to establish   convergence rate for
KQ when using importance sampling distributions    similar result appeared in Thm    of Briol et al      for
samples from    see the Appendix  and was extended to
MCMC samples in Oates et al    An extension to

Figure   The performance of kernel quadrature is sensitive to the
choice of kernel  Here the same setup as Fig    was used with
      The kernel        cid    exp       cid cid  was used
for various choices of parameter  cid      The root mean
square error  RMSE  is sensitive to choice of  cid  for all choices of
  suggesting that online kernel learning could be used to improve
over the default choice of  cid      and        dashed lines 

the case of   misspeci ed kernel was considered in Kanagawa et al    However   limitation of this direction
of research is that it does not address the question of how
to select  cid 
The second result that we present is   consequence of the
recent work of Bach   who considered   particular
choice of  cid        depending on    xed       via the
  
     The following is
      
adapted from Prop    in Bach  
Theorem   Let            xn      be independent and
      For         and         log    
 

density          cid 
    cid 

  

 

  

  

      we have that
                 cid   cid   

with probability greater than      

 

Some remarks are in order 
    Bach   Prop   
showed that  for     integration error scales at an optimal rate in   up to logarithmic terms and  after   samples 
     ii  The distribution    is obtained from
is of size
minimising an upper bound on the integration error  rather
than the error itself  It is unclear to us how well    approximates an optimal sampling distribution for KQ   iii 
In general    is hard to compute  For the speci   case
            equal to Hs  and   uniform  the distribution    is also uniform  and hence independent of   
see Sec    of Bach   However  even for the simple
example of Sec       does not appear to have   closed
form  details in Appendix  An approximation scheme was
proposed in Sec    of Bach   but the error of this
scheme was not studied 
Optimal sampling for approximation in  cid     cid    with
weighted least squares  not in the kernel setting  was

On the Sampling Problem for Kernel Quadrature

considered in Hampton and Doostan   Cohen and
Migliorati  

  Goals

Our  rst goal was to formalise the sampling problem for
KQ  this is now completed  Our second goal was to develop   novel automatic approach to selection of  cid  called
SMCKQ  full details are provided in Sec   
Also  observe that the integrand   will in general belong
to an in nitude of Hilbert spaces  while for KQ   single
kernel   must be selected  This choice will affect the performance of the KQ estimator  for example  in Fig    the
problem of Sec    was reconsidered based on   class of
kernels        cid    exp       cid cid  parametrised by
 cid      Results showed that  for all choices of   parameter  the RMSE of KQ is sensitive to choice of  cid  In particular  the default choice of  cid      is not optimal  For this
reason  an extension that includes kernel learning  called
SMCKQ KL  is proposed in Sec   

  METHODS
In this section the SMCKQ and SMCKQ KL methods are
presented  Our aim is to explain in detail the main components  SMC  temp  crit  of Alg    To this end  Secs   
and   set up our SMC sampler to target tempered distributions  while Sec    presents   heuristic for the choice
of temperature schedule  Sec    extends the approach to
kernel learning and Sec    proposes   novel criterion to
determine when   desired error tolerance is reached 

  Thermodynamic Ansatz

To begin  consider      and   as  xed  The following
ansatz is central to our proposed SMCKQ method  An optimal distribution    in the sense of Eqn    can be wellapproximated by   distribution of the form

       

     

       

 

for   speci    but unknown   inverse temperature  parameter        Here   is   reference distribution to be speci ed and which should be chosen to be uninformative in
practice  It is assumed that all    exist       can be normalised  The motivation for this ansatz stems from Sec 
  where          and           can be cast
in this form with       and   an  improper  uniform
distribution on    In general  tempering generates   class
of distributions which overrepresent extreme events relative to         have heavier tails  This property has the
potential to improve performance for KQ  as demonstrated
in Sec   
The ansatz of Eqn    reduces the nonparametric sampling
problem for KQ to the onedimensional parametric prob 

lem of selecting   suitable         The problem can
be further simpli ed by focusing on   discrete temperature
ladder  ti  
   such that        ti   ti  and tT    
Discussion of the choice of ladder is deferred to Sec   
This reduced problem  where we seek an optimal index
                  is still nontrivial as no closedform expression is available for the RMSE at each candidate ti 
To overcome this impasse   novel approach to estimate the
RMSE is presented in Sec   

  Convex Ansatz  SMC 

The proposed SMCKQ algorithm requires   second ansatz 
namely that the RMSE is convex in   and possesses   global
minimum in the range         This second ansatz  borne
out in numerical results in Fig    motivates an algorithm
that begins at        and tracks the RMSE until an increase
is detected  say at ti  at which point the index            is
taken for KQ 
To realise such an algorithm  this paper exploited SMC
methods  Chopin    Del Moral et al    Here   
particle approximation  wj  xj  
   to     is  rst obtained where xj are independent draws from   wj  
   and    cid     Then  at iteration    the particle approximation to  ti  is reweighted  resampled and subject to   Markov transition  to deliver   particle approximation    cid 
   to  ti  This  resample move  algorithm  denoted SMC  is standard but  for completeness 
pseudocode is provided as Alg    in the Appendix 
At iteration      subset of size   is drawn from the unique 
elements in    cid 
   
   from the particle approximation to
 ti  and proposed for use in KQ    criterion crit  de ned
in Sec    is used to determine whether the resultant KQ
error has increased relative to  ti 
If this is the case 
then the distribution  ti  from the previous iteration is
taken for use in KQ  Otherwise the algorithm proceeds to
ti  and the process repeats  In the degenerate case where
the RMSE has   minimum at tT   the algorithm defaults to
standard KQ with  cid     
Both ansatz of the SMCKQ algorithm are justi ed through
the strong empirical results presented in Sec   

   

     cid 

  Choice of Temperature Schedule  temp 
The choice of temperature schedule  ti  
   in uences several aspects of SMCKQ      The SMC approximation to
 ti is governed by the  distance   in some appropriate
metric  between  ti  and  ti 
 ii  The speed at which
the minimum    can be reached is linear in the number of
 This ensures that kernel matrices have full rank  It does not
introduce bias into KQ  since in general  cid  need not equal  
However  to keep notation clear  we do not make this operation
explicit 

On the Sampling Problem for Kernel Quadrature

temperatures between   and     iii  The precision of KQ
depends on the approximation      ti  Factors    iii  motivate the use of    ne schedule with   large  while  ii 
motivates   coarse schedule with   small 
For this work    temperature schedule was used that is
well suited to both     and  ii  while   strict constraint
ti   ti      was imposed on the grid spacing to acknowledge  iii  The speci   schedule used in this work
was determined based on the conditional effective sample
size of the current particle population  as proposed in the
recent work of Zhou et al    Full details are presented
in Algs    and   in the Appendix 

  Kernel Learning

 

 

 cid 

In Sec    we demonstrated the bene   of kernel learning
for KQ  From the Gaussian process characterisation of KQ
from Sec    it follows that kernel parameters   can be
estimated  conditional on   vector of function evaluations
   via maximum marginal likelihood 
 cid    arg max
       arg min

      log    
  
In SMCKQ KL  the function evaluations   are obtained
at the  rst     of    states  xj  
   and the parameters  
are updated in each iteration of the SMC  This demands repeated function evaluation  this burden can be reduced with
less frequent parameter updates and caching of all previous
function evaluations  The experiments in Sec    assessed
both SMCKQ and SMCKQ KL in terms of precision per
total number of function evaluations  so that the additional
cost of kernel learning was taken into account 

 

  Termination Criterion  crit 

The SMCKQ KL algorithm is designed to track the RMSE
as   is increased  However  the RMSE is not available in
closed form  In this section we derive   tight upper bound
on the RMSE that is used for the crit component in Alg 
 
From the worstcase characterisation of KQ presented in
Sec    we have an upper bound

                en   xj  
term en   xj  
    since   depends on  xj  

denoted henceforth as
The
en xj  
   can be computed in closed form  see the Appendix  This motivates

  cid   cid   

  

 

 This is   notational convention and is without loss of generality  In this paper these states were   random sample  without
replacement  of size    though strati ed sampling among the  
states could be used  More sophisticated alternatives that also involve the kernel    such as leverage scores  were not considered 
since in general these     introduce   vulnerability to misspeci ed
kernels and  ii  require manipulation of         kernel matrix
 Patel et al   

Algorithm   SMC Algorithm for KQ

function SMCKQ                   
input    integrand 
input    target disn 
input    kernel 
input    reference disn 
input    resample threshold 
input    num  func  evaluations 
input    num  particles 
      ti     Rmin    
       initialise states            
  cid 
        initialise weights            
  cid 
    crit      cid 
   
    est   error 
while test     Rmin  and ti     do
          Rmin    
        cid 
 wj  xj  
     cid 
ti   temp wj  xj  
   ti     next temp 
   cid 
   
     SMC wj  xj  
 next particle approx 
    crit      cid 

   
    est   error 

   

     cid 

zj  cid 

     xj     kernel mean eval             

end while
fj      xj   function eval             
Kj   cid      xj  xj cid   kernel eval        cid          
         cid      eval  KQ estimator 
return     

  

   ti  ti   

the following upper bound on MSE 
                   en xj  

  

 cid cid 

 

 cid 

 cid   cid   cid cid cid cid 

 

 

The term   can be estimated with the bootstrap approximation

 cid 

  cid 

  en xj  

    

en   xm    

  

    

  

 

  

where  xm   are independent draws from  xj  
In
SMCKQ the term   is an unknown constant and the
statistic    an empirical proxy for the RMSE  is monitored
at each iteration  The algorithm terminates once an increase
in this statistic occurs  For SMCKQ KL the term   is
nonconstant as it depends on the kernel hyperparameters 
then   can in addition be estimated as  cid     cid       cid    
and we monitor the product of   and  cid     cid    with termination when an increase is observed       test  de ned in
the Appendix 
Full pseudocode for SMCKQ is provided as Alg    while
SMCKQ KL is Alg    in the Appendix  To summarise  we
have developed   novel procedure  SMCKQ  and an extension SMCKQ KL  designed to approximate the optimal

On the Sampling Problem for Kernel Quadrature

KQ estimator based on the unavailable optimal distribution
in Eqn    where   is the unit ball of    Earlier empirical results in Sec    suggest that SMCKQ has potential
to provide   powerful and general algorithm for numerical integration  The additional computational cost of optimising the sampling distribution does however have to be
counterbalanced with the potential gain in error  and so this
method will mainly be of practical interest for problems
with expensive integrands or complex target distributions 
The following section reports experiments designed to test
this claim 

  RESULTS
Here we compared SMCKQ  and SMCKQ KL  against
the corresponding default approaches KQ  and KQKL  that
are based on  cid      Sec    below reports an assessment
in which the true value of integrals is known by design 
while in Sec    the methods were deployed to solve  
parameter estimation problem involving differential equations 

  Simulation Study

To continue our illustration from Sec    we investigated
the performance of SMCKQ and SMCKQ KL for integration of             sin    against the distribution
         Here the reference distribution was taken to
be          All experiments employed SMC with
      particles  random walk Metropolis transitions
 Alg    the resample threshold       and   maximum grid size       Dependence of the subsequent
results on the choice of   was investigated in Fig    in
the Appendix 
Fig     top  reports results for SMCKQ against KQ  for
 xed lengthscale  cid      Corresponding results for
SMCKQ KL against KQKL are shown in the bottom plot 
It was observed that SMCKQ  resp  SMCKQ KL  outperformed KQ  resp  KQKL  in the sense that  on   perfunction evaluation basis  the MSE achieved by the proposed method was lower than for the standard method 
The largest reduction in MSE achieved was about   orders
of magnitude  correspondingly   orders of magnitude in
RMSE    fair approximation to the       method  which
is approximately optimal for             results in Fig 
  was observed  The termination criterion in Sec    was
observed to be   good approximation to the optimal temperature     Fig    in Appendix  As an aside  we note that
the MSE was gated at   for all methods due to numerical condition of the kernel matrix      known feature of
the Gaussian kernel used in this experiment 
The investigation was extended to larger dimensions       
and       and more complex integrands   in the Ap 

Figure   Performance on for the running illustration of Figs   
and   The top plot shows SMCKQ against KQ  whilst the bottom
plot illustrates the versions with kernel learning 

pendix  In all cases  considerable improvements were obtained using SMCKQ over KQ 

  Inference for Differential Equations
Consider the model given by dx dt         with solution
     depending on unknown parameters   Suppose we
can obtain observations through the following noise model
 likelihood    ti      ti    ei at times                 
tn where we assume ei         for known       Our
goal is to estimate      for    xed  potentially large 
      To do so  we will use   Bayesian approach and
specify   prior    then obtain samples from the posterior          using MCMC  The posterior predictive

mean is then de ned as   cid     cid     cid        

and this can be estimated using an empirical average from
the posterior samples  This type of integration problem is
particularly challenging as the integrand requires simulating from the differential equation at each iteration  Furthermore  the larger   or the smaller the grid  the longer the
simulation will be and the higher the computational cost 
For   tractable testbed  we considered Hooke   law  given

On the Sampling Problem for Kernel Quadrature

  DISCUSSION
In this paper we formalised the optimal sampling problem for KQ    general  practical solution was proposed 
based on novel use of SMC methods  Initial empirical results demonstrate performance gains relative to standard
approach of KQ with  cid        more challenging example based on parameter estimation for differential equations
was used to illustrate the potential of SMCKQ for Bayesian
computation in combination with Stein   method 
Our methods were general but required userspeci ed
choice of an initial distribution   For compact state
spaces   we recommend taking   to be uniform  For
noncompact spaces  however  there is   degree of  exibility here and default solutions  such as wide Gaussian
distributions  necessarily require user input  However  the
choice of   is easier than the choice of  cid  itself  since  
is not required to be optimal  In our examples  improved
performance  relative to standard KQ  was observed for  
range of reference distributions  
  main motivation for this research was to provide an alternative to optimisationbased KQ that alleviates strong
dependence on the choice of kernel  Sec    This paper provides essential groundwork toward that goal  in developing samplingbased methods for KQ in the case of
complex and expensive integration problems  An empirical comparison of samplingbased and optimisationbased
methods is reserved for future work 
Two extensions of this research are identi ed  First  the
curse of dimension that is intrinsic to standard Sobolev
spaces can be alleviated by demanding  dominating mixed
smoothness  our methods are compatible with these  essentially tensor product  kernels  Dick et al    Second  the use of sequential QMC  Gerber and Chopin   
can be considered  motivated by further orders of magnitude reduction in numerical error observed for deterministic point sets  see Fig    in the Appendix 

ACKNOWLEDGEMENTS

FXB was supported by the EPSRC grant  EP   
CJO   MG we supported by the Lloyds Register Foundation Programme on DataCentric Engineering  WYC
was supported by the ARC Centre of Excellence in Mathematical and Statistical Frontiers  MG was supported
by the EPSRC grants  EP    EP   
EP    an EPSRC Established Career Fellowship 
the EU grant  EU    Royal Society Wolfson Research Merit Award  FXB  CJO  JC   MG were also supported by the SAMSI working group on Probabilistic Numerics 

Figure   Comparison of SMCKQ and KQ on the ODE inverse
problem  The top plot illustrates the physical system  the middle plot shows observations of the ODE  whilst the bottom plot
illustrates the superior performance of SMCKQ against KQ 

by the following second order homogeneous ODE given by

 

   
dt     

dx
dt

        

with initial conditions        and   cid      This
equation represents the evolution of   mass on   spring with
friction  Robinson    Chapter   More precisely   
denotes the spring constant    the damping coef cient representing friction and   the mass of the object  Since this
differential equation is an overdetermined system we  xed
      we get   damped oscilla 
      In this case  if  
tory behaviour as presented in Fig     top  Data were generated with                        
with lognormal priors with scale equal to   for all parameters 
To implement KQ under an unknown normalisation constant for   we followed Oates et al    and made use
of   Gaussian kernel that was adapted with Stein   method
 see the Appendix for details  The reference distribution
  was an wide uniform prior on the hypercube    
Brute force computation was used to obtain   benchmark
value for the integral  For the SMC algorithm  an independent lognormal transition kernel was used at each iteration
with parameters automatically tuned to the current set of
particles  Results in Fig    demonstrate that SMCKQ outperforms KQ for these integration problems  These results
improve upon those reported in Oates et al    for  
similar integration problem based on parameter estimation
for differential equations 

On the Sampling Problem for Kernel Quadrature

REFERENCES
   Bach  Sharp analysis of lowrank kernel matrix approx 

imations  In Proc     Conf  Learn  Theory   

   Bach  On the equivalence between kernel quadrature

rules and random features  arXiv   

      Bakhvalov  On approximate computation of integrals 
Vestnik MGU  Ser  Math  Mech  Astron  Phys  Chem   
    In Russian 

   Berlinet and    ThomasAgnan  Reproducing kernel
Hilbert spaces in probability and statistics  Springer Science   Business Media   

FX  Briol        Oates     Girolami  and       Osborne 
FrankWolfe Bayesian quadrature  Probabilistic integration with theoretical guarantees  In Adv  Neur  Inf  Proc 
Sys     

FX  Briol        Oates     Girolami        Osborne  and
   Sejdinovic  Probabilistic integration    role for statisticians in numerical analysis  arXiv     

   Chopin    sequential particle  lter method for static

models  Biometrika     

   Cohen and    Migliorati  Optimal weighted least 

squares methods  arXiv   

   Del Moral     Doucet  and    Jasra  Sequential monte
carlo samplers        Stat  Soc  Ser     Stat  Methodol 
   

   Diaconis  Bayesian Numerical Analysis  volume IV of
Statistical Decision Theory and Related Topics  pages
  SpringerVerlag  New York   

   Dick and    Pillichshammer  Digital nets and sequences 
Discrepancy Theory and Quasi Monte Carlo Integration  Cambridge University Press   

   Gunter     Garnett     Osborne     Hennig  and
   Roberts  Sampling for inference in probabilistic models with fast Bayesian quadrature  In Adv  Neur  Inf  Proc 
Sys   

   Hampton and    Doostan  Coherence motivated sampling and convergence analysis of least squares polynomial Chaos regression  Comput  Methods Appl  Mech 
Engrg     

   Hinrichs  Optimal importance sampling for the approximation of integrals     Complexity     

   Huszar and    Duvenaud  Optimallyweighted herding

is Bayesian quadrature  In Uncert  Artif  Intell   

   Kanagawa     Sriperumbudur  and    Fukumizu  Convergence guarantees for kernelbased quadrature rules in
misspeci ed settings  In Adv  Neur  Inf  Proc  Sys   

   Kersting and    Hennig  Active uncertainty calibration
In Proc  Conf  Uncert  Artif 

in bayesian ode solvers 
Intell   

     onig  Eigenvalues of compact operators with applications to integral operators  Linear Algebra Appl   
   

   Kristoffersen  The empirical interpolation method  Master   thesis  Department of Mathematical Sciences  Norwegian University of Science and Technology   

   Liu and       Lee  BlackBox Importance Sampling    

Conf  Artif  Intell  Stat   

   Ma     Garnett  and    Schneider  Active Area Search
via Bayesian Quadrature     Conf Artif  Intell  Stat   
 

      Mahoney  Randomized algorithms for matrices and
data  Found  Trends Mach  Learn     

   Dick        Kuo  and       Sloan  Highdimensional integration  The quasiMonte Carlo way  Acta Numerica 
   

      Oates     Cockayne  FX  Briol  and    Girolami 
Convergence Rates for   Class of Estimators Based on
Stein   Identity  arXiv   

   Drineas     MagdonIsmail        Mahoney  and      
Woodruff  Fast approximation of matrix coherence and
statistical leverage     Mach  Learn  Res   
 

      Eftang and    Stamm  Parameter multidomain  hp 
empirical interpolation        Numer  Methods in Eng   
   

   Gerber and    Chopin  Sequential quasi Monte Carlo 

      Statist  Soc        

      Oates     Girolami  and    Chopin  Control Functionals for Monte Carlo Integration        Stat  Soc  Ser 
   Stat  Methodol    To appear 

     Hagan  BayesHermite quadrature     Statist  Plann 

Inference     

      Osborne     Duvenaud     Garnett        Rasmussen     Roberts  and    Ghahramani  Active learning
of model evidence using Bayesian quadrature  In Adv 
Neur  Inf  Proc  Sys     

On the Sampling Problem for Kernel Quadrature

      Osborne     Garnett     Roberts     Hart     Aigrain 
and    Gibson  Bayesian quadrature for ratios  In Proc 
   Conf  Artif  Intell  Stat     

   Patel        Goldstein        Dyer    Mirhoseini  and
      Baraniuk  OASIS  Adaptive Column Sampling for
Kernel Matrix Approximation  arXiv   

   Paul     Ciosek        Osborne  and    Whiteson  Alternating Optimisation and Quadrature for Robust Reinforcement Learning  arXiv   

   Plaskota       Wasilkowski  and    Zhao  New averaging technique for approximating weighted integrals    
Complexity     

   Pr uher and     Simandl  Bayesian Quadrature in Nonlinear Filtering  In  th    Conf  Inform  Control Autom 
Robot   

      Rasmussen and    Ghahramani  Bayesian Monte

Carlo  In Adv  Neur  Inf  Proc  Sys   

   Robert and    Casella  Monte Carlo statistical methods 

Springer Science   Business Media   

      Robinson  An introduction to ordinary differential

equations  Cambridge University Press   

     arkk       Hartikainen     Svensson  and    Sandblom 
On the relation between Gaussian process quadratures
and sigmapoint methods  arXiv   

   Shi     Belkin  and    Yu 

Data spectroscopy 
Eigenspaces of convolution operators and clustering 
Ann  Statist     

   Simon  Trace Ideals and Their Applications  Cambridge

University Press   

   Smola     Gretton     Song  and    Sch olkopf   
Hilbert space embedding for distributions  In Algorithmic Learn  Theor  pages    

   Sommariva and    Vianello  Numerical cubature on
scattered data by radial basis functions  Computing   
   

      Temme  Special Functions  An Introduction to the
Classical Functions of Mathematical Physics  Wiley 
New York   

   Wendland  Scattered Data Approximation  Cambridge

University Press   

   Zhou        Johansen  and          Aston  Towards
Automatic Model Comparison  An Adaptive Sequential
Monte Carlo Approach     Comput  Graph  Statist   
   

