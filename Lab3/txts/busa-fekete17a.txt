Multiobjective Bandits  Optimizing the Generalized Gini Index

  obert BusaFekete   Bal azs Sz or enyi     Paul Weng     Shie Mannor  

Abstract

We study the multiarmed bandit  MAB  problem
where the agent receives   vectorial feedback that
encodes many possibly competing objectives to
be optimized  The goal of the agent is to  nd
  policy  which can optimize these objectives
simultaneously in   fair way  This multiobjective
online optimization problem is formalized by using the Generalized Gini Index  GGI  aggregation
function  We propose an online gradient descent
algorithm which exploits the convexity of the GGI
aggregation function  and controls the exploration
in   careful way achieving   distributionfree regret
       with high probability  We test our
algorithm on synthetic data as well as on an electric
battery control problem where the goal is to trade
off the use of the different cells of   battery in
order to balance their respective degradation rates 

  Introduction
The multiarmed bandit  MAB  problem  or bandit problem 
refers to an iterative decision making problem in which an
agent repeatedly chooses among   options  metaphorically
corresponding to pulling one of   arms of   bandit machine 
In each round  the agent receives   random payoff  which
is   reward or   cost that depends on the arm being selected 
The agent   goal is to optimize an evaluation metric       the
error rate  expected percentage of times   suboptimal arm is
played  or the cumulative regret  difference between the sum
of payoffs obtained and the  expected  payoffs that could
have been obtained by selecting the best arm in each round 
In the stochastic multiarmed bandit setup  the payoffs are
assumed to obey  xed distributions that can vary with the
arms but do not change with time  To achieve the desired goal 

 Equal contribution  Yahoo Research  New York  NY  USA
 Research Group on AI  Hungarian Acad  Sci  and Univ  of
Szeged  Szeged  Hungary  Technion Institute of Technology  Haifa 
Israel  SYSUCMU JIE  SEIT  SYSU  Guangzhou       China
 SYSUCMU JRI  Shunde       China  Correspondence to  Paul
Weng  paul weng fr 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

the agent has to tackle the classical exploration exploitation
dilemma  It has to properly balance the pulling of arms
that were found to yield low costs in earlier rounds and the
selection of arms that have not yet been tested often enough
 Auer et al      Lai   Robbins   
The bandit setup has become the standard modeling
framework for many practical applications  such as online
advertisement  Slivkins    or medical treatment design
 Press    to name   few  In these tasks  the feedback is
formulated as   single real value  However many realworld
online learning problems are rather multiobjective       the
feedback consists of   vectorial payoffs  For example  in our
motivating example  namely an electric battery control problem  the learner tries to discover    best  battery controller 
which balances the degradation rates of the battery cells      
components of   battery  among   set of controllers while
facing   stochastic power demand  Besides  there are several
studies published recently that consider multiobjective
sequential decision problem under uncertainty  Drugan  
Now      Roijers et al    Mahdavi et al   
In this paper  we formalize the multiobjective multiarmed
bandit setting where the feedback received by the agent is in
the form of   Ddimensional cost vector  The goal here is to
be both ef cient       minimize the cumulative cost for each
objective  and fair       balance the different objectives  One
natural way to ensure this is to try to  nd   cost vector on the
Pareto front that is closest to the origin or to some other ideal
point    generalization of this approach  when using the
in nite norm  is the Generalized Gini Index  GGI    wellknown inequality measure in economics  Weymark   
GGI is convex  which suggests applying the Online
Convex Optimization  OCO  techniques  Hazan   
ShalevShwartz    However    direct application of this
technique may fail to optimize GGI under noise  because
the objective can be only observed with   bias that is induced
by the randomness of the cost vectors and by the fact that the
performance is measured by the function value of the average
cost instead of the average of the costs  function value  The
solution we propose is an online learning algorithm which
is based on Online Gradient Descent  OGD   Zinkevich 
  Hazan    with additional exploration that enables
us to control the bias of the objective function  We also show
that its regret is almost optimal  up to   logarithmic factor 

Multiobjective Bandits  Optimizing the Generalized Gini Index

it matches the distributionfree lower bound of the stochastic
bandit problem  Auer et al      which naturally applies
to our setup when the feedback is onedimensional 
The paper is organized as follows  after we introduce the
formal learning setup  we brie   recall the necessary notions
from multiobjective optimization in Section   Next  in
Section   GGI is introduced and some of its properties are
described  In Sections   and   we present how to compute
the optimal policy for GGI  and de ne the regret notion 
Section   contains our main results where we de ne our
OGDbased algorithm and analyze its regret  In Section
  we test our algorithm and demonstrate its versatility in
synthetic and realworld batterycontrol experiments  In
Section   we provide   survey of related work  and  nally
conclude the paper in Section  

  Formal setup
The multiarmed or Karmed bandit problem is speci ed
by realvalued random variables            XK associated 
respectively  with   arms  that we simply identify by
the numbers             
In each time step    the online
learner selects one and obtains   random sample of the
corresponding distributions  These samples  which are
called costs  are assumed to be independent of all previous
actions and costs  The goal of the learner can be de ned
in different ways  such as minimizing the sum of costs over
time  Lai   Robbins    Auer et al     
In the multiobjective multiarmed bandit  MOMAB 
problem  costs are not scalar real values  but real vectors 
More speci cally    Dobjective Karmed bandit problem
             is speci ed by   realvalued multivariate random variables            XK over       Let
       Xk  denote the expected vectorial cost of arm  
where                        Furthermore    denotes the
matrix whose rows are the      
In each time step the learner can select one of the arms
and obtain   sample  which is   cost vector  from the
corresponding distribution  Samples are also assumed
to be independent over time and across the arms  but not
necessarily across the components of cost vectors  At time
step    kt denotes the index of the arm played by the learner
and     
kt    the resulting payoff  After
kt
playing   time steps  the empirical estimate of the expected
cost    of the kth arm is 

kt             

        

 Our setup is motivated by   practical application where
feedback is more naturally formulated in terms of cost  However the
stochastic bandit problem is generally based on rewards  which can
be easily turned into costs by using the transformation          
assuming that the rewards are from    

 

Tk   

tX   

    
  

        

 

   

    

where all operations are meant elementwise  Tk    is the
number of times the kth arm has been played       Tk     

Pt
             and   is the indicator function 

  Multiobjective optimization
In order to complete the MOMAB setting  we need to
introduce the notion of optimality of the arms  First  we
introduce the Pareto dominance relation   de ned as
follows  for any         RD 

                           vd        

 

 

Let    RD be   set of Ddimension vectors  The Pareto
front of    denoted    is the set of vectors such that 
                                 

In multiobjective optimization  one usually wants to
compute the Pareto front  or search for   particular element
of the Pareto front  In practice  it may be costly  and even
infeasible depending on the size of the solution space  to
determine all the solutions of the Pareto front  One may
then prefer to directly aim for   particular solution in the
Pareto front  This problem is formalized as   single objective
optimization problem  using an aggregation function 
An aggregation  or scalarizing  function  which is  
nondecreasing function     RD      allows every vector
to receive   scalar value to be optimized  The initial
multiobjective problem is then rewritten as follows 

min    

           

 

  solution to this problem yields   particular solution on the
Pareto front  Note that if   is not strictly increasing in every
component  some care is needed to ensure that the solution
of   is on the Pareto front 
Different aggregation function can be used depending on
the problem at hand  such as sum  weighted sum  min 
max   augmented  weighted Chebyshev norm  Steuer  
Choo    Ordered Weighted Averages  OWA   Yager 
  or Ordered Weighted Regret  OWR   Ogryczak et al 
  and its weighted version  Ogryczak et al   
In this study  we focus on the Generalized Gini Index
 GGI   Weymark      special case of OWA 

   multivariate function     RD     is said to be monotone
 nondecreasing  if for all  xed         RD such that       
implies that              

Multiobjective Bandits  Optimizing the Generalized Gini Index

  Generalized Gini Index
For   given            denotes the set                The
Generalized Gini Index  GGI   Weymark    is de ned
as follows for   cost vector                 xD    RD 

Gw     

wdx          

DXd 

is the permuwhere     SD  which depends on   
tation that sorts the components of   in   decreasing
order                  is the sorted vector
and weights wi   are assumed to be nonincreasing 
                       wD  Given this assumption 
Gw      max SD        max SD   
   and is
therefore   piecewiselinear convex function  Figure  
illustrates GGI on   biobjective optimization task 
To better understand GGI  we introduce its formulation in
terms of Lorenz vectors  The Lorenz vector of   is the vector
                     LD    where Ld    is the sum of the
  smallest components of    Then  GGI can be rewritten as
follows 

Gw     

DXd 

  dLd   

 

where                       is the vector de ned by
               wd   wd  with wD      Note that all
the components of    are nonnegative as we assume that
those of   are nonincreasing 
GGI  was originally introduced for quantifying the inequality of income distribution in economics  It is also known in
statistics  Buczolich   Sz ekely    as   special case of
Weighted Average Ordered Sample statistics  which does not
require that weights be nonincreasing and is therefore not
necessarily convex  GGI has been characterized by Weymark
  It encodes both ef ciency as it is monotone with
Pareto dominance and fairness as it is nonincreasing with
PigouDalton transfers     they are two principles
formulating natural requirements  which is an important
reason why GGI became   wellestablished measure of balancedness  Informally    PigouDalton transfer amounts to
increasing   lowervalued objective while decreasing another
highervalued objective by the same quantity such that the order between the two objectives is not reversed  The effect of
such   transfer is to balance   cost vector  Formally  GGI satis es the following fairness property     such that xi   xj 

      xj   xi  Gw      ei    ej    Gw   

where ei and ej are two vectors of the canonical basis 
As   consequence  among vectors of equal sum  the best

 Note that in this paper GGI is expressed in terms of costs and

therefore lower GGI values are preferred 

Figure   Point   is always preferred         GGI  to   due to Pareto
dominance    is always preferred to   due to the PigouDalton
transfer principle  depending on the weights of GGI    may be
preferred to   or the other way around  with           is
preferred to    points equivalent to   are on the dashed green line 
The optimal mixed solution is the black dot 

cost vector         GGI  is the one with equal values in all
objectives if feasible 
The original Gini index can be recovered as   special case
of GGI by setting the weights as follows 

         wd                

 

This yields   nice graphical interpretation  For    xed
vector   
the distribution of costs can be represented
as   curve connecting the points             
                   LD    An ideally fair distribution
with an identical total cost is given by the straight line connecting         LD           LD             
  LD    which equally distributes the total cost over all
components  Then     Gw     with weights   and     
  xi   is equal to twice the area between the two curves 

From now on  to simplify the presentation  we focus on GGI
with strictly decreasing weights in                   implies wd   wd  This means that GGI is strictly decreasing
with PigouDalton transfers and all the components of   
are positive  Based on formulation   Ogryczak   Sliwinski   showed that the GGI value of   vector   can be
obtained by solving   linear program  We shall recall their results and de ne the linear programbased formulation of GGI 
Proposition   The GGI score Gw    of vector   is the
optimal value of the following linear program

DXd 

    drd 
DPj 
minimize
subject to rd   bj     xj
   

bj  

bj   

           
           

Multiobjective Bandits  Optimizing the Generalized Gini Index

  Optimal policy
In the single objective case  arms are compared in terms of
their means  which induce   total ordering over arms  In the
multiobjective setting  we use the GGI criterion to compare
arms  One can compute the GGI score of each arm   as
Gw    if its vectorial mean    is known  Then an optimal
arm    minimizes the GGI score      

     argmin
    

Gw     

However  in this work  we also consider mixed strategies 
which can be de ned as         RK PK
       
          because they may allow to reach lower
GGI values than any  xed arm  see Figure     policy
parameterized by   chooses arm   with probability     An
optimal mixed policy can then be obtained as follows 

    argmin
  

In general  Gw PK

using mixed strategies is justi ed in our setting  Based on
Proposition   if the arms  means were known    could be
computed by solving the following linear program 

 

       

Gw  KXk 
           Gw    therefore
    drd  
KXk 
PK

bj    

           

           

         

bj      

DXd 

DXj 

     

 

minimize

subject to rd   bj    

      

  Regret
After playing   rounds  the average cost can be written as

        

 
 

    
kt

 

TXt 

Our goal is to minimize the GGI index of this term  Accordingly we expect the learner to collect costs so as their average

possible  As shown in the previous section  for   given bandit
instance with arm means                   the optimal

in terms of GGI  that is  Gw         should be as small as
           Gw   if
policy   achieves Gw PK

the randomness of the costs are not taken into account  We
consider the performance of the optimal policy as   reference
value  and de ne the regret of the learner as the difference of
the GGI of its average cost and the GGI of the optimal policy 

        Gw           Gw    

 

Note that GGI is   continuous function  therefore if the
learner follows   policy      that is  approaching    as
      then the regret is vanishing 
We shall also investigate   slightly different regret notion
called pseudoregret 

 

    

 

  Gw         Gw  
  PT

where         
       We will show that the
difference between the regret and pseudoregret of our
algorithm is        with high probability  thus having
  high probability regret bound        for one of them
implies   regret bound        for the other one 
Remark   The single objective stochastic multiarmed
bandit problem  Auer et al      Bubeck   CesaBianchi 
  can be naturally accommodated into our setup with
      and        In this case    implements the pure
strategy that always pulls the optimal arm with the highest
mean denoted by     Thus Gw         in this case 
Assuming that the learner plays only with pure strategies 
the pseudoregret de ned in   can be written as 

TXt 

KXk 

    

 

 

 
 

 kt          

 
 

Tk             

kt into   single real value Gw     

which coincides with the single objective pseudoregret
 see for example  Auer et al      apart from the fact
that we work with costs instead of rewards  Therefore our
notion of multiobjective pseudoregret can be seen as  
generalization of the single objective pseudoregret 
Remark   Singleobjective bandit algorithm can be applied in our multiobjective setting by transforming the multivariate payoffs     
  in
every time step    However  in general  this approach fails to
optimize GGI as formulated in   due to GGI   nonlinearity 
even if the optimal policy is   pure strategy  Moreover  applying   multiobjective bandit algorithm such as MOUCB
 Drugan   Now      would be inef cient as they were
developed to  nd all Paretooptimal arms and then to sample
them uniformly at random  This approach may be reasonable to apply only when         where         
             contains all the Pareto optimal arms  which
is clearly not the case for every MOMAB instance 

kt

  Learning algorithm based on OCO
In this section we propose an online learning algorithm called
MOOGDE  to optimize the regret de ned in the previous
section  Our method exploits the convexity of the GGI operator and formalizes the policy search problem as an online convex optimization problem  which is solved by Online Gradient Descent  OGD   Zinkevich    algorithm with projection to   gradually expanding truncated probability simplex 

Multiobjective Bandits  Optimizing the Generalized Gini Index

Algorithm   MOOGDE 
  Pull each arm once
  Set                
  for rounds                       do
Choose an arm kt according to    
 
Observe the sample     
 
Set     
      OGDEstep      rf    

 pK   ln 

 
 

  

 

return  

      

kt and compute      

  PT

Then we shall provide   regret analysis of our method  Due
to space limitation  the proofs are deferred to the appendix 

  MOOGDE
Our objective function to be minimized can be viewed as  
function of              Gw  where the matrix    
              contains the means of the arm distributions as
its columns  Note that the convexity of GGI implies the convexity of     Since we play with mixed strategies  the domain of our optimization problem is the Kdimensional probability simplex          RK PK
                 
which is   convex set  Then the gradient of     with respect
 PD
to    can be computed as     
   wd     where
  
  is the permutation that sorts the components of   in
  decreasing order  The means      are not known but
they can be estimated based on the costs observed so far as
given in   The objective function based on the empirical

               

    contains the empirical estimates for

mean estimates is denoted by         Gw      where
            

             in time step   as its columns 
Our MultiObjective Online Gradient Descent algorithm with
Exploration is de ned in Algorithm   which we shall refer
to as MOOGDE  Our algorithm is based on the wellknown
Online Gradient Descent  Zinkevich    Hazan   
that carries out the gradient step and the projection back onto
the domain in each round  The MOOGDE algorithm  rst
pulls each arm at once as an initialization step  Then in each
iteration  it chooses each arm   with probability    
    and it
computes       based on the empirical mean estimates  Next 
it carries out the gradient step based on rf       with  
step size    as de ned in line   and computes the projection
 

onto the nearest point of the convex set 

 

      RK 

 

KXk 

               

with         The gradient step and projection are carried
out using

OGDEstep         

 

       

 

The key ingredient of MOOGDE is the projection step
onto the truncated probability simplex   
  which ensures
that    
        for every         and          This
forced exploration is indispensable in our setup  since the
objective function     depends on the means of the arm
distributions  which are not known  To control the difference
between     and       we need  good  estimates for
              which are obtained via forced exploration 
That is why  the original OGD  Hazan    algorithm  in
general  fails to optimize GGI in our setting  Our analysis 
presented in the next section  focuses on the interplay
between the forced exploration and the bias of the loss
functions                   

Gw        Gw  is of order mink Tk   

  Regret analysis
The technical dif culty in optimizing GGI in an online fashion is that 
in general               
which  unless all the arms are sampled   linear number of
times  incurs   regret of the same order of magnitude  which
is typically too large  Nevertheless  an optimal policy  
determines   convex combination of several arms in the
form of   which is the optimal cost vector in terms
of GGI given the arm distribution at hand  Let us denote
                      Note that        Moreover 
arms in         with an individual GGI lower than arms in
  do not necessarily participate in the optimal combination 
Obviously  an algorithm that achieves         needs
to pull the arms in    linear time  and at the same time 
estimate the arms in         with   certain precision 
The main idea in the approach proposed in this paper
is  despite the above remarks 
to apply some online
convex optimization algorithm on the current esti 

  PT

          

mate         Gw      of the objective function

      Gw  use forced exploration of order     and
 nally show that the estimate       of the objective function
has error        along the trajectory generated by the
online convex optimization algorithm  In particular  we show
  PT
                   
that      
  PT
The intuitive reason for this is that    
          
  PT
                 which is based on the
following observation  an arm in   is either pulled often 
thus its mean estimate is then accurate enough  or an arm in
        is pulled only   few times  neverthelessPT
      
 
is then small enough to make the poor accuracy of its mean
estimate insigni cant  Below we make this argument formal
by proving the following theorem 
Theorem   With probability at least      
    

             Lq    ln DKT  

for any big enough     where   is the Lipschitz constant of

TXt 

 

 

 

 

Multiobjective Bandits  Optimizing the Generalized Gini Index

Gw   

Its proof follows four subsequent steps presented next 
Step   As   point of departure  we analyze OGDEstep in
  In particular  we compute the regret that is commonly
used in OCO setup for        

Lemma   De ne        as 

                   
      OGDEstep      rf    

with                   Then the following upper bound
is guaranteed for all       and for any        
TXt 
      
where sup   krf            pKD for all         

         

       

TXt 

TXt 

 
  

  

 

 

The proof of Lemma   is presented in Appendix    If the
projection is carried out onto    according to the OGE
algorithm instead of the truncated probability   
   the regret
bound in Lemma   would be improved only by   constant
factor  see Theorem   in  Hazan    As   side remark 
note that Lemma   holds for arbitrary convex function since
only the convexity of       is used in the proof 
Step   Next  we show that       converges to     as fast

as        along the trajectory        generated
by MOOGDE 
Proposition   With probability at least      DT      
TXt       
 
Gw   
  Ls       ln      ln 

      Gw   

TXt 

 

 

 

 

The proof of Proposition   is deferred to Appendix   
Proposition   combined with the fact that

Gw   

 

TXt       
TXt 

 
 

 
 

TXt 

the

used

convexity

       

of GGI 

  PT

 
where we

             

Gw         
               
  PT
          Ls       ln      ln  

Gw   
implies the following result 
Corollary   With probability at least      DT      
        

TXt 

and

 
 

 

 

 

Step   Next  we provide   regret bound for the pseudoregret
of MOOGDE by using Lemma   and Corollary   To this
end  we introduce some further notations  First of all  let

     argmin
  

   

 

 

 

 

 

    

   

LK
 

 
    

KD     

TXt 

TXt 

denote the set of all the minimum points of   over     As
we show later in Lemma      is   convex polytope  and
thus the set ext    of its extreme points is  nite 
Proposition   With probability at least      DT    
    

            Ls       ln      ln 
TXt     ln 
where                      ta ext   
iq  ln  
and       ext           LKD 
   

with
 
     inf     
The proof is deferred to Appendix    In the proof  rst
we decompose the regret into various terms which can be
upper bounded based on Lemma   and Corollary   This
implies that  
  will get arbitrarily close to some
        if   is big enough because  as we show later
in Lemma      is strictly positive  see Appendix    As
  consequence  for   big enough       the difference
between the       and     is vanishing as fast as
      which is crucial for   regret bound of order    
Step   Finally  Theorem   yields from Proposition   by
simplifying the right hand side  The calculation is presented
in Appendix   

min ext     mink      

      
   

  PT

max  

      

and

  

  

 

  Regret vs  pseudoregret
Next  we upperbound the difference of regret de ned in  
and the pseudoregret de ned in   To that aim  we  rst

Claim   For any               and any                 it

upperbound the difference of Tk    andPt
holds that Ph Tk     Pt
it holds that
Tk      Pt
    Pt
    is
  martingale  Besides                   by construction  The claim then follows by Azuma   inequality 

         ln      

As   kt           
       

                

       
   

       

Proof  

Based on Claim   and Prop    we upperbound the difference
between the pseudoregret and regret of MOOGDE 

Multiobjective Bandits  Optimizing the Generalized Gini Index

Corollary   With probability at least      

                Lr    ln DT    

 

The proof of Corollary   is deferred to Appendix   
According to Corollary   the difference between the regret
and pseudo regret is        with high probability  hence
Theorem   implies          bound for the regret of
MOOGDE 

  Experiments
To test our algorithm  we carried out two sets of experiments 
In the  rst we generated synthetic data from multiobjective
bandit instances with known parameters  In this way  we
could compute the pseudoregret   and  thus investigate
the empirical performance of the algorithms  In the second
set of experiments  we run our algorithm on   complex
multiobjective online optimization problem  namely an
electric battery control problem  Before presenting those
experiments  we introduce another algorithm that will serve
as   baseline 

    baseline method
In the previous section we introduced   gradientbased
approach that uses the mean estimates to approximate the
gradient of the objective function  Nevertheless  using
the mean estimates  the optimal policy can be directly
approximated by solving the the linear program given in
  We use the same exploration as MOOGDE  see line
  of Algorithm   More concretely  the learner solves the
following linear program in each time step   

Note that the solution of the learner program above regarding
  is in   
   We refer to this algorithm as MOLP  Note that
this approach is computationally expensive  since   linear
program needs to be solved at each time step  But the policy
of each step is always optimal restricted to the truncated
simplex   
  with respect to the mean estimates  unlike the
gradient descent method 

  Synthetic Experiments
We generated random multiobjective bandit instances for
which each component of the multivariate cost distributions

minimize

DXd 

subject to rd   bj    
        
        
bj      

    drd  
KXk 

bj    
DXj 
 kb   

   

           

           

Figure   The regret of the MOLP and MOOGDE  The regret and
its error is computed based on   repetitions and plotted in terms
of the number of rounds  The dimension of the arm distributions
was set to         which is indicated in the title of the panels 

obeys Bernoulli distributions with various parameters  The
parameters of each Bernoulli distributions are drawn uniformly at random from     independently from each other 
The number of arms   was set to     and the dimension
of the cost distribution was taken from         The
weight vector   of GGI was set to wd       Since
the parameters of the bandit instance are known  the regret
de ned in Section   can be computed  We ran the MOOGDE and MOLP algorithms with   repetitions  The
multiobjective bandit instance were regenerated after each
run  The regrets of the two algorithms  which are averaged
out over the repetitions  are plotted in Figure   along with the
error bars  The results reveal some general trends  First  the
average regrets of both algorithms converge to zero  Second
the MOLP algorithm outperforms the gradient descent algorithm for small number of round  typically       on the
more complex bandit instances        This fact might be
explained by the fact that the MOLP solves   linear program
for estimating   whereas the MOOGDE minimizes the
same objective but using   gradient descent approach with
projection  which might achieve slower convergence in terms
of regret  nevertheless its computational time is signi cantly
decreased compared to the baseline method 

  Battery control task
We also tried our algorithms on   more realistic domain  the
cell balancing problem  As the performance pro le of battery
cells  subcomponents of an electric battery  may vary due
to small physical and manufacturing differences  ef cient
balancing of those cells is needed for better performance and
longer battery life  We model this problem as   MOMAB
where the arms are different cell control strategies and the

Multiobjective Bandits  Optimizing the Generalized Gini Index

         

 

tPt

         

 

  PT

                   

goal of the learner is then to minimizePt

In the online convex optimization setup with multiple
objectives  Mahdavi et al    the learner   forecast
     is evaluated in terms of multiple convex loss functions
     
           
      in each time step    The
     
while keeping the other objectives below some prede ned
threshold        
           for all        
Note that  with linear loss functions  the multipleobjective
convex optimization setup boils down to linear optimization
with stochastic constraints  and thus it can be applied to solve
the linear program given in Section   whose solution is the
optimal policy in our setup  For doing this  however  each
linear stochastic constraint needs to be observed  whereas
we only assume bandit information 
In the approachability problem  Mannor et al     
Abernethy et al    there are two players  say   and   
Players   and   choose actions from the compact convex
sets    RK and    RD  respectively  The feedback to
the players is computed as   function             Rp   
given convex set     Rp is known to both players  Player
  wants to land inside with the cumulative payoffs      
player     goal is to minimize dist   
                 
where      and      are the actions chosen by player   and
  respectively  and dist         inf     ks    sk  whereas
player    called adversary  wants to prevent player   to
land in set    In our setup  Player   who generates the cost
vectors  is assumed to be stochastic  The set   consists of  
single value which is   and   corresponds to  Ibalpha 
thus        What makes our setup essentially different
from approachability is that    is not known to any player 
That is why Player         the learner  needs to explore the
action space which is achieved by forced exploration 
  Conclusion and future work
We introduced   new problem in the context of multiobjective multiarmed bandit  MOMAB  Contrary to
most previously proposed approaches in MOMAB  we
do not search for the Pareto front  instead we aim for  
fair solution  which is important for instance when each
objective corresponds to the payoff of   different agent  To
encode fairness  we use the Generalized Gini Index  GGI 
  wellknown criterion developed in economics  To optimize
this criterion  we proposed   gradientbased algorithm that
exploits the convexity of GGI  We evaluated our algorithm on
two domains and obtained promising experimental results 
Several multiobjective reinforcement learning algorithm
have been proposed in the literature    abor et al   
Roijers et al    Most of these methods make use of
  simple linear aggregation function  As   future work  it
would be interesting to extend our work to the reinforcement
learning setting  which would be useful to solve the electric
battery control problem even more  nely 

Figure   The regret of the MOOGDE and MOLP on the battery
control task  The regret is averaged over   repetitions and
plotted in terms of the number of rounds  The dimension of the arm
distributions was      

goal is to balance several objectives  stateof charge  SOC 
temperature and aging  More concretely  the learner loops
over the following two steps    she chooses   control
strategy for   short duration and   she observes its effect
on the objectives  due to stochastic electric consumption 
For the technical details of this experiments see Appendix   
We tackled this problem as   GGI optimization problem  The
results  averaged over   runs  are presented in Figure  
where we evaluated MOOGDE vs  MOLP  The dashed
green lines represent the regrets of playing  xed deterministic arms  Although MOOGDE and MOLP both learn to
play   mixed policy that is greatly better than any individual
arm  MOOGDE is computationally much more ef cient 

  Related work
The singleobjective MAB problem has been intensively
studied especially in recent years  Bubeck   CesaBianchi 
  nevertheless there is only   very limited number of
work concerning the multiobjective setting  To the best of
our best knowledge  Drugan   Now     considered  rst
the multiobjective multiarmed problem in   regret optimization framework with   stochastic assumption  Their work
consists of extending the UCB algorithm  Auer et al     
so as to be able to handle multidimensional feedback vectors
with the goal of determining all arms on the Pareto front 
Azar et al    investigated   sequential decision making
problem with vectorial feedback  In their setup the agent
is allowed to choose from    nite set of actions and then it
observes the vectorial feedback for each action  thus it is   full
information setup unlike our setup  Moreover  the feedback
is nonstochastic in their setup  as it is chosen by an adversary 
They propose an algorithm that can handle   general class
of aggregation functions  such as the set of bounded domain 
continuous  Lipschitz and quasiconcave functions 

Multiobjective Bandits  Optimizing the Generalized Gini Index

Acknowledgements
The authors would like to thank Vikram Bhattacharjee and
Orkun Karabasoglu for providing the battery model  This
research was supported in part by the European Communitys Seventh Framework Programme  FP  under
grant agreement    SUPREL 

References
Abernethy  Jacob    Bartlett  Peter    and Hazan  Elad 
Blackwell approachability and noregret learning are
equivalent  In COLT     The  th Annual Conference
on Learning Theory  June     Budapest  Hungary 
pp     

Auer     CesaBianchi     and Fischer     Finitetime
analysis of the multiarmed bandit problem  Machine
Learning       

Auer     CesaBianchi     Freund     and Schapire      
The nonstochastic multiarmed bandit problem  SIAM
Journal on Computing       

Azar  Yossi  Feige  Uriel  Feldman  Michal  and Tennenholtz 
Moshe  Sequential decision making with vector outcomes 
In ITCS  pp     

Boyd  Stephen and Vandenberghe  Lieven 

Convex
Optimization  Cambridge University Press  New York 
NY  USA   

Bubeck     and CesaBianchi     Regret analysis of
stochastic and nonstochastic multiarmed bandit problems 
Foundations and Trends in Machine Learning   
 

Buczolich  Zolt an and Sz ekely    abor    When is  
weighted average of ordered sample elements   maximum
likelihood estimator of the location parameter  Advances
in Applied Mathematics         

Dalton     The measurement of inequality of incomes 

Economic       

Dambrowski     Review on methods of stateof charge estimation with viewpoint to the modern LiFePO Li Ti   
lithiumion systems  In International Telecommunication
Energy Conference   

Drugan        and Now       Designing multiobjective
multiarmed bandits algorithms    study  In IJCNN  pp 
   

Gao  Lijun  Chen  Shenyi  and Dougal  Roger    Dynamic
lithiumion battery model for system simulation  IEEE
Trans  on Components and Packaging Technologies   
   

Hazan  Elad  Introduction to Online Convex Optimization 

NOW   

Johnson       Battery performance models in ADVISOR 

Journal of Power Sources     

Lai        and Robbins  Herbert  Asymptotically ef 
cient adaptive allocation rules  Advances in Applied
Mathematics     

Mahdavi  Mehrdad  Yang  Tianbao  and Jin  Rong  Stochastic convex optimization with multiple objectives  In NIPS 
pp     

Mannor  Shie  Tsitsiklis  John    and Yu  Jia Yuan  Online
learning with sample path constraints 
Journal of
Machine Learning Research      doi 
 

Mannor  Shie  Perchet  Vianney  and Stoltz  Gilles  Approachability in unknown games  Online learning meets
multiobjective optimization  In Proceedings of The  th
Conference on Learning Theory  COLT   Barcelona 
Spain  June     pp     

Ogryczak     and Sliwinski     On solving linear programs
with the ordered weighted averaging objective  Eur    
Operational Research     

Ogryczak     Perny     and Weng     On minimizing
ordered weighted regrets in multiobjective Markov
decision processes  In ADT  Lecture Notes in Arti cial
Intelligence   

Ogryczak     Perny     and Weng       compromise
programming approach to multiobjective Markov
decision processes  International Journal of Information
Technology   Decision Making     

Pigou     Wealth and Welfare  Macmillan   

Press       Bandit solutions provide uni ed ethical models
for randomized clinical trials and comparative effectiveness research  PNAS     

Roijers  Diederik    Vamplew  Peter  Whiteson  Shimon 
and Dazeley  Richard    survey of multiobjective
sequential decisionmaking 
   Artif  Intell  Res   
   

  abor  Zolt an  Kalm ar  Zsolt  and Szepesv ari  Csaba 
In ICML  pp 

Multicriteria reinforcement learning 
   

ShalevShwartz  Shai  Online learning and online convex
Foundations and Trends in Machine

optimization 
Learning     

Multiobjective Bandits  Optimizing the Generalized Gini Index

Slivkins     Contextual bandits with similarity information 

   Mach  Learn  Res     

Steuer       and Choo       An interactive weighted
Tchebycheff procedure for multiple objective programming  Mathematical Programming     

Tao  Gao  Research on LiMn    battery   state of charge estimation with the consideration of degradation  Technical
report  Tsinghua University   

Weymark  John    Generalized gini inequality indices 

Mathematical Social Sciences         

Yager       On ordered weighted averaging aggregation
operators in multicriteria decision making  IEEE Trans 
on Syst  Man and Cyb     

Zinkevich  Martin  Online convex programming and
generalized in nitesimal gradient ascent  In ICML   

