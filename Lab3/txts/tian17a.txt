An Analytical Formula of Population Gradient for twolayered ReLU network

and its Applications in Convergence and Critical Point Analysis

Yuandong Tian  

Abstract

      
 

work           PK

In this paper  we explore theoretical properties of
training   twolayered ReLU netx  with centered ddimensional spherical Gaussian input  
 ReLU  We train our network with gradient
descent on   to mimic the output of   teacher
network with the same architecture and  xed parameters    We show that its population gradient has an analytical formula  leading to interesting theoretical analysis of critical points and
convergence behaviors  First  we prove that critical points outside the hyperplane spanned by the
teacher parameters  outof plane  are not isolated and form manifolds  and characterize inplane criticalpoint free regions for two ReLU
case  On the other hand  convergence to    for
one ReLU node is guaranteed with at least    
  probability  if weights are initialized randomly with standard deviation upperbounded by
     consistent with empirical practice  For

network with many ReLU nodes  we prove that
an in nitesimal perturbation of weight initialization results in convergence towards     or its
permutation    phenomenon known as spontaneous symmetricbreaking  SSB  in physics  We
assume no independence of ReLU activations 
Simulation veri es our  ndings 

  Introduction

Despite empirical success of deep learning       Computer
Vision  He et al    Simonyan   Zisserman   
Szegedy et al    Krizhevsky et al    Natural
Language Processing  Sutskever et al    and Speech
Recognition  Hinton et al    it remains elusive how

 Facebook AI Research  Correspondence to  Yuandong Tian

 yuandong fb com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

and why simple methods like gradient descent can solve
the complicated nonconvex optimization during training 
In this paper  we focus on   twolayered ReLU network 

         

 

Xj 

   
 

  

 

Here       max      is the ReLU nonlinearity  We consider the setting that   student network is optimized to minimize the    distance between its prediction and the supervision provided by   teacher network of the same architecture with  xed parameters    Note that although the
network prediction  Eqn    is convex  when coupled with
loss          loss Eqn    the optimization becomes highly
nonconvex and has exponential number of critical points 

To analyze it  we introduce   simple analytic formula for
population gradient in the case of    loss  when inputs   are
sampled from zeromean spherical Gaussian  Using this
formula  critical point and convergence analysis follow 

For critical points  we show that critical points outside the
principal hyperplane  the subspace spanned by    form
manifolds  We also characterize the region in the principal
hyperplane that has no critical points  in two ReLU case 

We also analyze the convergence behavior under the population gradient  Using Lyapunov method  LaSalle   Lefschetz    for single ReLU case we prove that gra 

dient descent converges to    with at least      
upperbounded by      verifying common initializa 

probability  if initialized randomly with standard deviation

tion techniques  Bottou    Glorot   Bengio    He
et al    LeCun et al    For multiple ReLU case 
when the teacher parameters  wj  
   form an orthonormal basis  we prove that     symmetric weight initialization gets stuck at   saddle point and     particular
in nitesimal perturbation of   leads to convergence towards    or its permutation  The behavior that the population gradient  eld is invariant under certain symmetry but
the solution breaks it  is known as spontaneous symmetry
breaking in physics  Although such behaviors are known
practically  to our knowledge  we  rst formally characterize them in  layered ReLU network  Codes are available  

 github com yuandongtian ICML ReLU

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

   

Student	
Network

 

Teacher
Network

 

 

 

   

 

 

 

 
wj

 

Figure       We consider the student and teacher network as nonlinear neural networks with ReLU nonlinearity  The student network updates its weight   from the output of the teacher with
    The  layered ReLU network structure
 xed weights  
 Eqn    discussed in this paper  The  rst layer contains  xed
weights of value   while the second layers has   ReLU nodes 
Each node   has   ddimensional weight wj to be optimized 
Teacher network has the same architecture as the student 

 

  Related Works

linear network  many works

For multilayer
analyze its critical points and convergence behaviors 
 Saxe et al    analyzes its dynamics of gradient descent and  Kawaguchi    shows every local minimum
is global  On the other hand  very few theoretical works
have been done for nonlinear networks   Mei et al   
shows the global convergence for   single nonlinear node
whose derivatives of activation       are bounded and
      Similar to our approach   Saad   Solla    also
uses the studentteacher setting and analyzes the student
dynamics when the teacher   parameters    are orthonormal  However  their activation is Gaussian error function
erf    and only the local behaviors of the two critical
points  the initial saddle point near the origin and    are
analyzed  Recent paper  Zhang et al    analyzes  
similar teacherstudent setting on  layered network when
the involved function is harmonic  but it is unclear how the
conclusion is generalized to ReLU case  To our knowledge  our closeform formula for  layered ReLU network
is novel  as well as the critical point and convergence
analysis  Concurrent work  Brutzkus   Globerson   
proposes the same formula with   different approach 
and provides similar convergence analysis for one node 
For multiple nodes  they assume nonoverlapping shared
weights    special case of our assumption  Sec    that
weights are cyclically symmetric and orthonormal 

Many previous works analyze nonlinear network based
on the assumption of independent activations  the activations of ReLU  or other nonlinear  nodes are independent
of the input and or mutually independent  For example 
 Choromanska et al        relates the nonlinear ReLU
network with spinglass models when several assumptions
hold  including the assumption of independent activations
     and       Kawaguchi    proves that every local
minimum in nonlinear network is global based on similar
assumptions   Soudry   Carmon    shows the global

optimality of the local minimum in   twolayered ReLU
network  when independent multiplicative Bernoulli noise
is applied to the activations 
In practice  activations that
share the input are highly dependent  Ignoring such dependency misses important behaviors  and may lead to misleading conclusions  In this paper  no assumption of independent activations is made  Instead  we assume input to
follow spherical Gaussian distribution  which gives more
realistic and interdependent activations during training 

For sigmoid activation   Fukumizu   Amari    gives
complicated conditions for   local minimum to be
global when adding   new node to    layered network 
 Janzamin et al    gives guarantees for parameter recovery of    layered network learnt with tensor decomposition  In comparison  we analyze ReLU networks trained
with gradient descent  which is more popular in practice 

  Problem De nition

Denote   as the number of samples and   as the input dimension  The    byd matrix   is the input data and   
is the  xed parameter of the teacher network  Given the
current estimation    we have the following    loss 

      

 
 kg                  

 

Here we focus on population loss EX     where the input   is assumed to follow spherical Gaussian distribution       
Its gradient is the population gradient
EX  Jw     abbrev        In this paper  we study
critical points           and vanilla gradient dynamics
wt    wt         wt  where   is the learning rate 

  The Analytical Formula

Properties of ReLU  ReLU nonlinearity has useful properties  We de ne the gating function        diag Xw  
  as an    byN binary diagonal matrix  Its lth diagonal
element is   binary variable showing whether the neuron
is activated for sample    Using this notation   Xw   
    Xw which means      selects the output of   linear neuron  based on their activations  Note that      only
depends on the direction of   but not its magnitude 

     is also  transparent  with respect to derivatives  For
example  at differentiable regions  Jacobianw Xw   
 Xw            This gives   very concise rule for
gradient descent update in ReLU networks 

One ReLU node  Given the properties of ReLU  the population gradient       can be written as 
        EX               Xw       Xw 
Intuitively  this term vanishes when        and should

 

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

          if the data are evenly distributed 

be around  
since roughly half of the samples are blocked  However 
such an estimation fails to capture the nonlinear behavior 
If we de ne Population Gating  PG  function           
           Xw  then       can be written as 
                kwk               kwk        

Interestingly           has an analytic formula if the data
  follow spherical Gaussian distribution 

Theorem   Denote                       Xw where
  is   unit vector               xN   is the    byd
data matrix and        diag Xw     is   binary diagonal matrix  If xi           and thus biasfree  then 

             

 
 

         kwk sin    

 

where                  is the angle between   and   
See the link  for the proof of all theorems  Note that we
do not require   to be independent between samples  Intuitively  the  rst mass term  
       aligns with   and is
proportional to the amount of activated data whose ReLU
are on  When       the gating function is fully on and half
of the data contribute to the term  when       the gating
function is completely switched off  The gate is controlled
by the angle between   and the control signal    The second asymmetric term is aligned with    and is proportional
to the asymmetry of the activated data samples  Fig   

 

 
 

Note that the expectation analysis smooths out ReLU and
leaves only one singularity at the origin  where       is
not continuous  That is  if approaching from different directions towards             is different 
With the close form of           also has   close form 
sin    cid   
       
where                  The  rst term is from linear

   cid      kw  
kwk

     

approximation  while the second term shows the nonlinear
behavior 
For linear case         no gating  and thus     
           
For spherical Gaussian input   
EX            and                Therefore  the dy 

namics has only one critical point and global convergence
follows  which is consistent with its convex nature 

Extension to other distributions  From its de nition 
                            Xw  is linear to kwk 
regardless of the distribution of    On the other hand 
isotropy in spherical Gaussian distribution leads to the fact

 http yuandongtian com ssbsupp pdf

mass term

asymmetric term

 

 

 

 

 

 

 

   

 

 

 

 

Data	with
ReLU gating	on

      

sin  kwke

Figure   Decomposition of Population Gating  PG  function
          Eqn    into mass term and asymmetric term          
is computed from the portion of data with ReLU gate on  The
mass term is proportional to the amount of data  while the asymmetric term is related to the data asymmetry with respect to   

that             only depends on angles between vectors 
For other isotropic distributions  we could similarly derive 

                    kwkB  

 

where          gating fully on          gating
fully off  and              no asymmetry when  
and   are aligned  Although we focus on spherical Gaussian case  many following analysis  in particular critical
point analysis  can also be applied to Eqn   

Multiple ReLU node  For Eqn    that contains   ReLU
node  we could similarly write down the population gradient with respect to wj  note that ej   wj kwjk 
  cid wj   cid   

  cid    ej    
     cid 

      ej  wj      

Xj  

Xj  

 

 

 

  Critical Point Analysis

By solving Eqn     the normal equation    cid wj   cid     

we could identify all critical points of      However  it is
highly nonlinear and cannot be solved easily  In this paper 
we provide conditions for critical points using the structure
of Eqn    The case study for       gives examples for
saddle points and regions without critical points 

For convenience  we de ne   as the Principal Hyperplane
spanned by   ground truth weight vectors  Note that   is
at most   dimensional   wj  
   is said to be inplane  if
all wj     Otherwise it is outof plane 

  Normal Equation

The normal equation    cid wj   cid      

scalar equations and can be written as the following 

   contain Kd

               

 

where     diag sin        sin              
 diag    and            Here     
   
          
 wj    
    ith row  jth
column of   is   

     wj  wj           

    and       
   

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

Note that   and    are both Kby   matrices that only
depend on angles and magnitudes  and hence rotational invariant  This leads to the following theorem characterizing
the structure of outof plane critical points 

Theorem   If           then outof plane critical points
 solutions of Eqn    are nonisolated and lie in   manifold 

The intuition is to construct   rotational matrix that is not
identity matrix but keeps   invariant  Such matrices form
  Lie group   that transforms critical points to critical
points  Then for any outof plane critical point  there is
one matrix in   that changes at least one of its weights 
yielding   nonisolated different critical point 

This is due to the symmetry of

Note that Thm    also works for any general isotropic
in which             has the form of
distribution 
Eqn   
the input    which in turn affects the geometry of critical
points  The theorem also explains why we have  at
minima  Hochreiter et al    Dauphin et al    often occuring in practice 

  InPlane Normal Equation

To analyze inplane critical points  it suf ces to study gradient projections on   When  wj  is fullrank  the projections could be achieved by rightmultiplying both sides
by  ej   which gives     equations 

                  

 

This again shows decomposition of angles and magnitudes 
and linearity with respect to the norms of weight vectors 
Here       kw   kw           kwKk  and similarly for
      and     are    byK matrices that only depend
on angles  Entries of   and     are 

mjj            
jj            
  

    cos   
    cos   

      sin   
      sin   

  cos   
  cos   

   

 

     

Here index   is the jth column of Eqn       is from projection vector ej   and   is the kth weight magnitude 

Diagnoal constraints  For  diagonal  constraints        of
Eqn    we have cos   
jj    
   
    where            cos     sin   Therefore  we
arrive at the following subset of the constraints 

      and mjj        

      

 

 
 

  

Critical          

 
 

 

  

 

 
 

  

          

 

  

  

          

 
 

 

 

  

Figure   Separable property of critical points using Ljj   function
 Eqn    Checking the criticability of         
  can be
decomposed into two subproblems  one related to         
 
and the other is related to         

   

 

 

 

 

 

Separable Property  Interestingly  the plugging back operation leads to conditions that are separable with respect
to ground truth weight  Fig    To see this  we  rst de ne
the following quantity Ljj   which is   function between  
single  rather than    ground truth unit weight vector   
and all current unit weights  el  

  

 

mjj  

          

jj          

Ljj    
        el  is the angle between    and el 
jj    
     like Eqn    Note that
    Fig    illustrates the

where  
      
    cos  
     
   
    is the jth column of    
case when       Ljj   has the following properties 

         
      sin  

   and   

            

  cos   

 

          when there exists   so

Proposition   Ljj    
that      el  In addition  Ljj 
Intuitively  Ljj   characterizes the relative geometric relationship among    and  el  It is like determinant of   matrix whose columns are  el  and    With Ljj     we have

          always 

the following necessary conditions for critical points 

Theorem   If         and for   given parameter   
Ljj     
           or     for all            then

  cannot be   critical point 

  Case study        network

In this case  Mr and    
discuss the case that both    and    are in  

  are  by  matrices  Here we

Saddle points  When  
          and    are collinear 
Mr     is singular since    and    are identical 
From Eqn    if  
         they are both aligned
with the bisector angle of   
  and        

     

  and   

  cid 

 cid        then the current solution is   saddle

point  Note that this gives one constraint for two weight
magnitudes  and thus there exist in nite solutions 

Mr         

     

 

where Mr      and    
         are both Kby 
  matrices  Note that if Mr is fullrank  then we could
solve    from Eqn    and plug it back in Eqn    to check
whether it is indeed   critical point  This gives necessary
conditions for critical points that only depend on angles 

Region without critical points  We rely on the following conjecture that is veri ed empirically in an exhaustive
manner  Sec    It characterizes zerocrossings of     
function on   closed region           In comparison 
inplane   ReLU network has   parameters and is more dif 
 cult to handle    for         
  minus the rotational and scaling symmetries 

  and   

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

   

  

 

 

  

   

 

 
 

  

  

 

 
 

 

 

  

      

      

  

  

 

 
 

   

 

 
 

  

Figure   Critical point analysis for              changes sign
  is in out of the cone spanned by weights    and   
when  
    Two cases that        cannot be critical points 

Conjecture   If    is in the interior of Cone       then
  
      If    is in the exterior  then       

   

   

This is also empirically true for    Combined with
Thm    we know that  Fig   

Theorem   If Conjecture   is correct  then for   ReLU network                  is not   critical point  if they
both are in Cone   

  or both out of it 

    

When exact one    is inside Cone       whether
       is   critical point remains open 

  Convergence Analysis

Application of Eqn    also yields interesting convergence
analysis  We focus on in nitesimal analysis       when
learning rate       and the gradient update becomes  
 rstorder differential equation 

dw dt    EX  wJ   

 

Then the populated objective EX     does not increase 
dE      dt          dw dt                   
 
The goal of convergence analysis is to determine speci  
weight initializations    that leads to convergence to   
following the gradient descent dynamics  Eqn   

  Single ReLU case

Using Lyapunov method  LaSalle   Lefschetz    we
show that the gradient dynamics  Eqn    converges to   
when               kw         kw   
Theorem   When               kw         kw   
following the dynamics of Eqn    the Lyapunov function
         
 kw        has dV  dt     and the system is
asymptotically stable and thus wt      when      
The intuition is to represent dV  dt as    by  bilinear
form of vector  kwk kw    and the bilinear coef cient
matrix  as   function of angles  is negative de nite  except
for        Note that similar approaches do not apply to
regions including the origin because at the origin  the population gradient is discontinuous    does not include the

kw  

     kw
Convergent region

  

   

 

 

 

Sample
region

Successful samples

 

 

 

 

 

 

  Vd     

Figure       Sampling strategy to maximize the probability of
convergence      Relationship between sampling range   and desired probability of success      

origin and for any initialization        we could always
 nd   slightly smaller subset  
         kw        
kw      with       that covers    and apply Lyapunov
method within  Note that the global convergence claim
in  Mei et al    for    loss does not apply to ReLU 
since it requires        
Random Initialization  How to sample        without
knowing    Uniform sampling around origin with radius      kw   for any       results in exponentially
small success rate    kw          in highdimensional
space    better idea is to sample around the origin with
very small radius  but not at       so that   looks like
  hyperplane near the origin  and thus almost half samples
are useful  Fig      as shown in the following theorem 

Theorem   The dynamics in Eqn    converges to    with
probability at least       if the initial value    is
sampled uniformly from Br        kwk      with
        

  kw   

The idea is to lowerbound the probability of the shaded
area  Fig      Thm    gives an explanation for common
initialization techniques  Glorot   Bengio    He et al 
  LeCun et al    Bottou    that uses random

variables with      standard deviation 

  Multiple ReLU case

For multiple ReLUs  Lyapunov method on Eqn    yields
no decisive conclusion  Here we focus on the symmetric property of Eqn    and discuss   special case  that
the teacher parameters    
   and the initial weights
   
   
   respect the following symmetry  wj   Pj   and
    Pj    where Pj is an orthogonal matrix whose collection      Pj  
   forms   group  Without loss of generality  we set    as the identity  Then from Eqn    the
population gradient becomes 

   

  

  cid wj   cid    Pj         

 

This means that if all wj and   
  are symmetric under
group actions  so does their population gradients  There 

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

   

   

 

 

Figure   Spontaneous SymmetricBreaking  SSB  Objective  
gradient  eld is symmetric but the solution is not      Re ection
keeps the gradient  eld invariant  but transforms   to   and vice
versa      The Mexican hat example  Rotation keeps the objective
invariant  but transforms any local minimum to   different one 

fore  the trajectory  wt  also respects the symmetry      
Pj wt
    and we only need to solve one equation for
   wJ  instead of    here       kwk 

    wt

   wJ   

 

Xj  

         Pj                 Pj       

Eqn    has interesting properties  known as Spontaneous
SymmetricBreaking  SSB  in physics  Brading   Castellani    in which the equations of motion respect   certain symmetry but its solution breaks it  Fig    In our language  despite that the population gradient  eld    wJ 
and the objective       are invariant to the group transformation         for      Pj          and    wJ  remain
the same  its solution is not  Pj        Furthermore 
since   is  nite  as we will see  the  nal solution converges
to different permutations of    due to in nitesimal perturbations of initialization 

   

To illustrate such behaviors  consider the following example in which    
   forms an orthonormal basis and under this basis    is   cyclic group in which Pj circularly
shifts dimension by                           
In this case  if we start with            Pj  Pj   
   
                  under the basis of    then Eqn    is further reduced to   convergent    nonlinear dynamics and
Thm    holds  Please check Supplementary Materials for
the associated closeform of the    dynamics 

 

Theorem   For   biasfree twolayered ReLU network
          Pj    
   that takes spherical Gaussian inputs  if the teacher   parameters    
   form orthnomal
bases  then   when the student parameters is initialized to
be                   under the basis of    where         
                            then Eqn    converges to teacher   parameters    
    or             
  when               then it converges to   saddle
point          

            arccos       

Thm    suggests that when                        the system converges to      etc  Since          can be arbitrarily small    slightest perturbation around         leads

to   different  xed point Pj    for some    Unlike single
ReLU case  the initialization in Thm    is   dependent 
and serves as an example for the branching behavior 

Thm    also suggests that for convergence     and    can
be arbitrarily small  regardless of the magnitude of   
showing   global convergence behavior 
In comparison 
 Saad   Solla    uses Gaussian error function    
erf  as the activation  and only analyzes local behaviors
near the two  xed points  origin and   

In practice  even with noisy initialization  Eqn    and the
original dynamics  Eqn    still converge to     and its
transformations  We leave it as   conjecture  whose proof
may lead to an initialization technique for  layered ReLU
that is   independent 

Conjecture   If

the initialization            

  Pj  Pj        where   is noise and           

then Eqn    also converges to    with high probability 

  Simulations

  The analytical solution to         

We verify                             Xw   Eqn   
with simulation  We randomly pick   and   so that their
angle        is uniformly distributed in     The analytical formula             is compared with         
which is computed via sampling on the input   that
follows spherical Gaussian distribution  We use relative
RMS error  err   kE                       kF         
Fig      shows the error distribution with respect to angles 
For small   the gating function      and      mostly
overlap and give   reliable estimation  When           
and     overlap less and the variance grows  Note that our
convergence analysis operate on         and is not affected  In the following  we sample angles from    

Fig      shows that the formula is more accurate with more
samples  We also examine other zeromean distributions of
              As shown in Fig      the formula
still works for large    Note that the error is computed up to
  global scale  due to different normalization constants in
probability distributions  Whether Eqn    applies for more
general distributions remains open 

  Empirical Results in critical point analysis      

Conjecture   can be reduced to enumerate   complicated
but    function via exhaustive sampling  In comparison   
full optimization of  ReLU network constrained on principal hyperplane   involves   parameters   parameters
minus   degrees of symmetry  and is more dif cult to handle  Fig    shows that empirically    has no extra zerocrossing other than         or    As shown in Fig     
we have densely enumerated  
        and    on  

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

   

Distribution of relative RMS error on angle

   

Relative RMS error        sample  Gaussian distribution 

   

Relative RMS error        sample  Uniform distri 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

                                                     

Angle  in radius 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 Samples

 Samples

 Samples

Figure       Distribution of relative RMS error with respect to                Relative RMS error decreases with sample size 
showing the asympototic behavior of the analytical formula  Eqn    Note that the yaxis of the right plot is in log scale      Eqn    also
works well when the input data   are generated by other zeromean distribution         uniform distribution in    

   

Vector  cid eld in        plane       

   

Vector  cid eld in        plane       

 

 

 

 

 

 

 

 

 

 

 

 

 

 

                                                                                                           

                                                                                                           

 

 

   

Trajectory in        plane 

Saddle points

     

iter 

iter 

iter 

iter 

iter 

 

   

Convergence

 Iterations

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

Figure         Vector  eld in        plane following    dynamics  Thm    See Supplementary Materials for the closeform formula 
            Trajectory in        plane
for       and       Saddle points are visible  The parameters of teacher   network are at  
for             and       All trajectories start from  
  gradient descent takes
detours      Training curve  Interestingly  when   is larger the convergence is faster 

        Even  

  is aligned with  

      grid without  nding any counterexamples 

  Convergence analysis for multiple ReLU nodes

Fig      and     shows the    vector  eld in Thm  
Fig      shows the    trajectory towards convergence to
the teacher   parameters   
Interestingly  even when
we initialize the weights as     whose direction is
aligned with    at     the gradient descent still takes
detours to reach the destination  This is because at the beginning of optimization  all ReLU nodes explain the training error in the same way  both   and   increases  when
the  obvious  component is explained  the error pushes
some nodes to explain other components  Hence  specialization follows    increases but   decreases 
Fig    shows empirical convergence for       when
the initialization deviates from initialization                  
in Thm    Unless the deviation is large    converges to
   For more general network        PK
  
when aj     convergence follows  When some aj is negative  the network fails to converge to    even when the
   
student is initialized with the true values    

   aj   
 

  

  Extension to multilayer ReLU network

  natural question is whether the proposed method can be
extended to multilayer ReLU network  In this case  there is
similar subtraction structure for gradient as Eqn   

Proposition   Denote     as all nodes in layer    Denote
  
  and uj as the output of node   at layer   of the teacher
and student network  then the gradient of the parameters
wj immediate under node         is 

 wj        

  DjQj Xj    

 Qj   uj       

      

     

 

  

where Xc is the data fed into node    Qj and   
  are    
byN diagonal matrices  For any node            Qk  
Pj    wjkDjQj and similarly for   

The  layered network in this paper is   special case with
Qj     
       Despite the dif culty that Qj is now depends on the weights of upper layers  and the input Xc is
not necessarily Gaussian distributed  Proposition   gives  
mathematical framework to explore the structure of gradient  For example    similar de nition of Population Gradi 

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

 

 

 

 

 

 

 

 

 

 

 

 

noise     topw    

                                                                      

 Iteration

noise     topw      

                                                                      

 

 

 

 

 

 

 

 

 

 

 

 

noise     topw    

                                                                      

 Iteration

noise     topw      

                                                                      

 

 

 

 

 

 

 

 

 

 

 

 

noise     topw    

                                                                      

 Iteration

noise     topw      

                                                                      

 

 

 

 

 

 

 

 

 

 

 

 

 Iteration

 Iteration

 Iteration

noise     topw    

                                                                      

 Iteration

noise     topw      

                                                                      

 Iteration

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

Figure   Top row  Convergence when weights are initialized with noise   
 layered network converges to  
has   runs  Bottom row  Convergence for        PK
Large positive aj corresponds to fast convergence  When  aj  contains mixture signs  convergence to  

      where             noise  The
   Each experiment
   Here we    top weights aj at different numbers  rather than  

  until huge noise  Both teacher and student networks use        PK

  is not achieved 

   aj  

     

     

 
 

 
 

 

   

   

   

     

     

     

     

  and   

           and vary  

  in  
Figure   Quantity   
ReLU network  We     
   
 cos   sin   In this case   
  are both dependent vari 
    Cone          and
ables with respect to   When  
       otherwise negative  There are no extra zerocrossings 
      Examples   
          Empirical
                with grid size      
evaluation on  

      and  

  and  

ent function is possible 

  Conclusion and Future Work

In this paper  we study the gradient descent dynamics of  
 layered biasfree ReLU network  The network is trained
using gradient descent to reproduce the output of   teacher
network with  xed parameters    in the sense of    norm 
We propose   novel analytic formula for population gradient when the input follows zeromean spherical Gaussian
distribution  This formula leads to interesting critical point
and convergence analysis  Speci cally  we show that critical points out of the hyperplane spanned by    are not
isolated and form manifolds  For two ReLU case  we characterize regions that contain no critical points  For convergence analysis  we show guaranteed convergence for  
single ReLU case with random initialization whose stan 

dard deviation is on the order of      For multiple

ReLU case  we show that an in nitesimal change of weight
initialization leads to convergence to different optima 

Our work opens many future directions  First  Thm    characterizes the nonisolating nature of critical points in the
case of isotropic input distribution  which explains why often practical solutions of NN are degenerated  What if the
input distribution has different symmetries  Will such symmetries determine the geometry of critical points  Second 
empirically we see convergence cases that are not covered
by the theorems  suggesting the conditions imposed by the
theorems can be weaker  Finally  how to apply similar analysis to broader distributions and how to generalize the analysis to multiple layers are also open problems 

Acknowledgement We thank   eon Bottou  Ruoyu Sun  Jason Lee  Yann Dauphin and Nicolas Usunier for discussions and insightful suggestions 

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

References

Bottou    eon  Reconnaissance de la parole par reseaux
connexionnistes  In Proceedings of Neuro Nimes   pp 
  Nimes  France    URL http leon 
bottou org papers bottou   

Brading  Katherine and Castellani  Elena  Symmetries in
physics  philosophical re ections  Cambridge University Press   

Brutzkus  Alon and Globerson  Amir  Globally optimal
gradient descent for   convnet with gaussian inputs  International Conference on Machine Learning  ICML 
 

Choromanska  Anna  Henaff  Mikael  Mathieu  Michael 
Arous    erard Ben  and LeCun  Yann  The loss surfaces
of multilayer networks  In AISTATS     

Choromanska  Anna  LeCun  Yann 

and Arous 
  erard Ben  Open problem  The landscape of the
loss surfaces of multilayer networks  In Proceedings of
The  th Conference on Learning Theory  COLT  
Paris  France  July   volume   pp       

Dauphin  Yann    Pascanu  Razvan  Gulcehre  Caglar 
Cho  Kyunghyun  Ganguli  Surya  and Bengio  Yoshua 
Identifying and attacking the saddle point problem in
highdimensional nonconvex optimization  In Advances
in neural information processing systems  pp   
   

Fukumizu  Kenji and Amari  Shunichi  Local minima and
plateaus in hierarchical structures of multilayer perceptrons  Neural Networks     

Glorot  Xavier and Bengio  Yoshua  Understanding the dif 
 culty of training deep feedforward neural networks  In
Aistats  volume   pp     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Delving deep into recti ers  Surpassing humanlevel performance on imagenet classi cation 
In Proceedings of the IEEE International Conference on Computer Vision  pp     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition  Computer Vision anad Pattern Recognition  CVPR   

Hinton  Geoffrey  Deng  Li  Yu  Dong  Dahl  George   
Mohamed  Abdelrahman  Jaitly  Navdeep  Senior  Andrew  Vanhoucke  Vincent  Nguyen  Patrick  Sainath 
Tara    et al  Deep neural networks for acoustic modeling in speech recognition  The shared views of four
research groups  IEEE Signal Processing Magazine   
   

Hochreiter  Sepp  Schmidhuber    urgen  et al  Simplifying neural nets by discovering  at minima  Advances
in Neural Information Processing Systems  pp   
 

Janzamin  Majid  Sedghi  Hanie  and Anandkumar  Anima  Beating the perils of nonconvexity  Guaranteed
training of neural networks using tensor methods  CoRR
abs   

Kawaguchi  Kenji  Deep learning without poor local minima  Advances in Neural Information Processing Systems   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In Advances in neural information processing
systems  pp     

LaSalle        and Lefschetz     Stability by lyapunov  
second method with applications  New York  Academic
Press   

LeCun  Yann    Bottou    eon  Orr  Genevieve    and
  uller  KlausRobert  Ef cient backprop  In Neural networks  Tricks of the trade  pp    Springer   

Mei  Song  Bai  Yu  and Montanari  Andrea  The landscape
of empirical risk for nonconvex losses  arXiv preprint
arXiv   

Saad  David and Solla  Sara    Dynamics of online gradient descent learning for multilayer neural networks  Advances in Neural Information Processing Systems  pp 
   

Saxe  Andrew    McClelland  James    and Ganguli 
Surya  Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks  arXiv preprint
arXiv   

Simonyan  Karen and Zisserman  Andrew  Very deep convolutional networks for largescale image recognition 
International Conference on Learning Representations
 ICLR   

Soudry  Daniel and Carmon  Yair  No bad local minima 
Data independent training error guarantees for multilayer neural networks  arXiv preprint arXiv 
 

Sutskever  Ilya  Vinyals  Oriol  and Le  Quoc   

Sequence to sequence learning with neural networks 
In
Advances in neural information processing systems  pp 
   

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 

An Analytic Formula of Population Gradient for  layered ReLU and its Applications

Going deeper with convolutions  In Computer Vision and
Pattern Recognition  CVPR  pp     

Zhang  Qiuyi  Panigrahy  Rina  and Sachdeva  Sushant 
arXiv

Electronproton dynamics in deep learning 
preprint arXiv   

