Joint Dimensionality Reduction and Metric Learning    Geometric Take

Mehrtash Harandi     Mathieu Salzmann   Richard Hartley    

Abstract

To be tractable and robust to data noise  existing metric learning algorithms commonly rely on
PCA as   preprocessing step  How can we know 
however  that PCA  or any other speci   dimensionality reduction technique  is the method of
choice for the problem at hand  The answer is
simple  We cannot  To address this issue  in this
paper  we develop   Riemannian framework to
jointly learn   mapping performing dimensionality reduction and   metric in the induced space 
Our experiments evidence that  while we directly
work on highdimensional features  our approach
yields competitive runtimes with and higher accuracy than stateof theart metric learning algorithms 

  Introduction
 To make it tractable for the distance metric learning algorithms we perform dimensionality reduction by PCA to  
  dimensional subspace   Koestinger et al     Like
most of the metric learning methods we  rst center the
dataset and reduce the dimensionality to   ndimensional
space by PCA   Bohn   et al     ITML and LDML are
intractable when using   PCA dimensions   Guillaumin
et al   
These quotations  extracted from the metric learning literature  give rise to   simple question  Is PCA  or  more generally  dimensionality reduction    must to make metric
learning work on highdimensional data  such as that in
computer vision problems 
To quantify this  in the top portion of Table   we provide the area under the ROC curve of stateof theart metric
learning techniques applied to the ASLAN dataset  KliperGross et al    using various PCA dimensions  denoted

 Data  CSIRO  Canberra  Australia  Australian Na 
 CVLab  EPFL 
Mehrtash Harandi

tional University  Canberra  Australia
Switzerland 
Correspondence
 mehrtash harandi anu edu au 

to 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

by    These results suggest that stateof theart methods
either scale poorly with the dimensionality of the input and
thus require PCA to remain tractable       LDML  or require PCA to achieve an accuracy comparable to the other
baselines       KISSME 
In essence  this observation indicates that dimensionality
reduction is bene cial to     reduce the computational burden of the algorithms  and  ii  extract the relevant information from the original noisy data  However  it also raises an
additional question  Is PCA  or any other speci   dimensionality reduction technique  really the best method for
the problem at hand  In other words  Shouldn   we rather
learn the lowdimensional representation and the metric
jointly 
Motivated by these questions  in this paper  we introduce  
uni ed formulation for dimensionality reduction and metric learning  As suggested by our results on the ASLAN
dataset in the bottom row of Table   our method outperforms the stateof theart metric learning techniques 
Furthermore  despite the fact that we directly use highdimensional features as input  our method has comparable
runtimes to that of the fastest algorithms working on PCAbased lowdimensional representations 
In the context of Mahalanobis metric learning  several
methods have proposed to allow the metric   to have low
rank  thus inherently performing dimensionality reduction 
Most methods  however  achieve this implicitly  by letting
  be positive semide nite  Weinberger   Saul   
Davis et al    which  as opposed to explicit dimensionality reduction  does not reduce the computational cost
of these algorithms  As   consequence  they still need to
rely on PCA as   preprocessing step in practice  While  Lu
et al    explicitly decomposes     LLT   it enforces
orthogonality constraints on   to disambiguate the solutions  thus effectively only performing dimensionality reduction  and not metric learning  By contrast  our approach
lets us learn   complete Mahalanobis metric jointly with  
lowdimensional projection 
At the heart of our joint dimensionality reduction and metric learning formulation lie notions of Riemannian geometry and quotient spaces  More speci cally  we model the
projection to   lowdimensional space as   point on   Stiefel
manifold  and the metric in this space as   Symmetric Pos 

Joint Dimensionality Reduction and Metric Learning    Geometric Take

Method
NCA  Goldberger et al   
ITML  Davis et al   
LDML  Guillaumin et al   
LMNN  Weinberger   Saul   
KISSME  Koestinger et al   
GMML  Zadeh et al   
DRML  Ours 

     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     

Table   AUCs and training times for the stateof theart metric learning techniques and for our approach  DRML  on
the ASLAN dataset  KliperGross et al    The baseline algorithms were applied after projecting the features to  
pdimensional space by PCA  whereas our method learns the pdimensional representation and the metric jointly 

itive De nite  SPD  matrix  We then show that our search
space reduces to   quotient of the product space of the
Stiefel and SPD manifolds with the orthogonal group  By
building upon recent advances in optimization on Riemannian matrix manifolds  Absil et al    we therefore develop   mathematical framework that effectively and ef 
ciently lets us  nd   solution in this space  Furthermore 
we show that our formulation can be kernelized  This not
only lets us handle nonlinearity in the data  but also makes
our approach applicable to nonvectorial input data  such as
linear subspaces  Harandi et al    which have proven
bene cial for many recognition tasks 
We demonstrate the bene ts of our joint dimensionality reduction and metric learning approach over existing metric
learning schemes on several tasks  including action similarity matching  face veri cation and person reidenti cation 

  Mathematical Background
In this work  as most metric learning algorithms  we are interested in learning   Mahalanobis distance de ned below 
De nition    The Mahalanobis distance  The Mahalanobis distance between   and    in Rn is de ned as
                          

             cid        cid 
  

 

To have   valid metric  the Mahalanobis matrix   must
be positive de nite 

As will be shown in Section   our approach to learning
  Mahalanobis metric can be formulated as   nonconvex
optimization problem on   Riemannian manifold  This type
of problems can be expressed with the general form

have shown the bene ts of truly exploiting the geometry of
the manifold over standard constrained optimization  As
  consequence  these techniques have become increasingly
popular in diverse application domains  Mishra et al   
Harandi et al    Cunningham   Ghahramani   
  detailed discussion of Riemannian optimization goes beyond the scope of this paper  and we refer the interested
reader to  Absil et al   
As will be discussed in details in Section   we formulate
metric learning in the quotient space of the product space
of two Riemannian manifolds with the orthogonal group 
The two Riemannian manifolds at the heart of this formulation are the Stiefel manifold and the manifold of Symmetric
Positive De nite  SPD  matrices de ned below 
De nition    The Stiefel Manifold  The set of        
dimensional matrices         with orthonormal columns
endowed with the Frobenius inner product  forms   compact Riemannian manifold called the Stiefel manifold
St        Boothby   

St        cid       Rn             Ip   

 
De nition    The SPD Manifold  The set of         dimensional real  SPD matrices endowed with the Af ne Invariant Riemannian Metric  AIRM   Pennec et al   
forms the SPD manifold    
   

 cid       Rp     vT              Rp        
 

 

 

The dimensionality of St       and    
and         respectively 

  are np   

      

minimize      
            

 
where   is   Riemannian manifold       informally   
smooth surface that locally resembles   Euclidean space 
While Riemannian manifolds can often be explicitly encoded in terms of constraints on    the recent advances
in Riemannian optimization techniques  Absil et al   

  Our Approach
Our goal  as that of many other metric learning algorithms 
is to learn   Mahalanobis distance between the input measurements 
Ideally  this distance should re ect the class

 Note that the literature is divided between this choice and
another form of Riemannian metric  See  Edelman et al   
for details 

Joint Dimensionality Reduction and Metric Learning    Geometric Take

labels of the samples  Furthermore  motivated by our analysis of existing methods  which all bene   from   PCA preprocessing step  we also seek to reduce the dimensionality
of the data  However  in contrast to existing methods  we
propose to learn the lowerdimensional representation and
the Mahalanobis distance in that space jointly 
More speci cally  we want to learn   projection     Rn  
Rp and   Mahalanobis matrix        
  such that the
induced distance in Rp is more discriminative  To this
end  let      xi   xi  yi  
   be   set of triplets  where
xi   xi   Rn are the feature vectors of two training samples  and the label yi       determines whether xi and
 xi are similar  yi     or not  yi     The Mahalanobis
distance between xi and  xi in the lowdimensional space
can thus be written as
      xi   xi         xi        xi          xi        xi 
  

        xi            xi    xi   

 

To learn   latent space whose Mahalanobis distance re ects
class similarity  we make use of the logistic loss  More precisely  for each pair of samples  xi   xi  sharing the same
label       yi     we de ne the loss

 cid xi   xi yi       log    pi   

 cid 

with

pi   exp

                          

 cid 

 

       

 

Conversely  for   pair of samples  xj   xj  whose labels differ       yj     we de ne the loss

     

 cid xj   xj yj       log      
the loss of Eq    is minimized when
Intuitively 
      xi   xi      whereas the loss of Eq    is minid 
      xj   xj     
mized when   
The losses for all training triplets can be grouped into   cost
function of the form

 

            cid   cid 
 cid 

  yi 

 

  yi 

log    pi 

log      

 

                

       

which further encodes   regularizer on    This regularizer         
       allows us to exploit prior
knowledge on the Mahalanobis matrix  encoded by   reference matrix     Following common practice  Davis et al 
  Hoffman et al    we make use of the asymmetric Burg divergence  which yields
            Tr MM 

      log det MM 

         
 

In our experiments  since typically no strong prior is available  we simply use       Ip       the identity matrix 
Joint dimensionality reduction and metric learning can then
be achieved by minimizing the cost function of Eq          
  and    To avoid degeneracies  and following common
practice in dimensionality reduction  we constrain   to be
  matrix with orthonormal columns  That is 

        Ip 

 
With this constraint    is in fact   point on the Stiefel
manifold St       Since both   and   lie on Riemannian manifolds  albeit different ones  we propose to make
use of Riemannian optimization to solve our problem  as
described below 

  Manifoldbased Optimization

To determine   and    we need to solve the optimization
problem

          

min
    
             Ip     cid     

 

Jointly minimizing with respect to   and   can be
achieved by making use of the product space of the Stiefel
and SPD manifolds  Mp   St         
  Both St      
and    
  are smooth homogeneous spaces and their product preserves smoothness and differentiability  Absil et al 
  Thus  Mp can be given   Riemannian structure 
However  in our case    closer look at            reveals
that
                    RT             Op    
where Op is the orthogonal group  This implies that

 
is   right group action on Mp  The theorem below establishes an important property about the action of Op on Mp 
which will prove crucial to our develop our approach 

    Mp Op   Mp  cid       cid    cid   cid      RT     cid 
 cid      with
Theorem   The set    cid   cid St            
 cid       cid   cid cid      RT     cid           
 cid 

the equivalence relation

 

 

and Riemannian metric
                                 Tr  

      

 
  Tr          

forms   Riemannian quotient manifold 

Proof  See the supplementary material 

The search space of our problem therefore truly is this Riemannian manifold    To be able to perform Riemannian
optimization on    below  we derive the required entities 

Joint Dimensionality Reduction and Metric Learning    Geometric Take

The Geometry of  
The general theory of quotient manifolds  Lee    Absil et al    tells us that the equivalence relation splits
the tangent space of Mp at              into two complementary parts  the horizontal space   Mp and the vertical
space   Mp  These two spaces are such that
gp                   Mp and         Mp  
 
where gp is the Riemannian metric of the product manifold
Mp  The vertical space   Mp has the property that projecting any of its vectors to Mp via the exponential map
yields   point in the equivalence class of   Therefore  the
tangent space of   can be identi ed with the horizontal
space            cid    Mp 
 
        can be obtained from  
  tangent vector  
tangent vector       Mp by projection 
It can be
shown that the horizontal space at Mp  cid            
    Ip                 with     On is the set  details in
the supplementary material 

 cid cid 

 

 cid           

 cid 

 cid cid 

   

 

 

with     Sym                 Furthermore  we have
the following theorem to obtain the tangent vectors in   
Theorem    Projecting on the Horizontal Space  For
                    Mp  the horizontal vector       the
associated tangent vector in            is identi ed as

 cid                       cid   
           cid    

              

               cid  

with   the solution of the following Sylvester equation 

 

 

Proof  See the supplementary material 
To perform Newtontype optimization on    we also need
the form of the retraction                          
which follows from the retraction on Mp  In particular  we
suggest the following retraction 

                  cid cid uf         
    expm          cid   

 
Here uf        AT    which yields an orthogonal
matrix and expm  denotes the matrix exponential  Altogether  this provides us with the tools required to perform
Riemannian optimization to solve our problem  The only
missing mathematical entity is the Euclidean gradient of

Figure   Convergence behavior of our algorithm 

our loss function          and     which we provide in
the supplementary material 
In our experiments  we employed Conjugate Gradient descent on   to solve   In particular  we implemented
the operations required for our manifold within the manopt
Riemannian optimization toolbox  Boumal et al   
The code is available at https sites google 
com site mehrtashharandi 
In Fig    we illustrate the typical convergence behavior
of our algorithm using the ASLAN dataset  KliperGross
et al    In our experiments  we have observed that the
algorithm converges quite fast  typically in less than   iterations  thus making it scalable to learning large metrics 

  Computational Complexity

The complexity of each iteration of our algorithm to
solve   depends on the computational cost of the following major steps 

  Objective

function

evaluation 

           takes   mnp   mp         np 

Computing

  Euclidean gradient evaluation  Computing    takes
  mn    pn    np  and computing    takes
  mn    pn    np       Note that some computations are common to both    and      Hence
the total  ops for this step is less than the addition of
the Stiefel and SPD parts 

  Projecting         to the tangent space of Mp

takes           

  Projecting   tangent vector in Mp costs   np  to
form the Sylvester equation and      to solve it using the Bartels   Stewart algorithm  Bartels   Stewart 
 

  Retraction  For the Stiefel part  the retraction  St
takes   np        For the SPD part   SPD takes
    

 Cost Running time  sec  Joint Dimensionality Reduction and Metric Learning    Geometric Take

These steps are either linear or quadratic in    Therefore 
and as evidenced by our experiments  our approach can effectively and ef ciently handle highdimensional input features without any PCA preprocessing 
Remark   The number of unknowns determined by our
algorithm corresponds to the dimensionality of Mp  that
       
             
is  np    
      By contrast  the metric learning techniques that
utilize PCA as   preprocessing step only determine  
      
  unknowns  which is typically much smaller  As such 
our method can potentially better leverage large amounts
of training triplets  In our Big Data era  we believe this to
be an important strength of our approach 

             

             

  Discussion
An SPD matrix        
  can be decomposed as   DU  
with     Op and     diagonal matrix with positive elements  As such  the term         appearing in our loss
           can be written as

              DU           DV    

with St        cid           Thus  our optimization problem can be expressed by   loss            with   search
space de ned as St         Rp
  Theoretically  this representation has the same expressive power as our formulation if we ignore the invariance of   DV   to permutations 
However  in the context of  xedrank matrix factorization
 see  Mishra et al    Section   it has been shown
that  for   parametrization of the form           where
        St       modeling   as an SPD matrix is typically more effective than as   diagonal matrix with positive
elements  The argument there is that it  gives more  exibility to optimization algorithms   Mishra et al   
One can also factorize         as LLT with     Rn   
This factorization  though being widely used  is not invariant to the action of Op  meaning that replacing    
LR      Op will not change the loss  Such an invariance
hinders gradient descent algorithms  as shown for example in  Journ ee et al    Mishra et al    In Section   we empirically show that this is indeed the case for
the problem of interest here       metric learning 
In  Journ ee et al    the invariance induced by the action of Op in   factorization of the form LLT is taken into
account  In particular  the authors make use of   quotient
geometry to overcome the undesirable effects of the invariance in gradient descent optimization  There is   subtle 
yet important difference between our formulation and that
of  Journ ee et al    Our approach can bene   from  
factorization with redundancy  which is effective in practice  Furthermore  note that the geometry developed in our
paper can also handle the case where   Mahalanobis metric
is searched for       without recasting the problem as   fac 

Method
EucLLT
RimLLT
RimV DV  
  MW    Ours 

     
 
 
 
 

     
 
 
 
 

     
 
 
 
 

     
 
 
 
 

Table   AUC for various geometries on ASLAN 

torization problem  which is the case in techniques such
as  Globerson   Roweis    Koestinger et al   
Zadeh et al   
Before concluding this part  we contrast the aforementioned factorization for the experiment reported in Table  
To this end  we replace the term         in our loss with
  LLT       Rn    and optimize using Euclidean geometry  We call this solution EucLLT  
  LLT       Rn    and optimize using the geometry
developed in  Journ ee et al    We call this solution
RimLLT  
    DV         St       and     diagonal and positive matrix  We optimize using the geometry of the product manifold St         Rp
  We call this solution RimV DV    
Following the experiment shown in Table   we evaluate
the AUC for various dimensionalities using the aforementioned geometries  The results are provided in Table  
First  we note that the general practice       using Euclidean
geometry  is signi cantly outperformed by its Riemannian
counterparts  The quotient geometry developed in  Journ ee
et al    performs on par with our approach for low dimensionalities             However  for larger dimensionalities  our technique yields more accurate solutions 
suggesting that the redundancy in the formulation plays an
important role  The importance of the redundancy can also
be noticed by comparing RimV DV   against our solution  In terms of computation time  the diagonal form      
RimV DV   yields only slightly faster runtimes  In the
particular case of ASLAN  the training time for      
was reduced to    

  Kernelizing the Solution
We now show how our approach can handle nonlinearity
in the data  as well as generalize to nonvectorial input
data  such as linear subspaces  which have proven effective for video recognition  Turaga et al    Harandi
et al    Jayasumana et al    Following common practice when converting   linear algorithm to   nonlinear one       from PCA to kernel PCA  we make use
of   mapping of the input data to   Reproducing Kernel
Hilbert Space  RKHS  As shown below  the resulting algorithm then only depends on kernel values       it does not
explicitly depend on the mapping to RKHS  Since much
progress has recently been made in developing positive def 

Joint Dimensionality Reduction and Metric Learning    Geometric Take

  yi 

 

  yi 

 

log     pi 

log       

inite kernels for nonvectorial data  Harandi et al   
Jayasumana et al    Vishwanathan et al    this
makes our approach applicable to   much broader variety
of input types 
Speci cally  let           be   mapping from the input
space   to an RKHS   with corresponding kernel function
  xi  xj     cid xi   xj cid  Following the same formalism as before  we can de ne   cost function of the form

LH           cid   cid 
 cid 
with  pi   exp cid     xi   xi cid  and               cid xi   
 xj cid  

        cid xi     xj cid  with        

  and
    St    dim    Note that for universal kernel functions  such as the Gaussian kernel  dim        We
therefore need   formulation where only the kernel function appears  and not   explicitly  To this end  we exploit
the representer theorem  Sch olkopf et al    which
states that the mapping   lies in the span of the training data  and can thus be expressed as          
Here               dl    Rdim     is   matrix that stacks the representation of the   training samples
in the feature space  In this formalism  the orthogonality
constraint on   can be written as

              

 

        AT             AT            Ip  
where              
  is the kernel matrix with elements
                di  dj  Let us de ne St        cid     
          such that the orthogonality constraint becomes BT     Ip  This lets us write

            cid         cid  
        cid         cid 
 cid                  cid 
where Rl  cid           cid                dl cid     Thus 

 cid                  cid  

           

         

  BM BT
 

the cost de ned in Eq    can be rewritten as   function
of         LH         which  by taking            from
Eq    only depends on kernel values  Since this cost function has essentially the same form as the one derived in Section   and the variables   and   lie on the same types of
manifold as those of Section   we can use the same optimization strategy as before 

  Related Work
Metric learning is   wellstudied problem whose origins
can be traced back to the early eighties        Short  
Fukunaga    Here  we focus on the prime representatives that will be used as baselines in our experiments 

idea

based Metric

 Goldberger et al   

For   more thorough study  we refer the reader to the recent
book by  Bellet et al   
of Neighborhood Component Analysis
The
 NCA 
is to optimize the
error of   stochastic nearest neighbor classi er in the space
induced by the Mahalanobis metric  The InformationTheoretic Metric Learning  ITML  algorithm  proposed
by  Davis et al    learns   Mahalanobis metric by
exploiting   notion of margin between pairs of samples 
More precisely  the algorithm searches for   Mahalanobis
    an upper
matrix satisfying two types of constraints 
bound   on the distance between pairs of samples from
   xi   xi      
the same class       in our formalism    
     yi      ii    lower bound   on the distance beM  xi   xi      
tween pairs of dissimilar samples         
     yi    
The Large Margin Nearest Neighbors  LMNN  of  Weinberger   Saul    introduces the notion of local margins
for metric learning  In LMNN  learning the Mahalanobis
metric is expressed as   convex optimization problem that
encourages the   nearest neighbors of any training instance
xi to belong to the same class as xi  while keeping away
instances of other classes 
Logistic
Learning
Discriminant
 LDML   Guillaumin et al    relies on   Mahalanobis distancebased sigmoid function to encode the
likelihood that two samples belong to the same class  The
metric is then learned by maximizing the likelihood of the
sample pairs  xi   xi  that truly belong to the same class 
     yi     while minimizing that of the sample pairs that
do not       yi    
While effective  all the abovementioned techniques rely
on PCA as   preprocessing step to remain tractable  By
contrast  the ef cient  Keep It Simple and Straightforward
Metric   KISSME  algorithm of  Koestinger et al   
focuses on addressing largescale problems  KISSME assumes that the similar and dissimilar pairs are generated
from two independent Gaussian distributions  Computing the Mahalanobis metric then translates to maximizing   loglikelihood  which can be achieved in closedform  As illustrated in Table   however  this algorithm
requires PCA preprocessing to achieve accuracies comparable to the ones produced by the other algorithms  In
the spirit of KISSME  Geometric Mean Metric Learning  GMML   Zadeh et al    relies on the geodesic
connecting two covariance matrices to identify the Mahalanobis metric 
While effective and quite ef cient  the abovementioned
techniques usually rely on PCA as   preprocessing step
to reduce the dimensionality of the data  As evidenced by
our experiments  this preprocessing step is suboptimal 

Joint Dimensionality Reduction and Metric Learning    Geometric Take

Method
baseline  KliperGross et al   
NCA  Goldberger et al   
ITML  Davis et al   
LDML  Guillaumin et al   
LMNN  Weinberger   Saul   
KISSME  Koestinger et al   
GMML  Zadeh et al   
DRML
kDRML

HoG

AUC
CRR
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   

HoF

AUC
CRR
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   

HnF

AUC
CRR
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   

Figure   Examples from the ASLAN dataset 

Table   Average accuracy and AUC for the ASLAN dataset 

  Experimental Evaluation
We now evaluate our algorithms  DRML and kDRML 
and compare them with the representative baseline metric learning methods discussed above       NCA  Goldberger et al    LMNN  Weinberger   Saul 
  ITML  Davis et al    LDML  Guillaumin
et al    KISSME  Koestinger et al    and
GMML  Zadeh et al    as well as with datasetspeci   baselines mentioned below  Our experiments consist of two parts  First  we make use of benchmark datasets
where the data can be represented in vector  Euclidean 
form  and thus both DRML and kDRML are applicable  Second  we consider manifoldvalued data where only
kDRML applies 
In all our experiments  we followed the socalled restricted
protocol  That is  the only information accessible to the
algorithms is the similarity dissimilarity labels of pairs of
samples  the class labels of the samples are unknown  For
all the methods  we report the results obtained with the best
subspace dimension  Note that this means that not all methods use the same subspace dimension  However  it makes
the comparison more fair  since it truly shows the full potential of the algorithms 

  Experiments with Euclidean Data

ACTION SIMILARITY MATCHING 

As    rst experiment  we considered the task of action similarity recognition using the ASLAN dataset  KliperGross
et al    The ASLAN dataset contains   human
action clips collected from YouTube  spanning over  
unique action categories  see Fig    The sample distribution across the categories is highly uneven  with   classes
possessing only one video clip  The benchmark protocol focuses on action similarity  same notsame  rather
than action classi cation  and testing is performed on
previouslyunseen actions 
The dataset comes with   prede ned splits of the data 
where each split consists of   training and   testing
pairs of action videos  The ASLAN dataset also provides
three different types of descriptors  Histogram of Oriented

Gradients  HoG  Histogram of Optical Flow  HoF  and
  composition of both  referred to as HnF  The videos
are represented by spatiotemporal bags of features  Laptev
et al    with   codebook of size   For kDRML 
we used an RBF Gaussian kernel whose bandwidth was set
using Jaakkola   heuristic  Jaakkola et al   
In Table   we report the classi cation accuracy and the
Area Under the ROC Curve  AUC  of our algorithms and
of the baselines  Here  we also include the results of the
benchmark  KliperGross et al    which provides us
with   direct comparison of previously published results 
Note that DRML and kDRML outperform all the other algorithms  In general  kDRML performs better than DRML 
To further evidence the bene ts of jointly learning the
lowdimensional projection and the metric  we performed
the following experiment  using the HoG features  We
 xed the matrix   to the subspace obtained by PCA  and
learned the metric using our loss function  This resulted in
  drop in accuracy of roughly          CRR of  
This con rms our intuition that we can achieve better than
PCA by jointly learning the subspace and the metric 
Remark   In  KliperGross et al    it was shown
that other metrics       the cosine similarity  could outperform the Euclidean distance  used here as   baseline 
In principle  our framework can also be used to learn cosine similarities by generalizing the inner product  cid      cid  as
aT            Doing so  however  goes beyond the scope
of this paper 

PERSON REIDENTIFICATION 

For the task of person reidenti cation  we used the iLIDS
dataset  Zheng et al    The dataset consists of  
images of   pedestrians and was captured in an airport 
The number of images for each person varies from   to  
The dataset contains severe occlusions caused by people
and baggage 
In our experiments  we adopted the singleshot protocol 
That is  the dataset was randomly divided into two subsets  training and test  with   and   exclusive individuals  respectively  The random splitting was repeated  
times  In each partition  one image from each individual

Joint Dimensionality Reduction and Metric Learning    Geometric Take

Figure   Examples from the iLIDS dataset  Zheng et al   

     
     
Method
 
 
NCA  Goldberger et al   
 
 
kLFDAlin  Xiong et al   
 
 
kLFDAChi   Xiong et al   
 
 
ITML  Davis et al   
 
 
LDML  Guillaumin et al   
 
 
LMNN  Weinberger   Saul   
 
 
KISSME  Koestinger et al   
 
 
GMML  Zadeh et al   
 
 
DRML
     
kDRML
Table   CMC at rank       and   on the iLIDS dataset 

     
 
 
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 
 

in the test set was randomly selected as the reference image and the rest of the images were used as query images 
This process was repeated   times  We used the features
provided by the authors of  Xiong et al    These features describe each image using  bin histograms from the
RGB  YUV and HSV color channels  as well as texture histograms based on Local Binary Patterns  Ojala et al   
extracted from   nonoverlapping horizontal bands  For the
kernelbased solutions       kDRML and kLFDA  we used
the Chisquare kernel 
We report performance in terms of the Cumulative Match
Characteristic  CMC  curves for different rank values  
indicating that we search for   correct match among
the   nearest neighbors  Table   compares our results
with those of the baseline metric learning algorithms  as
well as with kernel Local Fisher Discriminant Analysis
 kLFDA   Xiong et al    which represents the stateof theart on this dataset  Our kDRML method achieves the
highest scores for all ranks  Note that kLFDA requires the
subject identities during training  while the other methods 
including ours  don    Despite this  kDRML outperforms
the stateof theart results of kLFDAChi 

  Experiments with ManifoldValued Data

To illustrate the fact that our algorithm generalizes to nonvectorial input data  we utilized the Youtube Faces  YTF 
dataset  Wolf et al    and represented each video as  
point on   Grassmann manifold  The YTF dataset contains
  videos of   subjects collected from the YouTube
website  These videos depict large variations in pose  illumination and expression  To evaluate the performance
of the algorithms  we followed the protocol suggested
in  Wolf et al    Speci cally  we used the   video
pairs of cially provided with the dataset  which are equally
divided into   folds  Each fold contains    same  and
   notsame  pairs  We used the provided LBP features

Method
baseline  Wolf et al   
NCA  Goldberger et al   
ITML  Davis et al   
LDML  Guillaumin et al   
LMNN  Weinberger   Saul   
KISSME  Koestinger et al   
GMML  Zadeh et al   
kDRML

Acc 
 
 
 
 
 
 
 
 

AUC
 
 
 
 
 
 
 
 

EER
 
 
 
 
 
 
 
 

Table   Average accuracy  AUC and EER on the YTF
dataset  Wolf et al   

and modeled each video by   subspace of dimensionality
  as described in  Wolf et al    As   result  each
video was modeled as   point on the Grassmann manifold
     where   is the dimensionality of the LBP
features  We used the projection kernel de ned as

kproj Si  Sj     cid ST

  Sj cid 
   

While kDRML directly uses   kernel function  some baselines       KISSME  do not  To still be able to report results for these baselines  we utilized kernel PCA  instead of
PCA  to create their inputs 
Table   summarizes the performance of the metric learning techniques and the baseline  Wolf et al    using
the same input       subspaces of dimensionality   Here
again  kDRML comfortably outperforms the other methods
for all the error metrics  For example  the gap in accuracy
between kDRML and its closest competitor       KISSME 
is more than  
Remark   Note that  as shown in  Feragen et al   
an RBF kernel of the form exp   
   with dg being
the geodesic distance on the Grassmann manifold is not  
positive de nite kernel  The projection kernel  however  has
been shown to be positive de nite  Hamm   Lee   
which  ultimately  is all we require to make our algorithm
applicable to manifoldvalued data  Furthermore  this kernel has proven effective in   variety of applications  Hamm
  Lee    Harandi et al   

  Conclusions and Future Work
In this paper  we have argued against treating dimensionality reduction as   preprocessing step to metric learning 
We have therefore introduced   framework that learns  
lowdimensional representation and   Mahalanobis metric
in this space in   uni ed manner  We have shown that the
resulting framework could be cast an optimization problem on the quotient space of the product space of two Riemannian manifolds with the orthogonal group  Our experiments have evidenced the bene ts of our uni ed approach
over stateof theart metric learning algorithms that rely on
PCA as   preprocessing step 
In the future  we plan to
study the use of other cost functions within our uni ed
framework  especially formulations based on the concept
of large margin 

Joint Dimensionality Reduction and Metric Learning    Geometric Take

References
Absil  PA  Mahony  Robert  and Sepulchre  Rodolphe 
Optimization algorithms on matrix manifolds  Princeton
University Press   

Bartels  Richard    and Stewart  GW  Solution of the matrix equation ax  xb     Communications of the ACM 
   

Bellet  Aur elien  Habrard  Amaury  and Sebban  Marc 
Metric Learning  Morgan   Claypool Publishers   

Bohn    Julien  Ying  Yiming  Gentric  St ephane  and Pontil  Massimiliano  Large margin local metric learning  In Proc  European Conference on Computer Vision
 ECCV  pp    Springer   

Boothby  William Munger  An introduction to differentiable manifolds and Riemannian geometry  volume  
Gulf Professional Publishing   

Boumal     Mishra     Absil       and Sepulchre    
Manopt    Matlab toolbox for optimization on manifolds  Journal of Machine Learning Research   
    URL http www manopt org 

Cunningham  John   and Ghahramani  Zoubin  Linear dimensionality reduction  Survey  insights  and generalizations  JMLR   

Davis  Jason    Kulis  Brian  Jain  Prateek  Sra  Suvrit 
Informationtheoretic metric
In Proc  Int  Conference on Machine Learn 

and Dhillon  Inderjit   
learning 
ing  ICML  pp     

Edelman  Alan  Arias  Tom as    and Smith  Steven    The
geometry of algorithms with orthogonality constraints 
SIAM journal on Matrix Analysis and Applications   
   

Feragen  Aasa  Lauze  Francois  and Hauberg  Soren 
Geodesic exponential kernels  When curvature and linearity con ict  In Proc  IEEE Conference on Computer
Vision and Pattern Recognition  CVPR  June  

Globerson  Amir and Roweis  Sam  Metric learning by
collapsing classes  In Proc  Advances in Neural Information Processing Systems  NIPS  volume   pp   
   

Goldberger  Jacob  Roweis  Sam  Hinton  Geoff  and
Salakhutdinov  Ruslan  Neighbourhood components
analysis  In Proc  Advances in Neural Information Processing Systems  NIPS   

Guillaumin  Matthieu  Verbeek 

Jakob  and Schmid 
Is that you  metric learning approaches for
In Proc  Int  Conference on Com 

Cordelia 
face identi cation 
puter Vision  ICCV  pp     

Hamm  Jihun and Lee  Daniel    Grassmann discriminant
analysis    unifying view on subspacebased learning  In
Proc  Int  Conference on Machine Learning  ICML  pp 
  ACM   

Harandi  Mehrtash  Salzmann  Mathieu 

Jayasumana 
Sadeep  Hartley  Richard  and Li  Hongdong  Expanding the family of Grassmannian kernels  An embedding
perspective  In Proc  European Conference on Computer
Vision  ECCV  pp    Springer   

Harandi  Mehrtash  Salzmann  Mathieu  and Hartley 
Richard  Dimensionality reduction on SPD manifolds 
IEEE
The emergence of geometryaware methods 
Trans  Pattern Analysis and Machine Intelligence   

Hoffman  Judy  Rodner  Erik  Donahue  Jeff  Kulis  Brian 
and Saenko  Kate  Asymmetric and category invariant
feature transformations for domain adaptation  Int  Journal of Computer Vision     

Huang  Gary    Ramesh  Manu  Berg  Tamara  and
LearnedMiller  Erik  Labeled faces in the wild   
database for studying face recognition in unconstrained
environments  Technical report  Technical Report  
University of Massachusetts  Amherst   

Jaakkola  Tommi  Diekhans  Mark  and Haussler  David 
Using the Fisher kernel method to detect remote protein
homologies  In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biology  pp    AAAI Press   

Jayasumana     Hartley     Salzmann     Li     and
Harandi     Kernel methods on Riemannian manifolds
with Gaussian RBF kernels  Pattern Analysis and Machine Intelligence  IEEE Transactions on   
   

Journ ee  Michel  Bach  Francis  Absil  PA  and Sepulchre 
Rodolphe  Lowrank optimization on the cone of positive semide nite matrices  SIAM Journal on Optimization     

KliperGross  Orit  Hassner  Tal  and Wolf  Lior  The action
similarity labeling challenge  IEEE Trans  Pattern Analysis and Machine Intelligence     

Koestinger  Martin  Hirzer  Martin  Wohlhart  Paul  Roth 
Peter    and Bischof  Horst  Large scale metric learning
from equivalence constraints  In Proc  IEEE Conference
on Computer Vision and Pattern Recognition  CVPR 
pp    IEEE   

Laptev     Marszalek     Schmid     and Rozenfeld 
   Learning realistic human actions from movies 
In
Proc  IEEE Conference on Computer Vision and Pattern
Recognition  CVPR  pp     

Joint Dimensionality Reduction and Metric Learning    Geometric Take

Zadeh  Pourya  Hosseini  Reshad  and Sra  Suvrit  GeoIn Proc  Int  Conference

metric mean metric learning 
on Machine Learning  ICML  pp     

Zheng  WeiShi  Gong  Shaogang  and Xiang  Tao  Associating groups of people  In BMVC  volume   pp   
 

Lee  John    Smooth manifolds  Springer   

Lu  Jiwen  Zhou  Xiuzhuang  Tan  YapPen  Shang 
Yuanyuan  and Zhou  Jie  Neighborhood repulsed metIEEE Trans  Patric learning for kinship veri cation 
tern Analysis and Machine Intelligence   
 

Mishra     Meyer     Bonnabel     and Sepulchre    
Fixedrank matrix factorizations and Riemannian lowrank optimization  Computational Statistics   
   

Ojala  Timo  Pietik ainen  Matti  and   aenp      Topi  Multexture
tiresolution grayscale and rotation invariant
IEEE Trans 
classi cation with local binary patterns 
Pattern Analysis and Machine Intelligence   
   

Pennec  Xavier  Fillard  Pierre  and Ayache  Nicholas   
Riemannian framework for tensor computing  Int  Journal of Computer Vision     

Sch olkopf  Bernhard  Herbrich  Ralf  and Smola  Alex   
In Computational

  generalized representer theorem 
learning theory  pp    Springer   

Short  Robert   and Fukunaga  Keinosuke  The optimal distance measure for nearest neighbor classi cation 
IEEE Transactions on Information Theory   
   

Turaga     Veeraraghavan     Srivastava     and Chellappa     Statistical computations on Grassmann and
Stiefel manifolds for image and videobased recognition 
IEEE Trans  Pattern Analysis and Machine Intelligence 
   

Vishwanathan    Vichy    Schraudolph  Nicol    Kondor 
Risi  and Borgwardt  Karsten    Graph kernels  Journal
of Machine Learning Research     

Weinberger  Kilian   and Saul  Lawrence    Distance metric learning for large margin nearest neighbor classi cation  Journal of Machine Learning Research   
   

Wolf  Lior  Hassner  Tal  and Maoz  Itay  Face recognition
in unconstrained videos with matched background similarity  In Proc  IEEE Conference on Computer Vision
and Pattern Recognition  CVPR  pp     

Xiong  Fei  Gou  Mengran  Camps  Octavia  and Sznaier 
Mario  Person reidenti cation using kernelbased metric learning methods  In Proc  European Conference on
Computer Vision  ECCV  pp    Springer   

