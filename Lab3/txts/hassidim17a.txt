Robust Guarantees of Stochastic Greedy Algorithms

Avinatan Hassidim   Yaron Singer  

Abstract

In this paper we analyze the robustness of
stochastic variants of the greedy algorithm for
submodular maximization  Our main result
shows that for maximizing   monotone submodular function under   cardinality constraint  iteratively selecting an element whose marginal contribution is approximately maximal in expectation is   suf cient condition to obtain the optimal approximation guarantee with exponentially
high probability  assuming the cardinality is suf 
 ciently large  One consequence of our result is
that the lineartime STOCHASTICGREEDY algorithm recently proposed in  Mirzasoleiman et al 
  achieves the optimal running time while
maintaining an optimal approximation guarantee  We also show that high probability guarantees cannot be obtained for stochastic greedy algorithms under matroid constraints  and prove an
approximation guarantee which holds in expectation  In contrast to the guarantees of the greedy
algorithm  we show that the approximation ratio
of stochastic local search is arbitrarily bad  with
high probability  as well as in expectation 

  Introduction
In this paper we study the guarantees of stochastic optimization algorithms for submodular maximization    function            is submodular if it exhibits   diminishing
returns property  That is  for any           and any
        the function respects 

fS      fT    

where fH     denotes the marginal contribution of an element       to   set             fH                  
and
Google
to 
Avinatan Hassidim  avinatan cs biu ac il  Yaron Singer
 yaron seas harvard edu 

Ilan University
Correspondence

 Harvard University 

 Equal

contribution

 Bar

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

      Many fundamental measures such as entropy  diversity  and clustering can be modeled as submodular functions  and as   result submodular optimization is heavily
studied in machine learning for well over   decade now 
It is well known that for the problem of maximizing  
monotone                         submodular function under   cardinality constraint  the celebrated greedy algorithm which iteratively adds the element whose marginal
contribution is maximal  obtains an approximation guarantee of       Nemhauser et al    This is optimal unless   NP  Feige  or alternatively  assuming polynomiallymany function evaluations  Nemhauser   Wolsey   
In recent years  there have been various adaptations of
the classic greedy algorithm to allow for scalable  distributed  and noiseresilient optimization  Most notably 
the STOCHASTICGREEDY algorithm recently proposed
by  Mirzasoleiman et al    is   lineartime algorithm
which at every step takes an element  which approximates
in expectation the element with the maximal marginal contribution  Mirzasoleiman et al  show that STOCHASTICGREEDY gives an approximation guarantee which is arbitrarily close to        in expectation  and does very well
in practice  Mirzasoleiman et al    Lucic et al   
Variants of this algorithm are used in clustering  Malioutov
et al    sparsi cation  Lindgren et al    Gaussian RBF kernels  Sharma et al    sensing  Li et al 
  and social data analysis  Zhuang et al   
More generally    greedy algorithm that iteratively adds elements that are only approximately maximal in expectation  may not necessarily be due to   design decision  but
rather an artifact of its application on large and noisy data
sets  see       Azaria et al    One can model this
uncertainty with   probability distribution    at every iteration    value       is being sampled  and the greedy
algorithm adds an element whose marginal contribution is
   approximation to the maximal marginal contribution 
In general  we refer to an algorithm that iteratively adds an
element whose marginal contribution is approximately optimal in expectation as   stochastic greedy algorithm  It is
easy to show that stochastic greedy algorithms give an approximation ratio of         in expectation  where   is
the mean of the distribution modeling the uncertainty  This
however is   weak guarantee as it leaves   nonnegligible

Robust Guarantees of Stochastic Greedy Algorithms

likelihood that the algorithm terminates with   solution
with   poor approximation guarantee  Indeed  as we later
show  there are cases where stochastic greedy algorithms
have desirable guarantees in expectation  but with constant
probability have arbitrarily bad approximation guarantees 

Do stochastic optimization algorithms for submodular
maximization have robust approximation guarantees 

  Our results

We prove the following results 

  Optimization under cardinality constraints  For
the problem of maximizing   monotone submodular
function under cardinality constraint    with uncertainty distribution   with expectation   we show that
for any         when      
    stochastic greedy
algorithm obtains an approximation of        
     at least        Furthermore  we prove that
this bound is optimal by showing that for any      
no algorithm can obtain an approximation ratio better
than                    For the special case in
which the function is modular  we prove an improved
bound of            at least         

  Optimization under matroid constraints  To further study the difference between guarantees that occur in expectation and guarantees which appear       
we study   generalization  where the greedy algorithm
is used to maximize   monotone submodular function
under intersection of matroid constraints  where   distribution   with mean   generates uncertainty  We
show that in this case  with   matroids the algorithm
obtains an approximation ratio of        in expectation  However  we show that even for   single matroid no algorithm can give    nite approximate guarantee      at least     implying that in general
stochastic greedy algorithms cannot obtain high probability guarantees under general matroid constraints 

  Stochastic local search  Finally    natural alternative
to greedy is local search  We show that even for cardinality constraints local search performs poorly  and
does not give any meaningful approximation guarantees when there is probabilistic uncertainty about the
quality of the elements  We contrast this with the
case where there is deterministic uncertainty about the
quality of the elements  in which we get an approximation ratio of       where   is our uncertainty  This implies that local search does not enjoy
the same robustness guarantees of the greedy algorithms under uncertainty of the quality of elements 

  Applications

The above results have several immediate consequences 
  Fast algorithms for submodular optimization  Our
analysis applies to the STOCHASTICGREEDY algorithm  Mirzasoleiman et al    thus showing its
approximation guarantee holds with high probability 
implying it is the optimal algorithm in terms of running time and approximation guarantee for the problem of maximizing   submodular function under   cardinality constraint    The same guarantee holds for the
variants studied in  Malioutov et al    Lindgren
et al    Sharma et al    Li et al   

  Submodular optimization under noise 

In  Hassidim   Singer    the problem of maximization
of submodular functions under   cardinality constraint
is considered when given access to   noisy oracle  Our
result simpli es the analysis of one of the algorithms
in this setting  and gives high probability results for
the inconsistent noise model  Singla et al   

  Organization of the paper

We describe the results for general monotone submodular
functions in Section   and the improved analysis for modular functions in Section   In Section   we consider the
more general problem of maximizing   submodular function under matroid constraints  and show the inapproximabilities of stochastic local search algorithms in Section  
Finally  we discuss experiments in Section  

  Submodular Functions
In this section we analyze the stochastic greedy algorithm
for general monotone submodular functions  We  rst analyze the algorithm  and then show the bound is tight 

  Upper bound

For   given cardinality constraint    the standard greedy algorithm begins with the empty set as its solution and at each
step              adds the element whose marginal contribution to the existing solution is largest  In the stochastic version  the algorithm may no longer add the element whose
marginal contribution is largest  Rather  the algorithm adds
the element whose marginal contribution is at least   factor of   from the maximal marginal contribution  where  
is drawn       from some distribution   with mean   We
give   formal description below 

 Formally  the algorithm in  Mirzasoleiman et al    does
not assume that the expected marginal contribution of the element
selected is approximately optimal  but rather that in expectation
its marginal contribution approximates that of some element in
the optimal solution  Nevertheless our analysis still applies 

Robust Guarantees of Stochastic Greedy Algorithms

Algorithm   STOCHASTICGREEDY
input  
       
  while         do
     
 
        arbitrary        fS        maxx   fS   
 
  end while
output  

The following lemma shows that when the sampled mean
of the distribution is close to   stochastic greedy algorithms obtain   near optimal performance guarantee 
Lemma   Let   be the set of   elements selected by  
in each iteration        
stochastic greedy algorithm     
the algorithm selects an element whose marginal contribution is an    approximation to the marginal contribution of
the element with the largest marginal contribution at that
stage  and let      
 

       Then 

 cid  

 cid 

 cid 

       

     
  

OPT 

Proof  Let Si               ai  and let the optimal solution
be             argmaxT             Note that for any
      we have that 

   Si       Si    fSi ai 
      max
   
     
 
   

fSi   

Rearranging  we get 

   Si       
 

           Si 

     Si 

 

We will show by induction that in every iteration        

 
 
     
 

 cid 
      cid 

  

 cid 

fSi   
        Si       Si 
            Si 

 cid 
 cid       
 cid 

 cid 

      
 

 cid 

   Si   

 cid 

The base case is when       and        and by  

           
 

             

     

     
 

     

For   general iteration       applying   for iteration      

and using the inductive hypothesis 

   Si 
     
 

       

       

     
 

      cid 

 

 cid 

  

 cid 
 cid 

       
 

       
 

   Si 

 cid 
 cid      cid 
 cid       

  

      
 

 cid 

      
 

 cid       

Since              the above inequality implies 

           Sk 

 

      cid 
 cid 
      
 cid 
 
 cid  
 cid      cid       

        

     

  

 cid 

 

 cid       

     

We can now apply concentration bounds on the previous
lemma and prove the main theorem 
Theorem   Let   be   monotone submodular function 
which is evaluated with uncertainty coming from   distribution   with mean   For any         suppose  
stochastic greedy algorithm is being used with      
   
Then                

the algorithm returns       

 

 cid 

       

   

 

  

OPT

Proof  Consider an application of the stochastic greedy algorithm with mean   and let              be the approximations to the marginal contributions made by       samples from   distribution in all iterations              Since all
the values     
   drawn from the distribution are bounded
from above by   by the Chernoff bound we have 

 cid 

  cid 

  

 
 

Pr

          

  

 

   

 cid 

 cid 

By applying Lemma   we get our result 

  Tight lower bound
Claim   For any         the competitive ratio of
stochastic greedy with mean   is at most           
with probability at least      

Proof  Consider maximizing   submodular function when
the oracle has probability   of returning the element with

Robust Guarantees of Stochastic Greedy Algorithms

the largest marginal contribution  and probability       to
return   random element  We present an instance where
greedy returns an approximation ratio of at most      
   with probability at least       We use the same bad
instance regardless of  
The construction is as follows  There are   special elements          plain elements and              
dummy elements  note that the total number of elements
is not    The value       depends only on the number of
special elements  plain elements and dummy elements contained in    all special elements are identical  Moreover 
dummy elements contribute nothing to    and hence  we
can write                  where   is the number of special
elements in    and   is the number of plain elements 
For       the value of   depends on   as follows 

          

kk   jkkj ki 
kk     ki     
kk     ki   
kk

          
           
              
              

 

Note that for       we have                    and
          Also  one can verify  by caseby case analysis
that this function is indeed monotone and submodular 
Since                    for every       as long as the
greedy algorithm did not choose any special element yet 
the marginal contribution of   special elements is equal to
the marginal contribution of   plain element 
Let   be the number of times in which the oracle supplied the greedy algorithm with the element with the maximal marginal contribution  By an additive version of
the Chernoff bound we have that with probability at least
      log           

 

          

  log  

condition on this event  Assuming by induction that the last
      non dummy elements which were chosen by greedy
were plain  the probability that   th non dummy element is
plain is      
    Taking   union bound over all   such
elements  gives that with probability        all   nondummy elements chosen by greedy were plain 
Combining this together with   union bound  we get that
with probability at least        greedy chose no special
 
  log   plain elements  and
elements  at most           
the rest are dummy elements that do not contribute to the
value of the function  This means that the value of the solution is at most kk        tkk    and thus with probability
at least        the ratio between the value of greedy and
the optimal value is at least 

kk         tkk  

kk

     

     

 cid       
 cid cid 

 

 cid  
 cid   cid    log   

 

     
 

         log   
             

 

  

Choosing       we get our desired bound 

we have that          cid 

  Modular Functions
In this section we show   tight upper bound for the special case in which the function is modular  Recall that  
function            is modular if for every set      
          Note that in this case
fS            for all       and       
Theorem   Let       be the set returned when applying
  stochastic greedy algorithm with mean         on  
modular function             Then  for any        
when      

               

We condition on this event  Next  we argue that with probability at least        all the elements for which the algorithm selected   random element       did not take an
element whose marginal contribution is     approximation
to the largest marginal contribution  are dummy elements 
Consider one of the times in which greedy was given   random element  The probability that it was not   dummy el 
           Applying   union bound 
ement is at most
the probability that in the       cases in which greedy was
supplied with   random element it was always   dummy
element is at least         We condition on this event 
We will now argue that with probability at least    all
the nondummy elements selected are plain  Consider the
 rst non dummy element chosen by the algorithm  With
probability at least          it is   plain element  We

   

             OPT 

Proof  Suppose that at every stage         element ai    
is selected and its marginal contribution is at least    of
the optimal marginal contribution at that stage  Let   be
the optimal solution and   cid    argmaxo           where
  is the solution retuned by the algorithm          
            ak  The basic idea in this proof is to observe
that since   cid  is not in the solution and throughout the iterations of the algorithm is always   feasible candidate  this
implies that every element ai in the solution     has value
 cid 
at least as large as  if    cid  Intuitively  if there are enough
elements in       for concentration bounds to kick in  we
                 and we would be
have that
done since                                        

     

Robust Guarantees of Stochastic Greedy Algorithms

The problem is that      may not be suf ciently large  and
we therefore need slightly more nuanced arguments 
We will partition       to two disjoint sets of low and high
valued elements                                cid  and
                              cid  Notice that 

     
                           
       
 

       

 cid 
   cid 

     

     

     

 cid 
 cid 

   

   

     cid   

     cid   

     cid   

           cid   

           cid 

 cid 
 cid 
 cid 

   

   

   

 cid 

 cid 

          cid 

Since   cid          it is   feasible choice for the algorithm at
every stage  and therefore by the de nition of the stochastic
greedy algorithm  for every element ai     we have that 

   ai       max
   

fS         max
     

         if    cid 

Thus  for       with probability          

 

 

     

 cid 

   

     

     
                           
 

   ai   

 cid 
   cid 

ai    

ai    

       cid 

       cid 

       cid 

ai  

  

   

   

ai  

   ai   

 if    cid   

 cid 

 cid 
 cid 
   
 cid 
   
 cid 
 cid 
   
 cid 
 cid 
 cid 
 cid 
 cid 

   

   

  

  

   

ai      

 if    cid   

   cid 
   cid 
   cid 
 cid 

ai      

ai        

     

 cid 

          kf    cid   

          cid 

 cid 
 cid cid 

     cid              cid 

 cid 

          cid 

 cid 

 

 

   

kf    cid   

          cid 

       
            
where inequality   is due to the fact that      cid     if    cid 
since        inequality   is an application of the Chernoff bound for       the last inequality is due to the
upper bound we established on      

The upper bound is tight  An obvious lower bound
holds for the degenerate case where in every stage the
marginal contribution of the element returned is          
approximation to the maximal marginal contribution with
probability   Clearly in this case  the approximation ratio is no better than    consider        elements where  
elements have value   and   elements have value  

  General Matroid Constraints
In this section we consider the more general problem of
maximizing   monotone submodular function under matroid constraints  Recall that   matroid is   pair      
where   is the ground set and   is   family of subsets
of   called independent that respects two axioms 
 
         cid          cid      and   if          and
          then                               The rank
of the matroid is the size of the largest set in    The cardinality constraint  is   special case of optimization under
matroid constraints  where the matroid is uniform 

Submodular maximization under matroid constraints 
The greedy algorithm for maximization under matroid constraints is   simple generalization of the uniform case  the
algorithm iteratively identi es the element whose marginal
contribution is maximal and adds it to the solution if it does
not not violate the matroid constraint       if adding the element to the set keeps the set in the family of feasible sets
   This algorithm obtains an approximation ratio of    
More generally  for an intersection of   matroids  this algorithm obtains an approximation guarantee of        

Stochastic greedy under intersection of matroids 
Consider an intersection of matroids    monotone submodular function   de ned over the independent sets  and an
uncertainty distribution   with mean   The stochastic
greedy algorithm begins with the solution       and set of
elements not yet considered        In every iteration the
algorithm maintains   solution   of elements that are in the
intersection of the   matroids and   value       is sampled  The algorithm then considers an arbitrary element
      whose marginal contribution is   maxx   fS   
and adds   to   if       is independent in all   matroids 
and discards   from   

  Stochastic greedy fails with high probability

We  rst show that unlike the special case of uniform matroids  even for   single matroid  it is generally impossible
to obtain high probability guarantees for maximization un 

 We note that unlike the uniform case  here greedy is not optimal  The optimal guarantee of        can be obtained via an
algorithm based on   continuous relaxation  Vondr      or
through local search  Filmus   Ward   

Robust Guarantees of Stochastic Greedy Algorithms

der matroid constraints  even when the function is modular
and the rank of the matroid is suf ciently large 
Claim   Even for   modular function and arbitrarily large
     stochastic greedy algorithm with mean   cannot obtain
an approximation better than   with probability greater
than        for maximization under matroid of rank   

Proof  Consider the following example  where the ground
set has two types of elements                 am  and    
            bk  where        The rank of the matroid
is    and   set is independent as long as it contains just  
single ai      De ne   modular function            but
   aj      for    cid    and also    bj      for any        
The distribution returns              and   otherwise 
In the  rst iteration of the algorithm  the element    is
correctly evaluated with probability    and with probability       it is evaluated as having value   in which case
we may assume that   random element is selected instead 
Therefore         the algorithm takes    and obtains the
optimal solution  However  if this is not the case  then     
                          the algorithm chooses
an element from   whose value is  
In this case  even
if    is later correctly evaluated it could not be considered
into the solution since its inclusion violates independence 
Hence  while the expected value of greedy is slightly larger
than    with probability at least             the value of
the solution would be  

  The guarantee holds in expectation

Although the approximation guarantee cannot hold with
high probability  we now show that in expectation stochastic greedy algorithms achieves the approximation guarantee of nonstochastic greedy when maximizing   monotone
submodular functions under an intersection of   matroids 
Theorem   Let   denote the intersection of       matroids on the ground set    and            be   monotone
submodular function  The stochastic greedy algorithm returns   solution           
         

OPT

 

      

 

An equivalent algorithm  To simplify the analysis  it
will be useful to consider the equivalent algorithm  which
at every iteration when the existing solution is    discards
all elements   for which            The following claim
due to Nemhauser et al  is later employed in our analysis 
Claim    Prop    in  Nemhauser et al    If for
          and pi    pi  with     pi     then 

        cid   

  cid 

pi       cid 

pi 

Algorithm   STOCHASTICMATROID GREEDY
             
  while    cid    do
                           
 
 
  end while

      
        arbitrary        fS        maxx   fS   

Proof of Lemma   Consider the value obtained by the
following procedure  An adversary chooses some maximal independent set          ak  Let Si                  ai 
with        and for every         let   cid 
  be the element
that maximizes the marginal contribution given Si  where
the maximization is over elements   such that Si       is
independent in all   matroids  That is   cid 

  is de ned as 

max

Si     fSi   

The value of the procedure is then 

  cid 

  

fSi    cid 
   

We will bound the value obtained by the procedure against
that of the optimal solution  and then argue that the value
obtained by the stochastic greedy is equivalent 
Let   denote the optimal solution  We have that 

               Sk       Sk   

fSk    

 

 cid 

    Sk

For   set   and   matroid Mp in the family    we de ne
rp    called the rank of   in Mp to be the cardinality of
the largest subset of   which is independent in Mp  and
de ne spp    called the span of   in Mp by 

spp               rp          rp   

If   is independent in Mp  we have that rp spp     
rp          In particular  we have that rp spp Si     
for every Si  Now in each             since   is an
independent set in Mp we have 

rp spp Si           spp Si       

which implies that  spp Si            
De ne Ui     
  spp Si  to be the set of elements which
are not part of the maximization in step       of the procedure  and hence cannot give value at that stage  We have 

  spp Si        cid 

 Ui        

 spp Si      iP

  

  

  

Robust Guarantees of Stochastic Greedy Algorithms

fSk         cid 

  

 cid 
  cid 

    Sk

 cid 
  cid 

  Vi

  

fSi   cid 
   

Let Vi    Ui   Ui      be the elements of   which are
not part of the maximization in step    but were part of the
maximization in step       If     Vi then it must be that 

fSk       fSi      fSi   cid 
   

since   was not chosen in step    Hence  we can upper
bound 

 

 Vi fSi   cid     

fSi   cid 

  

where the last inequality uses  cid  
    Vt     Ui       
    and the arithmetic claim proven in Claim   due
to  Nemhauser et al    Together with   we get 
  cid 

              

fSi    cid 
   

  

Finally  note that STOCHASTICMATROID GREEDY obtains            approximation of the value of the
procedure  in expectation  In each stage  one can add the
element chosen by the algorithm to the procedure  Hence 
at each stage STOCHASTICMATROID GREEDY and the
procedure have the same set of elements available  and the
same   cid 

  which maximizes the marginal contribution 

  Inapproximability of Local Search
In this section we consider variants of stochastic local
search algorithms  We show that unlike the greedy algorithm  stochastic local search algorithms can end up with
arbitrarily bad approximation guarantees 

Local search for submodular maximization  For    
            an  given   set of elements       we will use
    to denote the set without element ai                
 ai    solution   is   local maximum if no single element
ai in   can be exchanged for another element aj not in  
whose marginal contribution to     is greater  That is    is
  local maximum if for every ai     we have that 

fS   ai    max
    

fS     

It is not hard to show that for any monotone submodular function  if   is   local maximum it is     approximation to the optimal solution    local search algorithm begins with an arbitrarily set of size    and at every stage exchanges one of its elements with the element
whose marginal contribution is maximal to the set  until it reaches   local maximum  To guarantee that local

search algorithms converge in polynomial time  the convention is to seek approximate local maxima    solution  
is an  approximate local maximum if no element ai in  
can be exchanged for another element aj not in   whose
marginal contribution to     is greater by   factor of   It
is easy to show that an  approximate local maximum is  
      approximation  Filmus   Ward   

Stochastic local search    natural question is whether
local search enjoys the same robustness guarantees as the
greedy algorithm  We say that   solution   is   stochastic local maximum up to approximation   if no single element in   can be exchanged for another element not in  
whose expected marginal contribution is greater by   factor
  That is    is   stochastic local maximum with mean   if
for every ai     we have that 

  fS   ai        max
    

fS     

If we have uncertaintiy modeled by   distribution    
      solution is   stochastic local maximum         if
for every element ai in   we draw            

fS   ai         max
    

fS     

  stochastic local search algorithm will therefore begin
from an arbitrary solution   of size    and at every iteration swap an element from the solution with an element
outside the solution if   is not   stochastic local maximum
          More speci cally  the stochastic local search algorithm selects an element ai from   and replaces it with
another element aj whose expected marginal contribution
to     is at least fS   ai    and repeats this process until
no such elements are found  This is   similar abstraction of
stochastic greedy algorithms  applicable in settings when
one cannot evaluate the optimal marginal contribution exactly  but approximately well in expectation 

Consistent and inconsistent stochasticity  We consider
two approaches to model the way in which the random variables    are assigned to an element ai in the solution 

  Consistent  For each element ai         is   random
variable drawn independently from         and
 xed for the entire run of the algorithm 

  Inconsistent  At each step of the algorithm  for every element ai         is   random variable drawn
independently from   

Note that the solution converges  as the distribution  
makes the algorithm more conservative 

Robust Guarantees of Stochastic Greedy Algorithms

Figure   Function value as   function of solution size for greedy and local search with uncertainty

Inapproximability of stochastic local search  We show
that in both consistent and inconsistent models  stochastic
local search performs poorly  even for modular functions 
Consider   setting where there are   elements  and   modular function  For every       we have    ai       for
some negligible                     but           As
for           it returns   and        Assume      
Lemma   The expected approximation guarantee of
stochastic local search is at most          

Proof  At the  rst iteration  local search chooses an 
If
      we are done  and this is   local maxima  Otherwise 
local search chooses an  At iteration   local search starts
with ai  halts         the probability that   outputs  
and otherwise continues  The probability that it will not
halt for   steps and reach ai is           

We note that in the above proof we assumed that the local
search algorithm chooses an arbitrary element at every iteration  If one allows the stochastic local search to randomly
choose an element in every iteration   similar construction
shows an inapproximability of    log  

       

applied

to

algorithms

  Experiments
egonetwork
an
the
We
from  Leskovec   Krevl   
This network has
  nodes and   edges  The submodular function
we used is coverage  which models in uence in social
networks  In order to emphasize the implications of having
results        the graphs do not depict the average of many
runs  but instead each graph is   single run of the algorithm 
In greedy  we present the value of the solution at each
iteration    In local search and in random we sort elements
of the solution according to marginal contributions 
We start with greedy and describe the different distributions we used to model uncertainty  The same distributions
were used for local search  Both left panes include the
greedy algorithm without uncertainty  greedy  blue line 
and choosing   random set  random  black line  When
running stochastic greedy  we  rst sample   value       

and then pick   random element out of the elements that
have marginal contribution at least   maxa fS    The distribution   varies between the different lines  In the leftmost pane  APX  red line  depicts     which is the constant distribution   In Stochastic greedy with mean  
  is the uniform distribution on      purple line  and
in Stochastic greedy with mean     is the uniform distribution on    
It is expected that APX will behave
smoothly  as   is   degenerate distribution in this case
 note that there is still randomization in which element to
choose at every stage out of the eligible elements  However  we see that the      result kicks in  and the APX line
is similar  across many values of    to stochastic greedy
with mean   Raising the mean to   makes stochastic greedy behave almost like greedy when   gets large 
so in some cases stochastic greedy makes the same choice
greedy would make 
In the second pane  The purple line  exponential  depicts
  as an exponential distribution with       which gives
  mean of   The red line is uniform in     and the
yellow is   Gaussian with       and       We see
that all graphs are further away from Greedy compared to
the leftmost pane  and that higher variance is generally not
  good thing  although the differences are small 
The two right panes depict the same noise distributions as
the two left panes  but this time we use local search  or
stochastic local search  instead of greedy  It is easy to see
that   affects local search more than it affects greedy  The
plateau is caused since we sort the  nal solution and then
plot the elements  and since if some elements have   low
value of    they are likely to stay in the solution even if
they contribute very little  as in Lemma  

  Acknowledgements
     was supported by ISF        was supported
by NSF grant CCF  CAREER CCF   
Google Faculty Research Award  and   Facebook Faculty
Gift  We thank Andreas Krause for pointing the connection
between our result and  Mirzasoleiman et al   

llllllllllllllllllllllllllllll budget kfunction valuelllGREEDYAPX GREEDY    apx  STOCHASTIC GREEDY    mean  STOCHASTIC GREEDY    mean  RANDOMllllllllllllllllllllllllllllll budget kfunction valuelllGREEDYuniformexponentialnormalRANDOMllllllllllllllllllllllllllllll budget kfunction valuelllLSAPX LS    apx  STOCHASTIC LS    mean  STOCHASTIC LS    mean  RANDOMllllllllllllllllllllllllllllll budget kfunction valuelllLSuniformexponentialnormalRANDOMRobust Guarantees of Stochastic Greedy Algorithms

Vondr    Jan  Optimal approximation for the submodular
In STOC 

welfare problem in the value oracle model 
 

Zhuang  Hao  Rahman  Rameez  Hu  Xia  Guo  Tian  Hui 
Pan  and Aberer  Karl  Data summarization with social
contexts  In CIKM   

References
Azaria  Amos  Hassidim  Avinatan  Kraus  Sarit  Eshkol 
Adi  Weintraub  Ofer  and Netanely  Irit  Movie recommender system for pro   maximization  In Proceedings
of the  th ACM conference on Recommender systems 
pp    ACM   

Feige  Uriel    threshold of ln   for approximating set

cover  Journal of the ACM   

Filmus  Yuval and Ward  Justin    tight combinatorial algorithm for submodular maximization subject to   matroid constraint  In FOCS   

Hassidim  Avinatan and Singer  Yaron  Submodular opti 

mization under noise  In COLT   

Leskovec  Jure and Krevl  Andrej  SNAP Datasets  Stanford large network dataset collection  http snap 
stanford edu data  June  

Li  Chengtao  Sra  Suvrit  and Jegelka  Stefanie  Gaussian
quadrature for matrix inverse forms with applications  In
ICML   

Lindgren  Erik  Wu  Shanshan  and Dimakis  Alexandros    Leveraging sparsity for ef cient submodular
data summarization  In NIPS   

Lucic  Mario  Bachem  Olivier  Zadimoghaddam  Morteza 
and Krause  Andreas  Horizontally scalable submodular
maximization  In ICML   

Malioutov  Dmitry  Kumar  Abhishek  and Yen  Ian EnHsu  Largescale submodular greedy exemplar selection
with structured similarity matrices  In UAI   

Mirzasoleiman  Baharan  Badanidiyuru  Ashwinkumar 
Karbasi  Amin  Vondr    Jan  and Krause  Andreas 
Lazier than lazy greedy  In AAAI   

Nemhauser  George   and Wolsey  Leonard    Best algorithms for approximating the maximum of   submodular
set function  Mathematics of operations research   
   

Nemhauser  George    Wolsey  Laurence    and Fisher 
Marshall    An analysis of approximations for maximizing submodular set functions    Mathematical Programming     

Sharma  Dravyansh  Kapoor  Ashish  and Deshpande 
In ICML 

Amit  On greedy maximization of entropy 
 

Singla  Adish  Tschiatschek  Sebastian  and Krause  Andreas  Noisy submodular maximization via adaptive
sampling with applications to crowdsourced image collection summarization  In AAAI   

