Optimal Densi cation for Fast and Accurate Minwise Hashing

Anshumali Shrivastava  

Abstract

Minwise hashing is   fundamental and one of
the most successful hashing algorithm in the literature  Recent advances based on the idea of
densi cation  Shrivastava   Li        have
shown that it is possible to compute   minwise hashes  of   vector with   nonzeros  in
mere         computations    signi cant improvement over the classical   dk  These advances have led to an algorithmic improvement
in the query complexity of traditional indexing
algorithms based on minwise hashing  Unfortunately  the variance of the current densi cation
techniques is unnecessarily high  which leads to
signi cantly poor accuracy compared to vanilla
minwise hashing  especially when the data is
sparse  In this paper  we provide   novel densi 
cation scheme which relies on carefully tailored
 universal hashes  We show that the proposed
scheme is varianceoptimal  and without losing
the runtime ef ciency  it is signi cantly more accurate than existing densi cation techniques  As
  result  we obtain   signi cantly ef cient hashing scheme which has the same variance and
collision probability as minwise hashing  Experimental evaluations on real sparse and highdimensional datasets validate our claims  We believe that given the signi cant advantages  our
method will replace minwise hashing implementations in practice 

  Introduction and Motivation
Recent years have witnessed   dramatic increase in the dimensionality of modern datasets   Weinberger et al   
show dataset with   trillion   unique features  Many
studies have shown that the accuracy of models keeps
climbing slowly with exponential increase in dimensionality  Large dictionary based representation for images 

 Rice University  Houston  TX  USA  Correspondence to  An 

shumali Shrivastava  anshumali rice edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

speech  and text are quite popular  Broder    Fetterly
et al    Enriching features with cooccurrence information leads to blow up in the dimensionality   grams
are common for text representations  With vocabulary
size of    grams representation requires dimensionality of   Representing genome sequences with features
consisting of  contiguous characters  or higher   Ondov
et al    leads to around       dimensions 
To deal with the overwhelming dimensionality  there is an
increased emphasis on the use of hashing algorithms  such
as minwise hashing  Minwise hashing provides   convenient way to obtain   compact representation of the data 
without worrying about the actual dimensionality  These
compact representations are directly used in large scale
data processing systems for   variety of tasks 
Minwise hashing is de ned for binary vectors  Binary vectors can also be equivalently viewed as sets  over the universe of all the features  containing only attributes corresponding to the nonzero entries  Minwise hashing belongs
to the Locality Sensitive Hashing  LSH  family  Broder
et al    Charikar    The method applies   random permutation  or random hash function           
on the given set       and stores the minimum value after
the permutation mapping  Formally 

       min   

 

Given sets    and    it can be shown by elementary probability arguments that

                 

        
             

The quantity

   

        
          

 

           

 

 

 

is the well known Jaccard Similarity  or resemblance   
which is the most popular similarity measure in information retrieval applications  Broder   
The probability of collision  equality of hash values  under minwise hashing  is equal to the similarity of interest
   This particular property  also known as the LSH property  Indyk   Motwani    Charikar    makes minwise hash functions    suitable for creating hash buckets 

Optimal Densi cation for Fast and Accurate Minwise Hashing

which leads to sublinear algorithms for similarity search 
Because of this same LSH property  minwise hashing is
  popular indexing technique for   variety of largescale
data processing applications  which include duplicate detection  Broder    Henzinger    allpair similarity  Bayardo et al   
temporal correlation  Chien
  Immorlica    graph algorithms  Buehrer   Chellapilla    Chierichetti et al    Najork et al   
hashing time series  Luo   Shrivastava    and more 
It was recently shown that the LSH property of minwise
hashes can be used to generate kernel features for largescale learning  Li et al   
Minwise hashing is known to be theoretical optimal in
many scenarios  Bavarian et al    Furthermore  it was
recently shown to be provably superior LSH for angular
similarity  or cosine similarity  compared to widely popular Signed Random Projections  Shrivastava   Li     
These unique advantages make minwise hashing arguably
the strongest hashing algorithm both in theory and practice 
Hashing Cost is Bottleneck  The  rst step of algorithms
relying on minwise hashing is to generate  some large
enough    minwise hashes  or  ngerprints  of the data vecIn particular  for every data vector    hi        
tors 
         is repeatedly computed with independent permutations  or hash functions  These   hashes are used for
  variety of data mining tasks such as cheap similarity estimation  indexing for sublinear search  kernel features for
large scale learning  etc  Computing   hashes of   vector
  with traditional minwise hashing requires   dk  computation  where   is the number of nonzeros in vector   
This computation of the multiple hashes requires multiple
passes over the data  The number of hashes required by
the famous LSH algorithm is      which grows with the
size of the data   Li    showed the necessity of around
  hashes per data vector in largescale learning  Note 
advances in ef cient weighted minwise hashing by  Shrivastava    is not applicable for very sparse unweighed
minwise hashing 
Other Related Fast Sketches are not LSH  Two notable techniques for estimating Jaccard Similarity are   
bottomk sketches and   one permutation hashing  Li
et al    Although these two sketches are cheap
they do not satisfy the key LSH property
to compute 
and therefore are unsuitable for replacing minwise hashing  Shrivastava   Li        There are also substantial
empirical evidence that using these  nonLSH  sketches for
indexing leads to   drastic bias in the expected behavior 
leading to poor accuracy 
The Idea of  Densi ed  One Permutation Hashing  Recently   Shrivastava   Li      showed   technique for
densifying sparse sketches from one permutation hashing
which provably removes the bias associated with one per 

mutation hashing  Please see Section   for details  The
scheme is much cheaper than the later discovered alternatives which rely on continued sampling  or multiple hashes 
until no empty bins are left  Haeupler et al    Dahlgaard et al   
 Shrivastava   Li      was the
 rst success in creating ef cient hashing scheme which
satis es the LSH property analogous to minwise hashing
and at the same time the complete process only requires
         computations instead of the traditional bottleneck
of   dk  and only one hash function 
Current Densi cation is Inaccurate For Very Sparse
Datasets  The densi cation process although ef cient and
unbiased was shown to have unnecessarily higher variance 
It was shown in  Shrivastava   Li      that the traditional  densi cation  lacks suf cient randomness  It was
further revealed that densi cation could be provably improved by using   extra random bits  An improved variance was associated with   signi cant performance gain in
the task of nearneighbor search  In this work  we show
that even the improved densi cation scheme is far from
optimal  The  ndings of  Shrivastava   Li      leaves
an open curiosity  What is the best variance that can be
achieved with  densi cation  without sacri cing the running time  We close this by providing   varianceoptimal
scheme 
Our Contributions  We show that the existing densi cation schemes  for fast minwise hashing  are not only suboptimal but  worse  their variances do not go to zero with
increasing number of hashes  The variance with an increase
in the number of hashes converges to   positive constant 
This behavior implies that increasing the number of hashes
after   point will lead to no improvement  which is against
the popular belief that accuracy of randomized algorithms
keeps improving with an increase in the number of hashes 
To circumvent these issues we present   novel densi cation
scheme which has provably superior variance compared to
existing schemes  We show that our proposal has the optimal variance that can be achieved by densi cation  Furthermore  the variance of new methodology converges to zero
with an increase in the number of hashes    desirable behavior absent in prior works  Our proposal makes novel use of
 universal hashing which could be of independent interest
in itself  The bene ts of improved accuracy come with no
loss in computational requirements  and our scheme retains
the running time ef ciency of the densi cation 
We provide rigorous experimental evaluations of existing
solutions concerning both accuracy and running time ef 
ciency  on real highdimensional datasets  Our experiments
validate all our theoretical claims and show signi cant improvement in accuracy  comparable to minwise hashing 
with   signi cant gain in computational ef ciency 

Optimal Densi cation for Fast and Accurate Minwise Hashing

  Important Notations and Concepts
Equation         the LSH Property  leads to an estimator of
Jacard Similarity    using   hashes  de ned by 

  cid 

  

    

 
 

 hi      hi   

 

An obvious computational advantage of this scheme is that
it is likely to generate many hash values  at most    and only
requires one permutation   and only pass over the sets  or
binary vectors     It was shown that for any two sets   
and    we have   conditional collision probability similar
to minwise hashing 

Let Ei    cid hOP  
    cid hOP  
However      cid hOP  

 

 

 

      hOP  

 

      hOP  

 

 

        cid 
   cid cid  Ei    cid     
   cid cid  Ei    cid   cid   

 

 

 

      hOP  

Here Ei is an indicator random variable of the event that
the ith partition corresponding to both    and    are empty 
See Figure  
Any bin has   constant chance of being empty  Thus  there
is   positive probability of the event  Ei     for any
given pair    and    and hence for large datasets  big   
  constant fraction of data will consist of simultaneously
empty bins  there are        trials for the bad event  Ei  
  to happen  This fraction further increases signi cantly
with the sparsity of the data and    as both sparsity and  
increases the probability of the bad event  Ei     See
Table   for statistics of empty bins on real scenarios 
Unfortunately  whenever the outcome of the random permutation leads to simultaneous empty bins      
event
Ei     the LSH Property is not valid  In fact  there is not
suf cient information present in the simultaneous empty
partitions for any meaningful statistics  Hence  one permutation hashing cannot be used as an LSH  Simple heuristics
of handling empty bins as suggested in  Shrivastava   Li 
    leads to   signi cant bias and it was shown both theoretically and empirically that this bias leads to signi cant
deviation from the expected behavior of one permutation
hashing when compared with minwise hashing  Thus  one
permutation hashing although computationally lucrative is
not   suitable replacement for minwise hashing 

  The Idea of Densi cation

In  Shrivastava   Li      the authors proposed  densi cation  or reassignment of values to empty bins by
reusing the information in the nonempty bins to    the
bias of one permutation hashing  The overall procedure is
quite simple  Any empty bin borrows the values of the closest nonempty bins towards the circular right  or left  See
Figure   for an illustration  Since the positions of empty

Here   is the indicator function  In the paper  by variance 
we mean the variance of the above estimator  Notations like
  ar    will mean the variance of the above estimator
when the    is used as the hash function 
    will denote the set of integers            denotes
the number of points  samples  in the dataset    will be
used for dimensionality  We will use min    to denote the
minimum element of the set      permutation          
applied to   set   is another set     where       if and
only if           Our hashing will generate   hashes
hi              generally from different bins  Since
they all have same distribution and properties we will drop
subscripts  We will use   and    to denote the hashing
schemes of  Shrivastava   Li      and  Shrivastava  
Li      respectively 

  Background  Fast Minwise Hashing via

Densi cation

   Universal Hashing
De nitions    randomized function huniv             is
 universal if for all             with    cid     we have the
following property for any                 huniv     
   and huniv            
  
 Carter   Wegman    showed that the simplest way to
create    universal hashing scheme is to pick   prime number        sample two random numbers      and compute
huniv       ax      mod    mod  

  One Permutation Hashing and Empty Bins

It was shown in  Li et al    Dahlgaard et al   
that instead of computing the global minimum in Equation               min    an ef cient way to generate   sketches  using one permutation  is to  rst bin the
range space of          into   disjoint and equal partitions
followed by computing minimum in each bin  or partition 
Let    denote the ith partition of the range space of       
  Formally  the ith one permutation hashes  OPH  of   set
  is de ned as

 cid 

hOP  
 

     

min         
  

if            cid   
otherwise 

 

 In  Shrivastava   Li      they also needed an offset because the value of   hash in any bin was always reset between
    We do not need the offset if we use the actual values of
   

Optimal Densi cation for Fast and Accurate Minwise Hashing

  Lack of Randomness in Densi cation

It was pointed out in  Shrivastava   Li      that the
densi cation scheme of  Shrivastava   Li      has unnecessarily high variance  In particular  the probability of
two empty bins borrowing the information of the same nonempty bin was signi cantly higher  This probability was
due to poor randomization  load balancing  which hurts the
variance 
 Shrivastava   Li      showed that infusing
more randomness in the reassignment process by utilizing
  extra random bits provably improves the variance  See
Figure   for an example illustration of the method  The running time of the improved scheme was again          for
computing   hashes  This improvement retains the required
LSH property  however this time with improved variance 
An improved variance led to signi cant savings in the task
of nearneighbor search on real sparse datasets 

  Issues with Current Densi cation
Our careful analysis reveals that the variance  even with the
improved scheme  is still signi cantly higher  Worse  even
in the extreme case when we take       the variance
converges to   positive constant rather than zero  which implies that even with in nite samples  the variance will not
be zero  This positive limit further increases with the sparsity of the dataset  In particular  we have the following theorem about the limiting variances of existing techniques 
Theorem   Give any two  nite sets           with    
                            and           The
limiting variance of the estimators from densi cation and
improved densi cation when           is given by 
 cid 

 cid 
 cid       
 cid                      

       

   

 

 
 

   
 

   

 

     ar     
lim

lim
     ar     
 
 

           

Figure   Illustration of One Permutaion Hashing  OPH  and
the two existing Densi cation Schemes of  Shrivastava   Li 
      Densi cation simply borrows the value from nearby empty bins  Different sets Si with have different pattern of
empty nonempty bins  Since the pattern is random  the process is
similar to random reuse of unbiased values  which satis es LSH
property in each bin  Shrivastava   Li     

and nonempty bins were random  it was shown that densi cation  or reassignment  was equivalent to   stochastic
reselection of one hash from   set of existing informative
 coming from nonempty bins  hashes which have the LSH
property  This kind of reassignment restores the LSH property and collision probability for any two hashes  after reassignment  is exactly same as that of minwise hashing 
The densi cation generates   hashes with the required LSH
Property and only requires two passes over the one permutation sketches making the total cost of one permutation
hashing plus densi cation          This was   signi cant
improvement over   dk  with classical minwise hashing 
         led to an algorithmic improvement over randomized algorithms relying on minwise hashing  as hash computation cost is bottleneck step in all of them 

This convergence of variance to   constant value  despite
in nite samples  of the existing densi cation is also evident in our experimental  ndings  see Figure   where we
observe that the MSE  Mean Square Error  curves go  at
with increasing    Similar phenomenon was also reported
in  Shrivastava   Li      It should be noted that for
classical minwise hashing the variance is     
      for
any pair    and    Thus  current densi cation  although
fast  loses signi cantly in terms of accuracy  We remove
this issue with densi cation  In particular  we show in Theorem   that the limiting variance of the proposed optimal
densi cation goes to   Our experimental  ndings suggests
that the new variance is very close to the classical minwise hashing  In addition  the new densi cation retains the
speed of existing densi ed hashing thereby achieving the
best of the both worlds 

                              leads to                                                    Partition Range   into   Bins       One Permutation Hashing  GET MIN IN BIN    if EMPTY BIN                                                                                                     Densification   REASSIGN FROM RIGHT  CIRCULAR                                                                                                                   Improved Densification   REASSIGN FROM LEFT OR RIGHT   CIRCULAR  DEPENDING ON RANDOM BITS    randbits                                                                                                                                                                   Optimal Densi cation for Fast and Accurate Minwise Hashing

allowing load sharing between the two ends instead of one
 Bins   and   instead of just   However  the load balancing is far from optimal  The locality of information sharing is the main culprit with current densi cation schemes 
Note  the poor load balancing does not change the expectation but affects the variance signi cantly 
 cid  
For any given pairs of vectors    and    let   be the
number of simultaneous nonempty bins  out of        
   Ei          Note    is   random variable whose
value is different for every pair and depends on the outcome
of random   Formally  the variance analysis of  Shrivastava   Li      reveals that the probability that any two
simultaneous empty bin   and    Ep   Eq     reuses the
   with the densi cation scheme   
same information is
This probability was reduced down to  
   with    by utilizing   extra random bits to promote load balancing 

 

   is not quite perfect load balancing 

In   perp    
fect load balancing with   simultaneous nonempty bins 
the probability of two empty bins hitting the same nonempty bins is at best      
   Can we design   densi cation
scheme which achieves this   while maintaining the consistency of densi cation and at the same time does not hurt the
running time  It is not clear if such   scheme even exists 
We answer this question positively by constructing   densi 
 cation method with precisely all the above requirements 
Furthermore we show that achieving      
  is suf cient
for having the limiting variance of zero 

  Simple  Universal Hashing Doesn   Help

To break the locality of the information reuse and allow  
nonempty bin to borrow information from any other far
off bin consistently  it seems natural to use universal hashing  The hope is to have    universal hash function  Section   huniv             Whenever   bin   is empty 
instead of borrowing information from neighbors  borrow
information from bin huniv    The hash function allows
consistency across any two    and    hence preserves LSH
property  The value of huniv    is uniformly distributed  so
any bin is equally likely  Thus  it seems to break the locality on the  rst thought  If huniv    is also empty then we
continue using huniv huniv    until we reach   nonempty
bins whose value we reuse 
If     huniv     which
One issue is that of cycles 
has  
  chance  then this creates   cycle and the assignment will go into in nite loop    cycle can even occur if
huniv huniv        with both   and huniv    being empty 
Note  the process runs until it  nds   nonempty bin  However  cycles are not just our concern  Even if we manage
to get away with cycles  this scheme does not provide the
required load balancing 
  careful inspection reveals that there is   very signi cant

Figure   Illustration of Poor Load Balancing in the existing Densi cation strategies  The reassignment of bins is very local and
its not uniformly distributed  Thus  we do not use the information
in far off nonempty bins while densi cation 

  Optimal Densi cation
We argue that even with the improved densi cation there
is not enough randomness  or load balancing  in the reassignment process which leads to reduced variance 
For given set    the densi cation process reassigns every
empty bin with   value from one of the existing nonempty
bins  Note  the identity of empty and nonempty bins are
different for different sets  To ensure the LSH property 
the reassignment should be consistent for any given pair
of sets    and    In particular  as noted in  Shrivastava
  Li      given any arbitrary pair    and    whenever any given bin   is simultaneously empty       Ei    
the reassignment of this bin   should mimic the collision
probability of one of the simultaneously nonempty bin  
with Ej     An arbitrary reassignment  or borrow  of
values will not ensure this consistency across all pairs  We
would like to point out that the reassignment of    has no
idea about    or any other object in the dataset  Thus  ensuring the consistency is nontrivial  Although the current
densi cation schemes achieve this consistency by selecting
the nearest nonempty bin  as shown in  Shrivastava   Li 
    they lack suf cient randomness 

  Intuition  Load Balancing

In Figure   observe that if there are many contiguous
nonempty bins  Bins     and   then with densi cation
schemes    all of them are forced to borrow values from the
same nonempty bin  Bin   in the example  Even though
there are other informative bins  Bins     and   their information is never used  This local bias increases the probability     that two empty bins get tied to the same information  even if there are many other informative nonempty
bins  Adding   random bits improves this to some extent by

Example of Poor Load Balancing                                                                                                               Densification   ONLY   INFORMATIVE VALUE USED                                                                                                Improved Densification   ONLY   INFORMATIVE VALUES USED randbits                                                                                                                                                                                  Optimal Densi cation for Fast and Accurate Minwise Hashing

mality  Formally  with the optimal densi cation    we
have the following 

chance that both   and huniv    to be empty for any given
set    Observe that if   and huniv    are both empty  then
we are bound to reuse the information of the same nonempty bin for both empty bins   and huniv    We should
note that we have no control over the positions of empty
and nonempty bins  In fact  if no cycles happen then it
is not dif cult to show that the simple assignment using
universal hashing is equivalent to the original densi cation
  with the order of bins reshuf ed using huniv  It has
worse variance than   

  The Fix  Carefully Tailored  Universal Hashing

Algorithm   Optimal Densi cation
input   One Permutation Hashes hOP       of   
input huniv   

Initialize         
for       to   do

if OP       cid    then
       hOP      

else

attempt    
next   huniv    attempt 
while hOP    next   cid    do

attempt    
next   huniv    attempt 

end while
       hOP    next 

end if
end for
RETURN     

It turns out that there is   way to use universal hashing that
ensures no cycles as well as optimal load balancing  We
describe the complete process in Algorithm   The key is
to use    universal hashing huniv                 which
takes two arguments    The current bin id that needs to be
reassigned and   the number of failed attempt made so far
to reach   nonempty bin  This second argument ensures
no in nite loops as it changes with every attempt  So even
if we reach the same nonempty bin back  cycle  the next
time we will visit   new set of bins  Also  even if both   and
    huniv    attempti  are empty    and   are not bound
to end to the same nonempty bin  This is because in the
next attempt we seek bin value huniv    attempti     for
  which is independent of the huniv    attemptj  due to
 universality of the hash function huniv  Thus  the probability that any two empty bins reuse the information of the
same nonempty bin is  
 

  Analysis and Optimality

We denote the  nal   hashes generated by the proposed
densi cation scheme of Algorithm   using      for opti 

        
            
    
       

   

 
      

 

 

 

Theorem  

    cid            cid   

 
 

lim

  ar     
     ar       
 cid 
 cid 

 Nemp  

     

Nemp Nemp    

    Nemp

 cid 

 cid 

where Nemp is the number of simultaneous empty bins between    and    and the quantities   and   are given by

     

     Nemp     Nemp        Nemp     Nemp    

Nemp Nemp        Nemp    

    Nemp

 

Using the formula for     Nemp      from  Li et al   
we can precisely compute the theoretical variance  The interesting part is that we can formally show that the variance
of the proposed scheme is strictly superior compared to the
densi cation scheme with random bits improvements 

Theorem  

  ar        ar        ar   

 

Theorem   Among all densi cation schemes  where the
reassignment process for bin   is independent of the reassignment process of any other bin    Algorithm   achieves
the best possible variance 

Note  The variance can be reduced if we allow correlations
in the assignment process  for example if we force bin   and
bin   to not pick the same bin during reassignments  this
will reduce   beyond the perfectly random load balancing
value of  
   However  such tied reassignment will require
more memory and computations for generating structured
hash functions  Also  we use only one hash function  or
permutation  If we allow multiple independent hash function then with additional computational and memory cost
the variance can be further reduced 

  Running Time

We show that the expected running time of our proposal 
including all constants  is very similar to the running time
of the existing densi cation schemes 
Given set   with          we are interested in computing
  hash values  The  rst step involves computing   one permutation hashes  or sketches  which only requires   single

Optimal Densi cation for Fast and Accurate Minwise Hashing

Figure   Average MSE in Jaccard Similarity Estimation with the Number of Hash Values     Estimates are averaged over  
repetitions  With Optimal densi cation the variance is very close to costly minwise hashing  which is signi cantly superior to existing
densi cation schemes  Note the log scale of yaxis 
   
 
 
 
 
 
 
 
 

makes          
         The number of sketches is usually of
the order of nonzeros  Even for very good concentration 
the size of the sketches   is rarely much larger than the size
of the set    Even when   is   times   the value of   is
approximately   Thus  the quantity   is negligible 
It should be further noted that the implementation cost of
densi cation scheme    is        which in not very different from the cost of our proposal 

Sim      
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Pair  
Pair  
Pair  
Pair  
Pair  
Pair  
Pair  
Pair  

Table   Statistics of Pairs vectors  binary  with their Similarity
and Sparsity  nonzeros 
pass over the elements of    This takes max              
time  Now the densi cation Algorithm   requires   for loop
of size   and within each for loop  if the bin is empty  it
requires an additional while loop  Let Nemp be the number of empty bins  and therefore   Nemp
is the probability
that the while loop will terminate in one iteration  next is
not empty  Therefore  the expected number of iteration
that each while loop will run is   binomial random variable
  Thus  the expected running time
with expectation
of the algorithm is given by
  Running Time             Nemp   Nemp
              where    

    Nemp
Nemp

  Nemp

 

 

 

Nnot emp

The quantity    which is the ratio of number of empty bins
to the number of nonempty bins  is generally very small 
It is rarely more than   to   in practice  Observe that randomly throwing   items into   bins  the expected number
of empty bins is   Nemp          
    Which

       ke   

  Evaluations
Our aim is to verify the theoretical claims of these papers
empirically  We show that our proposal can replace minwise hashing for all practical purposes  To establish that 
we focus on experiments with the following objectives 
  Verify that our proposed scheme has   signi cantly
better accuracy  variance  than the existing densi cation
schemes  Validate our variance formulas 
  Empirically quantify the impact of optimal variance in
practice  How does this quanti cation change with similarity and sparsity  Verify that the proposal has accuracy
close to vanilla minwise hashing 
  Verify that there is no impact on running time of the proposed scheme over existing densi cation schemes  and our
proposal is signi cantly faster than vanilla minwise hashing  Understand how the running time changes with change
in sparsity and   

  Accuracy
For objectives   and   we selected   different word pairs
embedding  generated from new  corpus  with varying
level of similarity and sparsity  We use the popular termdocument vector representation for each word  The statis 

Num of Hashes    MSE Pair      VanillaImprovedProposedPropTheoryMinHash TheoryNum of Hashes    MSE Pair      VanillaImprovedProposedPropTheoryMinHash TheoryNum of Hashes    MSE Pair      VanillaImprovedProposedPropTheoryMinHash TheoryNum of Hashes    MSE Pair      VanillaImprovedProposedPropTheoryMinHash TheoryNum of Hashes    MSE Pair      VanillaImprovedProposedPropTheoryMinHash TheoryNum of Hashes    MSE Pair      VanillaImprovedProposedPropTheoryMinHash TheoryNum of Hashes    MSE Pair      VanillaImprovedProposedPropTheoryMinHash TheoryNum of Hashes    MSE Pair      VanillaImprovedProposedPropTheoryMinHash TheoryOptimal Densi cation for Fast and Accurate Minwise Hashing

Existing   

 
 
 
 

 
 
 
 

 
 
 
 

This Paper   
 
 
 
 
 
 
 
 

 
 
 
 

RCV 
URL
NEWS 

Vanilla MinHash
 
 
 
 

 
 
 
 

 
 
 
 

Table   Time  in milliseconds  requires to compute     and   hashes of the full data using the existing densi cation  proposed
densi cation and vanilla minwise hashing  Minhash can be    slower for computing   hashes on these datasets 

Avg  nonzeros    

RCV 
URL

News 

 
 
 

Dim    
 

 
 

Samples
 
 
 

Table   Basic Statistics of Datasets 

tics of these word vector pairs are summarized in Table  
For each word pairs  we generated   hashes using three
different schemes    Densi cation      Improved Densi cation    and the proposed densi cation     Algorithm   Using these hashes  we estimate the Jaccard similarity  Equation   We plot the mean square error  MSE 
with varying the number of hashes  Since the process is
randomized  we repeat the process   times  for every   
and report the average over independent runs  We report all
integer values of   in the interval    
It should be noted that since all three schemes have the
LSH Property  the bias is zero and hence the MSE is the
theoretical variance  To validate our variance formula  we
also compute and plot the theoretical value of the variance
 Equation   of the optimal scheme  Also  to understand
how all these fast methodologies compare with the accuracy of vanilla minwise hashing we also plot the theoretical
variance of minwise hashing which is     
 
From the results in Figure   we can conclude 
Conclusion   The proposed densi cation is signi cantly
more accurate  irrespective of the choice of sparsity and
similarity  than the existing densi cation schemes especially for large    Note the yaxis of plots is on log scale 
so the accuracy gains are drastic 
Conclusion   The gains with optimal densi cation is
more for sparse data 
Conclusion   The accuracy of optimal densi cation is
very close the accuracy of costly minwise hashing 
Conclusion   The theoretical variance of our proposal
overlaps with the empirical estimates  and it seems to go
to zero validating Theorem  
Conclusion   The variances  or the MSE  of existing densi cation seems to converge to constant and do not go to
zero con rming Theorem  

 

  Speed
To compute the runtime  we use three publicly available
text datasets    RCV    URL and   News  The di 

  Bins

  Bins

  Bins

RCV 
URL

News 

 
 
 

 
 
 

 
 
 

Table   Avg  Number of Empty Bins per vector  rounded  generated with One Permutation Hashing  Li et al    For sparse
datasets    signi cant   with   hashes  of the bins can be
empty and have no information  They cannot be used for indexing
and kernel learning  Fortunately  we can ef ciently densify them
with optimal densi cation  such that  all the generated   hashes
have variance similar to minwise hashing  The overall computational cost is signi cantly less compared to minwise hashing

mensionality and sparsity of these datasets are an excellent representative of the scale and the size frequently encountered in largescale data processing systems  such as
Google   SIBYL  Chandra et al    The statistics of
these datasets are summarized in Table  
We implemented three methodologies for computing
hashes    Densi cation Scheme      The Proposed   
 Algorithm   and   Vanilla Minwise Hashing  The methods were implemented in    Cheap hash function replaced costly permutations  Clever alternatives to avoid
mod operations were employed  These tricks ensured that
our implementations  are as ef cient as the possible  We
compute the wall clock time required to calculate  
  and   hashes of all the three datasets  The time
include the endto end hash computation of the complete
data  Data loading time is not included  The results are
presented in Table   All the experiments were done on
Intel     processor laptop with  GB RAM 
Also  to get an estimate of the importance of densi cation 
we also show the average number of empty bins generated
by only using one permutation hashing and report the numbers in Table   We can clearly see that the number of
empty bins is signi cantly larger and the hashes are unusable without densi cation  From Table   we conclude 
Conclusion   Optimal densi cation is as fast as traditional densi cation irrespective of   and the sparsity  However  optimal densi cation is signi cantly more accurate 
Conclusion   Both the densi cation scheme is signi 
cantly faster than minwise hashing  They are    faster
for computing   hashes on the selected datasets 

available

at http rush rice edu 

 Codes

are

fastestminwise html

Optimal Densi cation for Fast and Accurate Minwise Hashing

Acknowledgements
This work was supported by NSF IIS  Grant 

References
Bavarian  Mohammad  Ghazi  Badih  Haramaty  Elad 
Kamath  Pritish  Rivest  Ronald   
and Sudan 
Madhu  The optimality of correlated sampling  CoRR 
abs    URL http arxiv org 
abs 

Bayardo  Roberto    Ma  Yiming  and Srikant  Ramakrishnan  Scaling up all pairs similarity search  In WWW  pp 
   

Broder  Andrei    On the resemblance and containment
In the Compression and Complexity of

of documents 
Sequences  pp    Positano  Italy   

Broder  Andrei    Charikar  Moses  Frieze  Alan    and
Mitzenmacher  Michael  Minwise independent permutations  In STOC  pp    Dallas  TX   

Buehrer  Gregory and Chellapilla  Kumar    scalable pattern mining approach to web graph compression with
In WSDM  pp    Stanford  CA 
communities 
 

Carter     Lawrence and Wegman  Mark    Universal
classes of hash functions  In STOC  pp     

Chandra  Tushar  Ie  Eugene  Goldman  Kenneth  Llinares 
Tomas Lloret  McFadden  Jim  Pereira  Fernando  Redstone  Joshua  Shaked  Tal  and Singer  Yoram  Sibyl   
system for large scale machine learning  Technical report   

Charikar  Moses    Similarity estimation techniques from
rounding algorithms  In STOC  pp    Montreal 
Quebec  Canada   

Chien  Steve and Immorlica  Nicole  Semantic similarity
between search engine queries using temporal correlation  In WWW  pp     

Chierichetti  Flavio  Kumar  Ravi  Lattanzi  Silvio  Mitzenmacher  Michael  Panconesi  Alessandro  and Raghavan 
Prabhakar  On compressing social networks  In KDD 
pp    Paris  France   

Dahlgaard    ren  Knudsen  Mathias     Tejs  Rotenberg 
Eva  and Thorup  Mikkel  Hashing for statistics over kpartitions  In Foundations of Computer Science  FOCS 
  IEEE  th Annual Symposium on  pp   
IEEE   

Fetterly  Dennis  Manasse  Mark  Najork  Marc  and
Wiener  Janet      largescale study of the evolution of
web pages  In WWW  pp    Budapest  Hungary 
 

Haeupler  Bernhard  Manasse  Mark  and Talwar  Kunal 
Consistent weighted sampling made fast  small  and
easy  arXiv preprint arXiv   

Henzinger  Monika Rauch  Finding nearduplicate web
pages    largescale evaluation of algorithms  In SIGIR 
pp     

Indyk  Piotr and Motwani  Rajeev  Approximate nearest
neighbors  Towards removing the curse of dimensionality  In STOC  pp    Dallas  TX   

Li  Ping   bit consistent weighted sampling 

 

In KDD 

Li  Ping  Shrivastava  Anshumali  Moore  Joshua  and
  onig  Arnd Christian  Hashing algorithms for largescale learning  In NIPS  Granada  Spain   

Li  Ping  Owen  Art    and Zhang  CunHui  One permu 

tation hashing  In NIPS  Lake Tahoe  NV   

Luo  Chen and Shrivastava  Anshumali  Ssh  sketch  shingle    hash  for indexing massivescale time series  In
NIPS   Time Series Workshop  pp     

Najork  Marc  Gollapudi  Sreenivas  and Panigrahy  Rina 
Less is more  sampling the neighborhood graph makes
In WSDM  pp   
salsa better and faster 
Barcelona  Spain   

Ondov  Brian    Treangen  Todd    Melsted    all 
Mallonee  Adam    Bergman  Nicholas    Koren 
Sergey  and Phillippy  Adam    Mash  fast genome
and metagenome distance estimation using minhash 
Genome Biology     

Shrivastava  Anshumali  Simple and ef cient weighted
In Advances in Neural Information

minwise hashing 
Processing Systems  pp     

Shrivastava  Anshumali and Li  Ping  Densifying one
permutation hashing via rotation for fast near neighbor
search  In ICML  Beijing  China     

Shrivastava  Anshumali and Li  Ping  In defense of minIn Proceedings of the Seventeenth
hash over simhash 
International Conference on Arti cial Intelligence and
Statistics  pp       

Dahlgaard    ren  Knudsen  Mathias     Tejs  and Thorup  Mikkel  Fast similarity sketching  arXiv preprint
arXiv   

Shrivastava  Anshumali and Li  Ping  Improved densi cation of one permutation hashing  In UAI  Quebec  CA 
   

Optimal Densi cation for Fast and Accurate Minwise Hashing

Weinberger  Kilian  Dasgupta  Anirban  Langford  John 
Smola  Alex  and Attenberg  Josh  Feature hashing for
large scale multitask learning  In ICML  pp   
 

