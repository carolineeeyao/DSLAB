Differentially Private Submodular Maximization 

Data Summarization in Disguise

Marko Mitrovic   Mark Bun     Andreas Krause   Amin Karbasi  
Abstract

Many data summarization applications are captured by the general framework of submodular
maximization  As   consequence    wide range
of ef cient approximation algorithms have been
developed  However  when such applications involve sensitive data about individuals  their privacy concerns are not automatically addressed 
To remedy this problem  we propose   general and systematic study of differentially private
submodular maximization  We present privacypreserving algorithms for both monotone and
nonmonotone submodular maximization under
cardinality  matroid  and pextendible system
constraints  with guarantees that are competitive
with optimal solutions  Along the way  we analyze   new algorithm for nonmonotone submodular maximization under   cardinality constraint 
which is the  rst  even nonprivately  to achieve
  constant approximation ratio with   linear number of function evaluations  We additionally provide two concrete experiments to validate the ef 
 cacy of these algorithms 

  Introduction
  set function            is said to be submodular if
for all sets           and every element       we
have                                          That is 
the marginal contribution of any element   to the value of
the function       diminishes as the input set   increases 
The theory of submodular maximization uni es and generalizes diverse problems in combinatorial optimization  including the MaxCover  MaxCut  and Facility Location
problems  In turn  this theory has recently found numerous
applications to problems in machine learning  data science 
and arti cial intelligence    few such applications include
exemplarbased clustering  Krause   Gomes    feature selection for classi cation  Krause   Guestrin   
document and corpus summarization  Lin   Bilmes   

 Yale University  New Haven  CT  USA  Princeton University 
Princeton  NJ  USA  ETH Zurich  Zurich  Switzerland  Correspondence to  Marko Mitrovic  marko mitrovic yale edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Kirchhoff   Bilmes    Sipos et al    crowd teaching  Singla et al    and in uence maximization in social networks  Kempe et al   
Some of the most compelling use cases for these applications concern sensitive data about individuals  Mirzasoleiman et al        As   running example  let us
consider the speci   problem of determining which of  
collection of features       age  height  weight  etc  are
most relevant to   binary classi cation task       predicting whether an individual is likely to have diabetes 
In
this problem    sensitive training set takes the form    
 xi  yi  
   where each individual     data consists of features xi          xi   together with   class label yi  The goal
is to identify   small subset         of features which can
then be used to build   good classi er for    Many techniques exist for feature selection  including one based on
maximizing   submodular function which captures the mutual information between   subset of features and the class
label of interest  Krause   Guestrin    However  for
both legal       compliance with HIPAA regulations  and
ethical reasons  it is important that the selection of relevant
features does not compromise the privacy of any individual
who has contributed to the training data set  Unfortunately 
the theory of submodular maximization does not in itself
accommodate such privacy concerns 
To this end  we propose   systematic study of differentially
private submodular maximization to enable these applications based on submodular maximization  while provably
guaranteeing individuallevel privacy  The notion of differential privacy  Dwork et al    offers   strong protection of individuallevel privacy  Nevertheless  differential
privacy has been shown to permit useful data analysis and
machine learning tasks  In   nutshell  the de nition formalizes   guarantee that no individual   data should have too
signi cant an effect on the outcome of   computation  We
provide the formal de nition in Section   Such   privacy
guarantee is obtained through the introduction of random
noise  so private submodular maximization is conceptually
related to the problem of submodular maximization in the
presence of noise  Cheraghchi    Hassidim   Singer 
 
In this work  we study the following problem under various
conditions on the submodular objective function    monotone vs  nonmonotone  and various choices of the con 

Differentially Private Submodular Maximization

 cid  

straint    cardinality  matroid  or pextendible system 
Problem   Given   sensitive dataset   associated to
  submodular function fD           Find   subset
           that approximately maximizes fD    in  
manner that guarantees differential privacy with respect to
the input dataset   
An important special case of this problem was studied in prior work of Gupta et al    They considered the  combinatorial public projects  problem  Papadimitriou et al    where given   dataset    
            xn  the function fD takes the particular form
   fxi    for monotone submodular funcfD       
           and is to be maximized subject
 
tions fxi
to   cardinality constraint          We call functions of
this form decomposable  They presented   simple greedy
algorithm  which will be central to our work  together with
  tailored analysis which achieves strong accuracy guarantees in this special case 
However  there are many cases of Problem   which do not
fall into the combinatorial public projects framework  For
some problems  including feature selection via mutual information  the submodular function fD of interest depends
on the dataset   in ways much more complicated than averaging functions associated to each individual  The focus of
our work is on understanding Problem   in circumstances
which capture   broader class of useful applications and
constraints in machine learning  We summarize our speci   contributions in Section  
  The greedy paradigm
Even without concern for privacy  the problem of submodular maximization poses computational challenges  In particular  exact submodular maximization subject to   cardinality constraint is NPhard  One of the principal approaches to designing ef cient approximation algorithms
is to use   greedy strategy  Nemhauser et al    Consider the problem of maximizing   set function       subject to the cardinality constraint          In each of rounds
                 the basic greedy algorithm constructs Si
from Si  by adding the element vi        Si  which
maximizes the marginal gain    Si     vi       Si 
Nemhauser et al    famously showed that this algorithm yields      approximation to the optimal value
of       whenever   is   monotone submodular function 
In the combinatorial public projects setting  Gupta et al 
  showed how to make the greedy algorithm compatible with differential privacy by randomizing the procedure
for selecting each vi  This selection procedure is speci ed
by the differentially private exponential mechanism of McSherry   Talwar   which  probabilistically  guarantees that the vi selected in each round is almost as good as
the true marginal gain maximizer  Remarkably  Gupta et al 
  show that the cumulative privacy guarantee of the

resulting randomized greedy algorithm is not much worse
than that of   single run of the exponential mechanism 
This analysis is highly tailored to the structure of the combinatorial public projects problem  However  replacing this
tailored analysis with the more generic  advanced composition theorem  for differential privacy  Dwork et al 
  one still obtains useful results for the more general
class of  lowsensitivity  submodular functions 
  Our contributions
Table   summarizes the approximation guarantees we obtain for Problem   under increasingly more general classes
of submodular functions fD  read top to bottom  and increasingly more general types of constraints  read left to
right  In each entry  OPT denotes the value of the optimal
nonprivate solution  Below we draw attention to   few particular contributions  including some that are not expressed
in Table  
Nonmonotone objective functions  Submodular maximization for nonmonotone functions is signi cantly more
challenging than it is for monotone objectives  In particular  the basic greedy algorithm of Nemahauser et al  fails 
and cannot guarantee any constantfactor approximation 
Several works  Feldman et al    Mirzasoleiman et al 
    Buchbinder et al    Feldman et al    have
identi ed variations of the greedy algorithm that do yield
constantfactor approximations for nonmonotone objectives  However  it is not clear how to modify any of these
algorithms to accommodate differential privacy 
Our starting point is instead the  stochastic greedy  algorithm of Mirzasoleiman et al    which was originally designed to perform monotone submodular maximization in linear time  Drawing ideas from Buchbinder
et al    we give   new analysis of the stochastic
         
greedy algorithm to show that it also gives    
approximation for nonmonotone submodular functions 
To our knowledge  this is the  rst algorithm making exactly      function evaluations which achieves   constantfactor approximation for either monotone or nonmonotone
objectives  Moreover  it is immediately clear how to use
the exponential mechanism to make this algorithm differentially private 
This phenomenon is analogous to how stochastic variants
of gradient descent are more naturally suited to providing differential privacy than their deterministic counterparts  Song et al    Bassily et al    That is  our
results illustrate how techniques for making algorithms fast
are also helpful in making them privacypreserving 
General constraints  While   cardinality constraint is
perhaps the most natural to place on   submodular maximization problem  some machine learning problems      
personalized data summarization  Mirzasoleiman et al 
    require the use of more general types of con 

Differentially Private Submodular Maximization

 cid     

 

Comb  Pub  Proj 

Monotone

Nonmonotone

Cardinality

 cid  OPT  
 cid     
 cid     

 cid    log     
 cid  OPT  
 cid  OPT  

 

 

 cid 
 cid 
 cid     log     
 cid 
 cid     log     

 

 
 

 

 

 Gupta et al   

Matroid

 cid    log     
 cid 
 cid 
 cid     log     

 

 

 

  OPT  
  OPT  

 

 

pExtendible

 cid    log     
 cid 
 cid 
 cid     log     

 

 

 

   OPT  
   OPT  
 

 

Pr                  Pr      cid           

functions fxi            we say fD is  decomposable 
The problem of maximizing   decomposable submodular function was considered as the  combinatorial public
projects problem  by Papadimitriou et al   
We are interested in the problem of approximately maximizing   submodular function subject to differential privacy  The de nition of differential privacy relies on the
notion of neighboring datasets  which are simply tuples
     cid        that differ in at most one entry  If      cid  are
neighboring  we write       cid 
De nition     randomized algorithm             satis es    differential privacy if for all measurable sets
      and all neighboring datasets       cid 

Table   Guarantees of expected solution quality for privately maximizing   sensitivity    submodular function fD  The parameter  
represents either   cardinality constraint  or the size of the set returned  for matroid or pextendible system constraints  Full expressions
with explicit dependencies on differential privacy parameters     appear in the body of the paper 
straints  For instance  one may wish to maximize   submodular function       subject to       for an arbitrary
matroid    or subject to   being contained in an intersection of   matroids  more generally    pextendible system  For these types of constraints  the greedy algorithm
still yields   constant factor approximation for monotone
objective functions  Fisher et al    Jenkyns   
  alinescu et al    We show in this work that the analysis provided by   alinescu et al    for matroids and
pextendible families can be adapted to handle additional
error introduced for differential privacy 
General selection procedures  For worstcase datasets 
the exponential mechanism is optimal within each round
of private maximization  However  it may be suboptimal
for datasets enjoying additional structural properties  Fortunately  the greedy framework we use is  exible with regard to the choice of the selection procedure  For instance 
one can replace the exponential mechanism in   blackbox
manner with the  large margin mechanism  of Chaudhuri
et al    to obtain error bounds that replace the explicit dependence on log      in Table   with   term that
may be signi cantly smaller for real datasets  We give  
slightly simpli ed analysis of the large margin mechanism 
and present it in   manner suitable for greedy algorithms
which access the same data set multiple times   These guarantees are more complicated  but spelled out in Section  
For submodular functions exhibiting additional structure 
one may also be able to perform each maximization step
with the  choosing mechanism  of Beimel et al   
and Bun et al   
  Preliminaries
Let   be  nite set which we will refer to as the ground set
and let   be    nite set which we will refer to as the data
universe    dataset is an ntuple                 xn        
Suppose each dataset   is associated to   set function fD  
        The manner in which fD depends on   will be
applicationspeci    but it is assumed that the association
between   and fD is public information 
De nition     set function fD          is submodular
if for all sets           and every element         we
have fD           fD      fD           fD    
Moreover  If fD      fD     whenever         we say
fD is monotone  If for every dataset                 xn 
the function fD    
   fxi for monotone submodular
 

Composition of Differential Privacy  The analyses of
our algorithms rely crucially on composition theorems for
differential privacy  For   sequence of privacy parameters         
   we informally refer to the kfold adaptive composition of        differentially private algorithms
as the output of   mechanism    that behaves as follows on an input   
In each of rounds                 
the algorithm    selects an        differentially private
algorithm Mi possibly depending on the previous outcomes              Mi     but not directly on the sensitive dataset   itself  and releases Mi    For   formal
treatment of adaptive composition  see  Dwork et al   
Dwork   Roth   
Theorem    Dwork   Lei    Dwork et al   
Bun   Steinke    The kfold adaptive composition
of    differentially private algorithms satis es    
differential privacy where
         and       

Differentially private algorithms must be calibrated to the
sensitivity of the function of interest with respect to small
changes in the input dataset  de ned formally as follows 
De nition   The sensitivity of   set function fD       
   depending on   dataset    with respect to   constraint
       is de ned as
max
    cid  max

     fD      fD cid   

   cid  log cid  and      cid       for any

 Basic Composition 

       
 cid     

    

 Advanced Composition 

 cid  

Differentially Private Submodular Maximization

Exponential Mechanism  The
exponential mechanism  McSherry   Talwar    is   general primitive for solving discrete optimization problems 
Let
                be    quality  function measuring
how good   solution       is with respect to   dataset
         We say   quality function   has sensitivity   if
for all       and all neighboring datasets       cid  we
have                   cid     
Proposition   Let       and let             be   quality
function with sensitivity   De ne the exponential mechanism as the algorithm which selects every       with
probability proportional to exp        

exponential mechanism provides

   

  The
differential privacy 
  For every         

            max
   

              ln    

 

 

where    is the output of the exponential mechanism on
dataset   

The privacy guarantee and    with high probability  utility
guarantee of the exponential mechanism are due to McSherry   Talwar     simple proof of the utility guarantee in expectation appears in  Bassily et al   
  Monotone Submodular Maximization
In this section  we present   variant of the basic greedy algorithm which will enable maximization of monotone submodular functions  This algorithm simply replaces each
greedy selection step with   privacypreserving selection
algorithm denoted    The selection function   takes as
input   quality function                 and   dataset
   as well as privacy parameters     and outputs an element        We begin in the simplest case of monotone
submodular maximization with   cardinality constraint  Algorithm   The algorithm for more general constraints appears in Section  
Algorithm   was already studied by Gupta et al    in
the special case where fD is decomposable  and   is the
exponential mechanism  We generalize their result to the
much broader class of lowsensitivity monotone submodular functions 
Algorithm   Diff  Private Greedy  Cardinality  GO
Input  Submodular function fD           dataset   
cardinality constraint    privacy parameters    
Output  Size   subset of  
  Initialize       
  For                 

  De ne qi       Si         via qi         
     Si               Si 
  Compute vi      qi        
  Update Si    Si     vi 

  Return Sk

 

Theorem    Gupta et al    Suppose fD         
is  decomposable  cf  De nition   Let       and let
      be such that                    ln        Then
instantiating Algorithm   with     EM and parameter
      provides    differential privacy 
Moreover  for every         
     
 

OPT     ln    

   fD Sk   

 cid 

 cid 

 cid 

 cid 

where Sk    GEM   
Unfortunately  the privacy analysis of Theorem   makes essential use of the decomposability of fD  and does not directly generalize to arbitrary submodular functions of lowsensitivity  Replacing the privacy analysis of  Gupta et al 
  with the Composition Theorem   instead gives
Theorem   Suppose fD          is monotone and has
sensitivity   Then instantiating Algorithm   with     EM
and parameter       provides             
differential privacy  It also provides    differential privacy for every       with       
Moreover  for every         
     
 

       cid   ln 
OPT     ln    

   fD Sk   

where Sk    GEM   
  Matroid and pExtendible System Constraints
We now show how to extend Algorithm   to privately maximize monotone submodular functions subject to more general constraints  To start  we review the de nition of   pextendible system  Consider   ground set   and   nonempty downwardclosed family of subsets              if
       then       for every         Such an   is called  
family of independent sets  The pair       is said to be   pextendible system  Mestre    if for all            and
      such that           there exists   set           
such that         and                    Let      denote
the size of the largest independent set in   
The de nition of   matroid coincides with that of    
extendible system  with rank      For       the notion of   pextendible system strictly generalizes that of an
intersection of   matroids    slight modi cation of Algorithm   gives   uni ed algorithm for privately maximizing
  monotone submodular function subject to matroid and pextendible system constraints  presented as Algorithm  
Theorem   Suppose fD          is  decomposable
 cf  De nition   Let       and let       be such that
                   ln        Then instantiating Algorithm   with     EM and parameter       provides
   differential privacy  Moreover  for every         

 

 cid        ln    

 cid 

   fD       
     
where      GEM   

  OPT   
     

 

Differentially Private Submodular Maximization

Algorithm   Differentially Private Greedy  psystem  GO
Input  Submodular function fD           dataset    pextendible family       privacy parameters    
Output  Maximal independent subset of  
  Initialize      
  While       is not maximal 
  De ne                       via           
                      
  Compute vi               
  Update           vi 

  Return  
Theorem   Suppose fD          has sensitivity  
Then instantiating Algorithm   with     EM and parameter       provides               differential privacy  It also provides    differential privacy for every

      with               cid      ln 

Moreover  for every         

   fD       
     
where      GEM   

  OPT   
     

 cid        ln    

 cid 

 

  NonMonotone Submodular Maximization
We now consider the problem of privately maximizing an
arbitrary  possibly nonmonotone  submodular function under   cardinality constraint  In general  the greedy algorithm presented in Section   fails to give any constantfactor approximation  Instead  our algorithm in this section will be based on the  stochastic greedy  algorithm  rst
studied by Mirzasoleiman et al   
In each round 
the stochastic greedy algorithm  rst subsamples   random
    ln  fraction of the ground set for some      
 
and then greedily selects the item from this subsample that
maximizes marginal gain  Mirzasoleiman et al   
showed that for   monotone objective function    this algorithm provides             approximation to the optimal solution  Their original motivation was to improve the
running time of the greedy algorithm  from         evaluations of the objective function to linear       ln 
Unfortunately  the stochastic greedy algorithm does not
provide any approximation guarantee for nonmonotone
submodular functions  Buchbinder et al    instead
proposed    random greedy  algorithm that  in each iteration  randomly selects one of the   elements with the highest marginal gain  Buchbinder et al    showed that the
random greedy algorithm achieves      approximation to
the optimal solution  in expectation  using       function
evaluations  However  it is not clear how to adapt this algorithm to accommodate differential privacy  since its analysis has   brittle dependence on the sampling procedure 
We make two main contributions to the analysis of the

stochastic greedy and random greedy algorithms  First 
we show that running the stochastic greedy algorithm on
an exact  
  fraction of the ground set per iteration still
gives    approximation for monotone objectives 
         approximation even for
and moreover  gives    
nonmonotone objectives  Note that this algorithm evaluates the objective function on only      elements  and still
provides   constant factor approximation guarantee  This
makes our  subsamplegreedy  algorithm the fastest algorithm for maximizing   general submodular function subject to   cardinality constraint  albeit with slightly worse
approximation guarantees  Second  we show that the guarantees of this algorithm are robust to using   randomized
greedy selection procedure      
the exponential or large
margin mechanism  and hence it can be adapted to ensure
differential privacy 
We present the subsamplegreedy algorithm as Algorithm  
below  Assume that   is augmented by enough  dummy
elements  to ensure that       is an integer  each dummy
element   is de ned so that fD           fD    for
every set    We also explicitly account for an additional
set   of   dummy elements  and ensure that at least one
appears in every subsample 
Algorithm   Diff  Private  SubsampleGreedy  SGO
Input  Submodular function fD           dataset   
cardinality constraint    privacy parameters    
Output  Size   subset of  
  Initialize        dummy elements    

            uk 

  For                 

  Sample Vi       uniformly random subset of
size       and ui   random dummy element
  De ne qi    Vi ui         via qi         
     Si               Si 
  Compute vi      qi        
  Update Si    Si     vi 

  Return Sk with all dummy elements removed
Theorem   Suppose fD          has sensitivity  
Then instantiating Algorithm   with     EM provides
   differential privacy  and for every         
OPT     ln    

   fD       
 

   fD     cid 

     
 

 cid 
 cid 
        cid 
    OPT     ln    

where      SGEM    Moreover  if fD is monotone  then

 

OPT     ln    

 

 

 

The guarantees of Theorem   are of interest even without privacy  Letting MAX denote the selection procedure
which simply outputs the true maximizer  equivalently 

Differentially Private Submodular Maximization

   cid   ln cid 

which runs the exponential mechanism with      
we obtain the following nonprivate algorithm for maximizing   submodular function fD 
Corollary   Let fD          be any submodular function  Instantiating Algorithm   with     MAX gives

 cid 

     
 

OPT

   fD       
 

 cid 
        cid 

where      SGMAX    Moreover  if fD is monotone 
then

   fD     cid 

OPT     OPT  

  The Large Margin Mechanism
The accuracy guarantee of the exponential mechanism can
be pessimistic on datasets where       exhibits additional structure  For example  suppose that when the elements of   are sorted so that                    
               there exists an  cid  such that          cid 
    cid     Then only the top  cid  ground set items are relevant to the optimization problem  so running the exponential mechanism on these should maintain differential
privacy  but with error proportional to ln  cid  rather than to
ln     The large margin mechanism of Chaudhuri et al 
  like the exponential mechanism  generically solves
discrete optimization problems  However  it automatically
leverages this additional margin structure whenever it exists  Asymptotically  the error guarantee of the large margin
mechanism is always at most that of the exponential mechanism  but can be much smaller when the data exhibits  
margin for small  cid 
Formally  given   quality function                 and
parameters  cid               dataset   satis es the  cid   
margin condition if     cid                  
For each  cid                 de ne

 cid 

 cid 

  cid       

   

  ln cid 

  cid   

  ln 

 

 

 
  ln cid 

 

    cid 

   exp     

Recall that the Laplace distribution Lap    is speci ed by
the density function  
Replacing the exponential mechanism with the large margin mechanism gives analogues of our results for monotone
submodular maximization with   cardinality constraint 
monotone submodular maximization over   pextendible
system  and nonmonotone submodular maximization with
  cardinality constraint 
Theorem   Suppose fD          is monotone
and has sensitivity   Then instantiating Algorithm  
with     LMM and parameters         provides
It also provides    cid   
      differential privacy 

  differential privacy for every  cid      with       

  ln  cid  

 

Moreover  for every          there exists an event   with
Pr            such that

 cid 

 cid 

OPT    cid 

   fD Sk     

     
 

  

where Sk    GLMM    and   satis es the  cid       
margin condition with respect to every function of the form
qi         fD   Si          fD   Si  with     
  ln        cid   
Theorem   Instantiating Algorithm   with     LMM
under all of the conditions of Theorem   gives the same
privacy guarantee  replacing   with      and gives
  ln  cid  

    cid 

   fD         
     

  OPT 

 

 

 cid 

Theorem   Instantiating Algorithm   with     LMM
under all of the conditions of Theorem   gives the same
privacy guarantee and gives

   fD Sk       
 

OPT    cid 
   fD Sk        OPT    cid 

     
 

Moreover  if fD is monotone  then

  ln  cid  

 

 

 cid 

  

  

  ln  cid  

 

 

  

  Experimental Results
In this section we describe two concrete applications of our
mechanisms 
  Location Privacy
We analyze   dataset of   Uber pickups in Manhattan
in April    UberDataset  Each individual entry in the
dataset consists of the longitude and latitude coordinates of
the pickup location  We want to use this dataset to select
  public locations as waiting spots for idle Uber drivers 
while also guaranteeing differential privacy for the passengers whose locations appear in this dataset  We consider
two different public sets of locations   

  LP opular is   set of   popular locations in Manhattan 
  LGrid is   set of   locations spread evenly across

Manhattan in   gridlike manner 

We de ne   utility function          to be the normalized
Manhattan distance between   pickup location   and the
waiting location    That is  if pickup location   is located at
coordinates        and the waiting location   is located at
                   
 
coordinates        then           
where       is simply the Manhattan distance between the two furthest spread apart points in Manhattan 

 

 Under the assumption that each pickup corresponds to  

unique individual 

Differentially Private Submodular Maximization

    LP opular

    LGrid

    LP opular

    LGrid

    LP opular 
greedy

Nonprivate

    LP opular  EMbased greedy

    LGrid  Nonprivate greedy

    LGrid  EMbased greedy

Figure       and     show utility for various cardinalities         and              and show utility for various privacy parameters  
Utility is normalized to be between   and             shows   representative top   set under various settings 
This normalization ensures that                  for all
      In order to make sure we have   maximization problem  we de ne the following objective function  fD     

   cid 

   

         where            

min
   

Observation   The function fD is  decomposable for
       and hence has sensitivity  
This form of objective function is known to be monotone
submodular and so we can use the greedy algorithms studied in this paper  We use       and       For our settings of parameters   basic composition  outperforms  advanced composition  so the privacy budget of       is
split equally across the   iterations  meaning the mechanism at each iteration uses      
    Our  gures plot the
average utility across   simulations 
From Figures     and     we see that the results for both
LP opular and LGrid are relatively similar and unsurprising  The nonprivate greedy algorithm achieves the highest utility  but both the exponential mechanism  EM based
greedy and large margin mechanism  LMM based greedy
algorithms exhibit comparable utility while preserving  
high level of privacy 
Interestingly  we also see that the
utilities of the EMbased and LMMbased algorithms are
almost identical for both LP opular and LGrid  This indicates that our mechanisms are actually selecting good locations  rather than just getting lucky because there are   lot
of good locations to choose from 
Figures     and     show how the utility of the EMbased

and LMMbased algorithms vary with the privacy parameter   We can also think of this as varying the dataset size
for    xed   We          and take the average of  
simulations for each value of   We see that even for very
small   our algorithms outperform fully random selection 
As   increases  so does the utility  It is not shown in this
 gure  but varying   has very little effect 
From Figures           we see that the both the nonprivate
and private algorithms select public locations that are relatively close to each other  For example  for the LP opular
set of locations  the Empire State Building is close to the
New York Public Library  the Soho Grand Hotel is close to
NYU  and the Grand Army Plaza is close to the UN Headquarters  As   result  the private mechanisms manage to
achieve comparable utility  while also masking the users 
exact locations 
The theory described in Section   suggests that  at least
asymptotically  the large margin mechanismbased algorithm should outperform the exponential mechanismbased
algorithm  However  in our experiments  we  nd that the
large margin mechanism is generally only able to  nd  
margin in the  rst iteration of the greedy algorithm  This is
because the threshold for  nding   margin depends only on
    and   and thus it stays the same across all   iterations 
On the other hand  the marginal gain at each iteration drops
very quickly  so the mechanism fails to  nd   margin and
thus samples from all remaining locations  However  since
the large margin mechanism spends half of its privacy bud 

Differentially Private Submodular Maximization

    Graphical model of Naive Bayes

    Expected mutual information

    Representative top   features

Figure   Privately selecting health features  from national health examination surveys  that correlate most with diabetes 

get to try to  nd   margin  the sampling step gives slightly
worse guarantees than does the plain exponential mechanism  thus giving us the slightly weaker results we see in
the  gures 
  Feature Selection Privacy
We analyze   dataset created from   combination of National Health Examination Surveys ranging from   to
   NHANESDataset  There are         individuals in the dataset with information on whether or not they
have diabetes  along with       other potentially related
binary health features  Our goal is to privately select   of
these features that provide as much information about the
diabetes class variable as possible 
More speci cally  our goal is to maximize the mutual information between   and XS  where   is   binary random
variable indicating whether or not an individual has diabetes and XS is   random variable that represents   set  
of   binary health features  Mutual information takes the
form 

 cid         

 cid 

        

 

 cid 

 cid 

   

   

          

        log 

  

  cid 

                xk        

Under the Naive Bayes assumption  we suppose the
joint distribution on                Xk  takes the form
  xi      Therefore  we can
easily specify the entire probability distribution by  nding
each   xi      We estimate each   xi      by counting
frequencies in the dataset 
Our goal is to choose   size   subset   of the features in
order to maximize fD            XS  Mutual information  under the Naive Bayes assumption  for feature selection is known to be monotone submodular in    Krause  
Guestrin    and thus we can apply the greedy algorithms described in this paper 
Claim   In iteration   of the greedy algorithm  the sensitivity of fD    is     log   
We run   simulations with       and      
As we can see from Figure     our private mechanisms

 

 

maintain   comparable utility relative to the nonprivate
algorithm  We also observe an interesting phenomenon
where the expected utility obtained by our mechanism is
not necessarily monotonically increasing with the number
of features selected  This is an artifact of the fact that if
we are selecting   features  then composition requires us to
    This
divide   so that each iteration uses privacy budget  
is problematic for this particular application because there
happens to be one feature  insulin administration  that has
much higher value than the rest  Therefore  the reduced
probability of picking this single best feature  as   result of
the lower privacy parameter  
    is not compensated for by
selecting more features 
From Figure     we see that both the private and nonprivate mechanisms generally select insulin administration
as the top feature  However  while all three of the top features selected by the nonprivate algorithm are clearly related to diabetes  the nonprivate mechanisms tend to select
one feature  in our case  gender or having received   blood
transfusion  that may not be quite as relevant 
  Conclusion
We have presented   general framework for maximizing
submodular functions while guaranteeing differential privacy  Our results demonstrate that simple and  exible
greedy algorithms can preserve privacy while achieving
competitive guarantees for   variety of submodular maximization problems  for all functions under cardinality constraints  as well as for monotone functions under matroid
and pextendible system constraints  Via our motivation to
identify algorithms that could be made differentially private  we discovered   nonmonotone submodular maximization algorithm that achieves guarantees that are novel
even without concern for privacy  Finally  our experiments
show that our algorithms are indeed competitive with their
nonprivate counterparts 
Acknowledgments  This work was supported by DARPA
Young Faculty Award    AP  SimonsBerkeley
fellowship  and ERC StG   This work was done in
part while Amin Karbasi and Andreas Krause were visiting
the Simons Institute for the Theory of Computing 

DiabetesAsthmaVigorous ExerciseTaking InsulinHigh Blood PressureInsulinInsulinInsulinOverweightOverweightGenderHigh Blood PressureBlood TransfusionVigorous ExerciseNonPrivate GreedyExponential MechanismLarge Margin MechanismDifferentially Private Submodular Maximization

References
Bassily  Raef  Smith  Adam    and Thakurta  Abhradeep 
Private empirical risk minimization  Ef cient algorithms
and tight error bounds  In FOCS  pp     

Bassily  Raef  Nissim  Kobbi  Smith  Adam    Steinke 
Thomas  Stemmer  Uri  and Ullman  Jonathan  Algorithmic stability for adaptive data analysis  In STOC  pp 
   

Beimel  Amos  Nissim  Kobbi  and Stemmer  Uri  Private
learning and sanitization  Pure vs  approximate differential privacy  Theory of Computing     

Buchbinder  Niv  Feldman  Moran  Naor  Joseph  and
Schwartz  Roy  Submodular maximization with cardinality constraints  In SODA  pp     

Bun  Mark and Steinke  Thomas  Concentrated differential
privacy  Simpli cations  extensions  and lower bounds 
In TCC  pp     

Bun  Mark  Nissim  Kobbi  Stemmer  Uri  and Vadhan 
Salil    Differentially private release and learning of
threshold functions  In FOCS  pp     

  alinescu  Gruia  Chekuri  Chandra    al  Martin  and
Vondr ak  Jan  Maximizing   monotone submodular
function subject to   matroid constraint  SIAM Journal
on Computing   

Chaudhuri  Kamalika  Hsu  Daniel    and Song  Shuang 
The large margin mechanism for differentially private
maximization  In NIPS  pp     

Cheraghchi  Mahdi  et al  Submodular functions are noise

stable  In SODA   

Dwork  Cynthia and Lei  Jing  Differential privacy and ro 

bust statistics  In STOC  pp     

Dwork  Cynthia and Roth  Aaron  The algorithmic foundations of differential privacy  Foundations and Trends in
Theoretical Computer Science     

Dwork  Cynthia  McSherry  Frank  Nissim  Kobbi  and
Smith  Adam    Calibrating noise to sensitivity in private data analysis  In TCC  pp     

Dwork  Cynthia  Rothblum  Guy    and Vadhan  Salil   
Boosting and differential privacy  In FOCS  pp   
 

Feldman  Moran  Naor  Joseph  and Schwartz  Roy    uni 
 ed continuous greedy algorithm for submodular maximization  In FOCS   

Feldman  Moran  Harshaw  Christopher  and Karbasi 
Amin  Greed is good  Nearoptimal submodular maximization via greedy optimization  In COLT   

Fisher  Marshall    Nemhauser  George    and Wolsey 
Laurence    An analysis of approximations for maximizing submodular set functions   II  Mathematical Programming Study     

Gupta  Anupam  Ligett  Katrina  McSherry  Frank  Roth 
Aaron  and Talwar  Kunal  Differentially private combinatorial optimization  In SODA  pp     

Hassidim  Avinatan and Singer  Yaron  Submodular optimization under noise  CoRR  abs   
URL http arxiv org abs 

Jenkyns        The ef cacy of the  greedy  algorithm 
In South Eastern Conference on Combinatorics  Graph
Theory and Computing   

Kempe  David  Kleinberg  Jon  and Tardos   Eva  Maximizing the spread of in uence through   social network  In
KDD   

Kirchhoff  Katrin and Bilmes  Jeff  Submodularity for data
selection in statistical machine translation  In EMNLP 
 

Krause     and Guestrin     Nearoptimal nonmyopic value

of information in graphical models  In UAI   

Krause  Andreas and Gomes  Ryan    Budgeted nonpara 

metric learning from data streams  In ICML   

Lin  Hui and Bilmes  Jeff    class of submodular functions

for document summarization  In ACL   

McSherry  Frank and Talwar  Kunal  Mechanism design

via differential privacy  In FOCS  pp     

Mestre  Juli an  Greedy in approximation algorithms 

ESA  pp     

In

Mirzasoleiman  Baharan  Badanidiyuru  Ashwinkumar 
Karbasi  Amin  Vondrak  Jan  and Krause  Andreas 
Lazier than lazy greedy  In AAAI   

Mirzasoleiman  Baharan  Badanidiyuru  Ashwinkumar 
and Karbasi  Amin  Fast constrained submodular maximization  Personalized data summarization  In ICML 
   

Mirzasoleiman  Baharan  Zadimoghaddam  Morteza  and
Fast distributed submodular cover 

Karbasi  Amin 
Publicprivate data summarization  In NIPS     

Nemhauser  George    Wolsey  Laurence    and Fisher 
Marshall    An analysis of approximations for maximizing submodular set functions      Mathematical Programming   

Differentially Private Submodular Maximization

NHANESDataset  National health and nutrition examination survey       URL https wwwn 
cdc gov nchs nhanes default aspx 

Papadimitriou  Christos    Schapira  Michael  and Singer 
Yaron  On the hardness of being truthful  In FOCS  pp 
   

Singla  Adish  Bogunovic  Ilija  Bart ok    abor  Karbasi 
Amin  and Krause  Andreas  Nearoptimally teaching
the crowd to classify  In ICML   

Sipos  Ruben  Swaminathan  Adith  Shivaswamy  Pannaga 
and Joachims  Thorsten  Temporal corpus summarization using submodular word coverage  In CIKM   

Song  Shuang  Chaudhuri  Kamalika 

and Sarwate 
Anand    Stochastic gradient descent with differentially
private updates  In GlobalSIP  pp     

UberDataset 

Uber

city 

york
kaggle com fivethirtyeight 
uberpickups innew yorkcity 

URL

pickups

in

new
https www 

