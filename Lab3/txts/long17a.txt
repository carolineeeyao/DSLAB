Deep Transfer Learning with Joint Adaptation Networks

Mingsheng Long   Han Zhu   Jianmin Wang   Michael    Jordan  

Abstract

Deep networks have been successfully applied to
learn transferable features for adapting models
from   source domain to   different target domain 
In this paper  we present joint adaptation networks
 JAN  which learn   transfer network by aligning
the joint distributions of multiple domainspeci  
layers across domains based on   joint maximum
mean discrepancy  JMMD  criterion  Adversarial
training strategy is adopted to maximize JMMD
such that the distributions of the source and target
domains are made more distinguishable  Learning
can be performed by stochastic gradient descent
with the gradients computed by backpropagation
in lineartime  Experiments testify that our model
yields state of the art results on standard datasets 

  Introduction
Deep networks have signi cantly improved the state of the
arts for diverse machine learning problems and applications 
Unfortunately  the impressive performance gains come only
when massive amounts of labeled data are available for
supervised learning  Since manual labeling of suf cient
training data for diverse application domains onthe   is
often prohibitive  for   target task short of labeled data 
there is strong motivation to build effective learners that can
leverage rich labeled data from   different source domain 
However  this learning paradigm suffers from the shift in
data distributions across different domains  which poses  
major obstacle in adapting predictive models for the target
task  QuioneroCandela et al    Pan   Yang   
Learning   discriminative model in the presence of the shift
between training and test distributions is known as transfer
learning or domain adaptation  Pan   Yang    Previous

 Key Lab for Information System Security  MOE  Tsinghua National Lab for Information Science and Technology  TNList  NELBDS  School of Software  Tsinghua University  Beijing  
China  University of California  Berkeley  Berkeley   Correspondence to  Mingsheng Long  mingsheng tsinghua edu cn 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or
estimating instance importance without using target labels
 Huang et al    Pan et al    Gong et al    Recent deep transfer learning methods leverage deep networks
to learn more transferable representations by embedding
domain adaptation in the pipeline of deep learning  which
can simultaneously disentangle the explanatory factors of
variations behind data and match the marginal distributions
across domains  Tzeng et al      Long et al   
  Ganin   Lempitsky    Bousmalis et al   
Transfer learning becomes more challenging when domains
may change by the joint distributions of input features and
output labels  which is   common scenario in practical applications  First  deep networks generally learn the complex
function from input features to output labels via multilayer
feature transformation and abstraction  Second  deep features in standard CNNs eventually transition from general to
speci   along the network  and the transferability of features
and classi ers decreases when the crossdomain discrepancy
increases  Yosinski et al    Consequently  after feedforwarding the source and target domain data through deep
networks for multilayer feature abstraction  the shifts in the
joint distributions of input features and output labels still
linger in the network activations of multiple domainspeci  
higher layers  Thus we can use the joint distributions of the
activations in these domainspeci   layers to approximately
reason about the original joint distributions  which should
be matched across domains to enable domain adaptation  To
date  this problem has not been addressed in deep networks 
In this paper  we present Joint Adaptation Networks  JAN 
to align the joint distributions of multiple domainspeci  
layers across domains for unsupervised domain adaptation 
JAN largely extends the ability of deep adaptation networks
 Long et al    to reason about the joint distributions
as mentioned above  while keeping the training procedure
even simpler  Speci cally  JAN admits   simple transfer
pipeline  which processes the source and target domain data
by convolutional neural networks  CNN  and then aligns
the joint distributions of activations in multiple taskspeci  
layers  To learn parameters and enable alignment  we derive
joint maximum mean discrepancy  JMMD  which measures
the HilbertSchmidt norm between kernel mean embedding
of empirical joint distributions of source and target data 

Deep Transfer Learning with Joint Adaptation Networks

Thanks to   lineartime unbiased estimate of JMMD  we can
easily draw   minibatch of samples to estimate the JMMD
criterion  and implement it ef ciently via backpropagation 
We further maximize JMMD using adversarial training strategy such that the distributions of source and target domains
are made more distinguishable  Empirical study shows that
our models yield state of the art results on standard datasets 

the problem tractable  Zhang et al    As it is not easy
to justify which components of the joint distribution are
changing in practice  our work is transparent to diverse scenarios by directly manipulating the joint distribution without
assumptions on the marginal and conditional distributions 
Furthermore  it remains unclear how to account for the shift
in joint distributions within the regime of deep architectures 

  Related Work
Transfer learning  Pan   Yang    aims to build learning
machines that generalize across different domains following
different probability distributions  Sugiyama et al   
Pan et al    Duan et al    Gong et al    Zhang
et al    Transfer learning  nds wide applications in
computer vision  Saenko et al    Gopalan et al   
Gong et al    Hoffman et al    and natural language processing  Collobert et al    Glorot et al   
The main technical problem of transfer learning is how
to reduce the shifts in data distributions across domains 
Most existing methods learn   shallow representation model
by which domain discrepancy is minimized  which cannot
suppress domainspeci   exploratory factors of variations 
Deep networks learn abstract representations that disentangle the explanatory factors of variations behind data  Bengio
et al    and extract transferable factors underlying different populations  Glorot et al    Oquab et al   
which can only reduce  but not remove  the crossdomain
discrepancy  Yosinski et al    Recent work on deep
domain adaptation embeds domainadaptation modules into
deep networks to boost transfer performance  Tzeng et al 
      Ganin   Lempitsky    Long et al 
    These methods mainly correct the shifts in
marginal distributions  assuming conditional distributions
remain unchanged after the marginal distribution adaptation 
Transfer learning will become more challenging as domains
may change by the joint distributions          of input features   and output labels    The distribution shifts may
stem from the marginal distributions               covariate shift  Huang et al    Sugiyama et al    the
conditional distributions                 conditional shift
 Zhang et al    or both         dataset shift  QuioneroCandela et al    Another line of work  Zhang et al 
  Wang   Schneider    correct both target and conditional shifts based on the theory of kernel embedding of
conditional distributions  Song et al      Sriperumbudur et al    Since the target labels are unavailable 
adaptation is performed by minimizing the discrepancy between marginal distributions instead of conditional distributions  In general  the presence of conditional shift leads
to an illposed problem  and an additional assumption that
the conditional distribution may only change under locationscale transformations on   is commonly imposed to make

  Preliminary
  Hilbert Space Embedding

We begin by providing an overview of Hilbert space embeddings of distributions  where each distribution is represented
by an element in   reproducing kernel Hilbert space  RKHS 
Denote by     random variable with domain   and distribution       and by   the instantiations of      reproducing
kernel Hilbert space  RKHS    on   endowed by   kernel
        cid  is   Hilbert space of functions        cid    with
inner product  cid cid    Its element       satis es the reproducing property   cid            cid           Alternatively 
      can be viewed as an  in nitedimensional  implicit
feature map       where         cid     cid             cid cid    Kernel functions can be de ned on vector space  graphs  time
series and structured objects to handle diverse applications 
The kernel embedding represents   probability distribution
  by an element in RKHS endowed by   kernel    Smola
et al    Sriperumbudur et al    Gretton et al   

         cid  EX        

      dP    

 

 cid 

 

where the distribution is mapped to the expected feature map 
     to   point in the RKHS  given that EX          cid   cid   
The mean embedding    has the property that the expectation of any RKHS function   can be evaluated as an inner
product in     cid      cid    cid  EX                This kind
of kernel mean embedding provides us   nonparametric perspective on manipulating distributions by drawing samples
from them  We will require   characteristic kernel   such
that the kernel embedding         is injective  and that the
embedding of distributions into in nitedimensional feature
spaces can preserve all of the statistical features of arbitrary
distributions  which removes the necessity of density estimation of     This technique has been widely applied in many
tasks  including feature extraction  density estimation and
twosample test  Smola et al    Gretton et al   
While the true distribution       is rarely accessible  we
can estimate its embedding using    nite sample  Gretton
et al    Given   sample DX               xn  of size  
drawn        from       the empirical kernel embedding is

 cid    

 
 

  cid 

  

   xi 

 

Deep Transfer Learning with Joint Adaptation Networks

  

 cid cid 

part in RKHS norm  cid    cid   cid   with   rate of       

This empirical estimate converges to its population counter 
   
Kernel embeddings can be readily generalized to joint distributions of two or more variables using tensor product feature
spaces  Song et al      Song   Dai      joint
distribution   of variables            Xm can be embedded
into an mth order tensor product feature space   
 cid   cid  by
CX        cid  EX  

 cid  
 cid cid cid   cid cid cid 
 cid  
 cid cid cid   cid cid cid  dP cid            xm cid 

 cid 

 

 cid cid cid   cid cid     cid   cid        xm  is the feature map

 cid cid   cid cid cid     cid  
 cid cid   cid  
 cid cid  
 cid     cid   cid cid   cid  

 
where     denotes the set of   variables             Xm 
 cid cid                     cid  is the feature
on domain   
map endowed with kernel   cid  in RKHS   cid  for variable   cid 
  
in the tensor product Hilbert space  where the inner product
satis es  cid  
 cid    cid   cid    cid cid 
The joint embeddings can be viewed as an uncentered crosscovariance operator CX   by the standard equivalence between tensor and linear map  Song et al    That is 
given   set of functions                  their covariance can be
computed by EX  
When the true distribution               Xm  is unknown  we
can estimate its embedding using    nite sample  Song et al 
    of size
  Given   sample DX         
  drawn        from               Xm  the empirical joint
embedding  the crosscovariance operator  is estimated as

 cid    cid CX  

             

 cid 

 

 cid CX    

  cid 

  

 
 

 cid cid cid   cid 

 

 cid 

  

 

This empirical estimate converges to its population counterpart with   similar convergence rate as marginal embedding 

          xt
nt

          xs
ns

  and DXt    xt

  Maximum Mean Discrepancy
  be
Let DXs    xs
the sets of samples from distributions    Xs  and   Xt 
respectively  Maximum Mean Discrepancy  MMD   Gretton et al    is   kernel twosample test which rejects or
accepts the null hypothesis       based on the observed
samples  The basic idea behind MMD is that if the generating distributions are identical  all the statistics are the same 
Formally  MMD de nes the following difference measure 
DH         cid  sup
   
where   is   class of functions  It is shown that the class
of functions in an universal RKHS   is rich enough to
distinguish any two distributions and MMD is expressed as
the distance between their mean embeddings  DH         
 cid Xs         Xt    cid    The main theoretical result is that
      if and only if DH             Gretton et al   

 cid EXs     Xs    EXt

 cid   cid Xt cid cid cid     

In practice  an estimate of the MMD compares the square
distance between the empirical kernel mean embeddings as

 cid DH         

 cid 
 cid 

    xs
 

  

  

ns cid 
ns cid 
  cid xs
nt cid 
nt cid 
  cid xt
nt cid 
ns cid 
  cid xs

  

  

   xt
 

 cid 

    xt
 

 
  
 

 
  
 

 

   
nsnt

 

where  cid DH        is an unbiased estimator of DH       

  

  

    ys

  Joint Adaptation Networks
In unsupervised domain adaptation  we are given   source
   ns
domain Ds    xs
   of ns labeled examples and
  nt
  target domain Dt    xt
   of nt unlabeled examples 
The source domain and target domain are sampled from
joint distributions    Xs  Ys  and   Xt  Yt  respectively 
   cid     The goal of this paper is to design   deep neural
network           which formally reduces the shifts in
the joint distributions across domains and enables learning
both transferable features and classi ers  such that the target
risk Rt                        cid     can be minimized by
jointly minimizing the source risk and domain discrepancy 
Recent studies reveal that deep networks  Bengio et al 
  can learn more transferable representations than traditional handcrafted features  Oquab et al    Yosinski
et al    The favorable transferability of deep features
leads to several state of the art deep transfer learning methods  Ganin   Lempitsky    Tzeng et al    Long
et al      This paper also tackles unsupervised
domain adaptation by learning transferable features using
deep neural networks  We extend deep convolutional neural
networks  CNNs  including AlexNet  Krizhevsky et al 
  and ResNet  He et al    to novel joint adaptation
networks  JANs  as shown in Figure   The empirical error
of CNN classi er       on source domain labeled data Ds is

      xs

      ys

   

 

where    is the crossentropy loss function  Based on the
quanti cation study of feature transferability in deep convolutional networks  Yosinski et al    convolutional
layers can learn generic features that are transferable across
domains  Yosinski et al    Thus we opt to  netune
the features of convolutional layers when transferring pretrained deep models from source domain to target domain 
However  the literature  ndings also reveal that the deep
features can reduce  but not remove  the crossdomain distribution discrepancy  Yosinski et al    Long et al   

ns cid 

  

min

 

 
ns

Deep Transfer Learning with Joint Adaptation Networks

    Joint Adaptation Network  JAN 

    Adversarial Joint Adaptation Network  JANA 

Figure   The architectures of Joint Adaptation Network  JAN      and its adversarial version  JANA      Since deep features eventually
transition from general to speci   along the network  activations in multiple domainspeci   layers   are not safely transferable  And the
joint distributions of the activations    Zs          Zs    and   Zt          Zt    in these layers should be adapted by JMMD minimization 
  Zt          Zt    The resulting measure is called Joint
Maximum Mean Discrepancy  JMMD  which is de ned as
DL         cid   cid CZs           CZt       cid 
 cid   cid     
   
Based on the virtue of the kernel twosample test theory
 Gretton et al    we will have    Zs          Zs     
  Zt          Zt    if and only if DL           Given
source domain Ds of ns labeled points and target domain
Dt of nt unlabeled points drawn        from   and   respectively  the deep networks will generate activations in layers
  as  zs 
   The
empirical estimate of DL       is computed as the squared
distance between the empirical kernel mean embeddings as

  The deep features in standard CNNs must eventually
transition from general to speci   along the network  and the
transferability of features and classi ers decreases when the
crossdomain discrepancy increases  Yosinski et al   
In other words  even feedforwarding the source and target
domain data through the deep network for multilayer feature
abstraction  the shifts in the joint distributions    Xs  Ys 
and   Xt  Yt  still linger in the activations                 of
the higher network layers    Taking AlexNet  Krizhevsky
et al    as an example  the activations in the higher fullyconnected layers                     are not safely transferable for domain adaptation  Yosinski et al    Note
that the shift in the feature distributions    Xs  and   Xt 
mainly lingers in the feature layers           while the shift
in the label distributions    Ys  and   Yt  mainly lingers
in the classi er layer      Thus we can use the joint distributions of the activations in layers            Zs          Zs   
and   Zt          Zt    as good surrogates of the original
joint distributions    Xs  Ys  and   Xt  Yt  respectively 
To enable unsupervised domain adaptation  we should  nd
  way to match    Zs          Zs    and   Zt          Zt   

            zs   
 cid DL         

 cid 
 cid 

    zs cid 
 

 nt

  

  

 

 

 

 
  
 

            zt   
   and  zt 
 ns
 cid 
ns cid 
ns cid 
  cid cid zs cid 
 cid 
nt cid 
nt cid 
  cid cid zt cid 
 cid 
ns cid 
nt cid 
  cid cid zs cid 

 
  
 

    zt cid 
 

 cid  

 cid  

  

  

 

 cid 

    zt cid 
 

  Joint Maximum Mean Discrepancy

Many existing methods address transfer learning by bounding the target error with the source error plus   discrepancy
between the marginal distributions    Xs  and   Xt  of
the source and target domains  BenDavid et al    The
Maximum Mean Discrepancy  MMD   Gretton et al   
as   kernel twosample test statistic  has been widely applied to measure the discrepancy in marginal distributions
   Xs  and   Xt   Tzeng et al    Long et al   
  To date MMD has not been used to measure the
discrepancy in joint distributions    Zs          Zs    and
  Zt          Zt    possibly because MMD has not been directly de ned for joint distributions by  Gretton et al   
while in conventional shallow domain adaptation methods
the joint distributions are not easy to manipulate and match 
Following the virtue of MMD   we use the Hilbert space
embeddings of joint distributions   to measure the discrepancy of two joint distributions    Zs          Zs    and

   
nsnt

  

  

 cid  

      cid 

Remark  Taking   close look on the objectives of MMD  
and JMMD   we can  nd some interesting connections 
The difference is that  for the activations   cid  in each layer  cid   
   instead of putting uniform weights on the kernel function
   as in MMD  JMMD applies nonuniform weights 
  cid   cid 
re ecting the in uence of other variables in other layers
  cid  This captures the full interactions between different
variables in the joint distributions    Zs          Zs    and
  Zt          Zt    which is crucial for domain adaptation 
All previous deep transfer learning methods  Tzeng et al 
  Long et al    Ganin   Lempitsky    Tzeng
et al    Long et al    have not addressed this issue 

  Joint Adaptation Networks
Denote by   the domainspeci   layers where the activations are not safely transferable  We will formally reduce
the discrepancy in the joint distributions of the activations

XsXtZt   Zs   Zs Zt YsYtJMMD tiedtied   LAlexNetVGGnetGoogLeNetResNet XsXtZt   Zs   Zs Zt YsYtJMMD tiedtied   LAlexNetVGGnetGoogLeNetResNet Deep Transfer Learning with Joint Adaptation Networks

in layers            Zs          Zs    and   Zt          Zt   
Note that the features in the lower layers of the network
are transferable and hence will not require   further distribution matching  By integrating the JMMD   over the
domainspeci   layers   into the CNN error   the joint
distributions are matched endto end with network training 

network as shown in Figure     We maximize JMMD with
respect to these new parameters   to approach the virtue of
the original MMD   that is  maximizing the test power of
JMMD such that distributions of source and target domains
are made more distinguishable  Sriperumbudur et al   
This leads to   new adversarial joint adaptation network as

ns cid 

  

min

 

 
ns

       cid DL         

      xs

      ys

 

min

 

max

 

 
ns

      xs

      ys

       cid DL           

 

ns cid 

  

 cid DL         

 cid cid 
 cid cid 

 cid  

  cid 
  cid 

  

 
 

   
 

  

 cid  

  cid zs cid 

    zs cid 

     

  cid zs cid 

    zt cid 

     

  cid zt cid 

    zt cid 
   

  cid zt cid 

    zs cid 
   

 

 cid 
 cid 

 cid  

 cid  

 cid 
 cid 

where       is   tradeoff parameter of the JMMD penalty 
As shown in Figure     we set                     for
the JAN model based on AlexNet  last three layers  while
we set      pool       for the JAN model based on ResNet
 last two layers  as these layers are tailored to taskspeci  
structures  which are not safely transferable and should be
jointly adapted by minimizing CNN error and JMMD  
  limitation of JMMD   is its quadratic complexity  which
is inef cient for scalable deep transfer learning  Motivated
by the unbiased estimate of MMD  Gretton et al    we
derive   similar lineartime estimate of JMMD as follows 

 
where     ns  This lineartime estimate well  ts the minibatch stochastic gradient descent  SGD  algorithm  In each
minibatch  we sample the same number of source points
and target points to eliminate the bias caused by domain size 
This enables our models to scale linearly to large samples 

  Adversarial Training for Optimal MMD

The MMD de ned using the RKHS   has the advantage of
not requiring   separate network to approximately maximize
the original de nition of MMD   But the original MMD
  reveals that  in order to maximize the test power such
that any two distributions can be distinguishable  we require
the class of functions       to be rich enough  Although
 Gretton et al    shows that an universal RKHS is rich
enough  such kernelbased MMD may suffer from vanishing
gradients for lowbandwidth kernels  Moreover  it may be
possible that some widelyused kernels are unable to capture
very complex distances in high dimensional spaces such as
natural images  Reddi et al    Arjovsky et al   
To circumvent the issues of vanishing gradients and nonrich
function class of kernelbased MMD   we are enlightened
by the original MMD   which  ts the adversarial training
in GANs  Goodfellow et al    We add multiple fullyconnected layers parametrized by   to the proposed JMMD
  to make the function class of JMMD richer using neural

Learning deep features by minimizing this more powerful
JMMD  intuitively any shift in the joint distributions will be
more easily identi ed by JMMD and then adapted by CNN 
Remark  This version of JAN shares the idea of domainadversarial training with  Ganin   Lempitsky    but
differs in that we use the JMMD as the domain adversary
while  Ganin   Lempitsky    uses logistic regression 
As pointed out in   very recent study  Arjovsky et al   
our JMMDadversarial network can be trained more easily 

  Experiments
We evaluate the joint adaptation networks with state of the
art transfer learning and deep learning methods  Codes and
datasets are available at http github com thuml 

  Setup
Of ce   Saenko et al    is   standard benchmark for
domain adaptation in computer vision  comprising  
images and   categories collected from three distinct domains  Amazon     which contains images downloaded
from amazon com  Webcam     and DSLR     which
contain images respectively taken by web camera and digital SLR camera under different settings  We evaluate all
methods across three transfer tasks              and  
     which are widely adopted by previous deep transfer
learning methods  Tzeng et al    Ganin   Lempitsky 
  and another three transfer tasks              and
      as in  Long et al      Tzeng et al   
ImageCLEFDA  is   benchmark dataset for ImageCLEF
  domain adaptation challenge  which is organized by
selecting the   common categories shared by the following three public datasets  each is considered as   domain 
Caltech      ImageNet ILSVRC       and Pascal
VOC       There are   images in each category and
  images in each domain  We use all domain combinations and build   transfer tasks                        
            and        Different from Of ce  where
different domains are of different sizes  the three domains
in ImageCLEFDA are of equal size  which makes it   good
complement to Of ce  for more controlled experiments 

 http imageclef org adaptation

Deep Transfer Learning with Joint Adaptation Networks

We compare with conventional and state of the art transfer
learning and deep learning methods  Transfer Component
Analysis  TCA   Pan et al    Geodesic Flow Kernel
 GFK   Gong et al    Convolutional Neural Networks
AlexNet  Krizhevsky et al    and ResNet  He et al 
  Deep Domain Confusion  DDC   Tzeng et al   
Deep Adaptation Network  DAN   Long et al    Reverse Gradient  RevGrad   Ganin   Lempitsky    and
Residual Transfer Network  RTN   Long et al    TCA
is   transfer learning method based on MMDregularized
Kernel PCA  GFK is   manifold learning method that interpolates across an in nite number of intermediate subspaces
to bridge domains  DDC is the  rst method that maximizes
domain invariance by regularizing the adaptation layer of
AlexNet using linearkernel MMD  Gretton et al   
DAN learns transferable features by embedding deep features of multiple taskspeci   layers to reproducing kernel
Hilbert spaces  RKHSs  and matching different distributions
optimally using multikernel MMD  RevGrad improves domain adaptation by making the source and target domains
indistinguishable for   domain discriminator by adversarial
training  RTN jointly learns transferable features and adaptive classi ers by deep residual learning  He et al   
We examine the in uence of deep representations for domain adaptation by employing the breakthrough AlexNet
 Krizhevsky et al    and the state of the art ResNet  He
et al    for learning transferable deep representations 
For AlexNet  we follow DeCAF  Donahue et al    and
use the activations of layer      as image representation  For
ResNet   layers  we use the activations of the last feature
layer pool  as image representation  We follow standard
evaluation protocols for unsupervised domain adaptation
 Long et al    Ganin   Lempitsky    For both
Of ce  and ImageCLEFDA datasets  we use all labeled
source examples and all unlabeled target examples  We
compare the average classi cation accuracy of each method
on three random experiments  and report the standard error
of the classi cation accuracies by different experiments of
the same transfer task  We perform model selection by tuning hyperparameters using transfer crossvalidation  Zhong
et al    For MMDbased methods and JAN  we adopt
Gaussian kernel with bandwidth set to median pairwise
squared distances on the training data  Gretton et al   
We implement all deep methods based on the Caffe framework  and  netune from Caffeprovided models of AlexNet
 Krizhevsky et al    and ResNet  He et al    both
are pretrained on the ImageNet   dataset  We  netune
all convolutional and pooling layers and train the classi er
layer via back propagation  Since the classi er is trained
from scratch  we set its learning rate to be   times that
of the other layers  We use minibatch stochastic gradient
descent  SGD  with momentum of   and the learning rate
annealing strategy in RevGrad  Ganin   Lempitsky   

 

the learning rate is not selected by   grid search due to high
computational cost it is adjusted during SGD using the
following formula      
      where   is the training
progress linearly changing from   to              
and       which is optimized to promote convergence
and low error on the source domain  To suppress noisy activations at the early stages of training  instead of  xing the
adaptation factor   we gradually change it from   to   by  
 exp        and       is
progressive schedule      
 xed throughout experiments  Ganin   Lempitsky   
This progressive strategy signi cantly stabilizes parameter
sensitivity and eases model selection for JAN and JANA 

 

  Results

The classi cation accuracy results on the Of ce  dataset
for unsupervised domain adaptation based on AlexNet and
ResNet are shown in Table   As fair comparison with identical evaluation setting  the results of DAN  Long et al   
RevGrad  Ganin   Lempitsky    and RTN  Long et al 
  are directly reported from their published papers  The
proposed JAN models outperform all comparison methods
on most transfer tasks  It is noteworthy that JANs promote
the classi cation accuracies substantially on hard transfer
tasks             and        where the source and target
domains are substantially different and the source domain
is smaller than the target domain  and produce comparable
classi cation accuracies on easy transfer tasks        and
       where the source and target domains are similar
 Saenko et al    The encouraging results highlight the
key importance of joint distribution adaptation in deep neural networks  and suggest that JANs are able to learn more
transferable representations for effective domain adaptation 
The results reveal several interesting observations    Standard deep learning methods either outperform  AlexNet  or
underperform  ResNet  traditional shallow transfer learning
methods  TCA and GFK  using deep features  AlexNetfc 
and ResNetpool  as input  And traditional shallow transfer learning methods perform better with more transferable
deep features extracted by ResNet  This con rms the current
practice that deep networks learn abstract feature representations  which can only reduce  but not remove  the domain
discrepancy  Yosinski et al      Deep transfer learning methods substantially outperform both standard deep
learning methods and traditional shallow transfer learning
methods  This validates that reducing the domain discrepancy by embedding domainadaptation modules into deep
networks  DDC  DAN  RevGrad  and RTN  can learn more
transferable features    The JAN models outperform previous methods by large margins and set new state of the art
record  Different from all previous deep transfer learning
methods that only adapt the marginal distributions based
on independent feature layers  one layer for RevGrad and
multilayer for DAN and RTN  JAN adapts the joint distribu 

Deep Transfer Learning with Joint Adaptation Networks

RevGrad  Ganin   Lempitsky   

Method

AlexNet  Krizhevsky et al   

Table   Classi cation accuracy   on Of ce  dataset for unsupervised domain adaptation  AlexNet and ResNet 
     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

JAN  ours 
JANA  ours 

TCA  Pan et al   
GFK  Gong et al   
DDC  Tzeng et al   
DAN  Long et al   
RTN  Long et al   

JAN  ours 
JANA  ours 

ResNet  He et al   
TCA  Pan et al   
GFK  Gong et al   
DDC  Tzeng et al   
DAN  Long et al   
RTN  Long et al   

RevGrad  Ganin   Lempitsky   

Table   Classi cation accuracy   on ImageCLEFDA for unsupervised domain adaptation  AlexNet and ResNet 

Method

AlexNet  Krizhevsky et al   

DAN  Long et al   
RTN  Long et al   

JAN  ours 

ResNet  He et al   
DAN  Long et al   
RTN  Long et al   

JAN  ours 

     
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 

     
 
 
 
 
 
 
 
 

Avg
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Avg
 
 
 
 
 
 
 
 

tions of network activations in all domainspeci   layers to
fully correct the shifts in joint distributions across domains 
Although both JAN and DAN  Long et al    adapt multiple domainspeci   layers  the improvement from DAN to
JAN is crucial for the domain adaptation performance  JAN
uses   JMMD penalty to reduce the shift in the joint distributions of multiple taskspeci   layers  which re ects the shift
in the joint distributions of input features and output labels 
DAN needs multiple MMD penalties  each independently
reducing the shift in the marginal distribution of each layer 
assuming feature layers and classi er layer are independent 
By going from AlexNet to extremely deep ResNet  we can
attain   more indepth understanding of feature transferability    ResNetbased methods outperform AlexNetbased
methods by large margins  This validates that very deep
convolutional networks       VGGnet  Simonyan   Zisserman    GoogLeNet  Szegedy et al    and ResNet 
not only learn better representations for general vision tasks
but also learn more transferable representations for domain
adaptation    The JAN models signi cantly outperform
ResNetbased methods  revealing that even very deep networks can only reduce  but not remove  the domain discrepancy    The boost of JAN over ResNet is more signi cant
than the improvement of JAN over AlexNet  This implies

that JAN can bene   from more transferable representations 
The great aspect of JAN is that via the kernel trick there is
no need to train   separate network to maximize the MMD
criterion   for the ball of   RKHS  However  this has the
disadvantage that some kernels used in practice are unsuitable for capturing very complex distances in high dimensional spaces such as natural images  Arjovsky et al   
The JANA model signi cantly outperforms the previous domain adversarial deep network  Ganin   Lempitsky   
The improvement from JAN to JANA also demonstrates the
bene   of adversarial training for optimizing the JMMD in
  richer function class  By maximizing the JMMD criterion
with respect to   separate network  JANA can maximize the
distinguishability of source and target distributions  Adapting domains against deep features where their distributions
maximally differ  we can enhance the feature transferability 
The three domains in ImageCLEFDA are more balanced
than those of Of ce  With these more balanced transfer
tasks  we are expecting to testify whether transfer learning
improves when domain sizes do not change  The classi cation accuracy results based on both AlexNet and ResNet are
shown in Table   The JAN models outperform comparison
methods on most transfer tasks  but by less improvements 
This means the difference in domain sizes may cause shift 

Deep Transfer Learning with Joint Adaptation Networks

    DAN  Source  

    DAN  Target  

    JAN  Source  

    JAN  Target  

Figure   The tSNE visualization of network activations  ResNet  generated by DAN       and JAN       respectively 

    Adistance

    JMMD

Figure   Analysis      Adistance      JMMD      parameter sensitivity of       convergence  dashed lines show best baseline results 

    Accuracy         

    Convergence

  Analysis
Feature Visualization  We visualize in Figures      
the network activations of task       learned by DAN and
JAN respectively using tSNE embeddings  Donahue et al 
  Compared with the activations given by DAN in Figure       the activations given by JAN in Figures    
    show that the target categories are discriminated much
more clearly by the JAN source classi er  This suggests that
the adaptation of joint distributions of multilayer activations
is   powerful approach to unsupervised domain adaptation 
Distribution Discrepancy  The theory of domain adaptation  BenDavid et al    Mansour et al    suggests
Adistance as   measure of distribution discrepancy  which 
together with the source risk  will bound the target risk  The
proxy Adistance is de ned as dA           where  
is the generalization error of   classi er       kernel SVM 
trained on the binary problem of discriminating the source
and target  Figure     shows dA on tasks             
with features of CNN  DAN  and JAN  We observe that dA
using JAN features is much smaller than dA using CNN and
DAN features  which suggests that JAN features can close
the crossdomain gap more effectively  As domains   and
  are very similar  dA of task       is much smaller than
that of        which explains better accuracy of       
  limitation of the Adistance is that it cannot measure the
crossdomain discrepancy of joint distributions  which is
addressed by the proposed JMMD   We compute JMMD
  across domains using CNN  DAN and JAN activations
respectively  based on the features in      and groundtruth
labels in       the target labels are not used for model training  Figure     shows that JMMD using JAN activations is
much smaller than JMMD using CNN and DAN activations 

which validates that JANs successfully reduce the shifts in
joint distributions to learn more transferable representations 
Parameter Sensitivity  We check the sensitivity of JMMD
parameter        the maximum value of the relative weight
for JMMD  Figure     demonstrates the transfer accuracy
of JAN based on AlexNet and ResNet respectively  by varying                   on task       
The accuracy of JAN  rst increases and then decreases as
  varies and shows   bellshaped curve  This con rms the
motivation of deep learning and joint distribution adaptation 
as   proper tradeoff between them enhance transferability 
Convergence Performance  As JAN and JANA involve
adversarial training procedures  we testify their convergence
performance  Figure     demonstrates the test errors of
different methods on task        which suggests that JAN
converges fastest due to nonparametric JMMD while JANA
has similar convergence speed as RevGrad with signi cantly
improved accuracy in the whole procedure of convergence 

  Conclusion
This paper presented   novel approach to deep transfer learning  which enables endto end learning of transferable representations  Unlike previous methods that match the marginal
distributions of features across domains  the proposed approach reduces the shift in joint distributions of the network
activations of multiple taskspeci   layers  which approximates the shift in the joint distributions of input features and
output labels  The discrepancy between joint distributions
can be computed by embedding the joint distributions in  
tensorproduct Hilbert space  which can be scaled linearly
to large samples and be implemented in most deep networks 
Experiments testi ed the ef cacy of the proposed approach 

  WW DTransfer Task       ADistanceCNN  AlexNet DAN  AlexNet JAN  AlexNet   WW DTransfer Task JMMDCNN  AlexNet DAN  AlexNet JAN  AlexNet           Accuracy  AW  AlexNet AW  ResNet      Number of Iterations  Test ErrorRevGrad  ResNet JAN  ResNet JANA  ResNet Deep Transfer Learning with Joint Adaptation Networks

Acknowledgments
We thank Zhangjie Cao for conducting part of experiments 
This work was supported by NSFC    
National Key     Program of China  YFB 
 BAF    and Tsinghua TNList Lab Key Projects 

References
Arjovsky  Martin  Chintala  Soumith  and Bottou    on 
Wasserstein gan  arXiv preprint arXiv   

BenDavid     Blitzer     Crammer     Kulesza    
Pereira     and Vaughan          theory of learning from
different domains  Machine Learning   
 

Bengio     Courville     and Vincent     Representation
learning    review and new perspectives  IEEE Transactions on Pattern Analysis and Machine Intelligence
 TPAMI     

Bousmalis  Konstantinos  Trigeorgis  George  Silberman 
Nathan  Krishnan  Dilip  and Erhan  Dumitru  Domain
separation networks  In Advances in Neural Information
Processing Systems  NIPS  pp     

Collobert     Weston     Bottou     Karlen    
Kavukcuoglu     and Kuksa     Natural language processing  almost  from scratch  Journal of Machine Learning Research  JMLR     

Donahue     Jia     Vinyals     Hoffman     Zhang    
Tzeng     and Darrell     Decaf    deep convolutional
activation feature for generic visual recognition  In International Conference on Machine Learning  ICML 
 

Duan     Tsang        and Xu     Domain transfer multiple
kernel learning  IEEE Transactions on Pattern Analysis
and Machine Intelligence  TPAMI     

Ganin     and Lempitsky     Unsupervised domain adaptation by backpropagation  In International Conference on
Machine Learning  ICML   

Glorot     Bordes     and Bengio     Domain adaptation
for largescale sentiment classi cation    deep learning approach  In International Conference on Machine
Learning  ICML   

International Conference on Machine Learning  ICML 
 

Goodfellow     PougetAbadie     Mirza     Xu    
WardeFarley     Ozair     Courville     and Bengio 
   Generative adversarial nets  In Advances in Neural
Information Processing Systems  NIPS   

Gopalan     Li     and Chellappa     Domain adaptation for object recognition  An unsupervised approach 
In IEEE International Conference on Computer Vision
 ICCV   

Gretton     Borgwardt     Rasch     Sch lkopf     and
Smola       kernel twosample test  Journal of Machine
Learning Research  JMLR     

He     Zhang     Ren     and Sun     Deep residual
learning for image recognition  In IEEE Conference on
Computer Vision and Pattern Recognition  CVPR   

Hoffman     Guadarrama     Tzeng     Hu     Donahue 
   Girshick     Darrell     and Saenko     LSDA  Large
scale detection through adaptation  In Advances in Neural
Information Processing Systems  NIPS   

Huang     Smola        Gretton     Borgwardt        and
Sch lkopf     Correcting sample selection bias by unlabeled data  In Advances in Neural Information Processing
Systems  NIPS   

Krizhevsky     Sutskever     and Hinton        Imagenet
classi cation with deep convolutional neural networks 
In Advances in Neural Information Processing Systems
 NIPS   

Long  Mingsheng  Cao  Yue  Wang  Jianmin  and Jordan 
Michael    Learning transferable features with deep adaptation networks  In International Conference on Machine
Learning  ICML   

Long  Mingsheng  Zhu  Han  Wang  Jianmin  and Jordan 
Michael    Unsupervised domain adaptation with residual
transfer networks  In Advances in Neural Information
Processing Systems  NIPS  pp     

Mansour     Mohri     and Rostamizadeh     Domain
adaptation  Learning bounds and algorithms  In Conference on Computational Learning Theory  COLT   

Gong     Shi     Sha     and Grauman     Geodesic
 ow kernel for unsupervised domain adaptation  In IEEE
Conference on Computer Vision and Pattern Recognition
 CVPR   

Oquab     Bottou     Laptev     and Sivic     Learning
and transferring midlevel image representations using
convolutional neural networks  In IEEE Conference on
Computer Vision and Pattern Recognition  CVPR   

Gong     Grauman     and Sha     Connecting the
dots with landmarks  Discriminatively learning domaininvariant features for unsupervised domain adaptation  In

Pan        and Yang       survey on transfer learning 
IEEE Transactions on Knowledge and Data Engineering
 TKDE     

Deep Transfer Learning with Joint Adaptation Networks

Hilbert space embeddings and metrics on probability measures  Journal of Machine Learning Research  JMLR 
 Apr   

Sugiyama     Nakajima     Kashima     Buenau       
and Kawanabe     Direct importance estimation with
model selection and its application to covariate shift adaptation  In Advances in Neural Information Processing
Systems  NIPS   

Szegedy     Liu     Jia     Sermanet     Reed    
Anguelov     Erhan     Vanhoucke     and Rabinovich 
   Going deeper with convolutions  In IEEE Conference
on Computer Vision and Pattern Recognition  CVPR 
 

Tzeng     Hoffman     Zhang     Saenko     and Darrell 
   Deep domain confusion  Maximizing for domain
invariance  CoRR  abs   

Tzeng     Hoffman     Zhang     Saenko     and Darrell 
   Simultaneous deep transfer across domains and tasks 
In IEEE International Conference on Computer Vision
 ICCV   

Tzeng  Eric  Hoffman  Judy  Saenko  Kate  and Darrell 
Trevor  Adversarial discriminative domain adaptation 
arXiv preprint arXiv   

Wang     and Schneider     Flexible transfer learning under
support and model shift  In Advances in Neural Information Processing Systems  NIPS   

Yosinski     Clune     Bengio     and Lipson     How
transferable are features in deep neural networks  In Advances in Neural Information Processing Systems  NIPS 
 

Zhang     Sch lkopf     Muandet     and Wang     Domain adaptation under target and conditional shift  In
International Conference on Machine Learning  ICML 
 

Zhong     Fan     Yang     Verscheure     and Ren 
   Cross validation framework to choose amongst models and datasets for transfer learning  In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases  ECML PKDD  pp    Springer 
 

Pan        Tsang        Kwok        and Yang     Domain adaptation via transfer component analysis  IEEE
Transactions on Neural Networks  TNN   
 

QuioneroCandela     Sugiyama     Schwaighofer     and
Lawrence        Dataset shift in machine learning  The
MIT Press   

Reddi  Sashank    Ramdas  Aaditya    czos  Barnab   
Singh  Aarti  and Wasserman  Larry    On the high
dimensional power of   lineartime two sample test under meanshift alternatives  In Arti cial Intelligence and
Statistics Conference  AISTATS   

Saenko     Kulis     Fritz     and Darrell     Adapting
visual category models to new domains  In European
Conference on Computer Vision  ECCV   

Simonyan     and Zisserman     Very deep convolutional
networks for largescale image recognition  In International Conference on Learning Representations  ICLR 
   arXiv     

Smola  Alex  Gretton  Arthur  Song  Le  and Sch lkopf 
Bernhard    hilbert space embedding for distributions 
In International Conference on Algorithmic Learning
Theory  ALT  pp    Springer   

Song     Huang     Smola     and Fukumizu     Hilbert
space embeddings of conditional distributions with applications to dynamical systems  In International Conference on Machine Learning  ICML   

Song  Le and Dai  Bo  Robust low rank kernel embeddings
of multivariate distributions  In Advances in Neural Information Processing Systems  NIPS  pp   
 

Song  Le  Boots  Byron  Siddiqi  Sajid    Gordon  Geoffrey    and Smola  Alex  Hilbert space embeddings of
hidden markov models  In International Conference on
Machine Learning  ICML   

Song  Le  Fukumizu  Kenji  and Gretton  Arthur  Kernel
embeddings of conditional distributions    uni ed kernel
framework for nonparametric inference in graphical models  IEEE Signal Processing Magazine   
 

Sriperumbudur        Fukumizu     Gretton     Lanckriet 
   and Sch lkopf     Kernel choice and classi ability for
rkhs embeddings of probability distributions  In Advances
in Neural Information Processing Systems  NIPS   

Sriperumbudur  Bharath    Gretton  Arthur  Fukumizu 
Kenji  Sch lkopf  Bernhard  and Lanckriet  Gert RG 

