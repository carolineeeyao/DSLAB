Foresttype Regression with General Losses

and Robust Forest

Alexander Hanbo Li   Andrew Martin  

Abstract

This paper introduces   new general framework
for foresttype regression which allows the development of robust forest regressors by selecting from   large family of robust loss functions 
In particular  when plugged in the squared error
and quantile losses  it will recover the classical
random forest  Breiman    and quantile random forest  Meinshausen    We then use robust loss functions to develop more robust foresttype regression algorithms  In the experiments 
we show by simulation and real data that our robust forests are indeed much more insensitive to
outliers  and choosing the right number of nearest
neighbors can quickly improve the generalization
performance of random forest 

  Introduction
Since its development by Breiman   random forest
has proven to be both accurate and ef cient for classi cation and regression problems 
In regression setting  random forest will predict the conditional mean of   response
variable by averaging predictions of   large number of regression trees  Later then  many other machine learning
algorithms were developed upon random forest  Among
them  robust versions of random forest have also been proposed using various methodologies  Besides the sampling
idea  Breiman    which adds extra randomness  the
other variations are mainly based on two ideas    use
more robust criterion to construct regression trees  Galimberti et al    Brence   Brown    Roy   Larocque 
    choose more robust aggregation method  Meinshausen    Roy   Larocque    Tsymbal et al 
 
Meinshausen   generalized random forest to pre 

 University of California at San Diego  San Diego  California  USA  Zillow  Seattle  Washington  USA  Correspondence to 
Alexander Hanbo Li  alexanderhanboli gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

dict quantiles by discovering that besides calculating the
weighted mean of the observed response variables  one
could also get information for the weighted distribution of
observed response variables using the sets of local weights
generated by random forest  This method is strongly connected to the adaptive nearest neighbors procedure  Lin  
Jeon    which we will brie   review in section  
Different from classical kNN methods that rely on prede ned distance metrics  the dissimilarities generated by
random forest are data dependent and scaleinvariant 
Another stateof theart algorithm AdaBoost  Freund  
Schapire    Freund et al    has been generalized
to be applicable to   large family of loss functions  Friedman    Mason et al    Li   Bradic    Recent
development of more  exible boosting algorithms such as
xgboost  Chen   Guestrin    have become the goto
forest estimators with tabular or matrix data  One way in
which recent boosting algorithms have an advantage over
the random forest is the ability to customize the loss function used to reduce the in uence of outliers or optimize  
metric more suited to the speci   problem other than the
mean squared error 
In this paper  we will propose   general framework for
foresttype regression which can also be applied to   broad
family of loss functions 
It is claimed in  Meinshausen 
  that quantile random forest is another nonparametric approach which does not minimize an empirical loss 
However  we will show in fact both random forest and
quantile random forest estimators can be rederived as regression methods using the squared error or quantile loss
respectively in our framework 
Inspired by the adaptive
nearest neighbor viewpoint  we explore how random forest
makes predictions using the local weights generated by ensemble of trees  and connect that with locally weighted regression  Fan   Gijbels    Tibshirani   Hastie   
Staniswalis    Newey    Loader    Hastie  
Loader    The intuition is that when predicting the
target value                   at point    the observations
closer to   should receive larger weights  Different from
prede ning   kernel  random forest assigns the weights
data dependently and adaptively  After we illustrate the relation between random forest and local regression  we will
use random forest weights to design other regression algo 

rithms  By plugging robust loss functions like Huber loss
and Tukey   redescending loss  we get foresttype regression methods that are more robust to outliers  Finally  motivated from the truncated squared error loss example  we
will show that decreasing the number of nearest neighbors
in random forest will also immediately improve its generalization performance 
The layout of this paper is as follows  In Section   and  
we review random forest and adaptive nearest neighbors 
Section   introduces the general framework of foresttype
regression  In Section   we plug in robust regression loss
functions to get robust forest algorithms  In Section   we
motivate from the truncated squared error loss and investigate the importance of choosing right number of nearest
neighbors  Finally  we test our robust forests in Section
  and show that they are always superior to the traditional
formulation in the presence of outliers in both synthetic and
real data set 

  Random forest

Following the notation of Breiman   let   be the random parameter determining how   tree is grown  and data
                 For each tree     let   be the total
number of leaves  and Rl denotes the rectangular subspace
in   corresponding to the lth leaf  Then for every        
there is exactly one leaf   such that     Rl  Denote this
leaf by       
For each tree     the prediction of   new data point
      is the average of data values in leaf        that

is   cid          cid  

     Xi      Yi  where

  Xi        

   Xi Rl   

     Xj   Rl     

 

Finally  the conditional mean             is approxi 

mated by the averaged prediction of   trees   cid        
  cid  
  cid           After rearranging the terms  we can

write the prediction of random forest as

 cid        

  cid 

  

  Xi    Yi 

where the averaged weight   Xi     is de ned as

  cid 

  

  Xi      

 
 

  Xi        

 

 

From equation   the prediction of the conditional expectation             is the weighted average of the response
values of all observations  Furthermore  it is easy to show

that cid  

     Xi        

 cid         argmin

  

  cid 

  

Foresttype Regression with General Losses and Robust Forest

  Adaptive nearest neighbors

Lin and Jeon   studies the connection between random forest and adaptive nearest neighbor  They introduced
the socalled potential nearest neighbors  PNN    sample
point xi is called   kPNN to   target point   if there exists
  monotone distance metric under which xi is among the  
closest to   among all the sample points 
Therefore  any kNN method can be viewed as choosing
  points from the kPNNs according to some monotone
metric  For example  under Euclidean metric  the classical
kNN algorithm sorts the observations by their Euclidean
distances to the target point and outputs the   closest ones 
This is equivalent to weighting the kPNNs using inverse
   distance 
More interestingly  they prove that those observations with
positive weights   all belong to the kPNNs  Lin   Jeon 
  Therefore  random forests is another weighted kPNN method  but it assigns weights to the observations different from any kNN method under   prede ned monotonic distance metric  In fact  the random forest weights
are adaptive to the data if the splitting scheme is adaptive 

  General framework for foresttype

regression

In this section  we generalize the classical random forest to
  general foresttype regression  FTR  framework which is
applicable to   broad family of loss functions  In Section
  we motivate the framework by connecting random forest predictor with locally weighted regression  Then in Section   we formally propose the new foresttype regression framework  In Section   we rediscover the quantile
random forest estimator by plugging the quantile loss function into our framework 

  Squared error and random forest

Classical random forest can be understood as an estimator
of conditional mean         As shown in   the esti 

mator  cid       is weighted average of all response Yi    This

special form reminds us of the classical least squares regression  where the estimator is the sample mean  To be
more precise  we rewrite   as

  cid 

  

  Xi    Yi  cid          

 

Equation   is the estimating equation  rst order condition  of the locally weighted least squares regression  Ruppert   Wand   

  Xi    Yi    

 

Foresttype Regression with General Losses and Robust Forest

In classical local regression  the weight   Xi     serves
as   local metric between the target point   and observation Xi  Intuitively  observations closer to target   should
be given more weights when predicting the response at
   One common choice of such local metric is kernel
Kh Xi         Xi        For example  the tricube
kernel                          will ignore the impact of observations outside   window centered at   and increase the weight of an observation when it is getting closer
to    The form of kerneltype local regression is as follows 

  cid 

  

argmin

  

Kh Xi     Yi    

The random forest weight   Xi       de nes   similar
data dependent metric  which is constructed using the ensemble of regression trees  Using an adaptive splitting
scheme  each tree chooses the most informative predictors
from those at its disposal  The averaging process then assigns positive weights to these training responses  which
are called voting points in  Lin   Jeon    Hence
via the random forest voting mechanism  those observations close to the target point get assigned positive weights
equivalent to   kernel functionality  Friedman et al   

  Extension to general loss

Note that the formation   is just   special case when using
squared error loss                In more general form 
we have the following local regression problem 

 cid         argmin

   

  cid 

  

  Xi      Xi  Yi 

 

where   Xi     is   local weight    is   family of functions  and   is   general loss  For example  when local
weight is   kernel and   stands for polynomials of   certain degree  it reduces to local polynomial regression  Fan
  Gijbels    Random forest falls into this framework
with squared error loss    family of constant functions and
local weights   constructed from ensemble of trees 

Algorithm   Foresttype regression

Step   Calculate local weights   Xi     using ensemble or trees 
Step   Choose   loss   and   family   of function 
Then do the locally weighted regression

 cid         argmin

   

  cid 

  

  Xi    Yi    Xi 

In Algorithm   we summarize the foresttype regression
as   general twostep method  Note that here we only focus on local weights generated by random forest  which

uses ensemble of trees to recursively partition the covariate
space     However  there are many other data dependent
dissimilarity measures that can potentially be used  such as
kNN  mpdissimilarity  Aryal et al    shared nearest neighbors  Jarvis   Patrick    informationbased
similarity  Lin et al    massbased dissimilarity  Ting
et al    etc  And there are many other domain speci  
dissimilarity measures  To avoid distraction  we will only
use random forest weights throughout the rest of this paper 

  Quantile loss and quantile random forest

Meinshausen   proposed the quantile random forest
which can extract the information of different quantiles
rather than just predicting the average  It has been shown
that quantile random forest is more robust than the classical random forest  Meinshausen    Roy   Larocque 
  In this section  we show quantile random forest estimator is also   special case of Algorithm  
It is well
known that the  th quantile of an  empirical  distribution
is the constant that minimizes the  empirical  risk using  
th quantile loss function                     Koenker 
  Now let the loss function in Algorithm   be the
quantile loss       be the family of constant functions 
and   Xi     be random forest weights   Solving the
optimization problem

  cid 

  

  

  Xi      Yi    

we get the corresponding  rst order condition

 cid          argmin
  cid 
  Xi          Yi  cid               
Recall that cid  
  cid 

  Xi        Yi    cid           

     Xi         hence  we have

  

 

  

The estimator  cid        in   is exactly the same estimator
  the equation cid  
     Xi        Yi    cid         
will give us the median estimator  cid      Therefore  we

proposed in  Meinshausen    In particular  when    

have rediscovered quantile random forest from   totally different point of view as   local regression estimator with
quantile loss function and random forest weights 

  Robust forest
From the framework   quantile random forest is insensitive to outliers because of the more robust loss function  In
this section  we test our framework on other robust losses
and proposed  xedpoint method to solve the estimating

Foresttype Regression with General Losses and Robust Forest

equation  In Section   we choose the famous robust loss
   pseudo  Huber loss  and in Section   we further investigate   nonconvex loss   Tukey   biweight 

  Huber loss

The Huber loss  Huber et al   

 cid   

      

    
       

   

for        
elsewhere

is   wellknown loss function used in robust regression 
The penalty acts like squared error loss when the error is
within     but becomes linear outside this range  In this
way  it will penalize the outliers more lightly but still preserves more ef ciency than absolute deviation when data
is concentrated in the center and has light tails       Normal  By plugging Huber loss into the FTR framework  
we get   robust counterpart of random forest  The estimating equation is

wi    sign cid         Yi  min cid         Yi       

 

  cid 

  

Direct optimization of   with local weights is hard  hence
instead we will investigate the pseudoHuber loss  see Figure  

 cid cid 

 cid 
 cid     

 cid   

 

        

   

which is   smooth approximation of Huber loss  Charbonnier et al    The estimating equation

 cid cid YpH       Yi

 cid 

wpH

 

   

  cid 

  

   

 

is very similar to that of square error loss if we de ne   new
weight

 cid 

wpH

 

     

   

 cid  
 cid  

 cid YpH      

wi   

 cid cid YpH    Yi

 cid 

 

 

 

 

   wpH
   wpH

 

   Yi
   

 

 

Then the  pseudo  Huber estimator can be expressed as

     and
Figure   In the  rst row  we compare squared error loss  
pseudoHuber loss with different   In the second row  we plot
the scaling factor   of Huber loss  We observe that as   decreases to zero  the Huber loss becomes more linear and  at  and
the scaling factor shrinks more quickly as the input deviates from
zero 

and hence will shrink more to zero whenever  cid YpH    

Yi  is large  The tuning parameter   acts like   control of the
level of robustness    smaller   will lead to more shrinkage
on the weights of data that have responses far away from
the estimator 
The estimating equation   can be solved by  xpoint
method which we propose in Algorithm   For notation
simplicity  we will use wi   to denote   Xi  xj  where Xi
is the ith training point and xj is the jth testing point  The
convergence to the unique solution  if exists  is guaranteed
by Lemma  
Lemma   De ne

Informally  the estimator   can be viewed as   weighted
average of all the responses Yi    From   we know the
new weight for pseudoHuber loss has an extra scaling factor

 cid cid       

 cid 

 

      

 cid  
 cid  

  

  

 cid 
 cid 

wiYi
    Yi

 

wi

    Yi

 

 

 

 

Foresttype Regression with General Losses and Robust Forest

Algorithm   pseudoHuber loss  

Input  Test points  xj  
local weights wi    training responses  Yi  
tolerance  
while       do

   initial guess  cid    xj 

   and error

    Update the weights

Figure   We plot the scaling factor   of Tukey   biweight 
Compared to Huber scaling factor  see   it has   hard threshold at  

of redescending loss whose derivative will vanish to zero
as the input goes outside the interval     It is de ned
in the following way 

 cid 

      

 

for        
elsewhere 

Similarly  by rearranging the estimating equation  we have

 cid 

 
dy

      

  
 cid  
 cid Ytukey     
 cid  

 

where

wtukey Xi         Xi     max

   wtukey Xi    Yi
   wtukey Xi    

   
 cid 
 cid 

   

 

 cid cid Ytukey   Yi

 

 cid 

 

   

 

 cid 

   cid   

 

max

with an extra scaling factor  see Figure  

the  nal estimator actually only deIn another word 
pends on data with responses inside     and the importance of any data  Xi  Yi  will be shrinking to zero when

 cid Ytukey      Yi  gets closer to the boundary value  

  Truncated squared loss and nearest

neighbors

In this section  we will further use the framework   to investigate truncated squared error loss  and use this example to
motivate the relation between random forest generalization
performance and the number of adaptive nearest neighbors 

 cid 

 

wi  

 cid cid      xj  Yi
 cid  
 cid  

       
    Yi
       

   

 cid cid     xj   cid      xj 
 cid 

    

     

 cid 

   

    Update the estimator

 cid      xj   
  cid 

  

    Calculate error

   

 
 
             

end while
Output the pseudoHuber estimator 

 cid YpH  xj     cid      xj 

where cid  
Algorithm   can be written as  cid             cid      
   wi     Let     maxi      Yi  Then

and converges exponentially to   unique solution as long
as        

From Lemma   we know it is important to standardize the
responses Yi so that   will be of the same scale for different
problems  In practice  we observe that one will not need to
choose   that satis es the worstcase condition       in
order for convergence  but making   too small does lead
to slow convergence rate  For assigning the initial guess

 cid     two simplest ways are to either take the random forest

estimator we got or   constant vector equaling to the sample
mean  Throughout the rest of this paper  we will choose the
weights to be random forest weights  

  Tukey   biweight

Nonconvex function has played an important role in the
context of robust regression  Huber    Hampel et al 
  Unlike convex losses  the penalization on the errors can be bounded and hence the contribution of outliers in the estimating equation will eventually vanish  Our
forest regression framework   also incorporates the nonconvex losses which will show through the Tukey   biweight function     Huber    which is an example

Foresttype Regression with General Losses and Robust Forest

  Truncated squared error

For the truncated squared error loss

 cid   

    
   

 

      

for        
elsewhere

 cid 

the corresponding estimating equation is

  Xi    cid Ytrunc      Yi     

 cid Ytrunc   Yi 
wtrunc Xi         Xi        cid Ytrunc      Yi       

If we de ne   new weight

then the estimator for truncated squared loss is

 cid Ytrunc     

 cid  
 cid  

   wtrunc Xi    Yi
   wtrunc Xi    

 

 

The estimator   is like   trimmed version of the random
forest estimator   We  rst sort  Yi  
   and trim off the

responses where  cid Ytrunc      Yi      Therefore  for any
truncation level   the estimator cid Ytrunc    only depends on
data satisfying  cid Ytrunc      Yi      with the same local

random forest weights  

  Random Forest Nearest Neighbors

In classical random forest  all the data with positive weights

  are included when calculating the  nal estimator cid      

However  from section   we know in order to achieve
robustness  some of the data should be dropped out of consideration  For example  using the truncated squared error loss  we will only consider the data satisfying  Yi  
In classical random forest  the criterion of tree split is to reduce the mean squared error  then
in most cases  data points inside one terminal node will
tend to have more similar responses  So informally larger

 cid Ytrunc       
 cid Ytrim   Yi  will indicate smaller local weight   Xi    

Therefore  instead of solving for   we investigate   related estimator

 cid Ywt     

 cid 
 cid 

  Xi      Xi    Yi
  Xi      Xi    

 

where       is   constant in     Recall that in  Lin
  Jeon    they show all the observations with positive weights are considered voting points for random forest
estimator  However    implies that we should drop observations with weights smaller than   threshold in order
for the robustness  More formally  let   be   permutation
such that                             then  

is equivalent to cid        

  cid 

  

             

 

Then we can de ne the   random forest nearest neighbors
 kRFNN  of   to be                   and get

  cid 

predictor  cid Yk     
 cid              
where  cid                      cid  

  

             In
the numerical experiments  Section   we will test the
performance of the estimator   with different    and
show that by merely choosing the right number of nearest neighbors  one can largely improve the performance of
classical random forest 
Shi and Horvath   proposed   similar ensemble tree
based nearest neighbor method  In their approach  if the
observations Xi and Xj lie in the same leaf  then the similarity between them is increased by one  At the end  the
similarities are normalized by dividing the total number
of trees in the forest  Therefore  their weights  similart     Xi Rl    contrast
to   So different from their approach  for random forest  the similarity between Xi and Xj will be increased by
     Xp   Rl Xi  if they both lie in the same leaf
  Xi    This means the increment in the similarity also
depends on the number of data points in the leaf 

ities    Xi     will be   cid  

  Experiments
In this section  we plug in the quantile loss  Huber loss and
Tukey   biweight loss into the general forest framework
and compare these algorithms with random forest  Unless
otherwise stated  for both Huber and Tukey forest  the error
tolerance is set to be   and every forest is an ensemble
of   trees with maximum terminal node size   The
robust parameter   are set to be   and   for Huber
and Tukey forest  respectively 

  One dimensional toy example

We generate   training data points from   Uniform distribution on     and another   testing points from
the same distribution  The true underlying model is    
                  But on the training samples  we
choose   of the data and add noise     to the responses 
where    follows tdistribution with degree of freedom  
In Figure   we plot the true squared curve and different
forest predictions  It is clear that Huber and Tukey forest
achieve competitive robustness as quantile random forest 
and can almost recover the true underlying distribution  but
random forest is largely impacted by the outliers  We also
repeat the experiments for   times  and report the average mean squared error  MSE  mean absolute deviation
 MAD  and median absolute percentage error  MAPE  in
Table  

Foresttype Regression with General Losses and Robust Forest

Table   Comparison of the four methods in the setting   The
average MSE is reported in  rst row  and average MAD in second
row 

MSE

 

 

 

 

 

RF
QRF
HUBER
TUKEY

MAD

RF
QRF
HUBER
TUKEY

 
 
 
 

 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 

 

 

 
 
 
 

 
 
 
 

 
 
 
 

 

 
 
 
 

Figure   One dimensional comparison of random forest  quantile random forest  Huber forest and Tukey forest  All forests are
ensemble of   regression trees and the maximum number of
points in terminal nodes is  

Table   Comparison of random forest  RF  quantile random forest  QRF  Huber forest  Huber  and Tukey forest  Tukey  on one
dimensional example 

MEASURE

RF

QRF HUBER

TUKEY

MSE
MAD
MAPE

 
 
 

 
 
 

 
 
 

 
 
 

Table   Comparison of the four methods in the setting  

MSE

 

 

 

 

 

RF
QRF
HUBER
TUKEY

MAD

RF
QRF
HUBER
TUKEY

 
 
 
 

 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 

 

 

 
 
 
 

 
 
 
 

 
 
 
 

 

 
 
 
 

  Multivariate example

We generate data from   dimensional Normal distribution 
           cid    Then we test out algorithms on following models 

     cid 
     cid 

      
      

      and                 
      and               Toeplitz   

 
Then for each model  we randomly choose   proportion of
the training samples and add noise     where    follows
tdistribution with degree of freedom   The noise level
              The results are summarized
in Table   and   On the clean data  random forest still play
the best  however  Huber forest   performance is also competitive and lose less ef ciency than QRF and Tukey forest 
On the noisy data  all three robust methods outperform random forest  Among them  Huber forest is most robust and
stable 

  Nearest neighbors

In this section  we check how the number of adaptive nearest neighbors   in   will have impact on the performance
of kRFNN  We consider the same two models   and
  and keep both training sample size and testing sample size to be   The relations between MSE  MAD and
the number of adaptive nearest neighbors are illustrated in
Figure   Recall that kRFNN with all   neighbors is
equivalent to random forest  From the  gures  we clearly
observe   kink at       which is much less than  

  Real data

We take two regression datasets from UCI machine learning repository  Lichman    and one real estate dataset
from OpenIntro  For each dataset  we randomly choose  
observations for training and the rest for testing  MSE and
MAD are reported by averaging over   trials  The results
are presented in Table   To further test the robustness  we
then repeat the experiment but add extra    noise to  

Foresttype Regression with General Losses and Robust Forest

Table   Test on real data sets with extra noise 

MSE

RF

QRF

HUBER

TUKEY

CCS
AIRFOIL
AMES 

 
 
 

 
 
 

 
 
 

 
 
 

MAD

RF

QRF

HUBER

TUKEY

CCS
AIRFOIL
AMES 

 
 
 

 
 
 

 
 
 

 
 
 

expect even better performance after carefully tuning the
parameter 
Besides random forest weights  other data dependent similarities could also be used in Algorithm   We could also
design loss functions which optimizes   metric for speci  
problems  The  xedpoint method could be replaced by
other more ef cient algorithms  The framework could be
easily extended to classi cation problems  All these will
be potential future work 

 cid 
   

 

 cid 
   

  Appendix
  Proof of Lemma  

wi

   

 

wi

 

  

   

Proof  Because  cid             cid       which is    xed 
 cid cid cid      in order
 cid    Yi
 cid 

point method  we only need to show
for the existence and uniqueness of the solution  De ne the
normalized weight

 cid cid cid  
 cid    cid 
 cid 
 cid 
 cid wi  
 cid    Yi
 cid 
 cid cid cid  
 cid cid cid 
we have cid  
    cid wi     and
 cid cid cid cid cid cid    cid 
    cid 
             cid wj 
 cid wiYi
 cid 
  cid 
 cid wi  Yi  max
  cid 
 cid wi  Yi 
 cid cid cid     
 cid cid cid  

  max
     

  if       maxi      Yi       

 cid cid cid cid cid cid 

mini    

         Yi 

         Yj 

     Yi 

 cid 
 cid   
   Yi         Yi cid 

 

 Yi   
 

 

Therefore 

    Yj

  

  

   

   

 cid 
   

 

  

  

     

Figure   The performance of kRFNN against the number of
nearest neighbors 

of the standardized training data response variables everytime  The results are in Table   Robust forests outperform
random forest in most of the cases except for Ames data
sets  on which quantile random forest behaves poorly 

Table   Comparison of the four methods on two UCI repository
datasets    concrete compressive strength  CCS   Yeh   
  airfoil selfnoise  Airfoil  and one OpenIntro dataset  Ames
residential home sales  Ames 

MSE

RF

QRF

HUBER

TUKEY

CCS
AIRFOIL
AMES 

 
 
 

 
 
 

 
 
 

 
 
 

MAD

RF

QRF

HUBER

TUKEY

CCS
AIRFOIL
AMES 

 
 
 

 
 
 

 
 
 

 
 
 

  Conclusion and discussion
The experimental results show that Huber forest  Tukey forest and quantile random forest are all much more robust
than random forest in the presence of outliers  However 
without outliers  Huber forest preserves more ef ciency
than the other two robust methods  We did not cross validate the parameter   for different noise levels  so one would

Foresttype Regression with General Losses and Robust Forest

Acknowledgements
We would like to thank Stan Humphrys and Zillow for supporting this research  as well as three anonymous referees
for their insightful comments  Part of the implementation
in this paper is based on Zillow code library 

References
Aryal  Sunil  Ting  Kai Ming  Haffari  Gholamreza  and
Washio  Takashi  mpdissimilarity    data dependent
In Data Mining  ICDM   
dissimilarity measure 
IEEE International Conference on  pp    IEEE 
 

Breiman  Leo  Random forests  Machine learning   

   

Brence  MAJ John   and Brown  Donald    Improving the
robust random forest regression algorithm  Systems and
Information Engineering Technical Papers  Department
of Systems and Information Engineering  University of
Virginia   

Charbonnier  Pierre  BlancF eraud  Laure  Aubert  Gilles 
and Barlaud  Michel  Deterministic edgepreserving regularization in computed imaging  IEEE Transactions on
image processing     

Chen  Tianqi and Guestrin  Carlos  Xgboost    scalable
tree boosting system  In Proceedings of the  Nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  pp    ACM   

Fan  Jianqing and Gijbels  Irene  Local polynomial modelling and its applications  monographs on statistics and
applied probability   volume   CRC Press   

Freund  Yoav and Schapire  Robert      desiciontheoretic
generalization of online learning and an application to
In European conference on computational
boosting 
learning theory  pp    Springer   

Freund  Yoav  Schapire  Robert    et al  Experiments with
  new boosting algorithm  In icml  volume   pp   
   

Hampel  Frank    Ronchetti  Elvezio    Rousseeuw  Peter    and Stahel  Werner    Robust statistics  the approach based on in uence functions  volume   John
Wiley   Sons   

Hastie  Trevor and Loader  Clive  Local regression  Automatic kernel carpentry  Statistical Science  pp   
 

Huber  Peter    Robust statistics  Springer   

Huber  Peter   et al  Robust estimation of   location parameter  The Annals of Mathematical Statistics   
   

Jarvis  Raymond Austin and Patrick  Edward    Clustering
using   similarity measure based on shared near neighbors  IEEE Transactions on computers   
   

Koenker  Roger  Quantile regression  Number   Cam 

bridge university press   

Li  Alexander Hanbo and Bradic  Jelena  Boosting in the
presence of outliers  adaptive classi cation with nonconvex loss functions  Journal of the American Statistical Association   justaccepted   

Lichman     UCI machine learning repository    URL

http archive ics uci edu ml 

Lin  Dekang et al  An informationtheoretic de nition of
similarity  In ICML  volume   pp    Citeseer 
 

Lin  Yi and Jeon  Yongho  Random forests and adaptive
nearest neighbors  Journal of the American Statistical
Association     

Loader  Clive  Local regression and likelihood  Springer

Science   Business Media   

Mason  Llew  Baxter  Jonathan  Bartlett  Peter    and
Frean  Marcus    Boosting algorithms as gradient descent  In NIPS  pp     

Meinshausen  Nicolai  Quantile regression forests  Journal
of Machine Learning Research   Jun   

Friedman  Jerome  Hastie  Trevor  and Tibshirani  Robert 
The elements of statistical learning  volume   Springer
series in statistics Springer  Berlin   

Newey  Whitney    Kernel estimation of partial means and
  general variance estimator  Econometric Theory   
   

Friedman  Jerome    Greedy function approximation   
gradient boosting machine  Annals of statistics  pp 
   

Roy  MarieH el ene and Larocque  Denis  Robustness of
random forests for regression  Journal of Nonparametric
Statistics     

Galimberti  Giuliano  Pillati  Marilena  and Soffritti 
Gabriele  Robust regression trees based on mestimators 
Statistica     

Ruppert  David and Wand  Matthew    Multivariate locally
weighted least squares regression  The annals of statistics  pp     

Foresttype Regression with General Losses and Robust Forest

Shi  Tao and Horvath  Steve  Unsupervised learning with
random forest predictors  Journal of Computational and
Graphical Statistics     

Staniswalis  Joan    The kernel estimate of   regresJournal of
sion function in likelihoodbased models 
the American Statistical Association   
 

Tibshirani  Robert and Hastie  Trevor  Local likelihood estimation  Journal of the American Statistical Association     

Ting  Kai Ming  Zhu  Ye  Carman  Mark  Zhu  Yue 
and Zhou  ZhiHua  Overcoming key weaknesses of
distancebased neighbourhood methods using   data
In Proceedings of
dependent dissimilarity measure 
the  nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining  pp   
ACM   

Tsymbal  Alexey  Pechenizkiy  Mykola  and Cunningham 
  adraig  Dynamic integration with random forests  In
European conference on machine learning  pp   
Springer   

Yeh  IC  Modeling of strength of highperformance concrete using arti cial neural networks  Cement and Concrete research     

