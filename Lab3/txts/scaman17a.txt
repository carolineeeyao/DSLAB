Optimal Algorithms for Smooth and Strongly Convex

Distributed Optimization in Networks

Kevin Scaman   Francis Bach     ebastien Bubeck   Yin Tat Lee   Laurent Massouli    

Abstract

 

In this paper  we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings  centralized and decentralized communications over  
network  For centralized       master slave  algorithms  we show that distributing Nesterov  
is optimal and
accelerated gradient descent
achieves   precision       in time   
     
    ln  where    is the condition number of the  global  function to optimize    is
the diameter of the network  and    resp   
is the time needed to communicate values between two neighbors  resp  perform local computations  For decentralized algorithms based
on gossip  we provide the  rst optimal algorithm 
called the multistep dual accelerated  MSDA 
method  that achieves   precision       in time
 
    ln  where    is the conO 
dition number of the local functions and   is the
 normalized  eigengap of the gossip matrix used
for communication between nodes  We then verify the ef ciency of MSDA against stateof theart methods for two problems  leastsquares regression and classi cation by logistic regression 

       

  Introduction
Given the numerous applications of distributed optimization in machine learning  many algorithms have recently
emerged  that allow the minimization of objective functions
  de ned as the average  
   fi of functions fi which
 
are respectively accessible by separate nodes in   network
 Nedic   Ozdaglar    Boyd et al    Duchi et al 
  Shi et al    These algorithms typically alter 

 cid  

 MSRINRIA Joint Center  Palaiseau  France  INRIA  Ecole
Normale Sup erieure  Paris  France  Theory group  Microsoft Research  Redmond  United States  Correspondence to  Kevin Scaman  kevin scaman gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

nate local incremental improvement steps  such as gradient
steps  with communication steps between nodes in the network  and come with   variety of convergence rates  see
for example Shi et al      Jakoveti   et al   
Nedich et al   
Two main regimes have been looked at      centralized
where communications are precisely scheduled and     decentralized where communications may not exhibit   precise schedule  In this paper  we consider these two regimes
for objective functions which are smooth and stronglyconvex and for which algorithms are linearly  exponentially  convergent  The main contribution of this paper is
to propose new and matching upper and lower bounds of
complexity for this class of distributed problems 
The optimal complexity bounds depend on natural quantities in optimization and network theory  Indeed      for
  single machine the optimal number of gradient steps to
optimize   function is proportional to the square root of
the condition number  Nesterov    and     for mean
estimation  the optimal number of communication steps is
proportional to the diameter of the network in centralized
problems or to the square root of the eigengap of the Laplacian matrix in decentralized problems  Boyd et al   
As shown in Section   our lower complexity bounds happen to be combinations of the two contributions above 
These lower complexity bounds are attained by two separate algorithms  In the centralized case  the trivial distribution of Nesterov   accelerated gradient attains this rate 
while in the decentralized case  as shown in Section   the
rate is achieved by   dual algorithm  We compare favorably
our new optimal algorithms to existing work in Section  
Related Work  Decentralized optimization has been extensively studied and early methods such as decentralized gradient descent  Nedic   Ozdaglar    Jakoveti  
et al    or decentralized dual averaging  Duchi et al 
  exhibited sublinear convergence rates  More recently    number of methods with provable linear convergence rates were developed  including EXTRA  Shi
et al    Mokhtari   Ribeiro    augmented Lagrangians  Jakoveti   et al    and more recent approaches  Nedich et al    The most popular of

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

such approaches is the distributed alternating direction
method of multipliers  DADMM   Boyd et al   
Wei   Ozdaglar    Shi et al    and has led to
  large number of variations and extensions 
In   different direction  second order methods were also investigated  Mokhtari et al    Tutunov et al    However  to the best of our knowledge  the  eld still lacks
  coherent theoretical understanding of the optimal convergence rates and its dependency on the characteristics
of the communication network 
In several related  elds 
complexity lower bounds were recently investigated  including the sequential optimization of   sum of functions
 Arjevani   Shamir        distributed optimization in
 at       totally connected  networks  Shamir    Arjevani   Shamir    or distributed stochastic optimization  Shamir   Srebro   

  Distributed Optimization Setting
  Optimization Problem
Let           be   connected simple       undirected 
graph of   computing units and diameter   each having
access to   function fi  over     Rd  We consider minimizing the average of the local functions

  cid 

  

      

min
 Rd

 
 

fi 

 

in   distributed setting  More speci cally  we assume that 

  Each computing unit can compute  rstorder characteristics  such as the gradient of its own function or
its Fenchel conjugate  By renormalization of the time
axis  and without loss of generality  we assume that
this computation is performed in one unit of time 

  Each computing unit can communicate values      
vectors in Rd  to its neighbors  This communication
requires   time    which may be smaller or greater
than  

These actions may be performed asynchronously and in
parallel  and each node   possesses   local version of the
parameter  which we refer to as     Moreover  we assume that each function fi is  strongly convex and  
      the local condition
smooth  and we denote by       
number  We also denote by        and     respectively 
the strong convexity  smoothness and condition number
of the average  global  function     Note that we always
have          while the opposite inequality is  in general  not true  take for example            and
           for which        and       
However  the two quantities are close  resp  equal  when
the local functions are similar  resp  equal  to one another 

  Decentralized Communication

  large body of literature considers   decentralized approach to distributed optimization based on the gossip algorithm  Boyd et al    Nedic   Ozdaglar    Duchi
et al    Wei   Ozdaglar    In such   case  communication is represented as   matrix multiplication with  
matrix   verifying the following constraints 

    is an       symmetric matrix 

    is positive semide nite 

  The kernel of   is the set of constant vectors 

Ker       Span  where          cid 

    is de ned on the edges of the network  Wij  cid   

only if       or            

  Aij

The third condition will ensure that the gossip step converges to the average of all the vectors shared between the
nodes  We will denote the matrix   as the gossip matrix 
since each communication step will be represented using
it  Note that   simple choice for the gossip matrix is the
Laplacian matrix            where   is the adjacency

matrix of the network and     diag cid cid 

 cid  However 

in the presence of large degree nodes  weighted Laplacian
matrices are usually   better choice  and the problem of optimizing these weights is known as the fastest distributed
consensus averaging problem and is investigated by Xiao
  Boyd   Boyd et al   
We will denote by                       the spectrum of the gossip matrix     and its  normalized  eigengap
the ratio                  between the second
smallest and the largest eigenvalue  Equivalently  this is
the inverse of the condition number of   projected on the
space orthogonal to the constant vector   This quantity
will be the main parameter describing the connectivity of
the communication network in Section   and Section  

  Optimal Convergence Rates
In this section  we prove oracle complexity lower bounds
for distributed optimization in two settings  strongly convex and smooth functions for centralized       master slave 
and decentralized algorithms based on   gossip matrix    
In the  rst setting  we show that distributing accelerated
gradient descent matches the optimal convergence rate 
while  in the second setting  the algorithm proposed in Section   is shown to be optimal  Note that we will use the
notation           for                       
Cf   and will  for simplicity  omit the additive terms that
do not depend on the precision   in Corollary   and Corollary  

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

  Blackbox Optimization Procedures

The lower bounds provided hereafter depend on   new notion of blackbox optimization procedures for the problem
in Eq    where we consider distributed algorithms verifying the following constraints 

  Local memory  each node   can store past values in
   nite  internal memory Mi     Rd at time      
These values can be accessed and used at time   by
the algorithm run by node    and are updated either
by local computation or by communication  de ned
below  that is  for all           

Mi     Mcomp

      Mcomm

   

 

 

  Local computation  each node   can  at time    compute the gradient of its local function  fi  or its
    for   value     Mi   in the
Fenchel conjugate    
node   internal memory  that is  for all           
Mcomp
      Span  fi   
          Mi     
 
  Local communication  each node   can  at time   
share   value to all or part of its neighbors  that is  for
all           
Mcomm

 cid   cid 

Mj   

  Span

 cid 

 

 

   

      

  Output value  each node   must  at time    specify one
vector in its memory as local output of the algorithm 
that is  for all           

       Mi   

 

Hence    blackbox procedure will return   output values 
one for each node of the network and our analysis will
focus on ensuring that all local output values are converging to the optimal parameter of Eq    Moreover  we will
say that   blackbox procedure uses   gossip matrix   if
the local communication is achieved by multiplication of  
vector with     For simplicity  we assume that all nodes
start with the simple internal memory Mi      Note
that communications and local computations may be performed in parallel and asynchronously 

  Centralized Algorithms
 

 

In this section  we show that  for any blackbox optimiza 
   ln  gradient steps and
tion procedure  at least  
   ln  communication steps are necessary to
 
achieve   precision       where    is the global condition number and   is the diameter of the network  These
lower bounds extend the communication complexity lower
bounds for totally connected communication networks of

 

Arjevani   Shamir   and are natural since at least
   ln  steps are necessary to solve   strongly
 
convex and smooth problem up to    xed precision  and
at least   communication steps are required to transmit  
message between any given pair of nodes 
In order to simplify the proofs of the following theorems 
and following the approach of Bubeck   we will
consider the limiting situation       More speci 
cally  we now assume that we are working in  cid       
Theorem   Let   be   graph of diameter       and
size       and             There exists   functions
   cid      such that    is    strongly convex and   
fi
smooth  and for any       and any blackbox procedure
one has  for all           

          cid 

      rather than Rd 

   

 cid 

 cid   

                    
 

where           

     
  

 cid       cid 
 

The proof of Theorem   relies on splitting the function
used by Nesterov to prove oracle complexities for strongly
convex and smooth optimization  Nesterov    Bubeck 
  on two nodes at distance   One can show that most
dimensions of the parameters      will remain zero  and local gradient computations may only increase the number
of nonzero dimensions by one  Finally  at least   communication rounds are necessary inbetween every gradient computation  in order to share information between the
two nodes  The detailed proof is available as supplementary material 
Corollary   For any graph of diameter   and any blackbox procedure  there exists functions fi such that the time
to reach   precision       is lower bounded by

 cid 

 cid 

 cid 

 cid   

 cid cid 

 

 

  

     

ln

 

 

This optimal convergence rate is achieved by distributing
Nesterov   accelerated gradient descent on the global function  Computing the gradient of    is performed by sending all the local gradients  fi to   single node  denoted
as master node  in   communication steps  which may
involve several simultaneous messages  and then returning the new parameter     to every node in the network
 which requires another   communication steps  In practice  summing the gradients can be distributed by computing   spanning tree  with the root as master node  and asking for each node to perform the sum of its children   gradients before sending it to its parent  Standard methods as
described by Bertsekas   Tsitsiklis   can be used for
performing this parallelization of gradient computations 
This algorithm has three limitations   rst  the algorithm is
not robust to machine failures  and the central role played

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

by the master node also means that   failure of this particular machine may completely freeze the procedure  Second 
and more generally  the algorithm requires precomputing  
spanning tree  and is thus not suited to timevarying graphs 
in which the connectivity between the nodes may change
through time       in peerto peer networks  Finally  the
algorithm requires every node to complete its gradient computation before aggregating them on the master node  and
the ef ciency of the algorithm thus depends on the slowest of all machines  Hence  in the presence of nonuniform
latency of the local computations  or the slow down of  
speci   machine due to   hardware failure  the algorithm
will suffer   signi cant drop in performance 

  Decentralized Algorithms

The gossip algorithm  Boyd et al    is   standard
method for averaging values across   network when its connectivity may vary through time  This approach was shown
to be robust against machine failures  nonuniform latencies and asynchronous or timevarying graphs  and   large
body of literature extended this algorithm to distributed optimization  Nedic   Ozdaglar    Duchi et al   
Wei   Ozdaglar    Shi et al    Jakoveti   et al 
  Nedich et al    Mokhtari et al   
The convergence analysis of decentralized algorithms usually relies on the spectrum of the gossip matrix   used
for communicating values in the network  and more specifically on the ratio between the second smallest and the
largest eigenvalue of     denoted  
In this section  we
show that  with respect to this quantity and     reaching  
 
   ln  gradient steps
precision   requires at least  
and  
communication steps  by exhibiting
  gossip matrix such that   corresponding lower bound exists 
Theorem   Let         and         There exists  
gossip matrix   of eigengap          and  strongly
convex and  smooth functions fi    cid      such that  for
any       and any blackbox procedure using   one has 
for all           

 cid cid    

  ln 

 cid 

 cid 

 cid   

 
   
 

                   
 

     
  

   cid       cid 
 

where        is the local condition number 

The proof of Theorem   relies on the same technique as
that of Theorem   except that we now split the two functions on   subset of   linear graph  These networks have
 
the appreciable property that      
  and we can thus
use   slightly extended version of Theorem   to derive the
desired result  The complete proof is available as supplementary material 

Corollary   For any       there exists   gossip matrix
  of eigengap   and  strongly convex   smooth functions such that  with        for any blackbox procedure using   the time to reach   precision       is lower
bounded by

 cid 

 cid 

 

  

   

 cid 

 cid   

 cid cid 

 
 

ln

 

 

 

 

   

We will see in the next section that this lower bound is met
for   novel decentralized algorithm called multistep dual
accelerated  MSDA  and based on the dual formulation of
the optimization problem  Note that these results provide
optimal convergence rates with respect to    and   but do
not imply that   is the right quantity to consider on gen 
 
  may indeed be very large
eral graphs  The quantity  
compared to   for example for star networks  for which
 
   However  on many simple net 
      and  
works  the diameter   and the eigengap of the Laplacian
 
matrix are tightly connected  and      
  For exam 
 
        for
ple  for linear graphs            and  
 
      and
totally connected networks        and  
 
for regular networks   
 Alon   Milman 
  Finally  note that the case of totally connected networks corresponds to   previous complexity lower bound
on communications proven by Arjevani   Shamir  
and is equivalent to our result for centralized algorithms
with      

 
  ln   

   

 

 

  Optimal Decentralized Algorithms
In this section  we present   simple framework for solving
the optimization problem in Eq    in   decentralized setting  from which we will derive several variants  including
  synchronized algorithm whose convergence rate matches
the lower bound in Corollary   Note that the naive approach of distributing each  accelerated  gradient step by
gossiping does not lead to   linear convergence rate  as the
number of gossip steps has to increase with the number of
iterations to ensure the linear rate is preserved  We begin
with the simplest form of the algorithm  before extending
it to more advanced scenarios 

  SingleStep Dual Accelerated Method

  standard approach for solving Eq     see Boyd et al 
  Jakoveti   et al    consists in rewriting the optimization problem as

  cid 

  

min
 Rd

       min

  

 
 

fi   

 

Furthermore  the equality constraint            is
      where                   and
equivalent to  
  is   gossip matrix verifying the assumptions described

 

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

Algorithm   SingleStep Dual Accelerated method
Input  number of iterations       gossip matrix    

 
 

   

        

Rn         

 
   
 
 
Output         for          
               
  for       to       do
          
 
yt    xt    tW
 
xt         yt     yt
 
  end for

   xi    for all          

 

  exists and is de ned as

 
in Section   Note that  since   is positive semide nite 
       cid     where
       cid   is the singular value decomposition of    
 
 
The equality  
      implies that each row of   is conW     Span  and is thus equivalent
stant  since Ker 
to             This leads to the following primal version of the optimization problem 

where        cid  

min
 Rd      

 

   

   

 

   fi    Since Eq    is   convex
problem  it is equivalent to its dual optimization problem 

 

    

   

max
 Rd  

 
where         supx Rd   cid      cid          is the Fenchel
conjugate of     and  cid      cid    tr   cid    is the standard scalar
product between matrices 
The optimization problem in Eq    is unconstrained and
convex  and can thus be solved using   variety of convex
optimization techniques  The proposed singlestep dual accelerated  SSDA  algorithm described in Alg    uses Nesterov   accelerated gradient descent  and can be thought of
as an accelerated version of the distributed augmented Lagrangian method of Jakoveti   et al    for       The
algorithm is derived by noting that   gradient step of size
      for Eq    is

 
                
 
and the change of variable yt     

 

  leads to

   

   

 

yt    yt       yt   

 

   yj     

This equation can be interpreted as gossiping the gradients of the local conjugate functions    
   yi    since
    yt ij      
Theorem   The iterative scheme in Alg    converges to
     cid  where   is the solution of Eq    Furthermore  the time needed for this algorithm to reach any given
precision       is

 cid 

 cid    

 cid   

 cid cid 

 

       

ln

 

 

This theorem relies on proving that the condition number
of the dual objective function is upper bounded by   
    and
noting that the convergence rate for accelerated gradient
descent depends on the square root of the condition number
 see       Bubeck     detailed proof is available as
supplementary material 

  MultiStep Dual Accelerated Method

The main problem of Alg    is that it always performs the
same number of gradient and gossip steps  When communication is cheap compared to local computations    cid   
it would be preferable to perform more gossip steps than
gradient steps in order to propagate the local gradients further than the local neighborhoods of each node  This can be
achieved by replacing   by PK     in Alg    where PK
is   polynomial of degree at most    If PK     is itself  
gossip matrix  then the analysis of the previous section can
be applied and the convergence rate of the resulting algorithm depends on the eigengap of PK     Maximizing
this quantity for    xed   leads to   common acceleration scheme known as Chebyshev acceleration  Auzinger 
  Arioli   Scott    and the choice

PK          TK        

TK   

 

 

where       
  and TK are the Chebyshev polynomials
 Auzinger    de ned as                    and 
for all      

Tk       xTk      Tk   

 

Finally  verifying that this particular choice of PK     is
 cid  leads to
indeed   gossip matrix  and taking      cid   
Alg    with an optimal convergence rate with respect to  
and    
Theorem   The iterative scheme in Alg    converges to
     cid  where   is the solution of Eq    Furthermore  the time needed for this algorithm to reach any given
precision       is

 cid 

 cid 

 

  

   

 cid 

 cid   

 cid cid 

 
 

ln

 

 

 

The proof of Theorem   relies on standard properties of
Chebyshev polynomials that imply that  for the particular
choice of      cid   
    Hence 
Theorem   applied to the gossip matrix    cid    PK    
gives the desired convergence rate  The complete proof is
available as supplementary material 

 cid  we have

 

 PK     

  Discussion and Further Developments

 

 

We now discuss several extensions to the proposed algorithms 

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

 

 

 

 

 

 cid 

   

 cid   

          

Algorithm   MultiStep Dual Accelerated method
Input  number of iterations       gossip matrix    
    
 
   cK
   cK

        
 
           cK
 
   
 cK
   

Rn          
 

   xi    for all          

          
   
 cK
Output         for          
               
  for       to       do
          
 
yt    xt     ACCELERATEDGOSSIP        
 
xt         yt     yt
 
  end for
  procedure ACCELERATEDGOSSIP        
                
                            
  for       to       do
ak       ak   ak 
 
xk       xk             xk 
 
  end for
  return      xK
aK
  end procedure

   xi   

Computation of    
In practice  it may be hard
to apply the dual algorithm when conjugate functions are
hard to compute  We now provide three potential solutions
to this problem    warm starts may be used for the optimization problem    
   xi      argmin  fi      cid 
    
by starting from the previous iteration       This will
drastically reduce the number of steps required for convergence    SSDA and MSDA can be extended to composite functions of the form fi    gi Bi      cid cid 
  for
Bi   Rmi   and gi smooth  and for which we know how
to compute the proximal operator  This allows applications
in machine learning such as logistic regression  See supplementary material for details    Beyond the composite
case  one can also add   small  wellchosen  quadratic term
to the dual  and by applying accelerated gradient descent on
the corresponding primal  get an algorithm that uses primal
gradient computations and achieves almost the same guarantee as SSDA and MSDA  off by   log    factor 
Local vs  Global Condition Number  MSDA and SSDA
depend on the worst strong convexity of the local functions
mini     which may be very small    simple trick can be
used to depend on the average strong convexity  Using the
proxy functions gi    fi     cid cid 
  instead of fi 
where      
     is the average strong convexity  will
 
improve the local condition number from      maxi   
to
mini   

 cid 

 cid 
   

maxi        

 

   

 

Several algorithms  including EXTRA  Shi et al   
and DIGing  Nedich et al    have convergence rates
that depend on the strong convexity of the global func 

 cid 

 cid 

    ln 

       

tion     However  their convergence rates are not optimal 
and it is still an open question to know if   rate close to
can be achieved with   decenO
tralized algorithm 
Asynchronous Setting  Accelerated stochastic gradient
descent such as SVRG  Johnson   Zhang    or SAGA
 Defazio et al    can be used on the dual problem in
Eq    instead of accelerated gradient descent  in order
to obtain an asynchronous algorithm with   linear convergence rate  The details and exact convergence rate of such
an approach are left as future work 

  Experiments
In this section  we compare our new algorithms  singlestep
dual accelerated  SSDA  descent and multistep dual accelerated  MSDA  descent  to standard distributed optimization algorithms in two settings 
leastsquares regression
and classi cation by logistic regression  Note that these experiments on simple generated datasets are made to assess
the differences between existing stateof theart algorithms
and the ones provided in Section   and do not address the
practical implementation details nor the ef ciency of the
compared algorithms on realworld distributed platforms 
The effect of latency  machine failures or variable communication time is thus left for future work 

  Competitors and Setup

We compare SSDA and MSDA to four stateof theart distributed algorithms that achieve linear convergence rates 
distributed ADMM  DADMM   Shi et al    EXTRA
 Shi et al      recent approach named DIGing  Nedich
et al    and the distributed version of accelerated gradient descent  DAGD  described in Section   and shown
to be optimal among centralized algorithms 
When available in the literature  we used the optimal parameters for each algorithm  see Theorem   by Shi et al 
  for DADMM and Remark   by Shi et al   
for EXTRA  For the DIGing algorithm  the parameters
provided by Nedich et al    are very conservative 
and lead to   very slow convergence  We thus manually
optimized the parameter for this algorithm  The experiments are simulated using   generated dataset consisting of
    samples randomly distributed to the nodes of   network of size   In order to assess the effect of the connectivity of the network  we ran each experiment on two
networks  one       grid and an Erd osR enyi random
network of average degree   The quality metric used in
this section is the maximum approximation error

 
where   is the optimal parameter of the optimization prob 

et   max
   

               

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

    high communication time       

    high communication time       

    low communication time       

    low communication time       

Figure   Maximum approximation error for leastsquares regression on an Erd osR enyi random network of average degree  
      

Figure   Maximum approximation error for leastsquares regression on         grid graph       

lem in Eq    The computation time attributes   unit time
per local computation and   time   per communication 

  Leastsquares Regression

 
 

min
 Rd

      cid cid 
 

The regularized leastsquares regression problem consists
in solving the optimization problem
 cid       cid cid 

 
where     Rd   is   matrix containing the   data points 
and     Rm is   vector containing the   associated values  The task is thus to minimize the empirical quadratic
error between   function yi     Xi  of   variables and its
linear regression    Xi      cid 
    on the original dataset  for
           while smoothing the resulting approximation
by adding   regularizer   cid cid 
  For our experiments  we
 xed             and sampled         Gaussian
random variables Xi         of mean   and variance  
The function to regress is then yi     cid 
  
where            is an        Gaussian noise of variance   These data points are then distributed randomly
and evenly to the       nodes of the network  Note

 cos   cid 

 

 

that the choice of function to regress   does not impact the
Hessian of the objective function  and thus the convergence
rate of the optimization algorithms 
Figure   and Figure   show the performance of the compared algorithms on two networks          grid graph
and an Erd osR enyi random graph of average degree   All
algorithms are linearly convergent  although their convergence rates scale on several orders of magnitude 
In all
experiments  the centralized optimal algorithm DAGD has
the best convergence rate  while MSDA has the best convergence rate among decentralized methods  In all our experiments  MSDA outperforms SSDA  indicating that performing several communication rounds per gradient iteration never degrades the ef ciency of the algorithm  while
signi cantly improving it when    cid   

  Logistic Classi cation

The logistic classi cation problem consists in solving the
optimization problem

      yi   cid 

ln

    cid cid 
 

 

 cid 

  cid 

  

min
 Rd

 
 

   cid 

 time    max  approximation error  et     ADMMEXTRADIGingDAGDSSDAMSDA time    max  approximation error  et    time    max  approximation error  et    time    max  approximation error  et   Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

    high communication time       

    high communication time       

    low communication time       

    low communication time       

Figure   Maximum approximation error for logistic classi cation
on an Erd osR enyi random network of average degree       
 

Figure   Maximum approximation error for logistic classi cation
on         grid graph       

where     Rd   is   matrix containing   data points 
and          is   vector containing the   class assignments  The task is thus to classify   dataset by learning
  linear classi er mapping data points Xi to their associated
class yi       For our experiments  we  xed      
      and sampled         data points      for
the  rst class and     for the second  Each data point
Xi      yi    is   Gaussian random variable of mean
yi  and variance   where yi                is
the true class of Xi  These data points are then distributed
randomly and evenly to the       nodes of the network 
Figure   and Figure   show the performance of the compared algorithms for logistic classi cation on two networks          grid graph and an Erd osR enyi random
graph of average degree   As for leastsquares regression 
all algorithms are linearly convergent  and their convergence rates scale on several orders of magnitude  In this
case  the centralized optimal algorithm DAGD is outperformed by MSDA  although the two convergence rates are
relatively similar  Again  when the communication time is
smaller than the computation time    cid    performing
several communication rounds per gradient iteration will

improve the ef ciency of the algorithm and MSDA substantially outperforms SSDA  Note that  in Figure     DADMM requires   iterations to reach the same error obtained after only   iterations of SSDA  demonstrating  
substantial improvement over stateof theart methods 

  Conclusion
In this paper  we derived optimal convergence rates for
strongly convex and smooth distributed optimization in two
settings  centralized and decentralized communications in
  network  For the decentralized setting  we introduced
the multistep dual accelerated  MSDA  algorithm with  
provable optimal linear convergence rate  and showed its
high ef ciency compared to other stateof theart methods 
including distributed ADMM and EXTRA  The simplicity of the approach makes the algorithm extremely  exible 
and allows for future extensions  including timevarying
networks and an analysis for nonstrongly convex functions  Finally  extending our complexity lower bounds to
time delays  variable computational speeds of local systems  or machine failures would be   notable addition to
this work 

 time    max  approximation error  et     ADMMEXTRADIGingDAGDSSDAMSDA time    max  approximation error  et    time    max  approximation error  et    time    max  approximation error  et   Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

References
Alon     and Milman          isoperimetric inequalities
for graphs  and superconcentrators  Journal of Combinatorial Theory  series       

Arioli     and Scott     Chebyshev acceleration of iterative re nement  Numerical Algorithms   
 

Arjevani  Yossi and Shamir  Ohad  Communication complexity of distributed convex learning and optimization 
In Advances in Neural Information Processing Systems
  pp     

Arjevani  Yossi and Shamir  Ohad  On the iteration complexity of oblivious  rstorder optimization algorithms 
In  nd International Conference on Machine Learning 
pp       

Arjevani  Yossi and Shamir  Ohad  Dimensionfree iteration complexity of  nite sum optimization problems  In
Advances in Neural Information Processing Systems  
pp       

Auzinger     Iterative Solution of Large Linear Systems 

Lecture notes  TU Wien   

Bertsekas  Dimitri    and Tsitsiklis  John    Parallel and
distributed computation   numerical methods  PrenticeHall International   

Boyd  Stephen  Ghosh  Arpita  Prabhakar  Balaji  and
Shah  Devavrat 
Randomized gossip algorithms 
IEEE ACM Transactions on Networking  TON   SI 
   

Boyd  Stephen  Diaconis  Persi  Parrilo  Pablo  and Xiao 
Lin  Fastest mixing markov chain on graphs with symmetries  SIAM Journal on Optimization   
 

Boyd  Stephen  Parikh  Neal  Chu  Eric  Peleato  Borja  and
Eckstein  Jonathan  Distributed optimization and statistical learning via the alternating direction method of multipliers  Foundations and Trends in Machine Learning 
   

Bubeck    ebastien  Convex optimization  Algorithms and
complexity  Foundations and Trends in Machine Learning     

Defazio  Aaron  Bach  Francis  and LacosteJulien  Simon 
SAGA    fast incremental gradient method with support
In Adfor nonstrongly convex composite objectives 
vances in Neural Information Processing Systems   pp 
   

Duchi  John    Agarwal  Alekh  and Wainwright  Martin    Dual averaging for distributed optimization  Convergence analysis and network scaling  IEEE Transactions on Automatic control     

Jakoveti    Du san  Xavier  Joao  and Moura  Jos   MF  Fast
distributed gradient methods  IEEE Transactions on Automatic Control     

Jakoveti    Du san  Moura  Jos   MF  and Xavier  Joao  Linear convergence rate of   class of distributed augmented
lagrangian algorithms  IEEE Transactions on Automatic
Control     

Johnson  Rie and Zhang  Tong  Accelerating stochastic
gradient descent using predictive variance reduction  In
Advances in Neural Information Processing Systems  
pp     

Mokhtari     Shi     Ling     and Ribeiro       decentralized secondorder method with exact linear convergence rate for consensus optimization  IEEE Transactions on Signal and Information Processing over Networks     

Mokhtari  Aryan and Ribeiro  Alejandro  DSA  Decentralized double stochastic averaging gradient algorithm 
Journal of Machine Learning Research   
   

Nedic  Angelia and Ozdaglar  Asuman  Distributed subgradient methods for multiagent optimization  IEEE Transactions on Automatic Control     

Nedich     Olshevsky     and Shi     Achieving geometric convergence for distributed optimization over timevarying graphs  ArXiv eprints   

Nesterov  Yurii  Introductory lectures on convex optimization     basic course  Kluwer Academic Publishers 
 

Shamir  Ohad  Fundamental limits of online and distributed
algorithms for statistical learning and estimation  In Advances in Neural Information Processing Systems   pp 
   

Shamir  Ohad and Srebro  Nathan  Distributed stochastic optimization and learning  In  nd Annual Allerton
Conference on Communication  Control  and Computing
 Allerton  pp    IEEE   

Shi  Wei  Ling  Qing  Yuan  Kun  Wu  Gang  and Yin 
Wotao  On the linear convergence of the ADMM in decentralized consensus optimization  IEEE Transactions
on Signal Processing     

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

Shi  Wei  Ling  Qing  Wu  Gang  and Yin  Wotao  EXTRA 
An exact  rstorder algorithm for decentralized consensus optimization  SIAM Journal on Optimization   
   

Tutunov     Ammar        and Jadbabaie       distributed
newton method for large scale consensus optimization 
ArXiv eprints   

Wei  Ermin and Ozdaglar  Asuman  Distributed alternating
direction method of multipliers  In  st Annual Conference on Decision and Control  CDC  pp   
IEEE   

Xiao  Lin and Boyd  Stephen  Fast linear iterations for distributed averaging  Systems   Control Letters   
   

