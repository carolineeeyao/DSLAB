Subsampled Cubic Regularization for Nonconvex Optimization

Jonas Moritz Kohler   Aurelien Lucchi  

Abstract

We consider the minimization of nonconvex
functions that typically arise in machine learning  Speci cally  we focus our attention on  
variant of trust region methods known as cubic
regularization  This approach is particularly attractive because it escapes strict saddle points
and it provides stronger convergence guarantees
than  rstand secondorder as well as classical
trust region methods  However  it suffers from
  high computational complexity that makes it
impractical for largescale learning  Here  we
propose   novel method that uses subsampling
to lower this computational cost  By the use of
concentration inequalities we provide   sampling
scheme that gives suf ciently accurate gradient
and Hessian approximations to retain the strong
global and local convergence guarantees of cubically regularized methods  To the best of our
knowledge this is the  rst work that gives global
convergence guarantees for   subsampled variant of cubic regularization on nonconvex functions  Furthermore  we provide experimental results supporting our theory 

 cid 

 cid 

  cid 

  

 
 

  Introduction
In this paper we address the problem of minimizing an objective function of the form

     arg min
  Rd

       

fi   

 

 

where            Rd     is   not necessarily convex   regularized  loss function over   datapoints  Stochastic Gradient Descent  SGD  is   popular method to optimize this
type of objective especially in the context of largescale

 Department

of Computer Science 

ETH Zurich 
Switzerland 
Jonas Moritz Kohler
 jonas kohler student kit edu  Aurelien Lucchi  aurelien lucchi inf ethz ch 

Correspondence to 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

learning when   is very large 
Its convergence properties are well understood for convex functions  which arise
in many machine learning applications  Nesterov   
However  nonconvex functions are also ubiquitous and
have recently drawn   lot of interest due to the growing
success of deep neural networks  Yet  nonconvex functions are extremely hard to optimize due to the presence of
saddle points and local minima which are not global optima  Dauphin et al    Choromanska et al    In
fact  the work of  Hillar   Lim    showed that even
  degree four polynomial can be NPhard to optimize  Instead of aiming for   global minimizer  we will thus seek
for   local optimum of the objective  In this regard    lot of
attention has focused on   speci   type of functions known
as strict saddle functions or ridable functions  Ge et al 
  Sun et al    These functions are characterized
by the fact that the Hessian of every saddle point has   negative eigenvalue  Geometrically this means that there is  
direction of negative curvature where decreasing function
values can be found  Examples of strict saddle functions
include dictionary learning  orthogonal tensor decomposition and generalized phase retrieval  Ge et al    Sun
et al   
In this work  we focus our attention on trust region methods to optimize Eq    These methods construct and optimize   local model of the objective function within   region
whose radius depends on how well the model approximates
the real objective  One of the keys for ef ciency of these
methods is to pick   model that is comparably easy to optimize  such as   quadratic function  Conn et al   
Following the trust region paradigm  cubic regularization
methods  Nesterov   Polyak    Cartis et al     
suggest  nding the step sk that minimizes   cubic model of
the form

 cid 
     xk   
 
mk sk       xk     
 
where Hk       xk  and         
 Nesterov   Polyak    were able to show that  if the

 cid sk cid   
 

 cid 
 
kHksk  

  
 

 In the work of  Nesterov   Polyak       is assumed to
be the Lipschitz constant of the Hessian in which case the model
de ned in Eq    is   global overestimator of the objective      
                  Rd  We will elaborate on the role of   
in  Cartis et al      later on 

Subsampled Cubic Regularization for Nonconvex Optimization

step is computed by globally minimizing the cubic model
and if the Hessian Hk is globally Lipschitz continuous  Cubic regularization methods possess the best known worst
case complexity to solve Eq    an overall worstcase iteration count of order   for generating  cid    xk cid     
and of order   for achieving approximate nonnegative
curvature  However  minimizing Eq    in an exact manner impedes the performance of this method for machine
learning applications as it requires access to the full Hessian matrix  More recently   Cartis et al      presented
  method  hereafter referred to as ARC  which relaxed this
requirement by assuming that one can construct an approximate Hessian Bk that is suf ciently close to Hk in the
following way 

 cid Bk   Hk sk cid       cid sk cid                

 

Furthermore  they showed that it is suf cient to  nd an
approximate minimizer by applying   Lanczos method to
build up evolving Krylov spaces  which can be constructed
in   Hessianfree manner       by accessing the Hessians
only indirectly via matrixvector products  However there
are still two obstacles for the application of ARC in the
 eld of machine learning    The cost of the Lanczos process increases linearly in   and is thus not suitable for large
datasets and   there is no theoretical guarantee that quasiNewton approaches satisfy Eq    and  Cartis et al     
do not provide any alternative approximation technique 
In this work  we make explicit use of the  nitesum structure of Eq    by applying   subsampling technique in order
to provide guarantees for machine learning applications 
Towards this goal  we make the following contributions 

  We provide   theoretical Hessian sampling scheme
that is guaranteed to satisfy Eq    with high probability 

  We extend the analysis to inexact gradients and
prove that the convergence guarantees of  Nesterov  
Polyak    Cartis et al      can be retained 

  Since the dominant iteration cost lie in the construction of the Lanczos process and increase linearly in   
we lower the computational cost signi cantly by reducing the number of samples used in each iteration 
  Finally  we provide experimental results demonstrating signi cant speedups compared to standard  rst
and secondorder optimization methods for various
convex and nonconvex objectives 

  Related work
Sampling techniques for  rstorder methods 
In largescale learning  when    cid    most of the computational cost
of traditional deterministic optimization methods is spent

in computing the exact gradient information    common
technique to address this issue is to use subsampling in order to compute an unbiased estimate of the gradient  The
simplest instance is Stochastic Gradient Descent  SGD 
whose convergence does not depend on the number of datapoints    However  the variance in the stochastic gradient
estimates slows its convergence down  The work of  Friedlander   Schmidt    explored   subsampling technique for gradient descent in the case of convex functions 
showing that it is possible to maintain the same convergence rate as fullgradient descent by carefully increasing
the sample size over time  Another way to recover   linear
rate of convergence for stronglyconvex functions is to use
variancereduced methods  Johnson   Zhang    Defazio et al    Roux et al    Hofmann et al   
Daneshmand et al    Recently  the convergence of
SGD and its variancereduced counterparts has also been
extended to nonconvex functions  Ghadimi   Lan   
Reddi et al      but the techniques used in these papers
require using   randomized sampling scheme which is different from what is typically used in practice  Furthermore 
the guarantees these methods provide are only in terms of
convergence to critical points  However  the work of  Ge
et al    Sun et al    recently showed that SGD
can achieve stronger guarantees in the case of strict saddle
functions  Yet  the convergence rate has   polynomial dependency to the dimension   and the smallest eigenvalue of
the Hessian which can make this method fairly impractical 

Secondorder methods  For secondorder methods  the
problem of avoiding saddle points is even worse as they
might be attracted by saddle points or even points of local maximizers  Dauphin et al    Another predominant issue is the computation  and perhaps storage  of the
Hessian matrix  which requires   nd  operations as well
as computing the inverse of the Hessian  which requires
     computations  QuasiNewton methods such as the
wellknown    BFGS algorithm partially address this issue by requiring   nd      periteration cost  Nesterov 
  instead of   nd       An increasingly popular
alternative is to use subsampling techniques to approximate the Hessian matrix  such as done for example in  Byrd
et al    and  Erdogdu   Montanari    The latter method  named NewSamp  approximates the Hessian
with   lowrank approximation which reduces the complexity per iteration to   nd         with     being the sample size   Although this is   signi cant reduction in terms
of complexity  NewSamp yields   composite convergence
rate  quadratic at  rst but only linear near the minimizer 
Unlike NewSamp  our sampling scheme yields   locally
quadratic rate of convergence  as well as faster global con 

 Note that this method still requires   nd  computation for

the gradient as it only subsamples the Hessian 

Subsampled Cubic Regularization for Nonconvex Optimization

vergence  Our analysis also does not require using exact
gradients and can thus further reduce the complexity per
iteration 

shall  rst state the algorithm itself and elaborate further on
the type of local nonlinear models we employ as well as
how these can be solved ef ciently 

Cubic regularization and trust region methods  Trust
region methods are among the most effective algorithmic
frameworks to avoid pitfalls such as local saddle points
in nonconvex optimization  Classical versions iteratively
construct   local quadratic model and minimize it within  
certain radius wherein the model is trusted to be suf ciently
similar to the actual objective function  This is equivalent
to minimizing the model function with   suitable quadratic
penalty term on the stepsize  Thus    natural extension is
the cubic regularization method introduced by  Nesterov  
Polyak    that uses   cubic overestimator of the objective function as   regularization technique for the computation of   step to minimize the objective function  The
drawback of their method is that it requires computing the
exact minimizer of Eq    thus requiring the exact gradient
and Hessian matrix  However  nding   global minimizer
of the cubic model mk    may not be essential in practice
and doing so might be prohibitively expensive from   computational point of view   Cartis et al      introduced  
method named ARC which relaxed this requirement by letting sk be an approximation to the minimizer  The model
de ned by the adaptive cubic regularization method introduced two further changes  First  instead of computing the
exact Hessian Hk it allows for   symmetric approximation
Bk  Second  it introduces   dynamic positive parameter   
instead of using the global Lipschitz constant   
There have been efforts to further reduce the computational
complexity of this problem  For example   Agarwal et al 
  re ned the approach of  Nesterov   Polyak    to
return an approximate local minimum in time which is linear in the input representation  Similar improvements have
been made by  Carmon   Duchi    and  Hazan   Koren    These methods provide alternatives to minimize
the cubic model and can thus be seen as complementary to
our approach  Finally  the work of  Blanchet et al   
proposed   stochastic variant of   trust region method but
their analysis focused on randomized gradients only 

  Formulation
We are interested in optimizing Eq    in   largescale setting when the number of datapoints   is very large such that
the cost of solving Eq    exactly becomes prohibitive  In
this regard we identify   sampling scheme that allows us to
retain the convergence results of deterministic trust region
and cubic regularization methods  including quadratic local convergence rates and global secondorder convergence
guarantees as well as worstcase complexity bounds    detailed theoretical analysis is given in Section   Here we

  Objective function

Instead of using deterministic gradient and Hessian information as in Eq    we use unbiased estimates of the gradient and Hessian constructed from two independent sets
of points denoted by Sg and SB  We then construct   local cubic model that is  approximately  minimized in each
iteration 

 cid 
 cid 

 cid sk cid 

  
 

 

 cid 
 cid 
 
 
kBksk  
kgk  
mk sk       xk     
 
 fi xk 
 fi xk 

where gk    Sg 
and Bk    SB 
The model derivative with respect to sk is de ned as 
 mk sk    gk   Bksk    sk  where         cid sk cid     

  SB

  Sg

  Algorithm

Our Subsampled Cubic Regularization approach  SCR  is
presented in Algorithm   At iteration step    we subsample two sets of datapoints from which we compute  
stochastic estimate of the gradient and the Hessian  We
then solve the problem in Eq    approximately using the
method described in Section   and update the regularization parameter    depending on how well the model approximates the real objective  In particular  very successful
steps indicate that the model is  at least locally  an adequate approximation of the objective such that the penalty
parameter is decreased in order to allow for longer steps 
For unsuccessful iterations we proceed exactly the opposite
way  Readers familiar with trust region methods might see
that one can interpret the penalty parameter    as inversely
proportional to the trust region radius    

  Exact model minimization

Solving Eq    requires minimizing an unconstrained nonconvex problem that may have isolated local minima  As
shown in  Cartis et al      the global model minimizer
  
  is characterized by following systems of equations 
 Bk    

  cid     Bk    

     gk   

        cid   

kI   

kI   cid   
 

In order to  nd   solution we can express   
 Bk    
and obtain   univariate  nonlinear equation in   

    
kI gk  apply this in the second equation of  

    sk 

   

 

 cid cid Bk    

kI gk

 cid cid     

 
  

Subsampled Cubic Regularization for Nonconvex Optimization

Algorithm   Subsampled Cubic Regularization  SCR 
  Input 

Starting point      Rd            
                    and      
  for                 until convergence do
 
 
 

Sample gradient gk and Hessian Hk according to Eq      Eq    respectively
Obtain sk by solving mk sk   Eq    such that    holds
Compute    xk   sk  and

 

Set

 

Set

     

    

   xk       xk   sk 
   xk    mk sk 

xk   sk
xk

if       
otherwise

xk   

 cid 
 max min     cid gk cid     

  
  

if         very successful iteration 
if             successful iteration 
otherwise  unsuccessful iteration 

 

 

 

where        is the relative machine precision 

  end for

kI 

    max Bk    where
Furthermore  we need  
 Bk  is the leftmost eigenvalue of Bk  in order to guarantee the semipositive de niteness of  Bk    
Thus  computing the global solution of mk boils down to
 nding the root of Eq    in the above speci ed range of    
The problem can be solved by Newton   method  which involves factorizing Bk    kI for various    and is thus prohibitively expensive for large problem dimensions    See
Section   in  Cartis et al      for more details  In the
following Section we instead explore an approach to approximately minimize the model while retaining the convergence guarantees of the exact minimization 

  Approximate model minimization

 Cartis et al      showed that it is possible to retain the
remarkable properties of the cubic regularization algorithm
with an inexact model minimizer    necessary condition is
that sk satis es the two requirements stated in   
Assumption    Approximate model minimizer 

 cid 
 cid 
kBksk       cid sk cid     
kgk    
 
 cid 
kBksk       cid sk cid     
 

 
 

smk sk sk    

Note that the  rst equation is equal to  smk sk 
 cid 
 cid 
  
and the second to  
As shown in  Cartis et al      Lemma  
the
global minimizer of mk sk  in   Krylov subspace Kk  
span gk  Hkgk    
kgk    satis es this assumption independent of the subspace dimension  This comes in handy 

sk    

as minimizing mk in the Krylov subspace only involves
factorizing   tridiagonal matrix  which can be done at the
cost of      However    Lanczostype method must be
used in order to build up an orthogonal basis of this subspace which typically involves one matrixvector product
           per additional subspace dimension  see
Chapter   in  Conn et al    for more details 
Thus  in order to keep the per iteration cost of SCR low
and in accordance to ARC  we apply the following termination criterion to the Lanczos process in the hope to  nd  
suitable trial step before Kk is of dimensionality   
Assumption    Termination Criteria  For each outer iteration    assume that the Lanczos process stops as soon as
some Lanczos iteration   satis es the criterion
TC   cid mk si   cid        cid gk cid   
where        min   cid si   cid         

 

However  we argue that especially for high dimensional
problems  the cost of the Lanczos process may signi cantly
slow down cubically regularized methods and since this
cost increases linearly in    carefully subsampled versions
are an attractive alternative 

  Theoretical analysis
In this section  we provide the convergence analysis of
SCR  For the sake of brevity  we assume Lipschitz continuous Hessians right away but note that   superlinear local
convergence result as well as the global  rstorder conver 

Subsampled Cubic Regularization for Nonconvex Optimization

gence theorem can both be obtained without the former assumption 
First  we lay out some critical assumptions regarding the
gradient and Hessian approximations  Second  we show
that one can theoretically satisfy these assumptions with
high probability by subsampling  rstand secondorder
information  Third  we give   condensed convergence analysis of SCR which is widely based on  Cartis et al     
but adapted for the case of stochastic gradients  There  we
show that the local and global convergence properties of
ARC can be retained by subsampled versions at the price
of slightly worse constants 

  Assumptions
Assumption    Continuity  The functions fi      Rd 
gi and Hi are Lipschitz continuous for all    with Lipschitz
constants         and    respectively 

By use of the triangle inequality  it follows that these assumptions hold for all   and    independent of the sample size  Furthermore  note that the Hessian and gradient
norms are uniformly bounded as   consequence of   
In each iteration  the Hessian approximation Bk shall satisfy condition AM  from  Cartis et al      which we
restate here for the sake of completeness 
Assumption    Suf cient Agreement of   and   
 cid Bk     xk sk cid       cid sk cid                

 

We explicitly stress the fact that this condition is stronger
than the wellknown Dennis Mor   Condition  While quasiNewton approximations satisfy the latter  there is no theoretical guarantee that they also satisfy the former  Cartis
et al      Furthermore  any subsampled gradient shall
satisfy the following condition 
Assumption    Suf cient Agreement of    and   
 cid    xk      xk cid       cid sk cid                  

  Sampling Conditions

Based on probabilistic deviation bounds for random vectors and matrices  we now present sampling conditions
that guarantee suf cient steepness and curvature information in each iteration    In particular  the Bernstein inequality gives exponentially decaying bounds on the probability
of   random variable to differ by more than   from its mean
for any  xed number of samples  We use this inequality

 These bounds have lately become popular under the name of
concentration inequalities  Unlike classic limit theorems  such as
the Central Limit Theorem  concentration inequalities are specifically attractive for application in machine learning because of
their nonasymptotic nature 

to upper bound the  cid norm distance  cid       cid  as well as
the spectralnorm distance  cid       cid  by quantities involving the sample size     By applying the resulting bounds
in the suf cient agreement assumptions          and rearranging for     we are able to translate the latter into
concrete sampling conditions 

  GRADIENT SAMPLING

As detailed in the Appendix  the following Lemma arises
from the Vector Bernstein Inequality 
Lemma    Gradient deviation bound  Let the subsampled
gradient gk be de ned as in Eq    For        we have
with probability       that
 
 cid   xk        xk cid     

log       

 cid 

  

 Sg   

 

 

It constitutes   nonasymptotic bound on the deviation of
the gradient norms that holds with high probability  Note
how the accuracy of the gradients increases in the sample
size  This bound yields the following condition 

Theorem    Gradient Sampling  If
 Sg       

   log        

            
 
then gk satis es the suf cient agreement condition   
with probability      

     cid sk cid 

  HESSIAN SAMPLING

In analogy to the gradient case  we use the matrix version
of Bernstein   Inequality to derive the following Lemma 
Lemma    Hessian deviation bound  Let the subsampled
Hessian   be de ned as in Eq    For        we have
with probability       that

 cid 

 cid   xk      xk cid      

log   
 SB   

 

 

This  in turn  can be used to derive   Hessian sampling condition that is guaranteed to satisfy the suf cient agreement
condition     with high probability 

Theorem    Hessian Sampling  If
 SB       

  log   
    cid sk cid 

        and         

Subsampled Cubic Regularization for Nonconvex Optimization

then Bk satis es the strong agreement condition   
with probability      

As expected  the required sample sizes grow in the problem
dimensionality   and in the Lipschitz constants    and    
Finally  as outlined in the Appendix  Lemma   the stepsize tends to zero as SCR approaches   secondorder critical point  Consequently  the sample sizes must approach  
as the algorithm converges and thus we have

       as well as       as      

 

  Convergence Analysis

The entire analysis of cubically regularized methods is prohibitively lengthy and we shall thus establish only the crucial properties that ensure global  as well as fast local convergence and improve the worstcase complexity of these
methods over standard trust region approaches  Next to
the cubic regularization term itself  these properties arise
mainly from the penalty parameter updates and step acceptance criteria of the ARC framework  which give rise to  
good relation between regularization and stepsize  Further
details can be found in  Cartis et al     

  PRELIMINARY RESULTS
First  we note that the penalty parameter sequence    
is guaranteed to stay within some bounded positive range 
which is essentially due to the fact that SCR is guaranteed
to  nd   successful step as soon as the penalty parameter
exceeds some critical value  sup 
Lemma    Boundedness of     Let       and    hold 
Then

      inf    sup        

where  inf is de ned in Step   of Algorithm   and

 sup    

 
 

            

 

 

Furthermore  for any successful iteration the objective decrease can be directly linked to the model decrease via the
step acceptance criterion in Eq    The latter  in turn  can
be shown to be lower bounded by the stepsize which combined gives the following result 
Lemma    Suf cient function decrease  Suppose that sk
satis es    Then  for all successful iterations      

   xk       xk        xk      sk 

   
 

 inf  cid sk cid 

 

Finally  the termination criterion   also guarantees step
sizes that do not become too small compared to the respective gradient norm which leads to the following Lemma 
Lemma    Suf ciently long steps  Let       and   
hold  Furthermore  assume the termination criterion TC
    and suppose that xk      as       Then  for all
suf ciently large successful iterations  sk satis es

 cid cid    xk cid 

 cid sk cid      

where    is the positive constant

 cid 

    

     

 
                       sup     

 

   

  LOCAL CONVERGENCE

We here provide   proof of local convergence for any sampling scheme that satis es the conditions presented in Theorem   and Theorem   as well as the additional condition
that the sample size does not decrease in unsuccessful iterations  We show that such sampling schemes eventually
yield exact gradient and Hessian information  Based upon
this observation  we obtain the following local convergence
result  as derived in the Appendix 

Theorem    Quadratic local convergence  Let   
hold and assume that gk and Bk are sampled such
that   and   hold and  Sg    and  SB    are not decreased in unsuccessful iterations  Furthermore  let sk
satisfy    and

xk      as      

 
where      is positive de nite  Moreover  assume
the stopping criterion TC     Then 
 cid xk      cid 
 cid xk     cid             as                
That is  xk converges in qquadratically to    as    
  with high probability 

  GLOBAL CONVERGENCE TO FIRSTORDER

CRITICAL POINT

Lemma   and   allow us to lower bound the function decrease of   successful step in terms of the full gradient  fk
 as we will shorty detail in Eq    Combined with Lemma
  this allows us to give deterministic global convergence
guarantees using only stochastic  rst order information 

Subsampled Cubic Regularization for Nonconvex Optimization

Theorem    Convergence to  storder Critical
Points  Let          and    hold  Furthermore 
let     xk  be bounded below by some finf    
Then

    cid    xk cid     

lim

 

  GLOBAL CONVERGENCE TO SECONDORDER

CRITICAL POINT

Unsurprisingly  the secondorder convergence guarantee
relies mainly on the use of secondorder information so
that the stochastic gradients do neither alter the result nor
the proof as it can be found in Section   of  Cartis et al 
    We shall restate it here for the sake of completeness 

Theorem    Secondorder global convergence  Let
      and    hold  Furthermore  let     xk  be
bounded below by finf and sk be   global minimizer of
mk over   subspace Lk that is spanned by the columns
of the      orthogonal matrix Qk  As       asymptotically  Eq    any subsequence of negative left 
 cid 
most eigenvalues  min  
kH xk Qk  converges to
zero for suf ciently large  successful iterations  Hence

lim
    inf

    min  

 cid 
kH xk Qk     

 

Finally  if Qk becomes   full orthogonal basis of Rd
as       then any limit point of the sequence of
successful iterates  xk  is secondorder critical  provided such   limit point exists 

  WORSTCASE ITERATION COMPLEXITY

For the worstcase analysis we shall establish the two disjoint index sets Uj and Sj  which represent the unand successful SCR iterations that have occurred up to some iteration       respectively  As stated in Lemma   the penalty
parameter    is bounded above and hence SCR may only
take   limited number of consecutive unsuccessful steps 
As   consequence  the total number of unsuccessful iterations is at most   problem dependent constant times the
number of successful iterations 
Lemma    Number of unsuccessful iterations  For any
 xed       let Lemma   hold  Then we have that

 cid 

 cid 

 Uj   

 Sj     

log sup    log inf  

log 

 

 

Regarding the number of successful iterations we have already established the two key ingredients        suf cient

function decrease in each successful iteration  Lemma  
and  ii    step size that does not become too small compared to the respective gradient norm  Lemma   which
is essential to driving the latter below   at   fast rate  Combined they give rise to the guaranteed function decrease for
successful iterations
   xk       xk     
 

   cid    xk cid   

 inf  

 

which already contains the power of   that appears in the
complexity bound  Finally  by summing over all successful iterations one obtains the following  so far best know 
worstcase iteration bound to reach    rstorder criticality 

Theorem    Firstorder worstcase complexity  Let
         and    hold  Furthermore  be     xk 
bounded below by finf and TC applied     Then 
for       the total number of iterations SCR takes to
generate the  rst iterate   with  cid    xj cid      and
assuming       is

   cid 

             cid 

 

 

where

      

        finf
 inf  
 

and     

log sup    log inf  

log 

 

Note that the constants    and    involved in this upper
bound both increase in the gradient inaccuracy   and the
Hessian inaccuracy    via    and  sup  such that more
inaccuracy in the subsampled quantities may well lead to
an increased overall number of iterations 
Finally  we want to point out that similar results can be
established regarding   secondorder worstcase complexity bound similar to Corollary   in  Cartis et al     
which we do not prove here for the sake of brevity 

  Experimental results
In this section we present experimental results on realworld datasets where    cid     cid    They largely con rm
the analysis derived in the previous section  Please refer to
the Appendix for more detailed results and experiments on
higher dimensional problems 

  Practical implementation of SCR

We implement SCR as stated in Algorithm   and note
the following details  Following  Erdogdu   Montanari 
  we require the sampling conditions derived in Section   to hold with probability          which yields

Subsampled Cubic Regularization for Nonconvex Optimization

            

  COVTYPE       

  HIGGS       

Figure   Top  bottom  row shows log suboptimality of convex  nonconvex  regularized logistic regressions over time  avg  of   runs 

 cid  

 cid 

 cid 

the following practically applicable sampling schemes

 Sk       
 Sk       

  log   

    cid sk cid                

   log       
   cid sk cid 

              

 

The positive constants   and   can be used to scale the
sample size to   reasonable portion of the entire dataset
and can furthermore be used to offset    and      which
are generally expensive to obtain 
However  when choosing     for the current iteration    the
stepsize sk is yet to be determined  Based on the Lipschitz continuity of the involved functions  we argue that
the previous stepsize is   fair estimator of the current one
and this is con rmed by experimental results  Finally  we
would like to point out that the sampling schemes derived
in Eq    gives our method   clear edge over sampling
schemes that do not take any iteration information into account       linearly or geometrically increased samples 

  Baselines and datasets

We compare SCR to various optimization methods presented in Section   This includes SGD  with constant
stepsize  SAGA  Newton   method  BFGS  LBFGS and
ARC  More details concerning the choice of the hyperparameters are provided in the appendix  We ran experiments on the datasets      covtype and higgs  see details in the appendix  We experimented with   binary logistic regression model with two different regularizers   
standard  cid  penalty  cid   cid  and   nonconvex regularizer

     

   

      
   

 see  Reddi et al     

  Results

The results in Figure   con rm our intuition that SCR can
reduce ARCs computation time without losing its global
convergence property  Newton   method is the closest in
terms of performance  However  it suffer heavily from
an increase in   as can be seen by additional results provided in the appendix  Furthermore  it cannot optimize the
nonconvex version of covtype due to   singular Hessian 
Notably  BFGS terminates early on the nonconvex higgs
dataset due to   local saddle point  Finally  the high condition number of covtype has   signi cant effect on the performance of SGD  SAGA and LBFGS 

  Conclusion
In this paper we proposed   subsampling technique to estimate the gradient and Hessian in order to construct   cubic model analogue to trust region methods  We show that
this method exhibits the same convergence properties as its
deterministic counterpart  which are the best known worstcase convergence properties on nonconvex functions  Our
proposed method is especially interesting in the large scale
regime when    cid     Numerical experiments on both real
and synthetic datasets demonstrate the performance of the
proposed algorithm which we compared to its deterministic variant as well as more classical optimization methods 
As future work we would like to explore the adequacy of
our method to train neural networks which are known to be
hard to optimize due to the presence of saddle points 

Subsampled Cubic Regularization for Nonconvex Optimization

References
Agarwal  Naman  AllenZhu  Zeyuan  Bullins  Brian 
Hazan  Elad  and Ma  Tengyu  Finding local minima for
nonconvex optimization in linear time  arXiv preprint
arXiv   

Blanchet  Jose  Cartis  Coralia  Menickelly  Matt  and
Scheinberg  Katya  Convergence rate analysis of  
stochastic trust region method for nonconvex optimization  arXiv preprint arXiv   

Byrd  Richard    Chin  Gillian    Neveitt  Will  and Nocedal  Jorge  On the use of stochastic hessian information in optimization methods for machine learning 
SIAM Journal on Optimization     

Carmon  Yair and Duchi  John    Gradient descent ef 
ciently  nds the cubicregularized nonconvex newton
step  https arxiv org abs   

Cartis  Coralia  Gould  Nicholas IM  and Toint  Philippe   
Adaptive cubic regularisation methods for unconstrained
optimization  part    motivation  convergence and numerical results  Mathematical Programming   
     

Cartis  Coralia  Gould  Nicholas IM  and Toint  Philippe   
Adaptive cubic regularisation methods for unconstrained
optimization  part ii  worstcase functionand derivativeevaluation complexity  Mathematical programming   
     

Chang  ChihChung and Lin  ChihJen  Libsvm    library
for support vector machines  ACM Transactions on Intelligent Systems and Technology  TIST     

Choromanska  Anna  Henaff  Mikael  Mathieu  Michael 
Arous    erard Ben  and LeCun  Yann  The loss surfaces
of multilayer networks  In AISTATS   

Conn  Andrew    Gould  Nicholas IM  and Toint 

Philippe    Trust region methods  SIAM   

Daneshmand  Hadi  Lucchi  Aur elien  and Hofmann 
Thomas  Starting small   learning with adaptive sample
sizes  In International Conference on Machine Learning 
 

Dauphin  Yann    Pascanu  Razvan  Gulcehre  Caglar 
Cho  Kyunghyun  Ganguli  Surya  and Bengio  Yoshua 
Identifying and attacking the saddle point problem in
highdimensional nonconvex optimization  In Advances
in neural information processing systems  pp   
   

In Adfor nonstrongly convex composite objectives 
vances in Neural Information Processing Systems  pp 
   

Erdogdu  Murat   and Montanari  Andrea  Convergence
rates of subsampled newton methods  In Advances in
Neural Information Processing Systems  pp   
 

Friedlander  Michael   and Schmidt  Mark 

Hybrid
deterministicstochastic methods for data  tting  SIAM
Journal on Scienti   Computing       
 

Ge  Rong  Huang  Furong  Jin  Chi  and Yuan  Yang  Escaping from saddle pointsonline stochastic gradient for
tensor decomposition  In COLT  pp     

Ghadimi  Saeed and Lan  Guanghui  Stochastic  rstand
zerothorder methods for nonconvex stochastic programming  SIAM Journal on Optimization   
 

Gould  Nicholas IM  Porcelli     and Toint  Philippe   
Updating the regularization parameter in the adaptive cubic regularization algorithm  Computational optimization and applications     

Gross  David  Recovering lowrank matrices from few coef cients in any basis  IEEE Transactions on Information Theory     

Hazan  Elad and Koren  Tomer    lineartime algorithm for
trust region problems  Mathematical Programming   
   

Hillar  Christopher   and Lim  LekHeng  Most tensor
problems are nphard  Journal of the ACM  JACM   
   

Hofmann  Thomas  Lucchi  Aurelien  LacosteJulien  Simon  and McWilliams  Brian  Variance reduced stochastic gradient descent with neighbors  In Advances in Neural Information Processing Systems   pp   
Curran Associates  Inc   

Johnson  Rie and Zhang  Tong  Accelerating stochastic
gradient descent using predictive variance reduction  In
Advances in Neural Information Processing Systems  pp 
   

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple

layers of features from tiny images   

Defazio  Aaron  Bach  Francis  and LacosteJulien  Simon 
Saga    fast incremental gradient method with support

Nesterov  Yurii  Introductory lectures on convex optimiza 

tion  applied optimization  vol     

Subsampled Cubic Regularization for Nonconvex Optimization

Nesterov  Yurii and Polyak  Boris    Cubic regularization
of newton method and its global performance  Mathematical Programming     

Reddi  Sashank    Hefny  Ahmed  Sra  Suvrit  Poczos 
Barnabas  and Smola  Alex  Stochastic variance rearXiv preprint
duction for nonconvex optimization 
arXiv     

Reddi  Sashank    Sra  Suvrit    oczos  Barnab as  and
Smola  Alex  Fast incremental method for nonconvex
optimization  arXiv preprint arXiv     

Roux  Nicolas    Schmidt  Mark  and Bach  Francis     
stochastic gradient method with an exponential convergence rate for  nite training sets  In Advances in Neural
Information Processing Systems  pp     

Sun  Ju  Qu  Qing  and Wright  John  When are nonconvex
problems not scary  arXiv preprint arXiv 
 

