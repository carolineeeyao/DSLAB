Spectral Learning from   Single Trajectory under FiniteState Policies

Borja Balle   OdalricAmbrym Maillard  

Abstract

We present spectral methods of moments for
learning sequential models from   single trajectory  in stark contrast with the classical literature that assumes the availability of multiple       
trajectories  Our approach leverages an ef cient
SVDbased learning algorithm for weighted automata and provides the  rst rigorous analysis
for learning many important models using dependent data  We state and analyze the algorithm under three increasingly dif cult scenarios  probabilistic automata  stochastic weighted automata 
and reactive predictive state representations controlled by    nitestate policy  Our proofs include novel tools for studying mixing properties
of stochastic weighted automata 

  Introduction
Spectral methods of moments are   powerful tool for designing provably correct learning algorithms for latent variable models  Successful applications of this approach include polynomialtime algorithms for learning topic models  Anandkumar et al      hidden Markov models  Hsu et al    Siddiqi et al    Anandkumar
et al    mixtures of Gaussians  Anandkumar et al 
  Hsu   Kakade    predictive state representations  Boots et al    Hamilton et al    Bacon et al 
  Langer et al    weighted automata  Bailly 
  Balle et al    Balle   Mohri    Balle et al 
      Glaude   Pietquin    and weighted contextfree grammars  Bailly et al    Cohen et al   
  All these methods can be split into two classes depending on which spectral decomposition they rely on  The
 rst class includes algorithms based on an Singular Value
Decomposition  SVD  decomposition of   matrix contain 

 Amazon Research  Cambridge  UK  work done at
  Nord Europe  VilLancaster University 
leneuve   Ascq  France 
Correspondence to  Borja Balle
 pigem amazon co uk  OdalricAmbrym Maillard  odalric maillard inria fr 

 Inria Lille

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ing  estimated  moments of the target distribution       Hsu
et al    Boots et al    Balle et al      The
other class includes algorithms relying on symmetric tensor decomposition methods       Anandkumar et al   
Hsu   Kakade   The advantage of tensor methods is
that their output is always   proper probabilistic model  On
the other hand  SVD methods  which do not always output
  probabilistic model  provide learning algorithms for models which are provably nonlearnable using tensor methods    notable example is the class of stochastic weighted
automata that do not admit   probabilistic parametrization
 Jaeger    Denis   Esposito   
language processing  NLP  and reinforcement
Natural
learning  RL  are the two main application domains of
spectral learning  For example  SVD methods for learning weighted contextfree grammars have proved very successful in languagerelated problems  Cohen et al   
Luque et al    In the context of RL  ef cient SVD
methods for learning predictive state representations were
proposed in  Boots et al    Hamilton et al     
recent application of tensor methods to RL is given in  Azizzadenesheli et al    where the authors use   spectral
algorithm to obtain   PACRL learning result for POMDP
under memoryless policies  All these results have in common that they provide learning algorithms for models over
sequences  However  there is   fundamental difference between the nature of data in NLP and RL  With the exception of   few problems  most of NLP  safely  relies on
the assumption that        data from the target distribution
is available  In RL  however  the most general scenario assumes that the learner can only collect   single continuous
trajectory of data while all existing analysis of the SVD
method for sequential models  rely on the        assumption
 Hsu et al    Balle   Mohri    Glaude   Pietquin 
  Regarding tensor methods   Azizzadenesheli et al 
  gave the  rst analysis under dependent data satisfying certain mixing conditions 
The purpose of this paper is to provide the  rst rigorous
analyses of spectral SVD methods for learning sequential
models from noni      data  We provide ef cient algorithms with provable guarantees for learning several sequential models from   single trajectory  Speci cally  we

 See  Thon   Jaeger    for   survey of sequential models

learnable with SVD methods 

Spectral Learning from   Single Trajectory under FiniteState Policies

consider three models  probabilistic automata  stochastic
weighted automata  and PSRs under control from    nitestate policy  The  rst two results extend existing results in
the literature for        data  Hsu et al    Balle et al 
    The last result is an analog of the environment
learning result for POMDP   not the whole RL result  
of  Azizzadenesheli et al    with the difference that
our analysis provides guarantees under   much larger set
of policies  nitestate  as opposed to memoryless  This
result can also be interpreted as an extension of the batchbased PSR learning algorithm from  Boots et al   
to the noni      case  although they do not provide  nitesample guarantees  Our analysis is especially relevant since
the single trajectory spectral algorithm we analyze has been
used previously without an explicit instantiation or analysis  For example   Kulesza et al    Shaban et al   
present experiments with datasets containing single or few
long trajectories which are broken into short subsequences
and given as input to an spectral algorithm designed for
       data    more detailed list of our contributions is as
follows 

    singletrajectory spectrallearning algorithm for
probabilistic automata whose sample complexity depends on the mixing properties of the target automaton
 Section  

  An extension of this result showing that the same algorithm also learns stochastic weighted automata  Section  
In this case the analysis is more involved 
and requires   novel notion of mixing for stochastic
weighted automata and tools from the theory of linearinvariant cones  Berman   Plemmons   

    generalization of the algorithm that learns reactive
PSR controlled by    nitestate stochastic policy  Section   We provide for this algorithm  nitesample
bounds under   simple exploration assumption 

The most important tool in our analysis is   concentration
inequality for functions of dependent random variables 
These inequalities depend on the mixing coef cients of the
underlying process  We provide technical estimates for the
relevant mixing coef cients in each of the three cases listed
above  Our goal for future work is to extend   to prove
  PACRL for PSR under  nitestate policies  We also
think that the tools we develop to prove   can be used to
improve the sample complexity of algorithms for learning
stochastic weighted automata in the        case 
In Section   we start by recalling several facts about
weighted automata  spectral learning  and mixing that will
play   role in the sequel  For space reasons  most of our
proofs are deferred to the Supplementary Material 

  Background
Let   be    nite alphabet   cid  denote the set of words of
 nite length on     the set of all in nite words on   and
  be the empty word  Given two sets of words        cid 
we write      to denote the set of words  uv          
obtained by concatenating all words in   with all words in
   Let    be the set of probability distributions over
    member        is called   stochastic process
and   random in nite word     is called   trajectory 
Weighted and probabilistic automata   weighted  
nite automaton  WFA  with   states is   tuple    
 cid     cid  where    Rn are vectors of initial and
 nal weights respectively  and      Rn   are matrices of
transition weights    weighted automaton   computes  
function fA    cid      given by fA       cid Aw  where
Aw   Aw    Awt for          wt    WFA is minimal
if there does not exist another WFA with less states computing the same function    WFA   is stochastic if there
exists   stochastic process   such that for every      cid 
fA              that is    provides   representation for the probabilities of pre xes under the distribution
of     weighted automaton is irreducible if the labelled
directed graph with   vertices obtained by adding   transition from   to   with label   whenever          cid    is
strongly connected  It can be shown that irreducibility implies minimality  and that almost all WFA are irreducible in
the sense that the set of irreducible WFA are dense on the
set of all WFA  Balle et al   
  probabilistic  nite automaton  PFA  is   stochastic WFA
     cid     cid  where the weights have   probabilistic
interpretation  Namely    is   probability distribution over
            is the probability of emitting symbol   and
transitioning to state   starting from state    and        
for all         It is immediate to check that   PFA satisfying these conditions induces   stochastic process  However
not all stochastic WFA admit an equivalent PFA  Jaeger 
  Denis   Esposito   

If   is   PFA  then the matrix      cid 

     yields the
Markov kernel                    on the state space     after
marginalizing over the observations  It is easily checked
that   is rowstochastic  and thus        Furthermore 
for every distribution     Rn over     we have  cid 
       
for some other probability distribution   over     In the
case of PFA irreducibility coincides with the usual concept
of irreducibility of the Markov chain induced by   
Hankel matrices and spectral learning The Hankel matrix of   function      cid      is the in nite matrix
Hf     cid cid  with entries Hf             uv  Given  
    
nite sets        cid   
    RU   denotes the restriction
of matrix Hf to pre xes in   and suf xes in   
Fliess  theorem  Fliess    states that   Hankel matrix

Spectral Learning from   Single Trajectory under FiniteState Policies

Hf has  nite rank   if and only if there exists   WFA  
with   states such that     fA  This implies that   WFA  
with   states is minimal if and only if     rank HfA   The
spectral learning algorithm for WFA  Balle et al     
provides   mechanism for recovering such   WFA from
of Hf such that              
   nite subblock  
  there exists   set   cid  such that       cid       cid 
  cid  
    pair       that satis 
  rank Hf     rank  
 
 es these conditions is called   complete basis for    The
pseudocode of this algorithm is given below 

    
 

Algorithm   Spectral Learning for WFA
Input  number of states    Hankel matrix HU   
Find   cid  such that       cid       cid 
Let      HU cid  
Compute the rank   SVD        DV  cid 
Let hV       and take        cid hV
Let hU cid    HU cid  and take         cid hU cid 
foreach       do

Let      HU cid   and take          cid    

return      cid     cid 

The main strength of Algorithm   is its robustness to noise 

Speci cally  if only an approximation  cid HU    of the Hantomaton   and the automaton  cid   learned from  cid HU    can
be controlled in terms of the error  cid HU       cid HU    cid  see

kel matrix is known  then the error between the target au 

 Hsu et al    for   proof in the HMM case and  Balle 
  for   proof in the general WFA case  These tedious
but now standard arguments readily reduce the problem of
learning WFA via spectral learning to that of estimating the
corresponding Hankel matrix 
Classical applications of spectral learning assume one has
access to        samples from   stochastic process   In this
setting one can obtain   sample                    containing    nitelength trajectories from   and use them to

estimate   Hankel matrix  cid  
  cid 

 cid  

    
 

        

 
 

  

    
 

as follows 

       uv   

    

    
 

of  

    
fA

    
 

    
fA

     

ES cid  
vide   good approximation  cid  
fA    cid  

If        for some stochastic WFA  then obviously
and   large sample size   will pro 
  Explicit concentration bounds for Hankel matrices bounding the error
 cid  
In this paper we consider the more challenging setup where
we only have access to   sample       of size      
from   In particular  we show it is possible to replace the
empirical average above by     esaro average and still use
the spectral learning algorithm to recover the transition ma 

 cid  can be found in  Denis et al   

    
 

trices of   stochastic WFA  To obtain    nitesample analysis of this singletrajectory learning algorithm we prove
concentration results for   esaro averages of Hankel matrices  Our analysis relies on concentration inequalities for
functions of dependent random variables which depend on
mixing properties of the underlying process 
Mixing and concentration Let        be   stochastic
process and              random word drawn from  
For    cid         cid    and        we let          denote
the distribution of xt   xT conditioned on      xs     
With this notation we de ne the quantity

         cid     cid                   cid cid    

for any         and    cid      Then the  mixing
coef cients of   at horizon   are given by

      

sup

    cid 

         cid   

Mixing coef cients are useful in establishing concentration
properties of functions of dependent random variables  The
Lipschitz constant of   function            with respect
to the Hamming distance is de ned as

 cid   cid Lip   sup            cid   

where the supremum is taken over all pairs of words
     cid       differing in exactly one symbol  The following theorem proved in  Chazottes et al    Kontorovich
et al    provides   concentration inequality for Lipschitz functions of weakly dependent random variables 
Theorem   Let     and              Suppose
          satis es  cid   cid Lip  cid    and let                  xT  
Let
           where      are
the  mixing coef cients of   at horizon     Then the following holds for any      

     max    

 cid  

 cid  

 cid 

 

        EZ         cid  exp

 
 
with an identical bound for the other tail 

Theorem   shows that the mixing coef cient   is   key
quantity in order to control the concentration of   function of dependent variables 
In fact  upperbounding  
in terms of geometric ergodicity coef cients of   latent
variable stochastic process enables  Kontorovich   Weiss 
  to analyze the concentration of functions of HMMs
and  Azizzadenesheli et al    to provide PAC guarantees for an RL algorithm for POMDP based on spectral
tensor decompositions  Our Lemma   uses   similar but
more re ned bounding strategy that directly applies when
the transition and observation processes are not conditionally independent  Lemma   re nes this strategy further to
control   for stochastic WFA  for which there may be no
underlying Markov stochastic process in general  To the
best of our knowledge this yields the  rst concentration results for the challenging setting of stochastic WFA 

Spectral Learning from   Single Trajectory under FiniteState Policies

  SingleTrajectory Spectral Learning of PFA
In this section we focus on the problem of learning the transition structure of   PFA   using single trajectory is generated by    We provide   spectral learning algorithm for
this problem and    nitesample analysis consisting of  
concentration bound for the error on the Hankel matrix estimated by the algorithm  We assume the learner has access
to   single in nitelength trajectory        that is progressively uncovered  The algorithm uses   length   pre   from
  to estimate   Hankel matrix whose entries are   esaro averages  This Hankel matrix is then processed by the usual
spectral learning algorithm to recover an approximation to
an automaton with transition weights equivalent to those of
   We want to analyze the quality of the model learned by
the algorithm after observing the  rst   symbols from  
We start by showing that   esaro averages provide   consistent mechanism for learning the transition structure of   
Then we proceed to analyze the accuracy of the Hankel estimation step  As discussed in Section   this is enough to
obtain  nitesample bounds for learning PFA  The general
case of stochastic WFA is considered in Section  

  Learning with   esaro Averages is Consistent
Let      cid     cid  be   PFA computing   function fA  
 cid   and de ning   stochastic process       For
convenience we drop the subscript and just write   and  
Since we only have access to   single trajectory   from   we
cannot obtain an approximation of the Hankel matrix for  
by averaging over multiple        trajectories  Instead  we
compute   esaro averages over the trajectory   to obtain  
Hankel matrix whose expectation is related to   as follows 
   cid      be the function
For any       let  ft
      sw  where    sw   
       uw  We shall sometimes write fs     
   sw  Using the de nition of the function computed by
  WFA it is easy to see that

given by  ft         cid   
 cid 

 cid 
where    cid 

   

 cid 

   

   uw   

 cid AuAw     cid AsAw   

       cid   

     is the Markov kernel on the state space
    cid As we get
of    Thus  introducing  cid 
 ft       cid 
  Aw  Since   is   probability distribution   
is   Markov kernel  and probability distributions are closed
by convex combinations  then    is also   probability distribution over     Thus  we have just proved the following 

Lemma    Consistency  The   esaro average of   over
  steps   ft  is computed by the probabilistic automaton
 At    cid       cid  In particular    and  At have the same
number of states and the same transition probability matrices  Furthermore  if   is irreducible then  At is minimal 
The irreducibility claim follows from  Balle et al   

    
For convenience  in the sequel we write   
     block of the Hankel matrix Hf At
 
Remark   The irreducible condition simply ensures there
is   unique stationary distribution  and that the Hankel matrix of  At has the same rank as the Hankel matrix of  
 otherwise it could be smaller 

for the

 

    
  

kel matrix  cid  
and subscripts when not needed and write  cid Ht or  cid   when   
the expectation   cid    over     is equal to the Hankel ma 

  Spectral Learning Algorithm
Algorithm   describes the estimation of the empirical Hanfrom the  rst     symbols of   single trajectory using the corresponding   esaro averages  To avoid
cumbersome notations  in the sequel we may drop super
   and   are clear from the context  Note that by Lemma  
trix  Ht of the function  ft computed by the PFA  At 

Algorithm   Single Trajectory Spectral Learning
 Generative Case 

 

     cid  suf xes      cid 

Input  number of states    length    pre xes
Let     maxw        
Sample trajectory            xt        
foreach       and       do

Let  cid            
Apply the spectral algorithm to  cid   with rank  
Now we proceed to analyze the error  cid Ht    Ht in the Han 

  xs   uv    uv 

  Concentration Results

 cid   

  

kel matrix estimation inside Algorithm   In particular  we
provide concentration bounds that depend on the length   
the mixing coef cient   of the process   and the structure of the basis       The main result of this section is
the matrixwise concentration bound Theorem   where we
control the spectral norm of the error matrix  For comparison we also provide   simpler entrywise bound and recall
the equivalent matrixwise bound in the        setting 
Before trying to bound the concentration of the errors using Theorem   we need to analyze the mixing coef cient
of the process generated by   PFA  This is the goal of the
following result  whose proof is provided in Appendix   
Lemma    mixing for PFA  Let   be PFA and assume
that it is      geometrically mixing in the sense that for
some constants               we have
 cid At    cid At cid 

         
 
    sup
 cid 

 cid     cid cid 

 cid       

where the supremum is over all probability vectors  Then
we have     cid        

Spectral Learning from   Single Trajectory under FiniteState Policies

Theorem    Theorem   in  Denis et al    Let   be
  stochastic WFA with stopping probabilities and    
               be an        sample of size   from the disu        fA uv  Then 

tribution        cid  Let    cid 
 cid 
 cid   cm
 cid cid  
    
     

for all       we have

    
fA  cid   

 cid 

  

ec        

 

  
  

 cid 

 

 

 

  Sketch of the Proof of Theorem  

    

    
 

 cid cid  
    
       

In this section we sketch the main steps of the proof of
Theorem    the full proof is given in Appendices   and   
We focus on highlighting the main dif culties and paving
the path for the extension of Theorem   to stochastic WFA
given in Section  
The key of the proof is to study the function     
 cid  in view of applying Theorem   To
this end  we  rst control the  mixing coef cients using
Lemma   The next step is to control the Lipschitz constant  cid   cid Lip  This part is not very dif cult and we derive
 
after   few careful steps the bound  cid   cid Lip  cid   
The second and most interesting part of the proof is about
the control of      Let us give some more details 

Decomposition step We control  cid cid  
 cid   cid   cid 
      VE cid ft     ft     
  cid cid  
 cid  
uv      and  cid ft       
 cid  
 ft        cid ft       
  cid ft       ft     
 cid     
 cid    cid 

where we introduced                           
  bs    using the shorthand notation bs        xs       xs         Also 
   fs    where fs     
        This implies that we have   sum of variances  where each of the terms can be written as

Frobenius norm and get

 cid    cid 

 cid  by its

    
      

    
      

 cid 

     

    
 

    
 

fs   

bs   

 

 

 

 

 
  

  

  

  

 cid cid  

 

Remark     suf cient condition for the geometric conA
  is that   admits   spectral gap  In this case  
trol of  
can be chosen to be the modulus of the second eigenvalue
        of the transition kernel   
Before the main result of this section we provide   concentration result for each individual entry of the estimated
Hankel matrix as   warmup  see Appendix   
Theorem    Singletrajectory  entrywise  Let   be  
     geometrically mixing PFA and          trajectory
of observations  Then for any          and      

        cid 

    
            
 uv  
     

    
 

 cid cid 

 cid  log 

 cid 

 uv     

   

 

  

 cid     

with an identical bound for the other tail 

  naive way to handle the concentration of the whole Han 

kel matrix is to control the Frobenius norm  cid cid Ht    Ht cid  
 cid      To have better dependency with the dimension
cal Hankel matrix  cid   into blocks containing strings of the

by taking   union bound over all entries using Theorem  
However  the resulting concentration bound would scale as
 the matrix has dimension           can split the empiri 

same length  as suggested by the dependence of the bound
above on  uv  We thus introduce the maximal length
    maxw         and the set   cid                    cid 
for any  cid       We use these to de ne the quantity
nU    cid              cid      and introduce likewise
  cid  nV with obvious de nitions  With this notation we can
now state the main result of this section 
Theorem    Singletrajectory  matrixwise  Let   be as
          ft uv  be the probability mass and     min       nU nV  be the effective
dimension  Then  for all         we have

in Theorem   Let      cid 

 cid 

 

 cid cid  

    
       

    
 

 cid cid 

 

 cid   cid 
  
 

 

 LC
 

 cid 
 cid    
 cid 
 cid    ln 

  

     

 cid cid    

 

 cid     

  

Remark   Note that quantity nU nV in   can be exponentially smaller than       Indeed  for          
 cid    we
have             while nU nV      

For comparison  we recall   stateof theart concentration
bound for estimating the Hankel matrix of   stochastic language  from          trajectories 

   stochastic language is   probability distribution over  cid 

Slicing step An important observation is that each probability term satis es fs       cid As Aw  because of
the PFA assumption on     Furthermore  it follows from

  being   PFA that  cid     fs        for all   and   

This suggests that we group the terms in the sum over
         by length  so we write Wl          and de ne
Ll   maxw Wl         the maximum number of ways to
write   string of length   in   as   concatenation of   pre  
in   and   suf   in    Note that we always have Ll  cid    

Spectral Learning from   Single Trajectory under FiniteState Policies

  few more steps lead to the following bound

 cid    cid 

 cid   cid 
       

  cid cid  

    
 

 cid 
 cid 
    
      
 
 cid 
  

  

  Wl

 

 cid     cid cid  

  

 fs   fs   

 cid cid 
 cid   bs   bs cid   fs   fs cid   
 cid 
  cid 

 fs   fs   cid   

 ft uv   

        

We control the  rst term in   using

 cid 

 cid 

  

  Wl

       

  

 

 

 cid 

 cid 

   cid 
  

Cross terms Regarding the remaining  cross term in  
we        Wl and obtain the equation
  bs   bs cid      fs   fs cid   
    Aw cid 
As cid  

 
     cid As and transition
Ax corresponding to the  event 

where we introduced the vectors  cid 
matrix As cid  
        cid        cid sw  We now discuss two cases 
   cid  
First control If   cid           we use the simplifying fact that
        cid   to upper bound   by
   cid  

   cid 

    cid  

  cid Aw

   

 

 cid 

 cid 

As cid      cid 

 cid 
  Aw
  fs      fs cid     cid  fs     

  cid Aw

 

Second control When   cid     cid          we have    cid  
    cid   lw and As cid  
  and bound it using   older   inequality as follows 

   
    AwAs cid   lAw  Thus  we rewrite

 cid 

 cid 
As cid     cid 
  Aw cid cid As cid        cid 

  cid 

Aw   cid 

 cid 
  Aw
 cid cid 

  cid cid cid Aw cid   

 

Using Lemma   in Appendix   we bound the induced norm
  cid cid   cid   
as  cid As cid        cid 
 
 
is the
  cid      where  
 
mixing coef cient de ned in Lemma   Also  it holds that
 cid Aw cid   cid    Finally  since  cid 
  Aw is   subdistribution
 cid 
over states  we have the key equalities
 cid 

 cid 
 cid 

  Aw cid   

        cid cid 

  Aw   

         cid 

  Wl

  Wl

        fs     

fs uv   

         uv Wl

 

  Wl

The proof is concluded by collecting the previous bounds 
plugging them into   and using Lemma   to get

      cid 

        

  
     

 

 

 cid   

 

 cid 

  Extension to Stochastic WFA
We now generalize the results in previous section to the
case where the distribution over   is generated by   stochastic weighted automaton that might not have   probabilistic
representation  The key observation is that Algorithm  
can learn stochastic WFA without any change  and the consistency result in Lemma   extends verbatim to stochastic
WFA  However  the proof of the concentration bound in
Theorem   requires further insights into the mixing properties of stochastic WFA  Before describing the changes required in the proof  we discuss some important geometric
properties of stochastic WFA 

  The StateSpace Geometry of SWFA
Recall that   stochastic WFA  SWFA       cid     cid 
de nes   stochastic process    and computes   function
fA such that fA              where         It is
immediate to check that this implies that the weights of  
satisfy the properties       cid Ax   cid    for all      cid  and

 ii   cid At   cid      cid Aw      for all    cid    where
     cid 

     Without loss of generality we assume
throughout this section that   is   minimal SWFA of dimension    meaning that any SWFA computing the same
probability distribution than   must have dimension at least
Importantly  the weights in     and    are not ren 
quired to be nonnegative in this de nition  Nonetheless 
it follows from these properties that   is an eigenvector of
  of eigenvalue   exactly like in the case of PFA  We now
introduce further facts about the geometry of SWFA 
  minimal SWFA   is naturally associated with   proper
      pointed  closed  and solid  cone in     Rn called the
backward cone  Jaeger    and characterized by the following properties                for all     and
   cid    cid    for all      Condition   says that every transition matrix    leaves   invariant  and in particular the
backward vector Aw  belongs to   for all      cid 
The vector of  nal weights   plays   singular role in the
geometry of the state space of   SWFA  This follows from
facts about the theory of invariant cones  Berman   Plemmons    which provides   generalization of the classical Perron Frobenius theory of nonnegative matrices to
arbitrary matrices  We recall from  Berman   Plemmons 
  that   norm on Rn can be associated with every vector in the interior of    In particular  we will take the norm
associated with the  nal weights        This norm  denoted by  cid     cid  is completely determined by its unit ball
          Rn      cid      cid     where    cid     means
           In particular   cid   cid    inf    cid          rB 
Induced and dual norms are derived from  cid     cid  as usual 
When   is   PFA one can take   to be the cone of vectors in
Rn with nonnegative entries  in which case                
and  cid     cid  reduces to  cid     cid   Berman   Plemmons   
The following result shows that  cid     cid  provides the right

Spectral Learning from   Single Trajectory under FiniteState Policies

generalization to SWFA of the norm  cid cid  used in the Second control step of the proof for PFA  see Appendix   
Lemma   For any      cid       cid Aw cid   cid    and  ii 
 cid cid Aw cid     cid Aw 
It is also natural to consider mixing coef cients for stochastic processes generated by SWFA in terms of the dual  
norm  This provides   direct analog to Lemma   for PFA 
Lemma    mixing for SWFA  Let   be SWFA and assume that it is      geometrically mixing in the sense that
for some    cid           
 cid cid 
 
 
   

  At cid cid 

 cid       

  At    cid 
 cid     cid cid 

 cid 

sup
   cid 

   

Then the  mixing coef cient satis es     cid        
Remark     suf cient condition for the geometric conA
In this case
trol of  
 
  can be chosen to be the modulus of the second eigenvalue         of    Another suf cient condition is that
            where

is that   admits   spectral gap 

 cid 

 cid   cid 

 cid 

      sup

          cid   cid     cid     

 

  Concentration of Hankel Matrices for SWFA
We are now ready to extend the proof of Theorem   to
SWFA  Using that both PFA and SWFA de ne probability distributions over pre xes it follows that any argument
in Section   that only appeals to the function computed
by the automaton can remain unchanged  Therefore  the
only arguments that need to be revisited are described in
the Second control step 
In particular  we must provide
versions of   and   for SWFA 
Recalling that   older   inequality can be applied with
any pair of dual norms  we start by replacing the norms
 cid     cid  and  cid     cid  in   with the conenorms  cid     cid  and
 cid     cid cid  respectively  Next we use Lemma   to obtain 
for any      cid  the bound  cid Aw cid   cid    and the equation  cid cid Aw cid     cid Aw  which are direct analogs
of the results used for PFA  Then it only remains to relate the  norm of As cid        cid 
  cid  to the mixing coA
    Applying Lemma   in Appendix   yields
ef cients  
 cid As cid        cid 
  cid cid   cid   
 
  cid      Thus we obtain for
SWFA exactly the same concentration result that we obtained for empirical Hankel matrices estimated from   single trajectory of observations generated by   PFA 
Theorem    Singletrajectory  SWFA  Let   be   SWFA
that is      geometrically mixing with the de nition in
Lemma   Then the concentration bound in Theorem   also
holds for trajectories        

  The Controlled Case
This section describes the  nal contribution of the paper 
  generalization of our analysis of spectral learning from
  single trajectory the case of dynamical systems under
 nitestate control  We consider discretetime dynamical
systems with  nite set of observations   and  nite set of
actions    and let          We assume the learner has
access to   single trajectory      ot  at   cid  in   The trajectory is generated by coupling an environment de ning  
distribution over observations conditioned on actions and
  policy de ning   distribution over actions conditioned
on observations  Assuming the joint actionobservation
distribution can be represented by   stochastic WFA is
equivalent to saying that the environment corresponds to
  POMDP or PSR  and the policy has  nite memory 
To    some notation we assume the environment is represented by   conditional  stochastic WFA      cid     cid 
This implies the semantics fA     
with   states 
       ot      at  for the function computed by   
where          wt with wi    oi  au  For any      cid 
we shall write wA        at and wO        ot  We also
assume there is   stochastic policy   represented by   conditional PFA       cid   cid  with   states  that is 
fA         wA wO           at      ot  In particular     represents   stochastic policy that starts in   state
         sampled according to                 and at
each time step samples an action and changes state according to                st       at     ot      st     
The trajectory   observed by the learner is generated by
the stochastic process        obtained by coupling
  and      standard construction in the theory of
weighted automata  Berstel   Reutenauer    shows
that this process can be computed by the product automaton              cid     cid  where          
          and Bo     Ao           It is easy to verify
that   is   stochastic WFA with nk states computing the
function fB      fA   fA               
At this point  the spectral algorithm from Section   could
be used to learn   directly from   trajectory         However  since the agent interacting with environment   knows
the policy   we would like to leverage this information to
learn directly   model of the environment  This approach
is formalized in Algorithm   which provides   singletrajectory version of the algorithm in  Bowling et al   
for learning PSR from nonblind policies with        data 
The main difference with Algorithm   is that in the reactive
case we need   smoothing parameter   that will prevent the

entries in the empirical Hankel matrix  cid   to grow unbound 

edly plus that the policy   satis es an exploration assumption    plays   similar role in our analysis as the smoothing
parameter introduced in  Denis et al    for learning

 Such WFA are also called reactive predictive state represen 

tations in the RL literature 

Spectral Learning from   Single Trajectory under FiniteState Policies

Algorithm   Single Trajectory Spectral Learning
 Reactive Case 
Input  number of states    length               cid 

policy   smoothing coef cient  

Let     maxw        
Sample trajectory            ot Lat   using  
foreach       and       do

Let  cid          
 cid   
Apply the spectral algorithm to  cid   with rank  

  os as os uv as uv uv 

     as uv   os uv 

  

 
 

stochastic languages from factor estimates in the        case 
The difference is that in our case the smoothing parameter
must satisfy       where   is the exploration probability
of the policy   provided by the following assumption 
Assumption    Exploration  There exists some      
for each      cid  the policy   satis es
such that
 wA wO   cid      In particular  at every time step each
action       is picked with probability at least  
Before moving to the next section  where we provide  nitesample concentration results for the Hankel matrix estimated by Algorithm   we show that Algorithm   is consistent  that is it learns in expectation   WFA whose transition
matrices are equivalent to those of the environment    The
proof of the following lemma is provided in Appendix   
    
   computed in Alis
  block of the Hankel matrix corresponding to the stochastic WFA  At  cid       cid  where we introduced the modis   cid      We denote by
 ed vector
 ft the function computed by  At 

Lemma   The Hankel matrix  cid      cid  
gorithm   satis es   cid  
        cid   

    
  where   
 

    
         

    
 

  Concentration Results

tion error  cid cid  

    
 

    
       

Broadly speaking    concentration bound for the estima 
 cid  can be obtained by following  
proof strategy similar to the ones used in Theorems   and  
However  almost all the bounds used in the previous proofs
need to be reworked to account for     the effect of the extra
dependencies introduced by the policy   and  ii  the fact
that the target automaton   to be learned is not   stochastic WFA in the sense of Section   but rather   conditional
stochastic WFA 
Point     is addressed in our proof by introducing    normalized  reference process    corresponding to the coupling          Aunif between the environment   and
the uniform random policy that at each step takes each action independently with probability     Assuming the
smoothing parameter satis es       for some exploration

 

parameter    cf  Assumption   then    cid      This
observation is used  for example  to bound some variance
terms in      by replace occurrences of  ft with    unif
 
the function computed by taking the   esaro average of the
 rst   steps of     Ultimately  this makes our bound depend
not only on the mixing properties of     but also on those
of the normalized process    induced by the SWFA     Incidentally  this argument is also used to address point  ii 
by bounding quantities involving   by quantities computed
by   SWFA we can use again the arguments sketched in
Section  
Pursuing the ideas above  and assuming that    is        
geometrically mixing  we obtain the following bound
which can be compared to the one in  

 cid 

 cid 

   

  
     

 

    
   

  

      cid 

 

        

where     maxw               cid 
    cid 

            unif

 uv 

 

          ft uv  and

Theorem   Suppose that   is      geometrically mixing
and    is        geometrically mixing  Suppose   satis 
 es Assumption   and the smoothing coef cient   satis es
               and de ne           as
above  Then for any         we have
 cid 

      Let    cid 
 cid 
 cid cid  
    
       
 cid 
 cid 

        

 cid 

 cid   

 cid 

    
 

 cid 

  

 

 

 

 

  

   ln 

 

 cid     

    
   

  

  
   

On    nal note we remark that the dependence on    might
be unavoidable due to inherent increase in variance produced by importance sampling estimators 

  Conclusion
We present the  rst rigorous analysis of singletrajectory
SVDbased spectral
learning algorithms for sequential
models with latent variables  Our analysis highlights the
role of mixing properties of WFA and their relation with the
geometry of the underlying state space  In the controlled
case we obtain   result for control with  nitestate policies    much more general class than previously considered
memoryless policies  In future work we will use our results
to get upper con dence bounds on the predictions made by
the learned environment with the goal of solving the full
RL problem for PSR with complex control policies 

Spectral Learning from   Single Trajectory under FiniteState Policies

Acknowledgements
        acknowledges the support of the French Agence
Nationale de la Recherche  ANR  under grant ANR 
CE   project BADASS 

References
Anandkumar  Anima  Foster  Dean    Hsu  Daniel   
Kakade  Sham    and Liu  YiKai    spectral algorithm
for latent dirichlet allocation  In Advances in Neural Information Processing Systems  pp     

Anandkumar  Animashree  Ge  Rong  Hsu  Daniel   
Kakade  Sham    and Telgarsky  Matus  Tensor decompositions for learning latent variable models  Journal of
Machine Learning Research     

Azizzadenesheli  Kamyar  Lazaric  Alessandro 

and
Anandkumar  Animashree  Reinforcement learning of
pomdps using spectral methods  In  th Annual Conference on Learning Theory  pp     

Bacon  PierreLuc  Balle  Borja  and Precup  Doina  Learning and planning with timing information in markov
In Proceedings of the ThirtyFirst
decision processes 
Conference on Uncertainty in Arti cial Intelligence  pp 
  AUAI Press   

Bailly     Habrard     and Denis       spectral approach
for probabilistic grammatical inference on trees  In Algorithmic Learning Theory  pp     

Bailly  Rapha el  Quadratic weighted automata  Spectral
In Proceedalgorithm and likelihood maximization 
ings of the  rd Asian Conference on Machine Learning 
ACML   Taoyuan  Taiwan  November    
pp     

Balle     Learning FiniteState Machines  Algorithmic and
Statistical Aspects  PhD thesis  Universitat Polit ecnica
de Catalunya   

Balle  Borja and Mohri  Mehryar  Spectral learning of general weighted automata via constrained matrix completion  In Advances in neural information processing systems  pp     

Balle  Borja and Mohri  Mehryar  Learning weighted automata  In International Conference on Algebraic Informatics  pp    Springer   

Balle  Borja  Quattoni  Ariadna  and Carreras  Xavier   
spectral learning algorithm for  nite state transducers 
In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases  pp   
Springer   

Balle  Borja  Carreras  Xavier  Luque  Franco    and Quattoni  Ariadna  Spectral learning of weighted automata 
Machine learning       

Balle  Borja  Hamilton  William    and Pineau  Joelle 
Methods of moments for learning stochastic languages 
Uni ed presentation and empirical comparison 
In
ICML  pp       

Balle  Borja  Gourdeau  Pascale  and Panangaden  Prakash 
Bisimulation metrics for weighted automata  In  rd International Colloquium on Automata  Languages  and
Programming  ICALP   

Berman  Abraham and Plemmons  Robert    Nonnegative

matrices in the mathematical sciences  SIAM   

Berstel  Jean and Reutenauer  Christophe  Rational Series

and Their Languages  Springer   

Boots  Byron  Siddiqi  Sajid    and Gordon  Geoffrey   
Closing the learningplanning loop with predictive state
representations  The International Journal of Robotics
Research     

Bowling  Michael  McCracken  Peter  James  Michael 
Neufeld  James  and Wilkinson  Dana  Learning predictive state representations using nonblind policies  In
Proceedings of the  rd international conference on Machine learning  pp    ACM   

Chazottes  JR  Collet  Pierre    ulske  Christof  and Redig 
Frank  Concentration inequalities for random  elds via
coupling  Probability Theory and Related Fields   
   

Cohen        Stratos     Collins     Foster        and
Ungar     Experiments with spectral learning of latentvariable PCFGs  In Proceedings of NAACL   

Cohen        Stratos     Collins     Foster        and
Ungar     Spectral learning of latentvariable PCFGs 
Algorithms and sample complexity  Journal of Machine
Learning Research   

Denis  Franc ois  Gybels  Mattias  and Habrard  Amaury 
Dimensionfree concentration bounds on hankel matrices for spectral learning  Journal of Machine Learning
Research     

Denis  Franc ois and Esposito  Yann  On rational stochastic
languages  Fundamenta Informaticae     
 

Denis  Franc ois  Gybels  Mattias  and Habrard  Amaury 
Dimensionfree concentration bounds on hankel matrices for spectral learning  In ICML  pp     

Spectral Learning from   Single Trajectory under FiniteState Policies

Fliess     Matrices de Hankel  Journal de Math ematiques

Pures et Appliqu ees     

Rosenthal  Jeffrey    Convergence rates for markov chains 

Siam Review     

Glaude  Hadrien and Pietquin  Olivier  Pac learning of
probabilistic automaton based on the method of moments  In Proceedings of The  rd International Conference on Machine Learning  pp     

Shaban  Amirreza  Farajtabar  Mehrdad  Xie  Bo  Song  Le 
and Boots  Byron  Learning latent variable models by
improving spectral solutions with exterior point method 
In UAI  pp     

Siddiqi        Boots     and Gordon        Reducedrank

hidden Markov models  AISTATS   

Thon  Michael and Jaeger  Herbert  Links between multiplicity automata  observable operator models and predictive state representations   uni ed learning framework  Journal of Machine Learning Research   
   

Hamilton  William    Fard  Mahdi Milani  and Pineau 
Joelle  Ef cient learning and planning with compressed
Journal of Machine Learning Repredictive states 
search     

Hsu  Daniel and Kakade  Sham    Learning mixtures of
spherical Gaussians  moment methods and spectral decompositions  In Innovations in Theoretical Computer
Science   

Hsu  Daniel  Kakade  Sham    and Zhang  Tong    spectral algorithm for learning hidden markov models  Journal of Computer and System Sciences   
 

Jaeger  Herbert  Observable operator models for discrete
stochastic time series  Neural Computation   
   

Kontorovich  Aryeh and Weiss  Roi  Uniform chernoff and
dvoretzkykiefer wolfowitztype inequalities for markov
chains and related processes  Journal of Applied Probability     

Kontorovich  Leonid Aryeh  Ramanan  Kavita  et al  Concentration inequalities for dependent random variables
via the martingale method  The Annals of Probability 
   

Kontoyiannis  Ioannis and Meyn  Sean    Geometric ergodicity and the spectral gap of nonreversible markov
chains  Probability Theory and Related Fields   
   

Kulesza  Alex  Jiang  Nan  and Singh  Satinder  Lowrank
spectral learning with weighted loss functions  In Arti 
cial Intelligence and Statistics  pp     

Langer  Lucas  Balle  Borja  and Precup  Doina  LearnIn Proing multistep predictive state representations 
ceedings of the TwentyFifth International Joint Conference on Arti cial Intelligence  IJCAI   New York 
NY  USA    July   pp     

Luque  Franco    Quattoni  Ariadna  Balle  Borja  and
Carreras  Xavier  Spectral learning for nondeterministic
In Proceedings of the  th Condependency parsing 
ference of the European Chapter of the Association for
Computational Linguistics  pp    Association
for Computational Linguistics   

