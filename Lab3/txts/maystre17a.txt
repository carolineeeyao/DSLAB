Just Sort It    Simple and Effective Approach to Active Preference Learning

Lucas Maystre   Matthias Grossglauser  

Abstract

We address the problem of learning   ranking
by using adaptively chosen pairwise comparisons 
Our goal is to recover the ranking accurately but
to sample the comparisons sparingly  If all comparison outcomes are consistent with the ranking 
the optimal solution is to use an ef cient sorting
algorithm  such as Quicksort  But how do sorting
algorithms behave if some comparison outcomes
are inconsistent with the ranking  We give favorable guarantees for Quicksort for the popular
Bradley Terry model  under natural assumptions
on the parameters  Furthermore  we empirically
demonstrate that sorting algorithms lead to   very
simple and effective active learning strategy  repeatedly sort the items  This strategy performs as
well as stateof theart methods  and much better
than random sampling  at   minuscule fraction of
the computational cost 

 

Introduction

The problem of recovering   ranking over   items from
noisy outcomes of pairwise comparisons has attracted  in the
last century  much research interest  driven by applications
in sports  Elo    social sciences  Thurstone    Salganik   Levy    and more recently recommender
systems  Houlsby et al    Whereas pairwise comparison models and related inference algorithms have been
extensively studied  the issue of which pairwise comparisons to sample  also known as active learning  has received
signi cantly less attention  To understand the potential bene 
 ts of adaptively selecting samples  consider the case where
comparison outcomes are noiseless       consistent with  
linear order on   set of   items  If pairs of items are selected
at random  it is necessary to collect     comparisons
to recover the ranking  Alon et al    In contrast  by
using an ef cient sorting algorithm      log    adaptively

 School of Computer and Communication Sciences  EPFL 
Lausanne  Switzerland  Correspondence to  Lucas Maystre  lucas maystre ep ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

chosen comparisons are suf cient  In this work  we demonstrate that sorting algorithms can also be helpful in the noisy
setting  where some comparison outcomes are inconsistent
with the ranking  despite errors  sorting algorithms tend to
select informative samples  We focus on the Bradley Terry
 BT  model    widelyused probabilistic model of comparison outcomes  In this model  each item is associated with  
parameter on the real line  and the probability of observing
an incorrect outcome decreases as the distance between the
items  parameters increases 

First  we study the output of   single execution of Quicksort
when comparison outcomes are generated from   BT model 
under the assumption that the distance between adjacent
parameters is  stochastically  uniform across the ranking 
We measure the quality of   ranking estimate by its displacement with respect to the ground truth       the sum of
rank differences  We show that Quicksort   output is   good
approximation to the groundtruth ranking  no method comparing every pair of items at most once can do better  up to
constant factors  Furthermore  we show that by aggregating
  log     independent runs of Quicksort  it is possible to
recover the exact rank for all but   vanishing fraction of
the items  These theoretical results suggest that adaptive
sampling is able to bring   substantial acceleration to the
learning process 

Second  we propose   practical activelearning  AL  strategy
that consists of repeatedly sorting the items  We evaluate
our sortingbased method on three datasets and compare it
to existing AL methods  We observe that all the strategies
that we consider lead to better ranking estimates noticeably faster than random sampling  However  most strategies
are challenging to operate and computationally expensive 
thus hindering wider adoption  Schein   Ungar    In
this regard  sortingbased AL stands out  as    it is computationally speaking as inexpensive as random sampling 
   it is trivial to implement  and    it requires no tuning of
hyperparameters 

  Preliminaries and Notation

We consider   items that are represented by consecutive
integers                    Without loss of generality  we

  Simple and Effective Approach to Active Preference Learning

assume that the items are ranked by increasing preference 
           means that   is  in expectation  preferred to   
When   is preferred to   as   result of   pairwise comparison 
we denote the observation by        If        we say that
      is   consistent outcome and       an inconsistent  incorrect  outcome  In most of the paper  pairwise comparison
outcomes follow   Bradley Terry model with parameters

     cid 

     

   cid    Rn  denoted BT  The parame 

ters                represent the utilities of items             
and the probability of observing the outcome       is

             

 

    exp        

 

The probability of observing an inconsistent comparison
decreases with the distance between the items  This captures the intuitive notion that some pairs of items are easy
to compare and some are more dif cult  Zermelo   
Bradley   Terry   

  ranking   is   function that maps an item to its rank      
      rank of item    The  groundtruth  identity ranking
is denoted by id       id         To measure the quality of  
ranking   with respect to the groundtruth  we consider the
displacement

   

 

Xi 

        

also known as Spearman   footrule distance  Another metric
widely used in practice is the Kendall Tau distance  de ned
            Both metrics are equivalent up to   factor of two  such that bounds on   also
hold for    up to constant factors 

as      Pi  

Finally  we say that an event   holds with high probability
if           as       For   random variable   and
  sequence of numbers an  we say that       an  with
high probability if         can      as       for some
constant   that does not depend on   

Outline of the paper  We begin by brie   reviewing related literature in Section   Next  in Section   we study
the displacement of Quicksort   output under noisy comparisons  In Section   we empirically evaluate several AL
strategies on three datasets  Finally  we conclude in Section  

  Related Work

Passive setting  Recently  there have been   number of results on the sample complexity of the BT model  based on

  This convention greatly simpli es the notation throughout the
paper  but differs from that used in most of the preference learning
literature  In our paper  the item with rank   is the worst 

            Diaconis   Graham   

the assumption that all pairs of items are chosen before any
comparison outcome is revealed  Negahban et al   
Hajek et al    Rajkumar   Agarwal    Vojnovic
  Yun    In general  these results reveal that choosing
pairs of items uniformly at random is essentially optimal 
Furthermore  they suggest that the ranking induced by the
BT model cannot be recovered with less than     comparisons  Our work shows that by adaptively selecting pairs
based on observed outcomes  we observe substantial gains 

Active preference learning  AL approaches for learning
  ranking based on noisy comparison outcomes have been
studied under various assumptions  Braverman   Mossel
  examine   model where outcomes of pairwise comparisons are  ipped with   small  constant probability  Ailon
  considers an adversarial setting  comparison outcomes can be arbitrary  and investigates AL in the context
of  nding   ranking that minimizes the number of inconsistent outcomes  also known as the minimum feedbackarc
set problem on tournaments  MFAST  These theoretical
studies imply  in their respective settings  that     logk   
comparison outcomes are enough to recover   nearoptimal
ranking  Jamieson   Nowak   propose an ef cient
activeranking algorithm that is applicable if items can
be embedded in Rd       using   features  and assuming
that admissible rankings satisfy some geometric constraints 
Wang et al    study   collaborative preferencelearning
problem and show that   variant of uncertainty sampling   
wellknown AL strategy  works well for their problem  In
this work  we assume that we do not have access to item
features and that comparison outcomes follow   single BT
model 

Bayesian methods  From   practical standpoint  Bayesian
methods provide an effective way to select informative samples  MacKay    However  they can be dif cult to scale
if the number of items is large  Work on Bayesian active
preference learning includes Chu   Ghahramani  
Houlsby et al    Salimans et al    and Chen et al 
  We compare our AL strategy to these methods in
Section  

Multiarmed bandit  The dueling bandit problem  Yue
et al    is somewhat related to our work  In this problem  the goal is to identify the best item based on noisy comparison outcomes  using as few adaptively chosen samples
as possible  Two recent papers also extend the problem to
that of recovering the entire ranking  instead of only the top
element  The work of Sz   nyi et al    is the closest to
ours  as it also uses the BT model  One of their results is similar to our Theorem   They show that   quasilinear number
of comparisons is suf cient to recover the true ranking  under some conditions on   Heckel et al    investigate  
nonparametric model and develop some theoretical guarantees  In contrast to these works  our paper studies practical

  Simple and Effective Approach to Active Preference Learning

  Terminating case 

Algorithm   Quicksort
Require  set of items  
  if          then return list    
             
      element of   selected uniformly at random
  for             do
 
 
 
 

           

           

if       then

  Pairwise comparison 

else

  return Quicksort          Quicksort   

comparison budgets  we give theoretical guarantees for the
output obtained from   single call to Quicksort  and in our
experiments we never exceed     calls 

Quicksort  The Quicksort algorithm  Hoare    is one
of the most widely studied sorting procedures  Quicksort
has been shown to produce useful rankings beyond classic
sorting problems  For example  Ailon et al    show
that Quicksort produces  in expectation     approximation
to the MFAST problem  Quicksort combined with BT comparison outcomes has also been proposed as   probabilistic
ranking model  Ailon    We take advantage of some
of the properties of this ranking model in order to derive the
theoretical results of Section  

  Theoretical Results

In this section  we begin by studying the behavior and output of Quicksort under inconsistent comparison outcomes 
without any assumptions on the noise generating process 
Then  starting in Section   we focus on comparison outcomes generated by the BT model  Due to limited space 
most full proofs are deferred to the supplementary material
 Section   

Quicksort  Algorithm   is best described as   recursive
procedure  At each step of the recursion    pivot item   is
chosen uniformly at random  line   Then  during the partition operation  lines   every other item is compared to
  and added to the set   or    depending on the outcome 
If all comparison outcomes are consistent  it is wellknown
that Quicksort terminates after sampling     log    comparisons with high probability  What happens if we drop the
consistency assumption  The following two lemmas state
that these key properties remain valid  no matter which  and
how many  comparison outcomes are inconsistent 

Because              the recursive calls are made on
sets of items of strictly decreasing cardinality  and the algorithm terminates after    nite number of steps  Furthermore 
suppose that Quicksort samples an outcome for the pair
       Then either   or   is the pivot in   partition operation 
In either case  the pivot is not included in the recursive calls 
which ensures that        cannot be compared again 

Lemma   Quicksort samples     log    comparisons
      

Proof  sketch  We follow   standard analysis of Quicksort
 see       Dubhashi   Panconesi    Section   With
high probability  we choose    good  pivot       one that
results in   balanced partition    constant fraction of the
time  In this case  the depth of the call tree is   log    As
there are at most   comparisons at each level of the call tree 
we conclude that Quicksort uses     log    comparisons
in total  With respect to the standard proof  we need some
additional work to formalize the notion of  good  pivot to
the setting where comparison outcomes are not consistent
with   linear order 

Lemma   complements Theorem   in Ailon   Mohri  
which states that Quicksort samples     log    in expectation  These results might suggest that all properties of Quicksort carry over to the noisy setting  This is not the case  For
example  although Quicksort uses approximately    ln  
comparisons on average in the noiseless setting  Sedgewick
  Wayne    this number can be distinctly different
with inconsistent comparison outcomes 

Quicksort  and ef cient sorting algorithms in general  infer
most pairs of items  relative position by transitivity and thus
rely heavily on the consistency of comparison outcomes  In
the noisy case  it is therefore important to precisely understand the effect of an inconsistent outcome on the output of
the algorithm  this effect extends beyond the pair of items
whose comparison outcome was inconsistent  For this purpose  the next Lemma bounds the displacement of Quicksort   output as   function of the inconsistent outcomes 

Lemma   Let   be the set of pairs sampled by Quicksort
and whose outcome is inconsistent with id  Let   be the
output  Then 

            

       

Lemma   Quicksort always terminates and samples each
of the      possible comparisons at most once 

Proof  sketch  Consider the  rst partition operation  with
pivot    resulting in partitions   and    Denote the errors

Proof  The proof is identical to the consistent setting  Consider the state of   and   at the end of   partition operation 

      if comparison outcomes are uniformly random  all items
are  good  pivots        and the average number of comparisons
will be closer to   log    on average  for large   

  Simple and Effective Approach to Active Preference Learning

made during this partition operation by    We can show
that the displacement is bounded by

                          

       

where     and     represent the displacement of the
ordering induced by   on   and    respectively  In other
words  the total displacement can be decomposed into   term
that represents the  local  displacement due to the partition
operation and into two terms that account for errors in the
recursive calls  We obtain the desired result by recursively
bounding     and    

Informally  Lemma   states that the displacement can be
bounded by   sum of  local shifts  due to the inconsistent
outcomes and that the price to pay for any information inferred by transitivity is bounded by   factor two  Lemma   is
  crucial component of our subsequent analysis of BT noise 
and we believe that it can be useful in order to investigate
Quicksort under   wide variety of other noise generating
processes 

likely to result in   larger number of inconsistent outcomes 
Although the precise choice of this Poisson model is driven
by tractability concerns  in Section   we argue that it is
essentially equivalent to choosing the parameters independently and uniformly at random in the interval      
when   is  xed and   is large  We are now ready to state
our main result 

Theorem   Let   be sampled from   Poisson point process
of rate   Let   be the output of Quicksort using comparison
outcomes sampled from BT  Then        

        

max

 

              log   

 

 

Proof  sketch  Let zij be the indicator random variable
of the event  the comparison between   and   results in
an error  and let dij            The distance dij is  
sum of         exponential random variables       dij  
Gamma          and we can show that

   zij      cid 

 

    exp dij cid 

  Displacement in the Poisson Model

     exp dij             

From here on  we assume that comparison outcomes are
generated from BT  Clearly  any results on the displacement of   ranking estimated from samples of   BT model
will depend on   it is easy to construct   model instance
for which it is arbitrarily hard to recover the ranking  by
choosing parameters suf ciently close to each other  Our approach is as follows  We postulate   family of distributions
over   and we give bounds on the displacement that hold
with high probability 

We suppose that comparison outcomes are  in expectation 
uniformly noisy across the ranking       comparing two
elements at the bottom is    priori  as dif cult as comparing
two elements at the top or in the middle  This means that the
probability distribution over parameters              results
in  random  distances          that depend only on    One
such distribution arises if the parameters are drawn from  
Poisson point process of rate   That is 

                  xn    Exp 

    

  

Xk 

xk 

 

The average distance between two items separated by  
positions in the ordering is                   Although
the distance between adjacent items is constant in expectation  we allow some parameters to be arbitrarily close  The
parameter   controls the expected level of noise    large   is

  In particular  the expected minimum distance between two
items       the min of   exponential        decreases as     as
  increases 

Using Lemma   and the fact that every pair of items is
compared at most once  we  nd

          zij 

       Xi  
Xk 

    

 

                 

The random variables  zij  are not unconditionally independent  they are independent when conditioned on   but 
with some more work  we can show that Var         
By using   Chebyshev bound    follows 

In order to prove   we take advantage of   theorem due
to Ailon   which states that

                             

even if   and   were not directly compared with each other 
We use   Chernoff bound on dij to show that the relative
order between any two items separated by at least    log   
positions is correct with high probability  The second part
of the claim follows easily 

Note that any method that compares each pair of items at
most once results in   ranking estimate   with displacement
          with high probability  As there is only   single  possibly inconsistent  comparison outcome between
each pair of adjacent items  it is likely that   constant fraction of the items will be ranked incorrectly  resulting in  

  Simple and Effective Approach to Active Preference Learning

Algorithm   Multisort
Require  set of items     number of iterations  
       
  for                 do
 
 

    Quicksort    
         

  return Copeland aggregation of  

displacement that grows linearly in    Hence  our bound on
  shows that Quicksort is orderoptimal  in   

In light of Theorem     natural question to ask is as follows  How many comparisons are needed in order to  nd
the correct ranking  Clearly   nding the exact ranking is
dif cult  in fact      comparison outcomes are necessary
to discriminate the closest pair of items reliably  see supplementary material  Section    As such  we will focus on
 nding   ranking that matches the ground truth everywhere 
except at   vanishing fraction of the items 

Multiple runs of Quicksort likely produce different outputs 
because of the noisy comparison outcomes and because the
algorithm itself is randomized  the pivot selection is random  By aggregating   independent outputs of Quicksort 
is it possible to produce   better ranking estimate  Similarly to Sz   nyi et al    we combine the   outputs
             into an aggregate ranking   using Copeland  
method  The method assigns  to each item    score that corresponds to the number of items that it beats in   majority
of the rankings  and it then ranks the items by increasing
score  Copeland    We call the procedure Multisort
and describe it in Algorithm  

Theorem   Let   be sampled from   Poisson point process of rate   Let   be the output of Multisort using
       log     and comparison outcomes sampled from
BT  Then        

        

Proof  sketch  We use results on the order statistics of the
distances            xn  between successive items  as de 
 ned in   to partition the items into two disjoint subsets  
and    The set   contains   vanishing   log    fraction
of  bad  items that are dif cult to order  The set   is such
that the smallest distance dij from any item       to any
other item         is bounded from below by    log    
We can show that with        log     for any      
and         we have                   in   majority of the Quicksort outputs  with high probability  This
implies that         for all       with high probability 
Using   for items in    we have

             log           log   

with high probability 

Theorem   states that all but   vanishing fraction of items
are correctly ranked using     log     comparisons  This
result should be compared to the     comparisons needed
if samples are selected uniformly at random 

Empirical validation  In Figure   we illustrate the results
of Theorems   and   by running simulations for increasing
  and different values of   The bound on   is tight in
   but the dependence on   appears to be linear rather than
quadratic  The bound on maxi         appears to be tight
in   and   Finally  we compare the Copeland aggregation
of   outputs of Quicksort with the ranking induced by
the maximumlikelihood  ML  estimate  inferred from the
outcomes of all the pairwise comparisons sampled by the  
runs  Although the ranking induced by the ML estimate does
not bene   from the guarantees of Theorem   it performs
better in practice  We will make use of this observation in
Section  

 

Independent UniformlyDistributed Parameters

  different  perhaps more natural  assumption on the parameters   is to consider that they are drawn independently
and uniformly at random over some interval  That is 

                               

with              the order statistics of        the random
variables arranged in increasing order  From some elementary results on the joint distribution of order statistics  see 
     Arnold et al    we see that

                      Beta             

       Beta random variable rescaled between   and     
  Letting fk      be the probability density of       
    we have  for any  xed   and  

fk        xk cid   

  

     cid    

   xk     

We recognize the functional form of the density of  
Gamma      distribution  Hence  the Poisson model and
the        uniform model are essentially equivalent for  xed
  and large    and we can expect the results developed in
Section   to hold under this distribution as well 

  Experimental Results

In practice  the comparison budget for estimating   ranking from noisy data might typically be larger than that for
  single call to Quicksort  and it might not exactly match
the number of comparisons required to run   given number
of calls to Quicksort to completion  Building upon the observations made at the end of Section   we suggest the
following practical activelearning strategy  for   budget of

  Simple and Effective Approach to Active Preference Learning

 

 

     
     
     
     

 
 
 
 
 
 
 
 
 
 

maxi         

     
     
     
     

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 

   

Copeland

ML estimate

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

Number of items  

Number of items  

Number of runs  

Figure   Empirical validation of Theorem   and illustration of Theorem   Every simulation is repeated   times  and we report the
mean and the standard deviation  Left and middle  total and maximum displacement  respectively  for increasing   and different values of
  Right  displacement of the aggregate ranking   for increasing     xing       and       and using two different aggregation rules 

  pairwise comparisons  run the sorting procedure repeatedly until the budget is depleted  the last call might have
to be truncated  Then  retain only the set of   comparison
pairs and their outcomes and discard the rankings produced
by the sorting procedure  The  nal ranking estimate is then
induced from the ML estimate over the set of   comparison
outcomes 

In this section  we demonstrate the effectiveness of this sampling strategy on synthetic and realworld data  In particular 
we show that it is comparable to existing AL strategies at  
minuscule fraction of the computational cost 

maximizes the expected information gain  MacKay   
That is  the pair of items at iteration       is selected in

arg max

   

  qt      cid   qt cid   

 

where    denotes the entropy function    conceptually
similar but slightly different selection strategy is given by
Chen et al    Letting qij be the marginal distribution
of         the pair is selected in

arg max

   

  cid KL qt 

ij kqt

ij cid   

 

  Competing Sampling Strategies

To assess the relative merits of our sortingbased strategy  we
consider three strategies that we believe are representative
of the state of the art in active preference learning 

Uncertainty sampling  Developed in the context of classi 
 cation tasks  this popular activelearning heuristic suggests
to greedily sample the point that lies closest to the decision
boundary  Settles    In the context of   ranking task 
this corresponds to sampling the pair of items whose relative order is most uncertain  After   observations  given an
estimate of model parameters     the strategy selects the
   st pair uniformly at random in

where KL  denotes the Kullback Leibler divergence 
Computing the exact posterior is not analytically tractable
for the BT model  but   Gaussian approximation can be
found in time      Criteria   and   can be computed
in constant time for each pair of items  The dominating cost
is again that of estimating    or  in this case    

In addition to these existing AL strategies  we also include
in our experiments   variation of our sortingbased strategy
that uses Mergesort instead of Quicksort  In the noiseless
setting  Mergesort is known to use on average       fewer
comparisons than Quicksort per run  Knuth    but it
does not bene   from the theoretical guarantees developed
in Section  

arg min

   

  

      

  

  Running Time

This set can be computed in time     log    by sorting the
parameters  The parameters themselves need to be estimated 
     using  penalized  ML inference that in practice can be
the dominating cost 

Bayesian methods  If we have access to   full posterior
distribution qt  instead of   point estimate     we can
take advantage of the extra information on the uncertainty
of the parameters to improve the selection strategy    principled approach to AL consists of sampling the point that

In this section  we brie   discuss the running time of the
methods  We implement ML and Bayesian approximate
inference algorithms for the BT model as   Python library 
For ML inference  we  nd that the fastest running time is
achieved by   truncated Newton algorithm  even for large
   For approximate Bayesian inference  we use   variant
of the expectationpropagation algorithm outlined by Chu
  Ghahramani   All experiments are performed on

 See  http lucas maystre ch choix 

  Simple and Effective Approach to Active Preference Learning

Table   Time  in seconds  to select the       st pair  Values
indicated by   are below   See text for details 

     

Strategy

     

     

     

uncertainty
entropy
KLdivergence
Mergesort
Quicksort
random

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

  server with    core Xeon    processor running at
  GHz  Numerical computations take advantage of the
Intel Math Kernel Library 

We illustrate the running time of AL strategies as follows 
For           we generate outcomes for   comparisons pairs chosen uniformly at random among   items 
For each strategy  we then measure the time it takes to select
the    st pair of items adaptively  The results are presented in Table   Note that these numbers are intended to be
considered as orders of magnitude  rather than exact values 
as they depend on the particular combination of software
and hardware that we use  The running time of the Bayesian
AL strategies exceed   hours for       and the calls
were stopped ahead of completion  Our sortingbased methods  like random sampling  are the only AL strategies whose
running time is constant for increasing    and for increasing
   In fact  their running time is negligible in comparison to
the other strategies  including uncertainty sampling 

  Empirical Evaluation

We now investigate three datasets and measure the displacement of rankings estimated from adaptivelychosen samples 
as   function of the budget    Note that in order to use uncertainty sampling and Bayesian methods  it is necessary
to choose   regularization strength or prior variance in the
inference step  Different values can result in drastically different outcomes  in particular for uncertainty sampling  and 
in practice  choosing   good value can be   signi cant challenge  In the following  we report results for the values that
worked best   posteriori 

Synthetic dataset  We generate          parameters
             uniformly in          and draw samples
from BT  The groundtruth ranking is the one induced
by the parameters  Figure   presents results for      
and        plots for different values of   are presented in
the supplementary material  Section    and are qualitatively

 Observe that our sortingbased approach is entirely parameter 

free and is therefore not affected by this issue 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

uncertainty

entropy

KLdiv

Mergesort

Quicksort

random

 

               

Number of comparisons  

Figure   Synthetic dataset with       and       The experiment is repeated   times  and we report the mean and the
standard deviation  Compared to random sampling  AL results in
signi cantly better rankings for   given budget   

similar  In comparison to random sampling  AL is very
effective and results in signi cantly better ranking estimates
for any given number of comparisons  The two Bayesian
methods  though being the most computationally expensive 
perform the best for all values of    but are nearly indistinguishable from uncertainty sampling  The two sortingbased
strategies perform similarly  with   small edge for Mergesort  They are slightly worse than the Bayesian methods but
are still able to reap most of the bene ts of active learning 

Sushi dataset  Next  we consider   dataset of Sushi preferences  Kamishima   Akaho    In this dataset   
respondents give   strict ordering over   different types
of sushi  These   sushi are chosen among   larger set of
      items  To suit our purposes  we decompose each
 way partial ranking into pairwise comparisons  resulting
in     comparison outcomes  We use all comparisons
to      BT model that induces   groundtruth ranking 

The comparisons are dense  and there is at least one comparison outcome for almost all pairs  When an outcome for
pair        is requested  we sample uniformly at random over
all outcomes observed for this pair  In the rare case where
no outcome is available  we return       with probability
  This enables us to compare sampling strategies in  
realistic setting  where the assumptions of the BT model do
not necessarily hold anymore 

Results are shown in Figure    left  Once again  active learning performs noticeably better than random sampling  On
this realworld dataset  the performance of our sortingbased
strategies is indistinguishable from that of the Bayesian

  The BTinduced ranking is almost the same as that obtained
using the Copeland score  The results are very similar if the
Copeland aggregation is used as ground truth 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

  Simple and Effective Approach to Active Preference Learning

 

GIFGIF dataset

Mergesort

Quicksort

random

Sushi dataset

uncertainty

entropy

KLdiv

Mergesort

Quicksort

random

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Number of comparisons  

Number of comparisons  

 

Figure   Results on two realworld datasets  Every experiment is repeated   times  and we report the mean and the standard deviation 
Left  on the sushi dataset  sortingbased and Bayesian AL strategies have nearidentical performance starting from       Right  on
the GIFGIF dataset  most AL strategies are computationally too expensive except for sortingbased methods 

methods  after completing one entire call to the sorting procedure  slightly less than   comparisons  This result
should be interpreted in light of the time needed to select
all   pairs    fraction of   second for sortingbased strategies  and several hours for the Bayesian methods  Finally 
we observe that the performance of uncertainty sampling
progressively degrades as   increases    detailed analysis
reveals that uncertainty sampling increasingly focuses on
  small set of hardto discriminate pairs  symptomatic of  
wellknown issue  Settles   

GIFGIF dataset  GIFGIF  is   project of the MIT Media
Lab that aims at explaining the emotions communicated
by   collection of animated GIF images  Users of the website are shown   prompt with two images and   question 
 Which better expresses    where   is one of   emotions 
The users can click on either image  or use   third option 
neither  To date  over three million comparison outcomes
have been collected  For the purpose of our experiment  we
restrict ourselves to   single emotion  happiness  and we ignore outcomes that resulted in neither  We consider    
comparison outcomes over       items   signi cant
increase in scale compared to the Sushi dataset 

As the data  despite   relatively large number of comparisons  remains sparse  less than   comparisons per item on
average  we proceed as follows  We      BT model by using
all the available comparisons and use the induced ranking as
ground truth  We then generate new  synthetic comparison
outcomes from the BT model  In this sense  the experiment
enables us to compare sampling strategies by using   large
BT model with realistic parameters  The large number of
items makes uncertainty sampling and the two Bayesian

 See http www gif gf  Data available at http 

lucas maystre ch gifgifdata 

methods prohibitively expensive  We try   simpli ed  computationally less expensive version of uncertainty sampling
where  at every iteration  each item is compared to its two
closest neighbors  but this heuristic fails spectacularly  The
resulting displacement is over   larger than random sampling for       and is therefore not reported here  see
supplementary material  Section   

Figure    right  compares the displacement of random sampling to that of the two sortingbased sampling strategies
for increasing    The adaptive sampling approaches perform
systematically better  After   comparisons  the displacement of random sampling is     and     larger than
that of Quicksort and Mergesort  respectively  Conversely 
in order to reach any target displacement  Mergesort requires approximately   fewer comparisons than random
sampling 

  Conclusion

In this work  we demonstrate that active learning can substantively speed up the task of learning   ranking from
noisy comparisons gains both in theory and in practice 
With the advent of largescale crowdsourced ranking surveys  exempli ed by GIFGIF and wiki surveys  Salganik  
Levy    there is   clear need for practical AL strategies 
However  existing methods are complex and computationally expensive to operate even for   reasonable number of
items    few thousands  We show that   deceptively simple
idea repeatedly sorting the items is able to bring in all
the bene ts of active learning  is trivial to implement  and is
computationally no more expensive that random sampling 
Therefore  we believe that our method can be broadly useful for machinelearning practitioners interested in ranking
problems 

  Simple and Effective Approach to Active Preference Learning

Acknowledgments

Elo     The Rating Of Chess Players  Past   Present  Arco 

We thank Holly CogliatiBauereis  Ksenia Konyushkova 
Brunella Spinelli and the anonymous reviewers for careful
proofreading and helpful comments 

References

Ailon     Reconciling Real Scores with Binary ComparIn Adisons    Uni ed Logistic Model for Ranking 
vances in Neural Information Processing Systems  
Vancouver  BC  Canada   

Ailon     An Active Learning Algorithm for Ranking
from Pairwise Preferences with an Almost Optimal Query
Complexity  Journal of Machine Learning Research   
 Jan   

Ailon     and Mohri     Preferencebased learning to rank 

Machine Learning     

Ailon     Charikar     and Newman     Aggregating Inconsistent Information  Ranking and Clustering  Journal
of the ACM     

Alon     Bollob       Brightwell     and Janson     Linear
Extensions of   Random Partial Order  The Annals of
Applied Probability     

Arnold        Balakrishnan     and Nagaraja          First

Course in Order Statistics  SIAM   

Bradley        and Terry        Rank Analysis of Incomplete
Block Designs     The Method of Paired Comparisons 
Biometrika     

Braverman     and Mossel     Noisy sorting without resampling  In Proceedings of SODA  San Francisco  CA 
 

Chen     Bennett        CollinsThompson     and
Horvitz     Pairwise Ranking Aggregation in   Crowdsourced Setting  In Proceedings of WSDM  Rome 
Italy   

Chu     and Ghahramani     Extensions of Gaussian Processes for Ranking  Semisupervised and Active Learning 
In Proceedings of the NIPS   Workshop on Learning
to Rank  Whistler  BC  Canada   

Copeland           reasonable  social welfare function 

 

Diaconis     and Graham        Spearman   Footrule as
  Measure of Disarray  Journal of the Royal Statistical
Society  Series       

Dubhashi        and Panconesi     Concentration of Measure for the Analysis of Randomized Algorithms  Cambridge University Press   

 

Hajek     Oh     and Xu     Minimaxoptimal Inference
from Partial Rankings  In Advances in Neural Information Processing Systems   Montreal  QC  Canada   

Heckel     Shah        Ramchandran     and Wainwright 
      Active Ranking from Pairwise Comparisons and
when Parametric Assumptions Don   Help  preprint 
arXiv   cs LG  September  

Hoare           Quicksort  The Computer Journal   

   

Houlsby     Husz       Ghahramani     and Hern ndezlobato        Collaborative Gaussian Processes for Preference Learning 
In Advances in Neural Information
Processing Systems   Lake Tahoe  CA   

Jamieson     and Nowak     Active Ranking using Pairwise Comparisons  In Advances in Neural Information
Processing Systems   Granada  Spain   

Kamishima     and Akaho     Ef cient Clustering for Orders 

In Mining Complex Data  pp    Springer   

Knuth        The art of computer programming  sorting
and searching  volume   AddisonWesley   nd edition 
 

MacKay           Bayesian Methods for Adaptive Models 

PhD thesis  California Institute of Technology   

Negahban     Oh     and Shah     Iterative Ranking from
Pairwise Comparisons  In Advances in Neural Information Processing Systems   Lake Tahoe  CA   

Rajkumar     and Agarwal       Statistical Convergence
Perspective of Algorithms for Rank Aggregation from
Pairwise Data  In Proceedings of ICML   Beijing 
China   

Salganik        and Levy           Wiki Surveys  Open and
Quanti able Social Data Collection  PLOS ONE   
   

Salimans     Paquet     and Graepel     Collaborative
In Proceedings of

Learning of Preference Rankings 
RecSys  Dublin  Ireland   

Schein        and Ungar        Active learning for logistic
regression  an evaluation  Machine Learning   
   

Sedgewick     and Wayne     Algorithms  AddisonWesley 

 th edition   

Settles     Active Learning  Morgan   Claypool Publishers 

 

  Simple and Effective Approach to Active Preference Learning

Sz   nyi     BusaFekete     Paul     and   llermeier 
   Online Rank Elicitation for Plackett Luce    Dueling
Bandits Approach  In Advances in Neural Information
Processing Systems   Montreal  QC  Canada   

Thurstone         Law of Comparative Judgment  Psycho 

logical Review     

Vojnovic     and Yun     Parameter Estimation for Generalized Thurstone Choice Models  In Proceedings of ICML
  New York  NY   

Wang     Srebro     and Evans     Active Collaborative
Permutation Learning  In Proceedings of KDD  New
York  NY   

Yue     Broder     Kleinberg     and Joachims     The karmed dueling bandits problem  In Proceedings of COLT
  Montreal  QC  Canada   

Zermelo     Die Berechnung der TurnierErgebnisse als
ein Maximumproblem der Wahrscheinlichkeitsrechnung 
Mathematische Zeitschrift     

