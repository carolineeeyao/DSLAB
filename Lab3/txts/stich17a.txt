Approximate Steepest Coordinate Descent

Sebastian    Stich   Anant Raj   Martin Jaggi  

Abstract

We propose   new selection rule for the coordinate selection in coordinate descent methods
for hugescale optimization  The ef ciency of
this novel scheme is provably better than the ef 
ciency of uniformly random selection  and can
reach the ef ciency of steepest coordinate descent  SCD  enabling an acceleration of   factor
of up to    the number of coordinates  In many
practical applications  our scheme can be implemented at no extra cost and computational ef 
ciency very close to the faster uniform selection 
Numerical experiments with Lasso and Ridge regression show promising improvements  in line
with our theoretical guarantees 

  Introduction
Coordinate descent  CD  methods have attracted   substantial interest the optimization community in the last few
years  Nesterov    Richt arik   Tak        Due to
their computational ef ciency  scalability  as well as their
ease of implementation  these methods are the stateof theart for   wide selection of machine learning and signal processing applications  Fu    Hsieh et al    Wright 
  This is also theoretically well justi ed  The complexity estimates for CD methods are in general better than
the estimates for methods that compute the full gradient in
one batch pass  Nesterov    Nesterov   Stich   
In many CD methods 
the active coordinate is picked
at random  according to   probability distribution  For
smooth functions it is theoretically well understood how
the sampling procedure is related to the ef ciency of the
scheme and which distributions give the best complexity
estimates  Nesterov    Zhao   Zhang    AllenZhu et al    Qu   Richt arik    Nesterov   Stich 
  For nonsmooth and composite functions   that
appear in many machine learning applications   the pic 

 EPFL  Max Planck Institute for Intelligent Systems  Corre 

spondence to  Sebastian    Stich  sebastian stich ep ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ture is less clear 
For instance in  ShalevShwartz  
Zhang    Friedman et al      ShalevShwartz
  Tewari    uniform sampling  UCD  is used  whereas
other papers propose adaptive sampling strategies that
change over time  Papa et al    Csiba et al    Osokin et al    Perekrestenko et al   
  very simple deterministic strategy is to move along the
direction corresponding to the component of the gradient
with the maximal absolute value  steepest coordinate descent  SCD   Boyd   Vandenberghe    Tseng   Yun 
  For smooth functions this strategy yields always
better progress than UCD  and the speedup can reach   factor of the dimension  Nutini et al    However  SCD
requires the computation of the whole gradient vector in
each iteration which is prohibitive  except for special applications  cf  Dhillon et al    Shrivastava   Li  
In this paper we propose approximate steepest coordinate
descent  ASCD    novel scheme which combines the best
parts of the aforementioned strategies      ASCD maintains
an approximation of the full gradient in each iteration and
selects the active coordinate among the components of this
vector that have large absolute values   similar to SCD 
and  ii  in many situations the gradient approximation can
be updated cheaply at no extra cost   similar to UCD  We
show that regardless of the errors in the gradient approximation  even if they are in nite  ASCD performs always
better than UCD 
Similar to the methods proposed in  Tseng   Yun   
we also present variants of ASCD for composite problems 
We con rm our theoretical  ndings by numerical experiments for Lasso and Ridge regression on   synthetic dataset
as well as on the RCV   binary  dataset 

Structure of the Paper and Contributions 
In Sec    we
review the existing theory for SCD and     extend it to the
setting of smooth functions  We present  ii    novel lower
bound  showing that the complexity estimates for SCD and
UCD can be equal in general  We  iii  introduce ASCD
and the save selection rules for both smooth  Sec    and
to composite functions  Sec    We prove that  iv  ASCD
performs always better than UCD  Sec    and     it can
reach the performance of SCD  Sec    In Sec    we discuss important applications where the gradient estimate can
ef ciently be maintained  Our theory is supported by nu 

Approximate Steepest Coordinate Descent  ASCD 

merical evidence in Sec    which reveals that  vi  ASCD
performs extremely well on real data 

   cid    ei cid  with ei the standard
Notation  De ne     
unit vectors in Rn  We abbreviate  if           
convex function     Rn     with coordinatewise LiLipschitz continuous gradients  for constants Li    
                       satis es by the standard reasoning
 
for all     Rn and          function is coordinatewise
Lsmooth if Li     for                  For an optimization
problem minx Rn       de ne    cid    arg minx Rn      
and denote by   cid    Rn an arbitrary element   cid       cid 

        ei             if       Li

   

  Steepest Coordinate Descent
In this section we present SCD and discuss its theoretical
properties  The functions of interest are composite convex
functions     Rn     of the form

 

                   

separable  that is that is        cid  

where   is coordinatewise Lsmooth and   convex and
           In the
 rst part of this section we focus on smooth problems      
we assume that      
Coordinate descent methods with constant step size generate   sequence  xt    of iterates that satisfy the relation
 

xt    xt    

  itf    eit  

In UCD the active coordinate it is chosen uniformly at random from the set     it             SCD chooses the coordinate according to the GaussSouthwell  GS  rule 

it   arg max

    

       xt   

 

  Convergence analysis

With the quadratic upper bound   one can easily get  
lower bound on the one step progress
      xt       xt    xt    Eit
For UCD and SCD the expression on the right hand side
evaluates to

    itf  xt cid 
 cid   

 

 

 UCD xt     
 SCD xt     

 nL  cid    xt cid 
    cid    xt cid 

 

 

With CauchySchwarz we  nd

 

   SCD xt     UCD xt     SCD xt   

 if       ei     if       Li    

 
     Rn        

Hence  the lower bound on the one step progress of SCD is
always at least as large as the lower bound on the one step
progress of UCD  Moreover  the one step progress could
be even lager by   factor of    However  it is very dif cult
to formally prove that this linear speedup holds for more
than one iteration  as the expressions in   depend on the
   priori unknown  sequence of iterates  xt   

Strongly Convex Objectives  Nutini et al 
 
present an elegant solution of this problem for  strongly
convex functions  They propose to measure the strong
convexity of the objective function in the  norm instead
of the  norm  This gives rise to the lower bound
      xt         cid   

 SCD xt     

 

where   denotes the strong convexity parameter  By this 
they get   uniform upper bound on the convergence that
does not directly depend on local properties of the function 
like for instance  SCD xt  but just on   It always holds
      and for functions where both quantities are equal 
SCD enjoys   linear speedup over UCD 

Smooth Objectives  When the objective function   is
just smooth  but not necessarily strongly convex  then the
analysis mentioned above is not applicable  We here extend
the analysis from  Nutini et al    to smooth functions 
Theorem   Let     Rn     be convex and coordinatewise Lsmooth  Then for the sequence  xt    generated
by SCD it holds 

   xt         cid     LR 
 cid 

 cid 
 cid       cid cid                 

 

 

 

 

 

for      max
  cid    cid 

max
  Rn

Proof  In the proof we  rst derive   lower bound on the one
step progress  Lemma    similar to the analysis in  Nesterov    The lower bound for the one step progress of
SCD can in each iteration differ up to   factor of   from the
analogous bound derived for UCD  similar as in   All
details are given in Section    in the appendix 

Note that the    is essentially the diameter of the level set
at       measured in the  norm  In the complexity esti 
  where    is
mate of UCD    
the diameter of the level at       measured in the  norm
 cf  Nesterov   Wright   As in   we observe
with CauchySchwarz

  in   is replaced by nR 

      

      
   

 

    

 

     SCD can accelerate up to   factor of   over to UCD 

   function is  pstrongly convex in the pnorm        if
           Rn 

                 cid             cid      

   cid       cid 

Approximate Steepest Coordinate Descent  ASCD 

  Lower bounds

In the previous section we provided complexity estimates
for the methods SCD and UCD and showed that SCD can
converge up to   factor of the dimension   faster than UCD 
In this section we show that this analysis is tight  In Theorem   below we give   function     Rn      for which
the one step progress  SCD xt     UCD xt  up to   constant factor  for all iterates  xt    generated by SCD 
By   simple technique we can also construct functions for
which the speedup is exactly equal to an arbitrary factor
         For instance we can consider functions with
   separable  low dimensional structure  Fix integers     
such that  

      de ne the function     Rn     as

              

 
where    denotes the projection to Rs  being the  rst  
out of   coordinates  and     Rs     is the function from
Theorem   Then

 SCD xt         UCD xt   
for all iterates  xt    generated by SCD 
 cid Qx    cid  for
Theorem   Consider the function         
    In    
          Then
there exists      Rn such that for the sequence  xt   
generated by SCD it holds

   Jn  where Jn       

 

 cid   xt cid     

   cid   xt cid 
   

 

Proof  In the appendix we discuss   family of functions
de ned by matrices            
  Jn   In and de ne
corresponding parameters            such that for    de 
 ned as        ci 
for                  SCD cycles through
the coordinates  that is  the sequence  xt    generated by
SCD satis es

 

 xt    mod      cn

     xt    mod     
We verify that for this sequence property   holds 

 

  Composite Functions

The generalization of the GS rule   to composite problems   with nontrival   is not straight forward  The
 steepest  direction is not always meaningful in this setting 
consider for instance   constrained problem where this rule
could yield no progress at all when stuck at the boundary 
Nutini et al    discuss several generalizations of the
GaussSouthwell rule for composite functions  The GSs rule is de ned to choose the coordinate with the most
negative directional derivative  Wu   Lange    This
rule is identical to   but requires the calculation of subgradients of     However  the length of   step could be

arbitrarily small  In contrast  the GSr rule was de ned to
pick the coordinate direction that yields the longest step
 Tseng   Yun    The rule that enjoys the best theoretical properties  cf  Nutini et al    is the GSq rule 
which is de ned as to maximize the progress assuming  
quadratic upper bound on    Tseng   Yun    Consider the coordinatewise models

Vi            sy    

                    
for         The GSq rule is formally de ned as
    Vi      if      

    

iGS     arg min

min

 

 

  The Complexity of the GS rule

So far we only studied the iteration complexity of SCD 
but we have disregarded the fact that the computation of
the GS rule   can be as expensive as the computation
of the whole gradient  The application of coordinate descent methods is only justi ed if the complexity to compute
one directional derivative is approximately   times cheaper
than the computation of the full gradient vector  cf  Nesterov   By Theorem   this reasoning also applies
to SCD    class of function with this property is given by
functions     Rn    

  cid 

           Ax   

       

 

  

where   is         matrix  and where     Rd     
and      Rn     are convex and simple  that is the
time complexity   for computing their gradients is linear 
   yf                    This class of functions includes least squares  logistic regression  Lasso  and
SVMs  when solved in dual form 
Assuming the matrix is dense  the complexity to compute
the full gradient of   is    xF         dn  If the value
    Ax is already computed  one directional derivative
can be computed in time    iF            The recursive update of   after one step needs the addition of one
column of matrix   with some factors and can be done in
time      However  we note that recursively updating the
full gradient vector takes time   dn  and consequently the
computation of the GS rule cannot be done ef ciently 
Nutini et al    consider sparse matrices  for which the
computation of the GaussSouthwell rule becomes traceable  In this paper  we propose an alternative approach  Instead of updating the exact gradient vector  we keep track
of an approximation of the gradient vector and recursively
update this approximation in time     log    With these
updates  the use of coordinate descent is still justi ed in
case        

Approximate Steepest Coordinate Descent  ASCD 

Algorithm   Approximate SCD  ASCD 

Input             gradient oracle    method  
Initialize                   for        
for       to   do

 cid cid cid            ut 

    cid   
 

For         de ne
compute   and   bounds
 ut     max gt      rt     gt      rt   
 cid 
 cid       miny         gt   rt          gt   rt   
av         
compute active set
It   arg minI
Pick it         arg maxi It cid   
      xt it    xt it
Update  gt      gt      tgitj xt     cid  it
Update  rt      rt        itj     cid  it

    av         cid cid cid 

 xt   gt it   rt it      xt itf  xt 

update     xt  estimate

active coordinate

end for

  Algorithm
Is it possible to get the signi cantly improved convergence
speed from SCD  when one is only willing to pay the computational cost of only the much simpler UCD  In this section  we give   formal de nition of our proposed approximate SCD method which we denote ASCD 
The core idea of the algorithm is the following  While performing coordinate updates  ideally we would like to ef 
ciently track the evolution of all elements of the gradient 
not only the one coordinate which is updated in the current step  The formal de nition of the method is given in
Algorithm   for smooth objective functions  In each iteration  only one coordinate is modi ed according to some
arbitrary update rule    The coordinate update rule  
provides two things  First the new iterate xt  and secondly also an estimate    of the itth entry of the gradient at
the new iterate  Formally 

 xt             xt itf  xt 

 

such that the quality of the new gradient estimate    satis es

 itf  xt             

 

The nonactive coordinates are updated with the help of
gradient oracles with accuracy        see next subsection
for details  The scenario of exact updates of all gradient
entries is obtained for accuracy parameters           and
in this case ASCD is identical to SCD 

  Safe bounds for gradient evolution

ASCD maintains lower and upper bounds for the absolute values of each component of the gradient  cid    
 For instance  for updates by exact coordinate optimization

 linesearch  we have           

 if             These bounds allow to identify the coordinates on which the absolute values of the gradient are
small  and hence cannot be the steepest one  More precisely  the algorithm maintains   set It of active coordinates
 similar in spirit as in active set methods  see      Kim  
Park   Wen et al      coordinate   is excluded
from It if the estimated progress in this direction  cf   
is lower than the average of the estimated progress along
coordinate directions in It   ut 
    The
active set It can be computed in     log    time by sorting  All other operations take linear      time 

     It 

 cid 

 cid   

  It

Gradient Oracle  The selection mechanism in ASCD
crucially relies on the following de nition of    gradient
oracle  While the update   delivers the estimated active
entry of the new gradient  the additional gradient oracle is
used to update all other coordinates    cid  it of the gradient 
as in the last two lines of Algorithm  
De nition    gradient oracle  For   function
    Rn     and indices                    gradient oracle
with error  ij     is   function gij   Rn     satisfying
     Rn      

 jf       ei     gij         ij  

 
We denote by    gradient oracle   family  gij        of
 ijgradient oracles 

We discuss the availability of good gradient oracles for
many problem classes in more detail in Section   For example for least squares problems and general linear models 
   gradient oracle is for instance given by   scalar product
estimator as in   below  Note that ASCD can also handle
very bad estimates  as long as the property   is satis ed
 possibly even with accuracy  ij    

Initialization 
In ASCD the initial estimate     of the gradient is just arbitrarily set to   with uncertainty       
Hence in the worst case it takes    log    iterations until each coordinate gets picked at least once  cf  Dawkins
  and until corresponding gradient estimates are set
to   realistic value  If better estimates of the initial gradient
are known  they can be used for the initialization as long
as   strong error bound as in   is known as well  For
instance the initialization can be done with        if one
is willing to compute this vector in one batch pass 

Convergence Rate Guarantee  We present our  rst
main result showing that the performance of ASCD is provably between UCD and SCD  First observe that if in Algorithm   the gradient oracle is always exact        ij    
and if     is initialized with        then in each iteration
 itf  xt     cid    xt cid  and ASCD identical to SCD 
Lemma   Let imax   arg maxi     if  xt  Then
imax   It  for It as in Algorithm  

Approximate Steepest Coordinate Descent  ASCD 

Proof  This is immediate from the de nitions of It and the
upper and lower bounds  Suppose imax   It  then there exists    cid  imax such that  cid        ut imax  and consequently
 jf  xt     imaxf  xt 
Theorem   Let     Rn     be convex and coordinatewise Lsmooth  let  UCD   SCD   ASCD denote the expected
one step progress   of UCD  SCD and ASCD  respectively  and suppose all methods use the same stepsize
rule    Then

 

 UCD       ASCD       SCD         Rn  

 cid 
     if    
Proof  By   we get  ASCD       
where   denotes the corresponding index set of ASCD
 cid 
when at iterate    Note that for       it must hold that
     if    
 jf          
by de nition of   

       

       

     

 cid 

    cid 

Observe that the above theorem holds for all gradient oracles and coordinate update variants  as long as they are
used with corresponding quality parameters    as in  
and  ij  as in   as part of the algorithm 

Heuristic variants  Below also propose three heuristic
variants of ASCD  For all these variants the active set It
can be computed      but the statement of Theorem  
does not apply  These variants only differ from ASCD in
the choice of the active set in Algorithm  
uASCD  It   arg maxi   ut  
 cid ASCD  It   arg maxi   cid    

aASCD  It  cid            ut     maxi   cid    

 cid 

  Approximate Gradient Update
In this section we argue that for   large class of objective
functions of interest in machine learning  the change in the
gradient along every coordinate direction can be estimated
ef ciently 
Lemma   Consider     Rn     as in   with
twicedifferentiable     Rd      Then for two iterates
xt  xt    Rn of   coordinate descent algorithm      
xt    xt    teit  there exists        Rn on the line
segment between xt and xt        xt  xt  with
 iF  xt     iF  xt       cid ai         ait cid      cid  it
 
where ai denotes the ith column of the matrix   
Proof  For coordinates    cid  it the gradient  or subgradient
set  of    xt    does not change  Hence it suf ces to calculate the change     xt        xt  This is detailed in
the appendix 

LeastSquares with Arbitrary Regularizers  The least
squares problem is de ned as problem   with    Ax   
   cid Ax     cid 
  for       Rd  This function is twice differ 
 
entiable with     Ax    In  Hence   reduces to
 iF  xt     iF  xt       cid ai  ait cid      cid  it  

 

 

           

This formulation gives rise to various gradient oracles  
for the least square problems  For for    cid  it we easily
verify that the condition   is satis ed 
    
    

ij    cid ai  ait cid   ij    
ij   max cid ai cid cid aj cid    min        cid ai cid cid aj cid 
 ij    cid ai cid cid aj cid  where           denotes   function
with the property
            cid ai  aj cid     cid ai cid cid aj cid   
ij      ij    cid ai cid cid aj cid 
    
ij          cid ai cid cid aj cid   cid ai cid cid aj cid   ij    cid ai cid cid aj cid 
    
Oracle    can be used in the rare cases where the dot product matrix is accessible to the optimization algorithm without any extra cost  In this case the updates will all be exact 
If this matrix is not available  then the computation of each
scalar product takes time      Hence  they cannot be recomputed on the     as argued in Section   In contrast 
the oracles    and    are extremely cheap to compute  but
the error bounds are worse  In the numerical experiments
in Section   we demonstrate that these oracles perform surprisingly well 
The oracle    can for instance be realized by lowdimensional embeddings  such as given by the JohnsonLindenstrauss lemma  cf  Achlioptas   Matou sek
  By embedding each vector in   lowerdimensional

space of dimension   cid  log   cid  and computing the scalar

products of the embedding in time   log    relation  
is satis ed 

Updating the gradient of the active coordinate  So far
we only discussed the update of the passive coordinates 
For the active coordinate the best strategy depends on the
update rule   from   If exact line search is used  then
     itf  xt  For other update rules we can update the
gradient  itf  xt  with the same gradient oracles as for
the other coordinates  however we need also to take into
account the change of the gradient of    xt    If    is
simple  like for instance in ridge or lasso  the subgradients
at the new point can be computed ef ciently 

Bounded variation 
In many applications the Hessian
          is not so simple as in the case of square loss 
If we assume that
    
    Ax   cid      In for   constant            Rn 
then it is easy to see that the following holds  
   cid ai cid cid aj cid     cid ai         ait cid      cid ai cid cid aj cid   

the Hessian of   is bounded 

Approximate Steepest Coordinate Descent  ASCD 

Using this relation  we can de ne gradient oracles for more
general functions  by taking the additional approximation
factor   into account  The quality can be improved  if we
have access to local bounds on     Ax 

Heuristic variants  By design  ASCD is robust to high
errors in the gradient estimations   the steepest descent direction is always contained in the active set  However  instead of using only the very crude oracle    to approximate
all scalar products  it might be advantageous to compute
some scalar products with higher precision  We propose to
use   caching technique to compute the scalar products with
high precision for all vectors in the active set  and storing  
matrix of size   It      This presumably works well if
the active set does not change much over time 

  Extension to Composite Functions
The key ingredients of ASCD are the coordinatewise upper and lower bounds on the gradient and the de nition of
the active set It which ensures that the steepest descent direction is always kept and that only provably bad directions
are removed from the active set  These ideas can also be
generalized to the setting of composite functions   We
already discussed some popular GS  update rules in the
introduction in Section  
Implementing ASCD for the GSs rule is straight forward 
and we comment on the GSr in the appendix in Sec    
Here we exemplary detail the modi cation for the GSq
rule   which turns out to be the most evolved  the same
reasoning also applies to the GSLq rule from  Nutini et al 
  In Algo    we show the construction   based just
on approximations of the gradient of the smooth part    
of the active set    For this we compute upper and lower
bounds      on miny           if     such that
            if                      

       min

 

The selection of the active coordinate is then based on these
bounds  Similar as in Lemma   and Theorem   this set
has the property iGS        and directions are only discarded in such   way that the ef ciency of ASCDq cannot
drop below the ef ciency of UCD  The proof can be found
in the appendix in Section   

  Analysis of Competitive Ratio
In Section   we derived in Thm    that the one step
progress of ASCD is between the bounds on the onestep
progress of UCD and SCD  However  we know that the ef 
 ciency of the latter two methods can differ much  up to
  factor of    In this section we will argue that in certain
cases where SCD performs much better than UCD  ASCD
will accelerate as well  To measure this effect  we could for

Algorithm   Adaptation of ASCD for GSq rule

Input  Gradient estimate     error bounds   
For         de ne 
                     cid                
   cid     arg miny                 
 cid cid     arg miny             cid   

compute   and   bounds

minimize the model

compute   and    bounds on miny           if    
                cid         max     cid   cid          
 cid            cid cid     cid      max   cid cid          cid   
       min          cid                 cid cid     cid   
       min       cid           
av         
It   arg minI                   av         

compute active set

 cid 

       

 It      

instance consider the ratio 

 cid cid cid     It    if  xt     

    

 It 

   cid    xt cid 

 cid cid cid 

 

 

For general functions this expression is   bit cumbersome
to study  therefore we restrict our discussion to the class
of objective functions   as introduced in Sec    Of
course not all realworld objective functions will fall into
this class  however this problem class is still very interesting in our study  as we will see in the following  because it
will highlight the ability  or disability  of the algorithms to
eventually identify the right set of  active  coordinates 
For the functions with the structure    and   as in
Thm    the active set falls into the  rst   coordinates 
Hence it is reasonable to approximate    by the competitive ratio

 

    

 It 

 
It is also reasonable to assume that in the limit        
  constant fraction of the     will be contained in the active
set It  it might not hold       It     as for instance with
exact line search the directional derivative vanishes just after the update  In the following theorem we calculate   
for        the proof is given in the appendix 
Theorem   Let     Rn     be of the form  
For indices         de ne Ki
           It      It 
For     Ki de ne    
  min             Ij        
the number of iterations outside the active set        
limt  Ej Ki
Ei    
If there exists   constant       such that
limt        It    cs  then  with the notation    
limt       

 cid    
          cid  and the average     

 cid     cid 

 

   

 cs

 

cs               

 
where                                         
      sT        Especially             
    

 

 

Approximate Steepest Coordinate Descent  ASCD 

justi ed if   is large  for instance      
     Otherwise the
convergence on   is too fast  and the gradient approximations are too weak  However  notice that we assumed   to
be an uniform bound on all errors  If the errors have large
discrepancy the estimates become much better  this holds
for instance on datasets where the norm data vectors differs
much  or when caching techniques as mentioned in Sec   
are employed 

Figure   Competitive ratio     blue  in comparison with    
 red  and the lower bound          
    black  Simulation for
parameters                   and           

In Figure   we compare the lower bound   of the competitive ratio in the limit        with actual measurements of    for simulated example with parameters    
              and various           
We initialized the active set          but we see that the
equilibrium is reached quickly 

  Estimates of the competitive ratio

Based on this Thm    we can now estimate the competitive ratio in various scenarios  On the class   it holds
      as we argued before  Hence the competitive ratio   just depends on    This quantity measures how
many iterations   coordinate         is in average outside
of the active set It  From the lower bound we see that the
competitive ratio    approaches   constant for        if
           for instance       if         
As an approximation to    we estimate the quantities    
  
de ned in Thm       
   denotes the number of iterations
it takes until coordinate   enters the active set again  assuming it left the active set at iteration        We estimate
          where    denotes maximum number of iterations
   
      cid 
such that

  cid 

 cid 

        

 

 cid cid cid kf

   iij    
 

    

  

 cid cid cid cid 

xt    

For smooth functions  the steps         itf  xt  and if
we additionally assume that the errors of the gradient oracle
are uniformly bounded  ij     the sum in   simpli es

    

 itf  xt 

to  cid      
of   or    and we get       cid   
 cid 
ratio is constant for      cid   

 

 

For smooth  but not strongly convex function    the norms
of the gradient changes very slowly  with   rate independent

 cid  Hence  the competitive

For strongly convex function    the norm of the gradient
decreases linearly  say  cid    xt cid 
        
it decreases by half after each       iterations  Therefore
to guarantee            it needs to hold         
   
This result seems to indicate that the use of ACDM is only

        for      

  Empirical Observations
In this section we evaluate the empirical performance of
ASCD on synthetic and real datasets  We consider the following regularized general linear models 
 cid   cid 
     
   
     cid   cid   

 cid Ax     cid 
 cid Ax     cid 

 

 

min
  Rn
min
  Rn

 

 

that is    regularized least squares   as well as   
regularized linear regression  Lasso  in   respectively 
Datasets  The datasets     Rd   in problems  
and   were chosen as follows for our experiments  For
the synthetic data  we follow the same generation procedure as described in  Nutini et al    which generates very sparse data matrices  For completeness  full details of the data generation process are also provided in
the appendix in Sec     For the synthetic data we choose
      for problem   and       for problem  
Dimension       is  xed for both cases 
For real datasets  we perform the experimental evaluation
on RCV   binary training  which consists of     samples  each of dimension      Lewis et al    We
use the unnormalized version with all nonzeros values set
to    bagof words features 

Gradient oracles and implementation details  On the
RCV  dataset  we approximate the scalar products with
the oracle    that was introduced in Sec    This oracle
is extremely cheap to compute  as the norms  cid ai cid  of the
columns of   only need to be computed once 
On the synthetic data  we simulate the oracle    for various
precisions values   For this  we sample   value uniformly
at random from the allowed error interval   Figs    
and    show the convergence for different accuracies 
For the   regularized problems  we used ASCD with the
GSs rule  the experiments in  Nutini et al    revealed
almost identical performance of the different GS  rules 
We compare the performance of UCD  SCD and ASCD 
We also implement the heuristic version aASCD that was
introduced in Sec    All algorithm variants use the same
step size rule       the method   in Algorithm   We use
exact line search for the experiment in Fig      for all others we used    xed step size rule  the convergence is slower

 epochs       measuredexactlower bound epochs       epochs       Approximate Steepest Coordinate Descent  ASCD 

    Convergence for   

    Convergence for   

    True vs No Initialization for   

    Error Variation  ASCD 

Figure   Experimental results on synthetically generated datasets

    Convergence for   

    Convergence for   

    Line search for   

    Error Variation  ASCD 

Figure   Experimental results on the RCV binary dataset

for all algorithms  but the different effects of the selection
of the active coordinate is more distinctly visible 
ASCD is either initialized with the true gradient  Figs     
                or arbitrarely  with error bounds       in
Figs     and     Fig     compares both initializations 
Fig    shows results on the synthetic data  Fig    on the
RCV  dataset  All plots show also the size of the active
set It  The plots    and    are generated on   subspace
of RCV  with   and   randomly chosen columns 
respectively 
Here are the highlights of our experimental study 
  No initialization needed  We observe  see     
Figs            that initialization with the true gradient
values is not needed at beginning of the optimization
process  the cost of the initialization being as expensive
as one epoch of ASCD  Instead  the algorithm performs
strong in terms of learning the active set on its own  and
the set converges very fast after just one epoch 

  High errors toleration  The gradient oracle    gives
very crude approximations  however the convergence of
ASCD is excellent on RCV   Fig    Here the size of
the true active set is very small  in the order of   on
RCV  and ASCD is able to identify this set  Fig    
shows that almost nothing can be gained from more precise  and more expensive  oracles 

  Heuristic aASCD performs well  The convergence
behavior of ASCD follows theory  For the heuristic version aASCD  which computes the active set slightly

faster  but Thm    does not hold  performs identical
to ASCD in practice  cf  Figs      and sometimes
slightly better  This is explained by the active set used
in ASCD typically being larger than the active set of aASCD  Figs               

  Concluding Remarks
We proposed ASCD    novel selection mechanism for the
active coordinate in CD methods  Our scheme enjoys
three favorable properties      its performance can reach
the performance steepest CD   both in theory and practice   ii  the performance is never worse than uniform CD 
 iii  in many important applications  the scheme it can be
implemented at no extra cost per iteration 
ASCD calculates the active set in   safe manner  and picks
the active coordinate uniformly at random from this smaller
set  It seems possible that an adaptive sampling strategy
on the active set could boost the performance even further 
Here we only study CD methods where   single coordinate
gets updated in each iteration  ASCD can immediately also
be generalized to blockcoordinate descent methods  However  the exact implementation in   distributed setting can
be challenging 
Finally  it is an interesting direction to extend ASCD also
to the stochastic gradient descent setting  not only heuristically  but with the same strong guarantees as derived in this
paper 

Approximate Steepest Coordinate Descent  ASCD 

References
Achlioptas  Dimitris  Databasefriendly random projections 
Johnsonlindenstrauss with binary coins  Journal of Computer
and System Sciences         

AllenZhu     Qu     Richtarik     and Yuan     Even faster accelerated coordinate descent using nonuniform sampling   

Boyd  Stephen   and Vandenberghe  Lieven  Convex optimiza 

tion  Cambridge University Press   

Csiba  Dominik  Qu  Zheng  and Richt arik  Peter  Stochastic Dual
Coordinate Ascent with Adaptive Probabilities  In ICML  
  Proceedings of the  th International Conference on Machine
Learning   

Dawkins  Brian  Siobhan   problem  The coupon collector revis 

ited  The American Statistician     

Dhillon  Inderjit    Ravikumar  Pradeep  and Tewari  Ambuj 
Nearest Neighbor based Greedy Coordinate Descent  In NIPS
    Advances in Neural Information Processing Systems  
 

Nutini  Julie  Schmidt  Mark    Laradji  Issam    Friedlander 
Michael    and Koepke  Hoyt    Coordinate Descent Converges Faster with the GaussSouthwell Rule Than Random
Selection  In ICML  pp     

Osokin  Anton  Alayrac  JeanBaptiste  Lukasewitz  Isabella 
Dokania  Puneet    and LacosteJulien  Simon  Minding the
gaps for block frankwolfe optimization of structured svms 
In Proceedings of the  rd International Conference on International Conference on Machine Learning   Volume  
ICML  pp    PMLR   

Papa  Guillaume  Bianchi  Pascal  and Cl emenc on  St ephan 
Adaptive Sampling for
Incremental Optimization Using
Stochastic Gradient Descent  ALT      th International
Conference on Algorithmic Learning Theory  pp   
 

Perekrestenko  Dmytro  Cevher  Volkan  and Jaggi  Martin  Faster
Coordinate Descent via Adaptive Importance Sampling 
In
Proceedings of the  th International Conference on Arti cial
Intelligence and Statistics  volume   of Proceedings of Machine Learning Research  pp    Fort Lauderdale  FL 
USA    Apr   PMLR 

Friedman  Jerome  Hastie  Trevor      ing  Holger  and Tibshirani  Robert  Pathwise coordinate optimization  The Annals of
Applied Statistics     

Qu  Zheng and Richt arik  Peter  Coordinate descent with arbitrary
sampling    algorithms and complexity  Optimization Methods
and Software     

Friedman  Jerome  Hastie  Trevor  and Tibshirani  Robert  Regularization Paths for Generalized Linear Models via Coordinate
Descent  Journal of Statistical Software     

Richt arik  Peter and Tak      Martin  Parallel coordinate descent
methods for big data optimization  Mathematical Programming     

Fu  Wenjiang    Penalized regressions  The bridge versus the
lasso  Journal of Computational and Graphical Statistics   
   

ShalevShwartz  Shai and Tewari  Ambuj  Stochastic Methods
for   regularized Loss Minimization  JMLR   
 

Hsieh  ChoJui  Chang  KaiWei  Lin  ChihJen  Keerthi 
  Sathiya  and Sundararajan       Dual Coordinate Descent
Method for Largescale Linear SVM  In the  th International
Conference on Machine Learning  pp    New York 
USA   

Kim  Hyunsoo and Park  Haesun  Nonnegative matrix factorization based on alternating nonnegativity constrained least
squares and active set method  SIAM Journal on Matrix Analysis and Applications     

Lee  Daniel   and Seung    Sebastian  Learning the parts of objects by nonnegative matrix factorization  Nature   
   

Lewis  David    Yang  Yiming  Rose  Tony    and Li  Fan 
Rcv    new benchmark collection for text categorization research     Mach  Learn  Res     

Matou sek  Ji    On variants of the johnsonlindenstrauss lemma 

Random Structures   Algorithms     

Nesterov  Yu  Ef ciency of coordinate descent methods on hugescale optimization problems  SIAM Journal on Optimization 
   

Nesterov  Yurii and Stich  Sebastian    Ef ciency of the accelerated coordinate descent method on structured optimization problems  SIAM Journal on Optimization   
 

ShalevShwartz  Shai and Zhang  Tong  Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization 
JMLR     

Shrivastava  Anshumali and Li  Ping  Asymmetric LSH  ALSH 
for sublinear time maximum inner product search  MIPS  In
NIPS     Advances in Neural Information Processing Systems   pp     

Tseng  Paul and Yun  Sangwoon    coordinate gradient descent
method for nonsmooth separable minimization  Mathematical
Programming     

Wen  Zaiwen  Yin  Wotao  Zhang  Hongchao  and Goldfarb  Donald  On the convergence of an activeset method for   minimization  Optimization Methods and Software   
   

Wright  Stephen    Coordinate descent algorithms  Mathematical

Programming     

Wu  Tong Tong and Lange  Kenneth  Coordinate descent algorithms for lasso penalized regression  Ann  Appl  Stat   
   

Zhao  Peilin and Zhang  Tong  Stochastic optimization with importance sampling for regularized loss minimization  In Proceedings of the  nd International Conference on Machine
Learning  volume   of PMLR  pp    Lille  France   
PMLR 

