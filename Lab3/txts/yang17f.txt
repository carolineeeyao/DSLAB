  Richer Theory of Convex Constrained Optimization

with Reduced Projections and Improved Rates

Tianbao Yang   Qihang Lin   Lijun Zhang  

Abstract

This paper focuses on convex constrained optimization problems  where the solution is subject
to   convex inequality constraint  In particular 
we aim at challenging problems for which both
projection into the constrained domain and   linear optimization under the inequality constraint
are timeconsuming  which render both projected
gradient methods and conditional gradient methods        
the FrankWolfe algorithm  expensive  In this paper  we develop projection reduced
optimization algorithms for both smooth and
nonsmooth optimization with improved convergence rates under   certain regularity condition
of the constraint function  We  rst present   general theory of optimization with only one projection 
Its application to smooth optimization
with only one projection yields    iteration
complexity  which improves over the   
iteration complexity established before for nonsmooth optimization and can be further reduced
under strong convexity  Then we introduce   local error bound condition and develop faster algorithms for nonstrongly convex optimization at
the price of   logarithmic number of projections 
In particular  we achieve an iteration complex 

ity of  cid    for nonsmooth optimization
and  cid    for smooth optimization  where

        appearing the local error bound condition characterizes the functional local growth
rate around the optimal solutions  Novel applications in solving the constrained  cid  minimization
problem and   positive semide nite constrained
distance metric learning problem demonstrate
that the proposed algorithms achieve signi cant
speedup compared with previous algorithms 

 The University of Iowa  Iowa City  IA   USA  National
Key Laboratory for Novel Software Technology  Nanjing University  Nanjing   China  Correspondence to  Tianbao Yang
 tianbaoyang uiowa edu 

This is the long version of our paper appearing in the Proceedings
of the   th International Conference on Machine Learning  Sydney  Australia  PMLR     Copyright   by the author   

  Introduction
In this paper  we aim at solving the following convex constrained optimization problem 

min
  Rd

     

             

 

where       is   smooth or nonsmooth convex function and
     is   lowersemicontinuous and convex function  The
problem can  nd applications in machine learning  signal
processing  statistics  marketing optimization  and etc  For
example  in distance metric learning one needs to learn  
positive semide nite  PSD  matrix such that similar examples are close to each other and dissimilar examples are
far from each other  Weinberger et al    Xing et al 
  where the positive semide nite constraint can be
cast into   convex inequality constraint  Another example
arising in compressive sensing is to minimize the  cid  norm
of highdimensional vector subject to   measurement constraint  Cand es   Wakin    Although general interiorpoint methods can be applied to solve the problem with linear convergence  they suffer from exceedingly high computational cost periteration  Another solution is to employ the projected gradient  PG  method  Nesterov   
or the conditional gradient  CG  method  Frank   Wolfe 
  where the PG method needs to compute the projection into the constrained domain at each iteration and
CG needs to solve   linear optimization problem under
the constraint  However  for many constraints       PSD 
quadratic constraints  both projection into the constrained
domain and the linear optimization under the constraint are
timeconsuming  which restrict their capabilities to solving
these problems 
Recently  there emerges   new direction towards addressing the challenge of expensive projection that is to reduce
the number of projections  In the seminal paper  Mahdavi
et al    the authors have proposed two algorithms with
only one projection at the end of iterations for nonsmooth
convex and strongly convex optimization  respectively  The
idea of both algorithms is to move the constraint function
into the objective function and to control the violation of
constraint for intermediate solutions  While their developed algorithms enjoy an optimal convergence rate for nonsmooth optimization          iteration complexity 

Convex Constrained Optimization with Reduced Projections and Improved Rates

and   closeto optimal convergence rate for strongly con 

vex optimization        cid      there still lack of theory

balls for vectors and matrices    PSD constraint with  
bounded trace norm  the linear optimization over the constrained domain is much cheaper than projection into the
constrained domain  Jaggi    However  there still exist many constraints that render both projection into the
constrained domain and linear optimization under the constraint are comparably expensive  Examples include polyhedral constraints  quadratic constraints and   PSD constraint  
To tackle these complex constraints  the idea of optimization with   reduced number of projections was explored in
several studies since  Mahdavi et al    In   recent paper  Chen et al    the authors show that for stochastic
strongly convex optimization  the optimal convergence rate
can be achieved using   logarithmic number of projections 
In contrast  the developed theory in this paper implies that
only one projection is suf cient to achieve the optimal convergence rate for strongly convex optimization  and   logarithmic number of projections can be used to accelerate
convergence rates for nonstrongly convex optimization 
Cotter et al    proposed   stochastic algorithm for
solving heavily constrained problems with many constraint
functions by extending the work of  Mahdavi et al   
Nonetheless  their focus is not to improve the convergence
rates  Zhang et al    studied the smooth and strongly
convex optimization and they proposed   stochastic algorithm with    log     projections and proved an      
convergence rate  where   is the condition number and  
is the total number of iterations  Nonetheless  if the condition number is high the number of projections could be
very large  In addition  their algorithm utilizes the minibatch to avoid frequent projections in stochastic optimization  which is different from the present paper 
We note that several recent works also exploit different
forms of error bound conditions to improve the convergence  Wang   Lin    So    Hou et al    Zhou
et al    Yang   Lin    Xu et al    Most
notably  the technique used in our work is closely related
to  Yang   Lin    However  for constrained optimization problems the methods in  Yang   Lin    still need
to conduct projections at each iteration 
Finally  we comment on the differences between the proposed methods and the classical penalty methods that also
move the constraint into the objective using   penalty function  Bertsekas    The major differences are that    
the classical penalty methods typically require solving each
subproblem exactly while our methods do not require that 
and  ii  the classical penalty methods typically guarantee
asymptotic convergence while our methods have explicit
convergence rates 

 Indeed    linear optimization over   PSD constraint is ill 

posed because the PSD domain is unbounded 

and algorithms with reduced projections and faster rates
for smooth convex optimization and for convex optimization without strong convexity assumptions 
In this paper  we make signi cant contributions by developing   richer theory of convex constrained optimization
with reduced projections and faster rates  To be speci   
  we develop   general framework and theory of optimization with only one projection  where any favorable
smooth or nonsmooth convex optimization algorithms
can be employed to solve the intermediate augmented
unconstrained objective function  We discuss in full details the applicability of the proposed algorithms to problems with polyhedral  quadratic or PSD constraints 

  Applying the general theory to smooth convex optimization   with Nesterov   accelerated gradient methods yields an iteration complexity of    with only
one projection  In addition  when equipped with an optimal algorithm for strongly convex optimization the general theory implies the optimal iteration complexity of
   for strongly convex optimization with only one
projection  For smooth and strongly convex optimization  the general theory implies an iteration complexity
of    where         with only one projection
and   suf ciently large number of iterations 

  Building on the general framework and theory  we further develop an improved theory with faster convergence
rates for nonstrongly convex optimization at the price
of   logarithmic number of projections  In particular  we
show that under   mild local error bound condition  the

iteration complexities can be reduced to  cid   
for nonsmooth optimization and  cid    for smooth

optimization  where         is   constant in the local
error bound condition that characterizes the local growth
rate of functional values  To our knowledge  these are the
best convergence results with only   logarithmic number
of projections for nonstrongly convex optimization  We
also demonstrate their effectiveness for solving compressive sensing and distance metric learning problems 

  Related Work
The issue of high projection cost in projected gradient
descent has received increasing attention in recent years 
Most studies are based on the FrankWolfe technique that
eschews the projection in favor of   linear optimization
over the constrained domain  Jaggi    Hazan   Kale 
  LacosteJulien et al    Garber   Hazan   
It happens that for many bounded domains       bounded

 where  cid    suppresses   logarithmic factor 

 where the constraint function is assumed to be smooth 

Convex Constrained Optimization with Reduced Projections and Improved Rates

  Preliminaries
Let          Rd            denote the constrained domain    denote the optimal solution set and    denote the
optimal objective value  We denote by        the gradient
and by        the subgradient of   smooth or nonsmooth
function  respectively  When       is   nonsmooth function  we consider the problem as nonsmooth constrained
optimization  When both       and      are smooth  we
consider the problem as smooth constrained optimization 
  function       is Lsmooth if it has   Lipschitz continuous gradient        cid               cid      cid       cid  where
 cid     cid  denotes the Euclidean norm    function       is  
strongly convex if it satis es                       cid    
      

 cid       cid 

In the sequel  dist      denotes the distance of   to   set
       dist        minu   cid      cid  Let     be   hinge
operator that is de ned as         if       and        
if      
Throughout the paper  we make the the following assumptions to facilitate the development of our algorithms and
theory 
Assumption   For   convex minimization problem   we
assume     there exists   positive value       such that

 cid   cid     

min
    

        cid 

 

or more generally there exists   constant       for any
    Rd  such that   cid    arg minu Rd       cid       cid 
satis es

 cid   cid      cid         

 

 ii  there exists   strictly feasible solution such that       
   iii  both       and      are de ned everywhere and are
Lipschitz continuous with their Lipschitz constants denoted
by   and Gc  respectively 
We make several remarks about the assumptions  The inequality in   is introduced in  Mahdavi et al   
which is to ensure the distance from the  nal solution before projection to constrained domain   is not too large 
Note that the inequality in   is   more general condition
than   as seen from the following lemma 
Lemma   For any     Rd  let   cid    arg minc     cid    
  cid  If   holds  then   holds 

The above lemma is implicit in the proof of  Mahdavi et al 
  We will provide more discussions about Assumption       the key assumption  and exhibit the value of   for
  number of commonly seen constraints       polyhedral 
quadratic and PSD constraints  To make the presentation
more  uent  we postpone these discussions to Section  
The strict feasibility assumption  ii  allows us to explore
the KKT condition of the projection problem shown below 

Assumption  iii  imposes mild Lipschitz continuity conditions on both       and     
Traditional projected gradient descent methods need
to solve the following projection at each iteration
      arg minc     cid       cid  Conditional gradient methods        
the FrankWolfe technique  need to
solve the following linear optimization at each iteration
minu Rd        cid       For many constraint functions
 see Section   solving the projection problem and the linear optimization could be very expensive 

    General Theory of Optimization with

only one projection

In this section  we extend the idea of only one projection
proposed in  Mahdavi et al    to   general theory  and
then present optimization algorithms with only one projection for nonsmooth and smooth optimization  respectively 
To tackle the constraint  we introduce   penalty function
     parameterized by   which obeys the following certi cate  there exist constants       and        such
that

             
           such that         

 
From the above condition  it is clear that       It is notable that the penalty function      will also depend on
  however it will be set to   constant value  thus the dependence on   is omitted  We will construct such   penalty
function      for nonsmooth and smooth optimization in
next two subsections  We propose to optimize the following augmented objective function

min
  Rd

                   

 

ble  In order to obtain   feasible solution  we perform one

We can employ any applicable optimization algorithms to
optimize      pretending that there is no constraint  and

 nally obtain   solution  cid xT that is not necessarily feasiprojection to get cid xT    cid xT   The following theorem
allows us to convert the convergence of  cid xT for      to
that of cid xT for      
   and returns cid xT as the  nal solution  such that the following convergence of cid xT holds for any     Rd
  cid xT            BT        

Theorem   Let   be any iterative optimization algorithm
applied to minx      with   iterations  which starts with

 
where BT             when       Suppose that
Assumption   hold  then

   cid xT              
where cid xT    cid xT   and    is an optimal solution to  

     

      BT        

 

Convex Constrained Optimization with Reduced Projections and Improved Rates

Remark  It is worth mentioning that we omit some constant factors in the convergence bound BT         that
are irrelevant to our discussions  The notation BT        
emphasizes that it is   function of   and depends on   
and   target solution   and it will be referred to as BT   In
the next several subsections  we will see that by carefully
choosing the penalty function      we are able to provide
nice convergence for smooth and nonsmooth optimization
with only one projection  In the above theorem  we assume
the optimization algorithm   is deterministic  However   
similar result can be easily extended to   stochastic optimization algorithm   

BT         Then   follows due to           

Proof  First  we consider   cid xT       which implies that
 cid xT    cid xT   Due to the certi cate of        cid xT    
   cid xT   and                   Hence    cid xT    
  cid xT            BT                       
Next  we assume   cid xT       Inequality   implies that
   cid xT        cid xT                  BT          
By Assumption     we have    cid xT      cid cid xT  cid xT cid 
 cid cid xT  cid xT cid               cid xT          BT        
    cid cid xT  cid xT cid         BT        
where the last inequality follows that fact         cid xT    
         cid xT        cid xT      cid xT       cid cid xT  cid xT cid  because
the Lipschitz property and            cid xT   Therefore we

Combined with   we have

 cid cid xT  cid xT cid         BT          

     

 

have

Finally  we obtain

   cid xT                cid xT        cid xT        cid xT          
    cid cid xT  cid xT cid         BT        

   

     

      BT        

  Nonsmooth Optimization
Since an optimal convergence rate for general nonsmooth
optimization with only one projection has been attained
in  Mahdavi et al    in this subsection we present an
optimal convergence result for strongly convex problems 
For nonsmooth optimization  we can choose

            

and hence       We will use deterministic subgradient descent as an example to demonstrate the convergence
for       though many other optimization algorithms designed for nonsmooth optimization are applicable      

the stochastic subgradient method  The update of subgradient descent method is given by the following

xt    xt         xt 

                

 

  

 cid  

     cid xt      

aging  cid xT    
 cid xt       

where    is an appropriate step size  If       is  strongly
convex  the step size can be set as          and
the  nal solution can be computed by the  suf   avert     xt with        Rakhlin
et al    or by the polynomial decay averaging with
    xt and        Shamir   Zhang 
  Both schemes can attain BT         for
the convergence of       when       is  strongly convex 
Combining this with Theorem   we have the following
convergence result with the proof omitted due to its simplicity 
Corollary   Suppose that Assumption   holds and       is
 strongly convex  Set                       with    

   Let   run for   iterations with          Let cid xT
averaging  Then with only one projection cid xT    cid xT  

be computed by  suf   averaging or the polynomial decay

we achieve

   cid xT           

     

      Gc   

  

 

Remark  We note that the       is also achieved for
strongly convex optimization in  Zhang et al    Chen
et al    but with   logarithmic number of projections 
In contrast  Corollary   implies only one projection is suf 
 cient to achieve the optimal convergence for strongly convex optimization 

  Smooth Optimization
For smooth optimization  we consider both       and     
to be smooth   Let the smoothness parameter of       and
     be Lf and Lc  respectively 
In order to ensure the
augmented function      to be still   smooth function 
we construct the following penalty function

         ln     exp        

 

The following proposition shows that      is   smooth
function and obeys the condition in  
Proposition   Suppose      is Lcsmooth and GcLipschitz continuous  The penalty function in   is  
   smooth function and satis es           
 Lc      
      and  ii           ln      such that         

 

Then      is   smooth function and its smoothness parameter is given by LF   Lf    Lc      
    Next  we will
 it can be extended to when       is nonsmooth but its proxi 

 

mal mapping can be easily solved 

Convex Constrained Optimization with Reduced Projections and Improved Rates

establish the convergence for       using Nesterov   optimal accelerated gradient  NAG  methods  The update of
one variant of NAG can be written as follows

xt    yt      yt LF
yt    xt       xt    xt 

 

where the value of    can be set to different values depending on whether       is strongly convex or not  see
Corollary   Previous work have established the converT    
for smooth nonstrongly convex optimization and BT  
for smooth and strongly convex
 
optimization  By combining these results with Theorem  
and appropriately setting   we can achieve the following

gence of cid xT   xT for      in particular BT      LF
 cid 
 cid 
convergence of cid xT for      

LF exp  

 cid   

Corollary
holds 
dist                 is Lf  smooth and      is
Lcsmooth  Set                     with       
and      being   Let   run for   iterations and

that Assumption

Suppose

 

LF

 

 

 cid xT    xT  

  If       is convex  we can set    
   

 

 

  

  where     

   cid xT        

     

   
 

  
 

 GcD

with       and achieve
 Lf    Lc   
  ln  

 

     

      

 
 GcD

    

  ln  

      

 cid 

 cid 

 cid 

 

  If       is  strongly convex  we can set      
        and     

 
LF  
 
 

  and achieve

    with

 
 

LF  

 cid   
   cid xT           
as long as    cid  Lf  Lc   

  

 

   

     

 cid   

 

    ln    

   
Remark  The convergence results above indicate an
   iteration complexity for smooth optimization and
   with         for smooth and strongly
convex optimization with only one projection  All omitted
proofs can be found in  Yang et al   

 

  Improved Convergence for Nonstrongly

Convex Optimization

In this section  we will develop improved convergence for
nonstrongly convex optimization at   price of   logarithmic number of projections by considering an additional
condition on the target problem  To facilitate the presentation  we  rst introduce some notations  The  sublevel
set    and  level set    of the problem   are denoted by
                             and              
               respectively  Let   
  denote the closest
point in the  sublevel set    to           
  
    arg min
  

              

 cid       cid 

 

    

Let    denote the closest optimal solution in   to        
     arg minu   cid       cid 
In this section  we will make the following additional assumption about the problem  
Assumption   For   convex minimization problem   we
assume     there exist        and       such that      
minx             ii    is   nonempty convex compact
set   iii  the optimization problem   satis es   local error
bound condition       there exist         and       such
that for any        we have dist                
   where   denotes the optimal set and    denotes the
optimal value 
Remark  we would like to remark that the new assumption
only imposes mild conditions on the problem  In particular 
Assumption       supposes there is   lower bound of the optimal value    which usually holds in machine learning applications where the objective function if nonnegative  Assumption    ii  ensures that    is also bounded  Rockafellar    therefore the   in the local error bound is  nite 
which can be easily satis ed for   norm regularized or constraint problems  the local error bound condition holds for
  broad family of functions       semialgebraic functions
or real subanalytic functions  Jerome Bolte    Yang  
Lin    In Section   we will also demonstrate several
applications of the improved algorithms proposed in this
section by establishing the local error bound condition 
Although the local error bound condition is much weaker
than the strong convexity assumption  below we will propose novel algorithms leveraging this condition with faster
convergence and only   logarithmic number of projections 

  Nonsmooth Optimization
To establish an improved convergence for nonsmooth optimization  we develop   new algorithm shown in Algorithm
  based on subgradient descent  GD  method  to which
we refer as LoPGD  The algorithm runs for   epochs and
each epoch employs GD for minimizing                
      with   feasible solution xk      as   starting
point and   iterations of updates  At the end of each epoch 

the averaged solution cid xk is projected into the constrained

domain   and the solution xk will be used as the starting
point for next epoch  The step size    is decreased by half
every epoch starting from   given value   The theorem
below establishes the iteration complexity of LoPGD and
also exhibits the values of      and   To simplify nota 
   and           Gc 
tions  we let      
Theorem   Suppose Assumptions   and   hold  Let    
              cid log cid  and            
in Algorithm  
 
 
where   and   are constants appearing in the local error
bound condition  Then    xK          
Remark  Since the projection is only conducted at the
end of each epoch and the total number of epochs is at

Convex Constrained Optimization with Reduced Projections and Improved Rates

     xk

   xk

    

end for

Update xk

    xk 

Let xk
for                       do

          xk
Let cid xk  cid  
   
Let xk    cid xk  and          

Algorithm   LoPGD
  INPUT                   
  Initialization          
  for                   do
 
 
 
 
 
 
  end for
Algorithm   LoPNAG
  INPUT                      tK       
  Initialization          
  for                   do
 
 
 
 
 
 
  end for
most      cid log cid  so the total number of projecplexity in Theorem   is  cid    that improves the
tions is only   logarithmic number    The iteration com 
      we can achieve  cid    iteration complexity with

      xk
     
   
     xk
        xk
   
tk  xk    cid xk  and          

standard result of    without strong convexity  With

    xk 

Let yk
for                   tk     do

end for

Let cid xk   xk

Update xk
Update yk

     yk
     xk

Lk

only   log  projections 

  

  Smooth Optimization
Similar to nonsmooth optimization  we also develop   new
algorithm based on NAG shown in Algorithm   where
     is de ned using      in   Lk   LF  
is the
smoothness parameter of     and         
               
is   sequence with    updated as in Corollary   We refer
to this algorithm as LoPNAG  The key idea is to use to  
sequence of reducing values for    instead of using   small
value as in Corollary   and solve each augmented unconstrained problem         approximately with one projection  The theorem below exhibits the iteration complexity
of LoPNAG and reveals the values of      and            tK 
To simplify notations  we let      Lf    Lc 
Theorem   Suppose Assumptions   and   hold
and      
is Lcsmooth 
is Lf  smooth and     
   ln          cid log cid  and tk  
Let    
 
  max Gcp
in Algorithm   where   and   are constants appearing in
the local error bound condition  Then    xK          
of iterations is bounded by  cid    which improves the
Remark 
It is not dif cult to show that the total number

  ln  cid Lf    Lc   

one in Corollary   without strong convexity  If       is  

 

 

simple nonsmooth function whose proximal mapping can
be easily computed        cid  norm  we can replace step
  in Algorithm   by   proximal mapping to handle      
which gives the same convergence result in Theorem   An
example is presented in Section   for compressive sensing
with      

  Discussion of Assumption      
One might note that   key condition for developing the theory with reduced projections is Assumption       Although
Mahdavi et al    has brie   mentioned that the condition can be satis ed for   PSD cone or   Polytope   
bounded polyhedron  their discussion lacks of details in
particular on the value of   in   or   Below  we discuss
the condition in details about three types of constraints 

Polyhedral constraints  First  we show that when     
is   polyhedral function       its epigraph is   polyhedron
 not necessarily bounded  the inequality   is satis ed  To
this end  we explore the polyhedral error bound  PEB  condition  Gilpin et al    Yang   Lin    In particular  if we consider an optimization problem  minx Rd     
where the epigraph of      is polyhedron  Let    denote
the optimal set and    denote the optimal value of the problem above  The PEB says that there exists       such that
for any     Rd

dist                  

 

To show that the inequality   holds for   polyhedral
function    we can consider the optimization problem
minx Rd       The optimal set of the above problem
is given by           Rd            For any  
such that          let   cid    arg minc     cid       cid 
be the closest point in the optimal set to    Therefore if    is   polyhedral function so does       by
the PEB condition   there exists         such that
 cid       cid cid            minx             
Let us consider   concrete example  where the problem has
      bi                     
  set of af ne inequalities   cid 
There are two methods to encode this into   single constraint function          The  rst method is to use
      bi  which is   polyhedral funcc      max       cid 
tion and therefore satis es   The second method is to
use         cid Cx     cid  where       max     and
                cm cid  Thus          cid Cx     cid  The
inequality   is then guaranteed by Hoffman   bound and
the parameter   is given by the minimum nonzero eigenvalue of   cid    Wang   Lin    Note that the projection onto   polyhedron is   linear constrained quadratic
programming problem  and the linear optimization over  
polyhedron is   linear programming problem  Both have
polynomial time complexity that would be high if   and  
are large  Karmarkar    Kozlov et al   

Convex Constrained Optimization with Reduced Projections and Improved Rates

Quadratic constraint    quadratic constraint can take
the form of  cid Ax     cid      where     Rm   and
    Rm  Such   constraint appears in compressive sensing  Cand es   Wakin    where the goal is to reconstruct   sparse highdimensional vector   from   small
number of noisy measurements     Ax       Rm with
   cid     The corresponding optimization problem is

 

 cid   cid 

      cid Ax     cid     

minx Rd

 
where      cid cid  is an upper bound on the magnitude of the
noise  To check the Assumption     we note that       
 cid Ax   cid  and           cid Ax    Let us consider
that   has   full row rank   and denote by     Ax     
then on the boundary          we have  cid   cid   
  and
minimum eigenvalue of AA cid    Rm    Therefore the

 cid   cid   cid   cid   min AA cid  where  min AA cid      is the
Assumption     is satis ed with      cid   min AA cid  It

is notable that the projection and the linear optimization
under the quadratic constraint require solving   quadratic
programming problem and therefore could be expensive 
PSD constraint    PSD constraint    cid    for     Rd  
can be written as an inequality constraint  min       
where  min    denotes the minimum eigenvalue of   
The subgradient of         min    when  min     
  is given by Conv uu cid cid   cid      Xu          the
convex hull of the outer products of normalized vectors in
the null space of the matrix    In  Yang et al    we
show that if the dimension of the null space of   is   with
           the norm of the subgradient of      on the
     
boundary          is lower bounded by      
 
 
Finally  we note that computing   subgradient of      
only needs to compute one eigenvector corresponding to
the smallest eigenvalue  In contrast  both projection and
linear optimization under   PSD constraint could be very
expensive for highdimensional problems  In particular  the
projection onto   PSD domain needs to conduct   singular
value decomposition  The linear optimization over   PSD
cone is illposed due to that PSD cone is not compact  the
solution is either   or in nity  One may add an arti cial
constraint on the upper bound of the eigenvalues  According to  Jaggi    the time complexity for solving this
linear optimization problem approximately up to an accuracy level  cid  is       cid  with   being the number
of nonzeros in the gradient and  cid  decreasing iteratively
required in the FrankWolfe method  which could be much
more expensive especially for highdimensional problems
and in later iterations than computing the  rst eigenpairs
at each iteration in our methods 

 Here we use the square constraint to make it   smooth function so that the proposed algorithms for smooth optimization are
applicable by using proximal gradient mapping to handle the  cid 
norm 

 which is reasonable because    cid    

 

  Applications
  Compressive Sensing
We  rst consider   compressive sensing problem in  
Becker et al    proposed an optimization algorithm
based on the Nesterov   smoothing and the Nesterov   optimal method for the smoothed problem  known as NESTA 
It needs to perform the projection into the domain  cid Ax  
  cid      at every iteration and has an iteration complexity
of    In contrast  the presented algorithm with only
one projection in Section   using Nesterov   accelerated
proximal gradient method  Beck   Teboulle    to solve
the unconstrained problem enjoys an iteration complexity
of    Moreover  we present   theorem below showing
that the problem   satis es the local error bound condition with       and hence the presented LoPNAG
  iteration complexity with only   loga 

enjoys an  cid   

rithmic number of projections 
Theorem   Let          cid   cid          cid Ax     cid     
  denote the optimal set and    be the optimal solution
to   Assume that there exists    such that  cid Ax   cid   
  and    cid    Then for any           Rd such that
         and                there exists          
such that dist                    Hence  LoP 
  with

NAG can have an iteration complexity of  cid   

only   log  projections 
Next  we demonstrate the effectiveness of the LoPNAG for
solving the compressive sensing problem in   by comparing with NESTA  We generate   synthetic data for testing  In particular  we generate   random measurement matrix     Rm   with       and       The entries of the matrix   are generated independently with the
uniform distribution over the interval     The vector
     Rd is generated with the same distribution at   randomly chosen coordinates  The noise     Rm is   dense
vector with independent random entries with the uniform
distribution over the interval     where   is the noise
magnitude and is set to   Finally the vector   was obtained as     Ax     
We use the Matlab package of NESTA   For fair comparison  we also use the projection code in the NESTA package
for conducting projection  To handle the unknown smoothness parameter in the proposed algorithm  we use the backtracking technique  Beck   Teboulle    The parameter   is initially set to   and decreased by half every
  iterations after   projection and the target smoothing
parameter in NESTA is set to   For the value of   in
LoPNAG  we tune it from its theoretical value to several
smaller values and choose the one that yields the fastest
convergence  We report the results in Table   which
include different number of iterations  the corresponding

 

 http statweb stanford edu candes 

nesta 

Convex Constrained Optimization with Reduced Projections and Improved Rates

Table   LoPNAG vs  NESTA for solving the compressive sensing problem 

LoPNAG

Iters   Projs Rec  Err 
     
 
     
 
     
 
     
 
     
 

Objective
 
 
 
 
 

Time    
 
 
 
 
 

Iters   Projs
     
     
     
     
     

NESTA

Rec  Err 
 
 
 
 
 

Objective
 
 
 
 
 

Time    
 
 
 
 
 

Table   LoPGD vs  OPGD and PGD for solving the considered distance metric learning problem 

LoPGD

Iters   Projs Objective
     
     
     
     
     

 
 
 
 
 

Time    
 
 
 
 
 

OPGD

Iters   Projs Objective
     
     
     
     
     

 
 
 
 
 

Time    
 
 
 
 
 

Iters   Projs
     
     
     
     
     

PGD
Objective
 
 
 
 
 

Time    
 
 
 
 
 

number of projections  the recovery error of the found solution compared to the underlying true sparse solution  the
objective value       the  cid  norm of the found solution  and
the running time  Note that each iteration of NESTA requires two projections because it maintains two extra sequence of solutions  From the results  we can see that LoPNAG converges signi cantly faster than NESTA  Even with
only one projection  we are able to obtain   better solution
than that of NESTA after running   iterations 

 cid 

 
   

  Highdimensional Distance Metric Learning
Consider the following distance metric learning problem 

  yij  cid xi   xj cid 

      cid   cid off

      

     cid 

     
min
  cid 
where   denotes all pairs of training examples  yij    
indicates xi  xj belong to the same class and yij    
indicates they belong to different classes   cid   cid 
      cid Az
  cid    Aij  We note that such   formuand  cid   cid off
lation is useful for high dimensional problems due to the
 cid  regularizer    similar formulation with different forms
of loss function has been adopted in literature  Qi et al 
  We consider the square loss because it gives us
faster convergence with   logarithmic number of projections by LoPGD  Due to the presence of the nonsmooth
PSD constraint and the  cid  regularizer  Nesterov   accelerated proximal gradient methods can not be applied ef 
 ciently to solving   and the augmented unconstrained
problem  Nevertheless  we can apply the proposed LoPGD
method for solving the problem with   logarithmic number
of projections  Regarding the constant   in the local error
bound condition for   it still remains an open problem 
Nonetheless    local error bound condition with      
might be established under certain regularity condition of
the problem  Zhou   So    Cui et al    For example  Cui et al    provided   direct analysis of   local error bound condition with       for   class of constrained convex symmetric matrix optimization problems

regularized by nonsmooth spectral functions  including the
indicator function of   PSD constraint  They established
suf cient conditions  Theorem   for   local error bound
condition with       to hold  which reduces to   regularity condition for   depending on the optimal solutions of
the problem    thorough analysis of the regularity condition is much more involved and left as an open problem 
Next  we demonstrate the empirical performance of
LoPGD for solving   We use the coloncancer data
available on libsvm web portal  which has   features
and   examples  Fourty examples are used as training
examples to generate   pairs to learn the distance metric  The regularization parameter is set to       We
compare LoPGD  gradient descent method with only one
projection  referred to as OPGD  and standard projected
 
GD  referred to PGD  The step size in PGD and OPGD
   where   is the iteration index  We use the
is set to  
same tuned initial step size for all algorithms  The number
of iterations perepoch in LoPGD is set to   The penalization parameter   in both OPGD and LoPGD is tuned
and set to   In Table   we report the objective values  the
 of iterations projections  and running time across the  rst
  iterations  We can see that LoPGD converges dramatically faster than PGD and also much faster than OPGD 

  Conclusion
We have developed   general theory of optimization with
only one projection for   family of inequality constrained
convex optimization problems  It yields an improved iteration complexity for smooth optimization compared with
nonsmooth optimization  By exploring the local error
bound condition  we further develop new algorithms with  
logarithmic number of projections and achieve better convergence for both smooth and nonsmooth optimization
without strong convexity assumption  Applications in compressive sensing and distance metric learning demonstrate
the effectiveness of the proposed improved algorithms 

Convex Constrained Optimization with Reduced Projections and Improved Rates

Acknowledgements
We are grateful to all anonymous reviewers for their helpful comments     Yang is partially supported by National
Science Foundation  IIS  IIS     Zhang
thanks the support from NSFC   and JiangsuSF
 BK 

References
Beck  Amir and Teboulle  Marc    fast iterative shrinkageinverse problems 

thresholding algorithm for
linear
SIAM    Img  Sci     

Becker  Stephen  Bobin    er ome  and Cand es  Emmanuel   
Nesta    fast and accurate  rstorder method for sparse
recovery  SIAM    Img  Sci      ISSN  
 

Bertsekas  Dimitri    Constrained Optimization and Lagrange Multiplier Methods  Optimization and Neural
Computation Series  Athena Scienti      edition   
ISBN  

Cand es  Emmanuel    and Wakin  Michael    An introduction to compressive sampling  IEEE Signal Processing
Magazine       

Chen  Jianhui  Yang  Tianbao  Lin  Qihang  Zhang  Lijun 
and Chang  Yi  Optimal stochastic strongly convex optimization with   logarithmic number of projections  In
Proceedings of the ThirtySecond Conference on Uncertainty in Arti cial Intelligence  UAI   

Cotter  Andrew  Gupta  Maya    and Pfeifer  Jan    light
In Proceedings of
touch for heavily constrained SGD 
the  th Conference on Learning Theory  COLT  pp 
   

Cui  Ying  Ding  Chao  and Zhao  Xinyuan  Quadratic
growth conditions for convex matrix optimization problems associated with spectral functions  CoRR   

Frank  Marguerite and Wolfe  Philip 

An algorithm
for quadratic programming  Naval Research Logistics
 NRL     

Garber  Dan and Hazan  Elad  Faster rates for the frankwolfe method over stronglyconvex sets  In Proceedings
of the  nd International Conference on Machine Learning  ICML  pp     

Hazan  Elad and Kale  Satyen  Projectionfree online learning  In Proceedings of the International Conference on
Machine Learning  ICML   

Hou  Ke  Zhou  Zirui  So  Anthony ManCho  and Luo 
ZhiQuan  On the linear convergence of the proximal gradient method for trace norm regularization 
In
Advances in Neural Information Processing Systems
 NIPS  pp     

Jaggi  Martin  Revisiting frankwolfe  Projectionfree
sparse convex optimization  In Proceedings of the International Conference on Machine Learning  ICML  pp 
   

Jerome Bolte  Trong Phong Nguyen  Juan Peypouquet
Bruce Suter  From error bounds to the complexity of
 rstorder descent methods for convex functions  CoRR 
abs   

Karmarkar       new polynomialtime algorithm for linear
In Proceedings of the Sixteenth Annual
programming 
ACM Symposium on Theory of Computing  pp   
 

Kozlov       Tarasov       and Khachiyan       Polynomiale Loesbarkeit der konvexen quadratischen Programmierung  Zh  Vychisl  Mat  Mat  Fiz   
  ISSN  

LacosteJulien  Simon  Jaggi  Martin  Schmidt  Mark  and
Pletscher  Patrick  Blockcoordinate frankwolfe optimization for structural svms  In Proceedings of the International Conference on Machine Learning  ICML  pp 
   

Mahdavi     Yang     Jin     and Zhu     Stochastic
gradient descent with only one projection  In Advances
in Neural Information Processing Systems  NIPS  pp 
   

Nesterov  Yurii  Introductory lectures on convex optimization    basic course  volume   of Applied optimization 
Kluwer Academic Publishers   

Qi  GuoJun  Tang  Jinhui  Zha  ZhengJun  Chua  TatSeng  and Zhang  HongJiang  An ef cient sparse metric
learning in highdimensional space via   penalized logdeterminant regularization  In Proceedings of the  th
international conference on Machine learning  ICML 
pp     

Gilpin  Andrew  Pe na  Javier  and Sandholm  Tuomas 
Firstorder algorithm with log epsilon  convergence
for epsilonequilibrium in twoperson zerosum games 
Math  Program     

Rakhlin  Alexander  Shamir  Ohad  and Sridharan  Karthik 
Making gradient descent optimal for strongly convex
stochastic optimization  In Proceedings of the  th international conference on Machine learning  ICML   

Convex Constrained Optimization with Reduced Projections and Improved Rates

Rockafellar       Convex Analysis  Princeton mathematical

series  Princeton University Press   

Shamir  Ohad and Zhang  Tong  Stochastic gradient descent for nonsmooth optimization  Convergence results and optimal averaging schemes  In Proceedings of
the  th International Conference on Machine Learning
 ICML  pp     

So  Anthony ManCho  Nonasymptotic convergence analysis of inexact gradient methods for machine learning
without strong convexity  CoRR  abs   

Wang  PoWei and Lin  ChihJen  Iteration complexity of
feasible descent methods for convex optimization  Journal of Machine Learning Research   
 

Weinberger  Kilian    Blitzer 

John 

and Saul 
large
Lawrence   
In Advances
margin nearest neighbor classi cation 
in Neural Information Processing Systems  NIPS  pp 
   

Distance metric learning for

Xing     Ng     Jordan     and Russell     Distance
metric learning with application to clustering with sideIn Advances in Neural Information Proinformation 
cessing Systems  NIPS  volume   pp     

Xu  Yi  Yan  Yan  Lin  Qihang  and Yang  Tianbao  Homotopy smoothing for nonsmooth problems with lower
complexity than    In Advances in Neural Information Processing Systems  NIPS  pp     

Yang  Tianbao and Lin  Qihang  Rsg  Beating subgradient method without smoothness and strong convexity 
CoRR  abs   

Yang  Tianbao  Lin  Qihang  and Zhang  Lijun    richer
theory of convex constrained optimization with reduced
projections and improved rates  the long version of icml
paper  CoRR  arXiv   

Zhang  Lijun  Yang  Tianbao  Jin  Rong  and He  Xiaofei 
  logt  projections for stochastic optimization of smooth
In Proceedings of the
and strongly convex functions 
International Conference on Machine Learning  ICML 
pp     

Zhou  Zirui and So  Anthony ManCho    uni ed approach
to error bounds for structured convex optimization problems  arXiv   

Zhou  Zirui  Zhang  Qi  and So  Anthony ManCho    pnorm regularization  Error bounds and convergence rate
In Proceedings of the
analysis of  rstorder methods 
 nd International Conference on Machine Learning 
 ICML  pp     

