Algebraic Variety Models for HighRank Matrix Completion

Greg Ongie   Rebecca Willett   Robert    Nowak   Laura Balzano  

Abstract

We consider   generalization of lowrank matrix
completion to the case where the data belongs to
an algebraic variety       each data point is   solution to   system of polynomial equations  In this
case the original matrix is possibly highrank 
but it becomes lowrank after mapping each column to   higher dimensional space of monomial
features  Many wellstudied extensions of linear models  including af ne subspaces and their
union  can be described by   variety model  as
well as   rich class of nonlinear quadratic and
higher degree curves and surfaces  We study the
sampling requirements for matrix completion under   variety model with   focus on   union of
af ne subspaces  We also propose an ef cient
matrix completion algorithm that minimizes  
convex or nonconvex surrogate of the rank of
the matrix of monomial features  using the wellknown  kernel trick  to avoid working directly
with the highdimensional monomial matrix  We
show the proposed algorithm is able to recover
synthetically generated data up to the predicted
sampling complexity bounds  and outperforms
standard low rank matrix completion and subspace clustering algorithms in experiments with
real data 

  Introduction
Work in the last decade on matrix completion has shown
that it is possible to leverage linear structure in order to interpolate missing values in   lowrank matrix  Candes  
Recht    The highlevel idea of this work is that if
the data de ning the matrix belongs to   structure having
fewer degrees of freedom than the entire dataset  that structure provides redundancy that can be leveraged to complete

 Department of EECS  University of Michigan  Ann Arbor 
Michigan  USA  Department of ECE  University of Wisconsin  Madison  Wisconsin  USA  Correspondence to  Greg Ongie
 gongie umich edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

the matrix  The assumption that the matrix is lowrank is
equivalent to assuming the data lies on  or near    lowdimensional linear subspace 
It is of great interest to generalize matrix completion to
exploit lowcomplexity nonlinear structures in the data 
Several avenues have been explored in the literature  from
generic manifold learning  Lee et al    to unions of
subspaces  Eriksson et al    Elhamifar   Vidal   
to lowrank matrices perturbed by   nonlinear monotonic
function  Ganti et al    Song et al   
In each
case missing data has been considered  but there lacks  
clear  unifying framework for these ideas 
In this work we study the problem of completing   matrix whose columns belong to an algebraic variety       the
set of solutions to   system of polynomial equations  Cox
et al    This is   strict generalization of the linear
 or af ne  subspace model  which can be written as the set
of points satisfying   system of linear equations  Unions
of subspaces and unions of af ne spaces are also algebraic
varieties  Plus    much richer class of nonlinear curves 
surfaces  and their unions  are captured by   variety model 
The matrix completion problem using   variety model can
be   matrix of   data points where each column xi   Rn 
De ne      Rn   RN as the mapping that sends the
vector           xn  to the vector of all monomials in
     xn of degree at most    and let       denote the
matrix that results after applying    to each column of   
which we call the lifted matrix  We will show the lifted matrix is rank de cient if and only if the columns of   belong
to an algebraic variety  This motivates the following matrix
completion approach 

be formalized as follows  Let    cid            xs

 cid    Rn  

min
  

rank         such that              

 
where    represents   projection that restricts to some
observation set                                 The rank of
        depends on the choice of the polynomial degree  
and the underlying  complexity  of the variety  in   sense
we will make precise  Figure   shows three examples of
datasets that have lowrank in the lifted space for different
polynomial degrees   
In this work we investigate the factors that in uence the
sampling complexity of varieties as well as algorithms for

Algebraic Variety Models for HighRank Matrix Completion

completion  The challenges are     to characterize varieties
having lowrank  and therefore few degrees of freedom  in
the lifted space       determine when       is lowrank 
    devise ef cient algorithms for solving   that can exploit these few degrees of freedom in   matrix completion
setting  and     determine the tradeoffs relative to existing matrix completion approaches  This work contributes
considerable progress towards these goals 

Figure   Data belonging to algebraic varieties in    The original
data is full rank  but   nonlinear embedding of the matrix to  
feature space consisting of monomials of degree at most   is lowrank with rank    indicating the data has few degrees of freedom 

Our main contributions are as follows  We identify bounds
on the rank of   matrix       when the columns of the
data matrix   belong to an algebraic variety  We study
how many entries of such   matrix should be observed in
order to recover the full matrix from an incomplete sample 
We show as   case study that monomial representations
produce lowrank representations of unions of subspaces 
and we characterize the rank  The standard union of subspace representation as   discrete collection of individual
subspaces is inherently nonsmooth in nature  whereas the
algebraic variety allows for   purely continuous parameterization  This leads to   general algorithm for completion of
  data matrix whose columns belong to   variety  The algorithm   performance is showcased on data simulated as  
union of subspaces    union of lowdimensional parametric
surfaces  and real data from   motion segmentation dataset
and   motion capture dataset  The simulations show that
the performance of our algorithm matches our predictions
and outperforms other methods  In addition  the analysis of
the degrees of freedom associated with the proposed representations introduces several new research avenues at the
intersection of nonlinear algebraic geometry and random
matrix theory 

  Related work

There has been   great deal of research activity on matrix
completion problems since  Candes   Recht    where
the authors showed that one can recover an incomplete matrix from few entries using   convex relaxation of the rank
minimization optimization problem  For example  it is now
known that only   rn  entries are necessary and suf cient
 PimentelAlarc on et al      for almost every      
rank   matrix as long as the measurement pattern satis es

certain deterministic conditions  However  these methods
and theory are restricted to lowrank linear models    great
deal of real data exhibit nonlinear structure  and so it is of
interest to generalize this approach  Work in that direction has dealt with union of subspaces models  Eriksson
et al    Yang et al    Elhamifar    PimentelAlarc on et al      PimentelAlarcon   Nowak   
locally linear approximations  Lee et al    as well as
lowrank models perturbed by an arbitrary nonlinear link
function  Ganti et al    Song et al    Rao et al 
  In this paper we instead seek   more general model
that captures both linear and nonlinear structure  The variety model has as instances lowrank subspaces and their
union as well as quadratic and higher degree curves and
surfaces 
Work on kernel PCA  cf   Sanguinetti   Lawrence   
Nguyen   Torre    leverage similar geometry to
ours  In Kernel Spectral Curvature Clustering  Chen et al 
  the authors similarly consider clustering of data
points via subspace clustering in   lifted space using kernels  These works are algorithmic in nature  with promising numerical experiments  but do not systematically consider missing data or analyze relative degrees of freedom 
This paper also has close ties to algebraic subspace clustering  ASC   Vidal et al        Tsakiris  
Vidal    also known as generalized PCA  Similar to
our approach  the ASC framework models unions of subspaces as an algebraic variety  and makes use of monomial
liftings of the data to identify the subspaces  Characterizations of the rank of data belonging to union of subspaces
under the monomial lifting are used in the ASC framework
 Vidal et al    based on results in  Derksen   
The difference of the results in  Derksen    and those
in Prop    is that ours hold for monomial liftings of all degrees    not just        where   is the number of subspaces 
Also  the main focus of ASC is to recover unions of subspaces or unions of af ne spaces  whereas we consider data
belonging to   more general class of algebraic varieties  Finally  the ASC framework has not been adapted to the case
of missing data  which is the main focus of this work 

  Variety Models
As   toy example to illustrate our approach  consider   matrix

 cid 

 cid      

     

   

 
 

  
  

    

whose six columns satisfy the quadratic equation

                     

        ix        

        

for                 and some unknown constants         that
are not all zero  Generically    will be full rank  However 

             Algebraic Variety Models for HighRank Matrix Completion

suppose we vertically expand each column of the matrix to
make         matrix

   

  
  
  

   

 

                

 

 

  

 

  

 

      

 

  
  
  

 
 
 
 
 

 

  
  
  

  

 

      ix      

     we augment each column of   with     and with the
    This allows us
quadratic monomials   
to reexpress the polynomial equation   as the matrixvector product           where                   
In other words    is rank de cient  Suppose  for example  that we are missing entry    of    Since   is
full rank  there is no way to uniquely complete the missing entry by leveraging linear structure alone 
Instead 
we ask  Can we complete    using the linear structure
present in     Due to the missing entry    the  rst column of   will having the following pattern of missing entries         
     However  assuming the  ve
complete columns in   are linearly independent  we can
uniquely determine the nullspace vector   up to   scalar
multiple  Then from   we have

     

                                       

 

In general  this equation will yield at most two possibilities for    Moreover  there are conditions where we
can uniquely recover    namely when        and
            cid   
This example shows that even without   priori knowledge
of the particular polynomial equation satis ed by the data 
it is possible to uniquely recover missing entries in the original matrix by leveraging induced linear structure in the
matrix of expanded monomials  We now show how to considerably generalize this example to the case of data belonging to an arbitrary algebraic variety 

  Formulation

Let    cid            xs

 cid    Rn   be   matrix of   data points

where each column xi   Rn  De ne      Rn   RN as
the mapping that sends the vector           xn  to the
vector of all monomials in      xn of degree at most   

               RN

       

where             is   multiindex of nonnegative
    and        
integers  with        
In the context of kernel methods in machine learning  the
map    is often called   polynomial feature map  Muller
et al    Borrowing this terminology  we call        
feature vector  the entries of       features  and the range
of    feature space  Note that the number of features is

given by                 cid    

 cid  the number

 cid     cid    

of unique monomials in   variables of degree at most   

 

 

When           xs  is an       matrix  we use      
to denote the       matrix            xs 
The problem we consider is this  can we complete   partially observed matrix   under the assumption that      
is lowrank  This can be posed as the optimization problem given above in   We give   practical algorithm for
solving   relaxation of   in Section   Similar to previous work cited above on using polynomial feature maps 
our method leverages the kernel trick for ef cient computations  The success of this optimization and its relaxations
will depend on many factors  but clearly the rank of      
and the number of sampled entries will play an important
role  The number of samples  rank  and dimensions all
grow in the mapping to feature space  but they grow at different rates depending on the underlying geometry  it is not
immediately obvious what conditions on the geometry and
sampling rates impact our ability to determine the missing
entries  In the remainder of this section  we show how to
relate the rank of       to the underlying variety  and we
study the sampling requirements necessary for the completion of the matrix in feature space 

  Rank properties

To better understand what determines the rank of the matrix       we introduce some additional notation and
concepts from algebraic geometry  Let      denote the
space of all polynomials with real coef cients in   variables           xn  We model   collection of data as
belonging to   real  af ne  algebraic variety  Cox et al 
  which is de ned as the common zero set of   system of polynomials         

              Rn             for all       

Suppose the variety        is de ned by the  nite set of
polynomials           fq  where each fi has degree at
most    Let     RN   be the matrix whose columns are
given by the vectorized coef cients        of the polynomials fi              in     Then the columns of  
belong to the variety        if and only if             
In particular  assuming the columns of   are linearly independent  this shows that       has rank   min          
In particular  when the number of data points           
then       is rank de cient 
However  the exact rank of       could be much smaller
than min           especially when the degree   is large 
This is because the coef cients   of any polynomial that
vanishes at every column of   satis es             
We will  nd it useful to identify this space of coef cients
with    nite dimensional vector space of polynomials  Let
Rd    be the space of all polynomials in   real variables of
degree at most    We de ne the vanishing ideal of degree
  corresponding to   set     Rn  denoted by Id     to be

Algebraic Variety Models for HighRank Matrix Completion

subspace of polynomials belonging to Rd    that vanish at
all points in    

Id            Rd                for all       

We also de ne the nonvanishing ideal of degree   corresponding to    denoted by Sd     to be the orthogonal
complement of Id     in Rd   

Sd          Rd     cid      cid      for all     Id    

where the inner product  cid      cid  of polynomials        Rd   
is de ned as the inner product of their coef cient vectors 
Hence  the rank   of       can expressed in terms of the
dimension of nonvanishing ideal of degree   corresponding to           xs  the set of all columns of   
Speci cally  we have rank         min       where

    dim Sd           dim Id      

In general the dimension of the space Id     or Sd     is
dif cult to determine when   is an arbitrary set of points 
However  if we assume   is   subset of   variety     then
Id       Id     and hence

rank         dim Sd    

In certain cases dim Sd     can be computed exactly or
bounded using properties of the polynomials de ning    
For example  it is possible to compute the dimension of
Sd     directly from   Gr obner basis for the vanishing
In Section  
ideal associated with    Cox et al   
we show how to bound the dimension of Sd     in the case
where   is   union of subspaces 

  Sampling rate

Informally  the degrees of freedom of   class of objects is
the minimum number of free variables needed to describe
an element in that class uniquely  For example        rank
  matrix has              degrees of freedom  nr parameters to describe   linearly independent columns making up
  basis of the column space  and         parameters to describe the remaining       columns in terms of this basis 
It is impossible to uniquely complete   matrix in this class
if we sample fewer than this many entries 
We can make   similar argument to specify the minimum
number of samples needed to uniquely complete   matrix
that is lowrank when mapped to feature space  First  we
characterize how missing entries of the data matrix translate to missing entries in feature space  For simplicity  we
will assume   sampling model where we sample    xed
number of entries   from each column of the original data
matrix  Let     Rn represent   single column of the data
matrix  and            with       denote the indices of the sampled entries of    The pattern of revealed

entries in       corresponds to the set of multiindices 
                            for all        

of degree at most   in   variables      cid    

which has the same cardinality as the set of all monomials

 cid  If we call

this quantity    then the ratio of revealed entries in      
to the feature space dimension is

 

 
 

 

                       
                       

 

 

 

 cid    
 cid 
 cid   
 cid    
 cid      

 

 

we have the bounds cid   

which is on the order of    

     for small    More precisely 

 cid       

 cid  

 

 

 

 

In total  observing   entries per column of the data matrix
translates to   entries per column in feature space  Suppose the       lifted matrix       is rank    By the
preceding discussion  we need least              entries
of the feature space matrix       to complete it uniquely
among the class of all       matrices of rank    Hence  at
minimum we need to satisfy

                  

 

 cid    

Let    denote the minimal value of   such that    

 cid  achieves the bound   and set     cid    
 cid   

viding   through by the feature space dimension   and  
gives

 cid  Di 
 cid cid 

 cid 

 cid 

 

 

 

 

 
 

     
 

 

  
 

 

 
 

and so from   we see we can guarantee this bound with

 cid cid           
 cid   

 

 

  
 

   

 

 

 
 

     
 

 

 

 cid   
 cid 

 

 cid cid   

 

 

        for small   and large   

and this in fact will result in tight satisfaction of   because
      
At one extreme where the matrix       is full rank  then
        or         and according to   we need
           full sampling of every data column  At
the other extreme where instead we have many more data
points than the feature space rank       cid    then   gives
the asymptotic bound           
The above discussion bounds the degrees of freedom of
  matrix that is rankR in feature space  Of course  the
proposed variety model has potentially fewer degrees of
freedom than this  because additionally the columns of the
lifted matrix are constrained to lie in the image of the feature map  We use the above bound only as   rule of thumb

   

 

Algebraic Variety Models for HighRank Matrix Completion

for sampling requirements on our matrix  Furthermore  we
note that sample complexities for standard matrix completion often require that locations are observed uniformly at
random  whereas in our problem the locations of observations in the lifted space will necessarily be structured 
However  there is recent work that shows matrix completion can suceed without these assumptions  PimentelAlarc on et al      Chen et al    that gives reason
to believe random samples in the original space may allow
completion in the lifted space  and our empirical results in
Section   support this rationale 

  Case Study  Union of Af ne Subspaces
  union of af ne subspaces can be modeled as an
For example  with               
algebraic variety 
the union of
the plane       and the line
      is the zeroset of the quadratic polynomial
            
In general  if        Rn are af ne
spaces of dimension    and    respectively 
then we
can write           fi        for                and
          gi        for                where the fi
and gi are af ne functions  The union       can be expressed as the common zero set of all possible products of
the fi and gi               is the common zero set of  
system of               quadratic equations  Similarly 
  union of   af ne subspaces of dimensions      rk is  
      ri  polynomial

variety described by   system of cid  

equations of degree   
In this section we establish bounds on the feature space
rank for data belonging to   union of af ne subspaces  We
will make use of the following lemma that shows the dimension of   vanishing ideal is  xed under an af ne change
of variables 
Lemma   Let     Rn   Rn be an af ne change of variables               Ax      where     Rn and     Rn  
is invertible  Then for any     Rn 

dimId      dimId      

 
We omit the proof for brevity  but the result is elementary and relies on the fact the degree of   polynomial is
unchanged under an af ne change of variables  Our next
result establishes   bound on the feature space rank for  
single af ne subspace 
Proposition   If the columns of   matrix       belong to
an af ne subspace of dimension at most    then

for all      

rank        

 

 

 
Proof  By Lemma   dimId    is preserved under an
af ne transformation of    Note that we can always  nd
an af ne change of variables     Ax     with invertible     Rn   and     Rn such that in the coordinates

 cid      
 cid 

          yn  the variety   becomes

                yr                     yr     

For any polynomial        cid        the only mono 

mial terms in       that do not vanish on   are those of
the form   
    Furthermore  any polynomial in just
these monomials that vanishes on all of   must be the zero
polynomial  since the      yr are free variables  Hence 

       

 

Sd      span   

       

 

                 

 

     the nonvanishing ideal coincides with the space of
polynomials in   variables of degree at most    which has

dimension cid    

 cid  proving the claim 

 

We note that for   suf ciently large  the bound in   becomes an equality  provided the data points are distributed
generically within the af ne subspace  meaning they are
not the solution to additional nontrivial polynomial equations of degree at most   
Proposition   shows that points belonging to   single af ne
subspace of dimension   are mapped to   linear subspace of

 cid  under     Therefore  if the columns of  
dimension cid    
  linear subspaces each of dimension at most cid    
 cid  The
linear span of this union has dimension at most   cid    
 cid 

data matrix are drawn from   union of   af ne subspaces of
dimension    their image under    will belong to   union of

 

 

which yields the following result 
Proposition   If the columns of   matrix       belong to
  union of   af ne subspaces each of dimension at most   
then

 

rank          

for all      

 

 

 cid      

 cid 

 

In some cases the bound   is  nearly  tight  For example  if the data lies on the union of two rdimensional af ne
subspaces   and   that are mutually orthogonal  one can

 cid    Empirically  we observe

show  rank          cid    
 cid  
ratio is           cid   

 

that the bound in   is orderoptimal with respect to      
In this case  the feature space rank to dimension
and   
  Recall that the minimum samd for    cid     Hence the

pling rate is approximately         
mininum number of samples per column   should be

 

       

 
    

 

This rate is favorable to lowrank matrix completion approaches  which need       kr  for   union of   subspaces having dimension    At  rst glance  this bound suggests it is always better to take the degree   as large as
possible  However  this is only true for suf ciently large   
 The rank is one less than the bound in   because Sd     
Sd    has dimension one  coinciding with the space of constant
polynomials 

Algebraic Variety Models for HighRank Matrix Completion

To take advantage of the improved sampling rate implied
by   according to   we need the number of data vectors per subspace to be   rd  In other words  our model
is able to accommodate more subspaces with larger   but
at the expense of requiring exponentially more data points
per subspace  Note that if the number of data points is suf 
 ciently large  we could take     log   and require only
         observed entries per column  In this case  for
moderately sized               we should choose      
or   In fact  we  nd that for these values of   we get excellent empirical results for the recovery of union of subspaces
data  as shown in Section  

  Algorithm
There are several existing matrix completion algorithms
that could potentially be adapted to solve   relaxation of
the rank minimization problem   such as singular value
thresholding  Cai et al    or alternating minimization
 Jain et al    However  these approaches do not easily
lend themselves to  kernelized  implementations       ones
that do not require forming the highdimensional lifted matrix       explicitly  but instead make use of the ef ciently
computable kernel function for polynomial feature maps  

kd                         xT        

 
For matrices           xs            ys    Rn   
we use kd        to denote the matrix whose       th entry
is kd xi  yj  or equivalently 

kd                                    cid   

 
where     Rs   is the matrix of all ones  and  cid   denotes the entrywise dth power of   matrix    kernelized
implentation is critical for even modest sizes of    since the
number of rows of the lifted matrix scales exponentially
with   
One class of algorithm that kernelizes very naturally is the
iterative reweighted least squares  IRLS  approach of  Fornasier et al    Mohan   Fazel    for lowrank matrix completion  The algorithm also has the advantage of
being able to accommodate the nonconvex Schattenp relaxation of the rank penalty  in addition to the convex nuclear norm relaxation  Speci cally  we use an IRLS approach to solve the following varietybased matrix completion  VMC  optimization problem 

 cid     cid pSp

min
 

                

 VMC 

 Strictly speaking  kd is not kernel associated with the polynomial feature map    as de ned in   Instead  it is the kernel
of the related map          
              where    are

appropriately chosen multinomial coef cients 

Algorithm   Kernelized IRLS to solve  VMC 
Require  Initialize              Choose  min 

while not converged do

Step   Inverse power of kernel matrix
    kd      
          eig   
              

      

 

 

Step   Projected gradient descent step
       
               cid  kd      
                  
    max   min 

end while

where  cid    cid Sp is the Schattenp quasinorm de ned as

 cid    cid Sp  cid cid 

          cid   

             

 

with        denoting the ith singular value of     Algorithm   gives the pseudocode of the proposed IRLS algorithm for solving  VMC  which we derive below 
First  consider the simpler problem of minimizing the
Schattenp norm of   matrix variable   belonging to   constraint set    The main idea behind the IRLS approach is
reexpress the Schattenp quasinorm as

 cid    cid pSp

 

  tr        

      tr           

 
    Note if   is treated as conwhere             
stant  then   is   smooth  quadratic function of     This
motivates the following iterative approach 

 

Wn       
Yn    arg min

  Yn      

tr        Wn 

 

   

    

Here    is   sequence of smoothing parameters satisfying
      min as       where  min is close to zero  which
is included to improve numerical stability and avoid local
minima  this is equivalent to minimizing   smooth approximation of the Schattenp cost  Mohan   Fazel   
Making the substitution           in the above derivation  gives the following approach for solving  VMC 

Wn      Xn  Xn     nI 

 

   

Xn    arg min

 

tr        Wn                  

Rather than  nding the exact minimum in the   update  which could be costly 
following the approach
in  Mohan   Fazel    we instead take   single projected gradient descent step to update     
straightforward calculation shows that
the gradient of

Algebraic Variety Models for HighRank Matrix Completion

    Union of Subspaces

    Parametric Data

Figure   Phase transitions for matrix completion of synthetic variety data  In     we simulate data belonging to   union of   subspaces
for varying    In     we simulate data belonging union of few parametric curves and surfaces having known feature space rank    We
randomly undersample each column of the data matrix at the rate      The grayscale values   indicate the fraction of random trials
where the columns of the data matrix were successfully recovered up to the speci ed percentage  white is success  black is failure  In
all  gures the red dashed line indicates the predicted minimal sampling rate         determined by  

the objective         tr           
is given by
              cid  kd       where  cid  denotes an
entrywise product  Hence   projected gradient step with
stepsize        is given by

 Xn   Xn    nXn Wn  cid  kd Xn  Xn 
Xn                Xn 

 

Similar to  Mohan   Fazel    one can show that every limit point of the above iterates converges to   stationary point of   smoothed Schattenp cost for appropriate
choices of stepsizes     Heuristics are given in  Mohan
  Fazel    for updating the smoothing parameter    
which we adopt as well  Speci cally  we set         
where   and   are userde ned parameters  and update
        
  The appropriate choice of   and   will
depend on the scaling and spectral properties of the data 
Empirically  we  nd that setting        max  where
 max is the largest eigenvalue of the kernel matrix obtained
from the initialization  and       work well in   variety of settings  For all our experiments in Section   we   
      which was found to give the best matrix recovery
results for synthetic data  We also use   zero lled initialization    in all cases 
  Numerical Experiments
  Empirical validation of sampling bounds

In Figure   we report the results of two experiments to validate the predicted minimum sampling rate   in   on synthetic variety data 
In the  rst experiment we generated
      data matrices whose columns belong to   union of  
subspaces each of dimension    with              

      In the second experiment we generated data matrices of size       whose columns belong to   union
of randomly generated parametric surfaces of low dimension  where we sorted each dataset by its empirically determined feature space rank    For both experiments  we
undersampled each column of the matrix taking   entries
uniformly at random at various values of   and    and then
attempted to recover the missing entries using our proposed
IRLS algorithm for VMC  Algorithm   with       for
        For the union of subspaces data  we also compare with lowrank matrix completion in the original matrix domain via nuclear norm minimization  LRMC  and
nonconvex Schatten  minimization  LRMCNCVX 
implemented using Algorithm   with   linear kernel       
in   We said   column was successfully recovered if
 cid       cid cid   cid      where   is the recovered column
and    is the original column  For each pair of parameters
       or        we perform   random trials to determine
the probability of successful recovery 
Consistent with our theory  VMC is successful at recovering most of the data columns above the predicted minimum
sampling rate  substantially extending the range of recovery over LRMC  While VMC often fails to recover   of
the columns near the predicted rate  in fact   large proportion of the columns   are still successfully completed  Sometimes the recovery dips below the predicted
rate       VMC        in Fig      and VMC        in
Fig      However  since the predicted rate relies on what
is likely an overestimate of the true degrees of freedom  it
is not surprising that the VMC algorithm occasionally succeeds below this rate  too 

Algebraic Variety Models for HighRank Matrix Completion

  Motion segmentation of real data
In Figure   we apply VMC to the problem of motion segmentation  Kanatani    with missing data using the
Hopkins   dataset  Tron   Vidal    This data consists of several feature points tracked across frames of the
video  We reproduce the experimental setting in  Yang
et al    and simulate highrank data by undersampling frames of the dataset  We simulate missing trajectories by sampling uniformly at random from the feature
points across all frames  To obtain   clustering we  rst
completed the missing entries using VMC and then ran
the sparse subspace clustering  SSC  algorithm  Elhamifar   Vidal    on the result  calling this VMC SSC 
  similar approach of standard LRMC followed by SSC
 LRMC SSC  provides   consistent baseline for subspace
clustering with missing data  Yang et al    Elhamifar    We also compare against SSC with entrywise zero ll  SSCEWZF   Yang et al    We  nd
the VMC SSC approach gives similar or lower clustering
error than LRMC SCC for low missing rates  Likewise 
VMC SSC also substantially outperforms SSCEWZF for
high missing rates  Unlike SSCEWZF and the other algorithms in  Yang et al    VMC SSC also succeeds
in setting where the data is lowrank       when all frames
are retained  This is because the performance of VMC is
similar to standard LRMC in the lowrank setting 

Figure   Subspace clustering error on Hopkins   dataset for
varying rates of missing data and undersampling of frames 

  Completion of motion capture data

In Figure   we demonstrate VMC for completing timeseries trajectories from motion capture sensors using  
dataset from the CMU Mocap database   subject   trial
  Empirically  this dataset has been shown to be locally
lowrank over the time frames corresponding to each separate activity  and can be modeled as   union of subspaces
 Elhamifar    The data had measurements from    
  sensors at       time instants  We randomly undersampled the columns of this matrix and attempt to complete
the data using VMC  LRMC  and LRMCNCVX and measure the resulting completion error   cid       cid    cid   cid    
where   is the recovered matrix and    is the original
matrix  Similar to results on synthetic data  we  nd the

 http mocap cs cmu edu

VMC approach outperforms LRMCNCVX for appropriately chosen degree    In particular  VMC with        
perform similar for small missing rates  but VMC      
gives lower completion error over       for large missing
rates  consistent with the results in Figure  

Figure   Completion error on CMU Mocap dataset using the proposed VMC approach compared with convex and nonconvex
LRMC algorithms 

  Conclusion
We introduce   matrix completion approach that generalizes lowrank matrix completion to   much wider class of
variety models  including data belonging to   union of subspaces  We present   hypothesized sampling complexity
bound for the completion of   matrix whose columns belong to an algebraic variety    surprising result of our
analysis that that   union of   af ne subspaces of dimension   should be recoverable from   rk    measurements
per column  provided we have   rd  data points  columns 
per subspace  where   is the degree of the feature space
map  In particular  if we choose     log    then we need
only      measurements per column as long as we have
  rlog    columns per subspace  We additionally introduce
an ef cient algorithm based on an iterative reweighted least
squares approach that realizes these hypothesized bounds
on synthetic data  and reaches stateof theart performance
on for matrix completion on several real highrank datasets 
Our algorithm can easily accommodate other smooth kernels  including the popular Gaussian RBF kernel  Muller
et al      similar optimization formulation to ours
was presented in the recent preprint  Garg et al    using Gaussian RBF kernels in place of polynomial kernels 
showing good empirical results in   matrix completion context  However  analysis of the sample complexity in this
case is complicated by the fact that   feature space representation for Gaussian RBF kernel is necessarily in nitedimensional  Understanding the sample requirements in
this case would be an interesting avenue for future work 
Acknowledgements
For this work  Balzano and Ongie were supported in part
by ARO grant   NF  Willett and Nowak
were supported in part by NSF IIS  NSF CCF 
  and NIH      AI  and Nowak also
by AFOSR FA 

 missing rate clustering errorall framesSSCEWZFLRMC SSCVMC SSC   VMC SSC   missing rate clustering error  frames missing rate clustering error  frames missing rate completion errorLRMCLRMCNCVXVMC    VMC    Algebraic Variety Models for HighRank Matrix Completion

References
Cai  JianFeng  Cand es  Emmanuel    and Shen  Zuowei   
singular value thresholding algorithm for matrix completion  SIAM Journal on Optimization   
 

Candes  Emmanuel and Recht  Benjamin  Exact matrix
completion via convex optimization  Communications
of the ACM     

Chen  Guangliang  Atev  Stefan  and Lerman  Gilad  KerIn Computer
nel spectral curvature clustering  kscc 
Vision Workshops  ICCV Workshops    IEEE  th
International Conference on  pp    IEEE   

Chen  Yudong  Bhojanapalli  Srinadh  Sanghavi  Sujay 
and Ward  Rachel  Coherent matrix completion  In Proceedings of The  st International Conference on Machine Learning  pp     

Cox  David    Little  John  and   Shea  Donal 

Ideals 
Varieties  and Algorithms  Springer International Publishing   

Derksen  Harm  Hilbert series of subspace arrangements 
Journal of pure and applied algebra   
 

Elhamifar  Ehsan  Highrank matrix completion and clustering under selfexpressive models  In Advances in Neural Information Processing Systems  pp     

Elhamifar  Ehsan and Vidal  Ren    Sparse subspace clusIn Computer Vision and Pattern Recognition 
tering 
  CVPR   IEEE Conference on  pp   
IEEE   

Elhamifar  Ehsan and Vidal  Ren    Sparse subspace clustering  Algorithm  theory  and applications  IEEE transactions on pattern analysis and machine intelligence   
   

Eriksson  Brian  Balzano  Laura  and Nowak  Robert   
Highrank matrix completion  In AISTATS  pp   
 

Fornasier  Massimo  Rauhut  Holger  and Ward  Rachel 
Lowrank matrix recovery via iteratively reweighted
SIAM Journal on Optileast squares minimization 
mization    oct   doi   
 

Ganti  Ravi Sastry  Balzano  Laura  and Willett  Rebecca 
Matrix completion under monotonic single index models  In Advances in Neural Information Processing Systems  pp     

Garg  Ravi  Eriksson  Anders  and Reid  Ian  Nonlinear
dimensionality regularizer for solving inverse problems 
arXiv preprint arXiv   

Jain  Prateek  Netrapalli  Praneeth  and Sanghavi  Sujay  Lowrank matrix completion using alternating minimization  In Proceedings of the forty fth annual ACM
symposium on Theory of computing  pp    ACM 
 

Kanatani  Kenichi  Motion segmentation by subspace
In Computer Vision 
separation and model selection 
  ICCV   Proceedings  Eighth IEEE International Conference on  volume   pp    IEEE 
 

Lee  Joonseok  Kim  Seungyeon  Lebanon  Guy  and
Singer  Yoram  Local lowrank matrix approximation 
ICML      

Mohan  Karthik and Fazel  Maryam  Iterative reweighted
algorithms for matrix rank minimization  The Journal of
Machine Learning Research     

Muller  KR  Mika  Sebastian  Ratsch  Gunnar  Tsuda 
Koji  and Scholkopf  Bernhard  An introduction to
kernelbased learning algorithms  IEEE Transactions on
Neural Networks     

Nguyen  Minh   and Torre  Fernando  Robust kernel principal component analysis  In Advances in Neural Information Processing Systems  pp     

PimentelAlarc on     Balzano     Marcia     Nowak    
and Willett     Groupsparse subspace clustering with
missing data  In Statistical Signal Processing Workshop
 SSP    IEEE  pp    IEEE     

PimentelAlarcon  Daniel and Nowak  Robert 

The
informationtheoretic requirements of subspace clustering with missing data  In Proceedings of The  rd International Conference on Machine Learning  pp   
 

PimentelAlarc on  Daniel    Boston  Nigel  and Nowak 
Robert      characterization of deterministic sampling
patterns for lowrank matrix completion  IEEE Journal
of Selected Topics in Signal Processing   
   

Rao  Nikhil  Ganti  Ravi  Balzano  Laura  Willett  Rebecca 
and Nowak  Robert  On learning high dimensional strucIn Proceedings of the  st
tured single index models 
AAAI conference on arti cial intelligence   

Sanguinetti  Guido and Lawrence  Neil    Missing data
In European Conference on Machine

in kernel PCA 
Learning  pp    Springer   

Algebraic Variety Models for HighRank Matrix Completion

Song  Dogyoon  Lee  Christina    Li  Yihua  and Shah  Devavrat  Blind regression  Nonparametric regression for
latent variable models via collaborative  ltering  In Advances in Neural Information Processing Systems  pp 
   

Tron  Roberto and Vidal  Ren      benchmark for the comparison of    motion segmentation algorithms  In Computer Vision and Pattern Recognition    CVPR 
IEEE Conference on  pp    IEEE   

Tsakiris  Manolis   and Vidal  Ren    Algebraic clustering
of af ne subspaces  arXiv preprint arXiv 
 

Vidal  Ren    Soatto  Stefano  Ma  Yi  and Sastry  Shankar 
An algebraic geometric approach to the identi cation of
  class of linear hybrid systems  In Decision and Control    Proceedings   nd IEEE Conference on  volume   pp    IEEE   

Vidal  Ren    Ma  Yi  and Sastry  Shankar  Generalized
principal component analysis  GPCA  IEEE Transactions on Pattern Analysis and Machine Intelligence   
   

Vidal  Ren    Ma  Yi  and Sastry  Shankar  Generalized
Principal Component Analysis  Springer New York 
 

Yang  Congyuan  Robinson  Daniel  and Vidal  Ren   
Sparse subspace clustering with missing entries  In Proceedings of The  nd International Conference on Machine Learning  pp     

