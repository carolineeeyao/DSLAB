ZipML  Training Linear Models with Endto End Low Precision 

and   Little Bit of Deep Learning

Hantian Zhang   Jerry Li   Kaan Kara   Dan Alistarh     Ji Liu   Ce Zhang  

Abstract

Recently there has been signi cant interest in
training machinelearning models at low precision  by reducing precision  one can reduce computation and communication by one order of
magnitude  We examine training at reduced precision  both from   theoretical and practical perspective  and ask  is it possible to train models
at endto end low precision with provable guarantees  Can this lead to consistent orderof 
magnitude speedups  We mainly focus on linear
models  and the answer is yes for linear models 
We develop   simple framework called ZipML
based on one simple but novel strategy called
double sampling  Our ZipML framework is able
to execute training at low precision with no bias 
guaranteeing convergence  whereas naive quantization would introduce signi cant bias  We validate our framework across   range of applications  and show that it enables an FPGA prototype that is up to   faster than an implementation using full  bit precision  We further develop   varianceoptimal stochastic quantization
strategy and show that it can make   signi cant
difference in   variety of settings  When applied
to linear models together with double sampling 
we save up to another   in data movement
compared with uniform quantization  When
training deep networks with quantized models 
we achieve higher accuracy than the stateof theart XNORNet 

 ETH Zurich  Switzerland  Massachusetts Institute of
Technology  USA  IST Austria  Austria
 University of
Rochester  USA  Correspondence to  Hantian Zhang  hantian zhang inf ethz ch  Ce Zhang  ce zhang inf ethz ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Overview of theoretical results and highlights of empirical results  See Introduction for details 

  Introduction
The computational cost and power consumption of today  
machine learning systems are often driven by data movement  and by the precision of computation 
In our experience  in applications such as tomographic reconstruction  anomaly detection in mobile sensor networks  and
compressive sensing  the overhead of transmitting the data
samples can be massive  and hence performance can hinge
on reducing the precision of data representation and associated computation    similar trend is observed in deep
learning  where impressive progress has been reported
with systems using endto end reducedprecision representations  Hubara et al    Rastegari et al    Zhou
et al    Miyashita et al    The empirical success of these works inspired this paper  in which we try
to provide   theoretical understanding of endto end lowprecision training for machine learning models 
In this
context  the motivating question behind our work is  When

    Linear Regression       Reconstruction Bit Bit    FPGA Speed Up    Deep LearningMachine  Learning  ModelsData Movement ChannelsSpeed up because of our techniquesGradientInput SamplesModelLinear ModelsDe Sa et la  Alistarh et al    Double Sampling   DataOptimal EncodingStochastic RoundingVery Signi cant  Speed up  Up to    Deep LearningCourbariaux et al  Rastegari et al   DataOptimal EncodingSigni cant Speed up bit Full PrecisionDouble Sampling  bit EpochsTraining Loss Epochs    Linear Regression    LSSVM     bit Full PrecisionDouble Sampling  bit Hogwild FPGA  bitTime  seconds Training LossTime  seconds    Linear Regression    LSSVM     FPGA  bitFPGA  bitHogwild FPGA  bit bit Full PrecisionXNOR Optimal Epochs Uniform  bitTraining Loss Epochs bit Full PrecisionOptimal  bit  Uniform  bit overlap     bit Full Precision    Linear Model    Deep LearningTime  seconds    Logistic Regression Training LossTime  seconds    SVM   Chebyshev  bit bit Full Precision Chebyshev  bit bit Full PrecisionDeterministic Rounding  bit overlap     bit Full PrecisionDeterministic Rounding  bit overlap     bit Full PrecisionZipML  Training Linear Models with Endto End Low Precision

  cid 

  

training general machine learning models  can we lower
the precision of data representation  communication  and
computation  while maintaining provable guarantees 

In this paper  we develop ZipML    general framework to
answer this question  and present results obtained in the
context of this ZipML framework  Figure   encapsulates
our results      for linear models  we are able to lower
the precision of both computation and communication  including input samples  gradients  and model  by up to  
times  while still providing rigorous theoretical guarantees 
    our FPGA implementation of this framework achieves
up to   speedup compared with    bit FPGA implementation  or with    core CPU running Hogwild     
we are able to decrease data movement by   for tomographic reconstruction  while obtaining   negligible quality decrease  Elements of our framework generalize to    
model compression for training deep learning models  In
the following  we describe our technical contributions in
more detail 

  Summary of Technical Contributions

We consider the following problem in training generalized
linear models 

min

 

 

 
  

    cid 

     bk        

 

where    is   loss function and   is   regularization term
that could be  cid  norm   cid  norm  or even an indicator function representing the constraint  The gradient at the sample
 ak  bk  is 

gk   ak

     cid 
     bk 
   cid 
   

 

We denote the problem dimension by    We consider
the properties of the algorithm when   lossy compression scheme is applied to the data  samples  gradient 
and model 
to reduce the communication cost of the
algorithm that is  we consider quantization functions Qg 
Qm  and Qs for gradient  model  and samples  respectively 
in the gradient update 
xt    prox     xt    Qg gk Qm xt  Qs at   
 

where the proximal operator is de ned as

prox        argmin

 

 
 

 cid       cid         

Our Results  We summarize our results as follows 
Linear Models  When    is the least squares loss  we
 rst notice that simply doing stochastic quantization of data

samples       Qs  introduces bias of the gradient estimator
and therefore SGD would converge to   different solution 
We propose   simple solution to this problem by introducing   double sampling strategy  Qs that uses multiple samples to eliminate the correlation of samples introduced by
the nonlinearity of the gradient  We analyze the additional
variance introduced by double sampling  and  nd that its
impact is negligible in terms of convergence time as long
as the number of bits used to store   quantized sample is at
least  log    where   is the variance of the standard
stochastic gradient  This implies that the  bit precision
may be excessive for many practical scenarios 
We build on this result to obtain an endto end quantization strategy for linear models  which compresses all data
movements  For certain settings of parameters  endto end
quantization adds as little as   constant factor to the variance of the entire process 

Optimal Quantization and Extension to Deep Learning 
We then focus on reducing the variance of stochastic quantization  We notice that different methods for setting the
quantization points have different variances the standard
uniformlydistributed quantization strategy is far from optimal in many settings  We formulate this as an independent optimization problem  and solve it optimally with an
ef cient dynamic programming algorithm that only needs
to scan the data in   single pass  When applied to linear
models  this optimal strategy can save up to   communication compared with the uniform strategy 
We perform an analysis of the optimal quantizations for
various settings  and observe that the uniform quantization approach popularly used by stateof theart endto end
lowprecision deep learning training systems when more
than   bit is used is suboptimal  We apply optimal quantization to models and show that  with one standard neural
network  we outperform the uniform quantization used by
XNORNet and   range of other recent approaches  This
is related  but different  to recent work on model compression for inference  Han et al    To the best of our
knowledge  this is the  rst time such optimal quantization
strategies have been applied to training 

  Linear Models
In this section  we focus on linear models with possibly
nonsmooth regularization  We have labeled data points
                       aK  bK    Rn      and our goal
is to minimize the function

  cid 

  

       

 
 

 cid 

       

 

 cid   cid 

      bk cid 
 cid 
 cid cid 

 

      

ZipML  Training Linear Models with Endto End Low Precision

in sensor networks  and the associated computation      
each register can hold more numbers  This motivates us
to use lowprecision sample points to train the model  The
following will introduce the proposed lowprecision SGD
framework by meeting all three factors for SGD 

  BandwidthEf cient Stochastic Quantization

We propose to use stochastic quantization to generate  
lowprecision version of an arbitrary vector   in the following way  Given   vector    let       be   scaling factor
such that                 Without loss of generality 
let             We partition the interval     using
      separators                ls     for each
number   in         we quantize it to one of two nearest
separators  li       li  We denote the stochastic quantization function by         and choose the probability of
quantizing to different separators such that               
We use      when   is not relevant 

  Double Sampling for Unbiased Stochastic Gradient

The naive way to use lowprecision samples  at
 
  at  is

 gt    at   cid 

       atbt 
the naive apHowever 
proach does not work
 that is  it does not guarantee convergence  because
it is biased 

  gt    ata cid 

      atbt   Dax 

where Da is diagonal and its ith diagonal element is

    ai      
   

Since Da is nonzero  we obtain   biased estimator of the
gradient  so the iteration is unlikely to converge  The  gure
on the right illustrates the bias caused by   nonzero Da  In
fact  it is easy to see that in instances where the minimizer
  is large and gradients become small  we will simply diverge 
We now present   simple method to    the biased gradient
estimator  We generate two independent random quantizations and revise the gradient 

gt     at   at cid     bt   

 

This gives us an unbiased estimator of the gradient 

Overhead of Storing Samples  The reader may have noticed that one implication of double sampling is the overhead of sending two samples instead of one  We note that

Figure     schematic representation of the computational model 

     minimize the empirical least squares loss plus   nonsmooth regularization           cid  norm   cid  norm  and
constraint indicator function  SGD is   popular approach
for solving largescale machine learning problems 
It
works as follows  at step xt  given an unbiased gradient
estimator gt  that is    gt        xt  we update xt  by

xt    prox tR   xt    tgt   

where    is the prede ned step length  SGD guarantees the
following convergence property 
Theorem          Bubeck    Theorem   Let the
sequence  xt  
   be bounded  Appropriately choosing the
steplength  we have the following convergence rate for  

 cid 

 cid 

 

 
 

  cid 

  

 cid 

 cid   

 

xt

  min

 

         

 

 
 

 

where   is the upper bound of the mean variance

  cid 

  

     
 

  cid gt       xt cid 

There are three key requirements for SGD to converge 

  Computing stochastic gradient gt is cheap 
  The stochastic gradient gt should be unbiased 
  The stochastic gradient variance   dominates the convergence ef ciency  so it needs to be controlled appropriately 

The common choice is to uniformly select one sample 

gt       ull 

 

        cid 

           

 

    is   uniformly random integer from   to    We
abuse the notation and let at        Note that     ull 
        xt  Although it
is an unbiased estimator       ull 
has received success in many applications  if the precision
of sample at can be further decreased  we can save potentially one order of magnitude bandwidth of reading at      

 

 

ComputationStorageSampleStoreModelStoreGradientDeviceUpdateDeviceSampleModelGradientComputationStorageSampleStoreModelStoreGradientDeviceUpdateDevice Hard Drive DRAM CPU    Computation Model    One Example Realisation of the Computation Model  For singleprocessor systems  GradientDevice and UpdateDevice are often the same device    bit Full PrecisionDeterministic RoundingNaive Stochastic Sampling Our Approach Epochs Training LossZipML  Training Linear Models with Endto End Low Precision

this will not introduce   overhead in terms of data communication  Instead  we start from the observation that the
two samples can differ by at most one bit  For example 
to quantize the number   to either   or   Our strategy
is to  rst store the smallest number of the interval  here
  and then for each sample  send out   bit to represent
whether this sample is at the lower marker   or the upper marker   Under this procedure  once we store the
base quantization level  we will need one extra bit for each
additional sample  More generally  since samples are used
symmetrically  we only need to send   number representing the number of times the lower quantization level has
been chosen among all the sampling trials  Thus  sending
  samples only requires log    more bits 

  Variance Reduction

the mean variance

  cid gt  
From Theorem  
      cid  will dominate the convergence ef ciency  It is
not hard to see that the variance of the double sampling
based stochastic gradient in   can be decomposed into

 
 

 

 cid 

  cid gt       xt cid      cid     ull 

      xt cid 

 

    cid gt       ull 

 cid 

 

 

The  rst term is from the full stochastic gradient  which can
be reduced by using strategies such as minibatch  weight
sampling  and so on  Thus  reducing the  rst term is an
orthogonal issue for this paper  Rather  we are interested in
the second term  which is the additional cost of using lowprecision samples  All strategies for reducing the variance
of the  rst term can seamlessly combine with the approach
of this paper  The additional cost can be bounded by the
following lemma 
Lemma   The stochastic gradient variance using double
sampling in     cid gt       ull 

 cid  can be bounded by

 cid     at     at cid    cid    cid     cid   cid 

    cid     cid    cid    cid cid at cid cid   

 

where     at      cid   at    at cid  and  cid  denotes the element product 
Thus  minimizing     at  is key to reducing variance 

Uniform quantization 
It makes intuitive sense that  the
more levels of quantization  the lower the variance  The
following makes this quantitative dependence precise 
Lemma    Alistarh et al    Assume that quantization levels are uniformly distributed  For any vector
    Rn  we have that                Further  the variance of uniform quantization with   levels is bounded by
             cid          cid 
    cid   cid 

    min     

 

   

   ull     cid     cid   

Together with other results  it suggests the stochastic gradient variance of using double sampling is bounded by

  cid gt       xt cid     
   ull      cid     ull 

 

        cid  is the upper bound
where  
of using the full stochastic gradient  assuming that   and
all ak   are bounded  Because the number of quantization levels   is exponential to the number of bits we use
to quantize  to ensure that these two terms are comparable
 using   lowprecision sample does not degrade the convergence rate  the number of bits only needs to be greater than
 log     ull  Even for linear models with millions of
features    bits is likely to be  overkill 

  Optimal Quantization Strategy for

Reducing Variance

In the previous section  we have assumed uniformly distributed quantization points  We now investigate the choice
of quantization points and present an optimal strategy to
minimize the quantization variance term     at 

Problem Setting  Assume   set of real numbers    
            xN  with cardinality    WLOG  assume that all
numbers are in     and that              xN  
The goal is to partition      Ij  
   of     into   disjoint
intervals  so that if we randomly quantize every     Ij to
an endpoint of Ij  the variance is minimal over all possible
partitions of     into   intervals  Formally 

 cid 

  cid 

  

MV     

 
 

min
     

    

  cid 

Ij      

err xi  Ij 

xi Ij
Ij   lk     for    cid    

 

  

where err                       is the variance for point
      if we quantize   to an endpoint of            That
is  err       is the variance of the  unique  distribution  
supported on      so that EX          
Given an interval         we let    be the set of
 cid 
xj     contained in    We also de ne err      
err     cid 
xj   err xj     Given   partition   of     we let
    err     We let the optimum solution
be      argmin     err    breaking ties randomly 

  Dynamic Programming

We  rst present   dynamic programming algorithm that
solves the above problem in an exact way  In the next subsection  we present   more practical approximation algorithm that only needs to scan all data points once 

ZipML  Training Linear Models with Endto End Low Precision

Figure   Optimal quantization points calculated with dynamic
programming given   data distribution 

This optimization problem is nonconvex and nonsmooth 
We start from the observation that there exists an optimal
solution that places endpoints at input points 
Lemma   There is      so that all endpoints of any    
   are in        

Therefore  to solve the problem in an exact way  we just
need to select   subset of data points in   as quantization points  De ne          be the optimal total variance
for points in   dm  with   quantization levels choosing
dm   xm for all              Our goal is to calculate           This problem can be solved by dynamic
programing using the following recursion

min

          

                                 
where          denotes the total variance of points falling
in the interval  dj  dm  The complexity of calculating the
matrix     is             and the complexity of calculating the matrix     is   kN   The memory cost is
  kN      

  Heuristics

The exact algorithm has   complexity that is quadratic in
the number of data points  which may be impractical  To
make our algorithm practical  we develop an approximation
algorithm that only needs to scan all data points once and
has linear complexity to   

Discretization  We can discretize the range     into
  intervals                      dM    with    
              dM      We then restrict our algorithms to only choose   quantization points within these  
points  instead of all   points in the exact algorithm  The
following result bounds the quality of this approximation 
Theorem   Let the maximal number of data points in each
 small interval   de ned by  dm   
     and the maximal
length of small intervals be bounded by bN   and     
respectively  Let         
   be the
optimal quantization to   and the solution with discretization  Let cM   be the upper bound of the number of small
intervals crossed by any  large interval   de ned by   
Then we have the discretization error bounded by

    
   and          

    

MV      MV        bk

      

  bc 
   

 

Theorem   suggests that the mean variance using the discrete varianceoptimal quantization will converge to the optimal with the rate       

Dynamic Programming with Candidate Points  Notice
that we can apply the same dynamic programming approach given   candidate points 
In this case  the total
computational complexity becomes                 
with memory cost   kM       Also  to  nd the optimal
quantization  we only need to scan all   numbers once 
Figure   illustrates an example output for our algorithm 

 Approximation in AlmostLinear Time 
In the full
version of this paper  Zhang et al    we present an
algorithm which  given   and    provides   split using
at most    intervals  which guarantees    approximation
of the optimal variance for   intervals  using     log    
time  This is   new variant of the algorithm by  Acharya
et al    for the histogram recovery problem  We can
use the    intervals given by this algorithm as candidates
for the DP solution  to get   general  approximation using
  intervals in time     log       

  Applications to Deep Learning

In this section  we show that it is possible to apply optimal
quantization to training deep neural networks 

Stateof theart  We focus on training deep neural networks with   quantized model  Let   be the model and
     be the loss function  Stateof theart quantized networks  such as XNORNet and QNN  replace   with the
quantized version      and optimize for

minW       

With   properly de ned   
     we can apply the standard
backprop algorithm  Choosing the quantization function
  is an important design decision  For  bit quantization 
XNORNet searches the optimal quantization point  However  for multiple bits  XNORNet  as well as other approaches such as QNN  resort to uniform quantization 

Optimal Model Quantization for Deep Learning  We
can apply our optimal quantization strategy and use it as the
quantization function   in XNORNet  Empirically  this
results in quality improvement over the default multibits
quantizer in XNORNet  In spirit  our approach is similar
to the  bit quantizer of XNORNet  which is equivalent
to our approach when the data distribution is symmetric 
we extend this to multiple bits in   principled way  Another related work is the uniform quantization strategy in
log domain  Miyashita et al    which is similar to our
approach when the data distribution is  log uniform  However  our approach does not rely on any speci   assumption

Optimal Quantization PointsZipML  Training Linear Models with Endto End Low Precision

Dataset

Synthetic  
Synthetic  
Synthetic  
YearPrediction

cadata
cpusmall

Dataset
codrna
gisette

Dataset
CIFAR 

Dataset

Regression

Training Set
 
 
 
 
 
 
Classi cation

Testing Set
 
 
 
 
 
 

Training Set
 
 

Testing Set
 
 

Deep Learning

Training Set
 

Testing Set
 
Tomographic Reconstruction
  Projections Volumn Size
 

 

  Features
 
 
 
 
 
 

  Features
 
 

  Features
         

Proj  Size
 

Table   Dataset statistics 

of the data distribution  Han et al    use kmeans to
compress the model for inference  kmeans optimizes for
  similar  but different  objective function than ours  In this
paper  we develop   dynamic programming algorithm to do
optimal stochastic quantization ef ciently 

  Experiments
We now provide an empirical validation of our ZipML
framework 

Experimental Setup  Table   shows the datasets we use 
Unless otherwise noted  we always use diminishing stepsizes     where   is the current number of epoch  We
tune   for the full precision implementation  and use the
same initial step size for our lowprecision implementation 
 Theory and experiments imply that the lowprecision implementation often favors smaller step size  Thus we do
not tune step sizes for the lowprecision implementation 
as this can only improve the accuracy of our approach 

Summary of Experiments  Due to space limitations  we
only report on Synthetic   for regression  and on gisette
for classi cation  The full version of this paper  Zhang
et al    contains   several other datasets  and discusses   different factors such as impact of the number
of features  and   refetching heuristics  The FPGA implementation and design decisions can be found in  Kara
et al   

  Convergence on Linear Models

We validate that   with double sampling  SGD with
low precision converges in comparable empirical convergence rates to the same solution as SGD with full pre 

Figure   Linear models with endto end low precision 

cision  and   implemented on FPGA  our lowprecision
prototype achieves signi cant speedup because of the decrease in bandwidth consumption 

Convergence  Figure   illustrates the result of training
linear models      linear regression and     least squares
SVMs  with endto end lowprecision and full precision 
For low precision  we pick the smallest number of bits that
results in   smooth convergence curve  We compare the
 nal training loss in both settings and the convergence rate 
We see that  for both linear regression and least squares
SVM  using   or  bit is always enough to converge to
the same solution with comparable convergence rate  This
validates our prediction that doublesampling provides an
unbiased estimator of the gradient  Considering the size of
input samples that we need to read  we could potentially
save   memory bandwidth compared to using  bit 

Speedup  We implemented our lowprecision framework
on   stateof theart FPGA platform  The detailed implementation is described in  Kara et al    This implementation assumes the input data is already quantized and
stored in memory  data can be quantized during the  rst
epoch 
Figure   illustrates the result of   our FPGA implementation with quantized data    FPGA implementation with
 bit data  and   Hogwild  running with   CPU cores 
Observe that all approaches converge to the same solution  FPGA with quantized data converges   faster than
FPGA with full precision or Hogwild  The FPGA implementation with full precision is memorybandwidth bound 
and by using our framework on quantized data  we save up
to   memorybandwidth  which explains the speedup 

now

of MiniBatching  We

Impact
validate
the sensitivity  of the algorithm to the precision under batching  Equation   suggests that  as we increase
batch size 
the variance term corresponding to input
quantization may start to dominate the variance of the
stochastic gradient  However  in practice and for reasonable parameter settings  we found this does not occur 
convergence trends for small batch size         are the

 bit Full PrecisionDouble Sampling  bit EpochsTraining Loss Epochs    Linear Regression    LSSVM     bit Full PrecisionDouble Sampling  bit ZipML  Training Linear Models with Endto End Low Precision

Figure   FPGA implementation of linear models 

Figure   Optimal quantization strategy 

Figure   Impact of Using MiniBatch  BS Batch Size 

same as for larger sizes         Figure   shows that  if
we use larger minibatch size   we need more epochs
than using smaller minibatch size   to converge  but
for the quantized version  actually the one with larger
minibatch size converges faster 

  DataOptimal Quantization Strategy

We validate that  with our dataoptimal quantization strategy  we can signi cantly decrease the number of bits that
doublesampling requires to achieve the same convergence 
Figure     illustrates the result of using  bit and  bit
for uniform quantization and optimal quantization on the
YearPrediction dataset  Here  we only consider quantization on data  but not on gradient or model  because to compute the dataoptimal quantization  we need to have access
to all data and assume the data doesn   change too much 
which is not the case for gradient or model  The quantization points are calculated for each feature for both uniform
quantization and optimal quantization  We see that  while
uniform quantization needs  bit to converge smoothly  optimal quantization only needs  bit  We save almost  
number of bits by just allocating quantization points carefully 

Comparision with uniform quantization  We validate
that  with our dataoptimal quantization strategy  we can
signi cantly increase the convergence speed 
Figure   illustrates the result of training linear regression models  with uniform quantization points and optimal quantization points  Here  notice that we only quan 

Figure   Linear regression with quantized data on multiple
datasets 

tize data  but not gradient or model  We see that  if we
use same number of bits  optimal quantization always converges faster than uniform quantization and the loss curve is
more stable  because the variance induced by quantization
is smaller  As   result  with our dataoptimal quantization
strategy  we can either   get up to   faster convergence
speed with the same number of bits  or   save up to  
bits while getting the same convergence speed 
We also see from Figure       to     that if the dataset
has more features  usually we need more bits for quantization  because the variance induced by quantization is higher
when the dimensionality is higher 

  Extensions to Deep Learning

We validate that our dataoptimal quantization strategy
can be used in training deep neural networks  We take
Caffe   CIFAR  tutorial  Caf  and compare three different quantization strategies    Full Precision    XNOR 
  XNORNet implementation that  following the multibits
strategy in the original paper  quantizes data into  ve uniform levels  and   Optimal  our quantization strategy
with  ve optimal quantization levels  As shown in Figure     Optimal  converges to   signi cantly lower train 

 Hogwild FPGA  bitTime  seconds Training LossTime  seconds    Linear Regression    LSSVM     FPGA  bitFPGA  bitHogwild FPGA  bitTraining Loss EpochsBS   bitBS   bitBS   bitBS   bit bit Full PrecisionXNOR Optimal Epochs Uniform  bitTraining Loss Epochs bit Full PrecisionOptimal  bit  Uniform  bit overlap     bit Full Precision    Linear Model    Deep Learning Optimal  bitUniform  bit EpochsTraining Loss Epochs    synthetic     synthetic  Optimal  bitUniform  bit Epochs    synthetic  Optimal  bitUniform  bit Optimal  bitUniform  bit EpochsTraining Loss Epochs    YearPredictionMSD    cadata   Optimal  bitUniform  bit Epochs    cpusmall Optimal  bitUniform  bit ZipML  Training Linear Models with Endto End Low Precision

ing loss compared with XNOR  Also  Optimal  achieves
  points higher testing accuracy over XNOR  This illustrates the improvement obtainable by training   neural
network with   carefully chosen quantization strategy 

FPGA  There have been attempts to lower the precision
when training on such hardware  Kim et al    These
results are mostly empirical  we aim at providing   theoretical understanding  which enables new algorithms 

  Related Work
There has been signi cant work on  lowprecision
SGD   De Sa et al    Alistarh et al    These results provide theoretical guarantees only for quantized gradients  The model and input samples  on the other hand  are
much more dif cult to analyze because of the nonlinearity 
We focus on endto end quantization  for all components 

LowPrecision Deep Learning  Lowprecision training
of deep neural networks has been studied intensively and
many heuristics work well for   subset of networks  OneBit
SGD  Seide et al    provides   gradient compression heuristic developed in the context of deep neural networks for speech recognition  There are successful applications of endto end quantization to training neural networks that result in little to no quality loss  Hubara et al 
  Rastegari et al    Zhou et al    Miyashita
et al    Li et al    Gupta et al    They
quantize weights  activations  and gradients to low precision        bit  and revise the backpropagation algorithm
to be aware of the quantization function  The empirical
success of these works inspired this paper  in which we try
to provide   theoretical understanding of endto end lowprecision training for machine learning models  Another
line of research concerns inference and model compression
of   pretrained model  Vanhoucke et al    Gong et al 
  Han et al    Lin et al    Kim   Smaragdis 
  Kim et al    Wu et al    In this paper  we
focus on training and leave the study of inference for future
work 

LowPrecision Linear Models  Quantization is   fundamental topic studied by the DSP community  and there has
been research on linear regression models in the presence
of quantization error or other types of noise  For example  Gopi et al    studied compressive sensing with
binary quantized measurement  and   twostage algorithm
was proposed to recover the sparse highprecision solution
up to   scale factor  Also  the classic errorsin variable
model  Hall    could also be relevant if quantization
is treated as   source of  error  In this paper  we scope
ourselves to the context of stochastic gradient descent  and
our insights go beyond simple linear models  For SVM the
straw man approach can also be seen as   very simple case
of kernel approximation  Cortes et al   

Other Related Work  Precision of data representation is
  key design decision for con gurable hardwares such as

  Discussion and Future Work
Our motivating question was whether endto end lowprecision data representation can enable ef cient computation with convergence guarantees  We show that ZipML 
  relatively simple stochastic quantization framework can
achieve this for linear models  With this setting  as little as
two bits per model dimension are suf cient for good accuracy  and can enable   fast FPGA implementation 

NonLinear Models  We mainly discussed linear models       linear regression  in this paper  The natural question is that can we extend our ZipML framework to nonlinear models       logistic regression or SVM  which are
arguably more commonly used  In the full version on this
paper  we examine this problem  and  nd that our framework can be generalized to nonlinear settings  and that in
practice  bit is suf cient to achieve good accuracy on   variety of tasks  such as SVM and logistic regression  However  we notice that   strawman approach  which applies
naive stochastic rounding over the input data to just  bit
precision  converges to similar results  without the added
complexity  It is interesting to consider the rationale behind these results  Our framework is based on the idea of
unbiased approximation of the original SGD process  For
linear models  this is easy to achieve  For nonlinear models  this is harder  and we focus on guaranteeing arbitrarily
low bias  However  for   variety of interesting functions
such as hinge loss  guaranteeing low bias requires complex
approximations  In turn  these increase the variance  The
complexity of the approximation and the resulting variance
increase force us to increase the density of the quantization 
in order to achieve good approximation guarantees 

Hardware Selection  We choose to realize our implementation using FPGA because of its  exibility in dealing with lowprecision arithmetic  while CPU or GPU can
only do at least  bit computation ef ciently  However 
we do observe speed up in other environments   for example  the double sampling techniques are currently being applied to sensor networks with embedded GPUs and CPUs
to achieve similar speedup  We are currently conducting an
architecture exploration study which aims at understanding
the system tradeoff between FPGA  CPU  and GPU  This
requires us to push the implementation on all three architectures to the extreme  We expect this study will soon provide   full systematic answer to the question that on which
hardware can we get the most from the ZipML framework 

ZipML  Training Linear Models with Endto End Low Precision

Acknowledgements
CZ gratefully acknowledges the support from the Swiss
National Science Foundation NRP      
NVIDIA Corporation for its GPU donation  and Microsoft
Azure for Research award program 

References
Caffe CIFAR 

http caffe 
berkeleyvision org gathered examples 
cifar html 

tutorial 

Acharya  Jayadev  Diakonikolas  Ilias  Hegde  Chinmay 
Li  Jerry Zheng  and Schmidt  Ludwig  Fast and nearoptimal algorithms for approximating distributions by
histograms  In PODS   

Alistarh  Dan  Li 

Jerry  Tomioka  Ryota  and Vojnovic  Milan  QSGD  Randomized Quantization for
CommunicationOptimal Stochastic Gradient Descent 
arXiv   

Bubeck    ebastien  Convex optimization  Algorithms
and complexity  Foundations and Trends   cid  in Machine
Learning   

Cortes  Corinna  Mohri  Mehryar  and Talwalkar  Ameet 
On the impact of kernel approximation on learning accuracy  In AISTATS   

De Sa  Christopher    Zhang  Ce  Olukotun  Kunle  and
     Christopher  Taming the wild    uni ed analysis of
hogwildstyle algorithms  In NIPS   

Gong  Yunchao  Liu  Liu  Yang  Ming  and Bourdev 
Lubomir  Compressing deep convolutional networks using vector quantization  arXiv   

Gopi  Sivakant  Netrapalli  Praneeth  Jain  Prateek  and
Nori  Aditya  Onebit compressed sensing  Provable
support and vector recovery  In ICML   

Gupta  Suyog  Agrawal  Ankur  Gopalakrishnan  Kailash 
and Narayanan  Pritish  Deep learning with limited numerical precision  In ICML   

Hall  Daniel    Measurement error in nonlinear models   
modern perspective  nd ed  Journal of the American
Statistical Association   

Han  Song  Mao  Huizi  and Dally  William    Deep compression  Compressing deep neural networks with pruning  trained quantization and huffman coding  In ICLR 
 

Hubara  Itay  Courbariaux  Matthieu  Soudry  Daniel  ElYaniv  Ran  and Bengio  Yoshua  Quantized neural
networks  Training neural networks with low precision
weights and activations  arXiv   

Kara  Kaan  Alistarh  Dan  Zhang  Ce  Mutlu  Onur  and
Alonso  Gustavo  Fpga accelerated dense linear machine
learning    precisionconvergence tradeoff  In FCCM 
 

Kim  Jung Kuk  Zhang  Zhengya  and Fessler  Jeffrey   
Hardware acceleration of iterative image reconstruction
for xray computed tomography  In ICASSP   

Kim  Minje and Smaragdis  Paris  Bitwise neural networks 

arXiv   

Kim  YongDeok  Park  Eunhyeok  Yoo  Sungjoo  Choi 
Taelim  Yang  Lu  and Shin  Dongjun  Compression
of deep convolutional neural networks for fast and low
power mobile applications  arXiv   

Li  Fengfu  Zhang  Bo  and Liu  Bin  Ternary weight net 

works  arXiv   

Lin  Darryl  Talathi  Sachin  and Annapureddy  Sreekanth 
Fixed point quantization of deep convolutional networks 
In ICML   

Miyashita  Daisuke  Lee  Edward    and Murmann  Boris 
Convolutional neural networks using logarithmic data
representation  arXiv   

Rastegari  Mohammad  Ordonez  Vicente  Redmon 
Joseph  and Farhadi  Ali  Xnornet  Imagenet classi 
cation using binary convolutional neural networks 
In
ECCV   

Seide  Frank  Fu  Hao  Droppo  Jasha  Li  Gang  and Yu 
Dong   bit stochastic gradient descent and application
to dataparallel distributed training of speech dnns 
In
Interspeech   

Vanhoucke  Vincent  Senior  Andrew  and Mao  Mark   
Improving the speed of neural networks on cpus  In NIPS
Workshop on Deep Learning and Unsupervised Feature
Learning   

Wu  Jiaxiang  Leng  Cong  Wang  Yuhang  Hu  Qinghao 
and Cheng  Jian  Quantized convolutional neural networks for mobile devices  In CVPR   

Zhang  Hantian  Li  Jerry  Kara  Kaan  Alistarh  Dan 
Liu  Ji  and Zhang  Ce  The zipml framework for
training models with endto end low precision  The
cans 
the cannots  and   little bit of deep learning 
arXiv   

Zhou  Shuchang  Wu  Yuxin  Ni  Zekun  Zhou  Xinyu 
Wen  He  and Zou  Yuheng  Dorefanet  Training
low bitwidth convolutional neural networks with low
bitwidth gradients  arXiv   

