Dual Iterative Hard Thresholding  From Nonconvex Sparse Minimization to

Nonsmooth Concave Maximization

Bo Liu   XiaoTong Yuan   Lezi Wang   Qingshan Liu   Dimitris    Metaxas  

Abstract

Iterative Hard Thresholding  IHT  is   class of
projected gradient descent methods for optimizing sparsityconstrained minimization models 
with the best known ef ciency and scalability
in practice  As far as we know  the existing
IHTstyle methods are designed for sparse minimization in primal form 
It remains open to
explore duality theory and algorithms in such  
nonconvex and NPhard problem setting  In this
paper  we bridge this gap by establishing   duality theory for sparsityconstrained minimization
with  cid regularized loss function and proposing
an IHTstyle algorithm for dual maximization 
Our sparse duality theory provides   set of suf 
 cient and necessary conditions under which the
original NPhard nonconvex problem can be equivalently solved in   dual formulation  The
proposed dual IHT algorithm is   supergradient
method for maximizing the nonsmooth dual objective  An interesting  nding is that the sparse
recovery performance of dual IHT is invariant to
the Restricted Isometry Property  RIP  which is
required by virtually all the existing primal IHT
algorithms without sparsity relaxation  Moreover    stochastic variant of dual IHT is proposed
for largescale stochastic optimization  Numerical results demonstrate the superiority of dual IHT algorithms to the stateof theart primal
IHTstyle algorithms in model estimation accuracy and computational ef ciency 

  Introduction
Sparse learning has emerged as an effective approach
to alleviate model over tting when feature dimension

 Department of CS  Rutgers University  Piscataway  NJ 
  USA   BDAT Lab  Nanjing University of Information
Science   Technology  Nanjing  Jiangsu    China  Correspondence to  Bo Liu  lb cs rutgers edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

outnumbers training sample  Given   set of training
   in which xi   Rd is the feature repsamples xi  yi  
resentation and yi     the corresponding label  the following sparsityconstrained  cid norm regularized loss minimization problem is often considered in highdimensional
analysis 

  cid 

  

min
 cid   cid  

       

 
 

    cid xi  yi   

 cid   cid 

 
 

 

Here    is   convex loss function      Rd is the
model parameter vector and   controls the regularization
strength  For example  the squared loss                  
is used in linear regression and the hinge loss          
max      ab  in support vector machines  Due to the presence of cardinality constraint  cid   cid       the problem  
is simultaneously nonconvex and NPhard in general  and
thus is challenging for optimization    popular way to address this challenge is to use proper convex relaxation      
 cid  norm  Tibshirani    and ksupport norm  Argyriou
et al    as an alternative of the cardinality constraint  However  the convex relaxation based techniques tend to
introduce bias for parameter estimation 
In this paper  we are interested in algorithms that directly minimize the nonconvex formulation in   Early efforts mainly lie in compressed sensing for signal recovery 
which is   special case of   with squared loss  Among
others    family of the so called Iterative Hard Thresholding  IHT  methods  Blumensath   Davies    Foucart 
  have gained signi cant interests and they have been
witnessed to offer the fastest and most scalable solutions in
many cases  More recently  IHTstyle methods have been
generalized to handle generic convex loss functions  Beck
  Eldar    Yuan et al    Jain et al    as well
as structured sparsity constraints  Jain et al    The
common theme of these methods is to iterate between gradient descent and hard thresholding to maintain sparsity of
solution while minimizing the objective value 
Although IHTstyle methods have been extensively studied 
the stateof theart is only designed for the primal formulation   It remains an open problem to investigate the
feasibility of solving the original NPhard nonconvex formulation in   dual space that might potentially further im 

Dual Iterative Hard Thresholding

prove computational ef ciency  To  ll this gap  inspired by
the recent success of dual methods in regularized learning
problems  we systematically build   sparse duality theory
and propose an IHTstyle algorithm along with its stochastic variant for dual optimization 
Overview of our contribution  The core contribution of
this work is twofold in theory and algorithm  As the theoretical contribution  we have established   novel sparse Lagrangian duality theory for the NPhard nonconvex problem   which to the best of our knowledge has not been
reported elsewhere in literature  We provide in this part  
set of suf cient and necessary conditions under which one
can safely solve the original nonconvex problem through
maximizing its concave dual objective function  As the algorithmic contribution  we propose the dual IHT  DIHT 
algorithm as   supergradient method to maximize the nonsmooth dual objective  In high level description  DIHT iterates between dual gradient ascent and primal hard thresholding pursuit until convergence    stochastic variant of
DIHT is proposed to handle largescale learning problems  For both algorithms  we provide nonasymptotic convergence analysis on parameter estimation error  sparsity
recovery  and primaldual gap as well 
In sharp contrast
to the existing analysis for primal IHTstyle algorithms 
our analysis is not relying on Restricted Isometry Property
 RIP  conditions and thus is less restrictive in reallife highdimensional statistical settings  Numerical results on synthetic datasets and machine learning benchmark datasets demonstrate that dual IHT signi cantly outperforms the
stateof theart primal IHT algorithms in accuracy and ef 
 ciency  The theoretical and algorithmic contributions of
this paper are highlighted in below 

  Sparse Lagrangian duality theory  we established  
sparse saddle point theorem  Theorem     sparse
minimax theorem  Theorem   and   sparse strong
duality theorem  Theorem  

  Dual optimization  we proposed an IHTstyle algorithm along with its stochastic extension for nonsmooth dual maximization  These algorithms have
  in dual
been shown to converge at the rate of  
  in primaldual
parameter estimation error and  
gap  see Theorem   and Theorem   These guarantees are invariant to RIP conditions which are required
by virtually all the primal IHTstyle methods without
using relaxed sparsity levels 

  ln  

  ln  

Notation  Before continuing  we de ne some notations to
be used  Let     Rd be   vector and   be an index set  We
use HF     to denote the truncation operator that restricts  
to the set     Hk    is   truncation operator which preserves
the top    in magnitude  entries of   and sets the remaining
to be zero  The notation supp    represents the index set

of nonzero entries of    We conventionally de ne  cid   cid   
maxi       and de ne xmin   mini supp          For  
matrix     max     min    denotes its largest  smallest 
singular value 
Organization  The rest of this paper is organized as follows 
In
  we develop   Lagrangian duality theory for sparsityconstrained minimization problems  The dual IHTstyle algorithms along with convergence analysis are presented in
  The numerical evaluation results are reported in  
Finally  the concluding remarks are made in   All the
technical proofs are deferred to the appendix sections 

In   we brie   review some relevant work 

  Related Work
For generic convex objective beyond quadratic loss  the rate
of convergence and parameter estimation error of IHTstyle
methods were established under proper RIP  or restricted
strong condition number  bound conditions  Blumensath 
  Yuan et al      In  Jain et al    several
relaxed variants of IHTstyle algorithms were presented for
which the estimation consistency can be established without requiring the RIP conditions  In  Bahmani et al   
  gradient support pursuit algorithm is proposed and analyzed  In largescale settings where   full gradient evaluation on all data becomes   bottleneck  stochastic and variance reduction techniques have been adopted to improve
the computational ef ciency of IHT  Nguyen et al   
Li et al    Chen   Gu   
Dual optimization algorithms have been widely used in various learning tasks including SVMs  Hsieh et al    and
multitask learning  Lapin et al   
In recent years 
stochastic dual optimization methods have gained significant attention in largescale machine learning  ShalevShwartz   Zhang        To further improve computational ef ciency  some primaldual methods are developed
to alternately minimize the primal objective and maximize
the dual objective  The successful examples of primaldual
methods include learning total variation regularized model  Chambolle   Pock    and generalized Dantzig selector  Lee et al    More recently    number of stochastic variants  Zhang   Xiao    Yu et al   
and parallel variants  Zhu   Storkey    were developed to make the primaldual algorithms more scalable and
ef cient 

    Sparse Lagrangian Duality Theory
In this section  we establish weak and strong duality theory that guarantees the original nonconvex and NPhard
problem in   can be equivalently solved in   dual space 
The results in this part build the theoretical foundation of
developing dual IHT methods 

Dual Iterative Hard Thresholding

 cid  

 

        max

From now on we abbreviate li   cid xi        cid xi  yi  The
convexity of     cid xi  yi  implies that li    is also con 
 iu   li    be the convex
vex  Let   
conjugate of li    and       be the feasible set of
    According to the wellknown expression of li     
max      iu     
      the problem   can be reformulated into the following minimax formulation 
  cid 

       

 cid   cid 

 
 

 

min
 cid   cid  

 
 

  

The following Lagrangian form will be useful in analysis 

max

     iw cid xi     
  cid 
 cid iw cid xi     

 
 

  

     cid   

 cid   cid 

 
 

        

where                    is the vector of dual variables  We now introduce the following concept of sparse
saddle point which is   restriction of the conventional saddle point to the setting of sparse optimization 
De nition    Sparse Saddle Point    pair           Rd  
    is said to be   ksparse saddle point for   if  cid     cid     
and the following holds for all  cid   cid                

                            

 

Different from the conventional de nition of saddle point 
the ksparse saddle point only requires the inequality  
holds for any arbitrary ksparse vector    The following result is   basic sparse saddle point theorem for   
Throughout the paper  we will use   cid  to denote   subgradient  or supergradient  of   convex  or concave  function     and use      to denote its subdifferential  or
superdifferential 
Theorem    Sparse Saddle Point Theorem  Let      Rd
be   ksparse primal vector and         be   dual vector 
Then the pair         is   sparse saddle point for   if and
only if the following conditions hold 

       solves the primal problem in  
               cid       lN      cid xN  

 cid   

  

 cid  

 cid 

 

         Hk

    ixi

Proof    proof of this result is given in Appendix   
Remark   Theorem   shows that the conditions      
are suf cient and necessary to guarantee the existence of  
sparse saddle point for the Lagrangian form    This result
is different from from the traditional saddle point theorem
which requires the use of the Slater Constraint Quali cation to guarantee the existence of saddle point 

Remark   Let us consider    cid         
    ixi  
                Denote      supp      It is easy to verify that the condition     in Theorem   is equivalent to

 

         cid         

 wmin    
 

 cid    cid     cid 

The following sparse minimax theorem guarantees that the
min and max in   can be safely switched if and only if
there exists   sparse saddle point for       
Theorem    Sparse MiniMax Theorem  The minimax
relationship

max
    

min
 cid   cid  

         min
 cid   cid  

max
    

      

 

holds if and only if there exists   sparse saddle point        
for   

Proof    proof of this result is given in Appendix   

The sparse minimax result in Theorem   provides suf 
cient and necessary conditions under which one can safely
exchange   minmax for   maxmin  in the presence of sparsity constraint  The following corollary is   direct consequence of applying Theorem   to Theorem  
Corollary   The minimax relationship

max
    

min
 cid   cid  

         min
 cid   cid  

max
    

      

holds if and only if there exist   ksparse primal vector
     Rd and   dual vector         such that the conditions       in Theorem   are satis ed 

The minimax result in Theorem   can be used as   basis for
establishing sparse duality theory  Indeed  we have already
shown the following 

min
 cid   cid  

max
    

         min
 cid   cid  

     

This is called the primal minimization problem and it is
the minmax side of the sparse minimax theorem  The
other side  the maxmin problem  will be called as the
dual maximization problem with dual objective function
     min cid   cid              

max
    

     max
    

min
 cid   cid  

      

 

The following Lemma   shows that the dual objective function    is concave and explicitly gives the expression of
its superdifferential 
Lemma   The dual objective function    is given by

  cid 

  

    

 
 

   

         
 

 cid   cid 

Dual Iterative Hard Thresholding

 cid   

 cid  

 cid 

where      Hk
is concave and its superdifferential is given by

    ixi

   

  Moreover    

     

 
 

   cid     

      cid xN   

      

Particularly  if    is unique at   and    
       are
differentiable  then     is unique and it is the supergradient of   

Proof    proof of this result is given in Appendix   

Based on Theorem   and Theorem   we are able to further
establish   sparse strong duality theorem which gives the
suf cient and necessary conditions under which the optimal values of the primal and dual problems coincide 
Theorem    Sparse Strong Duality Theorem  Let      Rd
is   ksparse primal vector and         be   dual vector  Then   solves the dual problem in            
             and              if and only if the
pair         satis es the conditions       in Theorem  

Proof    proof of this result is given in Appendix   

We de ne the sparse primaldual gap          
 
           The main message conveyed by Theorem   is that the sparse primaldual gap reaches zero at the
primaldual pair         if and only if the conditions      
in Theorem   hold 
The sparse duality theory developed in this section suggests   natural way for  nding the global minimum of the
sparsityconstrained minimization problem in   via maximizing its dual problem in   Once the dual maximizer
  is estimated  the primal sparse minimizer    can then be
recovered from it according to the primadual connection
as given in the condition    
     Hk
Since the dual objective function    is shown to be concave  its global maximum can be estimated using any convex concave optimization method  In the next section  we
present   simple projected supergradient method to solve
the dual maximization problem 

 cid   

 cid  

    ixi

 cid 

  

  Dual Iterative Hard Thresholding
Generally     is   nonsmooth function since    the
conjugate function   
  of an arbitrary convex loss li is generally nonsmooth and   the term  cid   cid  is nonsmooth
with respect to   due to the truncation operation involved
in computing    Therefore  smooth optimization methods are not directly applicable here and we resort to subgradient type methods to solve the nonsmooth dual maximization problem in  

Algorithm   Dual Iterative Hard Thresholding  DIHT 
Input

  Training set  xi  yi  
   Regularization strength
parameter   Cardinality constraint    Stepsize  

Initialization         
for             do

         

     

   

   
    PF

         

    Dual projected supergradient ascent       
        

 cid 
where     
           cid 
     cid 
  is the
supergradient and PF   is the Euclidian projection
operator with respect to feasible set   
 cid 
    Primal hard thresholding 

 cid 
     
 cid 

   

 

 

 

 

 

 

   

  xi

 

 

  cid 

  

   
  

       Hk

end
Output       

  Algorithm

The Dual Iterative Hard Thresholding  DIHT  algorithm  as outlined in Algorithm   is essentially   projected supergradient method for maximizing    The
procedure generates   sequence of primadual pairs
                  from an initial pair       
and       At the tth iteration  the dual update step   
conducts the projected supergradient ascent in   to update     from     and      Then in the primal update step    the primal variable      is constructed from
    using   ksparse truncation operation in  
When   batch estimation of supergradient   cid  becomes
expensive in largescale applications  it is natural to consider the stochastic implementation of DIHT  namely SDIHT 
as outlined in Algorithm   Different from the batch computation in Algorithm   the dual update step    in Algorithm   randomly selects   block of samples  from   given
block partition of samples  and update their corresponding
dual variables according to   Then in the primal update
step    we incrementally update an intermediate accumulation vector       which records    
  xi as  
weighted sum of samples  In    the primal vector      is
updated by applying ksparse truncation on       The SDIHT is essentially   blockcoordinate supergradient method
for the dual problem  Particularly  in the extreme case
      SDIHT reduces to the batch DIHT  At the opposite extreme end with             each block contains
one sample  SDIHT becomes   stochastic coordinatewise
supergradient method 

 cid  

      

  

Dual Iterative Hard Thresholding

Algorithm   Stochastic Dual Iterative Hard Thresholding
 SDIHT 
Input

  Training set  xi  yi  
   Regularization strength
parameter   Cardinality constraint    Stepsize
    block disjoint partition       Bm  of the
sample index set        
Initialization               
for             do

         

     

    Dual projected supergradient ascent  Uniformly
          Bm 
randomly select an index block     
For all         

 cid 

 

  update    
   

 

as
         

 

 cid 

   
    PF
       

 

           
Set    
 
    Primal hard thresholding 
      Intermediate update 

 

 

 

 xj 

 

                 
  

   

       

 

 cid 

      

 

      Hard thresholding         Hk       

end
Output       

The dual update   in SDIHT is much more ef cient than
DIHT as the former only needs to access   small subset of
samples at   time  If the hard thresholding operation in primal update becomes   bottleneck       in highdimensional
settings  we suggest to use SDIHT with relatively smaller
number of blocks so that the hard thresholding operation in
   can be less frequently called 

  Convergence analysis

We now analyze the nonasymptotic convergence behavior of DIHT and SDIHT  In the following analysis  we
will denote     arg max        and use the abbreP                   Let     maxa      
viation    
be the bound of the dual feasible set   and    
     
maxi        cid 
For example  such quantities exist
when li and   
  are Lipschitz continuous  ShalevShwartz
  Zhang      We also assume without loss of generality that  cid xi cid      Let           xN     Rd   be the
data matrix  Given an index set     we denote XF as the
restriction of   with rows restricted to     The following
quantities will be used in our analysis 

 cid   cid   cid 
 cid   cid   cid 

  XF             cid   cid     cid   
  XF             cid   cid     cid   

 
max         sup
  Rn  

 
min         inf

  Rn  

Particularly   max          max    and  min        
 min    We say   univariate differentiable function      
is  smooth if                       cid   cid         cid     
   
   The following is our main theorem on the dual parameter estimation error  support recovery and primaldual gap
of DIHT 
Theorem   Assume that
     

    min      and            cid 

Set
    min          De ne constants     
 

is  smooth 

     max     

 cid 

     

    

  

li

    Parameter

estimation error 

sequence
      generated by Algorithm   satis es the
following estimation error inequality 

The

 cid   

 cid 

 

 cid       cid      

 

ln  
 

 

    Support recovery and primaldual gap  Assume ad 
 cid    cid     cid      Then 

ditionally that      wmin    
supp        supp      when

 cid     
 cid 

        

max   

    

ln

   

max   

    

 cid       

Moreover  for any       the primaldual gap satP       when     max       where     
is es    
     ln      
    

 cid 

 

       

Proof    proof of this result is given in Appendix   
Remark   The theorem allows       when  min        
  If       then  min       is allowed to be zero and thus
the stepsize can be set as        

     
                     
Consider primal suboptimality    
Since    
    always holds  the convergence rates
in Theorem   are applicable to the primal suboptimality
as well  An interesting observation is that these convergence results on    
  are not relying on the Restricted Isometry Property  RIP   or restricted strong condition number 
which is required in most existing analysis of IHTstyle algorithms  Blumensath   Davies    Yuan et al   
In  Jain et al    several relaxed variants of IHTstyle
algorithms are presented for which the estimation consistency can be established without requiring the RIP conditions  In contrast to the RIPfree sparse recovery analysis
in  Jain et al    our Theorem   does not require the
sparsity level   to be relaxed 
For SDIHT  we can establish similar nonasymptotic convergence results as summarized in the following theorem 
Theorem   Assume that li is  smooth  Set      
    min         

 mN  

Dual Iterative Hard Thresholding

    Parameter

estimation error 

sequence
      generated by Algorithm   satis es the
following expected estimation error inequality 

The

 cid   

 cid 

 

  cid       cid    mc 

 

ln  
 

 

    Support recovery and primaldual gap  Assume ad 
 cid    cid     cid      Then 
ditionally that      wmin    
for any         with probability at least       it
 cid 
holds that supp        supp      when

max   

 mc 

max   

    

    

ln

 

Moreover  with probability at least       the primalP       when     max      
dual gap satis es    

        

 cid   mc 
where     cid   mc   

     ln  mc   
    

 cid 

Proof    proof of this result is given in Appendix   
Remark   Theorem   shows that  up to scaling factors 
the expected or high probability iteration complexity of SDIHT is almost identical to that of DIHT  The scaling factor   appeared in    and    re ects   tradeoff between
the decreased periteration cost and the increased iteration
complexity 

  Experiments
This section dedicates in demonstrating the accuracy and
ef ciency of the proposed algorithms  We  rst show the
model estimation performance of DIHT when applied to
sparse ridge regression models on synthetic datasets  Then
we evaluate the ef ciency of DIHT SDIHT on sparse  cid 
regularized Huber loss and Hinge loss minimization tasks
using realworld datasets 

  Model parameter estimation accuracy evaluation

  synthetic model is generated with sparse model parameter             
  Each xi   Rd of the

         

 cid 

 cid cid 

  

 cid 

 cid 

 cid cid 

   

 cid 

 cid   

  training data examples  xi  
   is designed to have two components  The  rst component is the top   feature
dimensions drawn from multivariate Gaussian distribution
      Each entry in         independently follows
standard normal distribution  The entries of covariance
  The second component consist 
 ij  
  the left        feature dimensions  It follows       
where each entry in     Rd   is drawn from standard
normal distribution  We simulate two data parameter settings                               
In each data parameter setting   random data copies are

     
   cid   

 

    Model estimation error

    Percentage of support recovery success

Figure   Model parameter estimation performance comparison
between DIHT and baseline algorithms on the two synthetic
dataset settings  The varying number of training sample is denoted by   

produced independently  The task is to solve the following
 cid regularized sparse linear regression problem 

  cid 

  

min cid   cid  

 
 

lsq yi    cid xi   

 cid   cid 

 
 

 

sq       

where lsq yi    cid xi     yi     cid xi  The responses
 yi  
   are produced by yi      cid xi       where     
      The convex conjugate of lsq yi    cid xi  is known
as   
    yi    ShalevShwartz   Zhang     
We consider solving the problem under the sparsity level
        Two measurements are calculated for evaluation 
The  rst is parameter estimation error  cid        cid cid     cid  Apart from it we calculate the percentage of successful support recovery    SSR  as the second performance metric 
  successful support recovery is obtained if supp       
supp    The evaluation is conducted on the generated
batch data copies to calculate the percentage of successful
support recovery  We use   data copies as validation set
to select the parameter   from       and the percentage of successful support recovery is evaluated on the
other   data copies 
Iterative hard thresholding  IHT   Blumensath   Davies 
  and hard thresholding pursuit  HTP   Foucart   
are used as the baseline primal algorithms  The parameter
estimation error and percentage of successful support recovery curves under varying training size are illustrated in
Figure   We can observe from this group of curves that DIHT consistently achieves lower parameter estimation error
and higher rate of successful support recovery than IHT and
HTP  It is noteworthy that most signi cant performance gap
between DIHT and the baselines occurs when the training
size   is comparable to or slightly smaller than the sparsity
level    

  kw wk   wk IHT   HTP   DIHT   IHT   HTP   DIHT     PSSR IHT   HTP   DIHT   IHT   HTP   DIHT   Dual Iterative Hard Thresholding

    RCV       

    RCV       

    News       

    News       

Figure   Huber loss  Running time  in second  comparison between the considered algorithms 

    DIHT on RCV 

    SDIHT on RCV 

    DIHT on News 

    SDIHT on News 

Figure   Huber loss  The primaldual gap evolving curves of DIHT and SDIHT        for RCV  and       for News 

  Model training ef ciency evaluation

  HUBER LOSS MODEL LEARNING

We now evaluate the considered algorithms on the following  cid regularized sparse Huber loss minimization problem 

lHuber yix cid 

      

min
 cid   cid  

where

 
 

  

  cid 
   
 cid  yi      

 cid   cid 

 
 

 

       
           

yix cid 
yix cid 
otherwise

 

lHuber yix cid 

      

    yix cid 
      yix cid 

       
 
    

 

It is known that  ShalevShwartz   Zhang     
if yi        
otherwise

  
Huber     

 

   

 

 

Two binary benchmark datasets from LibSVM data repository  RCV           and News            
are used for algorithm ef ciency evaluation and comparison  We select   million samples from RCV  dataset for

 https www csie ntu edu tw cjlin 

libsvmtools datasets binary html

       

model training     cid     For news  dataset  all of the
    samples are used as training data     cid    
We evaluate the algorithm ef ciency of DIHT and SDIHT
by comparing their running time against three primal baseline algorithms  IHT  HTP and gradient hard thresholding
with stochastic variance reduction  SVRGHT   Li et al 
  We  rst run IHT by setting its convergence criterion
to be                
    or maximum number of
iteration is reached  After that we test the time cost spend
by other algorithms to make the primal loss reach        
The parameter update stepsize of all the considered algorithms is tuned by grid search  The parameter   is set to be
  For the two stochastic algorithms SDIHT and SVRGHT we randomly partition the training data into        
minibatches 
Figure   shows the running time curves on both datasets
under varying sparsity level   and regularization strength
       
It is obvious that under all tested
      con gurations on both datasets  DIHT and SDIHT
need much less time than the primal baseline algorithms 
IHT  HTP and SVRGHT to reach the same primal suboptimality  Figure   shows the primaldual gap convergence curves with respect to the number of epochs  This
group of results support the theoretical prediction in Theorem   and   that      converges nonasymptotically 

 Running timeIHTHTPSVRGHTDIHTSDIHT Running timeIHTHTPSVRGHTDIHTSDIHT Running timeIHTHTPSVRGHTDIHTSDIHT Running timeIHTHTPSVRGHTDIHTSDIHT Epoch Primaldual gap Epoch Primaldual gap Epoch Primaldual gap Epoch Primaldual gap Dual Iterative Hard Thresholding

    RCV       

    RCV       

    News       

    News       

Figure   Hinge loss  Running time  in second  comparison between the considered algorithms 

    DIHT on RCV 

    SDIHT on RCV 

    DIHT on News 

    SDIHT on News 

Figure   Hinge loss  The primaldual gap evolving curves of DIHT and SDIHT        for RCV  and       for News 

  HINGE LOSS MODEL LEARNING

We further test the performance of our algorithms when
applied to the following  cid regularized sparse hinge loss
minimization problem 

min
 cid   cid  

lHinge yix cid 

 
 
       max      yiw cid xi  It is stanwhere lHinge yix cid 
dard to know  Hsieh et al   

      

 cid   cid 

 
 

  cid 

  

 cid  yi  

  
Hinge     

if yi        

  otherwise

 

We follow the same experiment protocol as in    
to compare the considered algorithms on the benchmark
datasets  The time cost comparison is illustrated in Figure   and the primadual gap suboptimality is illustrated
in Figure   This group of results indicate that DIHT and
SDIHT still exhibit remarkable ef ciency advantage over
the considered primal IHT algorithms even when the loss
function is nonsmooth 
  Conclusion
In this paper  we systematically investigate duality theory
and algorithms for solving the sparsityconstrained minimization problem which is NPhard and nonconvex in its

primal form  As   theoretical contribution  we develop  
sparse Lagrangian duality theory which guarantees strong
duality in sparse settings  under mild suf cient and necessary conditions  This theory opens the gate to solve the
original NPhard nonconvex problem equivalently in   dual space  We then propose DIHT as    rstorder method to
maximize the nonsmooth dual concave formulation  The
algorithm is characterized by dual supergradient ascent and primal hard thresholding  To further improve iteration ef ciency in largescale settings  we propose SDIHT
as   block stochastic variant of DIHT  For both algorithms we have proved sublinear primaldual gap convergence
rate when the primal loss is smooth  without assuming RIPstyle conditions  Based on our theoretical  ndings and numerical results  we conclude that DIHT and SDIHT are theoretically sound and computationally attractive alternatives
to the conventional primal IHT algorithms  especially when
the sample size is smaller than feature dimensionality 
Acknowledgements
XiaoTong Yuan is supported in part by Natural Science Foundation of China  NSFC  under Grant  
Grant   and in part by Natural Science Foundation of Jiangsu Province of China  NSFJPC  under Grant
BK  Qingshan Liu is supported in part by NSFC
under Grant  

 Running timeIHTHTPSVRGHTDIHTSDIHT Running timeIHTHTPSVRGHTDIHTSDIHT Running timeIHTHTPSVRGHTDIHTSDIHT Running timeIHTHTPSVRGHTDIHTSDIHT Epoch Primaldual gap Epoch Primaldual gap Epoch Primaldual gap Epoch Primaldual gap Dual Iterative Hard Thresholding

References
Argyriou  Andreas  Foygel  Rina  and Srebro  Nathan  Sparse prediction with the ksupport norm  In Advances
in Neural Information Processing Systems   

Bahmani  Sohail  Raj  Bhiksha  and Boufounos  Petros   
Journal of

Greedy sparsityconstrained optimization 
Machine Learning Research   Mar   

Beck  Amir and Eldar  Yonina    Sparsity constrained
nonlinear optimization  Optimality conditions and algorithms  SIAM Journal on Optimization   
   

Blumensath  Thomas  Compressed sensing with nonlinear
observations and related nonlinear optimization problems  IEEE Transactions on Information Theory   
   

Blumensath  Thomas and Davies  Mike    Iterative hard
thresholding for compressed sensing  Applied and Computational Harmonic Analysis     

Chambolle  Antonin and Pock  Thomas     rstorder
primaldual algorithm for convex problems with applications to imaging  Journal of Mathematical Imaging
and Vision     

Chen  Jinghui and Gu  Quanquan  Accelerated stochastic block coordinate gradient descent for sparsity constrained nonconvex optimization  In Conference on Uncertainty in Arti cial Intelligence   

Foucart  Simon  Hard thresholding pursuit  an algorithm
for compressive sensing  SIAM Journal on Numerical
Analysis     

Hsieh  ChoJui  Chang  KaiWei  Lin  ChihJen  Keerthi 
  Sathiya  and Sundararajan  Sellamanickam    dual
coordinate descent method for largescale linear svm  In
International conference on Machine learning   

Jain  Prateek  Tewari  Ambuj  and Kar  Purushottam  On iterative hard thresholding methods for highdimensional
mestimation  In Advances in Neural Information Processing Systems   

Jain  Prateek  Rao  Nikhil  and Dhillon  Inderjit  Structured
sparse regression via greedy hardthresholding  arXiv
preprint arXiv   

Lapin  Maksim  Schiele  Bernt  and Hein  Matthias  Scalable multitask representation learning for scene classi 
 cation  In IEEE Conference on Computer Vision and
Pattern Recognition   

Lee  Sangkyun  Brzyski  Damian  and Bogdan  Malgorzata  Fast saddlepoint algorithm for generalized dantzig
selector and fdr control with the ordered  cid norm 
In
International Conference on Arti cial Intelligence and
Statistics   

Li  Xingguo  Zhao  Tuo  Arora  Raman  Liu  Han  and
Haupt  Jarvis  Stochastic variance reduced optimization
for nonconvex sparse learning  In International Conference on Machine Learning   

Nguyen  Nam  Needell  Deanna  and Woolf  Tina  Linear convergence of stochastic iterative greedy algorithms
with sparse constraints  arXiv preprint arXiv 
 

ShalevShwartz  Shai and Zhang  Tong  Accelerated minibatch stochastic dual coordinate ascent  In Advances in
Neural Information Processing Systems     

ShalevShwartz  Shai and Zhang  Tong  Stochastic dual coordinate ascent methods for regularized loss  The
Journal of Machine Learning Research   
   

Tibshirani  Robert  Regression shrinkage and selection via
the lasso  Journal of the Royal Statistical Society  Series
   Methodological  pp     

Yu  Adams Wei  Lin  Qihang  and Yang  Tianbao  Doubly stochastic primaldual coordinate method for empirical risk minimization and bilinear saddlepoint problem 
arXiv preprint arXiv   

Yuan  XiaoTong  Li  Ping  and Zhang  Tong  Gradient hard thresholding pursuit for sparsityconstrained opIn International Conference on Machine
timization 
Learning   

Yuan  XiaoTong  Li  Ping  and Zhang  Tong  Exact recovery of hard thresholding pursuit  In Advances in Neural
Information Processing Systems   

Zhang  Yuchen and Xiao  Lin  Stochastic primaldual coordinate method for regularized empirical risk minimization  In International Conference on Machine Learning 
 

Zhu  Zhanxing and Storkey  Amos    Stochastic parallel block coordinate descent for largescale saddle point
problems  In AAAI Conference on Arti cial Intelligence 
 

