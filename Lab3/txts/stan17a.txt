Probabilistic Submodular Maximization in SubLinear Time

Serban Stan   Morteza Zadimoghaddam   Andreas Krause   Amin Karbasi  

Abstract

In this paper  we consider optimizing submodular functions that are drawn from some unknown
distribution  This setting arises       in recommender systems  where the utility of   subset of
items may depend on   userspeci   submodular utility function  In modern applications  the
ground set of items is often so large that even
the widely used  lazy  greedy algorithm is not
ef cient enough  As   remedy  we introduce the
problem of sublinear time probabilistic submodular maximization  Given training examples of
functions       via user feature vectors  we seek
to reduce the ground set so that optimizing new
functions drawn from the same distribution will
provide almost as much value when restricted to
the reduced ground set as when using the full
set  We cast this problem as   twostage submodular maximization and develop   novel ef 
 cient algorithm for this problem which offers  
     
     approximation ratio for general mono 
 
tone submodular functions and general matroid
constraints  We demonstrate the effectiveness of
our approach on several realworld applications
where running the maximization problem on the
reduced ground set leads to two orders of magnitude speedup while incurring almost no loss 

  Introduction
Motivated by applications in data summarization  Lin  
Bilmes    Wei et al    Mirzasoleiman et al 
    and recommender systems  ElArini et al   
Yue   Guestrin    Mirzasoleiman et al      we
tackle the challenge of ef ciently solving many statistically
related submodular maximization problems  In these applications  submodularity arises in the form of userspeci  

 Yale University  New Haven  Connecticut  USA  Google
Research  New York  NY   USA  ETH Zurich 
Zurich  Switzerland 
Correspondence to  Amin Karbasi
 amin karbasi yale edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

utility functions for valuating sets of items  and   prototypical problem is to  nd sets of say   items with nearmaximal value  Krause   Golovin    Mirzasoleiman
et al      Even though ef cient greedy algorithms exist for submodular maximization  those become infeasible
when serving many users and optimizing over large item
collections  To this end  given training examples of  users
and their utility  functions drawn from some unknown distribution  we seek to reduce the ground set to   small  ideally sublinear  size  The hope is that optimizing new functions drawn from the same distribution will incur little loss
when optimized on the reduced set compared to optimizing
over the full set 
Optimizing the empirical objective is an instance of twostage submodular maximization    problem recently considered by Balkanski et al    who provide    
approximation guarantee for the case of coverage functions  more about it in the related work  One of our
key technical contributions is   computationally ef cient
novel localsearch based algorithm  called ReplacementGreedy  for twostage submodular maximization  that provides   constantfactor   approximation guarantee for
the general case of monotone submodular functions  Our
approach also generalizes to arbitrary matroid constraints 
and empirically compares favorably to prior work  We further analyze conditions under which our approach provably enables approximate submodular optimization based
on substantially reduced ground sets  resulting the  rst viable approach towards sublineartime submodular maximization 
Lastly  we demonstrate the effectiveness of our approach
on recommender systems for which we compare the utility
value and running time of maximizing   submodular function over the full data set and its reduced version returned
by our algorithm  We consistently observe that the loss we
incur is negligible  around   while the speed up is enormous  about two orders of magnitude 

Related work Submodular maximization has found
many applications in machine learning  ranging from feature variable selection  Krause   Guestrin    to dictionary learning  Das   Kempe    to data summarization  Wei et al      Lin   Bilmes    Tschiatschek
et al    to recommender systems  Yue   Guestrin 

Probabilistic Submodular Maximization in SubLinear Time

  ElArini et al      seminal result of Nemhauser
et al    proves that   simple greedy algorithm provides an optimal constant factor         approximation guarantee  Given the size of modern data sets  much
work has focused on solving submodular maximization at
scale  This work ranges from accelerating the greedy algorithm itself  Minoux    Mirzasoleiman et al   
    Badanidiyuru   Jan    Buchbinder et al   
to distributed  Kumar et al    Mirzasoleiman et al 
  and streaming  Krause   Gomes    Badanidiyuru et al    approaches  as well as algorithms based
on  ltering the ground set in multiple stages  Wei et al 
    Feldman et al    All of these approaches aim
to solve   single  xed problem instance  and have computational cost at least linear in the ground set size  In contrast 
we seek to solve multiple related problems with sublinear
effort 
Solving multiple submodular problems arises in online
submodular optimization  Streeter   Golovin    Hazan
  Kale    Jegelka   Bilmes    In this setting the
goal is to design algorithms that perform well in hindsight
 minimize some form of regret  The computational complexity of these algorithms is typically  super linear in the
ground set size  Studying online variants of our problem is
an interesting direction for future work 
Closest to our work is the approach of Balkanski et al 
  that we build on and compare within this paper 
For general submodular objectives  they propose two algorithms  one based on continuous optimization  with prohibitive computational complexity in terms of the size of
the ground set    which provides constant factor approximation guarantees only for large values of         cardinality constraint  as well as one that has exponential complexity in terms of    To circumvent the large computational complexity of the above algorithms  they also pro 
     
posed   heuristic local search method that offers    
   
approximation for the special case of coverage functions 
The query complexity of this algorithm is   km cid    log   
where  cid  is the size of the summary and   is the number
of considered submodular functions in the twostage optimization  One of our key contributions is   novel and
computationally ef cient algorithm for the twostage problem  More speci cally  our method ReplacementGreedy
         approximation guarantee for genprovides    
eral monotone submodular functions subject to   general
matroid constraint with only   rm cid    query complexity
 here   denotes the rank of the matroid  As argued by
Balkanski et al    twostage submodular maximization can be seen as   discrete analogue of representation
learning problems like dictionary learning  It is important
to note that the twostage submodular maximization problem is fractionally subadditive  XOS   Feige    Although it is tempting to use the XOS property of our two 

stage function especially given the positive results for social welfare XOS maximization  there are several obstacles
preventing us from doing so  First  evaluating the two stage
function is NPhard  so we cannot have access to oracle
value queries  Second  as shown by Singer   Shahar Dobzinski   Schapira   Ashwinkumar Badanidiyuru   any    approximation of   XOS requires exponentially many oracle value queries  Therefore
the XOS property itself is not suf cient to get any positive
algorithmic result for our problem  Nevertheless  we can
still provide constant factor approximation with computationally ef cient algorithms 
Repeatedly optimizing related classes of submodular functions is   key subroutine in many applications  such
as structured prediction  Lin   Bilmes    or linear submodular bandits  Yue   Guestrin   
In
both of these problems  one needs to repeatedly maximize weighted combinations of submodular functions  for
changing weights  Our work can be viewed as providing an
approach towards accelerating this central subroutine 

  Problem Setup
In this paper  we consider the problem of frequently optimizing monotone submodular functions           
that are drawn from some unknown probability distribution
   Hereby    denotes the ground set of size   over which
the submodular functions are de ned           we assume
that the maximum value of any function   drawn from  
does not exceed   This setting arises in many applications 
such as recommender systems  where the random function
    fu refers to the  predicted  valuation over sets of items
for   particular user    which may vary depending on their
features       Yue   Guestrin   These applications
typically dictate some constraints       one seeks to solve

      arg max                   

where       is   collection of feasible sets 
In this
paper we primarily consider cardinality constraints      
                      Our results will hold also in
the more general setting where   is the collection of independent sets in some matroid  Calinescu et al   
Throughout the paper   denotes the rank of the matroid  In
the special case of cardinality constraint  the rank is       
While NPhard  good approximation algorithms are known
for submodular maximization  For example  the classical
greedy algorithm of Nemhauser et al    or its accelerated variants  Minoux    Mirzasoleiman et al   
    Badanidiyuru   Jan    Buchbinder et al   
provide an optimal constant factor     approximation
for maximization under cardinality constraints  In modern
applications  however  the system may face   large number of users  and large collections of items   Hence the

Probabilistic Submodular Maximization in SubLinear Time

naive strategy of even greedily optimizing fu for each user
separately may be too costly 
To remedy this situation  in this paper we consider the following approach  Given training data         sample collection of functions            fm  we invest computation once
to obtain   reduced ground set   of size  cid   cid       
The hope is that optimizing new functions arising at test
time will provide almost as much value when restricting
the choice to items in    than when considering arbitrary
items in   while being substantially more computationally
ef cient 
More formally  the expected performance when using the
candidate reduced ground set   is

       Ef   max
      

      

 

where we use               and        to refer to
the collection of feasible sets restricted to those containing
only elements from    The optimum achievable performance would be    Our goal will be to pick   set  
of small size  cid  to maximize      or equivalently make
           small 

Special cases  Some observations are in order  If   is deterministic       puts all mass on   single function    then
we simply recover classical constrained submodular maximization  since              for sets up to size    If  
is known to be the uniform distribution over   functions 
     becomes

Gm     

 
 

max
      

fi    

 

Gm    is not generally submodular  but Balkanski et al 
  have developed approximation algorithms for maximizing Gm under the constraint that        cid       for the
problem 

Sm cid    arg max
    cid 

 
 

max
      

fi    

 

  cid 

  

  cid 

  

can sample functions from   and construct the empirical
average  similar to Problem   and try to optimize it by
 nding Sm cid  Hence  the total generalization error we incur
in this process is bounded by

error           Sm cid 
            
 cid   

compression error

       

 cid        Sm cid 

 

approximation error

Note that once we have error     for some small       then
maximizing over Sm cid  is almost as good as maximizing
over    but possibly much faster  To that end we need
to bound both compression error and approximation error 
In fact we can prove the following result for the required
number of samples to ensure small approximation error 
Theorem   For any         and any set   of size at most
 cid  we can ensure that

 max

    cid 

Pr

        Gm       

     

as long as       cid  log      log 

In contrast  the compression error cannot be made arbitrarily small in general as the following example shows 

Bad Example  Suppose               and   is the
uniform distribution over the functions            fn  where
fi    is   if          otherwise  Each fi is in fact modular 
Let       It is easy to see that              Hence to
achieve compression error less than       must be greater
than         In particular for           must be equal
to the full set 

Suf cient conditions for sublinear  cid  The above example shows that one needs additional structural assumptions 
One simple special case arises when the union of the optimal sets for all functions in the support of   is of size
 cid           

       for some     supp   cid cid     cid 

 cid cid             arg max

   

The general case 
In this paper  we consider the problem
of maximizing      for general distributions         we
seek to solve 

  
 cid    arg max
    cid 

    

 

  Probabilistic Submodular Maximization
Since the distribution   is unknown  we cannot  nd   
or its corresponding optimum value      
 cid    Instead  we
 Balkanski et al    only consider cardinality constraints 

 cid 

     if   is any distribution over at most   functions 
clearly  cid    km suf ces 
This assumption might be too strong in practice  however 
Instead  we will consider another set of assumptions that allow bounding the compression error in the case of cardinality constraints  We assume   is endowed with   metric   
This metric is extended to sets of equal size so that for any
sets       cid  of equal size          cid  is the weight of   minimal
matching of elements in   to elements in    cid  where the
weight of       cid  for       and   cid       cid  is        cid  We will
assume that any function       is LLipschitz continuous
                            cid    Ld       cid  for some constant

Probabilistic Submodular Maximization in SubLinear Time

  for all sets   and    cid  of size    Many natural submodular
functions arising in data summarization tasks satisfy this
condition  such as exemplarbased clustering and certain
logdeterminants  see        Mirzasoleiman et al     
Theorem   Suppose each function       is LLipschitz
continuous in      Then  for any       the compression
error is bounded by   as long as  cid    kn kL  where    is
the  covering number of     

The proofs of Theorems   and   are given in the appendix 

  Algorithm
From the discussions in the previous section  we concluded
that under appropriate statistical conditions  we can ensure
that the error in Eq    can be made small if we have enough
samples dictated by Theorem   However  our conclusion
heavily relied on the fact that we can  nd the set Sm cid  in
Problem   As we noted earlier  the objective function in
Problem   is not submodular in general  Balkanski et al 
  thus the classical greedy algorithm may not provide
any approximation guarantees 
Our proposed algorithm ReplacementGreedy works in  cid 
rounds where in each round it tries to augment the solution in   particular greedy fashion 
It starts out with an
empty set       and checks  in each round  whether  
new element can be added without violating the matroid
constraint       stay an independent set  or otherwise it can
be replaced with an element in the current solution while
increasing the value of the objective function  To describe
how these decisions are made we need   few de nitions 
Let

           fi           fi   

denote the marginal gain of adding   to the set   if
we consider function fi  Similarly  we can de ne the
gain of removing an element   and replacing it with  
as               fi                fi    Since
fi is monotone we know that              However 
            may or may not be positive  Let us consider
                                      This is the set
of all elements in   such that if we replace them with   we
will not violate the matroid constraint  Then  we de ne the
replacement gain of            set   as follows 

          

        
max  maxy                   

if             
    
In words           denotes how much we can increase the
value of fi    by either inserting   into   or replacing  
with one element of   while keeping   an independent
set  Finally  let Repi       be the element that should be
replaced by   to maximize the gain and stay independent 

 cid 

Formally 

Repi        

 cid 

arg maxy                   

if             
    

    cid  

With the above de nitions it is easy to explain how ReplacementGreedy works  At all times  it maintains   solution   and   collection of feasible solutions Ti     for
each function fi  all initialized to the empty set in the beginning  In each iteration  it picks the top element    from
the ground set   based on its total contribution to fi   
         Ti  and updates    Then ReplacementGreedy checks whether any of Ti   can be augmented 
This is done either by simply adding     without violating the matroid constraint  or replacing it with an element
from Ti  The condition       Ti      ensures that such
replacement is executed only if the gain is positive 

 cid  

   fi Ti  by cid  

aggregate value cid  

Why does ReplacementGreedy work  Note that
solving maxT      fi     is an NPhard problem  However  ReplacementGreedy  nds and maintains independent sets Ti     throughout the course of the algorithm  In
fact  the collection                Tm  lower bounds Gm   
   fi Ti  Moreover  each iteration increases the
by  
         Ti  What
 
we show in the following theorem is that after  cid  iterations 
the accumulation of those gains reaches   constant factor
approximation to the optimum value 
Theorem   In only   cid mnr  function evaluations ReplacementGreedy returns   set   of size at most  cid  along
with independent sets Ti        such that

 cid 

 cid 

Gm       
 

     
  

Gm Sm cid 

  few comments are in order  Balkanski et al    proposed an algorithm with        approximation guarantee for the case where fi   are coverage functions and the
constraint is   uniform matroid  This is achieved by solving  potentially large  linear programs while maintaining
    cid mn  log    function evaluations  In contrast  our result holds for any collection of monotone submodular functions and any matroid constraint  Our approximation guarantee        is better than         Finally  our
algorithm is arguably faster in terms of both running time
 as it simply runs   modi ed greedy method  and query
complexity  as it is linear in all the parameters 

  Experiments
In this section  we describe our experimental setting  We
offer details on the datasets we used and the baselines
we ran ReplacementGreedy against  We  rst show that
ReplacementGreedy is highly ef cient       high utility 

Probabilistic Submodular Maximization in SubLinear Time

Algorithm   ReplacementGreedy

      Ti               
for          cid  do
     arg maxx 
           
for all           do

 cid  
         Ti 

if       Ti      then

Ti   Ti      Repi    Ti 

end if
end for

end for
Return sets   and         Tm

low running time  when solving the twostage submodular maximization problem on two concrete summarization
applications  article summarization and image summarization  We then demonstrate sublinear submodular maximization by showing that ReplacementGreedy can ef 
ciently reduce the dataset while incurring minimum loss 
We test the performance of ReplacementGreedy on  
movie dataset where movies should be recommended to
users with userspeci   utility functions 

mizing the submodular function         cid  

  Baselines
LocalSearch  This is the main algorithm described in
Balkanski et al    In our experiments  we use    
  We initialize   by incrementally picking  cid  elements
such that at each step we maximize the sum of marginal
gains for the   functions fi 
GreedySum  It greedily selects  cid  elements while maxii  fi    To
 nd   elements for each fi it runs another greedy algorithm 
GreedyMerge  It ideally serves as an upper bound for the
objective value  by greedily selecting   elements for each
function fi and returning their union  GreedyMerge can
easily violate the cardinality constraint  cid  as its solution can
have as many as mk elements 

  Metrics
Objective value  We compare algorithms  solutions   according to the scores Gm    for the twostage  empirical 
problem  and      for the sublinear maximization problem 
Loss  For the sublinear maximization problem  we measure
the relative loss in performance when using the summary 
compared to the full ground set       report        
 We thank the authors for providing us with their implemen 

tation 

Running time  We also compare the algorithms based on
their wallclock running time  The experiments were ran
in   Python   environment on   OSX   machine 
The processor was     GHz Intel Core    with   GB
  MHz DDR  memory 

  TwoStage Submodular Maximization
Article summarization on Wikipedia  The aim is to select   small  highly relevant subset of wikipedia articles
from   larger corpus  For this task  we reproduce the experiment from Balkanski et al    on Wikipedia articles
for Machine Learning  The dataset contains       articles divided into       categories  where each category
represents   subtopic from Machine Learning  The relevance of   set   with respect to   category   is measured
by the submodular function fi that counts the number of
pages that belong to category   with   link to at least one
page in    These submodular functions are LLipschitz
with       by considering the distance between two articles as the fraction of all pages that have   link to exactly
one of these two articles  Fig     and    show the objective
values for             and varying  cid  and    xed  cid     
 and varying    We  nd that ReplacementGreedy and
LocalSearch perform the same while GreedySum falls
off somewhat for larger values of  cid  However  if we look at
the running times  log scale  in Fig     and    we observe
that ReplacementGreedy is considerably faster than LocalSearch and close to GreedySum 
Image summarization on VOC  For this application
we use   subset of the VOC  dataset  Everingham et al 
  where we consider       with       categories 
Each category indicates   certain visual queue appearing in
the image such as chair  bird  hand  etc  We wish to obtain
  subset of these images that are relevant to all the categories  To that end we use Exemplar Based Clustering      
Mirzasoleiman et al    Let    be the portion of the
ground set associated to category    For any set   we also
let Si          denote its subset that is part of category
   We de ne fi      Li      Li         where
miny Si         Here    measures
Li         
the distance between images        cid  norm  and    is an
auxiliary element  With respect to distance    our submodular functions are LLipschitz with       Also  images
are represented by feature vectors obtained from categories 
For example  if there were two categories   and    and
an image had features           its feature vector would be
    Again if we look at Fig      for  xed       and
varying  cid  and Fig      for  cid      and varying    we see
that ReplacementGreedy and LocalSearch achieve the
same objective value  However  Fig     and    show that
LocalSearch is signi cantly slower than ReplacementGreedy 

 cid 

   

Probabilistic Submodular Maximization in SubLinear Time

    Wikipedia       

    Wikipedia  cid     

    VOC       

    VOC  cid     

    Wikipedia       

    Wikipedia  cid     

    VOC       

    VOC  cid     

Figure   Objective values and runtimes for our experiments  The two columns on the left correspond to the Wikipedia dataset and the
two columns on the right correspond to the VOC  dataset  Note that the runtimes are all in logscale  We let       and vary  cid  or   
 cid      and vary   

Figure   Images chosen by our algorithm on the VOC  dataset  for the case when  cid      and      

  Sublinear Summarization
In this part  we experimentally show how ReplacementGreedy can reduce the size of the dataset to  cid   cid    without
incurring too much loss in the process  To do so  we consider   movie recommendation application 
Movie recommendation with missing ratings 
The
dataset consists of user ratings on   scale of   to   along
with some movie information  There are   genres  Animation  Comedy  Thriller  etc  and each movie can have
one or more of these genres assigned to it  The goal is to
 nd   small set   of movies with the property that each user
will be able to  nd   enjoyable movies from   
In our setting we consider the top   highest rated
movies  that were rated by at least   users  alongside the
top   users ordered by number of movies they rated from
this set  We assign   userspeci   utility function fi to each
user   as follows  Let Ai be the subset of   which user  
rated  Let   be the number of movie genres  Furthermore 
we let            represent the highest rating given by user  
to   movie from the set   with genre    We also de ne wi  
to be the proportion of movies in genre   that user   rated
out of all the moviegenre ratings she provided  So wi   will
be higher for the genres that the user provided more feed 

back on  indicating that she might like these genres better 
Then  the valuation function by user   is as follows 

  cid 

fi     

wi jR         

  

In words  the way   user evaluates   set   is by picking the
highest rated movie from each genre  contained in    and
then take   weighted average  If we de ne the distance between two movies to be the maximum difference of ratings
they received from the same user  the submodular functions
fi will be LLipschitz with      
For the experiment  we split the users in half  We use the
 rst   of them as   training set for the algorithms to built
up their reduced ground set   of size  cid  We then compare submodular maximization with cardinality constraint
      on the reduced sets    returned by the baselines 
and that of the whole ground set   To this end  we sample   users from the test group and compute their average
values  Given the size of this experiment  we were unable
to run LocalSearch alongside ReplacementGreedy and
GreedySum  In its place  we introduce the random baseline that simply returns   set of size  cid  at random  Fig   
shows what fraction of the utility is preserved if we reduce
the ground set by ReplacementGreedy  GreedySum 

Probabilistic Submodular Maximization in SubLinear Time

    Movielens

     cid     

     cid     

     cid     

    Movielens Completed

     cid     

     cid     

     cid     

Figure   Experimental results in the sublinear regime  The  rst row corresponds to the sublinear experiment where the data set has
missing entries  and the second row refers to the experiment where we used   completed userratings matrix  The  rst column portrays
the ratio between the mean objective obtained on the summary set versus the ground set for   random users from the test set  Columns
  through   showcase the loss versus runtime for  cid       cid      and  cid      respectively  In these experiments      

and RandomSelection  Clearly  RandomSelection performs poorly  ReplacementGreedy has the highest utility  starting from   and practically closing the gap by reducing the ground set to only   movies  GreedySum also
closely follows ReplacementGreedy  Fig            
show the loss versus the running time for  cid       cid     
and  cid      Of course  if we use use the whole ground set
  the loss will be zero  This is shown by GreedyMerge 
However  this comes at the cost of maximizing the utility
of each user on   movies  Instead  by using ReplacementGreedy  we see that the loss is negligible  even for
 cid      while the running time suddenly improves by two
orders of magnitude 
Complete matrix movie recommendation  The previous experiment suffers from the potential issue that values are estimated conservatively         user derives value
only if we happen to select movies that she actually rated
in the data  To explore this potential bias  we repeat the
same experiment  this time by  rst completing the matrix
of  movie  rating  using standard techniques  Cand es  
Recht    We again consider   users and divide them
into training and test sets  The results are shown in Fig     
            We observe exactly the same trends  Basically 
  dataset with size   is approximately as good as   data set
of size   if the reduced ground set is carefully selected
by ReplacementGreedy 

  Analysis
In this section  we prove that Algorithm ReplacementGreedy returns   valid solution   of at most  cid  elements
along with independent sets Ti for all different categories

 cid  

such that the aggregate value  
   fi Ti  is at least
       
 
         fraction of the optimum solution   ob 
 
jective value  namely Gm Sm cid  We note that the objective value of ReplacementGreedy   solution  Gm    is
   fi Ti  and therefore Algorithm Replaceat least  
 
mentGreedy is    
    approximation algorithm for
our two stage submodular maximization problem 

       

 cid  

Proof of Theorem   Since Algorithm ReplacementGreedy runs in  cid  iterations  and adds an element to  
in each iteration  the  nal output size      will not be
more than  cid  Each set Ti is initialized with   at the
beginning which is an independent set  Also whenever
ReplacementGreedy adds an element    to Ti  it removes Repi    Ti  By de nition  either Repi    Ti  is
equal to some element     Ti such that set     Ti    
is an independent set or Repi    Ti  is the empty set and
      Ti is an independent set  In either case  set Ti after
the update will remain an independent set  Therefore the
output of ReplacementGreedy consists of independent
sets Ti        for every category    It remains to lower
bound the aggregate values of these   sets 
We lower bound the aggregate increments of values incurred by adding each element we add to   in terms of
the gap between the current objective value  
   fi Ti 
 
and the optimum objective value Gm Sm cid  This way  we
can show that for each of the  cid  elements added to    the current objective value is incremented enough that in total we
reach at least  
     fraction of the optimum value  By
de nition of   and the update operations of Algorithm ReplacementGreedy  addition of element   to    increases
         Ti  We also

the summation cid  

   fi Ti  by cid  

       

 cid  

Probabilistic Submodular Maximization in SubLinear Time

 

 

 

  Ti

  Ti

fi Sm cid 

 Ti
 Ti

  Sm cid 
  Sm cid 

that submodularity of fi implies cid 
 fi Ti  We also have cid 
     cid 
 cid 
 cid  

It is wellknown  see Lemma   of  Bateni et al   
      Ti   
      Ti 
      Ti       because range of mapping   is   subset of Ti  and no two elements are mapped
to the same     Ti  By submodularity of fi  the latter term
      Ti       is upper bounded by fi Ti  We
conclude that in each step the total value is increased by at
        Ti   we assume  Sm cid     cid 
least  
 cid  
since objective function Gm is monotone increasing  In
other words  in each iteration          cid  the increment
Xt   Xt  is at least  
 cid  Xt  where Xt is
   fi Ti  at the end of
de ned to be the total value  
 
iteration    Solving this recurrence equation inductively
yields Xt    
 cid     Gm Sm cid  We note that
       denotes the total value before the algorithm starts 
The induction step is proved as follows 

 cid  
 cid  Gm Sm cid     

           

   fi Sm cid 

 

Xt    Xt   Gm Sm cid 

   Xt
 cid 

 cid 
       
 cid 

 Xt

  Xt    Gm Sm cid 
 cid 
       
 
 cid 
 

Gm Sm cid 

         
 cid 

 
 

 cid 

 

 
 

         
 cid 

   Gm Sm cid 

   Gm Sm cid 

At the end of Algorithm ReplacementGreedy  the total
     
value   cid  is at least  
    Gm Sm cid  which completes the proof 
 

 cid   cid Gm Sm cid     

           

  Conclusions
In this paper  we have studied the novel problem of sublinear time probabilistic submodular maximization  By investing computational effort once to reduce the ground set
based on training instances  faster optimization is achieved
on test instances  Our key technical contribution is ReplacementGreedy    novel algorithm for twostage submodular maximization  the empirical variant of our problem  Compared to prior approaches  ReplacementGreedy provides constant factor approximation guarantees
while applying to general submodular objectives  handling
arbitrary matroid constraints and scaling linearly in all relevant parameters 
Acknowledgments  This work was supported by DARPA
Young Faculty Award    AP  SimonsBerkeley
fellowship  and ERC StG   This work was done in
part while Amin Karbasi and Andreas Krause were visiting
Simons Institute for the Theory of Computing 

note that the selected element    maximizes this aggregate
increment  We prove   lower bound benchmark according
to the potential increments of values by elements in Sm cid  if
we add them instead of    In particular  we know that 

      Ti     

 Sm cid 

      Ti 

 cid 

  cid 

  Sm cid 

  

  cid 

  

This equation holds because the rightmost side is the average increments of values of optimum elements if we add
them instead of    and we know that    is the maximizer of the total value increments  Let Sm cid 
be the independent subset of Sm cid  with maximum fi value      
    arg maxA   Sm cid  fi    Since the   values are
Sm cid 
all nonnegative  we can narrow down the rightmost side of
  cid 
above equation  and imply that 

  cid 

 cid 

 

      Ti     

      Ti 

 

 Sm cid 

  

  Sm cid 

 

  

We can apply exchange properties of matroids to lower
bound the total   values  the rightmost side  for each category  By Corollary    of  Schrijver    we know
that there exists   mapping   that maps every element of
  Ti to either the empty set or an element of Ti   Sm cid 
Sm cid 
 
such that 

 

 

  for every     Sm cid 

  Ti  the set       Ti       is

an independent set  and
  for every         Sm cid 

  Ti  we either have      cid 
    or both of     and     are equal to the
empty set 

 

 

We note that Corollary    of  Schrijver    is stated
for equal size sets        Ti     Sm cid 
  which can be easily adapted to nonequal size sets and achieve the above
mapping by applying the exchange property of matroids iteratively  and making these two sets equal size  Given the
mapping   the next step is to lower bound each       Ti 
by how much we can increase the value of Ti by replacing     with   in set Ti  Since       Ti       is an
independent set  we have
      Ti    fi      Ti         fi Ti 

        Ti          Ti            
        Ti          Ti      

where the equality holds by de nition of    values  and the
last inequality holds by submodularity of fi  Combining
this lower bound on   with Equation   implies that in each
step  adding element    to set   increases the total value of
the current solution by at least 

      Ti          Ti      

  cid 

 cid 

  

  Sm cid 

 

 Ti

 

 Sm cid 

Probabilistic Submodular Maximization in SubLinear Time

References
Ashwinkumar Badanidiyuru  Shahar Dobzinski  Sigal Oren  Optimization with demand oracles  EC   

Badanidiyuru  Ashwinkumar and Jan  Vondr ak  Fast algorithms for maximizing submodular functions  In SODA 
 

Badanidiyuru  Ashwinkumar  Mirzasoleiman  Baharan 
Karbasi  Amin  and Krause  Andreas  Streaming submodular maximization  Massive data summarization on
the     In ACM KDD   

Balkanski  Erik  Krause  Andreas  Mirzasoleiman  Baharan  and Singer  Yaron  Learning sparse combinatorial
representations via twostage submodular maximization 
In Proc  International Conference on Machine Learning
 ICML  June  

Bateni  MohammadHossein  Hajiaghayi  MohammadSubmodular
Taghi  and Zadimoghaddam  Morteza 
In Approximation 
secretary problem and extensions 
Randomization  and Combinatorial Optimization  Algorithms and Techniques  pp    Springer   

Buchbinder  Niv  Feldman  Moran  Naor  Joesph  Sef 
and Schwartz  Roy  Submodular maximization with cardinality constrains  In SODA   

Calinescu  Gruia  Chekuri  Chandra    al  Martin  and
Vondr ak  Jan  Maximizing   monotone submodular
function subject to   matroid constraint  SIAM Journal
on Computing     

Cand es     and Recht     Exact matrix completion via
convex optimization  In Foundations of Computational
Mathematics   

Das  Abhimanyu and Kempe  David 

Submodular
meets spectral  Greedy algorithms for subset selection  sparse approximation and dictionary selection 
arXiv   

ElArini  Khalid  Veda  Gaurav  Shahaf  Dafna  and
Guestrin  Carlos  Turning down the noise in the blogosphere  In KDD   

Everingham     Van Gool     Williams           Winn    
and Zisserman     The PASCAL Visual Object Classes
Challenge    VOC  Results   

Feige  Uriel  On maximizing welfare when utility functions

are subadditive  SIAM Journal on Computing   

Goemans  Michel  Chernoff bounds  and some applica 

tions   

Hazan  Elad and Kale  Satyen  Online submodular mini 

mization  In NIPS   

Jegelka  Stefanie and Bilmes  Jeff    Online submodular
In Internaminimization for combinatorial structures 
tional Conference on Machine Learning  ICML  Bellevue  Washington   

Krause     and Guestrin     Nearoptimal nonmyopic value

of information in graphical models  In UAI   

Krause  Andreas and Golovin  Daniel 

Submodular
In Tractability  Practical
function maximization 
Approaches to Hard Problems  Cambridge University
Press   

Krause  Andreas and Gomes  Ryan    Budgeted nonpara 

metric learning from data streams  In ICML   

Kumar  Ravi  Moseley  Benjamin  Vassilvitskii  Sergei  and
Vattani  Andrea  Fast greedy algorithms in mapreduce
and streaming  In SPAA   

Lin  Hui and Bilmes  Jeff    class of submodular functions

for document summarization  In ACL   

Lin  Hui and Bilmes  Jeff  Learning mixtures of submodular shells with application to document summarization 
In Uncertainty in Arti cial Intelligence  UAI  Catalina
Island  USA  July   AUAI 

Minoux  Michel  Accelerated greedy algorithms for maxIn Proc  of the  th
imizing submodular set functions 
IFIP Conference on Optimization Techniques  Springer 
 

Mirzasoleiman  Baharan  Karbasi  Amin  Sarkar  Rik  and
Krause  Andreas  Distributed submodular maximization 
Identifying representative elements in massive data  In
NIPS   

Mirzasoleiman  Baharan  Badanidiyuru  Ashwinkumar 
Karbasi  Amin  Vondrak  Jan  and Krause  Andreas 
Lazier than lazy greedy  In AAAI   

Mirzasoleiman  Baharan  Badanidiyuru  Ashwinkumar 
and Karbasi  Amin  Fast constrained submodular maximization  Personalized data summarization  In ICML 
   

Mirzasoleiman  Baharan  Karbasi  Amin  Sarkar  Rik  and
Krause  Andreas  Distributed submodular maximization 
Journal of Machine Learning Research       

Feldman  Moran  Harshaw  Christopher  and Karbasi 
Amin  Greed is good  Nearoptimal submodular maximization via greedy optimization  In COLT   

Mirzasoleiman  Baharan  Karbasi  Amin  Sarkar  Rik  and
Krause  Andreas  Distributed submodular maximization 
Journal of Machine Learning Research  JMLR     

Probabilistic Submodular Maximization in SubLinear Time

Mirzasoleiman  Baharan  Zadimoghaddam  Morteza  and
Fast distributed submodular cover 

Karbasi  Amin 
Publicprivate data summarization  In NIPS     

Nemhauser  George    Wolsey  Laurence    and Fisher 
Marshall    An analysis of approximations for maximizing submodular set functions      Mathematical Programming   

Schrijver  Lex  Combinatorial optimizationpolyhedra and
ef ciency  Algorithms and Combinatorics   
 

Shahar Dobzinski  Noam Nisan and Schapira  Michael 
Approximation algorithms for combinatorial auctions
with complementfree bidders  STOC   

Singer  Yaron  Budget feasible mechanisms  FOCS   

Streeter  Matthew and Golovin  Daniel  An online algoIn NIPS 

rithm for maximizing submodular functions 
 

Tschiatschek  Sebastian  Iyer  Rishabh  Wei  Haochen  and
Bilmes  Jeff  Learning Mixtures of Submodular FuncIn Neural
tions for Image Collection Summarization 
Information Processing Systems  NIPS   

Wei  Kai  Liu  Yuzong  Kirchhoff  Katrin  and Bilmes  Jeff 
Using document summarization techniques for speech
data subset selection  In Proceedings of Annual Meeting
of the Association for Computational Linguistics  Human Language Technologies   

Wei  Kai  Iyer  Rishabh  and Bilmes  Jeff  Fast multistage

submodular maximization  ICML     

Wei  Kai  Liu  Yuzong  Kirchhoff  Katrin  Bartels  Chris 
and Bilmes  Jeff  Submodular subset selection for largescale speech training data  In ICASSP     

Yue  Yisong and Guestrin  Carlos  Linear submodular bandits and their application to diversi ed retrieval  NIPS 
 

