Globally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

Alon Brutzkus   Amir Globerson  

Abstract

Deep learning models are often successfully
trained using gradient descent  despite the worst
case hardness of the underlying nonconvex optimization problem  The key question is then under what conditions can one prove that optimization will succeed  Here we provide   strong result of this kind  We consider   neural net with
one hidden layer and   convolutional structure
with no overlap  and   ReLU activation function  For this architecture we show that learning is NPcomplete in the general case  but that
when the input distribution is Gaussian  gradient
descent converges to the global optimum in polynomial time  To the best of our knowledge  this
is the  rst global optimality guarantee of gradient
descent on   convolutional neural network with
ReLU activations 

  Introduction
Deep neural networks have achieved stateof theart performance on many machine learning tasks in areas such as natural language processing  Wu et al    computer vision
 Krizhevsky et al    and speech recognition  Hinton
et al    Training of such networks is often successfully performed by minimizing   highdimensional nonconvex objective function  using simple  rstorder methods
such as stochastic gradient descent 
Nonetheless  the success of deep learning from an optimization perspective is poorly understood theoretically 
Current results are mostly pessimistic  suggesting that even
training    node neural network is NPhard  Blum  
Rivest    and that the objective function of   single
neuron can admit exponentially many local minima  Auer
et al    Safran   Shamir    There have been re 

 Tel Aviv University  Blavatnik School

puter Science 
 alonbrutzkus mail tau ac il 
 gamir cs tau ac il 

Correspondence

of ComAlon Brutzkus
Globerson

to 
Amir

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

cent attempts to bridge this gap between theory and practice  Several works focus on the geometric properties of
loss functions that neural networks attempt to minimize 
For some simpli ed architectures  such as linear activations  it can be shown that there are no bad local minima
 Kawaguchi    Extension of these results to the nonlinear case currently requires very strong independence assumptions between the activations of the neurons and the
inputs  Kawaguchi   
Since gradient descent is the main  workhorse  of deep
learning it is of key interest to understand its convergence
properties  However  there are no results showing that
gradient descent is globally optimal for nonlinear models  except for the case of many hidden neurons  Andoni
et al    and nonlinear activation functions that are not
widely used in practice  Zhang et al    Here we provide the  rst such result for   neural architecture that has
two very common components  namely   ReLU activation
function and   convolution layer 
The architecture considered in the current paper is shown in
Figure   We refer to these models as nooverlap networks 
  nooverlap network can be viewed as   simple convolution layer with non overlapping  lters  followed by   ReLU
activation function  and then average pooling  Formally  let
    Rm denote the  lter coef cients  and assume the input
  is in Rd  De ne         and assume that   is integral 
Partition   into   nonoverlapping parts and denote      the
ith part  Finally  de ne   to be the ReLU activation function  namely         max     Then the output of the
network in Figure   is given by 

          

 
 

           

 

 cid 

 

We note that such architectures have been used in several
works  Lin et al    Milletari et al    but we view
them as important  rstly because they capture key properties of general convolutional networks 
We address the realizable case  where training data is generated from   function as in Eq    with weight vector
   Training data is then generated by sampling   training points            xn from   distribution    and assigning
them labels using              The learning problem is

 See more related work in Section  

Globally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

then to  nd     that minimizes the squared loss  In other
words  solve the optimization problem 

    xi       yi 

 

 cid 

 

min
 

 
 

In the limit       this is equivalent to minimizing the
population risk 

 cid 

                    cid 

 cid      Ex  

 

Like several recent works  Hardt et al    Hardt   Ma 
  we focus on minimizing the population risk  leaving
the  nite sample case to future work  We believe the population risk captures the key characteristics of the problem 
since the large data regime is the one of interest 

Figure   Convolutional neural network with nonoverlapping  lters 
In the  rst layer     lter   is applied to nonoverlapping
parts of the input vector    and the output passes through   ReLU
activation function  The outputs of the neurons are then averaged
to give the output   

Our key results are as follows 

  Worst Case Hardness  Despite the simplicity of NoOverlap Networks  we show that learning them is in
fact hard if   is unconstrained  Speci cally  in Section   we show that learning NoOverlap Networks is
NP complete via   reduction from   variant of the set
splitting problem 

  Distribution Dependent Tractability  When  
corresponds to independent Gaussian variables with
            we show in Section   that NoOverlap
Networks can be learned in polynomial time using gradient descent 

The above two results nicely demonstrate the gap between
worstcase intractability and tractability under assumptions

on the data  We provide an empirical demonstration of this
in Section   where gradient descent is shown to succeed in
the Gaussian case and fail for   different distribution 
To further understand the role of overlap in the network 
we consider networks that do have overlap between the  lters  In Section   we show that in this case  even under
Gaussian distributed inputs  there will be nonoptimal local
minima  Thus  gradient descent will no longer be optimal
in the overlap case 
In Section   we show empirically
that these local optima may be overcome in practice by using gradient descent with multiple restarts 
Taken together  our results are the  rst to demonstrate
distribution dependent optimality of gradient descent for
learning   neural network with   convolutional like architecture and   ReLU activation function 

  Related Work
Hardness of learning neural networks has been demonstrated for many different settings  For example  Blum
  Rivest   show that learning   neural network with
one hidden layer with   sign activation function is NPhard
in the realizable case  Livni et al    extend this to
other activation functions and bounded norm optimization 
Hardness can also be shown for improper learning under
certain cryptographic assumptions       see Daniely et al 
  Klivans    Livni et al    Note that these
hardness results do not hold for the regression and tied parameter setting that we consider 
Due to the above hardness results  it is clear that the success of deeplearning can only be explained by making additional assumptions about the data generating distribution 
The classic algorithm by Baum   shows that intersection of halfspaces         speci   instance of   one hidden
layer network  is PAC learnable under any symmetric distribution  This was later extended in Klivans et al   
to logconcave distributions 
The above works do not consider gradient descent as the
optimization method  leaving open the question of which
assumptions can lead to global optimality of gradient descent  Such results have been hard to obtain  and we survey some recent ones below  One instance when gradient descent can succeed is when there are enough hidden
units such that random initialization of the  rst layer can
lead to zero error even if only the second layer is trained 
Such overspeci ed networks have been considered  Andoni et al    Livni et al    and it was shown that
gradient descent can globally learn them in some cases
 Andoni et al    However  the assumption of overspeci cation is very restrictive and limits generalization 
In contrast  we show convergence of gradient descent to  
global optimum for any network size and consider convo 

wReLU yxwwGlobally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

lutional neural networks with shared parameters  Another
interesting case is linear dynamical systems  where Hardt
et al    show that under independence assumptions
maximum likelihood is quasiconcave and hence solvable
with gradient ascent 
Recent work by Mei et al    shows that regression with   single neuron and certain nonlinear activation
functions  can be learned with gradient descent for subGaussian inputs  We note that their architecture is signi 
cantly simpler than ours  in that it uses   single neuron  In
fact  their regression problem can also be solved via methods for generalized linear models  Kakade et al   
Shamir   recently showed that there is   limit to what
distribution dependent results can achieve  Namely  it was
shown that for large enough onehidden layer networks 
no distributional assumption such as Gaussian inputs can
make gradient descent tractable  Importantly  the construction in Shamir   does not use parameter tying and thus
is not applicable to the architecture we study here 
Several works have focused on understanding the loss surface of neural network objectives  but without direct algorithmic implications  Kawaguchi   show that linear neural networks do not suffer from bad local minima 
Hardt   Ma   consider objectives of linear residual
networks and prove that there are no critical points other
than the global optimum  Soudry   Carmon   show
that in the objective of overparameterized neural networks
with dropoutlike noise  all differentiable local minima are
global  Other works  Safran   Shamir    Haeffele
  Vidal    give similar results for overspeci ed networks  All of these results are purely geometric and do not
have direct implications on convergence of optimization algorithms  Janzamin et al    and Goel et al   
suggest alternatives to gradientbased methods for learning
neural networks  However  these algorithms are not widely
used in practice  Finally  Choromanska et al    use
spin glass models to argue that  under certain generative
modelling and architectural constraints  local minima are
likely to have low loss values 
The theory of nonconvex optimization is closely related
to the theory of neural networks  Recently 
there has
been substantial progress in proving convergence guarantees of simple  rstorder methods in various machine learning problems  that don   correspond to typical neural nets 
These include for example matrix completion  Ge et al 
  and tensor decompositions  Ge et al   
Finally  recent work by Zhang et al    shows that neural nets can perfectly    random labelings of the data  Understanding this from an optimization perspective is largely
an open problem 

  Preliminaries
We use boldfaced letters for vectors and capital letters for
matrices  The ith row of   matrix   is denoted by ai 
In our analysis in Section   and Section   we assume that
the input feature     Rd is   vector of IID Gaussian random variables with zero mean and variance one  Denote
this distribution by    We consider networks with one hidden layer  and   hidden units  Our main focus will be on
NoOverlap Networks  but we begin with   more general
onehidden layer neural network with   fullyconnected
layer parameterized by     Rk   followed by average
pooling  The network output is then 

           

 
 

   wi     

 

 cid 

 

where     is the pointwise ReLU function 
We consider the realizable setting where there exists   true
    using which the training data is generated  The population risk  see Eq    is then 

 cid       EG cid                      cid   

 

As we show next   cid     can be considerably simpli ed 
First  de ne 

          EG                    

 

Simple algebra then shows that 

 cid   wi  wj       wi    

 cid 

   

 cid      

 
  

          

      

 
The next Lemma from Cho   Saul   shows that
        has   simple form 
Lemma    Cho   Saul    Section   Given       
Rd denote by      the angle between   and    Then 

 cid 

 cid 

 cid 

         

 cid   cid cid   cid 

 
 

sin       

      

cos     

   cid 

 cid 

The gradient of   with respect to   also turns out to have
  simple form  as stated in the lemma below  The proof is
deferred to the supplementary material 
Lemma   Let   be as de ned in Eq    Then   is differentiable at all points    cid    and

        

  

 

 
 

 cid   cid   

 cid   cid  sin       

 
 

        

 

 The variance per variable can be arbitrary  We choose one for

simplicity 

 cid 

 cid 

Globally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

 cid            im

 cid  where      Rl is   zero vector 

We conclude by specialcasing the results above to NoOverlap Networks  In this case  the entire model is speci ed by   single  lter vector     Rm  The rows wi
are mostly zeros  except for the indices          
          im  which take the values of    Namely  wi  
The same holds for the vectors   
  with   weight vector
   This simpli es the loss considerably  since for all   
   cid   cid  and for all    cid       wi  wj   
  wi  wi     
   cid   cid cid   cid  Thus the loss
   cid   cid  and   wi    
 
 cid    for NoOverlap Networks yields  up to additive factors in   
 
  

 cid   cid     kg          cid   cid cid   cid cid 

       

 cid     

 cid 

 

where        

  and          
   

  Learning NoOverlap Networks is

NPComplete

The NoOverlap Networks architecture is   simpli ed convolutional layer with average pooling  However  as we
show here  learning it is still   hard problem  This will
motivate our exploration of distribution dependent results
in Section  
Recall that our focus is on minimizing the squared error in
Eq    For this section  we do not make any assumptions
about    Thus   can be   distribution with uniform mass
on training points            xn  recovering the empirical risk
in Eq    We know that  cid    in Eq    can be minimized by
setting        and the corresponding squared loss  cid   
will be zero  However  we of course do not know    and
the question is how dif cult is it to minimize  cid    In what
follows we show that this is hard  Namely  it is an NPcomplete problem to  nd     that comes   close to the
minimum of  cid    for some constant  
We begin by de ning the SetSplitting byk Sets problem 
which is   variant of the classic SetSplitting problem
 Garey   Johnson    After establishing the hardness
of SetSplitting byk Sets  we will provide   reduction from
it to learning NoOverlap Networks 
De nition   The SetSplitting byk Sets decision problem
is de ned as follows  Given    nite set   of   elements and
  collection   of at most         subsets Cj of    do there
  Si     and
for all   and    Cj  cid  Si 
For       and without the upper bound on     this is
known as the SetSplitting decision problem which is NPcomplete  Garey   Johnson    Next  we show that
SetSplitting byk Sets is NPcomplete  The proof is via  
reduction from  SAT and induction  and is provided in the
supplementary material 

exist disjoint sets         Sk such that cid 

Proposition   SetSplitting byk Sets is NPcomplete
for all      

We next formulate the NoOverlap Networks optimization
problem 
De nition   The kNon OverlapOpt problem is de ned
as follows  The input is   distribution DX   over inputoutput pairs      where     Rd  If the input is realizable
by   nooverlap network with   hidden neurons  then the
output is   vector   such that 

 cid 

              cid 

EDX  

 

 

    

 

Otherwise an arbitrary weight vector is returned 

 

The above problem returns     that minimizes the
populationrisk up to
     accuracy  It is thus easier than
minimizing the risk to an arbitrary precision    see Section
  Theorem  
We prove the following theorem  which uses some ideas
from Blum   Rivest   but introduces additional constructions needed for the no overlap case 
Theorem   For all       the kNon OverlapOpt problem is NPcomplete 

de ne the vector xd       cid 

Proof  We will show   reduction from SetSplitting byk 
sets to kNon OverlapOpt  Assume   given instance of the
SetSplitting byk sets problem with   set   and collection
of subsets    Denote              and               
Let      Rd be the all zeros vector  For   vector     Rd 
de ne the vector di      Rkd to be the concatenation of
      vectors     followed by   and       vectors     and
let                     dk      Rk   
We next de ne   training set for kNon OverlapOpt  For
each element       de ne an input vector xi     ei 
where ei is the standard basis of Rd  Assign the label
  to this input  In addition  for each subset Cj    
yi    
ei  and label yd      
Thus we have           data points in Rk    Let DX   be  
uniform distribution over the training set points       each
point with probability at least  
We will now show that the given instance of SetSplitting 
byk sets has   solution       there exist splitting sets  if and
only if kNon OverlapOpt returns   weight vector with low
risk  First  assume there exist splitting sets      Sk  For
each           de ne the vector aSl   Rd such that
for all     Sl  aSl
       otherwise  De ne
  NoOverlap Network with     inputs and weight vector

kd since               

      and aSl

  Cj

 We assume that the population risk is ef ciently computable 
 The sets are disjoint  their union is   and for all   and   

Cj  cid  Si 

Globally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

     aS  aS    aSk     Rkd  Then for all          
we have 

 

 
 

  yi

 

   xi      

and for all   

    aSl    ei 

 cid  
 cid  
    aSl    cid 

 

  Cj

ei 

 

      yd  
   xd        
 
where the last equality follows since for all   and    Cj  cid 
Sl  There thus exists     for which the error in Eq    is zero
and kNon OverlapOpt will return   weight vector with
low risk 
Conversely  assume that kNon OverlapOpt returned  
     on DX   above  Dew   Rkd with risk less than
note by              wk  where wl   Rd  We will
show that this implies that there exist   splitting sets  For
all   cid    cid  in the training set it holds that 

 

      cid         cid 

kd

  EDX                   

 

    

 cid 

   

      

 
     

ei      

This implies that for all   and   
      ei        
 

 
   
 
    for           and
De ne sets Sl        wT
WLOG assume they are disjoint by arbitrarily assigning
points that belong to more than one set  to one of the sets
they belong to  We will next show that these Sl are splitl Sl     and no subset Cj is  

  ei    

  Cj

subset of some Sl 

ting  Namely  it holds that cid 
 cid  
tion of Sl we deduce that cid 
  cid 
plies that      cid 

   

Since      ei      
       
for all    it follows that for each       there exists    
      such that wT
     Therefore  by the de nil Sl      To show the second property  assume by contradiction that for some   and
 Cj 
   Cj   Sm  Then wT
     which im 

  ei    

  Cj

ei   
    wT

     

   cid 

ei 

 

    wT

 cid  

ei      

  Cj

  

  ei 

 

  Cj

 Cj 
       

        contradiction  This concludes our proof 

 

  NoOverlap Networks can be Learned for

Gaussian Inputs

In this section we assume that the input features   are generated via   Gaussian distribution    as in Section   We
will show that in this case  gradient descent will converge
with high probability to the global optimum of  cid     Eq   
in polynomial time 
In order to analyze convergence of gradient descent on
 cid  we need   characterization of all
the critical and
nondifferentiable points  We show that  cid  has   nondifferentiable point and   degenerate saddle point  Therefore  recent methods for showing global convergence of
gradientbased optimizers on nonconvex objectives  Lee
et al    Ge et al    cannot be used in our case  because they assume all saddles are strict   and the objective
function is continuously differentiable everywhere 
The characterization is given in the following proposition 
The proof relies on the fact that  cid    depends only on
 cid   cid cid   cid  and       and therefore          it can be assumed that    lies on one of the axes  Then by   symmetry
argument  in order to prove properties of the gradient and
the Hessian  it suf ces to calculate partial derivatives with
respect to at most three variables 
Proposition   Let  cid    be de ned as in Eq    Then the
following holds 
   cid    is differentiable if and only if    cid   
  For        cid    has three critical points 

      local maximum at      
      unique global minimum at       
      degenerate

saddle

point

at  

 

   

       

 

For             is not   local maximum and the
unique global minimum    is the only differentiable
critical point  

We next consider   simple gradient descent update rule for
minimizing  cid    and analyze its convergence  Let    
  denote the step size  Then the update at iteration   is
simply 

wt    wt    cid wt 

 

To conclude  we have shown that NoOverlap Networks are
hard to learn if one does not make any assumptions about
the training data  In fact we have shown that  nding    
     is hard  In the next section  we show
with loss at most
that certain distributional assumptions make the problem
tractable 

 

 The LHS is true because for   nonnegative random variable

                for all    and in our case         
kd  

Our main result  stated formally below  is that the above
update is guaranteed to converge to an   accurate solution
    iterations  We note that the dependence of the
after     
   saddle point is degenerate if the Hessian at the point has

only nonnegative eigenvalues and at least one zero eigenvalue 

   saddle point is strict if the Hessian at the point has at least

one negative eigenvalue 

 See Figure  

Globally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

follows from the fact that       is   local maximum 
The fact that wt stays away from the problematic points
allows us to show that  cid    has   Lipschitz continuous
gradient on the line between wt and wt  with constant
        By standard optimization analysis  Nesterov 
    iterations we will
  it follows that after         
have  cid   wt cid       for some             This in
 
 close to   
turn can be used to show that wt is   
Finally  since  cid        cid       cid  it follows that wt approximates the global minimum to within    accuracy 
Theorem   implies that gradient descent converges to  
point   such that  cid       
   in time   poly    where
  is the input dimension  The following corollary thus
follows 
Corollary   Gradient descent
the kNon 
OverlapOpt problem under the Gaussian assumption on
  with high probability and in polynomial time 

solves

  Empirical Illustration of Tractability Gap
The results in the previous sections showed that NoOverlap Networks optimization is hard in the general case 
but tractable for Gaussian inputs  Here we empirically
demonstrate both the easy and hard cases  The training data
for the two cases will be generated by using the same   
but different distributions over   
To generate the  hard  case  we begin with   set splitting
problem  In particular  we consider   set   with   elements and   collection   of   subsets of    each of size
  We choose Cj such that there exists subsets      that
split the subsets Cj  We use the reduction in Section  
to convert this into   NoOverlap Networks optimization
problem  This results in   training set of size  
Since we know the    that solves the set splitting problem  we can use it to label data from   different distribution 
Motivated by Section   we use   Gaussian distribution   as
de ned earlier and generate   training set of the same size
 namely   and labels given by the nooverlap network
with weight   
For these two learning problems we used AdaGrad  Duchi
et al    to optimize the empirical risk  plain gradient
descent also converges  but AdaGrad requires less tuning
of step size  For both datasets we used   random normal
initializer and for each we chose the best performing learning rate schedule  The training error for each setting as  
function of the number of epochs is shown in Figure   It is
clear that in the nonGaussian case  AdaGrad gets trapped

 The proof holds even for       where       is not   local

maximum 

polynomial in   

 Note that the complexity of   gradient descent iteration is

Figure   Colormap of  cid     Eq    in   dimensions for     
    and      

 

convergence rate on   is similar to standard results on convergence of gradient descent to stationary points       see
discussion in AllenZhu   Hazan   
Theorem   Assume  cid   cid      For any       and
  there exists             such that
          sin  
with probability at least       gradient descent initialized
randomly from the unit sphere with learning rate   will get
to   point   such that  cid         in     
    iterations 
The complete proof is provided in the supplementary material  Here we provide   high level overview  In particular 
we  rst explain why gradient descent will stay away from
the two bad points mentioned in Lemma  
First we note that the gradient of  cid    at wt is given by 

 cid wt       wt    wt     wt        

 
where    and    are two functions such that            
  and        if and only if  wt        Thus the gradient
is   sum of   vector in the direction of wt and   vector in
the direction of    At iteration       we have 
wt           wt    wt      wt      

 
 cid    we have
It follows that for       and  wt   
 wt       wt    Therefore  if        cid      we will
never converge to the saddle point in Lemma  
Next  assuming  cid   cid      and that              
 which occurs with probability       it can be shown that
the norm of wt is always bounded away from zero by  
constant       The proof is quite technical and
 Assumed for simplicity  otherwise  cid   cid  is   constant factor 
  can be found explicitly 
    hides   linear factor in   
    and    hide factors of  cid   cid          and  

Globally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

lapping  lter of size   with stride        for all        
                
  wi                  
where                Rl             is   vector of
  parameters and                         De ne
the following vectors wr            wl          
               
           and denote by
  
     the angle between two vectors   and   
One might wonder why the analysis of the overlapping
case should be any different than the nonoverlapping case 
However  even for    lter of size two  as above  the loss
function and consequently the gradient  are more complex
in the overlapping case  Indeed  the loss function in this
case is given by 

Figure   Training loss of Adagrad on the Gaussian and NonGaussian datasets  See Section   for details 

at   suboptimal point  whereas the Gaussian case is solved
optimally  In the Gaussian case AdaGrad converged to
   Therefore  given the Gaussian dataset we were able
to recover the true weight vector    whereas given the
data constructed via the reduction we were not  even though
both datasets were of the same size  We conclude that these
empirical  ndings are in line with our theoretical results 

  Networks with Overlapping Filters
Thus far we showed that the nonoverlapping case becomes
tractable under Gaussian inputs    natural question is
then what happens when overlaps are allowed  namely  the
stride is smaller than the  lter size  Will gradient descent
still  nd   global optimum  Here we show that this is in
fact not the case  and that with probability greater than  
 
gradient descent will get stuck in   suboptimal region  In
Section   we analyze this setting for   two dimensional
example and provide bounds on the level of suboptimality 
In Section   we report on an empirical study of optimization for networks with overlapping  lters  Our results suggest that by restarting gradient descent   constant number
of times  it will converge to the global minimum with high
probability  Complete proofs of the results are provided in
the supplementary material 

  Suboptimality of Gradient Descent for   
We consider an instance where there are           neurons and matrices          Rk   correspond to an over 
 We note that the value of   attained by the nonGaussian
case is quite high  since the zero weight vector in this case has
loss of order  

 cid       cid   cid     cid   cid            
         wr  wl      wl    
  
    wr    

     

where      
  

          
 cid   
        

 

         cid   cid cid   cid 

 cid         and         

 

 

 

Figure   The population risk for   network with overlapping  lters  with   two dimensional  lter                     
and Gaussian inputs 

Compared to the objective in Eq    which depends only on
 cid   cid   cid   cid  and       we see that the objective in Eq   
has new terms such as   wr    
    which has   more complicated dependence on the weight vectors    and    This
does not only have implications on the analysis  but also
on the geometric properties of the loss function and the dynamics of gradient descent  In particular  in Figure   we
see that the objective has   large suboptimal region which
is not the case when the  lters are nonoverlapping 
As in the previous section we consider gradient descent updates as in Eq    The following Proposition shows that
if   is initialized in the interior of the fourth quadrant of
   then it will stay there for all remaining iterations  The
proof is   straightforward inspection of the components of
the gradient  and is provided in the supplementary 

 Iteration LossNonGaussianGaussianGlobally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

    if wt is in the interior

Proposition   For any        
of the fourth quadrant of    then so is wt 
Note that in our example the global optimum    is in the
second quadrant  it   easy to show that it is also unique 
Hence  if initialized at the fourth quadrant  gradient descent
will remain in   suboptimal region  The suboptimality
can be clearly seen in Figure   In the proposition below
we formalize this observation by giving   tight lower bound
on the values of  cid    for   in the fourth quadrant  Specifk   
ically  we show that the suboptimality scales with     
The proof idea is to express all angles between all the vectors that appear in Eq    via   single angle parameter  
between   in the fourth quadrant and the positive xaxis 
Then it is possible to prove the relatively simpler one dimensional inequality that depends on  
Proposition   Let            
 
   
  then for all   in the fourth quadrant       
      cid   cid  and this lower bound is attained by
           

 
     

       

 

 

   

 

 

The above two propositions result in the following characterization of the suboptimality of gradient descent for
       and overlapping  lters 
Theorem   De ne      as in Proposition   Then with
probability    
      randomly initialized gradient descent
with learning rate        
    will get stuck in   suboptimal
region  where each point in this region has loss at least
      cid   cid  and this bound is tight 

     

  Empirical study of Gradient Descent for      

In Section   we showed that already for       networks with     Rm and  lter overlaps exhibit more complex behavior than those without overlap  This leaves open
the question of what happens in the general case under the
Gaussian assumption  for various values of      and overlaps  We leave the theoretical analysis of this question to
future work  but here report on empirical  ndings that hint
at what the solution should look like 
We experimented with   range of      and overlap values
 see supplementary material for details of the experimental setup  For each value of      and overlap we sampled   values of    from various uniform input distributions with different supports and several prede ned deterministic values  This resulted in more than   different
sampled    For each such    we ran gradient descent
multiple times  each initialized randomly from   different
   Using the results from these runs  we could estimate
the probability of sampling      that would converge to
the unique global minimum  Viewed differently  this is the
probability mass of the basin of attraction of the global optimum  We note that the uniqueness of the global minimum

follows easily from equating the population risk  Eq    to  
and the full proof is deferred to the supplementary material 
Our results are that across all values of       overlap and
   the probability mass of the basin of attraction is at least
  The practical implication is that multiple restarts of gra 
 
dient descent  in this case   few dozen  will  nd the global
optimum with high probability  We leave formal analysis
of this intriguing fact for future work 

  Discussion
The key theoretical question in deep learning is why it succeeds in  nding good models despite the nonconvexity of
the training loss  It is clear that an answer must characterize speci   settings where deep learning provably works 
Despite considerable recent effort  such   case has not been
shown  Here we provide the  rst analysis of   nonlinear architecture where gradient descent is globally optimal  for  
certain input distribution  namely Gaussian  Thus our speci   characterization is both in terms of architecture  nooverlap networks  single hidden layer  and average pooling  and input distribution  We show that learning in nooverlap architectures is hard  so that some input distribution restriction is necessary for tractability  Note however 
that it is certainly possible that other  nonGaussian  distributions also result in tractability  Some candidates would
be subGaussian and logconcave distributions 
Our derivation addressed the population risk  which for the
Gaussian case can be calculated in closed form  In practice  one minimizes an empirical risk  Our experiments in
Section   suggest that optimizing the empirical risk in the
Gaussian case is tractable  It would be interesting to prove
this formally  It is likely that measure concentration results
can be used to get similar results to those we had for the
population risk       see Mei et al    Xu et al   
for use of such tools 
Convolution layers are among the basic building block of
neural networks  Our work is among the  rst to analyze
optimization for these  The architecture we study is similar
in structure to convolutional networks  in the sense of using parameter tying and pooling  However  most standard
convolutional layers have overlap and use max pooling  In
Section   we provide initial results for the case of overlap  showing there is hope for proving optimality for gradient descent with random restarts  Analyzing max pooling
would be very interesting and is left for future work 
Finally  we note that distribution dependent tractability has
been shown for intersection of halfspaces  Klivans et al 
  which is   nonconvolutional architecture  However 
these results do not use gradient descent  It would be very
interesting to use our techniques to try and understand gradient descent for the population risk in these settings 

Globally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

Acknowledgements
This work was supported by the Blavatnik Computer Science Research Fund  the Intel Collaborative Research Institute for Computational Intelligence  ICRICI  and an ISF
Centers of Excellence grant 

References
AllenZhu  Zeyuan and Hazan  Elad  Variance reduction for faster nonconvex optimization  arXiv preprint
arXiv   

Andoni  Alexandr  Panigrahy  Rina  Valiant  Gregory  and
Zhang  Li  Learning polynomials with neural networks 
In Proceedings of the  th International Conference on
Machine Learning  pp     

Auer  Peter  Herbster  Mark  Warmuth  Manfred    et al 
Exponentially many local minima for single neurons 
Advances in neural information processing systems  pp 
   

Baum  Eric      polynomial time algorithm that learns
two hidden unit nets  Neural Computation   
   

Blum  Avrim   and Rivest  Ronald    Training    node
In Machine learning 

neural network is npcomplete 
From theory to applications  pp    Springer   

Cho  Youngmin and Saul  Lawrence    Kernel methods
In Advances in neural information

for deep learning 
processing systems  pp     

Choromanska  Anna  Henaff  Mikael  Mathieu  Michael 
Arous    erard Ben  and LeCun  Yann  The loss surfaces
of multilayer networks  In AISTATS   

Daniely  Amit  Linial  Nati  and ShalevShwartz  Shai 
From average case complexity to improper learning
In Proceedings of the  th Annual ACM
complexity 
Symposium on Theory of Computing  pp   
ACM   

Duchi  John  Hazan  Elad  and Singer  Yoram  Adaptive
subgradient methods for online learning and stochastic
optimization  Journal of Machine Learning Research 
 Jul   

Garey  Michael    and Johnson  David   

Computers and Intractability    Guide to the Theory of NPCompleteness        Freeman   Co  New York  NY 
USA    ISBN  

Ge  Rong  Huang  Furong  Jin  Chi  and Yuan  Yang  Escaping from saddle pointsonline stochastic gradient for
tensor decomposition  In COLT  pp     

Ge  Rong  Lee  Jason    and Ma  Tengyu  Matrix completion has no spurious local minimum  In Advances in
Neural Information Processing Systems  pp   
 

Goel  Surbhi  Kanade  Varun  Klivans  Adam  and Thaler 
Justin  Reliably learning the ReLU in polynomial time 
arXiv preprint arXiv   

Haeffele  Benjamin   and Vidal  Ren    Global optimality
in tensor factorization  deep learning  and beyond  arXiv
preprint arXiv   

Hardt  Moritz and Ma  Tengyu 

Identity matters in deep

learning  arXiv preprint arXiv   

Hardt  Moritz  Ma  Tengyu  and Recht  Benjamin  Gradient
descent learns linear dynamical systems  arXiv preprint
arXiv   

Hinton  Geoffrey  Deng  Li  Yu  Dong  Dahl  George   
Mohamed  Abdelrahman  Jaitly  Navdeep  Senior  Andrew  Vanhoucke  Vincent  Nguyen  Patrick  Sainath 
Tara    et al  Deep neural networks for acoustic modeling in speech recognition  The shared views of four
research groups  IEEE Signal Processing Magazine   
   

Janzamin  Majid  Sedghi  Hanie  and Anandkumar  Anima 
Beating the perils of nonconvexity  Guaranteed training
of neural networks using tensor methods  arXiv preprint
arXiv   

Kakade  Sham    Kanade  Varun  Shamir  Ohad  and
Kalai  Adam  Ef cient learning of generalized linear
and single index models with isotonic regression  In Advances in Neural Information Processing Systems   pp 
   

Kawaguchi  Kenji  Deep learning without poor local minima  In Advances In Neural Information Processing Systems  pp     

Klivans  Adam  Cryptographic hardness of learning  In Encyclopedia of Algorithms  pp    Springer   

Klivans  Adam    Long  Philip    and Tang  Alex   
Baums algorithm learns intersections of halfspaces with
respect to logconcave distributions  In Approximation 
Randomization  and Combinatorial Optimization  Algorithms and Techniques  pp    Springer   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In Advances in neural information processing
systems  pp     

Globally Optimal Gradient Descent for   ConvNet with Gaussian Inputs

Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht 
Benjamin  and Vinyals  Oriol  Understanding deep
CoRR 
learning requires rethinking generalization 
abs    URL http arxiv org 
abs 

Zhang  Qiuyi  Panigrahy  Rina  Sachdeva  Sushant  and
Rahimi  Ali  Electronproton dynamics in deep learning 
arXiv preprint arXiv   

Lee  Jason    Simchowitz  Max  Jordan  Michael    and
Recht  Benjamin  Gradient descent only converges to
minimizers  In Proceedings of the  th Conference on
Learning Theory  pp     

Lin  Min  Chen  Qiang  and Yan  Shuicheng  Network in

network  arXiv preprint arXiv   

Livni  Roi  ShalevShwartz  Shai  and Shamir  Ohad  On
the computational ef ciency of training neural networks 
In Advances in Neural Information Processing Systems 
pp     

Mei  Song  Bai  Yu  and Montanari  Andrea  The landscape
of empirical risk for nonconvex losses  arXiv preprint
arXiv   

Milletari  Fausto  Navab  Nassir  and Ahmadi  SeyedAhmad  Vnet  Fully convolutional neural networks for
In    Vision
volumetric medical image segmentation 
 DV    Fourth International Conference on  pp 
  IEEE   

Nesterov  Yurii  Introductory lectures on convex optimiza 

tion  pp     

Safran  Itay and Shamir  Ohad  On the quality of the initial
basin in overspeci ed neural networks  In Proceedings
of the  nd International Conference on Machine Learning  pp     

Shamir  Ohad  Distributionspeci   hardness of learning neural networks  arXiv preprint arXiv 
 

Soudry  Daniel and Carmon  Yair  No bad local minima 
Data independent training error guarantees for multilayer neural networks  arXiv preprint arXiv 
 

Wu  Yonghui  Schuster  Mike  Chen  Zhifeng  Le  Quoc   
Norouzi  Mohammad  Macherey  Wolfgang  Krikun 
Maxim  Cao  Yuan  Gao  Qin  Macherey  Klaus 
Klingner  Jeff  Shah  Apurva  Johnson  Melvin  Liu 
Xiaobing  Kaiser  Lukasz  Gouws  Stephan  Kato 
Yoshikiyo  Kudo  Taku  Kazawa  Hideto  Stevens  Keith 
Kurian  George  Patil  Nishant  Wang  Wei  Young  Cliff 
Smith  Jason  Riesa  Jason  Rudnick  Alex  Vinyals 
Oriol  Corrado  Greg  Hughes  Macduff  and Dean  Jeffrey  Google   neural machine translation system  Bridging the gap between human and machine translation 
CoRR  abs   

Xu  Ji  Hsu  Daniel    and Maleki  Arian  Global analysis of
expectation maximization for mixtures of two gaussians 
In Advances in Neural Information Processing Systems 
pp     

