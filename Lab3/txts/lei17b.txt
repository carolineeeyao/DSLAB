Doubly Greedy PrimalDual Coordinate Descent

for Sparse Empirical Risk Minimization

Qi Lei   Ian      Yen   Chaoyuan Wu   Inderjit    Dhillon       Pradeep Ravikumar  

Abstract

We consider the popular problem of sparse empirical risk minimization with linear predictors and
  large number of both features and observations 
With   convexconcave saddle point objective reformulation  we propose   Doubly Greedy PrimalDual Coordinate Descent algorithm that is able to
exploit sparsity in both primal and dual variables 
It enjoys   low cost per iteration and our theoretical analysis shows that it converges linearly
with   good iteration complexity  provided that
the set of primal variables is sparse  We then extend this algorithm further to leverage active sets 
The resulting new algorithm is even faster  and
experiments on largescale Multiclass data sets
show that our algorithm achieves up to   times
speedup on several stateof theart optimization
methods 

  Rd      

min

def
 

 
 

nXi 

                

 

  Introduction
Regularized empirical risk minimization with linear predictors is   key workhorse in machine learning  It has the
following general form 

where ai   Rd is one of the   data samples with   features 
           is   convex loss function of the linear predictor
       for            and     Rd     is   convex
regularization function for the coef cient vector     Rd 
The loss function    assigns   cost to the difference between
the linear predictor       and the associated label bi 
With continuous and discrete bi    captures regression and
classi cation problems respectively  As   popular instance 

 Department of ICES  University of Texas  Austin  Department
of CS  Carnegie Mellon University  Pittsburgh  Department of
CS  University of Texas  Austin  Amazon    Palo Alto  Correspondence to  Qi Lei  leiqi ices utexas edu  Ian      Yen
 eyan cs cmu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

when         max      biz  and         kxk 
   
reduces to the linear SVM  support vector machine  classi 
cation problem  While setting         log    exp biz 
we obtain the logistic regression problem 
We are interested in developing ef cient algorithms for solving this general problem   for the setting where the coef cient vector   is assumed to be sparse  Applications
where such   sparsity is natural include largescale multiclass multilabel classi cation  lowdegree polynomial data
mapping  Chang et al    ngram feature mapping  Sonnenburg   Franc    and random feature kernel machines  Rahimi   Recht    speci cally with   sparsity
constraint on the random features  Yen et al   
Our paper is organized as follows  In Section   we review
existing algorithms to solve the primal  dual as well as
primaldual formulations of the problem   In Section
  we present our Doubly Greedy PrimalDual Coordinate
Descent method for the convexconcave saddle point formulation of the problem   We propose an alternative method
that is more ef cient in practice with the use of incrementally increased active sets in both primal and dual variables 
In Section   we show linear convergence for our proposed
algorithm  and demonstrate the advantages of greedy methods with sparse variables  Finally in Section   we compare
the performance of our method with other stateof theart
methods on some realworld datasets  both with respect to
time and iterations 

  Formulation and related work
Notations  We use   to denote the data matrix  with rows
Ai   ai corresponding to samples  and the column Aj
corresponding to features  We use     to compactly denote
       Throughout the paper      denotes   norm
unless otherwise speci ed 
Assumptions  In order to establish equivalence of the primal  dual problem and the convexconcave saddle point
formulation  we make the following assumptions 

     the regularization for primal variable  is assumed to

be  strongly convex  formally 

              hrg        xi  

for any subgradient rg                   Rd  We

 
 ky   xk 

Doubly Greedy Primaldual Coordinate Descent for Sparse Empirical Risk Minimization

also assume that   has decomposable structure      

   smooth  for        

      Pi gi xi 
     is  
                             
or equivalently    
               

       

is Lipschitz continuous 

                
    

 
 

  Primal  dual and primaldual formulations
Under the assumption of strongly convex regularization  
and smooth loss function     minimizing   is equivalent
to maximizing its dual formulation 

  Rn           

max

   

 

 
 

   

    yi   

nXi 

or the unique solution for the following convexconcave
saddle point problem 

  Rd                 

min

max
  Rn

 
 

  Ax  

 
 

    yi 

nXi 

 
Note that           in   is also smooth with respect to
   since rx                     ai  therefore          
is   smooth with respect to    where   is de ned as
    maxi kaik   Zhang   Xiao    thus de ned the
condition number for the primaldual form as 

def
 

 

  
 

 

We share this de nition in this paper  The commonly used
condition number for the gradient descent of the primal form
is simply                 see  Nesterov   

  Related work
There has been   long line of work over the years to derive
fast solvers for the generic optimization problem   In
Table   we review the time complexity to achieve   error
with respect to either primal  dual or primaldual optimality
for existing methods 
Primal  accelerated  gradient descent  Nesterov   
  require    log   or      log 
if accelerated  iterations to achieve primal error less than  
Note that       is the condition number of   Since each iteration takes   nd  operations  the overall time complexity
is   nd       log   or   nd        log 
if accelerated  Due to the large per iteration cost for
large    stochastic variants that separately optimize each
   have proved more popular in big data settings  Examples include SGD  Bottou    SAG  Schmidt et al 
  SVRG  Johnson   Zhang    SAGA  Defazio et al    MISO  Mairal    and their accel 

erated versions  Xiao   Zhang    The stochastic
scheme of optimizing individual    is similar to updating each dual coordinate individually  Their time complexity thus matches that of dual coordinate descent methods  Hsieh et al    ShalevShwartz   Zhang     
Yang    Qu et al    which enjoy   time complexity
of   nd     log  and   further acceleration step
 ShalevShwartz   Zhang        will improve the
complexity to   nd         log  These stochasinstance of             log  in the accelerated

tic methods have   lower per iteration cost of      but each
step optimizes terms that are much less wellconditioned 
and consequently have   larger iteration complexity  for

case 
With the primaldual formulation   Zhang   Xiao   
introduce   novel stochastic primaldual coordinate method
 SPDC  which with acceleration achieves   time complexity

of   nd         log  matching that of acceler 

ated stochastic dual coordinate descent methods 
However  in practice  SPDC could lead to more expensive
computations for sparse data matrices due to dense updates 
For some special choices of the model   Zhang   Xiao 
  provided ef cient implementation for sparse feature
structures  but the average update time for each coordinate
is still much longer than that of dual coordinate descent 
Moreover  they cannot exploit intermediate sparse iterates
by methods such as shrinking technique  Hsieh et al   
We note moreover that acceleration is not always practically
useful in many realworld settings  other than in extremely
illconditioned situations  In particular    is typically of
the order of pn or   as shown in  Bousquet   Elisseeff 
  Zhang   Xiao    and therefore the conditioning

of          is not necessarily much better than     

    Our experiments also corroborate this  showing that
vanilla dual coordinate descent under reasonable precision
or condition number is not improved upon by SDPC 
Therefore we raise the following question  Does the primaldual formulation have other good properties that could be
leveraged to improve optimization performance 
For instance  some recent work with the primaldual formulation updates stochastically sampled coordinates  Yu et al 
  which has   reduced cost per iteration  provided the
data admits   lowrank factorization or when the proximal
mapping for primal and dual variables are relatively computational expensive  which however may not hold in practice 
so that the the noise caused by this preprocessing could hurt
test performance  Moreover  even when their assumptions
hold  their lowrank matrix factorization step itself may
dominate the total computation time 

Doubly Greedy Primaldual Coordinate Descent for Sparse Empirical Risk Minimization

  Our Contribution
In this paper  we try to address the key question above in
the setting of empirical risk minimization problems with
very large   and    and where the set of primal  and or dual 
variables are assumed to be sparse  We then show that the
primaldual formulation of the problem allows for naturally
leveraging available primal and or dual sparsity 

Table   Basic summary of runningtime complexity of existing
methods and our method  DGPD    is the number of samples    is
the dimension of samples and primal variables    is the condition
number for primaldual coordinate algorithms  For our method    is
the upper bound of sparsity in its primal variables  For DSPDC Yu
et al      is assumed to factorized as          Rn       
Rk    and     maxi kaik 

     

     
Time complexity
  dn      log  
   
  dn       log  
   
          
   
    log  
  dn     
  dn      
  dn      
  kn    dp  
              

if accelerated

    or
    log  
   

    log  
   
    log  
   
    log  
   

GD
AGD
SGD
MISO
SDCA
SVRG
SAG   
SPDC
DSPDC
ours

Extra assumption

 
 
 

 
 

  is factorized

  is sparse

In Table   we review the total time complexity to achieve  
accuracy  We can see that all algorithms that achieve   linear
convergence rate require running time that has   factor nd 
and in particular  none of their convergence rates involve
the sparsity of the primal or dual variables 
There have been some attempts to modify existing primal
or dual coordinate approaches in order to exploit sparsity
of either primal or dual variables  but these do not perform
very well in practice  One popular approach uses   shrinking
heuristic in updating dual coordinates  Hsieh et al   
which however still requires complexity linear to the number
of coordinates   and does not guarantee rate of convergence 
 Nutini et al    consider the idea of searching more important active coordinates to update in each iteration  Their
greedy updates yield an iteration complexity linear in  
instead of    where   and   are the parameters of strong
convexity with respect to    and    norms respectively 
However  with the commonly used    regularization term
       that is used to ensure  strong convexity  the term is
exactly      
    strongly convex  Moreover  in practice 
searching active coordinates causes considerable overhead 
While there have been some strategies proposed to address
this such as  Dhillon et al    that leverages nearest
neighbor search to reduce the searching time  these have
further requirements on the data structure used to store the
data  Overall  it thus remains to more carefully study the
optimization problem in order to facilitate the use of greedy

approaches to exploit primal or dual sparsity 
In this paper  we propose   Doubly Greedy PrimalDual
 DGPD  Coordinate method that greedily selects and updates both primal and dual variables  This method enjoys
an overall low time complexity under   sparsity assumption
on the primal variables 
Theorem   Main result   informal  For the empirical
risk minimization problem   with         regularization 
there exists an algorithm  DGPD  that achieves   error in
    time  where   is an upper bound
              
of the sparsity of the primal variables 

    log  

  The Doubly Greedy PrimalDual  DGPD 

Coordinate Descent method

Coordinatewise updates are most natural when   is separable  as is assumed for instance in the Stochastic PrimalDual
Coordinate method of  Zhang   Xiao    In this paper  to exploit sparsity in primal variables  we additionally
focus on the case where         
 kxk     kxk  With
respect to the loss function   it is assumed to be  
   smooth
and convex  For instance  setting    as the smooth hinge
loss ShalevShwartz   Zhang     

       

 

if biz    
if biz    
otherwise 

 
    biz
   
    biz 
the smoothness parameter      
  For the logit function
        log    exp biz  the smoothness parameter
     
When iterates are sparse  it is more ef cient to perform
greedy coordinate descent  We will provide   brief theoretical vignette of this phenomenon in Section   With this
motivation  our proposed method Doubly Greedy PrimalDual Coordinate Descent  DGPD  greedily selects and updates both the primal and dual variables  one coordinate  
time  Our overall method is detailed in Algorithm  
In Algorithm   we start from all zero vectors        
Rn  and         Rd  where    and    are the
iterates for primal and dual variables  and    and    are
two auxiliary vectors  maintained as     Ax and        
to cache and reduce computations 

Primal Updates 

In each iteration  we  rst compute the
optimal primal variable       for the current          

        arg min

              Eqn 

Then  we only update the coordinate      that will decrease
        the most      
   ek         Eqn 
       arg min
Both two processes cost      operations  Afterwards  we
update the value of   with Eqn    such that        Ax   

               

     

 

Doubly Greedy Primaldual Coordinate Descent for Sparse Empirical Risk Minimization

Algorithm   Doubly Greedy PrimalDual Coordinate method
  Input  Training data     Rn    dual step size    
  Initialize           Rd           Rn       Ax       Rn                 Rd
  for             do
 

Choose greedily the primal coordinate to update 
     
     
    arg min
        
       arg min
          

 
    
 

    

 

 

Update   to maintain the value of Ax 

    
 

    
 

    gk            
        
     

    gk     

 

      gk     

 

  

if         
otherwise 

 

 

 

Choose greedily the dual coordinate to update 

                   

        

 

 Aj

     arg max   

    

    
 
Update   to maintain the value of    

     

 
       arg max
 
 
    
          
      
             

       

 

 

 

 

  

if         
otherwise 

  end for
  Output             

                   

           

    

 Ai   

 

 

 

 

 

 

in      or   nnz Aj  operations  This greedy choice
of      and aggressive update induces   suf cient primal
progress  as shown in Lemma   

Dual Updates  We note that the updates are not exactly
symmetric in the primal   and dual   variables  The updates for the dual variables   do follow along similar lines
as    except that we use the GaussSouthwell rule to select
variables  and introduce   step size   This is motivated
by our convergence analysis  which shows that each primal
update step yields   large descent in the objective  while
each dual update only ascends the dual objective modulo
an error term  This required   subtle analysis to show that
the error terms were canceled out in the end by the progress
made in the primal updates  But to proceed with such an
analysis required the use of   step size in the dual updates 
to balance the progress made in the dual updates  and the
error term it introduced  Note moreover  that we are using
the GaussSouthwell rule to choose the variable to optimize
in the dual variables    while we simply use the coordinate that causes the most function descent in the primal
variables    This is because our choice of step size in the
dual updates required computations that are shared with our
current approach of selecting the optimal primal variable 
This does incur more overhead when compared to the Gauss
Southwell rule however  so that we simply use the latter for
optimizing   
The most signi cant feature in our method is that we select

and update one coordinate in both the primal and dual coordinates greedily  With   simple trick that maintains the
value of     Ax and          Lei et al    we
are able to select and update primal and dual coordinates
in      and      operations respectively  This happens
when computing the value of Ax and      which are the
bottleneck in computing the gradient or updating the variables  An extension to choose and update   batch of primal
and dual coordinate is straightforward  We provide further
discussions on the designing of Algorithm   in Section  
In this paper  we have not incorporated an extrapolation acceleration scheme to our algorithm  As noted earlier 
in practice the condition number   is usually comparable to
   thus adding an extrapolation term that reduces the con 

ditioning from    top   is not necessarily materially

advantageous in real applications  Meanwhile  an extrapolation step usually worsens the stability of the algorithm  and
is not easily combined with incorporating greedy updates 
which is crucial to the leveraging the primal or dual sparsity
structure in this paper  We thus defer an accelerated extension of our algorithm incorporating extrapolation term to
future work 
For Algorithm   each iteration can be seen to have   cost
of          while in Section   we show that the iteration complexity for our method is       
     log 
assuming that the primal variables are ssparse  Therefore the overall time complexity for our algorithm is
      

            log  which is cheaper than the

Doubly Greedy Primaldual Coordinate Descent for Sparse Empirical Risk Minimization

time complexity of even the accelerated SPDC algorithm

       

tioned cases 

   nd log  except for extremely ill condi 

    Practical Extension of DGPD
In real application settings  Algorithm   has some drawbacks  When data is sparse  we still require      and     
operations to update primal and dual variables  Even when
the data is dense  to  nd the greedy coordinate and to update
it requires comparable time complexity  which suggests we
should  nd some ways to eliminate overhead in practice 
To resolve these issues  we introduce the Doubly Greedy
PrimalDual Coordinate method with Active Sets in Algorithm   We make use of what we call active sets  that
contains the newly selected coordinates as well as the current nonzero variables  We construct these active sets Ax
and Ay for both primal and dual variables  Initially  they
are set as empty sets  In each iteration  we recurrently select
coordinates outside the active sets with the GaussSouthwell
rule  and add them to Ax and Ay  We then optimize all the
variables within the active sets  Once   primal dual variable
gets set to   we can drop it from the corresponding active
sets  This practice keeps the active sets Ax and Ay as the
support of primal and dual variables  Notice     xk  is  
when xk is zero  so that the variable selection step for primal
variables can be simpli ed as stated in  
Now the time complexity per iteration becomes  Ax    
 Ay    The sparsity in primal variables is encouraged by the
choice of       regularization  Meanwhile  as shown by
 Yen et al      sparse set of primal variables usually
induces   sparse set of dual variables  Therefore  Ax   
and  Ay    in practice  and the cost per iteration is
sublinear to nd  We present further details in Section  

  Ef cient Implementation for Sparse Data Matrix
Suppose we are given   sparse data matrix   with number
of nonzero elements of each column and each row bounded
by nnzy and nnzx respectively  one can further reduce the
cost for computing   and   from     Ay      Ax 
to   nnzx Ay    nnzy Ax  by storing both  Ai  
   and
   as sparse vectors and computing     and Ax as
 Aj  

    yi  Ax   Xj Ax

 
In our implementation  whenever the active sets Ay  Ax
are expanded  we further maintain   submatrix      which
contains only rows in Ay and columns in Ax  so the primal
and dual updates     only costPi Ay nnz Ai Ax 
This results in each update costing less than the search
steps  and therefore  in practice  one can conduct multiple
rounds of updates     before conducting the search
    which in our experiment speeds up convergence

      Xi Ay

Ajxj 

signi cantly 

  Convergence Analysis
In this section  we introduce the primal gap    and dual gap
   and analyze the convergence rate in terms of their sum 
which we call primal and dual suboptimality        
De nition   For the following convexconcave funcnPn
tion        
       yi  with
    Ax    
its primal form      
  miny         and dual form
  maxx         we de ne the primal gap at itD   
eration   as

def
          
def

def

def

   
 

  the dual gap at iteration   as

                      
   
 
and suboptimality as

def

             
       
   

     

    def

Theorem   Suppose in     is  strongly convex
      regularization  and    is  
   smooth  Let    
maxi    kaik  Then DGPD achieves
   

 

  

     

      

if step size     satis es that

   

   

     

kx                    

 
Suppose kx                   if we choose step size    
        then it requires
 
 

iterations for achieving   primal and dual suboptimality 

    

    log

 
 

 

Proof sketch  The proof analysis is straightforward with
the introduction of primal and dual suboptimality   We
divide the proof into primaldual progress  primal progress 
and dual progress 

  PrimalDual Progress  Lemma   
       

       

   

 

     

 

 

 

         yt      xt  yt 
 
nhAi                  
 
nhAi            

 

 
 

         

    

 This result can be easily connected to traditional convergence
analysis in primal or dual form  Notice       is suf cient requirement that dual gap                    therefore the dual
variable      converges to optimal    with the same convergence
rate 

Doubly Greedy Primaldual Coordinate Descent for Sparse Empirical Risk Minimization

Update the active set     

Algorithm   Doubly Greedy PrimalDual Coordinate method with Active Sets
  Input  Training data     Rn    dual step size    
  Initialize           Rd           Rn    
       
  for             do
 
nhAk           gk            
     
     
    arg min
     
       arg max
    
if         
        
    
if         

          

     

      

 
    
 

    

 
 

 

 

 

 

 

  greedily based on the optimal primal variable       and update   in its active set 

 

Update the active set     

  greedily based on the value of ryL           and update   in its active set 

 hAk         

 
 

       

 

 

 

       arg max
        
        
    
    

     arg max   

      

 

    
 
 
Kick out   variables from active sets 

 

  end for
  Output             

          

nhAi            
           

   

    
        

        

        

   

 

if         
if         

 

 

          
           

   

   

 

 

 

 

This lemma connects the descent in PD suboptimality with
primal progress and dual progress  The third term and
the second terms respectively represent the potential dual
progress if we used the optimal       and the irrelevant part
generated from the difference between       and     

  Primal Progress  Lemma   
                        

 

kx                 

   
 
 

This inequality simply demonstrates function loss from primal update is at least   ratio of primal gap 

  Dual Progress  Lemma   

 

 
nhAi                  
 
nhAi            
 
   
 
   
    kx             
   
  

         

   

 
 

    

 

Finally  we establish the relation between the dual progress
in our algorithm with dual gap and difference between      
and      Now we can prove our main theorem  

For cleaner notation  write      
              
            By combining   and   to   we get 

           

       
   

 

     

       

 

         yt      xt  yt        

 

 
 

 

 

 

   

     
     

bkx             
         yt      xt  yt        
                            
               yt      xt  yt        
          yt           yt 
        
   
          yt           yt 
            
         
Here the second inequality comes from strong convexity of
        The fourth inequality comes from Lemma   
Therefore when       
        or suf ciently          
    we get        
       Since      
            therefore                 

        

 

 

Doubly Greedy Primaldual Coordinate Descent for Sparse Empirical Risk Minimization

 
   

 at  Therefore when                log 
        
  Analysis on greedy methods for sparse iterates
In this section  we give   simple analysis of the greedy variable selection rule showing that when iterate and minimizer
of   generic optimization problem are sparse  its convergence rate is faster than choosing random coordinates  We
de ne the optimization problem in the space of Rn 

  where   is  strongly convex Lsmooth 

     

min
  Rn

 rif       ei    rif           Rn

Under this setting    random coordinate descent method with
step size  

   achieves                     

   where    is the next iterate of   

Under the assumption that the current iterate   and the
optimal    are both ksparse  we thereby conduct greedy
coordinate descent rule                 ei  where     
satis es      ei    mini       ei  With LLipchitz
continuity  we have 

nL         

        ei         

 
 

min

     kkak 

The last two inequalities are obtained by constraining   
to be of the form          and by the convexity of    For
the ksparse    and          is at most  ksparse  and for
  Hereby we obtain 
any  ksparse vector    kak 
             
  
 kx    xk 
              Lk kx    xk 
  
            
             
Therefore               
 kL           and when
       this convergence rate could be much better than
randomized coordinate descent 

  min
  min
 
 

           

 kL

 kL

 

  min

  min
  min

    hrf     eii  
  
 
 
    hrf     eii  
  
 
    eik 
    hrf      xi  
  
 
    xk 
                       
  
 
    xk 
                          
  
 
 kx    xk 
 
             
  
 kx    xk 

 
 

  min
  min
  min

  Experiment
In this section  we implement the DoublyGreedy PrimalDual Coordinate Descent algorithm with Active Sets  and
compare its performance with other stateof theart methods
for  regularized Empirical Risk minimization  including Primal Randomized Coordinate Descent  PrimalRCD 
 Richt rik   Tak      Dual Randomized Coordinate
Descent  DualRCD       SDCA   ShalevShwartz   Zhang 
    and the Stochastic PrimalDual Coordinate Method
 SPDC   Zhang   Xiao   
We conduct experiments on largescale multiclass data sets
with linear and nonlinear feature mappings  as shown in
Table   For Mnist and Aloi we use Random Fourier  RF 
and Random Binning  RB  feature proposed in  Rahimi  
Recht    to approximate effect of RBF Gaussian kernel
and Laplacian Kernel respectively  The features generated
by Random Fourier are dense  while Random Binning gives
highly sparse data 
We give results for         and          
where Figure   shows results for             and
others can be found in Appendix    In the above six  gures 
we compare the running time with objective function  While
in the below  gures  the xaxis is number of iterations  For
the baseline methods  one iteration is one pass over all the
variables  and for our method  it is several   passes over
the active sets  From the  gures  we can see that in all
cases  DGPD has better performance than other methods 
Notice for clear presentation purposes we use logscale for
MnistRB time  AloiRB time and RCVtime  where our
algorithm achieves improvements over others of orders of
magnitude 
The result shows that  by exploiting sparsity in both the
primal and dual  DGPD has much less cost per iteration
and thus is considerably faster in terms of training time 
while by maintaining an active set it does not sacri ce much
in terms of convergence rate  Note since in practice we
perform multiple updates after each search  the convergence
rate  measured in outer iterations  can be sometimes even
better than DualRCD 

  Acknowledgements
     acknowledges the support of NSF via CCF 
IIS  and CCF       acknowledges the
support of ARO via   NF  and NSF via IIS 
  IIS  IIS  and DMS 
and NIH via    GM  as part of the Joint
DMS NIGMS Initiative to Support Research at the Interface
of the Biological and Mathematical Sciences 

Doubly Greedy Primaldual Coordinate Descent for Sparse Empirical Risk Minimization

Table   Data statistics and number of nonzero primal   dual variables from DGPD                
 nzprimal

 nonzero sample

 train samples

 test samples

 classes

Data set
MnistRF
AloiRF
MnistRB
AloiRB

RCV Regions

Sector

 features
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 nzdual
 
 
 
 
 
 

RCVTime

DGPD
DualRCD
PrimalRCD
SPDCdense

   

   

time

SectorTime

DGPD
DualRCD
PrimalRCD
SPDC

time

RCVIteration

DGPD
DualRCD
PrimalRCD
SPDCdense

 

 

 

iter

SectorIteration

DGPD
DualRCD
PrimalRCD
SPDC

   

   

 

 

 

 

MnistRF Time

AloiRF Time

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 

   

   

   

DGPD
DualRCD
PrimalRCD
SPDCdense

DGPD
DualRCD
PrimalRCD
SPDCdense

 

 

 

 

 
time

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
time

MnistRB Time

AloiRB Time

 
 
 
 
 
 
 
 

 

 
 

 
 

 

 
 
 
 
 
 
 

 

 
 
 

   

   

   

   

   

DGPD
DualRCD
PrimalRCD
SPDC

   

time

   

MnistRF Iteration

DGPD
DualRCD
PrimalRCD
SPDCdense

   

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 

   

   

   

 
 
 
 
 
 
 
 

 

 
 

 
 

 

 
 
 
 
 
 
 

 

 
 
 

   

   

   

   

time

AloiRF Iteration

DGPD
DualRCD
PrimalRCD
SPDC

   

   

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 

   

   

   

 
 
 
 
 
 
 
 

 

 
 

 
 

 
 
 

 

 
 
 
 

 

 
 
 

   

 

DGPD
DualRCD
PrimalRCD
SPDCdense

   

   

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 

   

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 

   

   

   

   

   

 
 
 
 
 
 
 
 

 

 
 

 
 

 

 
 
 
 
 
 
 

 

 
 
 

   

 

 

 

 

 

 

 

 

 

 

 

 

iter

MnistRB Iteration

iter

AloiRB Iteration

DGPD
DualRCD
PrimalRCD
SPDC

   

   

   

   

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 

   

   

   

   

   

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 

DGPD
DualRCD
PrimalRCD
SPDC

 
 
 
 
 
 
 
 

 

 
 

 
 

 
 
 

 

 
 
 
 

 

 
 
 

   

   

 

 

 

 

 

 

iter

   

 

 

 
iter

 

 

   

 

 

 

 

 

 

iter

Figure   Relative Objective versus Time  the upper   rows  and versus   iterations  the lower   rows  for            

Doubly Greedy Primaldual Coordinate Descent for Sparse Empirical Risk Minimization

References
Bottou    on  Largescale machine learning with stochastic gradient descent  In Proceedings of COMPSTAT  pp   
Springer   

Bousquet  Olivier and Elisseeff  Andr  Stability and generalization  The Journal of Machine Learning Research   
 

Chang  YinWen  Hsieh  ChoJui  Chang  KaiWei  Ringgaard 
Michael  and Lin  ChihJen  Training and testing lowdegree
polynomial data mappings via linear svm  The Journal of Machine Learning Research     

Chen  Jie  Wu  Lingfei  Audhkhasi  Kartik  Kingsbury  Brian  and
Ramabhadrari  Bhuvana  Ef cient onevs one kernel ridge regression for speech recognition  In Acoustics  Speech and Signal
Processing  ICASSP    IEEE International Conference on 
pp    IEEE   

Richt rik  Peter and Tak    Martin  Iteration complexity of randomized blockcoordinate descent methods for minimizing  
composite function  Mathematical Programming   
   

Schmidt  Mark  Le Roux  Nicolas  and Bach  Francis  Minimizing
 nite sums with the stochastic average gradient  Mathematical
Programming  pp     

ShalevShwartz  Shai and Zhang  Tong  Accelerated minibatch
stochastic dual coordinate ascent  In Advances in Neural Information Processing Systems  pp       

ShalevShwartz  Shai and Zhang  Tong  Stochastic dual coordinate
ascent methods for regularized loss  The Journal of Machine
Learning Research       

ShalevShwartz  Shai and Zhang  Tong  Accelerated proximal
stochastic dual coordinate ascent for regularized loss minimization  Mathematical Programming     

Defazio  Aaron  Bach  Francis  and LacosteJulien  Simon  Saga 
  fast incremental gradient method with support for nonstrongly convex composite objectives  In Advances in Neural
Information Processing Systems  pp     

Sonnenburg    ren and Franc  Vojtech  Cof      computational
framework for linear svms 
In Proceedings of the  th International Conference on Machine Learning  ICML  pp 
   

Wu  Lingfei  Yen  Ian EH  Chen  Jie  and Yan  Rui  Revisiting
random binning features  Fast convergence and strong parallelizability  In Proceedings of the  nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 
pp    ACM   

Xiao  Lin and Zhang  Tong    proximal stochastic gradient method
with progressive variance reduction  SIAM Journal on Optimization     

Yang  Tianbao  Trading computation for communication  Distributed stochastic dual coordinate ascent  In Advances in Neural Information Processing Systems  pp     

Yen  Ian EH  Huang  Xiangru  Zhong  Kai  Ravikumar  Pradeep 
and Dhillon  Inderjit    Pdsparse    primal and dual sparse
approach to extreme multiclass and multilabel classi cation  In
Proceedings of the  nd International Conference on Machine
Learning   

Yen  Ian EnHsu  Lin  TingWei  Lin  ShouDe  Ravikumar 
Pradeep    and Dhillon  Inderjit    Sparse random feature
algorithm as coordinate descent in hilbert space  In Advances in
Neural Information Processing Systems  pp     

Yu  Adams Wei  Lin  Qihang  and Yang  Tianbao  Doubly stochastic primaldual coordinate method for empirical risk minimization and bilinear saddlepoint problem  arXiv preprint
arXiv   

Zhang  Yuchen and Xiao  Lin  Stochastic primaldual coordinate
method for regularized empirical risk minimization  arXiv
preprint arXiv   

Dhillon  Inderjit    Ravikumar  Pradeep    and Tewari  Ambuj 
Nearest neighbor based greedy coordinate descent  In Advances
in Neural Information Processing Systems  pp   
 

Hsieh  ChoJui  Chang  KaiWei  Lin  ChihJen  Keerthi    Sathiya 
and Sundararajan  Sellamanickam    dual coordinate descent
method for largescale linear svm  In Proceedings of the  th
international conference on Machine learning  pp   
ACM   

Johnson  Rie and Zhang  Tong  Accelerating stochastic gradient
descent using predictive variance reduction  In Advances in
Neural Information Processing Systems  pp     

Lei  Qi  Zhong  Kai  and Dhillon  Inderjit    Coordinatewise
power method  In Advances in Neural Information Processing
Systems  pp     

Mairal  Julien  Incremental majorizationminimization optimization with application to largescale machine learning  SIAM
Journal on Optimization     

Nesterov     Introductory Lectures on Convex Optimization   

Basic Course  Springer Science   Business Media   

Nesterov  Yu  Smooth minimization of nonsmooth functions 

Mathematical programming     

Nutini  Julie  Schmidt  Mark  Laradji  Issam    Friedlander 
Michael  and Koepke  Hoyt  Coordinate descent converges
faster with the gausssouthwell rule than random selection 
arXiv preprint arXiv   

Qu  Zheng  Richt rik  Peter  and Zhang  Tong  Randomized
dual coordinate ascent with arbitrary sampling  arXiv preprint
arXiv   

Rahimi  Ali and Recht  Benjamin  Random features for largescale
kernel machines  In Advances in neural information processing
systems  pp     

