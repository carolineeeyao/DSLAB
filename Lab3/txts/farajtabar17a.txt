Fake News Mitigation via Point Process Based Intervention

Mehrdad Farajtabar   Jiachen Yang   Xiaojing Ye   Huan Xu   Rakshit Trivedi   Elias Khalil   Shuang Li  

Le Song   Hongyuan Zha  

Abstract

We propose the  rst multistage intervention
framework that tackles fake news in social networks by combining reinforcement learning with
  point process network activity model  The
spread of fake news and mitigation events within
the network is modeled by   multivariate Hawkes
process with additional exogenous control terms 
By choosing   feature representation of states 
de ning mitigation actions and constructing reward functions to measure the effectiveness of
mitigation activities  we map the problem of fake
news mitigation into the reinforcement learning framework  We develop   policy iteration
method unique to the multivariate networked
point process  with the goal of optimizing the
actions for maximal total reward under budget
constraints  Our method shows promising performance in realtime intervention experiments
on   Twitter network to mitigate   surrogate fake
news campaign  and outperforms alternatives on
synthetic datasets 

  Introduction
The recent proliferation of malicious fake news in social
media has been   source of widespread concern  Given
that more than   of      adults turn to social media for
news  with   doing so often  fake news can have potential realworld consequences on   large scale  Gottfried  
Shearer    For example  within the  nal three months
of the        presidential election  news stories that favored either of the two nominees later proved to be fake 
were shared over   million times on Facebook alone  and
over half of those who recalled seeing fake news stories
believed them  Allcott   Gentzkow    An analysis
by Buzzfeed News shows that the top   false election stories from hoax websites generated nearly   million more
user engagement activities on Facebook than the top   sto 

 School of Computational Science and Engineering  Geor 
 Department of Mathematics and Statistics  Georgia
gia Tech 
 School of Industrial and Systems EngineerState University 
ing  Georgia Tech  Correspondence to  Mehrdad Farajtabar
 mehrdad gatech edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ries from reputable major news outlets  Silverman   
Therefore  there is an urgent call to develop effective rectifying strategies to mitigate the impact of fake news 
Policies to counter fake news can be categorized by the
level of manual oversight and the aggressiveness of action
required  Aggressively acting on fake news has various
drawbacks  For example  Facebook   strategy allows users
to report stories as potential fake news  sends these stories
to factchecking organizations  and  ags them as disputed
in users  newsfeed  Mosseri    Such direct action on
the offending news requires   high degree of human oversight  which can be costly and slow and also may violate
civil rights  The reportand ag mechanism is also open
to abuse by adversaries who maliciously report real news 
Given these disadvantages  we consider an alternative strategy  optimizing the performance of real news propagation
over the network  Intuitively  we want people   exposure to
real news to match their exposure to fake news 
We face several key modeling and computational issues 
For example  how to quantify the uncertainty of user activities and news propagation within the network  How
to measure the effect of mitigation incentives and activities  Is it possible to steer the spontaneous user mitigation activities by an intervention strategy  To address these
questions  we model the temporal randomness of fake news
and mitigation events  valid news  as multivariate point
processes with self and mutual excitations  in which the
control incentivizes more spontaneous mitigation events
by contributing to the exogenous activity of campaigner
nodes  The in uence of fake news and mitigation activities
is quanti ed using event exposure counts       the number
of times that   user is exposed to fake or real news posts
from other users whom she follows 
Our key contributions are as follows  We present the  rst
formulation of fake news mitigation as the problem of optimal point process intervention in   network  The goal is
to optimize the activity policy of   set of campaigner nodes
to mitigate   fake news process stemming from another set
of nodes  It creates opportunities for designing   variety
of objectives       minimizing the number of users who
see fake news but were not reached by real news  We give
the  rst derivation of secondorder statistics of random exposure counts in the nonstationary case  which is essential in policy evaluation and improvement  By de ning  

Fake News Mitigation via Point Process Based Intervention

Figure   The framework of point process based intervention for countering fake news    Of ine learning of value function approximation weight vector using LSTD from transition samples generated from model    Realtime intervention loop that uses feature
representation of network state to choose optimal exogenous incentive for mitigator nodes 
state space for the network  formulating actions as exogenous intensity  and de ning reward functions  we map the
fake news mitigation problem to an optimal policy problem in   Markov decision process  MDP  which is solved
by modelbased leastsquares temporal difference learning
 LSTD  speci   to the context of point processes  Furthermore  to the best of our knowledge  we are the  rst to conduct   realtime point process intervention experiment 
Related work  The emergence of social media as   prominent news source in the past few years raises concomitant
concerns about the quality  truthfulness  and credibility of
information presented  Mitra et al    To reduce the
amount of laborintensive manual factchecking  there have
been research efforts devoted to building classi ers to detect factuality of information  predicting credibility level of
posts  and detecting controversial information from inquiry
phrases  Mitra et al    Zeng et al    Zhao et al 
  These works mainly focused on extracting linguistic features from texts to determine the credibility of news
and posts  Our focus in this paper  however  is to design
an incentive strategy so that users can spontaneously take
action to mitigate   realworld fake news epidemic 
Steering user activities by adding external incentives to the
exogenous intensity of Hawkes processes was  rst considered in  Farajtabar et al    In  Farajtabar et al   
  multistage campaigning method to optimally distribute
incentive resources based on dynamic programming was
developed 
In these previous works  objective functions
were designed using expected values of exposure counts
rather than the stochastic exposure process  which may reduce the accuracy of solutions  Furthermore  it faced the
demanding problem of computing the costto go using the
Hawkes model  while we address this using linear function
approximation  For stationary Hawkes processes  second
order statistics was derived in  Bacry   Muzy       

however  it is essential to compute both  rst and second
order statistics for Hawkes processes in the nonstationary
stages due to time sensitivity of the fake news mitigation
task  and we derive it for the  rst time in this paper  Recent
work has also applied methods in stochastic differential
equations to the context of point processes  to  nd the best
intensity for information guiding  Wang et al    and
achieving highest visibility  Zarezade et al    While
these works consider networks with only   single process 
our work focuses on optimizing   mitigation process with
respect to   second competing process  Finally  it   notable
that our approach is related to but much more general than
the in uence maximization problems  Kempe et al   
Bharathi et al    as it allows recurrent activity in social
networks  in contrast to binary infection states    variety of
objectives  not only maximization  and budget constraints 
Reinforcement learning tackles the problem of  nding
good policies for actions to take in MDP where exact solutions are intractable  either due to size or lack of complete knowledge  Largescale policy evaluation and iteration problems can be tackled by function approximation 
which reduces the solution dimension using feature vector
basis  Sutton   Barto    By adding control terms to  
multivariate Hawkes process model of random network activities  fake news mitigation can be formulated as   policy
optimization problem in an MDP  To address the randomness of Hawkes processes  batch reinforcement learning using samples collected from the trajectory of    xed behavior policy can be applied  Antos et al    In particular  linear Least Squares Temporal Difference  LSTD  uses
  batch of samples to learn   linear approximation of the
value function under   policy with provable convergence
 Bradtke   Barto    This policy evaluation step alternates with   modelbased policy improvement step in  
policy iteration to arrive at successively improved policies 

     fake news Mitigator   AdamBeckyCharlesDianaMitigator   Twitter NetworktFake newsM tttt   AdamBeckyA     follows BModel  ij LSTDSamplesx   xSw nL   nLuikt     Realtime experiment loopuk argmaxu         Fake News Mitigation via Point Process Based Intervention

  Preliminaries and Problem Statement
Multivariate Hawkes processes  Hawkes process is
  doubly stochastic point process with selfexcitations 
meaning that past events increase the chance of arrivals of
new events  Hawkes    and has been extensively used
to model activities in social networks  Farajtabar et al 
  Linderman   Adams    He et al    Rizoiu
et al  Lee et al    Let    be the time of the  th event 
then the Hawkes process can be represented by the count 

ing process        Pt           that tracks the number
of events up to time    where      is the standard Heaviside function such that          if       and     if
      The conditional intensity function of   point process
is de ned as the probability of observing an event in an in 
 nitesimal window given the history  For Hawkes process

it is given by          Pt           Here        is

the exogenous  base  intensity and     is the Hawkes kernel that describes how fast the excitement of   past event
decays  In this paper  we employ the standard  stationary 
exponential Hawkes kernel                th    with
    In an ndimensional multivariate Hawkes process  MHP  there are   such processes              Nn   
that can also mutually excite one another  and the conditional intensity                          Rn
  is
given by             
          dN     Here         
              Nn      Nn
                        Rn
 
and    ij    ij       ije th    We let     
denote the  ltration of       generated by the  algebra
of history               of this point process  where
                  is the identity  node  of the  th event 
Network activities  We model the activities of both fake
news and mitigation events as MHP in the network  Basically  MHP is   networked point process model with
dependent dimensions  nodes  and can capture the underlying dynamics of networks and activities  Blundell
et al    Xu et al    Guo et al    De ne
    where Fi    counts
the number of times user   shares   piece of news from
the fake campaign up to time    Similarly  de ne        
  for the mitigation process 
Correspondingly  we have   intensity functions          
     
  
and two sets of exogenous intensities    and     
Goal  Given that both       and       are modeled by the
Hawkes processes  our goal is to  nd the optimal mitigation campaign by imposing interventions to users such that
the mitigation effect  rigorously de ned in sec    can be
maximized or equivalently the fake news be recti ed under
budget constraints  To this end  we measure the in uence
of fake news and mitigation activities using event exposures  describe the mechanism of mitigation interventions 
and quantify the effect of interventions mathematically 

                      Fn      Nn
              Mn      Nn

      and            

               

               

Event exposure  Event exposure is   quantitative measure
of campaign in uence  and is represented as   counting

process                      En    Here  Ei    records

the number of times user   is exposed  she or one of her
neighbors performs an activity  to   campaign       by time
   Let   be the adjacency matrix of the user network      
bij     if user   follows user    Assume bii     for all
   Then the exposure process is given by        BN    
We de ne        BF     and        BM     as the fake
news and mitigation processes  respectively 
Intervention  Suppose we can perform intervention by
incentivizing   subset of users in the kth stage during
time        for                 For simplicity we consider uniform time duration              for all   
since generalization to nonuniform time durations is trivial 
In order to steer the mitigation activities to counter
the fake news  criteria given below  at these stages  we imi     to the expose an additional constant intervention uk
ogenous intensity    during time        for each stage
                The mitigation activity intensity at the kth stage is              uk     
          dM     for
           Note that the intervention itself exhibits
  stochastic nature  adding uk
to    is equivalent to ini
centivizing user   to increase her activity rate but it is still
uncertain when she will perform an activity  which appropriately mimics the randomness in the real world 
Reward function  For each stage    xk  de ned later  is
the state of the whole MDP that encodes all the information
from previous stages and uk is the current control imposed
at this stage  Let Mk
dMj   
be the number of times user   is exposed to the mitigation campaign by time            within stage    then
the goal is to steer the expected total number of exposure
      xk  uk  using uk       the sum of reward functions
Mk
  xk  uk   rigorously de ned in sec    is maximized 
Problem statement  By observing the counting process in previous stages  summarized in   sequence of xk 
and taking the future uncertainty into account  the control problem is to design   policy   such that the controls uk    xk  can maximize the total discounted ob 

      xk  uk    Pj bijR  

jective         kRk  where         is the discount

rate and Rk is the observed reward at stage   
In addition  we may have constraints on the amount of control 
such as   budget constraint on the sum of all interventions
to users at each stage  or   cap over the amount of intensity   user can handle    feasible set or an action space
over which we  nd the best intervention is represented as
Uk      Rn   ck   Ck               Here  ck
  is the
price per unit increase of exogenous intensity of user   and
Ck      is the total budget at stage    Also    
  is the cap
on the amount of activities of the user   

  

Fake News Mitigation via Point Process Based Intervention

            Xk 

 kRk    

 

  Proposed Method
In this section  we present the formulation of reward functions in terms of event exposures of fake news and mitigation activities  Then we derive the key statistics of the
MHP required for reward function evaluation  followed by
the policy iteration scheme to  nd the optimal intervention 

  Fake news mitigation
As we discussed above  the total reward of policy   is de 
 ned by the value function

for the initial state    of fake and mitigation processes 
where the observed reward   quanti es the effect of mitigation activities       in each stage and         is the
discount rate  In this paper  we consider two types of reward functions        
  Correlation Maximization  One possible way is to require correlation between mitigation exposures and fake
news exposures  people exposed more to fake news should
also be exposed more to the true news  to counter the fake
news campaign  Therefore  we can form the reward function   in stage   as follows 

  xk  uk   

 
nMk    xk  uk        xk  uk 
  Difference Minimization  Suppose the goal is to minimize the number of unmitigated fake news events  then we
can form   reward function   in stage   as the least squares
of unmitigated numbers 
  xk  uk     
These are two sample realizations of the MHPMDP based
intervention one can formulate  among many others  To
solve the policy optimization problem argmax       
for     de ned in   we need to evaluate the value function     for any given policy   which requires the  rst
and second order statistics  moments  of any multivariate
Hawkes processes       as we derive next 

   Mk    xk  uk          xk  uk 

 

  Second order statistics of nonstationary MHP
For an ndim MHP       with standard exponential kernel
    the following proposition provides closedform solution of the mean intensity            for both constant
and timevarying exogenous intensity    
Proposition    Theorem    Farajtabar et al     
Let       be an ndimensional MHP de ned in sec   
with exogenous intensity     and Hawkes kernel      
Ae th    then the mean intensity     is given by

                                          

 

       ds     

Let          
      ds be the compensator of       then
by DoobMeyer   decomposition theorem             is
  zero mean martingale  This implies that the  rst order
statistics         can be obtained by                 
     
      ds      
      ds using eq   
To evaluate the reward function   de ned previously 
we need to derive second order statistics of multivariate
Hawkes process       in its nonstationary stage  The following theorem states the key ingredients for the second
order statistics  The proof is provided in the appendix 
Theorem   Let       be an ndim MHP with exogenous
intensity   and Hawkes kernel   de ned in sec    then the
second order statistics of       for           is given by
EhdN     dN                  dt dt 

          dt dt          dt dt 

 

      uk 

where            is given in         diag     
is diagonal  and   is the unique solution of
                                            
 
Moreover                       for all          
Based on Theorem   we can compute second order statistics such as   Ni   Nj    for all      and          
  State Representation
Hawkes process is nonMarkovian and one needs complete knowledge of the history to characterize the entire
process  However  when the standard exponential kernel
         Ae              is employed  the effect of
history up to time    on the future       can be cleverly
summarized by one scalar per dimension  Simma   Jordan    Farajtabar et al    For            de ne
       
      by convention 
yk
then the intensity due to events of all previous   stages can
be written as     
In
other words  yk is suf cient to encode the information of
activities in the past   stages that are relevant to future 
Note that we have two separate yk
  to track the dynamics of both mitigation and fake processes 
Also  in order to tackle objectives over multiple stages  we
add aggregated number of events at   previous     time
intervals over all dimensions  De ne   vector zk   RnL
                 
where zk
dNi    for          
and            In other words  zk
       records the number of events of ith dimension in the lth interval of length
   prior to time     For example  choosing        and
setting       means that events from the two most recent
stages are counted  Similarly  we have two separate zk
  and
  corresponding to the two processes  Now  the state veczk
tor xk     nL   is the concatenation of the above four
vectors xk    yk

  Ae      dN       yke     

         and yi

  and yk

      

 

    yk

    zk

    zk

   

Fake News Mitigation via Point Process Based Intervention

Algorithm   LSTD policy iteration in point processes

Algorithm   Realtime fake news mitigation

Input  set of samples    feature    discount  
repeat

Initialize        and       
for each state      do
                          
                

end for
          
for each state      do
      argmax
end for
until          
return   

 

                          

  Least Squares Temporal Difference
The optimal value function satis es the Bellman equation 

                             

 

     

PD

where    is the next state after taking action based on
policy   at state    Least squares temporal difference
learning  LSTD  is   sampleef cient procedure for policy
evaluation  which subsequently facilitates policy improvement  The value function is approximated by         
        where    is the dth feature of state  
and   
  is its coef cient for policy   This can be compactly represented as                 where       
                    The following presents our choice
of features  and explains the policy evaluation and improvement steps inspired by LSTD   Sutton   Barto   
Features  The number of events in   few recent consecutive intervals of point processes have been used as   reliable
feature to parameterize point processes  Parikh et al   
Qin   Shelton    Lian et al    Following their
work we take   prior intervals of length    for each dimension of the fake news process and record the number of
events in that period as one feature    
      
for           and            This will count for nL
features  Similarly we take nL features from the mitigation process  Finally  we add   last feature   
 nL      as
the bias term  Therefore        zk
      and the feature
space has dimension      nL    
Policy Evaluation  Substituting the approximation into the
Bellman equation  we have 

         zk

    zk

                               

 

To  nd the best    of    we have to consider all possible
   however  since the state space is in nitedimensional 
enumerating all states is impossible and we utilize   set  
of samples                 xS 

Input  network    learned    feature    discount  
repeat

Observe state   of the network activities
    argmaxa                          
Add   to base exogenous intensity   and generate mitigation event times  ti  using point process model
Create posts at times  ti  using campaigner accounts

until end of campaign

              

Let   xs         RD                   RD  and
        xs   xs       Then de ne matrices of
  
current features                           RS   and next
features                            RS    the rewards
     RS  and the sample value funcr       
tions as                        xS    RS  Appendix   presents how we leverage the  rst and second
order statistics of Hawkes process to  nd           and
        Given the above de nition  the Bellman optimality of eq    can be written in matrix format 
                             

 
where     is the Bellman optimality operator    way to
 nd   good estimate is to force the approximate value function to be    xed point of the optimality equation under
the Bellman operator                      Lagoudakis  
Parr    For that  the  xed point has to lie in the
space of approximate value functions  spanned by the basis functions        lies in that space by de nition  but
        may have an orthogonal component and must be projected  This is achieved by the orthogonal projection operator           Therefore the approximate value
function     must be invariant under one application of the
Bellman operator     followed by orthogonal projection 

                     

 
By substituting the linear approximation          into
the above equation and some manipulations  we get  
      linear systems of equations         where
               and           and whose solution
is the  tted coef cients    It has been shown that the estimated    converges to the best    as the available number
of samples tends to in nity  Bradtke   Barto    Appendix   presents   detailed derivation 
Policy Improvement  The second part of the algorithm
implements policy improvement       getting an improved
policy   via onestep lookahead as follows 

      argmax

 

                  

 

LSTD  alternates between the policy improvement and
policy evaluation iteratively until    converges  Bradtke
  Barto    Alg    summarizes this procedure 

Fake News Mitigation via Point Process Based Intervention

theory

empirical

theory

empirical

theory

empirical

theory

empirical

 

 
 
 
 
 

 
 
 
 
 
 

 

 
 
 

 

 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 
 

 

 
 
 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 
 

 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

time

time

time

time

Figure   Empirical and theoretical second order moments of   Hawkes process    dNi    dNj    for   random pairs        and       
and varying   from   to  

LSTD in Hawkes context  LSTD is particularly suitable
to the problem we are interested in 
It learns the value
function       and as such  policy improvement can be
challenging without knowing the model  Because of this 
methods that aim to learn the Qfunction         such as
LSPI  Lagoudakis   Parr    are widely applied  The
downside of Qfunction based methods is that they typically require more samples than learning the value function  Yet  in our setup  learning the value function is suf 
cient  by writing the actionvalue function as          
                  and observing that the learned model
of the multivariate Hawkes process enables analytical computation of the expectation  see Appendix   for details 

       

  Xl 

ln izk 
  

            

nL ln izk 

       

wiE zk

       wnL iE zk

         

 nL 

 

 

nXi 
nXi 

           

 
nE zk
 
nE zk
          zk

 

             
 

 
nE zk

   

   

          zk
      zk

 
nE zk
     
  difference

 

  correlation

      zk
   

We require much fewer samples to learn       compared
to learning an approximate         and in particular
compared to LSPI we avoid explicitly discretizing the continuous action space from which the action   is chosen 
the policy improvement step
We further remark that
 nds the optimal action   at any state   by computing
argmaxu                   where the action   to be
optimized appears in the calculation of both the expected
current reward and the expected value at the next state  This
optimization problem is convex under our choice of reward
functions and the form of the Hawkes conditional intensity 
After learning the optimal policy  implicit by    of the
linearlyapproximated value function  we start at the realtime intervention part  By observing the state we  nd the
optimal intervention intensity by simply solving eq   
Alg    summarizes the realtime mitigation procedure 

  Experiments
We evaluate our fake news mitigation framework by both
simulated and realtime realworld experiments and show
our approach  Leastsquares Temporal Difference  LTD 
signi cantly outperforms several stateof theart methods
and alternatives  CEC  an approximate dynamic programming  OPL  an open loop optimization  CLS    centrality
based measure  EXP  an exposure based centrality measure  and RND  the random policy  Their details are given
in appendix    Before explaining the intervention results
we verify the theoretical second order statistics in Fig   
whose details can be found in appendix    Furthermore  we
examine convergence properties and representative power
of linear features in Fig    with the details postponed to
appendix   due to space limitations 
  Synthetic Experiments
Setup  For all except the experiment over network size 
the networks were generated synthetically with      
nodes  Endogenous intensity coef cients were set as aij  
     To mimic real world networks  sparsity was set to
       each edge was kept with probability   The in 
 uence matrix was scaled appropriately such that the spectral radius is   random number smaller than one to ensure
the stability of the process  The Hawkes kernel parameter
was set to       which means loosing roughly     of in 
 uence after   unit of time  minutes  hours  etc  Both fake
news and mitigation processes obey these network settings 
Among   nodes  we assume   nodes create fake news and
another   nodes can be incentivized  via the exogenous
intensity  to spread true news  Each stage has length of
       The discount factor was set to       For determining features  we set       and we choose        for
simplicity  The upper bound for the intervention intensity
was chosen by           The price of each person
was ck
      and the total budget at stage   was randomly
generated as Ck               randomly sampled states were used for the LSTD algorithm  To evaluate
  policy  learnt by an algorithm  we simulated the network
under that policy   times and take the discounted total reward averaged over these   runs as an empirical valuation
of the policy  Furthermore  each single run was simulated
for   consecutive stages  from the eleventh stage onward 
the objectives contribute   of the total reward and can
be safely discarded  For all experiments  the above settings
are assumed unless it is explicitly mentioned otherwise 

Fake News Mitigation via Point Process Based Intervention

LTD CEC OPL EXP CLS

LTD CEC OPL EXP CLS

LTD CEC OPL EXP CLS

LTD CEC OPL EXP CLS

 

 

 
 
 
 
 
 
 

 
 
 
 

 
 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 
 
 

 
 
 
 

 

 

 

 
 

 

 

 

 

 
 
 
 
 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

 

network size
   

campaign size
   

sparsity
   

stage length
   

Figure   Performance improvement of different methods over the random policy on synthetic networks for correlation maximization

LTD CEC OPL EXP CLS

LTD CEC OPL EXP CLS

LTD CEC OPL EXP CLS

LTD CEC OPL EXP CLS

 

 
 
 
 
 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 
 
 
 
 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

 

 
 
 
 
 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

 

network size
   

campaign size
   

sparsity
   

stage length
   

Figure   Performance improvement of different methods over the random policy on synthetic networks for distance minimization

 

 

 

 

 
 
 
 

 

 
 
 
 
 
 
 
 

 

 

 

 
  samples

 

    Correlation

 

 

 

 

 
 
 
 

 

 
 
 
 
 
 
 
 

 

 

 

 

 
  samples
    Difference

Figure   Convergence of linear approximated value function

Intervention results  Fig    demonstrates the performance
of different methods  Performance of   policy is quanti 
 ed as the ratio of the total reward achieved by running the
policy  over the total reward achieved by the random policy  RND  This allows us to compare the effectiveness of
the algorithms over   variety of settings  All the results
reported are averages over   runs with random networks
generated according to the above setup  Overall  it is clear
that LTD is almost consistently the best  It improves over
the random policy by roughly   percent  CEC is the second best and shows the effectiveness of multistage and
closed loop intervention  This validates our intuition that
although CEC computes the reward from both fake news
and mitigation processes  the lack of explicit features corresponding to previous events in its value function prevents
it from learning the reason for the reward  Roughly  OPL
is the third best algorithm  due to its negligence of the state
and the actual events that occurred  Next  comes the EXP
algorithm followed by the CLS  The poor performance of
these  compared to others  shows that structural properties
are not suf cient to tackle the fake news mitigation problem  EXP is roughly better than CEC because it heuristically takes into account the fake news exposure 
Fig     shows the performance with respect to increasing

network size  The difference between alternative methods
and the gap between LTD and others increase with the network size  Furthermore  the performance of all methods
show an increase over random policy when the problem
size gets larger  This illustrates the fact that ef cient distribution of budget matters more when confronted with problems of increasing complexity and size 
Fig     shows the performance with respect to increasing the mitigation campaign size  Larger campaigns imply
greater  exibility of intervention  which can be exploited
by clever algorithms to achieve higher performance 
Fig     shows the performance with respect to increasing
sparsity of the network  Interestingly  the performance of
all the algorithms move towards to the random policy as
the network becomes denser  This can be understood by
considering   complete graph  so that no matter how and to
whom we distribute the mitigation budget  all the nodes are
exposed to the mitigation campaign almost equally  However  since real social networks are usually sparse  the effectiveness of the proposed method stands out 
Finally  Fig     shows the performance with respect to the
length of an stage  Longer stage lengths increase the potential for   good policy to attain higher reward than   random policy  and this is re ected by the sharp increase and
larger performance gap between LTD and others for longer
lengths  We observe the same patterns for the distance minimization in Fig    problem and avoid repeating them 
  Real experiments
In this section we explain our realtime intervention results 
To the best of our knowledge  we are the  rst to employ
  realtime experiment to evaluate   point process based
social network intervention strategy 

Fake News Mitigation via Point Process Based Intervention

 
 

 
 

 

 
 
 
 
 
 
 

 
 
 
 
 
 
 

 

 

 

 

LTD

CEC

EXP RND

    Correlation

 

 
 
 
 
 
 

 
 
 

 

 

 
 
 
 
 
 
 

 

 

 

 

LTD

CEC

EXP RND

    Difference

Figure   Results of fake news mitigation on Twitter network

Setup  Using  ve Twitter accounts  each of which made
 ve posts on machine learning topics at random times per
day for   span of two months  Nov Dec    we accumulated   network of   real users with   directed edges in total  We used this historical data to learn
the network parameters  ij      using maximum likelihood  similar to related work  Zhou et al    Farajtabar et al    with one hour as the time resolution
and the kernel decay parameter   set to   As illustrated in Fig  the optimal policy was learned using LSTD
and policy improvement  Then the realtime experiment
starts  Two of the accounts  interpreted as the source of
fake news  continued to behave using the same randomized policy as they did in the data collection stage  while
the posting times of the other three accounts were generated from              produced by our LTD strategy or
  competitor strategy  Each policy was run for   stages of
length   hours  Therefore             Since both
fake news and mitigation accounts were tweeting random
posts on machine learning  we assume negligible bias in
the content that can confound the performance  At the end
of each stage  all retweets by users within the network 
of the posts made during the two most recent stages were
used to construct the feature vector and compute the value
function  which was used to  nd the optimal intervention
for the next stage  The methods CEC and OPL belong to
the same category  and it has been shown that CEC outperforms OPL in  Farajtabar et al    Furthermore  EXP
and CLS also belong to similar families and our synthetic
experiments con rm the superiority of the former  So  to
save time in real interventions  we only test CEC from the
 rst and EXP from the second pair  and compare them with
the random policy  RND  and with our algorithm  LTD 
Realtime intervention results  Fig    shows the performance of our results compared to competitors  The results
show that our approach outperforms the other three baselines by   reasonable margin  As expected CEC is the second best algorithm with   margin of   for the correlation
maximization objective  It translates to increase in amount
of correlation equal to   which is   noticeable amount 
Furthermore  in the difference minimization task  our approach reached around   in difference  This means that we
decreased the difference in exposure to the two processes to
less   per user  which is considerable improvement  For
both tasks  LTD made more mitigation posts over all day 

 
 

 
 

 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 
 

 
 

 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

LTD CEC OPL EXP CLS RND

LTD CEC OPL EXP CLS RND

    Correlation

    Difference

Figure   Rank correlation for prediction

time phases than it did over all nighttime phases  whereas
the competitor strategies did the opposite  This could be
  reason for its better performance  One surprising fact is
that the number of retweets by users outside the network 
which was not used for our features  can exceed the number of retweets by users within the network  This is because
the  hashtag  feature on Twitter allows posts to be seen by
  much larger set of users  who do not necessarily follow
the source accounts  In addition to retweets  users can also
 like    post  indicating that they were exposed to fake or
real news  while we measured this  we did not include it in
the reward  Future experiments can use these two observations to widen the experimental scope and more accurately
measure the effectiveness of   mitigation strategy  Despite
having these limitations  our experiment serves as   proofof concept for the applicability of point process based intervention in networks  and to the best of our knowledge 
is the  rst to verify the superiority of   method in   realtime  realworld intervention setting 
Prediction evaluation results  The previous part describes
the more interesting evaluation scheme of realtime intervention in   social media platform  In this part  we used
historical real data to mimic this procedure  We extracted
  full  stage trajectory of events from the  month random policy historical data  For any of these   pairs  the
methods were evaluated according to how well they predict
the relative ordering among these   trajectories  with respect to the objective function  To evaluate each method 
we created   sorted list of these   trajectories according
to increasing objective  and created   second list sorted
by increasing closeness to the intervention method  This
closeness is the mean squared error between the prescribed
intervention and actual intensity  which we inferred using
maximum likelihood  Then  by computing the rank correlation of the two sorted lists  and repeating for each of the
 ve methods  we can  nd out how well they perform on the
prediction task    better predictor is expected to be   better
mitigation strategy  Fig    shows the performance 
  short discussion is presented in appendix   
Acknowledgement  This project is supported in part by
NSF IIS  CNS  NSF DMS 
NSF IIS  NIH BIGDATA    GM 
NSF CAREER IIS  ONR   
NVIDIA  Intel  and Amazon AWS 

Fake News Mitigation via Point Process Based Intervention

References
Allcott  Hunt and Gentzkow  Matthew  Social media and
fake news in the   election  Technical report  National Bureau of Economic Research   

Antos  Andr as  Szepesv ari  Csaba  and Munos    emi 
Valueiteration based  tted policy iteration 
learning
with   single trajectory  In Approximate Dynamic Programming and Reinforcement Learning    ADPRL
  IEEE International Symposium on  pp   
IEEE   

Bacry  Emmanuel and Muzy  JeanFranc ois  Hawkes
model for price and trades highfrequency dynamics 
Quantitative Finance       

Bacry  Emmanuel and Muzy  JeanFrancois  Second order
statistics characterization of hawkes processes and nonparametric estimation  arXiv preprint arXiv 
   

Bertsekas  Dimitri    Dynamic programming and optimal

control  volume    

Bharathi  Shishir  Kempe  David  and Salek  Mahyar  Competitive in uence maximization in social networks 
In
International Workshop on Web and Internet Economics 
pp    Springer   

Blundell  Charles  Beck  Jeff  and Heller  Katherine   
Modelling reciprocating relationships with hawkes processes  In Advances in Neural Information Processing
Systems  pp     

Bradtke  Steven    and Barto  Andrew   

Linear
leastsquares algorithms for temporal difference learning  Machine Learning     
ISSN
 
URL
http dx doi org BF 

doi 

 BF 

Chen  Duanbing       Linyuan  Shang  MingSheng 
Zhang  YiCheng  and Zhou  Tao  Identifying in uential
nodes in complex networks  Physica    Statistical mechanics and its applications     

De Arruda  Guilherme Ferraz  Barbieri  Andr   Luiz 
Rodr guez  Pablo Mart    Rodrigues  Francisco   
Moreno  Yamir  and da Fontoura Costa  Luciano  Role
of centrality for the identi cation of in uential spreaders
in complex networks  Physical Review     
 

Farajtabar  Mehrdad  Du  Nan  GomezRodriguez  Manuel 
Valera  Isabel  Zha  Hongyuan  and Song  Le  Shaping
social activity by incentivizing users 
In Advances in
neural information processing systems  pp   
 

Farajtabar  Mehrdad  Wang  Yichen  Rodriguez 
Manuel Gomez  Li  Shuang  Zha  Hongyuan  and
Song  Le  Coevolve    joint point process model for
information diffusion and network coevolution 
In
Advances in Neural Information Processing Systems 
pp     

Farajtabar  Mehrdad  Ye  Xiaojing  Harati  Sahar  Song  Le 
and Zha  Hongyuan  Multistage campaigning in social
networks  In Advances in Neural Information Processing
Systems  pp     

Gao  Zhenxiang  Shi  Yan  and Chen  Shanzhi 

Identifying in uential nodes for ef cient routing in opportunistic
networks  Journal of Communications     

Gottfried  Jeffrey and Shearer  Elisa  News Use Across Social Media Platforms   Pew Research Center  May
 

Guo  Fangjian  Blundell  Charles  Wallach  Hanna  and
Heller  Katherine  The bayesian echo chamber  Modeling social in uence via linguistic accommodation  In
Arti cial Intelligence and Statistics  pp     

Hawkes  Alan    Spectra of some selfexciting and mutually exciting point processes  Biometrika   
 

He  Xinran  Rekatsinas  Theodoros  Foulds  James  Getoor 
Lise  and Liu  Yan  Hawkestopic    joint model for
network inference and topic modeling from textbased
cascades  In Proceedings of the  nd International Conference on Machine Learning  ICML  pp   
 

Kempe  David  Kleinberg  Jon  and Tardos   Eva  Maximizing the spread of in uence through   social network 
In Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining 
pp    ACM   

Lagoudakis  Michail   and Parr  Ronald  Leastsquares
policy iteration  Journal of machine learning research 
 Dec   

Lee  Young  Lim  Kar Wai  and Ong  Cheng Soon  Hawkes
processes with stochastic excitations  In Proceedings of
The  rd International Conference on Machine Learning  pp     

Lian  Wenzhao  Henao  Ricardo  Rao  Vinayak  Lucas 
Joseph    and Carin  Lawrence    multitask point process predictive model  In ICML  pp     

Linderman  Scott   and Adams  Ryan    Discovering latent network structure in point process data  In ICML 
pp     

Fake News Mitigation via Point Process Based Intervention

Mitra  Tanushree  Wright     and Gilbert  Eric    parsimonious language model of social media credibility across
disparate events  In Proc  CSCW   

Mosseri  Adam  Addressing hoaxes and fake news   
URL http newsroom fb com news newsfeed fyiaddressing hoaxesand fakenews 

Parikh  Ankur    Gunawardana  Asela  and Meek  Christopher  Conjoint modeling of temporal dependencies in
event streams  BMAW  Preface   

Qin  Zhen and Shelton  Christian    Auxiliary gibbs sampling for inference in piecewiseconstant conditional intensity models  In UAI  pp     

Rizoiu  MarianAndrei  Xie  Lexing  Sanner  Scott  Cebrian  Manuel  Yu  Honglin  and Van Henteryck  Pascal 
Expecting to be hip  Hawkes intensity processes for social media popularity 

media messages 
ence on Web and Social Media   

In Tenth International AAAI Confer 

Zhao  Zhe  Resnick  Paul  and Mei  Qiaozhu  Enquiring
minds  Early detection of rumors in social media from
enquiry posts  In Proceedings of the  th International
Conference on World Wide Web  pp    ACM 
 

Zhou  Ke  Zha  Hongyuan  and Song  Le  Learning social infectivity in sparse lowrank networks using multidimensional hawkes processes 
In Proceedings of the
Sixteenth International Conference on Arti cial Intelligence and Statistics  pp     

Silverman  Craig 

This

analysis

shows

how
outperURL

fake

news

election

viral
formed real news on facebook 
https www buzzfeed com craigsilverman viralfake electionnews outperformedreal newson 
facebook 

stories
 

Simma  Aleksandr and Jordan  Michael    Modeling events
arXiv preprint

with cascades of poisson processes 
arXiv   

Sutton  Richard    and Barto  Andrew    Introduction to
Reinforcement Learning  MIT Press  Cambridge  MA 
USA   st edition    ISBN  

Twitter  Types of tweets and where they appear    URL

https support twitter com articles 

Wang  Yichen  Theodorou  Evangelos  Verma  Apurv  and
Song  Le  Steering opinion dynamics in information diffusion networks  CoRR  abs    URL
http arxiv org abs 

Xu  Qiaofeng  Yang  Deshan  Tan  Jun  Sawatzky  Alex 
and Anastasio  Mark    Accelerated fast iterative shrinkage thresholding algorithms for sparsityregularized
conebeam ct image reconstruction  Medical Physics 
   

Zarezade     Upadhyay     Rabiee     and GomezRodriguez     Redqueen  An online algorithm for smart
broadcasting in social networks 
In WSDM   Proceedings of the  th ACM International Conference on
Web Search and Data Mining   

Zeng  Li  Starbird  Kate  and Spiro  Emma      uncon 
 rmed  Classifying rumor stance in crisisrelated social

