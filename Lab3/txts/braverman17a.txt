Clustering High Dimensional Dynamic Data Streams

Vladimir Braverman   Gereon Frahling   Harry Lang   Christian Sohler   Lin    Yang  

Abstract

We present data streaming algorithms for the kmedian problem in highdimensional dynamic
geometric data streams      
streams allowing
both insertions and deletions of points from  
discrete Euclidean space               Our algorithms use   poly   log   space time and
maintain with high probability   small weighted
set of points    coreset  such that for every set
of   centers the cost of the coreset      
approximates the cost of the streamed point set 
We also provide algorithms that guarantee only
positive weights in the coreset with additional
logarithmic factors in the space and time complexities  We can use this positivelyweighted
coreset to compute        approximation for
the kmedian problem by any ef cient of ine kmedian algorithm  All previous algorithms for
computing        approximation for the kmedian problem over dynamic data streams required space and time exponential in    Our algorithms can be generalized to metric spaces of
bounded doubling dimension 

  Introduction
The analysis of very large data sets is still   big challenge  Particularly  when we would like to obtain information from data sets that occur in the form of   data stream
like  for example  streams of updates to   data base system 
internet traf   and measurements of scienti   experiments
in astroor particle physics      
In
such scenarios it is dif cult and sometimes even impossible
to store the data  Therefore  we need algorithms that process the data sequentially and maintain   summary of the
data using space much smaller than the size of the stream 
Such algorithms are often called streaming algorithms  for
more introduction on streaming algorithms  please refer to
 Muthukrishnan   
One fundamental technique in data analysis is clustering 
The idea is to group data into clusters such that data inside

 Liu et al   

 Johns Hopkins University  USA  Linguee GmbH  TU Dortmund  Correspondence to  Lin    Yang  lyang jhu edu 
Christian Sohler  christian sohler tudortmund de 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

the same cluster is similar and data in different clusters is
different  Center based clustering algorithms also provide
for each cluster   cluster center  which may act as   representative of the cluster  Often data is represented as vectors
in Rd and similarity between data points is often measured
by the Euclidean distance  Clustering has many applications ranging from data compression to unsupervised learning 
In this paper we are interested in clustering problems over
dynamic data streams       data streams that consist of updates  for example  to   database  Our stream consists of
insert and delete operations of points from              
We assume that the stream is consistent       there are no
deletions of points that are not in the point set and no insertions of points that are already in the point set  We consider
the kmedian clustering problem  which for   given   set of
points     Rd asks to compute   set   of   points that
minimizes the sum of distances of the input points to their
nearest points in   
  Our Results
We develop the  rst      approximation algorithm for
the kmedian clustering problem in dynamic data streams
that uses space polynomial in the dimension of the data  To
our best knowledge  all previous algorithms required space
exponentially in the dimension  Formally  our main theorem states 
Theorem    Main Theorem  Fix         positive
integers   and   Algorithm   makes   single pass over the
dynamic streaming point set         outputs   weighted
set    such that with probability at least     is an  

coreset for kmedian of size   cid kd   cid  where    
log   The algorithm uses    cid kd   cid  bits in the worst

case  processes each update in time    dL  and outputs
the coreset in time poly            after one pass of the
stream 
The theorem is restated in Theorem   and the proof is
presented in Section   The coreset we constructed may
contain negatively weighted points  Thus na ve of ine algorithms do not apply directly to  nding kclustering solutions on the coreset  We also provide an alternative approach that output only nonnegatively weighted coreset 
The new algorithm is slightly more complicated  The space
complexity and coreset size is slightly worse than the one
with negative weights but still polynomial in      and
log   and optimal in   up to polylogk factor 
Theorem    Alternative Results  Fix         posi 

Clustering High Dimensional Dynamic Data Streams

tive integers   and   Algorithm   makes   single pass over
the streaming point set         outputs   weighted set
  with nonnegative weights for each point  such that with
probability at least     is an  coreset for kmedian of

size    cid kd   cid  The algorithm uses    cid kd   cid bits

in the worst case  For each update of the input  the algorithm needs poly         log    time to process and outputs the coreset in time poly            after one pass of
the stream 
The theorem is restated in Theorem   in Section   and the
proof is presented therein  Both approaches can be easily
extended to maintain   coreset for   general metric space 
  Our Techniques
From   high level  both algorithms can be viewed as
  combination of the ideas introduced by Frahling and
Sohler  Frahling   Sohler    with the coreset construction by Chen  Chen   
To explain our highlevel idea  we  rst summarize the idea
of Chen  Chen    In their construction  they  rst obtain      bicriterion solution  Namely  nd   set of
at most    centers such that the kmedian cost to these
   centers is at most  OPT  where OPT is the optimal
cost for   kmedian solution  Around each of the   
points  they build logarithmically many concentric ring regions and sample points from these rings  Inside each ring 
the distance from   point to its center is upper and lower
bounded  Thus the contribution to the optimal cost from
the points of this ring is lower bounded by the number of
points times the inner diameter of the ring  To be more precise  their construction requires   partition of       sets
of the original data points satisfying the following propi  Pi diam Pi   cid 
 OPT  They then sample   set of points from each part
to estimate the cost of an arbitrary kset from    up to
  Pi diam Pi  additive error  Combining the samples of the   cid  parts  this gives an additive error of at most
 OPT and therefore an  coreset 
The  rst dif culty in generalizing the construction of Chen
to dynamic streams is that it depends on  rst computing
approximate centers  which seems at  rst glance to require
two passes  Surprisingly  since we would like to be polynomial in    we can resolve this dif culty using   gridbased construction  The grid structure can be viewed as  
   ary tree of cells  The root level of the tree is   single
cell containing the entire set of points  Going down   level
through the tree  each parent cell is split evenly into    subcells  Thus in total there are log    grid levels  Each cell
of the  nest level contains at most   single point 
Without using any information of   precomputed    
bicriterion solution to the kmedian problem  as it does
in  Chen    our  rst idea  similar to the idea used in
 Indyk    is to use   randomly shifted grid       shift
each coordinate of the cell of the root level by   random
value    where   is uniformly chosen from            
and rede ne the tree by splitting cells into    subcells recursively  We show that with high probability  in each

erty  for the partition               Pk cid cid 

mediately show that cid 

level  at most       cells are close to  or containing    center of an optimal solution to the kmedian  For the remaining cells  we show that each of them cannot contain
too many points  since otherwise they would contribute too
much to the cost of the optimal solution  since each point
in these cells is far away from each of the optimal centers  We call the cells containing too many points in  
level heavy cells  The immediate nonheavy children of
the heavy cells form   partition of the entire point sets
     
the cells that are not heavy  but have heavy parents  Let             Ck cid  be these cells  and we can imi  Ci diam Ci     OPT for some
         If we can identify the heavy cells       use
heavy hitter algorithms  and sample points from their immediate nonheavy children in   dynamic stream  we will
obtain   construction similar to Chen  Chen   
Our second idea allows us to signi cantly reduce space requirements and also allows us to do the sampling more easily  For each point    the cells containing it form   path on
the grid tree  We write each point as   telescope sum as
the cell centers on the path of the point   recall that the
grids of each level are nested and the    is the root of the
tree  For example  let               cL be the cell centers of
the path  where cL      and de ne   to be the zero vector 
Then     cL   cL    cL    cL                    
In this way  we can represent the distance from   point to
  set of points as the distance of cell centers to that set of
points  For example  let        be   set of points  and
        be the distance from   to the closest point in   
Then             cL         cL         cL      
  cL                     Thus we can decompose the
cost   set   into       levels  the cost in level         
is the center of
the cell containing   in level   and the cost in the  st
level is          where   is the entire points set  Since
     is bounded by the cell diameter of
   cl
the level  we can sample points from the nonheavy cells
of the entire level  and guarantee that the cost of that level
is wellapproximated  Notice that     we do not need to
sample       points from every part of the partition  thus
we save     factor on the space and     we do not need
to sample the actual points  but only an estimation of the
number of points in each cell  thus the sampling becomes
much easier  there is no need to store the sampled points 
In the above construction  we are able to obtain   coreset 
but the weights can be negative due the the telescope sum 
It is not easy  nd an of ine kmedian algorithm to output
the solutions from   negativelyweighted coreset  To remove the negative weights  we need to adjust the weights
of cells  But the cells with   small number of points  compared to the heavy cells  are problematic   the samplingbased empirical estimations of the number of points in them
has too much error to be adjusted 
In our second construction  we are able to remove all the
negative weights  The major difference is that we introduce
  cutoff on the telescope sum  For example           

          cl 

          cl 

 

     where cp
 

 

is  cid 

    cl

Clustering High Dimensional Dynamic Data Streams

   cl   

 

        cl   

 

 

      cl   

  cL
          
where      is   cutoff level of point   such that the cell containing   in level      is heavy but no longer heavy in level
         We then sample point   with some probability
de ned according to      In other words  we only sample points from heavy cells and not from nonheavy ones 
Since   heavy cell contains enough points  the samplingbased estimation of the number of points is accurate enough
and thus allows us to adjust them to be all positive 
Finally  to handle the insertion and deletions  we use     
heavy hitter algorithm to identify the heavy cells  We use
pseudorandom hash functions       Nisan   construction
 Nisan    Indyk      or kwise independent hash
functions  to do the sampling and use   KSet data structure  Ganguly    to store the sampled points in the dynamic stream 
  Related Work
There is   rich history in studies of geometric problems
in streaming model  Among these problems some excellent examples are  approximating the diameter of  
point set  Feigenbaum et al    Indyk    approximately maitain the convex hull  Cormode   Muthukrishnan    Hershberger   Suri    the minvolume
bounding box  Chan    Chan   Sadjad    maintain  nets and  approximations of   data stream  Bagchi
et al    Clustering problem is another interesting and
popular geometric problem studied in streaming model 
There has been   lot of works on clustering data streams
for the kmedian and kmeans problem based on coresets  HarPeled   Mazumdar    HarPeled   Kushal 
  Chen    Feldman et al      Feldman
  Langberg    Additionally  Charikar et al   
Guha et al    Meyerson    studied the problem in
the more general metric space  The currently best known
algorithm for kmedian problem in this setting is an   
approximation using   kpolylogn  space  Charikar et al 
  However  all of the above methods do not work for
dynamic streams 
The most relevant works to ours are those by Indyk  Indyk 
  Indyk   Price  Indyk   Price    and Frahling
  Sohler  Frahling   Sohler    Indyk  Indyk   
introduced the model for dynamic geometric data streamings  He studied algorithms for  the weight of  minimum
weighted matching  minimum bichromatic matching and
minimum spanning tree and kmedian clustering  He gave
  exhaustive search       approximation algorithm for kmedian and      bicriterion approximation algorithm 
Indyk   Price  Indyk   Price    studied the problem of sparse recovery under Earth Mover Distance  They
show   novel connection between EMD EMD sparse recovery problem to kmedian clustering problem on   two
dimensional grid  The most related work to current one
is Frahling   Sohler  Frahling   Sohler    who develop   streaming      approximation algorithms for kmedian as well as other problems over dynamic geometric
data streams  All previous constructions for higher dimen 

where cost        cid 

which is to minimize cost        cid 

sional grid require space exponential in the dimension   
  Preliminaries
                 and
For integer        we denote    
                       for integer intervals  We
      
will consider   point set   from the Euclidean space
              Without loss of generality  we always assume
  is of the form    for some integer    since otherwise we
can always pad   without loss of   factor more than   Our
streaming algorithm will process insertions and deletions
of points from this space  We study the kmedian problem 
            among
all sets   of   centers from Rd and where         denotes
the Euclidean distance between   and   and         for
  set of points   denotes the distance of   to the closest
point in    The following de nition is from  HarPeled  
Mazumdar   
De nition   Let        be   point set    small
weighted set   is called an  coreset for the kmedian problem  if for every set of   centers        we have  
        cost         cost                 cost      
    wt           and wt    is the
weight of point       
Through out the paper  we assume parameters      
    unless otherwise speci ed  For our algorithms and
   
constructions we de ne   nested grid with   levels  in the
following manner 
De nition of grids Let                 vd  be   vector chosen uniformly at random from           Partition the
space              into   regular Cartesian grid    with
sidelength   and translated so that   vertex of this grid
falls on    Each cell of this grid can be expressed as      
                       vd   nd  vd    nd    
for some             nd    Zd  For       de ne the regular
grid Gi as the grid with sidelength    aligned such that
each cell of Gi  contains    cells of Gi  The  nest grid
is GL where      cid log   cid  the cells of this grid therefore
have sidelength at most   and thus contain at most   single
input point  Each grid forms   partition of the pointset   
There is   dary tree such that each vertex at depth   corresponds to   cell in Gi  and this vertex has    children which
are the cells of Gi  that it contains  For convenience  we
de ne    as the entire dataset and it contains   single cell
   For each cell    we also treat it as   subset of the input
points               if there is no confusion 
We denote         as the optimal solution for kmedian
and OPT as the optimal cost for    The proof of the following lemma is delayed to Section   
Lemma   Fix   set         then with probability at
least       for every level          the number of cells
that satisfy                 is at most       

 For simplicity of the presentation  we de ne the coreset for
all sets of   centers         but it can be generalized to all sets
of   centers     Rd with an additional polylog  factor in
the space  We discuss this point further in Section  

  Outline
In Section   we introduce the coreset with negative
weights  In Section   we introduce   modi ed construction with all positive weights  Section   comes with the
 nal remarks 
  Generally Weighted Coreset
In this section  we present our generally weighted coreset
construction 
In Section   we introduce the telescope
sum representation of   point   and the coreset framework 
In Section   we illustrate our coreset framework with an
of ine construction  In Section   we present an one pass
streaming algorithm that implements our coreset framework 
  The Telescope Sum and Coreset Framework
Our  rst technical idea is to write each point as   telescope
sum  We may interpret this sum as replacing   single point
by   set of points in the following way  Each term     
   of the sum can be viewed as   pair of points   and   
where   has weight   and   has weight   The purpose
of this construction is that the contribution of each term
         or the corresponding two points  is bounded  This
can be later exploited when we introduce and analyze our
sampling procedure 
We now start to de ne the telescope sum  which will relate
to our nested grids  For each     Gi  denote       or simply    as its center  For each point         de ne         as
the cell that contains   in Gi  and ci
  is the center of        
Then we can write

      

   

    ci 
ci

 

 

  cid 

  

  cid 

  

where we set   
       we also call this the cell center of
the  st level for convenience  The purpose of this can
be seen when we consider the distance of   to an arbitrary
kcenters         we can write the cost of   single point
   as

              

        

  ci

          ci 

 

    

      since the cells of GL contain   single
Note that cL
point  Thus the cost of the entire set cost       can be
written as 

  ci

          ci 

 

      

    

      

 

  cid 

 cid 

  

   

 cid 

   

the terms cid 
 cid 

As one can see  we transform the cost de ned using the
original set of points to the  cost  de ned using cell centers  To estimate the cost  it remains to estimate each of
     for          and
       In other words  assign weights to each
of the centers of the grid cells  For          and   cell
    Gi  denote CP as the parent cell of   in grid Gi 

          ci 

        

      ci

 

Clustering High Dimensional Dynamic Data Streams

 

Thus we can rewrite the cost term as follows 
          ci 

cost Gi      

  ci

 

  Gi

    

   
                CP     

 cid 
 cid 
 cid 
 cid 
   cid                 CP     cid 
 cid 
   cid 

         cid    

            

 cid 

   

  Gi

  Gi

  cid Gi 

  Gi     cid 

 

 

 

For       we denote cost                
      
Then this leads to our following coreset construction
framework 
Generally Weighted Construction The coreset   in the
construction is composed by   weighted subset of centers
of grid cells  The procedure of the construction is to assign some  integer  value to each cell center  For instance 

maintain   integer valued function cid      on cells  using small
amount of space   cid    is called the value of the cell    Let

  be the center of    then the weight for   is

wt     cid       cid 

  cid   cid Gi   cid  

 
And for the Lth grid GL  the weight for each cell   is just

 cid    Note that there might be negative weights for some
As   na ve example  we set  cid          as the exact numwill show  if we allow  cid    as an approximation of     up

cells 
ber of points of   cell    Then we would expect the cells
in every level except those in GL have weight   In other
words  we stored the entire point set as the coreset  As we

to additive error  we can compress the number of nonzero
weighted centers to be   smaller number 

 cid   cid 

  Gi

 cid 

De nition   Given   grid structure  and   real valued
           as follows  for          and        

function  cid      on the set of cells  We de ne   function  cid cost  
 cid 
 cid cost Gi      
   cid 

 cid            cid    

 cid            

   containing the entire set of points 

and  cid cost          cid         where    is the cell in
Lemma   Fix an integer valued function  cid      on the set
of cells and parameter          
    Let   be the set of
all cell centers with weights assigned by Equation   If
 cid           recall that    is the  rst cell containing the
entire dataset  and for any        with         and

  Gi     cid 

  cid Gi 

 

Clustering High Dimensional Dynamic Data Streams

        

 cid cid cid     OPT

     

 

then   is an  coreset for kmedian 
Proof  Given an arbitrary set of centers        

wt          

 cid cid cid cost Gi        cid cost Gi    
 cid 
         cid cid       cid 
 cid 
 cid                cid 
 cid   cid 
 cid 
 cid cost Gi    

  Gi

  Gi

   

  cid  

    

  cid   cid Gi 

  cid Gi 
  Gi     cid 

cost        

 cid 
 cid 

    

 

 

    
          

            

 cid   cid cid 
 cid         cid 

    cid 

         and every kset        

 cid cid cid cid cost Gi       cost Gi    

 cid cid cid     OPT      

 

 

 

It

  Si

  Si

   ci

   ci

     CZ

 cid 

The  rst

is exact 

      

      

Note
that
term con 

          ci 

diam         cid 

     CZ
   kd                 OPT

It follows from Lemma   that    is an  coreset 
Let Si be the sampled points of level    Fix   kset    
    for each          by equation   we have that 

 cid cost Gi      cid   CZ cid   cid   cid                 CP     cid   
 cid 
  cid cost Gi       cost Gi    
          ci 
tributes   to the difference  cid cost Gi       cost Gi    
since each  cid   
      cid 

remains to bound the
Denote
error contribution from the second part 
Recall
that   cid 
is the centers of the bicriterion solution and
is the set of cells with distance less than     
CZ cid 
to   cid  where   is the sidelength of   cell  Let   be
event that  CZ cid        cid           kL  By
Lemma     happens with probability at least      
Conditioning on   happening  for each point         CZ cid 
we have that        cid    diam      Therefore 
       cid   
   OPT  By Lemma   with probability at least
   
     Since there are at
most  kd many different ksets from     thus  for    xed
         with probability at least      
   for all ksets
       
By the union bound  with probability at least         is
the desired coreset 
It remains to bound the size of    Conditioning on  
happening  then  CZ cid      kL  For each level   
since each point from cells    cid  CZ cid  contributes at least
      to the bicriterion solution  there are at most
  iOPTd  points in cells not in CZ cid  By   Chernoff
bound  with probability at least            the number of points sampled from cells    cid  CZ cid  of level   is
upper bounded by     kL  log  
    Thus for all levels  with probability at least       the number of points
sampled is upper bounded by     kL  log  
    which
is also an upper bound of the number of cells occupied
by sampled points  Now we bound the number of nonzero weighted centers 
In the coreset construction  if  
cell center has nonzero weight  then either itself or one

of its children cells has nonzero assigned value cid    Thus

 cid cid cid cid cost Gi       cost Gi    

 cid cid cid     OPT      

      kL 

the number of nonzero weigted centers is upper bound
by   times the number of nonzero valued cells  Thus
          kL  log  

Lemma   Fix           if   set of cells   from

grid Gi satis es cid        diam       OPT for some    

       let   be   set of independent samples from
the point set         with probability

   

 cid       

 

    

     min

  

ln  kd   

 

   

 cid 

It follows that  cost         cost          OPT 

    cid 

            cid 

  An Of ine Construction
In this section  we assume we have    bicriterion approximation to kmedian  Let   cid       cid 
    be
the centers and   is the cost satisfying OPT        OPT 
This can be done using  Indyk      We will show how
we construct the coreset base on the framework described
in the last section 
An Of ine Construction For each point in level    we
sample it with probability             count the number
of points exactly  and set  cid          For each level    
     we pick the set of all cells   satisfying        cid   
     where   is the side length of    Denote the set of
of these cells exactly  and set cid          For the points in
these cells as CZ cid  We count the number of points in each
the rest of cells  for each          we sample the points
with probability

 cid          

    

     min

ln

      kd

 

   

 

 cid 

uniformly and independently  Denote Si as the set of sampled points at level    For each    cid  CZ cid  set

 cid       Si       

Then  from the bottom level to the top level  we assign the
weight to the cell centers of each of the cells and their

parent cells with nonzero  cid    using   Denote   as the

coreset  which contains the set of cell centers of nonzero
weight 
Theorem   Fix           then with probability at
least       the of ine construction   is an  coreset for
kmedian and that

 cid    kL 

 

       

log

 
 

 

kL 
 

 cid 

 

Proof of Theorem   By de nition  cid          it is suf 
 ce to show that with probability at least       for every

Clustering High Dimensional Dynamic Data Streams

where         aOPT for some       then for    xed set
        with probability at least           kd 

 cid cid cid 
 cid 

   

      

   ci

          ci 

 

      

   ci

          ci 

 

    cid cid     OPT

     

 

 cid   Gi
that cid      diam       OPT for some parameter   The

The proof is   straightforward application of Bernstein inequality  It is presented in Section   
  The Streaming Algorithm
For the streaming algorithm  the  rst challenge is that we
do not know the actual value of OPT  neither do we have an
   bicriterion solution  To handle this  we will show
that we do not need an actual set of centers of an approximate solution  and that   conceptual optimal solution suf 
 ces  We will guess logarithmically many values for OPT
to do the sampling  We rerun the algorithm in parallel for
each guess of OPT 
The second challenge is that we cannot guarantee the sum
diam    to be upper bounded by  OPT as required
in Lemma   We will show that we can split the set of
cells into two parts  The  rst part satis es the property
second part satis es that    diam      aOPT   for some
constant   
For the  rst part  we use   similar sampling procedure
as we did in the of ine case  The challenge here is that
there might be too many points sampled when the algorithm is midway through the stream  and these points may
be deleted later in the stream  To handle this case  we
use   data structure called KSet structure with parameter
   Ganguly    We will insert  with deletions    multiset of points          into the KSet  The data structure processes each stream operation in   log    time 
At each point of time  it supports an operation RETRSET 
that with probability at least       either returns the set
of items of   or returns Fail  Further  if the number
of distinct items     is at most    then RETRSET returns
  with probability at least       The space used by the
KSet data structure is     log       log     log   
The KSet construction also returns the frequency of each
stored points upon the RETRSET operation 
For the second part  we call these cells heavy  We  rst upper bound the number of heavy cells by    for some      
We use   heavy hitter algorithm HEAVYHITTER to retrieve an approximation to the number of points in these
cells  The guarantee is given in the following theorem 
In an insertiondeletion stream  it may that although the
stream has arbitrary large length  at any moment   much
smaller number of elements are active  that is  inserted and
not yet deleted  We de ne the size of   stream to be the
maximum number of active elements at any point of the
stream 
Theorem    Larsen et al    Theorem   Fix      
    Given   stream  of insertions and deletions  of

size   consisting of items from universe     there exists an
algorithm HEAVYHITTER           that makes   single
pass over the stream and outputs   set of pairs    With
probability at least       the following holds 
      
 

      

 cid  
   cid  
The algorithm uses   cid      

for each      fi          
   
        
  if for any         and    
then      fi      
  for each      fi          fi   fi     
    log  

      

     cid  
     cid  
 cid cid  
  log   cid  bits of space 

   
        

   
        

  log    update time and        polylog    query
time 
Thus  using HEAVYHITTER  we are guaranteed that the
error of the number of points in heavy cells is upper
bounded by   times the number of points in the nonheavy
cells  The  rst heavy hitter algorithm that achieves an   
guarantee is by  Charikar et al    who has the same
space and update time as that of the above algorithm  However the update time is slow           log    time to output
the set of heavy hitters 
Lastly  we will use fully independent random hash function to sample the points  We will use Nissan   pseudorandom generator to derandomize the hash functions by
the method of  Indyk      Our main theorem for this
section is as follows  The formal proof of this theorem is
postponed to Section   
Theorem    Main Theorem  Fix           positive integers   and   Algorithm   makes   single pass over
the streaming point set         outputs   weighted set
   such that with probability at least     is an  coreset
for kmedian of size  
  where     log  
The algorithm uses

       

 cid 

 

 cid       
 cid 

    

 cid cid      

 cid cid 

 

 

   

 

log

dkL
 

 

    
 

dL  log dkL
 

in the worst case  processes each update in
and outputs the coreset in time

bits
time  
poly            after one pass of the stream 
  Positively Weighted Coreset
In this section  we will introduce   modi cation to our previous coreset construction  which leads to   coreset with all
positively weighted points  The full algorithm and proofs
are postponed to Section    We present the main steps in
this section 
The high level idea is as follows  When considering the estimate of the number of points in   cell  the estimate is only
accurate when it truly contains   large number of points 
However  in the construction of the previous section  we
sample from each cell of each level  even though some of
the cells contain   single point  For those cells  we cannot
adjust their weights from negative to positive  since doing
so would introduce large error  In this section  we introduce
an ending level to each point  In other words  the number of

 cid 
 cid 

 cid 

Clustering High Dimensional Dynamic Data Streams

Algorithm   CoreSet           construct    coreset
for dynamic stream   

    

 

Initization 
Initialize   grid structure 
                 
     cid log  cid 
        min
          

 cid       

 cid   cid 

 

 cid 

 

    
        
 

ln  kd   
 
 
ln  
       

 cid 

 

 cid 

   

 

   kd   

For each       and          construct fully in 
           with
dependent hash function ho  
  rho   ho                 
Initialize KSet instances KSo   with error probability
       size parameter   
Initialize HEAVYHITTER                 
 cid         instances  HH  HH          HHL  one for
  level 
Update    
for each update  op         

 op    Insert  Delete 

 Insert    Delete 

         
for each         

    the center of the cell contains   at level   
ci
HHi update op  ci
  
for each        

if ho         

KSo   update op  ci

  

Query 
Let    be the smallest   such that no instance of
KSo  KSo          KSo   returns Fail 
     
for       to   

for each cell center   in level   

Let   be the cell containing   
if      

      
    GetFreq    HHi  KSo         

else 

if       

   cid 

  cid     cid Gi 

GetFreq      cid  HHi  KSo         
Assign weight       to   
if        cid   

           

else 

Assign weight   to   
if    cid   

           

return   

points of   cell is estimated by sampling only if it contains
many points  Thus  the estimates will be accurate enough

Algorithm   GetFreq    HH  KS      retrieve the correct freuquency of cell center    given the instance of
HEAVYHITTER and Kset 
fS      the frequency of   returned by HH 
fK      the frequency of   returned by KS 
  cid                 
    the set of
HEAVYHITTER 
if        

topk cid  heavy hitters returned by

return fS   

else 

return fK     

 

and allow us to rectify the weights to be all positive 
  Reformulation of the Telescope Sum
identi cation scheme  
De nition     heavy cell
is   map          heavy  nonheavy  such that 
      heavy and for cell     Gi for         
  if          dOPT
  If        nonheavy  then     cid    nonheavy for every subsell   cid  of   
  For every cell   in level           nonheavy 
  For each               Gi          heavy   

     then        heavy 

  where       is   positive universal constant 

 kL
The output for   cell not speci ed by the above conditions
can be arbitrary  We call   cell heavy if it is identi ed heavy
by    Note that   heavy cell does not necessarily contain  
large number of points  but the total number of these cells
is always bounded 
In the sequel  heavy cells are de ned by an arbitrary  xed
identi cation scheme unless otherwise speci ed 
De nition   Fix   heavy cell identi cation scheme   
For level          let           Gi be the cell in Gi
containing    The ending level      of   point       is the
largest level   such that            heavy  and          
   nonheavy 
Note that the ending level is uniquely de ned if   heavy
cell identi cation scheme is  xed  We now rewrite the telescope sum for   as follows 

 cid 

    cid 

 cid 
    we write             cid     

      and cL

where   

   

  

 

 

 

  cL

    cl   

    ci 
ci
    cid   
       For arbitrary kcenters    

          ci 

 cid   ci

 

           cl   

 

  
              

  cL
  The New Construction  with arbitrary weights 
For these heavy cells  we use HEAVYHITTER algorithms
to obtain accurate estimates of the number of points in these
cells  thus providing   heavy cell identi cation scheme  For
the nonheavy cells  we only need to sample points from the
bottom level  GL  but with   different probability for points
with different ending levels 
We now describe the new construction  This essentially

Clustering High Dimensional Dynamic Data Streams

has the same gaurantee as the simpler construction from
the previous section  however the bene   here is that  as
shown in the next subsection  it can be modi ed to output
only positive weights  In the following paragraph  the esti 

specify the conditions these estimations must satisfy 
NonNegatively Weighted Construction Fix an arbitrary
heavy cell identi cation scheme    Let Pl be all the points

mations cid    are given as   blackbox  In proposition    we
with ending level           For each heavy cell    let  cid   
be an estimation of number of points of     we also call cid   
the value of cell    For each nonheavy cell   cid  let  cid   cid     
Let   be   set samples of   constructed as follows     
                   SL  where Sl is   set of       samples
from Pl with probability     Here    for          is
 cid 
rede ned as     
where       and       are universal constants  Our
coreset   is composed by all the sampled points in   and
the cell centers of heavy cells  with each point   assigned
  weight       and for each cell center   of   heavy cell
    Gi  the weight is 

 cid       

 cid     dk

     kL 

log  kL 

 cid 

    

min

    

log

 

   

 

wt     cid       cid 

 cid   cid     Si     

  

 

 

  cid   cid Gi   cid   

  cid  is heavy

For each nonheavy cell   except for those in the bottom
level  wt          The weight of each point from   is
the value of the corresponding cell in the bottom level 
  Ensuring NonNegative Weights
We now provide   procedure to rectify all the weights for
the coreset constructed in the last subsection  The idea is
similar to the method used in  Indyk   Price    The
procedure is shown in Algorithm   After this procedure 
there will be no negative weights in the coreset outputs 

 cid cid   cid           cid Ck cid   

 cid 

Algorithm   RectifyWeights
 
input the estimates of number of points in each cell and the
weighted sampled points  output   weighted coreset with
nonnegative weights 
for       to   

for each heavy cell   center in Gi 

if wt       

children cell   cid    Gi   cid   cid  is nonnegative 

Decrease the value of the children heavy cells
in level Gi  and sampled points Si arbitrarily by total  wt    amount  such that for each
and for each sampled point     Si  the weight
is nonnegative 

return Recti ed Coreset

Theorem   Fix           positive integers   and
  Algorithm   makes   single pass over the streaming
point set         outputs   weighted set   with nonnegative weights for each point  such that with probability

at least     is an  coreset for kmedian of size

 cid 

 cid       
 cid 

 

 cid       

 

 

  log kL
 
where     log   The algorithm uses

     

 

 dL    

  log  dkL

 

 cid cid 

 cid 

 cid 

log  dkL
 

bits in the worst case  For each update of the input  the
algorithm needs poly          log    time to process and
outputs the coreset in time poly              log    after one pass of the stream 
  Experiments
We illustrate our construction using an of ine construction
on Gaussian mixture data in    As shown in Figure   in
Section    we randomly generated   points from   
then rounded the points to   grid of size       Our
coreset uses log        levels of grids  The storage in
each level is very sparse  As shown in Figure     only  
points are stored in total  We compared the  median costs
estimated using the coreset and the dataset  the resulting
difference is very small  as illustrated in Figure    

   

   

Figure       The layer structure of the coreset  Cells with more
weight are shaded darker      The relative error of    median
cost function  Using only   points  the global maximum error
was under  
  Concluding Remark
We develop algorithms that make   single pass over the dynamic stream and output  with high probability    coreset
for the original kmedian problem  Both the space complexity and the size of the coreset are polynomially dependent on    whereas the only previous known bounds are
exponential in    We constructed our coreset for the possible solutions in discrete space     but it is easy to modify
the coreset to be   coreset in continuous space       note
that we still require the input dataset to be from   discrete
space  The way to do this is by modifying the sampling
probability    in the algorithm       replacing the factor of
ln kdL  to ln kdL  Then any kset
from      can be rounded to the closest kset in   
and the cost only differs by         factor while the space
bound changes only by   polylog  factor  Lastly  we
remark that the coreset scheme can be easily modi ed to
other metric spaces      
the lp metric  The space bound
depends on the doubling dimension of the metric 
As shown in our experiments       implementation using
our framework is very ef cient  We believe that   highdimensional implementation will be ef cient as well  We
leave the full implementation as   future project 

Clustering High Dimensional Dynamic Data Streams

Acknowledgment
   Braverman is supported by the NSF Grants IIS 
EAGER CCF    and CAREER CCF    
Lang is supported by the FrancoAmerican Fulbright Commission     Lang thanks INRIA    Institut national de
recherche en informatique et en automatique  for hosting
him during the writing of this paper     Sohler is Supported by DFG within the Collaborative Research Center
SFB    Providing Information by ResourceConstrained
Analysis  project       Yang is supported by the NSF
Grant IIS 
References
Bagchi  Amitabha  Chaudhary  Amitabh  Eppstein  David 
and Goodrich  Michael    Deterministic sampling and
range counting in geometric data streams  ACM Transactions on Algorithms  TALG     

Chan  Timothy    Faster coreset constructions and data
stream algorithms in  xed dimensions  In Proceedings
of the twentieth annual symposium on Computational geometry  pp    ACM   

Chan  Timothy   and Sadjad  Bashir    Geometric optimization problems over sliding windows  International
Journal of Computational Geometry   Applications   
     

Charikar  Moses  Chekuri  Chandra  Feder  Tom as  and
Motwani  Rajeev 
Incremental clustering and dynamic
information retrieval  In Proceedings of the twentyninth
annual ACM symposium on Theory of computing  pp 
  ACM   

Charikar  Moses  Chen  Kevin  and FarachColton  MarIn Intertin  Finding frequent items in data streams 
national Colloquium on Automata  Languages  and Programming  pp    Springer   

Charikar  Moses    Callaghan  Liadan  and Panigrahy 
Rina  Better streaming algorithms for clustering problems  In Proceedings of the thirty fth annual ACM symposium on Theory of computing  pp    ACM   

Chen  Ke  On coresets for kmedian and kmeans clustering in metric and euclidean spaces and their applications  SIAM    Comput      doi 
  URL http dx doi org 
 

Cormode  Graham and Muthukrishnan     Radial histograms for spatial streams  DIM ACS Technical Report 
   

Feigenbaum  Joan  Kannan  Sampath  and Zhang  Jian 
in the streaming and sliding 

Computing diameter
window models  Algorithmica     

Feldman  Dan and Langberg  Michael    uni ed frameIn Prowork for approximating and clustering data 
ceedings of the  rd ACM Symposium on Theory of

Computing  STOC   San Jose  CA  USA    June
  pp     
doi   
  URL http doi acm org 
 

Feldman  Dan  Monemizadeh  Morteza  and Sohler  Christian    PTAS for kmeans clustering based on weak
In Proceedings of the  rd ACM Symposium
coresets 
on Computational Geometry  Gyeongju  South Korea 
June     pp      doi   
  URL http doi acm org 
 

Feldman  Dan  Schmidt  Melanie  and Sohler  Christian  Turning big data into tiny data  Constantsize
coresets for kmeans  PCA and projective clustering 
In Proceedings of
the TwentyFourth Annual ACMSIAM Symposium on Discrete Algorithms  SODA  
New Orleans  Louisiana  USA  January     pp 
   
doi   
  URL http dx doi org 
 

Frahling  Gereon and Sohler  Christian  Coresets in dyIn Proceedings of the
namic geometric data streams 
thirtyseventh annual ACM symposium on Theory of
computing  pp    ACM   

Ganguly  Sumit  Counting distinct items over update
streams  In International Symposium on Algorithms and
Computation  pp    Springer   

Guha  Sudipto  Mishra  Nina  Motwani  Rajeev  and
  Callaghan  Liadan  Clustering data streams  In Foundations of computer science    proceedings   st annual symposium on  pp    IEEE   

HarPeled  Sariel and Kushal  Akash  Smaller coresets
In Proceedings
for kmedian and kmeans clustering 
of the  st ACM Symposium on Computational Geometry  Pisa  Italy  June     pp      doi 
  URL http doi acm 
org 

HarPeled  Sariel and Mazumdar  Soham  On coresets for
kmeans and kmedian clustering  In Proceedings of the
 th Annual ACM Symposium on Theory of Computing 
Chicago  IL  USA  June     pp     
doi    URL http doi 
acm org 

Hershberger  John and Suri  Subhash  Adaptive sampling
In Profor geometric problems over data streams 
ceedings of the twentythird ACM SIGMODSIGACT 
SIGART symposium on Principles of database systems 
pp    ACM   

Indyk  Piotr  Highdimensional computational geometry 

PhD thesis  Citeseer     

Clustering High Dimensional Dynamic Data Streams

Indyk  Piotr  Stable distributions  pseudorandom generators  embeddings and data stream computation  In Foundations of Computer Science    Proceedings   st
Annual Symposium on  pp    IEEE     

Indyk  Piotr  Better algorithms for highdimensional proximity problems via asymmetric embeddings  In Proceedings of the fourteenth annual ACMSIAM symposium on
Discrete algorithms  pp    Society for Industrial
and Applied Mathematics   

Indyk  Piotr  Algorithms for dynamic geometric problems
over data streams  In Proceedings of the thirtysixth annual ACM symposium on Theory of computing  pp   
  ACM   

Indyk  Piotr and Price  Eric  Kmedian clustering  modelbased compressive sensing  and sparse recovery for earth
mover distance  In Proceedings of the fortythird annual
ACM symposium on Theory of computing  pp   
ACM   

Larsen  Kasper Green  Nelson  Jelani  Nguy en  Huy    and
Thorup  Mikkel  Heavy hitters via clusterpreserving
clustering  arXiv preprint arXiv   

Liu  Zaoxing 

Ivkin  Nikita  Yang  Lin  Neyrinck 
Mark  Lemson  Gerard  Szalay  Alexander  Braverman 
Vladimir  Budavari  Tamas  Burns  Randal  and Wang 
Xin  Streaming algorithms for halo  nders  In eScience
 eScience    IEEE  th International Conference
on  pp    IEEE   

Meyerson  Adam  Online facility location  In Foundations
of Computer Science    Proceedings   nd IEEE
Symposium on  pp    IEEE   

Muthukrishnan  Shanmugavelayutham  Data streams  Al 

gorithms and applications  Now Publishers Inc   

Nisan  Noam 

Pseudorandom generators for spacebounded computation  Combinatorica   
 

