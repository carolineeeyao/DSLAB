Spherical Structured Feature Maps for Kernel Approximation

Yueming Lyu  

Abstract

We propose Spherical Structured Feature  SSF 
maps to approximate shift and rotation invariant kernels as well as bthorder arccosine kernels  Cho   Saul    We construct SSF
maps based on the point set on       dimensional sphere Sd  We prove that the inner
product of SSF maps are unbiased estimates for
above kernels if asymptotically uniformly distributed point set on Sd  is given  According
to  Brauchart   Grabner    optimizing the
discrete Riesz senergy can generate asymptotically uniformly distributed point set on Sd 
Thus  we propose an ef cient coordinate decent
method to  nd   local optimum of the discrete
Riesz senergy for SSF maps construction  Theoretically  SSF maps construction achieves linear
space complexity and loglinear time complexity 
Empirically  SSF maps achieve superior performance compared with other methods 

  Introduction
Kernel methods such as Gaussian processes  GPs   Rasmussen    Srinivas et al    Snoek et al   
and support vector machines  SVMs   Chang   Lin   
Fan et al    have been successfully used in many statistical modeling and machine learning tasks  Despite of
strong expressive power  kernel methods usually cannot
scale up to the large scale datasets with   samples due to
the need of manipulating     Gram matrix  Recently  random feature maps  Rahimi et al    Rahimi   Recht 
  Sutherland   Schneider    have demonstrated
their effectiveness on kernel approximation to scale up kernel methods  Roughly speaking    shift invariant kernel
                     Rd     can be approximated
by                   where   is the explicit mapped
 
   where    
feature constructed as          WT   
 Department of Computer Science  City University of Hong
Kong  Tat Chee Avenue  Hong Kong   Correspondence to  Yueming Lyu  LV Yueming outlook com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

denotes the nonlinear function      Rd   is constructed
by         samples drawn from   distribution de ned by
   Therefore  the training and inference of kernel methods
can be greatly accelerated by working directly on the primal space of   For example  Gaussian Processes  GPs 
have      computation and      storage complexity 
By using feature maps  it reduces to              computation and             storage complexity  All these
elegant properties make random feature maps promising
for large scale kernel methods  Thus  many kernel methods  Le   Wilson    Cutajar et al    Oliva et al 
  have been proposed to deal with large scale statistical
learning by directly working on feature maps 
Generally  two aspects of random feature maps are mostly
concerned by literature for scaling up kernel methods  One
is the approximation accuracy of feature maps while the
other is the computational cost of feature maps construction  To achieve better approximation accuracy   Yang 
  Avron et al    employ QMC  Dick et al   
sampling instead of standard Monte Carlo sampling to construct feature maps  By mapping QMC points on     
through the inverse cumulative distribution function  they
construct more effective feature maps  To reduce time
complexity   Le et al    propose Fastfood to construct
feature maps  Bene ting from the special structured matrix multiplication  it reduces time complexity of feature
maps construction from        to     log    However 
it achieves computational ef ciency at the expense of increasing the variance of approximation  Recently   Feng
et al    employ the property of circulant matrix to accelerate feature maps construction of Gaussian kernel without increasing the variance   Choromanski   Sindhwani 
  generalize the Fastfood and circulant feature maps
to   model and particularly discuss the structured matrix
with lowdisplacement rank  Despite of the success of  
model  it still cannot achieve better approximation accuracy
compared with feature maps obtained with fully Gaussian
matrix 
To achieve better approximation accuracy and loglinear
time complexity  we propose Spherical Structured Feature
 SSF  maps to approximate shift and rotation invariant kernels as well as bthorder arccosine kernels  Cho   Saul 
  Speci cally  We construct SSF maps based on the
point set on       dimensional sphere Sd  where the

Spherical Structured Feature Maps for Kernel Approximation

points are columns of   particular structured matrix produced by   discrete Fourier matrix  The points on Sd 
for SSF maps construction can be generated by optimizing the discrete Riesz senergy  According to  Brauchart
et al    optimizing the discrete Riesz senergy  for  
in some ranges  can generate QMC designs on Sd  which
usually can achieve smaller approximation error compared
with fully random methods  Moreover  Because of special structure of the point set  SSF maps construction can
achieve loglinear time complexity via Fast Fourier Transform  FFT 
Our contributions are summarized as follows 

  We propose Spherical Structured Feature  SSF  maps
to approximate shift and rotation invariant kernels as
well as bthorder arccosine kernels  Cho   Saul 
  We prove that the inner product of SSF maps
are unbiased estimates for above kernels if asymptotically uniformly distributed point set on       dimensional sphere Sd  is given 

  We propose an ef cient coordinate decent method
to  nd   local optimum of the discrete Riesz senergy  Brauchart   Grabner   
thereby approximately generating asymptotically uniformly distributed points on Sd 

  We can construct SSF maps with linear space complexity and loglinear time complexity  Empirically 
SSF maps achieve superior performance compared
with other methods 

  Background and Preliminaries
We provide   brief review of random feature maps and the
discrete Riesz senergy in this section as preliminaries 

  Random Feature Maps

Random feature maps can be viewed as equal weight approximation of multidimensional integrals  One earlier
work  Rahimi et al    approximates the shift invariant
kernels based on the Bochner   Theorem 
Theorem   Bochner   Theorem  Rudin        continuous shift invariant scaled kernel function          
           Rd     is positive de nite if and only if
it is the Fourier Transform of   unique  nite probability
measure   on Rd 

Rd         Twp   dw

 

         cid 

For   real valued kernel                         can
ensure the imaginary parts of the integral vanish  According to the Bochner   theorem  there is   oneto one corre 

spondence between the kernel functions         and probability densities      de ned on Rd 
Shift and rotation invariant kernels are shift invariant
kernels with the rotation invariant property                
  Rx  Rz  given any rotation     SO    where SO   
denotes rotation groups  The Gaussian kernel          
  cid     cid 
  is   member of this family  From Bochner  
theorem 
the corresponding probability density is also
Gaussian  For   general Gaussian RBF kernel          
              it can be transformed into rotation invariant form by using        in the original domain 
bthorder arccosine kernels are rotation invariant kernels  As discussed in  Cho   Saul    bthorder arccosine kernels have the following form 
   cid   cid  

Kb          

  Jb 

 

where     cos cid  xT  

 cid 
   cid   cid  

 cid   cid cid   cid 

bthorder arccosine kernels have trivial dependence on the
norm of   and    The dependence on the angle is de ned
by function Jb  bthorder arccosine kernels are rotation invariant kernels but not shift invariant kernels in general  For example  the zeroorder   and  rstorder  
arccosine kernel are not shift invariant kernels 

               

 

           

   cid   cid   cid   cid   sin           cos  

 

 

The bthorder arccosine kernel Kb       can be reformulated via the integral representation 

Kb          cid 

 

 
 wT   

Rd   wT     wT   wT   

    dw
 
where    is   step function                if       and  
otherwise  and the density   is standard Gaussian 
Feature maps  Both Monte Carlo and QuasiMonte Carlo
approximation  Dick et al    are equal weight approximation to integrals  Based on equal weight approximation 
the feature maps can be constructed as 

  cid wT
    cid    cid wT

    cid            

           

 

  cid 

  

 
where wi            are samples constructed by Monte
Carlo or QuasiMonte Carlo methods      is   nonlinear
function depending on the kernel    is the explicit  nite
dimensional feature map  For Gaussian kernel with bandwidth   the associated nonlinear function is   complex exponential function         eix  For   zeroorder arccosine kernel in   and  rstorder arccosine kernel in  
the associated nonlinear functions are step function        
     and ReLU activation function         max     respectively 

Spherical Structured Feature Maps for Kernel Approximation

  Discrete Riesz senergy

The discrete Riesz senergy is related to the equal weight
numerical integration and uniformly distributed point set 
Equal weight numerical integration over   ddimensional
sphere Sd        Rd     cid   cid      uses equal weight
summation of  nite point evaluations of the integrands to
approximate the integrals 

Sd              

 

   vi 

 

 cid 

  cid 

  

where   denotes the normalized surface area measure on
Sd 
According to  Brauchart   Grabner    the point set
          vN     Sd   is asymptotically uniformly distributed if equation   holds true 

   vi   cid 

  cid 

  

lim
  

 
 

Sd          

 

The discrete Riesz senergy   otz    Brauchart  
Grabner    is de ned as equation  

 

Es      

  cid 
  cid 

  

  cid 
  cid 

  

    cid  

    cid  

 

 cid vi vj cid  

 

     cid   

 

log

 

 cid vi vj cid 

       

Theorem    Brauchart   Grabner    For      
the optimum Npoint con guration of the Riesz senergy
on Sd is asymptotically uniformly distributed       the normalized surface area measure   on Sd 
According to  Brauchart et al    Brauchart   Grabner 
  the discrete Riesz senergy can serve as   criterion
to construct the point set           vN     Sd   for
QMC designs  Particularly   Brauchart et al    have
proved that maximizing the discrete Riesz senergy with
        can generate QMC designs for functions in
Sobolev space  They also prove that QMC designs have
higher convergence rate of worstcase error than fully randomly chosen points for functions in Sobolev space 

  Spherical Structured Feature Maps
In this section  we propose SSF maps to approximate shift
and rotation invariant kernels as well as bthorder arccosine kernels by employing their rotation invariant property 

  Feature Maps for Shift and Rotation Invariant

Kernels

Shift and rotation invariant kernels are highly symmetric and structured because they satisfy both shift invariant property and rotation invariant property  Rotation invariant property means that             Rx  Rz  given
any rotation     SO    where SO    denotes rotation
groups  To bene   from rotation invariant property  it is
reasonable to construct the feature maps by using spherical
equal weight approximation in equation   and  
The feature maps for real valued shift and rotation invariant
kernels         can be constructed as equation  

         

  cos cid tM  xT vN

 cos cid   xT   

 cid    sin cid   xT   
 cid  

 cid    sin cid tM  xT vN

 cid   

 
              vN     Sd   denotes the
where tj    
point set asymptotically uniformly distributed on Sd and
    denotes the inverse cumulative distribution function
      the nonnegative radial scale 
Theorem              is an unbiased estimate of  
real valued shift and rotation invariant kennel        
Proof  From Bochner   Theorem    shift invariant kernel
        can be written as equation   Let      cid   cid and
     be the density function of    Because of the rotation
invariant property of         we achieve equation  

NM

         cid 
 cid 

  

 

 cid 
 cid 

Sd    ir       vp   drd   
Sd               vd   dt

 

where    denotes the nonnegative real values 
For real valued kernel         the imaginary parts of the
integral vanish  We can achieve equation  
            

         cid 

    dt

 cid 

 cid 

 cid 

Sd  cos

 
According to the property of asymptotically uniformly
distributed point set   in equation   and the onedimensional QMC rule  we obtain equation  

 

lim

                 

 cid 

  

 cid  cos cid tj zT vi
 cid 

  cid 
  cid 
  sin cid tj xT vi
 cid 
  cid 
 cid 

 cos cid tj xT vi
 cid  sin cid tj zT vi
 cid 
  cid 
 cid 

 tj         vi

cos

  

  

  

            

    dt

lim

    

 

   

 cid 

 cid 

  lim

    

 

   

Sd  cos

 

         

 
 cid 

Spherical Structured Feature Maps for Kernel Approximation

Proposition   Let           using point set   to
approximate   real valued shift and rotation invariant kernel
        by using equation   is equal to using point set
  to approximate        

                                   

 

Proof  Note that cosine function is an even function  Thus 
we obtain equation  
 tj         vi

 cid tj         vi

  cos

 cid 

 cid 

 cid 

cos

Thus  we achieve equation  

                

 tj         vi

 cid 
 cid 
 cid tj         vi
 cid 
 cid 
 cid 

 tj         vi

cos

cos

  cos

  cid 
  cid 
  cid 

  cid 
  cid 
  cid 

  

  

  

  

  

  

   

 NM

   

 NM

   

 NM

                  

 

 

 cid 

Proposition   shows that for   shift and rotation invariant
kernel  computing   points can achieve the same approximation effect compared with using    points 

  Feature Maps for bthorder Arccosine Kernels

In this subsection  we discuss the feature maps for bthorder arccosine kernels  We discuss them separately because they are rotation invariant kernels but not shift invariant kernels in general  Moreover  they are closely related to
deep neural networks  Cho   Saul    which demonstrate super performance in many areas 
Lemma   The bthorder arccosine kernels can be calculated as equation  

Sd   cid vT   cid   cid vT   cid 
 cid 
where       max  sign        Cb  cid 

 vT   vT       

  bp   dr 
Cb is   constant that is independent of   and         is
the density function of the chidistribution with   degrees
freedom  For example  the constants associated with the
zero   rst and secondorder arccosine kernels are       
       and              respectively 
Proof  From equation   we can achieve equation  

Kb         Cb

 

  

Kb          cid 
   cid 

Rd  cid wT   cid   cid wT   cid      dw

Rd   wT     wT   wT   

 

 wT   

 

    dw

 

Let      cid   cid  Since   is standard Gaussian  by taking
rotation invariant property  we obtain equation  

Rd  cid wTx cid   cid wTz cid      dw
Kb          cid 
 cid 
   cid 
 cid 
   cid 
  bp   dr cid 
   cid 
Sd   cid vT   cid   cid vT   cid      
 cid 

 cid rbvT   cid   cid rbvT   cid          dr
    cid vT   cid   cid vT   cid          dr
Sd   cid vT   cid   cid vT   cid      

Sd 
Sd 
  
   Cb

  
  

 
Since Kb       is rotation invariant  we have Kb        
Kb      Together with equation   we achieve
equation  

Sd   cid vT   cid   cid vT   cid 
 cid 

 vT   vT       

Kb         Cb

 
 cid 

The feature maps for   bthorder arccosine kernel Kb      
can be constructed as equation  

 cid  Cb
   cid vT
    cid     cid vT
    cid     cid vT
 cid vT

    cid     
    cid          

       

 

Theorem              is an unbiased estimate of  
bthorder arccosine kernel Kb      
Proof  According to the Lemma   and the property of
the asymptotically uniformly distributed point set    we
obtain equation  
             
lim
  lim
  

  cid 
Sd   cid vT   cid   cid vT   cid     vT   vT       

 cid vT
    cid   cid vT

    cid     vT

    vT

    

 cid 

Cb
 

  

  Cb
  Kb      

 
 cid 

From equation   and   we observe that the approximation is actually operated on the       dimensional
domain instead of ddimensional domain  Cho   Saul 
  Generally  the approximation error of Quasi Monte
Carlo methods with   points depends on the dimension of
integration    lower dimension leads to smaller approximation error  thus the feature maps in equation   can
achieve lower approximation error 
The feature maps in equation   are closely related to
the bidirectional activation neural network  Speci cally 
the feature maps for the  rstorder arccosine kernel are
related to the bidirectional ReLU activation function  An
et al    which has the distance preservation property
compared with ReLU 
From equation   and   we know that the feature
maps actually rely on the point set           The
design of the point set   will be discussed in section  

Spherical Structured Feature Maps for Kernel Approximation

  Design of Matrix  
We have discussed the construction of SSF maps in last
section  However  one unsolved problem is how to obtain
the matrix           We employ the discrete Riesz
senergy as the objective function to obtain matrix   because it can generate asymptotically uniformly distributed
points on Sd   Brauchart   Grabner    Moreover 
to achieve computation and storage ef ciency for feature
maps construction   we add   structured constraint to the
matrix    In this section  we show the structure of matrix
   rst and then the optimization of discrete Riesz senergy 
It is worth noting that matrix   can be used not only for
kernel approximation  but also for approximation of general integrals over hypersphere  Moreover  by using FFT 
matrix   can accelerate the integral approximation which
involves projection operations  In addition  it only needs to
store the indexes with linear storage cost            instead
of to explicitly store the matrix with cost       

  Structure of Matrix  
Since   can be constructed by                   we
only need to de ne structured matrix    To achieve loglinear time complexity of SSF maps construction  we construct   by extracting rows from   discrete Fourier matrix 
The complexity analysis of SSF maps construction based
on matrix   is given in section  
Mathematically  the construction of matrix   is shown as
follows  Without loss of generality  we assume that    
                   Let     Cn   be         discrete
  Let              km             
is the       thentry of    
Fourier matrix  Fk      
where    
be   subset of indexes 
The structured matrix   can be de ned as equation  

 ikj

 

 cid  ReF   ImF 

ImF  ReF 

 cid 

where    in equation   is the matrix constructed by  
rows of    

  Rd  

 

    Cm  

 

     
 

   

 ik   

 

 

 ikm 

 

 

  

 
 
 

 

 ik   

 

 

 ikmn

 

 

With the   given in equation   it is easy to verify that
 cid vi cid      for            Thus  each column of matrix
  is   point on Sd 

  Minimize the Discrete Riesz senergy

With structured matrix   de ned in equation   our goal
is to select   subset of indexes   that optimizes the discrete

   cid 

   cid 

  

    cid  

Riesz senergy  Speci cally  we will discuss how to minimize the Riesz  energy in equation   The other Riesz
senergy can be optimized in   similar way 

      

log

 

 cid ui uj cid 

 

where                        
In the following  we will discuss how to minimize equation   by using   coordinate decent method 
Theorem   Let           with   de ned in  
the discrete Riesz  energy of   can be calculated as equation  

 cid 

  cid 
  cid 

  

log

  

log

 cid 

  

     Re  

 

 cid 

  cid 

 iks  

 

 

 

 cid 

  

 iksp

 

 

 

 

  cid 

  

 

             

     Im  

where   is   constant independent of the choice of  
Proof  Since                    we obtain
equation  

log  cid ui   uj cid 

   cid 

    cid  

log  cid vi cid 

  

            cid 
  cid 
  cid 
  cid 
  cid 
  cid 

   
 

       

       

    cid  

  

  

  

  cid 
  cid 

 log  cid vi   vj cid    log  cid vi   vj cid 
log  cid vi   vj cid cid vi   vj cid 

 cid cid     vT

  vj

 cid     vT

  vj

 cid 

    cid  

  

    cid  

log

 
Recall that         By separating the summation term
into two parts  each part has      term  we achieve equation  

            

log

 

     vT

  vj 

 cid 

 cid 

   cid 
   cid 
 cid 

  

log

 

   cid 
 cid 
 cid 

log

    cid  

 cid 

 
     vT

     vT

  vj 

 cid 

  vj 

 cid 

  cid 
  cid 

       
 

  cid 

  

    cid  

  

    

 cid 

 
Let             vn  and          vn        
be the matrix consisting of the  rst   and last   columns of
  respectively  We can obtain equation  

VT nV        

  ImF 
   ImF   ReF 

  ReF 
   

 

Spherical Structured Feature Maps for Kernel Approximation

Note that all diagonal elements of VT nV     are
zero  By further separating the  rst summation term of
equation   into two parts  we obtain equation  

 

log cid 
 cid 

  

     

  cid 
 cid 
   cid 
 cid 

   cid 
 cid 
 cid 
 cid 

            

   cid 

 
 

  

      cid    

log

 
     vT

log

 

  

    cid  

       
 

  

      cid    

  

    cid  

log

 

  cid 
  cid 
  cid 

  cid 
  cid 
  cid 

 cid 

  vj 

 cid 

log
 
     vT

  vj 

 cid 

     cid 
 cid 
 cid 

     vT

  vj 

 cid 

     vT

  vj 

 

 cid 

 cid 

 cid 

 cid 

To be concise  let             zn     
    
For               cid     we achieve equation  

 vT

  vj     Rez 

  zj   

 
  Re

  iksp  

 
For                      cid         we attain equation  

 vT

  vj     Imz 

  zj     

 
  Im

  iksp  

 
 mod    where

In equation   and            
mod denotes the modulus operation on integers 
Note that   
 
equation  

  zj has at most       distinct values when    cid 
 mod      Together with equation   we achieve
 cid 

 cid 

log

 

     vT

  vj 

   cid 

  cid 

  

  cid 

  

 cid 
 cid 

      cid    
     vT

 

 cid 

  vj 

 cid 

log
 
     Rez 

 cid 
 cid 
 cid 

      cid    

  

    cid  

log

 

  

  cid 
 cid 
   cid 
 cid 
 cid 
 cid 
 cid 

            
 

  

    cid  

log

  cid 
  cid 

       
 

  

  cid 
  cid 

  

  

  

  

  cid 
  cid 
  cid 
  cid 
 cid 
  cid 
 cid 

  

  

log

log

 

     Re  

 

        

log

     Im  

 

  cid 

  

     Re  

 

  iksp   

     Imz 

  zj   

 cid 

 cid 

  zj 

  cid 

  

 cid 
 cid 

 

  cid 
  cid 

  

  

  iksp   

  iksp   

 cid 

        

log

 

     Im  

  iksp   

Algorithm  

Initialization  random sample              km  from

           without replacement  Set cid          

repeat

Set        ikq      ikq      ikq     

    arg max
kq   
        ik 

  kq  in  
        ik 

       

Set       
for       to   do

Set    cid      
Set cid          

Find   
Update        ik 

  by   

end for

until   does not change

From Theorem   we know that minimizing      is
equivalent to maximizing    which is de ned in equation  

 cid 

  cid 
 cid 

  

    

log

  cid 

  

     Im  

 

  cid 

  

 

log

     Re  

 

  iksp   

 cid 

  cid 

  

  iksp   

 cid 

 

By keeping all the indexes in              km   xed
except the qth element  we can obtain equation  

 cid 

  cid 
 cid 
     Re cid hp     ikqp   cid     

 cid 
     Im cid hp     ikqp   cid     
 cid 

log

  

  kq   

  cid 

 

log

  

where kq              hp  

  cid 

    cid  

 

  iksp   

 cid 

With equation   we can maximize    by maximizing   kq  with other indexes  xed each time  Let    
      hn           ikq      ikq      ikq     
             Rm is the vector of all ones    coordinate
ascent method to maximize    is given in Algorithm  
Obviously  it is   discrete optimization problem  Algorithm   can  nd   local optimum  The time complexity of
the Algorithm   is     mn  where   denotes the number
of outer iteration  Empirically  the outer iteration   is less
than ten 

  Fast Feature Maps Construction
In this section  we will discuss how to construct SSF maps
in loglinear time complexity and linear space complexity
by using the structure property of   
Theorem   Assume that                        Let

 
 cid 

Spherical Structured Feature Maps for Kernel Approximation

culant  Choromanski   Sindhwani    matrices  QMC
with Halton set and QMC with Sobol set  Avron et al 
  For Halton set and Sobol set  the implementation
in MATLAB are employed in the experiments  The scrambling and shifting techniques are used for Haltonset and
Sobolset  In all the experiments  we           the number of onedimensional QMC points  for SSF maps 

Figure   Convergence of the Logarithmic Energy

 cid 

 

 cid 

 cid    

 cid 

  

      and          ix    Cm  Given
   
             km                let     Cn with
        Other elements outside the index set   are equal
to zero  Given   de ned in equation   equation  
holds 

VT      

   Re       Im       

 
Proof  Let     Rn   be   diagonal matrix with all diagonal elements inside the index set   equal to one   the others
equal to zero 

 cid   cid    

 cid 

         ImF 
         ReF 

  
     
     

VT      
 

 cid  ReF   ImF 
 cid   ReF 
 cid 
 cid  Re    
 cid  Re      
 cid  Re      
 cid 

ImF  ReF 
 ImF 
   
Im    
   
Im      
Im      

 cid 

   
 

   
 
   
 
   
 

Thus  the projection operation VT    previously mentioned
in equation   and   can be calculated by Fast Fourier
Transform algorithm  FFT  in     log    time complexity 
Because scaling and taking nonlinear transform can be  nished in      the total time complexity to construct SSF
maps is     log   
All steps to construct SSF maps are summarized as follows 

  diagonal matrix where diagonal elements are uniformly
sampled from    

    Compute cid   by cid     Dx  where            is
    Construct   such that     cid        cid    other elements
    Compute VT cid   by equation   via FFT 

outside the index set   are equal to zero 

    Construct feature maps       via equation   or  
For each        pair   the index set   only need to be computed once 
It takes      space to store   For shift
and rotation invariant kernels  it takes       space to store
 tj            and takes               space to
store   and    For bthorder arccosine kernels  it only
needs to store one parameter Cb and takes      space to
store   and    By setting        the total space complexity to store the projection matrix is     

  Empirical Studies
We compare SSF maps with feature maps obtained by fully
Gaussian  Cho   Saul    Rahimi et al    the Cir 

Figure   Speedup of the Feature Maps Construction

  Convergence and Speedup
First  the convergence of the logarithmic energy      
in equation   with              is shown in
Figure   From Figure   we  nd that it takes less than
ten iterations             for Algorithm   to  nd   local
optimum 
Second  the speedup results of all methods are shown in

Spherical Structured Feature Maps for Kernel Approximation

     cid cid     cid  

 cid   cid  

for Gaussian Kernel

     cid cid     cid  

 cid   cid  

for Zeroorder Arc Kernel

     cid cid     cid  

 cid   cid  

for Firstorder Arc Kernel

     cid cid     cid 

 cid   cid  for Gaussian Kernel

     cid cid     cid 

 cid   cid  for Zeroorder Arc Kernel

     cid cid     cid 

 cid   cid  for Firstorder Arc Kernel

Figure   Relative Mean and Max Reconstruction Error for Gaussian  Zeroorder and Firstorder Arccosine Kernel on MNIST

Figure   We set        for all the methods  The speedup
of fully Gaussian projection is the baseline  We can observe
that the speedup of QMC with Halton set is constant as the
dimension   increases and is slower than the baseline  The
speedup of both SSF maps and the Circulant increase fast
as dimension increases  which is consistent with theoretical
analysis  The speedup of Sobol set is not shown because
the inbuilt Sobolset routine of MATLAB does not support
dimension larger than  

  Approximation Accuracy

We evaluate reconstruction error of Gaussian kernel  zeroorder arccosine kernel and  rstorder arccosine kernel on
CIFAR   Krizhevsky   Hinton    MNIST  LeCun
  Cortes    usps and dna dataset  MNIST is   handwritten digit image dataset  which contains   samples
with  dimensional features pixel  For CIFAR  with
  samples  the  dimensional gist feature  Gong
et al    are employed in the experiments  Both the
relative Frobenius error      
  and the relative

 cid cid     cid  

 cid   cid  

 cid cid     cid 

 cid   cid 

elementwise maximum error      

ated  where   and  cid   denote the exact and approximated
 cid cid 
Gram matrices respectively  The Frobenius norm and
the elementwise maximum norm are de ned as  cid   cid    

   Xij  and  cid   cid    max

 Xij  respectively 

  are evalu 

 cid 

 

   

The reconstruction error in the experiments is the mean
value over   independent runs  The dimensions of the feature maps are set to                 where   is the
dimension of the data  For MNIST and CIFAR  dataset 
each run randomly select   samples to construct the
Gram matrix  The mean value of the reconstruction errors
with different norms on MNIST are shown in Figure   Results on the other datasets are similar to that of Figure  
One can refer to the supplementary material for results on
other datasets 
Figure   shows that the feature maps obtained with fully
Gaussian matrix  the Circulant matrix  QMC with Halton
set and QMC with Sobol set have similar reconstruction
error  SSF maps have the smallest approximation error
among  ve methods  Especially for the  rstorder arccosine kernel  it achieves nearly one fth relative mean error and oneseventh relative max error of other methods 
Moreover  even if       SSF maps can achieve about
onethird relative mean error and half of the relative max
error of other methods for Gaussian Kernel approximation 

  Conclusion
We propose Spherical Structured Feature  SSF  maps to approximate shift and rotation invariant kernels as well as bthorder arccosine kernels  SSF maps can achieve computation and storage ef ciency as well as better approximation
accuracy 

Spherical Structured Feature Maps for Kernel Approximation

Acknowledgements
We thank the anonymous reviewers for their valuable comments and suggestions 

References
An  Senjian  Boussaid  Farid  and Bennamoun  Mohammed  How can deep recti er networks achieve linear
separability and preserve distances  In ICML  pp   
   

Avron  Haim  Sindhwani  Vikas  Yang  Jiyan  and Mahoney  Michael    Quasimonte carlo feature maps for
shiftinvariant kernels  Journal of Machine Learning Research     

Brauchart     Saff     Sloan     and Womersley     Qmc
designs  optimal order quasi monte carlo integration
schemes on the sphere  Mathematics of computation   
   

Brauchart  Johann   and Grabner  Peter    Distributing
many points on spheres  minimal energy and designs 
Journal of Complexity     

Chang  ChihChung and Lin  ChihJen  Libsvm    library
for support vector machines  ACM Transactions on Intelligent Systems and Technology  TIST     

Cho  Youngmin and Saul  Lawrence    Kernel methods
In Advances in neural information

for deep learning 
processing systems  pp     

Choromanski  Krzysztof and Sindhwani  Vikas  Recycling
randomness with structure for sublinear time kernel expansions   

Cutajar  Kurt  Bonilla  Edwin    Michiardi  Pietro  and Filippone  Maurizio  Practical learning of deep gaussian
processes via random fourier features  arXiv preprint
arXiv   

Dick  Josef  Kuo  Frances    and Sloan  Ian    Highthe quasimonte carlo way 

dimensional integration 
Acta Numerica     

Fan  RongEn  Chang  KaiWei  Hsieh  ChoJui  Wang 
XiangRui  and Lin  ChihJen  Liblinear    library for
large linear classi cation  Journal of machine learning
research   Aug   

Feng  Chang  Hu  Qinghua  and Liao  Shizhong  Random
feature mapping with signed circulant matrix projection 
In IJCAI  pp     

Gong  Yunchao  Lazebnik  Svetlana  Gordo  Albert  and
Perronnin  Florent  Iterative quantization    procrustean

approach to learning binary codes for largescale image
IEEE Transactions on Pattern Analysis and
retrieval 
Machine Intelligence     

  otz  Mario  On the riesz energy of measures  Journal of

Approximation Theory     

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple

layers of features from tiny images   

Le  Quoc  Sarl os  Tam as  and Smola  Alex  Fastfoodapproximating kernel expansions in loglinear time 
In
Proceedings of the international conference on machine
learning   

Le  Zichao Yang Alexander   Smola and Wilson  Song Andrew Gordon    la cartelearning fast kernels     

LeCun  Yann and Cortes  Corinna  MNIST handwritten
digit database    URL http yann lecun 
com exdb mnist 

Oliva  Junier    Dubey  Avinava  Poczos  Barnabas 
Schneider  Jeff  and Xing  Eric    Bayesian nonparametric kernellearning  In Proceedings of the  th International Conference on Arti cial Intelligence and Statistics  pp     

Rahimi  Ali and Recht  Benjamin  Weighted sums of random kitchen sinks  Replacing minimization with randomization in learning  In Advances in neural information processing systems  pp     

Rahimi  Ali  Recht  Benjamin  et al  Random features for
largescale kernel machines  In NIPS  volume   pp   
 

Rasmussen  Carl Edward  Gaussian processes for machine

learning   

Rudin  Walter  Fourier analysis on groups  John Wiley  

Sons   

Snoek  Jasper  Larochelle  Hugo  and Adams  Ryan   
Practical bayesian optimization of machine learning algorithms  In Advances in neural information processing
systems  pp     

Srinivas  Niranjan  Krause  Andreas  Kakade  Sham    and
Seeger  Matthias  Gaussian process optimization in the
bandit setting  No regret and experimental design  arXiv
preprint arXiv   

Sutherland  Dougal   and Schneider  Jeff 

On the
arXiv preprint

error of
random fourier
arXiv   

features 

Yang     Sindhwani     Avron    Mahoney    Quasimonte carlo feature maps for shiftinvariant kernels 
ICML     

