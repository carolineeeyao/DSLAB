Towards Kmeans friendly Spaces  Simultaneous Deep

Learning and Clustering

Bo Yang   Xiao Fu   Nicholas    Sidiropoulos   Mingyi Hong  

Abstract

Most learning approaches treat dimensionality
reduction  DR  and clustering separately       sequentially  but recent research has shown that
optimizing the two tasks jointly can substantially
improve the performance of both  The premise
behind the latter genre is that the data samples are
obtained via linear transformation of latent representations that are easy to cluster  but in practice 
the transformation from the latent space to the
data can be more complicated  In this work  we
assume that this transformation is an unknown
and possibly nonlinear function  To recover the
 clusteringfriendly  latent representations and to
better cluster the data  we propose   joint DR and
Kmeans clustering approach in which DR is accomplished via learning   deep neural network
 DNN  The motivation is to keep the advantages
of jointly optimizing the two tasks  while exploiting the deep neural network   ability to approximate any nonlinear function  This way  the proposed approach can work well for   broad class
of generative models  Towards this end  we carefully design the DNN structure and the associated joint optimization criterion  and propose an
effective and scalable algorithm to handle the formulated optimization problem  Experiments using different real datasets are employed to showcase the effectiveness of the proposed approach 

 Department of Electrical and Computer Engineering  University of Minnesota  Minneapolis MN   USA   Department
of Industrial and Manufacturing Systems Engineering 
Iowa
State University  Ames  IA   USA  Correspondence to 
Bo Yang  yang umn edu  Xiao Fu  xfu umn edu 
Nicholas    Sidiropoulos  nikos ece um edu  Mingyi Hong
 mingyi iastate edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Introduction
Clustering is one of the most fundamental tasks in data
mining and machine learning  with an endless list of applications  It is also   notoriously hard task  whose outcome
is affected by   number of factors   including data acquisition and representation  use of preprocessing such as dimensionality reduction  DR  the choice of clustering criterion and optimization algorithm  and initialization  Ertoz
et al    Banerjee et al    Since its introduction
in   by Lloyd  published much later in    Lloyd 
  Kmeans has been extensively used either alone or
together with suitable preprocessing  due to its simplicity
and effectiveness  Kmeans is suitable for clustering data
samples that are evenly spread around some centroids  cf 
the  rst sub gure in Fig    but many reallife datasets do
not exhibit this  Kmeans friendly  structure  Much effort
has been spent on mapping highdimensional data to   certain space that is suitable for performing Kmeans  Various
techniques  including principal component analysis  PCA 
canonical correlation analysis  CCA  nonnegative matrix
factorization  NMF  and sparse coding  dictionary learning  were adopted for this purpose  In addition to these
linear DR operators         projection matrix  nonlinear
DR techniques such as those used in spectral clustering  Ng
et al    and sparse subspace clustering  Elhamifar  
Vidal    You et al    have also been considered 
In recent years  motivated by the success of deep neural networks  DNNs  in supervised learning  unsupervised
deep learning approaches are now widely used for DR prior
to clustering  For example  the stacked autoencoder  SAE 
 Vincent et al    deep CCA  DCCA   Andrew et al 
  and sparse autoencoder  Ng    take insights
from PCA  CCA  and sparse coding  respectively  and make
use of DNNs to learn nonlinear mappings from the data domain to lowdimensional latent spaces  These approaches
treat their DNNs as   preprocessing stage that is separately
designed from the subsequent clustering stage  The hope is
that the latent representations of the data learned by these
DNNs will be naturally suitable for clustering  However 
since no clusteringpromoting objective is explicitly incorporated in the learning process  the learned DNNs do not
necessarily output reduceddimension data that are suitable

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

for clustering   as will be seen in our experiments 
In  De Soete   Carroll    Patel et al    Yang et al 
  joint DR and clustering was considered  The rationale behind this line of work is that if there exists some
latent space where the entities nicely fall into clusters  then
it is natural to seek   DR transformation that reveals such
structure       which yields   low Kmeans clustering cost 
This motivates using the Kmeans cost in latent space as
  prior that helps choose the right DR  and pushes DR
towards producing Kmeans friendly representations  By
performing joint DR and Kmeans clustering  impressive
clustering results have been observed in  Yang et al   
The limitation of these works is that the observable data is
assumed to be generated from the latent clusteringfriendly
space via simple linear transformation  While simple linear
transformation works well in many cases  there are other
cases where the generative process is more complex  involving   nonlinear mapping 
Contributions In this work  we propose   joint DR and Kmeans clustering framework  where the DR part is implemented through learning   DNN  rather than   linear model 
Unlike previous attempts that utilize this joint DNN and
clustering idea  we made customized design for this unsupervised task  Although implementing this idea is highly
nontrivial  much more challenging than  De Soete   Carroll    Patel et al    Yang et al    where the
DR part only needs to learn   linear model  our objective is
wellmotivated  by better modeling the data transformation
process with   more general model    much more Kmeans 
friendly latent space can be learned   as we will demonstrate    sneak peek of the kind of performance that can be
expected using our proposed method can be seen in Fig   
where we generate four clusters of    data which are well
separated in the    Euclidean space and then transform
them to      space using   complex nonlinear mapping  cf    which destroys the cluster structure  One can
see that the proposed algorithm outputs reduceddimension
data that are most suitable for applying Kmeans  Our speci   contributions are as follows 
  Optimization Criterion Design  We propose an optimization criterion for joint DNNbased DR and Kmeans
clustering  The criterion is   combination of three parts 
namely  dimensionality reduction  data reconstruction  and
cluster structurepromoting regularization  We deliberately
include the reconstruction part and implement it using  
decoding network  which is crucial for avoiding trivial solutions  The criterion is also  exible   it can be extended to
incorporate different DNN structures       convolutional
neural networks  LeCun et al    Krizhevsky et al 
  and clustering criteria       subspace clustering 
  Effective and Scalable Optimization Procedure  The
formulated optimization problem is very challenging to

Figure   The learned    reduceddimension data by different
methods  The observable data is in the    space and is generated from    data  cf  the  rst sub gure  through the nonlinear
transformation in   The true cluster labels are indicated using
different colors 

handle  since it involves layers of nonlinear activation functions and integer constraints that are induced by the Kmeans part  We propose   judiciously designed solution
package  including empirically effective initialization and
  novel alternating stochastic gradient algorithm  The algorithmic structure is simple  enables online implementation 
and is very scalable 
  Comprehensive Experiments and Validation  We provide   set of syntheticdata experiments and validate the
method on different real datasets including various document and image copora  Evidently visible improvement
from the respective stateof art is observed for all the
datasets that we experimented with 
  Reproducibility  The code for the experiments is available at https github com boyangumn DCN 
  Background and Related Works
Given   set of data samples  xi     where xi   RM 
the task of clustering is to group the   data samples into  
categories  Arguably  Kmeans  Lloyd    is the most
widely adopted algorithm  Kmeans approaches this task
by optimizing the following cost function 

min

  RM    si RK 

kxi     sik 

NXi 
     sj            si           

 

 

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

where si is the assignment vector of data point   which has
only one nonzero element  sj   denotes the jth element of
si  and the kth column of         mk  denotes the centroid
of the kth cluster 
Kmeans works well when the data samples are evenly scattered around their centroids in the feature space  we consider datasets which have this structure as being  Kmeans 
friendly   cf  topleft sub gure of Fig    However  highdimensional data are in general not very Kmeans friendly 
In practice  using   DR preprocessing       PCA or NMF
 Xu et al    Cai et al    to reduce the dimension
of xi to   much lower dimensional space and then apply Kmeans usually gives better results  In addition to the above
classic DR methods that essentially learn   linear generative model from the latent space to the data domain  nonlinear DR approaches such as those used in spectral clustering
 Ng et al    Von Luxburg    and DNNbased DR
 Hinton   Salakhutdinov    Schroff et al    Hershey et al    are also widely used as preprocessing before Kmeans or other clustering algorithms  see also  Vincent et al    Bruna   Mallat   
Instead of using DR as   preprocessing  joint DR and clustering was also considered in the literature  De Soete  
Carroll    Patel et al    Yang et al    This
line of work can be summarized as follows  Consider
the generative model where   data sample is generated by
xi     hi  where     RM   and hi   RR  where
       Assume that the data clusters are wellseparated
in latent domain       where hi lives  but distorted by the
transformation introduced by     Reference  Yang et al 
  formulated the joint optimization problem as follows 

min

   si      kX     Hk 

     

              

NXi 

khi     sik 

 

 

     sj            si           

where                 xN                   hN   and    
  is   parameter for balancing data  delity and the latent
cluster structure  In   the  rst term performs DR and the
second term performs latent clustering  The terms    and
   are regularizations       nonnegativity or sparsity  to
prevent trivial solutions               RR    see details
in  Yang et al   
The data model         in the above line of work
may be oversimpli ed  The data generating process can be
much more complex than this linear transform  Therefore 
it is well justi ed to seek powerful nonlinear transforms 
     DNNs  to model this data generating process  while
at the same time make use of the joint DR and clustering
idea  Two recent works   Xie et al    and  Yang et al 

Input

Clustering 
module

Latent 
features

Figure     problematic joint deep clustering structure  To avoid
clutter  some links are omitted 
  made such attempts 
The idea of  Xie et al    and  Yang et al    is to
connect   clustering module to the output layer of   DNN 
and jointly learn DNN parameters and clusters  Speci 
cally  the approaches look into an optimization problem of
the following form

NXi 

     xi     

 

min

   bL  

where    xi    is the network output given data sample
xi    collects the network parameters  and   denotes parameters of some clustering model  For instance    stands
for the centroids   and assignments  si  if the Kmeans
clustering formulation   is adopted  The    in   denotes some clustering loss       the KullbackLeibler  KL 
divergence loss in  Xie et al    and agglomerative
clustering loss in  Yang et al    An illustration of
this kind of approaches is shown in Fig    This idea seems
reasonable  but is problematic    global optimal solution
to Problem   is    xi        and the optimal objective value bL     can always be achieved  Another type
of trivial solutions are simply mapping arbitrary data samples to tight clusters  which will lead to   small value of
bL   but this could be far from being desired since there is

no provision for respecting the data samples xi    see the
bottommiddle sub gure in Fig     Deep Clustering Network  DCN      reconstruction  and the bottomleft sub 
 gure in Fig     DEC  This issue also exists in  Yang et al 
 
  Proposed Formulation
We are motivated to model the relationship between the observable data xi and its clusteringfriendly latent representation hi using   nonlinear mapping      

hi      xi   

        RM   RR 

where       denotes the mapping function and   denote the set of parameters 
In this work  we propose to
employ   DNN as our mapping function  since DNNs have
the ability of approximating any continuous mapping using
  reasonable number of parameters  Hornik et al   
We want to learn the DNN and perform clustering simultaneously  The critical question here is how to avoid trivial solutions in this unsupervised task  In fact  this can be

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

min
    
   si 

NXi        xi  xi   

 
 
  kf  xi      sik 

resolved by taking insights from   The key to prevent
trivial solution in the linear DR case lies in the reconstruction part       the term kX     Hk 
  in   This term
ensures that the learned hi   can  approximately  reconstruct the xi   using the basis     This motivates incorporating   reconstruction term in the joint DNNbased DR
and Kmeans 
In the realm of unsupervised DNN  there
are several welldeveloped approaches for reconstruction  
     the stacked autoencoder  SAE  is   popular choice for
serving this purpose  To prevent trivial lowdimensional
representations such as allzero vectors  SAE uses   decoding network      to map the hi   back to the data domain and requires that   hi    and xi match each other
well under some metric       mutual information or least
squaresbased measures 
By the above reasoning  we come up with the following
cost function 

 

     sj            si           

where we have simpli ed the notation    xi    and
  hi    to    xi  and   hi  respectively  for conciseness 
The function     RM     is   certain loss function that
measures the reconstruction error  In this work  we adopt
the leastsquares loss          kx   yk 
  other choices
such as  norm based  tting and the KL divergence can
also be considered        is   regularization parameter
which balances the reconstruction error versus  nding Kmeans friendly latent representations 
Fig    presents the network structure corresponding to the
formulation in   Compare to the network in Fig    our
latent features are also responsible for reconstructing the
input  preventing all the aforementioned trivial solutions 
On the lefthand side of the  bottleneck  layer are the socalled encoding or forward layers that transform raw data
to   lowdimensional space  On the righthand side are the
 decoding  layers that try to reconstruct the data from the
latent space  The Kmeans task is performed at the bottleneck layer  The forward network  the decoding network 
and the Kmeans cost are optimized simultaneously  In our
experiments  the structure of the decoding networks is  
 mirrored version  of the encoding network  and for both
the encoding and decoding networks  we use the recti ed
linear unit  ReLU  activationbased neurons  Nair   Hinton    Since our objective is to perform DNNdriven
Kmeans clustering  we will refer to the network in Fig   
as the Deep Clustering Network  DCN  in the sequel 
We should remark that the proposed optimization criterion
in   and the network in Fig    are very  exible  Other
types of networks       deep convolutional neural networks

Input

Reconstruction

Latent 
features

Clustering 
module

Figure   Proposed deep clustering network  DCN 

 LeCun et al    Krizhevsky et al    can be used 
For the clustering part  other clustering criteria       Ksubspace and soft Kmeans  Law et al    Banerjee
et al    are also viable options  Nevertheless  we will
concentrate on the proposed DCN in the sequel  as our interest is to provide   proofof concept rather than exhausting the possibilities of combinations 
  Optimization Procedure
Optimizing   is highly nontrivial since both the cost
function and the constraints are nonconvex  In addition 
there are scalability issues that need to be taken into account  In this section  we propose   pragmatic optimization
procedure including an empirically effective initialization
method and an alternating optimization based algorithm for
handling  
  Initialization via Layerwise PreTraining
For dealing with hard nonconvex optimization problems
like that in   initialization is usually crucial  To initialize the parameters of the network             we use the
layerwise pretraining method as in  Bengio et al   
for training autoencoders  This pretraining technique may
be avoided in largescale supervised learning tasks  For
the proposed DCN which is completely unsupervised  however  we  nd that the layerwise pretraining procedure is
important no matter the size of the dataset  We refer the
readers to  Bengio et al    for an introduction of layerwise pretraining  After pretraining  we perform Kmeans
to the outputs of the bottleneck layer to obtain initial values
of   and  si 
  Alternating Stochastic Optimization
Even with   good initialization  handling Problem   is
still very challenging  The commonly used stochastic gradient descent  SGD  algorithm cannot be directly applied
to jointly optimize        and  si  because the block
variable  si  is constrained on   discrete set  Our idea is to
combine the insights of alternating optimization and SGD 
Speci cally  we propose to optimize the subproblems with
respect to         one of     si  and       while keeping the other two sets of variables  xed 

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

  UPDATE NETWORK PARAMETERS
For  xed     si  the subproblem              is similar to training an SAE   but with an additional penalty term
on the clustering performance  We can take advantage of
the mature tools for training DNNs       backpropagation
based SGD and its variants  To implement SGD for updating the network parameters  we look at the problem       
the incoming data xi 

Li           xi  xi   

min
   

 
  kf  xi      sik 
   

 

  

The gradient of the above function over the network parameters is easily computable       OX Li         xi xi 
 
      xi 
    xi      si  where           is   collecand
tion of the network parameters and the gradients  
  
    xi 
can be calculated by backpropagation  Rumelhart
  
et al     strictly speaking  what we calculate here is
the subgradient          since the ReLU function is nondifferentible at zero  Then  the network parameters are
updated by

  

      OX Li 

 

where     is   diminishing learning rate 

  UPDATE CLUSTERING PARAMETERS
For  xed network parameters and    the assignment vector of the current sample       si  can be naturally updated
in an online fashion  Speci cally  we update si as follows 

 

 

if     arg min

    kf  xi    mkk   

otherwise 

 

sj     

 

   xi  where Ci

  Pi Ci

When  xing  si  and     the update of   is simple and
may be done in   variety of ways  For example  one can
simply use mk    Ci
  is the
recorded index set of samples assigned to cluster   from the
 rst sample to the current sample    Although the above
update is intuitive  it could be problematic for online algorithms  since the already appeared historical data      
           xi  might not be representative enough to model
the global cluster structure and the initial si   might be far
away from being correct  Therefore  simply averaging the
current assigned samples may cause numerical problems 
Instead of doing the above  we employ the idea in  Sculley    to adaptively change the learning rate of updating            mK  The intuition is simple  assume that the
clusters are roughly balanced in terms of the number of
data samples they contain  Then  after updating   for  
number of samples  one should update the centroids of the
clusters that already have many assigned members more

gracefully while updating others more aggressively  to keep
  be the count of the numbalance  To implement this  let ci
ber of times the algorithm assigned   sample to cluster  
before handling the incoming sample xi  and update mk
by   simple gradient step 

mk   mk    ci

    mk      xi  sk   

 

where the gradient step size  ci
  controls the learning rate 
The above update of   can also be viewed as an SGD
step  thereby resulting in an overall alternating block SGD
procedure that is summarized in Algorithm   Note that an
epoch corresponds to   pass of all data samples through the
network 

Algorithm   Alternating SGD
  Initialization  Perform   epochs over the data 
  for           do
 
 
 
  end for

Update network parameters by  
Update assignment by  
Update centroids by  

Algorithm   has many favorable properties  First  it can
be implemented in   completely online fashion  and thus
is very scalable  Second  many known tricks for enhancing performance of DNN training can be directly used  In
fact  we have used   minibatch version of SGD and batchnormalization  Ioffe   Szegedy    in our experiments 
which indeed help improve performance 
  Experiments
In this section  we use synthetic and realworld data to
showcase the effectiveness of DCN  We implement DCN
using the deep learning toolbox Theano  Theano Development Team   
  SyntheticData Demonstration
Our settings are as follows  Assume that the data points
have Kmeans friendly structure in   twodimensional domain  cf 
This twodimensional domain is   latent domain which we do not
observe and we denote the latent representations of the
data points as hi   in this domain  What we observe is
xi      that is obtained via the following transformation 

the  rst sub gure of Fig   

xi           hi   

 
where        and        are matrices whose
entries follow the zeromean unitvariance        Gaussian
distribution    is   sigmod function to introduce nonlinearity  Under the above generative model  recovering the
Kmeans friendly domain where hi   live seems very challenging 

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

We generate four clusters  each of which has   samples
and their geometric distribution on the    plane is shown
in the  rst sub gure of Fig    that we have seen before  The
other sub gures show the recovered    data from xi   using   number of DR methods  namely  NMF  Lee   Seung    local linear embedding  LLE   Saul   Roweis 
  Laplacian eigenmap  LapEig   Ng et al      the
 rst step of spectral clustering  and DEC  Xie et al   
We also present the result of using the formulation in  
 DCN     reconstruction  which is   similar idea as in  Xie
et al    For the three DNNbased methods  DCN 
DEC  and SAE   KM  we use   fourlayer forward network for dimensionality reduction  where the layers have
      and   neurons  respectively  the reconstruction
network used in DCN and SAE  and also in the pertraining
stage of DEC  is   mirrored version of the forward network  As one can see in Fig    all the DR methods except
the proposed DCN fail to map xi   to      domain that
is suitable for applying Kmeans  In particular  DEC and
DCN     reconstruction indeed give trivial solutions  the
reduceddimension data are separated to four clusters  and
thus    is small  But this solution is meaningless since the
data partitioning is arbitrary 
In the supplementary materials  two additional simulations
with different generative model than   are presented  and
similar results are observed  This further illustrates the
DCN   ability of recovering clusteringfriendly structure
under different nonlinear generative models 
  RealData Validation
In this section  we validate the proposed approach on several realdata sets which are all publicly available 
  BASELINE METHODS
We compare the proposed DCN with   variety of baseline
methods 
  Kmeans  KM  The classic Kmeans  Lloyd   
  Spectral Clustering  SC  The classic SC algorithm
 Ng et al   
  Sparse Subspace Clustering with Orthogonal Matching Pursuit  SSCOMP   You et al    SSC is
considered very competitive for clustering images  we use
the newly proposed greedy version here for scalability 
  Locally Consistent Concept Factorization  LCCF 
 Cai et al    LCCF is based on NMF with   graph
Laplacian regularization and is considered stateof theart
for document clustering 
  XRAY  Kumar et al    XRAY is an NMFbased
document clustering algorithm that scales very well 
  NMF followed by Kmeans  NMF KM  This approach applies NMF for DR  and then applies Kmeans to
the reduceddimension data 
  Stacked Autoencoder
followed by Kmeans
 SAE KM  This is also   twostage approach  We
use SAE for DR  rst and then apply Kmeans 

  Joint NMF and Kmeans  JNKM   Yang et al   
JNKM performs joint DR and Kmeans clustering as the
proposed DCN does   but the DR part is based on NMF 
  Deep Embedded Clustering  DEC   Xie et al   
DEC performs joint DNN and clustering  where the loss
function contains only clustering loss  without penalty on
reconstruction as in our method  We use the code  provided by the authors  For each experiment  we select the
baselines that are considered most competitive and suitable
for that application from the above pool 
  EVALUATION METRICS
We adopt standard metrics for evaluating clustering performance  Speci cally  we employ the following three metrics  normalized mutual information  NMI   Cai et al 
  adjusted Rand index  ARI   Yeung   Ruzzo   
and clustering accuracy  ACC   Cai et al    In   nutshell  all the above three measuring metrics are commonly
used in the clustering literature  and all have pros and cons 
But using them together suf ces to demonstrate the effectiveness of the clustering algorithms  Note that NMI and
ACC lie in the range of zero to one with one being the perfect clustering result and zero the worst  ARI is   value
within   to   with one being the best clustering performance and minus one the opposite 
  RCV 
We  rst test the algorithms on   largescale text corpus 
namely  the Reuters Corpus Volume   Version    RCV 
   The RCV    corpus  Lewis et al    contains
  documents  which were manually categorized into
  different topics  We use   subset of the documents
from the whole corpus  This subset contains   topics and
    documents and each document has   single topic
label  As in  Nitish et al    we pick the   most
frequently used words  in the tfidf form  as the features of
the documents 
We conduct experiments using different number of clusters  Towards this end  we  rst sort the clusters according
to the number of documents that they have in   descending
order  and then apply the algorithms to the  rst        
  clusters  respectively  Note that the  rst several clusters
have many more documents compared to the other clusters
 cf  Fig    This way  we gradually increase the number of
documents in our experiments and create cases with much
more unbalanced cluster sizes for testing the algorithms  
which means we gradually increase the dif culty of the experiments  To avoid unrealistic tuning  for all the experiments  we use   DCN whose forward network has  ve hidden layers which have           neurons 
respectively  The reconstruction network has   mirrored
structure  We set       for balancing the reconstruction

 https github com piiswrong dec

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

 

Table   Evaluation on the  Newsgroup dataset 

Methods DCN SAE KM LCCF NMF KM KM SC XARY JNKM
 
 
 

   
   
   

NMI
ARI
ACC

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 
imbalance clusters 

 

 

 

index of clusters

 

 

 

 

 

 

 

 
 
 

 
 
 
 
 
 

 

 

 
 
 
 

 

Figure   The sizes of   clusters in the experiment 

  Clust 

  Clust 

Methods

Table   Evaluation on the RCV    dataset
DCN SAE KM KM DEC XRAY
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

NMI
ARI
ACC
NMI
ARI
ACC
NMI
ARI
ACC
NMI
ARI
ACC
NMI
ARI
ACC

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

  Clust 

  Clust 

  Clust 

error and the clustering regularization 
Table   shows the results given by the proposed DCN 
SAE KM  KM  and XRAY  other baselines are not scalable enough to handle the RCV    dataset and thus are
dropped  One can see that for each case that we have tried 
the proposed method gives clear improvement relative to
the other methods  Particularly  the DCN approach outperforms the twostage approach       SAE KM  in almost all
the cases and for all the evaluation metrics   this clearly
demonstrates the advantage of using the joint optimization
criterion  We notice that the performance of DEC in this experiment is unsatisfactory  possibly because   this dataset
is highly unbalanced  cf  Fig    while DEC is designed
to produce balanced clusters    DEC gets trapped in trivial
solutions  as we discussed in Sec  
Fig     shows how NMI  ARI  and ACC change when the
proposed algorithm runs from epoch to epoch  One can see
  clear ascending trend of every evaluation metric  This
result shows that both the network structure and the optimization algorithm work towards   desired direction 
In
the future  it would be intriguing to derive  suf cient  conditions for guaranteeing such improvement using the proposed algorithm  Nevertheless  such empirical observation
in Fig     is already very interesting and encouraging 
We visualize the    learned embeddings of our network on the RCV   clusters dataset  using tSNE  Van der
Maaten   Hinton    as shown in Fig      We can see
that the proposed DCN method learns much improved results compared to the initialization  Also  the DEC method
does not get   desiable clustering result  possibly due to the

    Clustering
metrics      training epochs 

performance

    Visualization using tSNE 
From topleft
to bottomright 
Original data  DEC result  DCN
initialization  DCN result

Figure   Visualization on the  clusters subset of RCV   
   NEWSGROUP
The  Newsgroup corpus is   collection of   text
documents which are partitioned into   different newsgroups  Using this corpus  we can observe how the proposed method works with   relatively small amount of
samples  As the previous experiment  we use the tfidf
representation of the documents and pick the   most
frequently used words as the features  Since this dataset
is small  we include more baselines that are not scalable
enough for RCV    Among them  both JNKM and
LCCF are considered stateof art for document clustering 
In this experiment  we use   DNN with three forward layers which have     and   neurons  respectively  This
is   relatively  small network  since the  Newsgroup corpus may not have suf cient samples to      large network 
As before  the decoding network for reconstruction has  
mirrored structure of the encoding part  and the baseline
SAE KM uses the same network for the autoencoder part 
Table   summarizes the results of this experiment  As one
can see  LCCF indeed gives the best performance among
the algorithms that do not use DNNs  SAE KM improves
ARI and ACC quite substantially by involving DNN   this
suggests that the generative model may indeed be nonlinear  DCN performs even better by using the proposed
joint DR and clustering criterion  which supports our motivation that   Kmeans regularization can help discover  
clusteringfriendly space 
  RAW MNIST
In this and next subsections  we present two experiments
using two versions of the MNIST dataset  We  rst employ

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

Table   Evaluation on the raw MNIST dataset 

Table   Evaluation on preprocessed MNIST

Methods DCN SAE KM DEC KM SSCOMP
NMI
ARI
ACC

   
   
   

 
 
 

 
 
 

 
 
 

Methods DCN SAE KM KM  SSCOMP 
NMI
ARI
ACC

 
 
 

 
 
 

 
 
 

the raw MNIST dataset that has   data samples  Each
sample is         grayscale image containing   handwritten digit       one of               Same as  Xie et al 
  we use    layers forward network and set the number of neurons to be       and   respectively 
The reconstruction network is still    mirrored  version of
the forward network  The hyperparameter   is set to   We
use SSCOMP  which is   scalable version of SSC  and KM
as   baseline for this experiment 
Table   shows results of applying DCN  SAE KM  DEC 
KM and SSCOMP to the raw MNIST data   the other
baselines are not ef cient enough to handle   samples and thus are left out  One can see that our result is on
par with the result of DEC reported in  Xie et al   
and both methods outperform other methods by   large
margin  The DEC method performs very competitively on
this dataset  possibly because it is designed to favor balanced clusters  which is the case for MNIST dataset  On
the dataset RCV    with unbalanced clusters  the result
of DEC is not as satisfactory  see Fig      It is also interesting to note that our method yields approximately same
results as DEC in this balanced case  but DCN also works
well in unbalanced cases  as we have seen 
  PREPROCESSED MNIST
Besides the above experiment using the raw MNIST data 
we also provide another interesting experiment using preprocessed MNIST data  The preprocessing is done by  
recently introduced technique  namely  the scattering network  ScatNet   Bruna   Mallat    ScatNet is  
cascade of multiple layers of wavelet transform  which is
able to learn   good feature space for clustering   classi 
 cation of images  Utilizing ScatNet  the work in  You
et al    reported very promising clustering results on
MNIST using SSCOMP  Our objective here is to see if
the proposed DCN can further improve the performance
from SSCOMP  Our idea is simple  SSCOMP is essentially   procedure of constructing   similarity matrix of the
data  after obtaining this matrix  it performs Kmeans on
the rows of   matrix comprising several selected eigenvectors of the similarity matrix  Ng et al    Therefore  it
makes sense to treat the whole ScatNet   SSCOMP procedure as preprocessing for performing Kmeans  and one
can replace Kmeans by DCN to improve performance 
The results are shown in Table   One can see that the
proposed method exhibits the best performance among the
algorithms  We note that the result of using KM on the data
processed by ScatNet and SSCOMP is worse than that was

Figure   Clustering performance on MNIST with different  

reported in  You et al    This is possibly because we
use all the   samples  while only   subset was selected
for conducting the experiments in  You et al   
This experiment is particularly interesting since it suggests
that for any clustering algorithm that employs Kmeans as
  key component       spectral clustering and sparse subspace clustering  one can use the proposed DCN to replace Kmeans and   better result can be expected  This
is meaningful since many datasets are originally not suitable for Kmeans due to the nature of the data   but after preprocessing       kernelization and eigendecomposition  the preprocessed data is already more Kmeans 
friendly  and using the proposed DCN at this point can further strengthen the result 
  PARAMETER SELECTION
The parameter   is important  since it trades off between
the reconstruction objective and the clustering objective 
As we see from the experiments  the proposed DCN works
well with an appropriately chosen   Moreover  our experience suggests that the performance of our approach is
insensitive to the exact value of   Fig    shows how the
proposed method performs with different   on the MNIST
dataset  As we can see  although there is degradation of
performance as   gets inappropriately large  the degradation is mild  The proposed method gives satisfactory result
for   range of  
  Conclusion
In this work  we proposed   joint DR and Kmeans clustering approach where the DR part is accomplished via
learning   deep neural network  Our goal is to automatically map highdimensional data to   latent space where
Kmeans is   suitable tool for clustering  We carefully designed the network structure to avoid trivial and meaningless solutions and proposed an effective and scalable optimization procedure to handle the formulated challenging
problem  Synthetic and real data experiments showed that
the algorithm is very effective on   variety of datasets 

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

Acknowledgements
This work is supported by National Science Foundation under Projects NSF IIS  NSF ECCS  and
NSF CCF  The GPU used in this work was kindly
donated by NVIDIA 

References
Andrew     Arora     Bilmes     and Livescu     Deep
canonical correlation analysis 
In Proceedings of the
 th International Conference on International Conference on Machine Learning  pp     

Banerjee     Merugu     Dhillon        and Ghosh    
Clustering with bregman divergences  Journal of Machine Learning Research    Oct  

Bengio     Lamblin     Popovici     and Larochelle 
   Greedy layerwise training of deep networks 
In
Advances in Neural Information Processing Systems  
 NIPS   volume   pp     

Bruna     and Mallat    

Invariant scattering convolution
IEEE Transaction on Pattern Analysis Ma 

networks 
chine Intelligence     

Cai     He     and Han     Locally consistent concept
factorization for document clustering 
IEEE Transaction on Knowledge and Data Engineering   
   

De Soete     and Carroll        Kmeans clustering in  
lowdimensional euclidean space  In New Approaches in
Classi cation and Data Analysis  pp    Springer 
 

Elhamifar     and Vidal     Sparse subspace clustering 
Algorithm  theory  and applications  IEEE Transactions
on Pattern Analysis and Machine Intelligence   
   

Ertoz     Steinbach     and Kumar     Finding clusters
of different sizes  shapes  and densities in noisy  high
dimensional data  In Proceedings of Second SIAM International Conference on Data Mining  pp     

Hershey        Chen     Le Roux     and Watanabe    
Deep clustering  Discriminative embeddings for segmentation and separation  In Proceedings of   IEEE
International Conference on Acoustics  Speech and Signal Processing  pp    IEEE   

Hinton        and Salakhutdinov     Reducing the dimensionality of data with neural networks  Science   
   

Hornik     Stinchcombe     and White     Multilayer
feedforward networks are universal approximators  Neural Networks     

Ioffe     and Szegedy     Batch normalization  Accelerating deep network training by reducing internal covariate
shift  In Proceedings of the  nd International Conference on Machine Learning  pp     

Krizhevsky     Sutskever     and Hinton        Imagenet
classi cation with deep convolutional neural networks 
In Advances in Neural Infomation Processing Systems
   NIPS   pp     

Kumar     Sindhwani     and Kambadur     Fast conical
hull algorithms for nearseparable nonnegative matrix
factorization  In Proceedings of  th International Conference on Machine Learning  pp     

Law           Topchy     and Jain        Modelbased
clustering with probabilistic constraints 
In Proceedings of the   SIAM International Conference on Data
Mining  pp    SIAM   

LeCun     Bottou     Bengio     and Haffner     Gradientbased learning applied to document recognition  Proceedings of the IEEE     

Lee        and Seung        Learning the parts of objects by
nonnegative matrix factorization  Nature   
   

Lewis        Yang     Rose        and Li     RCV 
  new benchmark collection for text categorization research  Journal of Machine Learning Research   
  Apr  

Lloyd     Least squares quantization in PCM  IEEE Trans 

action on Information Theory     

Nair     and Hinton        Recti ed linear units improve
restricted boltzmann machines 
In Proceedings of the
 th International Conference on Machine Learning  pp 
   

Ng        Sparse autoencoder  CS   Lecture notes   

   

Ng        Jordan     and Weiss     On spectral clustering  Analysis and an algorithm 
In Advances in Neural Information Processing Systems    NIPS   pp 
   

Nitish     Hinton        Krizhevsky     Sutskever     and
Salakhutdinov     Dropout    simple way to prevent
neural networks from over tting  Journal of Machine
Learning Research     

Towards Kmeans friendly Spaces  Simultaneous Deep Learning and Clustering

Yeung        and Ruzzo        Details of the adjusted rand
index and clustering algorithms  supplement to the paper
an empirical study on principal component analysis for
clustering gene expression data  Bioinformatics   
   

You     Robinson     and Vidal     Scalable sparse subspace clustering by orthogonal matching pursuit  In Proceedings of Conference on Computer Vision and Pattern
Recognition  CVPR  volume    

Patel        Van Nguyen     and Vidal     Latent space
sparse subspace clustering  In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pp     

Rumelhart        Hinton        and Williams        Learning representations by backpropagating errors  Neurocomputing  foundations of research  pp     
Saul        and Roweis        Think globally     locally  unsupervised learning of low dimensional manifolds  Journal of Machine Learning Research     

Schroff     Kalenichenko     and Philbin     Facenet   
uni ed embedding for face recognition and clustering 
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp     

Sculley     Webscale kmeans clustering  In Proceedings
of the  th International Conference on World Wide Web
 WWW  pp    ACM   

Theano Development Team  Theano    Python framework
for fast computation of mathematical expressions  arXiv
eprints  abs  May   URL http 
arxiv org abs 

Van der Maaten     and Hinton        Visualizing data
using tsne  Journal of Machine Learning Research   
  Nov  

Vincent     Larochelle     Lajoie     Bengio     and Manzagol        Stacked denoising autoencoders  Learning
useful representations in   deep network with   local
denoising criterion  Journal of Machine Learning Research    Dec  

Von Luxburg       tutorial on spectral clustering  Statistics

and Computing     

Xie     Girshick     and Farhadi     Unsupervised deep
embedding for clustering analysis 
In Proceedings of
the  rd International Conference on Machine Learning   

Xu     Liu     and Gong     Document clustering based
on nonnegative matrix factorization  In Proceedings of
the  th annual international ACM SIGIR conference on
Research and development in informaion retrieval  pp 
  ACM   

Yang     Fu     and Sidiropoulos        Learning from
hidden traits  Joint factor analysis and latent clustering 
IEEE Transaction on Signal Processing  pp   
Jan   

Yang     Parikh     and Batra     Joint unsupervised learning of deep representations and image clusters  In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pp     

