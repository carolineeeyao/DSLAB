Sequence Tutor  Conservative FineTuning of Sequence Generation Models

with KLcontrol

Natasha Jaques     Shixiang Gu       Dzmitry Bahdanau     Jos   Miguel Hern andezLobato  

Richard    Turner   Douglas Eck  

Abstract

This paper proposes   general method for improving the structure and quality of sequences
generated by   recurrent neural network  RNN 
while maintaining information originally learned
from data  as well as sample diversity  An RNN
is  rst pretrained on data using maximum likelihood estimation  MLE  and the probability distribution over the next token in the sequence
learned by this model is treated as   prior policy  Another RNN is then trained using reinforcement learning  RL  to generate higherquality
outputs that account for domainspeci   incentives while retaining proximity to the prior policy of the MLE RNN  To formalize this objective  we derive novel offpolicy RL methods for
RNNs from KLcontrol  The effectiveness of the
approach is demonstrated on two applications   
generating novel musical melodies  and   computational molecular generation  For both problems  we show that the proposed method improves the desired properties and structure of the
generated sequences  while maintaining information learned from data 

  Introduction
The approach of training sequence generation models using
likelihood maximization suffers from known failure modes 
and it is notoriously dif cult to ensure multistep generated
sequences have coherent global structure  For example 
long shortterm memory  LSTM   Hochreiter   Schmidhuber    networks trained to predict the next character in sequences of text may produce text that has correct

 Google Brain  Mountain View  USA  Massachusetts Institute of Technology  Cambridge  USA  University of Cambridge 
Cambridge  UK  Max Planck Institute for Intelligent Systems 
Stuttgart  Germany  Universit   de Montr eal  Montr eal  Canada 
Correspondence to  Natasha Jaques  jaquesn mit edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

spelling  punctuation  and even   semblance of grammar 
but the generated text shifts so rapidly from topic to topic 
that it is almost completely nonsensical  see  Graves   
for an example  Similar networks trained to predict the
next note in   melody suffer from the same problem  the
generated music has no consistent theme or structure  and
appears wandering and random  In addition  these models
are prone to excessively repeating the same output token   
problem that has also been noted in the context of recurrent
dialog generation models  Li et al   
To ameliorate these problems we propose Sequence Tutor 
  novel approach which uses RL to impose structure on  
sequence generation RNN via taskspeci   rewards  while
simultaneously ensuring that information learned from data
is retained  This is accomplished by maintaining    xed
copy of   sequence generation RNN pretrained on data 
which is termed the Reward RNN  Rather than simply using the Reward RNN to supply part of the rewards to
our model  we derive novel offpolicy RL methods for sequence generation from KLcontrol that allow us to directly
penalize Kullback Leibler  KL  divergence from the policy
de ned by the Reward RNN  As   byproduct of minimizing
KL our objective includes an entropy regularization term
that encourages high entropy in the distribution of the RL
model  This is ideal for sequence generation tasks such as
text  music  or molecule generation  in which maintaining
diversity in the samples generated by the model is critical 
Sequence Tutor effectively combines both data and taskrelated goals  without relying on either as   perfect metric of task success  This is an important novel direction
of research  Much previous work on combining RL and
MLE has used MLE training simply as   way to bootstrap
the training of an RL model  Ranzato et al    Bahdanau et al    Li et al    since training with RL
from scratch is dif cult  However  this approach does not
encourage diversity of the generated samples  and can be
problematic when taskspeci   rewards are incomplete or
imperfect  Designing an appropriate reward de nition is
highly nontrivial  and often the handcrafted rewards cannot be fully trusted  Vedantam et al    Liu et al   
And yet  relying on data alone can be insuf cient when the
data itself contains biases  as has been shown for text data

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

 CaliskanIslam et al    or when domainspeci   constraints cannot be encoded directly into MLE training  By
learning   policy that trades off staying close to the data distribution while improving performance on speci   metrics 
Sequence Tutor reduces both of these problems 
This paper contributes to the sequence training and RL literature by    proposing   novel method for combining MLE
and RL training     showing the connection between KL
control and sequence generation     deriving the explicit
relationships among   generalized variant of  learning
 Rawlik et al    Glearning  Fox et al    and
Qlearning with log prior augmentation  and being the  rst
to empirically compare these methods and use them with
deep neural networks 
We explore the usefulness of our approach for two sequence generation applications  The  rst  music generation  is   dif cult problem in which the aesthetic beauty of
generated sequences cannot be fully captured in   known
reward function  but in which models trained purely on
data cannot produce wellstructured sequences  Through
an empirical study  we show that by imposing rules of music theory on   melody generation model  Sequence Tutor
is able to produce melodies which are varied  yet more harmonious  interesting  and rated as signi cantly more subjectively pleasing than those of the MLE model  Further 
Sequence Tutor is able to signi cantly reduce unwanted behaviors and failure modes of the original RNN  The effectiveness of Sequence Tutor is also demonstrated for computational molecular generation    task in which the goal is
to generate novel druglike molecules with desirable properties by outputting   string representation of the molecule
encoding  However  generating valid molecules can prove
dif cult  as it is hard for probabilistic models to learn all
the constraints that de ne physically realizable molecules
directly from data    omezBombarelli et al    We
show that Sequence Tutor is able to yield   higher percentage of valid molecules than the baseline MLE RNN  and
the generated molecules score higher on metrics of druglikeness and ease of synthesis 

  Related Work
Recent work has attempted to use both MLE and RL in
the context of structured prediction  While the attempts
were successful  the problems of maintaining information
about the data distribution and diversity in the generated
samples were not addressed  MIXER  Mixed Incremental CrossEntropy Reinforce 
 Ranzato et al    uses
BLEU score as   reward signal to gradually introduce   RL
loss to   text translation model  Bahdanau et al    applies an actorcritic method and uses BLEU score directly
to train   critic network to output the value of each word 
where the actor is again initialized with the policy of an

augmented maximum likelihood

RNN trained with nextstep prediction  Li et al    use
RL to improve   pretrained dialog model with heuristic
rewards  These approaches assume that the complete task
reward speci cation is available  They pretrain   good
policy with supervised learning so that RL can be used
to learn the true task objective  since it can be dif cult to
reach convergence when training with pure RL  However 
the original MLE policy of these models is overwritten by
the RL training process  In contrast  Sequence Tutor uses
rewards to correct certain properties of the generated data 
while learning most information from data and maintaining
this information  an important ability when the true reward
function is not available or imperfect 
Reward
 RAML 
 Norouzi et al    is an approach designed to improve
MLE training of   translation model by augmenting the
ground truth targets with additional outputs that are within
  small edit distance  and performing MLE training against
those as well  The authors show that their approach is
equivalent to minimizing KLdivergence between an RL
exponentiated payoff distribution based on edit distance 
and the MLE distribution  In contrast  our goal is generation rather than prediction  and we train an RL rather than
MLE model  The RAML approach  while an important
contribution  is only viable if it is possible to generate
additional MLE training samples that are similar in terms
of the reward function to the ground truth       samples
within   small edit distance  However in some domains 
including the two explored in this paper  generating similar
samples with high reward is not only not possible  but in
fact constitutes the entire problem under investigation 
Finally  our approach is related to KL control  Todorov 
  Kappen et al    Rawlik et al      branch
of stochastic optimal control  SOC   Stengel    There
is also   connection between this work and Maximum Entropy Inverse RL  Ziebart et al    which can be seen
as KL control with    at  improper prior  From KL control 
we take inspiration from two offpolicy  modelfree methods   learning  Rawlik et al    and Glearning  Fox
et al    Both approaches are derived from   KLregularized RL objective  where an agent maximizes the
reward while incurring additional penalty for divergence
from some prior policy  While our methods rely on similar derivations presented in these papers  our methods have
different motivations and forms from the original papers 
The original  learning  Rawlik et al    restricts the
prior policy to be the policy at the previous iteration and
solves the original RL objective with conservative  KLregularized policy updates  similar to conservative policy
gradient methods  Kakade    Peters et al    Schulman et al    The original Glearning  Fox et al   
penalizes divergence from   simple uniform prior policy
in order to cope with overestimation of target   values 

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

These techniques have not been applied to deep learning
techniques or with RNNs  or as   way to improve   pretrained MLE model  Our work is the  rst to explore these
methods in such   context  and includes   Qlearning model
with additional crossentropy reward as   comparable alternative  To the best of our knowledge  our work is the  rst
to provide comparisons among these three approaches 
There has also been prior work in the domain of generative modeling of music  Using RNNs for this purpose has
been explored in   variety of contexts  including generating Celtic folk music  Sturm et al    or improvising
the blues  Eck   Schmidhuber    Often  this involves
training the RNN to predict the next note in   monophonic
melody  however  as mentioned above  the melodies generated by this model tend to wander and lack musical structure  Some authors have experimented with encoding musical structure into   hierarchical RNN with layers dedicated
to generated the melody  drums  and chords  Chu et al 
  Other approaches have examined RNNs with richer
expressivity  latentvariables for notes  or raw audio synthesis  BoulangerLewandowski et al    Gu et al   
Chung et al    Recently  Wavenet produced impressive performance in generating music from raw audio using
convolutional neural networks with receptive  elds at various time scales  van den Oord et al    However  the
authors themselves note that  even with   receptive  eld
of several seconds  the models did not enforce longrange
consistency which resulted in secondto second variations
in genre  instrumentation  and sound quality       
Finally  prior work has successfully performed computational molecular generation with deep neural networks 
Segler et al    demonstrated that an LSTM trained
on sets of biologically active molecules can be used to
generate novel molecules with similar properties    omezBombarelli et al    trained   variational autoencoder
to learn   compact embedding of molecules encoded using
the SMILES notation  By interpolating in the embedding
space and optimizing for desirable metrics of drug quality 
the authors were able to decode molecules with high scores
on these metrics  However  producing embeddings that led
to valid molecules was dif cult  in some cases  as little as
  of generated sequences proved to be   valid molecule
encoding 

  Background
In RL  an agent interacts with an environment  Given the
state of the environment at time    st  the agent takes an action at according to its policy  at st  receives   reward
  st  at  and the environment transitions to state  st The
agent   goal is to maximize reward over   sequence of actions  with   discount factor of   applied to future rewards 
The optimal deterministic policy   is known to satisfy the

following Bellman optimality equation 
  st  at        st  at 

   Ep st st at max

at 

 
  st  at   

where   st  at      cid 

  cid      cid tr st cid  at cid  is the  
function of   policy   In Deep Qlearning  Mnih et al 
    neural network called the deep Qnetwork  DQN 
is trained to approximate           using the following
objective 

                   max

  cid      cid    cid               
 

where   is the exploration policy  and   is the parameters
of the target Qnetwork  Mnih et al    that is held  xed
during the gradient computation  The target Qnetwork is
updated more slowly than the Qnetwork  for example the
moving average of   can be used as   as proposed by
Lillicrap et al    Exploration can be performed with
either the  greedy method or Boltzmann sampling  Additional techniques such as   replay memory  Mnih et al 
  are used to stabilize and improve learning 

  Sequence Tutor
Given   trained sequence generation RNN  we would like
to impose domainspeci   rewards based on the structure
and quality of generated sequences  while still maintaining information about typical sequences learned from data 
Therefore  we treat the trained model as   blackbox prior
policy  and focus on developing   method that can tune
some properties of the model without interfering with the
original probability distribution learned from data  The
separation between the trained sequence model and the tuning method is important  as it prevents RL training from
overwriting the original policy  To accomplish this task 
we propose Sequence Tutor  An LSTM trained on data supplies the initial weights for three networks in the model   
recurrent Qnetwork and target Qnetwork  and   Reward
RNN  The Reward RNN is held  xed during training  and
treated as   prior policy which can supply the probability
of   given token in   sequence as originally learned from
data 
To apply RL to sequence generation  generating the next
token in the sequence is treated as an action    The state of
the environment consists of all of the tokens generated so
far       st           at  Given action at  we would
like the reward rt to combine information about the prior
policy   at st  as output by the Reward RNN  as well as
some domainor taskspeci   rewards rT   Figure   illustrates these ideas 

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

variants of Qlearning with minimal modi cations  which
give rise to different properties  Let              at 
represent the sequence       the reward of the sequence 
     be the prior distribution over   given by the trained
sequence model  and      be the policy of the Sequence
Tutor model  The objective is then to maximize the following expression with respect to      where DKL represents
the KL divergence of distributions 

       Eq           DKL         

 

             cid  

We express      in terms of   parametrized recurrent
policy  at st 
    at st  where
st            at  indicates that the system is non 
 cid  
Markovian  The prior policy is expressed similarly       
     at st  The reinforcement learning objective is the
following  where    below indicates expectation with respect to sequences sampled from  
       

  st  at     log   at st    log  at st 

 cid 

 

The difference between this equation and Eq    is that an
entropy regularizer is now included  and thus the optimal
policy is no longer deterministic  Below  we derive general temporaldifference based methods for the KLcontrol
problem for sequence generation 

  Recurrent Generalized  learning

Let    st  de ne the recurrent value function of the policy
  given by 

   st      

 cid 

  cid  

  st cid  at cid     log   at cid st cid 
  log  at cid st cid 

 

We de ne the generalized   function  analogous to   function for KL control  as below  We call this generalized  
function  as it was introduced in deriving  learning  Rawlik et al    and the following derivation is   generalization to the  learning algorithm 
 st  at      st  at     log   at st       st   

Note that the state st  is given deterministically by st  
         at  and at for sequence modeling  and thus
the expressions do not contain the usual stochastic dynamics   st st  at  The value function    st  can be recursively expressed in terms of  

   st      st  at      st 
    st  at    log  at st 

 
 

Fixing  st  at     st  at  and constraining   to be  
probability distribution  the optimal greedy policy update

Figure   An RNN pretrained on data using MLE supplies
the initial weights for the Qnetwork and target Qnetwork 
and    xed copy is used as the Reward RNN 

  Qlearning with log prior augmentation

The simplest and most na ve way to incorporate information about the prior policy is to directly augment the taskspeci   rewards with the output of the Reward RNN  In this
case  the total reward given at time   becomes 
          log          rT        

 

where   is   constant controlling the emphasis placed on
the taskspeci   rewards  Given the DQN objective in Eq 
  and modi ed reward function in Eq    the objective and
learned policy are 

       log          rM          

    max

  cid      cid    cid               

             arg max

         

 

 

 

This modi ed objective forces the model to learn that the
most valuable actions are those that conform to the music
theory rules  but still have high probability in the original
data  However  the DQN learns   deterministic policy  as
shown in Eq    which is not ideal for sequence generation  Therefore  after the model is trained  we generate
sequences by sampling from the softmax function applied
to the predicted Qvalues 

  KL Control for Sequence Generation

If we cast sequence generation as   sequential decisionmaking problem and the desired sequence properties in
terms of target rewards  the problem can be expressed
as   KL control problem for   nonMarkovian system 
KL control  Todorov    Kappen et al    Rawlik et al    is   branch of stochastic optimal control
 SOC   Stengel    which studies an RL  or control 
problem in which the agent tries maximizing its task reward while minimizing deviation from   prior policy  For
our purposes  we treat   trained MLE sequence model as
the prior policy  and thus the objective is to train   new policy  or sequence model  to maximize some rewards while
keeping close to the original MLE model  We show that
such KL control formulation allows us to derive additional

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

  can be derived  along with the corresponding optimal
value function 

 at st      st at 

   st    log

  st at 

 cid 

at

 

 

  Sequence Tutor implementation

Following from the above derivations  we compare three
methods for implementing Sequence Tutor  Qlearning
with log prior augmentation  based on Eq    generalized
 learning  based on Eq    and Glearning  based on
Eq      pretrained sequence generation LSTM is used
as the Reward RNN  to supply the cross entropy reward in
Qlearning and the prior policy in Gand generalized  
learning  These approaches are compared to both the original performance of the MLE RNN  and   model trained
using only RL and no prior policy  Model evaluation is
performed every   training epochs  by generating
  sequences and assessing the average rT and log       
The code for Sequence Tutor  including   checkpointed
version of the trained melody RNN is available at redacted
for anonymous submission 

  Experiment    Melody Generation
Music compositions adhere to relatively wellde ned structural rules  making music an interesting sequence generation challenge  For example  music theory tells that
groups of notes belong to keys  chords follow progressions 
and songs have consistent structures made up of musical
phrases  Our research question is therefore whether such
constraints can be learned by an RNN  while still allowing
it to maintain note probabilities learned from data 
To test this hypothesis  we developed several rules that we
believe describe pleasantsounding melodies  taking inspiration from   text on melodic composition  Gauldin   
We do not claim these characteristics are exhaustive or
strictly necessary for good composition  rather  they are an
incomplete measure of task success that can simply guide
the model towards traditional composition structure  It is
therefore crucial that the Sequence Tutor approach allows
the model to retain knowledge learned from real songs in
the training data  The rules comprising the musicspeci  
reward function rT        encourage melodies to  stay in
key  start with the tonic note  resolve melodic leaps  have
  unique maximum and minimum note  prefer harmonious
intervals  play motifs and repeat them  have   low autocorrelation at   lag of     and   beats  and avoid excessively
repeating notes  Interestingly  while excessively repeating
tokens is   common problem in RNN sequence generation
models  avoiding this behavior is also Gauldin    rst rule
of melodic composition      
To train the model  we begin by extracting monophonic
melodies from   corpus of   MIDI songs and encoding them as onehot sequences of notes  These melodies
are then used to train an LSTM with one layer of   cells 

 More information about both the note encoding and the re 

ward metrics is available in the supplementary material 

Given Eq    and   the following Bellman optimality
equation for generalized   function is derived 
 st  at      st  at     log   at st 

  log

exp st  at 

 

 cid 

at 

The  learning loss directly follows 
       st  at    yt  where
yt   log   at st      st  at       log

 cid 

  cid 

 
  st   cid 

  corresponds to sampling sequence trajectories from
an arbitrary distribution 
in practice  the experience replay  Mnih et al      indicates that it uses the target
network           is parametrized with recurrent neural networks  and for discrete actions    is effectively  
softmax layer on top of  

  Recurrent Glearning

We can derive another algorithm by parametrizing   indirectly by  st  at    log   at st      st  at  Substituting into above equations  we get   different temporaldifference method 
LG        st  at    yt  where
yt     st  at       log
 at st      at st  exp   st  at 

 
    cid st eG st   cid  and

 cid 

  cid 

This formulation corresponds to Glearning  Fox et al 
  which can thus be seen as   special case of generalized  learning  Unlike   learning  which directly
builds knowledge about the prior policy into the   function  the Gfunction does not give the policy directly but instead needs to be dynamically mixed with the prior policy
probabilities  While this computation is straightforward
for discrete action domains as here  extensions to continuous action domains require additional considerations such
as normalizability of  function parametrization  Gu et al 
 
The KL controlbased derivation also has another bene  
in that the stochastic policies can be directly used as an
exploration strategy  instead of heuristics such as  greedy
or additive noise  Mnih et al    Lillicrap et al   

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

Optimization was performed with Adam  Kingma   Ba 
    batch size of   initial learning rate of   and  
stepwise learning rate decay of   every   steps  Gradients were clipped to ensure the    norm was less than  
and weight regularization was applied with      
Finally  the losses for the  rst   notes of each sequence
were not used to train the model  since it cannot reasonably
be expected to accurately predict them with no context  The
trained RNN eventually obtained   validation accuracy of
  and   log perplexity score of   This model was
used as described above to initialize the three subnetworks
in the Sequence Tutor model 
The Sequence Tutor model was trained using   similar con 
 guration to the one above  except with   batch size of
  and   reward discount factor of   The TargetQ network   weights   were gradually updated towards
those of the Qnetwork   according to the formula    
      where       is the TargetQ network update rate    strength of our model is that the in uence of
data and taskspeci   rewards can be explicitly controlled
by adjusting the temperature parameter    We replicated
our results for   number of settings for    we present results for    below because we believe them to be most
musically pleasing  however additional results are available
at https goo gl cTZy    Similarly  we replicated
the results using both  greedy and Boltzmann exploration 
and present the results using  greedy exploration below 

  Results

Table   provides quantitative results in the form of performance on the music theory rules to which we trained
the model to adhere  for example  we can assess the fraction of notes played by the model which belonged to the
correct key  or the fraction of melodic leaps that were resolved  The statistics were computed by randomly generating   melodies from each model 
Metric
Repeated notes
Mean autocorr  lag  
Mean autocorr  lag  
Mean autocorr  lag  
Notes not in key
Starts with tonic
Leaps resolved
Unique max note
Unique min note
Notes in motif
Notes in repeat motif

 
MLE
     
 
 
 
 
 
 
 
     
 
     
 
     
 
     
 
     
 
     
 
       

 
 
 

 

 

 
 
 

Table   Statistics of music theory rule adherence based on
  randomly initialized melodies generated by each
model  The top half of the table contains metrics that
should be near zero  while the bottom half contains metrics
that should increase  Bolded entries represent signi cant
improvements over the MLE baseline 

The results above demonstrate that the application of RL is
able to correct almost all of the targeted  bad behaviors 
of the MLE RNN  while improving performance on the
desired metrics  For example  the original LSTM model
was extremely prone to repeating the same note  after applying RL  we see that the number of notes belonging to
some excessively repeated segment has dropped from  
to nearly   in all of the Sequence Tutor models  While
the metrics for the   model did not improve as consistently 
the   and   models successfully learned to adhere to most
of the imposed rules  The degree of improvement on these
metrics is related to the magnitude of the reward given for
the behavior  For example    strong penalty of   was
applied each time   note was excessively repeated  while
  reward of only   was applied at the end of   melody
for unique extrema notes  which most likely explains the
lack of improvement on this metric  The reward values
could be adjusted to improve the metrics further  however
we found that these values produced pleasant melodies 
While the metrics indicate that the targeted behaviors of
the RNN have improved  it is not clear whether the models
have retained information about the training data  Figure
   plots the average log        as produced by the Reward
RNN for melodies generated by the models every  
training epochs  Figure    plots the average rT   Included
in the plot is an RL only model trained using only the music theory rewards  with no information about log       
Since each model is initialized with the weights of the
trained MLE RNN  we see that as the models quickly learn
to adhere to the music theory constraints  log        falls
from its initial point  For the RL only model  log       
reaches an average of   which is equivalent to an average        of approximately   or essentially   random
policy over the   actions with respect to the distribution
de ned by the Reward RNN  Figure    shows that each
of our models       and    attain higher log        values than this baseline  indicating they have maintained information about the data distribution  even over  
training steps  The Glearning implementation scores highest on this metric  at the cost of slightly lower average
rT   This compromise between data probability and adherence to music theory could explain the difference in the  
model   performance on the music theory metrics in Table
  Finally  we have veri ed that by increasing the   parameter it is possible to train all the models to have even
higher average log        but found that       produced
melodies that sounded better subjectively 
The question remains whether the RLtutored models actually produce more pleasing melodies  The sample melodies
used for the study are available here  goo gl XIYt   
we encourage readers to judge their quality for themselves 
To more formally answer this question  we conducted  
user study via Amazon Mechanical Turk in which partic 

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

    Reward RNN reward  log       

    Music theory reward

Figure   Average reward obtained by sampling  
melodies every   training epochs  The three models are compared to   model trained using only the music
theory rewards rT  

ipants were asked to rate which of two randomly selected
melodies they preferred on   Likert scale    total of  
ratings were collected  each model was involved in   of
these comparisons  Figure   plots the number of comparisons in which   melody from each model was selected as
the most musically pleasing    KruskalWallis   test of the
ratings showed that there was   statistically signi cant difference between the models             
MannWhitney   posthoc tests revealed that the melodies
from all three Sequence Tuner models       and    had
signi cantly higher ratings than the melodies of the MLE
RNN        The   and   melodies were also rated as
signi cantly more pleasing than those of the   model  but
did not differ signi cantly from each other 

  Discussion

Listening to the samples produced by the MLE RNN reveals that they are sometimes dischordant and usually dull 
the model tends to place rests frequently  repeat the same
note  and produce melodies with little variation  In contrast  the melodies produced by the Sequence Tutor models are more varied and interesting  The   model tends to

Figure   The number of times   melody from each model
was selected as most musically pleasing  Error bars re ect
the std  dev  of   binomial distribution    to the binary
win loss data from each model 

produce energetic and chaotic melodies  which include sequences of repeated notes  This repetition is likely because
the   policy as de ned in Eq    directly mixes        with
the output of the   network  and the MLE RNN strongly
favours repeating notes  The most pleasant melodies are
generated by the   and   models  These melodies stay
 rmly in key and frequently choose more harmonious interval steps  leading to melodic and pleasant melodies  However  it is clear they have retained information about the
training data  for example  the sample   wav in the sample directory ends with   seemingly familiar riff 
While we acknowledge that the monophonic melodies generated by these models   which are based on highly simplistic rules of melodic composition   do not approach the
level of artistic merit of human composers  we believe this
study provides   proofof concept that encoding even incomplete and partially speci ed domain knowledge using
our method can help the outputs of an LSTM adhere to  
more consistent structure  The musical complexity of the
songs is limited not just by the heuristic rules  but also by
the simple monophonic encoding  which cannot represent
the dynamics and expressivity of   musical performance 
Although these melodies cannot surpass those of human
musicians  attempting to train   model to generate aesthetically pleasing outputs in the absence of   better metric of
human taste than loglikelihood is   problem of broader interest to the arti cial intelligence community 

  Experiment II  Computational Molecular

Generation

As   followon experiment  we tested the effectiveness of
Sequence Tutor for generating   higher yield of synthetically accessible druglike molecules  Organic molecules
can be encoded using the commonly used SMILES representation  Weininger    For example  amphetamine

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

can be encoded as  CC   Cc ccccc  while creatine is
 CN CC            Using this character encoding  it
is straightforward to train an MLE RNN to generate sequences of SMILES characters  we trained such   model
using the same settings as described above for the melody
MLE RNN  However  only about   third of the molecules
generated using this simple approach are actually valid
SMILES encodings  Further  this approach does not directly optimize for metrics of molecule or drug quality 
These metrics include     the wateroctanol partition coef cient  logP  which is important in assessing the druglikeness of   molecule     synthetic accessibility  SA   Ertl
  Schuffenhauer      score from   that is lower if
the molecule is easier to synthesize  and    Quantitative Estimation of Druglikeness  QED   Bickerton et al     
more subjective measure of druglikeness based on abstract
ideas of medicinal aesthetics 
To optimize for these metrics  while simultaneously improving the percent yield of valid molecules from the RNN 
we constructed   reward function that incentivizes validity  logP  SA  and QED using an opensource library called
RDkit  http www rdkit org 
Included in the
reward function was   penalty for molecules with unrealistically large carbon rings  size larger than   as per previous work    omezBombarelli et al    Finally  after observing that the model could exploit the reward function by generating the simple molecule     repeatedly  or
 CCCCC   which produces an unrealistically high logP
value  we added penalties for sequences shorter than  or
with more consecutive carbon atoms than  any sequence
in the training data  Sequence Tutor was then trained using these rewards  the pretrained MLE RNN  and similar
settings to the  rst experiment  except with  greedy exploration with         batch size of   and discount factor
      For this experiment  we also made use of prioritized experience replay  Schaul et al    to allow the
model to more frequently learn from relatively rare valid
samples    value of       led to   higher yield of valid
molecules with high metrics  but still encouraged the diversity of generated samples 

  Results and discussion

As the   algorithm produced the best results for the music generation task  we focused on using this technique
for generating molecules  Table   shows the performance
of this model against the original MLE model according
to metrics of validity  druglikeness  and synthetic accessibility  Once again  Sequence Tutor is able to signi 
cantly improve almost all of the targeted metrics  However  it should be noted that the Sequence Tutor model
tends to produce simplistic molecules involving more carbon atoms than the MLE baseline       Sequence Tutor
may produce  SNCc ccccc  while the MLE produces

       ccc               Cl    which is the reason for the Sequence Tutor model   lower QED scores 
This effect is due to the fact that simple sequences are more
likely to be valid  have high logP and SA scores  and carbon is highly likely under the distribution learned by the
MLE model    higher reward for QED and further improvement of the taskspeci   rewards based on domain
knowledge could help to alleviate these problems  Overall 
the fact that Sequence Tutor can improve the percentage of
valid molecules produced as well as the logP and synthetic
accessibility scores serves as   proofof concept that Sequence Tutor may be valuable in   number of domains for
imparting domain knowledge onto   sequence predictor 

Metric
Percent valid
Mean logP
Mean QED
Mean SA penalty
Mean ring penalty

 

MLE
   
 
 
 
 

 
 
 
 

Table   Statistics of molecule validity and quality based
on   randomly initialized samples  Bolded entries
represent signi cant improvements over the MLE baseline 

  Conclusion and Future Work
We have derived   novel sequence learning framework
which uses RL to correct properties of sequences generated by an RNN  while maintaining information learned
from MLE training on data  and ensuring the diversity of
generated samples  By demonstrating   connection between our sequence generation approach and KLcontrol 
we have derived three novel RLbased methods for optimizing sequence generation models  These methods were
empirically compared in the context of   music generation
task  and further demonstrated on   computational molecular generation task  Sequence Tutor showed promising results in terms of both adherence to taskspeci   rules  and
subjective quality of the generated sequences 
We believe the Sequence Tutor approach of using RL to
re ne RNN models could be promising for   number of applications  including the reduction of bias in deep learning
models  While manually writing   domainspeci   reward
function may seem unappealing  that approach is limited
by the quality of the data that can be collected  and besides  even stateof theart sequence models often fail to
learn all the aspects of highlevel structure  van den Oord
et al    Graves    Further  the data may contain
hidden biases  as has been demonstrated for popular language models  CaliskanIslam et al    In contrast to
relying solely on possibly biased data  our approach allows
for encoding highlevel domain knowledge into the RNN 
providing   general  alternative tool for training sequence
models 

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

ACKNOWLEDGMENTS

This work was supported by Google Brain  the MIT Media Lab Consortium  and Canada   Natural Sciences and
Engineering Research Council  NSERC  We thank Greg
Wayne  Sergey Levine  and Timothy Lillicrap for helpful
discussions on RL and stochastic optimal control and Kyle
Kastner and Tim Cooijmans for valuable insight into training RNNs 

References
Dzmitry Bahdanau  Philemon Brakel  Kelvin Xu  Anirudh
Goyal  Ryan Lowe  Joelle Pineau  Aaron Courville  and
Yoshua Bengio  An actorcritic algorithm for sequence
prediction  arXiv preprint arXiv   

  Richard Bickerton  Gaia   Paolini    er emy Besnard 
Sorel Muresan  and Andrew   Hopkins  Quantifying the
chemical beauty of drugs  Nature chemistry   
 

BoulangerLewandowski  Bengio  and Vincent  Modeling
temporal dependencies in highdimensional sequences 
Application to polyphonic music generation and transcription  arXiv preprint   

CaliskanIslam  Bryson  and Narayanan  Semantics derived automatically from language corpora necessarily
contain human biases  arXiv preprint   

Hang Chu  Raquel Urtasun  and Sanja Fidler  Song from pi 
  musically plausible network for pop music generation 
arXiv preprint arXiv   

Junyoung Chung  Kyle Kastner  Laurent Dinh  Kratarth
Goel  Aaron   Courville  and Yoshua Bengio    recurrent latent variable model for sequential data  In Advances in neural information processing systems  NIPS 
pp     

Eck and Schmidhuber  Finding temporal structure in music  Blues improvisation with LSTM recurrent networks 
In Neural Networks for Signal Processing  pp   
IEEE   

Peter Ertl and Ansgar Schuffenhauer  Estimation of synthetic accessibility score of druglike molecules based on
molecular complexity and fragment contributions  Journal of cheminformatics     

Roy Fox  Ari Pakman  and Naftali Tishby  Taming the
noise in reinforcement learning via soft updates  arXiv
preprint arXiv   

Gauldin    practical approach to eighteenthcentury coun 

terpoint  Waveland Pr Inc   

Rafael   omezBombarelli  David Duvenaud  Jos   Miguel
Hern andezLobato  Jorge AguileraIparraguirre  Timothy   Hirzel  Ryan   Adams  and Al an AspuruGuzik 
Automatic chemical design using   datadriven conarXiv preprint
tinuous representation of molecules 
arXiv   

Alex Graves  Generating sequences with recurrent neural

networks  arXiv preprint   

Shixiang Gu  Zoubin Ghahramani  and Richard   Turner 
In Advances
Neural adaptive sequential monte carlo 
in Neural Information Processing Systems  NIPS  pp 
   

Shixiang Gu  Timothy Lillicrap  Ilya Sutskever  and Sergey
Levine  Continuous Deep QLearning with modelbased
acceleration  In ICML   

Sepp Hochreiter and   urgen Schmidhuber  Long shortterm

memory  Neural computation     

Sham   Kakade    natural policy gradient  In Advances
in neural information processing systems  NIPS  volume   pp     

Kappen    omez  and Opper  Optimal control as   graphical
model inference problem  Machine learning   
   

Diederik Kingma

and Jimmy Ba 
method for stochastic optimization 
arXiv   

Adam 

 
arXiv preprint

Jiwei Li  Will Monroe  Alan Ritter  and Dan Jurafsky  Deep
reinforcement learning for dialogue generation  arXiv
preprint arXiv   

Timothy   Lillicrap  Jonathan   Hunt  Alexander Pritzel 
Nicolas Heess  Tom Erez  Yuval Tassa  David Silver  and
Daan Wierstra  Continuous control with deep reinforcement learning  arXiv preprint arXiv   

ChiaWei Liu  Ryan Lowe  Iulian   Serban  Michael Noseworthy  Laurent Charlin  and Joelle Pineau  How not to
evaluate your dialogue system  An empirical study of
unsupervised evaluation metrics for dialogue response
generation  In EMNLP   

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex
Graves  Ioannis Antonoglou  Daan Wierstra  and Martin
Riedmiller  Playing atari with deep reinforcement learning  arXiv preprint arXiv   

Mohammad Norouzi  Samy Bengio  Navdeep Jaitly  Mike
Schuster  Yonghui Wu  Dale Schuurmans  et al  Reward augmented maximum likelihood for neural strucIn Advances In Neural Information
tured prediction 
Processing Systems  pp     

Sequence Tutor  Conservative FineTuning of Sequence Generation Models with KLcontrol

Peters    ulling  and Altun  Relative entropy policy search 

In AAAI  Atlanta   

Marc Aurelio Ranzato  Sumit Chopra  Michael Auli  and
Wojciech Zaremba  Sequence level training with recurrent neural networks  arXiv preprint arXiv 
 

Rawlik  Toussaint  and Vijayakumar  On stochastic optimal control and reinforcement learning by approximate
inference  Proceedings of Robotics  Science and Systems VIII   

Tom Schaul  John Quan  Ioannis Antonoglou  and David
Silver  Prioritized experience replay  arXiv preprint
arXiv   

Schulman  Levine  Moritz  Jordan  and Abbeel  Trust re 

gion policy optimization  In ICML   

Marwin HS Segler  Thierry Kogej  Christian Tyrchan  and
Mark   Waller  Generating focussed molecule libraries
for drug discovery with recurrent neural networks  arXiv
preprint arXiv   

Robert   Stengel  Stochastic optimal control  John Wiley

and Sons New York  New York   

Sturm  Santos  BenTal  and Korshunova  Music transcription modelling and composition using deep learning  arXiv preprint   

Emanuel Todorov 

Linearlysolvable markov decision
In Advances in neural information process 

problems 
ing systems  NIPS  pp     

  aron van den Oord  Sander Dieleman  Heiga Zen 
Karen Simonyan  Oriol Vinyals  Alex Graves  Nal
Kalchbrenner  Andrew Senior  and Koray Kavukcuoglu 
Wavenet    generative model for raw audio  CoRR
abs   

Ramakrishna Vedantam    Lawrence Zitnick  and Devi
Parikh  Cider  Consensusbased image description evaluation  In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp   
 

David Weininger  Smiles    chemical language and information system    introduction to methodology and encoding rules  In Proc  Edinburgh Math  SOC  volume  
pp     

Brian   Ziebart  Andrew   Maas    Andrew Bagnell  and
Anind   Dey  Maximum entropy inverse reinforcement
learning  In AAAI  volume   pp    Chicago 
IL  USA   

