Capacity Releasing Diffusion for Speed and Locality

Di Wang   Kimon Fountoulakis   Monika Henzinger   Michael    Mahoney   Satish Rao  

Abstract

Diffusions and related random walk procedures
are of central importance in many areas of machine learning  data analysis  and applied mathematics  Because they spread mass agnostically at
each step in an iterative manner  they can sometimes spread mass  too aggressively  thereby
failing to  nd the  right  clusters  We introduce  
novel Capacity Releasing Diffusion  CRD  Process  which is both faster and stays more local than the classical spectral diffusion process 
As an application  we use our CRD Process to
develop an improved local algorithm for graph
clustering  Our local graph clustering method
can  nd local clusters in   model of clustering
where one begins the CRD Process in   cluster whose vertices are connected better internally
than externally by an   log     factor  where  
is the number of nodes in the cluster  Thus  our
CRD Process is the  rst local graph clustering
algorithm that is not subject to the wellknown
quadratic Cheeger barrier  Our result requires  
certain smoothness condition  which we expect
to be an artifact of our analysis  Our empirical
evaluation demonstrates improved results  in particular for realistic social graphs where there are
moderately good but not very good clusters 

  Introduction
Diffusions and related random walk procedures are of central importance in many areas of machine learning  data
analysis  and applied mathematics  perhaps most conspicuously in the area of spectral clustering  Cheeger   
Donath   Hoffman    von Luxburg    Shi   Malik    community detection in networks  Ng et al 
  White   Smyth    Leskovec et al    Jeub
et al    socalled manifold learning  Belkin   Niyogi 

 EECS  UC Berkeley  Berkeley  CA  USA  ICSI and Statistics  UC Berkeley  Berkeley  CA  USA  Computer Science  University of Vienna  Vienna  Austria  Correspondence to  Di Wang
 wangd eecs berkeley edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Mahoney et al    and PageRankbased spectral
ranking in web ranking  Page et al    Gleich   
Particularly relevant for our results are local personalized
versions of PageRank  Jeh   Widom    and local distributed versions of spectral clustering  Spielman  
Teng    Andersen et al    Andersen   Peres 
  These latter algorithms can be used to  nd provablygood smallsized clusters in very large graphs without even
touching the entire graph  they have been implemented and
applied to billionnode graphs  Shun et al    and they
have been used to characterize the clustering and community structure in   wide range of social and information networks  Leskovec et al    Jeub et al   
Somewhat more formally  we will use the term diffusion on
  graph to refer to   process that spreads mass among vertices by sending mass along edges step by step according to
some rule  With this interpretation  classical spectral diffusion spreads mass by distributing the mass on   given node
equally to the neighbors of that node in an iterative manner 
  wellknown problem with spectral methods is that due
to their close relationship with random walks they sometimes spread mass  too aggressively  and thereby they
don    nd the  right  partition  In theory  this can be seen
with socalled Cockroach Graph  Guattery   Miller   
von Luxburg   
In practice  this is seen by the extreme sensitivity of spectral methods to highdegree nodes
and other structural heterogeneities in realworld graphs
constructed from very noisy data  Leskovec et al   
Jeub et al    More generally  it is wellknown that
spectral methods can be very sensitive to   small number
of random edges       in smallworld graphs  that  short
circuit  very distant parts of the original graph  as well as
other noise properties in realistic data  Empirically  this
is wellknown to be   particular problem when there are
moderately good but not very good clusters in the data 
  situation that is all too common in machine learning and
data analysis applications  Jeub et al   
Here  we introduce   novel Capacity Releasing Diffusion
 CRD  Process to address this problem  Our CRD Process is   type of diffusion that spreads mass according to
  carefullyconstructed pushrelabel rule  using techniques
that are wellknown from  owbased graph algorithms  but
modi ed here to release the capacity of edges to transmit
mass  Our CRD Process has better properties with respect

Capacity Releasing Diffusion for Speed and Locality

to limiting the spread of mass inside local wellconnected
clusters  It does so with improved running time properties 
We show that this yields improved local clustering algorithms  both in worstcase theory and in empirical practice 
  Capacity Releasing Diffusion  CRD 

We start by describing the generic CRD process in Figures   and   which lays down the dynamics of spreading
mass across the graph 
Importantly  this dynamical process is independent of any particular task to which it may
be applied  Later  in Section   we also present   concrete
CRD algorithm for the speci   task of local clustering that
exploits the dynamics of the generic CRD process 

  Begin with       mass at   single  given  vertex   

  Repeatedly perform   CRD step  and then double the mass

at every vertex 

Figure   Generic Capacity Releasing Diffusion  CRD  Process
The entire CRD process  Figure   repeatedly applies the
generic CRD inner process  which we call   CRD step  and
then it doubles the amount of mass at all vertices between
invocations    CRD step starts with each vertex   having
mass              where      is the degree of    and
spreads the mass so that at the end each vertex   has mass
            Observe that  essentially  each CRD step

Each vertex   initially has mass              and needs to
spread the mass so that              

  Each vertex   maintains   label       initially set to  

  Each edge   maintains      which is the mass moved
from   to    where                    We note that
both      and         are variables local to this inner
process  where      evolves across calls to this process 

  An edge             is eligible with respect to the labeling if it is downhill              and the mass        
moved along        is less than     

  The excess of   vertex is ex   

def

  max           

The inner process continues as long as there is   vertex   with
ex        it either sends mass over an eligible edge incident
to    or if there is none  then it increases      by  

Figure   Generic CRD Inner Process 

spreads the mass to   region of roughly twice the volume

 The relation between the generic CRD process and the CRD
algorithm for local graph clustering is analogous to the relation
between local random walks and   local spectral graph partitioning algorithm such as that of Andersen et al   

comparing to the previous step 
The generic CRD inner process  Figure   implements  
modi cation of the classic  pushrelabel  algorithm  Goldberg   Tarjan      for routing   sourcesink  ow 
The crucial property of our process  different from the standard pushrelabel  is that edge capacity is made available to
the process slowly by releasing  That is  we only allow     
units of mass to move across any edge        where      is
the label  or height  maintained by the CRD inner process 
Thus  edge capacity is released to allow mass to cross the
edge as the label of the endpoint rises  As we will see  this
difference is critical to the theoretical and empirical performance of the CRD algorithm 
  Example  Classical Versus Capacity Releasing

To give insight into the differences between classical spectral diffusion and our CRD Process  consider the graph in
Figure   There is    cluster     which consists of   paths 
each of length    joined at   common node    There is one
edge from   to the rest of the graph  and we assume the
other endpoint   has very high degree such that the vast majority of the mass arriving there is absorbed by its neighbors
in    While idealized  such an example is not completely
unrealistic  Leskovec et al    Jeub et al   

 

 

 

 

 

 

 cid 

 
 
 

 

Figure   An example where Capacity Releasing Diffusion beats
classical spectral diffusion by  cid 

Consider  rst classical spectral diffusion  with   random
walk starting from some vertex in    This process requires
 cid  steps to spread probability mass to   constant fraction of the nodes on the paths  and in this many steps  the
expected number of times to visit   is  cid  Because of the
edge to    each time we visit    we have       chance
of leaving    Thus  when  cid  is     the random walk is
expected to leave   and never return       the classical diffusion will leak out all the probability mass before even
spreading beyond   constant fraction of   
Consider next our CRD Process  starting with mass at the
vertex        which would be   worstcase starting node in
  for CRD  Assume that at some point the mass is spread
along   neighboring vertices on each of the   paths  To
continue the spread to    vertices in the next CRD step  the
labels will be raised to  at most     to allow the mass to
spread over the path of length     This enables the spread
along the paths  but it only releases   capacity of    to the
exiting edge        Since in this call    total of  zk mass

Capacity Releasing Diffusion for Speed and Locality

is in the set    at most    of the mass escapes  After
log  cid  CRD steps  the mass is spread over all the   length cid 
paths  and only     log  cid   fraction of the mass has escaped from    Thus if  cid        as before    factor of
   log  cid  less mass has escaped from   with the CRD Process than with the classical diffusion process 
Without the releasing  however  the mass escaping   would
be large  as even raising the label of vertex   to   would
allow an arbitrary amount of mass to leak out 
Finally  note that the  cid  mixing time makes spectral diffusions    cid  factor slower than CRD  This drawback of
spectral techniques can perhaps be resolved using sophisticated methods such as evolving sets  Andersen   Peres 
  though it comes easily with CRD 
  Our Main Results

         

We provide theoretical and empirical evidence that our
CRD algorithm is superior to classical diffusion methods
at  nding clusters  with respect to noise tolerance  recovery accuracy  cut conductance  and running time  Here
the cut conductance     of   cut         is      
  where          denotes the set of
min vol   vol      
edges between   and     and the volume vol    is the sum
of the degrees of the vertices in    In all these measures  we
break the quadratic Cheeger barrier for classical diffusions
 explained below  while presenting   local algorithm      
an algorithm whose running time depends on the volume
of the cluster found and not of the whole graph 
Our  rst main result  Section   presents   CRD algorithm
and its running time  The CRD algorithm is   parameterized specialization of the generic CRD Process  where we
limit the maximum label of vertices  as well as the maximum edge capacity  We prove that this specialization is
ef cient  in that it runs in time linear in the total mass and
the label limit  and it either succeeds in spreading the mass
or it leaves all unspread mass at nodes with high label  This
property is analogous to the ispoerimetric capacity control
provided by local spectral methods  and it is important for
locating cluster bottlenecks  We use this crucially in our
context to  nd low conductance clusters 
Our second main result  Section   concerns the use of the
CRD algorithm to  nd good local clusters in large graphs 
Our result posits the existence of    good  cluster    which
satis es certain conditions  Assumption   and   that naturally capture the notion of   local structure  The rather
weak Assumption   states that     internal connectivity
       see Section   for de nition  is   constant factor
better       larger  than the conductance     Assumption
  states that we have   smoothness condition which needs
 Unless otherwise noted  when speaking of the conductance

of   cut    we assume   to be the side of minimum volume 

that any subset       has polylog vol    times more
neighbors in      than in        Under these conditions 
we can recover   starting from any vertex in   
Both assumptions formalize the idea that the signal of the
local structure is stronger than the noise of the cluster by
some moderately large factor  More speci cally  Assumption   roughly says that the weakest signal of any subset
of   is   constant times stronger than the average noise of
   and Assumption   roughly says the signal of any subset is polylog vol    times stronger than the noise of the
subset 
We note that Assumption   is signi cantly weaker than the
factor in Zhu et al    where it is shown how to localize

  cluster   such that          cid    Their condition

is considerably stricter than our condition on the ratio between       and     especially when     is small  as
is common  Their algorithm relies on proving that   classical diffusion starting at   typical node keeps most of its
mass inside of    However  they do not need something
like our smoothness condition 
With the additional smoothness condition  we break the

using spectral diffusions  including Zhu et al    for
the  rst time with   local algorithm 
In particular  comparing to Zhu et al    under their parameter settings
 but with the smoothness condition  we identify   cluster

dependence on  cid    that is central to all approaches
with  cid    times less error  and we have    cid   
  cid    log  cid  as          cid  and  cid       

speedup in running time  This improvement is  up to  
log  cid factor  consistent with the behavior in the example of
the previous section where the improvement is    log  cid   

We note that with the additional smoothness condition  our
theoretical results hold for any starting node vs in    in
contrast to prior spectralbased results which only work
when starting from    good  node  where only   constant
fraction of the nodes in   are good  We expect the smoothness condition to be an artifact of our analysis       similar
results actually hold when starting at good nodes in    even
without this assumption 
Our third main result  Section   is an empirical illustration
of our method  We consider several social and information networks studied previously that are particularly challenging for spectral methods  In particular  while graphs
that have upwardsloping NCPs  Network Community Pro 
 les  have good small clusters  Leskovec et al    Jeub
et al    denser social networks with  at NCPs do
not have any verygood conductance clusters of any size 
They do  however  often have moderatelygood clusters 
but these are very dif cult for spectral methods to identify  Jeub et al    Our empirical results show that our
CRDbased local clustering algorithm is better able to iden 

Capacity Releasing Diffusion for Speed and Locality

tify and extract in   strongly local running time moderately
good quality clusters from several such social networks 
  Previous Work  Low Conductance Cuts 

Diffusions  and Multicommodity Flow

Spectral algorithms for computing eigenvalues use some
variant of repeated matrix multiplication  which for graphs
is   type of classical diffusion  For the Laplacian of   graph 
the convergence rate is    where   is the second
smallest eigenvalue by of this matrix  The Lanczos method

improves this rate to   cid  by cleverly and ef ciently

combining different iterations of the diffusions  See      
Orecchia et al    for more details on this 
One application of such   computation is to  nd   low conductance cut in   graph  The second eigenvector for  
   Cheeger 
can be used to  nd   cut of conductance     
 
  Donath   Hoffman    Let    be the minimum conductance in the graph  In his work  Cheeger al 
 
ready observed that randomwalk based diffusion can make
   
    error in estimating the conductance  informally known as the  quadratic  Cheeger barrier  and illustrated in our example  This  combined with the fact that
         gives   spectral method to  nd an   
   
conductance cut in   
SpielmanTeng   used local versions of diffusions
      those with small support  to compute recursive decompositions ef ciently  and then they used locality to
produce linear time partitioning algorithms  Andersen 
Chung and Lang   developed an improved version
that adjusts the standard diffusion by having mass set 
 
tled at vertices  resulting in signi cantly improved bounds
to   
   log    on the conductance of the returned cut
        in time     vol   
  AllenZhu  Lattanzi and Mirrokni   analyzed the behavior of the same algorithm
under certain wellconnected conditions  The EvoCut algorithm of Andersen and Peres   improved the running
time of this method to     vol   
  As all these methods are
  
based on spectral diffusion  their performance with respect
to conductance is subject to the Cheeger barrier  Other
processes have been proposed for random walks that mix
faster       nonbacktracking random walks  Alon et al 
  These too are subject to the Cheeger barrier asymptotically  Our result is the  rst to break this barrier in any
broad setting  where classical spectral methods fail 
Multicommodity  ow based methods are able to  nd clusters of conductance     log     Leighton   Rao   
bypassing the limit inherent in purely spectral methods   
semide nite programming approach  which can be viewed
as combining multicommodity  ow and spectral methods 
log     Arora et al 
yields cuts of conductance    
  These algorithms are very nonlocal       in the
sense that their running time depends on the size of the

 

  

whole graph  and it is not clear that they can be meaningfully localized  We do  however  use wellknown  owbased ideas in our algorithm 
In particular  recall that
pushrelabel and in general  shortestpath  based methods
have   celebrated history in algorithms  Goldberg   Tarjan    Using levels to release capacity  however  as we
do in our algorithm  is  to our knowledge  completely new 
  Capacity Releasing Diffusion
In this section  we describe our algorithm which implements   speci   version of the generic CRD Process  In
particular  it has some modi cations for ef ciency reasons 
and it terminates the diffusion when it  nds   bottleneck
during the process  The algorithm iteratively calls   subroutine CRDinner  which implements one CRD step 
For ef ciency reasons  CRDinner doesn   necessarily
carry out   full CRD step  where   full CRD step means
every node   has at most      mass at termination  In particular  CRDinner only makes   certain amount of  effort 
 which is tuned by   parameter   to spread the mass  and if
there is   bottleneck in the form of   cut that requires  too
much effort  for the diffusion to get through  then CRDinner may leave excess mass on nodes                   at
termination  More speci cally  given   CRDinner guarantees to overcome any bottleneck of conductance  
     if it doesn   carry out   full CRD step  then it returns
  cut of conductance    as   certi cate  We will discuss
CRDinner with more detail in Section  
  CRD Algorithm

Given   starting node vs  the CRD algorithm  Algorithm  
is essentially the CRD Process starting from vs  as described in Figure   The algorithm takes as input   parameter   which is used to tune CRDinner  Since CRDinner may stop short of   full CRD step due to   bottleneck 
we remove any excess mass remaining on nodes after calling CRDinner  Due to the excess removal  we may discard mass as the algorithm proceeds  In particular  as we
start with    vs  mass  and double the amount after every
CRD step  the amount of mass after the jth doubling is
   vs       if we never remove excess  When the actual
amount of mass is signi cantly smaller than    vs       
there must be   bottleneck         during the last CRD
step  such that   contains   large fraction of the mass  and
of the excess  and such that CRDinner cannot push any
more mass from   to     We terminate the CRD algorithm
when this happens  as the mass and  as we can show  thus
the volume of   must be large  while there are few edges
between   and     Thus   is   lowconductance cluster
around vs  Formally  the algorithm takes input parameters
  and    and it terminates either when the amount of mass
drops below      vs        after iteration    or after iteration   if the former never happens  It returns the mass on

Capacity Releasing Diffusion for Speed and Locality

Algorithm   CRD Algorithm    vs        
 

  For                
 
 
 

Initialization 
  vs      vs             cid  vs       
               
  Assertion               
  Call CRDinner with         get cut Kj
         min           
              vs       
 
 

  Return    and  

def
  Kj  Terminate 

 Kj empty if CRDinner  nishes full CRD step 

If  cid 

 
 
 
  End For
  Return     

def
  Kt 

the nodes          as well as the cut   returned by the
last CRDinner call in the former termination state 
The running time of our CRD algorithm is local       proportional to the volume of the region it spreads mass to 
rather than the volume of the entire graph  In particular 
each CRDinner call takes time linear in the amount of
mass  and as the amount of mass increases geometrically
before we terminate  the running time of the CRD algorithm is dominated by the last CRDinner call 
  CRD Inner Procedure

Now we discuss the CRDinner subroutine  Algorithm  
which aims to carry out one CRD step  In particular  each
node   has              mass at the beginning  and
CRDinner tries to spread the mass so each node   has
            mass at the end  Not surprisingly  as the CRD
step draws intuition from  ow routing  our CRDinner can
be viewed as   modi cation of the classic pushrelabel algorithm 
As described in Figure   we maintain   label      for
each node    and the net mass being pushed along each
edge  Although the graph is undirected  we consider each
edge            as two directed arcs        and       
and we use         to denote the net mass pushed from
  to    during the current CRDinner invocation  Under this notation  we have                    We
denote     def
       as the total amount of mass 
  max               as the amount of excess on
ex   
   and we let   be the input parameter tuning the  effort 
made by CRDinner  which will be clear shortly 
As noted earlier  to make CRDinner ef cient  we deviate
from the generic CRD step  In particular  we make the following modi cations 

   cid 

def

         and ex        We keep   list   of all active
nodes  and terminate CRDinner when   is empty 

  In addition to capacity releasing  the net mass along
any edge can be at most       Formally  for an arc
def
       its effective capacity is         
  min        
            
and its residual capacity is rm      
        The arc        is eligible iff            
      downhill  and rm           We only push mass
along eligible arcs 
  We enforce              for all   through the execution  This is assumed at the start  and we never push
mass to   if that would result in             

def

The parameter   in the  rst two modi cations limits the
work done by CRDinner  and it captures how hard CRDinner will try to carry out the full CRD step       when
     are in nitely large  CRDinner implements the full
CRD step  Given any   CRDinner makes enough effort
by allowing nodes to have height up to   and by using the
above edge capacities to overcome bottlenecks of conductance   during the diffusion process  If it doesn    nish
the full CRD step  then it returns   cut of conductance   
as certi cate 
Another motivation of tuning with parameter   is to keep
the diffusion local  Since CRDinner doesn   try to get
through lowconductance bottlenecks  the diffusion tends
to spread mass over wellconnected region  instead of pushing mass out of   bottleneck  This guarantees that the work
performed is linear in the volume of the returned cluster 
     that it is   strongly local algorithm  since only   small
fraction of mass can leak out of the cluster 
The third modi cation guarantees when CRDinner terminates with   lot of excess on nodes  the excess won   be
concentrated on   few nodes  as no node can have more
mass than twice its degree  and thus the cut returned must
contain   large region 

We have the following theorem for CRDinner 
Theorem   Given       and         such that
      vol    and                   at the start 
CRDinner terminates with one of the following cases 
  CRDinner  nishes the full CRD step             

    

  There are nodes with excess  and we can  nd   cut  
of conductance    Moreover                  
            and                      

  The label of any node can be at most    
  log     If   is raised to level    but still has
excess mass  CRDinner leaves the excess on    and
won   work on   any more  Formally    is active if

The running time is      log   
Proof sketch  Let    be the labels of nodes at termination  First note all nodes with excess must be on level   

Capacity Releasing Diffusion for Speed and Locality

Initialization 
                                                
                           log    

Algorithm   CRDinner     
 
 
 
  While   is not empty
 
 
 
 
 
 
 

  Let   be the lowest labeled node in   
  Push Relabel   
 
 
 
  Else If Push Relabel    increases      by  
 

If Push Relabel    pushes mass along       
If   becomes inactive  remove   from  
 
If   becomes active  add   to  
 

 

 

If           remove   from   

If there is any eligible arc       
  Push      

Push Relabel   
 
 
  Else
 

  Relabel   

Push      
      min  ex    rm                   
  Push   units of mass from   to   

                                           
                               

Relabel   
 

               

Moreover  since we only push from   node   if it has excess                   once   node has at least     
mass  it always has at least      mass  Note further that
         if and only if ex        at some point during
the process  Thus  we know the following            
                                              
                      
Let Bi                Since the total amount of mass
    is at most the volume of the graph  if        or
Bh     then we have case   of the theorem 
Otherwise  both Bh and    are nonempty  Let the level
cut Si     
  iBj be the set of nodes with label at least   
We have   level cuts Sh             where vol Sh      and
Sj   Si if        The conductance of these cuts  when
we go from Sh down to    lower bounds how much the
volume grows from Sh to    If all these cuts have  
conductance  by our choice of    the volume of    will be
much larger than     This gives   contradiction  since
any node        has             and we don   have
enough mass  It follows that at least one of the level cuts
has conductance   
As to the running time  the graph   is given implicitly 
and we only acess the list of edges of   node when it is
active  Each active node   has      mass  and the total
amount of mass is     so the algorithm touches   region
of volume at most     Thus  the running time has linear
dependence on     Using an amortization argument one

can show that the total work of the subroutine  in the worst
case  is               log   

  Local Graph Clustering
In this section  we provide theoretical evidence that the
CRD algorithm can identify   good local cluster in   large
graph if there exists one around the starting node  We de 
 ne set conductance         or internal connectivity  of  
set       is the minimum conductance of any cut in the
induced subgraph on   
Informally  for    good  cluster    any inside bottleneck
should have larger conductance than     and nodes in  
should be more connected to other nodes inside   than to
nodes outside  We capture the intuition formally as follows 
       
Assumption    
Assumption   There exists       such that any    
  with volB       volB    satis es

def
        

             

             log vol    log

   

 

      

Following prior work in local clustering  we formulate the
goal as   promise problem  where we assume the existence of an unknown target good cluster       satisfying Assumption   and   In the context of local clustering  we also assume vol      vol    Similar to prior
work  we assume the knowledge of   node vs      and
rough estimates       within constant factor  of the value
of       and vol    We use the CRD algorithm with
vs as the starting node                  and
     log vol   
  vs    With the parameters we use  the algorithm will terminate due to too much excess removed      
           vs        after some iteration    The region
where the diffusion spreads enough mass will be   good
approximation of   
Theorem   Starting from any vs      with the above
parameters  when the CRD algorithm terminates  if we let
                   then we have 
  vol              
  vol              
where     min        with the     from Assumption   and   The running time is    vol    log vol   
 

      vol   
      vol   

      

The theorem states that the cluster recovered by the CRD
algorithm has both good  degree weighted  precision and
recall with respect to    and that the stronger the  signal 
 relative to the  noise       the larger     the more
accurate our result approximates   
If the goal is to minimize conductance  then we can run one
extra iteration of the CRD algorithm after termination with
  smaller value for    not necessarily       as used in
previous iterations  In this case  we have the following 

Capacity Releasing Diffusion for Speed and Locality

Theorem   If we run the CRD algorithm for one extra
iteration  with         then CRDinner will end with
case   of Theorem   Let   be the cut returned  We have 
  vol                
  vol                
          
The running time is    vol    log vol   

      vol   
           vol   

 

 

 

 

Now we can search for the smallest   that gives case   of
Theorem   which must give   cut of conductance within
an    factor of the best we can hope for           If
we search with geometrically decreasing   values  then the
running time is   vol    log vol     
Theorem   and   hold due to the particular  owbased dynamics of the CRD algorithm  which tends to keep the diffusion local  without leaking mass out of   bottleneck 
Formally  for each CRD step  we can bound the total
amount of mass that starts on nodes in    and leaves  
at any point during the diffusion  We have the following
lemma    sketch of the proof of which is given 
Lemma   In the jth CRD step  let Mj be the total
amount of mass in   at the start  and let Lj be the amount
of mass that ever leaves   during the diffusion  Then
  log vol        Mj  when Mj   volB    and
Lj     
    Mj  when Mj   volB   
Lj       
Proof sketch  We have two cases 
corresponding to
whether the diffusion already spread   lot of mass over   
In the  rst case  if Mj   volB    then we use the upper bound   that is enforced on the net mass over any
edge to limit the amount of mass that can leak out 
In
particular Lj     vol          since there are
vol      edges from   to     and          
in CRDinner  As Mj    vol    we have Lj  
    
 
The second case is when Mj   volB    In this case   
combination of Assumption   and capacity releasing controls the leakage of mass  Intuitively  there are still many
nodes in   to which the diffusion can spread mass  For
the nodes in   with excess on them  when they push their
excess  most of the downhill directions go to nodes inside
   As   consequence of capacity releasing  only   small
fraction of mass will leak out 
Theorem   and   follow from straightforward analysis of
the total amount of leaked mass at termination  We sketch
the ideas for the proof of Theorem  
Proof sketch  Since we use           when we call
CRDinner  the diffusion will be able to spread mass over
nodes inside    since there is no bottleneck with conductance smaller than       in   

    Mj 

Thus  before every node   in   has      mass on it  in
which case we say   is saturated  there will be no excess
on nodes in   at the end of   CRD step  Consequently 
the amount of mass in   only decreases  compared to the
supposed    vs       amount in the jth CRD step  due to
mass leaving   
As long as the total amount Mj of mass in   at the start
of   CRD step is less than volB    the mass loss to   
is at most      log vol    fraction of the mass in
  each CRD step  After   log vol    CRD steps  Mj
reaches vol      and only      fraction of mass
has left   so far  After    more CRD steps  there will be
enough mass to saturate all nodes in    and each of these
CRD steps looses at most      fraction of the mass
to     Thus we loose at most      fraction of mass
before all nodes in   are saturated 
the
Once the diffusion has saturated all nodes in   
amount of mass in   will be  vol    at the start of every subsequent CRD step  At most vol           
  vol    mass can leave    and nodes in   can hold
vol    mass  so there must be   lot of excess  in    at the
end  Thus  the CRD algorithm will terminate in at most  
more CRD steps  since the amount of mass almost stops
growing due to excess removal 
At termination  the amount of mass is  vol    and only
   fraction of the mass is in     Since              
     and the total mass outside is   vol    we get
claim   of the theorem  In our simpli ed argument  all
nodes in   have saturated sinks       vol            at
termination  We get the small loss in claim   when we
carry out the argument in more detail 
The amount of mass grows geometrically before the CRD
algorithm terminates  so the running time is dominated by
the last CRD step  The total amount of mass is   vol   
in the last CRD step  and the running time follows Theorem   with          
The proof of Theorem   is very similar to Theorem  
and the conductance guarantee follows directly from Theorem   We leave the detailed proofs in the full version 
  Empirical Illustration
We have compared the performance of the CRD algorithm  Algorithm   the AndersenChung Lang local spectral algorithm  ACL    and the  owimprove algorithm  FlowImp   Andersen   Lang    Given   starting node vs and teleportation probability   ACL is   local algorithm that computes an approximate personalized
PageRank vector  which is then used to identify local structure via   sweep cut  FlowImp is    owbased algorithm
that takes as input   set of reference nodes and  nds   cluster around the given reference set with small conductance
value  Note that we only couple FlowImp with ACL  The

Capacity Releasing Diffusion for Speed and Locality

Table   Ground truth clusters

graph

feature

volume

nodes

cond 

 
 
 
 

 
 
 
 

 

 
 

 

 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

reason is that  while FlowImp needs   very good reference
set as input to give meaningful results in our setting  it can
be used as    clean up  step for spectral methods  since they
give good enough output  Note also that FlowImp has running time that depends on the volume of the entire graph 
as it optimizes   global objective  while our CRD algorithm
takes time linear in the volume of the local region explored 
We compare these methods on   datasets  one of which is
  synthetic grid graph  For the   realworld graphs  we
use the Facebook college graphs of John Hopkins  Hop 
Rice  Simmons  Sim  and Colgate  as introduced in Traud
et al    Each graph in the Facebook dataset comes
along with some features        dorm   and  class year
  We consider   set of nodes with the same feature as
   ground truth  cluster       students of year   We  lter out very noisy features via some reasonable thresholds 
and we run our computations on the the remaining features 
The clusters of the features we use are shown in Table  
We  lter bad clusters from all the ground truth clusters 
by setting reasonable thresholds on volume  conductance 
and gap  which is the ratio between the spectral gap of the
induced graph of cluster  and the cut conductance of the
cluster  In Table   we show the size and conductance of
the clusters of the features used in our experiments 
For the synthetic experiment  we measure performance by
conductance  the smaller the better  For realworld experiments  we use precision and recall  We also compare to
ACLopt which  cheats  in the sense that it uses ground
truth to choose the parameter   with best   score    combination of precision and recall 
For the synthetic data  we use   grid graph of size      
We add noise to the grid by randomly connecting two vertices  We illustrate the performance of the algorithms versus probability of random connection in Figure   The
range of probabilities was chosen consistent with theory 
As expected  CRD outperforms ACL in the intermediate
range  and the two method   performances meet at the endpoints  One view of this is that the random connections
initially adds noise to the local structure and eventually destroys it  CRD is more tolerant to this noise process 

Figure   Average results for       grid

Table   Results on Facebook graphs
feat 

ACLopt

CRD

ACL

mtr 

FlowImp

   

ID

 
 
 
 
 
 

 
 
 
 

Pr 
Re 
Pr 
Re 
Pr 
Re 
Pr 
Re 
Pr 
Re 
Pr 
Re 
Pr 
Re 
Pr 
Re 
Pr 
Re 
Pr 
Re 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

   
 
 
 
 
 

 

 

 
 
 
 
 
 
 

 

 

 

 

See Table   for results for realworld data  We run algorithms starting at each vertex in   random sample of half
the vertices in each cluster and report the median 
For clusters with good but not great conductance      
Rice   Colgate   CDR outperforms ACL and has
nearly identical performance to FlowImp  which  recall  is
  global algorithm  This is   consequence of CDR avoiding the trap of leaking mass out of the local structure  in
contrast to ACL  which leaks   large fraction of mass  For
clusters with great conductance  all methods perform very
well  and all methods perform poorly when the conductance of the clusters gets close to  
Here again  as with the synthetic data  we see that for high
conductance sets  which do not have good local structure 
and very good conductance sets  which have excellent local structure  all methods perform similarly  In the intermediate range       when there are moderately good but not
very good quality clusters  CDR shows distinct advantages 
as suggested by the theory 

Capacity Releasing Diffusion for Speed and Locality

Acknowledgements
SR and DW are supported by the National Science Foundation under Grant CCF  and CCF  MM
and KF would like to thank the Army Research Of ce
and the Defense Advanced Research Projects Agency for
partial support of this work  MH has received funding from the European Research Council under the European Union   Seventh Framework Programme  FP 
 ERC Grant Agreement no   

References
Alon  Noga  Benjamini  Itai  Lubetzky  Eyal  and Sodin 
Sasha  Nonbacktracking random walks mix faster 
Communications in Contemporary Mathematics   
   

Andersen     and Lang     An algorithm for improving

graph partitions  SODA   pp     

Andersen  Reid and Peres  Yuval  Finding sparse cuts locally using evolving sets  In STOC   pp   
 

Andersen  Reid  Chung  Fan  and Lang  Kevin  Local graph
partitioning using PageRank vectors  In FOCS   pp 
   

Arora  Sanjeev  Rao  Satish  and Vazirani  Umesh  Expander  ows  geometric embeddings and graph partitioning  Journal of the ACM  JACM     

Belkin     and Niyogi     Laplacian eigenmaps for dimensionality reduction and data representation  Neural Computation     

Cheeger  Jeff    lower bound for the smallest eigenvalue
of the Laplacian  In Proceedings of the Princeton conference in honor of Professor    Bochner   

Donath  William   and Hoffman  Alan    Lower bounds
for the partitioning of graphs  IBM Journal of Research
and Development     

Gleich        PageRank beyond the web  SIAM Review   

   

Goldberg  Andrew   and Tarjan  Robert      new approach to the maximum ow problem  Journal of the
ACM  JACM     

Goldberg  Andrew    and Tarjan  Robert Endre  Ef cient
maximum  ow algorithms  Commun  ACM   
   

Jeh  Glen and Widom  Jennifer  Scaling personalized web
search  In Proceedings of the  th international conference on World Wide Web  pp    ACM   

Jeub           Balachandran     Porter        Mucha       
and Mahoney        Think locally  act locally  Detection of small  mediumsized  and large communities in
large networks  Physical Review       

Leighton  Tom and Rao  Satish  An approximate max 
 ow mincut theorem for uniform multicommodity  ow
problems with applications to approximation algorithms 
In FOCS    pp    IEEE   

Leskovec     Lang       Dasgupta     and Mahoney 
     Community structure in large networks  Natural
cluster sizes and the absence of large wellde ned clusters  Internet Mathematics     

Mahoney        Orecchia     and Vishnoi          local
spectral method for graphs  with applications to improving graph partitions and exploring data graphs locally 
Journal of Machine Learning Research   
 

Ng       Jordan       and Weiss     On spectral clusIn NIPS   Protering  Analysis and an algorithm 
ceedings of the  th Annual Conference on Advances in
Neural Information Processing Systems   

Orecchia  Lorenzo  Sachdeva  Sushant  and Vishnoi 
Nisheeth    Approximating the exponential  the Lanczos method and an      time spectral algorithm for
In STOC   pp   
balanced separator 
ACM   

Page  Lawrence  Brin  Sergey  Motwani  Rajeev  and Winograd  Terry  The pagerank citation ranking  Bringing order to the web  Technical report  Stanford InfoLab   

Shi     and Malik     Normalized cuts and image segmentation  IEEE Transcations of Pattern Analysis and Machine Intelligence     

Shun     RoostaKhorasani     Fountoulakis     and Mahoney        Parallel local graph clustering  Proceedings of the VLDB Endowment     

Spielman  Daniel    and Teng  ShangHua  Nearlylinear
time algorithms for graph partitioning  graph sparsi cation  and solving linear systems  In STOC   pp   
   

Guattery     and Miller       On the quality of spectral
separators  SIAM Journal on Matrix Analysis and Applications     

Traud        Mucha        and Porter        Social structure of facebook networks  Physica    Statistical Mechanics and its Applications     

Capacity Releasing Diffusion for Speed and Locality

von Luxburg       tutorial on spectral clustering  Technical Report   Max Plank Institute for Biological Cybernetics  August  

White     and Smyth       spectral clustering approach to
 nding communities in graphs  In SDM   Proceedings of the  th SIAM International Conference on Data
Mining  pp     

Zhu  Zeyuan Allen  Lattanzi  Silvio  and Mirrokni  Vahab      local algorithm for  nding wellconnected clusters  In ICML   pp     

