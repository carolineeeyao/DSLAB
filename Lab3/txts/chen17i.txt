Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Guangyong Chen   Shengyu Zhang   Di Lin   Hui Huang   Pheng Ann Heng    

Abstract

While crowdsourcing has been   cost and time
ef cient method to label massive samples  one
critical issue is quality control  for which the key
challenge is to infer the ground truth from noisy
or even adversarial data by various users    large
class of crowdsourcing problems  such as those
involving age  grade  level  or stage  have an ordinal structure in their labels  Based on   technique of sampling estimated label from the posterior distribution  we de ne   novel separating
width among the labeled observations to characterize the quality of sampled labels  and develop
an ef cient algorithm to optimize it through solving multiple linear decision boundaries and adjusting prior distributions  Our algorithm is empirically evaluated on several real world datasets 
and demonstrates its supremacy over stateof 
theart methods 

  Introduction
Crowdsourcing has drawn increasing popularity in the  eld
of machine learning by annotating millions of items in  
short time with relatively low cost  Howe    Welinder   Perona  Deng et al    Jiang et al    This
provides   great opportunity to build up largescale training sets for complex models  such as deep neural networks
 Krizhevsky et al    and to reach consensus among
nonexperts  such as peer grading in today   popular massive open online course  MOOC  systems  However  the
quality of the collected results is often unreliable and diverse  and there are spammers who give random labels to
make easy money  or even adversaries who deliberately
give wrong answers  To address this issue  most crowd 

 The Chinese University of Hong Kong  Hong Kong  China 
 Shenzhen University  China   Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology  Shenzhen Institutes of Advanced Technology  Chinese Academy of
Sciences  Shenzhen  China  Correspondence to  Shengyu Zhang
 syzhang cse cuhk edu hk 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

sourcing systems resort to distributing each item to   number of redundant workers  This raises   challenging question of how to aggregate such noisy and redundant labels 
An intuitive and baseline approach for crowdsourcing is
to identify each item following the majority voting  MV 
result of workers  Unfortunately  this approach is errorprone since it treats each worker equally  and the accuracy severely deteriorates with the fraction of less quali ed
workers  spammers or adversaries  Weighted majority voting  WMV   Karger et al    method tries to address
this issue by associating each worker with   weight to characterize his expertise  Specially  maxmargin majority voting       method  Tian   Zhu    optimizes the associated variables in WMV by maximizing the minimal difference between the aggregated score of the potential true
label and the aggregated scores of others 
In   different approach  DawidSkene  DS  model  Dawid
  Skene    represents each worker   expertise by  
confusion matrix and uses   latent variable model to generate collected labels  which implicitly assumes   worker
to perform equally well across all items in   common
class  This model can be iteratively inferred by the famous
ExpectationMaximization  EM  method  Dempster et al 
  and works well in practice 
In particular   Zhang
et al    employs the spectral method  Anandkumar
et al    to initialize the DS model  and obtains an optimal convergence rate up to   logarithmic factor by EM
method  Recently   Zhou et al    Tian   Zhu   
proposed to improve the aggregating performance by integrating the merits of MV method with DS model  The performance of DS model and its variants often relies on the
specially conceived priors with some manually con gured
hyperparameters 
All the above mentioned approaches are for aggregating
general multiclass labels  In many practical applications 
however  the labels have   natural ordinal structure  For
instance  in MOOCs  students are often required to grade
their own assignments on an ordinal scale of    excellent 
   good     fair     pass  and    failure  In medical imaging  doctors are often required to mark images on an ordinal
scale of stage   stage   stage   and stage   Ordinal label faces an issue of diverse standards  For example  given
four assignments whose true grades are       and    

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

strict marker may rate them as       and   If we ignore
the ordinal structure  we may identify this marker as an adversary because all his answer labels are considered wrong 
But actually this marker grades all assignments in   correct
order  which should be incorporated to improve the crowdsourcing performance 
This inspires us to transform the Kclass ordinal labeling
to       binary classi cations  That is  instead of directly
using   label answer                          we use it
to answer       questions  Is the label greater than    for
all            In this way  the harsh marker   answer
  for the  rst assignment would give three correct answers
 on thresholds           and only one wrong answer  on
threshold       making the marker   answers highly useful in the aggregation 
For each binary problem  we can employ   Gibbs sampler
to generate label estimations from the generative model of
crowdsourcing  However  these label estimations could be
errorprone especially for the dif cult items  whose labels
may be sampled according to the uniform distribution  and
it is well known that the performance of   generative model
heavily relies on the specially conceived priors  To address these issues in binary crowdsourcing tasks  we de 
 ne   separating width to characterize the quality of label
estimations  and solve it by optimizing   linear decision
boundary  The similar idea has been previously explored in
 Cortes   Vapnik    and found   lot of success for supervising learning problems  By optimizing the separating
width among two classes  we can improve the sampling
accuracy and update the prior distributions automatically
during the learning process  To characterize the quality of
aggregating ordinal labels from   classes  we introduce
      decision boundaries to help optimize the separating
width  As demonstrated empirically  our method achieves
the best performance on the realworld datasets compared
to other stateof theart methods 
The rest of this paper is organized as follows  Sec    introduces some preliminary works for crowdsourcing tasks 
Sec    presents the generative model employed in this paper  Sec    derives the objective function for binary aggregating problem  which is extended for the ordinal case
in Sec    The derivations of inference method are discussed in Sec    Sec    evaluates the performance of our
method on some realworld datasets  and Sec    concludes
this paper 

  Problem Setting and Preliminary Work
In this section  we formalize the problem and survey some
preliminary methods  Suppose that there are   workers
and   items taken from   total of   classes  For item   
de ne an       matrix Ri by putting Ri
jk     if worker

jk     otherwise  Note that
  labels the item as    and Ri
Ri is   highly sparse matrix since each item is usually assigned to   small number of workers  The objective of  
crowdsourcing problem is to identify the true label zi of
item   based on the sparse matrices             RN 

  Majority Voting Method and its Variants

Majority Voting  MV  has been widely used to  nd the
most likely label for item   by solving the following problem 

zi   argk max   

  Riek 

 

where    is   allone column vector of dimension   and
ek is the kth standard basis  Weighted majority voting
 WMV   Karger et al    generalizes MV by assigning weight vector     RM  to the workers and solving
the following problem

zi   argk max    Riek 

 

Specially  maxmargin majority voting        Tian  
Zhu    de nes the crowdsourcing margin as the minimal difference between the aggregated score of the potential true label and aggregated scores of other labels  and
solves   by maximizing the sum of the crowdsourcing margins of all items 

  DawidSkene Model and its Variants

DawidSkene  DS  model has been another popular way to
aggregate collected labels by capturing the uncertainties of
labeling behaviors in   generative model  Compared with
WMV and      both of which characterize the expertise
of worker   by   scaler variable  DS model characterizes
the expertise of worker   with an individual confusion matrix Aj   RK    where the       th entry denotes the
probability that worker   labels   class   sample as class   
Denote      Aj  
   DS model aims to maximize the
likelihood of observed samples      Ri  
   as follows 

  Ri zi      zi dzi 

 

  cid 

 cid 
where   Ri zi        cid  

   

max

  

ln

 

 cid  
  Aj
jd and zi is  
            This likelilatent variable with   zi     
hood function can be optimized iteratively by EM method
 Dempster et al    as 
  cid 

  cid 

Estep    zi        exp

zid Ri

jd ln Aj

Ri

  

kd 

kd     cid 

  

Mstep  Aj

  

  

  zi     Ri

jd 

 

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

  

   Ri

jd ln Aj

 cid  

rule zi   argk max exp cid  

Thus  the collected labels are aggregated following the
kd  where
the unknown parameters   can be updated in Mstep
through maximum likelihood estimation  MLE  principle 
Recently  spectral methods have been applied to obtain  
better initialization of the DS model  Zhang et al   
which achieves an optimal convergence rate up to   logarithmic factor  By assuming some special structures of the
confusion matrices     Raykar et al    studies homogeneous DS model  and  Moreno et al    studies the
existence of clusters of workers 

  Recent Achievements

Some recent improvements have been achieved by combining MV related methods and DS related methods   Zhou
et al    assumes labels are generated according to  
distribution over workers  items and labels  which can be
inferred by minimizing its entropy with constraints developed from MV method and DS model   Tian   Zhu   
incorporates     method with DS model in   regularized
Bayesian framework  Zhu et al    and approximates
the posterior distribution over the true labels with   Gibbs
sampler  Nowadays  binary and general multiclass crowdsourcing problems have been widely studied in the literature  but the ordinal sibling has not received nearly as much
attention yet  The work  Zhou et al    tries to use
the ordinal structure and makes an assumption that workers have dif culty distinguishing between two adjacent ordinal classes whereas it is much easier to distinguish between two faraway classes  In this paper  we will develop  
novel objective function to aggregate the ordinal labels  and
achieve the best performance on the realworld datasets 

  Generative Model of Crowdsourcing
In this section  we present   fully Bayesian model to generate observed matrices    First we note that some items
may be intrinsically hard to label even for experts  which
is not uncommon in  for example  medical imaging  To
model such dif culty  we introduce   Kdimensional vector    to denote the prior distribution of true label of the
item   even for experts   For items clearly from category
   the vector    would be just the standard basis ek  Denote         
   We can obtain   joint distribution as
follows 

             

 Aj

kd Ri

jd

  zi    

 

  zi   

 

 

       

where   contains the confusion matrices of all workers like
DS model    is the label vector to be solved and    is an
indicator function  Since usually most workers just annotate   few items  we may not have suf cient samples to
infer      and   To overcome this  we formulate   fully

 cid 

  cid 

  

 

Figure   Illustrating how the Dirichlet density changes with respect to the scalar value  

Figure   Graphical model of our generative model 

Bayesian framework over   and   with prior from Dirichlet distributions  Minka      family that has found numerous successful applications  such as topic models  to
generate prior distributions  We assume that both workers 
expertise   and items  dif culty   are random variables
from the family of Dirichlet distributions

      

   
   

 

  

 
where   is the gamma function  As illustrated in Figure   the concentration parameter   controls the sparsity
preference of random vector    precisely described item  
should have    be associated with   small concentration parameter  resulting in   sparse prior vector  while   vaguely
described item should be associated with   large concentration parameter  To model the expertise Aj of the worker
   we also prefer that it has   small concentration parameter  We formulate the prior distributions over   and   as
follows 
      

         

      

 cid 

 cid 

  Aj

 

   

 

where         
and   gives the following joint distribution 

   and         

   Combining Eq   

                                  

The graphical model can be found in Fig   
Given the matrices    we can get the posterior distribution
over      and   which can be formulated as

 cid                dAz 

              

                 

 
 
We can obtain   classi er as argz max             
the item 

argz max cid                 dA  to label

       Learning to Aggregate Ordinal Labels by Maximizing Separating Width

parametrized by the hyperparameters   and   Conventionally  researchers mainly focus on how to approximate
the posterior distribution with better accuracy and runningtime performance with the  xed prior distributions  or updating the prior distributions by introducing new priors
over   and    Kim   Ghahramani    Moreno et al 
  However  the performance of the above generative
model heavily relies on the specially conceived priors to
incorporate domain knowledge  which transmits affects on
the posterior estimations through Bayes  rules  Given  
family of prior choices  we prefer the classi er with the
more powerful discriminative capability to achieve better
generalization performance 

  Maximizing the Separating Distance
  Binary Crowdsourcing Problem

Before we present the objective function to aggregate ordinal labels  we  rstly consider   simple case  the binary
crowdsourcing problem with       As shown in Eq   
by varying the hyperparameters   and   we can obtain
  series of posterior approximations to identify unlabeled
items via Bayes rule  Moreover  by  xing the hyperparameters   and   we can get multiple estimations of the true
label of one item  which are randomly sampled from the
posterior distribution over its true label  Thus  we are motivated to  nd the most favored set of label estimations over
all items 
For   better generalization performance  we try to maximize the separation width between two classes  As shown
in Fig    the label set   is preferred to the set   because
the set   has   larger separation width between two classes 
To evaluate the separating width of samples with the label
set      zi  
   with zi               we introduce   linear decision boundary    Ri    aT Rib with
    RM  and     RK  Our decision boundary is formulated refer to the formulas in Eq    and   where  
denote the worker expertise and   transforms worker   label into   scale variable  Thus  we de ne an optimization
problem as 

min
   

    

           cid   cid 
ziaT Rib    

 cid   cid 
 
         

 

    xT   and the minimal value         charwhere  cid   cid 
acterizes the separating width of the label set    This optimization problem can be understood from the objective
function used in support vector machine  SVM   Cortes  
Vapnik    where the objective function is to maximize
the margin width  cid baT cid        cid   cid 
  and the
constrains state that all samples lie on the correct side of
the margin   The constraint in the above optimization problem can be viewed as the inner product of Ri and   rank 

 cid   cid 

  matrix baT   One may wonder why con ning to rank 
measurements  Note that MV  Eq  and WMV  Eq 
are also of the rank  form  and our experiments also show
that using higher rank measurements actually makes the
generalization performance worse  see experiments in Appendix  Since   is   random variable generated from the
posterior distribution   we need to reformulate the objective function   as follows 

               cid   cid 

 cid   cid 
 
min
    
     Ep zi Ri ziaT Rib    

                                

 

         

  

min

   

    

 cid   cid 

  cid 

Practically  the labeled samples are often linearly inseparable by   single hyperplane  see Set   in Fig    To cope
with this issue  we relax the hard constrains by introducing
nonnegative slack variables     one for each sample  and
obtain    soft  model as follows 
               cid   cid 

 
 
     Ep zi Ri ziaT Rib          

 
   
         
                                
is used as   positive regularization parameter for
where  
later convenience  and        is the softmargin for item
 
  
If Ri lies on the correct side of the margin        
For sample on the wrong side     is proportional to the
distance to the margin  Thus  the value of    re ects the
dif culty of identifying item    or the error allowed to misclassify the item    The calculation of                
is intractable because it involves that of the marginal distribution        To address this issue  we introduce  
redundant distribution           and rewrite the optimization problem as follows 

min

      

                 cid   cid 

 cid   cid 

   

 
 

  cid 

  

   

 

     Eq zi ziaT Rib          

KL   cid       

         

where   and   are shorthand for           and
                respectively  to simplify the presentations in the rest of the paper  Let        ziaT Rib  we can
turn the optimization problem into the following one with
two regularization terms  mina                    
where   is de ned by
                 KL   cid       cid   cid 

 cid   cid 

 

  cid 

 cid 

  

zi 

   

  zi   

 

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Figure   Four label sets  Set   contains one possible label set  Set   contains another label set  which is preferred to the set   Set
  contains   label set  whose separating width can be estimated with the slack variables  Set   contains   set of ordinal labels  whose
separating width can be estimated by transforming the ordinal problem into multiple binary ones 
where       max      is the hinge loss function
widely used in training classi ers   The factor   is introduced here to simplify the derivations of inference methods
later  By optimizing the unknown parameters in the objective function in Eq  we can obtain the estimated labels
with the largest separating width 

  Ordinal Crowdsourcing Problem

As introduced in Section   transforming   Kclass ordinal labeling problem  what is the label of this item  to
       binary classi cation problems  Is the label of this
item greater than    for            allows us to exploit
more useful information from workers  The transform is
illustrated in Set   of Fig    where we have items coming
from   ordered classes            CK  We look for    decision boundaries  with boundary   discriminating classes
         Ct and classes Ct        CK  For the tth
binary question  we introduce   linear decision boundary
  Ribt  It is easily veri ed that all boundas ft Ri    aT
aries intersecting at the zero point  With      at   
   and
     bt   
     the loss function in Eq    becomes

                KL   cid       

  cid 
  cid 

  

  cid 

 

 cid at cid 

 cid bt cid 
  cid 

   

  zi 

 it   

  

zi 

  

where  it       sgnt zi aT
  Ribt with sgnt zi      if
zi     and sgnt zi      if zi      It is obvious that our ordinal model will degenerate into binary one when      
When       the ordinal label should be estimated by considering the predicted results from       binary problems 

  Inference Details
In this section  we present the implementation details to
infer the true labels and all other unknown parameters involved in ordinal crowdsourcing problems  Our inference
method consists of two parts  In the  rst part  we employ

  Gibbs sampler to approximately sample from the posterior distribution                     In the second
part  we update the hyperparameters     and linear decision boundaries based on the gradient method to achieve
the largest separating width 
To approximate the intractable posterior distribution   
there are two standard approaches  which are Variational
Bayesian  VB  and Gibbs sampling  Compared with the
Gibbs sampling method  VB is usually dif cult in its functional optimization  especially hard in our case due to the
hinge loss function  Moreover  VB often suffers from inaccuracy because of the potentially impractical assumption
of independence of variables  Gibbs sampling is applicable
here  because it provides numerical approximations to the
integration problems in large dimensional spaces by generating an instance from the conditional distribution of each
variable in turn  It can be shown that the sequence of samples constitutes   Markov chain  which  nally converges to
the targeted posterior distribution as the stationary distribution 
Since the sampling process of the confusion matrices  
and the items  dif culties   can be developed in the standard way  we leave their derivations in Appendix and
mainly discuss the sampling process of true labels   here 
The dif culty of sampling   is mainly due to the hinge
loss function  it  We employ data augmented technique  Polson   Scott    to approximate the hinge loss
function  According to the equality  Andrews   Mallows 
 

exp it   

 zi   it Ri   it 
  exp   

 it

 

with  zi   it Ri     it   
 it    it 
and  it as   nonnegative augmented variable  we can reformulate the objective function in Eq  as follows 
                 KL   cid       

 cid 

 

aT
  atbT

  bt

 cid 

 cid 

 cid 

  zi  

 

 

  zi   it  ln

  it 

 zi   it Ri 

  it 

 Set  Set  Set  Set  Learning to Aggregate Ordinal Labels by Maximizing Separating Width

where the inequality comes from Jensen   inequality with
  new distribution   it  to help to approximate the hinge
loss function exp it  Note that the right hand
side of the inequality is tractable  minimizing which would
give an upper bound of the original optimization problem 
Before we sample the true labels of items  we need to  rst
generate augmented variables      it     
     When
 xing other random variables  we can generate the       th
augmented parameter according to the following generalized inverse Gaussian  GIG  distribution 

 it    
 

   
it

 

 

exp   
 

 it  

 
   
it
 it

 

 

where   is the normalization term  It has been shown that
can be drawn ef ciently with    time complexity
 
 it
 Michael et al   
 cid   
Here  we can sample the true labels of all items  Let
    zi   it Ri  Rewrite the objective function shown in Eq    with respect to   zi  as
follows 

           cid  

  

    zi 

  KL   cid     

 cid 

 cid 

  zi  

  zi   it  ln

  it 

 zi   it Ri 

  it

  cid 

  KL       cid          

 
Thus  with all other parameters  xed  we can sample zi  
    according to the following distribution 

  zi      Ri     zi       

 zi   it Ri
  

 

  

Let us examine the two terms on the right hand side  The
 rst term comes from the generative model of crowdsourcing  while the second term maximizes the separating width
of the estimated ordinal labels  For the binary crowdsourcing problem  we have only one decision boundary to measure the separating width  while for the ordinal crowdsourcing problem with   classes  we get       intersected decision boundaries to measure the separating width  After
obtaining   set of random samples to approximate the joint
posterior distribution   over all model parameters and augmented variables  the objective function shown in Eq   
becomes   parametric function with respect to         and
  Thus  we can intuitively update these parameters based
on the gradient method  The derivations of the updating
formulas over         and   can be found in Appendix 

Let  cid      cid     cid           dx denote the expectation of

      with respect to the distribution of      Our method is
outlined in Algorithm   in which each while iteration consists of two for loops  and the source code with demo can
be found on the website  In the  rst for loop  we employ

 http appsrv cse cuhk edu hk gychen 

       and the learning rates  

   by MV          and  

kd    cid  

Algorithm   Our Ordinal Crowdsourcing Method
  Input       Ri  
  Initializing      zi  
  while not convergence do
 
 
 
 
 
 
 
 

for           do
kd     Aj
Aj
   Ri
jd
        zi     
       
  
   
 it    
   it    
 
   
it
 it
    zi   it Ri
  

zi     Ri     zi       cid   

  zi     

exp   

 

it

 

end for
for               do
 at    cid bt cid 
at    
 bt    cid at cid 
bt    

 cid  
 cid  

    cid  
    cid  

      

  

at

bt

end for
                  
                 

  

  

      
         
         

 

 

 
 
 

 
  end while

 
 cid it cid  RibtbT

  RiT
 cid it cid   cid sgnt zi cid Ribt 
  RiRiT at
 cid it cid   cid sgnt zi cid aT
  Ri 

 
 cid it cid  aT

  

  Gibbs sampler to generate the random variables to approximate the posterior distribution  In the second part  we
solve the separating width by optimizing       decision
boundaries  and update the prior distributions by gradient
method  Compared with the traditional generative model
of crowdsourcing  including DS model and its variants  our
method introduces an augment variable  it to approximate
the hinge loss function  which is further involved in the
generation of true labels  This algorithm is iteratively implemented to reach   local optimum 

  Experiments and Discussions
To fully evaluate the ideas employed in this paper  we
present empirical studies of our aggregating method in
comparison with competitive ones not only on ordinal
crowdsourcing tasks  but also binary crowdsourcing tasks 
For our method  we con gure                       
               and initialize zi by the majority voting
result  In each run of our method  we generate   samples
to approximate the posterior distribution and discard the
 rst   samples as burnin steps  The reported error rate of
our method is averaged over   runs  and all experiments
are conducted in   PC with Intel Core     GHz CPU and
 GB RAM 

  Binary Crowdsourcing Tasks

We  rst evaluate our method on three binary benchmark
datasets shown in Table   include labeling bird species

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Table   The summary of realworld datasets used in the comparison experiments 

Items Workers Labels item
 
 

Name
Bird
RTE
TREC
Web
Age

Classes

 
 
 
 
 

 
 
 

 
 
 
 
 

 
 
 
 
 

  Ordinal Crowdsourcing Tasks

In this part  we report empirical results of our method on
ordinal benchmark datasets in comparison with competitive ones  We consider MV  MVDS  and GCrowdSVM as
baselines  and compare our method with Entropy     Zhou
et al    which consider the ordinal structures in labels  As shown in Table   we have two ordinal datasets 
One is to judge the relevance of queryURL pairs with
   level rating score  Web dataset  and the other is to
identify the age of each subject with    level rating score
 Age dataset  To evaluate the performance of aggregating ordinal labels  we de ne three following error measurements as              cid        cid          cid        cid  and
        cid        cid  Compared with the error rate    the
measures    and    take precision into consideration  and
may be preferred for aggregating ordinal labels when one
cares about the severity of error 
Table   summarizes the performance of all methods on
the ordinal datasets  and shows that our method consistently outperforms the others in predicting the ordinal labels of items  Similar with our method  GCrowdSVM
attempts to maximize the margin between the aggregated
score of potential true label and the aggregated score of
others  and achieves the better performance in comparison
with the stateof theart method to aggregate ordinal labels 
Entropy    Compared with GCrowdSVM  we treat the
problem of aggregating collected labels as the classi cation
problem  and introduce       decision boundaries to consider the ordinal relationship among categories  As shown
in Table   on the Web dataset  our method signi cantly reduces the average    error rate from   to   In addition  the average    error of our method is   which
is only slightly larger than the    error rate of   It
means that  even for the incorrect labels  zi outputted by
our algorithm  our  zi is not far away from its true answer
zi  resulting in   relatively small damage 
The Web dataset has been widely used in the evaluation
of ordinal crowdsourcing problem  On this dataset  our
method introduces   decision boundaries to measure the
separating width of generated true labels  To help to understand the linear decision boundaries learned by our method 
we illustrate the average value over   samples of  bt 
  
as Fig    It can be seen that the absolute value of all entries in bt is approximated to   To be more concrete  let us
consider the  rst decision boundary with    which calculate Rib  for the item    Thus  Rib  successfully reduces
the ordinal problem into   binary one  the jth entry in Ribi
would be   if worker   ranks item   as   and   if worker
  rank item   as       and   Note that at characterizes the
expertise of all workers for the tth binary problem  Fig   
contains three confusion matrices  including the averaged
confusion matrix of all workers  the confusion matrices of

Figure   The average value of   samples of  bt 
run 

   in   random

 Welinder et al     Bird dataset  recognizing textual
entailment  Snow et al     RTE dataset  and accessing the relevance of topicdocument pairs with   binary
judgment in TREC   crowdsourcing track  Gabriella  
Matthew     TREC dataset  The competitive methods include the pure majority voting estimator  refereed to
as MV  the EM method for DS model initialized by majority voting  refereed to as MVDS  the EM method for
DS model initialized by spectral method  refereed to as
OptDS   Zhang et al    the Gibbs sampler for the
Bayesian extension of      Tian   Zhu     referred
to as GCrowdSVM  the SVDbased algorithm proposed
in  Ghosh et al     referred to as GhSVD  and the
Eigenvalues of Ratio algorithm proposed in  Dalvi et al 
   referred to as EigRatio  The performance of all
methods are evaluated by error as             cid        cid 
where   contains true labels of items  available in all these
datasets  and    contains estimations given by our algorithm
 not using any information of    Noted that the reported
error rates of GCrowdSVM are the average over   random runs as our method 
As shown in Table   our method achieves the best performance among all methods on three benchmark datasets 
Without regards to the prior knowledges over workers  expertise and items  dif culties  we can degenerate our model
into MVDS model by setting           Comparing with the performance of MVDS model  especially
OptDS method  we present   more complicated generative model  leading to better predictive results  Compared
with GCrowdSVM method  our method updates prior distributions and improves the sampling accuracy by optimizing the separating width during the learning process  which
leads to the improvements of predictive performance on all
datasets 

 Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Binary Dataset

Bird
RTE
TREC

Table      error rate   in predicting the latent labels on three binary benchmark datasets 

Ours

 
 
 

GCrowdSVM OptDS MVDS MV
 
 
 
 
 
 

 
 
 

 
 
 

GhSVD EigRatio

 
 
 

 
 
 

Ordinal Dataset

Web

Age

  
  
  
  
  
  

Table   Errors in predicting the latent labels on two ordinal benchmark datasets 

Ours

 
 
 
 
 
 

GCrowdSVM Entropy    MVDS MV
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

Figure       the averaged confusion matrix over all worker      the confusion matrix of an expert      the confusion matrix of   spammer 

  Conclusions
In this paper  we develop   novel method to aggregate
ordinal labels by optimizing the separating width among
classes  To measure the separating width among ordinal
labels  we  rst investigate   binary case  and then extend
our achievements to the ordinal one  With       decision
boundaries  we de ne an optimization problem for measuring the separating width among ordinal classes  The newly
introduced boundaries not only help to optimize the hyperparameters  but also calibrate the estimated labels sampled
from the generative model    Gibbs sampler is adopted to
approximate the posterior distribution  while the gradient
method is used to calculate the separating width and optimize the hyperparameters 
As demonstrated on the ordinal datasets  which is the main
focus of this paper  our method consistently achieves the
best performance compared with competitive ones  and
the improvements on Web dataset are signi cant  As
demonstrated by the experimental results on the binary
datasets  our algorithm works slightly better than any previous method  Thus  our algorithm provides   uniform
method in both binary and ordinal cases  and can be practically useful for realworld applications 

Figure   Error rates per iteration of various estimators on the Web
dataset 

an expert and   spammer  It can be found that the spammer ranks all items randomly to make easy money  while
the expert has   confusion matrix similar to the identical
matrix  Our method can accurately estimate the confusion
matrices of all workers even given the averaged confusion
matrix acts like   spammer  Fig    summarizes the training
time and error rates after each iteration for all estimators on
the Web dataset  It can be found that the proposed method
coverages to   lower error rate and all other three methods
have error convergence curves all above ours 

       Time    Error rateMVDSEntropy   GCrowdSVMOursLearning to Aggregate Ordinal Labels by Maximizing Separating Width

Acknowledgements
We would like to thank anonymous reviewers for their
valuable comments to improve the presentation of this paper  This work was supported by the China   Program
 Project No   CB  and   grant from the National Natural Science Foundation of China  Project No 
  Shengyu Zhang was supported by Research
Grants Council of the Hong Kong         Project no 
CUHK 

References
Anandkumar     Liu        Hsu        Foster        and
Kakade          spectral algorithm for latent dirichlet
allocation  In Advances in Neural Information Processing Systems  pp     

Andrews        and Mallows        Scale mixtures of normal distributions  Journal of the Royal Statistical Society  Series    Methodological  pp     

Cortes     and Vapnik     Supportvector networks  Ma 

chine learning     

Dalvi     Dasgupta     and Kumar      and Rastogi    
Aggregating crowdsourced binary ratings  In Proceedings of the  nd international conference on World Wide
Web  pp    ACM   

Dawid        and Skene        Maximum likelihood estimation of observer errorrates using the em algorithm 
Applied statistics  pp     

Dempster        Laird        and Rubin        Maximum
likelihood from incomplete data via the em algorithm 
Journal of the royal statistical society  Series    methodological  pp     

Deng     Krause     and Li        Finegrained crowdsourcing for  negrained recognition  In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition  pp     

Gabriella     and Matthew     Overview of the trec  
In Proceedings of TREC  

crowdsourcing track 
 

Ghosh     Kale     and McAfee     Who moderates
the moderators  crowdsourcing abuse detection in usergenerated content  In Proceedings of the  th ACM conference on Electronic commerce  pp    ACM 
 

Howe     The rise of crowdsourcing  Wired magazine   

   

Jiang     Huang     Duan     and Zhao     Salicon 
Saliency in context  In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp 
  IEEE   

Karger        Oh     and Shah     Iterative learning for
reliable crowdsourcing systems  In Advances in Neural
Information Processing Systems  pp     

Kim        and Ghahramani     Bayesian classi er combi 

nation  In AISTATS  pp     

Krizhevsky     Sutskever     and Hinton        Imagenet
classi cation with deep convolutional neural networks 
In Advances in Neural Information Processing Systems 
pp     

Michael        Schucany        and Haas        Generating random variates using transformations with multiple
roots  The American Statistician     

Minka     Estimating   dirichlet distribution   

Moreno        Art esRodr guez     Teh        and
PerezCruz     Bayesian nonparametric crowdsourcing 
Journal of Machine Learning Research   
 

Polson        and Scott        Data augmentation for
support vector machines  Bayesian Analysis   
 

Raykar        Yu     Zhao        Valadez        Florin 
   Bogoni     and Moy     Learning from crowds  Journal of Machine Learning Research   Apr 
 

Snow       Connor     Jurafsky     and Ng        Cheap
and fast but is it good  evaluating nonexpert annotations for natural language tasks  In Proceedings of the
conference on empirical methods in natural language
processing  pp    Association for Computational
Linguistics   

Tian     and Zhu     Maxmargin majority voting for learnIn Advances in Neural Information

ing from crowds 
Processing Systems  pp     

Welinder     and Perona     Online crowdsourcing  Rating
annotators and obtaining costeffective labels  In  
IEEE Computer Society Conference on Computer Vision
and Pattern RecognitionWorkshops 

Welinder     Branson     Perona     and Belongie       
The multidimensional wisdom of crowds  In Advances in
Neural Information Processing Systems  pp   
 

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Zhang     Chen     Zhou     and Jordan        Spectral methods meet em    provably optimal algorithm for
crowdsourcing  Journal of Machine Learning Research 
   

Zhou     Basu     Mao     and Platt        Learning
from the wisdom of crowds by minimax entropy  In Advances in Neural Information Processing Systems  pp 
   

Zhou     Liu     Platt        and Meek     Aggregating ordinal labels from crowds by minimax conditional
entropy  In ICML  pp     

Zhu     Chen     and Xing        Bayesian inference with
posterior regularization and applications to in nite latent
svms  Journal of Machine Learning Research   
   

