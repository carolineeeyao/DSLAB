Learning Discrete Representations via Information Maximizing

SelfAugmented Training

Weihua Hu     Takeru Miyato     Seiya Tokui     Eiichi Matsumoto     Masashi Sugiyama    

Abstract

Learning discrete representations of data is   central machine learning task because of the compactness of the representations and ease of interpretation  The task includes clustering and
hash learning as special cases  Deep neural networks are promising to be used because they can
model the nonlinearity of data and scale to large
datasets  However  their model complexity is
huge  and therefore  we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for
applications of interest  To this end  we propose   method called Information Maximizing
SelfAugmented Training  IMSAT  In IMSAT 
we use data augmentation to impose the invariance on discrete representations  More speci 
cally  we encourage the predicted representations
of augmented data points to be close to those of
the original data points in an endto end fashion 
At the same time  we maximize the informationtheoretic dependency between data and their predicted discrete representations  Extensive experiments on benchmark datasets show that IMSAT
produces stateof theart results for both clustering and unsupervised hash learning 

  Introduction
The task of unsupervised discrete representation learning
is to obtain   function that maps similar  resp  dissimilar 
data into similar  resp  dissimilar  discrete representations 
where the similarity of data is de ned according to applications of interest 
It is   central machine learning task
because of the compactness of the representations and ease
 University of Tokyo  Japan  RIKEN AIP  Japan  Preferred
Networks  Inc  Japan  ATR Cognitive Mechanism Laboratories  Japan  Correspondence to  Weihua Hu  hu ms   utokyo ac jp  Takeru Miyato  takeru miyato gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

 
 
 

 
 
 

 
 
 

Discrete representations

Similarization

Information 
Maximization

Data augmentation

Figure Basic idea of our proposed method for unsupervised discrete representation learning  We encourage the prediction of
  neural network to remain unchanged under data augmentation
 Red arrows  while maximizing the informationtheoretic dependency between data and their representations  Blue arrow 

of interpretation  The task includes two important machine
learning tasks as special cases  clustering and unsupervised
hash learning  Clustering is widely applied to datadriven
application domains  Berkhin    while hash learning
is popular for an approximate nearest neighbor search for
large scale information retrieval  Wang et al   
Deep neural networks are promising to be used thanks to
their scalability and  exibility of representing complicated 
nonlinear decision boundaries  However 
their model
complexity is huge  and therefore  regularization of the networks is crucial to learn meaningful representations of data 
Particularly  in unsupervised representation learning  target representations are not provided and hence  are unconstrained  Therefore  we need to carefully regularize the networks in order to learn useful representations that exhibit
intended invariance for applications of interest       invariance to small perturbations or af ne transformation  Na ve
regularization to use is   weight decay  Erin Liong et al 
  Such regularization  however  encourages global
smoothness of the function prediction  thus  may not necessarily impose the intended invariance on the predicted discrete representations 

Learning Discrete Representations via Information Maximizing SelfAugmented Training

Instead  in this paper  we use data augmentation to model
the invariance of learned data representations  More specifically  we map data points into their discrete representations by   deep neural network and regularize it by encouraging its prediction to be invariant to data augmentation 
The predicted discrete representations then exhibit the invariance speci ed by the augmentation  Our proposed regularization method is illustrated as red arrows in Figure  
As depicted  we encourage the predicted representations of
augmented data points to be close to those of the original data points in an endto end fashion  We term such
regularization SelfAugmented Training  SAT  SAT is inspired by the recent success in regularization of neural networks in semisupervised learning  Bachman et al   
Miyato et al    Sajjadi et al    SAT is  exible to
impose various types of invariances on the representations
predicted by neural networks  For example  it is generally
preferred for data representations to be locally invariant 
     remain unchanged under local perturbations on data
points  Using SAT  we can impose the local invariance on
the representations by pushing the predictions of perturbed
data points to be close to those of the original data points 
For image data  it may also be preferred for data representations to be invariant under af ne distortion       rotation 
scaling and parallel movement  We can similarly impose
the invariance via SAT by using the af ne distortion for the
data augmentation 
We then combine the SAT with the Regularized Information Maximization  RIM  for clustering  Gomes et al 
  Bridle et al    and arrive at our Information Maximizing SelfAugmented Training  IMSAT  an
informationtheoretic method for learning discrete representations using deep neural networks  We illustrate the
basic idea of IMSAT in Figure   Following the RIM  we
maximize information theoretic dependency between inputs and their mapped outputs  while regularizing the mapping function  IMSAT differs from the original RIM in two
ways  First  IMSAT deals with   more general setting of
learning discrete representations  thus  is also applicable to
hash learning  Second  it uses   deep neural network for the
mapping function and regularizes it in an endto end fashion via SAT  Learning with our method can be performed
by stochastic gradient descent  SGD  thus  scales well to
large datasets 
In summary  our contributions are    an informationtheoretic method for unsupervised discrete representation
learning using deep neural networks with the endto end
regularization  and   adaptations of the method to clustering and hash learning to achieve the stateof theart performance on several benchmark datasets 
The rest of the paper is organized as follows  Related work
is summarized in Section   while our method  IMSAT  is

presented in Section   Experiments are provided in Section   and conclusions are drawn in Section  

  Related Work
Various methods have been proposed for clustering and
hash learning  The representative ones include Kmeans
clustering and hashing  He et al    Gaussian mixture model clustering  iterative quantization  Gong et al 
  and minimalloss hashing  Norouzi   Blei   
However  these methods can only model linear boundaries between different representations 
thus  cannot   
to nonlinear structures of data  Kernelbased  Xu et al 
  Kulis   Darrell    and spectral  Ng et al   
Weiss et al    methods can model the nonlinearity of
data  but they are dif cult to scale to large datasets 
Recently  clustering and hash learning using deep neural networks have attracted much attention 
In clustering  Xie et al    proposed to use deep neural networks to simultaneously learn feature representations and
cluster assignments  while Dilokthanakul et al    and
Zheng et al    proposed to model the data generation
process by using deep generative models with Gaussian
mixture models as prior distributions 
Regarding hashing learning    number of studies have
used deep neural networks for supervised hash learning
and achieved stateof theart results on image and text
retrievals  Xia et al    Lai et al    Zhang et al 
  Xu et al    Li et al    Relatively few studies have focused on unsupervised hash learning using deep
neural networks  The pioneering work is semantic hashing  which uses stacked RBM models to learn compact
binary representations  Salakhutdinov   Hinton   
Erin Liong et al    recently proposed to use deep neural networks for the mapping function and achieved stateof theart results  These unsupervised methods  however 
did not explicitly intended impose the invariance on the
learned representations  Consequently  the predicted representations may not be useful for applications of interest 
In supervised and semisupervised learning scenarios  data
augmentation has been widely used to regularize neural
networks  Leen   showed that applying data augmentation to   supervised learning problem is equivalent
to adding   regularization to the original cost function 
Bachman et al    Miyato et al    Sajjadi et al 
  showed that such regularization can be adapted to
semisupervised learning settings to achieve stateof theart performance 
In
scenarios 
unsupervised
Dosovitskiy et al    proposed to use data augmentation to model the invariance of learned representations 
Our IMSAT is different from Dosovitskiy et al   

representation

learning

Learning Discrete Representations via Information Maximizing SelfAugmented Training

in two important aspects    IMSAT directly imposes
the invariance on the learned representations  while
Dosovitskiy et al    imposes invariance on surrogate
classes  not directly on the learned representations   
IMSAT focuses on learning discrete representations that
are directly usable for clustering and hash learning  while
Dosovitskiy et al    focused on learning continuous
representations that are then used for other tasks such
as classi cation and clustering  Relation of our work to
denoising and contractive autoencoders  Vincent et al 
  Rifai et al    is discussed in Appendix   

  Method
Let   and   denote the domains of inputs and discrete representations  respectively  Given training samples 
               xN  the task of discrete representation learning is to obtain   function            that maps similar
inputs into similar discrete representations  The similarity
of data is de ned according to applications of interest 
We organize Section   as follows  In Section   we review the RIM for clustering  Gomes et al    In Section   we present our proposed method  IMSAT  for discrete representation learning  In Sections   and   we
adapt IMSAT to the tasks of clustering and hash learning 
respectively  In Section   we discuss an approximation
technique for scaling up our method 

  Review of Regularized Information Maximization

for Clustering

such that mutual

learns   probabilisThe RIM  Gomes et al   
information
tic classi er       
 Cover   Thomas    between inputs and cluster assignments is maximized  At the same time  it regularizes the complexity of the classi er  Let      and
                denote random variables for
     
data and cluster assignments  respectively  where   is the
number of clusters  The RIM minimizes the objective 

              

 

where    is the regularization penalty  and          is
mutual information between   and     which depends on
  through the classi er         Mutual information measures the statistical dependency between   and     and is
  iff they are independent  Hyperparameter       trades
off the two terms 

  Information Maximizing SelfAugmented Training
Here  we present two components that make up our IMSAT 
We present the Information Maximization part in Section
  and the SAT part in Section    

  INFORMATION MAXIMIZATION FOR LEARNING

DISCRETE REPRESENTATIONS

We extend the RIM and consider learning Mdimensional
discrete representations of data  Let the output domain be
           where Ym               Vm     
       Let                 YM      be   random variable
for the discrete representation  Our goal is to learn   multioutput probabilistic classi er              yM    that maps
similar inputs into similar representations  For simplicity 
we model the conditional probability              yM    by
using the deep neural network depicted in Figure   Under the model              yM  are conditionally independent
given   

             yM     

    

  ym   

 

Following the RIM  Gomes et al    we maximize the
mutual information between inputs and their discrete representations  while regularizing the multioutput probabilistic
classi er  The resulting objective to minimize looks exactly
the same as Eq    except that   is multidimensional in
our setting 

  REGULARIZATION OF DEEP NEURAL NETWORKS

VIA SELFAUGMENTED TRAINING

We present an intuitive and  exible regularization objective  termed SelfAugmented Training  SAT  SAT uses data
augmentation to impose the intended invariance on the
data representations  Essentially  SAT penalizes representation dissimilarity between the original data points and
augmented ones  Let         denote   prede ned data
augmentation under which the data representations should
be invariant  The regularization of SAT made on data point
  is

RSAT          

   

    

Vm ym 

  ym    log   ym      

 

where   ym    is the prediction of original data point   
and  is the current parameter of the network  In Eq   
the representations of the augmented data are pushed to be
close to those of the original data  Since probabilistic classi er        is modeled using   deep neural network  it
is  exible enough to capture   wide range of invariances
speci ed by the augmentation function     The regularization by SAT is then the average of RSAT           over
all the training data points 

RSAT       

 
 

    

RSAT  xn     xn 

 

Learning Discrete Representations via Information Maximizing SelfAugmented Training

The augmentation function   can either be stochastic or
deterministic 
It can be designed speci cally for the applications of interest  For example  for image data  af ne
distortion such as rotation  scaling and parallel movement
can be used for the augmentation function 
Alternatively  more general augmentation functions that do
not depend on speci   applications can be considered   
representative example is local perturbations  in which the
augmentation function is

              

 
where   is   small perturbation that does not alter the meaning of the data point  The use of local perturbations in SAT
encourages the data representations to be locally invariant 
The resulting decision boundaries between different representations tend to lie in low density regions of   data distribution  Such boundaries are generally preferred and follow the lowdensity separation principle  Grandvalet et al 
 
The two representative regulariztion methods based on local perturbations are  Random Perturbation Training  RPT 
 Bachman et al    and Virtual Adversarial Training
 VAT   Miyato et al    In RPT  perturbation   is sampled randomly from hypersphere         where   is  
hyperparameter that controls the range of the local perturbation  On the other hand  in VAT  perturbation   is chosen
to be an adversarial direction 

    arg max

  

 RSAT                   

The solution of Eq    can be approximated ef ciently by
  pair of forward and backward passes  For further details 
refer to Miyato et al   

 

  IMSAT for Clustering
In clustering  we can directly apply the RIM  Gomes et al 
  reviewed in Section  
Unlike the original
IMSAT  uses deep neuRIM  however  our method 
ral networks for the classi ers and regularizes them via
SAT  By representing mutual information as the difference between marginal entropy and conditional entropy
 Cover   Thomas    we have the objective to minimize 

RSAT                            

 
where    and    are entropy and conditional entropy 
respectively  The two entropy terms can be calculated as

 

 

                     

 

        

    

         

 
 

    

      xi 

where                   log      is the entropy
function  Increasing the marginal entropy       encourages the cluster sizes to be uniform  while decreasing
the conditional entropy         encourages unambiguous
cluster assignments  Bridle et al   
In practice  we can incorporate our prior knowledge on
cluster sizes by modifying        Gomes et al   
Note that         log     KL         where   is
the number of clusters  KL  is the KullbackLeibler
divergence  and   is   uniform distribution  Hence 
maximization of       is equivalent to minimization of
KL         which encourages predicted cluster distribution      to be close to    Gomes et al    replaced   in KL         with any speci ed class prior
     so that      is encouraged to be close to      In our
preliminary experiments  we found that the resulting     
could still be far apart from prespeci ed      To ensure
that      is actually close to      we consider the following constrained optimization problem 

  RSAT                
min
subject to KL              

 

where     is   tolerance hyperparameter that is set suf 
ciently small so that predicted cluster distribution      is
the same as class prior      up to  tolerance  Eq    can
be solved by using the penalty method  Bertsekas   
which turns the original constrained optimization problem
into   series of unconstrained optimization problems  Refer
to Appendix   for the detail 

  IMSAT for Hash Learning
In hash learning  each data point is mapped into   Dbit 
binary code  Hence  the original RIM is not directly applicable  Instead  we apply our method for discrete representation learning presented in Section  
The computation of mutual information              YD    
however  is intractable for large   because it involves  
summation over an exponential number of terms  each of
which corresponds to   different con guration of hash bits 
that mutual
Brown
information
             YD     can be expanded as
the sum of
interaction information  McGill   

showed

 

 

     

        

             YD          SY
where SY              YD  Note that   denotes interaction information when its argument is   set of random variables  Interaction information is   generalization of mutual
information and can take   negative value  When the argument is   set of two random variables  the interaction information reduces to mutual information between the two

Learning Discrete Representations via Information Maximizing SelfAugmented Training

random variables  Following Brown   we only retain
terms involving pairs of output dimensions in Eq        
all terms where       This gives us

    

  Yd             

  Yd  Yd    

 

This approximation ignores the interactions among hash
bits beyond the pairwise interactions  It is related to the
orthogonality constraint that is widely used in the literature
to remove redundancy among hash bits  Wang et al   
In fact  the orthogonality constraint encourages the covariance between   pair of hash bits to   Thus  it also takes
into account the pairwise interactions 
It follows from the de nition of interaction information and
the conditional independence in Eq    that

  Yd  Yd         Yd  Yd        Yd  Yd 

     Yd  Yd 

 

In summary  our approximated objective to minimize is

RSAT         
    

     Yd          

 
The  rst term regularizes the neural network  The second
term maximizes the mutual information between data and
each hash bit  and the third term removes the redundancy
among the hash bits 

  Yd  Yd   

  Approximation of the Marginal Distribution
To scale up our method to large datasets  we would like the
objective in Eq    to be amenable to optimization based
on minibatch SGD  For the regularization term  we use
the SAT in Eq    which is the sum of per sample penalties and can be readily adapted to minibatch computation 
For the approximated mutual information in Eq    we
can decompose it into three parts      conditional entropy
  Yd     ii  marginal entropy   Yd  and  iii  mutual information between   pair of output dimensions   Yd  Yd 
The conditional entropy only consists of   sum over per example entropies  see Eq    thus  can be easily adapted
to minibatch computation  However  the marginal entropy  see Eq    and the mutual information involve the
marginal distribution over   subset of target dimensions 
             
       xn  where                yM 
Hence  the marginal distribution can only be calculated using the entire dataset and is not amenable to the minibatch
setting  Following Springenberg   we approximate
the marginal distributions using minibatch data 

   

      

 

      

            

     

 

Table  Summary of the variants 

Method
Linear RIM
Deep RIM
Linear IMSAT  VAT 
IMSAT  RPT 
IMSAT  VAT 

Used classi er

Regularization
Weightdecay
Deep neural nets Weightdecay

Linear

Linear

Deep neural nets
Deep neural nets

VAT
RPT
VAT

In the case
where   is   set of data in the minibatch 
of clustering  the approximated objective that we actually
minimize is an upper bound of the exact objective that we
try to minimize  Refer to Appendix   of the supplementary
material for the detailed discussion 

  Experiments
In this section  we evaluate IMSAT for clustering and hash
learning using benchmark datasets 

  Implementation
In unsupervised learning  it is not straightforward to determine hyperparameters by crossvalidation  Therefore 
in all the experiments with benchmark datasets  we used
commonly reported parameter values for deep neural networks and avoided datasetspeci   tuning as much as possible  Speci cally  inspired by Hinton et al    we set
the network dimensionality to     for clustering across all the datasets  where   and   are input
and output dimensionality  respectively  For hash learning  we used smaller network sizes to ensure fast computation of mapping data into hash codes  We used
recti ed linear units  Jarrett et al    Nair   Hinton 
  Glorot et al    for all the hidden activations
and applied batch normalization  Ioffe   Szegedy   
to each layer to accelerate training  For the output layer 
we used the softmax for clustering and the sigmoids for
hash learning  Regarding optimization  we used Adam
 Kingma   Ba    with the step size   Refer
to Appendix   for further details  Our implementation based on Chainer  Tokui et al    is available at
https github com weihua imsat 

  Clustering
  DATASETS AND COMPARED METHODS
We evaluated our method for clustering presented in Section   on eight benchmark datasets  We performed experiments with two variants of the RIM and three variants
of IMSAT  each of which uses different classi ers and regularization  Table   summarizes these variants  We also
compared our IMSAT with existing clustering methods including Kmeans  DEC  Xie et al    denoising AutoEncoder  dAE Kmeans  Xie et al   

Learning Discrete Representations via Information Maximizing SelfAugmented Training

Table  Summary of dataset statistics 

 Classes Dimension  Largest class

Dataset
MNIST  LeCun et al   
Omniglot  Lake et al   
STL  Coates et al   
CIFAR   Torralba et al   
CIFAR   Torralba et al   
SVHN  Netzer et al   
Reuters  Lewis et al   
 news  Lang   

 Points
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

Table Comparison of clustering accuracy on eight benchmark datasets   Averages and standard deviations over twelve trials were
reported  Results marked with   were excerpted from Xie et al   

Method
Kmeans
dAE Kmeans
DEC
Linear RIM
Deep RIM
Linear IMSAT  VAT 
IMSAT  RPT 
IMSAT  VAT 

MNIST

Omniglot

 
   
   

   
   
   
   
   

 
 

   
   
   
   
   
   

STL
 
 

   
   
   
   
   
   

CIFAR 

CIFAR 

 
 

   
   
   
   
   
   

 
 

   
   
   
   
   
   

SVHN
 
 

   
   
   
   
   
   

Reuters

 news

 
 

   
   
   
   
   
   

 
 

   
   
   
   
   
   

  brief summary of dataset statistics is given in Table   In
the experiments  our goal was to discover clusters that correspond well with the groundtruth categories  For the STL 
CIFAR  and CIFAR  datasets  raw pixels are not suited
for our goal because color information is dominant  We
therefore applied  layer pretrained deep residual networks  He et al    to extract features and used them
for clustering  Note that since the residual network was
trained on ImageNet  each class of the STL dataset  which
is   subset of ImageNet  was expected to be wellseparated
in the feature space  For Omniglot    types of characters were sampled  each containing   data points  Each
data point was augmented   times by the stochastic af ne
distortion described in Appendix    For SVHN  each image was represented as    dimensional GIST feature
 Oliva   Torralba    For Reuters and  news  we
removed stop words and retained the   most frequent
words  We then used tfidf features  Refer to Appendix  
of the supplementary material for further details 

  EVALUATION METRIC
Following Xie et al    we set the number of clusters to the number of groundtruth categories and evaluated
clustering performance with unsupervised clustering accuracy  ACC 

ACC   max

    ln     cn 

 

 

    

 

where ln and cn are the groundtruth label and cluster
assignment produced using the algorithm for xn  respectively  The   ranges over all possible oneto one mappings
between clusters and labels  The best mapping can be ef 
 ciently computed using the Hungarian algorithm  Kuhn 
 

  HYPERPARAMETER SELECTION
In unsupervised learning  it is not straightforward to determine hyperparameters by crossvalidation  Hence  we
 xed hyperparameters across all the datasets unless there
was an objective way to select them 
For Kmeans 
we tried   different initializations and reported the results with the best objectives  For dAE Kmeans and
DEC  Xie et al    we used the recommended hyperparameters for the network dimensionality and annealing
speed 
Inspired by the automatic kernel width selection in spectral clustering  ZelnikManor   Perona    we set the
perturbation range    on data point   in VAT and RPT as

               

 

where   is   scalar and       is the Euclidian distance to
the tth neighbor of    In our experiments  we  xed    
  For Linear IMSAT  VAT  IMSAT  RPT  and IMSAT
 VAT  we  xed         and   respectively  which
performed well across the datasets 

Learning Discrete Representations via Information Maximizing SelfAugmented Training

Table Comparison of clustering accuracy on the Omniglot
dataset using IMSAT with different types of SAT 

Method
IMSAT  VAT 
IMSAT  af ne 
IMSAT  VAT   af ne 

Omniglot
   
   
   

For the methods shown in Table   we varied one hyperparameter and chose the best one that performed well
across the datasets  More speci cally  for Linear RIM and
Deep RIM  we varied the decay rate over            
              For the three variants of IMSAT  we varied   in
Eq    for                           We set   to be the
uniform distribution and let                in Eq   
for the all experiments 
Consequently  we chose   for decay rates in both Linear RIM and Deep RIM  Also  we set        
and   for Linear IMSAT  VAT  IMSAT  RPT  and IMSAT  VAT  respectively  We hereforth  xed these hyperparameters throughout the experiments for both clustering
and hash learning  In Appendix    we report all the experimental results and the criteria to choose the parameters 

  EXPERIMENTAL RESULTS
In Table   we compare clustering performance across eight
benchmark datasets  We see that IMSAT  VAT  performed
well across the datasets  The fact that our IMSAT outperformed Linear RIM  Deep RIM and Linear IMSAT  VAT 
for most datasets suggests the effectiveness of using deep
neural networks with an endto end regularization via SAT 
Linear IMSAT  VAT  did not perform well even with the
endto end regularization probably because the linear classi er was not  exible enough to model the intended invariance of the representations  We also see from Table   that
IMSAT  VAT  consistently outperformed IMSAT  RPT  in
our experiments  This suggests that VAT is an effective
regularization method in unsupervised learning scenarios 
We further conducted experiments on the Omniglot dataset
to demonstrate that clustering performance can be improved by incorporating domainspeci   knowledge in the
augmentation function of SAT  Speci cally  we used the
af ne distortion in addition to VAT for the augmented function of SAT  We compared the clustering accuracy of IMSAT with three different augmentation functions  VAT 
af ne distortion  and the combination of VAT   af ne distortion  in which we simply set the regularization to be

 
    RSAT  TVAT   

 
    RSAT  Ta ne 

 

where TVAT and Ta ne are augmentation functions of VAT
and af ne distortion  respectively  For Ta ne  we used the
stochastic af ne distortion function de ned in Appendix   

We report the clustering accuracy of Omniglot in Table  
We see that including af ne distortion in data augmentation
signi cantly improved clustering accuracy  Figure   shows
ten randomly selected clusters of the Omniglot dataset that
were found using IMSAT  VAT  and IMSAT  VAT   af ne
distortion  We observe that IMSAT  VAT   af ne distortion  was able to discover cluster assignments that are invariant to af ne distortion as we intended  These results
suggest that our method successfully captured the invariance in the handwritten character recognition in an unsupervised way 

  Hash Learning
  DATASETS AND COMPARED METHODS
We evaluated our method for hash learning presented
in Section   on two benchmark datasets  MNIST
and CIFAR  datasets 
Each data sample of CIFAR  is represented as    dimensional GIST feature  Oliva   Torralba    Our method was compared
against several unsupervised hash learning methods  spectral hashing  Weiss et al    PCAITQ  Gong et al 
  and Deep Hash  Erin Liong et al    We also
compared our method to the hash versions of Linear RIM
and Deep RIM  For our IMSAT  we used VAT for the regularization  We used the same hyperparameters as in Section  

  EVALUATION METRIC
Following Erin Liong et al    we used three evaluation metrics to measure the performance of the different
methods    mean average precision  mAP    precision at
      samples  and   Hamming lookup result where
the hamming radius is set as       We used the class labels to de ne the neighbors  We repeated the experiments
ten times and took the average as the  nal result 

  EXPERIMENTAL RESULTS
The MNIST and CIFAR  datasets both have   classes 
and contain   and   data points  respectively 
Following Erin Liong et al    we randomly sampled
  samples    per class  as the query data and used the
remaining data as the gallery set 
We tested performance for   and  bit hash codes  In
practice  fast computation of hash codes is crucial for fast
information retrieval  Hence  small networks are preferable  We therefore tested our method on three different network sizes  the same ones as Deep Hash  Erin Liong et al 
       and      Note that Deep
Hash used     and     for learning   and
 bit hash codes  respectively 
Table   lists the results for  bit hash  Due to the space

Learning Discrete Representations via Information Maximizing SelfAugmented Training

    IMSAT  VAT 

    IMSAT  VAT     ne 

Figure Randomly sampled clusters of Omniglot discovered using     IMSAT  VAT  and     IMSAT  VAT   af ne  Each row contains
randomly sampled data points in same cluster 

Table Comparison of hash performance for  bit hash codes   Averages and standard deviations over ten trials were reported 
Experimental results of Deep Hash and the previous methods were excerpted from Erin Liong et al   

Hamming ranking  mAP 
MNIST

CIFAR 

precision   sample    
MNIST

CIFAR 

precision        
MNIST

CIFAR 

Method
 Dimensions of hidden layers 
Spectral hash  Weiss et al   
PCAITQ  Gong et al   
Deep Hash  
Linear RIM
Deep RIM  
Deep RIM  
Deep RIM  
IMSAT  VAT   
IMSAT  VAT   
IMSAT  VAT   

 
 
 

   
   
   
   
   
   
   

 
 
 

   
   
   
   
   
   
   

 
 
 

   
   
   
   
   
   
   

 
 
 

   
   
   
   
   
   
   

 
 
 

   
   
   
   
   
   
   

 
 
 

   
   
   
   
   
   
   

constraint  we report the results for  bit hash codes in
Appendix    but the results showed   similar tendency as
that of  bit hash codes  We see from Table   that IMSAT
with the largest network sizes   achieved competitive performance in both datasets  The performance of IMSAT improved signi cantly when slightly bigger networks
  were used  while the performance of Deep RIM
did not improve much with the larger networks  We deduce that this is because we can better model the local
invariance by using more  exible networks  Deep RIM 
on the other hand  did not signi cantly bene   from the
larger networks  because the additional  exibility of the
networks was not used by the global function regularization via weightdecay 
In Appendix    our deduction is
supported using   toy dataset 

 Hence  we deduce that Deep Hash  which is only regularized by weightdecay  would not bene   much by using larger
networks 

  Conclusion   Future Work
In this paper  we presented IMSAT  an informationtheoretic method for unsupervised discrete representation
learning using deep neural networks  Through extensive
experiments  we showed that intended discrete representations can be obtained by directly imposing the invariance to
data augmentation on the prediction of neural networks in
an endto end fashion  For future work  it is interesting to
apply our method to structured data       graph or sequential data  by considering appropriate data augmentation 
Acknowledgements
We thank Brian Vogel for helpful discussions and insightful reviews on the paper  This paper is based on results
obtained from Hu   internship at Preferred Networks  Inc 

Learning Discrete Representations via Information Maximizing SelfAugmented Training

References
Bachman  Philip  Alsharif  Ouais  and Precup  Doina 

Learning with pseudoensembles  In NIPS   

Berkhin  Pavel    survey of clustering data mining techniques  In Grouping multidimensional data  pp   
Springer   

Bertsekas  Dimitri    Nonlinear programming  Athena sci 

enti   Belmont   

Bridle  John    Heading  Anthony       and MacKay 
David       Unsupervised classi ers  mutual information
and  phantom targets  In NIPS  pp     

Brown  Gavin    new perspective for information theoretic

feature selection  In AISTATS   

Coates  Adam  Lee  Honglak  and Ng  Andrew    An
analysis of singlelayer networks in unsupervised feature
learning  Ann Arbor     

Cover  Thomas   and Thomas  Joy    Elements of infor 

mation theory  John Wiley   Sons   

Dilokthanakul  Nat  Mediano  Pedro AM  Garnelo  Marta 
Lee  Matthew CH  Salimbeni  Hugh  Arulkumaran  Kai 
and Shanahan  Murray  Deep unsupervised clustering
with gaussian mixture variational autoencoders  arXiv
preprint arXiv   

Dosovitskiy  Alexey  Springenberg  Jost Tobias  Riedmiller  Martin  and Brox  Thomas  Discriminative unsupervised feature learning with convolutional neural networks  In NIPS  pp     

Erin Liong  Venice  Lu  Jiwen  Wang  Gang  Moulin 
Pierre  and Zhou  Jie  Deep hashing for compact binary
codes learning  In CVPR   

Glorot  Xavier  Bordes  Antoine  and Bengio  Yoshua 
Deep sparse recti er neural networks  In AISTATS   

Gomes  Ryan  Krause  Andreas  and Perona  Pietro  Discriminative clustering by regularized information maximization  In NIPS   

Gong  Yunchao  Lazebnik  Svetlana  Gordo  Albert  and
Perronnin  Florent  Iterative quantization    procrustean
approach to learning binary codes for largescale image
retrieval 
IEEE Transactions on Pattern Analysis and
Machine Intelligence     

Grandvalet  Yves  Bengio  Yoshua  et al  Semisupervised

learning by entropy minimization  In NIPS   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Delving deep into recti ers  Surpassing humanlevel performance on imagenet classi cation  In CVPR 
 

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition  In
CVPR   

Hinton  Geoffrey    Srivastava  Nitish  Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan    Improving
neural networks by preventing coadaptation of feature
detectors  arXiv preprint arXiv   

Ioffe  Sergey and Szegedy  Christian  Batch normalization 
Accelerating deep network training by reducing internal
covariate shift  In ICML   

Jarrett 

Kevin 

Kavukcuoglu 

Ranzato 
Marc Aurelio  and LeCun  Yann  What is the best
multistage architecture for object recognition 
In
ICCV   

Koray 

Kingma  Diederik and Ba  Jimmy  Adam    method for

stochastic optimization  In ICLR   

Koch  Gregory 

Siamese neural networks for oneshot
image recognition  PhD thesis  University of Toronto 
 

Kuhn  Harold    The hungarian method for the assignment
problem  Naval research logistics quarterly   
   

Kulis  Brian and Darrell  Trevor  Learning to hash with

binary reconstructive embeddings  In NIPS   

Lai  Hanjiang  Pan  Yan  Liu  Ye  and Yan  Shuicheng  Simultaneous feature learning and hash coding with deep
neural networks  In CVPR   

Lake  Brenden    Salakhutdinov  Ruslan  Gross  Jason 
and Tenenbaum  Joshua    One shot learning of simple
visual concepts  In CogSci   

Lang  Ken  Newsweeder  Learning to  lter netnews 

ICML  pp     

In

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  Proceedings of the IEEE   
   

He  Kaiming  Wen  Fang  and Sun  Jian  Kmeans hashing 
An af nitypreserving quantization method for learning
binary compact codes  In CVPR   

Leen  Todd    From data distributions to regularization in
invariant learning  Neural Computation   
 

Learning Discrete Representations via Information Maximizing SelfAugmented Training

machine learning systems  LearningSys  in the twentyninth annual conference on neural information processing systems  NIPS   

Torralba  Antonio  Fergus  Rob  and Freeman  William   
  million tiny images    large data set for nonparametric object and scene recognition  IEEE transactions on
pattern analysis and machine intelligence   
   

Vincent  Pascal  Larochelle  Hugo  Bengio  Yoshua  and
Manzagol  PierreAntoine  Extracting and composing
robust features with denoising autoencoders  In ICML 
 

Wang  Jun  Liu  Wei  Kumar  Sanjiv  and Chang  ShihFu 
Learning to hash for indexing big data   survey  Proceedings of the IEEE     

Weiss  Yair  Torralba  Antonio  and Fergus  Rob  Spectral

hashing  In NIPS   

Xia  Rongkai  Pan  Yan  Lai  Hanjiang  Liu  Cong  and
Yan  Shuicheng  Supervised hashing for image retrieval
via image representation learning  In AAAI   

Xie  Junyuan  Girshick  Ross  and Farhadi  Ali  Unsupervised deep embedding for clustering analysis  In ICML 
 

Xu  Jiaming  Wang  Peng  Tian  Guanhua  Xu  Bo  Zhao 
Jun  Wang  Fangyuan  and Hao  Hongwei  Convolutional neural networks for text hashing  In IJCAI   

Xu  Linli  Neufeld  James  Larson  Bryce  and SchuurIn NIPS 

mans  Dale  Maximum margin clustering 
 

ZelnikManor  Lihi and Perona  Pietro  Selftuning spectral

clustering  In NIPS   

Zhang  Ruimao  Lin  Liang  Zhang  Rui  Zuo  Wangmeng 
and Zhang  Lei  Bitscalable deep hashing with regularized similarity learning for image retrieval and person
reidenti cation  IEEE Transactions on Image Processing     

Zheng  Yin  Tan  Huachun  Tang  Bangsheng  Zhou 
Variational deep embedding   
arXiv preprint

Hanning  et al 
generative approach to clustering 
arXiv   

Lewis  David    Yang  Yiming  Rose  Tony    and Li  Fan 
Rcv    new benchmark collection for text categorization research  Journal of machine learning research   
 Apr   

Li  WuJun  Wang  Sheng  and Kang  WangCheng  Feature learning based deep supervised hashing with pairwise labels  In IJCAI   

McGill  William    Multivariate information transmission 

Psychometrika     

Miyato  Takeru  Maeda  Shinichi  Koyama  Masanori 
Nakae  Ken  and Ishii  Shin  Distributional smoothing
with virtual adversarial training  In ICLR   

Nair  Vinod and Hinton  Geoffrey    Recti ed linear units
improve restricted boltzmann machines  In ICML   

Netzer  Yuval  Wang  Tao  Coates  Adam  Bissacco 
Alessandro  Wu  Bo  and Ng  Andrew    Reading digits in natural images with unsupervised feature learning 
In NIPS workshop on deep learning and unsupervised
feature learning   

Ng  Andrew    Jordan  Michael    Weiss  Yair  et al  On
spectral clustering  Analysis and an algorithm  In NIPS 
 

Norouzi  Mohammad and Blei  David    Minimal loss

hashing for compact binary codes  In ICML   

Oliva  Aude and Torralba  Antonio  Modeling the shape
of the scene    holistic representation of the spatial envelope  International journal of computer vision   
   

Rifai  Salah  Vincent  Pascal  Muller  Xavier  Glorot 
Xavier  and Bengio  Yoshua  Contractive autoencoders 
Explicit invariance during feature extraction  In ICML 
 

Sajjadi  Mehdi  Javanmardi  Mehran  and Tasdizen  Tolga 
Regularization with stochastic transformations and perturbations for deep semisupervised learning  In NIPS 
 

Salakhutdinov  Ruslan and Hinton  Geoffrey  Semantic
hashing  International Journal of Approximate Reasoning     

Springenberg  Jost Tobias 

Unsupervised and semisupervised learning with categorical generative adversarial networks  In ICLR   

Tokui  Seiya  Oono  Kenta  Hido  Shohei  and Clayton 
Justin  Chainer    nextgeneration open source framework for deep learning  In Proceedings of workshop on

