Gradient Projection Iterative Sketch for LargeScale Constrained

LeastSquares

Junqi Tang   Mohammad Golbabaee   Mike    Davies  

Abstract

We propose   randomized  rst order optimization algorithm Gradient Projection Iterative
Sketch  GPIS  and an accelerated variant for ef 
 ciently solving large scale constrained Least
Squares  LS  We provide the  rst theoretical
convergence analysis for both algorithms  An
ef cient implementation using   tailored linesearch scheme is also proposed  We demonstrate
our methods  computational ef ciency compared
to the classical accelerated gradient method  and
the variancereduced stochastic gradient methods
through numerical experiments in various large
synthetic real data sets 

  Introduction
We are now in an era of boosting knowledge and large data 
In our daily life we have various signal processing and machine learning applications which involve the problem of
tackling   huge amount of data  These applications vary
from Empirical Risk Minimization  ERM  for statistical
inference  to medical imaging such as the Computed Tomography  CT  and Magnetic Resonance Imaging  MRI 
channel estimation and adaptive  ltering in communications  and in machine learning problems where we need to
train   neural network or   classi er from   large amount of
data samples or images  Many of these applications involve
solving constrained optimization problems  In   large data
setting   desirable algorithm should be able to simultaneously address high accuracy of the solutions  small amount
of computations and high speed data storage 
Recent advances in the  eld of randomized algorithms have
provided us with powerful tools for reducing the computation for large scale optimizations  From the latest literature we can clearly see two streams of randomized al 

 Institute for Digital Communications 

Edinburgh  Edinburgh  UK  Correspondence to 
   Tang ed ac uk 

the University of
Junqi Tang

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

follows   second line of

gorithms  the  rst stream is the stochastic gradient descent  SGD  and its variancereduced variants  Johnson  
Zhang   Kone cn     Richt arik   Defazio et al 
 AllenZhu    The stochastic gradient techniques are based on the computationally cheap unbiased
estimate of the true gradients with progressively reduced
estimation variance  Although there has been several works
on SGD techniques for performing constrained optimization  Xiao   Zhang   Kone cn   et al    to the
best of our knowledge  there are no results highlighting the
computational speed up one could achieve by exploiting the
data structure promoted by the constraint set 
research and
This paper
uses sketching techniques 
the crux of which is reducing the dimensionality of   large scale problem by
random projections       subGaussian matrices  Fast
JohnsonLindenstrauss Transforms  FJLT   Ailon   Liberty   Ailon   Chazelle    the Count Sketch
 Clarkson   Woodruff    the CountGauss Sketch
 Kapralov et al    or random subselection  so that
the resulting sketched problem becomes computationally tractable 
The metaalgorithms Classical Sketch
 CS Mahoney   Drineas et al   Pilanci  
Wainwright    and the Iterative Hessian Sketch  IHS 
 Pilanci   Wainwright    have been recently introduced for solving ef ciently large scale constrained LS
problems which utilize the random sketching idea combined with the fact that solutions have lowdimensional
structures such as sparsity in   properlychosen dictionary 
lowrank  etc 

  Main Contributions

  Novel  rst order solvers based on iterative sketches

for constrained Leastsquares

We propose   basic  rst order algorithm Gradient Projection Iterative Sketch  GPIS  based on the combination of the Classical Sketch  Pilanci   Wainwright    and Iterative Hessian Sketch  Pilanci
  Wainwright    for ef ciently solving the constrained Leastsquares  and also an accelerated variant by applying Nesterov   acceleration scheme  Nesterov   Nesterov     

Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

  Theoretical analysis for both GPIS and AccGPIS
Although there exists established theories for the
sketching programs
in  Pilanci   Wainwright 
 Pilanci   Wainwright    which describes
their estimation performance under the assumption
that the sketched programs are solved exactly  there is
no theoretical analysis of the use of  rst order methods within this framework  where each of the sketched
programs are only approximately solved  The paper is
the  rst one to provide this convergence analysis 

  Structure exploiting algorithms

In related theoretical works in sketching  Pilanci  
Wainwright   Pilanci   Wainwright    convex relaxation  Chandrasekaran   Jordan    and
the Projected Gradient Descent  PGD  analysis  Oymak et al    with greedy step sizes when the data
matrix is   Gaussian map  researchers have discovered
that the constraint set is able to be exploited to accelerate computation  In this paper   convergence analysis
of the proposed algorithms  which have an inner loop
and an outer loop  we show explicitly how the outer
loop   convergence speed is positively in uenced by
the constrained set   

  Sketched gradients versus stochastic gradients  

quality versus quantity
The proposed GPIS algorithm draws   different line
of research for  rst order randomized algorithms from
the SGD and its recently introduced variancereduced
variants such as SVRG  Johnson   Zhang    and
SAGA  Defazio et al    by utilizing randomized
sketching techniques and deterministic iterations instead of the stochastic iterations  This approach leads
to convenience in optimally choosing the step size by
implementing line search because it follows the classical results and techniques in  rst order optimization  Although such stochastic gradient algorithms
have good performance in terms of epoch counts when
  small minibatch size is used  this type of measure
does not consider at least three important aspects   
the computational cost of projection   proximal operator    the modern computational devices are usually
more suitable for vectorized   parallel computation   
the operational efforts to access new data batches each
iteration  note that the large data should be stored in
large memories  which are usually slow 
It is well known that the small batch size in stochastic gradients usually leads to   greater demand on the

 Meanwhile we can show empirically that the inner loop is
also being able to choose an aggressive step size with respect to
the constraint  This extra stepsize experiment can be found in the
supplementary material 

number of iterations  In the cases where the projection   proximal operator is costly to compute  for instance  if we wish to enforce sparsity in   transformed
domain  or an analytical domain  totalvariation  we
would need to use   large batch size in order to control computation which generally would not be favorable for stochastic gradients techniques as they usually achieves best performance when small batch size
is used  In this paper we have designed experiments
to show the time ef ciency of the sketched gradients with Countsketch  Clarkson   Woodruff   
and an aggressive linesearch scheme for nearoptimal
choice of step size each iteration  Nesterov   
compared to   minibatched version of the SAGA algorithm  Defazio et al    and the accelerated full
gradient method  Beck   Teboulle    in large
scale constrained leastsquare problems 

  Background

Consider   constrained Leastsquares regression problem
in the large data setting  We have the training data matrix
    Rn   with       and observation     Rn  Meanwhile we restrict our regression parameter to   convex constrained set   to enforce some desired structure such as
sparsity and lowrank 

 cid          cid     Ax cid 

 cid   

  cid    arg min
   

 

Then we de ne the error vector   as 
        Ax cid 

 

 

  standard  rst order solver for   is the projected gradient algorithm  we denote the orthogonal projection operator onto the constrained set   as PK 

xj    PK xj    AT  Axj     

 

Throughout the past decade researchers proposed   basic metaalgorithm for approximately solving the Leastsquares problem that we call the Classical Sketch  CS 
see     
 Mahoney     Drineas et al     Pilanci   Wainwright    which compresses the dimension of the LS and makes it cheaper to solve  The
JohnsonLindenstrauss theory  Johnson   Lindenstrauss 
   Dasgupta   Gupta    and the related topic
of Compressed Sensing  Donoho   Candes et al 
 Baraniuk et al    revealed that random projections can achieve stable embeddings of high dimensional
data into lower dimensions and that the number of measurements required is proportional to the intrinsic dimensionality of data  as opposed to the ambient dimension 
 In scenarios where we do not know the exact constraint    we
may wish to use regularized leastsquares instead of strict constraint  This paper focus on the constrained case and leave the
extension for the proximal setting as future work 

Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

 

end for
  
      
  
Run GPIHS iterates
for       to   do

which is manifested in the set of constraints    This motivates replacing the original constrained LS problem with  
sketched LS  Pilanci   Wainwright   

     arg min
   

 
where the sketching matrix     Rm       cid    is   random projection operator which satis es 

 

 cid   

 cid         cid Sy   SAx cid 
 cid  ST  

 cid 

    

 

 

When the embedding dimension   is larger than   certain
factor of the true solution   intrinsic dimension  measured
through   statistical tool called the Gaussian Width  Chandrasekaran et al    the Classical Sketch   ensures
  robust estimation of   cid  with   noise ampli cation factor
compared to the estimator given by solving the original LS
problem   and it has been shown that the smaller the embedding dimension   is  the bigger the noise ampli cation
factor will be  To get   sketching scheme for the scenarios where   high accuracy estimation is demanded    new
type of metaalgorithm Iterative Hessian Sketch  IHS  was
introduced by Pilanci and Wainwright  Pilanci   Wainwright   

xt    arg min

    ft     

 cid StA     xt cid 
 
  
 xT AT      Axt 

 

 

At the tth iteration of IHS   new sketch of the data matrix
StA and   full gradient AT    Axt  at the current estimate
xt is calculated to form   new sketched leastsquare problem  By repeating this procedure the IHS will converge to
the solution of the original problem   in typically   small
number of iterations  The iterative sketch essentially corrects the noise ampli cation and enables       LS accuracy in the order of log  

  outer loop iterations 

  Gradient Projection Iterative Sketch
  The Proposed Algorithms

Here we consider the combination of CS with the  rst order
PGD algorithm  the Gradient Projection Classical Sketch
 GPCS 

xi    PK xi             Axi       

 

Similarly we obtain the Gradient Projection Iterative Hessian Sketch  GPIHS  for solving IHS  
xi    PK xi StA    StA xi xt mAT  Axt   
 
Our proposed GPIS algorithm applies PGD to solve   sequence of sketched LS  starting with   CS step for   fast

Algorithm   Gradient Projection Iterative Sketch  
          

     

Initialization    
Given     Rn    sketch size    cid   
Prior knowledge  the true solution   belongs to set  
Run GPCS iterates  Optional 
Generate   random sketching matrix      Rm  
Calculate         
for       to    do
     PK   
  

                Ax 

        

      

Calculate     AT  Axt
Generate   random sketching matrix St   Rm  
Calculate At
for       to kt do
     PK xt
xt

         AtT

    xt

    StA

    mg 

  At

  xt

end for
xt 
    xt
kt
end for

initialization  and then is followed by further iterations of
IHS  We can observe from Algorithm   that sketches are
constructed in the outer loop and within the inner loop we
only need to access them  This property could be very useful when  for instance   is stored in   slow speed memory
and it is too large to be loaded at once into the fast memory  or in large scale image reconstruction problems such as
CT where due to its prohibited size   is constructed on the
    Note that thanks to the sketching each inner iteration of
GPIS is  
  times cheaper than   full PGD iterate in terms of
matrixvector multiplication  so intuitively we can see that
there is potential in Algorithm   to get computational gain
over the standard  rst order solver PGD 
Since it is wellknown that in convex optimization the standard  rst order method Projected proximal gradient descent can be accelerated by Nesterov   acceleration scheme
 Nesterov     Nesterov       Beck   Teboulle 
  our Algorithm   has potential to be further improved
by introducing Nesterov   acceleration  Here we propose
Algorithm     Accelerated Gradient Projection Iterative
Sketch  AccGPIS  which is based on the combination of
the accelerated PGD and iterative sketching 
One of the bene ts of deterministically minimising the
sketched cost function can bring is that the implementation
of the linesearch scheme can be easy and provably reliable
since the underlying sketched cost function each outer loop
is  xed  For example  Nesterov    provides   simple
linesearch scheme for gradient methods to make the step
size of each iteration to be nearly optimal  with rigorous

Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

Algorithm   Accelerated Gradient Projection Iterative
Sketch             

Algorithm   linesearch scheme for GPIS and AccGPIS
    xi  ft     cid ft xi           Nesterov   

           

Initialization    
Given     Rn    sketch size    cid   
Prior knowledge  the true solution   belongs to set  
Run GPCS iterates  Optional 
Generate   random sketching matrix      Rm  
Calculate         
 cid 
for       to    do
     PK   
  
        
Extrapolate   

                Az 
       
       

        

       
   
   

        

  

  

end for
  
      
Run GPIHS iterates
for       to   do

      
  

      

    StA

Calculate     AT  Axt
Generate   random sketching matrix St   Rm  
Calculate At
     
 cid 
for       to kt do
     PK zt
xt
        
Extrapolate zt

         AtT
       
  
     xt

        

    xt

     xt
 xt
  

    mg 

  At

  zt

  

end for
xt 
    zt 
end for

    xt
kt

convergence theory and also   explicit bound for the number of additional gradient calls  The linesearch scheme is
described by Algorithm   On the other hand in the stochastic gradient literature there are no practical strategies for ef 
 cient line search in the case of constrained optimization 
To the best of our knowledge  only the SAG paper  Schmidt
et al    addresses the issue of linesearch but their implementation is only for unconstrained optimization 

  Convergence Analysis
  General Theory

We start our theoretical analysis by some de nitions 
De nition   The Lipschitz constant   and strong convexity   for the LS   are de ned as the largest and smallest
singular values of the Hessian matrix AT   
      cid zd cid 
 

 
for all zd   Rd  where                 means the LS
  is nonstrongly convex 
De nition   Let   be the smallest closed cone at   cid  con 

     cid Azd cid 

 cid zd cid 

Input  update xi  sketched objective function ft    gradient vector  cid ft xi  line search parameters    and    
step size of previous iteration    
De ne composite gradient map mL 
mL   ft xi         xi    cid ft xi     
         
    PK xi    cid ft xi 
while ft      mL do

 cid     xi cid 

 

      
    PK xi    cid ft xi 

end while
Return xi      and       

taining the set       cid 

   cid     Rd              cid             cid   

 
Sd  be the unit sphere in Rd  Bd be the unit ball in Rd 
  be arbitrary  xed unitnorm vectors in Rn  The factors
  StA   St     and  St     are de ned as 

    StA    sup
    Bd

vT       tAT StT

 St      

 St      

inf   AC Sn 

supv AC Sn  vT    
 

  StT
  cid Stv cid 
supv range   Sn   cid Stv cid 
inf   range   Sn   cid Stv cid 

 

 

 

 

StA   
St      

 

 

 

 

For convenience  we denote each of this terms as      
    StA        St     and       St     Our theory hangs on these three factors and we will show that they
can be bounded with exponentially high probabilities for
Gaussian projections 
De nition   The optimal points xt
ft    are de ned as 
xt
 cid    arg min

 cid  of the sketch programs

 

    ft   

 cid xt

 cid      cid cid 
 

We also de ne   constant   for the simplicity of the theorems 

 

    max

 
We use the notation  cid   cid      cid Av cid  to describe the Anorm
of   vector   in our theory  After de ning these properties we can derive our  rst theorem for GPIS when       is
strongly convex              
Theorem    Linear convergence of GPIS when      
For  xed step sizes     
  the following bounds
hold  for        the initialization loop by GPCS 

 cid StA cid 

 

 cid   

      cid cid        

 cid   

      

 cid cid      cid   cid   

 
 

 cid 

Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

for       and xt
GPIHS 

    xt 

kt 

 the consecutive loops by

for       and xt

    xt 

 

 cid xN  

 

    cid cid    

 

 

 cid 

 cid    cid 

  

 cid   

      cid cid  
 cid 

  
 

maxt
  
    maxt   

 LR
        

 

where       for  xed step sizes       cid StA cid 
         for  
line search scheme described by Algorithm   with parameter        and       

 

    cid   

We include the proofs in our supplementary material 
It
is well known that for the case       the accelerated
gradients can potentially enjoy the improved linear rate
    but it demands the exact knowledge of the
value    which is often unavailable in practical setups  In
our implementation for the AccGPIS method in the experiments  we use the adaptive gradient restart scheme proposed by    Donoghue   Candes   

  Explicit Bounds for Gaussian Sketches

The theorems above provide us with   framework to describe the convergence of GPIS and AccGPIS in terms of
the constants     and   For Gaussian sketches  these constants  nd explicit bounding expressions in terms of the
sketch size   and the complexity of the constraint cone   
For this  we use the Gaussian Width argument  see      
 Chandrasekaran et al   
De nition   The Gaussian width    is   statistical
measure of the size of   set  
     Eg

 cid 

 cid 

 

vT  

 

sup
  

 slog   

       

will have       Sd   cid 

where     Rn is draw from        normal distribution 
The value of     Sd  is an useful measure of the tightness of the structure of   cid  For example  if   cid  is ssparse
and we model the sparsity constraint using an    ball  we
     which means
the sparser   cid  is  the smaller the       Sd  will be
 Chandrasekaran et al    As an illustration we now
quantify the bounds in our general theorems in terms of the
sketch size   and the Gaussian width of the transformed
   and the ambient dimension of
the solution domain     Now we are ready to provide the
explicit bounds for the factors        and    for the general
theorems  we denotes bm  
   Oymak
et al    and       AC   Sn  for the following
lemmas 
Proposition   If the stepsize     
size   satis es bm  

     sketch
   and the entries of the sketching

cone   AC   Sn     

       

 
 
  bm 

      
 
   

 

 

 

 cid xN  

 

    cid cid    

where we denote 

 cid    cid 
 cid 

  

 cid   

 cid 
 

 cid 
 cid 

      cid cid   
 cid 

 
 

 cid 
       kt

       

    

 

From Theorem   we can see that when we have strong convexity  aka       by choosing   appropriate step size the
GPCS loop will linearly converge to   suboptimal solution 
the accuracy of which depends on the value of  cid   cid 
and the following GPIHS iterations enjoys   linear convergence towards the optimal point 
When the leastsquares solution is relatively consistent
 cid   cid  is small  the GPCS loop will provide excellent initial convergence speed  otherwise it is not bene cial   that  
why we say that the GPCS loop is optional for our GPIS  
AccGPIS algorithm  For regression problems on data sets 
we advise not to run the GPCS iterates  but for signal image
processing applications  we would recommend it 
For the cases where the strong convexity is not guaranteed
      we show the     
    convergence rate for GPIS algorithm 
Theorem    Convergence guarantee for GPIS when    
  If we choose    xed number     of innerloops for    
       the following bounds hold  for      

 cid      
 cid 
 cid    cid 

   

 cid   

      cid cid    
    xt 

 

for       and xt

 cid xN  

 

    cid cid    

   cid   cid 

 

 

 cid   

      cid cid  
 cid 

 LR

 

  
 

  
  
maxt
    maxt   

 

  
where       for  xed step sizes       cid StA cid 
         for  
line search scheme described by Algorithm   with parameter        and       

 

     convergence rate 

For the Accelerated GPIS algorithm we also prove the desired     
Theorem  
 Convergence guarantee for Accelerated
GPIS when       If we choose    xed number     of
innerloops for            the following bounds hold 
for        

 cid 

 cid   

      cid cid    

    

           cid   cid 

 

Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

matrix St are       drawn from Normal distribution  then 

 cid 

    

     
 

 bm    
 

 bm  

     
     

 cid 

 

 

with probability at least          
   
Proposition   If the entries of the sketching matrix St are
      drawn from Normal distribution  then 
 bm      

 cid 

 cid 

 

    

 bm        

      
 
 

   

 

 

With probability at least         
Proposition   If the entries of the sketching matrix St are
      drawn from Normal distribution  and the sketch size  
satis es bm  

           
   

   then 

 

 

 
     
 bm    
      bm  
     
with probability at least          
   

 

 We include the proofs in the supplementary material  We
would like to point out that our bound on factor    in proposition   has revealed that the outerloop convergence of
GPIS and AccGPIS relies on the Gaussian Width of the
solution   cid  and the choice of the sketch size   

 
    
      

 
   

    cid 

 

 

We can then observe that the larger the sketch size   is
with respect to    the faster the outer loop convergence
of GPIS and AccGPIS can be  but on the other hand we
should not choose   too large otherwise the innerloop iteration become more costly   this tradeoff means that there
is always   sweet spot for the choice of   to optimize the
computation 
Our theory is conservative in   sense that it does not provide guarantee for   sketch size which is below the ambient
dimension   since the factors    and    which are related
to the inner loop prohibit this 
Although the Gaussian sketch provides us strong guarantees  due to computational cost of dense matrix multiplication  which is of   mnd  it is not computationally attractive in practice 
In the literature of randomized numerical linear algebra and matrix sketching  people usually
use the random projections with fast computational structures such as the Fast JohnsonLindenstrauss Transform
 Ailon   Liberty   Ailon   Chazelle    Count
sketch  Clarkson   Woodruff    and CountGauss
sketch Kapralov et al    which cost   nd log   

  nnz    and   nnz           respectively  These
fast sketching methods provide signi cant speed up in practice compared to Gaussian sketch when    cid    

  Implementation for GPIS and AccGPIS in

Practice

In this section we describe our implementation of GPIS and
AccGPIS algorithm in the experiments 

  Count sketch In this paper we choose the Count
Sketch as our sketching method since it can be calculated in   streaming fashion and we observe that
this sketching method provides the best computational
speed in practice    MATLAB implementation for ef 
 ciently applying the Count Sketch can be found in
 Wang   

  Line search We implement the linesearch scheme
given by  Nesterov    and is described by Algorithm   for GPIS and AccGPIS in our experiments
with parameters        and       

  Gradient restart for AccGPIS We choose   ef 
cient restarting scheme gradient restart proposed by
   Donoghue   Candes   

  Numerical Experiments
  Settings for Environments and Algorithms

We run all the numerical experiments on   DELL laptop
with   GHz Intel Core     CPU and   GB RAM 
MATLAB version     
We choose two recognized algorithms to represent the the
full gradients methods and the  incremental  stochastic gradient method  For the full gradient  we choose the Accelerated projected gradient descent  Beck   Teboulle   
 Nesterov      with linesearch method described in Algorithm   and gradient restart to optimize its performance 
For the stochastic gradients we choose   minibatched version of SAGA  Defazio et al    with various batch
sizes              and       We use the
  The
step size suggested by SAGA   theory which is  
    
code for the minibatch SAGA implementation can be found
in  https github com mdeff saga  We get the estimated
value for    by averaging the largest singular value of each
batch  note that we do not count this into the elapsed time
and epoch counts for SAGA  The sketch size of our proposed methods for each experiments are list in Table  
We use the    projection operator provided by the SPGL 
toolbox  Van Den Berg   Friedlander    in the experiments 

Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

Table   Sketch sizes     for GPIS and AccGPIS for each experiments

SYN 

SYN 

SYN  MAGIC  YEAR

 

 

 

 

 

  Synthetic Data Sets

We start with some numerical experiments on synthetic
problems  Table   to gain some insights into the algorithms  We begin by focusing on    norm constrained
problems   We generate synthetic constrained leastsquare
problems by  rst generating   random matrix sized   by   
then perform SVD on such matrix and replace the singular values with   logarithmically decaying sequence   The
details of the procedure can be found in supplementary materials  Similarly we generate   synthetic problem  Syn 
for lowrank recovery with nuclearnorm constraint  This
is also called the multiple response regression with   generalized form of the Leastsquares 

   cid    arg min
 cid   cid cid  

 cid     AX cid 
   

 

  Real Data Sets

We  rst run an unconstrained leastsquares regression on
the Yearprediction  Millionsong  data set from UCI Machine Learning Repository  Lichman    after we normalize each column of the data matrix  We use this example to demonstrate our algorithms  performance in unconstrained problems 
Then we choose Magic  Gamma Telescope data set from
 Lichman    to generate   constrained Leastsquare
regression problem  The original number of features for
Magic  are     and we normalize each columns of the
original data matrix and additional irrelevant random features as the same way as the experiments in  Langford
et al   ShalevShwartz   Tewari    to the data
sets so that the regressor   cid  can be chosen to select the
sparse set of relevant features by again solving   For
this case we  rst precalculate the   norm of the original
program   solution and then set it as the radius of our   
constraint  The details of the real data sets can be found in
Table  

  Discussion

We measure the performance of the algorithms by the wallclock time  simply using the tic toc function in MATLAB 

 In practice we would consider the    regularized least
squares  but in this paper we focus on the constrained case to make
simple illustrations 

Figure   Experimental results on Millionsong Year prediction
data set  unconstrained LS regression experiment 

     cid 

and the epoch counts  The yaxis of each plot is the relative error log            cid 
  The values below   are
reported as exact recovery of the leastsquare solution 
In all the experiments  our methods achieve the best performance in terms of wallclock time  We show that in
many cases the sketched gradient methods can outperform
leading stochastic gradient methods  Both sketched gradients and stochastic gradients can achieve reduced complexity compared to the  accelerated  full gradient method  but
since the sketched method has innerloops with deterministic iterations  the linesearch scheme of the classic gradient
descent method can be directly used to make each iteration   step size be near optimal  and unlike the stochastic
gradient  our methods do not need to access new minibatches from memory each iteration  which can save operational time in practice 
SAGA performs competitively in terms of epoch counts
 right hand  gures  which is generally achieved using  
small batch size of   Unfortunately the additional cost
of the projection per iteration can severely impact on the
wall clock time performance  The experiment on Syn 
and Syn  are similar but in Syn  we put the constraint on  
dictionary    hence in Syn  the projection operator has an
additional cost of performing such orthogonal transform 
In Syn   wallclock time plot we can see that SAGA with
      has the fastest convergence among all the batch size
choices  but in Syn  it becomes the worst batch size choice
for SAGA since it demands more iterations and hence more
calls on the projection operator  In Syn  we have   more
expensive projection operator since our constraint is on the
nuclearnorm of   matrix        and we can observe that the real convergence speed of SAGA with      
become much slower than any other methods in terms of

 For the unconstrained case  Millionsong data set  sized
      by   we also observe that  SAGA with       is
unattractive in wallclock time since it does not bene   from the
vectorized operation of MATLAB as larger choices of batch size
and takes too many iterations 

 time   log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       epochs log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

Table   Synthetic data set settings      denotes the dense dictionary which is   orthogonal transform      denotes sparsity
or rank of the ground truth

DATA SET

SIZE

  

SYN 
SYN 
SYN   LOW RANK 

   
   
   

 
 
 

 
 

 
 
 

 

 
  
 

Table   Chosen data sets for Leastsquare regression  RFs  number of relevant features

DATA SET

SIZE

RFS  

YEAR
MAGIC 

   

       

 
 

 
 

wallclock time  In this scenario the full gradient method
is much more competitive  However even here as the error
reduces the sketched gradient methods exhibit   computational advantage 

  Conclusions
We propose two sketched gradient algorithms GPIS and
AccGPIS for constrained Leastsquare regression tasks 
We provide theoretical convergence analysis of the proposed algorithms for general sketching methods and
high probability concentration bounds for the Gaussian
sketches  The numerical experiments demonstrates that for
dense large scale overdetermined data sets our sketched
gradient methods performs very well compares to the
stochastic gradient method  minibatch  SAGA and the Accelerated full gradient method in terms of wallclock time
thanks to the bene ts of sketched deterministic iterations 
the ef cient implementation of the Countsketch and the
use of aggressive linesearch methods 

Acknowledgements
JT  MG and MD would like to acknowledge the support
from   MSCAITN Machine Sensing Training Network  MacSeNet  project   EPSRC Compressed
Quantitative MRI grant  number EP    and ERC
Advanced grant  project   CSENSE  respectively 
MD is also supported by   Royal Society Wolfson Research
Merit Award  The authors also give thanks to the anonymous reviewers for insightful comments 

Figure   Experimental results on  from top to button  Syn 
Syn  Syn  and Magic  data sets  The left column is for wallclock time plots  while the right column is for epoch counts

 time   log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       epochs log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       time   log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       epochs log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       time   log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       epochs log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       time   log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       epochs log errorGPISAccPGDAcc GPISminibatch SAGA       minibatch SAGA       minibatch SAGA       Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

References
Ailon     and Chazelle     The fast johnsonlindenstrauss
SIAM

transform and approximate nearest neighbors 
Journal on Computing     

Ailon     and Liberty     Fast dimension reduction using
rademacher series ondual bch codes  Discrete   Computational Geometry     

AllenZhu    

Katyusha  The  rst direct acceleraarXiv preprint

tion of stochastic gradient methods 
arXiv   

Baraniuk     Davenport     DeVore     and Wakin    
  simple proof of the restricted isometry property for
random matrices  Constructive Approximation   
   

Beck     and Teboulle       fast iterative shrinkagethresholding algorithm for
inverse problems 
SIAM Journal on Imaging Sciences     

linear

Candes     Romberg     and Tao     Stable signal recovery
from incomplete and inaccurate measurements  Communications on pure and applied mathematics   
   

Chandrasekaran     and Jordan        Computational and
statistical tradeoffs via convex relaxation  Proceedings
of the National Academy of Sciences     
    

Chandrasekaran     Recht     Parrilo        and Willsky 
      The convex geometry of linear inverse problems 
Foundations of Computational mathematics   
   

Clarkson        and Woodruff        Low rank approximation and regression in input sparsity time  In Proceedings of the forty fth annual ACM symposium on Theory
of computing  pp    ACM   

Johnson     and Zhang     Accelerating stochastic gradient
descent using predictive variance reduction  In Advances
in Neural Information Processing Systems   pp   
  Curran Associates  Inc   

Johnson        and Lindenstrauss     Extensions of lipschitz mappings into   hilbert space  Contemporary mathematics     

Kapralov     Potluru        and Woodruff        How
to fake multiply by   gaussian matrix  arXiv preprint
arXiv   

Kone cn       and Richt arik     Semistochastic gradient de 

scent methods  arXiv preprint arXiv   

Kone cn       Liu     Richt arik     and Tak         Minibatch semistochastic gradient descent in the proximal
setting  IEEE Journal of Selected Topics in Signal Processing     

Langford     Li     and Zhang     Sparse online learning via truncated gradient  Journal of Machine Learning
Research   Mar   

Lichman     UCI machine learning repository    URL

http archive ics uci edu ml 

Mahoney        Randomized algorithms for matrices and
data  Foundations and Trends   cid  in Machine Learning   
   

Nesterov     Gradient methods for minimizing composite

objective function  Technical report  UCL   

Nesterov     Gradient methods for minimizing composite functions  Mathematical Programming   
     

Nesterov     Introductory lectures on convex optimization 
  basic course  volume   Springer Science   Business
Media     

Dasgupta     and Gupta     An elementary proof of   theorem of johnson and lindenstrauss  Random Structures  
Algorithms     

  Donoghue     and Candes     Adaptive restart for accelerated gradient schemes  Foundations of computational
mathematics     

Defazio     Bach     and LacosteJulien     Saga   
fast incremental gradient method with support for nonIn Advances in
strongly convex composite objectives 
Neural Information Processing Systems  pp   
 

Donoho        Compressed sensing  Information Theory 

IEEE Transactions on     

Drineas     Mahoney        Muthukrishnan     and
Sarl os     Faster least squares approximation  Numerische Mathematik     

Oymak     Recht     and Soltanolkotabi     Sharp time 
data tradeoffs for linear inverse problems  arXiv preprint
arXiv   

Pilanci     and Wainwright        Randomized sketches
of convex programs with sharp guarantees  Information
Theory  IEEE Transactions on     

Pilanci     and Wainwright        Iterative hessian sketch 
Fast and accurate solution approximation for constrained
leastsquares  Journal of Machine Learning Research 
   

Gradient Projection Iterative Sketch for LargeScale Constrained LeastSquares

Schmidt     Le Roux     and Bach     Minimizing  nite
sums with the stochastic average gradient  Mathematical
Programming  pp     

ShalevShwartz     and Tewari     Stochastic methods for
  regularized loss minimization  Journal of Machine
Learning Research   Jun   

Van Den Berg     and Friedlander        Spgl    solver

for largescale sparse reconstruction   

Wang       practical guide to randomized matrix computations with matlab implementations  arXiv preprint
arXiv   

Xiao     and Zhang       proximal stochastic gradient
method with progressive variance reduction  SIAM Journal on Optimization     

