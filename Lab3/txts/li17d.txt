Fast kNearest Neighbour Search via Prioritized DCI

Ke Li   Jitendra Malik  

Abstract

Most exact methods for knearest neighbour
search suffer from the curse of dimensionality 
that is  their query times exhibit exponential dependence on either the ambient or the intrinsic
dimensionality  Dynamic Continuous Indexing
 DCI   Li   Malik    offers   promising way
of circumventing the curse and successfully reduces the dependence of query time on intrinsic
dimensionality from exponential to sublinear  In
this paper  we propose   variant of DCI  which
we call Prioritized DCI  and show   remarkable
improvement in the dependence of query time on
intrinsic dimensionality 
In particular    linear
increase in intrinsic dimensionality  or equivalently  an exponential increase in the number of
points near   query  can be mostly counteracted
with just   linear increase in space  We also
demonstrate empirically that Prioritized DCI signi cantly outperforms prior methods  In particular  relative to LocalitySensitive Hashing  LSH 
Prioritized DCI reduces the number of distance
evaluations by   factor of   to   and the memory consumption by   factor of  

  Introduction
The method of knearest neighbours is   fundamental
building block of many machine learning algorithms and
also has broad applications beyond arti cial intelligence 
including in statistics  bioinformatics and database systems      
 Biau et al    Behnam et al    Eldawy   Mokbel    Consequently  since the problem
of nearest neighbour search was  rst posed by Minsky  
Papert   it has for decades intrigued the arti cial intelligence and theoretical computer science communities
alike  Unfortunately  the myriad efforts at devising ef 
cient algorithms have encountered   recurring obstacle  the

 University of California  Berkeley  CA   United States 

Correspondence to  Ke Li  ke li eecs berkeley edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

curse of dimensionality  which describes the phenomenon
of query time complexity depending exponentially on dimensionality  As   result  even on datasets with moderately
high dimensionality  practitioners often have resort to na ve
exhaustive search 
Two notions of dimensionality are commonly considered 
The more familiar notion  ambient dimensionality  refers to
the dimensionality of the space data points are embedded
in  On the other hand  intrinsic dimensionality  characterizes the intrinsic properties of the data and measures the
rate at which the number of points inside   ball grows as
  function of its radius  More precisely  for   dataset with
intrinsic dimension    any ball of radius   contains at most
  rd  points  Intuitively  if the data points are uniformly
distributed on   manifold  then the intrinsic dimensionality
is roughly the dimensionality of the manifold 
Most existing methods suffer from some form of curse
of dimensionality  Early methods like kd trees  Bentley 
  and Rtrees  Guttman    have query times that
grow exponentially in ambient dimensionality  Later methods  Krauthgamer   Lee    Beygelzimer et al   
Dasgupta   Freund    overcame the exponential dependence on ambient dimensionality  but have not been
able to escape from an exponential dependence on intrinsic dimensionality  Indeed  since   linear increase in the
intrinsic dimensionality results in an exponential increase
in the number of points near   query  the problem seems
fundamentally hard when intrinsic dimensionality is high 
Recently  Li   Malik   proposed an approach known
as Dynamic Continuous Indexing  DCI  that successfully
reduces the dependence on intrinsic dimensionality from
exponential to sublinear  thereby making highdimensional
nearest neighbour search more practical  The key observation is that the dif culties encountered by many existing
methods  including kd trees and LocalitySensitive Hashing  LSH   Indyk   Motwani    may arise from their
reliance on space partitioning  which is   popular divideand conquer strategy  It works by partitioning the vector
space into discrete cells and maintaining   data structure

 The measure of intrinsic dimensionality used throughout
this paper is the expansion dimension  also known as the KRdimension  which is de ned as log     where   is the expansion
rate introduced in  Karger   Ruhl   

Fast kNearest Neighbour Search via Prioritized DCI

that keeps track of the points lying in each cell  At query
time  these methods simply look up of the contents of the
cell containing the query and possibly adjacent cells and
perform bruteforce search over points lying in these cells 
While this works well in lowdimensional settings  would
it work in high dimensions 
Several limitations of this approach in highdimensional
space are identi ed in  Li   Malik    First  because
the volume of space grows exponentially in dimensionality 
either the number or the volumes of cells must grow exponentially  Second  the discretization of the space essentially
limits the  eld of view  of the algorithm  as it is unaware
of points that lie in adjacent cells  This is especially problematic when the query lies near   cell boundary  as there
could be points in adjacent cells that are much closer to
the query  Third  as dimensionality increases  surface area
grows faster than volume  as   result  points are increasingly likely to lie near cell boundaries  Fourth  when the
dataset exhibits varying density across space  choosing  
good partitioning is nontrivial  Furthermore  once chosen 
the partitioning is  xed and cannot adapt to changes in density arising from updates to the dataset 
In light of these observations  DCI is built on the idea
of avoiding partitioning the vector space  Instead  it constructs   number of indices  each of which imposes an ordering of all data points  Each index is constructed so that
two points with similar ranks in the associated ordering are
nearby along   certain random direction  These indices are
then combined to allow for retrieval of points that are close
to the query along multiple random directions 
In this paper  we propose   variant of DCI  which assigns  
priority to each index that is used to determine which index
to process in the upcoming iteration  For this reason  we
will refer to this algorithm as Prioritized DCI  This simple
change results in   signi cant improvement in the dependence of query time on intrinsic dimensionality  Specifically  we show   remarkable result    linear increase in
intrinsic dimensionality  which could mean an exponential
increase in the number of points near   query  can be mostly
counteracted with   corresponding linear increase in the
number of indices 
In other words  Prioritized DCI can
make   dataset with high intrinsic dimensionality seem almost as easy as   dataset with low intrinsic dimensionality 
with just   linear increase in space  To our knowledge  there
had been no exact method that can cope with high intrinsic
dimensionality  Prioritized DCI represents the  rst method
that can do so 
We also demonstrate empirically that Prioritized DCI signi cantly outperforms prior methods  In particular  compared to LSH  it achieves     to  fold reduction in the
number of distance evaluations and    fold reduction in
the memory usage 

  Related Work
There is   vast literature on algorithms for nearest neighbour search  They can be divided into two categories 
exact algorithms and approximate algorithms  Early exact algorithms are deterministic and store points in treebased data structures  Examples include kd trees  Bentley    Rtrees  Guttman    and Xtrees  Berchtold
et al      which divide the vector space into  
hierarchy of halfspaces  hyperrectangles or Voronoi polygons and keep track of the points that lie in each cell  While
their query times are logarithmic in the size of the dataset 
they exhibit exponential dependence on the ambient dimensionality    different method  Meiser    partitions the
space by intersecting multiple hyperplanes  It effectively
trades off space for time and achieves polynomial query
time in ambient dimensionality at the cost of exponential
space complexity in ambient dimensionality 

Spill Tree 
RP Tree 

Karger   Ruhl 
Navigating Net 
Cover Tree 
Rank Cover Tree 

DCI 

Prioritized DCI 
 Proposed Method  

Figure   Visualization of the query time complexities of various
exact algorithms as   function of the intrinsic dimensionality   
Each curve represents an example from   class of similar query
time complexities  Algorithms that fall into each particular class
are shown next to the corresponding curve 

To avoid poor performance on worstcase con gurations of
the data  exact randomized algorithms have been proposed 
Spill trees  Liu et al    RP trees  Dasgupta   Freund    and virtual spill trees  Dasgupta   Sinha   
extend the ideas behind kd trees by randomizing the orientations of hyperplanes that partition the space into halfspaces at each node of the tree  While randomization enables them to avoid exponential dependence on the ambient
dimensionality  their query times still scale exponentially in
the intrinsic dimensionality  Whereas these methods rely
on space partitioning  other algorithms  Orchard   
Clarkson    Karger   Ruhl    have been proposed
that utilize local search strategies  These methods start with
  random point and look in the neighbourhood of the current point to  nd   new point that is closer to the query
than the original in each iteration  Like space partitioningbased approaches  the query time of  Karger   Ruhl   
scales exponentially in the intrinsic dimensionality  While

Fast kNearest Neighbour Search via Prioritized DCI

Query Time Complexity

     log        log   
         log   

Method
Exact Algorithms 
RP Tree
Spill Tree
Karger   Ruhl        log   
Navigating Net
Cover Tree
Rank Cover Tree
DCI
Prioritized DCI
 Proposed Method 

      log  
     log   
       log        for      
    max log          
    max log            

   log    max log          
for      

Approximate Algorithms 
kd Tree
BBD Tree
LSH

    log   
    log   
    dn 

 

Table   Query time complexities of various algorithms for  NN
search  Ambient dimensionality  intrinsic dimensionality  dataset
size and approximation ratio are denoted as         and      
  visualization of the growth of various time complexities as  
function of the intrinsic dimensionality is shown in Figure  

the query times of  Orchard    Clarkson    do not
exhibit such undesirable dependence  their space complexities are quadratic in the size of the dataset  making them impractical for large datasets    different class of algorithms
performs search in   coarseto ne manner  Examples include navigating nets  Krauthgamer   Lee    cover
trees  Beygelzimer et al    and rank cover trees  Houle
  Nett    which maintain sets of subsampled data
points at different levels of granularity and descend through
the hierarchy of neighbourhoods of decreasing radii around
the query  Unfortunately  the query times of these methods
again scale exponentially in the intrinsic dimensionality 
Due to the dif culties of devising ef cient algorithms for
the exact version of the problem  there has been extensive
work on approximate algorithms  Under the approximate
setting  returning any point whose distance to the query
is within   factor of       of the distance between the
query and the true nearest neighbour is acceptable  Many
of the same strategies are employed by approximate algorithms  Methods based on treebased space partitioning  Arya et al    and local search  Arya   Mount 
  have been developed  like many exact algorithms 
their query times also scale exponentially in the ambient
dimensionality  LocalitySensitive Hashing  LSH   Indyk
  Motwani    Datar et al    Andoni   Indyk 
  partitions the space into regular cells  whose shapes
are implicitly de ned by the choice of the hash function  It
achieves   query time of   dn  using   dn  space 
where   is the ambient dimensionality    is the dataset
size and           for large   in Euclidean space 

though the dependence on intrinsic dimensionality is not
made explicit 
In practice  the performance of LSH degrades on datasets with large variations in density  due
to the uneven distribution of points across cells  Consequently  various datadependent hashing schemes have
been proposed  Paulev   et al    Weiss et al    Andoni   Razenshteyn    unlike dataindependent hashing schemes  however  they do not allow dynamic updates
to the dataset    related approach    egou et al    decomposes the space into mutually orthogonal axisaligned
subspaces and independently partitions each subspace  It
has   query time linear in the dataset size and no known
guarantee on the probability of correctness under the exact or approximate setting    different approach  Anagnostopoulos et al    projects the data to   lower dimensional space that approximately preserves approximate
nearest neighbour relationships and applies other approximate algorithms like BBD trees  Arya et al    to the
projected data  Its query time is also linear in ambient dimensionality and sublinear in the dataset size  Unlike LSH 
it uses space linear in the dataset size  at the cost of longer
query time than LSH  Unfortunately  its query time is exponential in intrinsic dimensionality 
Our work is most closely related to Dynamic Continuous
Indexing  DCI   Li   Malik    which is an exact randomized algorithm for Euclidean space whose query time
is linear in ambient dimensionality  sublinear in dataset size
and sublinear in intrinsic dimensionality and uses space linear in the dataset size  Rather than partitioning the vector space  it uses multiple global onedimensional indices 
each of which orders data points along   certain random
direction and combines these indices to  nd points that are
near the query along multiple random directions  The proposed algorithm builds on the ideas introduced by DCI and
achieves   signi cant improvement in the dependence on
intrinsic dimensionality 
  summary of the query times of various prior algorithms
and the proposed algorithm is presented in Table   and their
growth as   function of intrinsic dimensionality is illustrated in Figure  

  Prioritized DCI
DCI constructs   data structure consisting of multiple composite indices of data points  each of which in turn consists
of   number of simple indices  Each simple index orders
data points according to their projections along   particular random direction  Given   query  for every composite
index  the algorithm  nds points that are near the query in
every constituent simple index  which are known as candidate points  and adds them to   set known as the candidate
set  The true distances from the query to every candidate
point are evaluated and the ones that are among the   clos 

Fast kNearest Neighbour Search via Prioritized DCI

est to the query are returned 
More concretely  each simple index is associated with  
random direction and stores the projections of every data
point along the direction  They are implemented using
standard data structures that maintain onedimensional ordered sequences of elements  like selfbalancing binary
search trees  Bayer    Guibas   Sedgewick   
or skip lists  Pugh    At query time  the algorithm
projects the query along the projection directions associated with each simple index and  nds the position where
the query would have been inserted in each simple index 
which takes logarithmic time  It then iterates over  or visits  data points in each simple index in the order of their
distances to the query under projection  which takes constant time for each iteration  As it iterates  it keeps track of
how many times each data point has been visited across all
simple indices of each composite index  If   data point has
been visited in every constituent simple index  it is added
to the candidate set and is said to have been retrieved from
the composite index 

Algorithm   Data structure construction procedure
Require    dataset   of   points            pn  the number of simple indices   that constitute   composite index and the number
of composite indices  
function CONSTRUCT         

 ujl            mL random unit vectors in Rd
 Tjl            mL empty binary search trees or skip
for       to   do

lists

jl     into Tjl with pi

jl being the key and

for       to   do

for       to   do
pi
jl   hpi  ujli
Insert  pi
  being the value

end for

end for

end for
return  Tjl  ujl         

end function

DCI has   number of appealing properties compared to
methods based on space partitioning  Because points are
visited by rank rather than location in space  DCI performs
well on datasets with large variations in data density 
It
naturally skips over sparse regions of the space and concentrates more on dense regions of the space  Since construction of the data structure does not depend on the dataset  the
algorithm supports dynamic updates to the dataset  while
being able to automatically adapt to changes in data density  Furthermore  because data points are represented in
the indices as continuous values without being discretized 
the granularity of discretization does not need to be chosen
at construction time  Consequently  the same data structure
can support queries at varying desired levels of accuracy 
which allows   different speedvs accuracy tradeoff to be

made for each individual query 
Prioritized DCI differs from standard DCI in the order in
which points from different simple indices are visited  In
standard DCI  the algorithm cycles through all constituent
simple indices of   composite index at regular intervals and
visits exactly one point from each simple index in each
pass  In Prioritized DCI  the algorithm assigns   priority
to each constituent simple index  in each iteration  it visits
the upcoming point from the simple index with the highest
priority and updates the priority at the end of the iteration 
The priority of   simple index is set to the negative absolute
difference between the query projection and the next data
point projection in the index 

Algorithm   knearest neighbour querying procedure
Require  Query point   in Rd  binary search trees skip lists and
their associated projection vectors  Tjl  ujl          the
number of points to retrieve    and the number of points to visit
   in each composite index
function QUERY   Tjl  ujl           

Cl   array of size   with entries initialized to           
qjl   hq  ujli                 
Sl             
Pl   empty priority queue         
for       to   do

jl     the node in Tjl whose key is the
jl     
jl   qjl 

closest to qjl
jl   with priority    

for       to   do

   

jl     

Insert    

into Pl

end for

end for
for        to        do
for       to   do
if  Sl       then

     

jl       

jl     the node with the highest priority
in Pl
jl       
jl   from Pl and insert the node

Remove      

jl

jl

      

in Tjl whose key is the next closest to qjl 
which is denoted as      
  with
priority      
Cl     
jl     Cl     
if Cl     
jl       then
Sl   Sl       
jl  

  qjl  into Pl

jl      

jl

end if

end if

end for

end for

return   points inSl    Sl that are the closest in

Euclidean distance in Rd to  

end function

Intuitively  this ensures data points are visited in the order
of their distances to the query under projection  Because
data points are only retrieved from   composite index when
they have been visited in all constituent simple indices  data

Fast kNearest Neighbour Search via Prioritized DCI

Complexity

Property
Construction     dn     log   
Query

  dk max log                 
mk log   max log               

Insertion
Deletion
Space

        log   
    log   
  mn 

Table   Time and space complexities of Prioritized DCI 

points are retrieved in the order of the maximum of their
distances to the query along multiple projection directions 
Since distance under projection forms   lower bound on the
true distance  the maximum projected distance approaches
the true distance as the number of projection directions increases  Hence  in the limit as the number of simple indices
approaches in nity  data points are retrieved in the ideal order  that is  the order of their true distances to the query 
The construction and querying procedures of Prioritized DCI are presented formally in Algorithms   and
To ensure the algorithm retrieves the exact   
 
nearest neighbours with high probability 
the analysis in the next section shows that one should choose
        max log                and     
 mk max log              where    denotes the
intrinsic dimensionality  Though because this assumes
worstcase con guration of data points  it may be overly
conservative in practice  so  these parameters may be chosen by crossvalidation 
We summarize the time and space complexities of Prioritized DCI in Table   Notably  the  rst term of the query
complexity  which dominates when the ambient dimensionality   is large  has   more favourable dependence on the intrinsic dimensionality    than the query complexity of standard DCI  In particular    linear increase in the intrinsic dimensionality  which corresponds to an exponential increase
in the expansion rate  can be mitigated by just   linear increase in the number of simple indices    This suggests
that Prioritized DCI can better handle datasets with high
intrinsic dimensionality than standard DCI  which is con 
 rmed by empirical evidence later in this paper 

  Analysis
We analyze the time and space complexities of Prioritized
DCI below and derive the stopping condition of the algorithm  Because the algorithm uses standard data structures 
analysis of the construction time  insertion time  deletion
time and space complexity is straightforward  Hence  this
section focuses mostly on analyzing the query time 
In highdimensional space  query time is dominated by the

time spent on evaluating true distances between candidate
points and the query  Therefore  we need to  nd the number of candidate points that must be retrieved to ensure the
algorithm succeeds with high probability  To this end  we
derive an upper bound on the failure probability for any
given number of candidate points  The algorithm fails if
suf ciently many distant points are retrieved from each
composite index before some of the true knearest neighbours  We decompose this event into multiple  dependent 
events  each of which is the event that   particular distant
point is retrieved before some true knearest neighbours 
Since points are retrieved in the order of their maximum
projected distance  this event happens when the maximum
projected distance of the distant point is less than that of
  true knearest neighbour  We start by  nding an upper
bound on the probability of this event  To simplify notation  we initially consider displacement vectors from the
query to each data point  and so relationships between projected distances of triplets of points translate relationships
between projected lengths of pairs of displacement vectors 
We start by examining the event that   vector under random onedimensional projection satis es some geometric
constraint  We then  nd an upper bound on the probability that some combinations of these events occur  which is
related to the failure probability of the algorithm 

   be        unit vectors in Rd drawn uniformly

  cos vs   vl   

Lemma   Let vl  vs   Rd be such that vl   vs 
      
Then Pr maxj hvl    ji    vs   
     
Proof  The event maxj hvl    ji     kvsk   is equivalent to the event that hvl    ji    kvsk       which is
the intersection of the events  hvl    ji    kvsk    Because       are drawn independently  these events are independent 
Let    be the angle between vl and      so that hvl    ji  
 vl  cos     Since     is drawn uniformly     is uniformly
distributed on     Hence 
    hvl    ji     kvsk 
Pr max
Pr hvl    ji    kvsk 
MYj 
Pr cos     kvsk 
kvlk 
MYj 
MYj Pr    cos kvsk 
kvlk  
     

cos kvsk 

at random 

 

 

 

 
 

kvlk      cos kvsk 

kvlk 

Fast kNearest Neighbour Search via Prioritized DCI

Lemma   For any set of events  Ei  
that at least    of them happen is at most  

   the probability
   Pr  Ei 

  PN

  

    

 ETl 

Consider

De ne      

 ET   ST        

to be Ei         
   Pr            Pj

Proof  For any set          de ne  ET to be the intersection of events indexed by   and complements of events not
indexed by           ET  Ti   Ei Ti    Ei  Observe
that    EToT    
are disjoint and that for any         
Ti   Ei   ST  
 ET   The event that at least    of Ei  
happen is SI        Ti   Ei  which is equivalent to
SI        ST  
 ET   We will
henceforth use   to denote                    Since
  is    nite set  we can impose an ordering on its elements
and denote the lth element as Tl  The event can therefore
be rewritten asS    
 ETl  We claim
   Pr   ETl  for all    
that PN
             We will show this by induction on   
For       the claim is vacuously true because probabilities are nonnegative  For       we observe that        
          ETj            ETj                     ETj 
for all    Since          ETj and          ETj are disjoint 
Pr          Pr          Pr          ETj 
Pr        which
quantity Pi Tj
Pi Tj Pr          Pr          ETj  by the above
For each     Tj   ETj   Ei  and so
 ETl    Ei       
 ETl           Be 
 ETj       
causen  ETlo    
 ETl     ETj  
are disjoint   ETj       
Hence   ETj         and so          ETj    ETj   Thus 
Pr           Tj  Pr   ETj   Pi Tj
Pr       
Pi Tj
   Pr           Tj  Pr   ETj   
follows that PN
Pr       
Pr          Pi  Tj
Pi Tj
Pr          Pr          Pr          ETj   
Pr        and  Tj 
   Pr         
   PN
  Pr   ETj    PN
   Pr       
   Pr   ETl 
   Pr            Pj 
hypothesis  PN
   Pr   ETl  which
   Pr            Pj
Therefore  PN
concludes the induction argument 
The lemma is   special case of this claim when         
 ETl 
  Ei andP    
since        

   Pr   ETl    Pr      

By the inductive

observation 

Because

    

    

    

the

   

It

is

  

  

Combining the above yields the following theorem  the
proof of which is found in the supplementary material 

least    of

ability that at
most  

Theorem   Let  vl
   
   and  vs
     
   be sets of vectors such that  vl
      vs
  
              
    Furthermore  let   ij            be random uniformly distributed unit vectors such that                iM
are independent for any given    Consider the events
        maxj hvl
    
     iji    vs
 vs
The probthese events occur is at
     where
  cos vs
max   vl
      
  PN
     Furthermore  if         it is
 vs
max    maxi vs
  Mo 
at most mini          
max   vl
  cos vs

We now apply the results above to analyze speci   properties of the algorithm  For convenience  instead of working
directly with intrinsic dimensionality  we will analyze the
query time in terms of   related quantity  global relative
sparsity  as de ned in  Li   Malik    We reproduce
its de nition below for completeness 
De nition   Given   dataset     Rd  let Bp    be the
set of points in   that are within   ball of radius   around
  point      dataset   has global relative sparsity of      
if for all   and     Rd such that  Bp       Bp   
 Bp    where      
Global
relative sparsity is related to the expansion
rate  Karger   Ruhl    and intrinsic dimensionality in
the following way    dataset with global relative sparsity
of       has     log   expansion and intrinsic dimensionality of   log   
Below we derive two upper bounds on the probability that
some of the true knearest neighbours are missing from the
set of candidate points retrieved from   given composite
index  which are in expressed in terms of    and    respectively  These results inform us how    and    should be
chosen to ensure the querying procedure returns the correct
results with high probability  In the results that follow  we
   to denote   reordering of the points  pi  
use       
  
so that      is the ith closest point to the query    Proofs are
found in the supplementary material 
Lemma   Consider points in the order they are retrieved from   composite index that consists of  
simple indices 
there are at
least    points that are not the true knearest neighbours but are retrieved before some of them is at most

The probability that

 

  kPn

        

  cos                       

Lemma   Consider point projections in   composite
index that consists of   simple indices in the order they
are visited  The probability that    point projections that
are not of the true knearest neighbours are visited before
all true knearest neighbours have been retrieved is at most

 

  mkPn

        

  cos                     

Fast kNearest Neighbour Search via Prioritized DCI

 

dataset

with

     

the

global
quantity

Lemma
relative

 
On
sparsity

for any  

 

there is

index
some   

index
some   

      and   given composite
there is

        
Pn
at most     max log             log   

  cos                       is

      and   given composite
there is

Lemma   For   dataset with global relative sparconsist 
sity
ing of   simple indices 
 
   max log             log    such that the probability that the candidate points retrieved from the composite
index do not include some of the true knearest neighbours
is at most some constant      
Lemma   For   dataset with global relative sparsity
consisting of   simple indices 
 
 mk max log           log    such that the probability that the candidate points retrieved from the composite
index do not include some of the true knearest neighbours
is at most some constant      
Theorem   For   dataset with global relative sparsity      
some   
        max log             log    and     
 mk max log           log    such that the algorithm returns the correct set of knearest neighbours with
probability of at least      
Now that we have found   choice of    and    that suf ces
to ensure correctness with high probability  we can derive
  bound on the query time that guarantees correctness  We
then analyze the time complexity for construction  insertion and deletion and the space complexity  Proofs of the
following are found in the supplementary material 
Theorem   For   given number of simple indices    the

algorithm takes   dk max log               
mk log   max log              time to re 

trieve the knearest neighbours at query time  where    denotes the intrinsic dimensionality 
Theorem   For   given number of simple indices    the
algorithm takes     dn    log    time to preprocess the
data points in   at construction time 
Theorem   The algorithm requires         log    time
to insert   new data point and     log    time to delete  
data point 
Theorem   The algorithm requires   mn  space in addition to the space used to store the data 

  Experiments
We compare the performance of Prioritized DCI to that
of standard DCI  Li   Malik    product quantization    egou et al    and LSH  Datar et al   
which is perhaps the algorithm that is most widely used

in highdimensional settings  Because LSH operates under the approximate setting  in which the performance metric of interest is how close the returned points are to the
query rather than whether they are the true knearest neighbours  All algorithms are evaluated in terms of the time
they would need to achieve varying levels of approximation quality 
Evaluation is performed on two datasets  CIFAR 
   Krizhevsky   Hinton    and MNIST  LeCun
et al    CIFAR  consists of     colour images of   types of objects in natural scenes and MNIST
consists of     grayscale images of handwritten digits 
The images in CIFAR  have   size of     and three
colour channels  and the images in MNIST have   size of
      and   single colour channel  We reshape each image into   vector whose entries represent pixel intensities
at different locations and colour channels in the image  So 
each vector has   dimensionality of           for
CIFAR  and           for MNIST  Note that the
dimensionalities under consideration are much higher than
those typically used to evaluate prior methods 
For the purposes of nearest neighbour search  MNIST is  
more challenging dataset than CIFAR  This is because
images in MNIST are concentrated around   few modes 
consequently  data points form dense clusters  leading to
higher intrinsic dimensionality  On the other hand  images in CIFAR  are more diverse  and so data points
are more dispersed in space  Intuitively  it is much harder
to  nd the closest digit to   query among   other digits
of the same category that are all plausible near neighbours
than to  nd the most similar natural image among   few
other natural images with similar appearance  Later results
show that all algorithms need fewer distance evaluations to
achieve the same level of approximation quality on CIFAR 
  than on MNIST 
We evaluate performance of all algorithms using crossvalidation  where we randomly choose ten different splits
of query vs  data points  Each split consists of   points
from the dataset that serve as queries  with the remainder
designated as data points  We use each algorithm to retrieve
the   nearest neighbours at varying levels of approximation quality and report mean performance and standard deviation over all splits 
Approximation quality is measured using the approximation ratio  which is de ned to be the ratio of the radius of
the ball containing the set of true knearest neighbours to
the radius of the ball containing the set of approximate knearest neighbours returned by the algorithm  The closer
the approximation ratio is to   the higher the approximation quality  In high dimensions  the time taken to compute
true distances between the query and the candidate points
dominates query time  so the number of distance evalua 

Fast kNearest Neighbour Search via Prioritized DCI

   

   

   

Figure   Comparison of the number of distance evaluations needed by different algorithms to achieve varying levels of approximation
quality on     CIFAR  and       MNIST  Each curve represents the mean over ten folds and the shaded area represents   standard
deviation  Lower values are better      Closeup view of the  gure in    

tions can be used as an implementationindependent proxy
for the query time 
For LSH  we used   hashes per table and   tables  which
we found to achieve the best approximation quality given
the memory constraints  For product quantization  we used
  dataindependent codebook with   entries so that the
algorithm supports dynamic updates  For standard DCI  we
used the same hyparameter settings used in  Li   Malik 
         and       on CIFAR  and       and
      on MNIST  For Prioritized DCI  we used two different settings  one that matches the hyperparameter settings
of standard DCI  and another that uses less space       
and       on both CIFAR  and MNIST 
We plot the number of distance evaluations that each algorithm requires to achieve each desired level of approximation ratio in Figure   As shown  on CIFAR  under the
same hyperparameter setting used by standard DCI  Prioritized DCI requires   to   fewer distance evaluations than standard DCI    to   fewer distance
evaluations than product quantization  and   to  
fewer distance evaluations than LSH to achieve same levels
approximation quality  which represents    fold reduction in the number of distance evaluations relative to LSH
on average  Under the more spaceef cient hyperparameter
setting  Prioritized DCI achieves    fold reduction compared to LSH  On MNIST  under the same hyperparameter setting used by standard DCI  Prioritized DCI requires
  to   fewer distance evaluations than standard
DCI    to   fewer distance evaluations than product quantization  and   to   fewer distance evaluations than LSH  which represents    fold reduction
relative to LSH on average  Under the more spaceef cient
hyperparameter setting  Prioritized DCI achieves    fold
reduction compared to LSH 
We compare the space ef ciency of Prioritized DCI to that

of standard DCI and LSH  As shown in Figure   in the
supplementary material  compared to LSH  Prioritized DCI
uses   less space on CIFAR  and   less space
on MNIST under the same hyperparameter settings used
by standard DCI  This represents    fold reduction in
memory consumption on CIFAR  and    fold reduction on MNIST  Under the more spaceef cient hyperparameter setting  Prioritized DCI uses   less space on
CIFAR  and   less space on MNIST relative to
LSH  which represents    fold reduction on CIFAR 
and    fold reduction on MNIST 
In terms of wallclock time  our implementation of Prioritized DCI takes   seconds to construct the data structure
and execute   queries on MNIST  compared to  
seconds taken by LSH 

  Conclusion
In this paper  we presented   new exact randomized algorithm for knearest neighbour search  which we refer to as
Prioritized DCI  We showed that Prioritized DCI achieves
  signi cant improvement in terms of the dependence of
query time complexity on intrinsic dimensionality compared to standard DCI  Speci cally  Prioritized DCI can to
  large extent counteract   linear increase in the intrinsic
dimensionality  or equivalently  an exponential increase in
the number of points near   query  using just   linear increase in the number of simple indices  Empirical results
validated the effectiveness of Prioritized DCI in practice 
demonstrating the advantages of Prioritized DCI over prior
methods in terms of speed and memory usage 

Acknowledgements  This work was
supported by
DARPA   NF  Ke Li thanks the Natural
Sciences and Engineering Research Council of Canada
 NSERC  for fellowship support 

Fast kNearest Neighbour Search via Prioritized DCI

References
Anagnostopoulos  Evangelos  Emiris  Ioannis    and Psarros  Ioannis 
Lowquality dimension reduction and
highdimensional approximate nearest neighbor  In  st
International Symposium on Computational Geometry
 SoCG   pp     

Andoni  Alexandr and Indyk  Piotr  Nearoptimal hashing
algorithms for approximate nearest neighbor in high dimensions  In Foundations of Computer Science   
FOCS   th Annual IEEE Symposium on  pp   
  IEEE   

Andoni  Alexandr and Razenshteyn  Ilya  Optimal datadependent hashing for approximate near neighbors  In
Proceedings of the FortySeventh Annual ACM on Symposium on Theory of Computing  pp    ACM 
 

Arya  Sunil and Mount  David    Approximate nearest
In SODA  vol 

neighbor queries in  xed dimensions 
ume   pp     

Arya  Sunil  Mount  David    Netanyahu  Nathan    Silverman  Ruth  and Wu  Angela    An optimal algorithm for approximate nearest neighbor searching  xed
dimensions  Journal of the ACM  JACM   
   

Bayer  Rudolf  Symmetric binary btrees  Data structure
and maintenance algorithms  Acta informatica   
   

Behnam  Ehsan  Waterman  Michael    and Smith  Andrew      geometric interpretation for local alignmentfree sequence comparison  Journal of Computational Biology     

Bentley  Jon Louis  Multidimensional binary search trees
used for associative searching  Communications of the
ACM     

Berchtold  Stefan  Keim  Daniel    and peter Kriegel 
Hans 
The Xtree  An index structure for highdimensional data  In Very Large Data Bases  pp   
 

Berchtold  Stefan  Ertl  Bernhard  Keim  Daniel    Kriegel 
HP  and Seidl  Thomas  Fast nearest neighbor search
in highdimensional space  In Data Engineering   
Proceedings   th International Conference on  pp 
  IEEE   

Biau    erard  Chazal  Fr ed eric  CohenSteiner  David  Devroye  Luc  Rodriguez  Carlos  et al    weighted knearest neighbor density estimate for geometric inference  Electronic Journal of Statistics     
Clarkson  Kenneth    Nearest neighbor queries in metric
spaces  Discrete   Computational Geometry   
   

Dasgupta  Sanjoy and Freund  Yoav  Random projection
trees and low dimensional manifolds 
In Proceedings
of the Fortieth Annual ACM Symposium on Theory of
Computing  pp    ACM   

Dasgupta  Sanjoy and Sinha  Kaushik  Randomized partition trees for nearest neighbor search  Algorithmica   
   

Datar  Mayur  Immorlica  Nicole  Indyk  Piotr  and Mirrokni  Vahab    Localitysensitive hashing scheme based
on pstable distributions  In Proceedings of the twentieth annual symposium on Computational geometry  pp 
  ACM   

Eldawy  Ahmed and Mokbel  Mohamed    SpatialHadoop 
  MapReduce framework for spatial data  In Data Engineering  ICDE    IEEE  st International Conference on  pp    IEEE   

Guibas  Leo   and Sedgewick  Robert    dichromatic
framework for balanced trees  In Foundations of Computer Science     th Annual Symposium on  pp 
  IEEE   

Guttman  Antonin  Rtrees    dynamic index structure
for spatial searching  In Proceedings of the   ACM
SIGMOD International Conference on Management of
Data  pp     

Houle  Michael   and Nett  Michael  Rankbased similarity
search  Reducing the dimensional dependence  Pattern
Analysis and Machine Intelligence  IEEE Transactions
on     

Indyk  Piotr and Motwani  Rajeev  Approximate nearest
neighbors  towards removing the curse of dimensionality  In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing  pp    ACM 
 

  egou  Herv    Douze  Matthijs  and Schmid  Cordelia 
Product quantization for nearest neighbor search  Pattern Analysis and Machine Intelligence  IEEE Transactions on     

Beygelzimer  Alina  Kakade  Sham  and Langford  John 
Cover trees for nearest neighbor 
In Proceedings of
the  rd International Conference on Machine Learning  pp    ACM   

Karger  David   and Ruhl  Matthias  Finding nearest
neighbors in growthrestricted metrics  In Proceedings
of the Thiryfourth Annual ACM Symposium on Theory
of Computing  pp    ACM   

Fast kNearest Neighbour Search via Prioritized DCI

Krauthgamer  Robert and Lee  James    Navigating nets 
simple algorithms for proximity search  In Proceedings
of the Fifteenth Annual ACMSIAM Symposium on Discrete Algorithms  pp    Society for Industrial and
Applied Mathematics   

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple
layers of features from tiny images  Technical report 
University of Toronto   

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  Proceedings of the IEEE   
   

Li  Ke and Malik  Jitendra  Fast knearest neighbour search
In International

via Dynamic Continuous Indexing 
Conference on Machine Learning  pp     

Liu  Ting  Moore  Andrew    Yang  Ke  and Gray  Alexander    An investigation of practical approximate nearest
neighbor algorithms  In Advances in Neural Information
Processing Systems  pp     

Meiser  Stefan  Point location in arrangements of hyperplanes  Information and Computation   
 

Minsky  Marvin and Papert  Seymour  Perceptrons  an in 

troduction to computational geometry  pp     

Orchard  Michael      fast nearestneighbor search algorithm  In Acoustics  Speech  and Signal Processing 
  ICASSP    International Conference on 
pp    IEEE   

Paulev    Lo      egou  Herv    and Amsaleg  Laurent  Locality sensitive hashing    comparison of hash function
types and querying mechanisms  Pattern Recognition
Letters     

Pugh  William  Skip lists    probabilistic alternative to balanced trees  Communications of the ACM   
   

Weiss  Yair  Torralba  Antonio  and Fergus  Rob  Spectral
hashing  In Advances in Neural Information Processing
Systems  pp     

