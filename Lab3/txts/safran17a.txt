DepthWidth Tradeoffs in Approximating Natural Functions with Neural

Networks

Itay Safran   Ohad Shamir  

Abstract

We provide several new depthbased separation
results for feedforward neural networks  proving
that various types of simple and natural functions
can be better approximated using deeper networks than shallower ones  even if the shallower
networks are much larger  This includes indicators of balls and ellipses  nonlinear functions
which are radial with respect to the    norm  and
smooth nonlinear functions  We also show that
these gaps can be observed experimentally  Increasing the depth indeed allows better learning
than increasing width  when training neural networks to learn an indicator of   unit ball 

  Introduction
Deep learning  in the form of arti cial neural networks  has
seen   dramatic resurgence in the past recent years  achieving great performance improvements in various  elds of
arti cial intelligence such as computer vision and speech
recognition  While empirically successful  our theoretical
understanding of deep learning is still limited at best 
An emerging line of recent works has studied the expressive power of neural networks  What functions can and
cannot be represented by networks of   given architecture
 see related work section below    particular focus has
been the tradeoff between the network   width and depth 
On the one hand  it is wellknown that large enough networks of depth   can already approximate any continuous
target function on      to arbitrary accuracy  Cybenko 
  Hornik    On the other hand  it has long been
evident that deeper networks tend to perform better than
shallow ones    phenomenon supported by the intuition that
depth  providing compositional expressibility  is necessary
for ef ciently representing some functions  Moreover  re 

 Weizmann Institute of Science  Rehovot  Israel  Correspondence to  Itay Safran  itay safran weizmann ac il  Ohad
Shamir  ohad shamir weizmann ac il 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

cent empirical evidence suggests that standard feedforward
deep networks are harder to optimize than shallower networks which lead to worse training error and testing error
 He et al   
To demonstrate the power of depth in neural networks   
clean and precise approach is to prove the existence of
functions which can be expressed  or wellapproximated 
by moderatelysized networks of   given depth  yet cannot
be approximated well by shallower networks  even if their
size is much larger  However  the mere existence of such
functions is not enough  Ideally  we would like to show
such depth separation results using natural  interpretable
functions  of the type we may expect neural networks to
successfully train on  Proving that depth is necessary for
such functions can give us   clearer and more useful insight
into what various neural network architectures can and cannot express in practice 
In this paper  we provide several contributions to this
emerging line of work  We focus on standard  vanilla feedforward networks  using some  xed activation function 
such as the popular ReLU  and measure expressiveness
directly in terms of approximation error  de ned as the expected squared loss with respect to some distribution over
the input domain  In this setting  we show the following 

  We prove that the indicator of the Euclidean unit ball 
   cid     cid   cid      in Rd  which can be easily approximated to accuracy   using    layer network with
     neurons  cannot be approximated to an accuracy higher than      using    layer network 
unless its width is exponential in    In fact  we show
the same result more generally  for any indicator of
an ellipsoid    cid     cid Ax     cid        where   is  
nonsingular matrix and   is   vector  The proof is
based on   reduction from the main result of  Eldan  
Shamir    which shows   separation between  
layer and  layer networks using   more complicated
and less natural radial function 

  We prove that any   

 cid 
   cid   cid  where     Rd and           is
piecewiselinear  cannot be approximated to accuracy
  by   depth   ReLU network of width less than

radial

function  

DepthWidth Tradeoffs in Approximating Natural Functions with Neural Networks

 min  exp    In contrast  such functions
can be represented exactly by  layer ReLU networks 
  We show that this depth width tradeoff can also be
observed experimentally  Speci cally  that when using standard backpropagation to learn the indicators
of the    and    unit balls   layer nets give significantly better performance compared to  layer nets
 even if much larger  Our theoretical results indicate that this gap in performance is due to approximation error issues  This experiment also highlights
the fact that our separation result is for   natural function that is not just wellapproximated by some  layer
network  but can also be learned well from data using
standard methods 

  Finally  we prove that any member of   wide family of nonlinear and twicedifferentiable functions
 including for instance    cid     in     which
can be approximated to accuracy   using ReLU networks of depth and width   poly log  cannot be approximated to similar accuracy by constantdepth ReLU networks  unless their width is at least
 poly  We note that   similar result appeared online concurrently and independently of ours
in  Yarotsky    Liang   Srikant    but the
setting is   bit different  see related work below for
more details 

RELATED WORK

The question of studying the effect of depth in neural network has received considerable attention recently  and studied under various settings  Many of these works consider   somewhat different setting than ours  and hence are
not directly comparable  These include networks which
are not plainvanilla ones      
 Cohen et al    Delalleau   Bengio    Martens   Medabalimi   
measuring quantities other than approximation error      
 Bianchini   Scarselli    Poole et al    focusing only on approximation upper bounds      
 Shaham
et al    or measuring approximation error in terms
of   type bounds       supx                 rather than
  type bounds Ex                       Yarotsky   
Liang   Srikant    We note that the latter distinction is important  Although    bounds are more common
in the approximation theory literature     bounds are more
natural in the context of statistical machine learning problems  where we care about the expected loss over some distribution  Moreover     approximation lower bounds are
stronger  in the sense that an    lower bound easily translates to   lower bound on    lower bound  but not vice
versa 

 To give   trivial example  ReLU networks always express
continuous functions  and therefore can never approximate   dis 

  noteworthy paper in the same setting as ours is  Telgarsky    which proves   separation result between
the expressivity of ReLU networks of depth   and depth
      log      for any   
This holds even for onedimensional functions  where   depth   network is shown
to realize   sawtooth function with exp      oscillations  whereas any network of depth       log     would
require   width superpolynomial in   to approximate it by
more than   constant 
In fact  we ourselves rely on this
construction in the proofs of our results in section   On
the  ip side  in our paper we focus on separation in terms
of the accuracy or dimension  rather than   parameter   
Moreover  the construction there relies on   highly oscillatory function  with Lipschitz constant exponential in  
almost everywhere  In contrast  in our paper we focus on
simpler functions  of the type that are likely to be learnable
from data using standard methods 
Our separation results in Sec     for smooth nonlinear
functions  are closely related to those of  Yarotsky   
Liang   Srikant    which appeared online concurrently and independently of our work  and the proof ideas
are quite similar  However  these papers focused on   
bounds rather than    bounds  Moreover   Yarotsky   
considers   class of functions different than ours in their
positive results  and  Liang   Srikant    consider networks employing   mix of ReLU and threshold activations 
whereas we consider   purely ReLU network 
Another relevant and insightful work is  Poggio et al 
  which considers width vs  depth and provide general results on expressibility of functions with   compositional nature  However  the focus there is on worsecase approximation over general classes of functions  rather than
separation results in terms of speci   functions as we do
here  and the details and setting is somewhat orthogonal to
ours 

  Preliminaries
In general  we let boldfaced letters such as    
            xd  denote vectors  and capital letters denote matrices or probabilistic events   cid cid  denotes the Euclidean
norm  and  cid cid  the  norm      denotes the indicator
function  We use the standard asymptotic notation    and
  to hide constants  and     and   to hide constants
and factors logarithmic in the problem parameters 
Neural Networks  We consider feedforward neural networks  computing functions from Rd to    The network
is composed of layers of neurons  where each neuron computes   function of the form    cid     cid        where  
continuous function such as    cid           in an    sense  yet
can easily approximate it in an    sense given any continuous
distribution 

DepthWidth Tradeoffs in Approximating Natural Functions with Neural Networks

                    cid 

hand for cid   cid 

      bn cid  we de ne  

is   weight vector    is   bias term and        cid    is  
nonlinear activation function  such as the ReLU function
            max     Letting           be   shortlayer of   neurons as    cid            By denoting the
output of the ith layer as Oi  we can de ne   network of
arbitrary depth recursively by Oi     Wi Oi   bi 
where Wi  bi represent the matrix of weights and bias of
the ith layer  respectively  Following   standard convention
for multilayer networks  the  nal layer   is   purely linear
function with no bias       Oh   Wh   Oh  We de ne the
depth of the network as the number of layers    and denote
the number of neurons ni in the ith layer as the size of the
layer  We de ne the width of   network as maxi    ni 
Finally    ReLU network is   neural network where all the
nonlinear activations are the ReLU function  We use  
layer  and  layer  to denote networks of depth   and  
In particular  in our notation    layer ReLU network has
the form

some

for
ddimensional vectors            wn  Similarly     layer
ReLU network has the form

              vn    bn 

and

  
parameters

   cid    cid 
    cid 

vi  

ui

  cid 

  

  

vi      cid 

      bi 

 cid   cid 

 cid 

  jx   bi  

  ci

 

 

 

by some other function    as cid 

for some parameters  ui  vi    bi    ci  wi   
Approximation error  Given some function   on   domain
  endowed with some probability distribution  with density function   we de ne the quality of its approximation
                   dx  
Ex                We refer to this as approximation in the   norm sense  In one of our results  Thm   
we also consider approximation in the   norm sense  de 
 ned as supx                   Clearly  this upperbounds
the  square root of the     approximation error de ned
above  so as discussed in the introduction  lower bounds
on the    approximation error         any distribution  are
stronger than lower bounds on the    approximation error 

approximated with  layer networks  no  layer network
can approximate it to high accuracy        any distribution 
unless its width is exponential in the dimension  This is
formally stated in the following theorem 
Theorem    Inapproximability with  layer networks 
The following holds for some positive universal constants
            and any network employing an activation
function satisfying Assumptions   and   in Eldan   Shamir
  For any        and any nonsingular matrix     Rd        Rd and       there exists   continuous probability distribution   on Rd  such
that for any function   computed by    layer network of
width at most    exp      and for the function        
   cid Ax     cid       we have

 cid 

                   dx     
    

Rd

We note that the assumptions from  Eldan   Shamir   
are very mild  and apply to all standard activation functions 
including ReLU  sigmoid and threshold  For completeness  the fully stated assumptions are presented in Subsection   
The formal proof of Thm     provided below  is based on  
reduction from the main result of  Eldan   Shamir   
which shows the existence of   certain radial function  depending on the input   only through its norm  and   probability distribution which cannot be expressed by    layer
network  whose width is less than exponential in the dimension   to more than constant accuracy    closer look
at the proof reveals that this function  denoted as     can
be expressed as   sum of     indicators of    balls of
various radii  We argue that if we could have accurately
approximated   given    ball indicator with respect to all
distributions  then we could have approximated all the indicators whose sum add up to     and hence reach   contradiction  By   linear transformation argument  we show the
same contradiction would have occured if we could have
approximated the indicators of an nondegenerate ellipse
with respect to any distribution  The formal proof is provided below 

  Indicators of    Balls and Ellipsoids
We begin by considering one of the simplest possible function classes on Rd  namely indicators of    balls  and more
generally  ellipsoids  The ability to compute such functions is necessary for many useful primitives  for example determining if the distance between two points in Euclidean space is below or above some threshold  either with
respect to the Euclidean distance  or   more general Mahalanobis distance  In this section  we show   depth separation result for such functions  Although they can be easily

Proof of Thm    Assume by contradiction that for   as described in the theorem  and for any distribution   there
exists    layer network     of width at most    exp     

such that cid 

 cid 

 cid 

             

   dx         
    

  Rd

Let    and    be         nonsingular matrix and vector respectively  to be determined later  We begin by performing
  change of variables       Ax                    

dx  

 cid 

  Rd

 cid cid cid det
 cid 

 
   

 cid     cid cid cid cid    dy  which yields
 cid     cid 
 cid cid cid 
 cid cid       
 cid     cid 
 cid     cid cid cid cid    dy      
 cid cid   cid cid cid det
 cid     cid 

      
      

      

In particular  let us choose the distribution   de ned as
        det           Az       where   is the  continuous 
distribution used in the main result of  Eldan   Shamir 
         
     Az     dz  which by the change of variables
     dx    
Plugging the de nition of   in Eq    and using the fact
that   det          det          we get

   note that   is indeed   distribution  since cid 
 det     cid 
     Az       dx     det     dz equals cid 
 cid 
 cid     cid 
 cid 

 cid     cid 

 cid cid       

 cid cid cid 

      

 

      
        dy    

 

  Rd

 

 

 cid 

 cid 

  Rd

     
 

  cid 

Letting       be an arbitrary parameter  we now pick     
     Recalling the de nition of   as    cid 
    and       
 
 cid cid cid 
 cid   
   cid Ax     cid       we get that
   cid   cid            
    cid cid  expresses    layer net 

Note that    
work composed with   linear transformation of the input 
and hence can be expressed in turn by    layer network
 as we can absorb the linear transformation into the parameters of each neuron in the  rst layer  Therefore  letting
 cid   cid     
       dy denote the norm in    function space  we showed the following  For any       there
exists    layer network  fz such that

 cid   
    cid      
 cid cid 

        dy    

 

   cid cid          fz  

   

 

 

 cid cid cid cid   

With this key result in hand  we now turn to complete the
proof  We consider the function    from  Eldan   Shamir 
  for which it was proven that no  layer network can
approximate it          to better than constant accuracy 
unless its width is exponential in the dimension    In particular    can be written as

       

        cid   cid     ai  bi   

where  ai  bi  are disjoint intervals           and
        where   is the dimension  Since    can also be
written as

      cid   cid    bi       cid   cid    ai   

 cid cid cid cid 

  cid 

  

  cid 

  

DepthWidth Tradeoffs in Approximating Natural Functions with Neural Networks

we get by Eq    and the triangle inequality that

 cid cid cid cid cid        cid 
    cid 

   

  

  

    

 

 

        fbi     fai 

 cid cid cid cid cid   
 cid cid cid cid   
 cid cid cid   cid cid    ai     fai 
 cid cid cid   

   cid cid    bi     fbi

 cid 

 cid cid cid cid cid 

 

width at most  nw  we get that cid  
   cid         

However  since   linear combination of     layer neural
networks of width at most   is still    layer network  of
           fbi     fai 
is    layer network  of width at most          exp     
which approximates    to an accuracy of less than   
   
   Hence  by picking         
suf ciently small  we get   contradiction to the result of
 Eldan   Shamir    that no  layer network of width
smaller than   exp cd   for some constant    can approximate    to more than constant accuracy  for   suf ciently
large dimension   

 

     

ately  hence approximating    cid   cid   cid     cid  

To complement Thm    we also show that such indicator functions can be easily approximated with  layer networks  The argument is quite simple  Using an activation
such as ReLU or Sigmoid  we can use one layer to approximate any Lipschitz continuous function on any bounded
interval  and in particular    cid     Given   vector     Rd 
we can apply this construction on each coordinate xi seperi   Similarly  we can approximate    cid   cid Ax     cid  for arbitrary
 xed matrices   and vectors    Finally  with    layer network  we can use the second layer to compute   continuous
approximation to the threshold function    cid           
Composing these two layers  we get an arbitrarily good approximation to the function    cid     cid Ax     cid             
any continuous distribution  with the network size scaling
polynomially with the dimension   and the required accuracy 
In the theorem below  we formalize this intuition 
where for simplicity we focus on approximating the indicator of the unit ball 
Theorem    Approximability with  layer networks 
Given       for any activation function   satisfying Assumption   in Eldan   Shamir   and any continuous
probability distribution   on Rd  there exists   constant   
dependent only on   and   function   expressible by    
 
layer network of width at most max
such that the following holds 

 cid 

        

 cid 

 cid 

            cid   cid            dx    

 cid 

Rd

where    is   constant depending solely on  

DepthWidth Tradeoffs in Approximating Natural Functions with Neural Networks

The proof of the theorem appears in the supplementary material

  An Experiment

In this subsection  we empirically demonstrate that indicator functions of    balls are indeed easier to learn with  
 layer network  compared to    layer network  even if
the  layer network is signi cantly larger  This indicates
that the depth width tradeoff for indicators of balls  predicted by our theory  can indeed be observed experimentally  Moreover  it highlights the fact that our separation
result is for simple natural functions  that can be learned
reasonably well from data using standard methods 
For our experiment  we sampled       data instances in
   with   direction chosen uniformly at random and  
norm drawn uniformly at random from the interval    
To each instance  we associated   target value computed
according to the target function            cid   cid      Another       examples were generated in   similar manner
and used as   validation set 
We trained   ReLU networks on this dataset 

  One  layer network  with    rst hidden layer of size
    second hidden layer of size   and   linear
output neuron 

  Four  layer networks  with hidden layer of sizes

      and   and   linear output neuron 

Training was performed with backpropagation  using the
TensorFlow library  We used the squared loss  cid      cid   
       cid  and batches of size   For all networks  we
chose   momentum parameter of   and   learning rate
starting at   decaying by   multiplicative factor of  
every   batches  and stopping at  
The results are presented in Fig    As can be clearly seen 
the  layer network achieves signi cantly better performance than the  layer networks  This is true even though
some of these networks are signi cantly larger and with
more parameters  for example  the  layer  width   network has    parameters  vs     parameters for the  
layer network  This gap in performance is the exact opposite of what might be expected based on parameter counting
alone  Moreover  increasing the width of the  layer networks exhibits diminishing returns  The performance improvement in doubling the width from   to   is much
larger than doubling the width from   to   or   to
  This indicates that one would need   much larger  
layer network to match the  layer  width   network  
performance  Thus  we conclude that the network   depth
indeed plays   crucial role  and that  layer networks are
inherently more suitable to express indicator functions of
the type we studied 

Figure   The experiment results  depicting the network   root
mean square error over the training set  top  and validation set
 bottom  as   function of the number of batches processed  Best
viewed in color 

     Radial Functions  ReLU Networks
Having considered functions depending on the    norm 
we now turn to consider functions depending on the   
norm  Focusing on ReLU networks  we will show   certain separation result holding for any nonlinear function 
which depends on the input   only via its  norm  cid   cid 
Theorem   Let        cid    be   function such that
for some          and        

Ex uniform on               ax           

inf
     

Then there exists   distribution   over       cid   cid       
    such that if    layer ReLU network       satis es

 cid 

 

    cid   cid             dx    

then its width must be at least  min  exp   
 where the   notation hides constants and factors logarithmic in     

The proof appears in the supplementary material  We note
that   controls how  linearly inapproximable  is   in   narrow interval  of width   around    and that   is generally dependent on   To give   concrete example  suppose
that                which cannot be approximated by
  linear function to an accuracy better than    in an  
  and        we
neighborhood of   By taking        
get that no  layer network can approximate the function

Batch number    RMSE  training set layer  width  layer  width  layer  width  layer  width  layer  width  Batch number    RMSE  validation set layer  width  layer  width  layer  width  layer  width  layer  width  DepthWidth Tradeoffs in Approximating Natural Functions with Neural Networks

ReLU network     cid   cid  

 cid   cid       at least with respect to some distribution  unless its width is  min  exp    On the  ip side 
   cid   cid  can be expressed exactly by    layer  width   
  xi     xi    where
the output neuron is simply the identity function  The same
argument would work for any piecewiselinear    More
generally  the same kind of argument would work for any
function   exhibiting   nonlinear behavior at some points 
Such functions can be wellapproximated by  layer networks  by approximating   with   piecewiselinear function  yet any approximating  layer network will have  
lower bound on its size as speci ed in the theorem 
Intuitively  the proof relies on showing that any good  
layer approximation of    cid   cid  must capture the non 
 cid  
linear behavior of   close to  most  points   satisfying
 cid   cid       However     layer ReLU network    cid 
   aj  cid wj    cid    bj  is piecewise linear  with nonlinearities only at the union of the   hyperplanes       
 cid wj    cid    bj     This implies that  most  points  
      cid   cid      must be  close to   hyperplane     
 cid wj    cid    bj     However  the geometry of the    ball
      cid   cid       is such that the   neighborhood of any single hyperplane can only cover    small  portion of that ball 
yet we need to cover most of the    ball  Using this and
an appropriate construction  we show that required number
of hyperplanes is at least   as long as     exp     
 and if   is smaller than that  we can simply use one neuron hyperplane for each of the    facets of the    ball  and
get   covering using    neurons hyperplanes  The formal
proof appears in the supplementary material 
We note that the bound in Thm    is of   weaker nature
than the bound in the previous section  in that the lower
bound is only polynomial rather than exponential  albeit
       different problem parameters    vs     Nevertheless 
we believe this does point out that    balls also pose   geometric dif culty for  layer networks  and conjecture that
our lower bound can be considerably improved  Indeed  at
the moment we do not know how to approximate   function
such as    cid   cid   cid      with  layer networks to better
than constant accuracy  using less than     neurons 
Finally  we performed an experiment similar to the one presented in Subsection   where we veri ed that the bounds
we derived are indeed re ected in differences in empirical performance  when training  layer nets versus  layer
nets  The reader is referred to Sec    for the full details of
the experiment and its results 

      Nonlinear Functions  ReLU Networks
In this section  we establish   depth separation result for approximating continuously twicedifferentiable      functions using ReLU neural networks  Unlike the previous re 

sults in this paper  the separation is for depths which can be
larger than   depending on the required approximation error  Also  the results will all be with respect to the uniform
distribution    over       As mentioned earlier  the results and techniques in this section are closely related to the
independent results of  Yarotsky    Liang   Srikant 
  but our emphasis is on    rather than    approximation bounds  and we focus on somewhat different network architectures and function classes 
Clearly  not all     functions are dif cult to approximate
        linear function can be expressed exactly with    
layer network  Instead  we consider functions which have
  certain degree of nonlinearity  in the sense that its Hessians are nonzero along some direction  on   signi cant
portion of the domain  Formally  we make the following
de nition 
De nition   Let    denote the uniform distribution on
      For   function              and some      
denote

        

  Sd             cid               

sup

         

where Sd          cid   cid      is the ddimensional unit
hypersphere  and   is the set of all connected and measurable subsets of      

In words         is the measure         the uniform distribution on       of the largest connected set in the domain
of    where at any point    has curvature at least   along
some  xed direction    The  prototypical  functions   we
are interested in is when      is lower bounded by   constant      
it is   if   is strongly convex  We stress that
our results in this section will hold equally well by considering the condition   cid              as well  however
for the sake of simplicity we focus on the former condition
appearing in Def    Our goal is to show   depth separation result inidividually for any such function  that is  for
any such function  there is   gap in the attainable error between deeper and shallower networks  even if the shallow
network is considerably larger 
As usual  we start with an inapproximability result  Speci 
cally  we prove the following lower bound on the attainable
approximation error of    using   ReLU neural network of
  given depth and width 
Theorem   For any     function               any
      and any function   on      expressible by   ReLU
network of depth   and maximal width    it holds that

 cid 

  

                   dx            

 

    

 

where       is   universal constant 

DepthWidth Tradeoffs in Approximating Natural Functions with Neural Networks

The theorem conveys   key tradeoff between depth and
width when approximating       function using ReLU networks  The error cannot decay faster than polynomially in
the width    yet the bound deteriorates exponentially in the
depth    As we show later on  this deterioration does not
stem from the looseness in the bound  For wellbehaved   
it is indeed possible to construct ReLU networks  where the
approximation error decays exponentially with depth 
The proof of Thm    appears in the supplementary material  and is based on   series of intermediate results  First 
we show that any strictly curved function  in   sense similar to De nition   cannot be wellapproximated in an
   sense by piecewise linear functions  unless the number of linear regions is large  To that end  we  rst establish some necessary tools based on Legendre polynomials  We then prove   result speci   to the onedimensional
case  including an explicit lower bound if the target function is quadratic  Thm    or strongly convex or concave
 Thm    We then expand the construction to get an error lower bound in general dimension    depending on the
number of linear regions in the approximating piecewiselinear function  Finally  we note that any ReLU network
induces   piecewiselinear function  and bound the number
of linear regions induced by   ReLU network of   given
width and depth  using   lemma borrowed from  Telgarsky    Combining this with the previous lower bound
yields Thm   
We now turn to complement this lower bound with an approximability result  showing that with more depth    wide
family of functions to which Thm    applies can be approximated with exponentially high accuracy  Speci cally 
we consider functions which can be approximated using  
moderate number of multiplications and additions  where
the values of intermediate computations are bounded  for
example    special case is any function approximable by  
moderatelysized Boolean circuit  or   polynomial 
The key result to show this is the following  which implies
that the multiplication of two  boundedsize  numbers can
be approximated by   ReLU network  with error decaying
exponentially with depth 
Theorem   Let                                   and
let       be arbitrary  Then exists   ReLU neural network

 cid cid      and depth cid  log cid   

  of width  cid log cid   

 cid cid     

 

 

satisfying

sup

          

                        

The idea of the construction is that depth allows us to compute highlyoscillating functions  which can extract highorder bits from the binary representation of the inputs 
Given these bits  one can compute the product by   procedure resembling long multiplication  as shown in Fig   

Figure   ReLU approximation of the function    cid     obtained
by extracting   bits  The number of linear segments grows exponentially with the number of bits and the approximating network
size 

and formally proven as follows 

       

 ixi    

Proof of Thm    We begin by observing that by using  
simple linear change of variables on    we may assume
without loss of generality that         as we can just
rescale   to the interval     and then map it back to its
original domain         where the error will multiply by
  factor of     Then by requiring accuracy  
   instead of
  the result will follow 
The key behind the proof is that performing bitwise operations on the  rst   bits of         yields an estimation
    ixi
be the binary representation of   where xi is the ith bit of
   then

of the product to accuracy  kM  Let      cid 

 cid 
  cid 
 cid cid cid cid cid   cid 
 cid cid cid cid cid   
 cid cid cid cid cid   cid 
Eq    implies cid cid cid cid cid           cid 
Requiring that  kM    
 cid  
     it suf ces to show the
existence of   network which approximates the function
    ixi     to accuracy  

 cid 
 cid cid cid cid cid              kM 
 cid cid cid cid cid     kM 
  where      cid log cid    

 ixi      

 ixi     

 ixi    

 ixi    

    

    

 cid cid 

 

  

 

  

      

But since

    

 

  

DepthWidth Tradeoffs in Approximating Natural Functions with Neural Networks

  resulting

This way both approximations will be at most  
in the desired accuracy of  
Before specifying the architecture which extracts the ith bit
of    we  rst describe the last   layers of the network  Let
the penultimate layer comprise of   neurons  each receiving both   and xi as input  and having the set of weights

 cid     cid  Thus  the output of the ith neuron in the

penultimate layer is

 cid iy   xi    cid 

     ixiy 

will be cid  

Let the  nal single output neuron have the set of weights
                Rk  this way  the output of the network

    ixi     as required 

We now specify the architecture which extracts the  rst
most signi cant   bits of    In Telgarsky   the author demonstrates how the composition of the function

                    

 

       

 cid   

 cid   

with itself   times      yields   highly oscillatory triangle
wave function in the domain     Furthermore  we observe that                  and thus                  
Now    linear shift of the input of    by     and composing the output with

 cid 
 cid   cid        cid cid    We stress
 cid  bits  which is also the magnitude

which converges to         as       results in an approximation of    cid  xi   
that choosing   such that the network approximates the bitwise product
  will require   to be of mag 
    but this poses no problem as representing such  
nitude  

number requires log cid   

     
 

to accuracy  

     
 

   
 

 

 
 

 

 

 

 

 

 cid 

 

of the size of the network  as suggested by the following
analysis 
Next  we compute the size of the network required to implement the above approximation  To compute   only two
neurons are required  therefore    can be computed using
  layers with   neurons in each  and  nally composing this
with   requires   subsequent layer with   more neurons 
To implement the ith bit extractor we therefore require  
network of size        Using dummy neurons to propagate the ith bit for        the architecture extracting the
  most signi cant bits of   will be of size            
Adding the  nal component performing the multiplication
estimation will require   more layers of width   and   respectively  and an increase of the width by   to propagate
  to the penultimate layer  resulting in   network of size
               

Thm    shows that multiplication can be performed very
accurately by deep networks  Moreover  additions can be

computed by ReLU networks exactly  using only   single layer with   neurons  Let         be arbitrary  then
        cid              is given in terms of ReLU summation
by

                               

Repeating these arguments  we see that any function which
can be approximated by   bounded number of operations
involving additions and multiplications  can also be approximated well by moderatelysized networks  This is
formalized in the following theorem  which provides an approximation error upper bound  in the    sense  which is
stronger than    for upper bounds 
Theorem   Let Ft    be the family of functions on the
domain      with the property that     Ft    is approximable to accuracy   with respect to the in nity norm 
using at most   operations involving weighted addition 
        cid                 where         are  xed  and multiplication          cid         where each intermediate computation stage is bounded in the interval         Then
there exists   universal constant    and   ReLU network  

of width and depth at most   cid   log cid   

 cid       log     cid  such

 

that

                  

sup

   

As discussed in Sec    this type of    approximation
bound implies an    approximation bound with respect
to any distribution  The proof of the theorem appears in
Sec    
Combining Thm    and Thm    we can state the following corollary  which formally shows how depth can be exponentially more valuable than width as   function of the
target accuracy  
Corollary   Suppose        Ft     where      
   poly  log   and          poly   Then
approximating   to accuracy   in the    norm using    xed
depth ReLU network requires width at least poly 
whereas there exists   ReLU network of depth and width at
most    log   which approximates   to accuracy   in
the in nity norm  where   is   polynomial depending solely
on   

Proof  The lower bound follows immediately from Thm   
For the upper bound  observe that Thm    implies an   approximation by   network of width and depth at most

 

    log          log     

 

 cid 

 cid 

which by the assumption of Corollary   can be bounded by
   log   for some polynomial   which depends solely
on   

DepthWidth Tradeoffs in Approximating Natural Functions with Neural Networks

Shaham  Uri  Cloninger  Alexander 

and Coifman 
Ronald    Provable approximation properties for deep
neural networks  Applied and Computational Harmonic
Analysis   

Telgarsky  Matus  Bene ts of depth in neural networks 

arXiv preprint arXiv   

Yarotsky  Dmitry  Error bounds for approximations with
deep relu networks  arXiv preprint arXiv 
 

Acknowledgements

This research is supported in part by an FP  Marie Curie
CIG grant  Israel Science Foundation grant   and
the Intel ICRICI Institute  We would like to thank Shai
ShalevShwartz for some illuminating discussions  and
Eran Amar for his valuable help with the experiments 

References
Bianchini     and Scarselli     On the complexity of shalIn ESANN 

low and deep neural network classi ers 
 

Cohen  Nadav  Sharir  Or  and Shashua  Amnon  On the
expressive power of deep learning    tensor analysis  In
 th Annual Conference on Learning Theory  pp   
   

Cybenko  George  Approximation by superpositions of  
sigmoidal function  Mathematics of control  signals and
systems     

Delalleau     and Bengio     Shallow vs  deep sumproduct

networks  In NIPS  pp     

Eldan  Ronen and Shamir  Ohad  The power of depth for
In  th Annual Confer 

feedforward neural networks 
ence on Learning Theory  pp     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition  arXiv
preprint arXiv   

Hornik  Kurt  Approximation capabilities of multilayer
feedforward networks  Neural networks   
 

Liang  Shiyu and Srikant     Why deep neural networks 

arXiv preprint arXiv   

Martens     and Medabalimi     On the expressive efarXiv preprint

 ciency of sum product networks 
arXiv   

Poggio  Tomaso  Mhaskar  Hrushikesh  Rosasco  Lorenzo 
Miranda  Brando  and Liao  Qianli 
Why and
when can deep but not shallow networks avoid the
arXiv preprint
curse of dimensionality 
arXiv   

  review 

Poole  Ben  Lahiri  Subhaneil  Raghu  Maithreyi  SohlDickstein  Jascha  and Ganguli  Surya  Exponential
expressivity in deep neural networks through transient
In Advances In Neural Information Processing
chaos 
Systems  pp     

