Tight Bounds for Approximate Carath odory and Beyond

Vahab Mirrokni     Renato Paes Leme     Adrian Vladu     Sam Chiuwai Wong    

Abstract

We present   deterministic nearlylinear time algorithm for approximating any point inside   convex
polytope with   sparse convex combination of the
polytope   vertices  Our result provides   constructive proof for the Approximate Carath odory
Problem  Barman    which states that any
point inside   polytope contained in the    ball
of radius   can be approximated to within   in

   norm by   convex combination of       
vertices of the polytope for       While for the
particular case of       this can be achieved by
the wellknown Perceptron algorithm  we follow  
more principled approach which generalizes to arbitrary       furthermore  this naturally extends
to domains with more complicated geometry  as it
is the case for providing an approximate Birkhoffvon Neumann decomposition  Secondly  we show
that the sparsity bound is tight for    norms  using an argument based on anticoncentration for
the binomial distribution  thus resolving an open
question posed by Barman  Experimentally  we
verify that our deterministic optimizationbased
algorithms achieve in practice much better sparsity than previously known samplingbased algorithms  We also show how to apply our techniques
to SVM training and rounding fractional points in
matroid and  ow polytopes 

  Introduction
The  exact  Carath odory Theorem is   fundamental
result in convex geometry which states that any point
  in   polytope     Rn can be expressed as   convex
combination of       vertices of     The approximate

 Equal contribution  Google Research  New York  NY  USA
 MIT  Cambridge  MA  USA  UC Berkeley  Berkeley  CA  USA 
Correspondence to  Vahab Mirrokni  mirrokni google com 
Renato
Adrian
Vladu  avladu mit edu  Sam Chiuwai Wong  samcwong berkeley edu 

 renatoppl google com 

Paes Leme

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

version states that if one is willing to tolerate an error of

  in    norm         vertices suf ce to approximate

   where   is the radius of the smallest    ball enclosing
    The key signi cance of the approximate Carath odory
Theorem is that the bound it provides is dimensionfree  and
consequently allows us to approximate any point inside the
polytope with   sparse convex combination of vertices 

The Approximate Carath odory Problem Given   polytope   contained inside the    ball of radius    and        
 nd vertices            vk of   such that           
and   
   vi          
kPk

The   version of this result is quite an old observation 
The earliest record is perhaps due to Novikoff   who
showed that the   version of Approximate Carath odory
can be obtained as   byproduct of the analyis of the Perceptron Algorithm  as pointed out by  Blum et al    The
fact that   sparse approximation can be obtained by   very
simple and ef cient algorithms found many applications in
Machine Learning  ShalevShwartz et al    use it to
minimize the loss of   linear predictor using   small number of features  Garber   Hazan   use it to speed up
conditional grandient methods 
The results described above focus on the   norm  The
interest for approximate Caratheodory in higher  pnorms
was sparked by   recent result of Barman   who used
it to improve algorithms for computing Nash equilibria in
game theory and algorithms for the kdensest subgraph in
combinatorial optimization  Another area where higher
norms are widely applied is in functional analysis where
the approximate Caratheodory Theorem is often referred as
Maurey   Lemma  Pisier   
Both Barman   proof and Maurey   original proof start from
    ivi of the exact Carath odory problem  interpret the coef cients    of the convex combination
as   probability distribution and generate   sparse solution
by sampling from the distribution induced by   Concentration inequalities are then applied to argue that the average
sampled solution is close to   in  pnorm  The proof is clean
and elegant  but is computationally expensive since it involves  rst computing   solution to the exact Carath odory

  solution    Pn 

Tight Bounds for Approximate Caratheodory and Beyond

          

  

 
log   

problem  which can take      even if the vertices are
given explicitly  The situation becomes even worse for
polytopes where it is not desirable to maintain an explicit
representation of all its vertices       the matching polytope 
since there may be exponentially many of them 
This is contrast with simple iterative solutions like the Perceptron for   which runs in nearlylinear time  The  rst
question we explore in this paper is how to obtain   deterministic nearlylinear time algorithm for higher    norms 
Our algorithm runs in        iterations  each of which
takes linear time 
Secondly  we resolve an open question posed by  Barman 
  who observed that the bound for the   bound was
tight and asked whether the    bound was also tight  Barman
gave           lower bound for       We resolve
the question by showing that the        is tight by
exhibiting   polytope   in the radiusD    ball and   point
  inside for which all convex combinations of       
vertices are more than  far from   in  pnorm 
Even though the dependence on   cannot be improved in
general  it can be greatly improved in   special case  If
  is far away from the boundary of          if the ball of
radius   around   is contained in     then there exists  
solution to the approximate Carath odory problem with

For the positive result  our technique involves writing approximate Carath odory as   convex minimization problem
and solving it by running Mirror Descent on   dual convex
function obtained via Sion   Theorem  Our technique is
inspired by the similarity with the problems of computing
Nash equilibria in games and solving packingcovering LPs 
When     log    our bound has the same sparsity as Lipton
and Young  Lipton   Young    and Plotkin  Shmoys
and Tardos  Plotkin et al   
The view of approximate Carath odory as solving   zerosum game also leads to our lowerbound  adapting   method
of Klein and Young  Klein   Young    for proving
conditional lower bounds on the running time for solving
positive LPs 
To show the potential of our technique  we note that   simple extension of our method implies   new algorithm for
SVM training  More speci cally  we obtain    convergence for arbitrary kernels  each iteration only requires
matrixvector operations involving the kernel matrix  so
we overcome the obstacle of having to explicitly store the
   reason to consider arbitrary    norms for       is that they
are particularly useful for inputs that are bounded in   and bene  
from extra structure such as ksparsity  Then  we can run the
algorithm for the  log   norm and obtain the desired   guarantee
using only   log    points  which is an improvement over the
  log    bound one could obtain by ignoring the sparsity 

kernel or compute its Cholesky factorization 
Finally  we show that our algorithm can also be obtained by
an instantiation of the FrankWolfe algorithm  One remarkable feature of our problem is that it connects three ways
in which sparsi cation has been done  via Mirror Descent
 or more commonly  via multiplicative weight update  as in
 Plotkin et al    Arora et al    Juditsky et al   
via FrankWolfe methods  Garber   Hazan    Jaggi 
  and by sampling  Lipton   Young    Lipton et al 
 

     

     

  Preliminaries
For     Rd  we de ne its  pnorm as kxkp  
 Pd
    xi    
for       and   norm by kxk   
maxi  xi  We note the    ball as              
Rd kxkp     
Given   norm      we de ne its dual norm     
as kyk   
maxx kxk      so that   lder   inequality holds with
equality        kyk  kxk  The dual norm of    norm
is    norm for  
We also denote the support of   by supp         xi    
  Approximate Carath odory problem
The  exact  Carath odory Theorem is   fundamental result in linear algebra which bounds the number of points
needed to describe   point in the convex hull of   set 
More precisely  given    nite set of points     Rd and
    conv       Px            Px             
there exist       points in            xd      such that
    conv            xd  On the plane  in particular  every
point in the interior of   convex polygon can be written as  
convex combination of three of its vertices 
The approximate version of Carath odory theorem bounds
the number of points needed to describe     conv    approximately  Formally  given   norm      an additive error
parameter   and   set of points     Bk      Rd  for
given     conv    we want   points            xk     such
that there exists      conv            xk  and ku          
  general result of this type is given by Maurey  Pisier 
  For    norm        Barman  Barman    showed
that         points suf ce  Notably  this bound is independent of the dimension of the ambient space 
Mirror Descent An overview is in the supplementary material  We simply state the guarantee here  Let   be  
strongly convex        norm     and   be its Fenchel dual 
For  Lipschitz convex                   norm     
Mirror Descent computes iterates with stepsize   as follows 

Tight Bounds for Approximate Caratheodory and Beyond

yt      zt   MD 
zt    zt rf  yt 
Let   ykx                           be the
Bregman divergence 
Theorem   In the setup described above with    
maxz     zkz  and      
then in    
  Pt rf  yt yt  
    iterations  it holds that  
            
  Nearly linear time deterministic algorithm
We present   nearly linear time deterministic algorithm
for the approximate Carath odory Problem  Barman  
original proof  Barman    involves solving the exact

pret   as   probability distribution over     iii  sample  
points from   according to   and   iv  argue by concentration bounds  Khintchine inequality to be precise  that the

Carath odory problem      write    Px        and interexpectation        
   xi       From an algorithkPk

mic point of view  this requires solving   linear program to
compute   and using randomness to sample xi  Our main
theorem shows that neither is necessary  There is   linear
time deterministic algorithm that doesn   require   solution
  to the exact problem 
Our algorithm is based on Mirror Descent  The idea is
to formulate the Carath odory problem as an optimization
problem  Inspired by early positive Linear Program solvers
     PlotkinShmoys Tardos  Plotkin et al    we convert this to   saddle point problem and solve its dual using
Mirror Descent  Using Mirror Descent guarantees   sparse
primal certi cate that would act as the desired convex combination 
Recall that we are given    nite set of points    
               vm       and     conv    Our goal
is to produce   sparse convex combination of the points in
  that is  close to   in  pnorm  Dropping the sparsity
constraint for now  we can formulate this problem as 

min

  kV     ukp

 PCARA 

where   is         matrix whose columns are the vectors            vm and        Rd Pi xi          
is the unit simplex  We refer to PCARA as the primal
Carath odory problem  By writing    norm as kxkp  
maxy kykq      for  
      PCARA is converted to
  saddle point problem 

     

min
  

max
      

          

 SCARA 

Sion   Theorem  Sion    is   generalization of von
Neumann   minimax theorem that allows us to swap

  

written as 

  min

the order of minimization and maximization for any
pair of compact convex sets  This leads to dual prob 

              max

lem  maxy     minx             which can be rey           DCARA 
Sparse solution by solving the dual  Since    
conv    there is   solution       such that         
So PCARA  and equivalent formulations SCARA and DCARA  have an optimal value of   Although the optimal
value is known  it still helps to optimize       since in the
process we obtain an  approximation in few iterations  If
each iteration updates only one coordinate of    then we will
obtain an approximate solution with sparsity equal to the
number of iterations  As we shall show  while the updates
of   are not sparse  the dual certi cate produced by Mirror
Descent will be 
To make this statement precise  consider the gradient
of    which is obtained by applying the envelope theorem  see  Afriat    rf               for    
arg maxx             This problem corresponds to
maximizing   linear function over the simplex  so the optimal solution is   corner of the simplex  In other words 
rf           vi where     arg maxi         We can
then use the Mirror Descent guarantee in Theorem   to
bound the norm of the average gradient  as formalized in
Theorem  
Remark   In fact   does not even have to be explicitly
given  All we need is to solve     arg maxi        
For explicitly given     this can be done in dn time by picking the best vertex  Sometimes  especially in combinatorial
optimization  we have   polytope  whose vertices are     represented by its constraints  Our result states that for these
alternate formulations  we can still obtain   sparse representation ef ciently if we can solve the linear optimization
problem over it fast 
Theorem   Consider    strongly convex function
             with respect to the  qnorm     
maxy        yk  and         Let     
          yT be the    rst iterates of the Mirror Descent algorithm  Theorem   with mirror map    minimizing
function   in DCARA  If rf  yt        vi    then

Proof  We consider the space          equipped with
the    norm  To apply the Mirror Descent framework  we
need  rst to show that the dual norm  the  pnorm  in this

 
 

   

TXt 

 

vi    

   

Tight Bounds for Approximate Caratheodory and Beyond

case  of the gradient is bounded  This is easy  since in
the approximate Carath odory problem  vi        so
krf    kp   ku   vikp   kukp   kvikp     So we can
take       in Theorem  
Since         maxx           and rf              
for     arg maxx             then         rf      
Also  since there exists    such that          one has that
                       Plugging those two facts in the
guarantee of Theorem   we get 

 
 

    yt    rf  yt   

 
 

 
 

   

TXt 

          

TXt 
rf  yt yt       
 
rf  yt 
TXt 
Taking the maximum over all          we get 
 
vi    
TXt 
TXt 
rf  yt 
      
TXt 

rf  yt  

     

  max

 

   

 
 

 
 

 
 

To complete the picture  we need to exhibit    strongly
convex function              with   small value of
    maxy        yk  and show that the gradient of
In
the Fenchel dual    can be computed ef ciently 
supplementary material we show that it suf ces to use
   We also discuss the form of the Fenchel
       
dual   and how to compute    We note that because
  is de ned in the ball      its Fenchel dual is different
from that of the function  
Proposition   The Fenchel dual of              
       

  can be computed explicitly 

  de ned in Rd 

  kyk 

  kyk 

  kyk 
       

 

  kzk 
kzkp    

 

if kzkp    
if kzkp    

 

 

Also               min kzkp  where     is   vector with  qnorm   such that        kzkp  This function can be explicitly computed as         sgn zi   
 zi     kzkp 
Theorem   Given   points            vn          Rd
with       and     conv            vn  there is   deterministic algorithm of running time   nd      that   outputs
  multiset vi          vi    for            such that
      

kPk
   vi    and ku    ukp    

  Improved bound when   is far from the boundary
If the point   that we are approximating is suf ciently far
from the boundary of the polytope     it is possible to make
recursive calls to the algorithm described in the previous
section  doubling the precision in each iteration  This allows
us to obtain   signi cantly better sparsity guarantee 
Theorem   Let   be   polytope contained inside the
unit    ball  and   point         If              then there
exists                 supported at         
 
     log  
coordinates such that Pi supp    xivi          
Corollary   If       satis es                
              
then there exists       supported on         
  coordinates such that
     log  
 Pi supp    xivi          

This highlights an interesting feature  namely that we can
achieve linear convergence via an adhoc method  even
though the dual formulation we are optimizing does not
immediately exhibit strong convexity  This is achieved via
iteratively rescaling the problem after solving to some  xed
accuracy depending on the parameter    The description of
the improved algorithm can be found in Section   of the
supplementary material   
Even more interestingly  the primal version of this problem 
which can be solved via the conditional gradient method
 see Section   does exhibit strong convexity  this can
then be used to provide   comparable guarantee  using  
purely primal method described in  Garber   Hazan   
We also mention  LacosteJulien   Jaggi    Shtern  
Beck    Pe   et al    which describe   similar
phenomenon occurring under various speci   assumptions
involving the domain  Such methods show up under the
name  accelerated FrankWolfe  The regimes in which they
work are however different from the one we are considering
here 

  Sparse solution via conditional gradient methods
The algorithm described in the previous section admits  
completely different analysis via conditional gradient methods  more precisely  via the FrankWolfe algorithm  Jaggi 
  Bubeck    The FrankWolfe method solves  
problem of the type minx         for    smooth convex
function   over   compact convex set   via successive calls
to   linear optimization oracle  that given   vector     Rd
returns xw   arg maxx        Formally  start with any
point        and de ne the following iteration 
yt   arg min

xt        xt        yt
 FW 
FrankWolfe guarantees that if    are suitably chosen
      
   being   popular choice  then    xt           

    rf  xt  

Tight Bounds for Approximate Caratheodory and Beyond

         for  smooth          some norm     and  
the radius of          the same norm 
  remarkable fact is that the algorithm that we obtain from
instantiating the FrankWolfe framework for our problem is
completely isomorphic to the mirror descent version  in the
sense that they produce the same set of vertices 
Theorem   For         kx   uk 
         and        
then for each    the vertex yt output by the FrankWolfe algorithm is the same vertex output by Mirror Descent described
in Theorem  

  Connection between Frank Wolfe and Mirror

Descent

Theorem   is an example of   setting where the same
algorithm can be obtained from   completely primal view 
through FrankWolfe and through   saddle point formulation  via Mirror Descent  The FrankWolfe approach is more
standard in optimization while the Mirror Descent approach
is standard in game theory and in  rstorder methods in Linear Programming  One might suspect that there is   deeper
connection between the two algorithms 
In what follows we point out   simple but somewhat surprising observation  FrankWolfe methods to minimize   over
  compact set   can be obtained by instantiating the Mirror
Descent framework for minimizing   dualized version of  
when the mirror map is the Fenchel dual of the objective
itself 
  similar connection between Mirror Descent and FrankWolfe methods for   different class of problems was shown
by Bach  Bach    We believe both observations are
facets of the same phenomenon 
In what follows we present   very short and clean argument
for why  in our speci   instance  the FrankWolfe and Mirror
Descent yield the same results 
By writing   as the dual of its Fenchel dual and applying
Sion   minmax theorem  we obtain 

        min

min
   

  max

     max
    min

    

   

          
          

De ne        minx              which is   concave
function over    By the envelope theorem 

rg          rf    where     arg min
   

   

The mirror descent iteration for maximizing   can be written as  xt    xt    trg zt  and zt      xt 
By choosing             we exactly recover FrankWolfe since  rg zt    yt   rf rf  xt    yt   xt 

so xt    xt    trg zt           xt    tyt where
yt   arg miny           arg miny   rf  xt   
While the proof of Bach is similar in spirit to ours  it assumes   different setup  First of all  both his and our proofs
work on the dual objective obtained via Sion   minmax
theorem  However  instead of directly using    as   mirror
map  Bach adds an extra stronglyconvex regularizer to his
objective  which he carries through as   proximal term  This
guarantees that the dual problem he solves is smooth  thus
achieving    convergence rate  Distinctively  the proof
we have shown above is applied directly on the dualized
objective with    as mirror map  and only achieves  pt
convergence rate  however  this rate is tight for the speci  
problem we are studying 

the error      
   vs  
tPt

  Experiments
We illustrate the performance of our algorithm in two numerical experiments  presented in the  gure below  We
ran both the original sampling algorithm  Barman   
Pisier     where vertices are sampled from an exact
convex combination  and our deterministic mirrordescent
based algorithm on   instances  Each of these instances
consisted of   vectors in    obtained by sampling  
      Gaussian matrix  then scaling each column by
the maximum    respectively   column norm  For each
instance we choose   convex combination of   and plot
when we sample vt at random
proportionally to the exact convex combination  blue plot 
and when we use the point vt output by the tth iteration of
Mirror Descent  red plot  We do it for both the   and  
norms  where in each case the input vectors are rescaled to
have unit  pnorm and the errors are measured with respect
to the    norm 
The plots from the   instances are overlapped  in order to
highlight how mirror descent performs systematically better
than random sampling 
One interesting observation is that mirror descent still performs better in practice despite the fact that rescaled Gaussian matrices are the worstcase instances for the problem 
as we show in the next section  in the sense that for those
families of instances  both the sampling and Mirror Descent algorithm are guaranteed to be optimal up to constant
factors 

  Lower bound
We showed that if   is         matrix whose columns
are contained in the unit    ball      then for any    
   there is         with  supp          such that
kV         kp     where supp         xi    

 

 

 
 

Tight Bounds for Approximate Caratheodory and Beyond

 

 

 

 

 
      error objective

 

 

 
 

 

 

 
      error objective

 

 

Figure   Quality of the solution       norm of the error   respectively   as   function of sparsity  Blue curves correspond to sampling 
red curves correspond to mirror descent  We overlapped the plots from   instances  in order to highlight that mirror descent performs
systematically better than random sampling  This is apparent in both cases  where one can see that the red curves approach zero faster
than the blue ones 

In this section we argue that no dimensionindependent
bound better then      is possible  This shows that the
sparsity bound in the approximate Carath odory theorem
is tight and improves Barman         lower bound
 Barman    Formally  we show that 
Theorem   There exists   constant   such that for every
      and          there exists       matrix   with
columns of unit    norm  and   point                 
such that for all        with sparsity  supp     Kp 
one has that kV      ukp      
In other words  even though   is   convex combination of
columns of     every  Kp sparse convex combination
of columns of   has distance at least   from   in  pnorm 
The full proof is in supplementary material 
Our lower bound incidentally implies that the optimal rate
of conditional gradient applied to   psmooth function is
  pR  this can be seen by considering the function exhibited in Theorem   and noticing that minimizing it via
conditional gradient to accuracy   requires  pR  iterations  since each iteration increases the number of nonzero
coordinates of the solution by at most   but  pR 
nonzeros are required  as shown by our lower bound for
approximate Carath odory 
Here we present   simple  constructive instance from which
we easily prove     lower bound  and sketch   tight

 In addition to this  lower bounds for the case       were
folklore  some proofs can be found in  Jaggi    Bubeck   
We point out that in this case  the simple proof from Section  
follows   very different approach from the classical ones 

    bound based on the probabilistic method 

    simple lower bound  
This relies on Sylvester   construction of Hadamard matrices  which are de ned for     that are powers of   The
construction is recursive as follows         and for every
  that is   power of  

     Hn Hn
Hn  Hn 

Proposition   The Sylvester matrix Hn de ned as above
is Hadamard  In other words  Hij     for all      and
        nI       its columns are mutually orthogonal 

Now we consider the polytope   formed by the convex hull
of the normalized columns of    One can easily check
that for the construction above the uniform combination of
columns is           where   is the vector of all    and
ei is the unit basis vector for the ith component  We show
that    is at distance greater than   from the convex hull of
any    columns of   
Theorem   Let Hn be as above and   be the convex
hull of the columns of             Let              

            Then any        satisfying   Hx          

has sparsity  supp     min    
  Tight lower bound    
We now establish   tight lower bound via   probabilistic
existence argument inspired by the construction of Klein

Tight Bounds for Approximate Caratheodory and Beyond

and Young  Klein   Young    The example used to
exhibit the lower bound is very simple  The proof of its
validity  however  is quite involved and requires   careful
probability analysis  We give an overview and provide
details in the supplementary material 

the formulation SCARA of

Overview  Recall
the
Carath odory problem as   saddle point problem described
in Section   If we translate all points such that       then
we can write the problem as 

min
  

max
      

     

which can be seen as   game between   player controlling  
and    The approximate Carath odory theorem states that if
the value of the game is   then the xplayer has   ksparse
strategy that guarantees   value of the game at most   for
        
For the lower bound  our goal is to design an instance of
this game with value   such that for all ksparse strategies
of the xplayer with     Cp  the yplayer can force the
game to have   value strictly larger than      
Probabilistic Construction  We de ne the matrix    
        where   is an       matrix with random  
entries       each entry of   is chosen at random from
    independently with probability   Note the the
   norm of the columns of   is equal to        as in Approximate Carath odory  We will show that the following
events happen with high probability 

  The center of the polytope de ned by the columns of

  is  close to                   

  For each set   of   coordinates  if   is restricted to
only    the yplayer can force the value of the game
to be at least   We prove so by exhibiting   strategy
for the yplayer such that     is at least   for all
coordinates in   

After bounding the probabilities of the events above  the

   posresult follows by taking the union bound over all  
sible subsets   of cardinality    This implies that with
nonzero probability  for the matrix constructed the yplayer
will always be able to force yT         regardless of what
    sparse strategy the xplayer chooses 

Approximate Birkhoffvon Neumann Decomposition
The classical Birkhoffvon Neumann Theorem states that
any      doubly stochastic matrix can be decomposed into
  convex combination of at most            permutation
matrices 
In  Farias et al    it was observed that such   decomposition can be used to recover   model for   probability
distribution described by  rst order marginal information 
furthermore  they showed that an approximate such decomposition can be recovered using   number of elements
that is only linear in   rather than quadratic  More precisely  given   doubly stochastic matrix    one can produce
  convex combination of      permutation matrices
           MT which approximates   within   in Frobenius

norm       kA  Pi   piMikF       similar result can

be rederived using  Garber   Hazan   
Within our framework  this is an immediate corollary  Indeed  in order to recover the result we can consider the
domain to be the   ball of radius pn  and the doublystochastic input be   convex combination of permutation
matrices  each of them being represented as   vector of norm
pn  Then  our algorithm recovers an approximate decomposition with the same guarantees  having sparsity     
Each call to the linear optimization oracle requires computing   minimumcost perfect bipartite matching  which
can be done in time        min pn     where   is
the number of nonzeros in the input  Lee   Sidford   
Cohen et al   
Furthermore  the bounds easily generalize to higher norms 
if instead we want to obtain   guarantee involving the
elementwise    norm of the error        our sparsity
becomes       

Fast rounding in polytopes with linear optimization oracles  The most direct application of our approach is to
ef ciently round   point in   polytope whenever it admits  
good linear optimization oracle  An obvious such instance
is the matroid polytope  Given an nelement matroid  
of rank   and   fractional point    inside its base polytope 
our algorithm produces   sparse distribution   over matroid
bases such that marginals are approximately preserved in
expectation  Speci cally  for         has   support of size
     
  and kEx           kp     furthermore  computing
  requires only   nr pp  calls to     independence

oracle  Another example is the  ow polytope 

 

  Applications
In the following  we discuss   number of applications of our
results and techniques  We brie   describe each of them
here and refer the reader to the supplementary material for
complete exposition 

Support vector machines  SVM  Training SVM can
also be formulated as minimizing   convex function  We
show that our technique of converting   problem to   saddle point formulation and solving the dual via Mirror Descent can be applied to the problem of training  SVMs 
This is based on   formulation introduced by Sch lkopf 

Tight Bounds for Approximate Caratheodory and Beyond

et al   Sch lkopf et al    Kitamura et al   Kitamura et al    show how SVMs can be trained using
Wolfe   algorithm  Replacing Wolfe   algorithm by Mirror Descent we obtain an  approximate solution in time

  max   

   kKk        where   is the kernel matrix 

This yields   constant number of iterations for polynomial
and RBF kernels whenever the empirical data belong to the
unit   ball  Our method does not need to explicitly store the
kernel matrix  since every iteration only requires   matrixvector multiplication  and the entries of the matrix can be
computed onthe   as they are needed  In the special case
of linear kernels  each iteration can be implemented in time
linear in input size  yielding   nearlylinear time algorithm
for linear SVM training 

Acknowledgements
AV was partially supported by NSF grants CCF 
and CCF  and an internship at Google Research
NYC 

References
Afriat  SN  Theory of maxima and the method of lagrange  SIAM

Journal on Applied Mathematics     

Arora  Sanjeev  Hazan  Elad  and Kale  Satyen  The multiplicative
weights update method    metaalgorithm and applications 
Theory of Computing      doi   toc 
      URL http dx doi org 
toc     

Bach  Francis    Duality between subgradient and conditional
gradient methods  SIAM Journal on Optimization   
    doi    URL http dx doi 
org 

Barman  Siddharth  Approximating nash equilibria and dense bipartite subgraphs via an approximate version of caratheodory  
theorem  In Proceedings of the FortySeventh Annual ACM
on Symposium on Theory of Computing  STOC   Portland  OR  USA  June     pp      doi 
  URL http doi acm org 
 

BenTal     and Nemirovski     Lectures on Modern Convex
Optimization  Analysis  Algorithms  and Engineering Applications  MPSSIAM Series on Optimization   
ISBN
  URL https books google com 
books id kCksvznHS oC 

Blum  Avrim  HarPeled  Sariel  and Raichel  Benjamin  Sparse
approximation via generating point sets 
In Proceedings of
the TwentySeven Annual ACMSIAM Symposium on Discrete
Algorithms  SIAM   

Bubeck    bastien  Theory of convex optimization for machine
learning  CoRR  abs    URL http arxiv 
org abs 

Cohen  Michael       adry  Aleksander  Sankowski  Piotr  and
Vladu  Adrian  Negativeweight shortest paths and unit capacity

minimum cost  ow in       log     time  In Proceedings of
the TwentyEighth Annual ACMSIAM Symposium on Discrete
Algorithms  pp    SIAM   

Fan  RongEn  Chang  KaiWei  Hsieh  ChoJui  Wang  XiangRui 
and Lin  ChihJen  LIBLINEAR    library for large linear
classi cation  Journal of Machine Learning Research   
    doi    URL http 
 doi acm org 

Farias  Vivek    Jagabathula  Srikanth  and Shah  Devavrat  Sparse
choice models  In Information Sciences and Systems  CISS 
   th Annual Conference on  pp    IEEE   

Garber  Dan and Hazan  Elad  Playing nonlinear games with
linear oracles  In  th Annual IEEE Symposium on Foundations
of Computer Science  FOCS     October    Berkeley  CA  USA  pp      doi   FOCS 
  URL http dx doi org FOCS 
 

Garber  Dan and Hazan  Elad  Faster rates for the frankwolfe
method over stronglyconvex sets  In ICML  pp     
Garber  Dan and Hazan  Elad    linearly convergent variant of the conditional gradient algorithm under strong convexity  with applications to online and stochastic optimization  SIAM Journal on Optimization     
doi    URL http dx doi org 
 

Jaggi  Martin  Convex optimization without projection steps 
CoRR  abs    URL http arxiv org 
abs 

Jaggi  Martin  Revisiting frankwolfe  Projectionfree sparse convex optimization  In Proceedings of the  th International Conference on Machine Learning  ICML   Atlanta  GA  USA 
  June   pp      URL http jmlr 
org proceedings papers   jaggi html 

Juditsky  Anatoli  Kilin Karzan  Fatma  and Nemirovski  Arkadi 
Randomized  rst order algorithms with applications to  
minimization  Math  Program      doi 
    URL http dx doi org 
   

Kakade  Sham    ShalevShwartz  Shai  and Tewari  Ambuj  Regularization techniques for learning with matrices    
Mach  Learn  Res    June  
ISSN  
  URL http dl acm org citation cfm id 
 

Kang  Donggu and Payor  James 

Flow rounding  CoRR 
abs    URL http arxiv org abs 
 

Kitamura  Masashi  Takeda  Akiko  and Iwata  Satoru  Exact
SVM training by wolfe   minimum norm point algorithm  In
IEEE International Workshop on Machine Learning for Signal
Processing  MLSP   Reims  France  September    
pp      doi   MLSP  URL http 
 dx doi org MLSP 

Klein  Philip    and Young  Neal    On the number of iterations
for dantzigwolfe optimization and packingcovering approximation algorithms  SIAM    Comput     
doi      URL http dx doi org 
   

Tight Bounds for Approximate Caratheodory and Beyond

Sch lkopf  Bernhard  Smola  Alexander    Williamson  Robert   
and Bartlett  Peter    New support vector algorithms  Neural Computation     
doi   
  URL http dx doi org 
 

ShalevShwartz  Shai  Online Learning  Theory  Algorithms  and
Applications  PhD thesis  The Hebrew University of Jerusalem 
July  

ShalevShwartz  Shai  Srebro  Nathan  and Zhang  Tong  Trading
accuracy for sparsity in optimization problems with sparsity
constraints  SIAM    on Optimization    August
  ISSN   doi    URL http 
 dx doi org 

ShalevShwartz  Shai  Singer  Yoram  Srebro  Nathan  and Cotter  Andrew  Pegasos  primal estimated subgradient solver
for SVM  Math  Program      doi   
    URL http dx doi org 
   

Shtern  Shimrit and Beck  Amir  Linearly convergent awaystep
conditional gradient for nonstrongly convex functions  Mathematical Programming  pp     

Sion  Maurice  On general minimax theorems  Pac     Math   
    ISSN   doi   pjm 

Tao     Topics in Random Matrix Theory  Graduate studies in mathematics  American Mathematical Soc  ISBN  
URL https books google com books id Hjq 
JHLNPT   

Wolff        aba 

  

Lectures on
Harmonic Analysis 
ISBN
  URL https books google com 
books id   jcHMvXuUC 

and Shubin    
Universi Series  AMS 

Zhu  Zeyuan Allen  Chen  Weizhu  Wang  Gang  Zhu  Chenguang 
and Chen  Zheng  Ppacksvm  Parallel primal gradient descent
kernel SVM  In ICDM   The Ninth IEEE International Conference on Data Mining  Miami  Florida  USA    December
  pp      doi   ICDM  URL
http dx doi org ICDM 

LacosteJulien  Simon and Jaggi  Martin  On the global linear
convergence of FrankWolfe optimization variants  In Cortes 
Corinna  Lawrence  Neil    Lee  Daniel    Sugiyama 
Masashi  and Garnett  Roman  eds  Advances in Neural
Information Processing Systems   Annual Conference on
Neural Information Processing Systems   December  
  Montreal  Quebec  Canada  pp      URL
http papers nips cc paper onthe 
globallinear convergenceof frankwolfe 
optimizationvariants 

Lee  Yin Tat and Sidford  Aaron  Path  nding methods for linear
programming  Solving linear programs in   prank  iterations
and faster algorithms for maximum  ow  In Foundations of
Computer Science  FOCS    IEEE  th Annual Symposium
on  pp    IEEE   

Lipton  Richard    and Young  Neal    Simple strategies for large
zerosum games with applications to complexity theory  In Proceedings of the TwentySixth Annual ACM Symposium on Theory of Computing    May   Montr al  Qu bec  Canada 
pp     
doi    URL
http doi acm org 

Lipton  Richard    Markakis  Evangelos  and Mehta  Aranyak 
Playing large games using simple strategies  In Proceedings
 th ACM Conference on Electronic Commerce  EC  San
Diego  California  USA  June     pp      doi 
  URL http doi acm org 
 

Nesterov     Introductory Lectures on Convex Optimization   
Basic Course  Applied Optimization  Springer    ISBN
  URL https books google com 
books id VyYLeml CgC 

Novikoff         On convergence proofs on perceptrons   

   

Paley    and Zygmund       note on analytic functions in the
unit circle  In Mathematical Proceedings of the Cambridge
Philosophical Society  volume   pp    Cambridge
University Press   

Pe    Javier  Rodr guez  Daniel  and Soheili  Negar  On the
von neumann and frankwolfe algorithms with away steps 
SIAM Journal on Optimization      doi   
    URL https doi org 
   

Pisier  Gilles  Remarques sur un   sultat non publi  de    Maurey 

  minaire Analyse fonctionnelle  dit  pp     

Plotkin  Serge    Shmoys  David    and Tardos   va  Fast approximation algorithms for fractional packing and covering
problems  In  nd Annual Symposium on Foundations of Computer Science  San Juan  Puerto Rico    October   pp 
    doi   SFCS  URL http 
 dx doi org SFCS 

Raghavan  Prabhakar and Thompson  Clark    Multiterminal
global routing    deterministic approximation scheme  Algorithmica      doi   BF  URL
http dx doi org BF 

