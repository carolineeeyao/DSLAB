Provable Alternating Gradient Descent for Nonnegative Matrix Factorization

with Strong Correlations

Yuanzhi Li   Yingyu Liang  

Abstract

Nonnegative matrix factorization is   basic tool
for decomposing data into the feature and weight
matrices under nonnegativity constraints  and in
practice is often solved in the alternating minimization framework  However 
it is unclear
whether such algorithms can recover the groundtruth feature matrix when the weights for different features are highly correlated  which is common in applications  This paper proposes   simple and natural alternating gradient descent based
algorithm  and shows that with   mild initialization it provably recovers the groundtruth in the
presence of strong correlations  In most interesting cases  the correlation can be in the same order
as the highest possible  Our analysis also reveals
its several favorable features including robustness to noise  We complement our theoretical
results with empirical studies on semisynthetic
datasets  demonstrating its advantage over several popular methods in recovering the groundtruth 

  Introduction
Nonnegative matrix factorization  NMF  is an important
tool in data analysis and is widely used in image processing  text mining  and hyperspectral imaging        Lee  
Seung    Blei et al    Yang   Leskovec   
Given   set of observations                        
the goal of NMF is to  nd   feature matrix    
               aD  and   nonnegative weight matrix    
                    such that        Ax    for any    or
    AX for short  The intuition of NMF is to write each
data point as   nonnegative combination of the features 

Authors

NJ 

USA 

listed in alphabetic order 
Princeton 

 Princeton University 
to 
Yuanzhi Li  yuanzhil cs princeton edu  Yingyu Liang
 yingyul cs princeton edu 

Correspondence

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

By doing so  one can avoid cancellation of different features and improve interpretability by thinking of each     
as    unnormalized  probability distribution over the features  It is also observed empirically that the nonnegativity
constraint on the coef cients can lead to better features and
improved downstream performance of the learned features 
Unlike the counterpart which factorizes     AX without assuming nonnegativity of    NMF is usually much
harder to solve  and can even by NPhard in the worse
case  Arora et al      This explains why  despite all
the practical success  NMF largely remains   mystery in
theory  Moreover  many of the theoretical results for NMF
were based on very technical tools such has algebraic geometry        Arora et al      or tensor decomposition      
 Anandkumar et al    which undermine
their applicability in practice  Arguably  the most widely
used algorithms for NMF use the alternative minimization
scheme  In each iteration  the algorithm alternatively keeps
  or   as  xed and tries to minimize some distance between   and AX  Algorithms in this framework  such
as multiplicative update  Lee   Seung    and alternative nonnegative least square  Kim   Park    usually
perform well on real world data  However  alternative minimization algorithms are usually notoriously dif cult to analyze  This problem is poorly understood  with only   few
provable guarantees known  Awasthi   Risteski    Li
et al    Most importantly  these results are only for
the case when the coordinates of the weights are from essentially independent distributions  while in practice they
are known to be correlated  for example  in correlated topic
models  Blei   Lafferty    As far as we know  there
exists no rigorous analysis of practical algorithms for the
case with strong correlations 
In this paper  we provide   theoretical analysis of   natural algorithm AND  Alternative Nonnegative gradient
Descent  that belongs to the practical framework  and show
that it probably recovers the groundtruth given   mild initialization  It works under general conditions on the feature
matrix and the weights  in particular  allowing strong correlations 
It also has multiple favorable features that are
unique to its success  We further complement our theoretical analysis by experiments on semisynthetic data  demon 

Provable Alternating Gradient Descent for Nonnegative Matrix Factorization with Strong Correlations

 Decode 

     

 Update                 

where   is   threshold parameter 

 

yz cid        zz cid cid 

 

 cid 

    

 cid 
 cid 
 cid    if      

strating that the algorithm converges faster to the groundtruth than several existing practical algorithms  and providing positive support for some of the unique features of our
algorithm  Our contributions are detailed below 

  Contributions

In this paper  we assume   generative model of the data
points  given the groundtruth feature matrix    In each
round  we are given          where   is sampled       
from some unknown distribution   and the goal is to recover the groundtruth feature matrix    We give an algorithm named AND that starts from   mild initialization
matrix and provably converges to    in polynomial time 
We also justify the convergence through   sequence of experiments  Our algorithm has the following favorable characteristics 

  SIMPLE GRADIENT DESCENT ALGORITHM

The algorithm AND runs in stages and keeps   working
matrix      in each stage  At the tth iteration in   stage 
after getting one sample    it performs the following 

     

 

otherwise 

    is the MoorePenrose pesudoinverse of    and
  is the update step size  The decode step aims at recovering the corresponding weight for the data point  and the
update step uses the decoded weight to update the feature
matrix  The  nal working matrix at one stage will be used
as the    in the next stage  See Algorithm   for the details 
At   high level  our update step to the feature matrix can be
thought of as   gradient descent version of alternative nonnegative least square  Kim   Park    which at each
iteration alternatively minimizes            cid     AZ cid 
 
by  xing   or    Our algorithm  instead of performing an complete minimization  performs only   stochastic gradient descent step on the feature matrix  To see
this  consider one data point   and consider minimizing
           cid     Az cid 
  with    xed  Then the gradient
of   is just              Az   cid  which is exactly the
update of our feature matrix in each iteration 
As to the decode step  when       our decoding can be
regarded as   oneshot approach minimizing  cid     AZ cid 

 

 We also consider the noisy case  see  

restricted to       Indeed  if for example projected gradient descent is used to minimize  cid     AZ cid 
    then the
projection step is exactly applying   to   with        
key ingredient of our algorithm is choosing   to be larger
than zero and then decreasing it  which allows us to outperform the standard algorithms 
Perhaps worth noting  our decoding only uses    Ideally  we would like to use       as the decoding matrix
in each iteration  However  such decoding method requires
computing the pseudoinverse of      at every step  which
is extremely slow 
Instead  we divide the algorithm into
stages and in each stage  we only use the starting matrix
in the decoding  thus the pseudoinverse only needs to be
computed once per stage and can be used across all iterations inside  We can show that our algorithm converges in
polylogarithmic many stages  thus gives us to   much better running time  These are made clear when we formally
present the algorithm in Section   and the theorems in Section   and  

  HANDLING STRONG CORRELATIONS

The most notable property of AND is that it can provably
deal with highly correlated distribution   on the weight   
meaning that the coordinates of   can have very strong
correlations with each other  This is important since such
correlated   naturally shows up in practice  For example 
when   document contains the topic  machine learning  it
is more likely to contain the topic  computer science  than
 geography   Blei   Lafferty   
Most of the previous theoretical approaches for analyzing alternating between decoding and encoding  such
as  Awasthi   Risteski    Li et al    Arora
et al    require the coordinates of   to be pairwiseindependent  or almost pairwiseindependent  meaning
  xixj      xi   xj  In this paper  we show that algorithm AND can recover    even when the coordinates
are highly correlated  As one implication of our result 
when the sparsity of   is    and each entry of   is in
    AND can recover    even if each   xixj   
 min   xi    xj  matching  up to constant  the
highest correlation possible  Moreover  we do not assume
any prior knowledge about the distribution   and the result
also extends to general sparsities as well 

  PSEUDOINVERSE DECODING

One of the feature of our algorithm is to use MoorePenrose
pesudoinverse in decoding 
Inverse decoding was also
used in  Li et al    Arora et al      However  their algorithms require carefully  nding an inverse
such that certain norm is minimized  which is not as ef 
cient as the vanilla MoorePenrose pesudoinverse  It was
also observed in  Arora et al    that MoorePenrose

Provable Alternating Gradient Descent for Nonnegative Matrix Factorization with Strong Correlations

pesudoinverse works equally well in practice  but the experiment was done only when        In this paper  we
show that MoorePenrose pesudoinverse also works well
when    cid     both theoretically and empirically 

  THRESHOLDING AT DIFFERENT  

Thresholding at   value       is   common trick used in
many algorithms  However  many of them still only consider    xed   throughout the entire algorithm  Our contribution is   new method of thresholding that  rst sets   to
be high  and gradually decreases   as the algorithm goes 
Our analysis naturally provides the explicit rate at which
we decrease   and shows that our algorithm  following this
scheme  can provably converge to the groundtruth    in
polynomial time  Moreover  we also provide experimental
support for these choices 

  ROBUSTNESS TO NOISE

We further show that the algorithm is robust to noise  In
particular  we consider the model             where  
is the noise  The algorithm can tolerate   general family of
noise with bounded moments  we present in the main body
the result for   simpli ed case with Gaussian noise and provide the general result in the appendix  The algorithm can
recover the groundtruth matrix up to   small blowup factor times the noise level in each example  when the groundtruth has   good condition number  This robustness is also
supported by our experiments 

  Related Work
Practical algorithms  Nonnegative matrix factorization
has   rich empirical history  starting with the practical algorithms of  Lee   Seung        It has been
widely used in applications and there exist various methods
for NMF        Kim   Park    Lee   Seung    Cichocki et al    Ding et al      However  they
do not have provable recovery guarantees 
Theoretical analysis  For theoretical analysis   Arora
et al      provided    xedparameter tractable algorithm for NMF using algebraic equations  They also provided matching hardness results  namely they show there is
no algorithm running in time  mW       unless there is  
subexponential running time algorithm for  SAT   Arora
et al      also studied NMF under separability assumptions about the features  and  Bhattacharyya et al   
studied NMF under related assumptions  The most related work is  Li et al    which analyzed an alternating minimization type algorithm  However  the result
only holds with strong assumptions about the distribution
of the weight    in particular  with the assumption that the
coordinates of   are independent 

Topic modeling  Topic modeling is   popular generative
model for text data  Blei et al    Blei    Usually  the model results in NMF type optimization problems
with  cid   cid      and   popular heuristic is variational inference  which can be regarded as alternating minimization in KLdivergence  Recently  there is   line of theoretical work analyzing tensor decomposition  Arora et al 
      Anandkumar et al    or combinatorial
methods  Awasthi   Risteski    These either need
strong structural assumptions on the wordtopic matrix   
or need to know the distribution of the weight    which is
usually infeasible in applications 

  Problem and De nitions
We use  cid   cid  to denote the  norm of   matrix     cid   cid 
is the  norm of   vector    We use      to denote the ith row and      to denote the ith column of   matrix   
 max   min    stands for the maximum  minimal 
singular value of    respectively  We consider   generative
model for nonnegative matrix factorization  where the data
  is generated from 

              RW  

where    is the groundtruth feature matrix  and   is   nonnegative random vector drawn from an unknown distribution   The goal is to recover the groundtruth    from
       samples of the observation   
Since the general nonnegative matrix factorization is NPhard  Arora et al      some assumptions on the distribution of   need to be made  In this paper  we would like to
allow distributions as general as possible  especially those
with strong correlations  Therefore  we introduce the following notion called            general correlation conditions  GCC  for the distribution of   
De nition    General Correlation Conditions  GCC  Let
      xx cid  denote the second moment matrix 
   cid   cid      and xi             
          
           
          
       cid         
     cid   
     

The  rst condition regularizes the sparsity of    The second condition regularizes each coordinate of xi so that
there is no xi being large too often  The third condition

 Section   considers the noisy case 
 Throughout this paper  the sparsity of   refers to the  cid  norm 
which is much weaker than the  cid  norm  the support sparsity  For
example  in LDA  the  cid  norm of   is always  

Provable Alternating Gradient Descent for Nonnegative Matrix Factorization with Strong Correlations

regularizes the maximum pairwise correlation between xi
and xj  The fourth condition always holds for       since
  xx cid  is   PSD matrix  Later we will assume this condition holds for some       to avoid degenerate cases  Note
that we put the weight     before   such that   de ned
in this way will be   positive constant in many interesting
examples discussed below 
To get   sense of what are the ranges of       and   given
sparsity    we consider the following most commonly studied nonnegative random variables 
Proposition    Examples of GCC 

  If   is chosen uniformly over ssparse random vectors
with     entries  then                   and
         
   

  If   is uniformly chosen from Dirichlet distribution
    then           and      

sD

with parameter       
with          
   

For these examples  the result in this paper shows that we
can recover    for aforementioned random variables   as
long as          In general  there is   wide range
of parameters             such that learning    is doable
with polynomially many samples of   and in polynomial
time 
However  just the GCC condition is not enough for recovering    We will also need   mild initialization 
De nition    cid initialization  The initial matrix    satis 
 es for some  cid       

               for some diagonal matrix   and

offdiagonal matrix   

   cid   cid     cid   cid      cid     
   

th column                  cid 

The condition means that the initialization is not too far
away from the groundtruth    For any         the ij cid   Ej        So the
condition means that each feature      has   large fraction of the groundtruth feature      and   small fraction
of the other features    can be regarded as the magnitude
of the component from the groundtruth in the initialization  while   can be regarded as the magnitude of the error
terms  In particular  when       and       we have
        The initialization allows   to be   constant
away from    and the error term   to be  cid   in our theorems
 cid  can be as large as   constant 
In practice  such an initialization is typically achieved by
setting the columns of    to reasonable  pure  data points
that contain one major feature and   small fraction of some
others        lda    Awasthi   Risteski   

for                   do

Algorithm   Alternating Nonnegative gradient Descent
 AND 
Input  Threshold values                       
         
  for                   do
 
 
 
 
 
            
  end for
Output           

               cid                cid       cid 

On getting sample      do 
         

 cid       cid 

end for

  Algorithm
The algorithm is formally describe in Algorithm   It runs
in   stages  and in the jth stage  uses the same threshold
   and the same matrix    for decoding  where    is
either the input initialization matrix or the working matrix
obtained at the end of the last stage  Each stage consists
of   iterations  and each iteration decodes one data point
and uses the decoded result to update the working matrix 
It can use   batch of data points instead of one data point 
and our analysis still holds 
By running in stages  we save most of the cost of computing     as our results show that only polylogarithmic
stages are needed  For the simple case where          
the algorithm can use the same threshold value      
for all stages  see Theorem   while for the general case 
it needs decreasing threshold values across the stages  see
Theorem   Our analysis provides the hint for setting the
threshold  see the discussion after Theorem   and Section   for how to set the threshold in practice 

  Result for   Simpli ed Case
In this section  we consider the following simpli ed case 

                  

 

That is  the weight coordinates xi   are binary 
Theorem    Main  binary  For the generative model  
there exists  cid      such that for every            
GCC   and every       Algorithm AND with    
poly     
   for    
polylog     
    and an  cid  initialization matrix    outputs  
matrix   such that there exists   diagonal matrix    cid   
   
with  cid       cid      using poly     
    samples and iterations  as long as

        
  

          

       

poly     

 

 cid  kD 

 cid 

     

 

  

Provable Alternating Gradient Descent for Nonnegative Matrix Factorization with Strong Correlations

Therefore  our algorithm recovers the groundtruth    up
to scaling  The scaling in unavoidable since there is no assumption on    so we cannot  for example  distinguish
   from     Indeed  if we in addition assume each column of    has norm   as typical in applications  then
we can recover    directly  In particular  by normalizing
each column of   to have norm   we can guarantee that
 cid       cid      
In many interesting applications  for example  those in
Proposition           are constants  The theorem implies
that the algorithm can recover    even when         
In this case    xixj  can be as large as      the same
order as min   xi    xj  which is the highest possible correlation 

  Intuition

The intuition comes from assuming that we have the  correct decoding  that is  suppose magically for every     
our decoding                          Here and in this
subsection    is   shorthand for    The gradient descent is then                                   cid 
Subtracting    on both side  we will get

                                    cid 

Since         cid  is positive semide nite  as long as
          cid   cid    and   is suf ciently small       will
converge to    eventually 
However  this simple argument does not work when    cid 
   and thus we do not have the correct decoding  For example  if we just let the decoding be                we will
have                          Ay                    
Thus  using this decoding  the algorithm can never make
any progress once   and    are in the same subspace 
The most important piece of our proof is to show that after thresholding                 is much closer to     
than       Since   and    are in the same subspace  inspired by  Li et al    we can write    as        
for   diagonal matrix   and an offdiagonal matrix    and
thus the decoding becomes                Ex   
Let us focus on one coordinate of      that is      
   
   ix   
    Eix    where Ei is the ith row of Ei  The
term    ix   
is   nice term since it is just   rescaling of
 
    
  while Eix    mixes different coordinates of      For
 
        and
simplicity  we just assume for now that     
         In our proof  we will show that the threshold will
remove   large fraction of Eix    when     
      and keep  
large fraction of    ix   
      Thus  our decoding is much more accurate than without thresholding  To
show this  we maintain   crucial property that for our decoding matrix  we always have  cid Ei cid       Assuming

  when     

this  we  rst consider two extreme cases of Ei 

  Ultra dense  all coordinates of Ei are in the order of
 
  Since the sparsity of      is    as long as    
 
 
   Eix    will not pass   and thus     
  will be
  
decoded to zero when     

     

  Ultra sparse  Ei only has few coordinate equal to  
and the rest are zero  Unless      has those exact coordinates equal to    which happens not so often  then
    
  will still be zero when     

     

Of course  the real Ei can be anywhere in between these
two extremes  and thus we need more delicate decoding
lemmas  as shown in the complete proof 

Furthermore  more complication arises when each     
is
not just in     but can take fractional values  To hani
dle this case  we will set our threshold   to be large at the
beginning and then keep shrinking after each stage  The intuition here is that we  rst decode the coordinates that we
are most con dent in  so we do not decode     
to be nonzero when     
      Thus  we will still be able to remove  
large fraction of error caused by Eix    However  by setting the threshold   so high  we may introduce more errors
to the nice term    ix   
as well  since    ix   
  might not
 
 cid    Our main contribution is to
be larger than   when     
 
show that there is   nice tradeoff between the errors in Ei
terms and those in      terms such that as we gradually decreases   the algorithm can converge to the groundtruth 

 

  Proof Sketch

For simplicity  we only focus on one stage and the expected
update  The expected update of      is given by

                 yz cid          zz cid 

Let us write              where   is diagonal
and    is offdiagonal  Then the decoding is given by

                    

Let     be the diagonal part and the offdiagonal part of
      
The key lemma for decoding says that under suitable conditions    will be close to    in the following sense 
Lemma    Decoding  informal  Suppose   is small and
       Then with   proper threshold value   we have

  xx cid      zx cid    xz cid      zz cid 

Now  let us write              Et  Then applying the
above decoding lemma  the expected update of      Et is
     Et        Et   Rt

Provable Alternating Gradient Descent for Nonnegative Matrix Factorization with Strong Correlations

where       xx cid  and Rt is   small error term 
Our second key lemma is about this update 
Lemma    Update  informal  Suppose the update rule is
      Et         Et               Rt

for some PSD matrix   and  cid Rt cid      cid cid  Then
 cid     Et     cid     cid           cid     min  

  cid cid 

 

 

 min 

Applying this on our update rule with       and
      we know that when the error term is suf 
ciently small  we can make progress on  cid    Et cid 
Furthermore  by using the fact that       and    is small 
and the fact that   is the diagonal part of        we
can show that after suf ciently many iterations   cid       cid 
blows up slightly  while  cid Et cid  is reduced signi cantly  Repeating this for multiple stages completes the proof 
We note that most technical details are hidden  especially
for the proofs of the decoding lemma  which need to show
that the error term Rt is small  This crucially relies on
the choice of   and relies on bounding the effect of the
correlation  These then give the setting of   and the bound
on the parameter   in the  nal theorem 

  More General Results
  Result for General  
This subsection considers the general case where    
      Then the GCC condition is not enough for recovery  even for              and       For example 
GCC does not rule out the case that   is drawn uniformly
over       sparse random vectors with    
      entries 
 cid 
when one cannot recover even   reasonable approximation
      shows up in all
of    since   common vector  
the samples  This example shows that the dif culty arises
if each xi constantly shows up with   small value  To avoid
this    general and natural way is to assume that each xi 
once being nonzero  has to take   large value with suf 
cient probability  This is formalized as follows 
De nition    Decay condition    distribution of   satis es
the orderq decay condition for some constant       if for
all         xi satis es that for every      
Pr xi       xi  cid         

 

When       each xi  once being nonzero  is uniformly
distributed in the interval     When   gets larger  each
xi  once being nonzero  will be more likely to take larger

values  We will show that our algorithm has   better guarantee for larger    In the extreme case when       xi will
only take     values  which reduces to the binary case 
In this paper  we show that this simple decay condition 
combined with the GCC conditions and an initialization
with constant error  is suf cient for recovering   
Theorem    Main  There exists  cid      such that for
every            GCC   satisfying the orderq condition 
every       there exists      and   sequence of      
such that Algorithm AND  with  cid initialization matrix   
outputs   matrix   such that there exists   diagonal matrix
   cid   
    samples
and iterations  as long as

    with  cid       cid      with poly     

 cid 

 

 cid 

     

     

 

kD   
    

  

   

As mentioned  in many interesting applications         
      where our algorithm can recover    as long as
          
     This means   xixj       
    
   
  factor of  
   away from the highest possible correlation min   xi    xj         Then  the larger   
the higher correlation it can tolerate  As   goes to in nity 
we recover the result for the case           allowing
the highest order correlation 
The analysis also shows that the decoding threshold should
   where    is the error matrix at the
be    
beginning of the stage  Since the error decreases exponentially with stages  this suggests to decrease   exponentially
with stages  This is crucial for AND to recover the groundtruth  see Section   for the experimental results 
  Robustness to Noise

 cid   cid   cid 

 cid   

 

We now consider the case when the data is generated from
            where   is the noise  For the sake of demonstration  we will just focus on the case when xi      

and   is random Gaussian noise        cid   

    cid     

more general theorem can be found in the appendix 
De nition    cid   initialization  The initial matrix   
satis es for some  cid         

                    for some diagonal matrix  

and offdiagonal matrix   
   cid   cid     cid   cid      cid     
Theorem    Noise  binary  Suppose each xi      
There exists  cid      such that for every            GCC
   every       Algorithm AND with     poly     
       

     cid   cid     

 In fact  we will make the choice explicit in the proof 
 we make this scaling so  cid cid     

Provable Alternating Gradient Descent for Nonnegative Matrix Factorization with Strong Correlations

 

          

        
   and an  cid   initialization
poly     
   for       min    outputs   such that there exists
  diagonal matrix    cid   

 
 cid 

    with

 cid       cid     

     

 max   
 min   

 cid 
 cid  kD 

 

  

 cid 

 

using poly     

    iterations  as long as      

The theorem implies that the algorithm can recover the
groundtruth up to    max   
 min    times   the noise level in
each sample  Although stated here for Gaussian noise for
simplicity  the analysis applies to   much larger class of
noises  including adversarial ones  In particular  we only
need to the noise   have suf ciently bounded  cid   cid cid 
see the appendix for the details  For the special case of
Gaussian noise  by exploiting its properties  it is possible
to improve the error term with   more careful calculation 
though not done here 
  Experiments
To demonstrate the advantage of AND  we complement the
theoretical analysis with empirical study on semisynthetic
datasets  where we have groundtruth feature matrices and
can thus verify the convergence  We then provide support
for the bene   of using decreasing thresholds  and test its
robustness to noise 
In the appendix  we further test its
robust to initialization and sparsity of    and provide qualitative results in some real world applications   
Setup  Our work focuses on convergence of the solution to the groundtruth feature matrix  However  realworld datasets in general do not have groundtruth  So we
construct semisynthetic datasets in topic modeling   rst
take the wordtopic matrix learned by some topic modeling method as the groundtruth    and then draw   from
some speci   distribution   For fair comparison  we use
one not learned by any algorithm evaluated here  In particular  we used the matrix with   topics computed by the
algorithm in  Arora et al    on the NIPS papers dataset
 about   documents  average length about   Based
on this we build two semisynthetic datasets 
  DIR  Construct         matrix    whose
columns are from   Dirichlet distribution with parameters               Then the dataset is
        

  CTM  The matrix   is of the same size as above 
while each column is drawn from the logistic normal
prior in the correlated topic model  Blei   Lafferty 
  This leads to   dataset with strong correlations 

 The

public
PrincetonML AND NMF 

code

is

on https github com 

Note that the wordtopic matrix is nonnegative  While
some competitor algorithms require   nonnegative feature
matrix  AND does not need such   condition  To demonstrate this  we generate the following synthetic data 

  NEG  The entries of the matrix    are        samples
from the uniform distribution on     The matrix   is the same as in CTM 

Finally  the following dataset is for testing the robustness
of AND to the noise 
  NOISE     and   are the same as in CTM  but    
        where   is the noise matrix with columns

    cid  with the noise level  

drawn from    cid   

Competitors  We compare the algorithm AND to the following popular methods  Alternating Nonnegative Least
Square  ANLS  Kim   Park    multiplicative update
 MU  Lee   Seung    LDA  online version  Hoffman
et al    and Hierarchical Alternating Least Square
 HALS  Cichocki et al   
Evaluation criterion  Given the output matrix   and the
ground truth matrix    the correlation error of the ith
column is given by

           min

      cid            cid 

Thus  the error measures how well the ith column of   
is covered by the best column of   up to scaling  We  nd
the best column since in some competitor algorithms  the
columns of the solution   may only correspond to   permutation of the columns of   
We also de ne the total correlation error as

  cid 

        

        

We report the total correlation error in all the experiments 

  

Initialization 
In all the experiments  the initialization
matrix    is set to               where   is the
identity matrix and   is   matrix whose entries are       
samples from the uniform distribution on    
Note that this is   very weak initialization  since       
  cid   Uj       and the magnitude of
  cid   Uj       can be larger than

    Ui        cid 
the noise component  cid 

the signal part     Ui       

 We use the implementation in the sklearn package  http 

 scikitlearn org 
columns of    without permutation 

 In the Algorithm AND  the columns of   correspond to the

Provable Alternating Gradient Descent for Nonnegative Matrix Factorization with Strong Correlations

    on DIR dataset

    on CTM dataset

    on NEG dataset

Figure   The performance of different algorithms on the three datasets  The xaxis is the running time  in seconds  the yaxis is the
logarithm of the total correlation error 

    different thresholds on DIR

    different thresholds on CTM

    robustness to noise

Figure   The performance of the algorithm AND with different thresholding schemes  and its robustness to noise  The xaxis is the
running time  in seconds  the yaxis is the logarithm of the total correlation error        Using different thresholding schemes on the
DIR CTM dataset   Decreasing thresold  refers to the scheme used in the original AND   Constant threshold    refers to using the
threshold value   throughout all iterations      The performance in the presence of noises of various levels 

Hyperparameters and Implementations  For most experiments of AND  we used       iterations for each
stage  and thresholds          For experiments
on the robustness to noise  we found       leads to
better performance  Furthermore  for all the experiments 
instead of using one data point at each step  we used the
whole dataset for update 
  Convergence to the GroundTruth

Figure   shows the convergence rate of the algorithms on
the three datasets  AND converges in linear rate on all three
datasets  note that the yaxis is in logscale  HALS converges on the DIR and CTM datasets  but the convergence
is in slower rates  Also  on CTM  the error oscillates  Furthermore  it doesn   converge on NEG where the groundtruth matrix has negative entries  ANLS converges on DIR
and CTM at   very slow speed due to the nonnegative least
square computation in each iteration    All the other algo 

rithms do not converge to the groundtruth  suggesting that
they do not have recovery guarantees 
  The Threshold Schemes

Figure     shows the results of using different thresholding
schemes on DIR  while Figure     shows that those on
CTM  When using   constant threshold for all iterations 
the error only decreases for the  rst few steps and then stop
decreasing  This aligns with our analysis and is in strong
contrast to the case with decreasing thresholds 
  Robustness to Noise
Figure     shows the performance of AND on the NOISE
dataset with various noise levels   The error drops at the
 rst few steps  but then stabilizes around   constant related
to the noise level  as predicted by our analysis  This shows
that it can recover the groundtruth to good accuracy  even
when the data have   signi cant amount of noise 

 We also note that even the thresholding of HALS and ALNS
designed for nonnegative feature matrices is removed  they still

do not converge on NEG 

 Time in seconds log Error ANDANLSMULDAHALS Time in seconds log Error ANDANLSMULDAHALS Time in seconds log Error ANDANLSMULDAHALS Time in seconds log Error Decreasing thresholdConstant threshold  Constant threshold  Time in seconds log Error Decreasing thresholdConstant threshold  Constant threshold  Time in seconds log Error noise level  noise level  noise level  noise level  noise level  noise level  Provable Alternating Gradient Descent for Nonnegative Matrix Factorization with Strong Correlations

Blei  David    Ng  Andrew    and Jordan  Michael    La 

tent dirichlet allocation  JMLR     

Cichocki  Andrzej  Zdunek  Rafal  and Amari  Shunichi 
Hierarchical als algorithms for nonnegative matrix and
   tensor factorization  In International Conference on
Independent Component Analysis and Signal Separation  pp    Springer   

Ding     Rohban       Ishwar     and Saligrama    
Topic discovery through data dependent and random
projections  arXiv preprint arXiv   

Ding     Rohban       Ishwar     and Saligrama     Ef 
 cient distributed topic modeling with provable guarantees  In AISTAT  pp     

Hoffman  Matthew  Bach  Francis    and Blei  David   
In adOnline learning for latent dirichlet allocation 
vances in neural information processing systems  pp 
   

Kim  Hyunsoo and Park  Haesun  Nonnegative matrix factorization based on alternating nonnegativity constrained
least squares and active set method  SIAM journal on
matrix analysis and applications     

Lee  Daniel   and Seung    Sebastian  Unsupervised
learning by convex and conic coding  NIPS  pp   
   

Lee  Daniel   and Seung    Sebastian  Learning the parts
of objects by nonnegative matrix factorization  Nature 
   

Lee  Daniel   and Seung    Sebastian  Algorithms for
nonnegative matrix factorization  In NIPS  pp   
 

Li  Yuanzhi  Liang  Yingyu  and Risteski  Andrej  Recovery guarantee of nonnegative matrix factorization via alternating updates  Advances in neural information processing systems   

Yang  Jaewon and Leskovec  Jure  Overlapping community
detection at scale    nonnegative matrix factorization approach  In Proceedings of the sixth ACM international
conference on Web search and data mining  pp   
ACM   

Acknowledgements
This work was supported in part by NSF grants CCF 
  DMS  Simons Investigator Award  Simons Collaboration Grant  and ONRN 
This work was done when Yingyu Liang was visiting the
Simons Institute 

References
Ldac software  https github com bleilab 
ldac blob master readme txt    Accessed   

Anandkumar     Kakade     Foster     Liu     and Hsu 
   Two svds suf ce  Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation 
Technical report   

Anandkumar     Hsu     Javanmard     and Kakade    
Learning latent bayesian networks and topic models under expansion constraints  In ICML   

Arora     Ge     and Moitra     Learning topic models  

going beyond svd  In FOCS     

Arora     Ge     Halpern     Mimno     Moitra    
Sontag     Wu     and Zhu       practical algorithm
for topic modeling with provable guarantees  In ICML 
 

Arora     Ge     Ma     and Moitra     Simple  ef cient 
and neural algorithms for sparse coding  In COLT   

Arora  Sanjeev  Ge  Rong  Kannan  Ravindran  and Moitra 
Ankur  Computing   nonnegative matrix factorization 
provably  In STOC  pp    ACM     

Arora  Sanjeev  Ge  Rong  Koehler  Frederic  Ma  Tengyu 
and Moitra  Ankur  Provable algorithms for inference in
topic models  In Proceedings of The  rd International
Conference on Machine Learning  pp     

Awasthi  Pranjal and Risteski  Andrej  On some provably
correct cases of variational inference for topic models 
In NIPS  pp     

Bhattacharyya  Chiranjib  Goyal  Navin  Kannan  Ravindran  and Pani  Jagdeep  Nonnegative matrix factorization under heavy noise  In Proceedings of the  nd
International Conference on Machine Learning   

Blei  David and Lafferty  John  Correlated topic models 
Advances in neural information processing systems   
   

Blei  David    Probabilistic topic models  Communica 

tions of the ACM   

