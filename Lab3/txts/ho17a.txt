Multilevel Clustering via Wasserstein Means

Nhat Ho   XuanLong Nguyen   Mikhail Yurochkin   Hung Hai Bui   Viet Huynh   Dinh Phung  

Abstract

We propose   novel approach to the problem of
multilevel clustering  which aims to simultaneously partition data in each group and discover
grouping patterns among groups in   potentially
large hierarchically structured corpus of data 
Our method involves   joint optimization formulation over several spaces of discrete probability
measures  which are endowed with Wasserstein
distance metrics  We propose   number of variants of this problem  which admit fast optimization algorithms  by exploiting the connection to
the problem of  nding Wasserstein barycenters 
Consistency properties are established for the estimates of both local and global clusters  Finally 
experiment results with both synthetic and real
data are presented to demonstrate the  exibility
and scalability of the proposed approach   

  Introduction
In numerous applications in engineering and sciences  data
are often organized in   multilevel structure  For instance 
  typical structural view of text data in machine learning
is to have words grouped into documents  documents are
grouped into corpora    prominent strand of modeling and
algorithmic works in the past couple decades has been to
discover latent multilevel structures from these hierarchically structured data  For speci   clustering tasks  one may
be interested in simultaneously partitioning the data in each
group  to obtain local clusters  and partitioning   collection
of data groups  to obtain global clusters  Another concrete
example is the problem of clustering images       global
clusters  where each image contains partions of multiple
annotated regions       local clusters   Oliva and Torralba 

 Department of Statistics  University of Michigan  USA 
 Adobe Research   Center for Pattern Recognition and Data Analytics  PRaDA  Deakin University  Australia  Correspondence
to  Nhat Ho  minhnhat umich edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   
is

https github com 

 Code

moonfolk MultilevelWasserstein Means

available

at

  While hierachical clustering techniques may be employed to  nd   treestructed clustering given   collection
of data points  they are not applicable to discovering the
nested structure of multilevel data  Bayesian hierarchical
models provide   powerful approach  exempli ed by in 
 uential works such as  Blei et al    Pritchard et al 
  Teh et al    More speci   to the simultaneous and multilevel clustering problem  we mention the paper of  Rodriguez et al    In this interesting work   
Bayesian nonparametric model  namely the nested Dirichlet process  NDP  model  was introduced that enables the
inference of clustering of   collection of probability distributions from which different groups of data are drawn 
With suitable extensions 
this modeling framework has
been further developed for simultaneous multilevel clustering  see for instance   Wulsin et al    Nguyen et al 
  Huynh et al   
The focus of this paper is on the multilevel clustering problem motivated in the aforementioned modeling works  but
we shall take   purely optimization approach  We aim to
formulate optimization problems that enable the discovery
of multilevel clustering structures hidden in grouped data 
Our technical approach is inspired by the role of optimal
transport distances in hierarchical modeling and clustering
problems  The optimal transport distances  also known as
Wasserstein distances  Villani    have been shown to
be the natural distance metric for the convergence theory
of latent mixing measures arising in both mixture models
 Nguyen    and hierarchical models  Nguyen   
They are also intimately connected to the problem of clustering   this relationship goes back at least to the work of
 Pollard    where it is pointed out that the wellknown
Kmeans clustering algorithm can be directly linked to the
quantization problem   the problem of determining an optimal  nite discrete probability measure that minimizes its
secondorder Wasserstein distance from the empirical distribution of given data  Graf and Luschgy   
If one is to perform simultaneous Kmeans clustering for
hierarchically grouped data  both at the global level  among
groups  and local level  within each group  then this can
be achieved by   joint optimization problem de ned with
suitable notions of Wasserstein distances inserted into the
objective function  In particular  multilevel clustering requires the optimization in the space of probability mea 

Multilevel Clustering via Wasserstein Means

 cid 

 cid 

  cid 

  

ui    

    Rk   ui    

ally  for any       the probability simplex is denoted
by     
  Finally  let
Ok   resp  Ek  be the set of probability measures
with at most  resp  exactly    support points in  
Wasserstein distances For any elements   and   cid  in
Pr  where       the Wasserstein distance of order  
between   and   cid  is de ned as  cf   Villani   
Wr      cid   

 cid       cid rd      

 cid  

 cid 

 

inf

     cid 

sures de ned in different levels of abstraction  including
the space of measures of measures on the space of grouped
data  Our goal  therefore  is to formulate this optimization
precisely  to develop algorithms for solving the optimization problem ef ciently  and to make sense of the obtained
solutions in terms of statistical consistency 
The algorithms that we propose address directly   multilevel clustering problem formulated from   purely optimization viewpoint  but they may also be taken as   fast
approximation to the inference of latent mixing measures
that arise in the nested Dirichlet process of  Rodriguez
et al    From   statistical viewpoint  we shall establish   consistency theory for our multilevel clustering
problem in the manner achieved for Kmeans clustering
 Pollard    From   computational viewpoint  quite
interestingly  we will be able to explicate and exploit the
connection betwen our optimization and that of  nding the
Wasserstein barycenter  Agueh and Carlier    an interesting computational problem that have also attracted much
recent interests        Cuturi and Doucet   
In summary  the main contributions offered in this work include       new optimization formulation to the multilevel
clustering problem using Wasserstein distances de ned on
different levels of the hierarchical data structure   ii  fast algorithms by exploiting the connection of our formulation to
the Wasserstein barycenter problem   iii  consistency theorems established for proposed estimates under very mild
condition of data   distributions   iv  several  exibile alternatives by introducing constraints that encourage the borrowing of strength among local and global clusters  and    
 nally  demonstration of ef ciency and  exibility of our
approach in   number of simulated and real data sets 
The paper is organized as follows  Section   provides preliminary background on Wasserstein distance  Wasserstein
barycenter  and the connection between Kmeans clustering and the quantization problem  Section   presents several optimization formulations of the multilevel clusering
problem  and the algorithms for solving them  Section  
establishes consistency results of the estimators introduced
in Section   Section   presents careful simulation studies
with both synthetic and real data  Finally  we conclude the
paper with   discussion in Section   Additional technical
details  including all proofs  are given in the Supplement 

  Background
For any given subset     Rd  let    denote the space of
Borel probability measures on   The Wasserstein space
of order       of probability measures on   is de 
 ned as Pr   
 
where  cid cid  denotes Euclidean metric in Rd  Addition 

 cid   cid rdG       

        

 cid 

 cid 

 

 

where       cid  is the set of all probability measures on
  that have marginals   and   cid  In words     
        cid 
is the optimal cost of moving mass from   to   cid  where
the cost of moving unit mass is proportional to rpower of
Euclidean distance in   When   and   cid  are two discrete
measures with  nite number of atoms  fast computation of
Wr      cid  can be achieved  see        Cuturi    The
details of this are deferred to the Supplement 
By   recursion of concepts  we can speak of measures of
measures  and de ne   suitable distance metric on this abstract space  the space of Borel measures on Pr  to be
denoted by Pr Pr  This is also   Polish space  that
is  complete and separable metric space  as Pr  is  
Polish space  It will be endowed with   Wasserstein metric of order   that is induced by   metric Wr on Pr 
for any
as follows  cf  Section   of  Nguyen   
    cid    Pr Pr 
Wr     cid   

        cid        cid 

 cid  

 cid 

   

 

inf

Pr 

where the in mum in the above ranges over all    
     cid  such that      cid  is the set of all probability
measures on Pr    Pr  that has marginals   and   cid 
In words  Wr     cid  corresponds to the optimal cost of
moving mass from   to   cid  where the cost of moving unit
mass in its space of support Pr  is proportional to the
rpower of the Wr distance in Pr  Note   slight notational abuse   Wr is used for both Pr  and Pr Pr 
but it should be clear which one is being used from context 

Wasserstein barycenter Next  we present   brief
overview of Wasserstein barycenter problem   rst studied
by  Agueh and Carlier    and subsequentially many
others        Benamou et al    Solomon et al   
 Alvarez Estebana et al    Given probability measures               PN      for       their Wasserstein barycenter      is such that

       arg min
    

 iW  

      Pi 

 

  cid 

  

Multilevel Clustering via Wasserstein Means

  cid 

where        denote weights associated with
           PN   When            PN are discrete measures with
 nite number of atoms and the weights   are uniform  it
was shown by  Anderes et al    that the problem of
 nding Wasserstein barycenter      over the space   
in   is reduced to search only over   much simpler space
Ol  where    
si         and si is the number of
components of Pi for all            Ef cient algorithms
for  nding local solutions of the Wasserstein barycenter
problem over Ok  for some       have been studied
recently in  Cuturi and Doucet    These algorithms
will prove to be   useful building block for our method as
we shall describe in the sequel  The notion of Wasserstein
barycenter has been utilized for approximate Bayesian inference  Srivastava et al   

  

Kmeans as quantization problem The wellknown Kmeans clustering algorithm can be viewed as solving an
optimization problem that arises in the problem of quantization    simple but very useful connection  Pollard   
Graf and Luschgy    The connection is the following  Given   unlabelled samples            Yn     Assume
that these data are associated with at most   clusters where
      is some given number  The Kmeans problem  nds
the set   containing at most   elements                  that
minimizes the following objective

  Yi    

 

  cid 

  

inf

     

 
 

  cid 

 
 

  

Let Pn  
 Yi be the empirical measure of data
           Yn  Then  problem   is equivalent to  nding  
discrete probability measure   which has  nite number of
support points and solves 

inf

  Ok 

   

      Pn 

 

Due to the inclusion of Wasserstein metric in its formulation  we call this   Wasserstein means problem  This problem can be further thought of as   Wasserstein barycenter
problem where      
In light of this observation  as
noted by  Cuturi and Doucet    the algorithm for  nding the Wasserstein barycenter offers an alternative for the
popular Loyd   algorithm for determing local minimum of
the Kmeans objective 

  Clustering with multilevel structure data
Given   groups of nj exchangeable data points Xj   where
                   nj       data are presented in  
twolevel grouping structure  our goal is to learn about the
twolevel clustering structure of the data  We want to obtain simultaneously local clusters for each data group  and
global clusters among all groups 

nj cid 

  Multilevel Wasserstein Means  MWM  Algorithm

  

 

 
nj

For any                  we denote the empirical measure
for group   by    
 Xj    Throughout this secnj
tion  for simplicity of exposition we assume that the number of both local and global clusters are either known or
bounded above by   given number  In particular  for local
clustering we allow group   to have at most kj clusters for
                 For global clustering  we assume to have  
group  Wasserstein  means among the   given groups 

   Gj     
nj

High level
idea For local clustering  for each    
             performing   Kmeans clustering for group    as
expressed by   can be viewed as  nding    nite discrete
measure Gj   Okj   that minimizes squared Wasserstein distance    
  For global clustering  we are
interested in obtaining clusters out of   groups  each of
which is now represented by the discrete measure Gj  for
                 Adopting again the viewpoint of Eq   
provided that all of Gjs are given  we can apply Kmeans
quantization method to  nd their distributional clusters 
The global clustering in the space of measures of measures
on   can be succintly expressed by

 cid 

 cid 

 Gj

 

  cid 

  

inf

  EM    

   
 

  

 
 

However  Gj are not known   they have to be optimized
through local clustering in each data group 

MWM problem formulation We have arrived at an objective function for jointly optimizing over both local and
global clusters

   

   Gj     
nj

       

     

 
 

 Gj  

 

  cid 

  

inf

Gj Okj  
  EM    

  cid 

  

 
 

  cid 

   Gj     
nj

We call the above optimization the problem of Multilevel Wasserstein Means  MWM  The notable feature of
MWM is that its loss function consists of two types of distances associated with the hierarchical data structure  one
 
is distance in the space of measures          
and the other in space of measures of measures      
     
 Gj   By adopting Kmeans optimization to
   
both local and global clustering  the multilevel Wasserstein
means problem might look formidable at the  rst sight 
Fortunately  it is possible to simplify this original formulation substantially  by exploiting the structure of   
Indeed  we can show that formulation   is equivalent
to the following optimization problem  which looks much
simpler as it involves only measures on  

  

inf

Gj Okj   

   

   Gj     
nj

   

  
  

 Gj    
 

 

  cid 

  

  

   

      

  min
    

      Hi  and    
where   
            HM   with each Hi      The proof of this
equivalence is deferred to Proposition    in the Supplement  Before going into to the details of the algorithm for
solving   in Section   we shall present some simpler cases  which help to illustrate some properties of the
optimal solutions of   while providing insights of subsequent developments of the MWM formulation  Readers
may proceed directly to Section   for the description of
the algorithm in the  rst reading 

  PROPERTIES OF MWM IN SPECIAL CASES
Example   Suppose kj     and nj     for all        
   and       Write            Under this
setting  the objective function   can be rewritten as

  

  

    
where Gj      for any            From the result of
Theorem    in the Supplement 

  cid 

  cid 

inf
   

  cid 

  

inf
   

           
   

 

 cid     Xj   cid       

           

 

max

max
     

     

      

  

 cid 

Multilevel Clustering via Wasserstein Means

  cid 

 cid Xi     Xj   cid 

   

      

      

     min

 

 
 

  

   cid           cid 

      

      

where   in the above sum varies over all the permutation of                and the second inequality is due to
CauchySchwarz   inequality 
It implies that as long as
   is small  the optimal solution Gi and Gj of  
   
will be suf ciently close to each other  By letting      
we also achieve the same conclusion regarding the asymptotic behavior of Gi and Gj with respect to            
Example   kj     and nj     for all           and
      Write            Moreover  assume that
there is   strict subset   of                such that

 cid 

max
    Ac

     

      
  

 cid  min

      Ac

     

      

  

  and    

  when
     the distances of empirical measures    
  and   belong to the same set   or Ac are much less than
those when   and   do not belong to the same set  Under
this condition  by using the argument from part     we can
write the objective function   as
 cid     Xj   cid   

         

   

 

   

  cid 
 cid 
 cid 
  cid 

   

  

  Ac

  

inf
   

    

inf
   

    

 cid     Xj   cid   

   

         
 Ac 

 

  and    

The above objective function suggests that the optimal solutions         equivalently  Gi and Gj  will not be close to
each other as long as   and   do not belong to the same set
  or Ac          
  are very far  Therefore  the two
groups of  local  measures Gj do not share atoms under
that setting of empirical measures 
The examples examined above indicate that the MWM
problem in general do not  encourage  the local measures
Gj to share atoms among each other in its solution  Additionally  when the empirical measures of local groups are
very close  it may also suggest that they belong to the same
cluster and the distances among optimal local measures Gj
can be very small 

  ALGORITHM DESCRIPTION

Now we are ready to describe our algorithm in the general
case  This is   procedure for  nding   local minimum of
Problem   and is summarized in Algorithm   We prepare the following details regarding the initialization and
updating steps required by the algorithm 

  cid 
  cid 

  

inf

    

  cid 

 cid      

  

  

   

   Gj    

     cid 
  cid 

     

  

 

where second in mum is achieved when      
 

Thus  objective function   may be rewritten as

 cid     Xj   cid     cid        

   cid   

  cid 

  

  cid 

  cid 

  

  

inf
   

  cid 

  

   

 cid 

 cid 

 cid 

Xj     for all            As    
Write        
  we can check that the unique optimal solutions for the
above optimization problem are     

             
          for any            If we further
  cid  
assume that our data Xj   are       samples from probability
measure     having mean      EX         for any    
       the previous result implies that     cid     for almost
surely as long as     cid      As   consequence  if    are
pairwise different  the multilevel Wasserstein means under
that simple scenario of   will not have identical centers
among local groups 
On the other hand  we have    

 cid cid           cid  Now  from the de nition of

   Gi  Gj     cid        cid   

 cid  mn

mn    

Wasserstein distance

Multilevel Clustering via Wasserstein Means

Algorithm   Multilevel Wasserstein Means  MWM 

Input  Data Xj    Parameters kj    
Output  prob  measures Gj and elements Hi of   
Initialize measures   
while      

 
have not converged do

  elements    

      

         

 

 

of          

for           

       

         
   

   

   Gj     
nj

 

for           

   

       

 

       

   

 
and     
 

 

  Update      
for       to   do
ij   arg min
   
    
    arg min
    
Gj Okj  
   

 

   Gj       
ij

    
end for
  Update      
for       to   do
ij   arg min
    

end for
for       to   do

Ci        il      for           
     

   

  arg min
Hi   

  Ci

 

   Hi      

 

 

 cid 

end for
           

end while

 

  The initialization of local measures   

      the initialization of their atoms and weights  can be obtained
by performing Kmeans clustering on local data Xj  
for            The initialization of elements    
of
    is based on   simple extension of the Kmeans
algorithm  Details are given in Algorithm   in the
Supplement 

 

  The updates     

 

can be computed ef ciently by
simply using algorithms from  Cuturi and Doucet 
  to search for local solutions of these barycenter problems within the space Okj   from the atoms
and weights of     
   

  Since all     

 

are  nite discrete measures   nding
over the whole space   
the updates for      
can be reduced to searching for   local solution within
     Ci 

space Ol    where         cid 

 supp     

 

 

  Ci

 

from the global atoms      
of        Justi cation of
this reduction is derived from Theorem    in the Supplement  This again can be done by utilizing algorithms from  Cuturi and Doucet    Note that  as
     becomes very large when   is large  to speed up
the computation of Algorithm   we impose   threshold               for      in its implementation 

The following guarantee for Algorithm   can be established 
Theorem   Algorithm   monotonically decreases the
objective function   of the MWM formulation 

  Multilevel Wasserstein Means with Sharing

As we have observed from the analysis of several speci  
cases  the multilevel Waserstein means formulation may
not encourage the sharing components locally among  
groups in its solution  However  enforced sharing has been
demonstrated to be   very useful technique  which leads
to the  borrowing of strength  among different parts of
the model  consequentially improving the inferential ef 
ciency  Teh et al    Nguyen    In this section 
we seek to encourage the borrowing of strength among
groups by imposing additional constraints on the atoms of
           Gm in the original MWM formulation   Denote
Gj   OK      EM       supp Gj   
AM SK  
SK          
for any given          where the
constraint set SK has exactly   elements  To simplify the
exposition  let us assume that kj     for all           
Consider the following locally constrained version of the
multilevel Wasserstein means problem

 cid 

 cid 

inf

   

   Gj     
nj

       

     

 
 

 Gj  

 

  cid 

  

  cid 

  

where SK  Gj     AM SK in the above in mum  We call
the above optimization the problem of Multilevel Wasserstein Means with Sharing  MWMS  The local constraint
assumption supp Gj    SK had been utilized previously
in the literature   see for example the work of  Kulis and
Jordan    who developed an optimizationbased approach to the inference of the HDP  Teh et al    which
also encourages explicitly the sharing of local group means
among local clusters  Now  we can rewrite objective function   as follows

inf

SK  Gj    BM SK

   

   Gj     
nj

   

  
  

 Gj    
 

 

 cid 

Gj   OK                  HM    

where BM SK  
supp Gj    SK          
  The high level idea
of  nding local minimums of objective function   is to
 rst  update the elements of constraint set SK to provide the
supports for local measures Gj and then  obtain the weights
of these measures as well as the elements of global set  
by computing appropriate Wasserstein barycenters  Due to
space constraint  the details of these steps of the MWMS
Algorithm  Algorithm   are deferred to the Supplement 

  cid 
 cid 

  

Multilevel Clustering via Wasserstein Means

  Consistency results
We proceed to establish consistency for the estimators introduced in the previous section  For the brevity of the
presentation  we only focus on the MWM method  consistency for MWMS can be obtained in   similar fashion 
Fix    and assume that     is the true distribution of data
Xj   for                  Write                 Gm  and
                nm  We say       if nj     for
                 De ne the following functions

fn       

   

   Gj     
nj

       

     

         

   

   Gj            

     

  cid 
  cid 

  

  

  cid 
  cid 

  

  

 
 

 
 

 Gj  

 Gj  

where Gj   Okj       EM     as            The
 rst consistency property of the WMW formulation 
Theorem   Given that          for           
Then  there holds almost surely  as      

fn       

inf

Gj Okj  
  EM    

inf

Gj Okj  
  EM    

           

  cid 

  

 

           cid Gnm

The next theorem establishes that the  true  global and loeach   there is an optimal solution  cid Gn 
cal clusters can be recovered  To this end  assume that for
in short  cid  
 Hn  of the objective function   Moreover 
there exist    not necessarily unique  optimal solution minimizing         over Gj   Okj   and     EM    
Let   be the collection of such optimal solutions  For any
Gj   Okj   and     EM     de ne

     cid Hn  or

          

inf

      

   

   Gj    

         

       

Given the above assumptions  we have the following result

regarding the convergence of  cid  
for all            Then  we have   cid  

Theorem   Assume that   is bounded and         
      almost surely 

   cid Hn        as

 Hn 

 

 

Remark 
    The assumption   is bounded is just for
the convenience of proof argument  We believe that the
conclusion of this theorem may still hold when     Rd 
 ii  If              there exists an unique optimal solution      minimizing         over Gj   Okj   and
  cid Gnj
    EM     the result of Theorem   implies that
      
as      

        for           and   cid Hn       

  Empirical studies
  Synthetic data

In this section  we are interested in evaluating the effectiveness of both MWM and MWMS clustering algorithms by
considering different synthetic data generating processes 
Unless otherwise speci ed  we set the number of groups
      number of observations per group nj     in
      dimensions  number of global clusters      
with   atoms  For Algorithm    MWM  local measures
Gj have   atoms each  for Algorithm    MWMS  number of atoms in constraint set SK is   As   benchmark
for the comparison we will use   basic  stage Kmeans
approach  the details of which can be found in the Supplement  The Wasserstein distance between the estimated
distributions                    Gm               HM   and the data
generating ones will be used as the comparison metric 
Recall that the MWM formulation does not impose constraints on the atoms of Gi  while the MWMS formulation
explicitly enforces the sharing of atoms across these measures  We used multiple layers of mixtures while adding
Gaussian noise at each layer to generate global and local
clusters and the noconstraint  NC  data  We varied number of groups   from   to   We notice that the
 stage Kmeans algorithm performs the best when there is
no constraint structure and variance is constant across clusters  Fig      and       this is  not surprisingly    favorable setting for the basic Kmeans method  As soon as we
depart from the  unrealistic  constantvariance  nosharing
assumption  both of our algorithms start to outperform the
basic threestage Kmeans  The superior performance is
most pronounced with localconstraint  LC  data  with or
without constant variance conditions  See Fig        It is
worth noting that even when group variances are constant 
the  stage Kmeans is no longer longer effective because
now fails to account for the shared structure  When      
and group sizes are larger  we set SK     Results are reported in Fig            These results demonstrate the
effectiveness and  exibility of our both algorithms 

  Real data analysis

We applied our multilevel clustering algorithms to two realworld datasets  LabelMe and StudentLife 
LabelMe dataset consists of     annotated images
which are classi ed into   scene categories including tall
buildings  inside city  street  highway  coast  open country  mountain  and forest  Oliva and Torralba      Each
image contains multiple annotated regions  Each region 
which is annotated by users  represents an object in the
image  As shown in Figure   the left image is an image
from open country category and contains   regions while
the right panel denotes an image of tall buildings category

Multilevel Clustering via Wasserstein Means

Figure   Data with   lot of small groups      NC data with constant variance      NC data with nonconstant variance     
LC data with constant variance      LC data with nonconstant variance

Figure   Data with few big groups      NC data with constant variance      NC data with nonconstant variance      LC
data with constant variance      LC data with nonconstant variance

Table   Clustering performance for LabelMe dataset 

Methods
Kmeans
TSKmeans
MC 
MWM
MWMS

NMI
 
 
 
 
 

ARI
 
 
 
 
 

AMI
 
 
 
 
 

Time    

 
 
 
 
 

Figure   Examples of images used in LabelMe dataset 
Each image consists of different annotated regions 

including   regions  Note that the regions in each image
can be overlapped  We remove the images containing less
then   regions and obtain     images 
We then extract GIST feature  Oliva and Torralba   
for each region in   image  GIST is   visual descriptor to represent perceptual dimensions and oriented spatial structures of   scene  Each GIST descriptor is    
dimensional vector  We further use PCA to project GIST
features into   dimensions  Finally  we obtain    
 documents  each of which contains regions as observations  Each region now is represented by    dimensional
vector  We now can perform clustering regions in every image since they are visually correlated  In the next level of
clustering  we can cluster images into scene categories 
StudentLife dataset is   large dataset frequently used in
pervasive and ubiquitous computing research  Data signals

consist of multiple channels       WiFi signals  Bluetooth
scan  etc  which are collected from smartphones of   students at Dartmouth College over    week spring term in
  However  in our experiments  we use only WiFi signal strengths  We applied   similar procedure described in
 Nguyen et al    to preprocess the data  We aggregate the number of scans by each Wi  access point and
select   Wi  Ids with the highest frequencies  Eventually  we obtain    documents  with totally approximately
  million  dimensional data points 
Experimental results  To quantitatively evaluate our proposed methods  we compare our algorithms with several
baseline methods  Kmeans  threestage Kmeans  TSKmeans  as described in the Supplement  MC SVI without
context  Huynh et al    Clustering performance in
Table   is evaluated with the image clustering problem for
LabelMe dataset  With Kmeans  we average all data points

 Number of groupsAlgorithm  Algorithm  Kmeans Number of groupsWasserstein distance to truthAlgorithm  Algorithm  Kmeans Number of groupsAlgorithm  Algorithm  Kmeans Number of groupsAlgorithm  Algorithm  Kmeans Number of observations per groupAlgorithm  Algorithm  Kmeans Number of observations per groupWasserstein distance to truthAlgorithm  Algorithm  Kmeans Number of observations per groupAlgorithm  Algorithm  Kmeans Number of observations per groupAlgorithm  Algorithm  KmeansskyfieldtreeflowersskybuildingbuildingbuildingbuildingbuildingcarcarcarcarcarcarsidewalkroadposterMultilevel Clustering via Wasserstein Means

   

   

Figure   Clustering representation for two datasets      Five image clusters from Labelme data discovered by MWMS
algorithm  tagclouds on the left are accumulated from all images in the clusters while six images on the right are randomly
chosen images in that cluster      StudentLife discovered network with three node groups    discovered student clusters 
  student nodes    discovered activity location  from Wi  data  and two edge groups    Student to cluster assignment 
  Student involved to activity location  Node sizes  of discovered nodes  depict the number of element in clusters while
edge sizes between Student and activity location represent the popularity of student   activities 

to obtain   single vector for each images  Kmeans needs
much less time to run since the number of data points is
now reduced to     For MC SVI  we used stochastic
varitational and   parallelized Sparkbased implementation
in  Huynh et al    to carry out experiments  This implementation has the advantage of making use of all of  
cores on the test machine  The running time for MC SVI
is reported after scanning one epoch  In terms of clustering
accuracy  MWM and MWMS algorithms perform the best 
Fig     demonstrates  ve representative image clusters
with six randomly chosen images in each  on the right 
which are discovered by our MWMS algorithm  We also
accumulate labeled tags from all images in each cluster to
produce the tagcloud on the left  These tagclouds can be
considered as visual ground truth of clusters  Our algorithm can group images into clusters which are consistent
with their tagclouds 
We use StudentLife dataset to demonstrate the capability of
multilevel clustering with largescale datasets  This dataset
not only contains   large number of data points but presents
in high dimension  Our algorithms need approximately  
hour to perform multilevel clustering on this dataset  Fig 
   presents two levels of clusters discovered by our algorithms  The innermost  blue  and outermost  green  rings
depict local and global clusters respectively  Global clusters represent groups of students while local clusters shared
between students  documents  may be used to infer loca 

tions of students  activities  From these clusteing we can
dissect students  shared location  activities       Student
      mainly takes part in activity location      
  Discussion
We have proposed an optimization based approach to multilevel clustering using Wasserstein metrics  There are several possible directions for extensions  Firstly  we have
only considered continuous data  it is of interest to extend
our formulation to discrete data  Secondly  our method requires knowledge of the numbers of clusters both in local
and global clustering  When these numbers are unknown  it
seems reasonable to incorporate penalty on the model complexity  Thirdly  our formulation does not directly account
for the  noise  distribution away from the  Wasserstein 
means  To improve the robustness  it may be desirable to
make use of the  rstorder Wasserstein metric instead of
the secondorder one  Finally  we are interested in extending our approach to richer settings of hierarchical data  such
as one when group levelcontext is available 
Acknowledgement  This research is supported in part by
grants NSF CAREER DMS  NSF CNS 
the Margaret and Herman Sokol Faculty Award and research gift from Adobe Research  XN  DP gratefully acknowledges the partial support from the Australian Research Council  ARC  and AOARD  FA 

                                                                                                                                                                      Multilevel Clustering via Wasserstein Means

   Oliva and    Torralba  Modeling the shape of the scene 
  holistic representation of the spatial envelope  International Journal of Computer Vision     

   Pollard  Quantization and the method of kmeans  IEEE
Transactions on Information Theory     

   Pritchard     Stephens  and    Donnelly 

Inference of
population structure using multilocus genotype data  Genetics     

   Rodriguez     Dunson  and      Gelfand  The nested
   Amer  Statist  Assoc   

Dirichlet process 
   

   Solomon     Fernando     Payr       Cuturi     Butscher 
   Nguyen     Du  and    Guibas  Convolutional wasserstein distances  Ef cient optimal transportation on geoIn The International Conference and
metric domains 
Exhibition on Computer Graphics and Interactive Techniques   

   Srivastava     Cevher     Dinh  and    Dunson  Wasp 
Scalable bayes via barycenters of subset posteriors  In
Proceedings of the Eighteenth International Conference
on Arti cial Intelligence and Statistics   

      Teh        Jordan        Beal  and       Blei  Hierarchical Dirichlet processes     Amer  Statist  Assoc   
   

  edric Villani  Topics in Optimal Transportation  Ameri 

can Mathematical Society   

      Wulsin        Jensen  and    Litt  Nonparametric
multilevel clustering of human epilepsy seizures  Annals of Applied Statistics     

     

 Alvarez Estebana     del Barrioa       CuestaAlbertosb  and    Matr an     xedpoint approach to
barycenters in wasserstein space  Journal of Mathematical Analysis and Applications     

References
   Agueh and    Carlier  Barycenters in the wasserstein
space  SIAM Journal on Mathematical Analysis   
   

   Anderes     Borgwardt  and    Miller  Discrete wasserstein barycenters  optimal transport for discrete data 
http arxiv org abs   

      Benamou     Carlier     Cuturi     Nenna  and
   Payr    Iterative bregman projections for regularized
SIAM Journal on Scienti  
transportation problems 
Computing     

     Blei       Ng  and      Jordan  Latent Dirichlet al 

location     Mach  Learn  Res     

   Cuturi  Sinkhorn distances  lightspeed computation of
optimal transport  Advances in Neural Information Processing Systems    

   Cuturi and    Doucet  Fast computation of wasserstein
barycenters  Proceedings of the  st International Conference on Machine Learning   

   Graf and    Luschgy  Foundations of quantization for
probability distributions  SpringerVerlag  New York 
 

   Huynh     Phung     Venkatesh     Nguyen     Hoffman  and    Bui  Scalable nonparametric bayesian multilevel clustering  Proceedings of Uncertainty in Arti 
cial Intelligence   

   Kulis and       Jordan  Revisiting kmeans  new algorithms via bayesian nonparametrics  Proceedings of
the  th International Conference on Machine Learning 
 

ThanhBinh Nguyen  Vu Nguyen  Svetha Venkatesh  and
Dinh Phung  Mcnc  Multichannel nonparametric clustering from heterogeneous data  In Proceedings of ICPR 
 

   Nguyen     Phung     Nguyen     Venkatesh  and
   Bui  Bayesian nonparametric multilevel clustering
with grouplevel contexts  Proceedings of the  st International Conference on Machine Learning   

   Nguyen  Convergence of latent mixing measures in  
nite and in nite mixture models  Annals of Statistics   
   

   Nguyen  Borrowing strengh in hierarchical bayes 
Posterior concentration of the dirichlet base measure 
Bernoulli     

