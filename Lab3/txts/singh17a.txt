Nonparanormal Information Estimation

Shashank Singh   Barnab as   oczos  

Abstract

We study the problem of using       
samples
from an unknown multivariate probability distribution   to estimate the mutual information of   
This problem has recently received attention in
two settings    where   is assumed to be Gaussian and   where   is assumed only to lie in  
large nonparametric smoothness class  Estimators proposed for the Gaussian case converge in
high dimensions when the Gaussian assumption
holds  but are brittle  failing dramatically when
  is not Gaussian  while estimators proposed for
the nonparametric case fail to converge with realistic sample sizes except in very low dimension 
Hence  there is   lack of robust mutual information estimators for many realistic data  To address this  we propose estimators for mutual information when   is assumed to be   nonparanormal  or Gaussian copula  model    semiparametric compromise between Gaussian and nonparametric extremes  Using theoretical bounds and
experiments  we show these estimators strike  
practical balance between robustness and scalability 

  Introduction
This paper is concerned with the problem of estimating
entropy or mutual information of an unknown probability
density   over RD  given          samples from    Entropy
and mutual information are fundamental information theoretic quantities  and consistent estimators for these quantities have   host of applications within machine learning 
statistics  and signal processing  For example  entropy estimators have been used for goodnessof   testing  Goria
et al    parameter estimation in semiparametric models  Wolsztynski et al    texture classi cation and image registration  Hero et al      change point de 

 Carnegie Mellon University  Pittsburgh  USA  Correspon 

dence to  Shashank Singh  sss andrew cmu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tection  Bercher   Vignat    and anomaly detection
in networks  Noble   Cook    Nychis et al   
Berezi nski et al    Mutual information is   popular nonparametric measure of dependence  whose estimators have been used in feature selection  Peng et al   
Shishkin et al    clustering  Aghagolzadeh et al 
  learning graphical models  Chow   Liu   
fMRI data processing  Chai et al    prediction of protein structures  Adami    boosting and facial expression recognition  Shan et al    and  tting deep nonlinear models  Hunter   Hodas    Estimators for both
entropy and mutual information have been used in independent component and subspace analysis  LearnedMiller  
Fisher    Szab   et al     
Motivated by these and other applications  several very recent lines of work  discussed in Section   have studied
information estimation  focusing largely on two settings 
  Gaussian Setting 
If   is known to be Gaussian 
there exist information estimators with mean squared error

 cid  and an  almost matching 

 Birg     Massart   

 MSE  at most   log cid     
 cid 

minimax lower bound of       Cai et al   
  Nonparametric Setting  If   is assumed to lie in   nonparametric smoothness class  such an sorder    older or
Sobolev class  then the minimax MSE is of asymptotic order  cid  max
In the Gaussian setting  consistent estimation is tractable
even in the highdimensional case where   increases fairly
quickly with    as long as         However  optimal
estimators for the Gaussian setting rely heavily on the assumption of joint Gaussianity  and their performance can
degrade quickly when the data deviate from Gaussian  Especially in high dimensions  it is unlikely that data are
jointly Gaussian  making these estimators brittle in practice  In the nonparametric setting  the theoretical convergence rate decays exponentially with    and  it has been
found empirically that information estimators for this setting fail to converge at realistic sample sizes in all but very
low dimensions  Also  most nonparametric estimators are
sensitive to tuning of bandwidth parameters  which is chal 

 

 cid 

        

    

 We will collectively call the closely related problems of entropy and mutual information estimation information estimation 
 Here    encodes the degree of smoothness  roughly corre 

sponding to the number of continuous derivatives of   

Nonparanormal Information Estimation

lenging for information estimation  since no empirical error
estimate is available for crossvalidation 
Given these factors  though the Gaussian and nonparametric cases are fairly well understood in theory  there remains
  lack of practical information estimators for the common
case where data are neither exactly Gaussian nor very low
dimensional  The main goal of this paper is to  ll the gap
between these two extreme settings by studying information estimation in   semiparametric compromise between
the two  known as the  nonparanormal           Gaussian copula  model  see De nition   The nonparanormal
model  analogous to the additive model popular in regression  Friedman   Stuetzle    limits complexity of interactions among variables but makes minimal assumptions
on the marginal distribution of each variable  The result
scales better with dimension than nonparametric models 
while being more robust than Gaussian models 
Proofs of our main results  as well as additional experiments and details are given in the extended version of this
paper   All code can be found on GitHub 

  Problem statement and notation
There are   number of distinct generalizations of mutual
information to more than two variables  The de nition
we consider is simply the difference between the sum of
marginal entropies and the joint entropy 
De nition  
information  Let
           XD be Rvalued random variables with   joint
probability density     RD     and marginal
densities      pD         The multivariate mutual
information      of                 XD  is de ned by

 Multivariate mutual

 cid 

 cid 

 cid  

 cid cid 

        
   

log

    

   pj Xj 

  cid 

 

  Xj        

 

  

where          EX   log      denotes entropy of   

This notion of multivariate mutual information  originally
due to Watanabe    who called it  total correlation  measures total dependency  or redundancy  within
  set of   random variables 
It has also been called
the  multivariate constraint   Garner    and  multiinformation   Studen     Vejnarov      Many related information theoretic quantities can be expressed in

 accessible

at

http www contrib andrew 

cmu edu sss publications papers 
nonparanormalinformation estimation pdf

 https github com sss 

nonparanormalinformation

terms of      and can thus be estimated using estimators
of      Examples include pairwise mutual information
                             which measures dependence between  potentially multivariate  random variables   and     conditional mutual information

                     cid 

  Xj    

  

which is useful for characterizing how much dependence
within   can be explained by   latent variable    Studen  
  Vejnarov      and transfer entropy          directed
information  TX       which measures predictive power
of one time series   on the future of another time series
         is also related to entropy via Eq    but  unlike the above quantities  this relationship depends on the
marginal distributions of    and hence involves some additional considerations  as discussed in Section  
We now de ne the class of nonparanormal distributions 
from which we assume our data are drawn 
      
De nition  
Gaussian copula model    random vector    
            XD   is said to have   nonparanormal distribution  denoted     NPN       if there exist functions  fj  
   such that each fj         is   diffeomorphism   and               for some  strictly  positive
de nite     RD   with    on the diagonal       each
                  is called the latent covariance of  
and   is called the marginal transformation of   

 Nonparanormal distribution 

The nonparanormal family relaxes many constraints of
the Gaussian family  Nonparanormal distributions can be
multimodal  skewed  or heavytailed  can encode noisy
nonlinear dependencies  and need not be supported on RD 
Minimal assumptions are made on the marginal distributions  any desired continuously differentiable marginal cumulative distribution function  CDF  Fi of variable Xi corresponds to marginal transformation fi       Fi   
 where   is the standard normal CDF  As examples  for
  Gaussian variable    the  dimensional case      
      and                 is nonparanormal when
               tanh        or any other diffeomorphism  On the other hand  the limits of the Gaussian copula
appear  for example  when            which is not bijective  then  if          the Gaussian copula approximation
of        models    and    as independent 

   diffeomorphism is   continuously differentiable bijection

              such that    is continuously differentiable 

 Setting              and each        ensures model identi 
 ability  but does not reduce the model space  since these parameters can be absorbed into the marginal transformation   

Nonparanormal Information Estimation

We are now ready to formally state our problem 
Formal Problem Statement  Given         
samples
     Xn   NPN       where   and   are both unknown  we would like to estimate     
Other notation    denotes the dimension of the data      
    RD   and     RD   RD  For   positive integer                 denotes the set of positive integers
less than    inclusive  For consistency  where possible  we
use         to index samples and         to index dimensions  so that       Xi   denotes the jth dimension of the
ith sample  Given   data matrix     Rn    our estimators depend on the empirical rank matrix

           with Ri    

 Xi   Xk   

 

  cid 

  

 cid   cid 

  
   

      

For   square matrix     Rk        denotes the determinant
of    AT denotes the transpose of    and

 cid   cid    max
    Rk
 cid   cid     

 cid Ax cid 

and

 cid   cid    

denote the spectral and Frobenius norms of    respectively 
When   is symmetric                       
are its eigenvalues 

  Related Work and Our Contributions
  The Nonparanormal

Nonparanormal models have been used for modeling dependencies among highdimensional data in   number of
 elds  such as graphical modeling of gene expression
data  Liu et al    of neural data  Berkes et al   
and of  nancial time series  Malevergne   Sornette   
Wilson   Ghahramani    Hern andezLobato et al 
  extreme value analysis in hydrology  Renard  
Lang    Aghakouchak    and informative data
compression  Rey   Roth   
With one recent exception  Ince et al    previous
information estimators for the nonparanormal case  Calsaverini   Vicente    Ma   Sun    Elidan   
rely on fully nonparametric information estimators as subroutines  and hence suffer strongly from the curse of dimensionality  Very recently  Ince et al    proposed
what we believe is the  rst mutual information estimator
tailored speci cally to the nonparanormal case  their estimator is equivalent to one of the estimators  IG  described
in Section   we study  However  they focused on its applications to neuroimaging data analysis  and did not study
its performance theoretically or empirically 

  Information Estimation

max

    

 cid 

 cid 

 cid cid 

        

Our motivation for studying the nonparanormal family
comes from trying to bridge two recent approaches to information estimation  The  rst has studied fully nonparametric entropy estimation  assuming only that data are
drawn from   smooth probability density    where smoothness is typically quanti ed by     older or Sobolev exponent       roughly corresponding to the continuous
differentiability of    In this setting  the minimax optimal
MSE rate has been shown by Birg     Massart   to
  This rate slows exponenbe  
tially with the dimension    and  while many estimators
have been proposed    al et al    Sricharan et al   
  Singh     oczos        Krishnamurthy et al 
  Moon   Hero        Singh     oczos     
Moon et al    for this setting  their practical use is
limited to   few dimensions 
The second area is in the setting where data are assumed to
be drawn from   truly Gaussian distribution  Here the highdimensional case is far more optimistic  While this case
had been studied previously  Ahmed   Gokhale   
Misra et al    Srivastava   Gupta    Cai et al 
  recently provided   precise  nitesample analysis
based on deriving the exact probability law of the log 

 

determinant log  cid  of the scatter matrix  cid  From this 
 cid  and   highdimensional central limit the 
  log cid     

they derived   deterministic bias correction  giving an estimator for which they prove an MSE upper bound of
orem for the case       as        but       
Cai et al    also prove   minimax lower bound of
     on MSE  with several interesting consequences 
First  consistent information estimation is possible only if
        Second  since  for small      log        
   this lower bound essentially matches the above upper
bound when     is small  Third  they show this lower
bound holds even when restricted to diagonal covariance
matrices  Since the upper bound for the general case and
the lower bound for the diagonal case essentially match  it
follows that Gaussian information estimation is not made
easier by structural assumptions such as   being bandable 
sparse  or Toeplitz  as is common in  for example  stationary Gaussian process models  Cai   Yuan   
This      lower bound extends to our more general nonparanormal setting  However  we provide   minimax lower
bound suggesting that the nonparanormal setting is strictly
harder  in that optimal rates depend on   Our results imply

 Few  depends on   and    but Kandasamy et al    suggest nonparametric estimators should only be used with   at most
  Rey   Roth   tried using several nonparametric information estimators on the Communities and Crime UCI data set
             but found all too unstable to be useful 

Nonparanormal Information Estimation

nonparanormal information estimation does become easier
if   is assumed to be bandable or Toeplitz 
  closely related point is that known convergence rates for
the fully nonparametric case require the density   to be
bounded away from   or have particular tail behavior  due
to singularity of the logarithm near   and resulting sensitivity of Shannon informationtheoretic functionals to regions of low but nonzero probability  In contrast  Cai et al 
  need no lowerbound type assumptions in the Gaussian case  In the nonparanormal case  we show some such
condition is needed to prove   uniform rate  but   weaker
condition    positive lower bound on     suf ces 
The main contributions of this paper are the following 

  We propose three estimators   cid IG   cid    and  cid      for the
on the mean squared error of  cid    providing the  rst

mutual information of   nonparanormal distribution 
    

  We prove upper bounds  of order     

upper bounds for   nonparanormal information estimator  This bound suggests nonparanormal estimators
scale far better with   than nonparametric estimators 
  We prove   minimax lower bound suggesting that  unlike the Gaussian case  dif culty of nonparanormal information estimation depends on the true  

  We give simulations comparing our proposed estimators to Gaussian and nonparametric estimators  Besides con rming and augmenting our theoretical predictions  these help characterize the settings in which
each nonparanormal estimator works best 

  We present entropy estimators based on  cid IG   cid    and
 cid      Though nonparanormal entropy estimation re 

quires somewhat different assumptions from mutual
information estimation  we show that entropy can also
be estimated at the rate     

    

  Nonparanormal Information Estimators
In this section  we present three different estimators  IG 
   and      for the mutual information of   nonparanormal
distribution  We begin with   lemma providing common
motivation for all three estimators 
Since mutual information is invariant to diffeomorphisms
of individual variables  it is easy to see that the mutual information of   nonparanormal random variable is the same
as that of the latent Gaussian random variable  Speci cally 
Lemma    Nonparanormal mutual information  Suppose     NPN       Then 

          
 

log  

 Ince et al    proposed  cid IG for use in neuroimaging data
analysis  To the best of our knowledge cid    and cid    are novel 

 

Lemma   shows that mutual information of   nonparanormal random variable depends only the latent covariance  
the marginal transformations are nuisance parameters  allowing us to avoid dif cult nonparametric estimation  the
estimators we propose all plug different estimates of   into
Eq    after   regularization step described in Section  

  Estimating   by Gaussianization

The  rst estimator cid   of   proceeds in two steps  First  the

data are transformed to have approximately standard normal marginal distributions    process Szab   et al     
referred to as  Gaussianization  By the nonparanormal assumption  the Gaussianized data are approximately jointly
Gaussian  Then  the latent covariance matrix is estimated
by the empirical covariance of the Gaussianized data 
More speci cally  letting   denote the quantile function
of the standard normal distribution and recalling the rank
matrix   de ned in   the Gaussianized data

 for                

 cid 

 cid Xi      

 cid  Ri  
empirical covariance cid      

     

 

are obtained by transforming the empirical CDF of the each
dimension to approximate   Then  we estimate   by the

 cid  
    cid Xi cid    

   

  Estimating   by rank correlation

The second estimator actually has two variants     and
     respectively based on relating the latent covariance to
two classic rankbased dependence measures  Spearman  
  and Kendall     For two random variables   and   with
CDFs FX   FY             and   are de ned by

          Corr FX     FY     
            Corr sign       cid  sign        cid 

and

respectively  where

Corr         

                   

 cid          

denotes the standard Pearson correlation operator and
   cid     cid  is an IID copy of           and   generalize to
the Ddimensional setting in the form of rank correlation
matrices              with         Xj  Xk  and
          Xj  Xk  for each           
   and    are based on   classical result relating the correlation and rankcorrelation of   bivariate Gaussian 
Theorem  
Gaussian joint distribution with covariance   Then 

 Kruskal    Suppose         has  

 cid   

 

 cid 

 cid   

 

 cid 

Corr            sin

       

  sin

         

 

Nonparanormal Information Estimation

       NPN      
Proposition   Suppose      Xn
Then  there exists   constant       such that  for any

in the Appendix  We  rst bound the bias of cid   
      the bias of cid     is at most
 cid   
 cid cid cid     

 cid cid cid   cid cid    

 cid     

 cid 

  log

 

 

   
 

 

 

wise into the transform    cid    sin cid   

  and   are often preferred over Pearson correlation for
their relative robustness to outliers and applicability to nonnumerical ordinal data  While these are strengths here as
well  the main reason for their relevance is that they are
invariant to marginal transformations       for diffeomorphisms                                      and
                           As   consequence  the
identity provided in Theorem   extends unchanged to the
case           NPN       This suggests an estimate for
  based on estimating   or   and plugging this element 

    cid  or    cid  sin cid   
    cid 
 cid     cid Corr   
 cid cid  where
is applied elementwise  Similarly cid    sin cid   
 cid      

 cid      sin
 cid   cid 
 cid  

is the empirical correlation of the rank matrix    and sine

sign Xi       cid    sign Xi       cid   

respectively  Speci cally    is de ned by

 cid 
 cid   
 cid 

  where

 

  cid cid   

  Regularization and estimating  

Unfortunately  unlike usual empirical correlation matrices 

none of  cid     cid  or  cid  is almost surely strictly positive

de nite  As   result  directly plugging into the mutual information functional   may give   or be unde ned  To
correct for this  we propose   regularization step  in which
we project each estimated latent covariance matrix onto the
 closed  cone      of symmetric matrices with minimum
eigenvalue       Speci cally  for any       let

      cid     RD         AT             cid   
position  cid               QQT   QT     ID and  
For any symmetric matrix     RD   with eigendecomis diagonal  the projection Az of   onto      is de ned
as Az     zQ  where    is the diagonal matrix with
jth nonzero entry          max          We call this  
 projection  because Az   argminB     cid       cid    see 
Applying this regularization to  cid     cid  or  cid  gives  
     Henrion   Malick  
strictly positive de nite estimate  cid       cid    or  cid    re 
 cid cid cid cid  
 cid cid cid 
 cid IG        
 cid          
 cid cid cid cid  
 cid cid cid   
 cid          
  Upper Bounds on the Error of cid    
of the estimator cid    based on Spearman     Proofs are given

spectively  of   We can then estimate   by plugging this
into Equation   giving our three estimators 

Here  we provide  nitesample upper bounds on the error

 cid cid cid cid    

 cid cid cid   

and

log

log

log

 

 

 

where    is the projection of   onto     

The  rst term of the bias stems from nonlinearity of the
logdeterminant function in Equation   which we analyze
via Taylor expansion  The second term 

 cid 

 cid   

 cid 

 

   
   

log

log

   

     

is due to the regularization step and is actually exact  but is
dif cult to simplify or bound without more assumptions on
the spectrum of   and choice of    which we discuss later 

We now turn to bounding the variance of  cid      We  rst
provide an exponential concentration inequality for  cid    

around its expectation  based on McDiarmid   inequality 
       NPN      
Proposition   Suppose      Xn
Then  for any         

  cid cid cid cid cid         cid cid    

 cid cid cid cid     

 cid      exp

 cid 

 cid 

 

  nz 
   

Such exponential concentration bounds are useful when
one wants to simultaneously bound the error of multiple
uses of an estimator  and hence we present it separately as
it may be independently useful  However  for the purpose
of understanding convergence rates  we are more interested
in the variance bound that follows as an easy corollary 
Corollary   Suppose      Xn

Then  for any       the variance of cid     is at most

       NPN      

  cid cid    

 cid       

 

   

Given these bias and variance bounds    bound on the MSE

of cid     follows via the usual biasvariance decomposition 

Theorem   Suppose     NPN       Then  there exists   constant   such that

 cid cid cid        

 cid cid 

 

   

 cid    

   

  log     
 

 

 

 cid 

Nonparanormal Information Estimation

  natural question is now how to optimally select the regularization parameter    While the bound   is clearly convex in    it depends crucially on the unknown spectrum of
  and  in particular  on the smallest eigenvalues of   As
  result  it is dif cult to choose   optimally in general  but
we can do so for certain common subclasses of covariance
matrices  For example  if   is Toeplitz or bandable      
for some         all                then the smallest eigenvalue of   can be bounded below  Cai   Yuan 
  When   is bandable  as we show in the Appendix 
this bound can be independent of    In these cases  the
following somewhat simpler MSE bound can be used 
Corollary   Suppose     NPN       and suppose
        Then  there exists   constant       such that

 cid cid cid        

 cid cid 

 

  CD 
   

 

 

 cid cid cid 

  log

 where

 cid cid cid cid 

 cid  

  Lower Bounds in terms of  
If      Xn

             are Gaussian  for the plugin

   XiX  
 

 cid     

is the empirical covariance matrix  Cai et al   

true correlation matrix   This follows from the  stability 
of Gaussians       that nonsingular linear transformations
of Gaussian random variables are Gaussian  In particular 

estimator cid        
showed that the distribution of cid       is independent of the
 cid         log  cid    log     log  cid 
and  cid  has the same distribution as log cid  does
of cid   and con dence intervals that do not depend on  

in the special case that     ID is the identity  This property is both somewhat surprising  given that       as
      and useful  leading to   tight analysis of the error

It would be convenient if any nonparanormal information
estimators satis ed this property  Unfortunately  the main
result of this section is   negative one  showing that this
property is unlikely to hold without additional assumptions 
Proposition   Consider the  dimensional case

            

 

 

with    

   

     Xn

and let         Suppose an estimator  cid      cid      of
        
  log      is   function of the empirical rank
  depending only    such that the worstcase MSE of  cid  
matrix     Nn  of    Then  there exists   constant    
 cid     log     cid 
over         satis es

 cid cid cid          

 cid cid 

sup

 

   
 

 

 cid   

 cid 

Clearly  this lower bound tends to   as       As written 
this result lower bounds the error of rankbased estimators
in the Gaussian case when       However  to the best
of our knowledge  all methods for estimating   in the nonparanormal case are functions of    and prior work  Hoff 
  has shown that the rank matrix   is   generalized
suf cient statistic for    and hence for    in the nonparanormal model  Thus  it is reasonable to think of lower
bounds for rankbased estimators in the Gaussian case as
lower bounds for any estimator in the nonparanormal case 
The proof of this result is based on the simple observation that the rank matrix can take only  nitely many values 
Hence  as         tends to be perfectly correlated  providing little information about   whereas the dependence
of the estimand    on   increases sharply  This is intuition
is formalized in the Appendix using Le Cam   lemma for
lower bounds in twopoint parameter estimation problems 

 see Cai et al   

  Empirical Results
We compare   mutual information estimators 

   cid    Gaussian plugin estimator with biascorrection
   cid IG  Nonparanormal estimator using Gaussianization 
   cid    Nonparanormal estimator using Spearman    
   cid      Nonparanormal estimator using Kendall    
   cid IkNN  Nonparametric estimator using knearest neigh 

bor  kNN  statistics 

For    and      we used   regularization constant    
  We did not regularize for IG  Although this implies   IG         this is extremely unlikely for
even moderate values of   and never occurred during our
experiments  which all use       We thus omit denoting
dependence on    For IkNN  except as noted in Experiment
        based on recent analysis  Singh     oczos     
suggesting that small values of   are best for estimation 
Suf cient details to reproduce experiments are given in the
Appendix  and MATLAB source code is available at  Omitted for anonymity  We report MSE based on          trials of each condition    con dence intervals were consistently smaller than plot markers and hence omitted to
avoid cluttering plots  Except as speci ed otherwise  each
experiment had the following basic structure  In each trial 
  correlation matrix   was drawn by normalizing   random
covariance matrix from   Wishart distribution  and data
             drawn  All   estimators were
     Xn
computed from      Xn and squared error from true mutual information  computed from   was recorded  Unless
speci ed otherwise        and      
Since our nonparanormal information estimators are func 

tions of ranks of the data  neither the true mutual information nor our nonparanormal estimators depend on the
marginal transformations  Thus  except in Experiment  
where we show the effects of transforming marginals  and
Experiment   where we add outliers to the data  we perform all experiments on truly Gaussian data  with the understanding that this setting favors the Gaussian estimator 
All experimental results are displayed in Figure  
Experiment    Dependence on    We  rst show nonparanormal estimators have  parametric       depen 

dence on    unlike cid IkNN  which converges far more slowly 
For large    MSEs of cid IG cid    and cid    are close to that of cid   
Gaussianity of the marginals  unlike  cid    We applied   non 

Experiment    NonGaussian Marginals  Next  we
show nonparanormal estimators are robust
to nonlinear transformation   to   fraction         of dimeni     
sions of Gaussian data  That is  we drew      Zn
      and then used data      Xn  where

 cid     Zi   

Zi  

Xi    

if       
if       

                

 

for   diffeomorphism     Here  we use         ez  The

forms poorly even when   is quite small  Poor performance

Experiment    Outliers  We now show that nonparanormal estimators are far more robust to the presence of out 

Appendix shows similar results for several other      cid   perof cid IkNN may be due to discontinuity of the density at      
liers than  cid   or  cid IkNN  To do this  we added outliers to the
formly at random from     Performance of  cid   degrades rapidly even for small    cid IkNN can fail for atomic
distributions   cid IkNN     whenever at least   samples are
nored trials where cid IkNN     but cid IkNN ceased to give any

data according to the method of Liu et al    After drawing Gaussian data  we independently select  cid   cid 
samples in each dimension  and replace each        uni 

identical  This mitigate this  we increased   to   and ig 

 nite estimates when   was suf ciently large 
For small values of   nonparanormal estimators surprisingly improve  We hypothesize this is due to convexity of
the mutual information functional Eq    in   By Jensen  
inequality  estimators which plugin an approximately un 

biased estimate  cid  of   are biased towards overestimating

   Adding random  uncorrelated  noise reduces estimated
dependence  moving the estimate closer to the true value 
If this nonlinearity is indeed   major source of bias  it
may be possible to derive   von Misestype bias correction
 see Kandasamy et al    accounting for higherorder
terms in the Taylor expansion of the logdeterminant 
Experiment    Dependence on   Here  we verify our results in Section   showing that MSE of rankbased estima 

Nonparanormal Information Estimation

dent of   Here  we set       and   as in Eq    varying

tors approaches   as       while MSE of cid   is indepen 
        Indeed  the MSE of  cid   does not change  while
the MSEs of  cid IG   cid    and  cid    all increase as       This
than of  cid   only when        cid    appears to perform far
better than cid IG and cid    in this regime  Performance of IkNN

increase seems mild in practice  with performance worse

degrades far more quickly as       This phenomenon is
explored by Gao et al    who lower bound error of
IkNN in the presence of strong dependencies  and proposed
  correction to improve performance in this case 

This is likely because  for small   the main source of erwhen       When       and   is  xed  both

It is also interesting that errors of cid    and cid    drop as      
ror is the variance of  cid  and  cid   as   log         
  sin cid  and sin cid    are asymptotically normal es 
   cid    and cid    are asymptotically normal es 

timates of   with asymptotic variances proportional to
   Klaassen   Wellner    By the delta method 
since dI
timates of    with asymptotic variances proportional to  
and hence vanishing as      

      

  Estimating Entropy
Thus far  we have discussed estimation of mutual information      Mutual information is convenient because
it is invariant under marginal transformation  and hence
               depends only on   While the entropy      does depend on the marginal transform   
fortunately  by Eq         differs from      only
by   sum of univariate entropies  Univariate nonparametric estimation of entropy in has been studied extensively  and there exist several estimators       based on
sample spacings  Beirlant et al    kernel density estimates  Moon et al    or knearest neighbor methods  Singh     oczos      that can estimate   Xj  at
the rate  cid     in MSE under relatively mild conditions
on the marginal density pj  While the precise assumptions
vary with the choice of estimator  they are mainly     that
pj be lower bounded on its support or have particular      
exponential  tail behavior  and     that pj be smooth  typically quanti ed by     older or Sobolev condition  Details
of these assumptions are in the Appendix 
Under

 cid       cid HD and   constant       such that
Combining these estimators with an estimator  say cid      of

  cid Hj     Xj        

since there exist estimators

these conditions 

        

 

mutual information gives an estimator of entropy 

 cid      cid  

    cid Hj  cid     

If we assume      

    is bounded below by   positive

Nonparanormal Information Estimation

    Experiment  

    Experiment  

    Experiment  

    Experiment  

Figure   Plots of log MSE  plotted over     logsample size log        fraction   of dimensions with nonGaussian marginals     
fraction   of outlier samples in each dimension  and     covariance     Cov       Note that the xaxis in     is decreasing 

constant  combining inequality   with Corollary   gives

 cid cid cid           

 cid cid 

 

  CD 
 

 

where   differs from in   but is independent of   and   

  Conclusions and Future Work
This paper suggests nonparanormal information estimation
as   practical compromise between the intractable nonparametric case and the limited Gaussian case  We proposed
three estimators for this problem and provided the  rst upper bounds for nonparanormal information estimation  We
also gave lower bounds showing how dependence on   differs from the Gaussian case and demonstrated empirically
that nonparanormal estimators are more robust than Gaussian estimators  even in dimensions too high for nonparametric estimators 
Collectively  these results suggest that  by scaling to moderate or high dimensionality without relying on Gaussianity  nonparanormal information estimators may be effective tools with   number of machine learning applications 
While the best choice of information estimator inevitably
depends on context  as an offthe shelf guide for practitioners  the estimators we suggest  in order of preference  are 
  fully nonparametric if           max     

   cid    if     is small and data may have outliers 
   cid    if     is small and dependencies may be strong 
   cid IG otherwise 
   cid   only given strong belief that data are nearly Gaussian 

There are many natural open questions in this line of work 
First  in the nonparanormal model  we focused on estimating mutual information      which does not depend
on marginal transforms    and entropy  which decomposes
into      and  dimensional entropies  In both cases  additional structure imposed by the nonparanormal model allows estimation in higher dimensions than fully nonparametric models  Can nonparanormal assumptions lead to

higher dimensional estimators for the many other useful
nonlinear functionals of densities       Lp norms distances
and more general         enyi or Tsallis  entropies  mutual
informations  and divergences  that do not decompose 
Second  there is   gap between our upper bound rate of
 cid cid 
     and the only known lower bound of     
 from the Gaussian case  though we also showed that
bounds for rankbased estimators depend on   Is quadratic
dependence on   optimal  How much do rates improve
under structural assumptions on   Upper bounds should

be derived for other estimators  such as  cid IG and  cid      The

     lower bound proof of Cai et al    for the Gaussian case  based on the CramerRao inequality  Van den
Bos    is unlikely to tighten in the nonparanormal
case  since Fisher information is invariant to diffeomorphisms of the data  Hence    new approach is needed if
the lower bound in the nonparanormal case is to be raised 
Finally  our work applies to estimating the logdeterminant
log   of the latent correlation in   nonparanormal model 
Besides information estimation 
the work of Cai et al 
  on estimating log   in the Gaussian model was
motivated by the role of log   in other multivariate statistical tools  such as quadratic discriminant analysis  QDA 
and MANOVA  Anderson    Can our estimators lead
to more robust nonparanormal versions of these tools 

research is

ACKNOWLEDGEMENTS
This
supported in part by DOE grant
DESC  and NSF grant IIS  to      and by
an NSF Graduate Research Fellowship to      under Grant
No  DGE  Any opinions   ndings  and conclusions or recommendations expressed in this material are
those of the author    and do not necessarily re ect the
views of the National Science Foundation 

References
Adami  Christoph 

Physics of Life Reviews     

Information theory in molecular biology 

SampleSize log   log MSE   IG     IKNNnonGaussianFraction Fractionofoutliers Cov               Nonparanormal Information Estimation

Aghagolzadeh  Mehdi  SoltanianZadeh  Hamid  Araabi     and
Aghagolzadeh  Ali    hierarchical clustering based on mutual
information maximization  In Image Processing    ICIP
  IEEE International Conference on  volume   pp    
IEEE   

Aghakouchak  Amir  Entropy copula in hydrology and climatol 

ogy     Hydrometeorology     

Ahmed  Nabil Ali and Gokhale  DV  Entropy expressions and
their estimators for multivariate distributions  IEEE Trans  on
Information Theory     

Anderson  TW  Multivariate statistical analysis  Wi ey and Sons 

New York  NY   

Beirlant  Jan  Dudewicz  Edward    Gy or    aszl    and Van der
Meulen  Edward    Nonparametric entropy estimation  An
overview  International Journal of Mathematical and Statistical Sciences     

Bercher  JF and Vignat  Christophe  Estimating the entropy of  
signal with applications  IEEE Trans  on Signal Processing   
   

Garner  Wendell    Uncertainty and structure as psychological

concepts   

Goria  Mohammed Nawaz  Leonenko  Nikolai    Mergel  Victor    and Novi Inverardi  Pier Luigi    new class of random
vector entropy estimators and its applications in testing statistical hypotheses  Nonparametric Statistics   
 

Henrion  Didier and Malick    er ome  Projection methods in conic
optimization  In Handbook on Semide nite  Conic and Polynomial Optimization  pp    Springer   

Hern andezLobato  Jos   Miguel  Lloyd  James    and Hern andezLobato  Daniel  Gaussian process conditional copulas with applications to  nancial time series  In Advances in Neural Information Processing Systems  pp     

Hero  Alfred    Ma  Bing  Michel  Olivier  and Gorman  John 
Alphadivergence for classi cation  indexing and retrieval  revised   

Hero  Alfred    Ma  Bing  Michel  Olivier JJ  and Gorman  John 
Applications of entropic spanning graphs  IEEE Signal Processing Magazine     

Berezi nski  Przemys aw  Jasiul  Bartosz  and Szpyrka  Marcin 
An entropybased network anomaly detection method  Entropy     

Hoff  Peter    Extending the rank likelihood for semiparametric
copula estimation  The Annals of Applied Statistics  pp   
   

Berkes  Pietro  Wood  Frank  and Pillow  Jonathan    Characterizing neural dependencies with copula models  In Advances in
Neural Information Processing Systems  pp     

Hunter  Jacob   and Hodas  Nathan    Mutual information for  tting deep nonlinear models  arXiv preprint arXiv 
 

Birg    Lucien and Massart  Pascal  Estimation of integral func 

tionals of   density  Annals of Stat  pp     

Cai    Tony and Yuan  Ming  Adaptive covariance matrix estimation through block thresholding  Annals of Stat   
   

Cai    Tony  Liang  Tengyuan  and Zhou  Harrison    Law of
log determinant of sample covariance matrix and optimal estimation of differential entropy for highdimensional Gaussian
distributions     of Multivariate Analysis     

Calsaverini  Rafael   and Vicente  Renato  An informationtheoretic approach to statistical dependence  Copula information  EPL  Europhysics Letters     

Chai  Barry  Walther  Dirk  Beck  Diane  and FeiFei  Li  Exploring functional connectivities of the human brain using multivariate information analysis  In Advances in neural information processing systems  pp     

Chow    and Liu  Cong  Approximating discrete probability distributions with dependence trees  IEEE transactions on Information Theory     

Elidan  Gal  Copulas in machine learning  In Copulae in mathe 

matical and quantitative  nance  pp    Springer   

Friedman  Jerome   and Stuetzle  Werner  Projection pursuit re 

gression  JASA     

Gao  Shuyang  Ver Steeg  Greg  and Galstyan  Aram  Ef cient
estimation of mutual information for strongly dependent variables  In AISTATS   

Ince  Robin AA  Giordano  Bruno    Kayser  Christoph  Rousselet  Guillaume    Gross  Joachim  and Schyns  Philippe     
statistical framework for neuroimaging data analysis based on
mutual information estimated via   gaussian copula  Human
brain mapping     

Kandasamy  Kirthevasan  Krishnamurthy  Akshay  Poczos  Barnabas  Wasserman  Larry  and Robins  James    Nonparametric von mises estimators for entropies  divergences and mutual
informations  In Advances in Neural Information Processing
Systems  pp     

Klaassen  Chris AJ and Wellner  Jon    Ef cient estimation in
the bivariate normal copula model  normal margins are least
favourable  Bernoulli     

Krishnamurthy  Akshay  Kandasamy  Kirthevasan  Poczos  Barnabas  and Wasserman  Larry  Nonparametric estimation of
renyi divergence and friends  In International Conference on
Machine Learning  pp     

Kruskal  William    Ordinal measures of association  JASA   

   

LearnedMiller        and Fisher        ICA using spacings estimates of entropy  Journal of Machine Learning Research   
   

Liu  Han  Han  Fang  Yuan  Ming  Lafferty  John  and Wasserman  Larry  Highdimensional semiparametric gaussian copula
graphical models  The Annals of Statistics   
 

Ma  Jian and Sun  Zengqi  Mutual information is copula entropy 

Tsinghua Science   Tech     

Nonparanormal Information Estimation

Malevergne  Yannick and Sornette  Didier  Testing the gaussian
copula hypothesis for  nancial assets dependences  Quantitative Finance     

Singh  Shashank and   oczos  Barnab as  Generalized exponential
In

concentration inequality for   enyi divergence estimation 
ICML  pp       

Singh  Shashank and   oczos  Barnab as  Finitesample analysis of
 xedk nearest neighbor density functional estimators  In Advances in Neural Information Processing Systems  pp   
     

Singh  Shashank and   oczos  Barnab as  Analysis of knearest
neighbor distances with application to entropy estimation 
arXiv preprint arXiv     

Sricharan  Kumar  Raich  Raviv  and Hero  Alfred    knearest
In IEEE
neighbor estimation of entropies with con dence 
International Symposium on Information Theory  ISIT  pp 
  IEEE   

Sricharan  Kumar  Wei  Dennis  and Hero  Alfred    Ensemble
estimators for multivariate entropy estimation  IEEE Transactions on Information Theory     

Srivastava  Santosh and Gupta  Maya    Bayesian estimation
In IEEE Interof the entropy of the multivariate Gaussian 
national Symposium on Information Theory  ISIT  pp   
  IEEE   

Studen    Milan and Vejnarov    Jirina  The multiinformation function as   tool for measuring stochastic dependence  In Learning
in graphical models  pp    Springer   

Szab    Zolt an    oczos  Barnab as  and   orincz  Andr as  Undercomplete blind subspace deconvolution  Journal of Machine
Learning Research   May     

Szab    Zolt an    oczos  Barnab as  Szirtes    abor  and   orincz 
Andr as  Post nonlinear independent subspace analysis  In International Conference on Arti cial Neural Networks  pp   
  Springer     

Van den Bos  Adriaan  Parameter estimation for scientists and

engineers  John Wiley   Sons   

Watanabe  Satosi  Information theoretical analysis of multivariate
correlation  IBM    of research and development   
 

Wilson  Andrew and Ghahramani  Zoubin  Copula processes 
In Advances in Neural Information Processing Systems  pp 
   

Wolsztynski     Thierry     and Pronzato     Minimumentropy
estimation in semiparametric models  Signal Process   
    ISSN  

Misra  Neeraj  Singh  Harshinder  and Demchuk  Eugene  Estimation of the entropy of   multivariate normal distribution    
Multivariate Analysis     

Moon  Kevin and Hero  Alfred  Multivariate fdivergence estiIn Advances in Neural Information

mation with con dence 
Processing Systems  pp       

Moon  Kevin   and Hero  Alfred    Ensemble estimation of mulIn IEEE International Symposium on

tivariate fdivergence 
Information Theory  ISIT  pp    IEEE     

Moon  Kevin    Sricharan  Kumar  Greenewald  Kristjan  and
Hero  Alfred    Improving convergence of divergence functional ensemble estimators  In IEEE International Symposium
on Information Theory  ISIT  pp    IEEE   

Moon  Kevin    Sricharan  Kumar  and Hero III  Alfred   
Ensemble estimation of mutual information  arXiv preprint
arXiv   

Noble  Caleb   and Cook  Diane    Graphbased anomaly detec 

tion  In KDD  pp    ACM   

Nychis  George  Sekar  Vyas  Andersen  David    Kim  Hyong 
and Zhang  Hui  An empirical evaluation of entropybased
In Proceedings of the  th ACM
traf   anomaly detection 
SIGCOMM conference on Internet measurement  pp   
ACM   

  al    avid    oczos  Barnab as  and Szepesv ari  Csaba  Estimation
of   enyi entropy and mutual information based on generalized
nearestneighbor graphs  In Advances in Neural Information
Processing Systems  pp     

Peng  Hanchuan  Long  Fuhui  and Ding  Chris  Feature selection based on mutual information criteria of maxdependency 
maxrelevance  and minredundancy  IEEE Trans  on Pattern
Analysis and Machine Intelligence     

Renard  Benjamin and Lang  Michel  Use of   Gaussian copula for multivariate extreme value analysis  some case studies
in hydrology  Advances in Water Resources   
 

Rey    elanie and Roth  Volker  MetaGaussian information botIn Advances in Neural Information Processing Sys 

tleneck 
tems  pp     

Shan  Caifeng  Gong  Shaogang  and McOwan  Peter    Conditional mutual infomation based boosting for facial expression
recognition  In BMVC   

Shishkin  Alexander  Bezzubtseva  Anastasia  Drutsa  Alexey 
Shishkov 
Ilia  Gladkikh  Ekaterina  Gusev  Gleb  and
Serdyukov  Pavel  Ef cient highorder interactionaware feature selection based on conditional mutual information  In Advances in Neural Information Processing Systems  pp   
   

Singh  Shashank and   oczos  Barnab as  Exponential concentration of   density functional estimator  In Advances in Neural
Information Processing Systems  pp       

