  Uni ed Maximum Likelihood Approach for Estimating Symmetric

Properties of Discrete Distributions

Jayadev Acharya   Hirakendu Das   Alon Orlitsky   Ananda Theertha Suresh  

Abstract

Symmetric distribution properties such as support size  support coverage  entropy  and proximity to uniformity  arise in many applications 
Recently  researchers applied different estimators and analysis tools to derive asymptotically
sampleoptimal approximations for each of these
properties  We show that   single  simple  plugin
estimator pro le maximum likelihood  PML 
is sample competitive for all symmetric properties  and in particular is asymptotically sampleoptimal for all the above properties 

  Introduction
  Symmetric distribution properties

def

           pk    pi    Pk

Let  
   pi              
denote the collection of all discrete distributions over  nite
or in nite support    distribution property is   mapping
         It is symmetric if it remains unchanged under
relabeling of domain symbols  namely if it is determined
by just the probability multiset               pk  Many
important properties are symmetric  For example 
Support size                      plays an important
role in population and vocabulary estimation 
Support coverage Sm     Px        the exShannon entropy        Px      log  

pected number of elements observed in   samples  arises
in ecological and biological studies        Colwell et al 
 

     central to
information theory  Cover   Thomas    has numerous

 Equal contribution  Cornell University  Ithaca  NY  Yahoo
Inc  Sunnyvale  CA  University of California  San Diego
 Google Research 
Jayadev Acharya
 acharya cornell edu  Hirakendu Das  hdas yahooinc com  Alon Orlitsky  alon ucsd edu  Ananda Theertha
Suresh  theertha google com 

Correspondence to 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

applications 

Distance to uniform kp uk   Px         where
  is the uniform distribution over the domain   of    This
distance measure appears in the error of hypothesis testing 
and the uniform distribution is arguably one of the commonest discrete distributions 

  Distribution estimation
Considerable research  over many years  has focused on estimating distribution properties  In the common setting  an
unknown underlying distribution       generates   independent samples     def
              Xn  and the objective is
to estimate   given property       as accurately as possible 
Speci cally  an estimator for   distribution   over   is  
function              mapping observed samples to  
property estimate  The sample complexity of    is the smallest number of samples it requires to estimate   property  
with accuracy   and con dence probability   for all distributions in   collection     

def
 

            
minnn                                Po 

The sample complexity of estimating   is the lowest sample complexity of any estimator 

           min

  

            

By taking the median of about log  
  independent estimators  the error rate can be driven down from   constant to
  Therefore  the sample complexity depends on   only
through   factor of at most log  
    For simplicity  we therefore abbreviate                by             
  Result summary
Recent research has shown that while simple estimators for
the aforementioned properties require sample size   proportional to the support size    more sophisticated techniques need only   sublinear sample size        log   

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

However  each of the problems was approximated via different estimators and analysis techniques  that for some
properties were rather complex 
Motivated by the principle of maximum likelihood  we
show that   single  simple  plugin estimator pro le maximum likelihood  PML   Orlitsky et al      is competitive for estimating any symmetric property  Its sample
complexity is at most quadratically worse than that of any
estimator 
Speci cally  we show that if   symmetric property can be
estimated using   samples with con dence   then the PML
plugin estimator can estimate it using as many samples
with con dence  epn  While this increase may seem high 
note that it is subexponential  We show that if   property
has an estimator that has   small bounded difference constant  how much the estimator changes when we change
one sample  then the error probability reduces exponentially with    Please see Section   Combined  these two
facts imply that for properties with locallysmooth estimators  the PML plugin estimator is optimal up to   constant 
CPML      We then show that all the above properties have locallysmooth estimators  hence they can be estimated by the PML plugin estimator with up to   constant
factor more than the optimal number of samples 

  Outline
The rest of the paper is organized as follows  In Section  
we describe existing results and those shown in this paper 
In Section   we formally de ne the quantities involved and
state the results  In Section   we de ne pro les and PML 
In Section   we outline the new approach  In Section  
we demonstrate auxiliary results for maximum likelihood
estimators  In Section   we outline how we apply maximum likelihood to support  support coverage  entropy  and
uniformity  In Section   we provide the details for support  and support coverage and in the appendix we outline
results for distance to uniformity and entropy 

  Previous and New Results
  Previous Results
Plugin estimation is   general approach for estimating distribution properties  It uses the samples     to  nd an approximation    of    and lets       estimate      
One of the most common distribution estimators  dating
back to Fisher is maximum likelihood  that for clarity we
call sequence maximum likelihood  SML   Aldrich   
To any sample xn it assigns the distribution   that maximizes   xn  The SML estimate is exceedingly simple to
def
derive  The multiplicity Nx
  Nx xn  of symbol   is the
number of times it appears in the sequence xn  The empiri 

cal frequency estimator assigns to each symbol    the fracdef
tion      
  Nx   of times it appears in the sample xn 
For example  if     bananas  empirical frequency would
assign                     and                  
It can be readily shown that SML is exactly the empirical
frequency estimator 
While the SML plugin estimator performs well in the
limit of many samples  sophisticated techniques have recently yielded more accurate estimators for several important symmetric properties 

Support size  With  nitely many samples       cannot
be estimated to any accuracy as many symbols with arbitrarily small probability may not be observed  Motivated by databases  where each entry appears at least
once   Raskhodnikova et al    considered distributions
whose nonzero probabilities are at least  
   

   

 

def

                         

 

 

        log  

def
  Sm     

         

 
log     log   

def
and estimated the normalized support      
        
It can be shown that CSML           
   
Yet  Valiant   Valiant      Wu   Yang    showed
that             
Support coverage  Here too we consider the normalized
coverage  Sm   
 Good   Toulmin 
  proposed the Good Toulmin  GT  estimator that
achieves CGT   Sm             Recently   Orlitsky et al    derived   simple estimator showing that
    Sm         
     Zou et al   
derived   more complex estimator with similar dependence
on   but worse dependence on  
Shannon entropy  Since elements with arbitrarily small
probability can contribute to an arbitrarily high entropy 
     cannot be estimated over aribtrary support with
 nitely many samples  Therefore researchers are mostly
interested in estimating entropy of distributions with support size at most   

 

log     log  

def

  

                  

It can be shown that CSML             
     Paninski    Moreover   Paninski    showed that
             is sublinear in     Valiant   Valiant 
    showed that the optimal dependence on   is    log  
and  Wu   Yang    Jiao et al    obtained the
optimal dependence on both    and   and showed that

 

                  
 
log      
that   kp uk              
log      

  showed that this bound is tight 

Distance to uniform   Valiant   Valiant      showed

  and  Jiao et al 

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

These results are summarized in Table  
Other properties were considered as well 
 BarYossef
et al    Acharya et al    Caferov et al   
Obremski   Skorski    estimated   enyi entropy
and  Bu et al    estimated KL divergence   Canonne 
  surveyed testing whether distributions have certain
properties  and  Jiao et al    studied the performance
of SML estimators for several properties  Closest to this
work in terms of approach and techniques are  Acharya
et al            Valiant   Valiant    Orlitsky   Suresh    that design algorithms whose sample
complexity is provably close to the best possible regardless
of the domain size 

  Pro le Maximum Likelihood
Symmetric distribution properties do not depend on the
symbol labels  They are determined by   simple suf cient
statistic  the number of elements appearing any given number of times  The pro le of   sequence      denoted      
is the multiset of the multiplicities of all the symbols appearing in      For example                           
          as two symbols appearing once  two appearing twice  and one symbol appearing  ve times  removing the association of the individual symbols with the
multiplicities  Pro les are also referred to as histograms
of histograms  Batu et al    histogram order statistics  Paninski    and  ngerprints  Valiant   Valiant 
   
Motivated by the principle of maximum likelihood   Orlitsky et al          discarded the symbol labels  and
considered the pro le maximum likelihood  PML  distribution that maximizes the probability of the observed pro le 
  number of PML properties were established   Orlitsky
et al        proved PML   existence  consistency 
and some of its properties   Orlitsky et al       
Orlitsky   Pan    Pan et al    described additional properties and derived the PML distributions of several short and simple pro les   Orlitsky et al        provide   uni ed review of several of these results   Anevski
et al    contains   combination of previouslyknown
and new results    related distributionestimation approach
is described in  Orlitsky et al       
Several approaches were taken to computing the PML
distribution 
Algebraic computation was considered
in  Acharya et al      combination of the EM and
MCMC algorithms have shown excellent results for calculating the PML distribution and applying it to supportsize
estimation  Orlitsky et al        Pan    and  
summary of some of the results appears in  Orlitsky et al 
     Vontobel      derived the Bethe approximation of these algorithms 

Following the  rst draft of this work   Vatedka   Vontobel    showed that both theoretically and empirically
plugin estimators obtained from the PML estimate yield
good estimates for symmetric functionals of Markov distributions 

  New Results
We show that replacing the SML plugin estimator by PML
yields   uni ed estimator that  like the best results shown
via specialized techniques developed  is optimal 
Theorem   There is   uni ed approach based on PML
distribution that achieves the optimal sample complexity
for the problems of estimating the entropy  support  support coverage  and distance to uniformity 

We prove in Corollary   that the PML approach is competitive with respect to any symmetric property  For symmetric properties  these results are perhaps   justi cation of
Fisher   thoughts on Maximum Likelihood 

 Of course nobody has been able to prove that maximum
likelihood estimates are best under all circumstances  Maximum
likelihood estimates computed with all the information available
may turn out to be inconsistent  Throwing away   substantial part
of the information may render them consistent 

      Fisher   thoughts on Maximum Likelihood  Le Cam   

To prove these PML guarantees  we establish two results
that are of interest on their own right 

  With   samples  PML estimates any symmetric property of   with essentially the same accuracy  and at
most   pn times the error  of any other estimator  This
follows by combining Theorem   with Lemma  

  For   large class of symmetric properties  including
all those mentioned above  if there is an estimator that
uses   samples  and has an error probability   we
design an estimator using      samples  whose error probability is nearly exponential in    We remark
that this decay is much faster than applying the median
trick  This result follows by combining McDiarmid  
inequality with Lemma  

Combined  these results prove that PML plugin estimators
are sampleoptimal 
We also introduce the notion of  approximate ML distributions  described in De nition   These distributions are
more relaxed version of PML  hence may be more easily
computed  yet they provide essentially the same performance guarantees 

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Property name

Entropy

Support size

Support coverage

Distance to  

 
     
    
  
     
   
 Sm   
 
kp   uk    

 

CSML

 
 

  log  
 

 
 
 

  
 

log  

 
 

log   log   
 
 
log   log  
 
 

 

log  

 
 

PML

Theorem   and Section  
Theorem   and Section  
Theorem   and Section  
Theorem   and Section  

Table   Estimation complexity for various properties up to   constant factor  For all properties shown  PML achieves the best known
results up to   constant factor  The details of where the optimal sample complexity was derived for each problem is discussed in
Section  

  Formal De nitions and Results
In the past  different sophisticated estimators were used for
every property in Table   We show that the simple plugin
estimator that uses any PML approximation     has optimal
performance guarantees for all these properties 
In the next theorem  assume   is at least the optimal sample
complexity of estimating entropy  support  support coverage  and distance to uniformity  given in Table   respectively 
Theorem   For all          any plugin exp  pn 

approximate PML    satis es 

Entropy

                              

Support size

               

 

                  

 

   

Support coverage

     Sm              Sm         

Distance to uniformity

     kp   uk             kp   uk       

  PML  Pro le Maximum Likelihood
  Preliminaries
For   sequence      recall that the multilplicity Nx is the
number of times   appears in      Discarding  the labels 
pro le of   sequence  Orlitsky et al      is de ned below  Let    be all pro les of lengthn sequences  Then 
                    In particular    pro le of   lengthn sequence is an unordered
partition of    Therefore      the number of pro les
of lengthn sequences is equal to the partition number of
   Then  by the HardyRamanujan bounds on the partition
number 

For          denote       or       if for some universal

constant             Denote       if both       and       

Lemma    Hardy   Ramanujan   
exp pn 
For   distribution    the probability of   pro le   is de ned
as

   

  

def

  XX       

      

the probability of observing   sequence with pro le   Under        sampling       PX       Qn
     Xi 
For example  the probability of observing   sequence with
pro le         is the probability of observing   sequence with one symbol appearing once  and one symbol
appearing twice    sequence with   symbol   appearing
twice and   appearing once              has probability
         Appropriately normalized  for any    the probability of the pro le     is
nYi 
             

  Xi   

  Xa    

        

 

where the normalization factor is independent of    The
summation is   monomial symmetric polynomial in the
probability values  See  Pan    for more examples 

  PML Estimation Scheme
Recall that pXn is the distribution maximizing the probability of      Similarly  de ne  Orlitsky et al     

  

def
  max
   

  

as the distribution in   that maximizes the probability of
observing   sequence with pro le  
For example  for         For        from  

     arg max

        

  kXa  

Note that in contrast  SML only maximizes one term of this
expression 
We give two examples from the table in  Orlitsky et al 
    to distinguish between SML and PML distributions 

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

and also show an instance where PML outputs distributions
with   larger support than those appearing in the sample 
Example   Let                       Suppose             
then the SML distribution is     However  the distribution in   that maximizes the probability of the pro le
              is     Another example  illustrating the power of PML to predict new symbols is      
         with pro le                   The SML distribution is       but the PML is   uniform distribution over   elements  namely          

Suppose we want to estimate   symmetric property      
of an unknown distribution      given   independent
samples  Our high level approach using PML is described
below 
Input  Class of distributions    symmetric function
    sample    
  Compute      arg maxp         
  Output      

There are   few advantages of this approach  as is true with
any plugin approach      the computation of PML is agnostic to the function   at hand   ii  there are no parameters
to be tuned   iii  techniques such as Poisson sampling or
median tricks are not necessary   iv  well motivated by the
maximumlikelihood principle 

Comparison to the linearprogramming plugin estimator  Valiant   Valiant      Our approach is perhaps closest in  avor to the plugin estimator of  Valiant  
Valiant      Indeed  as mentioned in  Valiant   
their linearprogramming estimator is motivated by the
question of estimating the PML  Their result was the  rst
estimator to provide sample complexity bounds in terms
of the alphabet size  and accuracy the problems of entropy
and support estimation  Before we explain the differences
of the two approaches  we brie   explain their approach 
De ne        to be the number of elements that appear
  times  For example  when                             
            and        Valiant   Valiant     
design   linear program that uses SML for high values of
  and formulate   linear program to  nd   distribution for
which     are close to the observed     They then
plugin this estimate to estimate the property  On the other
hand  our approach  by the nature of ML principle  tries to
 nd the distribution that best explains the entire pro le of
the observed data  not just some partial characteristics  It
therefore has the potential to estimate any symmetric property and estimate the distribution closely in any distance
measures  competitive with the best possible  For example  the guarantees of the linear program approach are suboptimal in terms of the desired accuracy   For entropy

    whereas  Valiant
estimation the optimal dependence is  
    This is more prominent for
  Valiant      yields  
support size and support coverage  which have optimal
dependence of polylog   
    whereas  Valiant   Valiant 
    gives    
  dependence  Besides  we analyze the
 rst method proposed for estimating symmetric properties 
designed from the  rst principles  and show that in fact it
is competitive with the optimal estimators for various problems 

  Proof Outline
Our arguments have two components 
In Section   we
prove   general result for the performance of plugin estimation via maximum likelihood approaches 
Let   be   class of distributions over    and          be
  function  For        let

pz

def
  arg max
   

    

be the maximumlikelihood estimator of   in    Upon observing       pz   is the ML estimator of    In Theorem  
we show that if there is an estimator that achieves error
probability   then the ML estimator has an error probability at most     We note that variations of this result in the
asymptotic statistics were studied before  see  Lehmann  
Casella    Our contribution is to use these results in
the context of symmetric properties and show sample complexity bounds in the nonasymptotic regime 
We emphasize that  throughout this paper   will be the set
of pro les of length    and   will be distributions induced
over pro les by lengthn        samples  Therefore  we
have           By Lemma   if there is   pro le based
estimator with error probability   then the PML approach
will have error probability at most   exp pn  Such arguments were used in hypothesis testing to show the existence
of competitive testing algorithms for fundamental statistical problems  Acharya et al     
At its face value this seems like   weak result  Our second
key step is to prove that for the properties we are interested 
it is possible to obtain very sharp guarantees  For example 
we show that if we can estimate the entropy to an accuracy
  with error probability   using   samples  then we can
estimate the entropy to accuracy   with error probability
exp    using only    samples  Using this sharp concentration  the new error probability term dominates    
and we obtain our results  The arguments for sharp concentration are based on modi cations to existing estimators
and   new analysis  Most of these results are technical and
are in the appendix 

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

  Maximum Likelihood Property Estimation
We establish performance guarantees of ML property estimation in   general setup  Recall that   is   collection
of distributions over    and           Given   sample   from an unknown        we want to estimate      
The maximum likelihood approach is the following twostep procedure 

  Find pZ   arg maxp       
  Output    pZ  

We bound the performance of this approach in the following theorem 
Theorem   Suppose there is an estimator           
such that for any    and       

then

Pr                   

Pr             pZ                  

 

 

By the triangle inequality  for all such   

Proof  Consider symbols with          and       
separately    distribution   with          outputs   with
probability at least   For   to hold  we must have 
By the de nition of ML  pz      

                 
         and for   to hold for pz    pz               
            pz                       pz               

Thus if          then PML satis es the required guarantee with zero probability of error  and any error occurs
only when          We bound this probability as follows 
When       

Pr             Xz       

              

For some problems  it might be easier to just approximate
the ML  instead of  nding it exactly  We de ne an approximation ML as follows 
De nition    approximate ML  Let       For       
 pZ    is    approximate ML distribution if  pz     
    pz     When   is pro les of lengthn     approximate
PML is   distribution     such that             
The next result proves guarantees for any  approximate
ML estimator 
Theorem   Suppose there exists an estimator satisfying   For any      and        any  approximate
ML  pZ satis es 

Pr             pZ               

The proof is very similar to the previous theorem and is
presented in the Appendix   

  Competitiveness of ML via Median Trick
Suppose for   property       there is an estimator with
sample complexity   that achieves an accuracy   with
probability of error at most   The standard method to
boost the error probability is the median trick      Obtain
  log  independent estimates using     log 
independent samples   ii  Output the median of these estimates  This is an  accurate estimator of       with error
probability at most   By de nition  estimators are   mapping from the samples to    However  in many applications
the estimators map from   much smaller  some suf cient
statistic  of the samples  Denote by Zn the space consisting of all suf cient statistics that the estimator uses  For
example  estimators for symmetric properties  such as entropy typically use the pro le of the sequence  and hence
Zn      Using the mediantrick  we get the following
result 
Corollary   Let      Zn     be an estimator of       with
accuracy   and errorprobability   The ML estimator
achieves accuracy   with probability at least   using

min     

  

  log Zn      samples 

Proof  Since   is the number of samples to get error probability   by the Chernoff bound  the error after    samples is at most exp      Therefore 
the error probability of the ML estimator for accuracy   is at
most exp     Zn  which we desire to be at most
 

For estimators that use the pro le of sequences       
exp pn  Plugging this in the previous result shows that
the PML based approach has   sample complexity of at
most      This result holds for all symmetric properties  independent of   and the alphabet size    For the
problems mentioned earlier  something much better in possible  namely the PML approach is optimal up to constant
factors 

  Sample optimality of PML
  Sharp Concentration for Some Properties
To obtain sampleoptimality guarantees for PML  we need
to drive the error probability down much faster than the
median trick  We achieve this by using McDiarmid   inequality stated below  Let               Suppose    gets
  independent samples     from an unknown distribution 
Moreover  changing one of the Xj to any     changed    by

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

at most    Then McDiarmid   inequality  bounded difference inequality   Boucheron et al    states that 

   
nc 

     

Pr                                  exp 

This inequality can be used to show strong error probability
bounds for many problems  We mention   simple application for estimating discrete distributions 
Example   It is well known  Devroye   Lugosi   
that SML requires     samples to estimate   in   distance with probability at least   In this case            

         and therefore    is at most     Using

McDiarmid   inequality  it follows that SML has an error
probability of       exp    while still using    
samples 

Px  Nx

def

Plugging this in  

Let Bn be the bias of an estimator          of       namely
Bn

                       By the triangle inequality 
                 
                                               
  Bn                           
   
Pr                        Bn    exp 

With this in hand  we need to show that    can be bounded
for estimators for the properties we consider  In particular 
we will show that
Lemma   Let     be    xed constant  For entropy 
support  support coverage  and distance to uniformity there
exist pro le based estimators that use the optimal number
of samples  given in Table   have bias   and if we change
any of the samples  changes by at most       
    where   is  
positive constant 

   
nc 

 

We prove this lemma by proposing several modi cations to
the existing sampleoptimal estimators  The modi ed estimators will preserve the sample complexity up to constant
factors and also have   small    The proof details are given
in the appendix 
Using   with Lemma  
Theorem   Let   be the optimal sample complexity of estimating entropy  support  support coverage and distance to
uniformity  given in table   and   be   large positive con 

stant  Let          then any for any   exp  pn 

the  PML estimator estimates entropy  support  support

coverage  and distance to uniformity to an accuracy of  

with probability at least     exp pn 
Proof  Let       By Lemma   for each property of
interest  there are estimators based on the pro les of the
samples such that using nearoptimal number of samples 
they have bias   and maximum change if we change any of
the samples is at most       for some constant    Hence 
by McDiarmid   inequality  an accuracy of   is achieved

with probability at least   exp      Now

suppose    is any  approximate PML distribution  Then by
Theorem  

Pr                  

 

       
  exp         pn 
 
  exp pn 

 

where in the last step we used         pn  and  
exp pn 
  Support and Support Coverage
We analyze both support coverage and the support estimation via   single approach  We  rst start with support coverage  Recall that the goal is to estimate Sm    the expected
number of distinct symbols that we see after observing  
samples from    By the linearity of expectation 

Sm      Xx  

  INx Xm    Xx  

                

The problem is closely related to the support coverage
problem  Orlitsky et al    where the goal is to estimate Ut      the number of new distinct symbols that we
observe in       additional samples  Hence
        Ut 

Sm         nXi 

where               We show that the modi cation of
an estimator in  Orlitsky et al    is also nearoptimal
and satis es conditions in Lemma   We propose to use the
following estimator

 

     

 Sm     

       SGT

nXi 
       Pn
where   SGT
          Pr        and   is  
Poisson random variable with mean   and              
The above theorem also works for any         for

 

any    

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

We remark that the proof also holds for Binomial smoothed
random variables as discussed in  Orlitsky et al   
We need to bound the maximum coef cient and the bias to
apply Lemma   We  rst bound the maximum coef cient
of this estimator 
Lemma   For all        the maximum coef cient of
 Sm    is at most     er   

Proof  For any    the coef cient of    is          Pr    
   It can be upper bounded as    Pt
     
er   

    rt  

  

  

  Support Estimator
Recall that the quantity of interest in support estimation is
      which we wish to estimate to an accuracy of  

Proof of Lemma   for support  Note that we are interested
in distributions with all the non zero probabilities are at
least     We propose to estimate       using  Sm     
for       log  
    then by
   
Lemma   the maximum coef cient of  Sm     is at most
     
  is at most
 
          
To bound the bias  note that for this choice of  

    which for    

If we choose     log  

  log    log   

  log  

 

The next lemma bounds the bias of the estimator 
Lemma   For all        the bias of the estimator is
bounded by

     Sm      Sm         er      min            

           Sm     Xx
 Xx

Similarly  by Lemma  

         
  mp      ke  log  

   

  
 

 

Proof  As before let              

    Sm      Sm   
nXi 
           SGT

 

 

       Xx  
              
                        

      SGT

 

       Xx  

Hence by Lemma   and Corollary   in  Orlitsky et al 
  we get

     Sm      Sm     er      min            

Using the above two lemmas we prove results for both the
observed support coverage and support estimator 

  Support Coverage Estimator
Recall that the quantity of interest in support coverage estimation is Sm      which we wish to estimate to an accuracy of  

Proof of Lemma   for observed  If we choose     log  
   
then by Lemma   the maximum coef cient of  Sm    
is at most  
is at
most            Similarly  by Lemma  

    which for         log   

     

  log  

log 

 
      Sm      Sm   
for all        

 
 

     er      me       

 
      Sm          
 
      Sm      Sm     
 
     er      ke     
 

 
         Sm   
 
     

 
 

for all        

  Discussion and Future Directions
We studied estimation of symmetric properties of discrete
distributions using the principle of maximum likelihood 
and proved optimality of this approach for   number of
problems    number of directions are of interest  We believe that the lower bound requirement on   is perhaps an
artifact of our proof technique  and that the PML based approach is indeed optimal for all ranges of   Approximation
algorithms for estimating the PML distributions would be
  fruitful direction to pursue  Given our results  approximations stronger than exp    would be very interesting  In the particular case when the desired accuracy is  
constant  even an exponential approximation would be suf 
 cient for many properties  We plan to apply the heuristics proposed by  Vontobel    for various problems we
consider  and compare with the state of the art provable
methods 

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Acknowledgements
The authors thank the reviewers for valuable feedback and
the NSF for support through grants CIF  CIF 
  CRIICIF  and   Cornell University
startup grant  Jayadev Acharya thanks Clement Canonne 
Jiantao Jiao  and Pascal Vontobel for interesting discussions 

References
Acharya  Jayadev  Das  Hirakendu  Mohimani  Hosein  Orlitsky  Alon  and Pan  Shengjun  Exact calculation of
pattern probabilities  In ISIT  pp       

Acharya  Jayadev  Das  Hirakendu  Jafarpour  Ashkan  Orlitsky  Alon  and Pan  Shengjun  Competitive closeness
testing  COLT     

Acharya  Jayadev  Das  Hirakendu  Jafarpour  Ashkan 
Suresh 
Competitive classi cation and

Orlitsky  Alon 
Ananda Theertha 
closeness testing  In COLT   

Shengjun 

Pan 

and

Acharya  Jayadev  Jafarpour  Ashkan  Orlitsky  Alon  and
Suresh  Ananda Theertha  Optimal probability estimation with applications to prediction and classi cation  In
COLT     

Acharya  Jayadev  Jafarpour  Ashkan  Orlitsky  Alon  and
Suresh  Ananda Theertha    competitive test for uniformity of monotone distributions  In AISTATS     

Acharya 

Orlitsky 

Jayadev 
Ananda Theertha 
complexity of estimating   enyi entropy 
 

and Tyagi  Himanshu 

Alon 

Suresh 
The
In SODA 

Aldrich  John        sher and the making of maximum likelihood   Statistical Science   
   

Anevski  Dragi  Gill  Richard    and Zohren  Stefan  Estimating   probability mass function with unknown labels 
arXiv preprint arXiv   

BarYossef  Ziv  Kumar  Ravi  and Sivakumar     Sampling
algorithms  lower bounds and applications  In Symposium on Theory of computing  pp    ACM   

Batu  Tugkan  Fortnow  Lance  Rubinfeld  Ronitt  Smith 
Warren    and White  Patrick  Testing that distributions
are close  In FOCS  pp     

Boucheron     Lugosi     and Massart     Concentration
Inequalities    Nonasymptotic Theory of Independence 
OUP Oxford   

Bu  Yuheng  Zou  Shaofeng  Liang  Yingbin  and Veeravalli  Venugopal    Estimation of KL divergence between largealphabet distributions  In ISIT   

Caferov  Cafer  Kaya  Bar    ODonnell  Ryan  and Say 
AC Cem  Optimal bounds for estimating entropy with
pmf queries 
In International Symposium on Mathematical Foundations of Computer Science  pp   
Springer   

Cai    Tony  Low  Mark    et al  Testing composite hypotheses  hermite polynomials and optimal estimation of
  nonsmooth functional  The Annals of Statistics   
   

Canonne  Cl ement      survey on distribution testing 
Your data is big  but is it blue  Electronic Colloquium
on Computational Complexity  ECCC     

Colwell  Robert    Chao  Anne  Gotelli  Nicholas   
Lin  ShangYi  Mao  Chang Xuan  Chazdon  Robin   
and Longino  John    Models and estimators linking
individualbased and samplebased rarefaction  extrapolation and comparison of assemblages  Journal of plant
ecology     

Cover  Thomas    and Thomas  Joy    Elements of infor 

mation theory   ed  Wiley   

Devroye  Luc and Lugosi    abor  Combinatorial methods

in density estimation  Springer   

Good  IJ and Toulmin  GH  The number of new species 
and the increase in population coverage  when   sample
is increased  Biometrika     

Hardy  Godfrey   and Ramanujan  Srinivasa  Asymptotic
formula  in combinatory analysis  Proceedings of the
London Mathematical Society     

Jiao  Jiantao  Venkat  Kartik  Han  Yanjun  and Weissman  Tsachy  Maximum likelihood estimation of
functionals of discrete distributions 
arXiv preprint
arXiv   

Jiao  Jiantao  Venkat  Kartik  Han  Yanjun  and Weissman 
Tsachy  Minimax estimation of functionals of discrete
distributions  IEEE Transactions on Information Theory 
   

Jiao  Jiantao  Han  Yanjun  and Weissman  Tsachy  Minimax estimation of the    distance  In ISIT  pp   
 

Le Cam  Lucien Marie  Maximum likelihood  an introduc 

tion  JSTOR   

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Lehmann  Erich Leo and Casella  George  Theory of point
estimation  volume   Springer Science   Business
Media   

Obremski  Maciej and Skorski  Maciej  Renyi entropy es 

timation revisited  In APPROX   

Orlitsky     Pan     Sajama  Santhanam     Viswanathan 
   and Zhang     Pattern maximum likelihood  Computation and experiments  Arxiv     

Orlitsky  Alon and Pan  Shengjun  The maximum likeli 

hood probability of skewed patterns  In ISIT   

Orlitsky  Alon and Suresh  Ananda Theertha  Competitive
In

distribution estimation  Why is goodturing good 
NIPS  pp     

Orlitsky  Alon  Santhanam  Narayana    and Zhang  Junan 
Always good turing  Asymptotically optimal probability
estimation  In FOCS   

Orlitsky  Alon  Sajama     Santhanam  NP  Viswanathan 
   and Zhang  Junan  Algorithms for modeling distributions over large alphabets  In ISIT     

Orlitsky  Alon  Santhanam  Narayana    Viswanathan  Krishnamurthy  and Zhang  Junan  On modeling pro les
instead of values  In UAI     

Orlitsky  Alon  Santhanam  Narayana    and Zhang  Junan  Universal compression of memoryless sources over
unknown alphabets  IEEE Transactions on Information
Theory       

Orlitsky 

Alon 

Santhanam 

Viswanathan  Krishna  and Zhang 
 size  and order in distribution modeling     

Junan 

Narayana

Prasad 
Low

Orlitsky  Alon  Santhanam  Narayana  Viswanathan  Krishnamurthy  and Zhang  Junan  Convergence of pro 
 le based estimators  In Proceedings of the   IEEE
International Symposium on Information Theory  ISIT 
pp       

Orlitsky 

Alon 

Santhanam 

Narayana
Viswanathan  Krishna  and Zhang  Junan 
retical and experimental
probabilities  In Information Theory Workshop   

Prasad 
Theoresults on modeling low

Orlitsky  Alon  Suresh  Ananda Theertha  and Wu  Yihong 
Optimal prediction of the number of unseen species 
Proceedings of the National Academy of Sciences   
doi   pnas 

Orlitsky  Alon  Santhanam  Narayana  Viswanathan  Krishnamurthy  and Zhang  Junan  On estimating the probability multiset part ii  Properties of the pattern maximum
likelihood estimator  Arxiv     

Pan  Shengjun  On the theory and application of pattern
maximum likelihood  PhD thesis  UC San Diego   

Pan  Shengjun  Acyarya  Jayadev  and Orlitsky  Alon  The
maximum likelihood probability of uniquesingleton 
ternary  and length  patterns  pp     

Paninski  Liam  Estimation of entropy and mutual infor 

mation  Neural computation     

Raskhodnikova  Sofya  Ron  Dana  Shpilka  Amir  and
Smith  Adam  Strong lower bounds for approximating
distribution support size and the distinct elements problem  SIAM Journal on Computing     

Timan        Theory of Approximation of Functions of  

Real Variable  Pergamon Press   

Valiant  Gregory and Valiant  Paul  Estimating the unseen  an   log   sample estimator for entropy and support size  shown optimal via new clts  In STOC     

Valiant  Gregory and Valiant  Paul  The power of linear

estimators  In FOCS  pp    IEEE     

Valiant  Gregory and Valiant  Paul 

Instanceby instance
optimal identity testing  Electronic Colloquium on Computational Complexity  ECCC     

Valiant  Gregory John  Algorithmic approaches to statistical questions  PhD thesis  University of California 
Berkeley   

Vatedka  Shashank and Vontobel  Pascal    Pattern maximum likelihood estimation of  nitestate discretetime
markov chains  In ISIT   

Vontobel  Pascal    The bethe approximation of the pattern maximum likelihood distribution  In IEEE ISIT  pp 
   

Vontobel  Pascal    The bethe and sinkhorn approximations of the pattern maximum likelihood estimate and
their connections to the valiantvaliant estimate  In Information Theory and Applications Workshop  ITA  pp 
   

Wu  Yihong and Yang  Pengkun  Chebyshev polynomials 
moment matching  and optimal estimation of the unseen 
CoRR  abs   

Orlitsky  Alon  Santhanam  Narayana  Viswanathan  Krishnamurthy  and Zhang  Junan  On estimating the probability multiset part    The pattern maximum likelihood
approach  Arxiv     

Wu  Yihong and Yang  Pengkun  Minimax rates of entropy estimation on large alphabets via best polynomial
approximation  IEEE Trans  Information Theory   
   

  Uni ed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Zou  James  Valiant  Gregory  Valiant  Paul  Karczewski 
Konrad  Chan  Siu On  Samocha  Kaitlin  Lek  Monkol 
Sunyaev  Shamil  Daly  Mark  and MacArthur  Daniel   
Quantifying unobserved proteincoding variants in human populations provides   roadmap for largescale sequencing projects  Nature Communications    EP 
   

white

