ExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithmsJialeiWang LinXiao AbstractWeconsiderempiricalriskminimizationoflinearpredictorswithconvexlossfunctions Suchproblemscanbereformulatedasconvexconcavesaddlepointproblemsandsolvedbyprimal dual rstorderalgorithms However primaldualal gorithmsoftenrequireexplicitstronglyconvexregularizationinordertoobtainfastlinearconvergence andtherequireddualproximalmappingmaynotadmitclosed formoref cientsolution Inthispaper wedevelopbothbatchandrandomizedprimaldualalgorithmsthatcanex ploitstrongconvexityfromdataadaptivelyandarecapableofachievinglinearconvergenceevenwithoutregularization Wealsopresentdualfreevariantsofadaptiveprimal dualalgorithmsthatdonotneedthedualproximalmapping whichareespeciallysuitableforlogisticregression IntroductionWeconsidertheproblemofregularizedempiricalriskminimization ERM oflinearpredictors Leta an Rdbethefeaturevectorsofndatasamples     RbeaconvexlossfunctionassociatedwiththelinearpredictionaTix fori   andg Rd Rbeaconvexregularizationfunctionforthepredictorx Rd ERMamountstosolvingthefollowingconvexoptimizationproblem minx RdnP   def nPni   aTix       Thisformulationcoversmanywellknownclassi cationandregressionproblems Forexample logisticregressionisobtainedbysetting     log exp biz wherebi Forlinearregressionproblems theloss DepartmentofComputerScience TheUniversityofChicago Chicago Illinois USA MicrosoftResearch Redmond Washington USA Correspondenceto JialeiWang jialei uchicago edu LinXiao lin xiao microsoft com Proceedingsofthe thInternationalConferenceonMachineLearning Sydney Australia PMLR Copyright bytheauthor   functionis       bi andwegetridgeregressionwithg   kxk andtheelasticnetwithg   kxk kxk LetA   an Tbethenbyddatamatrix Throughoutthispaper wemakethefollowingassumptions Assumption Thefunctions   gandmatrixAsatisfy Each iis stronglyconvexand smoothwhere and and gis stronglyconvexwhere where   min ATA ThestrongconvexityandsmoothnessmentionedabovearewithrespecttothestandardEuclideannorm denotedaskxk xTx See     Nesterov Sections and fortheexactde nitions Weallow whichsimplymeans iisconvex LetR maxi kaik andassuming thenR isapopularde nitionofconditionnumberforanalyzingcomplexitiesofdifferentalgorithms ThelastconditionabovemeansthattheprimalobjectivefunctionP   isstronglyconvex evenif Therehavebeenextensiveresearchactivitiesinrecentyearsondevelopingef cientlyalgorithmsforsolvingproblem Abroadclassofrandomizedalgorithmsthatexploitthe nitesumstructureintheERMproblemhaveemergedasverycompetitivebothintermsoftheoreticalcomplexityandpracticalperformance Theycanbeputintothreecategories primal dual andprimaldual PrimalrandomizedalgorithmsworkwiththeERMproblem directly Theyaremodernversionsofrandomizedincrementalgradientmethods     Bertsekas Nedic Bertsekas equippedwithvariancereductiontechniques EachiterationofsuchalgorithmsonlyprocessonedatapointaiwithcomplexityO   TheyincludesSAG Rouxetal SAGA Defazioetal andSVRG Johnson Zhang Xiao Zhang whichallachievetheiterationcomplexityO cid     log cid to ndan optimalsolution Infact theyarecapableofexploitingthestrongconvexityfromdata meaningthattheconditionnumberR inthecomplexitycanbereplacedbythemorefavorableoneR   Thisimprovementcanbeachievedwithoutexplicitknowledgeof fromdata ExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithmsDualalgorithmssolveFencheldualof bymaximizingD   def nPni   yi   cid nPni yiai cid usingrandomizedcoordinateascentalgorithms Here iandg denotestheconjugatefunctionsof iandg TheyincludeSDCA ShalevShwartz Zhang Nesterov andRicht arik Tak     TheyhavethesamecomplexityO cid     log cid butcannotexploitstrongconvexity ifany when fromdata Primaldualalgorithmssolvetheconvex concavesaddlepointproblemminxmaxyL     whereL     def nPni cid yihai xi   yi cid     Inparticular SPDC Zhang Xiao achievesanacceleratedlinearconvergenceratewithiterationcomplexityO cid   nR log cid whichisbetterthantheaforementionednonacceleratedcomplexitywhenR   Lan Zhou developeddualfreevariantsofacceleratedprimal dualalgorithms butwithoutconsideringthelinearpredictorstructureinERM Balamurugan Bach extendedSVRGandSAGAtosolvingsaddlepointproblems Acceleratedprimalanddualrandomizedalgorithmshavealsobeendeveloped Nesterov Fercoq Richt arik andLinetal   developedacceleratedcoordinategradientalgorithms whichcanbeappliedtosolvethedualproblem AllenZhu developedanacceleratedvariantofSVRG AccelerationcanalsobeobtainedusingtheCatalystframework Linetal   TheyallachievethesameO cid   nR log cid complexity Acommonfeatureofacceleratedalgorithmsisthattheyrequiregoodestimateofthestrongconvexityparameter Thismakeshardforthemtoexploitstrongconvexityfromdatabecausetheminimumsingularvalue ofthedatamatrixAisveryhardtoestimateingeneral Inthispaper weshowthatprimaldualalgorithmsareca pableofexploitingstrongconvexityfromdataifthealgorithmparameters suchasstepsizes aresetappropriately Whiletheseoptimalsettingdependsontheknowledgeoftheconvexityparameter fromthedata wedevelopadaptivevariantsofprimal dualalgorithmsthatcantunetheparameterautomatically Suchadaptiveschemesrelycriticallyonthecapabilityofevaluatingtheprimal dualoptimalitygapsbyprimal dualalgorithms Amajordisadvantageofprimaldualalgorithmsisthattherequireddualproximalmappingmaynotadmitclosed formoref cientsolution WefollowtheapproachofLan Zhou toderivedualfreevariantsoftheprimal dualalgorithmscustomizedforERMproblemswiththelinearpredictorstructure andshowthattheycanalsoexploitstrongconvexityfromdatawithcorrectchoicesofparametersorusinganadaptationscheme Algorithm BatchPrimalDual BPD Algorithminput parameters initialpoint       fort doy   prox   cid           cid     prox   cid     ATy   cid                 endfor BatchprimaldualalgorithmsWe rststudybatchprimaldualalgorithms byconsideringa batch versionoftheERMproblem minx Rd cid     def   Ax     cid whereA Rn   Wemakethefollowingassumptions Assumption Thefunctionsf gandmatrixAsatisfy fis stronglyconvexand smoothwhere and and gis stronglyconvexwhere where   min ATA Usingconjugatefunctions wecanderivethedualof asmaxy Rn cid     def       ATy cid andtheconvexconcavesaddlepointformulationisminx Rdmaxy Rn cid       def     yTAx     cid Weconsidertheprimaldual rstorderalgorithmproposedbyChambolle Pock forsolvingthesaddlepointproblem giveninAlgorithm whereprox foranyconvexfunction Rn isde nedasprox argmin Rn cid     cid Assumingthatfissmoothandgisstronglyconvex Chambolle Pock showedthatAlgorithm achievesacceleratedlinearconvergencerateif However theydidnotconsiderthecasewhereadditionalorthesolesourceofstrongconvexitycomesfromf Ax Inthefollowingtheorem weshowhowtosettheparameters and toexploitbothsourcesofstrongconvexitytoachievefastlinearconvergence Theorem SupposeAssumption holdsand     istheuniquesaddlepointofLde nedin LetL kAk   max ATA IfwesettheparametersinAlgorithm as Lq Lq and max     where   cid   cid   ExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithmsthenwehave cid cid kx       ky       tC                 tC whereC cid cid kx     cid cid ky     TheproofofTheorem isgiveninAppendicesBandC Herewegiveadetailedanalysisoftheconvergencerate Substituting and in intotheexpressionsfor yand xin andassuming   wehave     cid   cid Lq       SincetheoverallconditionnumberoftheproblemisL itisclearthat yisanacceleratedconvergencerate Nextweexamine xintwospecialcases Thecaseof but Inthiscase wehave Lp and Lq andthus             Thereforewehave max       Thisindeedisanacceleratedconvergencerate recoveringtheresultofChambolle Pock Thecaseof but Inthiscase wehave     and Lq and           Noticethat   istheconditionnumberoff Ax Nextweassume Landexaminehow xvarieswith If   meaningfisbadlyconditioned then         Becausetheoverallconditionnumberis   thisisanacceleratedlinearrate andsois max     If   meaningfismildlyconditioned then           Thisrepresentsahalfacceleratedrate becausetheoverallconditionnumberis     If     fisasimplequadraticfunction then         Thisratedoesnothaveacceleration becausetheoverallconditionnumberis     Algorithm AdaptiveBatchPrimalDual AdaBPD input problemconstants Land initialpoint     andadaptationperiodT Compute and asin and using fort doy   prox   cid           cid     prox   cid     ATy   cid                 ifmod     then BPDAdapt cid                 cid endifendforAlgorithm BPDAdapt simpleheuristic input previousestimate adaptionperiodT primalanddualobjectivevalues         ts   TifP                     then else endifCompute and asin and using output newparameters Insummary theextentofaccelerationinthedominatingfactor   whichdetermines dependsontherelativesizeof and       therelativeconditioningbetweenthefunctionfandthematrixA Ingeneral wehavefullaccelerationif   Thetheorypredictsthattheaccelerationdegradesasthefunctionfgetsbetterconditioned However inournumericalexperiments weoftenobserveaccelerationevenif getscloserto AsexplainedinChambolle Pock Algorithm isequivalenttoapreconditionedADMM Deng Yin characterizedvariousconditionsforADMMtoobtainlinearconvergence butdidnotderivetheconvergencerateforthecaseweconsiderinthispaper AdaptivebatchprimaldualalgorithmsInpractice itisoftenveryhardtoobtainagoodestimateoftheproblem dependentconstants especially   min ATA inordertoapplythealgorithmicparametersspeci edinTheorem Hereweexploreheuristicsthatcanenableadaptivetuningofsuchparameters whichoftenleadtomuchimprovedperformanceinpractice AkeyobservationisthattheconvergencerateoftheBPDalgorithmchangesmonotonicallywiththeoverallconvexityparameter regardlessoftheextentofacceleration Inotherwords thelarger is thefastertheconvergence Therefore ifwecanmonitortheprogressofExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithmsAlgorithm BPDAdapt robustheuristic input previousrateestimate periodT constantsc andc and         ts   TComputenewrateestimate                     if   then elseif   then else endif Lq Lq Compute using orset output newparameters theconvergenceandcompareitwiththepredictedconvergencerateinTheorem thenwecanadjusttheestimatedparameterstoexploitstrongconvexityfromdata Morespeci cally iftheobservedconvergencerateisslowerthanthepredictedrate thenweshouldreducetheestimateof otherwiseweshouldincrease forfasterconvergence WeformalizetheabovereasoninginAlgorithm calledAdaBPD Thisalgorithmmaintainsanestimate ofthetrueconstant andadjustiteveryTiterations WeuseP   andD   torepresenttheprimalanddualobjectivevaluesatP     andD     respectively WegivetwoimplementationsofthetuningprocedureBPDAdapt Algorithm isasimpleheuristicfortuningtheestimate wheretheincreasinganddecreasingfactor canbechangedtoothervalueslargerthan Algorithm isamorerobustheuristic Itdoesnotrelyonthespeci cconvergencerate establishedinTheorem Instead itsimplycomparesthecurrentestimateofobjectivereductionrate withthepreviousestimate Italsospeci esanontuningrangeofchangesin speci edbytheinterval     Thecapabilityofaccessingboththeprimalanddualobjectivevaluesallowsprimal dualalgorithmstohavegoodestimateoftheconvergencerate whichenableseffectivetuningheuristics Automatictuningofprimaldualalgo rithmshavealsobeenstudiedby     Malitsky Pock andGoldsteinetal butwithdifferentgoals RandomizedprimaldualalgorithmInthissection wecomebacktotheERMproblemandconsideritssaddle pointformulationin Duetoits nitesumstructureinthedualvariablesyi wecandeveloperandomizedalgorithmstoexploitstrongconvexityfromdata Inparticular weextendthestochasticprimaldualcoordi nate SPDC algorithmbyZhang Xiao SPDCisAlgorithm AdaptiveSPDC AdaSPDC input parameters initialpoint     andadaptationperiodT Set     fort dopickk   uniformlyatrandomfori   doifi ktheny     prox   cid       aTk     cid elsey         iendifendforx   prox   cid     cid                 ak cid cid                       ak                 ifmod       then SPDCAdapt cid     sn     sn Ts cid endifendforaspecialcaseoftheAdaSPDCalgorithminAlgorithm bysettingtheadaptionperiodT noadaption ThefollowingtheoremisprovedinAppendixE Theorem SupposeAssumption holds Let     bethesaddlepointofthefunctionLde nedin andR max ka   kank IfwesetT inAlgorithm noadaption andlet Rq   Rqn and max     where   cid   cid       thenwehave cid cid   cid kx       cid   cid ky       cid tC   cid                 cid tC whereC cid cid kx     cid cid ky     TheexpectationE istakenwithrespecttothehistoryofrandomindicesdrawnateachiteration BelowwegiveadetaileddiscussionontheexpectedconvergencerateestablishedinTheorem Thecasesof but Inthiscasewehave Rp   and Rqn and                   ExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithmsHence   TheserecovertheparametersandconvergencerateofthestandardSPDC Zhang Xiao Thecasesof but Inthiscasewehave     and Rq and     nR       nR nR cid   cid SincetheobjectiveisR smoothand nstronglyconvex yisanacceleratedrateif   otherwise     For   weconsiderdifferentsituations If   thenwehave   nR whichisanacceleratedrate Sois max     If Rand   then   nR whichrepresentsanacceleratedrate TheiterationcomplexityofSPDCiseO nR whichisbetterthanthatofSVRGinthiscase whichiseO nR If Rand   thenweget   nR Thisisahalfacceleratedrate becauseinthiscaseSVRGrequireseO nR iterations versuseO nR forSPDC If Rand meaningthe   sarewellconditioned thenweget   nR nR whichisanonacceleratedrate ThecorrespondingiterationcomplexityisthesameasSVRG ParameteradaptationforSPDCTheSPDCAdaptprocedurecalledinAlgorithm followsthesamelogicsasthebatchadaptionschemesinAlgorithms and andweomitthedetailshere OnethingweemphasizehereisthattheadaptationperiodTisintermsofepochs ornumberofpassesoverthedata Inaddition weonlycomputetheprimalanddualobjectivevaluesaftereachpassoreveryfewpasses becausecomputingthemexactlyusuallyneedtotakeafullpassofthedata Unlikethebatchcasewherethedualitygapdecreasesmonotonically thedualitygapforrandomizedalgorithmscan uctuatewildly SoinsteadofusingonlythetwoendvaluesP   Tn     Tn andP       wecanusemorepointstoestimatetheconvergenceratethroughalinearregression SupposetheprimaldualobjectivevaluesforthelastT passesare                 andweneedtoestimate rateperpass suchthatP         cid     cid     Wecanturnitintoalinearregressionproblemaftertakinglogarithmandobtaintheestimate throughlog   PTt tlogP           Algorithm DualFreeBPDAlgorithminput parameters initialpoint     Set     andv     fort dov                           prox   cid     ATy   cid                 endfor DualfreePrimal dualalgorithmsComparedwithprimalalgorithms onemajordisadvantageofprimaldualalgorithmsistherequirementofcomputingtheproximalmappingofthedualfunctionf or   whichmaynotadmitclosedformedsolutionoref cientcomputation Thisisespeciallythecaseforlogisticregression oneofthemostpopularlossfunctionsusedinclassi cation Lan Zhou developed dualfree variantsofprimaldualalgorithmsthatavoidcomputingthedualprox imalmapping TheirmaintechniqueistoreplacetheEuclideandistanceinthedualproximalmappingwithaBreg mandivergencede nedovertheduallossfunctionitself WeshowhowtoapplythisapproachtosolvethestructuredERMproblemsconsideredinthispaper Theycanalsoexploitstrongconvexityfromdataifthealgorithmicparametersaresetappropriatelyoradaptedautomatically DualfreeBPDalgorithmFirst weconsiderthebatchsetting Wereplacethedualproximalmapping computingy   inAlgorithm withy   argminy cid     yTA             cid whereDistheBregmandivergenceofastrictlyconvexkernelfunctionh de nedasDh                                 Algorithm isobtainedintheEuclideansettingwithh   kyk andD       ky       Hereweusef asthekernelfunction andshowthatitallowsustocomputey   in veryef ciently Thefollowinglemmaexplainsthedetails Cf Lan Zhou Lemma Lemma Letthekernelh   intheBregmandivergenceD Ifweconstructasequenceofvectors     suchthatv     andforallt               thenthesolutiontoproblem isy         Proof Supposev         truefort thenD                             ExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithmsThesolutionto canbewrittenasy   argminynf   yTA     cid         Ty cid   argminyn cid cid     cid           cid Tyo argmaxyn cid           cid Ty       argmaxynv   Ty             whereinthelastequalityweusedthepropertyofconjugatefunctionwhenfisstronglyconvexandsmooth Moreover                 whichcompletestheproof AccordingtoLemma weonlyneedtoprovideinitialpointssuchthatv     iseasytocompute Wedonotneedtocompute       directlyforanyt becauseitiscanbeupdatedasv   in Consequently wecanupdatey   intheBPDalgorithmusingthegradientf     withouttheneedofdualproximalmapping TheresultingdualfreealgorithmisgiveninAlgorithm Theorem SupposeAssumption holdsandlet     betheuniquesaddlepointofLde nedin IfwesettheparametersinAlgorithm as Lq Lp and max     where   cid cid   thenwehave cid cid kx               tC                 tC whereC cid cid kx     cid cid       Theorem isprovedinAppendicesBandD Assuming   wehave     Lq     Again wegaininsightsbyconsiderthespecialcases If and then   Land     So max     isanacceleratedrate If and then   Land     Thus max       isnotaccelerated Thisconclusiondoesnotdependsontherelativesizesof and   anditisthemajordifferencefromtheEuclideancaseinSection Algorithm AdaptiveDualFreeSPDC ADFSPDC input parameters initialpoint     andadaptationperiodT Set     andv         fori nfort dopickk   uniformlyatrandomfori   doifi kthenv           aTk                   elsev                     iendifendforx   prox   cid     cid                 ak cid cid                       ak                 ifmod       then SPDCAdapt cid     sn     sn Ts cid endifendforIfboth and thentheextentofaccelerationdependsontheirrelativesize If isonthesameorderas orlarger thenacceleratedrateisobtained If ismuchsmallerthan thenthetheorypredictsnoacceleration DualfreeSPDCalgorithmFollowingthesameapproach wecanderiveanAdaptiveDual FreeSPDCalgorithm giveninAlgorithm Onrelatedwork ShalevShwartz Zhang and ShalevShwartz introduceddualfreeSDCA Thefollowingtheoremcharacterizesthechoiceofalgorithmicparametersthatcanexploitstrongconvexityfromdatatoachievelinearconvergence proofgiveninAppendixF Theorem SupposeAssumption holds Let     bethesaddlepointofLin andR max ka   kank IfwesetT inAlgorithm nonadaption andlet Rp   Rq   and max     where   cid   cid       thenwehave cid cid   cid kx       cid   cid         cid tC   cid                 cid tC whereC cid cid kx     cid cid       ExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithms NumberofpassesAdaBPDOpt BPDBPDPrimalAGPrimaloptimalitygap Numberofpasses Numberofpassessynthetic nsynthetic nsynthetic nFigure Comparisonofbatchprimaldualalgorithmsforaridgeregressionproblemwithn andd NumberofpassesADFSPDCDF SPDCSPDCSVRGSAGAKatyushaPrimaloptimalitygap Numberofpasses Numberofpassescpuact ncpuact ncpuact nFigure Comparisonofrandomizedalgorithmsforridgeregressionproblems NowwediscusstheresultsofTheorem infurtherdetails Thecasesof but Inthiscasewehave Rp   and     and               TherateisthesameasforSPDCinZhang Xiao Thecasesof but Inthiscasewehave     and   thus     nR           nR WenotethattheprimalfunctionnowisR smoothand nstronglyconvex Wediscussthefollowingcases If   thenwehave   nRand     Therefore max       Otherwise wehave   nR and yisofthesameorder Thisisnotanacceleratedrate andwehavethesameiterationcomplexityasSVRG Finally wegiveconcreteexamplesofhowtocomputetheinitialpointsy andv suchthatv         Forsquaredloss   bi and   bi Sov             bi Forlogisticregression wehavebi and   log   bi Theconjugatefunctionis   bi log bi bi log bi ifbi and otherwise Wecanchoosey   biandv   suchthatv         Forlogisticregression wehave overthefulldomainof   However each iislocallystronglyconvexinboundeddomain Bach ifz     thenweknow minz     exp   ThereforeitiswellsuitableforanadaptationschemesimilartoAlgorithm thatdonotrequireknowledgeofeither or PreliminaryexperimentsWepresentpreliminaryexperimentstodemonstratetheeffectivenessofourproposedalgorithms First weconsiderbatchprimaldualalgorithmsforridgeregressionoverasyntheticdataset ThedatamatrixAhassizesn andd anditsentriesaresampledfrommultivariatenormaldistributionwithmeanzeroandcovari ancematrix ij     WenormalizealldatasetsExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithms NumberofpassesADFSPDCDF SPDCSPDCSVRGSAGAKatyushaPrimaloptimalitygap Numberofpasses Numberofpassessynthetic nsynthetic nsynthetic   NumberofpassesADFSPDCDF SPDCSPDCSVRGSAGAKatyushaPrimaloptimalitygap Numberofpasses Numberofpassesrcv nrcv nrcv nFigure Comparisonofrandomizedalgorithmsforlogisticregressionproblems suchthatai ai maxjkajk toensurethemaximumnormofthedatapointsis Weuse regularizationg   kxk withthreechoicesofparameter   nand   whichrepresentthestrong medium andweaklevelsofregularization respectively Figure showstheperformanceoffourdifferentalgorithms theprimalacceleratedgradient PrimalAG algorithm Nesterov using asstrongconvexityparameter theBPDalgorithm Algorithm thatusesthesame and theoptimalBPDalgorithm OptBPD thatuses min ATA   ncomputedfromdata andtheAdaBPDalgorithm Algorithm withtherobustadaptationheuristic Algorithm withT   andc Asexpected theperformanceofPrimalAGisverysimilartothatofBPD andOptBPDhasthefastestconvergence TheAdaBPDalgorithmcanpartiallyexploitstrongconvexityfromdatawithoutknowledgeof NextwecompareDFSPDC Algorithm withoutadaption andADFSPDC Algorithm withadaption againstseveralstateof theartrandomizedalgorithmsforERM SVRG Johnson Zhang SAGA Defazioetal Katyusha AllenZhu andthestandardSPDCmethod Zhang Xiao ForSVRGandKatyusha anacceleratedvariantofSVRG wechoosethevariancereductionperiodasm   Thestepsizesofallalgorithmsaresetastheiroriginalpapersuggested ForAdaSPDCandADF SPDC weusetherobustadaptationschemewithT   andc We rstcomparetheserandomizedalgorithmsforridgeregressionoverthecpuactdatafromtheLibSVMwebsite https www csie ntu edu tw cjlin libsvm TheresultsareshowninFigure Withrelativelystrongregularization   allmethodsperformsimilarlyaspredictedbytheory When becomessmaller thenonacceleratedalgorithms SVRGandSAGA automaticallyexploitstrongconvexityfromdata sotheybecomefasterthanthenonadaptiveacceleratedmethods Katyusha SPDCandDFSPDC Theadaptiveacceleratedmethod ADFSPDC hasthefastestconvergence Thisindicatesthatourtheoreticalresults whichpredictnoaccelerationinthiscase maybefurtherimproved Finallywecomparethesealgorithmsforlogisticregressiononthercv dataset fromLibSVMwebsite andanothersyntheticdatasetwithn andd generatedsimilarlyasbeforebutwithcovariancematrix ij     ForthestandardSPDC wecomputethecoordinatewisedualproximalmappingusingafewstepsofscalarNewton smethodtohighprecision ThedualfreeSPDCalgorithmsonlyusegradientsofthelogisticfunction TheresultsarepresentedinFigure Forbothdatasets thestrongconvexityfromdataisveryweak andtheacceleratedalgorithmsperformsbetter ExploitingStrongConvexityfromDatawithPrimalDualFirst OrderAlgorithmsReferencesAllenZhu Zeyuan Katyusha Acceleratedvariancereductionforfastersgd ArXiveprint Bach Francis Adaptivityofaveragedstochasticgradientdescenttolocalstrongconvexityforlogisticregression JournalofMachineLearningResearch Balamurugan PalaniappanandBach Francis Stochasticvariancereductionmethodsforsaddlepointproblems InAdvancesinNeuralInformationProcessingSystems NIPS pp Bertsekas DimitriP Incrementalgradient subgradient andproximalmethodsforconvexoptimization Asurvey InSra Suvrit Nowozin Sebastian andWright StephenJ eds OptimizationforMachineLearning chapter pp MITPress Chambolle AntoninandPock Thomas   rstorderprimal dualalgorithmforconvexproblemswithapplicationstoimaging JournalofMathematicalImagingandVision Chambolle AntoninandPock Thomas Ontheergodicconvergenceratesofa rstorderprimal dualalgorithm MathematicalProgramming SeriesA Defazio Aaron Bach Francis andLacosteJulien Simon Saga Afastincrementalgradientmethodwithsupportfornonstronglyconvexcompositeobjectives InAdvancesinNeuralInformationProcessingSystems pp Deng WeiandYin Wotao Ontheglobalandlinearconvergenceofthegeneralizedalternatingdirectionmethodofmultipliers JournalofScienti cComputing Fercoq OliverandRicht arik Peter Accelerated parallel andproximalcoordinatedescent SIAMJournalonOptimization Goldstein Tom Li Min Yuan Xiaoming Esser Ernie andBaraniuk Richard Adaptiveprimaldualhybridgra dientmethodsforsaddlepointproblems arXivpreprintarXiv Johnson RieandZhang Tong Acceleratingstochasticgradientdescentusingpredictivevariancereduction InAdvancesinNeuralInformationProcessingSystems pp Lan GuanghuiandZhou Yi Anoptimalrandomizedincrementalgradientmethod arXivpreprintarXiv Lin Hongzhou Mairal Julien andHarchaoui Zaid Auniversalcatalystfor rstorderoptimization InAdvancesinNeuralInformationProcessingSystems pp   Lin Qihang Lu Zhaosong andXiao Lin Anacceleratedrandomizedproximalcoordinategradientmethodanditsapplicationtoregularizedempiricalriskmini mization SIAMJournalonOptimization   Malitsky YuraandPock Thomas   rstorderprimal dualalgorithmwithlinesearch arXivpreprintarXiv Nedic AngeliaandBertsekas DimitriP Incrementalsubgradientmethodsfornondifferentiableoptimization SIAMJournalonOptimization Nesterov Yurii IntroductoryLecturesonConvexOptimization ABasicCourse Kluwer Boston Nesterov Yurii Ef ciencyofcoordinatedescentmethodsonhugescaleoptimizationproblems SIAMJournalonOptimization Richt arik PeterandTak     Martin Iterationcomplexityofrandomizedblockcoordinatedescentmethodsformin imizingacompositefunction MathematicalProgramming Roux NicolasL Schmidt Mark andBach Francis Astochasticgradientmethodwithanexponentialconvergenceratefor nitetrainingsets InAdvancesinNeuralInformationProcessingSystems pp ShalevShwartz Shai SDCAwithoutduality regularization andindividualconvexity InProceedingsofThe rdInternationalConferenceonMachineLearning pp ShalevShwartz ShaiandZhang Tong Stochasticdualcoordinateascentmethodsforregularizedlossminimiza tion JournalofMachineLearningResearch Feb ShalevShwartz ShaiandZhang Tong Acceleratedproximalstochasticdualcoordinateascentforregularizedlossminimization MathematicalProgramming Xiao LinandZhang Tong Aproximalstochasticgradientmethodwithprogressivevariancereduction SIAMJournalonOptimization Zhang YuchenandXiao Lin Stochasticprimaldualco ordinatemethodforregularizedempiricalriskminimization InProceedingsofThe ndInternationalConferenceonMachineLearning pp 