SoftDTW    Differentiable Loss Function for TimeSeries

Marco Cuturi   Mathieu Blondel  

Abstract

We propose in this paper   differentiable learning
loss between time series  building upon the celebrated dynamic time warping  DTW  discrepancy  Unlike the Euclidean distance  DTW can
compare time series of variable size and is robust to shifts or dilatations across the time dimension  To compute DTW  one typically solves
  minimalcost alignment problem between two
time series using dynamic programming  Our
work takes advantage of   smoothed formulation of DTW  called softDTW  that computes the
softminimum of all alignment costs  We show
in this paper that softDTW is   differentiable
loss function  and that both its value and gradient can be computed with quadratic time space
complexity  DTW has quadratic time but linear
space complexity  We show that this regularization is particularly well suited to average and
cluster time series under the DTW geometry   
task for which our proposal signi cantly outperforms existing baselines  Petitjean et al   
Next  we propose to tune the parameters of   machine that outputs time series by minimizing its
   with groundtruth labels in   softDTW sense 

  Introduction

The goal of supervised learning is to learn   mapping that
links an input to an output objects  using examples of such
pairs  This task is noticeably more dif cult when the output objects have   structure       when they are not vectors  Bakir et al    We study here the case where each
output object is   time series  namely   family of observations indexed by time  While it is tempting to treat time
as yet another feature  and handle time series of vectors
as the concatenation of all these vectors  several practical

 CREST  ENSAE  Universit   ParisSaclay  France  NTT
Communication Science Laboratories  Seikacho  Kyoto  Japan 
Correspondence to  Marco Cuturi  marco cuturi ensae fr 
Mathieu Blondel  mathieu mblondel org 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Input

Output

Figure   Given the  rst part of   time series  we trained two
multilayer perceptron  MLP  to predict the entire second part 
Using the ShapesAll dataset  we used   Euclidean loss for the  rst
MLP and the softDTW loss proposed in this paper for the second
one  We display above the prediction obtained for   given test
instance with either of these two MLPs in addition to the ground
truth  Oftentimes  we observe that the softDTW loss enables us
to better predict sharp changes  More time series predictions are
given in Appendix   

issues arise when taking this simplistic approach  Timeindexed phenomena can often be stretched in some areas
along the time axis    word uttered in   slightly slower pace
than usual  with no impact on their characteristics  varying
sampling conditions may mean they have different lengths 
time series may not synchronized 

The DTW paradigm  Generative models for time series
are usually built having the invariances above in mind 
Such properties are typically handled through latent variables and or Markovian assumptions    utkepohl   
Part      simpler approach  motivated by geometry 
lies in the direct de nition of   discrepancy between time
series that encodes these invariances  such as the Dynamic
Time Warping  DTW  score  Sakoe   Chiba     
DTW computes the best possible alignment between two
time series  the optimal alignment itself can also be of interest  see      Garreau et al    of respective length  
and   by computing  rst the       pairwise distance matrix between these points to solve then   dynamic program
 DP  using Bellman   recursion with   quadratic  nm  cost 

The DTW geometry  Because it encodes ef ciently   useful class of invariances  DTW has often been used in   discriminative framework  with   kNN or SVM classi er  to
predict   real or   class label output  and engineered to run

SoftDTW    Differentiable Loss Function for TimeSeries

faster in that context  Yi et al    Recent works by
Petitjean et al    Petitjean   Ganc arski   have 
however  shown that DTW can be used for more innovative tasks  such as time series averaging using the DTW
discrepancy  see Schultz   Jain   for   gentle introduction to these ideas  More generally  the idea of synthetising time series centroids can be regarded as    rst attempt
to output entire time series using DTW as    tting loss 
From   computational perspective  these approaches are 
however  hampered by the fact that DTW is not differentiable and unstable when used in an optimization pipeline 

SoftDTW  In parallel to these developments  several authors have considered smoothed modi cations of Bellman   recursion to de ne smoothed DP distances  Bahl  
Jelinek    Ristad   Yianilos    or kernels  Saigo
et al    Cuturi et al    When applied to the
DTW discrepancy  that regularization results in   softDTW
score  which considers the softminimum of the distribution
of all costs spanned by all possible alignments between
two time series  Despite considering all alignments and
not just the optimal one  softDTW can be computed with
  minor modi cation of Bellman   recursion  in which all
 min    operations are replaced with     As   result 
both DTW and softDTW have quadratic in time   linear
in space complexity with respect to the sequences  lengths 
Because softDTW can be used with kernel machines  one
typically observes an increase in performance when using
softDTW over DTW  Cuturi    for classi cation 

Our contributions  We explore in this paper another
important bene   of smoothing DTW  unlike the original
DTW discrepancy  softDTW is differentiable in all of its
arguments  We show that the gradients of softDTW      
to all of its variables can be computed as   byproduct of
the computation of the discrepancy itself  with an added
quadratic storage cost  We use this fact to propose an alternative approach to the DBA  DTW Barycenter Averaging  clustering algorithm of  Petitjean et al    and
observe that our smoothed approach signi cantly outperforms known baselines for that task  More generally  we
propose to use softDTW as    tting term to compare the
output of   machine synthesizing   time series segment
with   ground truth observation  in the same way that  for
instance    regularized Wasserstein distance was used to
compute barycenters  Cuturi   Doucet    and later
to    discriminators that output histograms  Zhang et al 
  Rolet et al    When paired with    exible
learning architecture such as   neural network  softDTW
allows for   differentiable endto end approach to design
predictive and generative models for time series  as illustrated in Figure   Source code is available at https 
 github com mblondel softdtw 

Structure  After providing background material  we show

in   how softDTW can be differentiated       the locations
of two time series  We follow in   by illustrating how
these results can be directly used for tasks that require to
output time series  averaging  clustering and prediction of
time series  We close this paper with experimental results
in   that showcase each of these potential applications 

Notations  We consider in what follows multivariate discrete time series of varying length taking values in     Rp 
  time series can be thus represented as   matrix of   lines
and varying number of columns  We consider   differentiable substitutioncost function     Rp   Rp      which
will be  in most cases  the quadratic Euclidean distance between two vectors  For an integer   we write JnK for the set
             of integers  Given two series  lengths   and   
we write An            for the set of  binary  alignment matrices  that is paths on         matrix that connect
the upperleft     matrix entry to the lowerright       
one using only       moves  The cardinal of An   is
known as the delannoy             number  that number
grows exponentially with   and   

  The DTW and softDTW loss functions

We propose in this section   uni ed formulation for the
original DTW discrepancy  Sakoe   Chiba    and
the Global Alignment kernel  GAK   Cuturi et al   
which can be both used to compare two time series    
            xn    Rp   and                 ym    Rp   

  Alignment costs  optimality and sum

Given the cost matrix           cid xi  yj cid ij   Rn   

the inner product hA           of that matrix with an alignment matrix   in An   gives the score of    as illustrated
in Figure   Both DTW and GAK consider the costs of all
possible alignment matrices  yet do so differently 

DTW         min

  An  

hA           

  

GA         XA An  

  hA        

 

DP Recursion  Sakoe   Chiba   showed that the
Bellman equation   can be used to compute DTW 
That recursion  which appears in line   of Algorithm    disregarding for now the exponent   only involves  min   
operations  When considering kernel   
GA and  instead  its
integration over all alignments  see      Lasserre  
Cuturi et al    Theorem   and the highly related formulation of Saigo et al       use an old algorithmic appraoch  Bahl   Jelinek    which consists
in     replacing all costs by their negexponential   ii  replace  min    operations with     operations  These
two recursions can be in fact uni ed with the use of   soft 

SoftDTW    Differentiable Loss Function for TimeSeries

  

  

  

  

  

  

  
  
  
  

Figure   Three alignment matrices  orange  green  purple  in addition to the topleft and bottomright entries  between two time
series of length   and   The cost of an alignment is equal to the
sum of entries visited along the path  DTW only considers the
optimal alignment  here depicted in purple pentagons  whereas
softDTW considers all delannoy             possible alignment matrices 

minimum operator  which we present below 

Uni ed algorithm Both formulas in Eq    can be computed with   single algorithm  That formulation is new to
our knowledge  Consider the following generalized min
operator  with   smoothing parameter      

min            an   mini   ai 
  logPn

With that operator  we can de ne  softDTW 

     
     ai       

dtw         min hA                An   

The original DTW score is recovered by setting   to  
When       we recover dtw      log   
GA  Most
importantly  and in either case  dtw  can be computed
using Algorithm   which requires  nm  operations and
 nm  storage cost as well   That cost can be reduced to
   with   more careful implementation if one only seeks
to compute dtw       but the backward pass we consider next requires the entire matrix   of intermediary
alignment costs  Note that  to ensure numerical stability  the operator min  must be computed using the usual

logsum exp stabilization trick  namely that logPi ezi  
 maxj zj    logPi ezi maxj zj  

  Differentiation of softDTW

Algorithm   Forward recursion to compute dtw      
and intermediate alignment costs

  Inputs        smoothing       distance function  
         ri                JnK      JmK
  for                 do
 
 
 
  end for
  Output   rn      

for                 do
ri      xi  yj    min ri    ri    ri   
end for

average time series under the DTW metric  Petitjean et al 
  Schultz   Jain    To recover the gradient of
dtw                 we only need to apply the chain rule 
thanks to the differentiability of the cost function 

 xdtw        cid        
    cid  

  

 

where         is the Jacobian of               linear
map from Rp   to Rn    When   is the squared Euclidean
distance  the transpose of that Jacobian applied to   matrix
    Rn   is   being the elementwise product 

               cid    

mBT         yBT cid   

With continuous data     is almost always likely to be
unique  and therefore the gradient in Eq    will be de 
 ned almost everywhere  However  that gradient  when it
exists  will be discontinuous around those values   where
  small change in   causes   change in    which is likely
to hamper the performance of gradient descent methods 

The case       An immediate advantage of softDTW
is that it can be explicitly differentiated    fact that was also
noticed by Saigo et al    in the related case of edit
distances  When       the gradient of Eq    is obtained
via the chain rule 

    cid  
   dtw        cid        
GA       XA An  

  

 

    

 

where       

  hA      iA 

  small variation in the input   causes   small change
in dtw       or dtw       When considering dtw 
that change can be ef ciently monitored only when the
optimal alignment matrix    that arises when computing
dtw       in Eq    is unique  As the minimum over  
 nite set of linear functions of   dtw  is therefore locally
the cost matrix   with gradient   
differentiable       
  fact that has been exploited in all algorithms designed to

is the average alignment matrix   under the Gibbs distribution        hA         de ned on all alignments in
An    The kernel   
GA       can thus be interpreted as
the normalization constant of      Of course  since An  
has exponential size in   and      naive summation is not
tractable  Although   Bellman recursion to compute that
average alignment matrix      exists  see Appendix   
that computation has quartic       complexity  Note that

 SoftDTW    Differentiable Loss Function for TimeSeries

this stands in stark contrast to the quadratic complexity obtained by Saigo et al    for editdistances  which is due
to the fact the sequences they consider can only take values
in    nite alphabet  To compute the gradient of softDTW 
we propose instead an algorithm that manages to remain
quadratic  nm  in terms of complexity  The key to achieve
this reduction is to apply the chain rule in reverse order of
Bellman   recursion given in Algorithm   namely backpropagate    similar idea was recently used to compute the
gradient of ANOVA kernels in  Blondel et al   

  Algorithmic differentiation

Differentiating algorithmically dtw       requires doing
 rst   forward pass of Bellman   equation to store all intermediary computations and recover      ri    when
running Algorithm   The value of dtw      stored
in rn   at the end of the forward recursion is then impacted by   change in ri   exclusively through the terms
in which ri   plays   role  namely the triplet of terms
ri    ri    ri      straightforward application of
the chain rule then gives

 rn  
 ri  

   rn  
 ri  

 ri  

 ri  

   rn  
 ri   

 ri   

 ri  

   rn  
 ri   

 ri   

 ri  

 

ei  

   

ei  

      

ei   

      

ei   

 

  

 

in which we have de ned the notation of the main object
of interest of the backward recursion  ei      rn  
  The
 ri  
Bellman recursion evaluated at           as shown in line  
of Algorithm    here      is  xi  yj  yields  

ri            min ri    ri    ri   

which  when differentiated       ri   yields the ratio 

 ri  

 ri  

    ri    cid   ri        ri         ri   cid   

The logarithm of that derivative can be conveniently cast
using evaluations of min  computed in the forward loop 

  log

 ri  

 ri  

  min ri    ri    ri      ri  

  ri            ri   

Similarly  the following relationships can also be obtained 

  log

 ri   

 ri  

  log

 ri   

 ri  

  ri      ri          

  ri      ri          

We have therefore obtained   backward recursion to compute the entire matrix      ei    starting from en    
 rn  
    down to    To obtain    dtw       notice
 rn  
that the derivatives        the entries of the cost matrix  
can be computed by  rn  
  ei         ei   
    
and therefore we have that

   rn  
 ri  

 ri  
    

    cid  
   dtw        cid        

  

where   is exactly the average alignment     
in
Eq    These computations are summarized in Algorithm   which  once   has been computed  has complexity
nm in time and space  Because min  has    Lipschitz
continuous gradient  the gradient of dtw  is  Lipschitz
continuous when   is the squared Euclidean distance 

Algorithm   Backward recursion to compute    dtw      

  Inputs        smoothing       distance function  
         dtw            xi  yj    
                       JnK      JmK
  ei      en           JnK      JmK
  ri      rn           JnK      JmK
            en        rn      rn  
  for                  do
 
 

for                  do
    exp  
    exp  
    exp  
ei     ei         ei          ei       
end for

   ri     ri          
   ri      ri          
   ri      ri          

 

 
 
 
  end for

  Output     dtw        cid       
    cid  

 

  Learning with the softDTW loss

  Averaging with the softDTW geometry

We study in this section   direct application of Algorithm  
to the problem of computing Fr echet means   of time
series with respect to the dtw  discrepancy  Given  
family of   times series            yN   namely   matrices
of   lines and varying number of columns             mN  
we are interested in de ning   single barycenter time series   for that family under   set of normalized weights
          Our goal is thus
to solve approximately the following problem  in which we
have assumed that   has  xed length   

                  such thatPN

min

  Rp  

  
mi

 

Xi 

dtw    yi 

 

Note that each dtw    yi  term is divided by mi  the
length of yi  Indeed  since dtw  is an increasing  roughly
linearly  function of each of the input lengths   and mi  we
follow the convention of normalizing in practice each discrepancy by     mi  Since the length   of   is here  xed
across all evaluations  we do not need to divide the objective of Eq    by    Averaging under the softDTW geometry results in substantially different results than those that
can be obtained with the Euclidean geometry  which can
only be used in the case where all lengths                 

SoftDTW    Differentiable Loss Function for TimeSeries

ri   

ri   

ri   

    

    

ri  

ri  

ri  

     

     

ri   

ri   

ri   

ei  

 

   ri   ri        

 

ei   

 

   ri    ri         

 

ei  

 

   ri   ri        

 

ei   

Figure   Sketch of the computational graph for softDTW  in the forward pass used to compute dtw   left  and backward pass used to
compute its gradient    dtw   right  In both diagrams  purple shaded cells stand for data values available before the recursion starts 
namely cost values  left  and multipliers computed using forward pass results  right  In the left diagram  the forward computation of
ri   as   function of its predecessors and      is summarized with arrows  Dotted lines indicate   min  operation  solid lines an addition 
From the perspective of the  nal term rn    which stores dtw       at the lower right corner  not shown  of the computational graph 
  change in ri   only impacts rn   through changes that ri   causes to ri     ri    and ri    These changes can be tracked using
Eq    and appear in lines   in Algorithm   as variables          as well as in the purple shaded boxes in the backward pass
 right  which represents the recursion of line   in Algorithm  

mN are equal  as can be seen in the intuitive interpolations
we obtain between two time series shown in Figure  

Nonconvexity of dtw      natural question that arises
from Eq    is whether that objective is convex or not  The
answer is negative  in   way that echoes the nonconvexity
of the kmeans objective as   function of cluster centroids
locations  Indeed  for any alignment matrix   of suitable
size  each map     hA           shares the same convexity concavity property that   may have  However  both min
and min  can only preserve the concavity of elementary
functions  Boyd   Vandenberghe    pp  Therefore dtw  will only be concave if   is concave  or become
instead    nonconvex   soft  minimum of convex functions
if   is convex  When   is   squaredEuclidean distance 
dtw  is   piecewise quadratic function of    as is also the
case with the kmeans energy  see for instance Figure  
in Schultz   Jain   Since this is the setting we consider here  all of the computations involving barycenters
should be taken with   grain of salt  since we have no way
of ensuring optimality when approximating Eq   

Smoothing helps optimizing dtw    Smoothing can be
regarded  however  as   way to  convexify  dtw   
Indeed  notice that dtw  converges to the sum of all costs
as       Therefore  if   is convex  dtw  will gradually
become convex as   grows  For smaller values of   one
can intuitively foresee that using min  instead of   minimum will smooth out local minima and therefore provide  
better  although slightly different from dtw  optimization
landscape  We believe this is why our approach recovers
better results  even when measured in the original dtw 
discrepancy  than subgradient or alternating minimization
approaches such as DBA  Petitjean et al    which can 
on the contrary  get more easily stuck in local minima  Evidence for this statement is presented in the experimental
section 

    Euclidean loss

    SoftDTW loss      

Figure   Interpolation between two time series  red and blue  on
the Gun Point dataset  We computed the barycenter by solving Eq 
  with     set to         and    
The softDTW geometry leads to visibly different interpolations 

  Clustering with the softDTW geometry

The  approximate  computation of dtw  barycenters can
be seen as    rst step towards the task of clustering time
series under the dtw  discrepancy  Indeed  one can naturally formulate that problem as that of  nding centroids
           xk that minimize the following energy 

min

  xk Rp  

 
mi

 

Xi 

min
    

dtw xj  yi 

 

To solve that problem one can resort to   direct generalization of Lloyd   algorithm   in which each centering
step and each clustering allocation step is done according
to the dtw  discrepancy 

  Learning prototypes for time series classi cation

One of the defacto baselines for learning to classify time
series is the   nearest neighbors  kNN  algorithm  combined with DTW as discrepancy measure between time series  However  kNN has two main drawbacks  First  the
time series used for training must be stored  leading to
potentially high storage cost  Second  in order to com 

SoftDTW    Differentiable Loss Function for TimeSeries

pute predictions on new time series  the DTW discrepancy must be computed with all training time series  leading to high computational cost  Both of these drawbacks
can be addressed by the nearest centroid classi er  Hastie
et al        Tibshirani et al    This method
chooses the class whose barycenter  centroid  is closest
to the time series to classify  Although very simple  this
method was shown to be competitive with kNN  while requiring much lower computational cost at prediction time
 Petitjean et al    SoftDTW can naturally be used
in   nearest centroid classi er  in order to compute the
barycenter of each class at train time  and to compute the
discrepancy between barycenters and time series  at prediction time 

  Multistepahead prediction

SoftDTW is ideally suited as   loss function for any task
that requires time series outputs  As an example of such  
task  we consider the problem of  given the  rst            
observations of   time series  predicting the remaining
  Rp      be
                 observations  Let xt   
the submatrix of     Rp   of all columns with indices between   and    where                 Learning to predict
the segment of   time series can be cast as the problem

min
 

 

Xi 

dtw cid      

    xt  

 

 cid   

where     is   set of parameterized function that take
as input   time series and outputs   time series  Natural
choices would be multilayer perceptrons or recurrent neural networks  RNN  which have been historically trained
with   Euclidean loss  Parlos et al    Eq 

  Experimental results

Throughout this section  we use the UCR  University
of California  Riverside  time series classi cation archive
 Chen et al    We use   subset containing   datasets
encompassing   wide variety of  elds  astronomy  geology 
medical imaging  and lengths  Datasets include class information  up to   classes  for each time series and are split
into train and test sets  Due to the large number of datasets
in the UCR archive  we choose to report only   summary
of our results in the main manuscript  Detailed results are
included in the appendices for interested readers 

  Averaging experiments

In this section  we compare the softDTW barycenter approach presented in   to DBA  Petitjean et al   
and   simple batch subgradient method 

Table   Percentage of the datasets on which the proposed softDTW barycenter is achieving lower DTW loss  Equation   with
      than competing methods 

Random

initialization

Euclidean mean

initialization

Comparison with DBA
     
     
     
     

 
 
 
 

 
 
 
 

Comparison with subgradient method
     
     
     
     

 
 
 
 

 
 
 
 

their barycenter  For quantitative results below  we repeat
this procedure   times and report the averaged results  For
each method  we set the maximum number of iterations
to   To minimize the proposed softDTW barycenter
objective  Eq    we use LBFGS 

Qualitative results  We  rst visualize the barycenters obtained by softDTW when       and       by DBA
and by the subgradient method  Figure   shows barycenters obtained using random initialization on the ECG 
dataset  More results with both random and Euclidean
mean initialization are given in Appendix   and   

We observe that both DBA or softDTW with low smoothing parameter   yield barycenters that are spurious  On
the other hand    descent on the softDTW loss with suf 
 ciently high   converges to   reasonable solution  For
example  as indicated in Figure   with DTW or softDTW
      the small kink around       is not representative of any of the time series in the dataset  However 
with softDTW       the barycenter closely matches the
time series  This suggests that DTW or softDTW with too
low   can get stuck in bad local minima 

When using Euclidean mean initialization  only possible if
time series have the same length  DTW or softDTW with
low   often yield barycenters that better match the shape of
the time series  However  they tend to over    they absorb
the idiosyncrasies of the data  In contrast  softDTW is able
to learn barycenters that are much smoother 

Quantitative results  Table   summarizes the percentage
of datasets on which the proposed softDTW barycenter
achieves lower DTW loss when varying the smoothing parameter   The actual loss values achieved by different
methods are indicated in Appendix   and Appendix   

Experimental setup  For each dataset  we choose   class
at random  pick   time series in that class and compute

As   decreases  softDTW achieves   lower DTW loss than
other methods on almost all datasets  This con rms our

SoftDTW    Differentiable Loss Function for TimeSeries

Figure   Comparison between our proposed soft barycenter and
the barycenter obtained by DBA and the subgradient method 
on the ECG  dataset  When DTW is insuf ciently smoothed 
barycenters often get stuck in   bad local minimum that does not
correctly match the time series 

    SoftDTW      

    DBA

claim that the smoothness of softDTW leads to an objective that is better behaved and more amenable to optimization by gradientdescent methods 

Figure   Clusters obtained on the CBF dataset when plugging our
proposed soft barycenter and that of DBA in Lloyd   algorithm 
DBA absorbs the idiosyncrasies of the data  while softDTW can
learn much smoother barycenters 

  kmeans clustering experiments

  Timeseries classi cation experiments

We consider in this section the same computational tools
used in   above  but use them to cluster time series 

Experimental setup  For all datasets  the number of clusters   is equal to the number of classes available in the
dataset  Lloyd   algorithm alternates between   centering
step  barycenter computation  and an assignment step  We
set the maximum number of outer iterations to   and the
maximum number of inner  barycenter  iterations to  
as before  Again  for softDTW  we use LBFGS 

Qualitative results  Figure   shows the clusters obtained
when runing Lloyd   algorithm on the CBF dataset with
softDTW       and DBA  in the case of random initialization  More results are included in Appendix    Clearly 
DTW absorbs the tiny details in the data  while softDTW
is able to learn much smoother barycenters 

Quantitative results  Table   summarizes the percentage
of datasets on which softDTW barycenter achieves lower
kmeans loss under DTW       Eq    with       The
actual loss values achieved by all methods are indicated in
Appendix   and Appendix    The results con rm the same
trend as for the barycenter experiments  Namely  as   decreases  softDTW is able to achieve lower loss than other
methods on   large proportion of the datasets  Note that
we have not run experiments with smaller values of   than
  since dtw  is very close to dtw  in practice 

In this section  we investigate whether the smoothing in
softDTW can act as   useful regularization and improve
classi cation accuracy in the nearest centroid classi er 

Experimental setup  We use   of the data for training 
  for validation and   for testing  We choose   from
  logspaced values between   and  

Quantitative results  Each point in Figure   above the diagonal line represents   dataset for which using softDTW
for barycenter computation rather than DBA improves the
accuracy of the nearest centroid classi er  To summarize 
we found that softDTW is working better or at least as well
as DBA in   of the datasets 

  Multistepahead prediction experiments

In this section  we present preliminary experiments for the
task of multistepahead prediction  described in  

Experimental setup  We use the training and test sets prede ned in the UCR archive  In both the training and test
sets  we use the  rst   of the time series as input and the
remaining   as output  ignoring class information  We
then use the training set to learn   model that predicts the
outputs from inputs and the test set to evaluate results with
both Euclidean and DTW losses  In this experiment  we
focus on   simple multilayer perceptron  MLP  with one

SoftDTW    Differentiable Loss Function for TimeSeries

Table   Percentage of the datasets on which the proposed softDTW based kmeans is achieving lower DTW loss  Equation  
with       than competing methods 

Random

initialization

Euclidean mean

initialization

Comparison with DBA
     
     
     
     

 
 
 
 

 
 
 
 

Comparison with subgradient method
     
     
     
     

 
 
 
 

 

 

 
 

Figure   Each point above the diagonal represents   dataset
where using our softDTW barycenter rather than that of DBA
improves the accuracy of the nearest nearest centroid classi er 
This is the case for   of the datasets in the UCR archive 

hidden layer and sigmoid activation  We also experimented
with linear models and recurrent neural networks  RNNs 
but they did not improve over   simple MLP 

Implementation details  Deep learning frameworks such
as Theano  TensorFlow and Chainer allow the user to specify   custom backward pass for their function  Implementing such   backward pass  rather than resorting to automatic
differentiation  autodiff  is particularly important in the
case of softDTW  First  the autodiff in these frameworks
is designed for vectorized operations  whereas the dynamic
program used by the forward pass of Algorithm   is inherently elementwise  Second  as we explained in   our
backward pass is able to reuse logsum exp computations
from the forward pass  leading to both lower computational
cost and better numerical stability  We implemented   custom backward pass in Chainer  which can then be used to
plug softDTW as   loss function in any network architecture  To estimate the MLP   parameters  we used Chainer  
implementation of Adam  Kingma   Ba   

Qualitative results  Visualizations of the predictions obtained under Euclidean and softDTW losses are given in
Figure   as well as in Appendix    We  nd that for sim 

Table   Averaged rank obtained by   multilayer perceptron
 MLP  under Euclidean and softDTW losses  Euclidean initialization means that we initialize the MLP trained with softDTW
loss by the solution of the MLP trained with Euclidean loss 

Training loss

Random

initialization

Euclidean

initialization

When evaluating with DTW loss

Euclidean
softDTW      
softDTW      
softDTW      
softDTW      

 
 
 
 
 

When evaluating with Euclidean loss

Euclidean
softDTW      
softDTW      
softDTW      
softDTW      

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

ple onedimensional time series  an MLP works very well 
showing its ability to capture patterns in the training set 
Although the predictions under Euclidean and softDTW
losses often agree with each other  they can sometimes be
visibly different  Predictions under softDTW loss can con 
 dently predict abrupt and sharp changes since those have
  low DTW cost as long as such   sharp change is present 
under   small time shift  in the ground truth 

Quantitative results    comparison summary of our
MLP under Euclidean and softDTW losses over the UCR
archive is given in Table   Detailed results are given in
the appendix  Unsurprisingly  we achieve lower DTW loss
when training with the softDTW loss  and lower Euclidean
loss when training with the Euclidean loss  Because DTW
is robust to several useful invariances    small error in the
softDTW sense could be   more judicious choice than an
error in an Euclidean sense for many applications 

  Conclusion

We propose in this paper to turn the popular DTW discrepancy between time series into   full edged loss function
between ground truth time series and outputs from   learning machine  We have shown experimentally that  on the
existing problem of computing barycenters and clusters for
time series data  our computational approach is superior to
existing baselines  We have shown promising results on the
problem of multistepahead time series prediction  which
could prove extremely useful in settings where   user   actual loss function for time series is closer to the robust perspective given by DTW  than to the local parsing of the
Euclidean distance 

Acknowledgements  MC gratefully acknowledges the
support of   chaire de   IDEX Paris Saclay 

SoftDTW    Differentiable Loss Function for TimeSeries

References

Bahl    and Jelinek  Frederick  Decoding for channels with
insertions  deletions  and substitutions with applications
to speech recognition  IEEE Transactions on Information Theory     

Bakir  GH  Hofmann     Sch olkopf     Smola  AJ  Taskar 
   and Vishwanathan  SVN 
Predicting Structured
Data  Advances in neural information processing systems  MIT Press  Cambridge  MA  USA   

Bellman  Richard  On the theory of dynamic programming 
Proceedings of the National Academy of Sciences   
   

Blondel  Mathieu  Fujino  Akinori  Ueda  Naonori  and
Ishihata  Masakazu  Higherorder factorization machines  In Advances in Neural Information Processing
Systems   pp     

Boyd  Stephen and Vandenberghe  Lieven  Convex Opti 

mization  Cambridge University Press   

Chen  Yanping  Keogh  Eamonn  Hu  Bing  Begum  Nurjahan  Bagnall  Anthony  Mueen  Abdullah  and Batista 
Gustavo  The ucr time series classi cation archive 
July  
www cs ucr edu eamonn time 
series data 

Cuturi  Marco  Fast global alignment kernels  In Proceedings of the  th international conference on machine
learning  ICML  pp     

Cuturi  Marco and Doucet  Arnaud  Fast computation of
Wasserstein barycenters  In Proceedings of the  st International Conference on Machine Learning  ICML 
  pp     

Cuturi  Marco  Vert  JeanPhilippe  Birkenes  Oystein  and
Matsui  Tomoko    kernel for time series based on
global alignments 
In   IEEE International Conference on Acoustics  Speech and Signal ProcessingICASSP  volume   pp  II   

Fr echet  Maurice  Les  el ements al eatoires de nature quelconque dans un espace distanci    In Annales de   institut
Henri Poincar    volume   pp    Presses universitaires de France   

Garreau  Damien  Lajugie    emi  Arlot  Sylvain  and Bach 
Francis  Metric learning for temporal sequence alignment 
In Advances in Neural Information Processing
Systems  pp     

Hastie  Trevor  Tibshirani  Robert  and Friedman  Jerome 
The Elements of Statistical Learning  Springer New York
Inc   

Kingma  Diederik and Ba 

Jimmy 
method for stochastic optimization 
arXiv   

Adam 

 
arXiv preprint

Lasserre  Jean    Linear and integer programming vs
linear integration and counting    duality viewpoint 
Springer Science   Business Media   

Lloyd  Stuart  Least squares quantization in pcm 

IEEE

Trans  on Information Theory     

  utkepohl  Helmut  New introduction to multiple time series analysis  Springer Science   Business Media   

Parlos  Alexander    Rais  Omar    and Atiya  Amir   
Multistep ahead prediction using dynamic recurrent
neural networks  Neural networks     

Petitjean  Franc ois and Ganc arski  Pierre  Summarizing  
set of time series by averaging  From steiner sequence
to compact multiple alignment  Theoretical Computer
Science     

Petitjean  Franc ois  Ketterlin  Alain  and Ganc arski  Pierre 
  global averaging method for dynamic time warping 
with applications to clustering  Pattern Recognition   
   

Petitjean  Franc ois  Forestier  Germain  Webb  Geoffrey   
Nicholson  Ann    Chen  Yanping  and Keogh  Eamonn 
Dynamic time warping averaging of time series allows
faster and more accurate classi cation 
In ICDM  pp 
  IEEE   

Ristad  Eric Sven and Yianilos  Peter    Learning stringIEEE Transactions on Pattern Analysis

edit distance 
and Machine Intelligence     

Rolet     Cuturi     and Peyr       Fast dictionary learning with   smoothed Wasserstein loss  Proceedings of
AISTATS   

Saigo  Hiroto  Vert  JeanPhilippe  Ueda  Nobuhisa  and
Akutsu  Tatsuya 
Protein homology detection using
string alignment kernels  Bioinformatics   
   

Saigo  Hiroto  Vert  JeanPhilippe  and Akutsu  Tatsuya 
Optimizing amino acid substitution matrices with   local
alignment kernel  BMC bioinformatics     

Sakoe  Hiroaki and Chiba  Seibi    dynamic programming
approach to continuous speech recognition  In Proceedings of the Seventh International Congress on Acoustics 
Budapest  volume   pp     

Sakoe  Hiroaki and Chiba  Seibi  Dynamic programming algorithm optimization for spoken word recognition  IEEE Trans  on Acoustics  Speech  and Sig  Proc 
   

SoftDTW    Differentiable Loss Function for TimeSeries

Schultz  David and Jain  Brijnesh  Nonsmooth analysis
and subgradient methods for averaging in dynamic time
warping spaces  arXiv preprint arXiv   

Tibshirani  Robert  Hastie  Trevor  Narasimhan  Balasubramanian  and Chu  Gilbert  Diagnosis of multiple cancer
types by shrunken centroids of gene expression  Proceedings of the National Academy of Sciences   
   

Yi  ByoungKee  Jagadish  HV  and Faloutsos  Christos 
Ef cient retrieval of similar time sequences under time
warping  In Data Engineering    Proceedings   th
International Conference on  pp    IEEE   

Zhang     Frogner     Mobahi     ArayaPolo     and
Poggio     Learning with   Wasserstein loss  Advances
in Neural Information Processing Systems    

