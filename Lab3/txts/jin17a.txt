How to Escape Saddle Points Ef ciently

Chi Jin   Rong Ge   Praneeth Netrapalli   Sham    Kakade   Michael    Jordan  

Abstract

This paper shows that   perturbed form of gradient descent converges to   secondorder stationary point in   number iterations which depends
only polylogarithmically on dimension       it is
almost  dimensionfree  The convergence rate
of this procedure matches the wellknown convergence rate of gradient descent to  rstorder
stationary points  up to log factors  When all saddle points are nondegenerate  all secondorder
stationary points are local minima  and our result
thus shows that perturbed gradient descent can
escape saddle points almost for free  Our results
can be directly applied to many machine learning
applications  including deep learning  As   particular concrete example of such an application 
we show that our results can be used directly to
establish sharp global convergence rates for matrix factorization  Our results rely on   novel
characterization of the geometry around saddle
points  which may be of independent interest to
the nonconvex optimization community 

  Introduction
Given   function     Rd        gradient descent aims to
minimize the function via the following iteration 

xt    xt       xt 

where       is   step size  Gradient descent and its variants       stochastic gradient  are widely used in machine
learning applications due to their favorable computational
properties  This is notably true in the deep learning setting  where gradients can be computed ef ciently via backpropagation  Rumelhart et al   
Gradient descent is especially useful in highdimensional
settings because the number of iterations required to reach

 University of California  Berkeley  Duke University
 Microsoft Research India  University of Washington  Correspondence to  Chi Jin  chijin berkeley edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  point with small gradient is independent of the dimension
 dimensionfree  More precisely  for   function that is  cid 
gradient Lipschitz  see De nition   it is well known that
gradient descent  nds an  rstorder stationary point      
  point   with  cid      cid      within  cid            cid  iterations  Nesterov    where    is the initial point and
   cid  is the optimal value of    This bound does not depend
on the dimension of    In convex optimization   nding an
 rstorder stationary point is equivalent to  nding an approximate global optimum 
In nonconvex settings  however  convergence to  rstorder
stationary points is not satisfactory  For nonconvex functions   rstorder stationary points can be global minima 
local minima  saddle points or even local maxima  Finding   global minimum can be hard  but fortunately  for
many nonconvex problems  it is suf cient to  nd   local
minimum 
Indeed    line of recent results show that  in
many problems of interest  all local minima are global minima       in tensor decomposition  Ge et al    dictionary learning  Sun et al      phase retrieval  Sun
et al      matrix sensing  Bhojanapalli et al   
Park et al    matrix completion  Ge et al   
and certain classes of deep neural networks  Kawaguchi 
  Moreover  there are suggestions that in more general deep networks most of the local minima are as good as
global minima  Choromanska et al   
On the other hand  saddle points  and local maxima  can
correspond to highly suboptimal solutions in many problems  see       Jain et al    Sun et al      Furthermore  Dauphin et al    argue that saddle points
are ubiquitous in highdimensional  nonconvex optimization problems  and are thus the main bottleneck in training neural networks  Standard analysis of gradient descent
cannot distinguish between saddle points and local minima 
leaving open the possibility that gradient descent may get
stuck at saddle points  either asymptotically or for   suf 
ciently long time so as to make training times for arriving at
  local minimum infeasible  Ge et al    showed that
by adding noise at each step  gradient descent can escape
all saddle points in   polynomial number of iterations  provided that the objective function satis es the strict saddle
property  see Assumption    Lee et al    proved
that under similar conditions  gradient descent with random initialization avoids saddle points even without adding

How to Escape Saddle Points Ef ciently

Algorithm   Perturbed Gradient Descent  Metaalgorithm 

for               do
xt   xt      

if perturbation condition holds then
xt    xt       xt 

   uniformly       

noise  However  this result does not bound the number of
steps needed to reach   local minimum 
Previous work explains why gradient descent avoids saddle points in the nonconvex setting  but not why it is ef 
cient all of them have runtime guarantees with high polynomial dependency in dimension    For instance  the number of iterations required in Ge et al    is at least
    which is prohibitive in high dimensional setting
such as deep learning  typically with millions of parameters  Therefore  we wonder whether gradient descent type
of algorithms are fundamentally slow in escaping saddle
points  or it is the lack of our theoretical understanding
while gradient descent is indeed ef cient  This motivates
the following question  Can gradient descent escape saddle points and converge to local minima in   number of
iterations that is  almost  dimensionfree 
In order to answer this question formally  this paper investigates the complexity of  nding  secondorder stationary
points  For  Hessian Lipschitz functions  see De nition
  these points are de ned as  Nesterov   Polyak   

 cid      cid     

and

 min          

 

Under the assumption that all saddle points are strict
      for any saddle point xs   min    xs      all
secondorder stationary points       are local minima 
Therefore  convergence to secondorder stationary points
is equivalent to convergence to local minima 
This paper studies   simple variant of gradient descent
 with phasic perturbations  see Algorithm   For  cid smooth
functions that are also Hessian Lipschitz  we show that perturbed gradient descent will converge to an  secondorder
stationary point in    cid          cid  where     hides
polylog factors  This guarantee is almost dimension free
 up to polylog    factors  answering the above highlighted
question af rmatively  Note that this rate is exactly the
same as the wellknown convergence rate of gradient descent to  rstorder stationary points  Nesterov    up
to log factors  Furthermore  our analysis admits   maximal step size of up to  cid  which is the same as that in
analyses for  rstorder stationary points 
As many real learning problems present strong local geometric properties  similar to strong convexity in the global
setting  see       Bhojanapalli et al    Sun   Luo 
  Zheng   Lafferty    it is important to note that
our analysis naturally takes advantage of such local struc 

ture  We show that when local strong convexity is present 
the  dependence goes from   polynomial rate    to linear convergence  log  As an example  we show that
sharp global convergence rates can be obtained for matrix
factorization as   direct consequence of our analysis 

  Our Contributions

This paper presents the  rst sharp analysis that shows that
 perturbed  gradient descent  nds an approximate secondorder stationary point in at most polylog    iterations  thus
escaping all saddle points ef ciently  Our main technical
contributions are as follows 

  For  cid gradient Lipschitz   Hessian Lipschitz functions  possibly nonconvex  gradient descent with appropriate perturbations  nds an  secondorder stationary point in    cid          cid  iterations  This
rate matches the wellknown convergence rate of gradient descent to  rstorder stationary points up to log
factors 

  Under   strictsaddle condition  see Assumption   
the same convergence result applies for local minima 
This means that gradient descent can escape all saddle
points with only logarithmic overhead in runtime 

  When the function has local structure  such as local
strong convexity  see Assumption      the above results can be further improved to linear convergence 
We give sharp rates that are comparable to previous
problemspeci   local analysis of gradient descent
with smart initialization  see Section  

  All the above results rely on   new characterization of
the geometry around saddle points  points from where
gradient descent gets stuck at   saddle point constitute
  thin  band  We develop novel techniques to bound
the volume of this band  As   result  we can show that
after   random perturbation the current point is very
unlikely to be in the  band  hence  ef cient escape
from the saddle point is possible  see Section  

  Related Work

Over the past few years  there have been many problemspeci   convergence results for nonconvex optimization 
One line of work requires   smart initialization algorithm
to provide   coarse estimate lying inside   local neighborhood  from which popular local search algorithms enjoy
fast local convergence  see       Netrapalli et al   
Candes et al    Sun   Luo    Bhojanapalli et al 
  While there are not many results that show global
convergence for nonconvex problems  Jain et al   
show that gradient descent yields global convergence rates
for matrix squareroot problems  Although these results

How to Escape Saddle Points Ef ciently

Table   Oracle models and iteration complexity for convergence
to secondorder stationary point

Algorithm

Iterations

Ge et al   

Levy  
This Work

  poly   
    poly 
  log   

Oracle

Gradient
Gradient
Gradient

Agarwal et al      log    Hessianvector
Carmon et al      log    Hessianvector

  log    Hessianvector

Carmon   Duchi

 

Nesterov   Polyak

 

Curtis et al   

  

  

Hessian

Hessian

give strong guarantees  the analyses are heavily tailored to
speci   problems  and it is unclear how to generalize them
to   wider class of nonconvex functions 
For general nonconvex optimization  there are   few previous results on  nding secondorder stationary points 
These results can be divided into the following three categories  where  for simplicity of presentation  we only highlight dependence on dimension   and   assuming that all
other problem parameters are constant from the point of
view of iteration complexity 
Hessianbased  Traditionally  only secondorder optimization methods were known to converge to secondorder
stationary points  These algorithms rely on computing the
Hessian to distinguish between  rstand secondorder stationary points  Nesterov   Polyak   designed   cubic
regularization algorithm which converges to an  secondorder stationary point in    iterations  Trust region
algorithms  Curtis et al    can also achieve the same
performance if the parameters are chosen carefully  These
algorithms typically require the computation of the inverse
of the full Hessian per iteration  which can be very expensive 
Hessianvector productbased    number of recent papers have explored the possibility of using only Hessianvector products instead of full Hessian information in order
to  nd secondorder stationary points  These algorithms require   Hessianvector product oracle  given   function   
  point   and   direction    the oracle returns            
Agarwal et al    and Carmon et al    presented
accelerated algorithms that can  nd an  secondorder stationary point in   log    steps  Also  Carmon  
Duchi   showed by running gradient descent as  
subroutine to solve the subproblem of cubic regularization

 which requires Hessianvector product oracle  it is possible to  nd an  secondorder stationary pointin   log   
iterations  In many applications such an oracle can be implemented ef ciently  in roughly the same complexity as
the gradient oracle  Also  when the function has   Hessian
Lipschitz property such an oracle can be approximated by
differentiating the gradients at two very close points  although this may suffer from numerical issues  thus is seldom used in practice 
Gradientbased  Another recent line of work shows that it
is possible to converge to   secondorder stationary point
without any use of the Hessian  These methods feature
simple computation per iteration  only involving gradient
operations  and are closest to the algorithms used in practice  Ge et al    showed that stochastic gradient descent could converge to   secondorder stationary point in
poly    iterations  with polynomial of order at least four 
This was improved in Levy   to        poly 
using normalized gradient descent  The current paper improves on both results by showing that perturbed gradient descent can actually  nd an  secondorder stationary
point in   polylog    steps  which matches the guarantee for converging to  rstorder stationary points up to
polylog factors 

  Preliminaries
In this section  we will  rst introduce our notation  and then
present some de nitions and existing results in optimization which will be used later 

  Notation

We use bold uppercase letters      to denote matrices and
bold lowercase letters      to denote vectors  Aij means
the       th entry of matrix    For vectors we use  cid cid  to
denote the  cid norm  and for matrices we use  cid cid  and  cid cid  
to denote spectral norm and Frobenius norm respectively 
We use  max   min      to denote the largest  the
smallest and the ith largest singular values respectively 
and  max   min      for corresponding eigenvalues 
For   function     Rd      we use      and      to
denote its gradient and Hessian  and    cid  to denote the global
minimum of     We use notation    to hide only absolute constants which do not depend on any problem parameter  and notation     to hide only absolute constants and
log factors  We let     
      denote the ddimensional ball
centered at   with radius    when it is clear from context 
we simply denote it as Bx    We use PX   to denote projection onto the set     Distance and projection are always
de ned in   Euclidean sense 

How to Escape Saddle Points Ef ciently

  Gradient Descent

The theory of gradient descent often takes its point of departure to be the study of convex optimization 
De nition     differentiable function     is  cid smooth
 or  cid gradient Lipschitz  if 

        cid               cid     cid cid        cid 

De nition     twicedifferentiable function     is  
strongly convex if      min          
Such smoothness guarantees imply that the gradient can
not change too rapidly  and strong convexity ensures that
there is   unique stationary point  and hence   global minimum  Standard analysis using these two properties shows
that gradient descent converges linearly to   global optimum   cid   see       Bubeck et al   
Theorem   Assume     is  cid smooth and  strongly convex  For any       if we run gradient descent with step
size      

 cid    iterate xt will be  close to   cid  in iterations 

 cid 
 

log

 cid        cid cid 

 

In   more general setting  we no longer have convexity  let
alone strong convexity  Though global optima are dif cult
to achieve in such   setting  it is possible to analyze convergence to  rstorder stationary points 
De nition   For   differentiable function     we say
that   is    rstorder stationary point if  cid      cid   
  we also say   is an  rstorder stationary point if
 cid      cid     
Under an  cid smoothness assumption  it is well known that
by choosing the step size      
 cid    gradient descent converges to  rstorder stationary points 
Theorem    Nesterov    Assume that the function
    is  cid smooth  Then  for any       if we run gradient
 cid  and termination condition
descent with step size      
 cid      cid      the output will be  rstorder stationary
point  and the algorithm will terminate within the following
number of iterations 

 cid            cid 

 

 

Note that the iteration complexity does not depend explicitly on intrinsic dimension  in the literature this is referred
to as  dimensionfree optimization 
Note that    rstorder stationary point can be either   local
minimum or   saddle point or   local maximum  For minimization problems  saddle points and local maxima are undesirable  and we abuse nomenclature to call both of them
 saddle points  in this paper  The formal de nition is as
follows 

De nition   For   differentiable function     we say
that   is   local minimum if   is    rstorder stationary
point  and there exists       so that for any   in the  
neighborhood of    we have               we also say  
is   saddle point if   is    rstorder stationary point but
not   local minimum  For   twicedifferentiable function
    we further say   saddle point   is strict  or nondegenerate  if  min          
For   twicedifferentiable function     we know   saddle
point   must satify  min          
Intuitively  for
saddle point   to be strict  we simply rule out the undetermined case  min           where Hessian information alone is not enough to check whether   is   local
minimum or saddle point  In most nonconvex problems 
saddle points are undesirable 
To escape from saddle points and  nd local minima in  
general setting  we move both the assumptions and guarantees in Theorem   one order higher 
In particular  we
require the Hessian to be Lipschitz 
De nition     twicedifferentiable function     is  
Hessian Lipschitz if 

        cid               cid     cid        cid 

That is  Hessian can not change dramatically in terms of
spectral norm  We also generalize the de nition of  rstorder stationary point to higher order 
De nition   For    Hessian Lipschitz function    
we say that   is   secondorder stationary point if
 cid      cid      and  min           we also say   is
 secondorder stationary point if 

 cid      cid     

and  min          

 

Secondorder stationary points are very important in nonconvex optimization because when all saddle points are
strict  all secondorder stationary points are exactly local
minima 
Note that the literature sometime de nes  secondorder
stationary point by two independent error terms       letting  cid      cid       and  min             We instead follow the convention of Nesterov   Polyak  
by choosing     
   to re ect the natural relations between the gradient and the Hessian 

 

  Main Result
In this section we show that it possible to modify gradient descent in   simple way so that the resulting algorithm
will provably converge quickly to   secondorder stationary
point 

How to Escape Saddle Points Ef ciently

 

Algorithm
PGD     cid               
      max log    cid  
gthres    
tnoise    tthres    
for               do

      fthres    

 

Perturbed

Gradient

Descent 

            

   cid   

 

 cid         
     
    tthres    
    

 cid 

 cid 
 

if  cid    xt cid    gthres and     tnoise   tthres then

 xt   xt 
xt    xt      

tnoise    

   uniformly       

if     tnoise   tthres and    xt       xtnoise     fthres
then
xt    xt       xt 

return  xtnoise

The algorithm that we analyze is   perturbed form of gradient descent  see Algorithm   The algorithm is based on
gradient descent with step size   When the norm of the
current gradient is small   gthres   which indicates that the
current iterate  xt is potentially near   saddle point  the algorithm adds   small random perturbation to the gradient 
The perturbation is added at most only once every tthres iterations 
To simplify the analysis we choose the perturbation    to
be uniformly sampled from   ddimensional ball  The use
of the threshold tthres ensures that the dynamics are mostly
those of gradient descent  If the function value does not
decrease enough  by fthres  after tthres iterations  the algorithm outputs  xtnoise  The analysis in this section shows that
under this protocol  the output  xtnoise is necessarily  close 
to   secondorder stationary point 
We  rst state the assumptions that we require 
Assumption    Function     is both  cid smooth and  
Hessian Lipschitz 

The Hessian Lipschitz condition ensures that the function
is wellbehaved near   saddle point  and the small perturbation we add will suf ce to allow the subsequent gradient
updates to escape from the saddle point  More formally  we
have 
Theorem   Assume that     satis es    Then there
exists an absolute constant cmax such that  for any    
       cid 
                    cid  and constant    
cmax  PGD     cid                will output an  secondorder stationary point  with probability   and terminate
in the following number of iterations 

 cid   cid            cid 

 

 

 cid    cid  

 cid cid 

 

log 

 

 Note that uniform sampling from   ddimensional ball can
      cid   cid  where    

be done ef ciently by sampling  
Uniform    and         Id   Harman   Lacko   

 

 min          cid     

Strikingly  Theorem   shows that perturbed gradient descent  nds   secondorder stationary point in almost the
same amount of time that gradient descent takes to  nd
 rstorder stationary point  The step size   is chosen as
  cid  which is in accord with classical analyses of convergence to  rstorder stationary points  Though we state
the theorem with   certain choice of parameters for simplicity of presentation  our result holds even if we vary the
parameters up to constant factors 
Without loss of generality  we can focus on the case    
 cid  as in Theorem  
In the case      cid  standard
gradient descent without perturbation Theorem  easily
solves the problem  This is because by    we always have
  which means that all  
secondorder stationary points are  rst order stationary
points 
We believe that the dependence on at least one log   factor in the iteration complexity is unavoidable in the nonconvex setting  as our result can be directly applied to the
principal component analysis problem  for which the best
known runtimes  for the power method or Lanczos method 
incur   log   factor due to random initialization  Establishing this formally is still an open question however 
To provide some intuition for Theorem   consider an iterate xt which is not yet an  secondorder stationary point 
By de nition  either   the gradient     xt  is large  or
  the Hessian     xt  has   signi cant negative eigenvalue  Traditional analysis works in the  rst case  The
crucial step in the proof of Theorem   involves handling
the second case  when the gradient is small  cid    xt cid   
gthres and the Hessian has   signi cant negative eigenvalue
 min    xt     
  then adding   perturbation  followed by standard gradient descent for tthres steps  decreases the function value by at least fthres  with high probability  The proof of this fact relies on   novel characterization of geometry around saddle points  see Section  
If we are able to make stronger assumptions on the objective function we are able to strengthen our main result  This
further analysis is presented in the next section 

  Functions with Strict Saddle Property

In many real applications  objective functions further admit
the property that all saddle points are strict  Ge et al   
Sun et al        Bhojanapalli et al    Ge et al 
  In this case  all secondorder stationary points are
local minima and hence convergence to secondorder stationary points  Theorem   is equivalent to convergence to
local minima 
To state this result formally  we introduce   robust version
of the strict saddle property  cf  Ge et al   
Assumption    Function     is      strict saddle 

How to Escape Saddle Points Ef ciently

That is  for any    at least one of following holds 

   cid      cid     
   min          
    is  close to    cid    the set of local minima 

Algorithm   Perturbed Gradient Descent with Local Improvement  PGDli     cid                 

     PGD     cid               
for               do
xt    xt    

    xt 

Intuitively  the strict saddle assumption states that the Rd
space can be divided into three regions      region where
the gradient is large      region where the Hessian has  
signi cant negative eigenvalue  around saddle point  and
  the region close to   local minimum  With this assumption  we immediately have the following corollary 
Corollary   Let     satisfy    and    Then  there
exists an absolute constant cmax such that  for any    
                  cid  constant     cmax  and letting
    min    PGD     cid                will output  
point  close to    cid  with probability       and terminate
in the following number of iterations 

 cid   cid            cid 

 

 

 cid    cid  

 cid cid 

 

log 

 

Corollary   shows after  nding  secondorder stationary
point by Theorem   where     min    the output is
also in the  neighborhood of some local minimum 
Note although Corollary   only explicitly asserts that the
output will lie within some  xed radius   from   local
minimum  In many real applications  we further have that
  can be written as   function   which decreases linearly or polynomially depending on   while   will be nondecreasing         In these cases  the above corollary further gives   convergence rate to   local minimum 

  Functions with Strong Local Structure

The convergence rate in Theorem   is polynomial in  
which is similar to that of Theorem   but is worse than the
rate of Theorem   because of the lack of strong convexity 
Although global strong convexity does not hold in the nonconvex setting that is our focus  in many machine learning
problems the objective function may have   favorable local
structure in the neighborhood of local minima  Ge et al 
  Sun et al        Sun   Luo    Exploiting
this property can lead to much faster convergence  linear
convergence  to local minima  One such property that ensures such convergence is   local form of smoothness and
strong convexity 
Assumption      In    neighborhood of the set of local
minima    cid  the function     is  strongly convex  and
 smooth 

Here we use different letter   to denote the local smoothness parameter  in contrast to the global smoothness parameter  cid  Note that we always have      cid 

However  often even local  strong convexity does not
hold  We thus introduce the following relaxation 
Assumption      In    neighborhood of the set of local
minima    cid  the function     satis es      regularity
condition if for any   in this neighborhood 

 
 

 cid     PX  cid     cid 

 cid         PX  cid     cid     
 

 cid      cid 
 
Here PX  cid    is the projection on to the set    cid  Note    
regularity condition is more general and is directly implied
by standard  smooth and  strongly convex conditions 
This regularity condition commonly appears in lowrank
problems such as matrix sensing and matrix completion 
and has been used in Bhojanapalli et al    Zheng  
Lafferty   where local minima form   connected set 
and where the Hessian is strictly positive only with respect
to directions pointing outside the set of local minima 
Gradient descent naturally exploits local structure very
well  In Algorithm   we  rst run Algorithm   to output
  point within the neighborhood of   local minimum  and
then perform standard gradient descent with step size  
   
We can then prove the following theorem 
Theorem   Let     satisfy       and      or
     Then there exists an absolute constant cmax such
for any                             cid 
that 
constant     cmax  and letting     min   
PGDli     cid                  will output   point that is  
close to    cid  with probability   in the following number
of iterations 

 cid   cid            cid 

 

 

 cid    cid  

 cid 

 

log 

 cid 

 

 

 
 

log

 
 

Theorem   says that if strong local structure is present 
the convergence rate can be boosted to linear convergence
    In this theorem we see that sequence of iterations
 log  
can be decomposed into two phases  In the  rst phase  perturbed gradient descent  nds    neighborhood by Corollary   In the second phase  standard gradient descent takes
us from   to  close to   local minimum  Standard gradient descent and Assumption      or      make sure that
the iterate never steps out of    neighborhood in this second phase  giving   result similar to Theorem   with linear
convergence 

How to Escape Saddle Points Ef ciently

  Example   Matrix Factorization
As   simple example to illustrate how to apply our general theorems to speci   nonconvex optimization problems  we consider   symmetric lowrank matrix factorization problem  based on the following objective function 

 

       

min

  Rd  

     cid   cid 
 

 cid UU cid      cid cid 
  

 
 
where   cid    Rd   
For simplicity  we assume
rank   cid       and denote  cid 
 
 
     cid  Clearly  in this case the global minimum of function value is zero  which is achieved at   cid    TD  where
TDT cid  is the SVD of the symmetric real matrix   cid 
The following two lemmas show that the objective function
in Eq    satis es the geometric assumptions      and
     Moreover  all local minima are global minima 
Lemma   For any      cid 
  the function       de ned in
Eq    is  smooth and  Hessian Lipschitz  inside
the region    cid   cid     
Lemma   For function       de ned in Eq    all local minima are global minima  The set of global minima
is    cid       cid   RR cid      cid        Furthermore 
   strict saddle  and satf     is    
     
is es      
   
     cid 
neighborhood of    cid 
One caveat is that since the objective function is actually
  fourthorder polynomial with respect to    the smoothness and Hessian Lipschitz parameters from Lemma   naturally depend on  cid   cid  Fortunately  we can further show
that gradient descent  even with perturbation  does not increase  cid   cid  beyond   max cid   cid   cid 
  Then  applying Theorem   gives 
Theorem   There exists an absolute constant cmax such
that the following holds  For the objective function in
for any       and constant     cmax 
Eq   
and for       max cid   cid   cid 
  the output of
PGDli       
  will be  
     cid 
close to the global minimum set    cid  with probability    
after the following number of iterations 

 regularity condition in    

   

 cid 
           

   cid 
   cid 

   cid 

     

   cid 

   cid 

 cid 

 cid   

 cid 

 cid 
 

 

 

log 

 cid    

 cid 

 cid 
 

 

 cid 
 
 cid 
 

log

 cid 
 
 

 cid 

 

Theorem   establishes global convergence of perturbed
gradient descent from an arbitrary initial point    including exact saddle points  Suppose we initialize at       
then our iteration complexity becomes 

  cid   cid  log   cid     cid  log cid 

   cid   

 cid 

where  cid     cid 
  is the condition number of the matrix
  cid  We see that in the  rst phase  to move from   neighborhood of the solution  our method requires   number of

iterations scaling as      cid  We suspect that this strong
dependence on condition number arises from our generic
assumption that the Hessian Lipschitz is uniformly upper
bounded  it may well be the case that this dependence can
be reduced in the special case of matrix factorization via  
 ner analysis of the geometric structure of the problem 

  Proof Sketch for Theorem  
In this section we will present the key ideas underlying
the main result of this paper  Theorem   We will  rst
argue the correctness of Theorem   given two important
intermediate lemmas  Then we turn to the main lemma 
which establishes that gradient descent can escape from
saddle points quickly  We present full proofs of all these
results in Appendix    Throughout this section  we use
     gthres  fthres and tthres as de ned in Algorithm  

  Exploiting Large Gradient or Negative Curvature

Recall that an  secondorder stationary point is   point
with   small gradient  and where the Hessian does not
have   signi cant negative eigenvalue  Suppose we are
currently at an iterate xt that is not an  secondorder stationary point       it does not satisfy the above properties  There are two possibilities    The gradient is large 
 cid    xt cid    gthres  or   Around the saddle point we have
 cid    xt cid    gthres and  min    xt     

 

The following two lemmas address these two cases respectively  They guarantee that perturbed gradient descent will
decrease the function value in both scenarios 

Lemma    Gradient  Assume that     satis es    Then
for gradient descent with stepsize      
 cid    we have
   xt       xt     

 cid    xt cid 

  

 informal  Assume that     satLemma    Saddle 
If xt satis es  cid    xt cid    gthres and
is es
 min    xt     
  then adding one perturbation
step followed by tthres steps of gradient descent  we have
   xt tthres        xt     fthres with high probability 

We see that Algorithm   is designed so that Lemma  
can be directly applied  According to these two lemmas  perturbed gradient descent will decrease the function value either in the case of   large gradient  or around
strict saddle points  Computing the average decrease in
function value yields the total iteration complexity  Since
Algorithm   only terminate when the function value decreases too slowly  this guarantees that the output must be
 secondorder stationary point  see Appendix   for formal
proofs 

How to Escape Saddle Points Ef ciently

have   quadratic function  the stuck region Xstuck consists
of points   such that       has   small    component  This
is   straight band in two dimensions and    at disk in high
dimensions  However  when the Hessian is not constant 
the shape of the stuck region is distorted  In two dimensions  it forms    narrow band  as plotted in Figure   on
top of the gradient  ow  In three dimensions  it forms  
 thin pancake  as shown in Figure  
The major challenge here is to bound the volume of this
highdimensional non at  pancake  shaped region Xstuck 
  crude approximation of this  pancake  by    at  disk 
loses polynomial factors in the dimensionalilty  which
gives   suboptimal rate  Our proof relies on the following
crucial observation  Although we do not know the explicit
form of the stuck region  we know it must be very  thin 
therefore it cannot have   large volume  The informal statement of the lemma is as follows 

Lemma    informal  Suppose    satis es the precondition of Lemma   and let    be the smallest eigendirection of        For any         and any two points
 
              if          re  and      
   then
at least one of      is not in the stuck region Xstuck 

 

Using this lemma it is not hard to bound the volume of
the stuck region  we can draw   straight line along the   
direction which intersects the perturbation ball  shown as
purple line segment in Figure   For any two points on
this line segment that are at least    
   away from each
other  shown as red points      in Figure   by Lemma  
we know at least one of them must not be in Xstuck  This
implies if there is one point      Xstuck on this line segment 
then Xstuck on this line can be at most an interval of length
 
  around     This establishes the  thickness  of Xstuck
   
in the    direction  which is turned into an upper bound on
the volume of the stuck region Xstuck by standard calculus 

  Conclusion
This paper presents the  rst  nearly  dimensionfree result
for gradient descent in   general nonconvex setting  We
present   general convergence result and show how it can
be further strengthened when combined with further structure such as strict saddle conditions and or local regularity convexity 
There are still many related open problems  First  in the
presence of constraints  it is worthwhile to study whether
gradient descent still admits similar sharp convergence results  Another important question is whether similar techniques can be applied to accelerated gradient descent  We
hope that this result could serve as    rst step towards  
more general theory with strong  almost dimension free
guarantees for nonconvex optimization 

Figure   Pertubation ball in    and  thin pancake  stuck region

Figure    Narrow band  stuck region in    under gradient  ow

  Escaping from Saddle Points Quickly

The proof of Lemma   is straightforward and follows from
traditional analysis  The key technical contribution of this
paper is the proof of Lemma   which gives   new characterization of the geometry around saddle points 
Consider   point    that satis es the the preconditions of
Lemma    cid      cid    gthres and  min        
 
  After adding the perturbation           we can
view    as coming from   uniform distribution over       
which we call the perturbation ball  We can divide this
perturbation ball        into two disjoint regions    an escaping region Xescape which consists of all the points    
       whose function value decreases by at least fthres after tthres steps      stuck region Xstuck            Xescape 
Our general proof strategy is to show that Xstuck consists of
  very small proportion of the volume of perturbation ball 
After adding   perturbation to     point    has   very small
chance of falling in Xstuck  and hence will escape from the
saddle point ef ciently 
Let us consider the nature of Xstuck  For simplicity  let us
imagine that    is an exact saddle point whose Hessian has
only one negative eigenvalue  and       positive eigenvalues  Let us denote the minimum eigenvalue direction as
   In this case  if the Hessian remains constant  and we

 How to Escape Saddle Points Ef ciently

References
Agarwal  Naman  AllenZhu  Zeyuan  Bullins  Brian 
Hazan  Elad  and Ma  Tengyu  Finding approximate local minima for nonconvex optimization in linear time 
arXiv preprint arXiv   

Bhojanapalli  Srinadh  Neyshabur  Behnam  and Srebro 
Nathan  Global optimality of local search for low
rank matrix recovery  arXiv preprint arXiv 
 

Bubeck    ebastien et al  Convex optimization  Algorithms
and complexity  Foundations and Trends   cid  in Machine
Learning     

Candes  Emmanuel    Li  Xiaodong  and Soltanolkotabi 
Mahdi  Phase retrieval via wirtinger  ow  Theory and
algorithms  IEEE Transactions on Information Theory 
   

Carmon  Yair and Duchi  John    Gradient descent ef 
ciently  nds the cubicregularized nonconvex newton
step  arXiv preprint arXiv   

Carmon  Yair  Duchi  John    Hinder  Oliver  and Sidford 
Aaron  Accelerated methods for nonconvex optimization  arXiv preprint arXiv   

Choromanska  Anna  Henaff  Mikael  Mathieu  Michael 
Arous    erard Ben  and LeCun  Yann  The loss surface
of multilayer networks  arXiv   

Curtis  Frank    Robinson  Daniel    and Samadi  Mohammadreza    trust region algorithm with   worstcase iteration complexity of  mathcal     epsilon  for
nonconvex optimization  Mathematical Programming 
pp     

Dauphin  Yann    Pascanu  Razvan  Gulcehre  Caglar 
Cho  Kyunghyun  Ganguli  Surya  and Bengio  Yoshua 
Identifying and attacking the saddle point problem in
highdimensional nonconvex optimization  In Advances
in Neural Information Processing Systems  pp   
   

Ge  Rong  Huang  Furong  Jin  Chi  and Yuan  Yang  Escaping from saddle points online stochastic gradient
for tensor decomposition  In COLT   

Ge  Rong  Lee  Jason    and Ma  Tengyu  Matrix completion has no spurious local minimum  In Advances in
Neural Information Processing Systems  pp   
 

Harman  Radoslav and Lacko  Vladim    On decompositional algorithms for uniform sampling from nspheres
and nballs  Journal of Multivariate Analysis   
   

Jain  Prateek  Jin  Chi  Kakade  Sham    and Netrapalli 
Praneeth  Computing matrix squareroot via non convex
local search  arXiv preprint arXiv   

Kawaguchi  Kenji  Deep learning without poor local minima  In Advances In Neural Information Processing Systems  pp     

Lee  Jason    Simchowitz  Max  Jordan  Michael    and
Recht  Benjamin  Gradient descent only converges to
In Conference on Learning Theory  pp 
minimizers 
   

Levy         The power of normalization  Faster evasion of

saddle points  arXiv preprint arXiv   

Nesterov  Yu 

Introductory lectures on convex program 

ming volume    Basic course  Lecture notes   

Nesterov  Yurii and Polyak  Boris    Cubic regularization
of newton method and its global performance  Mathematical Programming     

Netrapalli  Praneeth  Jain  Prateek  and Sanghavi  Sujay 
In AdPhase retrieval using alternating minimization 
vances in Neural Information Processing Systems  pp 
   

Park  Dohyung  Kyrillidis  Anastasios  Caramanis  Constantine  and Sanghavi  Sujay 
Nonsquare matrix
sensing without spurious local minima via the burermonteiro approach  arXiv preprint arXiv 
 

Rumelhart  David    Hinton  Geoffrey    and Williams 
Ronald    Learning representations by backpropagating
errors  Cognitive modeling     

Sun  Ju  Qu  Qing  and Wright  John  Complete dictionary
recovery over the sphere    Overview and the geometric picture  IEEE Transactions on Information Theory 
   

Sun  Ju  Qu  Qing  and Wright  John    geometric analIn Information Theory  ISIT 
ysis of phase retrieval 
  IEEE International Symposium on  pp   
IEEE     

Sun  Ruoyu and Luo  ZhiQuan  Guaranteed matrix completion via nonconvex factorization  IEEE Transactions
on Information Theory     

Zheng  Qinqing and Lafferty  John  Convergence analysis
for rectangular matrix completion using burermonteiro
arXiv preprint
factorization and gradient descent 
arXiv   

