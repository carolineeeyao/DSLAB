  Simulated Annealing Based Inexact Oracle

for Wasserstein Loss Minimization

Jianbo Ye   James    Wang   Jia Li  

Abstract

Learning under   Wasserstein loss         Wasserstein loss minimization  WLM  is an emerging
research topic for gaining insights from   large
set of structured objects  Despite being conceptually simple  WLM problems are computationally challenging because they involve minimizing over functions of quantities       Wasserstein
distances  that themselves require numerical algorithms to compute 
In this paper  we introduce   stochastic approach based on simulated
annealing for solving WLMs  Particularly  we
have developed   Gibbs sampler to approximate
effectively and ef ciently the partial gradients of
  sequence of Wasserstein losses  Our new approach has the advantages of numerical stability
and readiness for warm starts  These characteristics are valuable for WLM problems that often require multiple levels of iterations in which
the oracle for computing the value and gradient
of   loss function is embedded  We applied the
method to optimal transport with Coulomb cost
and the Wasserstein nonnegative matrix factorization problem  and made comparisons with the
existing method of entropy regularization 

  Introduction
An oracle is   computational module in an optimization
procedure that is applied iteratively to obtain certain characteristics of the function being optimized  Typically  it
calculates the value and gradient of loss function         In
the vast majority of machine learning models  where those
loss functions are decomposable along each dimension
      Lp norm  KL divergence  or hinge loss  rxl     or
 College of Information Sciences and Technology  The Pennsylvania State University  University Park  PA   Department of
Statistics  The Pennsylvania State University  University Park 
PA  Correspondence to  Jianbo Ye  jxy ist psu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ryl    is computed in      time    being the complexity of outcome variables   or    This part of calculation is
often negligible compared with the calculation of full gradient with respect to the model parameters  But this is no
longer the case in learning problems based on Wasserstein
distance due to the intrinsic complexity of the distance 
We will call such problems Wasserstein loss minimization
 WLM  Examples of WLMs include Wasserstein barycenters  Li   Wang    Agueh   Carlier    Cuturi  
Doucet    Benamou et al    Ye   Li    Ye
et al      principal geodesics  Seguy   Cuturi   
nonnegative matrix factorization  Rolet et al    Sandler   Lindenbaum    barycentric coordinate  Bonneel et al    and multilabel classi cation  Frogner
et al   
Wasserstein distance is de ned as the cost of matching two
probability measures  originated from the literature of optimal transport  OT   Monge    It takes into account
the crossterm similarity between different support points
of the distributions    level of complexity beyond the usual
vector data treatment       to convert the distribution into  
vector of frequencies  It has been promoted for comparing
sets of vectors       bagof words models  by researchers
in computer vision  multimedia and more recently natural
language processing  Kusner et al    Ye et al     
However  its potential as   powerful loss function for machine learning has been underexplored  The major obstacle
is   lack of standardized and robust numerical methods to
solve WLMs  Even to empirically better understand the
advantages of the distance is of interest 
As   longstanding consensus  solving WLMs is challenging  Cuturi   Doucet    Unlike the usual optimization in machine learning where the loss and the  partial 
gradient can be calculated in linear time  these quantities
are nonsmooth and hard to obtain in WLMs  requiring solution of   costly network transportation problem        
OT  The time complexity       log    is prohibitively
high  Orlin    In contrast to the Lp or KL counterparts  this step of calculation elevates from   negligible
fraction of the overall learning problem to   dominant portion  preventing the scaling of WLMs to large data  Recently  iterative approximation techniques have been developed to compute the loss and the  partial  gradient at com 

  Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

plexity       Cuturi    Wang   Banerjee   
However  nontrivial algorithmic efforts are needed to incorporate these methods into WLMs because WLMs often
require multilevel loops  Cuturi   Doucet    Frogner
et al    Speci cally  one must recalculate through
many iterations the loss and its partial gradient in order to
update other model dependent parameters 
We are thus motivated to seek for   fast inexact oracle
that     runs at lower time complexity per iteration  and
 ii  accommodates warm starts and meaningful early stops 
These two properties are equally important for ef ciently
obtaining adequate approximation to the solutions of   sequence of slowly changing OTs  The second property ensures that the subsequent OTs can effectively leverage the
solutions of the earlier OTs so that the total computational
time is low  Approximation techniques with low complexity per iteration already exist for solving   single OT  but
they do not possess the second property  In this paper  we
introduce   method that uses   timeinhomogeneous Gibbs
sampler as an inexact oracle for Wasserstein losses  The
Markov chain Monte Carlo  MCMC  based method naturally satis es the second property  as re ected by the intuition of physicists that MCMC samples can ef ciently
 remix from   previous equilibrium 
We propose   new optimization approach based on Simulated Annealing  SA   Kirkpatrick et al    Corana
et al    for WLMs where the outcome variables are
treated as probability measures  SA is especially suitable
for the dual OT problem  where the usual Metropolis sampler can be simpli ed to   Gibbs sampler  To our knowledge  existing optimization techniques used on WLMs are
different from MCMC  In practice  MCMC is known to
easily accommodate warm start  which is particularly useful in the context of WLMs  We name this approach GibbsOT for short  The algorithm of GibbsOT is as simple
and ef cient as the Sinkhorn   algorithm     widely accepted method to approximately solve OT  Cuturi   
We show that GibbsOT enjoys improved numerical stability and several algorithmic characteristics valuable for
general WLMs  By experiments  we demonstrate the effectiveness of GibbsOT for solving optimal transport with
Coulomb cost  Benamou et al    and the Wasserstein
nonnegative matrix factorization  NMF  problem  Sandler
  Lindenbaum    Rolet et al   

  Related Work
Recently  several methods have been proposed to overcome
the aforementioned dif culties in solving WLMs  Representatives include entropic regularization  Cuturi   
Cuturi   Doucet    Benamou et al    and Bregman ADMM  Wang   Banerjee    Ye et al     
The main idea is to augment the original optimization prob 

lem with   strongly convex term such that the regularized
objective becomes   smooth function of all its coordinating parameters  Neither the Sinkhorn   algorithm nor Bregman ADMM can be readily integrated into   general WLM 
Based on the entropic regularization of primal OT  Cuturi  
Peyr     recently showed that the Legendre transform
of the entropy regularized Wasserstein loss and its gradient can be computed in closed form  which appear in the
 rstorder condition of some complex WLM problems  Using this technique  the regularized primal problem can be
converted to an equivalent Fencheltype dual problem that
has   faster numerical solver in the Euclidean space  Rolet et al    But this methodology can only be applied
to   certain class of WLM problems of which the Fencheltype dual has closed forms of objective and full gradient 
In contrast  the proposed SAbased approach directly deals
with the dual OT problem without assuming any particular
mathematical structure of the WLM problem  and hence is
more  exible to apply 
More recent approaches base on solving the dual OT problems have been proposed to calculate and optimize the
Wasserstein distance between   single pair of distributions
with very large support sets   often as large as the size
of an entire machine learning dataset  Montavon et al 
  Genevay et al    Arjovsky et al    For
these methods  scalability is achieved in terms of the support size  Our proposed method has   different focus on
calculating and optimizing Wasserstein distances between
many pairs all together in WLMs  with each distribution
having   moderate support size       dozens or hundreds 
We aim at scalability for the scenarios when   large set of
distributions have to be handled simultaneously  that is  the
optimization cannot be decoupled on the distributions  In
addition  existing methods have no onthe   mechanism
to control the approximation quality at   limited number of
iterations 

  Preliminaries of Optimal Transport
In this section  we present notations  mathematical backgrounds  and set up the problem of interest 
De nition    Optimal Transportation  OT  Let    
            where    is the set of mdimensional
simplex    
    hq        The set of transdef 
portation plans between   and   is de ned as       
 
     Rm                  ZT           Let
    Rm   
be the matrix of costs  The optimal transport cost between   and   with respect to   is

       Rm

def 

 

        

def 
  min

      hZ  Mi  

 

In particular    is often called the coupling set 

  Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Now we relate primal version of  discrete  OT to   variant of its dual version  One may refer to Villani   for
the general background of the KantorovichRubenstein duality  In particular  our formulation introduces an auxiliary
parameter CM for the sake of mathematical soundness in
de ning Boltzmann distributions 
De nition    Dual Formulation of OT  Let CM     denote vector             gm   by    and vector             hm  
by    We de ne the dual domain of OT by

def 

    

                Rm   

  CM   gi   hj   Mi                            

 

Informally  for   suf ciently large CM  subject to         
the LP problem Eq    can be reformulated as  

           max

     hp  gi   hq  hi  

 

Let the optimum set be      Then any optimal point
                   constructs    projected  subgradient such that           and              The main
computational dif culty of WLMs comes from the fact that
 projected  subgradient    is not ef ciently solvable 
Note that      is an unbound set in Rm    In order
to constrain the feasible region to be bounded  we alternatively de ne

                                

 

One can show that the maximization in      as Eq   
is equivalent
to the maximization in      because
hp        hq      
  Simulated Annealing for Optimal

Transport via Gibbs Sampling

Following the basic strategy outlined in the seminal paper of simulated annealing  Kirkpatrick et al    we
present the de nition of Boltzmann distribution supported
on      below which  as we will elaborate  links the
dual formulation of OT to   Gibbs sampling scheme  Algorithm   below 
De nition    Boltzmann Distribution of OT  Given  
temperature parameter       the Boltzmann distribution

 However  for any proper   and strictly positive       there
exists CM such that the optimal value of primal problem is equal
to the optimal value of the dual problem  This modi cation is
solely for an adhoc treatment of   single OT problem  In general
cases of            when CM is pre xed  the solution of Eq   
may be suboptimal 

of OT is   probability measure on        Rm   
such that

              exp   

 

 hp  gi   hq  hi   

 

It is   wellde ned probability measure for an arbitrary  
nite CM    

The basic concept behind SA states that the samples from
the Boltzmann distribution will eventually concentrate at
the optimum set of its deriving problem                as
      However  since the Boltzmann distribution is often dif cult to sample    practical convergence rate remains
mostly unsettled for speci   MCMC methods 
Because      de ned by Eq     also   has   conditional independence structure among variables    Gibbs
sampler can be naturally applied to the Boltzmann distribution de ned by Eq    We summarize this result below 
Proposition   Given any                   and any
CM     we have for any   and   

gi   Ui   
hj   Lj   

and

def 
 

min

     
def 
  max
     

 Mi     hj   

 gi   Mi     

 

 

 

 

gi  bLi   
hj   bUj   

def 
  max
     
def 
  max
     

 CM   hj   
 CM   gi   

Here Ui   Ui    and Lj   Lj    are auxiliary variables 
Suppose   follows the Boltzmann distribution by Eq   
gi   are conditionally independent given    and likewise
hj   are also conditionally independent given    Furthermore  it is immediate from Eq    that each of their conditional probabilities within its feasible region  subject to
CM  satis es

hjqj

 

  gi      exp  gipi
      bLi      gi   Ui   
  hj      exp 
      Lj      hj   bUj   
where            and           
Remark   As CM     bUj        and bLi     
  For            and            one can approximate the conditional probability   gi    and   hj   
by exponential distributions 

 

By Proposition    our proposed timeinhomogeneous
Gibbs sampler is given in Algorithm   Speci cally in Algorithm   the variable    is  xed to zero by the de nition

  Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Algorithm   Gibbs Sampling for Optimal Transport
Given                    and         and
                       for                  we de ne the following Markov chain

  Randomly sample

            

For                    let

       Exponential 
  Mi   

  max         
              qj

      

 

     

 
    
 

  Randomly sample

For                    let

            

       Exponential 
  min     Mi         
   

              pi

       

      

 
    
 

 

 

and sampling     
 

of      But we have found in experiments that by calculating      
in Algorithm   according
 
to Eq    one can still generate MCMC samples from
     such that the energy quantity hp  gi   hq  hi converges to the same distribution as that of MCMC samples
from      Therefore  we will not assume        from
now on and develop analysis solely for the unconstrained
version of GibbsOT 
Figure   illustrates the behavior of the proposed Gibbs sampler with   cooling schedule at different temperatures  As
  decreases along iterations  the   percentile band for
sample   becomes thinner and thinner 
Remark   Algorithm   does not specify the actual cooling schedule  nor does the analysis of the proposed Gibbs
sampler in Theorem    We have been agnostic here for  
reason  In the SA literature  cooling schedules with guaranteed optimality are often too slow to be useful in practice 
To our knowledge  the guaranteed rate of SA approach is
worse than the combinatorial solver for OT  As   result   
wellaccepted practice of SA for many complicated optimization problems is to empirically adjust cooling schedules    strategy we take for our experiments 
Remark   Although the exact cooling schedule is not speci ed  we still provide   quantitative upper bound of the chosen temperature   at different iterations in Appendix  
Eq    One can calculate such bound at the cost of
  log   at certain iterations to check whether the current
temperature is too high for the used GibbsOT to accurately

approximate the Wasserstein gradient  In practice  we  nd
this bound helps one quickly select the beginning temperature of GibbsOT algorithm 
De nition    Notations for Auxiliary Statistics  Besides
the Gibbs coordinates   and    the GibbsOT sampler naturally introduces two auxiliary variables    and    Let
 
Likewise  denote the collection of     
  by vectors
 
     and      respectively  The following sequence of auxiliary statistics

and       hU    

      hL   

  iT

  iT

                 

                

and     

                                

def 
 

            

          

          

                

for                 is also   Markov chain  They can be rede ned equivalently by specifying the transition probabilities   zn zn  for                          the conditional
                  for                 and           
for                    
One may notice that the alternative representation converts
the Gibbs sampler to one whose structure is similar to  
hidden Markov model  where the      chain is conditional
independent given the      chain and has  factored  exponential emission distributions  We will use this equivalent
representation in Appendix   and develop analysis based
on the      chain accordingly 
Remark   We now consider the function
  hp  xi   hq  yi  

and de ne   few additional notations  Let    Ut  Lt  be
denoted by    zt    where        or     
If      are
independently resampled according to Eq    and  
we will have the inequalities that

        

def 

           zn       zn   

loss
the equilibrium of Boltzmann distribution

Both       and          converges to the exact
         at
            as        
  GibbsOT  An Inexact Oracle for WLMs
In this section  we introduce   nonstandard SA approach
for the general WLM problems  The main idea is to replace
the standard Boltzmann energy with an asymptotic consistent upper bound  outlined in our previous section  Let

    

   pi  qi 

   Xi 

 The conditional quantity    zn            zn is the sum
of two Gamma random variables  Gamma            
Gamma           where        or           

  Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

 
 
 
 
 
 
 
 
 
 
 

 
 
   
   

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 

 
 
   
   

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 

 
 
   
   

 

 

 

 

 

 

      iterations

      iterations

      iterations

Figure   The Gibbs sampling of the proposed SA method  From left to right is an illustrative example of   simple    optimal transportation problem with Coulomb cost and plots of variables for solving this problem at different number of iterations        
using the inhomogeneous Gibbs sampler  Particularly  the   percentile of the exponential distributions are marked by the gray area 
be our prototyped objective function  where   represents  
Markov chain is roughly considered mixed if every   iteration the quantity          almost  stops increasing  
dataset  pi  qi are prototyped probability densities for representing the ith instance  We now discuss how to solve
by default  say  for some   
min    
To minimize the Wasserstein losses          approximately in such WLMs  we propose to instead optimize
its asymptotic consistent upper bound         at equilibrium of Boltzmann distribution             using its stochastic gradients              and               
Therefore  one can calculate the gradient approximately 

                                  

we terminate the Gibbs iterations 

     

   pi Ui     qi Li 

   Xi 

where    is the Jacobian  Ui  Li are computed from
Algorithm   for the problem    pi  qi  respectively  Together with the iterative updates of model parameters  
one gradually anneals the temperature     The equilibrium
of             becomes more and more concentrated  We assume the inexact oracle at   relatively higher temperature
is adequate for early updates of the model parameters  but
sooner or later it becomes necessary to set   smaller to better approximate the exact loss 
It is well known that the variance of stochastic gradient
usually affects the rate of convergence  The reason to replace          with       as the inexact oracle  for some
      is motivated by the same intuition  The variances
of MCMC samples     
of Algorithm   can be very
 
large if pi   and qj   are small  making the embedded
 rstorder method inaccurate unavoidably  But we  nd the
variances of max min statistics      
  are much smaller 
Fig    shows an example  The bias introduced in the replacement is also well controlled by decreasing the temperature parameter     For the sake of ef ciency  we use
  very simple convergence diagnostics in the practice of
GibbsOT  We check the values of         such that the

      
 

      

 

  Applications of GibbsOT
  Toy OT Examples
   Case with Euclidean Cost  We  rst illustrate the differences between the approximate primal solutions computed by different methods by replicating   toy example
in  Benamou et al    The toy example calculates the
OT between two    twomode distributions  We visualize
their solved coupling as      image in Fig    at the budgets
in terms of different number of iterations  Given their different convergence behaviors  when one wants to compromise with using preconverged primal solutions in WLMs 
he or she has to account for the different results computed
by different numerical methods  even though they all aim
at the Wasserstein loss 
As   note  Sinkhorn  BADMM and GibbsOT share the
same computational complexity per iteration  The difference in their actual CPU time comes from the different
arithmetic operations used  BADMM may be the slowest
because it requires log  and exp  operations  When
memory ef ciency is of concern  both the implementations of Sinkhorn and GibbsOT can be modi ed to take
only           additional memory besides the space for
caching the cost matrix   
Two Electrons with Coulomb Cost in DFT  In quantum
mechanics  Coulomb cost  or electronelectron Coulomb
repulsion  is an important energy functional in Density
Functional Theory  DFT  Numerical methods that solve

  Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

    For GibbsOT  we use   geometric temperature
scheme such that              at the nth iteration 
where   is the max iteration number  For the unbounded
Coulomb cost  Bregman ADMM  Wang   Banerjee   
does not converge to   solution close to the true optimum 

  Wasserstein NMF
We now illustrate how the proposed GibbsOT can be used
as   readyto plugin inexact oracle for   typical WLM  
Wasserstein NMF  Sandler   Lindenbaum    Rolet
et al    The data parallelization of this framework is
natural because the GibbsOT samplers subject to different
instances are independent 
Problem Formulation  Given   set of discrete probability measures     
    data  over Rd  we want to estimate   model        
   such that for each    
there exists   membership vector               
PK
      where each    is again   discrete probability measure to be estimated  Therefore  Wasserstein
      where
NMF reads min Pn

                  is the collection of membership vectors  and   is the Wasserstein distance  One can write the
problem by plugging Eq    in the dual formulation 

       PK

      

      

min
 

max
   fi  

  

    
   xi  

  hhbw    gii   hw    hiii  
   Xn
         Xm
      XK
fi                 

   
      

 

  

 

 

where bw         is the weight vector of discrete probability measure      and         mi is the weight vector of         denotes the transportation cost matrix
between the supports of two measures  The global optimization solves all three sets of variables         In the
      xi  
sequel  we assume support points of      
  
are shared and pre xed 
Algorithm  At every epoch  one updates variables either
sequentially  indexed by    or all together  It is done by
 rst executing the GibbsOT oracle subject to the ith instance and then updating      and the membership vector     accordingly at   chosen step size     At the
end of each epoch  the temperature parameter   is adjusted
   mi   For
each instance    the algorithm proceeds with the following
steps iteratively 

       where       
nPn

           

  Initiate from the last computed     sample subject
to instance    execute the GibbsOT Gibbs sampler at

 

 
 

 

 
 
 

 

 
 
 

 

 
 
 
 

 

 
 
 
 
 

 

 
 
 
 
 

IBP  rho  

IBP  rho  

IBP  rho  

BADMM

SimulAnn

Figure     simple example for OT between two    distribution  The solutions by Iterative Bregman Projection  BADMM 
and GibbsOT are shown in pink  while the exact solution
by linear programming is shown in green 
Images in the
rows from top to bottom present results at different iterations
            The left three columns are by IBP
with                 where     is discretized with
      uniformly spaced points  The fourth column is by BADMM  with default parameter       The last column is
the proposed GibbsOT  with   geometric cooling schedule  With
  properly selected cooling schedule  one can achieve fast convergence of OT solution without comprising much solution quality 

the multimarginal OT problem with unbounded costs remains an open challenge in DFT  Benamou et al   
We consider two uniform densities on    domain    
with Coulomb cost                   which has analytic
solutions  Coulumb cost is different from the usual metric
cost in the OT literature  which is unbounded and singular at        As observed in  Benamou et al    the
entropic regularized primal solution becomes more concentrated at boundaries  which is not physically plausible  This
effect is not observed in the GibbsOT solution as shown in
Appendix Fig    As shown by Fig   the variables     
in computation are always in bounded range  with an overwhelming probability  thus the algorithm does not endure
any numerical dif culties 
For entropic regularization  Benamou et al     
we empirically select the minimal   which does not cause
numerical over ow before   iterations  in which    

  Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Figure   The recovered primal solutions for two uniform    distribution with Coulumb cost  The approximate solutions are shown in
pink  while the exact solution by linear programming is shown in green  Top row  entropic regularization with         Bottom row 
GibbsOT  Images in the rows from left to right present results at different max iterations              

constant temperature   until   mixing criterion is met 
and get Ui 

  For                  update           based on gradient    
  Ui using the iterates of online mirror descent
 MD  subject to the stepsize    Beck   Teboulle 
 

  Also update the membership vector          based
on gradient  hv  Uii         hv    Uii   using the
iterates of accelerated mirror descent  AMD  with
restarts subject to the same stepsize    Krichene
et al   

We note that the practical speedups we achieved via the
above procedure is the warmstart feature in Step   If one
uses   blackbox OT solver  this dimension of speedups is
not viable 
Results  We investigate the empirical convergence of the
proposed Wasserstein NMF method by two datasets  one
is   subset of MNIST handwritten digit images which contains   digits of   and the other is the ORL  face
dataset  Our results are based on        implementation
with vectorization  In particular  we set          
for both datasets  The learned components are visualized together with alternative approaches  smoothed WNMF  Rolet et al    and regular NMF  in Appendix
Figs    and   From these  gures  we observe that our
learned components using GibbsOT are shaper than the
smoothed WNMF  This can be explained by the fact that
GibbsOT can potentially push for higher quality of approximation by gradually annealing the temperature  We
also observe that the learned components might possess
some saltand pepper noise  This is because the Wasserstein distance by de nition is not very sensitive to the subpixel displacements  On   singlecore of     GHz Intel

Core    CPU  the average time spent for each epoch for
these two datasets are   seconds and   seconds  respectively  It is about two magnitude faster than fully solving all OTs via   commercial LP solver  

  Discussions
The solution of primal OT  MongeKantorovich problem 
have many direct interpretations  where the solved transport is   coupling between two measures  Hence  it could
be well motivated to consider regularizing the solution on
the primal domain in those problems  Cuturi    Meanwhile  the solution of dual OT can be meaningful in its
own right  For instance  in  nance  the dual solution is
directly interpreted as the vanilla prices implementing robust static superhedging strategies  The entropy regularized OT  under the Fencheltype dual  provides   smoothed
unconstrained dual problem as shown in  Cuturi   Peyr   
  In this paper  we develop GibbsOT  whose solutions respect the dual feasibility of OT and are subject to  
different regularization effect as explained by  Abernethy
  Hazan    It is   numerical stable and computational
suitable oracle to handle WLM 
Acknowledgement  This material is based upon work supported by the National Science Foundation under Grant
Nos  ECCS  and DMS  The authors
would also like to thank anonymous reviewers for their
valuable comments 

 We use the specialized network  ow solver in Mosek
 https www mosek com  for the computation  which is
found faster than general simplex or IPM solver at moderate problem scale 

  Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Figure   NMF components learned by different methods     
  on the   digit   images  Top  regular NMF  Middle 
WNMF with entropic regularization              
  Bottom  WNMF using GibbsOT  It is observed that the
components of WNMF with entropic regularization are smoother
than those optimized with GibbsOT 

References
Abernethy  Jacob and Hazan  Elad  Faster convex optimization  Simulated annealing with an ef cient universal barrier  arXiv preprint arXiv   

Agueh  Martial and Carlier  Guillaume  Barycenters in the
Wasserstein space  SIAM    Math  Analysis   
   

Figure   NMF components learned by different methods     
  on the ORL face images  Top  regular NMF  Middle  WNMF with entropic regularization              
  Bottom  WNMF using GibbsOT  in which the salt and
pepper noises are observed due to the fact that Wasserstein distance is insensitive to the subpixel mass displacement  Cuturi  
Peyr     

Wasserstein GAN  arXiv preprint arXiv 
 

Arjovsky  Martin  Chintala  Soumith  and Bottou    eon 

Beck  Amir and Teboulle  Marc  Mirror descent and non 

  Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

linear projected subgradient methods for convex optimization  Operations Research Letters   
 

Benamou  JeanDavid  Carlier  Guillaume  Cuturi  Marco 
Nenna  Luca  and Peyr    Gabriel  Iterative Bregman projections for regularized transportation problems  SIAM   
on Scienti   Computing         

Benamou  JeanDavid  Carlier  Guillaume  and Nenna 
Luca    numerical method to solve multimarginal optimal transport problems with Coulomb cost  In Splitting
Methods in Communication  Imaging  Science  and Engineering  pp    Springer   

Bonneel  Nicolas  Peyr    Gabriel  and Cuturi  Marco 
Wasserstein barycentric coordinates  Histogram regression using optimal transport  ACM Trans  on Graphics 
   

Corana  Angelo  Marchesi  Michele  Martini  Claudio  and
Ridella  Sandro  Minimizing multimodal functions of
continuous variables with the simulated annealing algorithm corrigenda for this article is available here  ACM
Trans  on Mathematical Software     

Cuturi  Marco  Sinkhorn distances  Lightspeed computation of optimal transport  In Advances in Neural Information Processing Systems  pp     

Cuturi  Marco and Doucet  Arnaud  Fast computation of
In Proc  Int  Conf  Machine

Wasserstein barycenters 
Learning  pp     

Cuturi  Marco and Peyr    Gabriel    smoothed dual approach for variational Wasserstein problems  SIAM    on
Imaging Sciences     

Kusner  Matt  Sun  Yu  Kolkin  Nicholas  and Weinberger 
Kilian  From word embeddings to document distances 
In Proc  of the Int  Conf  on Machine Learning  pp   
   

Li  Jia and Wang  James    Realtime computerized annotation of pictures  IEEE Trans  Pattern Analysis and
Machine Intelligence     

Monge  Gaspard    emoire sur la th eorie des   eblais et des

remblais  De   Imprimerie Royale   

Montavon  Gr egoire    uller  KlausRobert  and Cuturi 
Marco  Wasserstein training of restricted boltzmann machines  In Lee        Sugiyama     Luxburg       
Guyon     and Garnett      eds  Advances in Neural Information Processing Systems   pp     

Orlin  James      faster strongly polynomial minimum
cost  ow algorithm  Operations Research   
   

Rolet  Antoine  Cuturi  Marco  and Peyr    Gabriel  Fast
dictionary learning with   smoothed Wasserstein loss  In
AISTAT   

Sandler  Roman and Lindenbaum  Michael  Nonnegative
matrix factorization with earth mover   distance metric 
In Proc  of the Conf  on Computer Vision and Pattern
Recognition  pp    IEEE   

Seguy  Vivien and Cuturi  Marco  Principal geodesic analysis for probability measures under the optimal transport
metric  In Advances in Neural Information Processing
Systems  pp     

Villani    edric  Topics in Optimal Transportation  Num 

ber   American Mathematical Soc   

Frogner  Charlie  Zhang  Chiyuan  Mobahi  Hossein 
Araya  Mauricio  and Poggio  Tomaso    Learning with
  Wasserstein loss  In Advances in Neural Information
Processing Systems  pp     

Wang  Huahua and Banerjee  Arindam  Bregman alternating direction method of multipliers  In Advances in
Neural Information Processing Systems  pp   
 

Genevay  Aude  Cuturi  Marco  Peyr    Gabriel  and Bach 
Francis  Stochastic optimization for largescale optimal
transport  In Lee        Sugiyama     Luxburg       
Guyon     and Garnett      eds  Advances in Neural Information Processing Systems   pp     

Kirkpatrick  Scott  Gelatt     Daniel  Jr  and Vecchi 
Mario    Optimization by simmulated annealing  Science     

Krichene  Walid  Bayen  Alexandre  and Bartlett  Peter   
Accelerated mirror descent in continuous and discrete
time  In Advances in Neural Information Processing Systems  pp     

Ye  Jianbo and Li  Jia  Scaling up discrete distribution clustering using admm  In Proc  of the Int  Conf  on Image
Processing  pp    IEEE   

Ye  Jianbo  Li  Yanran  Wu  Zhaohui  Wang  James    Li 
Wenjie  and Li  Jia  Determining gains acquired from
word embedding quantitatively using discrete distribution clustering 
In Proc  of the Annual Meeting of the
Association for Computational Linguistics     

Ye  Jianbo  Wu  Panruo  Wang  James    and Li  Jia 
Fast discrete distribution clustering using Wasserstein
barycenter with sparse support  IEEE Trans  on Signal
Processing       

