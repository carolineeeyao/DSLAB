HighDimensional VarianceReduced Stochastic Gradient

ExpectationMaximization Algorithm

Rongda Zhu   Lingxiao Wang   Chengxiang Zhai   Quanquan Gu  

Abstract

We propose   generic stochastic expectationmaximization  EM  algorithm for the estimation
of highdimensional latent variable models  At the
core of our algorithm is   novel semistochastic
variancereduced gradient designed for the Qfunction in the EM algorithm  Under   mild condition on the initialization  our algorithm is guaranteed to attain   linear convergence rate to the unknown parameter of the latent variable model  and
achieve an optimal statistical rate up to   logarithmic factor for parameter estimation  Compared
with existing highdimensional EM algorithms 
our algorithm enjoys   better computational complexity and is therefore more ef cient  We apply
our generic algorithm to two illustrative latent
variable models  Gaussian mixture model and
mixture of linear regression  and demonstrate the
advantages of our algorithm by both theoretical
analysis and numerical experiments  We believe
that the proposed semistochastic gradient is of
independent interest for general nonconvex optimization problems with bivariate structures 

  Introduction
As   popular algorithm for the estimation of latent variable models  the expectationmaximization  EM  algorithm
 Dempster et al    Wu    has been widely used
in machine learning and statistics  Jain et al    Tseng 
  Han et al    Little   Rubin    Although EM
is wellknown to often converge to an empirically good local
estimator  Wu     nite sample theoretical guarantees
for its performance have not been established until recent

 Facebook  Inc  Menlo Park  CA    Department of Computer Science  University of Virginia  Charlottesville  VA  
USA  Department of Computer Science  University of Illinois
at UrbanaChampaign  Urbana  IL   Correspondence to 
Quanquan Gu  qg   virginia edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

studies  Balakrishnan et al    Wang et al    Yi  
Caramanis    Speci cally  the  rst local convergence
theory and  nite sample statistical rates of convergence for
the conventional EM algorithm and its gradient ascent variant were established in Balakrishnan et al    Later
on  Wang et al    extended the conventional EM algorithm as well as gradient ascent EM algorithm to the
highdimensional setting  where the number of parameters
is comparable to or even larger than the sample size   
key idea used in their algorithms is an additional truncation
step after the maximization step  Mstep  which is able to
exploit the intrinsic sparse structure of the highdimensional
latent variable models  Yi   Caramanis   also proposed   highdimensional EM algorithm  which  instead
of using truncation  uses   regularized Mestimator in the
Mstep  In the highdimensional setting  the gradient EM
algorithm is especially appealing  because exact maximization based Mstep can be very time consuming  or even
illposed  Nonetheless  gradient EM algorithms can still
be computationally prohibitive when the number of observations is also large  since they need to calculate the full
gradient at each iteration  whose time complexity is linear
in the sample size 
In this paper  we address the aforementioned computational
challenge in the largescale highdimensional setting  by
proposing   novel variancereduced stochastic gradient EM
algorithm with theoretical guarantees  Our algorithm is
along the line of gradient EM algorithms  Balakrishnan
et al    Wang et al    where the Mstep is achieved
by onestep gradient ascent rather than  regularized  exact
maximization  Yi   Caramanis    Instead of using
  full gradient at each iteration as in existing gradient EM
algorithms  we signi cantly reduce the computational cost
by utilizing stochastic variancereduced gradient  which
is inspired by recent advances in stochastic optimization
 Roux et al    Johnson   Zhang    Shalev Shwartz
  Zhang    Defazio et al    Zhang   Gu   
To accommodate the special bivariate structure of the Qfunction       the expected value of the log likelihood function  with respect to the conditional distribution of the latent
variable given the observed variable under the current estimate of the model parameter  in EM algorithm  we design  
novel semistochastic variancereduced gradient which sets

HighDimensional VarianceReduced Stochastic Gradient ExpectationMaximization Algorithm

norm        of   as  cid   cid    cid cid  

    vj   cid    Specif 

our work apart from all existing methods and greatly helps
reduce the intrinsic variance of the stochastic gradient of the
Qfunction in the EM algorithm  We apply our algorithm to
two popular latent variable models and thorough numerical
experiments are provided to backup our theory  In particular 
we summarize our major contributions as follows 
  We propose   novel highdimensional EM algorithm by
incorporating variance reduction into the stochastic gradient method for EM  Speci cally  we design   novel
semistochastic gradient tailored to the bivariate structure
of the Qfunction in the EM algorithm  To the best of our
knowledge  this is the  rst work ever that brings variance
reduction into the stochastic gradient EM algorithm in
the highdimensional scenario 
  We prove that our proposed algorithm converges at  
linear rate to the unknown model parameter and achieves
the bestknown statistical rate of convergence with   mild
condition on the initialization 
  We show that the proposed algorithm has an improved
overall computational complexity over the stateof theart algorithm  Speci cally  to achieve an optimization

error of   our algorithm needs   cid          log cid 
algorithm  Wang et al    is   cid   log cid  As

gradient evaluations  where   is the sample size    is
the mini batch size that will be discussed later  and   is
the restricted condition number  In contrast  the gradient
complexity of the stateof theart highdimensional EM
long as          which holds in most real cases  the
overall gradient complexity of our algorithm is less than
Wang et al   
  Different from the proof technique used in existing work
 Balakrishnan et al    Wang et al    Yi   Caramanis    which analyzes both the population and
sample versions of the Qfunction  we directly analyze
the sample version of the Qfunction  Our proof is much
simpler and provides   good interface to analyze the semistochastic gradient 

The rest of the paper is organized as follows  We introduce
the related work in Section   and then present our algorithm
and its applications to two representative latent variable
models in Section   We demonstrate the main theoretical
result as well as its implication to speci   latent variable
models in Section   followed by experimental results in
Section   Finally  we conclude our paper and point out
some future work in Section  
Notation  Let      Aij    Rd   be   matrix and
                vd cid    Rd be   vector  We de ne the  cid   
 Throughout this paper  we consider the calculation of the
gradient of the Qfunction over   data point as   unit gradient
evaluation cost  And we use the gradient complexity       the
number of gradient evaluation units  to fairly compare different
algorithms 

 cid cid  

     

ically   cid   cid  denotes the number of nonzero entries of   
 cid   cid   
  and  cid   cid    maxj  vj  For      
we de ne  cid   cid   as the operator norm of    Speci cally 
 cid   cid  is the spectral norm  We let  cid   cid    maxi    Aij 
For an integer       we de ne                    For an
index set         and vector     Rd  we use vI   Rd to
denote the vector where  vI     vj if        and  vI      
otherwise  We use supp    to denote the index set of its
nonzero entries  and supp       to denote the index set of
top   largest  vj      is used to denote some absolute constants  The values of these constants may be different from
case to case   max    and  min    are used to denote
the largest and smallest eigenvalues of matrix    We use
       to denote the ball centered at   with radius   

  Related Work
In this section  we discuss some related work in detail  Even
with its long history in theory and practice of the EM algorithm  Dempster et al    Wu    Tseng    the
 nite sample statistical guarantees on EM algorithm have
not been pursued until recent research  Balakrishnan et al 
  Wang et al    Yi   Caramanis    In   pioneering work by Balakrishnan et al    both statistical
and computational analysis of EM algorithm was conducted
in the classical regime  Speci cally  the authors treated
EM algorithms as   special perturbed form of standard gradient methods  and they showed that with an appropriate
initialization  their algorithm achieves   locally linear convergence rate to the unknown model parameter  However 
their work is limited to the classical regime  While in the
highdimensional regime  when data dimension is much
larger than the number of samples  the Mstep is often intractable or even not well de ned  In order to extend this
work to the highdimensional scenario  Wang et al   
addressed this challenge by inserting   truncation step to
enforce the sparsity of the parameter  They proved that
their algorithm also enjoys locally linear convergence to the
model parameter up to certain statistical error  Yi   Caramanis   proposed   highdimensional extension of EM
algorithms via   regularized Mestimator  and provided similar theoretical guarantees  In addition  both Balakrishnan
et al    and Wang et al    proposed gradient variants of the EM algorithm  which can be computationally
faster than exact maximization based EM 
Although the gradient based EM algorithms  Balakrishnan
et al    Wang et al    have been proved to achieve
guaranteed performance  these deterministic approaches
can incur huge computational cost in big data and highdimensional scenario since they need costly calculation of
full gradient at each iteration  Stochastic gradient methods
are   common workaround to large scale optimization  Bot 

HighDimensional VarianceReduced Stochastic Gradient ExpectationMaximization Algorithm

tou    Gemulla et al    because one only needs to
calculate   minibatch of the stochastic gradients each time 
However  due to the intrinsic variance introduced by the
stochastic gradient  these methods often have   slower convergence rate compared with full gradient methods  Therefore    lot of variance reduction techniques have been proposed to reduce the variance of the stochastic gradient and
pursue   faster convergence rate  One of the most popular methods is the stochastic variancereduced gradient
 SVRG   Johnson   Zhang    Inspired by this method 
various machine learning tasks  Li et al    Chen   Gu 
  Garber   Hazan    have utilized the stochastic
variance reduction technique to provide improved performance of nonconvex optimization with univariate structures 
Recently  Reddi et al    AllenZhu   Hazan  
also analyzed SVRG for the general univariate nonconvex
 nitesum optimization  Motivated by all of these SVRG
methods  one natural question is that  can we accelerate
gradient based EM algorithms using SVRG  We show in
this work that the answer is in the af rmative  Since all
the aforementioned SVRG methods can not be applied to
the special bivariate structure of the Qfunction  in order to
incorporate the variance reduction technique into stochastic
gradient based EM algorithms  we need to construct   new
semistochastic gradient 

  Methodology
In this section  we present our proposed algorithm  We
 rst introduce the general framework of the EM method 
and then give two representative highdimensional latent
variable models as examples before going into the details of
our algorithm 

  Background

We now brie   review the latent variable model and the
conventional EM algorithm  Let       be the observed
random variable and       be the latent random variable
with joint distribution         and conditional distribution
       with the model parameter     Rd  Given  
observations  yi  
   of     the EM algorithm aims at maximizing the Qfunction

 cid 

  cid 

 QN    cid   

 
 

 

  

  cid   yi    log   yi     dz 

Particularly  in the lth iteration of EM algorithm  we evaluate  QN       in the Estep  and perform the maximization of  QN       on   in the Mstep  For example  in the
standard gradient ascent implementation of EM algorithm 
the Mstep is given by

               QN        

where    QN   denotes the gradient on the  rst variable
and   is the learning rate 
In the highdimensional regime  we assume     Rd is
sparse with  cid cid       In order to ensure the sparsity of the
estimator  Wang et al    proposed to use   truncation
step       Tstep  following the Mstep 

  Illustrative Examples

We now introduce two representative latent variable models
as running examples for highdimensional EM algorithms 
Example    Sparse Gaussian Mixture Model  The random variable     Rd is given by

               

where   is   random variable with                
      and           is   Gaussian random vector 
with   being the covariance matrix    and   are independent  and  cid cid       We assume   is known for
simplicity 
Example    Mixture of Sparse Linear Regression  Let
    Rd         be   Gaussian random vector  and
          be   univariate normal random variable  The
random variable       is given by

          cid      

where   is   random variable with                
      Here      and   are independent  and
 cid cid       In addition  we assume that   is known 

  Proposed Algorithm

   cid 
   cid  

Now we present our highdimensional EM algorithm based
on stochastic variancereduced gradient ascent  The outline
of the proposed algorithm is described in Algorithm  
Since our algorithm is based on stochastic gradient  we divide the   samples into   minibatches  Di  
 cid 
   and de ne
function  qi  
   on these minibatches       qi   cid   
    cid   yj    log   yj     dz  where we let  
  Di
be the minibatch size  and     nb  Let Qn   cid   
   qi   cid  It is easy to show that Qn   cid   
 QN    cid 
Note that in Algorithm   to ensure the sparsity of the output estimator   we use the hard thresholding operator  Blumensath   Davies    Hs      vsupp      which only
keeps the largest   entries in magnitude of   vector     Rd 
The sparsity parameter   controls the sparsity level of the
estimated parameter  and is critical to the estimation error
as we will show later 
We can see that there are two layers of iterations in our
algorithm  For each outer iteration  we  rst conduct Estep 

HighDimensional VarianceReduced Stochastic Gradient ExpectationMaximization Algorithm

Algorithm   Variance Reduced Stochastic Gradient EM
Algorithm  VRSGEM 
  Parameter  Sparsity Parameter    Maximum Number of
Outer Iterations    Number of Inner Iterations     learning
rate  

  Initialization 
  For       to      
 

 cid    Hs init 
 cid     cid   
     cid 

Estep 
Evaluate Qn

 

 

 cid cid   cid  with the dataset
 cid     Qn cid cid 

 
 
 
 
 
  End For

Mstep 
Randomly select jl uniformly from                
For       to jl
Randomly select   from     uniformly
        qi
                 
Tstep        Hs   
End For

 cid   cid cid     qi

 cid cid cid cid   cid 

 cid       jl 
  Output   cid     cid   
where we compute the averaged gradient  cid  based on the

whole dataset and the model parameter from last outer iteration  This averaged gradient will be used repetitively
in the Mstep for variance reduction  In Mstep  we have
the inner iterations  We  rst determine the number of inner
iterations  which is randomly selected from      uniformly 
At each inner iteration  we make use of the variance reduction technique  Note that we extend the variance reduction
idea originally proposed by Johnson   Zhang   to
the bivariate structure of the Qfunction  Speci cally  we
design   novel semistochastic gradient on minibatches as

        qi   cid     qi cid cid   cid  which  xes the

second variable within each outer iteration for the sake of
convergence guarantee  While the standard gradient implementation of EM algorithm  Wang et al    uses
   QN         to update the parameter at each iteration 
our newly designed semistochastic gradient is proved to
better reduce the variance and attain   lower gradient complexity  After  nishing all the inner iterations  we use the
output from the last inner iteration as the output of this outer
iteration  We use the output from the last outer iteration as
the  nal estimator 
We believe our newly proposed semistochastic gradient is
of independent interest for the stochastic optimization of
functions with bivariate structures  to prove   faster rate of
convergence 

  Main Theoretical Results
In this section  we show the main theory on the theoretical
guarantees of our proposed Algorithm   We also present
the implications of our algorithm applied to two models

described in Section  
To facilitate the technical analysis of our algorithm  we focus on the resampling version of Algorithm   following the
convention of previous work  Wang et al    Yi   Caramanis    The key difference between the resampling
version and Algorithm   is that we split the whole dateset
into   subsets and use one subset for each outer iteration 
The details of the resampling version of our algorithm is
provided in the longer version of this paper  It is worth
noting that the resampling version of our algorithm is able
to decouple the dependence between consecutive outer iterations  and it is only used to simplify the technical proof 
In practice including our experiment  we use Algorithm  
rather than the resampling version 
Before we present the main results  we introduce three conditions that are essential for our analysis 
Condition    Smoothness  For any        
    cid cid    where         is   modeldependent
constant  for any         qi  in Algorithm   satis es the
smoothness condition with respect to the  rst variable with
parameter   

 cid cid qi       qi   cid cid      cid cid     

 cid cid 

Condition   says that the gradient of qi  we use in
each inner iteration is Lipschitz continuous with respect to
the  rst variable when the  rst and second variables are
within the ball     cid cid    There exists   wide range
of models with this condition holding 
 
Condition    Concavity  For
    cid cid    where         is   modeldependent
constant  the function Qn  satis es the strong concavity
condition with parameter  

all      

 cid Qn       Qn   cid cid 

     
   cid     cid 
 

Condition   requires Qn  to be strongly concave with
respect to the  rst variable when the  rst and second variables are within the ball     cid cid    This is   reasonable requirement when   is large enough 
Condition    Firstorder stability  For the true model
parameter   and any         cid cid    where    
    is   modeldependent constant  Qn
 rstorder stability with parameter  

 cid cid  satis es the
 cid cid Qn       Qn   cid cid     cid cid     cid cid 

Condition   requires that the gradient  Qn  is
stable with regard to the second variable  with the second
variable within the ball     cid cid    There are actually

HighDimensional VarianceReduced Stochastic Gradient ExpectationMaximization Algorithm

various versions of this condition in previous work  Yi  
Caramanis    Balakrishnan et al    on population
version        Qn  Here we impose the condition
on the sample Qfunction       Qn  because our proof
technique directly analyzes the sample Qfunction  Intuitively  when the sample size   is suf ciently large  Qn 
and    should be close  Therefore  this condition should
hold for Qn  as well 
Due to the space limit  we verify the above conditions for
the two examples in the longer version of this paper  We use
       to denote the condition number  

  Theory for the Generic Model
With the above conditions on qi  and Qn  we have
the following theorem to characterize the estimation error

of our estimator  cid    returned by the resampling version of
that  cid cid init    cid cid      cid cid cid cid  where        
 cid            cid 

Algorithm  
Theorem   Suppose qi  satis es Condition   and
Qn  satis es Conditions     We also assume
If
        and   and   are chosen such that

 

   

         

 

     

      

gorithm   satis es

where               and        

then the estimator  cid    from the resampling version of AlE cid cid cid   cid cid       cid cid init    cid cid 
 cid   cid cid cid 
 cid cid Qn
 cid        

 cid 

           

 

   
 
  

 

and the contraction parameter   in Theorem   can be
simpli ed as

 
 

   

 

 

 

         

Therefore  if we choose      cid     cid  we can

obtain       ensuring the linear convergence rate 
Remark   The right hand side of   in Theorem  
consists of two terms  The  rst term stands for the optimization error  The second term is the statistical error 
According to Remark   we can ensure the linear convergence rate of our algorithm  Thus for any error bound
      we need       log cid init    cid  iterations
to let the optimization error    cid init    cid      which
outer iteration  we need to compute   gradients of qi 
and one full gradient  Since we have        which
is suggested in Remark   the gradient complexity of our

basically requires   cid  log cid  outer iterations  For each
algorithm would be   cid          log cid  Nevertheis   cid   log cid  As long as          the gradient

less  for the stateof theart gradient based highdimensional
EM algorithm  Wang et al    its gradient complexity

complexity of our algorithm is less than that of Wang et al 
  Since in big data scenarios    is always very large
and   as the batch size is relatively small  this condition is
naturally satis ed in most real applications 
The second term on the righthand side of   stands for
the upper bound of the statistical error  which depends on
speci   models as we will introduce later 

  Implications for Speci   Models

 

Now we apply our algorithm to the two examples introduced
in Section  

where cid            

Remark   As suggested in Theorem   that by choosing
an appropriate learning rate     suf ciently large number of
inner iterations     and sparsity parameter   such that      
we can achieve   linear convergence rate  Here we give an
example to show that such   is achievable  If we choose
step size         and truncation parameter   satis es

 cid        

   

 cid 

  

   

   

where

   

Then  we can get

 

       

       

   

   

   

                   

 

 

  SPARSE GAUSSIAN MIXTURE MODEL

the

same

  Under

Corollary
Theorem  

The next corollary gives the implication of our main theory
for sparse Gaussian mixture models 

 cid min max cid cid cid cid 
bility at least           the estimator  cid     cid    from the

suppose  cid cid init    cid cid 
  cid cid cid     cid cid       cid cid init    cid cid 

of
 
Then with proba 

resampling version of Algorithm   satis es

conditions

and

 cid 

   log     log  

    

where      min cid cid cid     

 

 

 

min  cid  and    

 

  

HighDimensional VarianceReduced Stochastic Gradient ExpectationMaximization Algorithm

the signalto noise ratio  SNR  Next cid             is of the

Proof Sketch  For sparse Gaussian mixture model  we
have Conditions   to   hold with parameters    
 min       max  and                
   min  where      cid cid  denotes
same order as    For the term  cid Qn   cid  in  
we have the following inequality holds with probability at
least         

 cid 

 cid Qn   cid 
 cid cid   

   

 cid min 

 cid cid 

log     log  

 

 

This completes the proof 

Remark   We can see that the parameters in Conditions   and   are determined by the covariance matrix  
which is reasonable because   actually denotes the variance
of the data  For Condition   we need to introduce the
signalto noise ratio  SNR  The concept of SNR in parameter estimation is also proposed in Balakrishnan et al   
Dasgupta   Schulman   Since we have extended the
covariance matrix of noise from identity matrix in previous
work to any positive de nite matrix  our SNR is also   little
bit different from their de nition  Generally speaking  for
GMM with lower SNR  the variance of the noise makes
it harder or even impossible for the algorithm to converge 
Therefore  it is always reasonable to have   requirement for
the SNR of GMM to be large enough for reliable parameter
estimation  Spectral method  Anandkumar et al    can
be used to match the requirement on initialization for GMM 
however  we  nd that random initialization also performs
reasonably well in practice as we will show later 
According to Remark   by choosing appropriate learning rate  
inner iterations     and sparsity parameter
   we can ensure linear convergence rate of our algorithm  Therefore  from Corollary   we know that af 

ter   cid  log cid      log   log    cid cid  number of iterations  the
output of our algorithm attains   cid    log     log       sta 

tistical error  which matches the bestknown error bound
 Wang et al    Yi   Caramanis    for Gaussian
mixture model up to   logarithmic factor log    Note that
the extra logarithmic factor is due to the resampling strategy 

  MIXTURE OF SPARSE LINEAR REGRESSION

The implication of our main theory for mixture of linear
regression is presented in the following corollary 

suppose  cid cid init    cid cid 

 cid min max cid cid cid cid 
bility at least           the estimator  cid     cid    from the

of
 
Then with proba 

Corollary
Theorem  

  Under

conditions

same

and

the

resampling version of Algorithm   satis es

  cid cid cid     cid cid       cid cid init    cid cid 
 cid max 

 cid cid   

    

 cid 

 cid cid 

   log     log  

 

 

where       

is   constant  We also show that cid   is of the same order as   

Proof Sketch  For mixture of linear regression  we have
Conditions   to   hold with parameters      max 
     min  and      max  where        
Next  for the term  cid Qn   cid  in   we have the
following inequality holds with probability at least       

 cid Qn   cid 

    cid max cid cid     

max cid cid 

log     log  

 

 

This completes the proof 
Remark   According to Remark   our algorithm can achieve   linear convergence rate with appropriate learning rate  
and
Thus Corollary   tells
sparsity parameter   

us that after   cid  log cid      log   log    cid cid  number of
  cid    log     log       statistical error  which matches

the output of our algorithm achieves

outer iterations 

iterations    

inner

the bestknown statistical error  Yi   Caramanis   
for mixture of linear regression up to   logarithmic factor
from the resampling strategy  Speci cally  the dependence
on  cid cid  is due to the fundamental limits of EM  which
also appears in Balakrishnan et al    Yi   Caramanis
  There is also   spectral method  Chaganty   Liang 
  helping the initialization of MLR  but we use random
initialization which also performs well in our experiments 

  Experiments
In this section  we present experiment results to validate
our theory  For parameter estimation  we use Gaussian
mixture model and mixture of linear regression  and compare our proposed variancereduced stochastic gradient
EM algorithm  VRSGEM  with two stateof theart highdimensional EM algorithms as baselines 
   HDGEM  HighDimensional Gradient EM algorithm
proposed in Wang et al    the gradient variant of
highdimensional EM method enforcing sparsity structure 
   HDREM  HighDimensional Regularized EM algorithm
proposed in Yi   Caramanis   the method based
on decaying regularization 

Since highdimensional scenario is much more challenging 
we only compare our algorithm with highdimensional EM
algorithms 

HighDimensional VarianceReduced Stochastic Gradient ExpectationMaximization Algorithm

   

Figure   Comparison of optimization error  cid cid     cid cid  and overall estimation error  cid cid       cid  for GMM                    

   

   

   

              errors against iterations          errors against training time 

   

Figure   Comparison of optimization error  cid cid       cid cid  and overall estimation error  cid cid       cid  for GMM              

   

   

   

                    errors against iteration          errors against training time 

  Experimental Setup

For each latent variable model  we compare both the op 

timization error  cid cid       cid cid  featuring the convergence of
error  cid cid       cid  featuring the overall estimation accuracy

the estimator to the local optima  and the overall estimation

with regard to the true model parameter   We also show
the convergence comparison in terms of training time  All
the comparisons are under two different parameter settings 
                         and       
                  For VRSGEM  we choose
            and       across all settings and models 
Besides the comparison of different algorithms  we also
verify our statistical rate of convergence by plotting the sta 

tistical error  cid cid     cid  against cid    log      Speci cally 

we          and show the plots of three cases       
       and        with varying   
In each experiment setting  we run   trials and show
the averaged results  The learning rate   is tuned by grid
search and   is chosen by cross validation  We use random
initialization 

  Gaussian Mixture Model

We test VRSGEM on Gaussian mixture models introduced
in Section   For the sake of simplicity and better matching the problem setting of the baseline methods  the co 

variance matrix   of   is chosen to be   diagonal matrix with all elements being   We randomly set two elements to  max      and another two elements to
 min      The results are shown in Figures   and  
From Figures     and     we can see that all three algorithms have linear convergence as Corollary   suggests 
VRSGEM clearly enjoys   faster convergence rate than the
baselines  Moreover  as shown in Figures     and     the
performance on overall estimation error of our algorithm
is as good as HDGEM  which is far better than HDREM 
In terms of time consumption  our algorithm also enjoys  
remarkable advantage over the baselines as shown in Figures             and    
The statistical error results are shown in Figure   From
Figure     we can see that statistical error of VRSGEM
settings of    verifying results in Corollary  

shows   linear dependency on cid    log     across different

  Mixture of Linear Regression

Similar to the setting for GMM  we use the same covariance
matrix   in Section   for   here  For     we let      
We show the results in Figures   and  
From Figures     and     we can see that VRSGEM
achieves linear convergence which is consistent with Corollary   and our algorithm signi cantly outperforms the

Iteration Index Optimization Error HDGEMHDREMVRSGEMIteration Index Overall Estimation Error HDGEMHDREMVRSGEMTraining Time  in Seconds Optimization Error HDGEMHDREMVRSGEMTraining Time  in Seconds Overall Estimation Error HDGEMHDREMVRSGEMIteration Index Optimization Error HDGEMHDREMVRSGEMIteration Index Overall Estimation Error HDGEMHDREMVRSGEMTraining Time  in Seconds Optimization Error HDGEMHDREMVRSGEMTraining Time  in Seconds Overall Estimation Error HDGEMHDREMVRSGEMHighDimensional VarianceReduced Stochastic Gradient ExpectationMaximization Algorithm

   

Figure   Comparison of optimization error  cid cid       cid cid  and overall estimation error  cid cid       cid  for MLR                    

   

   

   

              errors against iterations          errors against training time 

   

Figure   Comparison of optimization error  cid cid       cid cid  and overall estimation error  cid cid       cid  for MLR              

   

   

   

                    errors against iteration          errors against training time 

variance reduced gradient  We show that with an appropriate
initialization  the proposed algorithm converges at   linear
rate and attains the optimal statistical rate  We apply our
proposed algorithm to two popular latent variable models
in the highdimensional regime and numerical experiments
are provided to support our theory 
It is worth noting that  the proposed algorithm is directly
applicable to the classical regime  by dropping the Tstep 
It will give rise to an accelerated stochastic extension of
conventional EM algorithm  and the corresponding theory
in this paper can be extended to the classical regime analogously  Balakrishnan et al    We will investigate
this byproduct in our future work  We also plan to extend
our algorithm to the estimation of highdimensional latent
variable models with lowrank parameters  Yi   Caramanis 
 

Acknowledgments
We would like to thank the anonymous reviewers for their
helpful comments  This research was sponsored in part
by the National Science Foundation under Grant Numbers
CNS  CNS  IIS  IIS 
IIS  The views and conclusions contained in this
paper are those of the authors and should not be interpreted
as representing any funding agencies 

    GMM

Figure   Statistical error  cid cid     cid  of VRSGEM against
 cid    log     with  xed       and varying    and   

    MLR

baselines in terms of optimization error  In terms of overall
estimation error shown in Figures     and     VRSGEM
is as good as HDGEM and beats HDREM by   remarkable margin  Our algorithm also beats the baselines in
time consumption for convergence as we can see in Figures             and     Overall  VRSGEM achieves
the best performance among all the methods 
In addition  from Figure     we can see that for MLR 

the statistical error of VRSGEM is of order cid    log     

which supports Corollary  

  Conclusions and Future Work
We propose   novel accelerated stochastic gradient EM
algorithm based on   uniquely constructed semistochastic

Iteration Index Optimization Error HDGEMHDREMVRSGEMIteration Index Overall Estimation Error HDGEMHDREMVRSGEMTraining Time  in Seconds Optimization Error HDGEMHDREMVRSGEMTraining Time  in Seconds Overall Estimation Error HDGEMHDREMVRSGEMIteration Index Optimization Error HDGEMHDREMVRSGEMIteration Index Overall Estimation Error HDGEMHDREMVRSGEMTraining Time  in Seconds Optimization Error HDGEMHDREMVRSGEMTraining Time  in Seconds Overall Estimation Error HDGEMHDREMVRSGEMps logd   kb         ps logd   kb         HighDimensional VarianceReduced Stochastic Gradient ExpectationMaximization Algorithm

References
AllenZhu  Zeyuan and Hazan  Elad  Variance reduction for faster nonconvex optimization  arXiv preprint
arXiv   

Anandkumar  Animashree  Ge  Rong  Hsu  Daniel  Kakade 
Sham    and Telgarsky  Matus  Tensor decompositions
for learning latent variable models  Journal of Machine
Learning Research     

Balakrishnan  Sivaraman  Wainwright  Martin    and Yu 
Bin  Statistical guarantees for the EM algorithm  From
population to samplebased analysis  arXiv preprint
arXiv   

Blumensath  Thomas and Davies  Mike    Iterative hard
thresholding for compressed sensing  Applied and Computational Harmonic Analysis     

Bottou    eon  Largescale machine learning with stochastic
gradient descent  In Proceedings of COMPSTAT 
pp    Springer   

Chaganty  Arun Tejasvi and Liang  Percy  Spectral experts for estimating mixtures of linear regressions  arXiv
preprint arXiv   

Chen  Jinghui and Gu  Quanquan  Accelerated stochastic
block coordinate gradient descent for sparsity constrained
nonconvex optimization  In Proceedings of the ThirtySecond Conference on Uncertainty in Arti cial Intelligence  pp    AUAI Press   

Dasgupta  Sanjoy and Schulman  Leonard    probabilistic analysis of EM for mixtures of separated  spherical
Gaussians  Journal of Machine Learning Research   
   

Defazio  Aaron  Bach  Francis  and LacosteJulien  Simon 
Saga    fast incremental gradient method with support
In Adfor nonstrongly convex composite objectives 
vances in Neural Information Processing Systems  pp 
   

Dempster        Laird        and Rubin        Maximum
likelihood from incomplete data via the EM algorithm 
Journal of the Royal Statistical Society  Series    Statistical Methodology      ISSN  

Garber  Dan and Hazan  Elad  Fast and simple pca via
convex optimization  arXiv preprint arXiv 
 

Gemulla  Rainer  Nijkamp  Erik  Haas  Peter    and Sismanis  Yannis  Largescale matrix factorization with
distributed stochastic gradient descent  In Proceedings
of the  th ACM SIGKDD international conference on
Knowledge discovery and data mining  pp    ACM 
 

Han  Jiawei  Pei  Jian  and Kamber  Micheline  Data mining 

concepts and techniques  Elsevier   

Jain  Anil    Murty    Narasimha  and Flynn  Patrick   
Data clustering    review  ACM computing surveys
 CSUR     

Johnson  Rie and Zhang  Tong  Accelerating stochastic
gradient descent using predictive variance reduction  In
Advances in Neural Information Processing Systems  pp 
   

Li  Xingguo  Zhao  Tuo  Arora  Raman  Liu  Han  and
Haupt  Jarvis  Stochastic variance reduced optimizaarXiv preprint
tion for nonconvex sparse learning 
arXiv   

Little  Roderick JA and Rubin  Donald    Statistical analy 

sis with missing data  John Wiley   Sons   

Reddi  Sashank    Hefny  Ahmed  Sra  Suvrit    ocz os 
Barnab as  and Smola  Alex  Stochastic variance rearXiv preprint
duction for nonconvex optimization 
arXiv   

Roux  Nicolas    Schmidt  Mark  and Bach  Francis     
stochastic gradient method with an exponential convergence rate for  nite training sets  In Advances in Neural
Information Processing Systems  pp     

Shalev Shwartz  Shai and Zhang  Tong  Stochastic dual
coordinate ascent methods for regularized loss minimization  Journal of Machine Learning Research   Feb 
   

Tseng  Paul  An analysis of the EM algorithm and entropylike proximal point methods  Mathematics of Operations
Research      ISSN    

Wang  Zhaoran  Gu  Quanquan  Ning  Yang  and Liu  Han 
High dimensional expectationmaximization algorithm 
Statistical optimization and asymptotic normality  arXiv
preprint arXiv   

Wu        Jeff  On the convergence properties of the EM
algorithm  The Annals of Statistics     
  doi   aos 

Yi  Xinyang and Caramanis  Constantine  Regularized em
algorithms    uni ed framework and statistical guarantees  In Advances in Neural Information Processing
Systems  pp     

Zhang  Aston and Gu  Quanquan  Accelerated stochastic block coordinate descent with optimal sampling  In
Proceedings of the  nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining 
pp    ACM   

