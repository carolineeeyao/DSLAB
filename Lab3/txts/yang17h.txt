Scalable Bayesian Rule Lists

Hongyu Yang   Cynthia Rudin   Margo Seltzer  

Abstract

We present an algorithm for building probabilistic rule lists that is two orders of magnitude faster
than previous work  Rule list algorithms are
competitors for decision tree algorithms  They
are associative classi ers  in that they are built
from premined association rules  They have  
logical structure that is   sequence of IFTHEN
rules  identical to   decision list or onesided
decision tree 
Instead of using greedy splitting and pruning like decision tree algorithms 
we aim to fully optimize over rule lists  striking   practical balance between accuracy  interpretability  and computational speed  The algorithm presented here uses   mixture of theoretical bounds  tight enough to have practical
implications as   screening or bounding procedure  computational reuse  and highly tuned
language libraries to achieve computational ef 
 ciency  Currently  for many practical problems 
this method achieves better accuracy and sparsity
than decision trees  with practical running times 
The predictions in each leaf are probabilistic 

  Introduction
Our goal is to build   competitor for decision tree and rule
learning algorithms in terms of accuracy  interpretability 
and computational speed  Decision trees are widely used 
particularly in industry  because of their interpretability 
Their logical IFTHEN structure allows predictions to be
explained to users  However  decision tree algorithms have
the serious  aw that they are constructed using greedy splitting from the top down  They also use greedy pruning of
nodes  They do not globally optimize any function  instead
they are composed entirely of local optimization heuristics 
If the algorithm makes   mistake in the splitting near the

 Massachusetts Institute of Technology  Cambridge  Massachusetts  USA  Duke University  Durham  North Carolina 
USA  Harvard University  Cambridge  Massachusetts  USA  Correspondence to  Hongyu Yang  hongyuy mit edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

top of the tree  it is dif cult to undo it  and consequently
the trees become long and uninterpretable  unless they are
heavily pruned  in which case accuracy suffers  In general 
decision tree algorithms are computationally tractable  not
particularly accurate  and less sparse and interpretable than
they could be  This leaves users with no good alternative if
they desire an accurate yet sparse logical classi er 
Several important ingredients provide the underpinning for
our method including 

      principled objective  which is the posterior distribution for the Bayesian Rule List  BRL  model  see
Letham et al    We optimize this objective over
rule lists  Our algorithm is called Scalable Bayesian
Rule Lists  SBRL 

 ii    useful statistical approximation that narrows the
search space  We assume that each rule in the rule list
contains  captures    number of observations that is
bounded below  Because of this approximation  the
set of conditions de ning each leaf is   frequent pattern  This means the antecedents within the rule list
are all frequent patterns  All of the possible frequent
patterns can be premined from the dataset using one
of the standard frequent pattern mining methods  This
leaves us with   much smaller optimization problem 
we optimize over the set of possible premined antecedents and their order to create the rule list 

 iii  High performance language libraries to achieve
computational ef ciency  Optimization over rule lists
is done through repeated low level computations  At
every iteration  we make   change to the rule list
and need to evaluate the new rule list on the data 
High performance calculations  novel to this problem 
speed up this evaluation 

 iv  Computational reuse  When we evaluate   rule list
on the data that has been modi ed from   previous rule
list  we need only to change the evaluation of points
below the change in the rule list  Thus we can reuse
the computation above the change 

    Analytical bounds on BRL   posterior that are tight
enough to be used in practice for screening association

Scalable Bayesian Rule Lists

Table   Rule list for the Mushroom dataset from the UCI repository  data available from Bache   Lichman    PP  the probability that the label is positive  the mushroom is edible 

RULELIST

IF
ELSE IF
ELSE IF
ELSE IF

ELSE IF
ELSE IF
ELSE IF
ELSE IF
ELSE

BRUISES NO ODOR NOTIN NONE FOUL 
ODOR FOUL GILLATTACHMENT FREE 
GILLSIZE BROAD RINGNUMBER ONE 
STALKROOT UNKNOWN STALKSURFACE ABOVERING SMOOTH 
STALKROOT UNKNOWN RINGNUMBER ONE 
BRUISES NO VEILCOLOR WHITE 
STALKSHAPE TAPERING RINGNUMBER ONE 
HABITAT PATHS 
 DEFAULT RULE 

PP

 
 
 
 

 
 
 
 
 

rules and providing bounds on the optimal solution 
These are provided in two theorems in this paper 

Through   series of controlled experiments  we show that
SBRL is over two orders of magnitude faster than the previous best code for this problem 
For example  Table   presents   rule list that we learned for
the UCI Mushroom dataset  see Bache   Lichman   
This rule list is   predictive model for whether   mushroom
is edible  It was created in about   seconds on   laptop and
achieves perfect outof sample accuracy 

  Review of Bayesian Rule Lists
Scalable Bayesian Rule Lists maximizes the posterior distribution of the Bayesian Rule Lists algorithm  Our training
set is  xi  yi  
   where the xi     encode features  and
yi are labels  which in our case are binary  either   or    
Bayesian rule list has the following form 
  obeys    then     Binom 
if
    Beta      
else if   obeys    then     Binom 
    Beta      
 
else if   obeys am then     Binom   
     Beta    Nm 
else     Binom      Beta      
Here  the antecedents  aj  
   are conditions on the    
that are either true or false  for instance  if   is   patient  aj
is true when     age is above   years old and   has diabetes  otherwise false  The vector         has   prior
parameter for each of the two labels  Values   and   are
prior parameters  in the sense that each rule   prediction
    Binomial    and       Beta  The notation Nj
is the vector of counts  where Nj   is the number of observations xi that satisfy condition aj but none of the previous

conditions      aj  and that have label yi      where  
is either   or   Nj is added to the prior parameters   from
the usual derivation of the posterior for the Betabinomial 
The default rule is at the bottom  which makes predictions
for observations that are not satis ed by any of the conditions  When an observation satis es condition aj but not
     aj  we say that the observation is captured by rule
   Formally 

De nition   Rule   captures observation    denoted
Captr         when     argmin   cid  such that aj cid xi   
True 

Bayesian Rule Lists is an associative classi cation method 
in the sense that the antecedents are  rst mined from the
database  and then the set of rules and their order are
learned  The rule mining step is fast  and there are fast
parallel implementations available  Any frequent pattern
mining method will suf ce  since the method needs only
to produce those conditions with suf ciently high support
in the database  The support of antecedent aj is denoted
supp aj  which is the number of observations that obey
condition aj    condition is   conjunction of expressions
 feature values       age  and color white  The
hard part is learning the rule list  which is what this paper
focuses on  It is an optimization over subsets of rules and
their permutations 
The likelihood for the model discussed above is 

Likelihood                cid  

 Nj Nj 

 Nj Nj   

  

where   denotes the rules in the list and their order     
   aj      
   Intuitively  one can see that having more
of one class and less of the other class will make the likelihood larger  To see this  note that if Nj  is large and Nj 
is small  or vice versa  the likelihood for rule   is large 
We next discuss the prior  It has three terms  one governing
the number of rules   in the list  one governing the size cj
of each antecedent    and one governing the choice of antecedent condition aj of rule   given its size  Speci cally 
cj is the cardinality of antecedent aj  also written  aj  the
number of conjunctive clauses in antecedent aj       if  
is    green  and     this has cardinality   Notation     includes the antecedents before   in the rule list 
if there are any                          includes
the cardinalities of the antecedents before   in the rule list 
Notation   is the set of premined antecedents  The prior
is 

                     cid  

     cj           aj      cj   

The  rst term is the prior for the number of rules in the list 
Here  the number of rules   is Poisson  truncated at the

Scalable Bayesian Rule Lists

total number of preselected antecedents 

 cid   

     
      

          

                 

 cid 

 cj  cj  

where   is   hyperparameter  The second term in the prior
governs the number of conditions in each rule  The size of
rule   is cj which is Poisson  truncated to remove values
for which no rules are available with that cardinality 
  cj           
cj   Rj       
where Rj        is the set of cardinalities available after removing the  rst    rules  and   is   hyperparameter 
The third term in the prior governs the choice of antecedent 
given that we have determined its size through the second
term  We have aj selected from   uniform distribution over
antecedents in   of size cj  excluding those in     

  Rj                

  aj      cj       
aj   Qcj                     aj          cj 
As usual  the posterior is the likelihood times the prior 
                                         

is

the

the

and

full model 

posterior
is what we optimize to obtain

This
                 
the best rule lists 
The hyperparameter   is chosen by the user to be the desired size of the rule list  and   is chosen as the desired
number of terms in each rule  The parameters   and  
are usually chosen to be   to avoid favoring one class label
over another 
Given the prior parameters     and   along with the set
of premined rules    the algorithm must select which rules
from   to use  along with their order 

  Representation
We use an MCMC scheme  at each time    we choose  
neighboring rule list at random by adding  removing  or
swapping rules  starting with the basic algorithm of Letham
et al    as   starting point  However  to optimize performance we use   more ef cient rule list representation
that is amenable to fast computation 
Expressing computation as bit vectors  The vast majority of the computational time spent constructing rule sets
lies in determining which rules capture which observations
in   particular rule ordering  The na ve implementation of
these operations calls for various set operations   checking
whether   set contains an element  adding an element to  
set  and removing an element from   set  However  set operations are typically slow  and hardware does little to help
with ef ciency 

We convert all set operations to logical operations on bit
vectors  for which hardware support is readily available 
The bit vector representation is ef cient in terms of both
memory and computation  Before beginning the algorithm 
for each rule  we compute the bit vector representing the
samples for which the rule generates   true value  For   one
million sample data set  or more precisely up to  
observations  each rule carries with it     KB vector
 since   byte consists of   bits  which  ts comfortably in
most    caches 
Representing intermediate state as bit vectors  For each
rule list we consider  we maintain similarly sized vectors
for each rule in the set indicating which  unique  rule in
the set captures which observation  Representing the rules
and rule lists this way allows us to explore the rule list state
space  reusing signi cant computation  For example  consider   rule list containing   rules  Imagine that we wish
to delete rule   from the set  The na ve implementation
recomputes the  captures  vector for every rule in the set 
Our implementation updates only rules        using logical operators acting upon the rule list  captures  vector for
   and the rule    captures  vector for each rule       
This shortens the run time of the algorithm in practice by
approximately  
  fast algebra for computational reuse  Our use of
bit vectors transforms the large number of set operations
 performed in   traditional implementation  into   set of
boolean operations on bit vectors  We have custom implementations  discussed in the full version of this work 
Yang et al    for the following      Remove rule   uses
boolean operations on bit vectors to redistribute the observations captured by rule   to the rules below it in the list 
 ii  Insert rule   shifts the rules below   down one position 
determines which observations are captured by the new
rule  and removes those observations from the rules below
it   iii  Swap consecutive rules updates only which observations were captured for the two swapped rules   iv  Generalized swap subroutine updates only observations captured
for all rules between the two rules to be swapped  All operations use only bit vector computations 
Ablation study  Having transformed expensive set operations into bit vector operations  we can now leverage
both hardware vector instructions and optimized software
libraries  We investigated four alternative implementations 
each improving ef ciency from the previous one      First 
we have the original python implementation here for comparison 
 ii  Next  we retained our python implementation but converted from set operations to bit operations 
 iii  Then  we used the python gmpy library to perform
the bit operations   iv  Finally  we moved the implementation from Python to    represented the bit vectors as multiprecision integers  used the GMP library  which is faster on

Scalable Bayesian Rule Lists

 

 

Algorithm   Calculating bj  

Initialization  index        

for       to cid   
for    cid  
for    cid   

 cid  do
 cid  downto   do
 cid  downto   do

end for
if        cid    then
   

index   index    
bindex    

index   index    
bindex    

end for

end if
end for

Figure   Boxplots of log  runtime among different implementations  From the original python code  the  nal code is over two
orders of magnitude faster 

large data sets  and implemented the algebra for computational reuse outlined above  To evaluate how each of these
steps improved the computation time of the algorithm  we
conducted   controlled experiment where each version of
the algorithm  corresponding to the four steps above  was
given the same data  the UCI adult dataset  divided into
three folds  same set of rules  and same number of MCMC
iterations   to run  We created boxplots for the log 
of the run time over the different folds  which are shown
in Figure   The  nal code is over two orders of magnitude faster than the original optimized python code  that of
Letham et al   

  Bounds
We prove two bounds  First we provide an upper bound
on the number of rules in   maximum   posteriori rule list 
This allows us to narrow our search space to rule lists below
  certain size  Second we provide   constraint that eliminates certain pre xes of rule lists  This prevents our algorithm from searching in regions of the space that provably
do not contain the maximum   posteriori rule list 

  Upper bound on the number of rules in the list

Given the number of features  the parameter   for the size
of the list  and parameters   and   we can derive an
upper bound for the size of   maximum   posteriori rule
list  This formalizes how the prior on the number of rules
is strong enough to overwhelm the likelihood 
We are considering binary antecedents and binary features
      aj is true if female  so the total number of possible

antecedents of each size can be calculated directly  When
creating the upper bound  within the proof  we hypothetically exhaust antecedents from each size category in turn 
starting with the smallest sizes  We discuss this further below 
Let  Qc  be the number of antecedents that remain in the
pile that have   logical conditions  The sequence of     that
we de ne next is   lower bound for the possible sequence of
 Qc    In particular           etc  represents the sequence
of sizes of rules that would provide the smallest possible
 Qc    Intuitively  the sequence of     arises when we deplete the antecedents of size   then deplete all of size   etc 
The number of ways to do this is given by the bindex values 
computed as Algorithm   where   is the number of features  and                   is   vector of length     
We will use   within the theorem below  In our notation 
rule list   is de ned by the antecedents and the probabilities
on the right side of the rules         al      

  

Theorem   The size    of any MAP rule list     with parameters     and         obeys      mmax 
where
 cid cid 

 cid 

 cid 
  cid            cid 

mmax   min

       max

  cid         

    

With parameters       and       this reduces to 

  cid cid 

  

  cid cid 

  

bj

 

 cid cid 

bj

 

 cid 

 cid 
  cid            cid 

mmax   min

       max

  cid         

    

The proof is in the longer version of this paper  Yang et al 
  Theorem   tends to signi cantly reduce the size of
the space  Without this bound  it is possible that the search
would consider extremely long lists  without knowing that
they are provably nonoptimal 

py originpy bitOppy gmpyC bitOp reuse AlgorithmsLog  of runtime in secondScalable Bayesian Rule Lists

  Pre   Bound

We next provide   bound that eliminates certain regions
of the rule space from consideration  Consider   rule list
beginning with antecedents      ap  If the best possible
rule list starting with      ap cannot beat the posterior of
the best rule list we have found so far  then we know any
rule list starting with      ap is suboptimal  In that case 
we should stop exploring rule lists that start with      ap 
This is   type of branch and bound strategy  in that we
have now eliminated  bounded  the entire set of lists starting with      ap  We formalize this intuition below 
Denote the rule list antecedents at iteration   by dt  
     The current best posterior probabil 
 at
ity has value   
    that is

    at

  at

mt

  
    max
  cid  

Posterior dt cid 

 xi  yi  

  

Let us consider   rule list with antecedents    
        am     Let dp denote   pre   of length   of
the rule list         dp           ap  where         ap
are the same as the  rst   antecedents in    We want to determine whether   rule list starting with dp could be better
than the best we have seen so far 
De ne  dp xi  yi  
 dp xi  yi  

   as follows 

    
 max    max    

       

 cid   
 cid cid  
   cid  
   cid  

  

 cid cid  
     cj         
   cid  
   cid  

 cid   

 Nj Nj 

   Nj 
   Nj   

 Nj Nj 

   Nj 
   Nj 

 cid 

 Qcj  

Here  Nj  is the number of points captured by rule   with
label   and Nj  is the number of points captured by rule  
with label  

Nj         Captr        and yi    
Nj         Captr        and yi    

The result states that for   rule list with pre   dp  if the
upper bound on the posterior   dp  is not as high as the
posterior of the best rule list we have seen so far  then dp is
  bad pre    which cannot lead to   MAP solution  It tells
us we no longer need to consider rule lists starting with dp 

Theorem   For rule list      dp  ap    am     if

 dp xi  yi  

       
   

then for       and       we have

   cid  argmaxd cid Posterior   cid xi  yi  

  

 

Theorem   is implemented in our code in the following
way  for each random restart  the initial rule in the list is
checked against the bound of Theorem   If the condition
        
  holds  we throw out this initial rule and choose
  new one  because that rule provably cannot be the  rst
rule in an optimal rule list  Theorem   provides   substantial computational speedup in  nding high quality or optimal solutions  In some cases  it provides   full order of
magnitude speedup  The proofs are lengthy and contained
in the longer version of this work  Yang et al   

  Experiments
We provide   comparison of algorithms along three dimensions  solution quality  AUC   area under the ROC
curve  sparsity  and scalability  As baselines  we chose
popular algorithms to represent the sets of uninterpretable
methods and the set of  interpretable  methods  To represent uninterpretable methods  we chose logistic regression  SVM RBF  random forests  RF  and boosted decision trees  ADA  To represent the class of  interpretable 
algorithms  we chose CART     RIPPER  Cohen   
CBA  Liu et al    and CMAR  Li et al    Other
works  see Letham et al    Wang   Rudin     
have accuracy interpretability comparisons to Bayesian
Rule Lists and Falling Rule Lists  so our main effort here
will be to study the scalability component  Implementation
details are in the full version  Yang et al   
We benchmark using publicly available datasets  see Bache
  Lichman    that have interpretable features  the Tic
Tac Toe dataset  where the goal is to determine whether the
    player wins  the Adult dataset  where we aim to predict
whether an individual makes over    peryear  the Mushroom dataset  where the goal is to predict whether   mushroom is edible  the Nursery dataset  where the goal is to
predict whether   child   application to nursery school will
be in either the  very recommended  or  special priority 
categories  and the Telco customer churn dataset  see WatsonAnalytics    where the goal is to predict whether  
customer will leave the service provider 
Evaluations of prediction quality  sparsity  and timing were
done using  fold cross validation  The prior parameters
were  xed at       and         For the   for each
dataset  we  rst let   be   and ran SBRL once with the
above parameters  Then we  xed   at the length of the
returned rule list for that dataset 
It is possible that the
solution quality would increase if SBRL ran for   larger
number of iterations  For the purpose of providing   controlled experiment  the number of iterations was  xed at
  for each of the   chains of SBRL  which we ran in
series on   laptop  Every time SBRL started   new rule
list  we checked the initial rule in the list to see whether the
upperbound on its posterior  by Theorem   was greater

Scalable Bayesian Rule Lists

Figure   Scatter plot of AUC against the number of leaves  sparsity  for Tic Tac Toe  There is   triangle for each of   folds  for
several settings of CART and     parameters 

Table   Run Time on Adult dataset

RUN TIME

LR

SVM RF

ADA

CART

  

RIPPER

CBA

CMAR

SBRL

MEAN
MEDIAN
STD

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

than the best rule list we had found so far  If not  the rule
was replaced until the condition was satis ed 
Tic Tac Toe  Each observation in this dataset is   tic tac
toe board after the game has  nished  If there are       in
  row  the label of the board is   otherwise   This should
not be   dif cult learning problem since there are solutions
with perfect accuracy on the training set that generalize to
the test set  Figure   shows   scatter plot of AUC vs  number of leaves  sparsity  where each triangular marker represents an evaluation of one algorithm  on one fold  with
one parameter setting  We tried many different parameter
settings for CART  in blue  and many different parameter settings for     in gray  none of which were able to
achieve points on the ef cient frontier de ned by SBRL 
SBRL   run time was on average       seconds 
Adult  For the Adult dataset  results are in Figure   Figure   and Table   Adult contains   observations and
  features  where each observation is an individual  and
the features are census data  including demographics  income levels  and other  nancial information  Here  SBRL 
which was untuned and forced to be sparse  performed only
slightly worse than several of the uninterpretable methods 
Its AUC performance dominated those of the CART and
   algorithms  As the scatter plot shows  even if CART
were tuned on the test set  it would have performed at
around the same level  perhaps slightly worse than SBRL 
The timing for SBRL was competitive  at about   seconds 
where   seconds were MCMC iterations 
If the chains
were computed in parallel rather than in series  it would
speed up computation further 

Figure   Comparison of AUC among different methods on the
Adult dataset 

Figure   Scatter plots of AUC against the number of leaves for
the Adult dataset    folds are included  along with results from
several different settings of CART and     parameters  The
bottom is   zoomedin version of the top 

Mushroom  Table   contains an SBRL rule list for Mushroom  other results are in Yang et al    SBRL attains
perfect test accuracy on this dataset 
Nursery  Results from the Nursery dataset are shown in
Figure     similar story holds as for the previous datasets 
SBRL is on the optimal frontier of accuracy sparsity without tuning and with reasonable run time 

llLRSVMRFADACARTC RIPPERCBACMARSBRL AlgorithmsAUCScalable Bayesian Rule Lists

Figure   Scatter plot of AUC against the number of leaves  sparsity  for Nursery    folds are included  along with results from
several different settings of CART and     parameters 

Figure   Scatter plot of AUC against the number of leaves  sparsity  for Telco    folds are included  along with results from
several different settings of CART and     parameters 

Table   Run Time on Telco dataset

RUN TIME

LR

SVM RF

ADA

CART

  

RIPPER

CBA

CMAR

SBRL

MEAN
MEDIAN
STD

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 

  Related Works and Discussion
Rule lists are   type of onesided decision tree  and any decision tree can be written as   rule list by enumerating the
leaves  Thus SBRL is   competitor for CART  Breiman
et al    CART is currently still popular in industry  CART and other decision tree methods  also decision list methods and associative classi cation methods 
form trees from the top down using greedy splitting and
greedy pruning  see       Quinlan    Clark   Niblett 
  Cendrowska    Rivest    Quinlan    Liu
et al    Li et al    Yin   Han    Marchand  
Sokolova    Vanhoof   Depaire    Rudin et al 
  Since our work does not use greedy splitting and
pruning  it is closer to Bayesian tree models  Dension et al 
  Chipman et al      which are built greedily
from the top down  but then the trees change according to
an MCMC scheme  which allows for more exploration of
the search space  However even with MCMC  the chains
tend to center on local optima  It may be possible to use
our techniques to build trees  where one would mine rules
and create   globally optimal tree 
There are   series of works from the mid   onwards
on  nding optimal decision trees using dynamic programming and search techniques       Bennett   Blue   
Auer et al    Dobkin et al    Boros et al   
Garofalakis et al    Farhangfar et al    mainly
working with  xed depth trees  The number of trees of  

Figure   Comparison of AUC of ROC among different methods
on Telco 

Telco  Figure   Figure   and Table   show the results for
the Telco dataset  which contains   observations and  
features  Similar observations hold for this dataset  The
model from one of the ten folds is provided in Table  

  Scalability
We used the USCensus  data  see Bache   Lichman 
  to test the scalability of SBRL on large datasets  We
used   observations and set SBRL   parameter to
extract   rules as problem     and about   observations with   rules as problem     The runtime
comparison with CART is shown in Table   For problem     the run times are similar  for     SBRL is slower
  hours  which is not prohibitive for important problems  One can see why CART does not perform as well
in high dimensions  as it often spends less time on harder
problems than on easier ones  details are in  Yang et al 

llllllLRSVMRFADACARTC RIPPERCBACMARSBRL AlgorithmsAUCScalable Bayesian Rule Lists

Table   Example of rule list for TelcoCustomer Churn dataset
fold    CV  PP  positive probability  TA  test accuracy 

RULELIST

IF

ELSE IF
ELSE IF

ELSE IF
ELSE IF

ELSE IF
ELSE

 STREAMING 

 INTERNETSER 

CONTRACT ONE YEAR

 
MOVIES YES  
  CONTRACT ONE YEAR  
 
TENURE YEAR
VICE FIBER OPTIC  
  CONTRACT TWO YEAR  
  INTERNETSERVICE FIBER OPTIC  ONLINESECURITY NO  
  ONLINEBACKUP NO  TECHSUPPORT NO  
  DEFAULT  

PP

 

 
 

 
 

 
 

TA

 

 
 

 
 

 
 

Table   Run time  in seconds  and AUC of SBRL on USCensus  dataset       million data points and   thousand rules    
  thousand data points and   thousand rules 

RUN TIME   

SBRL

CART

AUC

SBRL

CART

   
   

 
 

 
 

   
   

 
 

 
 

 xed depth is much larger than the number of rule lists of  
 xed depth and are therefore more dif cult to optimize  Nijssen   Fromont   use premined rules to form trees 
but in   different way than our method  There  the user premines all possible leaves  enumerating all conditions leading to that leaf   By contrast  in associative classi cation 
we mine only small conjunctions  and their ordered combination creates leaves  As as result  Nijssen   Fromont
  warn about issues related to running out of memory 
An extension of this work  CORELS   Angelino et al 
  does not provide probabilistic predictions  but is able
to provide   certi cate of optimality to   globally optimal
rule list  This indicates that SBRL is probably also achieving optimality  however  because SBRL is probabilistic 
the proof of optimality is much more dif cult  To clarify   nding the optimal solution for both methods should
be approximately equally dif cult  but proving optimality for SBRL is much more dif cult  However  there is
  clear practical bene   to having probabilistic predictions
like those of SBRL  One can postprocess CORELS to have
probabilistic predictions by computing             leaf 
for each leaf  but this is not the same as optimizing likelihood and obtaining these probabilities directly like SBRL 
There are several sub elds that produce disjunctive normal
form  DNF  classi ers rather than rule lists  including rule
learning induction  and associative classi cation  which
stemmed possibly from work in the     Michalski 
  and throughout the    and     Cendrowska 
  Clark   Niblett    Cohen    The vast majority of these techniques are not probabilistic  and aim for
covering the positive class without covering the negative
class  Rijnbeek   Kors   aim to produce globally op 

timal DNF models  There is recent work on probabilistic
DNF   that is similar to SBRL  Wang et al     
Teleoreactive programs  Nilsson    use   rule list
structure and could bene   from learning this structure
from data 
SBRL aims to produce interpretable models  Interpretability has been   fundamental topic in arti cial intelligence for
  long time  see   uping    Bratko    Dawes   
Vellido et al    GiraudCarrier    Holte   
Shmueli    Huysmans et al    Freitas    Because the rule lists created by our method are designed to be
interpretable  one would probably not want to boost them
using AdaBoost to form more complicated models  This
contrasts with  for instance  Friedman   Popescu  
who linearly combine premined rules 
Rule lists and their variants are currently being used for
text processing  King et al    discovering treatment
regimes  Zhang et al    Lakkaraju   Rudin   
Wang   Rudin      and creating medical risk assessments  Letham et al    among other applications 

Conclusion
We  nish by stating why when one would want to use this
particular method  SBRL is not meant as   competitor for
black box classi ers such as neural networks  support vector machines  gradient boosting or random forests 
It is
useful when machine learning tools are used as   decision
aid to humans  who need to understand the model in order
to trust it and make datadriven decisions  SBRL does not
use   greedy splitting pruning procedure like decision tree
algorithms  CART     which means that it more reliably computes high quality solutions  at the possible expense of additional computation time  Many of the decision tree methods do not compute sparse or interpretable
trees  as we have seen with    Our code is   strict improvement over the original Bayesian Rule Lists algorithm
if one is looking for   maximum   posteriori solution  It is
faster because of careful use of lowlevel computations and
theoretical bounds 

Code

Code for SBRL is available at
https github com Hongyuy sbrlmod
Link to   package SBRL on CRAN  https 
 cran rproject org web packages sbrl 
index html

the following link 

Acknowledgement

The authors would like to acknowledge partial funding provided by NSF  Philips  Wistron  and Siemens 

Scalable Bayesian Rule Lists

References
Angelino  Elaine  LarusStone  Nicholas  Alabi  Daniel  Seltzer 
Margo  and Rudin  Cynthia  Certi ably optimal rule lists
the  rd ACM
for categorical data 
SIGKDD Conference of Knowledge  Discovery  and Data Mining  KDD   

In Proceedings of

Auer  Peter  Holte  Robert    and Maass  Wolfgang  Theory
and applications of agnostic PAClearning with small decision
trees  pp    Morgan Kaufmann   

Bache     and Lichman     UCI machine learning repository 

  http archive ics uci edu ml 

Bennett  Kristin    and Blue  Jennifer    Optimal decision trees 
Technical report         Math Report No    Rensselaer Polytechnic Institute   

Boros  Endre  Hammer  Peter    Ibaraki  Toshihide  Kogan 
Alexander  Mayoraz  Eddy  and Muchnik  Ilya  An impleIEEE Transactions on
mentation of logical analysis of data 
Knowledge and Data Engineering     

Bratko     Machine learning  between accuracy and interpretability  In Della Riccia  Giacomo  Lenz  HansJoachim  and Kruse 
Rudolf  eds  Learning  Networks and Statistics  volume  
of International Centre for Mechanical Sciences  pp   
Springer Vienna   

Breiman  Leo  Friedman  Jerome    Olshen  Richard   
and Stone  Charles    Classi cation and Regression Trees 
Wadsworth   

Cendrowska     PRISM  An algorithm for inducing modular
rules  International Journal of ManMachine Studies   
   

Chipman  Hugh    George  Edward    and McCulloch  Robert   
Bayesian treed models  Machine Learning   
 

Chipman  Hugh    George  Edward    and McCulloch  Robert   
BART  Bayesian additive regression trees  The Annals of Applied Statistics     

Clark  Peter and Niblett  Tim  The CN  induction algorithm  Ma 

chine Learning     

Cohen  William    Fast effective rule induction 

In In Proceedings of the Twelfth International Conference on Machine
Learning  pp    Morgan Kaufmann   

Dawes  Robyn    The robust beauty of improper linear models
in decision making  American Psychologist   
 

Dension     Mallick     and Smith           Bayesian CART

algorithm  Biometrika     

Dobkin  David  Fulton  Truxton  Gunopulos  Dimitrios  Kasif  Simon  and Salzberg  Steven  Induction of shallow decision trees 
Citeseer   

Farhangfar  Alireza  Greiner  Russell  and Zinkevich  Martin   
fast way to produce optimal  xeddepth decision trees  In International Symposium on Arti cial Intelligence and Mathematics  ISAIM   Fort Lauderdale  Florida  USA  January
     

Freitas  Alex    Comprehensible classi cation models    position
paper  ACM SIGKDD Explorations Newsletter   
 

Friedman  Jerome    and Popescu  Bogdan    Predictive learning
via rule ensembles  The Annals of Applied Statistics   
   

Garofalakis  Minos  Hyun  Dongjoon  Rastogi  Rajeev  and Shim 
Kyuseok  Ef cient algorithms for constructing decision trees
In Proceedings of the Sixth ACM SIGKDD
with constraints 
International Conference on Knowledge Discovery and Data
Mining  KDD  pp    New York  NY  USA   
ACM 

GiraudCarrier  Christophe  Beyond predictive accuracy  what 
In Proceedings of the ECML  Workshop on Upgrading
Learning to MetaLevel  Model Selection and Data Transformation  pp     

Holte  Robert    Very simple classi cation rules perform well
on most commonly used datasets  Machine Learning   
   

Huysmans  Johan  Dejaeger  Karel  Mues  Christophe  Vanthienen  Jan  and Baesens  Bart  An empirical evaluation of the
comprehensibility of decision table  tree and rule based predictive models  Decision Support Systems     

King  Gary  Lam  Patrick  and Roberts  Margaret  Computerassisted keyword and document set discovery from unstructured text  American Journal of Political Science   

Lakkaraju  Himabindu and Rudin  Cynthia  Learning cost effective and interpretable treatment regimes in the form of rule
In Proceedings of Arti cial Intelligence and Statistics
lists 
 AISTATS   

Langley     Crafting papers on machine learning 

In Langley 
Pat  ed  Proceedings of the  th International Conference on
Machine Learning ICML  pp    Stanford  CA   
Morgan Kaufmann 

Letham  Benjamin  Rudin  Cynthia  McCormick  Tyler    and
Madigan  David 
Interpretable classi ers using rules and
Bayesian analysis  Building   better stroke prediction model 
Annals of Applied Statistics     

Li  Wenmin  Han  Jiawei  and Pei  Jian  CMAR  accurate and ef 
cient classi cation based on multiple classassociation rules  In
IEEE International Conference on Data Mining  pp   
 

Liu  Bing  Hsu  Wynne  and Ma  Yiming 

Integrating classi 
cation and association rule mining  In Proceedings of the  th
International Conference on Knowledge Discovery and Data
Mining  KDD  pp     

Marchand  Mario and Sokolova  Marina  Learning with decision
lists of datadependent features  Journal of Machine Learning
Research     

Michalski        On the quasioptimal solution of the general
covering problem  In Proceedings of the   International Symposium on Information Processing  FCIP   pp   
 

Scalable Bayesian Rule Lists

Yin  Xiaoxin and Han  Jiawei  CPAR  classi cation based on predictive association rules  In Proceedings of the   SIAM International Conference on Data Mining  ICDM  pp   
 

Zhang  Yichi  Laber  Eric    Tsiatis  Anastasios  and Davidian 
Marie  Using decision lists to construct interpretable and parsimonious treatment regimes  Biometrics     

Nijssen  Siegfried and Fromont  Elisa  Optimal constraintbased
decision tree induction from itemset lattices  Data Mining and
Knowledge Discovery     

Nilsson  Nils    Teleoreactive programs for agent control  Jour 

nal of Arti cial Intelligence Research     

Quinlan     Ross  Learning Ef cient Classi cation Procedures
and Their Application to Chess End Games  pp   
Springer Berlin Heidelberg  Berlin  Heidelberg   

Quinlan     Ross     Programs for Machine Learning  Morgan

Kaufmann   

Rijnbeek  Peter    and Kors  Jan    Finding   short and accurate
decision rule in disjunctive normal form by exhaustive search 
Machine Learning     

Rivest  Ronald    Learning decision lists  Machine Learning   

   

Rudin  Cynthia  Letham  Benjamin  and Madigan  David  Learning theory analysis for association rules and sequential event
prediction  Journal of Machine Learning Research   
   

  uping  Stefan  Learning interpretable models  PhD thesis  Uni 

versit at Dortmund   

Shmueli  Galit  To explain or to predict  Statistical Science   

  August  

Vanhoof  Koen and Depaire  Beno    Structure of association
rule classi ers    review  In Proceedings of the International
Conference on Intelligent Systems and Knowledge Engineering
 ISKE  pp     

Vellido  Alfredo  Mart nGuerrero 

and Lisboa 
Paulo      Making machine learning models interpretable  In
Proceedings of the European Symposium on Arti cial Neural
Networks  Computational Intelligence and Machine Learning 
 

Jos     

Wang  Fulton and Rudin  Cynthia  Falling rule lists  In Proceedings of Arti cial Intelligence and Statistics  AISTATS     

Wang  Fulton and Rudin  Cynthia  Causal falling rule lists  CoRR 
abs      URL http arxiv org abs 
 

Wang  Tong  Rudin  Cynthia  Doshi  Finale  Liu  Yimin  Klamp 
Erica  and MacNeille  Perry  Bayesian or   of and   for interpretable classi cation  In SIAM International Conference on
Data Mining  ICDM   

Wang  Tong  Rudin  Cynthia  Doshi  Finale  Liu  Yimin  Klamp 
Erica  and MacNeille  Perry    Bayesian framework for learning rule sets for interpretable classi cation  Journal of Machine
Learning Research    Accepted 

WatsonAnalytics 

https community watsonanalytics com wpcontent uploads WA FnUseC  TelcoCustomer 
Churn csv 

IBM 

 

Yang  Hongyu  Rudin  Cynthia  and Seltzer  Margo  Scalable
Bayesian rule lists for interpretable machine learning  CoRR 
abs    URL http arxiv org abs 
 

