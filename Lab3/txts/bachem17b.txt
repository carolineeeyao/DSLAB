Distributed and Provably Good Seedings for kMeans in Constant Rounds

Olivier Bachem   Mario Lucic   Andreas Krause  

Abstract

The kmeans  algorithm is the state of the art
algorithm to solve kMeans clustering problems
as the computed clusterings are   log    competitive in expectation  However  its seeding step
requires   inherently sequential passes through
the full data set making it hard to scale to massive data sets  The standard remedy is to use the
kmeans cid  algorithm which reduces the number
of sequential rounds and is thus suitable for   distributed setting  In this paper  we provide   novel
analysis of the kmeans cid  algorithm that bounds
the expected solution quality for any number of
rounds and oversampling factors greater than   
the two parameters one needs to choose in practice  In particular  we show that kmeans cid  provides provably good clusterings even for   small 
constant number of iterations  This theoretical
 nding explains the common observation that
kmeans cid  performs extremely well in practice
even if the number of rounds is low  We further
provide   hard instance that shows that an additive error term as encountered in our analysis is
inevitable if less than      rounds are employed 

  Introduction
Over the last several years  the world has witnessed the
emergence of data sets of an unprecedented scale across
different scienti   disciplines  This development has created   need for scalable  distributed machine learning algorithms to deal with the increasing amount of data  In this
paper  we consider largescale clustering or  more specifically  the task of  nding provably good seedings for kMeans in   massive data setting 
Seeding   the task of  nding initial cluster centers  
is critical to  nding good clusterings for kMeans 
In
fact  the seeding step of the state of the art algorithm
kmeans   Arthur   Vassilvitskii    provides the

 Department of Computer Science  ETH Zurich  Correspon 

dence to  Olivier Bachem  olivier bachem inf ethz ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

theoretical guarantee on the solution quality while the subsequent re nement using Lloyd   algorithm  Lloyd   
only guarantees that the quality does not deteriorate  While
the kmeans  seeding step guarantees   solution that is
  log    competitive with the optimal solution in expectation  it also requires   inherently sequential passes through
the data set  This makes it unsuitable for the massive data
setting where the data set is distributed across machines and
computation has to occur in parallel 
As   remedy  Bahmani et al 
  propose the
kmeans cid  algorithm which produces seedings for kMeans with   reduced number of sequential iterations 
Whereas kmeans  only samples   single cluster center in each of   rounds  kmeans cid  samples in expectation
 cid  points in each of   iterations  Provided   is small enough 
this makes kmeans cid  suitable for   distributed setting as
the number of synchronizations is reduced 
Our contributions  We provide   novel analysis of
kmeans cid  that bounds the expected solution quality for
any number of rounds   and any oversampling factor  cid      
the two parameters that need to be chosen in practice  Our
bound on the expected quantization error includes both  
 traditional  multiplicative error term based on the optimal solution as well as   scaleinvariant additive error term
based on the variance of the data  The key insight is that
increased  This shows that kmeans cid  provides provably
good clusterings even for   small  constant number of iterations and explains the commonly observed phenomenon
that kmeans cid  works very well even for small   
We further provide   hard instance on which kmeans cid 
provably incurs an additive error based on the variance of
the data and for which an exclusively multiplicative error
guarantee cannot be achieved  This implies that an additive
error term such as the one in our analysis is in fact necessary if less than       rounds are employed 

this additive error term vanishes at   rate of cid   

 cid   if   or  cid  is

  cid 

  Background   related work
kMeans clustering  Let   denote   set of points in Rd 
The kMeans clustering problem is to  nd   set   of  
cluster centers in Rd that minimizes the quantization error

        

         

 cid 

   

 cid 

   

 cid       cid 
 

min
   

Distributed and Provably Good Seedings for kMeans in Constant Rounds

 cid 

wx       

Sample       with probability
           

Algorithm   kmeans  seeding
Require  weighted data set         number of clusters  
wx cid 
      sample single       with probability
  cid   wx cid 
  for                 do
 
 
  Return  
We denote the optimal quantization error by  OPT    
while the variance of the data is de ned as Var      
        where      is the mean of    
kmeans  seeding  Given   data set   and any set of
cluster centers         the   sampling strategy selects  
new center by sampling each point       with probability

  cid   wx cid      cid   

 cid 
       
  cid       cid    cid   

      

The seeding step of kmeans   Arthur   Vassilvitskii 
  detailed for potentially weighted data sets in Algorithm   selects an initial cluster center uniformly at random and then sequentially adds       cluster centers using    sampling whereby   is always the set of previously
sampled centers  Arthur   Vassilvitskii   show that
the solution quality  kmeans  of kmeans  seeding is
bounded in expectation by

  kmeans       log         OPT    

The computational complexity of kmeans  seeding is
  nkd  where   is the number of data points and   the dimensionality  Unfortunately  the iterations in kmeans 
seeding are inherently sequential and  as   result  the algorithm requires   full passes through the data  This makes
the algorithm unsuitable for the distributed setting 
kmeans cid  seeding  As   remedy  Bahmani et al   
propose the algorithm kmeans cid  which aims to reduce
the number of sequential iterations  The key component
of kmeans cid  is detailed in Algorithm   in what we call
kmeans cid  overseeding  First    data point is sampled as
the  rst cluster center uniformly at random  Then  in each
of   sequential rounds  each data point       is independently sampled with probability min
and
added to the set of sampled centers   at the end of the
round  The parameter  cid      is called the oversampling factor and determines the expected number of sampled points
in each iteration 
At the end of Algorithm   one obtains an oversampled
solution with   cid  cluster centers in expectation  The full
kmeans cid  seeding algorithm as detailed in Algorithm  
reduces such   solution to   centers as follows  First  each
of the centers in the oversampled solution is weighted by
the number of data points which are closer to it than the

   cid        
      

 cid 

 cid 

Algorithm   kmeans cid  overseeding
Require  data set       rounds    oversampling factor  cid 
      sample   point uniformly at random from  
  for                   do
 
 
 
 
  Return  

Add   to   cid  with probability min

  cid     
for       do

          cid 

   cid        
      

 cid 

 cid 

Algorithm   kmeans cid  seeding
Require  data set       rounds    oversampling factor  cid 
      Result of Algorithm   applied to          cid 
  for       do
 

Xc   points       whose closest center in   is  
 ties broken arbitrarily but consistently 
wc    Xc 

 
      Result of Algorithm   applied to       
  Return  

other centers  Then  kmeans  seeding is run on the
weighted oversampled solution to produce   set of    
nal centers  The total computational complexity of Algorithm   is   nt cid    in expectation 
The key intuition behind kmeans cid  is that  if we choose
  large oversampling factor  cid  the number of rounds   can
be small   certainly much smaller than    preferably even
constant  The step in lines   and   in Algorithm   can be
distributed over several machines and after each round the
set   can be synchronized  Due to the low number of synchronizations       rounds  Algorithm   can be ef ciently
run in   distributed setting 
Other related work  Celebi et al    provide an
overview over different seeding methods for kMeans    
sampling and kmeans  style algorithms have been independently studied by both Ostrovsky et al    and
Arthur   Vassilvitskii   This research direction has
led to polynomial time approximation schemes based on
  sampling  Jaiswal et al      constant factor
approximations based on sampling more than   centers
 Ailon et al    Aggarwal et al    and the analysis of hard instances  Arthur   Vassilvitskii    Brunsch     oglin    Recently  algorithms to approximate
kmeans  seeding based on Markov Chain Monte Carlo
have been proposed by Bachem et al        Finally 
kmeans  has been used to construct coresets   small
data set summaries   for kMeans clustering  Lucic et al 
  Bachem et al    Fichtenberger et al    Ackermann et al    and Gaussian mixture models  Lucic
et al   

   popular choice is the MLLib library of Apache Spark

 Meng et al    which uses kmeans cid  by default 

Distributed and Provably Good Seedings for kMeans in Constant Rounds

  Intuition and key results
In this section  we provide the intuition and the main results
behind our novel analysis of kmeans cid  and defer the formal statements and the formal proofs to Section  
  Solution quality of kmeans cid 
Solution quality of Algorithm   We  rst consider Algorithm   as it largely determines the  nal solution quality 
Algorithm   with its use of kmeans  to obtain the  nal
  cluster centers  only adds an additional   log    factor
as shown in Theorem   Our key result is Lemma    see
Section   which guarantees that  for  cid       the expected
error of solutions computed by Algorithm   is at most

           

Var        OPT    

 

 cid   

 cid  

  cid 

 cid  

The  rst term may be regarded as   scaleinvariant additive error  It is additive as it does not depend on the optimal quantization error  OPT     It is scaleinvariant since
both the variance and the quantization error are scaled by
  if we scale the data set   by       The second term is  
 traditional  multiplicative error term based on the optimal
quantization error 
Given    xed oversampling factor  cid  the additive error term
decreases exponentially if the number of rounds   is increased  Similarly  for    xed number of rounds    it de 

creases polynomially at   rate   cid   

 cid  if the over sampling

factor  cid  is increased  This result implies that even for   constant number of rounds one may obtain good clusterings by
increasing the oversampling factor  cid  This explains the empirical observation that often even   low number of rounds
  is suf cient and that increasing  cid  increases the solution
quality  Bahmani et al    The practical implications
of this result are nontrivial  Even for the choice of      
and  cid       one retains at most   of the variance as
an additive error  Furthermore  state of the art uniform deviation bounds for kMeans include   similar additive error
term  Bachem et al   
Comparison to previous result  Bahmani et al   
show the following result 
reFor    
turned by Algorithm   with   rounds 
     Corollary   of Bahmani

exp cid      cid   cid        cid 
 cid       
 cid  

et al    bounds the expected quality of   by

Let   be the set

 

   

 
where   denotes the quantization error of   based on the
 rst  uniformly sampled center in kmeans cid  The key difference compared to our result is as follows  First  even as
we increase  cid  the factor   is always nonnegative  Hence 
regardless of the choice of  cid  the additive   term is reduced

         

 
     

 OPT    

  per round  This means that  given the analby at most  
ysis in Bahmani et al    one would always obtain  
constant additive error for   constant number of rounds   
even as  cid  is increased 
Guarantee for Algorithm   Our main result   Theorem     bounds the expected quality of solutions produced
by Algorithm   As in Bahmani et al    one loses another factor of   ln    compared to   due to Algorithm  
Theorem   Let              and  cid       Let   be   data
set in Rd and   be the set returned by Algorithm   Then 

           

ln  

Var      ln   OPT    

 cid 

 cid cid   

 cid  

  cid 

    hard instance for kmeans cid 
We consider the case          which captures the scenario
where kmeans cid  is useful in practice as for       one
may simply use kmeans  instead 
Theorem   For any                        and  cid     
there exists   data set   of size        such that

           
 

   Var    

 cid   

where   is the output of Algorithm   or Algorithm   applied to   with   and  cid  Furthermore 

Var          OPT         and       

where   is the largest distance between any points in    

Theorem   shows that there exists   data set on which
kmeans cid  provably incurs   nonnegligible error even if
the optimal quantization error is zero  This implies that
kmeans cid  with           cannot provide   multiplicative guarantee on the expected quantization error for general data sets  We thus argue that an additive error bound
such as the one in Theorem   is required  We note that the
upper bound in   and the lower bound in Theorem   ex 
 cid   dependence on the oversampling factor  cid 
hibit the same  
for   given number of rounds   
Furthermore  Theorem   implies that  for general data
sets  kmeans cid  cannot achieve the multiplicative error
of   log    in expectation as claimed by Bahmani et al 
  In particular  if the optimal quantization error is
 Note that        Var      Arthur   Vassilvitskii   
 To see this  let            be the quantization error of the
 rst sampled center in Algorithm   and choose   small enough
such that the choice of       log   leads to           For
  in Theorem    OPT         which implies that the desired
multiplicative guarantee would require             However 
the nonnegligible  additive error in Theorem   and Var        
implies that            

Distributed and Provably Good Seedings for kMeans in Constant Rounds

zero  then kmeans cid  would need to return   solution with
quantization error zero  While we are guaranteed to remove
  constant fraction of the error in each round  the number
of required iterations may be unbounded 

  Theoretical analysis
Proof of Theorem   The proof is divided into four
steps  First  we relate kmeans cid style oversampling to
kmeans style   sampling in Lemmas   and   Second  we analyze   single iteration of Algorithm   in
Lemma   Third  we bound the expected solution quality
of Algorithm   in Lemma   Finally  we use this to bound
the expected solution quality of Algorithm   in Theorem  
Lemma   Let   be    nite set and let            be
  set function that is nonnegative and monotonically decreasing                           for all       
Let   be   probability distribution where  for each       
Ea denotes an independent event that occurs with probability qa       Let   be the set of elements       for
which the event Ea occurs 
Let   be the probability distribution on   where   single

      is sampled with probability qa cid 
EP          EQ           cid 

    qa 
Then  with   denoting the empty set  we have that

    qa    

Proof  To prove the claim  we  rst construct   series of
subevents of the events  Ea     and then use them to
recursively bound EP       
Let        For each        let ia be an independent random variable drawn uniformly at random from
               For each       and                    let
Fai be an independent event that occurs with probability

 cid 

 cid   

  Fai   

    qa
 

 

 cid 

 cid   

For each       and                    denote by Eai the
event that occurs if     ia and both Ea and Fai occur  By
design  all these events are independent and thus

qa
 

    qa
 

  Eai      Ea   Fai   ia       
 
for each       and                    Furthermore  for any
     cid      with    cid    cid  and any      cid                   the
events Eai and Ea cid   cid  are independent 
For                   let Gi be the event that none of the
events  Eai cid       cid   occur      
 cid 
 cid 

Gi  

Eai cid 

  cid  

   

where   denotes the complement of    For convenience 
let    be the event that occurs with probability one 
Let                     be any enumeration of    For    
              and                   de ne the event

Gi     Gi     cid 

Eaj cid    

   cid  

We note that by de nition Gi    Gi  and Gi      Gi
for                   
For                   and                   we have

       Gi       cid Eaj     Gi  

 cid 
 cid   cid         Eaj     Gij
 cid           Gi   

    cid 

Eaj cid      Gij

 

 

  cid  

  cid  

 cid 

Eaj   cid 

 

 

 

  cid Eaj     Gi  

  cid  
  sum of    nite geometric series and we have

We now bound the individual terms  The event Gi   implies that the events  Eaj   cid   cid   did not occur  Furthermore  Eaj   is independent of the events  Eaj cid    cid 
  cid  
for   cid   cid     Hence  we have

 cid 
Eaj           cid 
 cid     
  cid Eaj  
 cid 
  cid   Eaj   cid cid 
  cid     cid 
  cid Eaj  
 cid 
      cid cid 
  cid   Eaj   cid cid 
  cid Eaj  
 cid 
  cid Eaj   cid cid   
   cid 
  cid Eaj   cid cid  is
are disjoint  Using   we observe that cid 
where the last equality follows since the events  Eaj   cid   cid  
 cid 
 cid   cid 
 cid 
   cid    qa
 cid   
   cid    qa
 cid 
 cid   
 cid   
 cid     

     cid 
 cid    qa
 cid   
 cid    qa
 cid       aj 
  cid         Eaj     Gij

The event Eaj   implies that   contains aj  Hence  since  
is monotonically decreasing  we have

  cid Eaj     Gi  

  cid Eaj   cid cid   

Together with   and   this implies

 cid 

  cid  

    qa
 

    qa
 

qa
 

 

 

 

 

qa
 

 

 

qa
 

  cid  

 

qa
 

 

Distributed and Provably Good Seedings for kMeans in Constant Rounds

Using   and   this implies
       Gi      qaj
   aj 
 
Applying this result iteratively for                   implies

    qaj
 

 cid 
 cid 

 cid            Gi   
 cid     aj 
 cid    cid         Gi   

 cid 

    qaj cid 
 

 cid 
 cid 

  cid  

    qaj
 

qaj
 

   cid 
     cid 

  

 

  

       Gi   

Note that         qa
nonnegative  This implies that for                  

       Gi   cid 

   

where

   

      for all       and that   is
 cid 

              cid      Gi   
 cid 
 cid 

 cid 

qa
 

    qa
 

 

   

Since Gi    Gi  and Gi      Gi  we have for    
             

                   Gi 

qa
 

Applying this result iteratively  we obtain

 cid  
 cid 

   

 cid 

   

qa
 

For         it holds that log           and hence

cm  

 cid 

 cid 
 cid 

   
  exp

    qa
 

  

 cid cid 

 cid 

 cid 

   

 cid 
    cid 

 

  exp

 cid 

    qa  

log

    qa
 

This implies that
           
     

          cid 

    qa        

qa
 

We show the main claim by contradiction  Assume that

EP          EQ           cid 

    qa    

       Gi   cid 
 cid    cid 

   

         

ci 

  

Since           we have

 cid cid 

   

 cid 

  

  cid 

  

ci   

ci   

 
     

 

        cm      

qa
 

If EQ           the contradiction follows directly
from   Otherwise  EQ           implies that there
exists an       such that
EP               EQ           cid 

    qa    

 

By de nition  we have

 cid 

 cid 

   

   

    qa
 

       cid 

   

 cid 
     cid 
 cid 
 cid   

   

 cid   

 cid 

 

 

qa
 

   

 cid 

   

Thus  there exists          suf ciently large such that

qa
  

   

   

       
     

qa
  

 

 cid 

Together with   this implies

               cid 

qa
   

          cid 
    qa    

qa
  

   

       EQ           cid 

   

    qa      

which is   contradiction to   and thus proves the claim 

Lemma   extends Lemma   to kmeans cid style sampling
probabilities of the form qa   min    cid pa 
Lemma   Let  cid      Let   be    nite set and let         
  be   set function that is nonnegative and monotonically
decreasing                           for all        For

each        let pa     and cid 

    pa    

Let   be the probability distribution where  for each       
Ea denotes an independent event that occurs with probability qa   min    cid pa  Let   be the set of elements      
for which the event Ea occurs 
Let   be the probability distribution on   where   single

      is sampled with probability pa cid 
EP           EQ           cid cid 

    pa 
Then  with   denoting the empty set  we have that

    pa    

Proof  Let    be the set of elements       such that  cid pa  
  and    the set of elements       such that  cid pa     By
de nition  every element in    is sampled almost surely 
             This implies that almost surely
                         

 

If       the result follows trivially since

EP                  EQ      

Distributed and Provably Good Seedings for kMeans in Constant Rounds

Similarly  if       the result follows directly from
Lemma   with qa    cid pa  For the remainder of the proof 
we may thus assume that both    and    are nonempty 
For        let qa    cid pa and de ne the nonnegative and
monotonically decreasing function

Similarly  we have
      cid     

 

  

       

  

       

    
  

       

   

      cid   
        cid   

       

        

  

    cid     

       
Since          it follows that
  

 cid      cid   cid          
 cid 

    cid        
 
Since            and thus            for all    
   we have

pag     cid 

paf    

 

      

       

    

    

Combining       and   leads to

EP                EQ           cid cid 
           cid      cid   cid    cid       
Since        we have

    pa    

which proves the main claim 

Lemma   bounds the solution quality after each iteration of
Algorithm   based on the solution before the iteration 
Lemma   Let       and  cid      Let   be   data set in Rd
and denote by  OPT     the optimal kMeans clustering
cost  Let   denote the set of cluster centers at the beginning
of an iteration in Algorithm   and   cid  the random set added
in the iteration  Then  it holds that

           cid   

          OPT    

 cid   

 cid 

  cid 

                   

Let       cid 
pa and       cid 
EP                    cid 

plied to    qa and   implies that

    

    

pa
  

    

pa  Lemma   ap 

         cid     

Let

and de ne

   cid      cid   cid    cid   

   

  

    

       

       

 

  

By design        Furthermore

 cid      log  cid   

Since    is nonempty and pa    
that       

 cid    This implies

 cid  for all        it follows

Since    cid      cid   cid      we have

  cid       cid        
  

 

         cid        

Hence 

   

  

       

    

       

 cid      cid   cid    cid          
 cid      cid   cid    cid       

Since         and           for any        we
may write       
EP               

      cid      cid   cid    

 cid 

pa
  

    

 

By de nition  we have
              

       

 

  

       

   

  

       

      

Since              we thus have
     

             

 cid 

 cid 

pa
  

    

    

pa

       

     

 

Proof  The proof relies on applying Lemma   to each cluster of the optimal solution  Let OPT denote any clustering
achieving the minimal cost  OPT     on     We assign all
the points       to their closest cluster center in OPT
with ties broken arbitrarily but consistently  For     OPT
we denote by Xc the subset of   assigned to    For each
    OPT  let

      cid    Xc 
  cid 
 cid 
By de nition      Xc is included in   cid 
 cid 
 cid         
  cid       cid    

qa   min

 

 cid 

 

  with probability

For each     OPT  we de ne the monotonically decreasing function fc    Xc      to be

fc   cid 

      Xc       cid 
  

Distributed and Provably Good Seedings for kMeans in Constant Rounds

For each     OPT  Lemma   applied to Xc    cid 
implies

  and fc

 cid 

 cid 

  fc   cid 

    

       

    cid     fc   

  Xc
 cid 

   

  cid Xc
 cid 
 cid 
  Xc
  cid       cid    fc 

      

 cid   

 cid  

  cid 

Lemma   Let              and  cid       Let   be   data
set in Rd and   be the random set returned by Algorithm  
Then 

 

           

Var        OPT    

Since fc       Xc         the  rst term is equivalent
to sampling   single element from Xc using    sampling 
Hence  by Lemma   of Arthur   Vassilvitskii   we
have for all     OPT
 cid 
       

    cid     fc       OPT Xc 

 cid 

 

  Xc

  cid Xc

For each     OPT  we further have

 cid 
 cid 
  Xc
  cid       cid      fc      cid ucuc      

      

 cid 
 cid 
  Xc
  cid       cid      

       

 Xc    
      

 

 cid 

 

where

uc  

We have that

log  cid uc    cid uc        cid uc     cid uc
 

which implies

  cid uc uc          
  cid 

      

 

Combining     and   we obtain

  fc   cid 

    OPT Xc   

 
  cid 

      

 

Since

and

we thus have

  OPT

           cid     cid 
 cid 
 cid 

    OPT   

  OPT

 cid   

  fc   cid 
  

 Xc OPT 

           cid   

          OPT    

  cid 

Proof  The algorithm starts with   uniformly sampled initial cluster center    We iteratively apply Lemma   for
each of the   rounds to obtain

           st OPT      

 cid   

 cid  

  cid 

         

where

 cid   

 cid   

  cid 

 

  cid 

  

st  

For  cid       we have      

st     cid 

  cid       and hence
 cid 

 

 
ei  

      

 

ei   

  

  

 

 

By Lemma   of Arthur   Vassilvitskii   we have
that             Var     Together with    
and                 this implies the required
result 

With Lemma   we are further able to bound the solution
quality of Algorithm   and prove Theorem  

Proof of Theorem   Let   be the set returned by Algorithm   For any         let bx denote its closest point
in   with ties broken arbitrarily  By the triangle inequality
and since                   for any      
                 bx        bx    

and hence

         

 cid 
 cid 

   

   

   

       

     bx     

 cid 

   

  bx    

 

            

wx        

 cid 

   

which concludes the proof 

An iterated application of Lemma   allows us to bound the
solution quality of Algorithm   in Lemma  

Let OPTX be the optimal kMeans clustering solution on
  and OPT      the optimal solution on the weighted set
       By Theorem   of Arthur   Vassilvitskii  

 cid 

   

wx            

   

   

 cid 
 cid 
 cid 

   

   

   

Distributed and Provably Good Seedings for kMeans in Constant Rounds

kmeans  produces an      log        approximation to the optimal solution  This implies that

wx   cid    OPT     

 cid 

Consider   single iteration of Algorithm   where     Ci 
In this case  all points in Xj with       are added to   cid 
with probability zero and for       each point xj is added
to   cid  with probability

wx      OPTX  

  bx  OPTX  

 

min

 

 cid    xj   
  cid     xj cid   

 

 cid    xj   
 cid cid      

in cid 

By the union bound  the probability that any of the points

    xj  are sampled is bounded by

 cid 

 cid 

 cid 

 cid 

   

 cid    xj   
 cid cid      

 
  

 

By the triangle inequality and since              
it holds for any       that

  bx  OPTX            bx           OPTX  

and hence cid 

   

  bx  OPTX               OPT    

The point xi is not sampled with probability at most

 cid 

 cid 

 cid 

 

    min

 

 cid    xi   
  cid     xj cid   

      min

   cid     
  

 cid 

 cid 

Combining     and   we obtain

                        OPT    

Finally  by Lemma   we have

 cid   

 cid  

          log       

Var    
    log       OPT    

  cid 

Proof of Theorem   For this proof  we explicitly construct   data set  Let  cid      and consider points in onedimensional Euclidean space  For                    set

 cid 

 cid cid        cid cid   

  

xi  

as well as

 cid 

xt   

 cid cid   

   

Let the data set   consist of the    points  xi     
as well as       points at the origin  Since          
the optimal kMeans clustering solution consists of      
points placed at each of the  xi      and at   By
design  this solution has   quantization error of zero and the
variance is nonzero        OPT         and Var        
as claimed 
Choose  cid     
    The maximal distance   between
any two points in   is bounded by              cid 
Since            this implies            as claimed 
For                    let Ci consist of   and all xj with       
By design  we have    Ci      as well as   xj  Ci   
  for        For        we have   xj  Ci      xj   
For any                       we thus have

  xj       cid cid     

 cid 

   

   
  

 

  

 cid 

By the union bound    single iteration of Algorithm   with
    Ci hence samples exactly the set   cid     xi  with

probability at least cid     
alently        With probability cid     

In Algorithm   the  rst center is sampled uniformly at random from     Since half of the elements in   are placed at
  the  rst center is at   or equiv 
  with probability at least  
  we then
sample exactly the points               xt in the   subsequent
  the solution
iterations  Hence  with probability at least  
produced by Algorithm   consists of   and all xi except
xt  Since xt  is closest to   this implies
 cid cid   

 cid      

  xt     

   

 

           
 

 
 

  

The variance of   is bounded by   single point at       

Var             

  xj       cid 

 cid 

  

Together with   we have that

           
 

   Var    

 cid   

The same result extends to the output of Algorithm   as it
always picks   subset of the output of Algorithm  

Acknowledgements
This research was partially supported by SNSF NRP  
ERC StG     Google Ph    Fellowship and an IBM
Ph    Fellowship  This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing 

Distributed and Provably Good Seedings for kMeans in Constant Rounds

Fichtenberger  Hendrik  Gill    Marc  Schmidt  Melanie 
Schwiegelshohn  Chris  and Sohler  Christian  Bico 
In EuroBirch meets coresets for kmeans clustering 
pean Symposium on Algorithms  pp    Springer 
 

Jaiswal  Ragesh  Kumar  Amit  and Sen  Sandeep   
simple   sampling based PTAS for kmeans and other
clustering problems  Algorithmica     

Jaiswal  Ragesh  Kumar  Mehul  and Yadav  Pulkit 

Improved analysis of   sampling based PTAS for kmeans
and other clustering problems  Information Processing
Letters     

Lloyd  Stuart  Least squares quantization in PCM  IEEE
Transactions on Information Theory   
 

Lucic  Mario  Bachem  Olivier  and Krause  Andreas 
Strong coresets for hard and soft Bregman clustering
with applications to exponential family mixtures 
In
International Conference on Arti cial Intelligence and
Statistics  AISTATS  May  

Lucic  Mario  Faulkner  Matthew  Krause  Andreas  and
Feldman  Dan  Training mixture models at scale via
coresets  To appear in Journal of Machine Learning Research  JMLR   

Meng  Xiangrui  Bradley  Joseph  Yuvaz     Sparks  Evan 
Venkataraman  Shivaram  Liu  Davies  Freeman  Jeremy 
Tsai     Amde  Manish  Owen  Sean  et al  Mllib  Machine learning in Apache Spark  Journal of Machine
Learning Research  JMLR     

Ostrovsky  Rafail  Rabani  Yuval  Schulman  Leonard   
and Swamy  Chaitanya  The effectiveness of Lloydtype
In Symposium on
methods for the kmeans problem 
Foundations of Computer Science  FOCS  pp   
IEEE   

References
Ackermann  Marcel      artens  Marcus  Raupach 
Christoph  Swierkot  Kamil  Lammersen  Christiane 
and Sohler  Christian  StreamKM    clustering algorithm for data streams  Journal of Experimental Algorithmics  JEA     

Aggarwal  Ankit  Deshpande  Amit  and Kannan  Ravi 
Adaptive sampling for kmeans clustering  In Approximation  Randomization  and Combinatorial Optimization  Algorithms and Techniques  pp    Springer 
 

Ailon  Nir  Jaiswal  Ragesh  and Monteleoni  Claire 
Streaming kmeans approximation  In Advances in Neural Information Processing Systems  pp     

Arthur  David and Vassilvitskii  Sergei  kmeans  The
advantages of careful seeding  In Symposium on Discrete
Algorithms  SODA  pp    SIAM   

Bachem  Olivier  Lucic  Mario  and Krause  Andreas 
Coresets for nonparametric estimation   the case of DPmeans  In International Conference on Machine Learning  ICML   

Bachem  Olivier  Lucic  Mario  Hassani  Hamed  and
Krause  Andreas  Fast and provably good seedings for
kmeans  In Advances in Neural Information Processing
Systems  NIPS  pp       

Bachem  Olivier  Lucic  Mario  Hassani     Hamed  and
Krause  Andreas  Approximate kmeans  in sublinear
In Conference on Arti cial Intelligence  AAAI 
time 
February    

Bachem  Olivier  Lucic  Mario  Hassani     Hamed  and
Krause  Andreas  Uniform deviation bounds for kIn To appear in International Conmeans clustering 
ference on Machine Learning  ICML   

Bahmani  Bahman  Moseley  Benjamin  Vattani  Andrea 
Kumar  Ravi  and Vassilvitskii  Sergei  Scalable KMeans  Very Large Data Bases  VLDB   
   

Brunsch  Tobias and   oglin  Heiko    bad instance for kIn International Conference on Theory and
means 
Applications of Models of Computation  pp   
Springer   

Celebi    Emre  Kingravi  Hassan    and Vela  Patricio   
  comparative study of ef cient initialization methods
for the kmeans clustering algorithm  Expert Systems
with Applications     

