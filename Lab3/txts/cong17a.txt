Deep Latent Dirichlet Allocation with TopicLayer Adaptive

Stochastic Gradient Riemannian MCMC

Yulai Cong   Bo Chen   Hongwei Liu   Mingyuan Zhou  

Abstract

It is challenging to develop stochastic gradient
based scalable inference for deep discrete latent
variable models  LVMs  due to the dif culties
in not only computing the gradients  but also
adapting the step sizes to different latent factors
and hidden layers  For the Poisson gamma belief network  PGBN    recently proposed deep
discrete LVM  we derive an alternative representation that is referred to as deep latent Dirichlet
allocation  DLDA  Exploiting data augmentation and marginalization techniques  we derive
  blockdiagonal Fisher information matrix and
its inverse for the simplexconstrained global
model parameters of DLDA  Exploiting that
Fisher information matrix with stochastic gradient
MCMC  we present topiclayer adaptive stochastic gradient Riemannian  TLASGR  MCMC that
jointly learns simplexconstrained global parameters across all layers and topics  with topic and
layer speci   learning rates  Stateof theart results are demonstrated on big data sets 

  Introduction
The increasing amount and complexity of data call for largecapacity models  such as deep discrete latent variable models  LVMs  for unsupervised data analysis  Hinton et al 
  Bengio et al    Srivastava et al    Ranganath
et al    Zhou et al      and scalable inference methods  such as stochastic gradient Markov chain Monte Carlo
 SGMCMC  that provides posterior samples in   nonbatch
learning setting  Welling   Teh    Patterson   Teh 
  Ma et al    Unfortunately  most deep LVMs 

 National Laboratory of Radar Signal Processing  Collaborative Innovation Center of Information Sensing and Understanding 
Xidian University  Xi an  China   McCombs School of Business 
The University of Texas at Austin  Austin  TX   USA  Correspondence to  Bo Chen  bchen mail xidian edu cn  Mingyuan
Zhou  mingyuan zhou mccombs utexas edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

such as deep belief network  DBN   Hinton et al   
and deep Boltzmann machines  DBM   Salakhutdinov  
Hinton    use greedy layerwise training  without   principled way to jointly learn multilayers in an unsupervised
manner  Bengio et al    While SGMCMC has recently been successfully applied to several  shallow  LVMs 
such as mixture models  Welling   Teh    and mixedmembership models  Patterson   Teh    it has been
rarely applied to  deep  ones  probably due to the lack of
understanding on how to jointly learn the latent variables
of different layers and adjust the layer and topic speci  
learning rates in   nonbatch learning setting 
To investigate scalable SGMCMC inference for deep
LVMs  we focus our study on the recently proposed Poisson gamma belief network  PGBN  whose hidden layers
are parameterized with gamma distributed hidden units and
connected with Dirichlet distributed basis vectors  Zhou
et al      The PGBN is capable of extracting topics
from   text corpus at multiple layers and outperforms   large
number of topic modeling algorithms  However  the PGBN
is currently trained with   batch Gibbs sampler that is not
scalable to big data  In this paper  we focus on developing
scalable multilayer joint inference for the PGBN 
We will show that scalable multilayer joint inference of the
PGBN could be facilitated by its Fisher information matrix  FIM   Amari    Girolami   Calderhead   
Pascanu   Bengio    which  although seemingly impossible to derive and challenging to work with due to the
need to compute the expectations over trigamma functions 
is readily available under an alternative representation of
the PGBN  referred to as deep latent Dirichlet allocation
 DLDA  DLDA  derived by exploiting data augmentation
and marginalization techniques on the PGBN  can be considered as   multilayer generalization of latent Dirichlet
allocation  LDA   Blei et al    Following   general
framework for SGMCMC  Ma et al    the block diagonal structure of the FIM of DLDA makes it be easily
inverted to precondition the minibatch based noisy gradients to exploit the secondorder local curvature information 
leading to topiclayer adaptive step sizes based on the Riemannian manifold and the same asymptotic performance as
  natural gradient based batchlearning algorithm  Amari 
  Pascanu   Bengio    To the best of our knowl 

Deep Latent Dirichlet Allocation with TLASGR MCMC

edge  this is the  rst time that the FIM of   deep LVM is
shown to have an analytical and practical form  How we
derive the FIM for the PGBN using data augmentation and
marginalization techniques in this paper may serve as an
example to help derive the FIMs for other deep LVMs 
Besides presenting the analytical FIM of the PGBN  important for the marriage of   deep LVM and SGMCMC 
we make another contribution in showing how to facilitate
 cid  
SGMCMC for an LVM equipped with simplexconstrained
model parameters                           which means
    vk     and  vk      where               
by using   reducedmean simplex parameterization together
with   fast sampling procedure recently introduced in Cong
et al    Unlike other simplex parameterizations  the
reducedmean one does not make heuristic pseudolikelihood assumptions  Though it has previously been deemed
unsound  it is successfully integrated into our SGMCMC
framework to deliver stateof theart results  Exploiting the
analytical FIM of DLDA and novel inference for simplexconstrained parameters under   general SGMCMC framework  Ma et al    we present topiclayer adaptive
stochastic gradient Riemannian  TLASGR  MCMC for
DLDA  which automatically adjusts the learning rates of
global model parameters across all layers and topics  without the need to set the same learning rate for all that is
commonly used in practice due to the dif culty in identifying an appropriate combination of the learning rates for
different layers and topics 

  PGBN and SGMCMC
The generative model of the Poisson gamma belief network
 PGBN   Zhou et al      with   hidden layers  from top
to bottom  is expressed as
    Gam
 cid 
   
    Gam
   
 cid 
 cid 

        
 

     

       

 cid 

 cid 

 cid 

 cid 

 cid 

 

 

 

 

 

 

 
    Gam
   

    Pois
  

 

 

 

 

 

 
 

 
  

 
 

 

               
Kl

 cid   

where the jth observed or latent    dimensional count vecj   ZV   where               are factorized under
tors   
    RKl
the Poisson  Pois  likelihood  the hidden units    
 
of layer   are factorized under the gamma  Gam  likelihood into the product of the basis vector matrix      
and the hidden units of the

 cid    RKl Kl
    Dir cid   Kl 

next layer  where    
distributed and  Kl  is   Kl dimensional vector of all
ones  the gamma shape parameters           rKL  
at the top layer are shared across all         
      are
    Gam        and
gamma scale parameters  where     
    Beta       are
  
 

   cid      

 cid  are Dirichlet  Dir 

 cid   

  where   

 

 

 

 

 

    

   aj

   

     cid 

introduced to help reduce the dependencies between  
jk
and   
  The PGBN in   can be further extended under the BernoulliPoisson link as   
and under the Poisson randomized gamma link as   

     cid   
 cid  where aj   Gam       
      cid   

Gam cid   
can be directly visualized as cid cid   

The PGBN infers   multilayer deep representation of the
data  whose inferred basis vectors    
  at hidden layer  
    which are
their projections into the    dimensional probability simplex 
The information of the whole data set is compressed by the
PGBN into the inferred sparse network              
where    
indicates the connection strength between node
 basis vector     of layer       and node    of layer   
Moreover  the network structure can be inferred from the
data by combining the gammanegative binomial process
of Zhou   Carin   with   greedy layerwise training
strategy  Extensive experiments in Zhou et al      show
that the PGBN can extract basis vectors that are very speci   abstract in the bottom layer and become increasingly
more general when moving upwards from the bottom to top
hidden layers  and the    hidden units  
in the  rst hidden layer  which are unsupervisedly extracted and regularized with the deep network  are well suited for outof sample
prediction and being used as features for classi cation 
Despite all these attractive model properties  the current
inference of the PGBN relies on an upwarddownward
Gibbs sampler that requires processing all data in each
iteration and hence often does not scale well to big data
unless with parallel computing  To make its inference
scalable to allow processing   large amount of data suf 
 ciently fast on   regular personal computer  we resort to
SGMCMC that subsamples the data and utilizes stochastic gradients in each MCMC iteration to generate posterior
samples for globally shared model parameters  Let us denote the posterior of model parameters   given the data
     xj   as                    with potential funcj ln    xj      As in Theorem   of Ma et al              is the stationary distribution of the dynamics de ned by the stochastic differential

tion           ln        cid 
equation  SDE  dz         dt  cid      dW     if the

 

deterministic drift       is restricted to the form

                                       

        cid 

 
 zj

 Dij       Qij      

 

 
 

where       is   positive semide nite diffusion matrix 
     is   Wiener process        is   skewsymmetric curl
matrix  and       is the ith element of the compensation
vector     Thus one has   minibatch update rule as

 cid   cid   zt   zt cid     zt zt 
    cid 

 cid    zt        Bt

 cid cid 

    

zt    zt     

 cid 

 

 

Deep Latent Dirichlet Allocation with TLASGR MCMC

 cid 

where    denotes step sizes             ln        
      ln             the minibatch    the ratio of the
dataset size     to the minibatch size       and  Bt an estimate of the stochastic gradient noise variance satisfying  
positive de nite constraint as     zt        Bt  cid   
As shown in Ma et al    stochastic gradient Riemannian Langevin dynamics  SGRLD  of Patterson   Teh
         
  is   special case with             
   Bt     where       denotes the Fisher information
matrix  FIM  SGRLD is designed to solve the inference on
the probability simplex  where four different parameterizations of the simplexconstrained basis vectors are discussed 
including reducedmean  expandedmean  reducednatural 
and expandednatural  Here  we consider both expandedmean  previously shown to provide the best overall results 
and reducedmean  which  although discarded in Patterson
  Teh   due to its unstable gradients  is used in this
paper to produce stateof theart results 
Let us denote      RV
  as   vector on the probability simplex       RV
  as   nonnegative vector  and
     RV  
as   nonnegative vector constrained with
    vk     For convenience  the symbol
  will denote the operation of summing over the cor 
 vk
as an expandedmean parameterization of    and

      cid    
responding index  We use  cid            
 cid                cid 

 cid   as   reducedmean

 cid   cid   cid 

     vk

parametrization of     SGRLD focuses on   singlelayer
model with   multinomial likelihood nk   Mult          
and   Dirichlet distributed prior      Dir      For
inference  it adopts the expandedmean parameterization
of    and makes   heuristic assumption that      

 cid  While that heuristic pseudolikelihood assump 

Pois cid    

tion of SGRLD is neither supported by the original generative model nor rigorously justi ed in theory  it converts
  Dirichletmultinomial model into   gammaPoisson one 
allowing   simple sampling equation for    as

 

 

 cid cid cid cid    
 cid 
    cid 

 

 cid 
 nk  cid         
 cid cid cid cid cid   

 cid cid    

 cid 

 cid 

 cid    

 

   tdiag

 
where the absolute operation   is used to ensure positivevalued     Below we show how to eliminate that heuristic
assumption by parameterizing    with reducedmean  and
develop ef cient SGMCMC for the PGBN  which reduces
to LDA when the number of hidden layers is one 

 cid 

 cid    

 

  

  

  Deep Latent Dirichlet Allocation
While the original construction of PGBN in   makes it
seemingly impossible to compute the FIM  as shown in
Appendix    we  nd that  by exploiting data augmentation
and marginalization techniques  the PGBN generative model

generated with      cid  

can be rewritten under an alternative representation that
marginalizes out all the gamma distributed hidden units 
as shown in the following Lemma  where Log  denotes
the logarithmic distribution  Johnson et al       
SumLog       represents the sumlogarithmic distribution
   ui  ui   Log     Zhou et al 
 cid 
 cid cid cid cid 

    The proof is deferred to the Appendix 
Lemma   Denote     
        
 
                 where   

    which means     
      ln

       

     

 cid 

 cid 

 cid 

  ln

 cid 

 cid 

for

ln

ln

 

 

 

 

 

ln

     
    
    
 
 
            
  and          

     
  
 

With     
          
 
 
one may reexpress the hierarchical model of the PGBN as
deep latent Dirichlet allocation  DLDA  as

 

 

   
 

 

 cid     

 

 cid 

 

 cid 
 cid 

  Log    KL   Pois  ln       

       

  

    
  

 

vj

 cid 

    
  

      

  Mult

       cid KL
 cid 
 cid     
vj  cid Kl
vj  cid   

       
vkj 
      

 
  
vkj

     
vkj 

 cid 

 cid 

 cid 

 

  

vj

vj

 cid 

 

    

 

 

 

vj

 
 

 cid 

 cid     
 cid 
 cid 

  Mult

  SumLog     

      

 

  Mult
    
vkj
  SumLog     

 

vj       

   

      

kj

     
 

 

  

kj

   
 

 

 

 cid 

Note that the equations in the  rst four lines of   precisely
represent   random count matrix generated from   gammanegative binomial process that can also be generated from

    
kj

  Pois

  rk   Gam          

 cid 

 cid 

 

rkq   
  SumLog

 cid 

 

kj

      

    
kj

      

 
by letting        Zhou et al      When       the
PGBN whose  rk      are the points of   gamma process reduces to the gammanegative binomial process PFA of Zhou
  Carin   whose alternative representation is provided in Corollary    in the Appendix  Note that how we
reexpress the PGBN as DLDA is related to how Schein et al 
  reexpress their Poisson gamma dynamic systems
into an alternative representation that facilitates inference 
DLDA  designed to infer   multilayer representation of observed or latent highdimensional sparse count vectors  constrains all the basis vectors of different layers to probability
simplices  It is clear from   that   data point backpropagates its counts through the network one layer at   time via
  sumlogarithmic distribution to enlarge each element of
  Kldimensional count vector    multinomial distribution
to partition that enlarged count vector into   Kl    Kl
count matrix  and then   rowsum operation to aggregate

Deep Latent Dirichlet Allocation with TLASGR MCMC

 cid 

 cid 

that latent count matrix into   Kl dimensional count vector  where        is the feature dimension  Below we
show that such an alternative representation that repeats the
enlargepartition augment operation brings signi cant bene ts when it comes to deriving SGMCMC inference with
preconditioned gradients 

  Fisher Information Matrix of DLDA

In deep LVMs  whose parameters of different layers are
often highly correlated to each other  it is often dif cult to
tune the step sizes of different layers together and hence
one often chooses to train an unsupervised deep model in
  greedy layerwise manner  Bengio et al    which
is   sensible but not optimal training strategy  To address
that issue  we resort to the inverse of the FIM that is widely
used to precondition the gradients to adjust the learning
rates of different model parameters  Amari    Pascanu
  Bengio    Ma et al    Li et al    However 
it is often dif cult to compute the FIMs of deep LVMs as

           

   
    ln       

 

 

where   denotes the set of all global variables and   is the
set of all observed and local variables 
Although deriving the FIM for the PGBN generative model
shown in   seems impossible  we  nd it to be straightforward under the alternative DLDA representation in  
Since the likelihood of   is fully factorized with respect
to the global parameters            
  and    one may readily
show the FIM       of   has   block diagonal form as

 cid 

 cid 

diag

 

 

 

with the likelihood
the reducedmean parameterization  we have

kj

 

 

 cid 

 cid 

       

 cid 

    
vkj

     

      

 cid 
 cid cid 
 cid 

 cid 
 cid 
 cid 
   
KL
  Mult
 cid 
 cid 
     
vkj          
   cid 
         
      
      
  
  Pois rkq   
 cid 

  we have

   
 

jMult

 cid 

 cid 

 cid 

kj

 

    
 

 

 

 

ln

 cid 

 cid 

   
 

 
   
 

  

       
 

 cid 
 cid 
      cid 
where           cid 

where      
likelihood     

diag

kj

 

and

 cid 
 cid cid cid 

     
 

     
 

 

 

  Similarly  with the

             diag      

 

The block diagonal structure of the FIM of DLDA makes
it computationally appealing to apply its inverse for preconditioning  Under the framework suggested by   we
adopt the similar settings used in SGRLD  Patterson   Teh 
            and  Bt    
  that lets             
While other more sophisticated settings described in Ma
et al    including as special examples stochastic gradient Hamiltonian Monte Carlo in Chen et al    and

stochastic gradient thermostats in Ding et al    may
be used to further improve the performance  we choose this
speci   one to make   direct comparison with SGRLD 
By substituting the FIM       and the adopted settings
into   it is apparent that we only need to choose   single
step size     relying on the FIM to automatically adjust the
relatively learning rates for different parameters across all
layers and topics  Moreover  the blockdiagonal structure
of       will be carried over to its inverse       making it
simple to perform updating using   as described below 

  Inference on the Probability Simplex

As discussed in Section   to sample simplexconstrained
model parameters for   Dirichletmultinomial model  the
SGRLD of Patterson   Teh   adopts the expandedmean parameterization of simplexconstrained vectors and
makes   pseudolikelihood assumption to simplify the derivation of update equations  In this paper  without replying
on that pseudolikelihood assumption  we choose to use the
reducedmean parameterization of simplexconstrained vectors  despite being considered as an unsound choice in Patterson   Teh   In the following discussion  we omit
the layerindex superscript     for simplicity 
With the multinomial likelihood in   and the Dirichletmultinomial conjugacy  the conditional posterior of    can
be expressed as        Dir                  xV       
Taking the gradient with respect to      RV  
on the
summation of the negative loglikelihood of   minibatch   
scaled by             and the negative loglikelihood of
the Dirichlet prior  we have

 

       

  

   xV     
      

 

 

 cid       

  

 cid 
   cid 

 

 xvk 

  xj     xvkj

where
and
     
 
                          Note the gradient
in   becomes unstable when some components of    approach
zeros    key reason that this approach is mentioned but not
further pursued in Patterson   Teh  
However  after preconditioning the noisy gradient with the
inverse of the FIM  it is intriguing to  nd out that the stability
issue completely disappears  More speci cally  by plugging
both the FIM in   and noisy gradient in   into the SGMCMC update in     noisy estimate of the deterministic
drift de ned in   obtained using the current minibatch
can be expressed as

     
 
 

    
       
                      

   

 
          according to   as dewhere           
rived in detail in Appendix    Consequently  with  cid  de 

 

 cid        

 cid 

Deep Latent Dirichlet Allocation with TLASGR MCMC

 

 cid 

 cid   

    
Mk

       

         
Mk

 cid 
   cid 

diag              

noting the constraint that  vk     and cid    
    vk    
 cid 
 cid cid cid 

 cid                   

using   the sampling of    becomes

 
Even without the  cid  constraint  the multivariate normal
 MVN  simulation in   although easy to interpret and
numerically stable  is computationally expensive if the
Cholesky decomposition  with         complexity
 Golub   Van Loan    is adopted directly  Fortunately 
using Theorem   of Cong et al    the special structure of its covariance matrix allows an equivalent but substantially more ef cient simulation of       complexity
by transforming   random variable drawn from   related
MVN that has   diagonal covariance matrix  More specifically  the sampling of   can be ef ciently realized in  
   dimensional space as

 cid                   

 cid 

 cid 

       

      

 cid 

  
Mk
  
Mk

   

 cid cid 

Algorithm   TLASGR MCMC for DLDA  PGBN 
Input  Data minibatches 
Output  Global parameters of DLDA  PGBN 
  for         do
 
 

  Collect local information
Upwarddownward Gibbs sampling  Zhou et al      on
the tth minibatch for                  
  Update global parameters
for           and         Kl do

  and      

 

 

 

Update      

  with   then    

  with  

end for
Update       with   and then   with  

 
 
 
 
 
  end for

Note that as in   and   instead of having   single learning rate for all layers and topics    common practice due to
the dif culty to adapt the step sizes to different layers and or
topics  the proposed inference employs topiclayer adaptive
learning rates as         
       
adapting   single step size    to different topics and layers by
for                 
multiplying it with the weights       
and               Kl  We refer to the proposed inference alk
gorithm with adaptive learning rates as topiclayer adaptive
stochastic gradient Riemannian  TLASGR  MCMC  as summarized in Algorithm   that is simple to implement 

    where      

 

  Related Work
Both LDA  Blei et al    and the related Poisson factor
analysis  PFA   Zhou et al    are equipped with scalable inference  such as stochastic variational inference  SVI 
 Hoffman et al    Mimno et al    and SGRLD
 Patterson   Teh    However  both are shallow LVMs
whose modeling capacities are often insuf cient for big and
complex data  The deep Poisson factor models of Gan et al 
  and Henao et al    are proposed to generalize
PFA with deep structures  but both of them only explore the
deep information in binary topic usage patterns instead of
the full connection weights that are used in the PGBN  The
proposed DLDA shares some similarities with the pachinko
allocation model of Li   McCallum   in that they both
adopt layered construction and use Dirichlet distributed topics  Ranganath et al    propose deep exponential family
 DEF  which differs from the PGBN in connecting adjacent
layers via the gamma rate parameters and using blackbox
variational inference  BBVI   Ranganath et al   
Some commonly used neural networks  such as deep belief
network  DBN   Hinton et al    and deep Boltzmann
machines  DBM   Salakhutdinov   Hinton    have
also been modi ed for text analysis  Hinton   Salakhutdinov    Larochelle   Lauly    Srivastava et al 
  Although they may work well for certain text analysis tasks  they are not naturally designed for count data and
often yield latent structures that are not readily interpretable 

 

 

 

diag     

 
where   denotes the simplex constraint that  vk     and
    vk     More details on simulating   and  

 cid  

can be found in Examples   of Cong et al   
Similarly  with the gammaPoisson construction in   we
have                 as in Appendix    and
 rk

 cid     

 cid 

 
 

 cid 

   

 

  

 
which also becomes unstable if rk approaches zero  Substituting   and   into   leads to

 

 cid cid           
 cid 

 cid 

        

 

 rt

     

 

 

 cid cid 

 
KL

rt   

 cid       
 cid cid cid cid rt  
 cid 

   

 cid cid 

 

  

     
  

 

      diag  rt 

 
KL

 cid cid cid cid cid 

for which there is no stability issue 

  TopicLayer Adaptive Stochastic Gradient

Riemannian MCMC

for                  appearing
Note that       and      
 
as denominates in   and   respectively  are expectations that need to be approximately calculated  We update
them using annealed weighting  Polatkan et al    as

 cid 
 cid 

     
     

 cid 
 
 cid 
 

 cid 
 cid 

 cid 

 cid 

 cid 

 cid 
   
 cid 
   

 cid 

     

   

     

     

       

 

 

       

         

 
where    denotes averaging over the collected MCMC
 cid 
samples  For simplicity  we set  
       in this paper  which
is found to work well in practice 

     
 

 

Deep Latent Dirichlet Allocation with TLASGR MCMC

The neural variational document model  NVDM  of Miao
et al    even though using deep neural networks in its
variational autoencoder  VAE   Kingma   Welling   
still relies on   singlelayer model for data generalization 
Generally speaking  it is challenging to develop an ef cient
and principled multilayer joint learning algorithm for deep
LVMs  Scalable variational inference  such as BBVI  often
makes the restrictive mean eld assumption  Neural variational inference and learning  NVIL  relies on variance
reduction techniques that are often dif cult to be generalized for discrete LVMs  Mnih   Gregor    Rezende
et al    When   SGMCMC algorithm is used   
single learning rate is often applied for different variables
across all layers  Welling   Teh    Neal et al   
Chen et al    Ding et al    It is possible to improve SGMCMC by adjusting its noisy gradients with some
stochastic optimization technique  such as Adagrad  Duchi
et al    Adadelta  Zeiler    Adam  Kingma   Ba 
  and RMSprop  Tieleman   Hinton    For example  Li et al    show that preconditioning the gradients
with diagonal approximated FIM improves SGMCMC in
both training speed and predictive accuracy for supervised
learning where gradients are easy to calculate  Other efforts
exploiting similar preconditioning idea focus on shallow
and or binary models  Mimno et al    Patterson   Teh 
  Grosse   Salakhutdinov    Song et al   
Simsekli et al    and it is unclear how that idea can be
extended to deep LVMs whose gradients and FIM maybe
dif cult to approximate 

  Experiment results
We present experimental results on three benchmark corpora 
 Newsgroups  News  Reuters Corpus Volume    RCV 
that is moderately large  and Wikipedia  Wiki  that is huge 
 News consists of   documents with   vocabulary
size of   partitioned into   training documents
and   test ones  RCV  consists of   documents
with   vocabulary size of   where   documents
are randomly selected for testing  Wiki consists of   million documents randomly downloaded from Wikipedia using
the scripts provided in Hoffman et al    as in Hoffman
et al    Gan et al    and Henao et al   
we use   vocabulary with   words and randomly select
  documents for testing  To make   fair comparison 
these corpora  including the training testing partitions  are
set to be the same as those in Gan et al    and Henao
et al    To be consistent with the settings of Gan et al 
  and Henao et al    no precautions are taken in
the scripts for Wikipedia to prevent   testing document from
being downloaded into   minibatch for training 
We consider two related performance measures  The  rst
one is the commonlyused perheldout word perplexity cal 

culated as follows  for each test document  we randomly
select   of the word tokens to sample the local variables
speci   to the document  under the global model parameters of each MCMC iteration  after the burnin period  we
accumulate the  rst layer   Poisson rates in each collected
MCMC sample  in the end  we normalize these accumulated
Poisson rates to calculate the perplexity using the remaining
  word tokens  Similar evaluation methods have been
widely used       in Wallach et al    Paisley et al 
  and Zhou   Carin   Although   good measure
for overall performance  the perheldout word perplexity 
calculated based on multiple collected MCMC samples of
global parameters  may not be ideal to check the performance in real time to assess how ef cient an iterative algorithm improves its performance as time increases  Therefore 
we slightly modify it to provide   point perheldout word
perplexity calculated based on only the global parameters
of the most recent MCMC sample  For simplicity  we refer
to  point  perheldout word perplexity as  point  perplexity 
For comparison  we consider LDA of Blei et al    focused topic model  FTM  of Williamson et al    replicated softmax  RSM  of Hinton   Salakhutdinov  
nested Hierarchical Dirichlet process  nHDP  of Paisley
et al    DPFA of Gan et al    and DPFM of
Henao et al    For these methods  the perplexity
results are taken from Gan et al    and Henao et al 
  For the proposed algorithms  we set the minibatch
size as   and use as burnin   minibatches for both
 News and RCV  and   minibatches for Wiki  We
collect   samples to calculate perplexity  For point perplexity  given the global parameters of an MCMC sample 
we sample the local variables with   iterations and collect
one sample every two iterations during the last   iterations 
The hyperparameters of DLDA are set as         Kl 
            and                      Note    
and Kl are set similar to that of DPFM for fair comparisons 
while other hyperparameters follow Zhou et al     
To demonstrate the advantages of using the reducedmean
simplex parameterization and inverting the FIM for preconditioning to obtain topiclayer adaptive learning rates  we
consider four different inference methods 
  TLASGR  topiclayer adaptive stochastic gradient Riemannian MCMC for DLDA  as described in Algorithm  
  TLFSGR  topiclayer xed stochastic gradient Riemannian MCMC for DLDA that replaces the adaptive learning
rates         
  SGRLD  updating each    
  under the expandedmean
parameterization as in   served as   good scalable baseline
for comparison since it was shown in Patterson   Teh  
to perform signi cantly better than SVI  It differs from
TLFSGR mainly in using   different parameterization for

  of TLASGR with    cid   

      

     

Deep Latent Dirichlet Allocation with TLASGR MCMC

      singlelayer DLDA on  News

    DLDA of size   on RCV 

    DLDA of size   on Wiki

Figure   Plot of point perplexity as   function of time       News with   singlelayer DLDA with   topics      RCV  with   twolayer
DLDA with   and   topics in the  rst and second layers  respectively      Wiki with   twolayer DLDA  with   and   topics in the
 rst and second layers  respectively  Note   small subset of   documents from Wiki is used for demonstration 

Table   Perheldout word perplexities on   News  RCV  and
Wiki  For models except DLDA  the results are taken from Gan
et al    and Henao et al    Note that for Wiki  DPFM
with MCMC infers the global parameters on   subset of the corpus
with   MCMC iterations 

Size

Method
Model
TLASGR  
DLDA
TLASGR  
DLDA
TLASGR  
DLDA
TLFSGR
DLDA
TLFSGR
DLDA
TLFSGR
DLDA
SGRLD
DLDA
SGRLD
DLDA
SGRLD
DLDA
Gibbs
DLDA
Gibbs
DLDA
Gibbs
DLDA
SVI
DPFM
MCMC
DPFM
DPFASBN
SGNHT
DPFARBM SGNHT
nHDP
LDA
FTM
RSM

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

SVI
Gibbs
Gibbs
CD 

  News RCV  Wiki
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

   
  and adding   pseudolikelihood assumption 
  Gibbs  the upwarddownward Gibbs sampler in Zhou
et al     
Both TLASGR and TLFSGR differ from SGRLD mainly
in how the global parameters    
are updated  While
 
TLASGR uses topiclayer adaptive learning rates  both TLFSGR and SGRLD use   single learning rate    common
practice due to the dif culty of tuning the step sizes across
layers and topics  We keep the same stepsize schedule of
                as in Patterson   Teh   and Ma
et al   
Let us  rst examine how various inference algorithms perform on  News with   singlelayer DLDA of size  
which can be considered as   topic model that imposes an

 

asymmetric prior on   document   proportion over these
  topics  Under this setting  as shown in Fig      TLFSGR clearly outperforms SGRLD in providing lower point
perplexities as time progresses  which is not surprising as
under the reducedmean simplex parameterization  to derive its sampling equations  TLFSGR does not rely on  
pseudolikelihood assumption that is adopted by SGRLD
in its expandedmean simplex parameterization  Moreover 
TLASGR is found to further improve TLFSGR  suggesting
that even for   singlelayer model  replacing    xed learning
      with topicadaptive learning

rate as    cid   

      

could further improve the performance 

rates as       
Let us then examine how these algorithms perform on two
larger corpora RCV  and Wiki with   twolayer DLDA
of size   which improves the singlelayer one by capturing the cooccurrence patterns between the topics of the
 rst layer with those of the second layer  Zhou et al     
As show in Figs      and     it is clear that the proposed
TLASGR performs the best for the twolayer DLDA and
consistently outperforms TLFSGR as time progresses  In
comparison  SGRLD quickly improves its performance as
  function of time in the beginning but its point perplexity
remains higher even after   seconds  whereas Gibbs
sampling slowly improves its performance as   function of
time in the beginning but moves its point perplexity closer
and closer to that of TLASGR as time progresses 
Note that for  News  the point perplexity of the minibatch
based TLASGR quickly decreases as time increases  while
that of Gibbs sampling decreases relatively slowly  That discrepancy of convergence rate as   function of time becomes
much more evident for both RCV  and Wiki  as shown in
Figs      and     This is expected as both RCV  and
Wiki are much larger corpora  for which   minibatch based
inference algorithm can already make signi cant progress
in learning the global model parameters  before   batchlearning Gibbs sampler  nishes   single iteration that needs
to go through all documents 

 Time  Seconds Point perplexityDLDAGibbsDLDA SGRLDDLDATLFSGRDLDA TLASGR Time  Seconds Point perplexityDLDAGibbsDLDA SGRLDDLDATLFSGRDLDA TLASGR Time  Seconds Point perplexityDLDAGibbsDLDA SGRLDDLDATLFSGRDLDA TLASGR Deep Latent Dirichlet Allocation with TLASGR MCMC

   

   

   

   

   

   

   

   

   

Figure   Topiclayer adaptive learning rates inferred with   threelayer DLDA of size        News      RCV      Wiki 
Note the layeradaptive learning rate for layer   is obtained by
averaging over the topiclayer adaptive learning rates of all    
 
for               Kl 

To illustrate the working mechanism of TLASGR  we show
how its inferred learning rates are adapted to different layers
in Fig    By contrast  TLFSGR admits    xed learning
rate that leads to worse performance  Several interesting
observations can be made for TLASGR from Figs       
  for     higher layers prefer larger step sizes  which
may be explained by the enlargepartition augment data
generating mechanism of DLDA    larger datasets prefer
slower learning rates  re ected by the scales of the vertical
axes    and the relative learning rates between different
layers vary across different datasets 
To further verify the excellent performance of DLDA inferred with TLASGR  we compare   wide variety of models
and inference algorithms in Table   For  News and RCV 
DLDA with Gibbs sampling performs the best in terms of
perplexity and exhibits   clear trend of improvement as the
number of hidden layers increases  For Wiki    single iteration of the DLDA Gibbs sampler on the full corpus is so
expensive in both time and memory that its performance is
not reported  For DLDA on  News and RCV  TLASGR
only slightly underperforms Gibbs sampling  and the performance degradation from Gibbs sampling to TLASGR is signi cantly smaller than that from MCMC to SVI for DPFM 
That relative small degradation caused by changing from
Gibbs sampling to the minibatch based TLASGR could be
attributed to the Fisher ef ciency brought by the FIM  Generally speaking  in comparison to SGRLD  TLASGR brings
  clear boost in performance  which is particularly evident
for   deeper DLDA  and TLASGR consistently outperforms
TLFSGR that does not adapt its learning rates to different
topics and layers 

Figure   Learned dictionary atoms on MNIST digits with   threelayer GBN of size   after one full epoch  Shown in      
are example atoms for  
    respectively  learned with TLFSGR  and shown in       are example
ones learned with TLASGR 

    and  

     

MNIST  To further illustrate the advantages of using the
inverse of the FIM for preconditioning in   deep generative
model  and to visualize the bene ts of automatically adjusting the relative learning rates of different hidden layers 
we apply   threelayer Poisson randomized gamma gamma
belief network  PRGGBN   Zhou et al      to  
MNIST digits and present the learned dictionary atoms after
one full epoch  as shown in Fig    It is clear that  with
topiclayer adaptive learning rates  which are made possible by utilizing the FIM  TLASGR provides more effective
minibatch based stochastic updates to allow better information propagation between different hidden layers  extracting
more interpretable features at multiple layers 

  Conclusions
For scalable multilayer joint inference of the Poisson gamma
belief network  PGBN  we introduce an alternative representation of the PGBN  which is referred to as deep latent
Dirichlet allocation  DLDA  that can be considered as   multilayer generalization of latent Dirichlet allocation  We show
how to reparameterize the simplex constrained basis vectors 
derive   blockdiagonal Fisher information matrix  FIM 
and ef ciently compute the inverse of the FIM  leading to  
stochastic gradient MCMC algorithm referred to as topiclayer adaptive stochastic gradient Riemannian  TLASGR 
MCMC  The proposed TLASGRMCMC is able to jointly
learn the parameters of different layers with topiclayer 
adaptive step sizes  which makes DLDA  PGBN  much
more practical in   big data setting  Compelling experimental results on large text corpora and the MNIST dataset
demonstrated the advantages of TLASGRMCMC 

 Iterations Layeradaptive step sizes   Iterations Layeradaptive step sizes   Iterations Layeradaptive step sizes  Deep Latent Dirichlet Allocation with TLASGR MCMC

Acknowledgements
Bo Chen thanks the support of the Thousand Young Talent Program of China  NSFC   and NDPR 
   DZ  Hongwei Liu thanks the support
of NSFC for Distinguished Young Scholars  

References
Amari     Natural gradient works ef ciently in learning 

Neural Computation     

Bengio     Lamblin     Popovici     and Larochelle    
Greedy layerwise training of deep networks  In NIPS 
pp     

Blei        Ng        and Jordan        Latent Dirichlet

allocation  JMLR     

Chen     Fox        and Guestrin     Stochastic gradient
In ICML  pp   

Hamiltonian Monte Carlo 
 

Cong     Chen     and Zhou     Fast simulation of
hyperplanetruncated multivariate normal distributions 
Bayesian Analysis Advance Publication   

Ding     Fang     Babbush     Chen     Skeel        and
Neven     Bayesian sampling using stochastic gradient
thermostats  In NIPS  pp     

Duchi     Hazan     and Singer     Adaptive subgradient
methods for online learning and stochastic optimization 
JMLR   Jul   

Gan     Chen     Henao     Carlson     and Carin    
Scalable deep Poisson factor analysis for topic modeling 
In ICML  pp     

Girolami     and Calderhead     Riemann manifold
Langevin and Hamiltonian Monte Carlo methods  JRSSB     

Golub  Gene   and Van Loan  Charles    Matrix computa 

tions  volume   JHU Press   

Hoffman        Bach        and Blei        Online
learning for latent Dirichlet allocation  In NIPS  pp   
   

Johnson        Kotz     and Balakrishnan     Discrete
Multivariate Distributions  volume   Wiley New York 
 

Kingma     and Ba     Adam    method for stochastic

optimization  arXiv   

Kingma  Diederik   and Welling  Max  Autoencoding

variational Bayes  In ICLR  number    

Larochelle     and Lauly       neural autoregressive topic

model  In NIPS   

Li     Chen     Carlson     and Carin     Preconditioned
stochastic gradient Langevin dynamics for deep neural
networks  AAAI   

Li     and McCallum     Pachinko allocation  DAGstructured mixture models of topic correlations  In ICML 
pp     

Ma     Chen     and Fox       complete recipe for stochas 

tic gradient MCMC  In NIPS  pp     

Miao     Yu     and Blunsom     Neural variational infer 

ence for text processing  In ICML   

Mimno     Hoffman        and Blei        Sparse stochastic inference for latent Dirichlet allocation  In ICML  pp 
       

Mnih     and Gregor     Neural variational inference and

learning in belief networks   

Neal        et al  MCMC using Hamiltonian dynamics 
Handbook of Markov Chain Monte Carlo   
 

Paisley     Wang     and Blei     The discrete in nite logistic normal distribution for mixedmembership modeling 
In AISTATS   

Grosse        and Salakhutdinov     Scaling up natural
gradient by sparsely factorizing the inverse Fisher matrix 
In ICML  pp     

Paisley     Wang     Blei        and Jordan        Nested
hierarchical dirichlet processes  PAMI   
 

Henao     Gan     Lu     and Carin     Deep Poisson

factor modeling  In NIPS  pp     

Pascanu     and Bengio     Revisiting natural gradient for

deep networks  arXiv   

Hinton        and Salakhutdinov        Replicated softmax 
In NIPS  pp   

an undirected topic model 
 

Patterson     and Teh        Stochastic gradient Riemannian
Langevin dynamics on the probability simplex  In NIPS 
pp     

Hinton        Osindero     and Teh          fast learning
algorithm for deep belief nets  Neural Computation   
   

Polatkan     Zhou     Carin     Blei     and Daubechies 
     Bayesian nonparametric approach to image superresolution  PAMI     

Deep Latent Dirichlet Allocation with TLASGR MCMC

Zhou     Padilla     and Scott        Priors for random
count matrices derived from   family of negative binomial
processes     Amer  Statist  Assoc   
   

Ranganath     Gerrish     and Blei        Black box

variational inference  In AISTATS   

Ranganath     Tang     Charlin     and Blei        Deep

exponential families  In AISTATS   

Rezende  Danilo    Mohamed  Shakir  and Wierstra  Daan 
Stochastic backpropagation and approximate inference in
deep generative models  In ICML  pp     

Salakhutdinov     and Hinton        Deep Boltzmann ma 

chines  In AISTATS  volume   pp     

Schein     Zhou     and Wallach     Poisson gamma

dynamical systems  In NIPS  pp     

Simsekli     Badeau     Cemgil        and    Richard 
In

Stochastic quasiNewton Langevin Monte Carlo 
ICML   

Song     Henao     Carlson     and Carin     Learning
sigmoid belief networks via Monte Carlo expectation
maximization  In AISTATS  pp     

Srivastava     Salakhutdinov        and Hinton       
Modeling documents with deep Boltzmann machines  In
UAI   

Tieleman     and Hinton     Lecture  rmsprop  Divide
the gradient by   running average of its recent magnitude 
COURSERA  Neural networks for machine learning   
   

Wallach        Murray     Salakhutdinov     and Mimno 
   Evaluation methods for topic models  In ICML   

Welling     and Teh       Bayesian learning via stochastic
In ICML  pp   

gradient Langevin dynamics 
 

Williamson     Wang     Heller        and Blei       
The IBP compound Dirichlet process and its application
In ICML  pp   
to focused topic modeling 
 

Zeiler        Adadelta  an adaptive learning rate method 

arXiv   

Zhou     and Carin     Negative binomial process count

and mixture modeling  PAMI     

Zhou     Hannah     Dunson        and Carin     Betanegative binomial process and Poisson factor analysis  In
AISTATS  pp     

Zhou     Cong     and Chen     Augmentable gamma
belief networks  Journal of Machine Learning Research 
     

