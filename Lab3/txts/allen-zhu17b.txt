Doubly Accelerated Methods for Faster CCA

and Generalized Eigendecomposition

Zeyuan AllenZhu     Yuanzhi Li    

Abstract

We study kGenEV  the problem of  nding the
top   generalized eigenvectors  and kCCA  the
problem of  nding the top   vectors in canonicalcorrelation analysis  We propose algorithms
LazyEV and LazyCCA to solve the two problems
with running times linearly dependent on the input size and on    Furthermore  our algorithms
are doublyaccelerated  our running times depend only on the square root of the matrix condition number  and on the square root of the eigengap  This is the  rst such result for both kGenEV or kCCA  We also provide the  rst gapfree results  which provide running times that depend on  

  rather than the eigengap 

 

  Introduction
The Generalized Eigenvector  GenEV  problem and the
Canonical Correlation Analysis  CCA  are two fundamental problems in scienti   computing  machine learning  operations research  and statistics  Algorithms solving these
problems are often used to extract features to compare
largescale datasets  as well as used for problems in regression  Kakade   Foster    clustering  Chaudhuri et al 
  classi cation  Karampatziakis   Mineiro   
word embeddings  Dhillon et al    and many others 
GenEV  Given two symmetric matrices        Rd  
where   is positive de nite  The GenEV problem is to  nd
generalized eigenvectors            vd where each vi satis es
vi   arg max
  cid Bvj                
  Rd
The values   
  Avi are known as the generalized
eigenvalues  and it satis es         Following the
this paper
shall be found at http arxiv org abs 
 Microsoft Research  Princeton University  Correspondence
to  Zeyuan AllenZhu  zeyuan csail mit edu  Yuanzhi Li
 yuanzhil cs princeton edu 

 cid    cid Bv    

 cid cid   cid Av cid cid      

 Equal contribution  

Future version of

def    cid 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tradition of  Wang et al    Garber   Hazan    we

assume without loss of generality that         

    cid    Sxy    

CCA  Given matrices     Rn dx       Rn dy and den    cid    
noting by Sxx    
the CCA problem is to  nd canonicalcorrelation vectors
        
          arg max
 Rdx  Rdy

    cid     Syy    
   where     min dx  dy  and each pair

 cid   cid Sxx         cid Sxx                  

 cid cid Sxy cid 

such that

 cid Syy         cid Syy                  
  Sxy       are known as the
def   cid 

The values   
canonicalcorrelation coef cients  and

                   is always satis ed 

It is   fact that solving CCA exactly can be reduced to solving GenEV exactly  if one de nes     diag Sxx  Syy   
xy      Rd   for   def  dx  dy 
Rd   and       Sxy     cid 
see Lemma  
 This reduction does not always hold if
the generalized eigenvectors are computed only approximately 
Despite the fundamental importance and the frequent necessity in applications  there are few results on obtaining
provably ef cient algorithms for GenEV and CCA until
very recently 
In the breakthrough result of Ma  Lu and
Foster  Ma et al    they proposed to study algorithms
to  nd top   generalized eigenvectors  kGenEV  or top  
canonicalcorrelation vectors  kCCA  They designed an
alternating minimization algorithm whose running time is
only linear in the input matrix sparsity and nearlylinear in
   Such algorithms are very appealing because in reallife
applications  it is often only relevant to obtain top correlation vectors  as opposed to the less meaningful vectors in
the directions where the datasets do not correlate  Unfortunately  the method of Ma  Lu and Foster has   running time
that linearly scales with   and  gap  where
        is the condition number of matrix   in GenEV 
or of matrices   cid       cid   in CCA  and
  gap       is the eigengap      

in GenEV  or

  

     

  

in CCA 

These parameters are usually not constants and scale with

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

the problem size 

Challenge   Acceleration

 

For many easier scienti   computing problems  we are able
to design algorithms that have accelerated dependencies on
  and  gap  As two concrete examples  kPCA can be
gap as opposed
solved with   running time linearly in  
to  gap  Golub   Van Loan    computing    
  as
for   vector   can be solved in time linearly in
opposed to   where   is the condition number of matrix
   Shewchuk    Axelsson    Nesterov   
Therefore  can we obtain doublyaccelerated methods for
kGenEV and kCCA  meaning that the running times lingap  Before this paper 
early scale with both
for the general case       the method of Ge et al   Ge
et al    made acceleration possible for parameter  
but not for parameter  gap  see Table  

  and  

 

 

 

Challenge   GapFreeness

Since gap can be even zero in the extreme case  can we
design algorithms that do not scale with  gap  Recall
that this is possible for the easier task of kPCA  The block
 
Krylov method  Musco   Musco    runs in time linear
gap  where   is the approximain  
tion ratio  There is no gapfree result previously known for
kGenEV or kCCA even for      

  as opposed to  

 

Challenge   Stochasticity

 

 

 min   

  for   being the condition number of   

where  cid    maxi   cid Xi cid 

For matrixrelated problems  one can usually obtain
stochastic running times which requires some notations to
describe 
Consider   simple task of computing     for some vector    where accelerated methods solve it in time linearly
in
If    
    cid   is given in the form of   covariance matrix where
 
    Rn    then  accelerated  stochastic methods compute
 
 

    in   time linearly in    cid cid    instead of
   cid    cid  and Xi is the ith
row of     See Lemma   Since    cid cid       

 
stochastic methods are no slower than nonstochastic ones 
So  can we obtain   similar but doublyaccelerated
stochastic method for kCCA  Note that  if the doublyaccelerated requirement is dropped  this task is easier and
indeed possible  see Ge et al   Ge et al    However 
since their stochastic method is not doublyaccelerated  in
certain parameter regimes  it runs even slower than nonstochastic ones  even for       see Table  
Remark 
running time 
  Accelerated results are usually better because they are
  Note that   similar problem can be also asked for kGenEV
when   and   are both given in their covariance matrix forms 
We refrain from doing it in this paper for notational simplicity 

In general  if designed properly  for worst case

no slower than nonaccelerated ones in the worstcase 
  Gapfree results are better because they imply gap 

dependent ones 

  Stochastic results are usually better because they are no

slower than nonstochastic ones in the worstcase 

  Our Main Results
We provide algorithms LazyEV and LazyCCA that are
doublyaccelerated  gapfree  and stochastic 
For the general kGenEV problem  our LazyEV can be implemented to run in time 
 

knnz         

 

 cid  knnz   
 cid  knnz   

 

gap
 
 

 cid  
 cid  

 

 

 

 

 

gap
 

 

knnz         

 cid 
 cid 

or

 

 
  our algorithm LazyEV is doublyaccelerated 

in the gapdependent and gapfree cases respectively  Since
our running time only linearly depends on
gap
 resp 
For the general kCCA problem  our LazyCCA can be implemented to run in time

  and

 

 cid  knnz         cid   cid cid   cid       
 cid  knnz         cid   cid cid   cid       

 

 cid 
 cid 

or

 cid  
 cid  

gap
 

 

in the gapdependent and gapfree cases respectively 
Here  nnz          nnz      nnz     and  cid   
  maxi cid Xi cid cid Yi cid 
 min diag Sxx Syy  where Xi or Yi is the ith row vector
of   or     Therefore  our algorithm LazyCCA is doublyaccelerated and stochastic 
We fully compare our results with prior work in Table  
 for       and Table    for       and summarize our
main contributions 
  For       we outperform all relevant prior works  see
Table   Moreover  no known method was doublyaccelerated even in the nonstochastic setting 

  For       we obtain the  rst gapfree running time 
  Even for       we outperform most of the stateof 

thearts  see Table  

Note that for CCA with       previous result CCALin
only outputs the subspace spanned by the top   correlation
vectors but does not identify which vector gives the highest
correlation and so on  Our LazyCCA provides pervector

 If   method depends on   then one can choose     gap

and this translates to   gapdependent running time 

our kGenEV result in nonstochastic running time 

 Recalling Footnote   for notational simplicity  we only state

 Throughout the paper  we use the  cid   notation to hide poly 

logarithmic factors with respect to    gap          We use
nnz     to denote the time needed to multiply   to   vector 

Problem

Paper

kGenEV

GenELin  Ge et al   

LazyEV Theorem  
LazyEV Theorem  

Problem

Paper

kCCA

AppGrad  Ma et al   
CCALin  Ge et al   

CCALin  Ge et al   

LazyCCA  arXiv version 

LazyCCA  arXiv version 

LazyCCA  arXiv version 
LazyCCA  arXiv version 

 cid 
 cid 
 cid 

gap
 

  knnz      
  knnz      
  knnz      

gap
 

 

  

  

 cid 

 
 

gap
 

gap
 
 
 

Running time

Running time
  

 cid   cid  knnz   
 cid   cid  knnz   
 cid   cid  knnz   
 cid   cid  knnz         
 cid   cid  knnz      
 cid   cid  knnz      cid 
 cid   cid  knnz      cid 
 cid   cid  knnz      cid 
 cid   cid knnz         cid   
 cid   cid knnz         cid   

 
 cid  
 
 cid  
 
 cid  

 
gap
 
 

gap
 
 

    

 cid 

gap

gap

 

 cid 
 cid 
 cid 

 

 

  for outperformed 
 
 local conv 
 
 

 cid 
 cid 
 cid 

    

    

    

 cid cid 
 cid cid 

 

 cid 

 

gap   nnz      kd 
   nnz      kd 

 cid 

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

  for outperformed 
 

gapfree 

negative EV 

no
no
yes

no
yes
yes

gapfree 

stochastic 

no
no

no

no

yes

no

yes

no
no

yes

yes

yes

doubly

doubly

Table   Performance comparison on kGenEV and kCCA 

  

      and       max   
 min       
           max diag Sxx Syy 

In GenEV  gap        
In CCA  gap        
Remark   Stochastic methods depend on   modi ed condition number  cid  The reason  cid          is in Fact  

Remark   All nonstochastic CCA methods in this table have been outperformed because    cid cid       

 min diag Sxx Syy       cid      maxi cid Xi cid cid Yi cid 

 min diag Sxx Syy          and         

  

Remark   Doublystochastic methods are not necessarily interesting  We discuss them in Section  

 
 cid nc 

guarantees on all the top   correlation vectors 
  Our Side Results on DoublyStochastic Methods
Recall that when considering acceleration  there are two
parameters   and  gap  One can also design stochastic methods with respect to both parameters   and  gap 
meaning that

gap

 cid  
gap

with   running time proportional to    

 stochastic  or

 
 
 
 
instead of  
gap  nonstochastic 
The constant   is usually   We call such methods
doublystochastic 
Unfortunately  doublystochastic methods are usually
slower than stochastic ones  Take  CCA as an example 
The best stochastic running time  obtained exclusively by

us  for  CCA is nnz           cid   cid   
nnz           cid   cid   

 cid  In contrast 
 cid  Therefore  for  CCA 

if one uses   doublystochastic method  either  Wang
et al    or our LazyCCA  the running time becomes

 
 cid   
 
gap 

 cid  
gap

 
 

doublystochastic methods are faster than stochastic ones

only when

        

 cid 
 

The above condition is usually not satis ed  For instance 
   cid  is usually around   for most interesting datasets  cf 

the experiments of  ShalevShwartz   Zhang   
   cid  is between    and    in all the CCA experi 

ments of  Wang et al    and

  by Fact   it satis es  cid      so  cid  cannot be smaller
than      unless    cid     Even worse  parameter
        is usually much smaller than   Note that  
is scaling invariant  even if one scales   and   up by
the same factor    remains unchanged 

Nevertheless  to compare our LazyCCA with all relevant
prior works  we obtain doublystochastic running times for
kCCA as well  Our running time matches that of  Wang
et al    when       and no doublystochastic running
time for       was known before our work 

  Other Related Works
For the easier task of PCA and SVD  the  rst gapfree
result was obtained by Musco and Musco  Musco  
Musco    the  rst stochastic result was obtained by
Shamir  Shamir    and the  rst accelerated stochastic result was obtained by Garber et al   Garber   Hazan 
  Garber et al    The shiftand invert preconditioning technique of Garber et al  is also used in this paper 
For another
related problem PCR  principle compo 
 Note that item    cid      may not hold in the more general

setting of CCA  see Remark   

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

  for outperformed 
 

 cid 
 cid 
 cid 

 

 

gap

gap

 

  

 
gap

Running time

  nnz   
gap
 
gap   nnz   
 
gap
 
   
  nnz   

Running time
  

 cid   cid  nnz   
 cid   cid  nnz   
 cid   cid  nnz   
 cid 
nnz           cid   cid   
 cid 
nnz           cid   cid   
nnz           cid   cid   
 cid 
nnz           cid   cid   
nnz           cid   cid   
nnz           cid   cid   
nnz           cid   cid   
nnz           cid   cid   
 cid 
nnz           cid  
nnz           cid   cid   
nnz           cid   cid   

  for outperformed 
 
 
 
 
 
 

 cid  

 cid 

gap

 cid  

 cid 
 cid 
 cid 
 cid 

 
gap 
 
 
gap 
 
 
gap 
 cid  
 
 
gap
 cid   
 
 
 cid   
 
gap 

   

 see Remark  

 cid 
 cid 
 cid 

 
 cid   
 
gap 
 
 cid   
 
 

Problem

Paper

 GenEV

GenELin  Ge et al   

LazyEV Theorem  
LazyEV Theorem  

Problem

Paper

 CCA

AppGrad  Ma et al   

CCALin  Ge et al   

ALS  Wang et al   

SI  Wang et al   

CCALin  Ge et al   

ALS  Wang et al   
LazyCCA  arXiv version 
LazyCCA  arXiv version 

SI  Wang et al   

LazyCCA  arXiv version 
LazyCCA  arXiv version 

gapfree 

negative EV 

no

no

yes

no

yes

yes

gapfree 

stochastic 

no

no

no

no

no

no

no

yes

no

no

yes

no

no

no

no

yes

yes

yes

yes

doubly

doubly

doubly

Table   Performance comparison on  GenEV and  CCA 

In GenEV  gap    
In CCA  gap    

 

      and       max   
 min       
           max diag Sxx Syy 

 

 min diag Sxx Syy       cid      maxi cid Xi cid cid Yi cid 

 min diag Sxx Syy          and        

Remark   Stochastic methods depend on modi ed condition number  cid  the reason  cid          is in Def   

Remark   All nonstochastic CCA methods in this table have been outperformed because    cid cid       

Remark   Doublystochastic methods are not necessarily interesting  We discuss them in Section  
Remark   Some CCA methods have   running time dependency on         and this is intrinsic and cannot be removed 
In particular  if we scale the data matrix   and     the value   stays the same 
Remark   The only  nondoubly stochastic  doublyaccelerated method before our work is SI  Wang et al     for  CCA

only  Our LazyEV is faster than theirs by   factor  cid   cid   cid  Here    cid      and       are two

scalinginvariant quantities usually much greater than  

nent regression  we recently obtained an accelerated
method  AllenZhu   Li    as opposed the previously
nonaccelerated one  Frostig et al    however  the acceleration techniques there are not relevant to this paper 
For GenEV and CCA  many scalable algorithms have been
designed recently  Ma et al    Wang   Livescu   
Michaeli et al    Witten et al    Lu   Foster 
  However  as summarized by the authors of CCALin 
these cited methods are more or less heuristics and do not
have provable guarantees  Furthermore  for       the
AppGrad method  Ma et al    only provides local convergence guarantees and thus requires   warmstart whose

computational complexity is not discussed in their paper 
Finally  our algorithms on GenEV and CCA are based on
 nding vectors oneby one  which is advantageous in practice because one does not need   to be known and can stop
the algorithm whenever the eigenvalues  or correlation values  are too small  Known approaches for       cases
 such as GenELin  CCALin  AppGrad   nd all   vectors at
once  therefore requiring   to be known beforehand  As  
separate note  these known approaches do not need the user
to know the desired accuracy   priori but our LazyEV and
LazyCCA algorithms do 

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

  Preliminaries
We denote by  cid   cid  or  cid   cid  the Euclidean norm of vector
   We denote by  cid   cid   cid   cid     and  cid   cid Sq respectively the
spectral  Frobenius  and Schatten qnorm of matrix    for
      We write    cid    if      are symmetric and     
is positive semide nite  PSD  and write    cid    if     
are symmetric but       is positive de nite  PD  We denote by  max     and  min     the largest and smallest
eigenvalue of   symmetric matrix    and by    the condition number  max    min     of   PSD matrix   
Throughout this paper  we use nnz     to denote the time
to multiply matrix   to any arbitrary vector  For two matrices        we denote by nnz          nnz   nnz    
and by Xi or Yi the ith row vector of   or     We
also use poly               xt  to represent   quantity that
is asymptotically at most polynomial in terms of variables            xt  Given   column orthonormal matrix
    Rn    we denote by      Rn      the column
orthonormal matrix consisting of an arbitrary basis in the
space orthogonal to the span of     columns 
Given   PSD matrix   and   vector      cid Bv is the Bsemi 
norm of    Two vectors      are Borthogonal if   cid Bw  
  We denote by    the MoorePenrose pseudoinverse
of   if   is not invertible  and by    the matrix square
root of    satisfying     cid    All occurrences of   
   and    are for analysis purpose only  Our  nal
algorithms only require multiplications of   to vectors 

De nition    GenEV  Given symmetric matrices
       Rd   where   is positive de nite  The generalized eigenvectors of   with respect to   are            vd 
where each vi is
vi   arg max
  Rd

 cid cid cid   cid Av cid cid         cid Bv    

  cid Bvj                

 cid 

The generalized eigenvalues              satisfy     
  cid 
  Avi which can be negative 

Following  Wang et al    Garber   Hazan    we
assume without loss of generality that         
De nition    CCA  Given     Rn dx       Rn dy 
     cid    
letting Sxx    
the canonicalcorrelation vectors are         
where     min dx  dy  and for all        

    cid    Sxy    

    cid     Syy    

  

 cid 

          arg max
 Rdx  Rdy

 cid   cid Sxx         cid Sxx                  

 cid Sxy  such that

 cid Syy         cid Syy                  

 cid cid 

The corresponding canonicalcorrelation coef cients
             satisfy       cid 

  Sxy        

 cid  Sxx  

We emphasize that    always lies in     and is scalinginvariant  When dealing with   CCA problem  we also denote by     dx   dy 
Lemma    CCA to GenEV  Given   CCA problem with
matrices     Rn dx       Rn dy  let the canonicalcorrelation vectors and coef cients be             
where     min dx  dy  De ne    
   
spect to   has    eigenvalues     
ing generalized eigenvectors
 
maining dx   dy      eigenvalues are zeros 
De nition  
Lemma   We de ne condition numbers

  Then  the GenEV problem of   with rei  and correspond 
  The re 

 cid    Sxy
 cid cid  
 cid    

In CCA  let   and   be as de ned in

 cid cid    

  
and

 cid 

 cid 

 cid 

  Syy

  cid 

  

  

  

xy

 

 min   

  def        max   

 min    and  cid  def    maxi cid Xi cid cid Yi cid 
 
Fact    cid          and  cid        See full version 
Lemma   Given matrices     Rn dx       Rn dy 
let   and   be as de ned in Lemma   For every    
Rd  the Katyusha method  AllenZhu     nds   vector
  cid    Rd satisfying  cid   cid      Aw cid      in time
 cid   cid 

nnz         cid   cid cid   cid    log

 cid 

 cid 

 

 

 

  Leading Eigenvector via TwoSided

Shiftand Invert

We introduce AppxPCA  the multiplicative approximation
algorithm for computing the twosided leading eigenvector
of   symmetric matrix  AppxPCA  uses the shiftand invert
framework  Garber   Hazan    Garber et al   
and shall become our building block for the LazyEV and
LazyCCA algorithms in the subsequent sections 
Our pseudocode Algorithm   is   modi cation of Algorithm   in  Garber   Hazan    and reduces the eigenvector problem to oracle calls to an arbitrary matrix inversion oracle    The main differences between AppxPCA 
and  Garber   Hazan    are twofold 
First  given   symmetric matrix    AppxPCA  simultaneously considers an upperbounding shift together with  
lowerbounding shift  and try to perform power methods
with respect to          and          This allows us to determine approximately how close   is to the
largest and the smallest eigenvalues of    and decrease  
accordingly  In the end  AppxPCA  outputs an approximate
eigenvector of   that corresponds to   negative eigenvalue
if needed  Second  we provide   multiplicativeerror guarantee rather than additive as appeared in  Garber   Hazan 
  Without such guarantee  our  nal running time will
depend on

gap max     rather than  

gap 

 

 This is why the SI method of  Wang et al    also uses

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

  

 

  

   

Algorithm   AppxPCA             
Input     an approximate matrix inversion method      Rd      symmetric matrix satisfying     cid     cid        
 cid   cid    is   random unit vector  see Def   
      multiplicative error            numerical accuracy parameter  and         the con dence parameter 
 cid    is the parameter of RanInit  see Def   

   cid      RanInit                   
 cid cid      cid  log cid     
      cid  log cid     
 cid cid 
 cid   
 cid   
 cid    and cid     
 cid   
   cid     
 cid cid   cid 
Apply   to  nd  cid wt satisfying cid cid cid wt             cid wt 
wa    cid wm cid cid wm cid 
 cid  wa is roughly             cid    then normalized
 cid cid   cid 
Apply   to  nd va satisfying cid cid va             wa
Apply   to  nd  cid wt satisfying cid cid cid wt             cid wt 
 cid cid   cid 
wb    cid wm cid cid wm cid 
 cid  wb is roughly             cid    then normalized
Apply   to  nd vb satisfying cid cid vb             wb
 cid cid   cid 

 cid         PM       and        PM       see Lemma   

         
for       to    do

for       to    do

   

 

max   cid 

  va   cid 

 

  vb cid 

and                
   

   
       

  repeat
 
 
 
 
 
 
 
 
 
 
  until          
        
  if the last   cid 
 
 
 
  else
 
 
 
  end if

 

  vb then

  va     cid 
for       to    do

Apply   to  nd  cid wt satisfying cid cid cid wt              cid wt 
return      where   def   cid wm   cid cid wm cid 
Apply   to  nd  cid wt satisfying cid cid cid wt              cid wt 
return      where   def   cid wm cid cid wm cid 

for       to    do

 cid cid   cid 
 cid cid   cid 

 cid 

 cid   cid 

We prove in full version the following theorem 
Theorem    AppxPCA  informal  Let     Rd   be  
symmetric matrix with eigenvalues                 
  and eigenvectors            ud  Let     max   
With probability at least        AppxPCA  produces   pair
 sgn     satisfying
  if sgn     then   is an approx  positive eigenvector 
   cid ui     

  cid      cid 
  cid        cid 
The number of oracle calls to   is  cid   log  and each

  if sgn     then   is an approx  negative eigenvector 

 cid   cid 

   

   

   
 

   
 

 cid 

    

    

   cid ui     

time we call   it satis es
shiftand invert but depends on

 

gap 

in Table  

   max        
 

 min            max        
 min          

 min              
 min            
   

 

 

    and

We remark here that  unlike the original shiftand invert
method which chooses   random  Gaussian  unit vector in
Line   of AppxPCA  we have allowed this initial vector to
be generated from an arbitrary  conditioned random vector generator  for later use  de ned as follows 
De nition  
An algorithm RanInit    is    
conditioned random vector generator if     RanInit   
is   ddimensional unit vector and  for every        
every unit vector     Rd  with probability at least        it
satis es    cid        
    
This modi cation is needed in order to obtain our ef cient
implementations of GenEV and CCA  One can construct  
 conditioned random vector generator as follows 
Proposition   Given   PSD matrix     Rd    if we set
RanInit    def     
   cid Bv  where   is   random Gaussian
vector  then RanInit    is    conditioned random vector

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

generator for        
  LazyEV  Generalized Eigendecomposition
In this section  we construct an algorithm LazyEV that 
given symmetric matrix     Rd    computes approximately the   leading eigenvectors of   that have the largest
absolute eigenvalues  Then  for the original kGenEV
problem  we set       AB  and run LazyEV 
This is our plan to  nd the top   leading generalized eigenvectors of   with respect to   
Our algorithm LazyEV is formally stated in Algorithm  
The algorithm applies   times AppxPCA  each time computing an approximate leading eigenvector of   with  
multiplicative error   and projects the matrix   into
the orthogonal space with respect to the obtained leading
eigenvector  We state our main approximation theorem below 
Theorem    informal  Let     Rd   be   symmetric
matrix with eigenvalues                    and corresponding eigenvectors            ud  and            
If  pca is suf ciently small  LazyEV outputs    column  orthonormal matrix Vk               vk    Rd   which  with
probability at least        satis es 
    cid      where      uj          ud  and   is the
     cid    cid 
   
    For every                  cid 
Above  property     ensures the   columns of Vk have negligible correlation with the eigenvectors of   whose absolute eigenvalues are           property     ensures
    vi are all correct up to    
the Rayleigh quotients   cid 
error  We in fact have shown two more useful properties in
the full version that may be of independent interest 
The next theorem states that  if       AB  our
LazyEV can be implemented without the necessity to compute    or   
Theorem    running time  Let        Rd   be two
symmetric matrices satisfying    cid    and     cid     cid    
Suppose       AB  and RanInit    is de ned
in Proposition   with respect to    Then  the computation of       LazyEV             pca     can be
implemented to run in time

smallest index satisfying              

    vi     

 

 

 cid 

    to   vector  or

 cid  knnz         
   cid  
 cid 
 cid   
   cid  
 Meaning  pca     cid poly   

dient to multiply     to   vector 
 
       

   nnz   knnz      

 

 

 

if we use Conjugate gra 

   cid  The complete

where   is the time to multiply

speci cations of  pca is included in the full version  Since our  nal
running time only depends on log pca  we have not attempted
to improve the constants in this polynomial dependency 

Choosing parameter   as either gap or   our two main
theorems above immediately imply the following results
for the kGenEV problem   proved in full version 
Theorem    gapdependent GenEV  informal 
Let
       Rd   be two symmetric matrices satisfying    cid 
  and     cid     cid     Suppose the generalized eigenvalue and eigenvector pairs of   with respect to   are
   and it satis es                
    ui  
Then  LazyEV outputs       Rd   satisfying
 cid 

 Bnnz      knnz         

 cid 
  BW cid     

 cid 
  BV      

and  cid  

 cid   

 

 

in time

 cid  

 

gap

Here       uk          ud  and gap  

     

   

 

Theorem    gapfree GenEV  informal 
In the same
setting as Theorem   our LazyEV outputs      
            vk    Rd   satisfying  

 cid 
  BV       and

 cid cid   cid 

          cid cid   cid 
 cid   
 cid  

in time

  Avs
 

 cid 

   
     

       

 Bnnz      knnz         

 

 

 cid 

 

Ideas Behind Theorem  

Ideas Behind Theorems   and  

 
In Section   we discuss how to ensure accuracy  that is 
why does LazyEV guarantee to approximately  nd the top
eigenvectors of    In the full version of this paper  we also
discuss how to implement LazyEV without compute   
explicitly  thus proving Theorem  
 
Our approximation guarantee in Theorem   is   natural generalization of the recent work on fast iterative
methods to  nd the top   eigenvectors of   PSD matrix    AllenZhu   Li    That method is called
LazySVD and we summarize it as follows 
At   high level  LazySVD  nds the top   eigenvectors oneby one and approximately  Starting with         in the
sth iteration where         LazySVD computes approximately the leading eigenvector of matrix Ms  and call it
vs  Then  LazySVD projects Ms        vsv cid 
   Ms    
vsv cid 
While the algorithmic idea of LazySVD is simple  the analysis requires some careful linear algebraic lemmas  Most
notably  if vs is an approximate leading eigenvector of
Ms  then one needs to prove that the small eigenvectors
of Ms  somehow still  embed  into that of Ms after projection  This is achieved by   gapfree variant of the Wedin
theorem plus   few other technical lemmas  and we recommend interested readers to see the highlevel overview
section of  AllenZhu   Li   

    and proceeds to the next iteration 

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Algorithm   LazyEV             pca    
Input     an approximate matrix inversion method      Rd      matrix satisfying     cid     cid             the desired
rank            multiplicative error   pca         numerical accuracy  and           con dence parameter 
                
  for       to   do
 
 
 
 
  end for
  return Vk 

  is an approximate twosided leading eigenvector of Ms 
 cid  project   cid 
  to    
  
          VsV  cid 
   

     AppxPCA    Ms     pca      
    cid 

vs  cid     Vs    cid 

    cid 
Vs    Vs  vs 
Ms        vsv cid 

 cid cid cid     Vs    cid 

 cid  we also have Ms        VsV  cid 

    cid 

 

   Ms     vsv cid 
   

 cid cid 

 

 cid    cid 

In this paper  to relax the assumption that   is PSD  and to
 nd leading eigenvectors whose absolute eigenvalues are
large  we have to make several nontrivial changes  On
the algorithm side  LazyEV uses our twosided shiftand 
invert method in Section   to  nd the leading eigenvector
of Ms  with largest absolute eigenvalue  On the analysis
side  we have to make sure all lemmas properly deal with
negative eigenvalues  For instance 
  If we perform   projection   cid         vv cid       
vv cid  where   correlates by at most   with all eigenvectors of   whose absolute eigenvalues are smaller than  
threshold   then  after the projection  we need to prove
that these eigenvectors can be approximately  embedded  into the eigenspace spanned by all eigenvectors of
  cid  whose absolute eigenvalues are smaller than      
The approximation of this embedding should depend on
    and  

The full proof of Theorem   is in the arXiv version  It relies on   few matrix algebraic lemmas  including the aforementioned  embedding lemma 
  Conclusion
In this paper we propose new iterative methods to solve
the generalized eigenvector and the canonical correlation
analysis problems  Our methods  nd the most signi cant  
eigenvectors or correlation vectors  and have running times
that linearly scales with   
Most importantly  our methods are doublyaccelerated  the
running times have squareroot dependencies both with respect to the condition number of the matrix         and
with respect to the eigengap       gap  They are the  rst
doublyaccelerated iterative methods at least for      
They can also be made gapfree  and are the  rst gapfree
iterative methods even for  GenEV or  CCA 
Although this is   theory paper  we believe that if implemented carefully  our methods can outperform not only
previous iterative methods  such as GenELin  AppGrad 
CCALin  but also the commercial mathematics libraries
for sparse matrices of dimension more than     We

leave it   future work for such careful comparisons 
References
AllenZhu  Zeyuan  Katyusha  The First Direct Accelera 

tion of Stochastic Gradient Methods  In STOC   

AllenZhu  Zeyuan and Li  Yuanzhi  LazySVD  Even
Faster SVD Decomposition Yet Without Agonizing
Pain  In NIPS   

AllenZhu  Zeyuan and Li  Yuanzhi  Faster Principal Component Regression and Stable Matrix Chebyshev ApIn Proceedings of the  th International
proximation 
Conference on Machine Learning  ICML    

AllenZhu  Zeyuan and Orecchia  Lorenzo  Linear Coupling  An Ultimate Uni cation of Gradient and Mirror
Descent  In Proceedings of the  th Innovations in Theoretical Computer Science  ITCS    

AllenZhu  Zeyuan and Yuan  Yang 

Improved SVRG
for NonStrongly Convex or Sumof NonConvex Objectives  In ICML   

AllenZhu  Zeyuan  Richt arik  Peter  Qu  Zheng  and Yuan 
Yang  Even faster accelerated coordinate descent using
nonuniform sampling  In ICML   

Arora  Sanjeev  Rao  Satish  and Vazirani  Umesh    Expander  ows  geometric embeddings and graph partitioning  Journal of the ACM     

Aujol  JF and Dossal  Ch  Stability of overrelaxations
for the forwardbackward algorithm  application to  sta 
SIAM Journal on Optimization     

Axelsson  Owe    survey of preconditioned iterative methods for linear systems of algebraic equations  BIT Numerical Mathematics     

Chaudhuri  Kamalika  Kakade  Sham    Livescu  Karen 
and Sridharan  Karthik  Multiview clustering via canonical correlation analysis  In ICML  pp     

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

ShalevShwartz  Shai and Zhang  Tong  Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized
In Proceedings of the  st InterLoss Minimization 
national Conference on Machine Learning  ICML  
pp     

Shamir  Ohad    Stochastic PCA and SVD Algorithm with
an Exponential Convergence Rate  In ICML  pp   
   

Shewchuk  Jonathan Richard  An introduction to the conjugate gradient method without the agonizing pain   

Wang  Weiran and Livescu  Karen 

Largescale approximate kernel canonical correlation analysis  arXiv
preprint  abs   

Wang  Weiran  Wang  Jialei  Garber  Dan  and Srebro 
Nathan  Ef cient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis  In NIPS 
 

Witten  Daniela    Tibshirani  Robert  and Hastie  Trevor 
  penalized matrix decomposition  with applications to
sparse principal components and canonical correlation
analysis  Biostatistics  pp  kxp   

Dhillon  Paramveer  Foster  Dean    and Ungar  Lyle   
In

Multiview learning of word embeddings via cca 
NIPS  pp     

Frostig  Roy  Musco  Cameron  Musco  Christopher  and
Sidford  Aaron  Principal Component Projection Without Principal Component Analysis  In ICML   

Garber  Dan and Hazan  Elad  Fast and simple PCA via

convex optimization  ArXiv eprints  September  

Garber  Dan  Hazan  Elad  Jin  Chi  Kakade  Sham   
Musco  Cameron  Netrapalli  Praneeth  and Sidford 
Aaron  Robust shiftand invert preconditioning  Faster
and more sample ef cient algorithms for eigenvector
computation  In ICML   

Ge  Rong  Jin  Chi  Kakade  Sham    Netrapalli  Praneeth  and Sidford  Aaron  Ef cient Algorithms for
Largescale Generalized Eigenvector Computation and
Canonical Correlation Analysis  In ICML   

Golub  Gene    and Van Loan  Charles    Matrix ComISBN

putations  The JHU Press   th edition   
 

Kakade  Sham   and Foster  Dean    Multiview regression via canonical correlation analysis  In Learning theory  pp    Springer   

Karampatziakis  Nikos and Mineiro  Paul  Discriminative
features via generalized eigenvectors  In ICML  pp   
   

Lu  Yichao and Foster  Dean    Large scale canonical corIn NIPS 

relation analysis with iterative least squares 
pp     

Ma  Zhuang  Lu  Yichao  and Foster  Dean  Finding linear
structure in large datasets with scalable canonical correlation analysis  In ICML  pp     

Michaeli  Tomer  Wang  Weiran  and Livescu  Karen 
arXiv

Nonparametric canonical correlation analysis 
preprint  abs   

Musco  Cameron and Musco  Christopher  Randomized
block krylov methods for stronger and faster approximate singular value decomposition  In NIPS  pp   
   

Nesterov  Yurii    method of solving   convex programming problem with convergence rate      In Doklady AN SSSR  translated as Soviet Mathematics Doklady  volume   pp     

ShalevShwartz  Shai  SDCA without Duality  Regulariza 

tion  and Individual Convexity  In ICML   

