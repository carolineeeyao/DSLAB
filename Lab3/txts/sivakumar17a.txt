HighDimensional Structured Quantile Regression

Vidyashankar Sivakumar   Arindam Banerjee  

Abstract

Quantile regression aims at modeling the conditional median and quantiles of   response variable given certain predictor variables 
In this
work we consider the problem of linear quantile
regression in high dimensions where the number of predictor variables is much higher than
the number of samples available for parameter
estimation  We assume the true parameter to
have some structure characterized as having  
small value according to some atomic norm   
and consider the norm regularized quantile regression estimator  We characterize the sample complexity for consistent recovery and give
nonasymptotic bounds on the estimation error 
While this problem has been previously considered  our analysis reveals geometric and statistical characteristics of the problem not available in
prior literature  We perform experiments on synthetic data which support the theoretical results 

  Introduction

Considerable advances have been made over the past
decade on  tting highdimensional structured linear models when the number of samples   is much smaller than
the ambient dimensionality    Banerjee et al    Negahban et al    Chandrasekaran et al    Most
of the advances have been made for linear models  yi  
 cid xi   cid                         where     Rp is assumed
to be structured       sparse  group sparse  etc  Estimation of such structured   is usually done using Lassotype
regularized estimators  Negahban et al    Banerjee
et al    or Dantzigtype constrained estimators  Chandrasekaran et al    Chatterjee et al    other related estimators have also been explored  Hsu   Sabato 

 Department of Computer Science   Engineering  University
of Minnesota  Twin Cities  Correspondence to  Vidyashankar
Sivakumar  sivak umn edu  Arindam Banerjee  banerjee cs umn edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Vershynin    Such models have been extended
to generalized linear models  Banerjee et al    Negahban et al    matrix completion  Cand es   Recht 
  vector autoregressive models  Melnyk   Banerjee 
  among others 
In this paper  we consider the problem of structured quantile regression in highdimensions  which can be posed as
follows  given the response variable yi and covariates xi
the  th conditional quantile function of yi given xi is given
 cid          for some structured
by     
yi xi
 
  whose structure can be captured by   suitable atomic
norm           norm for sparsity       norm for group
  is the inverse of the conditional
sparsity  etc  Here    
yi xi
distribution function of yi given xi  We consider the following regularized estimator for the structured quantile regression problem 
     argmin Rp         nR 

 xi     cid xi   

  cid 

  

  argmin Rp

 
 

   yi    cid xi   cid     nR   

 
where                      is the asymmetric absolute deviation function  Koenker       is the indicator function  The goal is to get nonasymptotic bounds on
the estimation error  cid     cid 
Many previous papers analyze the asymptotic performance
of the estimator in    Li   Zhu    Kai et al   
Wu   Liu    Zou   Yuan    Wang et al    In
the nonasymptotic setting of interest in this paper  special
cases of the estimator in   have been studied in recent literature  Belloni   Chernozhukov    Kato    Fan
et al      primarily focusing on speci   norms like
the   norm and nonoverlapping group sparse norm 
In
contrast  our formulation and analysis is applicable to any
atomic norm    giving considerable  exibility in choosing   suitable structure for real world problems       hierarchical sparsity  ksupport norm  OWL norm etc  More
recently  Alquier et al    consider the more general
problem of norm regularized regression with Lipschitz loss
functions which includes   as   special case  They derive
similar results to ours for bounds on the estimation error 
but their analysis differs signi cantly and  in our opinion 
does not leverage and highlight the key geometric and sta 

HighDimensional Structured Quantile Regression

tistical characteristics of the problem 
In the setting of norm regularized regression with square
loss  including the widely studied Lasso estimator  Tibshirani    Negahban et al    Banerjee et al   
the sample complexity    of the estimator gets determined
by   certain restricted strong convexity  RSC  property
which simpli es to the restricted eigenvalue  RE  condition on the matrix        Bickel et al    Negahban
et al    in the noiseless setting       when       
the sample complexity determines   phase transition phenomenon so that the probability of recovering the structured   is minimal when        and one can exactly
recover   with high probability when        Our work
gives an equivalent sample complexity characterization for
structured quantile regression  which was not available in
prior work  The challenge in characterizing RSC in the
context of quantile regression stems partly from the nonsmoothness of the objective  so one has to work with subgradients  However  the unique aspect stems from the geometry of quantile regression  or as the authoritative book
on the topic puts it   How quantile regression works 
 Koenker   Section   In quantile regression  the
  samples get divided into three subsets    samples which
get exactly interpolated       yi    cid xi   cid         samples which lie below the curve       yi    cid xi   cid  and
             samples which lie above the curve      
yi    cid xi   cid  Note that when       all samples are interpolated  the loss is zero and the same   is   solution for
all quantiles   Quantile regression is clearly not working 
The Number of InterPolated Samples  NIPS    is an important quantity  inherent to structure in   and determines
the sample complexity of structured quantile regression  
In fact  we show that when       the RSC condition associated with the estimator in   is satis ed  When there
is no structure in   then          and quantile regression needs          samples to work  However  when  
has structure  such as sparsity or group sparsity    can be
substantially smaller than    Speci cally we show that  
is of the order of square of Gaussian width of the error set
 Talagrand    Chandrasekaran et al    for   class
of atomic norms which includes         group sparse  ksupport  Argyriou et al    and the OWL  Bogdan et al 
  norms  The Gaussian width as   measure of complexity of   set has been extensively used in prior literature
 Chandrasekaran et al    Tropp    Banerjee et al 
  For example  when   is sparse with   nonzero
entries  we show that     cs log   
When       and the RSC condition is satis ed  building on recent developments in highdimensional estimation  Negahban et al    Banerjee et al    we show
that choosing              where    is the
dual norm of    leads to nonasymptotic bounds on the
estimation error  cid      cid  While the speci cation of

   looks complex  with its dependency on the dual norm
and its dependency on   we show it is suf cient to set
   based on the Gaussian width  Talagrand    of the
unit norm ball for     Banerjee et al    Sivakumar
et al    Our analysis and results on the estimation
error bound for quantile regression  interestingly  has the
same order as that for regularized least squares regression
for general norms  Banerjee et al   
In contrast to
the least squares loss the quantile loss is more robust as
the estimation error is independent of the two norm of the
noise  We discuss results for the         group sparse and
ksupport norms as examples  precisely characterizing the
sample complexity for recovery and nonasymptotic error
bounds  Speci cally  our results for the   norm matches
those from existing literature on sparse quantile regression  Belloni   Chernozhukov   
The rest of the paper is organized as follows  In Section  
we discuss the problem formulation along with assumptions  review the general framework for analyzing regularized estimation problems and discuss the three atomic
norms used as examples throughout the paper 
In Section   we analyze the number of interpolated samples and
establish precise sample complexities for   class of atomic
norms in terms of the Gaussian widths of sets  In Section  
we establish key ingredients of the analysis and provide the
main bound  We present experimental results in Section  
and conclude in Section  

  Background and Preliminaries

Problem formulation  We outline the assumptions on the
data and estimator  Similar conditions are present in all
prior literature on quantile regression and we refer to Section   in Belloni   Chernozhukov   for examples
of data satisfying the conditions 
We consider data is generated as                Rn  
is the design matrix      Rp and     Rn is the noise  We
assume subGaussian design matrices     Rn   which includes the class of all bounded random variables  Note that
this is not   restrictive assumption as to avoid the quantile crossing phenomenon the covariate space has to be
bounded  Quantile crossing is when the value of the  th
quantile is greater than the  th quantile for some      
 See Section   in  Koenker    We do not make any
assumptions on the noise vector     Rn  More speci cally
the noise can be sampled from   heavy tailed distribution 
can be heteroscedastic as in the locationscale model where
      cid xi   cid            Rp     is noise independent of xi  bimodal and so on and so forth  Note that this setting is more
general than for the least squares loss 
We consider   parametric quantile regression model where
the  th conditional quantile function of the response vari 

HighDimensional Structured Quantile Regression

able yi given any xi   Rp is given by 

 

 cid   

 xi     cid xi   

    Rp           

   
yi xi
where    
is the inverse of the conditional distribution
yi xi
function of yi given xi  We will assume the conditional
density of yi evaluated at the conditional quantile  cid xi   
 cid 
is bounded away from zero uniformly for all   that is 
fyi xi cid xi   
 cid          for all   and all xi  The goal
is to estimate parameter   close to  
  using   observations of the data when        The estimator in this paper
belongs to the family of regularized estimators and is of the
form 

 

      argmin Rp         nR   

 cid  
 
      yi cid xi   cid      is the quanwhere         
tile loss function and    is any atomic norm  Examples
of atomic norms we consider in this paper are the        
nonoverlapping group sparse norm and the ksupport norm 
We present all results assuming any single         and
going forward drop the subscripts from  
Gaussian Width  All results will be in terms of the Gaussian width of suitable sets  For any set     Rp  the Gaussian width of the set   is de ned as  Gordon    Chandrasekaran et al   

  and      

 cid 

 cid 

 cid      cid 

sup
   

       Eg

 

 

where the expectation is over         Ip    Gaussian widths have been widely used in prior literature on
structured estimation  Chandrasekaran et al    Banerjee et al    Sivakumar et al    Tropp   
Highdimensional estimation  Our analysis is built on
developments over the past decade for highdimensional
structured regression for linear and generalized linear models using both regularized as well as constrained estimators
 Candes   Tao    Bickel et al    Chandrasekaran
et al    Negahban et al    Banerjee et al   
For the regularized formulation considered in this work
Banerjee et al    Negahban et al    have established   generalized analysis framework when the loss is
least squares or more generally the maximum likelihood
estimator for generalized linear models  We give   brief
overview of the main components of the analysis 
  Regularization parameter  In Banerjee et al   
Negahban et al    the regularization parameter is assumed to satisfy the following assumption 
              

 
With                  denoting the unit norm ball 
Banerjee et al    prove that with high probability  

value             satis es the above condition for
subGaussian design matrices  noise and the least squares
loss 
  Error set  The assumption on the regularization parameter ensures that the error vector           lies in the
following error set  Banerjee et al   

   

               

 
 

  

 

 

 cid 

 cid 

  The norm compatibility constant  It is de ned as follows  Negahban et al    Banerjee et al   

      sup
   

    
 cid   cid 

 

 

  Restricted Strong Convexity  RSC  In Banerjee et al 
  Negahban et al    the loss function is shown to
satisfy the following RSC condition with high probability
once the number of samples is of the order of the square of
the Gaussian width of the error set  that is            
                    cid      cid 
             inf
   cid   cid   
 

inf

For the squared loss  the RSC condition simpli es to the
Restricted Eigenvalue  RE  condition  Bickel et al   

inf
   

 
 

 cid Xu cid 

     cid   cid 
   

 

  Recovery Bounds  When RSC and bounds on the
regularization parameter are satis ed Banerjee et al   
prove the following deterministic error bound 
       

 cid cid     cid     cid     

 

 

 

where   is any constant 
Atomic Norms  We consider the class of atomic norms
for the regularizer  Mathematically consider   set     Rp
the collection of atoms that is compact  centrally symmetric
about the origin  that is                  Let  cid cid  
denote the gauge of   
      cid cid     inf             tconv   

 

  inf cid 

   

 cid 

   

 
For example when      ei  
   yields  cid cid      cid cid 
Although the atomic set   may contain uncountably many
vectors  we assume   can be decomposed as   union of  
sets      Ai        Am similar to the setting considered

ca      

caa  ca            

HighDimensional Structured Quantile Regression

     nonoverlapping group sparse norm 

in Chen   Banerjee   Such   decomposition assumption is satis ed by many popular atomic norms like the   
     group sparse norms  ksupport norm  OWL norm etc 
Throughout the paper we will illustrate our results on the
following norms 
     norm  For the    norm we will consider that   is an
ssparse vector  that is   cid cid      
 
Let
              GNG  denote   collection of groups which are
blocks of any vector     Rp  Let  NG denote   vector with
NG      if     GNG   else   
coordinates   
NG     The max 
 Gi  The norm
imum size of any group is     max
  NG  
    cid   cid  Let SG               NG 
with cardinality  SG    sG  We consider the true parameter
    Rp is sGsparse  that is   
  ksupport norm  The ksupport norm can be expressed
as an in mum convolution given by  Argyriou et al   

is given as     cid NG

NG    cid NG   SG 
 cid 

 cid ui cid 

   cid ui cid     

     inf cid 
   cid   cid      cid   cid      and   is   union of  cid  
Clearly it is an atomic norm for which         
Rp
subsets  that is                         
   More results
on the ksupport norm can be looked up in Chatterjee et al 
  Chen   Banerjee   We consider the setting
where  cid cid      and        For the results we require
the Gaussian widths of the unit norm ball  error set and the
norm compatibility constants for the norms  We provide
them below for reference  All values are in order notation 
   
 
 
 
 
sG
 

    
 
  log  
 

 cid cid 

Norm

 
 

 

  ui 

 cid 

 

 

  cid       log cid   

lsG   sG log NG
  cid 

  cid    

 
    
 
 
log  
    log NG
  cid 

  cid       log cid   

 

  

    
ksp

 

      cid    Rp  We are

     cid   cid cid                 

with   suitable choice for the regularization parameter ensures that the error vector lies in   restricted subset of Rp 
            
now interested in characterizing     sup
   
yi    cid xi   cid     cid xi        cid         that is  the maximum
number of interpolated samples with the error restricted to
  particular subset of Rp  Again if        quantile regression is not working  Since there are no restrictions on the
number of nonzero elements in the error vector      rst
crude estimate will be     min      cid   cid  which implies quantile regression will not work unless we have  
minimum of   samples  But intuitively   should depend on
properties of the error set    which the initial crude estimate
is failing to take advantage of 
Below we state   result which reinforces the intuition of the
relation between   and the properties of the set    Specifically we show that for the types of atomic norms considered in this work  which includes all popularly known vector norms  the number of interpolated samples does not exceed the product of the square of the norm compatibility
constant and the square of the Gaussian width of the unit
norm ball  For the norms considered  this is precisely the
square of the Gaussian width of the error set    For example for the    norm for an ssparse vector this evaluates to
an upper bound of         log    While the result statement considers subGaussian design matrices  the result will
also hold for design matrices sampled from heavytailed
distributions using arguments similar to Lecu     Mendelson   Sivakumar et al   

Theorem   Consider   has isotropic subGaussian rows
and   is an ssparse vector that can be written as   linear
combination of   atoms from an atomic set of cardinality
  

   

ciai  ai      ci            

 

  cid 

  Number of InterPolated Samples  NIPS 

  

We begin with intuitions on the geometry of the problem 
In the high sample  low dimension setting        when
       the quantile loss is   linear program and hence
the solutions are at the vertices  that is  where any   of the
  samples are interpolated  Mathematically we de ne the
quantity          yi    cid xi   cid     cid xi        cid      Rp 
and note that     sup
In the high diu Rp
mensional setting considered in this paper       and
hence with        the number of interpolated samples is        From an optimization perspective there
are multiple such solutions and all solutions are optimal
for all quantile parameters   But practically quantile regression is not working  Now introducing   regularizer

          

       

 
 

For the regularized quantile regression problem penalized
with the atomic norm       cid cid   

  cid 
      cid  denote the

    arg min
 Rp

         arg min
 Rp

let      cid                   

 
error set  let            and let           
         where       sup
is the norm
   
in the error set       is the
compatibility constant
Gaussian width of the unit norm ball and    and   
are some constants  Then with probability atleast    
exp      log em    exp      the number of in 

 cid   cid   cid   cid 

  

HighDimensional Structured Quantile Regression

terpolated samples 

    sup
   

     yi    cid xi       cid                     
 

where   is   constant 

 

To understand the intuition consider the case of the    norm 
For an error vector lying in   particular sdimensional subspace the maximum number of interpolated samples is
     with high probability  Extending the argument to all
such sdimensional subspaces by   union bound argument

 cid  subspaces  the maximum number of interpolated

on the cid  

samples when the error vector is any ssparse vector in pdimensional space is     log    Finally the argument is
extended to all vectors in the error set using the powerful Maurey   empirical approximation argument previously
employed in Sivakumar et al    Lecu     Mendelson
  Rudelson   Zhou  
Suprisingly in prior literature on structured high dimensional quantile regression  the importance of   has not been
explicitly discussed  This intuition about the importance of
  also shows up in an elegant form in the analysis of the
RSC condition in Section  
Below we provide results for the number of interpolated
samples for the         nonoverlapping group sparse and
ksupport norms 
For the      nonoverlapping group
sparse norm and the ksupport norm we  rst illustrate that
they are atomic norms  The results then follow from substituting known values for the norm compatibility constant
and Gaussian width of unit norm ball for the different
norms  For computation of these quantities for any general
norm we refer the interested reader to work in Vershynin
  Chen   Banerjee  
Corollary   For the    norm with   being an ssparse
vector  when                log    then with high probability 

     yi    cid xi        cid    cs log    

 

    sup
   

for some constant   

Before applying the result to the nonoverlapping group
sparse norm note that for any vector     Rp 

NG cid 

 cid 

   

cij ij  

 

  

 

where  ij is any unit norm vector in subspace de ned by
the group    For any group    let    denote the vector constructed from   such that it has component           if
       Now by de nition of atomic norm cij    cid   cid  for   

in the same direction of  ij  otherwise cij     Therefore
the group sparse norm is an atomic norm with   hierarchical set structure with the number of elements     NG in
the outer set with each element of the outer set itself being
  set of an in nite number of elements with any one element chosen for   particular vector   that is  cij  cid    for
only one   amongst an in nite number of     

Corollary   Consider the      nonoverlapping groupsparse norm with        sGl     sG log NG  With high
probability 

    sup
   

     yi    cid xi        cid      lsG   sG log NG   
 

for some constant   

 cid 

the

For

inf cid 

  ui 

expressed as 

ksupport

norm

 
 cid ui cid       can be similarly

 

   cid ui cid 

 

 cid cid sp

  cid 

  

 cid 

  

 

   

cij ij

 

where  ij is   unit vector in kdimensional subspace    The
difference compared to the nonoverlapping group sparse
norm is that many of the cij   can now be non zero in
the inner sum  This is comparably more complex than the
groupsparse norm where the inner set becomes   singleton
for some   but in terms of the analysis nothing changes 

Corollary   Consider the ksupport norm with          
    log cid   

  cid  With high probability 
     yi    cid xi        cid            log cid     cid   
 

    sup
   

for some constant   

  Structured Quantile Regression

In this section  we present results for the key components
in the general analysis framework of Banerjee et al   
which we brie   described in Section   of the paper  We
start with results on the regularization parameter by analyzing equation   before establishing sample complexity
bounds when the restricted strong convexity condition in
equation   is satis ed  Finally an    bound on the error
is obtained using   We will consider subGaussian design matrices throughout  All results are in terms of Gaussian widths of sets and the norm compatibility constant 
Results for        nonoverlapping group sparse and ksupport norms are given for illustration purposes 

HighDimensional Structured Quantile Regression

  Regularization Parameter   

We analyze the bound in equation   In prior literature  
bound has been established speci cally for the    norm in
Belloni   Chernozhukov    See Theorem   Below
we consider the case of any general atomic norm and obtain   result in terms of the Gaussian width of the unit norm
ball  The analysis follows from   similar result for the regularization parameter in Banerjee et al    for the least
squares case 
Theorem   Let     Rn   be   design matrix with independent isotropic subGaussian rows with subGaussian
norm  xi 
    De ne                    the unit
 cid   cid      Then the following
norm ball and let     sup
 
holds

             

 

 

 cid cid 

 cid 
 cid   

where   is any  xed constant depending only on the subGaussian norm   Moreover with probability atleast    
   exp

    exp cid   cid 
 cid                 

  

          

 

 

 

 
where         are absolute constants 

 cid             

 

 

  major difference to the least squares loss setting  is the
independence of the regularization parameter to assumptions on the noise vector  see for example Theorem   and
Theorem   in Banerjee et al    where the noise is explicitly assumed to be subGaussian and homoscedastic and
the noise enters the analysis through properties of  cid cid 
This gives the  exibility of considering noise vectors which
are heavy tailed or heteroscedastic  Indeed the most interesting applications of quantile regression arise in such settings 
Below we provide bounds for the regularization parameter for different norms by substituting known values of the
Gaussian width for the unit norm balls  The result for the   
norm matches with Theorem   in Belloni   Chernozhukov
  for the regularization parameter 
Corollary   If    is the    norm  with high probability

          

log      

 

 

 cid         

 
 

 

Corollary   If    is the      nonoverlapping group
sparse norm  with high probability 

           cid         cid     log NG    

 

 

 

Corollary   If    is the ksupport norm  with high probability 

           cid         cid       log cid   

  cid     

 

 

 

  Restricted Strong Convexity  RSC 

The loss needs to satisfy the RSC condition in equation  
Prior literature on structured quantile regression has not
discussed the RSC condition explicitly  though Fan et al 
    considers it for the quantile huber loss function 
We start by providing an intuition for the RSC formulation
for the quantile loss  The RSC condition equation   on
the error set   evaluates to the following 

 cid   cid xi   cid 

  cid 

  

 

inf
   

 
 

   yi    cid xi   

 cid           yi    cid xi   

 cid      dz  

      sup
   

 
   yi    cid xi        cid  is the numLet     sup
   
ber of interpolated samples  For any       if the model
can interpolate all points  that is        then   evaluates to zero 
In general  as shown in Section     gets
determined by the structure  For example for the    norm
        log    rather than the ambient dimensionality   
Thus  the sum over   points in   simply reduces to the
sum over the       points which are not interpolated  and
will ensure the RSC condition when       The intuition
of the NIPS property of Section   thus shows up elegantly
in the RSC condition 
In equation  

let      yi    cid xi   cid  vi  
consider

                   

and

 cid   cid xi   cid 
 cid  
  cid 

 
 
 

 
 

  

     vi 

  vi   

 

 
 

 
 

 cid   cid xi   cid 
  cid 
 cid   cid xi   cid 
  cid 
  cid 

  

  

 

 

 Fi          Fi 

fi   zdz     

 

 
  
   
  

fi   cid xi    cid      
       cid   cid 

 

 

  

 cid Xu cid 

 

The  rst line follows from the de nition of the cumulative distribution function  the second line by   simple Taylor series expansion  the last line by the assumption that
      where   is the ref   fi     and    cid Xu cid 
stricted eigenvalue  RE  constant  The RE condition is satis ed as the sample complexity bounds for satisfying the
 cid  
NIPS property is same as the RE condition  More generally
RSC is   condition on the minimum eigenvalue of the Jacoi  fi   cid xi    cid  restricted to the error set
bian matrix  
 

HighDimensional Structured Quantile Regression

 cid          and    The cid          term is minimized at

The two norm of the error depends on the two terms

the tails and hence has the effect of reducing the estimation
error  But typically this is dominated by the lower bound
on the density   term which makes the estimate less precise
in regions of low density  This is to be expected as there
are very few samples to make   very precise estimate in
low density regions  While similar observations are made
in page   of Koenker   the results are asymptotic
while we show nonasymptotic recovery bounds  Another
aspect we reiterate here is the independence of the results
from the form of the noise  All results make no assumptions on the noise apart from an assumption on the lower
bound of the noise density 
Below we provide recovery bounds for the different norms
we consider in the paper 

 

Corollary   For
  

log  

 

     
 

 

bility

the

 
and         log   with high proba 

norm when

  

  

 

 cid cid     

 
  log  
 
   

 

 

 

   This quantity has also been considered in prior literature
 see Section   in Koenker   and the proof in page
  also see condition    in Belloni   Chernozhukov
  While the above analysis is in expectation of the
quantity vi  we state the following result giving large deviation bounds for the above quantity 
Theorem   Consider     Rn   has subGaussian rows 
Let         fi cid xi   cid  be   uniform lower bound on the
conditional density for all xi in the support of    Let   de 
  Let the
note the RE constant satisfying  
number of samples     cw    where      is the Gaussian width of the error set   and   is some constant  Then
with probability atleast   exp    exp
    
   
where         and   are constants 

 cid   

     cid   cid 

  cid Xu cid 

 cid 

                   cid   cid 
   

inf

 

where        is   constant 

Below we provide results for different norms  The sample
complexity for the    norm matches the result in Belloni  
Chernozhukov    see equation  

Corollary   For the    norm with     cs log   with high
probability the following RSC condition is satis ed 

                   cid   cid 
   

inf

 

Corollary   For the      nonoverlapping group sparse
norm with       lsG   sG log NG  with high probability
the following RSC condition is satis ed 
                  cid   cid 
   

 

inf

Corollary   For the ksupport norm with          
  log cid   
  cid  with high probability the following RSC condition is satis ed 

                   cid   cid 
   

inf

 

  Recovery Bounds

Following the general framework outlined in Banerjee et al 
   see Theorem   we state the following high probability bound on the two norm of the error vector      
Theorem   For the quantile regression problem  when
       
             for some constants
      then with high probability 

         

 
 

 

 cid               

 
where     is the norm compatibility constant in the error
set 

   

 

 cid cid     

Corollary   For the      nonoverlapping group sparse
norm when     
and       lsG  
sG log NG  with high probability

 
     
 

  log NG

 

  

 

 cid lsG   sG log NG

 
   

 

 cid cid     

Corollary   For the ksupport norm when     
  cid  with high
  

and     cs     log cid   

    log cid   
   cid 
 

 

 
     
 
probability

 cid       log cid   

 
   

 

  cid 

 cid cid     

  Experiments

We perform simulations with synthetic data 

 cid 

 cid 
 cid 
 cid   cid cid   cid 

 cid cid 
 cid 
 cid   cid cid   cid 

  

 cid cid 

 

  Phase Transition
Data is generated as           
           

   
    Rp for the    norm and

               

 cid   cid cid   cid 

 cid   cid cid   cid 

             

             

               
                     
 
for the      group sparse norm with          
The noise                   is Gaussian with
zero mean and   variance  The design matrix    

             

 

 

 

 

 

 cid   cid cid   cid 

HighDimensional Structured Quantile Regression

documentation Li et al    The code was implemented
in Python  The plots in Figure   clearly show   phase transition for both the    and      group sparse norms for all
quantiles exemplifying the NIPS property described earlier 

  Robustness

We showcase the robustness enjoyed by quantile regression over ordinary least squares estimation against heavytailed noise and outliers  We consider the    norm with
    Rp  For
                          

               

 cid 

 cid cid 

 

 cid 

 cid 

 cid cid 

 

 cid 

heavytailed noise we consider the student tdistribution
with different degrees of freedom  with lower degrees of
freedom corresponding to heavier tailed data  To show the
robustness to outliers we randomly pick   certain percentage of samples from the dataset and multiply the noise
by   that is              for   certain proportion
of the dataset  We vary the proportion of contamination
from   to   We          for this simulation 
Again for both exercises  we run   simulations and plot
the mean and standard deviation of the estimation error
 cid     cid  The plots in Figure   show   the estimation
error against varying degrees of freedom of the student tdistribution and   estimation error against the percent contamination  The observations are in agreement with conventional wisdom on robustness of the quantile regression
estimator to heavytailed noise and outliers 

  Conclusions

The paper presents   general framework for the analysis
of nonasymptotic error and structured recovery for norm
regularized quantile regression for any atomic norm  Our
results are based on extending the general analysis framework outlined in Banerjee et al    Negahban et al 
  using insights from the geometry of the problem  In
particular we introduce the Number of InterPolated Samples  NIPS  as critical for determining the sample complexity for consistent recovery  We prove that once the number
of samples crosses the NIPS threshold  we start recovering the true parameter  This phase transition phenomena
for norm regularized quantile regression problems has not
been discussed in prior literature  We also prove that NIPS
is of the order of square of the Gaussian width of the error set for many atomic norms   which is the same order as
that for regularized least squares regression and match results from previous work for the    norm  Belloni   Chernozhukov   
Acknowledgements  We thank reviewers for their valuable comments  This work was supported by NSF grants
IIS 
IIS 
CCF  CNS  IIS  IIS 
NASA grant NNX AQ   

IIS 

IIS 

Figure   Probability of recovering true parameter versus the
rescaled sample size for    norm  top  and      group sparse
norm  bottom  There is   sharp phase transition when the number
of samples exceeds NIPS

Figure   Estimation error of Lasso and   penalized quantile
regression against different degrees of freedom of the student
tdistribution noise  top  and against percentage contamination
 bottom  Quantile regression is robust to heavytailed noise and
outliers

    Ip    is multivariate Gaussian with identity covariance  We vary                       For each
  we generate   datasets with the probability of success
de ned as the fraction of times we are able to faithfully estimate the true parameter  For       we run simulations
for           and for         we run simulations only for       For the optimization  we use the
Alternating Direction Method of Multipliers  Boyd et al 
  The details of the updates can be found in the  are

 Rescaled sample size     log   Probability of success  quantile    quantile    quantile    quantile    quantile   Rescaled sample size   msG sGNG Probability of success  quantile    quantile    quantile    quantile    quantile   Student tdistribution dof Estimation errorLassol  quantile regression Percent contamination Estimation errorLassol  quantile regressionHighDimensional Structured Quantile Regression

References

Alquier     Cottet     and Lecue     Estimation Bounds
and Sharp Oracle Inequalities of Regularized Procedures
with Lipschitz Loss Functions  arXiv   

Argyriou  Andreas  Foygel  Rina  and Srebro  Nathan 
Sparse Prediction with the kSupport Norm  In Neural
Information Processing Systems  NIPS  apr  

Banerjee  Arindam  Chen  Sheng  Fazayeli  Farideh  and
Sivakumar  Vidyashankar  Estimation with Norm Regularization  In Neural Information Processing Systems
 NIPS   

Belloni  Alexandre and Chernozhukov  Victor 

  
Penalized Quantile Regression in HighDimensional
Sparse Models  The Annals of Statistics   
 

Bickel  Peter    Ritov  Yaacov  and Tsybakov  Alexandre    Simultaneous analysis of Lasso and Dantzig selector  The Annals of Statistics     
ISSN  

Bogdan  Malgorzata  Berg  Ewout van den  Su  Weijie  and
Emmanuel  Candes  Statistical Estimation and Testing
via the Sorted    Norm  arXiv   

Boyd  Stephen  Parikh  Neal  Chu  Eric  Peleato  Borja  and
Eckstein  Jonathan  Distributed Optimization and Statistical Learning via the Alternating Direction Method of
Multipliers  Foundations and Trends in Machine Learning      ISSN  

Cand es  Emmanuel    and Recht  Benjamin  Exact Matrix
Completion via Convex Optimization  Foundations of
Computational Mathematics      ISSN
 

Candes  Emmanuel    and Tao  Terence  The Dantzig selector   statistical estimation when   is much larger than
   The Annals of Statistics     

Chandrasekaran  Venkat  Recht  Benjamin  Parrilo 
Pablo    and Willsky  Alan    The Convex Geometry
of Linear Inverse Problems  Foundations of Computational Mathematics     

Chatterjee  Soumyadeep  Chen  Sheng  and Banerjee 
Arindam  Generalized Dantzig Selector  Application to
the ksupport Norm  In Advances in Neural Information
Processing Systems   

Chen  Sheng and Banerjee  Arindam  Structured Estimation with Atomic Norms  General Bounds and Applications  In Advances in Neural Information Processing
Systems   

Fan  Jianqing  Fan  Yingying  and Barut  Emre  Adaptive
Robust Variable Selection  Annals of Statistics   
     

Fan  Jianqing  Li  Quefeng  and Wang  Yuyan  Robust Estimation of HighDimensional Mean Regression 
arXiv     

Gordon  Yehoram  Some Inequalitites for Gaussian Processes and Applications  Israel Journal of Mathematics 
   

Hsu  Daniel and Sabato  Sivan  Loss Minimization and
Parameter Estimation with Heavy Tails  Journal of Machine Learning Research     

Kai     Li     and Zou     New Ef cient Estimation and Variable Selection Methods for Semiparametric
VaryingCoef cient Partially Linear Models  The Annals
of Statistics     

Kato  Kengo  Group Lasso for High Dimensional Sparse

Quantile Regression Models  arXiv   

Koenker  Roger  Quantile Regression  Cambridge Univer 

sity Press   

Lecu    Guillaume and Mendelson  Shahar  Sparse recovery under weak moment assumptions  arXiv 
 

Li  Xingguo  Zhao  Tuo  Yuan  Xiaoming  and Liu  Han 
The  are package for high dimensional linear regression
and precision matrix estimation in    Journal of Machine
Learning Research     

Li        and Zhu       norm Quantile Regression  Journal
of Computational and Graphical Statistics   
 

Melnyk  Igor and Banerjee  Arindam  Estimating StrucIn International

tured Vector Autoregressive Model 
Conference on Machine Learning  ICML   

Negahban  Sahand    Ravikumar  Pradeep  Wainwright 
Martin    and Yu  Bin    Uni ed Framework for HighDimensional Analysis of MEstimators with Decomposable Regularizers  Statistical Science   
  ISSN  

Rudelson  Mark and Zhou  Shuheng  Reconstruction from
anisotropic random measurements  IEEE Transactions
on Information Theory    jun  

Sivakumar  Vidyashankar  Banerjee  Arindam  and Ravikumar  Pradeep  Beyond SubGaussian Measurements 
HighDimensional Structured Estimation with SubIn Advances in Neural InformaExponential Designs 
tion Processing Systems   

HighDimensional Structured Quantile Regression

Talagrand  Michel  Upper and Lower Bounds of Stochastic

Processes  Springer   

Tibshirani  Robert  Regression Shrinkage and Selection via
the Lasso  Journal of the Royal Statistical Society   
   

Tropp  Joel    Convex recovery of   structured signal from
independent random linear measurements  In Sampling
Theory     Renaissance  To appear  may  

Vershynin  Roman  Estimation in High Dimensions    geometric perspective  In Sampling Theory    Renaissance 
pp    Birkhauser  Basel   

Wang  Lan  Wu  Yichao  and Li  Runze  Quantile Regression for Analyzing Heterogeneity in Ultrahigh Dimension  Journal of the American Statistical Association 
   

Wu        and Liu        Variable Selection in Quantile

Regression  Statistica Sinica     

Zou     and Yuan     Composite Quantile Regression and
the Oracle Model Selection Theory  The Annals of Statistics     

