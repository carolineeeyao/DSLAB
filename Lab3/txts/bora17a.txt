Compressed Sensing using Generative Models

Ashish Bora   Ajil Jalal   Eric Price   Alexandros    Dimakis  

Abstract

The goal of compressed sensing is to estimate  
vector from an underdetermined system of noisy
linear measurements  by making use of prior
knowledge on the structure of vectors in the relevant domain  For almost all results in this literature  the structure is represented by sparsity
in   wellchosen basis  We show how to achieve
guarantees similar to standard compressed sensing but without employing sparsity at all 
Instead  we suppose that vectors lie near the range
of   generative model     Rk   Rn  Our main
theorem is that  if   is LLipschitz  then roughly
    log    random Gaussian measurements suf 
 ce for an   recovery guarantee  We demonstrate our results using generative models from
published variational autoencoder and generative
adversarial networks  Our method can use   
fewer measurements than Lasso for the same accuracy 

  Introduction
Compressive or compressed sensing is the problem of reconstructing an unknown vector      Rn after observing
      linear measurements of its entries  possibly with
added noise 

    Ax     

where     Rm   is called the measurement matrix and
    Rm is noise  Even without noise  this is an underdetermined system of linear equations  so recovery is impossible unless we make an assumption on the structure
Code for experiments in the paper can be found at 
https github com AshishBora csgm  University
of Texas
at Austin  Department of Computer Science
 University of Texas at Austin  Department of Electrical
and Computer Engineering  Correspondence to  Ashish Bora
 ashish bora utexas edu  Ajil Jalal  ajiljalal utexas edu 
Eric Price  ecprice cs utexas edu  Alexandros    Dimakis
 dimakis austin utexas edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

of the unknown vector    We need to assume that the unknown vector is  natural  or  simple  in some applicationdependent way 
The most common structural assumption is that the vector    is ksparse in some known basis  or approximately
ksparse  Finding the sparsest solution to an underdetermined system of linear equations is NPhard  but still convex optimization can provably recover the true sparse vector    if the matrix   satis es conditions such as the Restricted Isometry Property  RIP  or the related Restricted
Eigenvalue Condition  REC   Tibshirani    Candes
et al    Donoho    Bickel et al    The problem is also called highdimensional sparse linear regression
and there is vast literature on establishing conditions for
different recovery algorithms  different assumptions on the
design of   and generalizations of RIP and REC for other
structures  see       Bickel et al    Negahban et al 
  Agarwal et al    Loh   Wainwright    Bach
et al   
This signi cant interest is justi ed since   large number of
applications can be expressed as recovering an unknown
vector from noisy linear measurements  For example 
many tomography problems can be expressed in this framework     is the unknown true tomographic image and the
linear measurements are obtained by xray or other physical sensing system that produces sums or more general linear projections of the unknown pixels  Compressed sensing has been studied extensively for medical applications
including computed tomography  CT   Chen et al   
rapid MRI  Lustig et al    and neuronal spike train
recovery  Hegde et al    Another impressive application is the  single pixel camera   Duarte et al   
where digital micromirrors provide linear combinations to
  single pixel sensor that then uses compressed sensing reconstruction algorithms to reconstruct an image  These results have been extended by combining sparsity with additional structural assumptions  Baraniuk et al    Hegde
et al    and by generalizations such as translating
sparse vectors into lowrank matrices  Negahban et al 
  Bach et al    Foygel   Mackey    These
results can improve performance when the structural assumptions    the sensed signals  Other works perform  dictionary learning  seeking overcomplete bases where the
data is more sparse  see  Chen   Needell    and refer 

Compressed Sensing using Generative Models

ences therein 
In this paper instead of relying on sparsity  we use structure from   generative model  Recently  several neural
network based generative models such as variational autoencoders  VAEs   Kingma   Welling    and generative adversarial networks  GANs   Goodfellow et al   
have found success at modeling data distributions  In these
models  the generative part learns   mapping from   low
dimensional representation space     Rk to the high dimensional sample space        Rn  While training  this
mapping is encouraged to produce vectors that resemble
the vectors in the training dataset  We can therefore use
any pretrained generator to approximately capture the notion of   vector being  natural  in our domain  the generator de nes   probability distribution over vectors in sample
space and tries to assign higher probability to more likely
vectors  for the dataset it has been trained on  We expect
that vectors  natural  to our domain will be close to some
point in the support of this distribution       in the range of
  
Our Contributions  We present an algorithm that uses
generative models for compressed sensing  Our algorithm
simply uses gradient descent to optimize the representation
    Rk such that the corresponding image      has small
measurement error kAG      yk 
  While this is   nonconvex objective to optimize  we empirically  nd that gradient
descent works well  and the results can signi cantly outperform Lasso with relatively few measurements 
We obtain theoretical results showing that  as long as gradient descent  nds   good approximate solution to our objective  our output      will be almost as close to the true
   as the closest possible point in the range of   
The proof
is based on   generalization of
the Restricted Eigenvalue Condition  REC  that we call the SetRestricted Eigenvalue Condition  SREC  Our main theorem is that if   measurement matrix satis es the SREC
for the range of   given generator    then the measurement error minimization optimum is close to the true   
Furthermore  we show that random Gaussian measurement
matrices satisfy the SREC condition with high probability for large classes of generators  Speci cally  for dlayer
neural networks such as VAEs and GANs  we show that
  kd log    Gaussian measurements suf ce to guarantee
good reconstruction with high probability  One result  for
ReLUbased networks  is the following 

Theorem   Let     Rk   Rn be   generative
model from   dlayer neural network using ReLU activations  Let     Rm   be   random Gaussian matrix for
      kd log    scaled so Ai             For any
ky   AG      to within additive   of the optimum  Then

     Rn and any observation     Ax      letbz minimize

with          probability 

kG bz             min

  Rk kG                      
In the error bound above  the  rst two terms are the minimum possible error of any vector in the range of the generator and the norm of the noise  these are necessary for such  
technique  and have direct analogs in standard compressed
sensing guarantees  The third term   comes from gradient
descent not necessarily converging to the global optimum 
empirically    does seem to converge to zero  and one can
check postobservation that this is small by computing the

upper bound ky   AG bz   

While the above is restricted to ReLUbased neural networks  we also show similar results for arbitrary LLipschitz generative models  for         log    Typical neural networks have poly   bounded weights in each
layer  so     nO    giving for any activation  the same
  kd log    sample complexity as for ReLU networks 
Theorem   Let     Rk   Rn be an LLipschitz function  Let     Rm   be   random Gaussian matrix for
    scaled so Ai             For any
        log Lr
     Rn and any observation     Ax      letbz minimize
ky   AG      to within additive   of the optimum over
vectors with kbzk       Then with          probability 
kG bz          min

kG           

  Rk
kz    

The downside is two minor technical conditions  we only
optimize over representations   with kzk bounded by    and
our error gains an additive   term  Since the dependence
on these parameters is log rL  and   is something like
nO    we may set     nO    and      nO    while only
losing constant factors  making these conditions very mild 
In fact  generative models normally have the coordinates of

  be independent uniform or Gaussian  so kzk   pk  
nd  and   constant signalto noise ratio would have       
kx     pn    nd 
We remark that  while these theorems are stated in terms
of Gaussian matrices  the proofs only involve the distributional JohnsonLindenstrauss property of such matrices 
Hence the same results hold for matrices with subgaussian
entries or fastJL matrices  Ailon   Chazelle   

  Our Algorithm
All norms are  norms unless speci ed otherwise 
Let      Rn be the vector we wish to sense  Let    
Rm   be the measurement matrix and     Rm be the noise
vector  We observe the measurements     Ax      Given
  and    our task is to  nd   reconstruction    close to   

Compressed Sensing using Generative Models

  generative model is given by   deterministic function
    Rk   Rn  and   distribution PZ over     Rk  To
generate   sample from the generator  we can draw     PZ
and the sample then is      Typically  we have       
     the generative model maps from   low dimensional representation space to   high dimensional sample space 
Our approach is to  nd   vector in representation space
such that the corresponding vector in the sample space
matches the observed measurements  We thus de ne the
objective to be

loss      kAG      yk 

 

By using any optimization procedure  we can minimize
loss    with respect to    In particular  if the generative
model   is differentiable  we can evaluate the gradients
of the loss with respect to   using backpropagation and use
standard gradient based optimizers  If the optimization procedure terminates at     our reconstruction for    is     
We de ne the measurement error to be kAG      yk  and
the reconstruction error to be kG          
  Related Work
Several recent lines of work explore generative models for
reconstruction  The  rst line of work attempts to project
an image on to the representation space of the generator  These works assume full knowledge of the image 
and are special cases of the linear measurements framework where the measurement matrix   is identity  Excellent reconstruction results with SGD in the representation
space to  nd an image in the generator range have been
reported by  Lipton   Tripathi    with stochastic clipping and  Creswell   Bharath    with logistic measurement loss    different approach is introduced in  Dumoulin et al    and  Donahue et al    In their
method    recognition network that maps from the sample space vector   to the representation space vector   is
learned jointly with the generator in an adversarial setting 
  second line of work explores reconstruction with structured partial observations  The inpainting problem consists
of predicting the values of missing pixels given   part of
the image  This is   special case of linear measurements
where each measurement corresponds to an observed pixel 
The use of generative models for this task has been studied in  Yeh et al    where the objective is taken to be
  sum of    error in measurements and   perceptual loss
term given by the discriminator  Superresolution is   related task that attempts to increase the resolution of an image  We can view the observations as local spatial averages
of the unknown higher resolution image and hence cast this
as another special case of linear measurements  For prior
work on superresolution see       Yang et al    Dong
et al    Kim et al    and references therein 

We also take note of the related work of  Gilbert et al 
  that connects modelbased compressed sensing
with the invertibility of Convolutional Neural Networks 
Bayesian compressed sensing  Ji et al    and compressive sensing using Gaussian mixture models  Yang et al 
 
  related result appears in  Baraniuk   Wakin   
which studies the measurement complexity of an RIP condition for smooth manifolds  This is analogous to our
SREC for the range of    but the range of   is neither smooth  because of ReLUs  nor   manifold  because
of selfintersection  Their recovery result was extended
in  Hegde   Baraniuk    to unions of two manifolds 

  Theoretical Results
We begin with   brief review of the Restricted Eigenvalue
Condition  REC  in standard compressed sensing  The
REC is   suf cient condition on   for robust recovery to
be possible  The REC essentially requires that all  approximately sparse  vectors are far from the nullspace of the
matrix    More speci cally    satis es REC for   constant
    if for all approximately sparse vectors   

kAxk    kxk 

 

It can be shown that this condition is suf cient for recovery
of sparse vectors using Lasso  If one examines the structure of Lasso recovery proofs    key property that is used is
that the difference of any two sparse vectors is also approximately sparse  for sparsity up to     This is   coincidence
that is particular to sparsity  By contrast  the difference of
two vectors  natural  to our domain may not itself be natural  The condition we need is that the difference of any two
natural vectors is far from the nullspace of   
We propose   generalized version of the REC for   set
    Rn of vectors  the SetRestricted Eigenvalue Condition  SREC 
De nition   Let     Rn  For some parameters  
          matrix     Rm   is said to satisfy the
SREC        if             

kA             kx           

There are two main differences between the SREC and the
standard REC in compressed sensing  First  the condition
applies to differences of vectors in an arbitrary set   of
 natural  vectors  rather than just the set of approximately
ksparse vectors in some basis  This will let us apply the
de nition to   being the range of   generative model 
Second  we allow an additive slack term   This is necessary for us to achieve the SREC when   is the output of
general Lipschitz functions  Without it  the SREC depends

Compressed Sensing using Generative Models

on the behavior of   at arbitrarily small scales  Since there
are arbitrarily many such local regions  one cannot guarantee the existence of an   that works for all these local
regions  Fortunately  as we shall see  poor behavior at  
small scale   will only increase our error by   
The SREC de nition requires that for any two vectors in
   if they are signi cantly different  so the right hand side
is large  then the corresponding measurements should also
be signi cantly different  left hand side  Hence we can
hope to approximate the unknown vector from the measurements  if the measurement matrix satis es the SREC 
But how can we  nd such   matrix  To answer this  we
present two lemmas showing that random Gaussian matrices of relatively few measurements   satisfy the SREC for
the outputs of large and practically useful classes of generative models     Rk   Rn 
In the  rst lemma  we assume that the generative model
   is LLipschitz                 Rk  we have
kG              Lkz        

Note that state of the art neural network architectures
with linear layers   transposed  convolutions  maxpooling 
residual connections  and all popular nonlinearities satisfy
this assumption  In Lemma   in the Appendix we give  
simple bound on   in terms of parameters of the network 
for typical networks this is nO    We also require the input
  to the generator to have bounded norm  Since generative
models such as VAEs and GANs typically assume their input   is drawn with independent uniform or Gaussian inputs  this only prunes an exponentially unlikely fraction of
the possible outputs 
Lemma   Let     Rk   Rn be LLipschitz  Let

Lr

  log

be an   norm ball in Rk  For     if

Bk               Rk kzk     
     
       
then   random matrix     Rm   with IID entries such that
   satis es the SREC   Bk       
Aij       
with          probability 
All proofs  including this one  are deferred to Appendix   
Note that even though we proved the lemma for an    ball 
the same technique works for any compact set 
For our second lemma  we assume that the generative
model is   neural network such that each layer is   composition of   linear transformation followed by   pointwise
nonlinearity  Many common generative models have such
architectures  We also assume that all nonlinearities are

piecewise linear with at most two pieces  The popular
ReLU or LeakyReLU nonlinearities satisfy this assumption  We do not make any other assumption  and in particular  the magnitude of the weights in the network do not
affect our guarantee 
Lemma   Let     Rk   Rn be   dlayer neural network  where each layer is   linear transformation followed
by   pointwise nonlinearity  Suppose there are at most  
nodes per layer  and the nonlinearities are piecewise linear with at most two pieces  and let

 

       

  kd log   
Then   random matrix    
for some  
Rm   with IID entries Aij       
    satis es the
SREC   Rk          with          probability 
To show Theorems   and   we just need to show that
the SREC implies good recovery 
In order to make our
error guarantee relative to   error in the image space Rn 
rather than in the measurement space Rm  we also need
that   preserves norms with high probability  Cohen et al 
  Fortunately  Gaussian matrices  or other distributional JL matrices  satisfy this property 
Lemma   Let     Rm   by drawn from   distribution
that   satis es the SREC        with probability    
and   has for every  xed     Rn  kAxk    kxk with
probability       

For any      Rn and noise   let     Ax      Let bx

approximately minimize ky   Axk over            

Then 

    ky   Axk    

ky   Abxk   min
    min
with probability        
Combining Lemma   Lemma   and Lemma   gives
Theorems   and   In our setting    is the range of the

kbx          

    kx    xk  

            

generator  andbx in the theorem above is the reconstruction
  bz  returned by our algorithm 

  Models
In this section we describe the generative models used in
our experiments  We used two image datasets and two different generative model types    VAE and   GAN  This
provides some evidence that our approach can work with
many types of models and datasets 
In our experiments  we found that it was helpful to add  
regularization term      to the objective to encourage the

 

 
 

Compressed Sensing using Generative Models

    Results on MNIST

    Results on celebA

Figure   We compare the performance of our algorithm with baselines  We show   plot of per pixel reconstruction error as we vary the
number of measurements  The vertical bars indicate   con dence intervals 

optimization to explore more in the regions that are preferred by the respective generative models  see comparison to unregularized versions in Fig    Thus the objective
function we use for minimization is

kAG      yk        

Both VAE and GAN typically imposes an isotropic Gaussian prior on    Thus kzk  is proportional to the negative
loglikelihood under this prior  Accordingly  we use the
following regularizer 

        kzk 

 

where   measures the relative importance of the prior as
compared to the measurement error 

  MNIST with VAE
The MNIST dataset consists of about     images of
handwritten digits  where each image is of size    LeCun et al    Each pixel value is either    background 
or    foreground  No preprocessing was performed  We
trained VAE on this dataset  The input to the VAE is   vectorized binary image of input dimension   We set the
size of the representation space       The recognition
network is   fully connected   network 
The generator is also fully connected with the architecture
              We train the VAE using the Adam
optimizer  Kingma   Ba    with   minibatch size  
and   learning rate of   We use       in Eqn   
The digit images are reasonably sparse in the pixel space 
Thus  as   baseline  we use the pixel values directly for
sparse recovery using Lasso  We set shrinkage parameter
to be   for all the experiments 

  CelebA with DCGAN
CelebA is   dataset of more than     face images
of celebrities  Liu et al    The input images were
cropped to         RGB image  giving            
  inputs per image  Each pixel value was scaled so
that all values are between     We trained   DCGAN
 Radford et al    Kim    on this dataset  We set
the input dimension       and use   standard normal distribution  The architecture follows that of  Radford et al 
  The model was trained by one update to the discriminator and two updates to the generator per cycle  Each update used the Adam optimizer  Kingma   Ba    with
minibatch size   learning rate   and       We
use       in Eqn   
For baselines  we perform sparse recovery using Lasso on
the images in two domains         Discrete Cosine Transform  DDCT  and        Daubechies  Wavelet Transform  DDB  While we provide Gaussian measurements of the original pixel values  the    penalty is on either the DCT coef cients or the DB  coef cients of each
color channel of an image  For all experiments  we set the
shrinkage parameter to be   and   respectively for
 DDCT  and  DDB 

  Experiments and Results
  Reconstruction from Gaussian measurements
We take   to be   random matrix with IID Gaussian entries
with zero mean and standard deviation of     Each entry
of noise vector   is also an IID Gaussian random variable 
We compare performance of different sensing algorithms
qualitatively and quantitatively  For quantitative comparison  we use the reconstruction error              where   

 umEer Rf meaVurementV ecRnVtructLRn errRr  per pLxel LaVVRVA VA eg umber Rf meDsuremenWs ecRnsWrucWLRn errRr  per pLxel LDssR  DC LDssR  WDveleW DCGA DCGA egCompressed Sensing using Generative Models

is an estimate of    returned by the algorithm  In all cases 
we report the results on   held out test set  unseen by the
generative model at training time 
MNIST  The standard deviation of the noise vector is

set such that pE          We use Adam opti 

mizer  Kingma   Ba    with   learning rate of  
We do   random restarts with   steps per restart and
pick the reconstruction with best measurement error 
In Fig      we show the reconstruction error as we change
the number of measurements both for Lasso and our algorithm  We observe that our algorithm is able to get low
errors with far fewer measurements  For example  our
algorithm   performance with   measurements matches
Lasso   performance with   measurements  Fig    
shows sample reconstructions by Lasso and our algorithm 
However  our algorithm is limited since its output is constrained to be in the range of the generator  After  
measurements  our algorithm   performance saturates  and
additional measurements give no additional performance 
Since Lasso has no such limitation  it eventually surpasses
our algorithm  but this takes more than   measurements
of the  dimensional vector  We expect that   more
powerful generative model with representation dimension
      can make better use of additional measurements 
celebA  The standard deviation of the noise vector is

set such that pE          We use Adam opti 

mizer  Kingma   Ba    with   learning rate of  
We do   random restarts with   update steps per restart
and pick the reconstruction with best measurement error 
In Fig      we show the reconstruction error as we change
the number of measurements both for Lasso and our algorithm  In Fig    we show sample reconstructions by Lasso
and our algorithm  We observe that our algorithm is able
to produce reasonable reconstructions with as few as  
measurements  while the output of the baseline algorithms
is quite blurry  Similar to the results on MNIST  if we continue to give more measurements  our algorithm saturates 
and for more than   measurements  Lasso gets   better
reconstruction  We again expect that   more powerful generative model with       would perform better in the
highmeasurement regime 

  Superresolution
Superresolution is the task of constructing   high resolution image from   low resolution version of the same image  This problem can be thought of as special case of
our general framework of linear measurements  where the
measurements correspond to local spatial averages of the
pixel values  Thus  we try to use our recovery algorithm
to perform this task with measurement matrix   tailored
to give only the relevant observations  We note that this

measurement matrix may not satisfy the SREC condition
 with good constants   and   and consequently  our theorems may not be applicable 
MNIST  We construct   low resolution image by spatial
    pooling with   stride of   to produce       image 
These measurements are used to reconstruct the original
      image  Fig     shows reconstructions produced
by our algorithm on images from   held out test set  We
observe sharp reconstructions which closely match the  ne
structure in the ground truth 
celebA  We construct   low resolution image by spatial  
  pooling with   stride of   to produce         image 
These measurements are used to reconstruct the original
    image  In Fig    we show results on images from  
held out test set  We see that our algorithm is able to  ll in
the details to match the original image 

  Understanding sources of error
Although better than baselines  our method still admits
some error  This error can be decomposed into three components      Representation error  the unknown image is
far from the range of the generator     Measurement error 
The  nite set of random measurements do not contain all
the information about the unknown image     Optimization
error  The optimization procedure did not  nd the best   
In this section we present some experiments that suggest
that the representation error is the dominant term  In our
 rst experiment  we ensure that the representation error is
zero  and try to minimize the sum of other two errors  In
this setting  we observe that the reconstructions are almost
perfect  In the second experiment  we ensure that the measurement error is zero  and try to minimize the sum of other
two  Here  we observe that the total error obtained is very
close to the total error in our reconstruction experiments
 Sec   

  SENSING IMAGES FROM RANGE OF GENERATOR
Our  rst approach is to sense an image that is in the range
of the generator  More concretely  we sample      from
PZ  Then we pass it through the generator to get     
     Now  we pretend that this is   real image and try to
sense that  This method eliminates the representation error
and allows us to check if our gradient based optimization
procedure is able to  nd    by minimizing the objective 
In Fig     and Fig      we show the reconstruction error for
images in the range of the generators trained on MNIST
and celebA datasets respectively  We see that we get almost
perfect reconstruction with very few measurements  This
suggests that objective is being properly minimized and we
indeed get    close to         the sum of optimization error
and the measurement error is small in the absence of the

Compressed Sensing using Generative Models

    We show original images  top row  and reconstructions by
Lasso  middle row  and our algorithm  bottom row 

    We show original images  top row  low resolution version
of original images  middle row  and reconstructions  last row 

Figure   Results on MNIST  Reconstruction with   measurements  left  and Superresolution  right 

Figure   Reconstruction results on celebA with       measurements  of       dimensional vector  We show original images
 top row  and reconstructions by Lasso with DCT basis  second row  Lasso with wavelet basis  third row  and our algorithm  last row 

Figure   Superresolution results on celebA  Top row has the original images  Second row shows the low resolution    smaller  version
of the original image  Last row shows the images produced by our algorithm 

Figure   Results on the representation error experiments on celebA  Top row shows original images and the bottom row shows closest
images found in the range of the generator 

OrLgLnDOLDsso  DCT LDsso  WDveOeW DCGANOriginDOBOurreGDCGANOriginDODCGANCompressed Sensing using Generative Models

    Results on MNIST

    Results on celebA

Figure   Reconstruction error for images in the range of the generator  The vertical bars indicate   con dence intervals 

Figure   Results on the representation error experiments on
MNIST  Top row shows original images and the bottom row
shows closest images found in the range of the generator 
representation error 

  QUANTIFYING REPRESENTATION ERROR
We saw that in absence of the representation error  the overall error is small  However from Fig    we know that the
overall error is still nonzero  So  in this experiment  we
seek to quantify the representation error       how far are
the real images from the range of the generator 
From the previous experiment  we know that the    recovered by our algorithm is close to    the best possible value 
if the image being sensed is in the range of the generator 
Based on this  we make an assumption that this property is
also true for real images  With this assumption  we get an
estimate to the representation error as follows  We sample
real images from the test set  Then we use the full image in
our algorithm       our measurement matrix   is identity 
This eliminates the measurement error  Using these measurements  we get the reconstructed image      through
our algorithm  The estimated representation error is then
kG         We repeat this procedure several times over
randomly sampled images from our dataset and report average representation error values  The task of  nding the
closest image in the range of the generator has been studied in prior work  Creswell   Bharath    Dumoulin
et al    Donahue et al   
On the MNIST dataset  we get average per pixel represen 

tation error of   The recovered images are shown in
Fig    In contrast  with only   Gaussian measurements 
we get   per pixel reconstruction error of about   On
the celebA dataset  we get average per pixel representation
error of   The recovered images are shown in Fig   
In contrast  with only   Gaussian measurements  we get
  per pixel reconstruction error of about  
This suggests that the representation error is the major component of the total error  and thus   more  exible generative
model can help reduce it on both datasets 

  Conclusion
We demonstrate how to perform compressed sensing using generative models from neural nets  These models can
represent data distributions more concisely than standard
sparsity models  while their differentiability allows for fast
signal reconstruction  This will allow compressed sensing
applications to make signi cantly fewer measurements 
Our theorems and experiments both suggest that  after relatively few measurements  the signal reconstruction gets
close to the optimal within the range of the generator  To
reach the full potential of this technique  one should use
larger generative models as the number of measurements
increase  Whether this can be expressed more concisely
than by training multiple independent generative models of
different sizes is an open question 
Generative models are an active area of research with ongoing rapid improvements  Because our framework applies to
general generative models  this improvement will immediately yield better reconstructions with fewer measurements 
We also believe that one could also use the performance of
generative models for our task as one benchmark for the
quality of different models 

 umber Rf measurements eFRnstruFtiRn errRr  per pixel rRm test set rRm generatRr umber Rf measurements eFRnstruFtiRn errRr  per pixel rRm test set rRm generatRrCompressed Sensing using Generative Models

Acknowledgements
We would like to thank Philipp Kr ahenb uhl for helpful discussions  This research has been supported by NSF Grants
CCF         ARO YIP
  NF  and the William Hartwig fellowship 

References
Agarwal  Alekh  Negahban  Sahand  and Wainwright  Martin    Fast global convergence rates of gradient methods
for highdimensional statistical recovery 
In Advances
in Neural Information Processing Systems  pp   
 

Ailon  Nir and Chazelle  Bernard  The fast johnson 
lindenstrauss transform and approximate nearest neighbors 
SIAM Journal on Computing   
 

Bach  Francis  Jenatton  Rodolphe  Mairal  Julien  Obozinski  Guillaume  et al  Optimization with sparsityinducing penalties  Foundations and Trends    in Machine Learning     

Baraniuk  Richard   and Wakin  Michael    Random projections of smooth manifolds  Foundations of computational mathematics     

Baraniuk  Richard    Cevher  Volkan  Duarte  Marco   
and Hegde  Chinmay  Modelbased compressive sensing  IEEE Transactions on Information Theory   
   

Bickel  Peter    Ritov  Ya acov  and Tsybakov  Alexandre    Simultaneous analysis of lasso and dantzig selector  The Annals of Statistics  pp     

Candes  Emmanuel    Romberg  Justin    and Tao  Terence  Stable signal recovery from incomplete and inaccurate measurements  Communications on pure and
applied mathematics     

Chen  GuangHong  Tang  Jie  and Leng  Shuai  Prior image constrained compressed sensing  piccs    method
to accurately reconstruct dynamic ct images from highly
undersampled projection data sets  Medical physics   
   

Chen  Guangliang and Needell  Deanna  Compressed sensing and dictionary learning  Proceedings of Symposia in
Applied Mathematics     

Cohen     Dahmen     and DeVore     Compressed sensing and best kterm approximation     Amer  Math  Soc 
   

Creswell  Antonia and Bharath  Anil Anthony 

Inverting
the generator of   generative adversarial network  arXiv
preprint arXiv   

Donahue 

Jeff  Kr ahenb uhl  Philipp 

and Darrell 
Trevor  Adversarial feature learning  arXiv preprint
arXiv   

Dong  Chao  Loy  Chen Change  He  Kaiming  and Tang 
Xiaoou 
Image superresolution using deep convolutional networks  IEEE transactions on pattern analysis
and machine intelligence     

Donoho  David    Compressed sensing 

IEEE Transac 

tions on information theory     

Duarte  Marco    Davenport  Mark    Takbar  Dharmpal 
Laska  Jason    Sun  Ting  Kelly  Kevin    and Baraniuk 
Richard    Singlepixel imaging via compressive sampling  IEEE signal processing magazine   
 

Dumoulin  Vincent  Belghazi  Ishmael  Poole  Ben  Lamb 
Alex  Arjovsky  Martin  Mastropietro  Olivier  and
Courville  Aaron  Adversarially learned inference  arXiv
preprint arXiv   

Foygel  Rina and Mackey  Lester  Corrupted sensing 
Novel guarantees for separating structured signals  IEEE
Transactions on Information Theory   
 

Gilbert  Anna    Zhang  Yi  Lee  Kibok  Zhang  Yuting 
and Lee  Honglak  Towards understanding the invertibility of convolutional neural networks  arXiv preprint
arXiv   

Goodfellow  Ian  PougetAbadie  Jean  Mirza  Mehdi  Xu 
Bing  WardeFarley  David  Ozair  Sherjil  Courville 
Aaron  and Bengio  Yoshua  Generative adversarial nets 
In Advances in neural information processing systems 
pp     

Hegde  Chinmay and Baraniuk  Richard    Signal recovery on incoherent manifolds  IEEE Transactions on Information Theory     

Hegde  Chinmay  Duarte  Marco    and Cevher  Volkan 
Compressive sensing recovery of spike trains using  
structured sparsity model  In SPARS Signal Processing with Adaptive Sparse Structured Representations 
 

Hegde  Chinmay  Indyk  Piotr  and Schmidt  Ludwig   
nearlylinear time framework for graphstructured sparsity 
In Proceedings of the  nd International Conference on Machine Learning  ICML  pp   
 

Compressed Sensing using Generative Models

Ji  Shihao  Xue  Ya  and Carin  Lawrence  Bayesian compressive sensing  IEEE Transactions on Signal Processing     

Tibshirani  Robert  Regression shrinkage and selection via
the lasso  Journal of the Royal Statistical Society  Series
   Methodological  pp     

Vershynin  Roman 

analysis of
arXiv   

Introduction to the nonasymptotic
arXiv preprint

random matrices 

Yang  Jianbo  Yuan  Xin  Liao  Xuejun  Llull  Patrick 
Brady  David    Sapiro  Guillermo  and Carin  Lawrence 
Video compressive sensing using gaussian mixture models 
IEEE Transactions on Image Processing   
   

Yang  Jianchao  Wright  John  Huang  Thomas    and Ma 
Yi 
Image superresolution via sparse representation 
IEEE transactions on image processing   
   

Yeh  Raymond  Chen  Chen  Lim  Teck Yian  HasegawaJohnson  Mark  and Do  Minh    Semantic image inpainting with perceptual and contextual losses  arXiv
preprint arXiv   

Kim  Jiwon  Kwon Lee  Jung  and Mu Lee  Kyoung  Accurate image superresolution using very deep convolutional networks  In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition  pp   
   

Kim  Taehoon    tensor ow implementation of  deep
networks 

convolutional
https github com carpedm DCGANtensor ow 
 

adversarial

generative

Kingma  Diederik and Ba 

Jimmy 
method for stochastic optimization 
arXiv   

Adam 

 
arXiv preprint

Kingma  Diederik   and Welling  Max  Autoencoding
arXiv preprint arXiv 

variational bayes 
 

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  Proceedings of the IEEE   
   

Lipton  Zachary   and Tripathi  Subarna  Precise recovery
of latent vectors from generative adversarial networks 
arXiv preprint arXiv   

Liu  Ziwei  Luo  Ping  Wang  Xiaogang  and Tang  Xiaoou 
Deep learning face attributes in the wild  In Proceedings
of the IEEE International Conference on Computer Vision  pp     

Loh  PoLing and Wainwright  Martin    Highdimensional
regression with noisy and missing data  Provable guarantees with nonconvexity  In Advances in Neural Information Processing Systems  pp     

Lustig  Michael  Donoho  David  and Pauly  John   
Sparse mri  The application of compressed sensing for
rapid mr imaging  Magnetic resonance in medicine   
   

Matou sek  Ji    Lectures on discrete geometry  volume  

Springer Science   Business Media   

Negahban  Sahand  Yu  Bin  Wainwright  Martin    and
Ravikumar  Pradeep      uni ed framework for highdimensional analysis of mestimators with decomposable regularizers 
In Advances in Neural Information
Processing Systems  pp     

Radford  Alec  Metz  Luke  and Chintala  Soumith  Unsupervised representation learning with deep convolutional generative adversarial networks  arXiv preprint
arXiv   

