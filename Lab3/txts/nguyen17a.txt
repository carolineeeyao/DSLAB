The Loss Surface of Deep and Wide Neural Networks

Quynh Nguyen   Matthias Hein  

Abstract

While the optimization problem behind deep
neural networks is highly nonconvex  it is frequently observed in practice that training deep
networks seems possible without getting stuck in
suboptimal points  It has been argued that this
is the case as all local minima are close to being globally optimal  We show that this is  almost  true  in fact almost all local minima are
globally optimal  for   fully connected network
with squared loss and analytic activation function given that the number of hidden units of one
layer of the network is larger than the number
of training points and the network structure from
this layer on is pyramidal 

  Introduction
The application of deep learning  LeCun et al    has
in recent years lead to   dramatic boost in performance in
many areas such as computer vision  speech recognition or
natural language processing  Despite this huge empirical
success  the theoretical understanding of deep learning is
still limited  In this paper we address the nonconvex optimization problem of training   feedforward neural network 
This problem turns out to be very dif cult as there can
be exponentially many distinct local minima  Auer et al 
  Safran   Shamir    It has been shown that the
training of   network with   single neuron with   variety of
activation functions turns out to be NPhard  Sima   
In practice local search techniques like stochastic gradient
descent or variants are used for training deep neural networks  Surprisingly  it has been observed  Dauphin et al 
  Goodfellow et al    that in the training of stateof theart feedforward neural networks with sparse connectivity like convolutional neural networks  LeCun et al 
  Krizhevsky et al    or fully connected ones one

 Department of Mathematics and Computer Science  Saarland University  Germany  Correspondence to  Quynh Nguyen
 quynh cs unisaarland de 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

does not encounter problems with suboptimal local minima  However  as the authors admit themselves in  Goodfellow et al    the reason for this might be that there
is   connection between the fact that these networks have
good performance and that they are easy to train 
On the theoretical side there have been several interesting
developments recently  see     
 Brutzkus   Globerson 
  Lee et al    Poggio   Liao    Rister   Rubin    Soudry   Hoffer    Zhou   Feng   
For some class of networks one can show that one can train
them globally optimal ef ciently  However  it turns out that
these approaches are either not practical  Janzamin et al 
  Haeffele   Vidal    Soltanolkotabi    as
they require      knowledge about the data generating measure  or they modify the neural network structure and objective  Gautier et al    One class of networks which
are simpler to analyze are deep linear networks for which it
has been shown that every local minimum is   global minimum  Baldi   Hornik    Kawaguchi    While
this is   highly nontrivial result as the optimization problem is nonconvex  deep linear networks are not interesting
in practice as one ef ciently just learns   linear function  In
order to characterize the loss surface for general networks 
an interesting approach has been taken by  Choromanska
et al      By randomizing the nonlinear part of   feedforward network with ReLU activation function and making some additional simplifying assumptions  they can relate it to   certain spin glass model which one can analyze 
In this model the objective of local minima is close to the
global optimum and the number of bad local minima decreases quickly with the distance to the global optimum 
This is   very interesting result but is based on   number
of unrealistic assumptions  Choromanska et al      It
has recently been shown  Kawaguchi    that if some of
these assumptions are dropped one basically recovers the
result of the linear case  but the model is still unrealistic 
In this paper we analyze the case of overspeci ed neural
networks  that is the network is larger than what is required
to achieve minimum training error  Under overspeci cation  Safran   Shamir    have recently analyzed under
which conditions it is possible to generate an initialization
so that it is in principle possible to reach the global optimum with descent methods  However  they can only deal
with one hidden layer networks and have to make strong

The Loss Surface of Deep and Wide Neural Networks

assumptions on the data such as linear independence or
cluster structure  In this paper overspeci cation means that
there exists   very wide layer  where the number of hidden units is larger than the number of training points  For
this case  we can show that   large class of local minima
is globally optimal  In fact  we will argue that almost every critical point is globally optimal  Our results generalize
previous work of  Yu   Chen    who have analyzed  
similar setting for one hidden layer networks  to networks
of arbitrary depth  Moreover  it extends results of  Gori  
Tesi    Frasconi et al    who have shown that for
certain deep feedforward neural networks almost all local
minima are globally optimal whenever the training data is
linearly independent  While it is clear that our assumption
on the number of hidden units is quite strong  there are several recent neural network structures which contain   quite
wide hidden layer relative to the number of training points
     in  Lin et al    they have   training samples
and the network has one hidden layer with   hidden
units and  Ba   Caruana    have   million training
samples and   layer with   hidden units  We refer to
 Ciresan et al    Neyshabur et al    Vincent et al 
  Caruana et al    for other examples where the
number of hidden units of one layer is on the order of the
number of training samples  We conjecture that for these
kind of wide networks it still holds that almost all local
minima are globally optimal  The reason is that one can
expect linear separability of the training data in the wide
layer  We provide supporting evidence for this conjecture
by showing that basically every critical point for which the
training data is linearly separable in the wide layer is globally optimal  Moreover  we want to emphasize that all of
our results hold for neural networks used in practice  There
are no simplifying assumptions as in previous work 

  Feedforward Neural Networks and

Backpropagation

We are mainly concerned with multiclass problems but
our results also apply to multivariate regression problems 
Let   be the number of training samples and denote by
                xN      RN                    yN     
RN   the input resp  output matrix for the training data
   where   is the input dimension and   the
 xi  yi  
number of classes  We consider fullyconnected feedforward networks with   layers  indexed from                 
which correspond to the input layer   st hidden layer  etc 
and output layer  The network structure is determined by
         Rd             
the weight matrices  Wk  
Rnk nk     RnL    where nk is the number of hidden units of layer    for consistency  we set         nL  
         Rn          RnL 
   and the bias vectors  bk  
We denote by           the space of all possible parameters of the network  In this paper      denotes the set

of integers                and        the set of integers from
  to    The activation function           is assumed at
least to be continuously differentiable  that is          
In this paper  we assume that all the functions are applied
componentwise  Let fk  gk   Rd   Rnk be the mappings
from the input space to the feature space at layer    which
are de ned as

          fk       gk    gk         
  fk      bk
for every             Rd  In the following  let Fk  
 fk    fk            fk xN      RN nk and Gk  
 gk    gk            gk xN      RN nk be the matrices
that store the feature vectors of layer   after and before applying the activation function  One can easily check that

      XW       bT
   
Fk    Fk Wk      bT

   

for         

In this paper we analyze the behavior of the loss of the
network without any form of regularization  that is the  nal
objective           of the network is de ned as

 

 Wk  bk  

  

  fLj xi    yij 

 

  

  

where           is assumed to be   continuously differentiable loss function  that is           The prototype
loss which we consider in this paper is the squared loss 
       which is one of the standard loss functions in
the neural network literature  We assume throughout this
paper that the minimum of   is attained 
The idea of backpropagation is the core of our theoretical
analysis  Lemma   below shows wellknown relations for
feedforward neural networks  which are used throughout
the paper  The derivative of the loss       
the value of
unit   at layer   evaluated at   single training sample xi is
 gkj  xi    We arrange these vectors
denoted as  kj xi   
for all training samples into   single matrix     de ned as

 

                      xN      RN nk  

In the following        denotes the Hadamard product between two matrices              ij   AijBij 
Lemma   Let             Then it holds

 cid 

 cid 

 

  cid 

  cid 

 cid 

 cid 

      

  cid FL          cid GL 
      

      cid Gk            

     

   Wk    

   bk       

     

     
             
   
              

The Loss Surface of Deep and Wide Neural Networks

Note that Lemma   does not apply to nondifferentiable
activation functions like the ReLU function   ReLU     
max     However  it is known that one can approximate this activation function arbitrarily well by   smooth
  log                 softplus 
function             
satis es lim         ReLU    for any       

  Main Result
We  rst discuss some prior work and present then our main
result together with extensive discussion  For improved
readability we postpone the proof of the main result to
the next section which contains several intermediate results
which are of independent interest 

  Previous Work

Our work can be seen as   generalization of the work of
 Gori   Tesi    Yu   Chen    While  Yu   Chen 
  has shown that for   onehidden layer network  that
if            then every local minimum is   global minimum  the work of  Gori   Tesi    considered also
multilayer networks  For the convenience of the reader 
we  rst restate Theorem   of  Gori   Tesi    using our
previously introduced notation  The critical points of   continuously differentiable function     Rd     are the points
where the gradient vanishes  that is            Note that
this is   necessary condition for   local minimum 
Theorem    Gori   Tesi    Let           be
de ned as in   with least squares loss           Assume                 to be continuously differentiable with
strictly positive derivative and

 cid   
         
lim
  
lima   cid   

        

 cid cid   
lima 
         
lima   cid cid   
        

   of   which satis es

Then every critical point  Wl  bl  
the conditions
  rank Wl    nl for all         
                  implies      

is   global minimum 

While this result is already for general multilayer networks  the condition                 implies       is
the main caveat  It is already noted in  Gori   Tesi   
that  it is quite hard to understand its practical meaning 
as it requires prior knowledge of   at every critical point 
Note that this is almost impossible as   depends on all
the weights of the network  For   particular case  when
the training samples  biases added  are linearly independent       rank              the condition holds automatically  This case is discussed in the following Theorem

  where we consider   more general class of loss and
activation functions 

  First Main Result and Discussion
  function     Rd     is real analytic if the corresponding multivariate Taylor series converges to       on an open
subset of Rd  Krantz   Parks    All results in this
section are proven under the following assumptions on the
loss activation function and training data 

Assumptions  

samples       xi  cid  xj for all    cid    

  There are no identical

training

    is analytic on    strictly monotonically increasing

and
      is bounded or
    there are positive             

     
     for       and              for      
            and if   cid        then   is   global minimum

These conditions are not always necessary to prove some of
the intermediate results presented below  but we decided to
provide the proof under the above strong assumptions for
better readability  For instance  all of our results also hold
for strictly monotonically decreasing activation functions 
Note that the above conditions are not restrictive as many
standard activation functions satisfy them 

 

Lemma   The sigmoid activation function      
       the tangent hyperbolic       tanh    and the
softplus function        
  log         for       satisfy
Assumption  

The conditions on   are satis ed for any twice continuously differentiable convex loss function    typical example is the squared loss           or the PseudoHuber loss  Hartley   Zisserman    given as       

 cid           which approximates    for small

  and is linear with slope   for large    But also nonconvex loss functions satisfy this requirement      
the
BlakeZisserman  corrupted Gaussian and Cauchy functions  Hartley   Zisserman         
As   motivation for our main result  we  rst analyze the
case when the training samples are linearly independent 
which requires        It can be seen as   generalization
of Corollary   in  Gori   Tesi   
Theorem   Let           be de ned as in   and let
the Assumptions   hold  If the training samples are linearly independent  that is rank              then every
critical point  Wl  bl  
   of   for which the weight matril  have full column rank  that is rank Wl    nl
ces  Wl  
for          is   global minimum 

The Loss Surface of Deep and Wide Neural Networks

Theorem   implies that the weight matrices of potential
saddle points or suboptimal local minima need to have low
rank for one particular layer  Note however that the set
of low rank weight matrices in   has measure zero  At
the moment we cannot prove that suboptimal low rank local minima cannot exist  However  it seems implausible
that such suboptimal low rank local minima exist as every neighborhood of such points contains full rank matrices
which increase the expressiveness of the network  Thus it
should be possible to use this degree of freedom to further
reduce the loss  which contradicts the de nition of   local
minimum  Thus we conjecture that all local minima are
indeed globally optimal 
The main restriction in the assumptions of Theorem   is
the linear independence of the training samples as it requires           which is very restrictive in practice  We
prove in this section   similar guarantee in our main Theorem   by implicitly transporting this condition to some
higher layer    similar guarantee has been proven by  Yu
  Chen    for   single hidden layer network  whereas
we consider general multilayer networks  The main ingredient of the proof of our main result is the observation in
the following lemma 
Lemma   Let           be de ned as in   and let
       be given 
the Assumptions   hold  Let  Wl  bl  
Assume there is some                 the following holds

  rank Fk          
 cid 
  rank Wl    nl               
 cid 
   Wk 
 bk 

 cid 
 cid 

 Wl  bl  

 Wl  bl  

   

   

  

  

then  Wl  bl  

   is   global minimum 

The  rst condition of Lemma   can be seen as   generalization of the requirement of linearly independent training inputs in Theorem   to   condition of linear independence of the feature vectors at   hidden layer  Lemma
  suggests that if we want to make statements about the
global optimality of critical points  it is suf cient to know
when and which critical points ful ll these conditions  The
third condition is trivially satis ed by   critical point and
the requirement of full column rank of the weight matrices
is similar to Theorem   However  the  rst one may not
be ful lled since rank Fk       is dependent not only on
the weights but also on the architecture  The main dif culty
of the proof of our following main theorem is to prove that
this  rst condition holds under the rather simple requirement that nk         for   subset of all critical points 
But before we state the theorem we have to discuss   particular notion of nondegenerate critical point 

De nition    Block Hessian  Let           be  
twicecontinuously differentiable function de ned on some
open domain     Rn  The Hessian          subset of variables                 xn  is denoted as  
Sf             
When          we write          Rn   to denote the full
Hessian matrix 

We use this to introduce   slightly more general notion of
nondegenerate critical point 

De nition    Nondegenerate critical point  Let  
 
      be   twicecontinuously differentiable function de 
 ned on some open domain     Rn  Let       be  
critical point                  then

    is nondegenerate for   subset of variables    

            xn  if  

Sf     is nonsingular 

    is nondegenerate if        is nonsingular 

Note that   nondegenerate critical point might not be nondegenerate for   subset of variables  and vice versa  if it is
nondegenerate on   subset of variables it does not necessarily imply nondegeneracy on the whole set  For instance 

        

 
 
 
 

   
 
 
 
 
 
 

 
 
 
 

          

 
 
 
 

 
 
 
 

   
   
   
   

Clearly  det           but det            cid    and
det        cid    but det               The concept
of nondegeneracy on   subset of variables is crucial for
the following statement of our main result 
Theorem   Let           be de ned as in   and let
the Assumptions   hold  Suppose nk         for some
           Then every critical point     
   of  
which satis es the following conditions

      

    

      

      

   is nondegenerate on  Wl  bl          
    
for some subset                       satisfying
           

      

     has full column rank  that is  rank    

    

     

nl for              

is   global minimum of  

First of all we note that the full column rank condition of
     in Theorem   and   implicitly requires that
 Wl  
nk    nk            nL  This means the network needs
to have   pyramidal structure from layer       to    It is
interesting to note that most modern neural network architectures have   pyramidal structure from some layer  typically the  rst hidden layer  on  Thus this is not   restrictive

The Loss Surface of Deep and Wide Neural Networks

requirement  Indeed  one can even argue that Theorem  
gives an implicit justi cation as it hints on the fact that such
networks are easy to train if one layer is suf ciently wide 
Note that Theorem   does not
require fully nondegenerate critical points but nondegeneracy is only
needed for some subset of variables that includes layer
      As   consequence of Theorem   we get directly  
stronger result for nondegenerate local minima 
Corollary   Let           be de ned as in   and let
the Assumptions   hold  Suppose nk         for some
           Then every nondegenerate local minimum
    
     has full column
rank  that is rank    

   of   for which     
    

      

    

      nl  is   global minimum of  

Let us discuss the implications of these results  First  note
that Theorem   is slightly weaker than Theorem   as it
requires also nondegeneracy wrt to   set of variables including layer       Moreover  similar to Theorem   it
does not exclude the possibility of suboptimal local minima of low rank in the layers  above  layer       On the
other hand it makes also very strong statements  In fact  if
nk         for some            then even degenerate
saddle points local maxima are excluded as long as they
are nondegenerate with respect to any subset of parameters of upper layers that include layer       and the rank
condition holds  Thus given that the weight matrices of the
upper layers have full column rank   there is not much room
left for degenerate saddle points local maxima  Moreover 
for   onehidden layer network for which           
every nondegenerate critical point with respect to the output layer parameters is   global minimum  as the full rank
condition is not active for onehidden layer networks 
Concerning the nondegeneracy condition of main Theorem   one might ask how likely it is to encounter degenerate points of   smooth function  This is answered by an
application of Sard   Morse theorem in  Milnor   
Theorem       Morse     If         Rd     is
twice continuously differentiable  Then for almost all    
Rd with respect to the Lebesgue measure it holds that   cid 
de ned as   cid               cid      cid  has only nondegenerate
critical points 

Note that the theorem would still hold if one would draw
  uniformly at random from the set      Rd    cid   cid     
for any       Thus almost every linear perturbation   cid  of
  function   will lead to the fact all of its critical points are
nondegenerate  Thus  this result indicates that exact degenerate points might be rare  Note however that in practice the Hessian at critical points can be close to singular
 at least up to numerical precision  which might affect the
training of neural networks negatively  Sagun et al   
As we argued for Theorem   our main Theorem   does

not exclude the possibility of suboptimal degenerate local
minima or suboptimal local minima of low rank  However 
we conjecture that the second case cannot happen as every neighborhood of the local minima contains full rank
matrices which increase the expressiveness of the network
and this additional  exibility can be used to reduce the loss
which contradicts the de nition of   local minimum 
As mentioned in the introduction the condition nk      
looks at  rst sight very strong  However  as mentioned in
the introduction  in practice often networks are used where
one hidden layer is rather wide  that is nk is on the order of
   typically it is the  rst layer of the network  As the condition of Theorem   is suf cient and not necessary  one
can expect out of continuity reasons that the loss surface of
networks where the condition is approximately true  is still
rather well behaved  in the sense that still most local minima are indeed globally optimal and the suboptimal ones
are not far away from the globally optimal ones 

  Proof of Main Result
For better readability  we  rst prove our main Theorem  
for   special case where   is the whole set of upper layers 
                             and then show how to extend
the proof to the general case where                        
Our proof strategy is as follows  We  rst show that the
output of each layer are real analytic functions of network
parameters  Then we prove that there exists   set of parameters such that rank Fk            Using properties of real analytic functions  we conclude that the set
of parameters where rank Fk           has measure
zero  Then with the nondegeneracy condition  we can apply the implicitfunction theorem to conclude that even if
rank Fk           is not true at   critical point  then still
in any neighborhood of it there exists   point where the
conditions of Lemma   are true and the loss is minimal 
By continuity of   this implies that the loss must also be
minimal at the critical point 
We introduce some notation frequently used in the proofs 
Let                Rd    cid       cid       be the open ball
in Rd of radius   around   
Lemma   If the Assumptions   hold  then the output of
each layer fl for every         are real analytic functions
of the network parameters on   
Proof 
Any linear function is real analytic and the
set of real analytic functions is closed under addition 
multiplication and composition  see      Prop    and
Prop    in  Krantz   Parks    As we assume
that the activation function is real analytic  we get that
all the output functions of the neural network fk are real
analytic functions of the parameters as compositions of
real analytic functions 
 

The Loss Surface of Deep and Wide Neural Networks

for every cid Wl  bl

 cid  
      

 cid cid    

      
 

 cid  

 cid      it holds

    

rank Fk           
The  nal proof of our main Theorem   is heavily based
on the implicit function theorem  see       Marsden   

 

The concept of real analytic functions is important in our
proofs as these functions can never be  constant  in   set of
the parameter space which has positive measure unless they
are constant everywhere  This is captured by the following
lemma 
Lemma    Nguyen    Mityagin    If     Rn  
  is   real analytic function which is not identically zero
then the set      Rn             has Lebesgue measure
zero 

In the next lemma we show that there exist network parameters such that rank Fk           holds if nk        
Note that this is only possible due to the fact that one uses
nonlinear activation functions  For deep linear networks 
it is not possible for Fk to achieve maximum rank if the
layers below it are not suf ciently wide  To see this  one
considers Fk   Fk Wk      bT
  for   linear network 
then rank Fk    min rank Fk  rank Wk      since
the addition of   rankone term does not increase the rank
of   matrix by more than one  By using induction  one gets
rank Fk    rank Wl              for every        
The
where
rank Fk           together with the previous lemma
will then be used to show that the set of network parameters
where rank Fk           has measure zero 
Lemma   If the Assumptions   hold and nk        
for some            then there exists at least one set of
parameters  Wl  bl  

   such that rank Fk           

parameters

existence

network

of

Now we combine the previous lemma with Lemma   to
conclude the following 

  

 cid 

 cid  

 cid cid Wl  bl

Lemma   If
      for some           

the Assumptions   hold and nk  
then the set    
has Lebesgue mea 

 cid cid cid  rank Fk          

sure zero 
We conclude that for nk         even if there are network parameters such that rank Fk            then every
neighborhood of these parameters contains network parameters such that rank Fk           
Corollary   If the Assumptions   hold and nk     
for every       there exists at least one  cid Wl  bl
 cid  
for some            then for any given     
 cid cid    
   and
    
 cid cid cid  rank Fk          
 cid 

 cid 
 cid  
 cid cid Wl  bl
 cid  
 cid 
 cid cid Wl  bl
 cid  

  
The ball  
has positive Lebesgue measure while   has measure zero due to Lemma   Thus 

     rank Fk           

Proof  Let    

    

    

      

      
 

    

 

 

Theorem   Let     Rs   Rt   Rt be   continuously
differentiable function  Suppose          Rs   Rt and
           If the Jacobian matrix          

   

   

 

 

    Rt  

 
 vt

 

Jv        

 
  
   
is nonsingular at       
then there is an open ball
       for some       and   unique function    
         Rt such that             for all    
       Furthermore    is continuously differentiable 

  
 vt

  

 cid 

 cid 

            vec Wk     bT

With all the intermediate results proven above  we are  
nally ready for the proof of the main result 
Proof of Theorem   for case                      
Let us divide the set of all parameters of the network
into two subsets where one corresponds to all parameters of all layers up to    for that we denote    
 vec       bT
       and the other corresponds to the remaining parameters  for that we denote
           vec WL     bT
     vec Wk     bT
      By abuse
of notation  we write        to denote  
 Wl  bl  
 
Let     dim        dim    and          Rs Rt be the
corresponding vectors for the critical point     
  
Let     Rs   Rt   Rt be   map de ned as         
           Rt  which is the gradient mapping of  
       all parameters of the upper layers from        to
   Since the gradient vanishes at   critical point  it holds
that                       The Jacobian of  
         is the principal submatrix of the Hessian of         
          Rt    As the critv  that is  Jv          
ical point is assumed to be nondegenerate with respect
to    it holds that Jv          
        is nonsingular  Moreover    is continuously differentiable since
          due to Assumption   Therefore    and
       satisfy the conditions of the implicit function theorem   Thus there exists an open ball          Rs
for some       and   continuously differentiable function
             Rt such that

      

    

 cid 

                        
        

      nl             that
By assumption we have rank    
is the weight matrices of the  upper  layers have full column rank  Note that     
     corresponds to the weight

    

The Loss Surface of Deep and Wide Neural Networks

matrix part of    where one leaves out    
   Thus there
exists   suf ciently small   such that for any           
the weight matrix part  Wl  
     of   has full column
rank  In particular  this  combined with the continuity of
  implies that for   potentially smaller           it
holds for all            that

                    

     of       Rt

and that the weight matrix part  Wl  
has full column rank 
Now  by Corollary   for any           there exists  
            such that the generated output matrix  Fk at
layer   of the corresponding network parameters of    satis 
 es rank   Fk            Moreover  it holds for         
that             and the weight matrix part    Wl  
     of
   has full column rank  Assume         corresponds to the
following representation

      vec          bT
      vec   Wk      bT
We obtain the following

            vec   Wk      bT

           vec   WL      bT

 

 cid 
 cid 
               Wk 
               bk 
rank   Wl    nl               
rank   Fk          

   Wl   bl  
   Wl   bl  

  

       Rs
      Rt
 cid 
 cid 

   

  

   

 cid 

 cid 

 cid 

  

   Wl   bl  

Thus  Lemma   implies that    Wl   bl  
   is   global minimum of   Let       
          Note that
this construction can be done for any         In particular  let    
   be   strictly monotonically decreasing
sequence such that       and limr         By Corollary   and the previous argument  we can choose for any
         point  ur            such that  vr    ur  has
full rank and  ur   vr       Moreover  as limr      
  it follows that limr   ur      and as   is   continuous function  it holds with  vr    ur  that limr   vr  
limr   ur     limr   ur             Thus we
get limr ur   vr           and as   is   continuous
function it holds

 cid 

 cid 

    
lim

 ur   vr 

             

as   attains the global minimum for the whole sequence
 ur   vr 

Proof of Theorem   for general case
In the general case                       the previous proof
can be easily adapted  The idea is that we    all layers in
                       In particular  let
     vec       bT
     vec WI     bTI          vec WI       bTI      

            vec Wk     bT

 cid 

    

 cid 

 cid 

 cid 

 cid 

    

      

 Wl  bl  

Let     dim        dim    and          Rs Rt be the
   Let     Rs   Rt  
corresponding vectors at     
    
Rt be   map de ned as             
with
            
   
The only difference
from
                      are hold  xed  They are not contained in the arguments of   thus will not be involved in
our perturbation analysis  In this way  the full rank property of the weight matrices of these layers are preserved 
which is needed to obtain the global minimum 

      
    
is

layers

that

all

the

  

  

  Relaxing the Condition on the Number of

Hidden Units

We have seen that nk         is   suf cient condition which leads to   rather simple structure of the critical
points  in the sense that all local minima which have full
rank in the layers       to   and for which the Hessian is
nondegenerate on any subset of upper layers that includes
layer       are automatically globally optimal  This suggests that suboptimal locally optimal points are either completely absent or relatively rare  We have motivated before
that networks with   certain wide layer are used in practice  which shows that the condition nk         is not
completely unrealistic  On the other hand we want to discuss in this section how it could be potentially relaxed  The
following result will provide some intuition about the case
nk         but will not be as strong as our main result
  which makes statements about   large class of critical
points  The main idea is that with the condition nk     
the data is linearly separable at layer    As modern neural
networks are expressive enough to represent any function 
see  Zhang et al    for an interesting discussion on
this  one can expect that in some layer the training data becomes linearly separable  We prove that any critical point 
for which the  learned  network outputs at any layer are
linearly separable  see De nition   is   global minimum
of the training error 

     Rd from   classes  Cj  

De nition    Linearly separable vectors    set of vecj  is called lintors  xi  
     Rd and
early separable if there exist   vectors  aj  
  xi   bj     for xi   Cj
  scalars  bj  
  xi   bj     for xi   Cj for every                 
and aT

       so that aT

In this section  we use   slightly different loss function than
in the previous section  The reason is that the standard least
squares loss is not necessarily small when the data is linearly separable  Let            Cm denote   classes  We

The Loss Surface of Deep and Wide Neural Networks

Theorem   Let            be de ned as in   and let
the Assumptions   hold  Then it follows 

  Every critical point of   for which the feature vectors
contained in the rows of Fk are linearly separable and
all the weight matrices  Wl  
     have full column
rank is   global minimum 

  If the training inputs are linearly separable then every
critical point of   for which all the weight matrices
   have full column rank is   global minimum 
 Wl  

 

Note that the second statement of Theorem   can be considered as   special case of the  rst statement  In the case
where       and training inputs are linearly separable  the
second statement of our Theorem   recovers the similar
result of  Gori   Tesi    Frasconi et al    for onehidden layer networks 
Even though the assumptions of Theorem   and Theorem   are different in terms of class of activation and loss
functions  their results are related  In fact  it is well known
that if   set of vectors is linearly independent then they are
linearly separable  see          Barber    Thus
Theorem   can be seen as   direct generalization of Theorem   The caveat  which is also the main difference to
Theorem   is that Theorem   makes only statements
for all the critical points for which the problem has become
separable at some layer  whereas there is no such condition
in Theorem   However  we still think that the result is
of practical relevance  as one can expect for   suf ciently
large network that stochastic gradient descent will lead to
  network structure where the data becomes separable at  
particular layer  When this happens all the associated critical points are globally optimal  It is an interesting question
for further research if one can show directly under some
architecture condition that the network outputs become linearly separable at some layer for any local minimum and
thus every local minimum is   global minimum 

  Discussion
the loss surface becomes wellOur results show that
behaved when there is   wide layer in the network 
Implicitly  such   wide layer is often present in convolutional
neural networks used in computer vision  It is thus an interesting future research question how and if our result can
be generalized to neural networks with sparse connectivity  We think that the results presented in this paper are  
signi cant addition to the recent understanding why deep
learning works so ef ciently  In particular  since in this paper we are directly working with the neural networks used
in practice without any modi cations or simpli cations 

  

  

Figure   An example of      

consider the objective function           from  

 cid 

 

 Wl  bl  

  

  cid fLj xi    yij

 cid   

 cid 

  

  

  cid 
  cid 
 cid fLj xi    yij
 cid fLj xi    yij

 cid 
  cid fLj xi    yij
 cid  xi   Cj
 cid  xi   Cj

  
  

 

 cid 

where the loss function now takes the new form

where       penalize the deviation from the label encoding
for the true class resp  wrong classes  We assume that the
minimum of   is attained over    Note that   is bounded
from below by zero as    and    are nonnegative loss functions  The results of this section are made under the following assumptions on the activation and loss function 

Assumptions  

cally increasing 

            and strictly monotoni 

                                          cid 

          and   cid 

               

     

                                          cid 

          and   cid 

               

     

In classi cation tasks  this loss function encourages higher
values for the true class and lower values for wrong classes 
An example of the loss function that satis es Assumption
  is given as  see Figure  

      

        
     
 

      

     
 
        

Note that for    label encoding    for the true
class and   for all wrong classes  one can rewrite   as

 cid 

 cid 

  cid 

  cid 

 

 Wl  bl  

  

 

max      yijfLj xi 

  

  

which is similar to the truncated squared loss  also called
squared hinge loss  used in the SVM for binary classi cation  Since   and   are continuously differentiable  all the
results from Lemma   still hold  Our main result in this
section is stated as follows 

 cid 

 cid 

The Loss Surface of Deep and Wide Neural Networks

Acknowledgment
The authors acknowledge support by the ERC starting
grant NOLEPRO  

References
Auer     Herbster     and Warmuth        Exponentially

many local minima for single neurons  In NIPS   

Ba     and Caruana     Do deep nets really need to be deep 

In NIPS   

Baldi     and Hornik     Neural networks and principle
component analysis  Learning from examples without
local minima  Neural Networks     

Gori     and Tesi     On the problem of local minima in
backpropagation  IEEE Transactions on Pattern Analysis and Machine Intelligence     

Haeffele        and Vidal     Global optimality in
tensor factorization  deep learning  and beyond   
arXiv   

Hartley     and Zisserman     Multiple view geometry
in computer vision  Cambridge University Press  New
York   

Janzamin     Sedghi     and Anandkumar     Beating
the perils of nonconvexity  Guaranteed training of neural networks using tensor methods  arXiv 
 

Barber     Bayesian Reasoning and Machine Learning 

Cambridge University Press  Berlin   

Kawaguchi     Deep learning without poor local minima 

In NIPS   

Brutzkus     and Globerson     Globally optimal gradient descent for   convnet with gaussian inputs   
arXiv 

Caruana     Lawrence     and Giles     Over tting in
neural nets  Backpropagation  conjugate gradient  and
early stopping  In NIPS   

Choromanska     Hena     Mathieu     Arous       
and LeCun     The loss surfaces of multilayer networks 
In AISTATS     

Choromanska     LeCun     and Arous        Open problem  The landscape of the loss surfaces of multilayer
networks  JMLR     

Ciresan        Meier     Gambardella        and
Schmidhuber     Deep  big  simple neural nets for
handwritten digit recognition  Neural Computation   
   

Dauphin     Pascanu     Gulcehre     Cho     Ganguli 
   and Bengio    
Identifying and attacking the saddle point problem in highdimensional nonconvex optimization  In NIPS   

Frasconi     Gori     and Tesi     Successes and failures of backpropagation    theoretical investigation 
Progress in Neural Networks  Architecture   

Krantz        and Parks          Primer of Real Analytic

Functions  Birkh auser  Boston  second edition   

Krizhevsky     Sutskever     and Hinton        Imagenet
classi cation with deep convolutional neural networks 
In NIPS   

LeCun     Boser     Denker       Henderson     Howard 
     Hubbard     and Jackel       Handwritten digit
recognition with   backpropagation network  In NIPS 
 

LeCun     Bengio     and Hinton     Deep learning  Na 

ture     

Lee        Simchowitz     Jordan        and Recht 
   Gradient descent only converges to minimizers  In
COLT   

Lin     Memisevic     and Konda     How far can we
go without convolution  Improving fullyconnected networks  preprint  arXiv   

Marsden 

Elementary
    Freeman and Company   

     

classical analysis 

Milnor     Lectures on HCobordism Theorem  Princeton

University Press  Princeton   

Mityagin     The zero set of   real analytic function   

arXiv 

Gautier     Nguyen     and Hein     Globally optimal
training of generalized polynomial neural networks with
nonlinear spectral methods  In NIPS   

Neyshabur     Salakhutdinov        and Srebro     Pathsgd  Pathnormalized optimization in deep neural networks  In NIPS   

Goodfellow        Vinyals     and Saxe        Qualitatively characterizing neural network optimization problems  In ICLR   

Nguyen       

Complex powers of analytic functions and meromorphic renormalization in qft   
arXiv 

The Loss Surface of Deep and Wide Neural Networks

Poggio     and Liao     Theory ii  Landscape of the empir 

ical risk in deep learning    arXiv 

Rister     and Rubin        Piecewise convexity of arti cial

neural networks    arXiv 

Safran     and Shamir     On the quality of the initial basin

in overspeci ed networks  In ICML   

Sagun     Bottou     and LeCun     Singularity of the

hessian in deep learning    arXiv 

Sima     Training   single sigmoidal neuron is hard  Neural

Computation     

Soltanolkotabi     Learning relus via gradient descent 

  arXiv 

Soudry     and Hoffer     Exponentially vanishing suboptimal local minima in multilayer neural networks 
  arXiv 

Vincent     Larochelle     Lajoie     Bengio     and Manzagol     Stacked denoising autoencoders  Learning useful representations in   deep network with   local denoising criterion  JLMR     

Yu     and Chen     On the local minima free condition of
backpropagation learning  IEEE Transaction on Neural
Networks     

Zhang     Bengio     Hardt     Recht     and Vinyals 
Oriol  Understanding deep learning requires rethinking
generalization  In ICLR   

Zhou     and Feng     The landscape of deep learning algo 

rithms    arXiv 

