Ef cient Regret Minimization in NonConvex Games

Elad Hazan   Karan Singh   Cyril Zhang  

Abstract

We consider regret minimization in repeated
games with nonconvex loss functions  Minimizing the standard notion of regret is computationally intractable  Thus  we de ne   natural notion of regret which permits ef cient optimization and generalizes of ine guarantees for convergence to an approximate local optimum  We
give gradientbased methods that achieve optimal
regret  which in turn guarantee convergence to
equilibrium in this framework 

  Introduction
Repeated games with nonconvex utility functions serve to
model many natural settings  such as multiplayer games
with riskaverse players and adversarial       GAN  training  However  standard regret minimization and equilibria
computation with general nonconvex losses are computationally hard  This paper studies computationally tractable
notions of regret minimization and equilibria in nonconvex
repeated games 
Ef cient online learning algorithms are intimately connected to convexity  This connection is natural  since in
  very broad sense  convexity captures ef cient computation in continuous mathematical optimization 
The recent success of nonconvex learning models  notably deep neural networks  motivates the need for ef 
cient nonconvex optimization  Since the latter is NPhard
in general  ef cient optimization methods for nonconvex
optimization are designed to  nd local minima of varied
quality  Stochastic gradientbased methods for nonconvex
optimization are currently stateof theart in training nonconvex machines 

 Computer Science 

to 

spondence
Karan
 cyril zhang princeton edu 

Princeton University 

CorreElad Hazan  ehazan princeton edu 
Zhang

Cyril

Singh  karans princeton edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

 Though by no means exclusively  PCA and other spectral

methods being   notable exception 

In this paper we investigate the generalization of the nonconvex statistical  or batch  learning model to online learning  The main question we ask is what kind of guarantees can be obtained ef ciently in an adversarial nonconvex
scenario 
After brie   discussing why standard regret is not   suitable metric of performance  we introduce and motivate local regret    surrogate for regret to the nonconvex world 
We then proceed to give ef cient algorithms for nonconvex online learning with optimal guarantees for this new
objective  In analogy with the convex setting  we discuss
the way our framework captures the of ine and stochastic
cases  In the  nal section  we describe   gametheoretic solution concept which is intuitively appealing  and  in contrast to Nash equilibria  ef ciently attainable by simple algorithms 

  Related work

The  eld of online learning is by now rich with   diverse
set of algorithms for extremely general scenarios  see     
 CesaBianchi   Lugosi    For bounded cost functions over   bounded domain  it is well known that versions
of the multiplicative weights method gives nearoptimal regret bounds  Cover    Vovk    Arora et al   
Despite the tremendous generality in terms of prediction 
the multiplicative weights method in its various forms
yields only exponentialtime algorithms for these general
scenarios  This is inevitable  since regret minimization implies optimization  and general nonconvex optimization is
NPhard  Convex forms of regret minimization have dominated the learning literature in recent years due to the fact
that they allow for ef cient optimization  see       Hazan 
  ShalevShwartz   
Nonconvex mathematical optimization algorithms typically  nd   local optimum  For smooth optimization 
gradientbased methods are known to  nd   point with gradient of squared norm at most   in     
    iterations  Nesterov      rate of     
    is known for stochastic
 We note here that we measure the squared norm of the
gradient  since it is more compatible with convex optimization 
The mathematical optimization literature sometimes measures the
norm of the gradient without squaring it 

Ef cient Regret Minimization in NonConvex Games

gradient descent  Ghadimi   Lan    Further accelerations in terms of the dimension are possible via adaptive
regularization  Duchi et al   
Recently  stochastic secondorder methods have been considered  which enable even better guarantees for nonconvex optimization  not only is the gradient at the point
returned small  but the Hessian is also guaranteed to be
close to positive semide nite      
the objective function
is locally almostconvex  see       Erdogdu   Montanari 
  Carmon et al    Agarwal et al       
The relationship between regret minimization and learning
in games has been considered in both the machine learning
literature  starting with  Freund   Schapire    and the
game theory literature by  Hart   MasColell    Motivated by  Hart   MasColell     Blum   Mansour 
  study reductions from internal to external regret  and
 Hazan   Kale    relate the computational ef ciency
of these reductions to  xed point computations 

  Setting
We begin by introducing the setting of online nonconvex
optimization  which is modeled as   game between  
learner and an adversary  During each iteration    the
learner is tasked with predicting xt from     Rn    convex decision set  Concurrently  the adversary chooses   loss
function ft          the learner then observes ft     via
access to    rstorder oracle  and suffers   loss of ft xt 
This procedure of play is repeated across   rounds 
The performance of the learner is measured through its regret  which is de ned as   function of the loss sequence
           fT and the sequence of online decisions            xT
made by the learner  We discuss our choice of regret measure at length in Section  
Throughout this paper  we assume the following standard
regularity conditions 
Assumption   We assume the following is true for each
loss function ft 
    ft is bounded   ft        
 ii  ft is LLipschitz   ft      ft        cid       cid 
 iii  ft is  smooth  has    Lipschitz gradient 
 cid ft       ft   cid     cid       cid 

bounded  local information may provide no information
about the location of   stationary point  This motivates us
to re ne our search criteria 
Consider  for example  the function sketched in Figure  
In this construction  de ned on the hypercube in Rn  the
unique point with   vanishing gradient is   hidden valley 
and gradients outside this valley are all identical  Clearly 
it is hopeless in an informationtheoretic sense to  nd this
point ef ciently  the number of value or gradient evaluations of this function must be exp    to discover the
valley 

Figure     dif cult  needle in   haystack  case for constrained
nonconvex optimization  Left    function with   hidden valley  with small gradients shown in yellow  Right  Regions with
small projected gradient for the same function  For smaller  
only points near the valley and bottomleft corner have small projected gradient 

To circumvent such inherently dif cult and degenerate
cases  we relax our conditions  and try to  nd   vanishing projected gradient  In this section  we introduce this
notion formally  and motivate it as   natural quantity of interest to capture the search for local minima in constrained
nonconvex optimization 
De nition    Projected gradient  Let           be
  differentiable function on   closed  but not necessarily
bounded  convex set     Rn  Let       We de ne
           Rn  the      projected gradient of    by

        

def
 

 
 

                     

where     denotes the orthogonal projection onto   
This can be viewed as   surrogate for the gradient which
ensures that the gradient descent step always lies within   
by transforming it into   projected gradient descent step 
Indeed  one can verify by de nition that

                            

  Projected Gradients and Constrained NonConvex

Optimization

In particular  when     Rn 

In constrained nonconvex optimization  minimizing the
gradient presents dif cult computational challenges 
In
general  even when objective functions are smooth and

          

 
 

                        

and we retrieve the usual gradient at all   

Ef cient Regret Minimization in NonConvex Games

We  rst note that there always exists   point with vanishing
projected gradient 
Proposition   Let   be   compact convex set  and suppose           satis es Assumption   Then  there
exists some point        for which

            

Proof  Consider the map            de ned by

     def                                 

This is   composition of continuous functions  noting that
the smoothness assumption implies that    is continuous 
and is therefore continuous  Thus   satis es the conditions
for Brouwer    xed point theorem  implying that there exists some        for which           At this point  the
projected gradient vanishes 
In the limit where  cid      cid  is in nitesimally small  the
projected gradient is equal to the gradient in the interior of
   on the boundary of    it is the gradient with its outwardfacing component removed  This exactly captures the  rstorder condition for   local minimum 
The  nal property that we note here is that an approximate
local minimum  as measured by   small projected gradient 
is robust with respect to small perturbations 
Proposition   Let   be any point in     Rn  and let     
be differentiable functions        Then  for any      

 cid           cid     cid        cid     cid     cid 

Proof  Let                and               De ne
their respective projections   cid               cid           so
that   cid                 and   cid                   
We  rst show that  cid   cid      cid cid     cid       cid 
By the generalized Pythagorean theorem for convex sets 
we have both  cid   cid      cid        cid cid      and  cid   cid      cid        cid cid   
  Summing these  we get

 cid   cid      cid    cid      cid           cid     

   cid   cid      cid cid     cid   cid      cid        cid 

   cid   cid      cid cid     cid       cid 
as claimed  Finally  by the triangle inequality  we have

this

implies

that

particular 

immediately

In
fact
 cid        cid     cid      cid 
As we demonstrate later  looking for   small projected gradient becomes   feasible task  In Figure   above  such  
point exists on the boundary of    even when there is no
 hidden valley  at all 

    Local Regret Measure

In the wellestablished framework of online convex optimization  numerous algorithms can ef ciently achieve optimal regret  in the sense of converging in terms of average
loss towards the best  xed decision in hindsight  That is 
for any        one can play iterates            xT such that

  cid 

  

 
 

 ft xt    ft        

Unfortunately  even in the of ine case  it is too ambitious
to converge towards   global minimizer in hindsight 
In
the existing literature  it is usual to state convergence guarantees towards an  approximate stationary point   that is 
there exists some iterate xt for which  cid    xt cid      As
discussed in the previous section  the projected gradient is
  natural analogue for the constrained case 
In light of the computational intractability of direct analogues of convex regret  we introduce local regret    new
notion of regret which quanti es the objective of predicting points with small gradients on average  The remainder
of this paper discusses the motivating roles of this quantity 
Throughout this paper  for convenience  we will use the
following notation to denote the slidingwindow time average of functions    parametrized by some window size
           

  cid 

  

Ft      def 

 
 

ft     

For simplicity of notation  we de ne ft    to be identically
zero for all       We de ne local regret below 
De nition    Local regret  Fix some       De ne the
wlocal regret of an online algorithm as

 cid           cid     cid        cid 

   cid                      cid 

Rw    

def
 

 cid   Ft   xt cid 

 cid   cid      cid cid 
 cid       cid     cid     cid 

 
 
 
   
 

as required 

When the window size   is understood by context  we omit
the parameter  writing simply local regret as well as Ft   
We turn to the  rst motivating perspective on local regret 
When an algorithm incurs local regret sublinear in       randomly selected iterate has   small timeaveraged gradient
in expectation 

  cid 

  

Ef cient Regret Minimization in NonConvex Games

Proposition   Let            xT be the iterates produced
by an algorithm for online nonconvex optimization which
incurs   local regret of Rw     Then 

 cid cid   Ft   xt cid cid    Rw    

Et Unif    

 

  An Ef cient NonConvex Regret

Minimization Algorithm

 

ft      bound on local regret truly captures   guarantee of
playing points with small gradients 

This generalizes typical convergence results for the gradient in of ine nonconvex optimization  we discuss concrete
reductions in Section  

  Why Smoothing is Necessary

Our approach  as given in Algorithm   is to play followthe leader iterates  approximated to   suitable tolerance
using projected gradient descent  We show that
this
method ef ciently achieves an optimal local regret bound

 cid  taking         iterations 

of   cid   

  

  

In this section  we show that for any online algorithm  an
adversarial sequence of loss functions can force the local

regret incurred to scale with   as  cid   

 cid  This demon 

strates the need for   timesmoothed performance measure
in our setting  and justi es our choice of larger values of
the window size   in the sections that follow 
Theorem   De ne         For any      
            and       there exists   distribution   on
 smooth   bounded cost functions            fT on   such
that for any online algorithm  when run on this sequence of
functions 

 cid   

 cid 

 

  

ED  Rw        
  

Proof  We begin by partitioning the   rounds of play into
 cid   
   cid  repeated segments  each of length    
For the  rst half of the  rst segment                   the
adversary declares that

  For odd    select ft           as follows 

 cid    with probability  

 
with probability  
 

  

ft     

  For even    ft       ft   

During the second half                        the adver 
   cid 
sary sets all ft        This construction is repeated  cid   
times  padding the  nal   mod    costs arbitrarily with
ft       
By this construction  at each round   at which ft    is
drawn randomly  we have Ft        ft      Furthermore  for any xt played by the algorithm   ft xt      with
probability at least  
     
The claim now follows from the fact that there are at least
 
in total 

  so that   cid cid   Ft   xt cid cid     
 cid  segments

  of these rounds per segment  and exactly cid   

  

We further note that the notion of timesmoothing captures nonconvex online optimization under limited concept drift  in online learning problems where Ft       

tolerance         convex body     Rn 

Algorithm   Timesmoothed online gradient descent
  Input  window size       learning rate          
   
  Set        arbitrarily 
  for                 do
 
 
 
 
 
  end for

Predict xt  Observe the cost function ft         
Initialize xt    xt 
while  cid   Ft   xt cid       do

Update xt    xt       Ft   xt 

end while

Theorem   Let            fT be the sequence of loss functions presented to Algorithm   satisfying Assumption  
Then 

    The wlocal regret incurred satis es

Rw                
    

 ii  The total number of gradient steps   taken by Algo 

rithm   satis es

   

 

 cid 

 
     

 

 cid   cid         cid   

Proof of     We note that Algorithm   will only play an
iterate xt if  cid   Ft   cid         Note that at      
   ft      ft      which
Ft   is zero  Let ht       
   Lipschitz  Then  for each           we have  
is   
bound on each cost

 cid   Ft   xt cid     cid     Ft      ht     xt cid 

   cid   Ft   cid     cid ht xt cid 
 

       

 

 

 

 cid 

 cid   

 

  
 

  

where the  rst inequality follows from Proposition  
Summing over all   gives the desired result 

Ef cient Regret Minimization in NonConvex Games

Proof of  ii  First  we require an additional property of the
projected gradient 
Lemma   Let     Rn be   closed convex set  and let
      Suppose           is differentiable  Then  for any
      

 cid             cid     cid        cid 

Proof  Let                and   cid           Then 

 cid             cid 

   cid        cid 
   cid          cid      cid     cid   cid         cid      cid 
   cid       cid    cid      cid     

 

 

inequality follows by the generalized

where the last
Pythagorean theorem 
For             let    be the number of gradient steps taken
in the outer loop at iteration       in order to compute the
iterate xt  For convenience  de ne       We establish  
progress lemma during each gradient descent epoch 
Lemma   For any            

Ft xt    Ft xt      

     
 

 cid 

 cid   

    

Proof  Consider   single iterate   of the inner loop  and
         Ft    We have  by
the next iterate   cid 
 smoothness of Ft 
Ft   cid    Ft       cid Ft      cid      cid   
     cid Ft     Ft   cid   
Thus  by Lemma  
Ft   cid    Ft       

 cid   cid      cid 
 cid   Ft   cid 

 cid   Ft   cid 

 
 

 cid 

 cid 

 
 

     
 

The algorithm only takes projected gradient steps when
 cid   Ft   cid        Summing across all    consecutive iterations in the epoch yields the claim 

To complete the proof of the theorem  we write the telescopic sum  understanding         

  cid 

FT  xT    

Ft xt    Ft xt 

 

  cid 
    cid 

  

  

  

Ft xt    Ft xt    ft xt    ft   xt 

 Ft xt    Ft xt   

    

 

 

Using Lemma   we have

FT  xT         
 

 

 cid 

     
 
  

 cid 

 

    cid 

  

   

whence

   

  cid 

  

    

 

 cid 
 cid 

 

 

as claimed 

  
     
 
     

 

 

 cid 

 cid      
 cid   
 cid   cid         cid   

 

  FT  xT  

Setting       and       gives the asymptotically optimal local regret bound  with        timeaveraged gradient steps  and thus        individual gradient oracle
calls  We further note that in the case where     Rn 
one can replace the gradient descent subroutine  the inner
loop  with nonconvex SVRG  AllenZhu   Hazan   
achieving   complexity of        gradient oracle calls 

  Implications for Of ine and Stochastic

NonConvex Optimization

In this section  we discuss the ways in which our online
framework generalizes the of ine and stochastic versions
of nonconvex optimization   that any algorithm achieving
  small value of Rw     ef ciently  nds   point with small
gradient in these settings  For convenience  for        
  cid        we denote by       cid  the uniform distribution on
time steps   through   cid  inclusive 

  Of ine nonconvex optimization

For of ine optimization on    xed nonconvex function
           we demonstrate that   bound on local regret translates to convergence  In particular  using Algorithm   one  nds   point       with  cid        cid     

 cid  calls to the gradient oracle  matching

while making   cid   

the best known result for the convergence of gradientbased
methods 
Corollary   Let           satisfy Assumption  
When online algorithm   is run on   sequence of   identical loss functions       it holds that for any            

 

Et        cid      xt cid    Rw   
     
 cid   

In particular  Algorithm   with parameter choices    
         

           and            

    yields

 

Et Dw    cid      xt cid     

Furthermore  the algorithm makes   cid   

 cid  calls to the gra 

 

dient oracle in total 

Ef cient Regret Minimization in NonConvex Games

When an online algorithm incurs small local regret in expectation  it has   convergence guarantee in of ine stochastic nonconvex optimization 
Proposition   Let             Suppose that online algorithm   is run on   sequence of   identical
loss functions       satisfying Assumption   with identical stochastic gradient oracles satisfying Assumption  
Sample             Then  over the randomness of   and
the oracles 

  cid cid    xt cid cid       Rw   
 cid cid    xt cid cid   

 cid  
    cid    xt cid 

     

 

Proof  Observe that

Et        

  Rw   
     
The claim follows by taking the expectation of both sides 
over the randomness of the oracles 

     

 

For   concrete onlineto stochastic reduction  we consider
Algorithm   which exhibits such   bound on expected local regret 

Algorithm   Timesmoothed online gradient descent with
stochastic gradient oracles
  Input  learning rate       window size      
  Set          Rn arbitrarily 
  for                 do
 
 
  end for

Predict xt  Observe the cost function ft   Rn     
Update xt    xt    

 cid   
    cid       xt 

 

Theorem   Let            ft satisfy Assumption   Then 
Algorithm   with access to stochastic gradient oracles

 cid     xt  satisfying Assumption   and   choice of    

 

    guarantees

   Rw      cid      cid   

 

 

Furthermore  Algorithm   makes   total of        calls to
the stochastic gradient oracles 

Using this expected local regret bound in Proposition  
we obtain the reduction claimed at the beginning of the section 
Corollary   Algorithm   with parameter choices    
    

          and      

 

    yields

  cid cid    xt cid cid     
 cid   

 cid 

stochastic gra 

Furthermore  the algorithm makes  
dient oracle calls in total 

 

Proof  Since ft            for all   
it follows that
Ft              for all        As   consequence  we
have

Et Dw   cid      xt cid     

 cid      xt cid 

  cid 
     
  Rw   
     

  

 

With the stated choice of parameters  Theorem   guarantees that

Et Dw   cid      xt cid     
 

 

 

     

   

Also  since the loss functions are identical  the execution
of line   of Algorithm   requires exactly one call to the
gradient oracle at each iteration  This entails that the total
number of gradient oracle calls made in the execution is
                 

   

  Stochastic nonconvex optimization

 

 cid 

We examine the way in which our online framework captures stochastic nonconvex optimization of    xed function     Rn      in which an algorithm has access to  

noisy stochastic gradient oracle  cid       We note that the
 cid   

reduction here will only apply in the unconstrained case 
it becomes challenging to reason about the projected gradient under noisy information  From   local regret bound 
we recover   stochastic algorithm with oracle complexity
  We note that this blackbox reduction recovers an
 
optimal convergence rate in terms of   but not  
In the setting  the algorithm must operate on the noisy estimates of the gradient as the feedback  In particular  for
any ft that the adversary chooses  the learning algorithm
is supplied with   stochastic gradient oracle for ft  The
discussion in the preceding sections may be viewed as  
special case of this setting with       We list the assumptions we make on the stochastic gradient oracle  which are
standard 
Assumption   We assume that each call to the stochas 

tic gradient oracle yields an        random vector  cid      
    Unbiased    cid cid      
 ii  Bounded variance    cid cid cid               cid cid     

with the following properties 

        

 cid 

Ef cient Regret Minimization in NonConvex Games

  An Ef cient Algorithm with SecondOrder

Guarantees

We note that by modifying Algorithm   to exploit secondorder information  our online algorithm can be improved to
play approximate  rstorder critical points which are also
locally almost convex  This entails replacing the gradient
descent epochs with   cubicregularized Newton method
 Nesterov   Polyak    Agarwal et al     
In this setting  we assume that we have access to each ft
through   value  gradient  and Hessian oracle  That is 
once we have observed ft  we can obtain ft     ft   
and  ft    for any    Let MinEig    be the minimum
 eigenvalue  eigenvector  pair for matrix    As is standard
for of ine secondorder algorithms  we must add the following additional smoothness restriction 
Assumption   ft is twice differentiable and has an   
Lipschitz Hessian 

 cid               cid      cid       cid 

Additionally  we consider only the unconstrained case
where     Rn  the secondorder optimality condition is
irrelevant when the gradient does not vanish at the boundary of   
The secondorder Algorithm   uses the same approach as
in Algorithm   but terminates each epoch under   stronger
approximateoptimality condition  We de ne

        max

 cid Ft   cid   
   
 

the quantity  cid  

so that
      xt  is termwise lower
bounded by the costs in Rw     but penalizes local concavity 
We characterize the convergence and oracle complexity
properties of this algorithm 
Theorem   Let            fT be the sequence of loss functions presented to Algorithm   satisfying Assumptions  
and   Choose       Then  for some constants      
in terms of           
    The iterates  xt  produced by Algorithm   satisfy

 cid 

 cid 

   min Ft   

 

  cid 

  

   xt          
    

 ii  The total number of iterations   of the inner loop taken

by Algorithm   satis es

             

Predict xt  Observe the cost function ft   Rn     
Initialize xt    xt 
while    xt        do
Update xt    xt     

 Ft   xt 

Let        MinEig cid Ft   xt cid 

Algorithm   Timesmoothed online Newton method
  Input  window size       tolerance      
  Set        arbitrarily 
  for                 do
 
 
 
 
 
 
 
 
 
 
 
 
 
  end for

Flip the sign of   so that  cid   Ft   xt cid     
Compute yt    xt    
  
if Ft   yt    Ft   xt  then

Set xt    yt 

if       then

end while

end if

end if

  

Proof of     For each             we have

   xt     
    

Let ht       
   Lipschitz and  
  

   xt    max

   ft      ft      Then  since ht    is

   smooth 

 cid cid Ft xt     ht xt cid 
   min Ft xt     ht xt cid 
 cid cid   
 cid   

 cid 

   
   
 

 cid cid 

 

  max

 

  
 

  

 

 

 
   
 

 

 

 
 

which is bounded by      for some             
 
The claim follows by summing this inequality across all
           

Proof of  ii  We  rst show the following progress lemma 
Lemma   Let      cid  be two consecutive iterates of the
inner loop in Algorithm   during round    Then 

Ft   cid    Ft             
 

 

Proof  Let   denote the step   cid       Let      Ft   
     Ft    and        MinEig   
Suppose that at time    the algorithm takes   gradient step 
so that        Then  by secondorder smoothness of Ft 
we have

Ft   cid    Ft       cid      cid   

 
 

 cid   cid       
 

 cid   cid 

Ef cient Regret Minimization in NonConvex Games

 cid   

    cid   
 cid cid cid cid cid   
 cid cid   

  

  

Supposing instead that the algorithm takes   secondorder
   whichever sign makes  cid      cid     
step  so that        
the thirdorder smoothness of Ft implies

  

Ft   cid    Ft       cid      cid   
   cid      cid   
   
   
 

 

 
 
 
 
 
 

uT Htu  
 cid   cid   
   
   
 

 

  
 

 cid   cid 
  
 
 cid   cid 

The lemma follows due to the fact that the algorithm takes
the step that gives   smaller value of Ft   cid 

The rest of the proof follows the same structure as that for
part  ii  of Theorem   summing Lemma   implies  
statement analogous to Lemma   which we telescope
over all epochs  For sake of completeness  we give the
proof in the appendix 

    Solution Concept for NonConvex Games
Finally  we discuss an application of our regret minimization framework to learning in kplayer    round iterated
games with smooth  nonconvex payoff functions  Suppose
that each player         has    xed decision set Ki   Rn 
and    xed payoff function fi         satis es Assumption   as before  Here    denotes the Cartesian product
of the decision sets Ki  each payoff function is de ned in
terms of the choices made by every player 
In such   game  it is natural to consider the setting where
players will only consider small local deviations from their
strategies  This is   natural setting  which models risk aversion  This setting lends itself to the notion of   local equilibrium  to replace the stronger condition of Nash equilibrium    joint strategy in which no player encounters   large
gradient on her utility  However   nding an approximate
local equilibrium in this sense remains computationally intractable when the utility functions are nonconvex 
Using the idea timesmoothing  we formulate   tractable relaxed notion of local equilibrium  de ned over some time
window    Intuitively  this de nition captures   state of
an iterated game in which each player examines the past
  actions played  and no player can make small deviations
to improve the average performance of her play against her
opponents  historical play  We formulate this solution concept as follows 
De nition    Smoothed local equilibrium  Fix some
   be
the payoff functions for   kplayer iterated game   
joint strategy    
    is an  approximate     
smoothed local equilibrium with respect to past iterates

            Let  cid fi            xk          cid  

            xk

             xk

if  for every player        

 cid 

 cid cid cid cid cid     

 fi    
 

 xi
  

where

 fi   cid   

def
  fi   

  cid          xi 
  cid 

     xi 

  cid 

          xk

  cid 

To achieve such an equilibrium ef ciently  we use Algorithm   which runs   copies of any online algorithm that
achieves   wlocal regret bound for some      

Algorithm   Timesmoothed game simulation
  Input  convex decision sets           Kk   Rn  payoff
functions fi              Kk       online algorithm   
window size            
  Initialize   copies            Ak  of   with window
size    where each Ai plays on decision set Ki 

  for                 do
Each Ai outputs xi
  
 
Show each Ai the online loss function
 

fi         fi   

            xi 

 

     xi 

 

          xk

   

  end for

We show this metaalgorithm yields   subsequence of iterates that satisfy our solution concept  with error parameter
dependent on the local regret guarantees of each player 
Theorem   For some   such that             the
    produced by Algorithm   is an
joint strategy    
 approximate     smoothed local equilibrium with re 

            xk

spect to cid   

             xk

  where

    cid   
 cid cid cid cid    cid 

  

  

   

Rw Ai    
     

 

The proof  which we give in the appendix  follows from
the same method as for the reductions in Section   after
summing the regret bounds from each Ai 

  Concluding Remarks
We have described how to extend the theory of online learning to nonconvex loss functions  while permitting ef cient
algorithms  Our de nitions give rise to ef cient online and
stochastic nonconvex optimization algorithms that converge to local optima of  rst and second order  We give
  game theoretic solution concept which we call local equilibrium  which  in contrast to existing solution concepts
such as Nash equilibrium  is ef ciently attainable in any
nonconvex game 

Ef cient Regret Minimization in NonConvex Games

Ghadimi  Saeed and Lan  Guanghui  Stochastic  rstand
zerothorder methods for nonconvex stochastic programming  SIAM Journal on Optimization   
 

Hart  Sergiu and MasColell  Andreu    simple adaptive
procedure leading to correlated equilibrium  Econometrica     

Hazan  Elad 

Introduction to online convex optimizaFoundations and Trends in Optimization   
doi   
URL http dx doi org 

tion 
   
 
 

ISSN  

Hazan  Elad and Kale  Satyen  Computational equivalence
of  xed points and no regret algorithms  and convergence to equilibria  In Platt       Koller     Singer    
and Roweis      eds  Advances in Neural Information
Processing Systems   pp    MIT Press  Cambridge  MA    URL http books nips cc 
papers files nips NIPS pdf 

Nesterov  Yurii  Introductory lectures on convex optimization  volume   Springer Science   Business Media 
 

Nesterov  Yurii and Polyak  Boris    Cubic regularization
of newton method and its global performance  Mathematical Programming     

ShalevShwartz  Shai  Online learning and online convex optimization  Foundations and Trends in Machine
Learning     

Vovk  Volodimir    Aggregating strategies  In Proceedings
of the Third Annual Workshop on Computational Learning Theory  COLT   pp     

Acknowledgements
This material is based upon work supported by the National
Science Foundation under Grant No    The authors
gratefully acknowledge Naman Agarwal  Brian Bullins 
Matt Weinberg  and Yi Zhang for helpful discussions 

References
Agarwal  Naman  AllenZhu  Zeyuan  Bullins  Brian 
Hazan  Elad  and Ma  Tengyu  Finding approximate local minima for nonconvex optimization in linear time 
arXiv preprint arXiv     

Agarwal  Naman  Bullins  Brian  and Hazan  Elad  Second order stochastic optimization for machine learning
in linear time  arXiv preprint arXiv     

AllenZhu  Zeyuan and Hazan  Elad  Variance reduction
In Proceedings of
for faster nonconvex optimization 
The  rd International Conference on Machine Learning  pp     

Arora  Sanjeev  Hazan  Elad  and Kale  Satyen  The multiplicative weights update method    metaalgorithm
Theory of Computing   
and applications 
   
 toc     
URL http www theoryofcomputing org 
articles     

doi 

Blum     and Mansour     From external to internal regret 

In COLT  pp     

Carmon  Yair  Duchi  John    Hinder  Oliver  and Sidford 
Aaron  Accelerated methods for nonconvex optimization  arXiv preprint    

CesaBianchi  Nicol   and Lugosi    abor 

Prediction 
Learning  and Games  Cambridge University Press 
 

Cover  Thomas  Universal portfolios  Math  Finance   

   

Duchi  John  Hazan  Elad  and Singer  Yoram  Adaptive
subgradient methods for online learning and stochastic
optimization  The Journal of Machine Learning Research     

Erdogdu  Murat   and Montanari  Andrea  Convergence
rates of subsampled newton methods  In Advances in
Neural Information Processing Systems  pp   
 

Freund  Yoav and Schapire  Robert      decisiontheoretic
generalization of online learning and an application to
boosting     Comput  Syst  Sci    August
 

