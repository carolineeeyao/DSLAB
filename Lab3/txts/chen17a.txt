Robust Structured Estimation with SingleIndex Models

Sheng Chen   Arindam Banerjee  

Abstract

In this paper  we investigate general singleindex
models  SIMs  in high dimensions  Based on
Ustatistics  we propose two types of robust estimators for the recovery of model parameters 
which can be viewed as generalizations of several existing algorithms for onebit compressed
sensing  bit CS  With minimal assumption on
noise  the statistical guarantees are established
for the generalized estimators under suitable conditions  which allow general structures of underlying parameter  Moreover  the proposed estimator is novelly instantiated for SIMs with monotone transfer function  and the obtained estimator can better leverage the monotonicity  Experimental results are provided to support our theoretical analyses 

 

  Introduction
In machine learning and statistics    linear model of the form      cid 
     is widely used to  nd the relationship between feature and response  which has gained overwhelming popularity for   very long time  Here       and     Rp
is the pair of observed response and feature measurement
    Rp is the unvector    is   zeromean noise  and  cid 
known parameter to be estimated  The simplicity of linear
model leads to its great interpretability and computational
ef ciency  which are often favored in practical applications  On theoretical side  even in highdimensional regime
where sample size is smaller than the problem dimension
   strong statistical guarantees have been established under mild assumptions for various estimators  such as Lasso  Tibshirani    and Dantzig selector  Candes   Tao 
  Despite its attractive merits  one main drawback
of linear models is the stringent assumption of linear relationship between   and    which may fail to hold in com 

 Department of Computer Science   Engineering  University of MinnesotaTwin Cities  Minnesota  USA  Correspondence to  Sheng Chen  shengc cs umn edu  Arindam Banerjee  banerjee cs umn edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

plicated scenarios  To introduce more  exibility  one option is to consider the general singleindex models  SIMs 
 Ichimura    Horowitz   Hardle   

          

 

 

 cid 

      

 

 
        is an unknown univariate transfer funcwhere  
tion         link function  This class of models enjoys rich
modeling power in the sense that it encompasses several
useful models as special cases  which are brie   described
below 

  Onebit Compressed Sensing 

In onebit compressed sensing  bit CS   Boufounos   Baraniuk 
  Plan   Vershynin    the response   is restricted to be binary             and the range
  is     Given the measureof transfer function  
 
ment vector    one can generate   from the Bernoulli
model 

 

  Ber

     

 

 

 cid 

 

 

        
 

 

 

 
In the noiseless case   
re ects the true sign of  cid 
rect for other  
level in some way 

      sign    and   always
     while   can be incor 
 
  whose shape determines the noise
  Generalized Linear Models  In generalized linear
models  GLMs   McCullagh    the transfer function is assumed to be monotonically increasing and
conditional distribution of     belongs to exponen 
  give rise to diftial family  Different choices of  
  is identity function
ferent members in GLMs  If  
 
         one has the simple linear models  while
 
the sigmoid function  
   cid   results in the logistic model for binary classi cation  In this work 
  other than
however  we have no access to exact  
knowing it is monotonic 
  Noise in Monotone Transfer  Instead of having the
general expectation form of   as GLMs  one could directly introduce the noise inside monotone transfer   
to model the randomness of    Plan et al   

       

 

        cid 

 

          

 

In this setting  the transfer function    is slightly different from the  
     
              

  in   which are related by  

 

Robust Structured Estimation with SingleIndex Models

the recovery of  cid 

  key advantage of SIM is its robustness  First  allowing
  prevents the misspeci cation of transfer funcunknown  
 
tion  which could otherwise lead to   poor estimate of  cid 
Secondly  the model in   makes minimal assumption on
the distribution of    thus being able to tolerate potentially
heavytailed noise 
  we are given   measurements of
In order to estimate  cid 
         Rp      denoted by  xi  yi  
   In this work 
we focus on the       regime  In such highdimensional
  is quite challenging as the
setting 
  is given  Over the last
problem is illposed even when  
decade  substantial progress has been made to address the
challenge by exploiting the apriori structure of parameter
  like sparsity  Tibshirani    For simple linear
 cid 
models or GLMs with known transfer  extensive studies
  can be consistently estimated
have shown that sparse  cid 
under mild assumptions  with much lower sample complexity than    Candes   Tao    Wainwright   
Bickel et al    Kakade et al    Negahban et al 
  Yang et al    Recently the notion of structure
has been suitably generalized beyond the unstructured
sparsity  Bach et al    and Gaussian width  Gordon 
  has emerged as   useful measure to characterize
the structural complexity which further determines the
   Chandrasekaran et al   
recovery guarantee of  cid 
Rao et al    Oymak et al    Amelunxen et al 
  Banerjee et al 
 
Vershynin    Tropp    Chen   Banerjee   
In the absence of exact  
though  bit CS and
related variants were wellstudied in recent years
 Boufounos   Baraniuk 
 
Plan   Vershynin    Gopi et al    Zhang et al 
  Chen   Banerjee      Zhu   Gu    Yi et al 
  Slawski   Li    Li    Slawski   Li 
  the exploration of general SIMs or the cases with
monotone transfers is relatively limited  especially in
the highdimensional regime  Kalai   Sastry   and
Kakade et al 
investigated the lowdimensional
and they proposed
SIMs with monotone transfers 
 
  and  cid 
perceptrontype algorithms to estimate both  
with provable guarantees on prediction error 
In high
dimension  general SIMs were studied by Alquier   Biau
  and Radchenko   in which only unstructured
  is considered  The algorithm developed
sparsity of  cid 
relies on reversible jump
in  Alquier   Biau   
MCMC  which could be slow 
In Radchenko    
 
  and  cid 
path  tting algorithm is designed to recover  
but only asymptotic guarantees are provided  Ganti et al 
  considered the highdimensional setting with
monotone transfer  and their iterative algorithm is based
on nonconvex optimization 
is hard to
establish the convergence  Besides  the prediction error
bound they derived is also weak  in the sense that it

  Chatterjee et al 

for which it

 

 

 

Jacques et al 

 

     General structure of  cid 

      or   norm regularization term  cid 

is even worse than the initialization of the algorithm 
Recently Oymak   Soltanolkotabi
  proposed  
  with
constrained leastsquares method to estimate  cid 
recovery error characterized by Gaussian width and related
quantities  Though their analysis considered the general
  it only holds for noiseless setting where
structure of  cid 
       cid 
  was also explored
in Vershynin   and Plan et al    Other types of
statistical guarantees for highdimensional SIMs is also
  in Neykov et al 
available  such as support recovery of  cid 
  It is worth noting that all the aforementioned statistical analyses rely on subGaussian noise or the transfer
function being bounded or Lipschitz  which indicates that
none of the results can immediately hold for heavytailed
noise  or without Lipschitzness and boundedness 
 
In this paper  we focus on the parameter estimation of  cid 
instead of the prediction of   given new    In particular  we
propose two families of generalized estimators  constrained
and regularized  for model   under Gaussian measure 
  is assumed to possess certain lowment  The parameter  cid 
complexity structure  which can be either captured by  
 
constraint  cid 
Our general approach is inspired by Ustatistics and the advances in  bit CS  and subsumes several existing  bit CS
algorithms as special cases  Similar to those algorithms 
our estimator is simple and often admits closedform solutions  Regarding the recovery analysis  there are two appealing aspects  First our results work for general structure  with error bound characterized by Gaussian width and
some other easyto compute geometric measures  Instanti 
  recovers preating our results with speci   structure of  cid 
viously established error bounds for  bit CS  Zhang et al 
  Chen   Banerjee      which are sharper than
those yielded by the general analysis in Plan   Vershynin
  Second  our analysis works with limited assumptions on the condition distribution of    In particular  our
estimator is robust to heavytailed noise and permit un 
  as well as nonLipschitz
bounded transfer functions  
ones  At the heart of our analysis is the generic chaining
method  Talagrand    an advanced tool in probability
theory  which has been successfully applied to sparse recovery  Koltchinskii    and dimensionality reduction
 Dirksen    etc  Another key ingredient in our proof is
  Hoeffdingtype concentration inequality for Ustatistics
 Lee    with subGaussian tails  which is less known
yet generalizes the popular one for bounded Ustatistics
 Hoeffding    Apart from  bit CS  we particularly
investigate the model   for which the generalized estimator is specialized in   novel way  The resulting estimator
better leverages the monotonicity of the transfer function 
which is also demonstrated through experiments  For the
ease of exposition  whenever we say  monotone  it means  monotonically increasing  by default  Throughout the

Robust Structured Estimation with SingleIndex Models

 

paper  we will use        
        and so on to denote absolute constants  which may differ from context to context 
Detailed proofs are deferred to the supplementary material
due to page limit 
The rest of the paper is organized as follows  In Section
  we introduce our estimators for SIMs along with their
recovery guarantees  We also provide   few examples in
 bit CS for illustration  Section   is focused on model  
for which we instantiate the general results in   new way 
  beyond unstructured sparsity are also
Other structures of  cid 
discussed  Section   provides the proof of our main results and the related lemmas 
In Section   we complement
our theoretical developments with some experiment results  The  nal section is dedicated to conclusions 

  Generalized Estimation for Structured

Parameter

  Assumptions and Preliminaries

 

In contrast  when  

For the sake of identi ability  we assume         
that
 cid 
      throughout the paper  At the  rst glimpse
of model   we may realize that it is dif cult to recov 
  due to unknown  
  is giver  cid 
  can be established under
en  the recovery guarantees of  cid 
mild assumptions of   and    such as boundedness or subGaussianity  If we know certain properties of the transfer
function like the monotonicity introduced in GLMs and  
  is largely restricted  and it is tempting
the structure of  
to expect that similar results will continue to hold  Unfortunately  we  rst have the following claim  which in 
  beyond strict
dicates that without other constraints on  
  cannot be consistently estimated under
monotonicity   cid 
general subGaussian  or bounded  measurement  even in
the noiseless setting of  

Claim   Suppose that each element xi of   is sampled
from Rademacher distribution         xi      
      
  xi         Under model   with noise      
there exists    cid cid    Sp  together with   monotone  cid    such
  and yi    cid      cid cid  xi  for dathat supp   cid cid    supp cid 
ta  xi  yi  
   with arbitrarily large sample size    while
   cid cid     cid 

      for some constant  

 

  is not possible for
Now that consistent estimation of  cid 
general subGaussian measurement  it might be reasonable
to focus on certain special cases  For this work  we assume
that   is standard Gaussian        For SIM   we additionally assume that the distribution of   depends on  
only through the value of  cid 
          the distribution of
    is  xed if  cid 
     is given  no matter what the exact  
is  This assumption is quite minimal  and it turns out that
the examples we provide in Section   all satisfy it  if noise
  is independent of   in   The same assumption is used

 

 

  

in Plan et al    as well 
Under the assumptions above  given          observations
                xm  ym  we de ne

  

                  xm  ym   

qi             ym    xi  
 
where all qi   Rm     are bounded functions with  qi   
  which are chosen along with   based on the properties
of the transfer function  In Section   and   we will see
examples for their choices  The vector     Rp is critical
due to the key observation below 

Lemma   Suppose the distribution of   in model   depends on   through  cid 

     and we de ne accordingly

 

 

bi             zm   cid 

 
   qi             ym cid 
  xm    zm   
With   being standard Gaussian          de ned in  
satis es

                 cid 

   

 

 

                     xm  ym     cid 

where    
           gm are        standard Gaussian 

  bi             gm   cid 

 
  

 

 
 
    gi  and

 

 

Note that Lemma   is true for all choices of qi  and the
proof is given in the supplement  This lemma presents an
insight towards the design of our estimator  that is  the di 
  can be approximated if we have   good sense
rection of  cid 
about Eu  As we will see in the sequel  the scalar   plays
  key role in the estimation error bound  which can give us
clues to the choice of qi  We can assume          that      
since we can  ip the sign of each qi 
The recovery analysis is built on the notion of Gaussian
width  Gordon    which is de ned for any     Rp
as          supv         where   is   standard Gaussian random vector  Roughly speaking       measures the
scaled width of set   averaged over each direction 

  Generalized Estimator

Inspired by Lemma   we de ne the vector    for the ob 
 
served data  xi  yi  

  

       

   xi    yi           xim  yim    

    

  

   im  
  im

 
which is an unbiased estimator of Eu  meaning that      
Eu    cid 

  When       we essentially have

    

 

       

   xi  yi   xj  yj 

 

 

      

   

Robust Structured Estimation with SingleIndex Models

      cid         

In fact     can be treated as   vector version of Ustatistics
  is
with order    Given       naive way to estimate  cid 
to simply normalize    
In high 
  is often structured  but the naive
dimensional setting   cid 
estimator fails to take such information into account  which
would lead to large error  To incorporate the prior knowl 
  we design two types of estimator  the conedge on  cid 
strained one and the regularized one 
Constrained Estimator  If we assume that  cid 
some structured set     Sp  then the estimation of  cid 
carried out via the constrained optimization

  belongs to
  is

       cid 

      cid       

 cid    argmin
 cid Rp

 
Here the set   can be nonconvex  as long as the optimization can be solved globally  Since the objective function is
very simple  we can often end up with   global minimizer 
Similar estimator has been used in Plan et al    but
they only focused on speci      
Regularized Estimator  If we assume that the structure of
  can be captured by certain norm       we may alterna 
 cid 
tively use the regularized estimator to  nd  cid 

 

 cid    argmin
 cid Rp

       cid     cid      

 cid       

 

The optimization is convex  thus the global minimum is
always attained  Previously this estimator was used in  bit
CS scenario with    norm  Zhang et al   

  Recovery Analysis

  re 

 

 

   

 cid cid cid       cid     cid 

Regarding the constrained estimator  the recovery of  cid 
lies on the geometry of  cid  which is described by
AK cid 
The set AK cid 
  that error  cid     cid 
characterizes the error of  cid 

 
  essentially contains all possible direction 
  could lie in  The following theorem

   cid     

    cone

Sp 

 

 

 

Theorem   Suppose that
the optimization   can be
solved to global minimum  Then the following error bound
holds for the minimizer  cid  with probability at least    
 
 

 

exp

 

 

 

     AK cid 
 cid cid cid   cid     cid 
 cid cid cid 

 

       

 

 

 

    AK cid 
 
 
     
 

 

 

 

where   is the subGaussian norm of   standard Gaussian
  are all absolute constant 
random variable  and     
Remark  Note that estimator is consistent as long as    
  The error bound inversely depends on the scale of  

   

which implies that we should construct suitable qi such
that   is large according to its de nition in Lemma   The
choice of qi further depends on the assumed property of
  Though dependency on   may prevent us from using
 
higherorder      is typically small in practice and can be
treated as constant 
For regularized estimator  we can similarly establish the recovery guarantee in terms of Gaussian width 

 
 cid cid cid        cid 
Theorem   De ne the following set for any      
    cid 
 
   
   
If we set           cid 
  
and it satis es         then with probability at least
 
     

Sp 
 
            

   

     cid 

   
 

   

    cone

exp

 

 

 

   cid  in   satis es
     
   cid     cid 
 
 
 

 

           

 

 

 cid cid cid   cid     cid 

 

 cid cid cid 

   

 
 
    and     

where  cid     cid 
             is the unit ball of norm      

    supv   cid cid cid 

 

 

 

  here the set   cid 

Remark  The geometry of the regularized estimator is slightly different from the constrained one  Instead of having AK cid 
  depends on the choice
of the regularization parameter   The same phenomenon
also appears in the  Banerjee et al    The geometric
measure  cid     cid 
  is called restricted norm compatibility  which is nonrandom  For many interesting cases  it is
easy to calculate  Negahban et al    Chen   Banerjee 
   

 

  Application to  bit CS

For  bit CS problem   the   de ned in   can be chosen
with       and qi   yi  ending up with

           yx and

    

 
 

yixi  

 

  

  

 

 

By such choice of    the   de ned in Lemma   is simply        
      with   being standard Gaussian random vector  Under reasonably mild noise    is likely to
take the sign of the linear measurement  which means that
 
    should be close to    or   if   is positive  or negf
ative  Thus we expect  
     to be positive most of time
and   to be large  Given the choice of    we can specialize our generalized constrained regularized estimator to
  is assumed to be ssparse 
obtain previous results  If  cid 
for constrained estimator  we can choose   straightforward
     cid     cid      Sp  which results in the ksupport
norm estimator  Chen   Banerjee     
       cid        cid      cid       

 cid ks   argmin
 cid Rp

Robust Structured Estimation with SingleIndex Models

 

Though   is nonconvex  the global minimizer can actually
be obtained in closed form 
   
 uj      
    otherwise

if  uj  is in    

 ks
   

 

  

  

where     is the absolutevalue counterpart of    with entries sorted in descending order  and the subscript takes the
top   entries  Similarly if the regularized estimator is instantiated with    norm       we obtain the socalled passive algorithm introduced in Zhang et al   

 cid ps   argmin
 cid Rp

       cid     cid        cid       

 

whose solution is given by  cid ps                   
where    is the elementwise softthresholding operator 
Si        max sign ui ui    Based on Theorem
  and   we can easily obtain the error bound for both ksupport norm estimator and passive algorithm 
Corollary   Assume that  xi  yi  
   follow  bit CS
 
model in   and    is given as   For any ssparse  cid 
 
with high probability   cid  produced by both   and  
       cid ks and  cid ps  satisfy

 

 cid cid cid   cid     cid 

 

 cid cid cid 

   

 

  log  

 

 

The proof is included in the supplementary material 
The above result was shown by Slawski   Li   and
Zhang et al    but their analyses do not consider the
general structure  Compared with     
  log      yielded by the general result in Plan   Vershynin   our
bound is much sharper 

 

    New Estimator for Monotone Transfer
In this section  we speci cally study model   Here we
further assume that    is strictly increasing  What is worth
mentioning is that the estimator we develop here can be
applied to GLMs as well  To avoid the confusion with  
and    de ned previously  we instead use new notations  
and    respectively in this section 

  Estimator with SecondOrder   

To motivate the design of    it is helpful to rewrite model
  by applying the inverse of    on both sides 

       cid 
  

 

          

 

Note that the new formulation resembles the linear model
    Inexcept that we have no access to the value of   
 yn    
              
stead  all we know about         
Rn is that it preserves the ordering of                 yn    

 

Put in another way    needs to satisfy the constraint that
ri   rj iff  yi   yj and ri   rj iff  yi   yj  To
move one step further  it is equivalent to sign yi   yj   
sign ri rj    sign cid 
  xi xj      based on model assumption  Hence the information contained in sample
 xi  yi  
   can be interpreted from the perspective of  
bit CS  where sign yi   yj  re ects the perturbed sign of
linear measurement  cid 
  xi   xj  Inspired by the   for
 bit CS  we may choose       and de ne       as
 
                  sign                       
 

   xi  yi   xj  yj   

    

 

 

       

      

   

Given the de nition of     Lemma   directly implies the
following corollary 

 

 

Corollary   Suppose that        and        are generated by model   where       follow Gaussian distribution        and the noise     are independent of      
and identically  but arbitrarily  distributed  Then the expectation of                 satis es
 
 

 

 

 

                    
  Eg    
 

 
 cid 
 
         
 

sign

 

  serves as the role of   in LemRemark  The scalar
  is always guaranteed to be strictly positive
ma   and  
  dis 
 
regardless how the noise is distributed  which keeps  cid 
tinguishable all the time  To see this  let          
 
Note that   is symmetric  thus   has the same distribution
as   where   is   Rademacher random variable  Therefore

 

     

 

 

where  

 

 

     sign               Eg     sign            
    Eg

sign          sign       

   

 

 

 

Since                             it follows that
sign           sign            sign         sign    
    sign        thus  sign         sign           is
always nonnegative  Find   large enough       such that
                 and we have

 

 

     sign                 Eg               
   Eg                  
 

                  
 
  achieve its maximum 
In the ideal noiseless case   
     
max     sign              
 
In the worst
 
case  if   and   are heavytailed and dominate    then
     
 

 
     
 

     

 
 

 

sign

 

Robust Structured Estimation with SingleIndex Models

 

Now we can instantiate the generalized estimator based on
    For example  if  cid 

  is ssparse  we estimate it by
 
       cid        cid      cid     

 

 cid    argmin
 cid Rp

  log    

which enjoys  
error rate as shown in
Corollary   The regularized estimator can also be obtained
with the same    according to   The bottleneck of computing  cid  lies in the calculation of       simple proposition
below enables us to get    in   fast manner 
Proposition   Given  xi  yi  
tion of              such that   
  
we have
                 

  be the permuta 
  Then
            

   let  
    

    

 

 
 

 
 

 
 

 

       

 
 

  

Remark  Based on the proposition above     can be ef 
ciently computed in   np     log    time           log   
time for sorting   and   np  time for the weighted sum of
all xi  This is   signi cant improvement compared with the
the naive calculation using   which takes        time 

  Beyond Unstructured Sparsity

 
  

 cid Gi

 

So far we have illustrated the Gaussian width based error bounds  viz   and   only through unstructured
  Here we provide two more examples  nonsparsity of  cid 
overlapping group sparsity and fused sparsity 
NonOverlapping Group Sparsity  Suppose the coordi 
  has been partitioned into   prede ned disjoint
nates of  cid 
groups           GK                  out of which only  
groups are nonzero  If we use the regularized estimator
  the optimal sowith    norm  cid   
lution can be similarly obtained as   with elementwise
softthresholding replaced by the groupwise one  The related geometric measures that appears in   can be found
in Banerjee et al    which are given by
 
  

 
 
  is said to be sfused sparse if the carFused Sparsity   cid 
dinality of the set   cid 
 
 
  
is smaller than    If we resort to the constrained estimator
  with      cid       cid        cid      the associated optimization can be solved by dynamic programming
 Bellman    The proposition below upper bounds the
corresponding Gaussian width   AK cid 
Proposition   For sfused sparse  cid 
 
of set AK cid 
satis es
      

  the Gaussian width
  with      cid        cid        cid     
  AK cid 

 
      
 
      

                 

   

 cid     cid 

  in  

log    

   

  log   

 

  

 
 

 

 

 

 

 

 

The proof can be found in  Slawski   Li    and we
provide   different one in supplementary material 

  Lemmas and Proof Sketch of Theorem  
Here we  rst present the important technical lemmas that
will be used in the proof of Theorem   The  rst one is the
Hoeffdingtype inequality for subGaussian Ustatistics 
In the literature  most of the studies are centered around
bounded Ustatistics  for which the celebrated concentration is established by Hoeffding   Yet it is not easy
to locate the counterpart for subGaussian case  Therefore
we provide the following result and attach   proof in the
supplementary material 

Lemma    Concentration for subGaussian Ustatistics 
De ne the Ustatistic
       

 

   zi          zim   

Un       

  

   im  
    im

with order   and kernel     Rd       based on   independent copies of random vector     Rd  denoted by
     zn  If            is subGaussian with    
   
 
then the following inequality holds for Un      with any
 
 
     
   Un        EUn              exp

 
  

 
 

   
 
 

 

in which   is an absolute constant 

As mentioned earlier in Section   generic chaining is the
key tool that our analysis relies on  Speci cally we utilize
Theorem   from  Talagrand   

Lemma    Generic chaining concentration  Given metric space        
if an associated stochastic process
 Zt     has subGaussian incremental       satis es the
condition
   Zu   Zv          exp
 

              
 
 

then the following inequality holds

   

       

 

 

 

 

 Zu   Zv                     diam        

 

sup
     

 

 

     exp

 

 

 

     and    are all absolute constants 

where     
The de nition of the above  functional   is complicated  and is not of great importance  We refer interested

Robust Structured Estimation with SingleIndex Models

reader to the books  Talagrand     Loosely speaking          can be thought of as   measure for the
size of set   under metric    What really matters is the
following relationship between  functional and Gaussian
width   see Theorem   in Talagrand  

Lemma    Majorizing measures theorem  For any set
    Rp  the  functional          metric and Gaussian
width satisfy the following inequality with an absolute constant   

                       

 

Equipped with these lemmas  we are ready to present the
proof sketch of Theorem     complete proof is deferred
to the supplementary material 
Proof Sketch of Theorem   We use the shorthand notation AK for the set AK cid 
 
  As  cid  attains the global minimum of   we have
 cid     cid 
   cid     cid 
           
   
     cid   cid 

           cid     cid 

   
   cid 

   cid 

 

 

 

   cid 

  
 

sup

  

 

 

 

 

 

 

 

  AK 

  
 

In order to bound the supremum above  we use the result from generic chaining  We de ne the stochastic
   AK  First  we
process  Zv              cid 
need to check the process has subGaussian incremental 
For simplicity  we denote    xi    yi             xim  yim  by
ui im  By the de nitions and properties of subGaussian
norm  Vershynin    it is not dif cult to show that
               for any       
 ui im        
 
 
AK     By Lemma   we have
   Zv   Zw          exp
  
Therefore we can conclude that  Zv  has subGaussian
        
 
incremental       
  
   Now applying Lemma   to  Zv  with   bit
 
calculation  we can obtain

the metric               

            

  

   

 

 

 

sup

  AK 

   

   AK        
 

 
 

 

 

 

 
 

 
 Zv         
     exp
 

 

Using    AK                   AK     implied
by Lemma   and taking        AK     we get
     AK      
 
     AK 
 
 

with probability at least        exp
  The inequality also uses the fact that    AK          AK   

       

  AK 

   cid 

  
 

sup

  

 

 

 

   which is   result of Lemma   in Maurer et al   
 See Lemma   in supplementary material  Lastly we turn
to the quantity    cid     cid 
   cid     cid 

       cid   cid          

 

 

   

 

 

     AK      
 

 
 

     and

 

We  nish the proof by letting          
 

    

 

  Experiment
 
In the experiment  we focus on model   with sparse  cid 
The problem dimension is  xed as       and the   
  is set to   Essentially we generate our data
parsity of  cid 
       from

        cid 

 

          

where            and             ranges from
  to   We choose three monotonically increasing    
             exp     which is bounded and Lipschitz               which is unbounded and nonLipschitz 
and          log    exp     which is unbounded but
Lipschitz  The sample size   varies from   to  
We use the estimator   in Section   The baselines
we compare with is the SILO and iSILO algorithm introduced in  Ganti et al    SILO does not quite take
the monotonicity in account 
In fact  it is the special
case of our generalized constrained estimator which uses
the constraint set  cid     cid     
the same choice of   as  bit CS  The original SILO use
    cid      which
is computationally less ef cient and statistically no better
than      cid     cid         Sp   Zhang et al   
Chen   Banerjee      Hence we also use   in SILO
for   fair comparison  iSILO relies on   speci   implementation of isotonic regression which explicitly restricts the
Lipschitz constant of    to be one  To    iSILO into our
setting  we remove the Lipschitzness constraint and perform the standard isotonic regression  Since the convergence
is not guaranteed for the iterative procedure of iSILO  the
number of its iterations is  xed to   The best tuning
parameter of iSILO is obtained by grid search 
The experiment results are shown in Figure   Overall
the iSILO algorithm works well under small noise  while
our estimator has better performance when the variance of
noise increases  To better demonstrate the robustness of
our estimator to heavytailed noise  instead of Gaussian
noise  we sample   from the Student     distribution with
degrees of freedom equal to   We repeat the experiments
for             and obtain the plots in Figure   We can see
that the error of our estimator consistently decreases for all
choice of   as   increases  For SILO and iSILO  the errors
are relatively large  and unable to shrink for large   even
when more data are provided 

Robust Structured Estimation with SingleIndex Models

    Error for              exp cid   

    Error for          log    exp   

Figure Recovery error vs  sample size     Our estimator has similar performance compared with iSILO  both of which outperform
SILO by   large margin      iSILO has smaller error when  cid  is small  while our estimator works better in highnoise regime     The
error of SILO is reduced compared with other     but iSILO fails to give further improvement over SILO when  cid  is large  Our estimator
still outperforms them when  cid   cid   

    Error for            

Figure  Recovery error vs  sample size  with             under heavytailed noise

  Conclusion
In this paper  we study the parameter estimation for the
highdimensional singleindex models  We propose two
classes of robust estimators  which generalize previous
works in two aspects  First we allow the diverse structure
      binary  monotone and etc  of the transfer function 
which can help us customize the estimators  Secondly the
structure of the true parameter can be general  either encoded by   constraint or   norm  With limited assumption on the noise  we can show that the estimation error can
be bounded by simple geometric measures under Gaussian

measurement  which subsumes the existing results for speci   settings  The experiment results also validate our theoretical analyses 

Acknowledgements
We thank Sreangsu Acharyya for helpful discussions related to the paper  The research was supported by NSF grants IIS  IIS  IIS  IIS 
  CCF  CNS    IIS 
IIS  NASA grant NNX AQ    and gifts from
Adobe  IBM  and Yahoo 

   Recoveryerror                             our estimatorSILOiSILO   Recoveryerror                             our estimatorSILOiSILO   Recoveryerror                             our estimatorSILOiSILO   Recoveryerror                             our estimatorSILOiSILORobust Structured Estimation with SingleIndex Models

References
Alquier     and Biau     Sparse singleindex model  Journal of Machine Learning Research     

Amelunxen     Lotz     McCoy        and Tropp       
Living on the edge  Phase transitions in convex programInform  Inference   
  with random data 
 

Bach     Jenatton     Mairal     and Obozinski     Optimization with sparsityinducing penalties  Foundations
and Trends    in Machine Learning     
Banerjee     Chen     Fazayeli     and Sivakumar     Estimation with norm regularization  In Advances in Neural Information Processing Systems  NIPS   

Bellman     On the approximation of curves by line segments using dynamic programming  Communications of
the ACM     

Ganti     Rao     Willett        and Nowak     Learning
single index models in high dimensions  arXiv preprint
arXiv   

Gopi     Netrapalli     Jain     and Nori     Onebit compressed sensing  Provable support and vector recovery 
In Proceedings of The  th International Conference on
Machine Learning   

Gordon     Some inequalities for gaussian processes and
applications  Israel Journal of Mathematics   
   

Hoeffding     Probability inequalities for sums of bounded
random variables  Journal of the American statistical
association     

Horowitz        and Hardle     Direct semiparametric
estimation of singleindex models with discrete covariates  Journal of the American Statistical Association   
   

Bickel        Ritov     and Tsybakov        Simultaneous
analysis of Lasso and Dantzig selector  The Annals of
Statistics     

Ichimura    

Semiparametric least squares  sls  and
weighted sls estimation of singleindex models  Journal
of Econometrics     

Boufounos       and Baraniuk         bit compressive
In Information Sciences and Systems   

sensing 
CISS    nd Annual Conference on   

Candes     and Tao     The Dantzig selector  statistical
estimation when   is much larger than    The Annals of
Statistics     

Chandrasekaran     Recht     Parrilo        and Willsky 
      The convex geometry of linear inverse problems 
Foundations of Computational Mathematics   
   

Chatterjee     Chen     and Banerjee     Generalized
dantzig selector  Application to the ksupport norm 
In Advances in Neural Information Processing Systems  NIPS   

Chen     and Banerjee     Onebit compressed sensing
with the ksupport norm  In International Conference on
Arti cial Intelligence and Statistics  AISTATS     

Chen     and Banerjee     Structured estimation with atomic norms  General bounds and applications  In Advances
in Neural Information Processing Systems     

Chen     and Banerjee     Structured matrix recovery via
the generalized dantzig selector  In Advances in Neural
Information Processing Systems   

Dirksen     Dimensionality reduction with subgaussian
matrices    uni ed theory  Foundations of Computational Mathematics     

Jacques     Laska        Boufounos        and Baraniuk 
      Robust  bit compressive sensing via binary stable embeddings of sparse vectors  IEEE Transactions on
Information Theory     

Kakade     Shamir     Sindharan     and Tewari 
   Learning exponential families in highdimensions 
In Proceedings of the
Strong convexity and sparsity 
Thirteenth International Conference on Arti cial Intelligence and Statistics   

Kakade        Kanade     Shamir     and Kalai     Ef 
 cient learning of generalized linear and single index
models with isotonic regression  In Advances in Neural
Information Processing Systems   

Kalai       and Sastry     The isotron algorithm  High 

dimensional isotonic regression  In COLT   

Koltchinskii     Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems  Lecture Notes
in Mathematics  Springer Berlin Heidelberg   

Lee        UStatistics  Theory and Practice  Taylor  

Francis   

Li     One scan  bit compressed sensing  In Proceedings
of the  th International Conference on Arti cial Intelligence and Statistics   

Maurer     Pontil     and RomeraParedes     An Inequality with Applications to Structured Sparsity and
Multitask Dictionary Learning  In Conference on Learning Theory  COLT   

Robust Structured Estimation with SingleIndex Models

Vershynin     Introduction to the nonasymptotic analysis
of random matrices  In Eldar     and Kutyniok      eds  Compressed Sensing  chapter   pp    Cambridge University Press   

Vershynin     Estimation in High Dimensions    Geometric Perspective  pp    Springer International Publishing   

Wainwright        Sharp thresholds for noisy and highdimensional recovery of sparsity using  constrained
IEEE Transactions on
quadratic programming Lasso 
Information Theory     

Yang     Wang     Liu     Eldar        and Zhang    
Sparse nonlinear regression  Parameter estimation under
nonconvexity  In Proceedings of the  nd International
Conference on Machine Learning   

Yi     Wang     Caramanis     and Liu     Optimal linear
estimation under unknown nonlinear transform  In Advances in Neural Information Processing Systems   

Zhang     Yi     and Jin     Ef cient algorithms for robust
onebit compressive sensing  In Proceedings of the  st
International Conference on Machine Learning  ICML 
   

Zhu     and Gu     Towards   Lower Sample Complexity
In Proceedfor Robust Onebit Compressed Sensing 
ings of the  nd International Conference on Machine
Learning   

McCullagh     Generalized linear models  European Jour 

nal of Operational Research     

Negahban     Ravikumar     Wainwright        and Yu 
     uni ed framework for the analysis of regularized
Mestimators  Statistical Science     

Neykov     Liu        and Cai       regularized least
squares for support recovery of high dimensional single
index models with gaussian designs     Mach  Learn 
Res     

Oymak     and Soltanolkotabi     Fast and reliable parameter estimation from nonlinear observations  arXiv
preprint arXiv   

Oymak     Thrampoulidis     and Hassibi    

The
squarederror of generalized lasso    precise analysis 
In Communication  Control  and Computing  Allerton 
   st Annual Allerton Conference on   

Plan     and Vershynin     Robust  bit compressed sensing and sparse logistic regression    convex programming approach  IEEE Transactions on Information Theory     

Plan     Vershynin     and Yudovina    

Highdimensional estimation with geometric constraints  Information and Inference   

Radchenko     High dimensional single index models 

Journal of Multivariate Analysis     

Rao     Recht     and Nowak     Universal Measurement Bounds for Structured Sparse Signal Recovery  In
International Conference on Arti cial Intelligence and
Statistics  AISTATS   

Slawski     and Li     bbit marginal regression  In Advances in Neural Information Processing Systems   

Slawski     and Li     Linear signal recovery from bbit quantized linear measurements  precise analysis of
the tradeoff between bit depth and number of measurements  arXiv preprint arXiv   

Talagrand     The Generic Chaining  Springer   

Talagrand     Upper and Lower Bounds for Stochastic

Processes  Springer   

Tibshirani     Regression shrinkage and selection via the
Lasso  Journal of the Royal Statistical Society  Series   
   

Tropp        Convex recovery of   structured signal from
independent random linear measurements  In Sampling
Theory    Renaissance   

