GramCTC  Automatic Unit Selection and Target Decomposition for Sequence

Labelling

Hairong Liu    Zhenyao Zhu    Xiangang Li   Sanjeev Satheesh  

Abstract

Most existing sequence labelling models rely on
   xed decomposition of   target sequence into
  sequence of basic units  These methods suffer
from two major drawbacks    the set of basic
units is  xed  such as the set of words  characters or phonemes in speech recognition  and  
the decomposition of target sequences is  xed 
These drawbacks usually result in suboptimal
performance of modeling sequences  In this paper  we extend the popular CTC loss criterion
to alleviate these limitations  and propose   new
loss function called GramCTC  While preserving the advantages of CTC  GramCTC automatically learns the best set of basic units  grams  as
well as the most suitable decomposition of target sequences  Unlike CTC  GramCTC allows
the model to output variable number of characters at each time step  which enables the model
to capture longer term dependency and improves
the computational ef ciency  We demonstrate
that the proposed GramCTC improves CTC in
terms of both performance and ef ciency on the
large vocabulary speech recognition task at multiple scales of data  and that with GramCTC we
can outperform the stateof theart on   standard
speech benchmark 

  Introduction
In recent years  there has been an explosion of interest in
sequence labelling tasks  Connectionist Temporal Classi 
 cation  CTC  loss  Graves et al    and Sequenceto sequence  seq seq  models  Cho et al    Sutskever
et al    present powerful approaches to multiple applications  such as Automatic Speech Recognition  ASR 
 Chan et al      Hannun et al    Bahdanau et al 

 Equal contribution

 Baidu Silicon Valley AI Lab   
Bordeaux Dr  Sunnyvale  CA   USA  Correspondence to 
Hairong Liu  liuhairong baidu com 

Proceedings of the  th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  machine translation    bastien et al    and
parsing  Vinyals et al    These methods are based on
     xed and carefully chosen set of basic units  such as
words  Sutskever et al    phonemes  Chorowski et al 
  or characters  Chan et al      and      xed
and predetermined decomposition of target sequences into
these basic units  While these two preconditions greatly
simplify the problems  especially the training processes 
they are also strict and unnecessary constraints  which usually lead to suboptimal solutions  CTC models are especially harmed by  xed basic units in target space  because
they build on the independence assumption between successive outputs in that space   an assumption which is often
violated in practice 
The problem with  xed set of basic units is obvious  it is
really hard  if not impossible  to determine the optimal set
of basic units beforehand  For example  in English ASR 
if we use words as basic units  we will need to deal with
the large vocabularysized softmax  as well as rare words
and data sparsity problem  On the other hand  if we use
characters as basic units  the model is forced to learn the
complex rules of English spelling and pronunciation  For
example  the  oh  sound can be spelled in any of following ways  depending on the word it occurs in        oa 
oe  ow  ough  eau  oo  ew   While CTC can easily model
commonly cooccuring grams together  it is impossible to
give roughly equal probability to many possible spellings
when transcribing unseen words  Most speech recognition
systems model phonemes  subphoneme units and senones
    
 Xiong et al      to get around these problems 
Similarly  stateof theart neural machine translation systems use presegmented word pieces       Wu et al     
aiming to  nd the best of both worlds 
In reality  groups of characters are typically cohesive units
for many tasks  For the ASR task  words can be decomposed into groups of characters that can be associated with
sound  such as  tion  and  eaux  For the machine translation task  there may be values in decomposing words as
root words and extensions  so that meaning may be shared
explicitly between  paternal  and  paternity  Since this
information is already available in the training data  it is
perhaps  better to let the model  gure it out by itself  At
the same time  it raises another import question  how to de 

GramCTC

compose   target sequence into basic units  This is coupled
with the problem of automatic selection of basic units  thus
also better to let the model determine  Recently  there are
some interesting attempts in these directions in the seq seq
framework  For example  Chan et al  Chan et al     
proposed the Latent Sequence Decomposition to decompose target sequences with variable length units as   function of both input sequence and the output sequence 
In this work  we propose GramCTC     strictly more general version of CTC   to automatically seek the best set
of basic units from the training data  called grams  and
automatically decompose target sequences into sequences
of grams  Just as sequence prediction with cross entropy
training can be seen as special case of the CTC loss with
   xed alignment  CTC can be seen as   special case of
GramCTC with    xed decomposition of target sequences 
Since it is   loss function  it can be applied to many seq seq
tasks to enable automatic selection of grams and decomposition of target sequences without modifying the underlying networks  Extensive experiments on multiple scales of
data validate that GramCTC can improve CTC in terms of
both performance and ef ciency  and that using GramCTC
the models outperform stateof thearts on standard speech
benchmarks 

  Related Work
The basic text units that previous works utilized for text
prediction tasks       automatic speech recognition  handwriting recognition  machine translation  and image captioning  can be generally divided into two categories  handcrafted ones and learningbased ones 
Handcrafted Basic Units 
Fixed sets of characters
 graphemes   Graves et al    Amodei et al   
wordpieces  Wu et al      Collobert et al   
Zweig et al      words  Soltau et al      bastien
et al    and phonemes  Lee and Hon    Sercu and
Goel    Xiong et al      have been widely used as
basic units for text prediction  but all of them have drawbacks  Using these  xed deterministic decompositions of
text sequences de nes   prior  which is not necessarily optimal for endto end learning 
  Wordsegmented models remove the component of
learning to spell and thus enable direct optimization towards reducing Word Error Rate  WER  However  these
models suffer from having to handle   large vocabulary
  million in  Soltau et al    outof vocabulary
words  Soltau et al      bastien et al    and
data sparsity problems  Soltau et al   

  Using characters results in much smaller vocabularies
        for English and thousands for Chinese  but it requires much longer contexts compared to using words or
wordpieces and poses the challenge of composing characters to words  Graves et al    Chan et al   

which is very noisy for languages like English 

  Wordpieces lie at the middleground of words and characters  providing   good tradeoff between vocabulary
size and context size  while the performance of using
word pieces is sensitive to the choice of the wordpiece
set and its decomposition 

  For the ASR task  the use of phonemes was popular
in the past few decades as it eases acoustic modeling
 Lee and Hon    and good results were reported
with phonemic models  Xiong et al      Sercu and
Goel    However  it introduces the uncertainties
of mapping phonemes to words during decoding  Doss
et al    which becomes less robust especially for
accented speech data 

Learningbased Basic Units  More recently  attempts
have been made to learn basic unit sets automatically 
 Luong and Manning    proposed   hybrid WordCharacter model which translates mostly at the word level
and consults the character components for rare words 
Chan et al  Chan et al      proposed the Latent Sequence Decompositions framework to decomposes target
sequences with variable lengthed basic units as   function
of both input sequence and the output sequence 
There exist some earlier works on the  unit discovery  task
 Cartwright and Brent    Goldwater et al     
standard problem with MLE solutions to this task is that
there are degenerate solutions       predicting the full corpus with probability   at the start  Often Bayesian priors or
 minimum description length  constraints are used to remedy this 

  GramCTC
  CTC
CTC  Graves et al    is   very popular method in
seq seq learning since it does not require the alignment
information between inputs and outputs  which is usually
expensive  if not impossible  to obtain 
Since there is no alignment information  CTC marginalizes
over all possible alignments  That is  it tries to maximize
       where   is input  and   represent  
valid alignment  For example  if the size of input is   and
the output is  hi  whose length is   there are three possible
alignments   hi   hi  and  hi  where   represents blank 
For the details  please refer to the original paper  Graves
et al   

          cid 

  From CTC to GramCTC
In CTC  the basic units are  xed  which is not desirable in
some applications  Here we generalize CTC by considering
  sequence of basic units  called gram  as   whole  which
is usually more reasonable in many applications 

GramCTC

Figure   Illustration of the states and the forwardbackward transitions for the label  CAT  Here we let   be the set of all unigrams and
bigrams of the English alphabet  The set of all valid states   for the label      CAT  are listed to the left  The set of states and transitions
that are common to both vanilla and GramCTC are in black  and those that are unique to GramCTC are in orange  In general  any
extension that collapses back to   is   valid transition   For example  we can transition into  CAT    from  CAT     CA     CA 
  and  CA    but not from  CAT    or  CAT   

Let   be   set of ngrams of the set of basic units   of the
target sequence  and   be the length of the longest gram in
     GramCTC network has   softmax output layer with
    units  that is  the probability over all grams in   and
one additional symbol  blank  To simplify the problem  we
also assume         
For an input sequence   of length     let     Nw    be
the sequence of network outputs  and denote by yt
  as the
probability of the kth gram at time    where   is the index
of grams in   cid         blank  then we have

  cid 

      

      cid  

yt
  

 

  

Just as in the case of CTC  here we refer to the elements
of   cid   as paths  and denote them by   which represents
  possible alignment between input and output  The difference is that for each word in the target sequence  it may
be decomposed into different sequences of grams  For example  the word  hello  can only be decomposed into the
sequence                     for CTC  assume unigram
CTC here  but it also can be decomposed into the sequence
 he   ll      if  he  and  ll  are in   
For each   we map it into   target sequence in the same
way as CTC using the collapsing function that   removes
all repeated labels from the path and then   removes all
blanks  Note that essentially it is these rules which de 

 This is because there may be no valid decompositions for
some target sequences if    cid     Since GramCTC will  gure
out the ideal decomposition of target sequences into grams during
training  this condition guarantees that there is at least one valid
decomposition for every target sequence 

termine the transitions between the states of adjacent time
steps in Figure   This is   manyto one mapping and we
denote it by    Note that other rules can be adopted here
and the general idea presented in this paper does not depend
on these speci   rules  For   target sequence         represents all paths mapped to    Then  we have

        

    

 

 cid 

     

This equation allows for training sequence labeling models without any alignment information using CTC loss  because it marginalizes over all possible alignments during
training  GramCTC uses the same effect to enable the
model to marginalize over not only alignments  but also
decompositions of the target sequence 
Note that for each target sequence    the set      has
     more paths than it does in CTC  This is because
there are      times more valid states per time step  and
each state may have   valid transition from      states in
the previous time step  The original CTC method is thus 
  special case of GramCTC when       and      
While the quadratic increase in the complexity of the algorithm is non trivial  we assert that it is   trivial increase in
the overall training time of typical neural networks  where
the computation time is dominated by the neural networks
themselves  Additionally  the algorithm extends generally
to any arbitrary   and need not have all possible ngrams
up to length  

  The ForwardBackward Algorithm
To ef ciently compute        we also adopt the dynamic
programming algorithm  The essence here is identifying

 	  	blank ATCblankACAblankTATblankC  	 State cid 

  

 cid 

similarly  we can de ne the backward variable       as 

  cid 

yt cid 
   cid   

     

def
 

     lb        

  cid  

For the initialization and recursion of       we have

  yT

 
yT
gi
 
 

        

      
 
    si
 
otherwise

                

 

and

       

 

 

     yt
  
when       
   
             yt
      
gj
 
when     sj

                sj
      

    

  and gj

   cid  gj
       yt
gj
 
    gj

   

when     sj

  and gj

   cid 
 cid 

where   

      sj

    

  BackPropagation
Similar to CTC  we have the following expression 
                 

        

         

   

yt
sg

 

 

 

 

  and gj

      to sj

      to gj

the states of the problem  so that we may solve future
states by reusing solutions to earlier states  In our case  the
state must contain all the information required to identify
all valid extensions of an incomplete path   such that the
collapsing function will eventually collapse the complete
  back to    For GramCTC  this can be done by collapsing all but the last element of the path   Therefore  the
state is   tuple          where the  rst item is   collapsed
path  representing   pre   of the target label sequence  and
                is the length of the last gram  li     
used for making the pre          is valid and means that
blank was used  We denote the gram  li      by gj
     
and the state          as sj
      For readability  we will further shorten sj
    For   state    its
corresponding gram is denoted by sg  and the positions of
the  rst character and last character of sg are denoted by
     and      respectively  During dynamic programming 
we are dealing with sequence of states  for   state sequence
  its corresponding gram sequences is unique  denoted by
   
Figure   illustrates partially the dynamic programming process for the target sequence  CAT  Here we suppose  
contains all possible unigrams and bigrams  Thus  for
each character in  CAT  there are three possible states associated with it    the current character    the bigram
ending in current character  and   the blank after current
character  There is also one blank at beginning  In total we
have   states 
Supposing the maximum length of grams in   is   we  rst
scan   to get the set   of all possible states  such that for all
       its corresponding gj
      cid                  and
sj
                For   target sequence    de ne the forward
variable       for any       to the total probability of all
 cid 
valid paths pre xes that end at state   at time   

  cid 

yt cid 
   cid   

 

     

def
 

              

  cid 

Following this de nition  we have the following rules for
initialization

    

 
  
gi
 
 

      
 
    si
otherwise

                  

 

     

and recursion

       

 

 

     yt
  
when       
   
    
             yt
gj
 
when     sj
  and gj

    
                sj

when     sj

  and gj

    

   cid  gj
       yt
gj
 
    gj
   

GramCTC

   cid 

      sj

    and yt

where   
at time   
The total probability of the target sequence   is then expressed in the following way 

  is the probability of blank

        

    sj   

The derivative with regards to yt

       
 yt
 

 

 
yt
 

 

  is 

 cid 

  lab     

         

 

where lab       is the set of states in   whose corresponding gram is    This is because there may be multiple states
corresponding to the same gram 
For the backpropagation  the most important formula is the
partial derivative of loss with regard to the unnormalized
  
output ut

 cid 

  yt

     
yt
kZt

  lab     

         

 

  ln       

 ut
 

 cid 

def

 

where Zt

   

         

yt

sg

 

GramCTC

    Training curves before  blue  and after  orange  autore nement of grams 

    Training curves without  blue  and with
 orange  jointtraining

    Jointtraining Architecture

Figure    Figure     compares the training curves before  blue  and after  orange  autore nement of grams  They look very similar 
although the number of grams is greatly reduced after re nement  which makes training faster and potentially more robust due to less
gram sparsity  Figure     Training curve of model with and without jointtraining  The model corresponding to the orange training
curve is jointly trained together with vanilla CTC  such models are often more stable during training  Figure     Typical jointtraining
model architecture   vanilla CTC loss is best applied   few levels lower than the GramCTC loss 

  Methodology
Here we describe additional techniques we found useful in
practice to enable the GramCTC to work ef ciently as well
as effectively 

  Iterative Gram Selection
Although GramCTC can automatically select useful
grams  it is challenging to train with   large    The total number of possible grams is usually huge  For example 
in English  we have   characters  then the total number of
bigrams is       the total number of trigrams are
              which grows exponentially and quickly
becomes intractable  However  it is unnecessary to consider many grams  such as  aaaa  which are obviously useless 
In our experiments  we  rst eliminate most of useless
grams from the statistics of   huge corpus  that is  we count
the frequency of each gram in the corpus and drop these
grams with rare frequencies  Then  we train   model with
GramCTC on all the remaining grams  By applying  decoding  the trained model on   large speech dataset  we get
the real statistics of gram   usage  Ultimately  we choose
high frequency grams together with all unigrams as our  
nal gram set    Table   shows the impact of iterative gram
selection on WSJ  without LM  Figure    shows its corresponding training curve  For details  please refer to Section
 

  Joint Training with Vanilla CTC
GramCTC needs to solve both decomposition and alignment tasks  which is   harder task for   model to learn than
CTC  This is often manifested in unstable training curves 
forcing us to lower the learning rate which in turn results

in models converging to   worse optima  To overcome this
dif culty  we found it bene cial to train   model with both
the GramCTC  as well as the vanilla CTC loss  similar
to jointtraining CTC together with CE loss as mentioned
in  Sak et al    Joint training of multiple objectives
for sequence labelling has also been explored in previous
works  Kim et al    Kim and Rush   
  typical jointtraining model looks like Figure     and the
corresponding training curve is shown in Figure     The
effect of jointtraining are shown in Table   and Table   in
the experiments 

  Experiments
We test the GramCTC loss on the ASR task  while both
CTC and the introduced GramCTC are generic techniques
for other sequence labelling tasks  For all of the experiments  the model speci cation and training procedure are
the same as in  Amodei et al      The model is   recurrent neural network  RNN  with   twodimensional convolutional input layers  followed by   forward  Fwd  or bidirectional  Bidi  Gated Recurrent layers    cells each  and
one fully connected layer before   softmax layer  In short
hand  such   model is written as      Conv   KxN GRU 
The network is trained endto end with the CTC  GramCTC or   weighted combination of both  This combination
is described in the earlier section 
In all experiments  audio data is is sampled at  kHz  Linear FFT features are extracted with   hop size of  ms
and window size of  ms  and are normalized so that
each input feature has zero mean and unit variance  The
network inputs are thus spectral magnitude maps ranging
from  kHz with   features per  ms frame  At each
epoch    of the utterances are randomly selected to add

GramCTCC ATCTC   AT GramCTCC ATCTCC ATGram CTC

Loss
CTC  unigram
CTC  bigram
GramCTC  handpick
GramCTC  all unigrams   bigrams
GramCTC  autore nement

WER
 
 
 
 
 

Table   Results of different gram selection methods on WSJ
dataset 

background noise to  The optimization method we use
is stochastic gradient descent with Nesterov momentum 
Learning hyperparameters  batchsize  learningrate  momentum  and etc  vary across different datasets and are
tuned for each model by optimizing   holdout set  Typical
values are   learning rate of   and momentum of  

  Data and Setup
Wall Street Journal  WSJ  This corpora consists primarily of read speech with texts drawn from   machinereadable corpus of Wall Street Journal news text  and contains about   hours speech data  We used the standard
con guration of train si  dataset for training  dev  for
validation and eval  for testing  This is   relatively  clean 
task and often used for model prototyping  Miao et al 
  Bahdanau et al    Zhang et al    Chan et al 
   
FisherSwitchboard  This is   commonly used English
conversational
telephone speech  CTS  corpora  which
contains   hours CTS data  Following the previous
works  Zweig et al      Povey et al    Xiong et al 
    Sercu and Goel    evaluation is carried out on
the NIST   CTS test set  which comprises both Switchboard  SWB  and CallHome  CH  subsets 
   Speech Dataset  We conduct large scale ASR experiments on   noisy internal dataset of   hours  This
dataset contains speech collected from various scenarios 
such as different background noises  far eld  different accents  and so on  Due to its inherent complexities  it is  
very challenging task  and can thus validate the effectiveness of the proposed method for realworld application 

  Gram Selection
We employ the WSJ dataset for demonstrating different
strategies of selecting grams for GramCTC  since it is  
widely used dataset and also small enough for rapid idea
veri cation  However  because it is small  we cannot use
large grams here due to data sparsity problem  Thus  the
autore ned gram set on WSJ is not optimal for other larger
datasets  where larger grams could be effectively used  but
the procedure of re nement is the same for them 
We  rst train   model using all unigrams and bigrams  

Loss

 stride 

 

WER

 

CTC  unigram  
 
CTC  bigram
GramCTC
 

 
 
 

 
 
 
 

Epoch Time  mins 

 
 
 
 

Table   Performances with different model strides on WSJ
dataset 

unigrams and       bigrams  in total   grams 
and then do decoding with the obtained model on another
speech dataset to get the statistics of the usage of grams 
Top   bigrams together with all   unigrams  autore ned grams  are used for the second round of training 
For comparison  we also present the result of the best handpicked grams  as well as the results on unigrams  All the
results are shown in Table  
Some interesting observations can be found in Table  
First  the performance of autore ned grams is only slightly
better than the combination of all unigrams and all bigrams  This is probably because WSJ is so small that gram
learning suffers from the data sparsity problem here  similar to wordsegmented models  The autore ned gram set
contains only   small subset of bigrams  thus more robust  This is also why we only try bigrams  not including higherorder grams  Second  the performance of best
handpicked grams is worse than autore ned grams  This
is desirable  It is timeconsuming to handpick grams  especially when you consider highorder grams  The method
of iterative gram selection is not only fast  but usually better  Third  the performance of GramCTC on autore ned
grams is only slightly better than CTC on unigrams  This
is because GramCTC is inherently dif cult to train  since
it needs to learn both decomposition and alignment  WSJ
is too small to provide enough data to train GramCTC 
  Sequence Labelling in Large Stride
Using   large time stride for sequence labelling with RNNs
can greatly boost the overall computation ef ciency  since
it effectively reduces the time steps for recurrent computation  thus speeds up the process of both forward inference
and backward propagation  However  the largest stride that
can be used is limited by the gram set we use  The  unigram  CTC has to work in   high time resolution  small
stride  in order to have enough number of frames to output every character  This is very inef cient as we know the
same acoustic feature could correspond to several grams of
different lengths            igh   eye    The larger the
grams are  the larger stride we are potentially able to use 
DS   Amodei et al    employed nonoverlapping bigram outputs to allow for   larger stride  This imposes an
arti cial constraint forcing the model to learn  not only the
spelling of each word  but also how to split words into bigrams  For example  part is split as  pa  rt  but the word

GramCTC

Figure   Maxdecoding results  without collapsing  of CTC and GramCTC on utterances from Switchboard dataset  The predicted
characters  by CTC  or grams  by GramCTC  at each timestep are separated by   As the GramCTC model is trained with doubled
stride as that of CTC model  we place the grams at   doubled width as we do with characters for better viewing  The   represents
blank 

apart is forced to be decomposed as  ap  ar     GramCTC removes this constraint by allowing the model to decompose words into larger units into the most convenient
or sensible decomposition  Comparison results show this
change enables GramCTC to work much better than bigram CTC  as in Table  
In Table   we compare the performance of trained model
and training ef ciency on two strides    and   For GramCTC  we use the autore ned gram set from previous section  As expected  using stride   almost cuts the training time per epoch into half  compared to stride   From
stride   to stride   the performance of unigram CTC
drops quickly  This is because small grams inherently need
higher time resolutions  As for GramCTC  from stride  
to stride   its performance decreases   little bit  while in
experiments on the other datasets  GramCTC constantly
works better in stride   One possible explanation is that
WSJ is too small for GramCTC to learn large grams well 
In contrast  the performance of bigram CTC is not as good
as that of GramCTC in either stride 

  Decoding Examples
Figure   illustrates the maxdecoding results of both CTC
and GramCTC on nine utterances  Here the label set for
CTC is the set of all characters  and the label set for GramCTC is an autore ned gram set containing all unigrams
and some highfrequency highorder grams  Here Gram 

CTC uses stride   while CTC uses stride  
From Figure   we can  nd that    GramCTC does automatically  nd many intuitive and meaningful grams  such
as  the   ng  and  are    It also decomposes the sentences into segments which are meaningful in term of pronunciation  This decomposition resembles the phonetic decomposition  but in larger granularity and arguably more
natural    Since GramCTC predicts   chunk of characters    gram  each time  each prediction utilizes larger context and these characters in the same predicted chunk are
dependent  thus potentially more robust  One example is
the word  will  in the last sentence in Figure     Since
the output of network is the probability over all grams  the
decoding process is almost the same as CTC  still endto 
end  This makes such decomposition superior to phonetic
decomposition  In summary  GramCTC combines the advantages of both CTC on characters and CTC on phonemes 

  Comparison with Other Methods
  WSJ DATASET

The model used here is      conv      Bidi GRU 
with   CTC or GramCTC loss  The results are shown
in Table   For all models we trained  language model
can greatly improve their performances  in term of WER 
Though this dataset contains very limited amount of text
data for learning gram selection and decomposition  Gram 

True Text what were they doing down there  CTC                                                                                                                                                 GramCTC           hat         we    re    re          the                     do          ng                own         the   the   re                       True Text that is very exciting  CTC                                                                                                                                   GramCTC                hat                    ve                            ex                                  ng                                         True Text that sounds great  CTC                                                                                                                                             GramCTC                hat                     so    und                                             re                                                                 True Text now where would that be  CTC                                                                                                                                                          GramCTC               now                                      he    re                            ld                hat                     be                                   True Text did you get my email today  CTC                                                                                                                                                                     GramCTC     did   did               you               get               my                           ma    il                      to              day                                  True Text oh how long are you going to be there  CTC                                                                                                                                                                               GramCTC          oh                      how         lo    ng                are         you               go    go          ng          to          be          the   the   re                 True Text well   thought she is in washington  CTC                                                                                                                                                                            GramCTC                                           ho                                        she         was         in                            sh          ng          on    on                 True Text did they stay with you for the whole two weeks  CTC                                                                                                                                                                                                   GramCTC     did               the               st                      it    it                you         for   for         the         who   le                two         we    we                       True Text he will take the luggage  CTC                                                                                                                                                                       GramCTC          he                           ill                                      ta    ta                                  the                     lu                           age   age           	GramCTC

Architecture
Phoneme CTC   trigram LM  Miao et al   
Grapheme CTC   trigram LM  Miao et al   
Attention   trigram LM  Bahdanau et al   
DeepConv LAS   no LM  Zhang et al   
DeepConv LAS   LSD   no LM  Chan et al     
Temporal LS   Cov   LM
 Chorowski and Navdeep   
Vanilla CTC   no LM  ours 
Vanilla CTC   LM  ours 
GramCTC   no LM  ours 
GramCTC   LM  ours 

WER
 
 
 
 
 

 
 
 
 
 

Table   Comparison with previous published results with endto 
end training on WSJ speech dataset  The numbers in bold are the
best results with and without   language model

CTC can still improve the vanilla CTC notably 

  FISHERSWITCHBOARD

The acoustic model trained here is composed of two   
convolutions and six bidirectional GRU layer in   dimension  The corresponding labels are used for training
Ngram language models 

  Switchboard English speech    
  Fisher English speech Part            
  Fisher English speech Part            

We use   sample of the Switchboard  portion of the NIST
  dataset     RT  for tuning language model
hyperparameters  The evaluation is done on the NIST
  set  This con guration forms   standard benchmark
for evaluating ASR models  Results are in Table  
We compare our model against best published results on
indomain data  These results can often be improved using
outof domain data for training the language model  and
sometimes the acoustic model as well  Together these techniques allow  Xiong et al      to reach   WER of  
on the SWBD set 

     SPEECH DATASET

Finally  we experiment on   large noisy dataset collected by
ourself for building largevocabulary Continuous Speech
Recognition  LVCSR  systems  This dataset contains about
  hours speech in   diversity of scenarios  such as far 
 eld  background noises  accents  In all cases  the model
is      Conv      Fwd GRU  LA Conv  with only  
change in the loss function 
 LA Conv  refers to   look
ahead convolution layer as seen in  Amodei et al   
which works together with forwardonly RNNs for deployment purpose 
As with the FisherSwitchboard dataset  the optimal stride
is   for GramCTC and   for vanilla CTC on this dataset 
Thus  in both experiments  both GramCTC and vanilla

Architecture

IteratedCTC  Zweig et al     
BLSTM   LF MMI  Povey et al   
LACE   LF MMI    Xiong et al     
Dilated convolutions  Sercu and Goel   
Vanilla CTC  ours 
GramCTC  ours 
Vanilla CTC   GramCTC  ours 

SWBD
CH
WER WER
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Table   Comparison with previous published results on FisherSwitchboard benchmark  SWBD  and  CH  represent Switchboard and Callhome portions  respectively  using indomain data 
We only list results using single models here 

Architecture
Vanilla CTC
GramCTC
Vanilla CTC   GramCTC

WER No LM  WER With LM 

 
 
 

 
 
 

Table   LVCSR results on    speech dataset 

CTC   GramCTC are trained mush faster than vanilla
CTC itself  The result is shown in Table   GramCTC
performs better than CTC  After jointtraining with vanilla
CTC and alignment information through   CE loss  its performance is further boosted  which veri es jointtraining
helps training 
In fact  with only   small additional cost
of time  it effectively reduces the WER from   to
   without language model 

  Conclusions and Future Work
In this paper  we have proposed the GramCTC loss to
enable automatic decomposition of target sequences into
learned grams  We also present techniques to train the
GramCTC in   clean and stable way  Our extensive experiments demonstrate the proposed GramCTC enables the
models to run more ef ciently than the vanilla CTC  by
using larger stride  while obtaining better performance of
sequence labelling  Comparison experiments on multiplescale datasets show the proposed GramCTC obtains stateof theart results on various ASR tasks 
An interesting observation is that the learning of GramCTC implicitly avoids the  degenerated solution  that occurring in the traditional  unit discovery  task  without involving any Bayesian priors or the  minimum description
length  constraint  Using   small gram set that contains
only short  up to   in our experiments  as well as highfrequency grams may explain the success here 
We will continue investigating techniques of improving the
optimization of GramCTC loss  as well as the applications
of GramCTC for other sequence labelling tasks 

GramCTC

References
Alex Graves  Santiago Fern ndez  Faustino Gomez  and
  rgen Schmidhuber  Connectionist temporal classi cation  labelling unsegmented sequence data with recurrent neural networks  In Proceedings of the  rd international conference on Machine learning  pages  
ACM   

Kyunghyun Cho  Bart Van Merri nboer  Caglar Gulcehre 
Dzmitry Bahdanau  Fethi Bougares  Holger Schwenk 
and Yoshua Bengio  Learning phrase representations using rnn encoderdecoder for statistical machine translation  arXiv preprint arXiv   

Ilya Sutskever  Oriol Vinyals  and Quoc   Le  Sequence
to sequence learning with neural networks  In Advances
in neural information processing systems  pages  
   

William Chan  Navdeep Jaitly  Quoc Le  and Oriol Vinyals 
Listen  attend and spell    neural network for large
vocabulary conversational speech recognition  In  
IEEE International Conference on Acoustics  Speech
and Signal Processing  ICASSP  pages  
IEEE     

Awni    Hannun  Carl Case  Jared Casper  Bryan Catanzaro  Greg Diamos  Erich Elsen  Ryan Prenger  Sanjeev Satheesh  Shubho Sengupta  Adam Coates  and Andrew    Ng  Deep speech  Scaling up endto end speech
recognition  CoRR  abs   

Dzmitry Bahdanau  Jan Chorowski  Dmitriy Serdyuk 
Yoshua Bengio  et al  Endto end attentionbased large
vocabulary speech recognition  In   IEEE International Conference on Acoustics  Speech and Signal Processing  ICASSP  pages   IEEE   

Jean   bastien  Kyunghyun Cho  Roland Memisevic  and
Yoshua Bengio  On using very large target vocabulary
for neural machine translation   

Oriol Vinyals   ukasz Kaiser  Terry Koo  Slav Petrov  Ilya
Sutskever  and Geoffrey Hinton  Grammar as   foreign
language  In Advances in Neural Information Processing
Systems  pages    

Jan   Chorowski  Dzmitry Bahdanau  Dmitriy Serdyuk 
Kyunghyun Cho  and Yoshua Bengio  Attentionbased
models for speech recognition  In Advances in Neural
Information Processing Systems  pages    

  Xiong    Droppo    Huang    Seide    Seltzer    Stolcke    Yu  and   Zweig  The microsoft   conversational speech recognition system  arXiv preprint
arXiv     

Yonghui Wu  Mike Schuster  Zhifeng Chen  Quoc   
Le  Mohammad Norouzi  Wolfgang Macherey  Maxim
Krikun  Yuan Cao  Qin Gao  Klaus Macherey  Jeff
Klingner  Apurva Shah  Melvin Johnson  Xiaobing
Liu  Lukasz Kaiser  Stephan Gouws  Yoshikiyo Kato 
Taku Kudo  Hideto Kazawa  Keith Stevens  George
Kurian  Nishant Patil  Wei Wang  Cliff Young  Jason
Smith  Jason Riesa  Alex Rudnick  Oriol Vinyals  Gregory    Corrado  Macduff Hughes  and Jeffrey Dean 
Google   neural machine translation system  Bridging
the gap between human and machine translation  CoRR 
abs     

William Chan  Yu Zhang  Quoc Le  and Navdeep Jaitly 

Latent sequence decompositions  In Arxiv     

Dario Amodei  Rishita Anubhai  Eric Battenberg  Carl
Case  Jared Casper  Bryan Catanzaro  Jingdong Chen 
Mike Chrzanowski  Adam Coates  Greg Diamos  et al 
Deep speech   Endto end speech recognition in english and mandarin  arXiv preprint arXiv 
 

Yonghui Wu  Mike Schuster  Zhifeng Chen  Quoc  
Le  Mohammad Norouzi  Wolfgang Macherey  Maxim
Krikun  Yuan Cao  Qin Gao  Klaus Macherey  et al 
Google   neural machine translation system  Bridging
the gap between human and machine translation  arXiv
preprint arXiv     

Ronan Collobert  Christian Puhrsch  and Gabriel Synnaeve 
Wav letter  an endto end convnetbased speech recognition system  arXiv preprint arXiv   

Geoffrey Zweig  Chengzhu Yu  Jasha Droppo  and Andreas Stolcke  Advances in allneural speech recognition  arXiv preprint arXiv     

Hagen Soltau  Hank Liao  and Hasim Sak 

Neural
speech recognizer  Acousticto word lstm model for
arXiv preprint
large vocabulary speech recognition 
arXiv   

KF Lee and HW Hon 

Largevocabulary speakerindependent continuous speech recognition using hmm 
In Acoustics  Speech  and Signal Processing   
ICASSP    International Conference on  pages
  IEEE   

Tom Sercu and Vaibhava Goel  Dense prediction on
sequences with timedilated convolutions for speech
recognition  arXiv preprint arXiv   

Wayne Xiong  Jasha Droppo  Xuedong Huang  Frank
Seide  Mike Seltzer  Andreas Stolcke  Dong Yu  and Geoffrey Zweig  Achieving human parity in conversational
speech recognition  arXiv preprint arXiv 
   

GramCTC

networks for asr based on latticefree mmi  Submitted to
Interspeech   

Jan Chorowski and Jaitly Navdeep  Towards better decoding and language model integration in sequence to sequence models  arXiv preprint arXiv   

William Chan  Navdeep Jaitly  Quoc   Le  and Oriol
arXiv preprint

Listen  attend and spell 

Vinyals 
arXiv   

Mathew Magimai Doss  Todd   Stephenson  Herv 
Bourlard  and Samy Bengio  Phonemegrapheme based
speech recognition system  In Automatic Speech Recognition and Understanding    ASRU    IEEE
Workshop on  pages   IEEE   

MinhThang Luong and Christopher   Manning  Achievtranslation
arXiv preprint

ing open vocabulary neural machine
with hybrid wordcharacter models 
arXiv   

Timothy Andrew Cartwright and Michael   Brent  Segmenting speech without   lexicon  The roles of
phonotactics and speech source  arXiv preprint cmplg   

Sharon Goldwater  Thomas   Grif ths  and Mark Johnson 
Contextual dependencies in unsupervised word segmentation  In Proceedings of the  st International Conference on Computational Linguistics and the  th annual
meeting of the Association for Computational Linguistics  pages   Association for Computational Linguistics   

Hasim Sak  Andrew    Senior  Kanishka Rao  and
Fran goise Beaufays  Fast and accurate recurrent neural
network acoustic models for speech recognition  CoRR 
abs   

Suyoun Kim  Takaaki Hori  and Shinji Watanabe 

Joint
ctcattention based endto end speech recognition using
multitask learning  arXiv preprint arXiv 
 

Yoon Kim and Alexander   Rush  Sequencelevel knowlarXiv preprint arXiv 

edge distillation 
 

Yajie Miao  Mohammad Gowayyed  and Florian Metze 
Eesen  Endto end speech recognition using deep rnn
models and wfstbased decoding  In Automatic Speech
Recognition and Understanding  ASRU    IEEE
Workshop on  pages   IEEE   

Yu Zhang  William Chan  and Navdeep Jaitly  Very deep
convolutional networks for endto end speech recognition  arXiv preprint arXiv   

Geoffery Zweig  Ghengzhu Yu  Jasha Droppo  and Andreas Stolcke  Advances in allneural speech recognition  arXiv preprint arXiv     

Daniel Povey  Vijayaditya Peddinti  Daniel Galvez  Pegah
Ghahrmani  Vimal Manohar  Xingyu Na  Yiming Wang 
and Sanjeev Khudanpur  Purely sequencetrained neural

