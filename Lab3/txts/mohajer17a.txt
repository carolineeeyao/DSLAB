Active Learning for TopK Rank Aggregation from Noisy Comparisons

Soheil Mohajer   Changho Suh   Adel Elmahdy  

Abstract

We explore an active topK ranking problem
based on pairwise comparisons that are collected
possibly in   sequential manner as per our design choice  We consider two settings    topK
sorting in which the goal is to recover the topK
items in order out of   items    topK partitioning where only the set of topK items is desired 
Under   fairly general model which subsumes as
special cases various models       Strong Stochastic Transitivity model  BTL model and uniform
noise model  we characterize upper bounds on
the sample size required for topK sorting as well
as for topK partitioning  As   consequence  we
demonstrate that active ranking can offer significant multiplicative gains in sample complexity
over passive ranking  Depending on the underlying stochastic noise model  such gain varies from
around
log log    We also present an
algorithm that is applicable to both settings 

log log   to    log  

log  

  Introduction
Ranking is prevalent in   wide variety of applications  social choice  Caplin   Nalebuff    Azari Sou ani et al 
  web search and information retrieval  Dwork et al 
  recommendation systems  Baltrunas et al   
and crowd sourcing  Chen et al    to name   few  The
goal of the problem is to bring   consistent ordering to  
collection of items  given partial preference information 
The two main paradigms among   large volume of works
on ranking include spectral ranking algorithms  Negahban
et al    Dwork et al    Brin   Page    and
maximum likelihood estimation  Ford    These methods focus on  nding the entire ordering  not being tailored
for many practical applications in which only   few signi 

cant items  say topK  are often desired to be retrieved 
In an effort to exploit the more practically relevant scenario   Chen   Suh    investigated the topK rank
aggregation which aims to recover the correct set of topranked items only  It characterized the minimax limit on
the sample size       sample complexity  under the BradleyTerry Luce  BTL  model  Bradley   Terry    Luce 
  where pairs of two items are compared  However  this
development is limited to   passive measurement setting in
which pairwise data are simply given prior to analysis 
Many applications of interest often admit interaction with
users  This enables us to select comparison pairs of items
in an adaptive manner  This way of ranking provides the
potential to reduce   large number of blindly collected measurements while maintaining   ranking accuracy  Tschopp
et al    This motivates us to examine an adaptive
measurement setting  in which pairwise comparisons are
gathered interacting with   ranker  termed active ranking 
In particular  we intend to address the following two questions      how much can active ranking offer performance
improvements over passive ranking      how does the limit
on the sample size for topK ranking scale with   
To answer this question  we consider   general model in
which the pairwise comparison probabilities are arbitrary
subject to   mild condition  see   in Section   for details 
and thus which includes as special cases various models
like the BTL model  Strong Stochastic Transitivity  SST 
model  Fishburn    Shah et al    and uniform
noise model  Braverman   Mossel    Two ranking
tasks are taken into consideration      topK sorting which
takes care of detailed ordering within topK items   ii  topK partitioning which concerns only the correct set of them 
Contribution  Our contributions are twofolded  The  rst
lies in deriving an upper bound on the sample size 

 

 cid 
 cid         min

       log   

max log log    log   

  

    

 Pij  Sorting 
        PK    Partitioning 

min
     

 cid 

 

 

 ECE  University of Minnesota  Twin Cities  MN  USA   EE 
KAIST  Daejeon  South Korea  Correspondence to  Soheil Mohajer  soheil umn edu  Changho Suh  chsuh kaist ac kr  Adel
Elmahdy  adel umn edu 

where 

    

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

 Note that the notation    here is de ned slightly differently

from the one in  Chen   Suh   

Active Learning for TopK Rank Aggregation from Noisy Comparisons

Here Pij   Pi   indicates the probability of item   being
preferred over item    and       or       denotes the
parameter        topK sorting  or partitioning  Without
loss of generality  we assume that the ground truth ranking
is the order of    cid     cid     cid     Notice that the sample
complexity bound reads     log log      for the small
  regime             log    and     log       for
the large   regime               For the regime
      log    of practical interest  this exhibits signi cant
multiplicative gains of active ranking over passive ranking 
For instance  in the case of topK sorting  when specializing
our result into the uniform noise model and BTL model  one
can demonstrate that the factor gains are  
and

 cid     log  

 cid 

log log  

  respectively  See Table   for further details 

 cid  log  

 cid 

 

log log  

Our second contribution is to develop   computationallyef cient  nearly lineartime algorithm that can achieve the
above bound promised  The algorithm is based on standard
algorithms in TCS literature for the noiseless sorting and
partitioning  where each pairwise order can be retrieved
using   single comparison or the transitive property of the
ranking  However  in   noisy setting of our interest  single
comparison results are not reliable    key distinction in our
work is to employ repeated pairwise comparisons for each
comparison to combat the noise effect 
Here is how our algorithm works in details  It builds upon
  HEAP datastructure and can be applied to both settings
of sorting and partitioning  The main idea of identifying
topK items in   dataset of   items is to partition the set
of   items into   subsets  and then identify the top item in
each subset using singleelimination tournament  Next   
maxHEAP is built to single out the top  item among the  
winners of the tournaments  After that  this item is removed
from the system and the maxHEAP is updated by replacing
it by the second top item of the subset this top item belongs
to  We repeat this process until we identify topK items 
See Fig    for an explanatory example 
It is worth mentioning that the performance of our algorithm
depends on the separability parameter     The algorithm
is supposed to correctly distinguish between items   and
        in partitioning  or the top   items and the rest
 in sorting  The smaller     the less reliable the results
of comparison between tobe distinguished items  hence 
more repetitions are needed for   decision  We characterize
the minimum number of repeated comparisons required to
ensure that the retrieved items are the correct winners  The
carefully chosen number for repeated comparisons together
with   couple of bounding techniques play   key role to
derive the above sample complexity bound  Finally  we
conduct several experiments to corroborate our main results 
Related work   Chen   Suh    explored the topK partitioning problem under the nonadaptive comparison model 

and characterized the optimal sample complexity  Subsequently  sample complexity analyses were made with regard
to different yet popular ranking paradigms such as simple
counting methods  Shah   Wainwright    and spectral
methods  Jang et al          RankCentrality  Negahban et al    In this work  we examine an adaptive
measurement setting under   fairly general model  thereby
showing that active ranking can signi cantly outperform
passive ranking for   variety of scenarios 
Recently   Braverman et al    developed an active ranking algorithm  Interesingly  for the       case and under
the uniform noise model  their algorithm can achieve the
same sample complexity as ours for   certain target error
rate   Sz   nyi et al    also focused on the       case
but under the BTL model  thus developing an algorithm
which however yields   larger sample complexity than ours 
Most recently   Heckel et al    proposed an algorithm
for   general problem setting which encompasses topK
sorting and partitioning of our interest  We found that their
algorithm is outperformed by ours when specializing it to
our settings  See   for detailed discussion 
There has been   proliferation of active ranking algorithms  BusaFekete et al    Jamieson   Nowak   
Maystre   Grossglauser    Ailon    Braverman
  Mossel    Wauthier et al    While interesting
ranking schemes are developed for perfect ranking  BusaFekete et al    Jamieson   Nowak    Maystre  
Grossglauser    and approximate ranking  Jamieson  
Nowak    Ailon    Braverman   Mossel   
Wauthier et al    they are not customized for topK
ranking of our interest 
Moreover  the bestK identi cation with adaptive sampling
has been extensively explored under the name of the multiarmed bandit problem  Gabillon et al    Bubeck et al 
  Jamieson et al    Yue et al    for   socalled
valuebased model in which the observation on each item is
drawn only from the distribution underlying this individual 
Also there are many related yet different problem settings
considered in prior literature  Azari Sou ani et al   
Hajek et al    Lu   Boutilier    Eriksson   
  Problem Formulation
Comparison model  We denote by             comparison graph in which items   and   are compared if and only if
       belongs to the edge set    More precisely    multiedge
graph is taken into consideration to accommodate repeated
measurements for an observed pair  We take into account
an adaptive comparison graph in which the edge set is dynamically selected interacting with   ranker  Speci cally 
for   sample instance            where   indicates the total
sample size  an edge et    it  jt  is chosen based on the
pairwise outcomes obtained up to       

Active Learning for TopK Rank Aggregation from Noisy Comparisons

Pairwise comparisons  Given et          the outcome of
the tth comparison  denoted by Yt  is generated according to

 cid    with probability Pij

 

Yt  

  with probability     Pij 

where Yt     indicates that item   is preferred over item   
The outcomes Yt   are independent across    We also represent the collection of suf cient statistics as

Yij  

  et     et  

Yt       Yij              

 

 cid 

Sortedby Probabilities  SP  model  Without loss of generality  assume that the ground truth ranking is the order
of    cid     cid     cid     In fact  the SST model  Fishburn 
  Shah et al    suggests one way to relate the ranking to the model parameters Pij   by putting the following
constraint  Pik   Pjk for all    cid         whenever item
   cid  item    In this work  we introduce   more general
model  which we call Sortedby Probabilities  SP  model 
by relaxing the constraint as 

Pij  

 
 

whenever    cid    

 

Notice that the new constraint   is weaker  thus spanning
  larger parameter space  One can readily verify that our
model also subsumes as special cases other prominent models  Observe that whenever    cid    

Pij  

         
   
   
   
wi wj

wi

 uniform noise model 
 BTL model 

 

 cid   

where   denotes an arbitrary constant       and wi
indicates the score of item   
Performance metric and goal  Given the pairwise comparisons  one wishes to know whether or not the topK ordered
items  or the topK set  are identi able  In light of this  we
consider the probability of error Pe 

 cid         cid     cid     cid      

       cid       

sorting 
partitioning 

Pe   

where   is any ranking scheme that returns an order of
  indices  Our goal in this work is to characterize the
sample complexity   
   de ned as the minimum sample
size above which topK ranking is feasible  in other words 
Pe can be vanishingly small as   grows 

Remark   It should be noted that there are other frameworks in which different performance metrics are taken
into consideration   regret   Yue et al    and  PAClearning   Sz   nyi et al    both introduced in the

bandit literature  Actually our work focuses on the worst
case scenario as the  error rate   loss  that we considered is the most stringent criterion among others  Hence  the
sample complexity of our model provides an upper bound
for other criteria  Interestingly  the sample complexity of the
proposed algorithm is superior  lower  to that of the PLPAC
algorithm  Sz   nyi et al    under the PAC criterion 
See Section   for details 

wK  wK 

 cid 

 cid 

 cid  wi wi 

 cid  wK wK 

  Main Results
As noted in the passive ranking setup  Chen   Suh   
the most crucial part of topK partitioning under the BTL
model hinges on separating the two items near the boundary  being re ected in
  Similarly for topK
sorting  one can easily show that the key measure would be 
  We  nd that in our general model 
mini   
the corresponding key measure is the one de ned in  
 wi wj   under the BTL
model  Hence  we will use this measure to express our upper
bound on sample complexity as below 
Theorem   With probability exceeding      log      the
topK order  or topK set  can be identi ed provided that

Observe in   that Pij       wi wj

wi wi 

SK           log   

max log log    log   

  

 

 

Here         are some universal positive constants 

 

See Section   for the proof of Theorem   and algorithm
description  There are three points to make  The  rst is that
the above bound is          target error rate that scales like
poly log    Aiming at   smaller target error  we need   larger
sample size  Secondly  the term     affected by  Pij    
 see   captures how noisy the comparison data is      
 Pij     is   sort of the dif culty level of separating item
  from item    So the result in Theorem   coincides with our
intuition because smaller    means more dif cult to rank 
which results in an increase of sample complexity  The last
point is regarding the performance of our algorithm  Since
sorting naturally produces   partitioning  our algorithm is
tailored for the sorting  and hence favors the sorting performance relative to partitioning  Notice for partitioning
that when        the sample complexity bound reads the
order of   log    which is certainly far from optimality 
Hence  in the next subsection  we provide several interesting
remarks with an emphasis on topK sorting in which we
advocate the performance of our algorithm 

 We ignore the tie situation as we consider   strict order of
ranking  Precisely speaking  the constraint is called Strict Strong
Stochastic Transitivity  SSST  property  Fishburn   

 For topK partitioning  we assume monotonicity in Pij 
Pij   Pik whenever            which holds still under   fairly
general model like SST 

Active Learning for TopK Rank Aggregation from Noisy Comparisons

  TopK Sorting
Penalty due to noisy measurements  As mentioned earlier  topK sorting has been extensively explored in the
TCS literature  but only the noiseless setting has been the
main focus  in which sample complexity is characterized as
 Cormen et al   

Snoiseless            log   

 
Comparing   to   we see that the penalty factor in sample complexity due to noisy measurements is 

 cid  max log log    log   

 cid 

 

 

 

  

Actually it is not clear whether or not this penalty factor is
fundamental due to the lack of the optimality result  However  the gap  if any  is up to poly log    as long as    is
not too small  scales at most with poly log    This implies
that the degradation over the noiseless setting is low and
therefore our algorithm performs very close to optimum 
How the limit scales with    Observe in   that 
        log   
         

 cid    log log  
 cid 
 cid    log   
 cid 

   

SK  

  

 

  

We make one interesting observation in the       log   
regime of practical interest  the bound is irrelevant to  
under some measurement model  One such example is the
uniform noise model where       

Suniform      

 

 

 cid    log log  

 cid 

 

for every    However  this phenomenon does not carry
over to other noisy models  like the BTL model in which
the noise quality varies according to associated preference
scores  Note that

 cid 

  log log  

 cid  wi wi 

wi wi 

 cid 

mini   

 cid 

But our result still suggests that the phenomenon may hold
universally for   variety of statistical models as long as  
is small enough 
Active vs  passive ranking  For illustrative purpose  let
us focus on the interesting regime of       log    and
consider two models    uniform noise model    BTL
model  In the uniform noise model  ShahWainwright  Shah
  Wainwright    characterized the passive ranking sample complexity for   certain observation model 

Spassive
uniform      

 

 

for every choice of    This together with   demonstrates
that the factor gain due to active measurements is quite substantial   
  In the BTL model  ChenSuh  Chen

 cid     log  

 cid 

log log  

 cid     log  

 cid 

 

  Suh    characterized the sample complexity under
passive ranking as 

 cid 

  log  

 cid  wi wi 

 cid 

wi wi 

mini   

 cid 

 

 

Spassive
BTL      

 cid  log  

 cid 

log log  

Comparing this to   we see that the factor gain is
  which is not quite signi cant but still scales
 
with   and hence exhibits respectful improvements in high
dimensional regimes  The comparisons are summarized in
Table   See Section   for experimental results on this 
Robustness  Theorem   suggests that the performance gap
between sorting and partitioning is not signi cant  For instance  under the uniform noise model       are the same 
yielding the same sample complexity bound  For the BTL
model       would be similar if wi   are equidistant  Experimental results on this are provided in the supplemental 
Computational complexity    noticeable feature of our algorithm is its low computational complexity  It runs in time
     in the practicallyrelevant regime of       log   
For general    it is nearly linear in                 log   
Here  this complexity assumes that the input fed to our algorithm is the suf cient statistic of the outcome comparisons 
Yij  see   rather than the entire collection of Yt   associated with the pair        This will be evident later when
describing the algorithm 

  Comparison to Related Work

 Braverman et al    developed an active ranking algorithm for the       case and derived the orderwise tight
sample complexity in terms of target error rate under the
uniform noise model  More concretely  suppose we want
the error probability not to exceed   target error rate   Then 
the result of  Braverman et al    implies

 cid    log 

 cid 

 

 

 

log log  

Notice that our result   admits the target error rate that
scales like
log    implying that their algorithm can achieve
the same sample complexity as ours for   certain scenario
log    see Section   for experimental rein which      
sults  For   relaxed target error like
log log    their algorithm
achieves   slightly smaller sample complexity by   factor of
log log log   
 Sz   nyi et al    also developed   topselection algorithm and analyzed sample complexity under the BTL
model as well as   lessstringent PAC criterion  Their sample complexity bound reads around     log    thus yields
  larger one compared to ours 
In another relevant paper   Heckel et al    proposed
an active ranking algorithm for   general setting  which
subsumes topK sorting  as well as topK partitioning 

SBTL      

 

 

      

 

 

Active Learning for TopK Rank Aggregation from Noisy Comparisons

Active Measurement

Passive Measurement

Noise Model

Uniform Noise

Pij            

BTL model

Pij   wi

wi wj

 

 

 cid 

 cid    log log  
 cid 
 cid  wi wi 

  log log  

 

wi wi 

mini   

 cid 

 cid 

 cid 

 

 

 cid     log  
 cid 
 cid  wi wi 

  log  

 

wi wi 

mini   

Gain

 cid     log  
 cid  log  

log log  

log log  

 cid 
 cid 

 cid 

 cid 

 

 

Table   TopK sorting  Multiplicative gains of active ranking  this work  over passive ranking for the uniform noise model  Shah  
Wainwright    and BTL model  Chen   Suh    in the practically relevant regime       log   

 

 cid 

      sign   

 cid     log 

as   special case  They also provided   lower bound on
the sample complexity for   class of parametric models 
which is only   log    away from the achievable sample
complexity  It is worth mentioning that under the uniform
noise model  the algorithm in  Heckel et al    requires
pairwise comparisons to achieve an error
 
rate of   which is very expensive compared to our algorithm 
Here   key to note is that the uniform noise model does not
   to the class of parametric models considered by  Heckel
et al    in which the CDF function   in Pij    wi  
wj  is assumed to be differentiable  which does not hold in
the uniform noise model where        
On the other hand  applying the result of  Heckel et al 
  on the BTL model  we get lower and upper bounds
on the sample complexity  To see this in details  consider
  special case of the topK partitioning problem  where
              wK  and wK    wK        wn 
 cid 
In this case   Heckel et al    implies

 
  
  
where cl     and cu     Notice that the asymptotic
multiplicative gap between the lower and upper bounds are
on the order of log    log log    Moreover  the large
constantfactor gap yields   signi cant performance gap in
the actual experiment  For instance  see Fig      where
             and       Observe that
the lower and upper bounds are       and      
comparisons  respectively  which are far apart 

  SBTL     cu

 cid   

 cid   

 cid   

 
  

 cid 

log log

cl

log

log

 

 cid 

 

  Proposed Ranking Algorithm
In this section we present our algorithms for sorting and
partitioning tasks  and provide upper bounds for the sample
complexity  We use     to denote the number of pairwise
comparisons required in the algorithm 

  Top  Selection  SingleElimination Tournament

We  rst focus on the special case of identifying the top
item             The proposed algorithm is essentially  
customized singleelimination tournament  which consists
of multiple layers  In each layer items are paired in   ran 

Algorithm   SELECT      

Input   
Data                         
Output     the index of item with the highest score 
 Assume     is   power of   for simplicity 
       
for       to     do          end for
for  cid      to log   do
for       to   cid  do
     
for       to   do
et                
        Yt
end for
if      
else            
end if
end for

  then               

 Yt is de ned in Eq   

end for
  

    

dom manner  and one of the items in each pair is selected
to proceed to the next layer  while the other one is eliminated  This decision is made based on pairwise comparisons
between the two items  The distinctive feature relative to
conventional singleelimination tournament is that in order
to combat against the uncertainty of the observations  we repeat each binary comparison multiple  say    times  It turns
out that for   suf ciently large    the algorithm will output
the index of the top item with overwhelming probability 
The algorithm builds   binary tree of depth  cid log    cid   see
Fig    We denote the ith item index in layer  cid  by   cid   
Initially  we randomly locate items on the leaves of the tree 
that are denoted by     for                  Then  in each
iteration    pair in layer  cid  is tested  and the winner will
proceed to layer  cid      Hence  half of the existing items
will be eliminated in each iteration  until we get to the root
layer in which there would be only one surviving item  The
algorithm is formally presented in Algorithm  
Number of measurements  The algorithm consists of
 cid log    cid  layers  and    cid  pairs are being tested in layer  cid 

Active Learning for TopK Rank Aggregation from Noisy Comparisons

Algorithm   TOP         
Input  Integers   and  
Data  Array                        
Output  Indices of topK                 
             cid     cid 
for       to       do
Ci   iQ    iQ        min        
          iQ   SELECT Ci    

end for
                       
BuildHeap         
for       to   do
        
     cid     cid 
Cj   Cj        
     SELECT Ci    
Heapify           

end for

when   scales with         Rather  we  rst split the
dataset   of   items into   groups each of size     
namely groups               CK  Then we identify the top
item in each subgroup using SELECT  and form   short
list that includes all winners from the subgroups  Then we
build    max HEAP datastructure for the short list obtained
from the   winners  The HEAP structure allows us to easily
extract the top item from the short list  Once the top item of
the short list is identi ed and removed from the list  we go
back to its home subgroup  identify the second top item in
that subgroup  and insert it to the short list  We maintain
the HEAP structure of the short list during the process  to
be able to easily extract the next top item of the list  We
repeat this procedure for     rounds until we retrieve all
the top   items  The main algorithm  TOP is presented in
Algorithm   For the sake of completeness  we also present
the algorithms required to build the heap structure and to
insert   new item to an existing heap in the supplemental 
Sample complexity  The sample complexity of the algorithm can be simply evaluated in terms of the input parameters as follows  We  rst identify the top entry of each of the
  subgroups  Later  during the iterative phase of the algorithm  we need to repeat SELECT algorithm on the remaining elements of subgroups for another   iterations  which
results in    runs of SELECT  each on subgroups of size
at most     items  Each run of algorithm SELECT with
parameter   on   dataset Ci requires    SELECT Ci    
pairwise comparison  as given in  
In order to build the heap structure on   subwinners we
need to make      binary decisions  where we repeat each
comparison   times to deal with the noise  Moreover  in
each iteration one new item is added to the short list  We
need to make   log    binary decisions to maintain the

Figure   The singleelimination tournament with repeated comparisons for       items  In each round  layer  items are paired 
  decision between   pair of items is made based on the majority
rule among   comparisons  Note that   wrong decision in   pair
that does not include item         when   beats   in spite of    cid   
will not affect the ultimate output of the algorithm 

Hence  the total number of tests is cid cid log    cid 

  cid       
Each test requires   binary comparisons  yielding   total
number of measurements given by

 cid 

   SELECT                    mn 
for   dataset of size          It can be shown that if

 

      ln  

log log    

 

   

 
then Pe TopSelection     log    for any       This
implies that  with high probability  the SELECT algorithm
can successfully select the top item using   total of

  

 

 cid    log log    

 cid 

 

 

   SELECT          

pairwise comparisons 

Remark   Parallel to this work  the topselection problem
is studied in  Braverman et al    under the uniform
noise model  The algorithm in  Braverman et al     rst
identi es the top item in   subset of size    log   items  and
then iteratively re nes the estimate by further comparing
that to other items in the set  It is shown that the number of
measurements required grows linearly with    when   constant  nonvanishing  error probability is desired  However 
in order to achieve   vanishing error probability scaling
as  poly log    the algorithm of  Braverman et al   
requires the same number  up to   constant factor  of pairwise comparisons  as the one presented above 

  TopK Sorting    Heapbased Algorithm
In this section we generalize our proposed algorithm to
 nd the top   items along with their order  The proposed
algorithm is built based on the singleelimination algorithm 
which can  nd the the single top item with high probability 
  trivial generalization is to repeat the SELECT algorithm
for   times  which requires   large number of comparisons

   Active Learning for TopK Rank Aggregation from Noisy Comparisons

into   groups  and run the singleelimination tournament
in each group  The winners will proceed to the HEAP algorithm  and then the HEAP outputs items one by one  The
only difference lies in the performance metric which concerns only the set of   items reported by the algorithm 
regardless of the order  It turns our that   similar analysis holds for this problem  except the fact that the number
of repeated comparisons     has   weaker dependency on
the data  More precisely  the algorithm is robust against
wrong binary decision between two items from     and
similarly between two items from         One needs to
choose   such that the algorithm  with high probability 
can correctly distinguish when         is compared against
            Consequently  we show that   depends only
on       which captures the gap between PK    and  
Remark   Note that the proposed algorithms do not require the knowledge of     and can be executed with any   
It depends only on the measurement budget  However  dependency of the performance of the algorithms on    is
inevitable  since the success rate depends on the separability
parameter as discussed earlier 

  Simulation Results
In this section  we empirically evaluate the performance of
the proposed algorithm by conducting Monte Carlo simulations on synthetic data and developing   benchmark comparison against the stateof theart active and passive ranking
algorithms in the literature  In an effort to guarantee fairness in the performance comparison  we jointly investigate
the average number of pairwise comparisons and the corresponding empirical success rate of identifying topK items 
The source code of our algorithm  is provided to allow for
reproducible research  In addition  we present   more detailed discussion on the performance of our algorithm under
various simulation parameters in the supplemental 

We assess the performance of the proposed algorithm
against two recent active ranking algorithms  the  rst is
proposed by  Braverman et al    coined  Braverman 
while the second algorithm is proposed by  Heckel et al 
  coined  Heckel  For Braverman algorithm  we
sweep over an algorithm parameter  denoted by   in the
paper  such that       to measure the corresponding success rate  Heckel algorithm employs   con dence interval
during the process  and we select the following empirical
con dence interval model  that is   function of the algorithm round     and the tolerance parameter        
    

log   log         

 cid 

 The

source

accessible
https github com aelmahdy 
ActiveLearning fromNoisy Comparisons git

via GitHub

code

is

at

 

  Comparison to Prior Active Ranking Algorithms

Figure   Sorting algorithm for       using heap data structure  Items are split into       groups of equal size  namely
                 The top items of each subgroups is identi ed
using singleelimination tournament    heap datastructure is then
built for the short list of the winners of the tournaments  This
provides   binary tree  with the property that children of each node
have   rank lower than their parent  Then the root will be reported
as the top item  Next  one goes back to the homesubgroup of
the root  to  nd the secondtop item of that group  The top item
found in the heap will be replaced by the second top item  and
heap will be rearranged to maintain its property  Iterating on this
for            times  one can identify items              

heap structure  Similar to BuildHeap  we repeat each
comparison for   times  and then decide based on   majority
rule  Therefore  we have

   TOP           cid          SELECT Ci    
    BuildHeap      KN  InsertHeap      
         mn                         log   
    mn   mK log   

 
It turns out  from the analysis of probability of error  that the
proposed algorithm can successfully sort the top   items if

 cid  max log    log log   

 cid 

     

  

  

Plugging this into   we can  nd the sample complexity
of the TOP algorithm as

 cid         log    max log    log log   

SK    

 cid 

 

Remark   The repetition parameter used in SELECT algorithm  and the one used in HEAP can be potentially different 
and accordingly optimized  However  our analysis shows
that   choice of distinct parameters can provide   marginal
gain only for   tiny range of    Thus  we rather choose the
same parameter for the sake of simplicity 

  Partitioning

The algorithm we propose for partitioning is exactly identical to that of the sorting  We  randomly  split the items

             shortlistheap Active Learning for TopK Rank Aggregation from Noisy Comparisons

    Uniform noise model 

    BTL model 

    Active vs  Passive algorithms 

Figure   Performance Evaluation of various active and passive ranking algorithms      Active algorithms under the uniform noise model
for different values of   when                           and        Heckel algorithm parameter      Active
algorithms under BTL model for different values of   when              wi                  and        Heckel
algorithm parameter      Performance Comparison between the proposed TopK algorithm versus the Copeland Counting algorithm
 with       and       under the uniform noise model for different values of   when                 

The plots in Fig      indicate the performance of different
active ranking algorithms under the uniform noise model
to identify the top  and top  items  respectively  when
                     and       The
 rst plot shows slight improvement in the total number of
pairwise comparisons required to achieve   target success
rate for Braverman algorithm over our Top  algorithm 
We also observe that the performance gap between the two
algorithms is negligible at   higher success rate  On the
other hand  the second plot in Fig      depicts the signi 
cant improvement of our TopK algorithm over Braverman
algorithm  when       It is evident that the sample
complexity of Braverman algorithm scales with   when it
is employed for topK identi cation  We can also see that
the performance gap between Partitioning and Sorting is
insigni cant  These  ndings are consistent with our analysis of the sample complexity of both algorithms  Hence 
the two prime merits of the proposed TopK algorithm are 
  its superior performance compared to Braverman algorithm when       and   error and sample complexity
of Braverman algorithm is limited to uniform noise model 
while our algorithm provably performs well for   wide class
of pairwise comparison models  de ned by   We refer the
reader to the supplemental for an insightful remark about
our algorithm 

 Braverman algorithm is extended to topK ranking  We  rst
run the algorithm to retrieve the topitem  Then  we remove it from
the set of items and rerun the algorithm to  nd the second top item 
We keep doing that until we  nd the topK items 

The plots in Fig      also depict performance comparison
between our and Heckel algorithm for TopK ranking under
the uniform noise model  As it is clear from the  gure  the
proposed algorithm in this work performs signi cantly better than Heckel algorithm for this noise model  Furthermore 
Fig      shows that our algorithm for identifying top 
and top  ranked items also achieves   better performance
compared to Heckel algorithm under the BTL model 

  Active vs  Passive Ranking

We compare the performance of our active ranking algorithm against   passive ranking algorithm proposed by  Shah
  Wainwright    coined  Shah  This simple yet robust algorithm hinges on Copeland counting algorithm that
recovers the topK items that win the maximum number
of pairwise comparisons  Moreover  the algorithm makes
no assumptions on the probability model of pairwise comparisons  The algorithm parameters are the probability of
making   comparison on any trial     the number of trials    
and       Note that the number of noisy comparisons associated with each pair of items follows   binomial distribution
with parameters   and   
The plots in Fig      illustrate the performance of the
two algorithms under the uniform noise model to recover
the top  and top  respectively  when          
                 and       As predicted by our
analysis of the sample complexity gain  we can observe the
considerable gain due to active measurements  even when
the total number of items is modest 

 Total Number of Pairwise Comparisons Success RateProposed  Top Braverman  Top Heckel  Top Total Number of Pairwise Comparisons Success RateProposed  Top  PartitioningProposed  Top  SortingBraverman  Top Heckel  Top Total Number of Pairwise Comparisons Success RateProposed  Top Heckel  Top Total Number of Pairwise Comparisons Success RateProposed  Top Heckel  Top Total Number of Pairwise Comparisons Success RateProposed  Top Shah  Top Total Number of Pairwise Comparisons Success RateProposed  Top Shah  Top Active Learning for TopK Rank Aggregation from Noisy Comparisons

Acknowledgment
The authors would like to thank the reviewers who gave
useful comments     Suh was supported by the National
Research Foundation of Korea  NRF  grant funded by the
Korea government  MSIP  Ministry of Science  ICT   Future Planning   No           

References
Ailon     Active learning ranking from pairwise preferences with almost optimal query complexity  Journal of
Machine Learning     

Azari Sou ani     Chen     Parkes        and Xia    
Generalized methodof moments for rank aggregation  In
Neural Information Processing Systems  pp   
 

Baltrunas     Makcinskas     and Ricci     Group recommendations with rank aggregation and collaborative
 ltering  In ACM Conference on Recommender Systems 
pp    ACM   

Bradley        and Terry        Rank analysis of incomplete block designs     the method of paired comparisons 
Biometrika     

Braverman     and Mossel     Noisy sorting without resampling  In ACMSIAM symposium on Discrete algorithms 
pp     

Braverman     Mao     and Weinberg        Parallel algorithms for select and partition with noisy comparisons 
STOC   

Brin     and Page     The anatomy of   largescale hypertextual web search engine  Computer Networks and ISDN
systems     

Bubeck     Wang     and Viswanathan     Multiple identi 
 cation in multiarmed bandits  In International Conference on Machine Learning   

BusaFekete       llermeier     and Sz   nyi    
Preferencebased rank elicitation using statistical models 
The case of mallows  In International Conference on
Machine Learning  pp     

Caplin     and Nalebuff     Aggregation and social choice 

  mean voter theorem  Econometrica  pp     

Chen     Bennett        CollinsThompson     and
Horvitz     Pairwise ranking aggregation in   crowdsourced setting  In ACM Conference on Web Search and
Data Mining  pp    ACM   

Chen     and Suh     Spectral MLE  TopK rank aggregation from pairwise comparisons  In International Conference on Machine Learning  pp     

Cormen        Leiserson        Rivest        and Stein    
Introduction to algorithms  MIT press   rd edition   

Dwork     Kumar     Naor     and Sivakumar     Rank
aggregation methods for the web  In International conference on World Wide Web  pp    ACM   

Eriksson     Learning to topK search using pairwise comparisons  In International Conference on Arti cial Intelligence and Statistics  pp     

Fishburn       Binary choice probabilites  on the varieties
of stochastic transitivity  Journal of Mathematical Psychology     

Ford        Solution of   ranking problem from binary
comparisons  American Mathematical Monthly  pp   
   

Gabillon     Ghavamzadeh     Lazaric     and Bubeck 
   Multibandit best arm identi cation  In Neural Information Processing Systems   

Hajek     Oh     and Xu     Minimaxoptimal inference
from partial rankings  In Neural Information Processing
Systems  pp     

Heckel     Shah     Ramchandran     and Wainwright 
   Active ranking from pairwise comparisons and when
parametric assumptions don   help  arXiv 
 

Jamieson     Malloy     Nowak     and Bubeck     liiUCB  An optimal exploration algorithm for multiarmed
bandits  In Conference on Learning Theory   

Jamieson        and Nowak     Active ranking using pairIn Neural Information Processing

wise comparisons 
Systems  pp     

Jang     Kim     Suh     and Oh     Topk ranking from
pairwise comparisons  When spectral ranking is optimal 
arXiv preprint arXiv   

Lu     and Boutilier     Learning Mallows models with
In International Conference on

pairwise preferences 
Machine Learning  pp     

Luce       

Individual choice behavior    theoretical

analysis  Wiley   

Maystre     and Grossglauser     Robust active ranking
from sparse noisy comparisons  In arXiv 
 

Active Learning for TopK Rank Aggregation from Noisy Comparisons

Negahban     Oh     and Shah     Rank centrality  Ranking from pairwise comparisons  Operations Research 
 

Shah        and Wainwright        Simple  robust and
optimal ranking from pairwise comparisons    URL
http arxiv org abs 

Shah        Balakrishnan     Guntuboyina     and Wainwright        Stochastically transitive models for pairwise
comparisons  Statistical and computational issues  International Conference on Machine Learning   

Sz   nyi     BusaFekete     Paul     and   llermeier    
Online rank elicitation for PlackettLuce    dueling bandits approach  In Neural Information Processing Systems 
 

Tschopp     Diggavi     Delgosha     and Mohajer    
Randomized algorithms for comparisonbased search  In
Neural Information Processing Systems   

Wauthier     Jordan     and Jojic     Ef cient ranking
from pairwise comparisons  In International Conference
on Machine Learning  pp     

Yue     Broder     Kleinberg     and Joachims     The
karmed dueling bandits problem  Journal of Computer
and System Sciences   

