Bayesian Models of Data Streams with Hierarchical Power Priors

Andr es Masegosa     Thomas    Nielsen   Helge Langseth   Dar   RamosL opez   Antonio Salmer on  

Anders    Madsen    

Abstract

Making inferences from data streams is   pervasive problem in many modern data analysis applications  But it requires to address the problem of continuous model updating  and adapt to
changes or drifts in the underlying data generating distribution 
In this paper  we approach
these problems from   Bayesian perspective covering general conjugate exponential models  Our
proposal makes use of nonconjugate hierarchical priors to explicitly model temporal changes
of the model parameters  We also derive   novel
variational inference scheme which overcomes
the use of nonconjugate priors while maintaining the computational ef ciency of variational
methods over conjugate models  The approach is
validated on three real data sets over three latent
variable models 

  Introduction
Flexible and computationally ef cient models for streaming data are required in many machine learning applications  and in this paper we propose   new class of models
for these situations  Speci cally  we are interested in models suitable for domains that exhibit changes in the underlying generative process  Gama et al    We envision
  situation  where one receives batches of data at discrete
points in time  As each new batch arrives  we want to glean
information from the new data  while also retaining relevant information from the historical observations 
Our modelling is inspired by previous works on Bayesian
recursive estimation    Ozkan et al      arn     
power priors  Ibrahim   Chen    and exponential for 

 Department of Mathematics  Unversity of Almer    Almer   
Spain  Department of Computer and Information Science  Norwegian University of Science and Technology  Trondheim  Norway  Department of Computer Science  Aalborg University  Aalborg  Denmark  Hugin Expert      Aalborg  Denmark  Correspondence to  Andr es Masegosa  andresmasegosa ual es 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

getting approaches  Honkela   Valpola    However 
all of these methods were developed for slowly changing processes  where the rate of change anticipated by the
model is controlled by   quantity that must be set manually  Our solution  on the other hand  can accommodate
both gradual and abrupt concept drift by continuously assessing the similarity between new and historic data using
  fully Bayesian paradigm 
Building Bayesian models for data streams raises computational problems  as data may arrive with high velocity
and is unbounded in size  We therefore develop an approximate variational inference technique based on   novel
lowerbound of the data likelihood function  The appropriateness of the approach is investigated through experiments
using both synthetic and reallife data  giving encouraging results  The proposed methods are released as part
of an opensource toolbox for scalable probabilistic machine learning  http www amidsttoolbox com 
 Masegosa et al        Caba nas et al   

  Preliminaries
In this paper we focus on conjugate exponential Bayesian
network models for performing Bayesian learning on
streaming data  To simplify the presentation  we shall initially focus on the model structure shown in Figure      
This model includes the observed data     xi     global
hidden variables  or parameters             set of local
hidden variables           and   vector of  xed  hyper 
parameters denoted by   Notice how the dynamics of the
process is not included in the model of Figure       the
model will be set in the context of data streams in Section   where we extend it to incorporate explicit dynamics
over the  global  parameters to capture concept drift 
With the conditional distributions in the model belonging
to the exponential family  we have that all distributions are
of the following form
ln      pa      
ln hY       pa      tY        aY     pa    

where pa     denotes the parents of   in the directed
acyclic graph of the induced Bayesian network model  The
scalar functions hY and aY   are the base measure and

Bayesian Models of Data Streams with HPPs

 

  

 

 

   

  

zi

xi

zi  

xi  

               

               

   

   

Figure   Left  gure displays the core of the probabilistic model
examined in this paper  Right  gure includes   temporal evolution
model for    as described in Section  

the lognormalizer  respectively  the vector functions     
and tY   are the natural parameters and the suf cient
statistics vectors  respectively  The subscript   means that
the associated functional forms may be different for the different factors of the model  but we may remove the subscript when clear from the context  By also requiring that
the distributions are conjugate  we have that the posterior
distribution for each variable in the model has the same
functional form as its prior distribution  Consequently 
learning       conditioning the model on observations  only
changes the values of the parameters of the model  and not
the functional form of the distributions 
Variational inference is   deterministic technique for  nding tractable posterior distributions  denoted by    which
approximates the Bayesian posterior          that is often intractable to compute  More speci cally  by letting  
be   set of possible approximations of this posterior  variational inference solves the following optimization problem
for any model in the conjugate exponential family 

KL             

 

min

     

where KL denotes the KullbackLeibler divergence between two probability distributions 
In the mean  eld variational approach the approximation
family   is assumed to fully factorize  Extending the notation of Hoffman et al    we have that

  cid 

         

      

  cid 

  cid 

  zi       

  

  

  

where   is the number of local hidden variables  which is
assumed  xed for all                  The parameterizations
of the variational distributions are made explicit  in that  
parameterize the variational distribution of   while   has
the same role for the variational distribution of   
To solve the minimization problem in Equation   the

variational approach exploits the transformation
ln                      KL               
 
where    is   lower bound of ln       since KL is nonnegative    and    are introduced in     notation to make
explicit the function   dependency on    the data sample 
and     the natural parameters of the prior over   As
ln       is constant  minimizing the KL term is equivalent
to maximizing the lower bound  Variational methods maximize this lower bound by applying   coordinate ascent that
iteratively updates the individual variational distributions
while holding the others  xed  Winn   Bishop    The
key advantage of having   conjugate exponential model is
that the gradients of the   function can be always computed
in closed form  Winn   Bishop   

  Related Work
Bayesian inference on streaming data has been widely studied  Ahmed et al    Doucet et al    Yao et al 
  In the context of variational inference  there are two
main approaches  Ghahramani   Attias   Broderick et al    propose recursive Bayesian updating of
the variational approximation  The streaming variational
Bayes  SVB  algorithm  Broderick et al    is the most
known approach of this category  Alternatively  one could
cast the inference problem as   stochastic optimization
problem  Stochastic variational inference  SVI   Hoffman
et al    and the closely related population variational
Bayes  PVB   McInerney et al    are prominent examples from this group  SVI assumes the existence of    xed
data set observed in   sequential manner  and in particular
that this data set has   known  nite size  This is unrealistic
when modeling data streams  PVB addresses this problem
by using the frequentist notion of   population distribution 
   which is assumed to generate the data stream by repeatedly sampling   data points at the time    parameterizes
the size of the population  and helps control the variance of
the population posterior  Unfortunately    must be speci 
 ed by the user  No clear rule exists regarding how to set
it  and McInerney et al    show that its optimal value
may differ from one data stream to another 
The problem of Bayesian modeling of nonstationary data
streams       with concept drift  Gama et al    is
not addressed by SVB  as it assumes data exchangeability  An online variational inference method  which exponentially forgets the variational parameters associated with
old data  was proposed by Honkela   Valpola   The
socalled power prior approach  Ibrahim   Chen    is
also based on an exponential forgetting mechanisms  and
has nice theoretical properties  Ibrahim et al    Nevertheless  both approaches rely on   hyperparameter determining forgetting  which has to be set manually  PVB can

Bayesian Models of Data Streams with HPPs

also adapt to concept drift  because the variance of the variational posterior never decreases below   given threshold
indirectly controlled by    but again  the hyperparameter
has to be set manually 
  time series based modeling approach for concept drift using implicit transition models was pursued by  Ozkan et al 
    arn     Unfortunately  the implicit transition model depends on   hyperparameter determining the
forgettingfactor  which has to be manually set  In this paper we build on this approach  adapt it to variational settings  and place   hierarchical prior on its forgetting parameter  This greatly improves the  exibility and accuracy of
the resulting model when making inferences over drifting
data streams 

  Hierarchical Power Priors
In this section we extend the model in Figure       to also
account for the dynamics of the data stream being modeled 
We shall here assume that only the parameters   in Figure       are timevarying  which we will indicate with the
subscript             First we brie   describe the approach
on which the proposed model is based  Afterwards  we introduce the hierarchical power prior and detail   variational
inference procedure for this model class 

  Power Priors as Implicit Transition Models

In order to extend the model in Figure       to data streams 
we may introduce   transition model        to explicitly model the evolution of the parameters over time  enabling the estimation of the predictive density at time   
          
                  
 
However  this approach introduces two problems  First of
all  in nonstationary domains we may not have   single
transition model or the transition model may be unknown 
Secondly  if we seek to position the model within the conjugate exponential family in order to be able to compute
the gradients of   in closedform  we need to ensure that
the distribution family for    is its own conjugate distribution  thereby severely limiting model expressivity  we can 
     not assign   Dirichlet distribution to    
Rather than explicitly modeling the evolution of the    parameters as in Equation   we instead follow the approach
of   arn     and  Ozkan et al    who de ne the
time evolution model implicitly by constraining the maximum KL divergence over consecutive parameter distributions  Speci cally  by de ning
          

                    
 

 cid 

 cid 

one can restrict
the space of possible distributions
         supported by an unknown transition model 
by the constraint

KL                      

 

  arn     and  Ozkan et al    seek to approximate
         by the distribution           having maximum entropy under the constraint in   for continuous
distributions the maximum entropy can be formulated relative to an uninformative prior density pu    which corresponds to the KullbachLeibler divergence between the
two distributions  This approach ensures that we will not
underestimate the uncertainty in the parameter distribution
and the particular solution being sought takes the form
                        tpu     

 
where            is indirectly de ned by   which in turn
depends on the user de ned parameter  
In our streaming data setting we follow assumed density  ltering  Lauritzen    and the SVB approach
 Broderick et al    and employ the approximation
                  where        is
the variational distribution calculated in the previous time
step  Using this approximation in   and   we can express    in terms of     in which case   becomes
                    tpu     

 

which we use as the prior density for time step    Now  if
pu    belong to the same family as        then
            will stay within the same family and have
natural parameters                 where    are the
natural parameters of pu    Thus  under this approach 
the transitioned posterior remains within the same exponential family  so we can enjoy the full  exibility of the
conjugate exponential family       computing gradients of
the   function in closed form  an option that would not
be available if one were to explicitly specify   transition
model as in Equation  
So  at each time step  we simply have to solve the following variational problem  where only the prior changes with
respect to the original SVB approach 

        xt                   

arg max
    

As stated in the following lemma  this approach coincides
with the socalled power priors approach  Ibrahim   Chen 
    term that we will also adopt in the following 
Lemma   The Bayesian updating scheme described by
Figure       and Equation   but with     xed to   constant value  is equivalent to the recursive application of

Bayesian Models of Data Streams with HPPs

the Bayesian updating scheme of power priors  Ibrahim  
Chen    This scheme is expressed as follows 
                      

where    and    is the observation at time    historical
observation  and time    current observation  respectively 

Proof sketch  Translate the recursive Bayesian updating
approach of power priors into an equivalent two time
slice model  where   is given   prior distribution  
and    is   Dirac delta function 
The distribution           in this model
to
          which  in turn  is equivalent  up to proportionality  to              Note that the last
   term can alternatively be expressed as            
               
 cid 

is equivalent

The perspective provided by Lemma   introduces   well
known result of power priors  which is also applicable in
the current context  see the discussion after Theorem   in
 Ibrahim et al     the power prior is an optimal prior
to use and in fact minimizes the convex combination of KL
divergences between two extremes  one in which no historical data is used and the other in which the historical
data and current data are given equal weight  As noted
in  Olesen et al     Ozkan et al    this schema
works as   moving window with exponential forgetting of
past data  where the effective number of samples or  more
technically  the socalled equivalent sample size of the posterior  Heckerman et al    converges to 

lim
   ESSt  

 xt 
     

 

if the size of the data batches is constant 
For the experimental results reported in Section   we shall
refer to the method outlined above as SVB with power priors  SVBPP 

  The Hierarchical Power Prior Model
In the approach taken by  Ozkan et al     and  by extension  SVBPP  the forgetting factor    is userde ned 
In this paper  we instead pursue    hierarchical  Bayesian
approach and introduce   prior distribution over    allowing
the distribution over     and thereby the forgetting mechanism  to adapt to the data stream 
As we shall see below  in order to support   variational updating scheme we need to restrict the prior distribution over
    effectively limiting the choice of prior distribution to

 For instance  the ESS of   Beta distribution is equal to the
sum of the components of    and  in turn  equal to the number of
data samples seen so far plus the prior   pseudosamples 

either an exponential distribution or   normal distribution
with  xed variance  both of which should be truncated to
the interval     Unless explicitly stated otherwise  we
shall for now assume   truncated exponential distribution
with natural parameter   as prior distribution over    

      

  exp   
    exp 

 

 

The resulting model can be illustrated as in Figure      
We shall refer to models of this type as hierarchical power
prior  HPP  models 

  Variational Updating

For updating the model distributions we pursue   variational approach  where we seek to maximize the evidence
lower bound   in Equation   for time step    However 
since the model in Figure       does not de ne   conjugate
exponential distribution due to the introduction of      we
cannot maximize   directly  Instead we will derive    double  lower bound              and use this lower bound as  
proxy for the updating rules for the variational posteriors 
bound
First
LHP              xt     
the
HPP model we obtain
LHP              xt        Eq ln   xt  Zt   
  Eq ln            
  Eq        Eq ln   Zt   
  Eq          Eq       

lower
in Equation  

instantiating

 

for

of

all 

by

the

where    is the variational parameter for the variational distribution for     as we shall see later     is   scalar and is
therefore not shown in boldface  For ease of presentation
we shall sometimes drop from LHP              xt     
the subscript as well as the explicit speci cation of the parameters when these is otherwise clear from the context 
We now de ne  LHP              xt      as
 LHP              xt        Eq ln   xt  Zt   
  Eq   Eq ln              Eq   Eq ln pu   
  Eq        Eq ln   Zt   
  Eq          Eq       
which provide   lower bound for   
Theorem    LHP   gives   lower bound for LHP    
 LHP              xt        LHP              xt     

 

Proof sketch  The inequality derives by using Equation   and observing that ag                   
 tag             ag    because the lognormalizer

Bayesian Models of Data Streams with HPPs

ag is always   convex function  Wainwright et al   
 cid 
Full details are given in the supplementary material 
Rather than seeking to maximize   we will instead maximize     The gap between the two bounds is determined
only by the lognormalizer of            

         Eq tag             ag   
  ag                 

 

Thus  maximizing    wrt  the variational parameters    and
  also maxmizes    By the same observation  we also have
that the  natural  gradients are consistent relative to the two
bounds 
Corollary  
  

      

    

  

  

      

Proof  Follows immediately from Equation   because
 cid 
the difference does not depend of    and    

Thus  updating the variational parameters    and    in HPP
models can be done as for regular conjugate exponential
models of the form in Figure  
In order to update    we rely on     which we can maximize
using the natural gradient wrt      Sato    and which
can be calculated in closed form for   restricted distribution
family for    
Lemma   Assuming that the suf cient statistics function
for    is the identity function             then we have

  

    KL        pu   

  KL                       

 

Proof sketch  Based on   straightforward algebraic derivation of the gradient using standard properties of the exponential family  Full details are given in the supplementary
 cid 
material 

Note that the truncated exponential distribution  see Equation   satis es the restriction expressed in Lemma   and
also note that the variational posterior        will be  
truncated exponential density too 
On the other hand  observe that the form of the natural gradient of    have an intuitive semantic interpretation  which also extends to the coordinate ascent variational message passing framework  Winn   Bishop   
as shown by Masegosa et al      Speci cally  using the constant   as   threshold  we see that if the uninformed prior pu    provides   better    to the variational posterior at time   than the variational parameters
   from the previous time step  KL        pu     

Figure   Relationship between    and Eq   

    KL               then we will get   negative value for    when performing coordinate ascent using
Equation   This in turn implies that Eq      because Eq                   plotted in Figure  
which means that we have   higher degree of forgetting for
past data  If        then Eq      and less past data is
forgotten  Figure   graphically illustrates this tradeoff 

  The Multiple Hierarchical Power Prior Model

The HPP model can immediately be extended to include
multiple power priors    
  one for each global parameter
 
    In this model the    
     are pairwise independent  The
latter ensures that optimizing the    can be performed as
above  since the variational distribution for each    
can
 
be updated independently of the other variational distribu 
  for    cid     This extended model allows lotions over    
cal model substructures to have different forgetting mechanisms  thereby extending the expressivity of the model  We
shall refer to this extended model as   multiple hierarchical
power prior  MHPP  model 

 

  Experiments
  Experimental Setup

In this section we will evaluate the following methods 

  Streaming variational Bayes  SVB 
  Four versions of Population Variational Bayes
 PVB  Populationsize   equal to the average size
of each databatch  or   equal to    xed value     
    in Section   and         in Section  
Learningrate       or      

  Two versions of SVBPP        or      
  Two versions of SVBHPP    single shared    denoted SVBHPP  or separate     parameters  SVBMHPP 

The underlying variational engine is the VMP algorithm
 Winn   Bishop    for all models  VMP was termi 

 We do not compare with SVI  because SVI is   special case

of PVB when   is equal to the total size of the stream 

 wE          KL lt  au        KL lt  lt No drift zonew      KL lt  au        KL lt  lt Drift zoneBayesian Models of Data Streams with HPPs

justed  Notice that the values for this model is to be read off
the alternative yaxis  We can detect the the concept drift 
by identifying where the ESS rapidly declines 
Evolution of Expected Forgetting factor 
In Figure  
 right  the series denoted       shows the evolution
of Eq    for the arti cial data set  Notice how the model
clearly identi es abrupt concept drift at time steps      
and       The series denoted         illustrates
the evolution of the parameter when we increase the batch
size to   samples  We recognize   more con dent assessment about the absence of concept drift as more data is
made available 

  Evaluation using Real Data Sets

  DATA AND MODELS

For this evaluation we consider three real data sets from
different domains 
Electricity Market  Harries    The data set describes
the electricity market of two Australian states  It contains
  instances of   attributes  including   class label comparing the change of the electricity price related to   moving average of the last   hours  Each instance in the data
set represents   minutes of trading  during our analysis
we created batches such that xt contains all information
associated with month   
The data is analyzed using   Bayesian linear regression
model  The binary class label is assumed to follow   Gaussian distribution in order to    within the conjugate model
class  Similarly  the marginal densities of the predictive attributes are also assumed to be Gaussian  The regression
coef cients are given Gaussian prior distributions  and the
variance is given   Gamma prior  Note that the overall distribution does not fall inside the conditional conjugate exponential family  Hoffman et al    hence PVB cannot
be applied here  because lowerbound   gradient cannot be
computed in closedform 
GPS  Zheng et al        This data set contains     GPS trajectories  timestamped   and   coordinates  totalling more than   million observations  To
reduce the datasize we kept only one out of every ten measurements  We grouped the data so that xt contains all
data collected during hour   of the day  giving   total of
  batches of this stream 
Here we employ   model with one independent Gaussian
mixture model per day of the week  each mixture with  
components  This enables us to track changes in the users 
pro les across hours of the day  and also to monitor how
the changes are affected by the day of the week 
Finance  reference withheld  The data contains monthly
aggregated information about
the  nancial pro le of

Figure        in the BetaBinomial model arti cial data set

nated after   iterations or if the relative increase in the
lower bound fell below   All priors were uninformative  using either  at Gaussians   at Gamma priors or uniform Dirichlet priors  We set       for the HPP priors 
Variational parameters were randomly initialized using the
same seed for all methods 

  Evaluation using an Arti cial Data Set

First  we illustrate the behavior of the different approaches
in   controlled experimental setting  We produced an arti 
 cial data stream by generating   samples        xt   
  from   Binomial distribution at each time step  We arti cially introduce concept drift by changing the parameter
  of the Binomial distribution        for the  rst   time
steps  then       for the following   time steps  and
 nally       for the last   time steps  The data stream
was modelled using   BetaBinomial model 
Parameter Estimation  Figure   shows the evolution of
Eq    for the different methods  We recognize that SVB
simply generates   running average of the data  as it is not
able to adapt to the concept drift  The results from PVB
depend heavily on the learning rate   where the higher
learning rate  which results in the more aggressive forgetting  works better in this example  Recall  though  that  
needs to be handtuned to achieve an optimal performance 
As expected  the choice of   does not have an impact 
because the present model has no local hidden variables
 cf  Section   SVBPP produces results almost identical to PVB when   matches the learning rate of PVB      
          Finally  SVBHPP provides the best results 
almost mirroring the true model 
Equivalent Sample Size  ESS  Figure    left  gives the
evolution of the equivalent sample size  ESSt  for the different methods   The ESS of PVB is always given by the
constant    For SVB  the ESS monotonically increases
as more data is seen  while SVBPP exhibits convergence
to the limiting value computed in Equation     different
behaviour is observed for SVBHPP  It is automatically ad 

 For this model  ESS is simply computed by summing up the

components of the    de ning the Beta posterior 

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lRealSVBPVB PVB SVB PP SVB PP SVB HPPTime stepEstimated   valueBayesian Models of Data Streams with HPPs

Figure   ESSt  left  and Eq     right  in the BetaBinomial model arti cial data set

around     customers over    nonconsecutive 
months  Three attributes were extracted per customer  in
addition to   classlabel telling whether or not the customer
will default within the next   months 
We      na ve Bayes model to this data set  where the distribution at the leafnodes is  component mixture of Gaussians distribution  The distribution over the mixture node is
shared by all the attributes  but not between the two classes
of customers 
  detailed description of all the models  including their
structure and their variational families  is given at the supplementary material 

  EVALUATION AND DISCUSSION

 cid    xt  zt       xt dztd    Figure  

To evaluate the different methods discussed  we look at
the test marginal loglikelihood  TMLL  Speci cally  each
data batch is randomly split in   train data set  xt  and  
test data set   xt  containing two thirds and one third of
the data batch  respectively  Then  TMLLt is computed
as TMLLt    xt 
 left  shows for each method the difference between its
TMLLt and that obtained by SVB  which is considered the
baseline method  To improve readability  we only plot the
results of the best performing method inside each group of
methods  The righthand side of Figure   shows the development of Eq    over time for SVBHPP and SVBMHPP 
For SVBHPP we only have one  tparameter  and its value
is given by the solid line  SVBMHPP utilizes one     for
each variational parameter  In this case  we plot Eq   
 
at each point in time to indicate the variability between the
different estimates throughout the series  Finally  we compute each method   aggregated test marginal loglikelihood
   TMLLt  and report these values in Table  
For the electricity data set  we can see that the two proposed
methods  SVBHPP and SVBMHPP  perform best  All
models are comparable during the  rst nine months  which
is   period where our models detect no or very limited con 

measure cid  

 

 The numbers of variational parameters are     and   for

the Electricity  GPS and Financial model  respectively 

 

cept drift  cf  top right plot or Figure   However  after
this period  both SVBHPP and SVBMHPP detects substantial drift  and is able to adapt better than the other methods  which appear unable to adjust to the complex concept
drift structure in the latter part of the data  SVBHPP and
SVBMHPP continue to behave at   similar level  mainly
because when drift happens it typically includes   high proportion of the parameters of the model 
For the GPS data set  we can observe how the SVBMHPP
is superior to the rest of the methods  particularly towards
the end of the series  When looking at Figure    middle
right panel  we can see that   signi cative proportion of
      at
the model parameters are drifting       Eq   
all times  while another proportion of the parameters show
  quite stable behavior  values above   This complex
pattern is not captured well by SVBHPP  which ends up
assuming no concept drift after the initial timestep 
The  nancial data set shows   different behavior  During the  rst months  SVBMHPP slightly outperforms the
rest of the approaches  but after month   SVBPP with
      is superior  with SVBMHPP second  Looking at
the     
 values of SVBMHPP  we observe that there is
 
signi cant concept drift in some of the parameters over the
 rst few months  However  only   few parameters exhibit
noteworthy drift after the  rst third of the sequence  Apparently  the simple SVBPP approach has the upper hand
when the drift is constant and fairly limited  at least when
the optimal forgetting factor   has been identi ed 
We conclude this section by highlighting that the performance of SVBPP and PVB depend heavily on the hyperparameters of the model  cf  Table   As an example  consider SVBPP for the  nancial data set  While it was the
best overall with       it is inferior to SVBMHPP if
      Similarly  PVB   performance is sensitive both
to    see in particular the results for the GPS data  and  
 nancial data  These hyperparameters are hard to    
as their optimal values depend on data characteristics  see
Broderick et al    McInerney et al    for similar
conclusions  We therefore believe that the fully Bayesian
formulation is an important strong point of our approach 

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lSVBPVB SVB PP SVB HPPTime stepESS   Equivalent sample sizelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lE           Time stepE   Bayesian Models of Data Streams with HPPs

DATA SET

SVB

ELECTRICITY
GPS
FINANCE

 
 
 

PVB

PVB

PVB

                   xt       xt 
     
     

     

     

PVB

 
 

 
 

 
 

 
 

SVBPP
     

SVBPP
     

SVBHPP

SVBMHPP

 
 
 

 
 
 

 
 
 

 
 
 

Table   Aggregated Test Marginal LogLikelihood 

    TMLLt improvement over SVB

    Eq    for SVBHPP and SVBMHPP

Figure   Results of the competing methods for the three reallife data sets  See text for discussion 

  Conclusions and Future Work
We have introduced   new class of Bayesian models for
streaming data  able to capture changes in the underlying
generative process  Unlike existing solutions to this problem  aimed at modeling slowly changing processes  our
proposal is able to handle both abrupt and gradual concept
drift following   Bayesian approach  The new model accounts for the dynamics of the data stream by assuming
that only the global parameters evolve over time  We intro 

duce the socalled hierarchical power priors  where   prior
on the learning rate is given allowing it to adapt to the data
stream  We have addressed the complexity of the underlying inference tasks by developing an approximate variational inference scheme that optimizes   novel lower bound
of the likelihood function 
As future work we aim to provide   sound approach to
semantically characterize concept drift by inspecting the
    

  values provided by SVBMHPP 

 

llllllllllllllllllllllllllllllll lSVB HPPSVB MHPPSVB PP SVB PP ElectricityTime stepTMLL Test marginal log likelihood       HPPE     MHPPElectricityTime stepE   llllllllllllllllllllllll lPVB xt SVB HPPSVB MHPPSVB PP GPSTime stepTMLL Test marginal log likelihood       HPPE     MHPPGPSTime stepE   lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lPVB xt SVB HPPSVB MHPPSVB PP FinanceTime stepTMLL Test marginal log likelihood       HPPE     MHPPFinanceTime stepE   Bayesian Models of Data Streams with HPPs

Acknowledgements
This work was partly carried out as part of the AMIDST
project  AMIDST has received funding from the European Union   Seventh Framework Programme for research 
technological development and demonstration under grant
agreement no   Furthermore 
this research has
been partly funded by the Spanish Ministry of Economy
and Competitiveness 
through projects TIN 
JIN  TIN      TIN     and
by ERDF funds 

References
Ahmed  Amr  Ho  Qirong  Teo  Choon Hui  Eisenstein  Jacob  Smola  Alexander    and Xing  Eric    Online inference for the in nite topiccluster model  Storylines from
streaming text  In AISTATS  pp     

Broderick  Tamara  Boy  Nicholas  Wibisono  Andre  Wilson  Ashia    and Jordan  Michael    Streaming variational Bayes  In Advances in Neural Information Processing Systems   pp    Curran Associates 
Inc   

Caba nas  Rafael  Mart nez  Ana    Masegosa  Andr es   
RamosL opez  Dar    Samer on  Antonio  Nielsen 
Thomas    Langseth  Helge  and Madsen  Anders   
Financial data analysis with PGMs using AMIDST  In
Data Mining Workshops  ICDMW    IEEE  th International Conference on  pp    IEEE   

Hoffman  Matthew    Blei  David    Wang  Chong  and
Paisley  John  Stochastic variational inference  Journal
of Machine Learning Research     

Honkela  Antti and Valpola  Harri  Online variational
Bayesian learning  In  th International Symposium on
Independent Component Analysis and Blind Signal Separation  pp     

Ibrahim  Joseph   and Chen  MingHui  Power prior distributions for regression models  Statistical Science  pp 
   

Ibrahim  Joseph    Chen  MingHui  and Sinha  Debajyoti 
On optimality properties of the power prior  Journal of
the American Statistical Association   
 

  arn    Miroslav  Approximate Bayesian recursive estima 

tion  Information Sciences     

Lauritzen  Steffen    Propagation of probabilities  means 
and variances in mixed graphical association models 
Journal of
the American Statistical Association   
   

Masegosa        Mart nez        Langseth     Nielsen 
      Salmer on     RamosL opez     and Madsen 
      dVMP  Distributed variational message passing 
In PGM  JMLR  Workshop and Conference Proceedings  volume   pp       

Doucet  Arnaud  Godsill  Simon  and Andrieu  Christophe 
On sequential Monte Carlo sampling methods for
Bayesian  ltering  Statistics and computing   
   

Masegosa  Andres    Martinez  Ana    and Borchani  Hanen  Probabilistic graphical models on multicore cpus
using Java   IEEE Computational Intelligence Magazine       

Gama  Jo ao   Zliobait    Indr    Bifet  Albert  Pechenizkiy 
Mykola  and Bouchachia  Abdelhamid    survey on
concept drift adaptation  ACM Computing Surveys   
   

Ghahramani  Zoubin and Attias     Online variational
Bayesian learning  In Slides from talk presented at NIPS
workshop on Online Learning   

Harries  Michael  Splice  comparative evaluation  Electricity pricing  NSWCSE TR  School of Computer Siene and Engineering  The University of New
South Wales   

Heckerman  David  Geiger  Dan  and Chickering  David   
Learning Bayesian networks  The combination of
knowledge and statistical data  Machine learning   
   

Masegosa  Andr es    Mart nez  Ana    RamosL opez 
Dar    Caba nas  Rafael  Salmer on  Antonio  Nielsen 
Thomas    Langseth  Helge  and Madsen  Anders   
Amidst    Java toolbox for scalable probabilistic machine learning  arXiv preprint arXiv   

McInerney  James  Ranganath  Rajesh  and Blei  David 
The population posterior and Bayesian modeling on
In Advances in Neural Information Processstreams 
ing Systems   pp    Curran Associates  Inc 
 

Olesen  Kristian    Lauritzen  Steffen    and Jensen 
Finn    ahugin    system creating adaptive causal probIn Proceedings of the Eighth interabilistic networks 
national conference on Uncertainty in arti cial intelligence  pp    Morgan Kaufmann Publishers Inc 
 

Bayesian Models of Data Streams with HPPs

 Ozkan  Emre 

 Smdl    aclav  Saha  Saikat  Lundquist 
Christian  and Gustafsson  Fredrik  Marginalized adaptive particle  ltering for nonlinear models with unknown
timevarying noise parameters  Automatica   
    ISSN  

Sato  MasaAki  Online model selection based on the variational Bayes  Neural Computation   
 

Wainwright  Martin    Jordan  Michael    et al  Graphical
models  exponential families  and variational inference 
Foundations and Trends   cid  in Machine Learning   
   

Winn  John    and Bishop  Christopher    Variational
Journal of Machine Learning Re 

message passing 
search     

Yao  Limin  Mimno  David  and McCallum  Andrew  Ef 
 cient methods for topic model inference on streaming
document collections  In Proceedings of the  th ACM
SIGKDD international conference on Knowledge discovery and data mining  pp    ACM   

Zheng  Yu  Li  Quannan  Chen  Yukun  Xie  Xing  and
Ma  WeiYing  Understanding mobility based on gps
data  In Proceedings of the  th International Conference on Ubiquitous Computing  UbiComp   pp   
  New York  NY  USA    ACM 
ISBN  
  doi   

Zheng  Yu  Zhang  Lizhu  Xie  Xing  and Ma  WeiYing 
Mining interesting locations and travel sequences from
gps trajectories  In Proceedings of the  th International
Conference on World Wide Web  WWW   pp   
  New York  NY  USA    ACM 
ISBN  
  doi   

Zheng  Yu  Xie  Xing  and Ma  WeiYing  Geolife    collaborative social networking service among user  location and trajectory  IEEE Data Eng  Bull   
 

