Prediction and Control with Temporal Segment Models

Nikhil Mishra   Pieter Abbeel     Igor Mordatch  

Abstract

We introduce   method for learning the dynamics
of complex nonlinear systems based on deep generative models over temporal segments of states
and actions  Unlike dynamics models that operate over individual discrete timesteps  we learn
the distribution over future state trajectories conditioned on past state  past action  and planned
future action trajectories  as well as   latent prior
over action trajectories  Our approach is based
on convolutional autoregressive models and variational autoencoders  It makes stable and accurate predictions over long horizons for complex 
stochastic systems  effectively expressing uncertainty and modeling the effects of collisions  sensory noise  and action delays  The learned dynamics model and action prior can be used for
endto end  fully differentiable trajectory optimization and modelbased policy optimization 
which we use to evaluate the performance and
sampleef ciency of our method 

  Introduction
The problem of learning dynamics   where an agent learns
  model of how its actions will affect its state and that of
its environment   is   key open problem in robotics and
reinforcement learning  An agent equipped with   dynamics model can leverage modelpredictive control or modelbased reinforcement learning  RL  to perform   wide variety of tasks  whose exact nature need not be known in
advance  and without additional access to the environment 
In contrast with modelfree RL  which seeks to directly
learn   policy  mapping from states to actions  in order to
accomplish   speci   task  learning dynamics has the advantage that dynamics models can be learned without taskspeci   supervision  Since dynamics models are decoupled
from any particular task  they can be reused across different

 University of California  Berkeley  OpenAI  Correspondence

to  Nikhil Mishra  nmishra berkeley edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tasks in the same environment  Additionally  learning differentiable dynamics models  such as those based on neural
networks  enables the use of endto end backpropagationbased methods for policy and trajectory optimization that
are much more ef cient than modelfree methods 
Typical approaches to dynamics learning build   onestep
model of the dynamics  predicting the next state as   function of the current state and the current action  However 
when chained successively for many timesteps into the future  the predictions from   onestep model tend to diverge
from the true dynamics  either due to the accumulation of
small errors or deviation from the regime represented by
the data the model was trained on  Any learned dynamics
model is only valid under the distribution of states and actions represented by its training data  and onestep models
make no attempt to deal with the fact that they cannot make
accurate predictions far outside this distribution 
When the true dynamics are stochastic  or the sensory measurements noisy or unreliable  these problems are only exacerbated  Moreover  the dynamics may be inherently dif 
 cult to learn  bifurcations such as collisions induce sharp
changes in state that are hard to model with certainty when
looking at   single timestep  There may also be hysteresis effects such as gear backlash in robots  or highorder
dynamics in hydraulic robot actuators and human muscles
that require looking at   history of past states 
We present   novel approach to learning dynamics based on
  deep generative model over temporal segments  we wish
to model the distribution over possible future state trajectories conditioned on planned future actions and   history
of past states and actions  By considering an entire segment of future states  our approach can model both uncertainty and complex interactions  like collisions  holistically
over   segment  even if it makes small errors at individual
timesteps  We also model   prior over action segments using   similar generative model  which can be used to ensure
that the action distribution explored during planning is the
same as the one the model was trained on  We show that
that our method makes better predictions over long horizons than onestep models  is robust to stochastic dynamics and measurements  and can be used in   variety of control settings while only considering actions from the regime
where the model is valid 

Prediction and Control with Temporal Segment Models

  Related Work
  number of options are available for representation of
learned dynamics models  including linear functions  Mordatch et al    Yip   Camarillo    Gaussian processes  Boedecker et al    Ko   Fox    Deisenroth   Rasmussen    predictive state representations
 PSRs   Littman et al    Rosencrantz et al    and
deep neural networks  Punjani   Abbeel    Fragkiadaki et al    Agrawal et al    Linear functions
are ef cient to evaluate and solve controls for  but have
limited expressive power  Gaussian processes  Williams  
Rasmussen    provide uncertainty estimates  but scaling them to large datasets remains   challenge  Shen et al 
Lawrence et al    PSRs and variants make multistep
predictions  but suffer from the same scalability challenges
as Gaussian processes  Our method combines the expressiveness and scalability of neural networks with the ability
to provide sampling and uncertainty estimates  modeling
entire segments to improve stability and robustness 
An alternative is to learn dynamics models in an online
fashion  constantly adapting the model based on an incoming stream of observed states and actions  Fu et al   
Mordatch et al    Yip   Camarillo    Lenz et al 
  However  such approaches are slow to adapt to
rapidlychanging dynamics modes  such as those arising
when making or breaking contact  and may be problematic
when applied on robots performing rapid motions 
Several approaches exist to improve the stability of models
that make sequential predictions  Abbeel   Ng   and
Venkatraman et al    consider alternative loss functions that improve robustness over long prediction horizons  Bengio et al    and Venkatraman et al   
also use simple curricula for   similar effect  While they all
consider multistep prediction losses  they only do so in the
context of training models that are intrinsically onestep 
Existing methods for video prediction  Finn   Levine 
  Oh et al    look at   history of previous states
and actions to predict the next frame  we take this   step
further by modeling   distribution over an entire segment
of future states that is also conditioned on future actions  In
this work  we focus on demonstrating the bene ts of   probabilistic segmentbased approach  these methods could easily be incorporated with ours to learn dynamics from images  but we leave this to future work 
Watter et al    and Johnson et al    use variational autoencoders to learn   lowdimensional latentspace
representation of image observations  Finn et al   
takes   similar approach  but without the variational aspect  These works utilized autoencoders as   means of dimensionality reduction  rather than for temporal coherence
like we do  to enable the use of existing control algorithms
based on locallylinear onestep dynamics models 

Temporallyextended actions were shown to be effective
in reinforcement learning  such as the options framework
 Sutton et al      or sequencing of subplans  Vezhnevets et al    Considering entire trajectories as opposed to single timesteps can also lead to simpler control policies  For example  there are effective and simple manuallydesigned control laws  Raibert     Pratt
et al    that formulate optimal actions as   function of
the entire future trajectory rather than   single future state 

  SegmentBased Dynamics Model
Suppose we have   nonlinear dynamical system with states
xt and actions ut  The conventional approach to learning
dynamics is to learn   function xt       xt  ut  using an
approximator such as   neural network  possibly recurrent 
We consider   more general formulation of the problem 
which is depicted in Figure   given segments  of length   
of past states       xt             xt  and actions     
 ut             ut  we wish to predict the entire segment of
future states        xt          xt    that would result from
taking actions        ut          ut    Treating these
four temporal segments as random variables  then we wish
to learn the conditional distribution                
We introduce dependency on past actions    to support
dynamics with delayed or  ltered actions 

Figure   An overview of the probabilistic model we wish to
learn  Given observed past states and actions        blue  and
planned future actions      green  we wish to sample possible
future state trajectories      red 

variational

With this in mind  we propose the use of   deep
conditional
 Kingma  
autoencoder
learn the distribuWelling   
our encoder will
tion                     over
latent codes   
learn to reconstruct     from
and our decoder will
          and   sample from    modeling the distribution                      Note that the random
variable   is   vector that describes an entire segment of
states      After training is complete  we can discard the

      UXtt HtHPrediction and Control with Temporal Segment Models

encoder  and the decoder will allow us to predict the future
state trajectory      using           as desired  sampling latent codes from an enforced prior               
Empirically  we observe that having the encoder model
          instead of                     gives
equivalent performance  and so we take this approach in
all of our experiments for simplicity 

  Model Architecture and Training

In the previous section we discussed   conditional variational autoencoder whose generative path serves as  
stochastic dynamics model  Here we will expand on some
of the architectural details    diagram of the entire training
setup is shown in Figure   For more details of the architectures used in our experiments  see Appendix   

Figure   The dynamics model that we learn  The encoder     
parametrizes   diagonal Gaussian distribution             
   
over latent codes describing   state trajectory     The autoregressive decoder       takes in segments of past states    past
actions    and future actions     along with   sample from   
and uses dilated causal convolutions to reconstruct    
The encoder network      explicitly parametrizes   Gaussian distribution over latent codes   with diagonal covariance 
It consists of   stack of  Dconvolutional layers 
whose output is  attened and projected into   single vector containing   mean    and variance  
   We then sample            
   in   differentiable manner using the
reparametrization trick  Kingma   Welling   
The decoder network       seeks to model   distribution
over   segment of states             xt          xt     The
causal nature of this segment    particular timestep is only

affected by the ones that occur before it  suggests that an
autoregressive model with dilated convolutions is appropriate  similar to architectures previously used for modeling audio  van den Oord et al      and image  van den
Oord et al      data  Like these works  we use layers
with the following activation function 

  kz 

tanh Wf            

  kz   cid   Wg            

 
where   denotes convolution   cid  denotes elementwise multiplication    is the sigmoid function    is the input to
the layer    is   latent code sampled from the output of
the decoder  and      are network weights to be learned 
We found that residual layers and skip connections between
layers give slightly better performance but are not essential 
We train the model parameters endto end  minimizing the
  loss between     and its reconstruction      along with
the KLdivergence of the latent code            
   from
       similarly to Kingma   Welling  

  Control with SegmentBased Models
Once we have learned   dynamics model  we want to utilize
it in order to accomplish different tasks  each of which can
be expressed as reward function   xt  ut  Trajectory optimization and policy optimization are two settings where
  dynamics model would commonly be used  and provide
meaningful ways with which to evaluate   dynamics model 

  Trajectory Optimization

In trajectory optimization  we wish to  nd   sequence of
actions that can be applied to accomplish   particular instance of   task  Speci cally  given   reward function    we
want to maximize the sum of rewards along the trajectory
that results from applying the actions            uT   beginning from an initial state    This can be summarized by
the following optimization problem 

 cid    cid 

 cid 

max
  uT

  xt  ut 

 
with xt        xt          

  

 

where   xt  ut  is the reward received at time    and    
            xT  is the sequence of states that would result
from taking actions                 uT  from initial state
   under dynamics model       The expectation is taken
over state trajectories sampled from the model 

  Latent Action Priors

If we attempt to solve the optimization problem as posed
in   the solution will often attempt to apply action sequences outside the manifold where the dynamics model

        ZZU UXP   Prediction and Control with Temporal Segment Models

is valid  these actions come from   very different distribution than the action distribution of the training data  This
can be problematic  the optimization may  nd actions that
achieve high rewards under the model  by exploiting it in
  regime where it is invalid  but that do not accomplish the
goal when they are executed in the real environment 
To mitigate this problem  we propose the use of another
conditional variational autoencoder  this one over segments
of actions 
In particular  given sequences of past actions       ut             ut  and future actions      
 ut          ut    we wish to model the the conditional distribution          The encoder learns           and
the decoder learns               We condition on   
to support temporal coherence in the generated action sequence  Like the dynamics model introduced in Section
  the encoder uses  Dconvolutional layers  and the decoder is autoregressive  with dilated causal convolutions 
The latent space that this autoencoder learns describes  
prior over actions that can be used when planning with  
dynamics model  hence we refer to this autoencoder over
action sequences as   latent action prior 
To incorporate   latent action prior  we divide an action
sequence                 uT  into segments          UK of
length    where   is determined such that     HK  and
       Then we can generate action sequences that are
similar to the ones in our training set by sampling different
latent codes            zK and using the decoder to sample
from      Uk Uk  zk                  The optimization problem posed in   can then be expressed as 

 cid 

 cid    cid 

  

 

  xt  ut 

max
  zK
with xt        xt          
ut        ut          

Figure   Trajectory optimization over latent codes  blue  The
action sequences are generated using the latent codes and the latent action prior       and the state trajectory using the generated
actions and the dynamics model      

in policy optimization  where the agent learns   policy that
dictates optimal behavior in order to accomplish the task in
the general case  In particular    policy is   learned function
 with parameters   that de nes   conditional distribution
over actions given states  denoted       The value of  
policy is de ned as the expected sum of discounted rewards
when acting under the policy  and can be expressed as 

 cid   cid 

 cid 

     

       xt  ut 

 

 

where the actions            uT and states            xT are
generated by the latent action prior and dynamics model
 see Figure   for an illustration  Since the dynamics model
is differentiable  the above optimization problem can be
solved endto end with backpropagation  While it is still
nonconvex  we are optimizing over fewer variables  and the
possible action sequences that are explored will be from the
same distribution as the model   training data  Moreover 
the gradients of the rewards with respect to the latent codes
are likely to have stronger signal than those with respect to
  single action  We used Adam  Kingma   Ba    with
step size   to perform this optimization and found that
it generally converged in around   iterations 

  Policy Optimization

Trajectory optimization enables an agent to accomplish  
single instance of   task  but more often  we are interested

  

where the actions are sampled from  ut xt  and   is  
discount factor  The goal of policy optimization is to maximize the value of the policy with respect to its parameters 
The class of algorithms known as policy gradient methods  Sutton et al      Peters   Schaal    attempt
to solve this optimization problem without considering  
dynamics model  They execute   policy   to get samples
                 xT   uT   rT from the environment  and then
update   to get an improved policy  relying on likelihood
ratio methods  Williams    to estimate the gradient  
 
because they cannot directly compute the derivatives of the
rewards   xt  ut  with respect to the actions            ut 
Modelbased policy optimization can be more ef cient than
traditional policygradient methods  because the gradient
 
  can be computed directly by backpropagation through
  differentiable model  However  its success hinges on the

                Ht Ht HtP           Prediction and Control with Temporal Segment Models

accuracy of the dynamics model  as the optimization can
exploit  aws in the model in the same way as discussed
in Section   Heess et al    use   modelbased approach where   onestep dynamics model is learned jointly
with   policy in an online manner  To evaluate the robustness of our models  we experiment with learning policies
of ine  where the dynamics model is learned through unsupervised exploration of the environment  and no environment interaction is allowed beyond this exploration 
Instead of   onestep policy of the form  ut xt  we also
explored using   segmentbased policy          that
generates actions using latent action prior       as follows 
         xt             xt ut             ut 
sample             

 ut          ut                         
and then acts according to action ut  The resulting policy
will learn to accomplish the task while only considering
actions for which the dynamics model is valid  In terms
of the options framework  Sutton et al      we can
think of this policy as considering   continuous spectrum of
options  all of which are consistent with both past observed
states and actions  and the data distribution under which the
dynamics model makes good predictions 
  Experiments
Our experiments investigate the following questions 
    How well do segmentbased models predict dynamics 
 ii  How does prediction accuracy transfer to control applications  How does this scale with the dif culty of the task
and stochasticity in the dynamics 
 iii  How is this affected by the use of latent action priors 
 iv  Is there any meaning or structure encoded by the latent
space learned by the dynamics model 
  Environments
In order for   dynamics model to be versatile enough for
use in control settings  the training data needs to contain  
variety of actions that explore   diverse subset of the state
space  Ef cient exploration strategies are an open problem in reinforcement learning and are not the focus of this
work  With this in mind  we base our experiments on  
simulated  DOF arm moving in   plane  as implemented
in the Reacher environment in OpenAI Gym  because performing random actions in this environment results in suf 
cient exploration  We consider the following environments
throughout our experiments  illustrated in Figure  
    The basic  unmodi ed Reacher environment 
 ii    version containing an obstacle that the arm can collide with  the obstacle cannot move  but its position is randomly chosen at the start of each episode 
 iii    version in which the arm can push   damped cylin 

drical object around the arena 

Figure   The environments we used in our experiments  From
left to right      the unmodi ed Reacher environment   ii    version with an obstacle   iii    version with an object to push  The
blue marker is used to visualize the goal during experiments involving control but has no effect on the dynamics 
To learn   dynamics model  the training data consists of  
collection of trajectories               xT   uT from the environment we wish to model  We used   trajectories in
the basic environment  and   in the other two  For all
environments  the state representation consists of the joint
angles  the joint velocities  and the endeffector position 
when obstacles are present  their positions and velocities
are also included  The actions are always the torques for
the arm   joints  In all experiments  the training set is comprised of trajectories of length       of the arm executing smooth random torques  and we used segments of
length       and  dimensional latent spaces 
While the segment length and dimensionality of the latent
space could be varied  we found that these values were reasonable choices for these environments  As the segment
length approaches   the model degenerates into   onestep
model  and for longer segments  its performance plateaus
because the states towards the end of the segment become
independent of those at the beginning  Likewise  we observed that this latentspace dimensionality was   good
tradeoff between expressiveness and information density 

  Baselines

We compare our method against the following baselines 
      onestep model    learned function xt       xt  ut 
where   is   fullyconnected neural network  It is trained
using   onestep prediction   loss on tuples  xt  ut  xt 
 ii    onestep model that is rolled out several timesteps
at training time  The model is still   learned function
xt       xt  ut  but it is trained with   multistep prediction loss  over   horizon of length     While this does
not increase the model   expressive power  we expect it to
be more robust to the accumulation of small errors      
Venkatraman et al    Abbeel   Ng  
 iii  An LSTM model  which can store information about
the past in   hidden state ht  xt  ht       xt  ut  ht 
and is trained with the same multistep prediction loss  also
over   horizon of     We expect that the LSTM can learn
fairly complex dynamics  but the hidden state dependencies
can make trajectory and policy optimization more dif cult 

Prediction and Control with Temporal Segment Models

  Results
  DYNAMICS PREDICTION
After learning   dynamics model  we evaluate it on   test
set of heldout trajectories by computing the average loglikelihood of the test data under the model 
For our method  we do this by obtaining samples from the
model   tting   Gaussian to the samples  and determining the loglikelihood of the true trajectory under the  tted
Gaussian  Since the baseline methods do not express uncertainty  but are trained using   loss  we interpret their predictions as the mean of   Gaussian distribution whose variance is constant across all state dimensions and timesteps
 since minimizing   loss is equivalent to maximizing this
loglikelihood  We then    the value of the variance constant to maximize the loglikelihood on the test set 
Figure   compares our method to the baselines in each
environment  The values reported are loglikelihoods per
timestep  averaged over   test set of   trajectories  Our
model and the LSTM are competitive in the basic environment  and both substantially better than the onestep models  but the LSTM   performance degrades on more challenging environments with collisions 

 ii  Pushing Task  the arm must push   cylindrical object to
the desired position  Like in the reaching task  the reward
function is the negative distance between the object and the
target  again minus   penalty on large torques 
The trajectoryoptimization results are summarized in Figure   For each task and dynamics model  we sampled  
target positions uniformly at random  solved the optimization problem as described in   or   and then executed
the action sequences in the environment in open loop 
Under each model 
the optimization  nds actions that
achieve similar modelpredicted rewards  but the baselines
suffer from large discrepancies between model prediction
and the true dynamics  Qualitatively  we notice that  on the
pushing task  the optimization exploits the LSTM and onestep models to predict unrealistic state trajectories  such as
the object moving without being touched or the arm passing
through the object instead of colliding with it  Our model
consistently performs better  and  with   latent action prior 
the true execution closely matches the model   prediction 
When it makes inaccurate predictions  it respects physical invariants  such as objects staying still unless they are
touched  or not penetrating each other when they collide 

Figure   Prediction quality of our method compared to several
baselines in   range of environments  The reported values are the
average loglikelihood per timestep on   test set  higher is better 
Our method signi cantly outperforms the baseline methods  even
in environments with complex dynamics such as collisions 
  CONTROL 
Next  we compare our method to the baselines on trajectory and policy optimization  Of interest is both the actual
reward achieved in the environment  and the difference between the true reward and the expected reward under the
model  If   control algorithm exploits the model to predict
unrealistic behavior  then the latter will be large 
We consider two tasks 
    Reaching Task  the arm must move its end effector to  
desired position  The reward function is the negative distance between the end effector and the target position  minus   quadratic penalty on applying large torques 

  Videos of our experimental results can be seen here 

https sites google com site temporalsegmentmodels 

Figure   Trajectory optimization on the reaching and pushing
tasks  The top plot reports the negative reward from openloop execution of the returned action sequences  lower is better  averaged
over   trials  and the bottom shows the difference between true
reward and modelpredicted reward  Our model  with   latent action prior  achieves both the best inenvironment performance and
the smallest discrepancy between environment and model 
Figure   depicts the results from policyoptimization  Section   in the form of learning curves for each task and
dynamics model  See Appendix   for model architectures
and hyperparameters  For comparison  we also plot the
performance of   traditional policy gradient method  Although this method and ours eventually achieve similar performance  ours does so much more ef ciently  learning the
policy of ine with fewer samples from the model than the
traditional method needed from the environment 

BasicPushing ObjectWith ObstacleEnvironment OursLSTMOne StepOne Step  rolled outReachingPushing Negative Reward in EnvironmentReachingPushing Discrepancy between Model and EnvironmentOurs  with latent priorOurs  without latent priorLSTMOne StepOne Step  rolled outPrediction and Control with Temporal Segment Models

Figure   Learning curves of policy optimization on the reaching
and pushing tasks  top and bottom  respectively  The quantities
plotted are the true performance in the environment  episode
average reward  Not only does our dynamics model  with an
action prior  consistently perform the best  it is considerably faster
than   modelfree policy gradient method 

  SENSORY NOISE AND DELAYED ACTIONS

To explore the effects of stochastic dynamics and delayed actions  we consider two more modi cations of the
Reacher environment  one in which there is considerable
Gaussian noise in the state observations       on data
in the range     and one in which actions are delayed  they do not take effect for       timesteps after
they are applied  These challenges commonly arise in realworld robotics applications  Atkeson et al    and so it
is important to be able to learn   useful dynamics model
in either setting  For both the noisystate and delayedaction environments  we learn   dynamics model with each
method  and then use it to learn   policy for the reaching
task  Figure   displays the resulting learning curves  Our
dynamics model performs much better than the baselines 
both with and without an action prior  Notably  using the
LSTM model results in   substantially worse policy than
ours even though its prediction accuracy is only slightly
lower  Because our model operates over segments  it implicitly learns to  lter noisy observations  This removes
the need to explicitly apply and tune    ltering process  as
is traditionally done 

Figure   Our dynamics model  both with and without   latent
action prior  can gracefully deal with noisy state observations and
delayed actions  as depicted by these learning curves from the
reaching task  Although the average rewards are slightly lower
than in the absence of noise or delays  policies trained with the
baseline models generally fail to perform the task 

  ANALYSIS OF LATENT SPACE

Variational autoencoders are known for learning lossy latent codes that preserve highlevel semantics of the data 
leaving the decoder to determine the lowlevel details  As
  result  we are curious to see whether our dynamics model
learns   latent space that possesses similar properties 
Applied to dynamics data  one might expect   latent code to
provide an overall description of what happens in the state
trajectory     it encodes  Alternatively  per the argument
made by Chen et al    it is also conceivable that the
decoder would ignore the latent code entirely  because the
segments           provide better information than  
about     However  we observe that our model does learn
  meaningful latent space  one that encodes uncertainty
about the future    particular latent code corresponds to
  particular future within the space of possible ones consistent with the given          
When the dynamics are simple and deterministic  such as
in the original Reacher environment  the model does express certainty by ignoring the latent code  With stochas 

 Episode  Reaching Task Episode  Pushing TaskLSTMLikelihood Ratio Policy GradientOne StepOne Step  rolled outOurs  with priorOurs  without prior Episode  Sensory Noise Episode  Delayed ActionsLSTMOne StepOne Step  rolled outOurs  with priorOurs  without priorPrediction and Control with Temporal Segment Models

ticity  such as in the previous section  it provides   spread
of reasonable state trajectories  Interestingly  when the dynamics are deterministic but complex  the model also uses
the latent codes to express uncertainty  This can occur regarding the orientations and velocities of objects immediately following   collision  as illustrated in Figure  

Figure   The qualitative effects of using   latent action prior 
as seen during trajectory optimization  The top plot shows an
example of action sequences from the training data  When we
optimize over latent codes  the actions look similar  bottom left 
but when we directly optimize over actions  the resulting sequence
looks unlike anything the model has seen before 
  Conclusion and Future Work
We presented   novel approach to dynamics learning based
on temporal segments  using   variational autoencoder to
learn the distribution over future state trajectories conditioned on past states  past actions  and planned future actions  We also introduced the latent action prior    variational autoencoder that models   prior over action segments  and showed how it can be used to perform control using actions from the same distribution as   dynamics
model   training data  Finally  through experiments involving trajectory optimization and modelbased policy optimization  we showed that the resulting method can model
complex phenomena such as collisions  is robust to sensory noise and action delays  and learns   meaningful latent
space that expresses uncertainty about the future 
The most prominent direction for future work that we plan
to explore  is the data collection procedure 
In our experiments  correlated random actions resulted in suf cient
exploration for the tasks we considered and allowed us
to demonstrate the bene ts of   segmentbased approach 
However  incorporating   more sophisticated exploration
strategy to gather data  in an iterative procedure  potentially
using the model   predictions to guide exploration  would
allow us to tackle   more diverse set of environments  both
simulated and realworld  The action prior and segmentbased policy could be used as   starting point for hierarchical reinforcementlearning algorithms  Leveraging existing
work on fewshot learning could help  netune   dynamics
model during the policy learning process  Such approaches
could yield signi cant advances in reinforcement learning 
improving both sample ef ciency and knowledge transfer
between related tasks 

Figure   An episode from the pushing environment  The arm
is about to swing counterclockwise and push the brown object 
the red path indicates the observed motion of the object  The blue
paths are samples from our model  given the same action sequence
and initial state  It correctly predicts collisions between arm and
object  and between object and wall  but expresses some uncertainty in the de ection angles and how far the object travels after
bouncing off the wall 

  EFFECT OF LATENT ACTION PRIOR

Our earlier experiments demonstrated the bene ts of   latent action prior  by only considering actions for which
the dynamics model is valid  the discrepancy between the
model and the true dynamics is minimized  resulting in
higher rewards achieved in the actual environment 
In this section  we qualitatively examine how the actions
returned by control algorithms differ as   consequence of
the latent action prior  An example is illustrated in Figure
  In the training data  the actions that the agent takes are
smooth  random torques  and we observe that when we use
an action prior  solutions from trajectory optimization look
similar  We contrast this with the solutions from optimizing
directly over actions  which are sharp and discontinuous 
unlike anything the dynamics model has seen before  This
lets us infer that the baselines perform poorly on the pushing task  as shown in Figure   because of large discrepancies between the model prediction and the true execution 

 TimestepSample of Actions from Training Data TimestepOptimize latent codes  with action prior TimestepOptimize actions directly  without prior Prediction and Control with Temporal Segment Models

Acknowledgements
Work done at Berkeley was supported in part by an ONR
PECASE award 

References
Abbeel  Pieter and Ng  Andrew    Learning  rstorder
markov models for control  In Advances in Neural Information Processing Systems  NIPS   

Agrawal  Pulkit  Nair  Ashvin  Abbeel  Pieter  Malik  Jitendra  and Levine  Sergey  Learning to poke by poking 
Experiential learning of intuitive physics  In Advances
in Neural Information Processing Systems  NIPS   

Atkeson  Christopher    Babu  BPW  Banerjee     Berenson     Bove  CP  Cui     DeDonato     Du     Feng    
Franklin     et al  What happened at the darpa robotics
challenge  and why  submitted to the DRC Finals Special
Issue of the Journal of Field Robotics   

Bengio  Samy  Vinyals  Oriol  Jaitly  Navdeep  and
Shazeer  Noam  Scheduled sampling for sequence prediction with recurrent neural networks  In Advances in
Neural Information Processing Systems  NIPS   

Boedecker  Joschka  Springenberg  Jost Tobias  Wl ng 
Jan  and Riedmiller  Martin  Approximate realtime optimal control based on sparse gaussian process models 
In Adaptive Dynamic Programming and Reinforcement
Learning  ADPRL   

Chen  Xi  Kingma  Diederik    Salimans  Tim  Duan  Yan 
Dhariwal  Prafulla  Schulman  John  Sutskever  Ilya  and
Abbeel  Pieter  Variational lossy autoencoder  arXiv
preprint arXiv   

Deisenroth  Marc Peter and Rasmussen  Carl Edward 
Pilco    modelbased and dataef cient approach to polIn International Conference on Machine
icy search 
Learning  ICML   

Finn  Chelsea and Levine  Sergey  Deep visual forearXiv preprint

for planning robot motion 

sight
arXiv   

Finn  Chelsea  Tan  Xin Yu  Duan  Yan  Darrell  Trevor 
Levine  Sergey  and Abbeel  Pieter  Deep spatial autoencoders for visuomotor learning  In International Conference on Robotics and Automation  ICRA   

Fragkiadaki  Katerina  Agrawal  Pulkit  Levine  Sergey 
and Malik  Jitendra  Learning visual predictive models
of physics for playing billiards  In International Conference on Learning Representations  ICLR   

Fu  Justin  Levine  Sergey  and Abbeel  Pieter  Oneshot
learning of manipulation skills with online dynamics
In International
adaptation and neural network priors 
Conference on Intelligent Robots and Systems  IROS 
 

Heess  Nicolas  Wayne  Gregory  Silver  David  Lillicrap 
Tim  Erez  Tom  and Tassa  Yuval 
Learning continuous control policies by stochastic value gradients 
In Advances in Neural Information Processing Systems
 NIPS   

Johnson  Matthew  Duvenaud  David    Wiltschko  Alex 
Adams  Ryan    and Datta  Sandeep    Composing
graphical models with neural networks for structured
representations and fast inference  In Advances in Neural Information Processing Systems  NIPS   

Kingma  Diederik and Ba  Jimmy  Adam    method for
stochastic optimization  In International Conference on
Learning Representations  ICLR   

Kingma  Diederik   and Welling  Max  Autoencoding
variational bayes  In International Conference on Learning Representations  ICLR   

Ko  Jonathan and Fox  Dieter  Gpbayes lters  Bayesian
 ltering using gaussian process prediction and obAuton  Robots   
servation models 
  URL http dblp unitrier de db 
journals arobots arobots html KoF 

Lawrence  Neil  Seeger  Matthias  Herbrich  Ralf  et al 
Fast sparse gaussian process methods  The informative
vector machine  Advances in Neural Information Processing Systems  NIPS   

Lenz 

Ian  Knepper  Ross 

and Saxena  Ashutosh 
Deepmpc  Learning deep latent features for model predictive control  In Robotics  Science and Systems  RSS 
 

Littman  Michael    Sutton  Richard    Singh  Satinder 
et al  Predictive representations of state  Advances in
Neural Information Processing Systems  NIPS   

Mordatch  Igor  Mishra  Nikhil  Eppner  Clemens  and
Abbeel  Pieter  Combining modelbased policy search
with online model learning for control of physical humanoids  In International Conference on Robotics and
Automation  ICRA   

Oh  Junhyuk  Guo  Xiaoxiao  Lee  Honglak  Lewis 
Richard    and Singh  Satinder  Actionconditional
video prediction using deep networks in atari games 
In Advances in Neural Information Processing Systems
 NIPS   

Prediction and Control with Temporal Segment Models

Venkatraman  Arun  Capobianco  Roberto  Pinto  Lerrel 
Hebert  Martial  Nardi  Daniele  and Bagnell    Andrew  Improved learning of dynamics models for control 
In International Symposium on Experimental Robotics
 ISER   

Vezhnevets  Alexander  Sasha  Mnih  Volodymyr  Agapiou  John  Osindero  Simon  Graves  Alex  Vinyals 
Oriol  and Kavukcuoglu  Koray 
Strategic attentive
writer for learning macroactions  In Advances in Neural
Information Processing Systems  NIPS   

Watter  Manuel  Springenberg  Jost  Boedecker  Joschka 
and Riedmiller  Martin  Embed to control    locally linear latent dynamics model for control from raw images 
In Advances in Neural Information Processing Systems
 NIPS   

Williams  Christopher KI and Rasmussen  Carl Edward 
Gaussian processes for regression  Advances in neural
information processing systems  pp     

Williams  Ronald    Simple statistical gradientfollowing
learning 

algorithms for connectionist reinforcement
Machine learning     

Yip  Michael    and Camarillo  David    ModelLess
Feedback Control of Continuum Manipulators in Constrained Environments  IEEE Transactions on Robotics 
  August   ISSN    
  doi   TRO   

Peters  Jan and Schaal  Stefan  Policy gradient methods
for robotics  In International Conference on Intelligent
Robots and Systems  IROS   

Pratt 

Jerry  Carff 

John  Drakunov  Sergey 

and
Capture point    step toGoswami  Ambarish 
In Proceedings
ward humanoid push recovery 
of
the Sixth IEEERAS International Conference on
Humanoid Robots  Humanoids   pp   
IEEE    URL http www ambarish com 
paper Pratt Goswami Humanoids pdf 

Punjani  Ali and Abbeel  Pieter  Deep learning heliIn International Conference

copter dynamics models 
on Robotics and Automation  ICRA   

Raibert       Legged Robots that Balance  Arti cial
Intelligence  MIT Press    ISBN  
URL
https books google com books 
id EXRiBnQ RwC 

Rosencrantz  Matthew  Gordon  Geoff  and Thrun  Sebastian  Learning low dimensional predictive representations  In International Conference on Machine Learning
 ICML   

Shen  Yirong  Ng  Andrew  and Seeger  Matthias  Fast
gaussian process regression using kdtrees  Advances in
Neural Information Processing Systems  NIPS 

Sutton  Richard    McAllester  David    Singh  Satinder    Mansour  Yishay  et al  Policy gradient methods
for reinforcement learning with function approximation 
In Advances in Neural Information Processing Systems
 NIPS     

Sutton  Richard    Precup  Doina  and Singh  Satinder 
Between mdps and semimdps    framework for temporal abstraction in reinforcement learning  Arti cial
Intelligence            ISSN  
doi  http dx doi org   
URL
http www sciencedirect com 
science article pii   

van den Oord    aron  Dieleman  Sander  Zen  Heiga  Simonyan  Karen  Vinyals  Oriol  Graves  Alex  Kalchbrenner  Nal  Senior  Andrew    and Kavukcuoglu  Koray  Wavenet    generative model for raw audio  CoRR 
abs      URL http arxiv org 
abs 

van den Oord  Aaron  Kalchbrenner  Nal  Espeholt  Lasse 
Vinyals  Oriol  Graves  Alex  et al  Conditional image
generation with pixelcnn decoders  In Advances in Neural Information Processing Systems  NIPS     

Venkatraman  Arun  Hebert  Martial  and Bagnell    AnImproving multistep prediction of learned time

drew 
series models  In AAAI  pp     

