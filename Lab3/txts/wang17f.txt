Ef cientDistributedLearningwithSparsityJialeiWang MladenKolar NathanSrebro TongZhang AbstractWeproposeanovel ef cientapproachfordistributedsparselearningwithobservationsran domlypartitionedacrossmachines Ineachroundoftheproposedmethod workermachinescomputethegradientofthelossonlocaldataandthemastermachinesolvesashifted regularizedlossminimizationproblem Afteranumberofcommunicationroundsthatscalesonlylogarithmicallywiththenumberofmachines andindependentofotherparametersoftheproblem theproposedapproachprovablymatchestheestimationerrorboundofcentralizedmethods IntroductionWeconsiderlearningasparselinearregressor minimizingthepopulationobjective argmin EX   Dr pY xX yqs wherepX YqPX   Rp YaredrawnfromanunknowndistributionDand   qisaconvexlossfunction basedonNi     samplestxi yiuNi drawnfromD andwhenthesupportS supportp   tjPrps   uof issmall     Inastandardsinglemachineset ting acommonempiricalapproachistominimizethe regularizedempiricalloss see     below Hereweconsiderasettingwheredataaredistributedacrossmmachines and forsimplicity assume thatN nm sothateachmachinejhasaccesstoni     observations fromthesamesourceD txji yjiuni equivalently thatN nmsamplesarerandomlypartitionedacrossmachines Themaincontributionofthepaperisanovelalgorithmforestimating inadistributedsetting Ourestimatoris UniversityofChicago USA ToyotaTechnologicalInstituteatChicago USA TencentAILab China Correspondenceto JialeiWang jialei uchicago edu MladenKolar mkolar chicagobooth edu NathanSrebro nati ttic edu TongZhang tongzhang tongzhangml org Proceedingsofthe thInternationalConferenceonMachineLearning Sydney Australia PMLR Copyright bytheauthor   Resultsinthepapereasilygeneralizetoasettingwhereeachmachinehasadifferentnumberofobservations abletoachievetheperformanceofacentralizedprocedurethathasaccesstoalldata whilekeepingcomputationandcommunicationcostslow Comparedtotheexistingoneshotestimationapproach Leeetal   ourmethodcanachievethesamestatisticalperformancewithoutperformingtheexpensivedebiasingstep Asthenumberofcommunicationroundsincreases theestimationaccuracyimprovesuntilmatchingtheperformanceofacentralizedprocedure whichhappensafterthelogarithmofthetotalnumberofmachinesrounds Furthermore ourresultscanbeachievedunderweakassumptionsonthedatageneratingprocedure Weassumethatthecommunicationoccursinrounds Ineachround machinesexchangemessageswiththemastermachine Betweentworounds eachmachineonlycomputesbasedonitslocalinformation whichincludeslocaldataandpreviousmessages Zhangetal   Shamir Srebro Arjevani Shamir Inanondistributedsetting ef cientestimationproceduresneedtobalancestatisticalef ciencywithcomputationef ciency runtime Inadistributedsetting thesituationismorecomplicatedandweneedtobalancetworesources localruntimeandnumberofroundsofcommunication withthestatisticalerror Thelocalruntimereferstotheamountofworkeachmachineneedstodo Thenumberofroundsofcommunicationreferstohowoftendolocalmachinesneedtoexchangemessageswiththemastermachine Wecompareourproceduretootheralgorithmusingtheaforementionedmetrics Weconsiderthefollowingtwobaselineestimatorsof thelocalestimatorusesdataavailableonlyonthemaster rst machineandignoresdataavailableonothermachines Inparticular itcomputesb local argmin nn   py   xx   yq usinglocallyavailabledata Thelocalprocedureisef cientinbothcommunicationandcomputation however theresultingestimationerrorislargecomparedtoanestimatorthatusesalloftheavailabledata Theotheridealizedbaselineisthecentralizedestimatorb centralize argmin mnm       pyji xxji yq Ef cientDistributedLearningwithSparsityApproachn ms logpms logp     logpCommunicationComputationCommunicationComputationCentralizen pTlassopmn pqn pTlassopmn pqAvgDebiaspp Tlassopn pq Thispaper EDSL   Tlassopn pqlogm plogm Tlassopn pqTable Comparisonofresourcesrequiredformatchingthecentralizederrorboundofvariousapproachesforhighdimensionaldis tributedsparselinearregressionproblems whereTlassopn pqistheruntimeforsolvingageneralizedlassoproblemofsizen   Unfortunately duetodatabeinghugeandcommunicationexpensive wecannotcomputethecentralizedestimator eventhoughitachievestheoptimalstatisticalerror Inarelatedsetting Leeetal   studiedaoneshotap proachtolearning calledAvgDebias thatisbasedonaveragingthedebiasedlassoestimators Zhang Zhang Understrongassumptionsonthedatageneratingprocedure theirapproachmatchesthecentralizederrorboundafteroneroundofcommunication Whileanencouragingresult therearelimitationstothisapproach thatwelistbelow ThedebiasingstepinAvgDebiasiscomputationallyheavyasitrequireseachlocalmachinetoestimateap pmatrix Forexample Javanmard section transformstheproblemofestimatingthedebiasingmatrix intopgeneralizedlassoproblems Thisiscomputationallyprohibitiveforhighdimensionalproblems Zhang Zhang Javanmard Montanari Incomparison ourprocedurerequiresonlysolvingone penalizedobjectiveineachiteration whichhasthesametimecomplexityascomputingb localin SeeSection fordetails AvgDebiasprocedureonlymatchesthestatisticaler rorrateofthecentralizedprocedurewhenthesamplesizepermachinesatis esn ms logp Ourapproachimprovesthissamplecomplexityton   logp AvgDebiasprocedurerequiresstrongconditionsonthedatageneratingprocess Forexample thedatamatrixisrequiredtosatisfythegeneralizedcoherenceconditionfordebiasingtowork Asweshowhere suchaconditionisnotneededforconsistenthighdimensionalestimationinadistributedsetting Instead weonlyrequirestandardrestrictedeigenvalueconditionthatarecommonlyassumedinthehighdimensionalestimationliterature Ourmethod EDSL addressestheaforementionedissues Thegeneralizedcoherencestatesthatthereexistsamatrix suchthat   Ip blogpn whereb istheempiricalcovariancematrix ofAvgDebias Table summarizestheresourcesrequiredfortheapproachesdiscussedabovetosolvethedistributedsparselinearregressionproblems ParallelWorkInparallelwork publiclyannouncedonarXivsimultaneouslywiththeresultsinthiscontribution Jordanetal presentamethodwhichisequivalenttothe rstiterationofourmethod andthusachievesthesamecomputationaladvantageoverAvgDebiasasdepictedintheleftcolumnofTable anddiscussedinthe rstandthirdbulletpointsabove Jordanetal extendtheideainwaysdifferentandorthogonaltothissubmission byconsideringalsolowdimensionalandBayesianinferenceproblems Still forhighdimensionalproblems theyonlyconsideraoneshotprocedure andsodonotachievestatisticaloptimalityinthewayourmethoddoes anddonotallowusingn ms logpsamplespermachine seerighthalfofTable Theimprovedoneshotapproachisthusaparallelcontribution madeconcurrentlybyJordanetal andbyus whilethemultistepapproachandaccompaniedreductioninrequirednumberofsamples discusseinthesecondbulletpointabove andimprovementinstatisticalaccuracyisadistinctcontributionofthisthissubmission OtherRelatedWorkAlargebodyofliteratureexistsondistributedoptimizationformodernmassivedatasets Dekeletal Duchietal Zhangetal   Zinkevichetal Boydetal Balcanetal Yang Jaggietal Maetal Shamir Srebro Zhang Xiao Leeetal   Arjevani Shamir Apopularapproachtodistributedestimationisaveragingestimatorsformedlocallybydifferentmachines Mcdonaldetal Zinkevichetal Zhangetal Huang Huo Divideand conquerproceduresalsofoundapplicationsinstatisticalinference Zhaoetal   Cheng Shang Luetal Shamir Srebro andRosenblatt Nadler showedthataveraginglocalestimatorsattheendwillhavebaddependenceoneitherconditionnumberordimensionoftheproblem Yang Jaggietal andSmithetal studieddistributedoptimizationusingstochastic dual coordinatedescent theseapproachestryto ndagoodbalancebetweencomputationandcommunication however theircommunicationcomEf cientDistributedLearningwithSparsityplexitydependsbadlyontheconditionnumber Asaresult theyarenotbetterthan rstorderapproaches suchas proximal acceleratedgradientdescent Nesterov intermsofcommunication Shamiretal andZhang Xiao proposedtrulycommunicationef cientdistributedoptimizationalgorithms Theyleveragedthelocalsecondorderinformationand asaresult obtainedmilderdependenceontheconditionnumbercomparedtothe rstorderapproaches Boydetal Shamir Srebro Maetal LowerboundswerestudiedinZhangetal   Bravermanetal andArjevani Shamir However itisnotclearhowtoextendtheseexistingapproachestoproblemswithnonsmoothob jectives includingthe regularizedproblems Mostoftheabovementionedworkisfocusedonestimatorsthatare asymptotically linear Averagingattheendreducesthevarianceofthetheselinearestimators resultinginanestimatorthatmatchestheperformanceofacen tralizedprocedure Zhangetal   studiedaveraginglocalestimatorsobtainedbythepenalizedkernelridgeregression withthe penaltywaschosensmallerthanusualtoavoidthelargebiasproblem Thesituationinahighdimensionalsettingisnotsostraightforward sincethesparsityinducingpenaltyintroducesthebiasinanonlinearway Zhaoetal   illustratedhowaveragingdebiasedcompositequantileregressionestimatorscanbeusedforef cientinferenceinahighdimensionalsetting Averagingdebiasedhighdimensionalestimatorswassub sequentlyusedinLeeetal   fordistributedestimation multitasklearning Wangetal andstatisticalinference Batteyetal Notation Weusernstodenotethesett nu ForavectoraPRn weletsupportpaq tj aj ubethesupportset     qPr   the qnormde nedas       iPrns ai qq   and   maxiPrns ai ForamatrixAPRn   weusethefollowingelementwise matrixnorms   maxiPrn   jPrn   aij DenoteInasn nidentitymatrix Fortwosequencesofnumberstanu   andtbnu   weusean Opbnqtodenotethatan Cbnforsome nitepositiveconstantC andforallnlargeenough Ifan Opbnqandbn Opanq weusethenotationan bn Wealsousean bnforan Opbnqandan bnforbn Opanq PaperOrganization WedescribeourmethodinSection andpresentthemainresultsinthecontextofsparselinearregressioninSection andprovideageneralizedtheoryinSection WedemonstratetheeffectivenessoftheproposalviaexperimentsinSection andconcludethepaperwithdiscussionsinSection InAppendix inSectionAweillustratesomeconcreteexamplesofthegeneralresultsinSection andallproofsaredeferredinSectionB MoreexperimentalresultsarepresentedinSectionC Algorithm Ef cientDistributedSparseLearning EDSL Input Datatxji yjiujPrms iPrns lossfunction     Initialization Themasterobtainsb byminimizing andbroadcastb toeveryworker fort doWorkers forj mdoifReceiveb tfromthemasterthenCalculategradient Ljpb tqandsendittothemaster endendMaster ifReceivet Ljpb tqumj fromallworkersthenObtainb   bysolvingtheshifted regularizedproblemin Broadcastb   toeveryworker endend MethodologyInthissection wedetailourprocedureforestimating inadistributedsetting Algorithm providesanoutlineofthestepsexecutedbythemasterandworkernodes LetLjp   nn   pyji xxji yq jPrms betheempiricallossateachmachine Ourmethodstartsbysolvingalocal regularizedMestimationprogram Atiterationt themaster rst machineobtainsb asaminimizerofthefollowingprogramminL     Thevectorb isbroadcastedtoallothermachines whichuseittocomputeagradientofthelocallossatb Inparticular eachworkercomputes Ljpb qandcommunicatesitbacktothemaster Thisconstitutesoneroundofcommunication Attheiterationt themastersolvestheshifted regularizedproblemb   argmin         mm   Ljpb tq   pb tq     Aminimizerb   iscommunicatedtoothermachines whichuseittocomputethelocalgradient Ljpb   qasbefore Formulation isinspiredbytheproposalinShamiretal wheretheauthorsstudieddistributedoptimizationEf cientDistributedLearningwithSparsityforsmoothandstronglyconvexempiricalobjectives ComparedtoShamiretal wedonotuseanyaveragingscheme whichwouldrequireadditionalroundsofcommunicationand moreover weaddan regularizationtermtoensureconsistentestimationinhighdimensions Differentfromthedistributed rstorderoptimizationapproaches there nedobjective leveragesbothglobal rstorderin formationandlocalhigherorderinformation Toseethis supposeweset   andthatLjp qisaquadraticobjectivewithinvertibleHessian Thenwehavethefollowingclosedformsolutionfor           pb tq	   jPrms Ljpb tq whichisexactlyasubsampledNewtonupdatingrule Unfortunatelyforhigh dimensionalproblems theHessianisnolongerinvertible anda regularizationisaddedtomakethesolutionwellbehaved Theregularizationparameter twillbechoseninaway sothatitdecreaseswiththeiterationnumbert Asaresultwewillbeabletoshowthatthe nalestimatorperformsaswellatthecentralizedsolution Wediscussindetailshowtochoose tinthefollowingsection MainResultWeillustrateourmaintheoreticalresultsinthecontextofsparselinearregressionmodelyji xxji   ji iPrns jPrms wherexjiisasubgaussianpdimensionalvectorofin putvariablesand jiisi     meanzerosubgaussiannoise Thelossfunctionconsideredistheusualthesquaredloss py byq py byq Withthisnotation thecentralizedapproachleadstothelassoestimator Tibshirani   centralize argmin mm   Ljp   wherethelossatworkerjisLjp     iPrnspyji   xjiyq Beforestatingthemainresult weprovidethede nitionofthesubgaussiannorm Vershynin De nition Subgaussiannorm Thesubgaussiannorm   ofasubgaussianpdimensionalrandomvectorX isde nedas   supxPSp supq   pE xX xy qq   whereSp isthepdimensionalunitsphere Wealsoneedanassumptionontherestrictedstrongconvexityconstant Negahbanetal Assumption Weassumethatthereexistsa suchthatforany PCpS       whereCpS     PRp Sc   uisarestrictedconeinRp andX rxT xT xT nsPRn pisthedatamatrixonthemastermachine Whenxjiarerandomlydrawnfromasubgaussiandistribution Assumption issatis edwithhighprobabilityaslongasn slogp Rudelson Zhou Wearenowreadytostatetheestimationerrorboundforb   obtainedusingAlgorithm Theorem Assumethatdataaregeneratedfromasparselinearregressionmodelin with xji Xand ji Let   mn jPrms iPrnsxji ji   maxj   xji 
     clogp   qn Thenfort wehave withprobabilityatleast     at   an     clogpp qmn at ns   clogpnp qn     at   an     clogpp qmn atnbns   clogpnp qn wherean     clogp   qnandbn     clogpnp qn WecansimplifytheboundobtainedinTheorem bylookingatthescalingwithrespectton     andp bytreating and Xasconstants Supposen   logpandset   clogpmn clogpn sclogpn  Ef cientDistributedLearningwithSparsityThefollowingerrorboundsholdforAlgorithm     Psclogpmn sclogpn      Pcslogpmn cslogpn sclogpn  Wecancomparetheaboveboundstotheperformanceofthelocalandcentralizedlasso Wainwright Meinshausen Yu Bickeletal Forb local wehave   local Psclogpnand   local Pcslogpn Forb centralize wehave   centralize Psclogpmnand   centralize Pcslogpmn Weseethatafteroneroundofcommunication wehave   Psclogpmn   logpnand   Pcslogpmn   logpn TheseboundsmatchtheresultsinLeeetal   withoutexpensivedebiasingstep Furthermore whenm ns logp theymatchtheperformanceofthecentralizedlasso Finally aslongast logmandn   logp itiseasytocheckthat sblogpn
  sblogpmn Therefore     Psclogpmnand     Pcslogpmn whichmatchesthecentralizedlassoperformancewithoutadditionalerrorterms Thatis aslongasn   logp theroundsofcommunicationtomatchescentralizedprocedureonlyincreaselogarithmicallywiththenumberofmachinesandindependentofotherparameters Differently fordistributedlearningmethodsstudiedintheliteratureformini mizingsmoothobjectives theroundsofcommunicationtomatchcentralizedprocedureincreasepolynomiallywithm seetable in Zhang Xiao Thisisbecausehereweexploittheunderlyingrestrictedstrongconvexityfromempiricallossfunctions whilepriorworkondistributedminimizationofsmoothobjectives Shamiretal Zhang Xiao onlyconsiderstrongconvexityexplicitlyfromregularization GeneralizedTheoryandProofSketchInordertoestablishTheorem weproveanerrorboundonb forageneralloss   qandb obtainedusingAlgorithm Tosimplifythepresentation weassumethatthedomainXisboundedandthatthelossfunction   qissmooth Assumption Theloss   qisLsmoothwithrespecttothesecondargument pa bq pa cq           cPRFurthermore pa bq Mforalla bPR Commonlyusedlossfunctionsinstatisticallearning includingthesquaredlossforregressionandlogisticlossforclassi cation satisfythisassumption Zhangetal   Next westatetherestrictedstrongconvexityconditionforagenerallossfunction Negahbanetal Assumption Thereexists suchthatforany PCpS qL                     withCpS     PRp Sc     Therestrictedstrongconvexityholdswithhighprobabilityforawiderangeofmodelsanddesignsanditiscom monlyassumedforshowingconsistentestimationinhighdimensions see forexample vandeGeer   uhlmann Negahbanetal Raskuttietal Rudelson Zhou fordetails Ourmaintheoreticalresultestablishesarecursiveestimationerrorbound whichrelatestheestimationerror     tothatofthepreviousiteration     Theorem SupposeAssumption and holds Let     jPrms Ljp     maxj   xji 
     clogp   qn   maxj   xji 
     	 Ef cientDistributedLearningwithSparsityThenwithprobabilityatleast wehave         jPrms Ljp   sL maxj   xji 
     clogp   qn sM maxj   xji 
     	 and         jPrms Ljp   sL maxj   xji 
     clogp   qn sM maxj   xji 
     	 Theorem upperboundstheestimationerror     asafunctionof     ApplyingTheorem iteratively weimmediatelyobtainthefollowingestimationerrorboundwhichdependsonthequalityoflocal regularizedestimation   Corollary SupposetheconditionsofTheorem aresatis ed Furthermore supposethatforallt wehaveM maxj   xji 
     Lclogp   qn Thenwithprobabilityatleast wehave     at       anq   at nq     jPrms Ljp   and     atnbn     anq   at nq     jPrms Ljp   wherean sL maxj   xji 
clogp   qnandbn sL maxj   xji 
clogp   qn ForthequadraticlosswehavethatM andtheconditionin holds Forothertypesoflosses conditionin willbetruefortlargeenoughwhenm   leadingtolocalexponentialrateofconvergenceuntilreachingstatisticaloptimalregion ProofSketchofTheorem We rstanalyzehowtheestimationerrorbounddecreasesafteroneroundofcommunication Inparticular webound     with     De neeL     tq           jPrms Ljpb tq   pb tq   Then eL     tq         jPrms Ljpb tq   pb tq Thefollowinglemmaboundsthe normof eL     tq Lemma Withprobabilityatleast wehave eL     tq   jPrms Ljp     maxj   xji 
     clogp   qn   maxj   xji 
     	 Thelemmaboundsthemagnitudeofthegradientofthelossatoptimumpoint Thiswillbeusedtoguideourchoiceofthe regularizationparameter   in Thefollowinglemmashowsthataslongas   islargeenough itisguaranteedthatb   isinarestrictedcone Lemma Suppose   eL     tq Thenwithprobabilityatleast wehaveb   PCpS   Basedontheconicconditionandrestrictedstrongconvexitycondition wecanobtaintherecursiveerrorboundstatedinTheorem followingtheproofstrategyasinNegahbanetal ApplicationsTheorem canbeusedtoestablishstatisticalguaranteesformoregeneralsparselearningproblems forexampleconsiderthelogisticregressionisapopularclassi cationmodelwherethebinarylabelyjiPt uisdrawnaccordingtoaBernoullidistribution Ppyji xjiq exppyjixxji yqexppyjixxji yq wecanestablishlocalexponentialconvergencewhenapplyingAlgorithm toestimate inthehighdimensionallogisticmodel SectionAinAppendixprovideformalguaranteesandmoreillustrativeexamples Ef cientDistributedLearningwithSparsitym     RoundsofCommunications EstimationErrorLocalProxGDCentralizeAvg DebiasEDSL RoundsofCommunications EstimationErrorLocalProxGDCentralizeAvg DebiasEDSL RoundsofCommunications EstimationErrorLocalProxGDCentralizeAvg DebiasEDSLn       Np   ij     RoundsofCommunications EstimationErrorLocalProxGDCentralizeAvg DebiasEDSL RoundsofCommunications EstimationErrorLocalProxGDCentralizeAvg DebiasEDSL RoundsofCommunications EstimationErrorLocalProxGDCentralizeAvg DebiasEDSLn       Np   ij     Figure Comparisonofvariousalgorithmsfordistributedsparselearningonsimulateddata rstrow sparselinearregression secondrow sparselogisticregression ExperimentsInthissectionwepresentempiricalcomparisonsbetweenvariousapproachesonbothsimulatedandrealworlddatasets Werunthealgorithmsforbothdistributedregressionandclassi cationproblems andcomparewiththefollowingalgorithms   Local ii Centralize iii Distributedproximalgradientdescent ProxGD iv AvgDebias Leeetal   withhardthresholding andv theproposedEDSLapproach SimulationsWe rstexaminethealgorithmsonsimulateddata WegeneratetxjiujPrms iPrnsfromamultivariatenormaldistributionwithmeanzeroandcovariancematrix Thecovariance controlstheconditionnumberoftheproblemandwewillvaryingittoseehowtheperformancechanges Weset ij     forthewellconditionedsettingand ij     fortheillconditionedsetting TheresponsevariabletyjiujPrms iPrnsaredrawnfrom and forregressionandclassi cationproblems respectively Forregression thenoise jiissampledfromastandardnormaldistribution Thetruemodel issettobessparse wherethe rstsentriesaresampledi     fromauniformdistributioninr   andtheotherentriesareset PleaserefertoSectionCinAppendixforfullexperimentalresultsandmoredetailstozero Werunexperimentswithvariouspn     sqsettings Theestimationerror     isshownversusroundsofcommunicationsforforProxGDandtheproposedEDSLalgorithm WealsoplottheestimationerrorofLocal AvgDebias andCentralizeashorizontallines sincethecommunicationcostis xedforforthesealgorithms Figure summarizetheresults averagedacross independenttrials Wehavethefollowingobservations TheAvgDebiasapproachobtainedmuchbetteres timationerrorcomparedtoLocalafteroneroundofcommunicationandsometimesperformedquiteclosetoCentralize However inmostcases thereisstillagapcomparedwithCentralize especiallywhentheproblemisnotwellconditionedormislarge ProxGDconvergesveryslowwhentheconditionnumberbecomesbad ij     case Astheorysuggests EDSLobtainedasolutionthatis   samplesizepermachine   problemdimension   numberofmachines   truesupportsize thesealgorithmshavezero oneshotandfullcommunica tions respectively Togivesomesensesaboutcomputationalcost foraproblemwithn   ateachroundEDSLtakesabout   whileAvgDebiastakesabout   Ef cientDistributedLearningwithSparsity RoundsofCommunications Classi cationError LocalProxGDCentralizeAvg DebiasEDSL RoundsofCommunications NormalizedMSELocalProxGDCentralizeAvg DebiasEDSL RoundsofCommunications NormalizedMSELocalProxGDCentralizeAvg DebiasEDSLmnist vs connect dnaFigure Comparisonofvariousapproachesfordistributedsparseregressionandclassi cationonrealworlddatasets competitivewithAvgDebiasafteroneroundofcom munication TheestimationerrordecreasestomatchperformanceofCentralizewithinfewroundsofcommunications typicallylessthan eventhoughthetheorysuggestsEDSLwillmatchtheperformanceofcentralizewithinOplogmqroundsofcommunication Aboveexperimentsillustrateourtheoreticalresultsin nitesamples Assuggestedbytheory whensamplesizepermachinenisrelativelysmall oneroundofcommunicationisnotsuf cienttomakeAvgDebiasmatchestheper formanceofcentralizedprocedure However EDSLcouldmatchtheperformanceofAvgDebiaswithoneroundofcommunicationandfurtherimprovetheestimationqual itybyexponentiallyreducingthegapbetweencentralizedprocedurewithAvgDebias untilmatchingthecentralizedperformance Thus theproposedEDSLimprovestheAvgDebiasapproachbothcomputationallyandstatistically RealworldDataEvaluationInthissection wecomparethedistributedsparselearningalgorithmsonseveralrealworlddatasets Foralldatasets weuse ofdatafortraining asheldoutvalida tionsetfortuningtheparameters andtheremaining fortesting Werandomlypartitiondata timesandreporttheaverageperformanceonthetestset Forregressiontasks theevaluationmetricisthenormalizedMeanSquaredError normalizedMSE whileforclassi cationtaskswereportthemissclassi cationerror Werandomlypartitionthedataonm machines AsubsetoftheresultsareplottedinFigure whereforsomedatasetstheperformanceofAvgDebiasissigni cantlyworsethanothers mostlybecausethedebiasingstepfails thusweomittheseplots Sincethereisnowellspeci edmodelonthesedatasets thecurvesbehavequitedifferentlyondifferentdatasets However alargegapbetweenthelocalandcentralizedprocedureisconsistentasthelateruses timesmoredata AvgDebiasoftenfailsontheserealdatasetsandperformsmuchworsethaninthesimulations themainreasonmightbethattheassumptions suchaswellspeci edmodelorgeneralizedcoherencecondition fail thenAvgDebiascantotallyfailandproducesolutionevenmuchworsethanthelocal Nevertheless theproposedEDSLperformsquiterobustonrealworlddatasets andcanoftenoutputasolutionwhichishighlycompetitivewiththecentralizedmodelwithinafewroundsofcommunications Wealsoobservedaslight zigzag behaviorforEDSLapproachonsomedatasets Forexample onthemushroomsdataset thepredictiveperformanceofEDSLisnotstable Insum theexperimentalresultsonrealworlddatasetsveri edthattheproposedEDSLmethodiseffectivefordistributedsparselearningproblems ConclusionandDiscussionWeproposedanovelapproachfordistributedlearningwithsparsity whichisef cientinbothcomputationandcommunication OurtheoreticalanalysisshowedthattheproposedmethodworksunderweakerconditionsthanAvg Debiasestimatorwhilematchesitserrorboundwithoneroundcommunication Furthermore theestimationerrorcanbeimprovedwithalogarithmicmoreroundsofcommunicationuntilmatchingthecentralizedprocedure Experimentsonbothsimulatedandreal worlddatademonstratethattheproposedmethodsigni cantlyimprovestheperformanceoveroneshotaveragingapproaches andmatchesthecentralizedprocedurewithfewiterations Theremightbeseveralwaystoimprovethiswork Asweseeinrealdataexperiments theproposedapproachcanstillperformslightlyworsethanthecentralizedapproachoncertaindatasets ItisinterestingtoexplorehowtomakeEDSLprovablyworkunderevenweakerassumptions Forexample EDSLrequiresOps logpqsamplespermachinetomatchthecentralizedmethodinOplogmqroundsofcommunications however itisnotclearwhetherthesamplesizerequirementcanbeimproved whilestillmaintaininglow communicationcost Lastbutnottheleast itisinterestingtoexplorepresentedideastoimprovethecompu tationalcostofcommunicationef cientdistributedmultitasklearningwithsharedsupport Wangetal Ef cientDistributedLearningwithSparsityReferencesArjevani YossiandShamir Ohad Communicationcomplexityofdistributedconvexlearningandoptimization ArXiveprints arXiv June Balcan MariaFlorina Blum Avrim Fine Shai andMansour Yishay Distributedlearning communicationcomplexityandprivacy InMannor Shie Srebro Nathan andWilliamson RobertC eds JMLRW CP COLT volume pp Battey Heather Fan Jianqing Liu Han Lu Junwei andZhu Ziwei Distributedestimationandinferencewithstatisticalguarantees ArXiveprints arXiv September Bickel PeterJ Ritov Ya acov andTsybakov AlexandreB SimultaneousanalysisoflassoandDantzigselector Ann Stat doi AOS Boyd StephenP Parikh Neal Chu Eric Peleato Borja andEckstein Jonathan Distributedoptimizationandstatisticallearningviathealternatingdirectionmethodofmultipliers Found TrendsMach Learn Braverman Mark Garg Ankit Ma Tengyu Nguyen HuyL andWoodruff DavidP Communicationlowerboundsforstatisticalestimationproblemsviaadistributeddataprocessinginequality ArXiveprints arXiv June Cheng GuangandShang Zuofeng Computationallimitsofdivide andconquermethod ArXiveprints arXiv December Dekel Ofer GiladBachrach Ran Shamir Ohad andXiao Lin Optimaldistributedonlinepredictionusingminibatches   Mach Learn Res ISSN Duchi JohnC Agarwal Alekh andWainwright MartinJ Dualaveragingfordistributedoptimization convergenceanalysisandnetworkscaling IEEETrans Automat Control ISSN doi TAC Duchi JohnC Jordan MichaelI Wainwright MartinJ andZhang Yuchen Optimalityguaranteesfordistributedstatisticalestimation ArXiveprints arXiv May Hoeffding Wassily Probabilityinequalitiesforsumsofboundedrandomvariables   Am Stat Assoc ISSN Huang ChengandHuo Xiaoming Adistributedonestepestimator ArXiveprints arXiv November Jaggi Martin Smith Virginia Tak ac Martin Terhorst Jonathan Krishnan Sanjay Hofmann Thomas andJordan MichaelI Communicationef cientdistributeddualcoordinateascent InAdvancesinNeuralInformationProcessingSystems pp Javanmard Adel Inferenceandestimationinhighdimensionaldataanalysis PhDdissertation StanfordUniversity Javanmard AdelandMontanari Andrea Con denceintervalsandhypothesistestingforhigh dimensionalregression   Mach Learn Res Oct Jordan MichaelI Lee JasonD andYang Yun Communicationef cientdistributedstatisticallearning arXivpreprintarXiv Lee JasonD Lin Qihang Ma Tengyu andYang Tianbao Distributedstochasticvariancereducedgradientmethodsandalowerboundforcommunicationcomplexity ArXiveprints arXiv July   Lee JasonD Sun Yuekai Liu Qiang andTaylor JonathanE Communicationef cientsparseregression aoneshotapproach ArXiveprints arXiv   Lu Junwei Cheng Guang andLiu Han Nonparametricheterogeneitytestingformassivedata ArXiveprints arXiv January Ma Chenxin Smith Virginia Jaggi Martin Jordan MichaelI Richtrik Peter andTak Martin Addingvs averagingindistributedprimaldualoptimization ArXiveprints arXiv February McCullagh   andNelder     Generalizedlinearmodels MonographsonStatisticsandAppliedProbability Chapman Hall London ISBN doi Secondedition ofMR Mcdonald Ryan Mohri Mehryar Silberman Nathan Walker Dan andMann GideonS Ef cientlargescaledistributedtrainingofconditionalmaximumen tropymodels InBengio   Schuurmans   Lafferty     Williams       andCulotta   eds AdvancesinNeuralInformationProcessingSystems pp CurranAssociates Inc Meinshausen NicolasandB uhlmann Peter Highdimensionalgraphsandvariableselectionwiththelasso Ann Stat Meinshausen NicolasandYu   Lassotyperecoveryofsparserepresentationsforhigh dimensionaldata Ann Stat Negahban SahandN Ravikumar Pradeep Wainwright MartinJ andYu Bin Auni edframeworkforhighdimensionalanalysisofm estimatorswithdecomposableregularizers Stat Sci Nesterov Yurii AmethodofsolvingaconvexprogramEf cientDistributedLearningwithSparsitymingproblemwithconvergencerateop     InSovietMathematicsDoklady volume pp Raskutti Garvesh Wainwright MartinJ andYu Bin Restrictedeigenvaluepropertiesforcorrelatedgaussiande signs TheJournalofMachineLearningResearch Ravikumar Pradeep Wainwright MartinJ andLafferty     Highdimensionalisingmodelselectionusing regularizedlogisticregression Ann Stat Rosenblatt JonathanandNadler Boaz Ontheoptimalityofaveragingindistributedstatisticallearning ArXiveprints arXiv July Rudelson MarkandZhou Shuheng Reconstructionfromanisotropicrandommeasurements InformationTheory IEEETransactionson Shamir OhadandSrebro Nathan Distributedstochasticoptimizationandlearning In ndAnnualAllertonConferenceonCommunication Control andComputing Allerton pp IEEE Shamir Ohad Srebro Nathan andZhang Tong Communicationef cientdistributedoptimizationusinganapproximatenewton typemethod InProceedingsofThe stInternationalConferenceonMachineLearning pp Smith Virginia Forte Simone Ma Chenxin Takac Martin Jordan MichaelI andJaggi Martin Cocoa Ageneralframeworkforcommunication ef cientdistributedoptimization arXivpreprintarXiv Tibshirani RobertJ Regressionshrinkageandselectionviathelasso     Stat Soc   ISSN vandeGeer SaraA Highdimensionalgeneralizedlinearmodelsandthelasso Ann Stat vandeGeer SaraA andB uhlmann Peter Ontheconditionsusedtoproveoracleresultsforthelasso Electron   Stat Vershynin Roman Introductiontothenonasymptoticanalysisofrandommatrices InEldar     andKutyniok   eds CompressedSensing TheoryandApplications CambridgeUniversityPress Wainwright MartinJ Sharpthresholdsforhighdimensionalandnoisysparsityrecoveryusing constrainedquadraticprogramming lasso IEEETrans Inf Theory ISSN doi TIT Wang Jialei Kolar Mladen andSrebro Nathan Distributedmultitasklearning ArXiveprints arXiv October Wu TongTong Chen YiFang Hastie TrevorJ Sobel Eric andLange KennethL Genomewideasso ciationanalysisbylassopenalizedlogisticregression Bioinformatics doi bioinformatics btp Yang Tianbao Tradingcomputationforcommunication Distributedstochasticdualcoordinateascent InBurges       Bottou   Welling   Ghahramani   andWeinberger     eds AdvancesinNeuralInformationProcessingSystems pp CurranAssociates Inc Yuan   andLin   Modelselectionandestimationinthegaussiangraphicalmodel Biometrika Zhang CunHuiandZhang StephanieS Con denceintervalsforlowdimensionalparametersinhighdimen sionallinearmodels     Stat Soc   Jul Zhang YuchenandXiao Lin Communicationef cientdistributedoptimizationofselfconcordantempiricalloss ArXiveprints arXiv Zhang Yuchen Wainwright MartinJ andDuchi JohnC Communicationef cientalgorithmsforstatisticaloptimization InAdvancesinNeuralInformationProcessingSystems pp Zhang Yuchen Duchi JohnC Jordan MichaelI andWainwright MartinJ Informationtheoreticlowerboundsfordistributedstatisticalestimationwithcom municationconstraints InAdvancesinNeuralInformationProcessingSystems pp   Zhang Yuchen Duchi JohnC andWainwright MartinJ Communicationef cientalgorithmsforstatisticaloptimization   Mach Learn Res   ISSN Zhang Yuchen Duchi JohnC andWainwright MartinJ Divideandconquerkernelridgeregression Adistributedalgorithmwithminimaxoptimalrates arXivpreprintarXiv   Zhao Tianqi Cheng Guang andLiu Han Apartiallylinearframeworkformassiveheterogeneousdata ArXiveprints arXiv October   Zhao Tianqi Kolar Mladen andLiu Han Ageneralframeworkforrobusttestingandcon denceregionsinhighdimensionalquantileregression ArXiveprints arXiv December   Zhu JiandHastie TrevorJ Classi cationofgenemicroarraysbypenalizedlogisticregression Biostatistics doi biostatistics kxg Zinkevich Martin Weimer Markus Smola AlexanderJ andLi Lihong Parallelizedstochasticgradientdescent InAdvancesinNeuralInformationProcessing pp CurranAssociates Inc 