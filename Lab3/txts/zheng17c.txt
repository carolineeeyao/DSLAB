Collect at Once  Use Effectively 

Making Noninteractive Locally Private Learning Possible

Kai Zheng     Wenlong Mou     Liwei Wang  

Abstract

Noninteractive Local Differential Privacy  LDP 
requires data analysts to collect data from users
through noisy channel at once 
In this paper 
we extend the frontiers of Noninteractive LDP
learning and estimation from several aspects  For
learning with smooth generalized linear losses 
we propose an approximate stochastic gradient
oracle estimated from noninteractive LDP channel using Chebyshev expansion  which is combined with inexact gradient methods to obtain an
ef cient algorithm with quasipolynomial sample complexity bound  For the highdimensional
world  we discover that under  cid norm assumption on data points  highdimensional sparse
linear regression and mean estimation can be
achieved with logarithmic dependence on dimension  using random projection and approximate
recovery  We also extend our methods to Kernel Ridge Regression  Our work is the  rst
one that makes learning and estimation possible
for   broad range of learning tasks under noninteractive LDP model 

  Introduction
Data privacy has become an increasingly important issue
in the age of data science  Differential Privacy  DP  proposed in   by Dwork et al Dwork et al    provide
  solid foundation and rigorous standard for private data
analysis  Since then  there has been extensive literature
studying the fundamental tradeoffs between differential
privacy and accuracy for query answering  Hardt   Rothblum    Hardt et al    Thaler et al    Wang

 Equal contribution  Key Laboratory of Machine Perception 
MOE  School of EECS  Peking University  Beijing  China  Correspondence to  Kai Zheng  zhengk pku edu cn  Wenlong Mou  mouwenlong pku edu cn  Liwei Wang  wanglw cis pku edu cn 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

et al    machine learning  Chaudhuri   Monteleoni 
  Chaudhuri et al    Rubinstein et al    Wang
et al    and statistical inference  Lei    Smith 
  For more details on DP results  please refer to the
excellent monograph written by Dwork and Roth  Dwork
  Roth    Intuitively    DP algorithm uses randomized response to defend against adversary  so that change
of one of data points could not be detected 
Despite the prevailing success of this notion in academia 
its applicability in data science practice could be limited 
For example  if data analysts just promise to follow the
differential privacy constraints  user will not feel their privacy are preserved  The promise could not be validated 
the mechanisms are complicated  and even worse  users do
not trust the data collector at all  Unfortunately  most of
differential privacy algorithms are based on adding noise
calibrated to stability of loss function  which essentially requires access to original data 
Borrowing ideas from classical wisdom on collecting sensitive survey data  Warner    Local Differential Privacy  LDP   Kasiviswanathan et al    Duchi et al 
    was proposed as   stronger notion of privacy to resolve this problem  LDP requires each of data points to
be passed through   noisy channel during collection  This
channel will ensure one can hardly tell anything about the
user based on what he have sent  The practical advantage of LDP is obvious  users will be comfortable sending
their sensitive information through noisy channels  which
are transparent and reliable  additionally  users can choose
their own privacy parameters  making it possible to associate with economic value  Therefore  this line of research has attracted lots of attention  Duchi et al       
Kairouz et al    Bassily   Smith    Kairouz et al 
 
Despite the analogy in de nition  the way in which LDP
achieves accurate results are fundamentally different from
classical DP  Essentially  the information collected from
each user is almost completely noisy  from which one needs
to obtain accurate results  The only way to do that is to
make the independently distributed noise cancel out with
each other in some sense  With sand being washed away
by waves  golds begin to appear 

Noninteractive Local DP Learning

Two local privacy notions have been discussed in existing
literature  the interactive model allows the algorithm to collect data sequentially  and decide what to ask based on information from previously asked users  The noninteractive
model  on the contrary  requires all data to be collected
at once  with no interactive queries allowed  Apparently
the noninteractive model is strictly stronger  and prohibition on interactive queries rules out most of SGDtype approaches  making the problem signi cantly harder  However  noninteractive LDP is more useful in realworld applications  as opportunities of interactive queries may not
be available in most settings 
In existing literature 
learning and inference under interactive and noninteractive LDP therefore are exhibiting different appearances 
In the interactive world  LDP
is promised with connection to Statistical Query  SQ 
model  Kearns   
from its very beginning  Kasiviswanathan et al    SQ algorithms for   wide
range of convex ERM problems were proposed by  Feldman et al    implying good risk bounds for LDP 
 Duchi et al      established matching upper and lower
bounds for convex risk minimization problems  On the
other hand  very few has been done in the noninteractive
setting  Existing works primarily focus on basic estimation problems such as means and discrete densities  Duchi
et al        Bassily   Smith    or some function calculations  Kairouz et al      Most of important
modern learning and inference tasks  including estimation
in linear models and convex ERM  are still poorly understood in noninteractive local DP settings 
For the highdimensional world  where    cid    while some
lowcomplexity constraints are imposed  we may hope the
error induced by privacy constraints to be logarithmically
dependent upon    In classical differential privacy literature  this has been be addressed using different techniques 
guarantee error bounds logarithmically dependent on dimension  Talwar et al    Smith   Thakurta   
However  lower bounds have been shown in local privacy
model even for highdimensional  sparse mean estimation  ruling out any good guarantees  Duchi et al   
The lower bound result illustrates fundamental dif culties
of local differential privacy  But if we still want to do highdimensional learning under local privacy  are there additional assumptions that helps 
Therefore  the starting point of this work lies on making
learning possible under the noninteractive LDP setting 
which is the hardest yet the most useful  We initiate the
 rst attempt towards   broad range of learning tasks beyond
simple distribution estimation  In particular  we investigate
two important classes of problems under noninteractive
LDP    Highdimensional sparse linear regression and
mean estimation    Generalized linear models  Our fo 

cus is to design corresponding mechanisms and study their
convergence rates with respect to the number and dimension of data  One can also consider optimal mechanisms
in terms of privacy parameters like  Geng   Viswanath 
  which is of independent interests 
Our Contributions  In this paper  we propose several ef 
 cient algorithms for learning and estimation problems under noninteractive LDP model  with good theoretical guarantees  In the following we summarize our contributions 
  High Dimensional Estimation  One of exciting
 ndings in this paper is about local privacy for highdimensional data  Roughly speaking  convergence rate
with logarithmic dependence on the dimension can be attained under LDP  if we assume data points are  cid  bounded 
This is in sharp contrast with informationtheoretic lower
bounds for  sparse mean estimation for  cid  bounded
data  Duchi et al    Valid algorithms are presented for
both sparse mean estimation and sparse linear regression 
respectively 
Intuitively  noninteractivity doesn   bring
about additional dif culties  since the loss functions are
quadratic forms  However  if we directly add noise to each
of data points and send it to the server  the aggregated noise
will lead to linear dependence on the dimension  Thus we
adopt the random projection technique  and send the noisy
version of projected data to the server  Based on the aggregated information  we can approximately recover the optimal solution via linear inverse problem 
  Learning Smooth Generalized Linear Models  Generalized linear problems which has additional smooth properties  we call the loss with respect to it as smooth generalized linear loss  SGLL  see rigorous de nition in section
  include many common loss functions  such as logistic
loss  square loss  etc  Optimizing such losses are intuitively
much more dif cult in noninteractive LDP model  as the
loss can be an arbitrary function wT    This even makes it
dif cult for us to obtain an unbiased estimator for objective
function  or its gradient  As   result  when we aggregate the
loss of noisy data together  it is even hard to ensure it converge to the population loss  Approximation theory techniques are introduced to tackle this problem  In particular 
we use polynomials of wT   to approximate nonlinear coef cients of gradients  Chebyshev bases  instead of Taylor
series  are used to get faster convergence within an arbitrary domain  Then we are able to build inexact stochastic
gradient oracles to arbitrarily speci ed accuracy  SIGM algorithm in  Dvurechensky   Gasnikov    is exploited
to  nd the minimizer with inexact gradients 
Other Related Work 
Local privacy dates back
to  Warner    who uses random responses to protect
privacy in surveys  In recent LDP literature  both  Duchi
et al      and  Kairouz et al    studied density estimation methods and their theoretical behaviors in

Noninteractive Local DP Learning

LDP model  Rather than statistical setting in above two
work   Bassily   Smith    considered how to produce
frequent items and corresponding frequencies of   dataset
in local model  Besides   Kairouz et al    investigated
optimality of LDP mechanisms based on information theoretical measures for statistical discrimination 
Approximation techniques are commonly used in DP literature   Thaler et al    employed polynomials for
marginal queries   Wang et al    leveraged trigonometric polynomials to answer smooth queries 
 Zhang
et al    also used polynomial approximations and get
basic convergence results in standard DP model  Besides 
the random projection and recovery has also been used in
DP learning  Kasiviswanathan   Jin    and local DP
histogram estimation  Bassily   Smith   
In standard DP model  both highdimensional sparse estimation and generalized linear model have been intensively studied   Kifer et al    and  Smith   Thakurta 
  considered the convergence of private LASSO estimator under RSC and incoherence assumptions 
 Talwar et al    considered constrained ERM of sparse
linear regression  and obtained    log      rate using
private FrankWolfe  Above results assume  cid bounded
data  By stronger assumption of  cid  bounded data   Kasiviswanathan   Jin    gave   general framework for
high dimensional empirical risk minimization  ERM  problem  There are several works to estimate generalized linear model under DP  with   particular emphasis on logistic regression  Objective and output perturbation are used
to get low excess risks  Chaudhuri   Monteleoni   
Chaudhuri et al    Both  Bassily et al    and
 Zhang et al    considered concrete private algorithms
to solve ERM  None of these existing results extends directly to noninteractive LDP setting 

  Preliminaries
Some notations                 Vectors are written in bold symbol  such as         represents univariate number  which has no relation with    For   vector              xd     xk represents the power of
each element            cid   cid 
 cid     Denote   
as the semipositive matrix space  ProjS  means projecting   matrix to    in terms of Frobenius norm      
eliminate all negative eigenvalues  For an univariate function               represents its kth derivative  and de ne
    dx  For the reason of limited
space  all omitted proof can be found in the supplementary 

 cid cid      cid cid      cid   

        

 

 

  Local Differential Privacy

Here we adopt the LDP de nition given in  Bassily  
Smith   

De nition     mechanism           is said to be    
local differential private or    LDP  if for any      cid      
and any  measurable  subset        there is

Pr            cid     Pr     cid          

If         
 cid    then              is    LDP  where

Just the same with basic results in DP  Dwork   Roth 
  there are corresponding basic results for LDP 
    Rd  and          Id       cid  ln 
Lemma    Gaussian Mechanism 
Rd cid   cid 
Lemma    Composition Theorem  Let Qi       Zi be
 cid  
an        LDP mechanism for         Then if           
then      is  cid  
   Zi is de ned to be                       Qk   

      cid  

      LDP 

The following simple mechanism add Gaussian noise to
preserve LDP of   vector  which serves as   basic tool in
LDP learning and estimation 

Algorithm   Basic Private Vector mechanism
Input    vector     Rd  privacy parameter     for LDP
 
Output  Private vector  
  Setting    
  if  cid   cid      then
      cid   cid 
 
  end if
             where          Id 

  ln 

 

Theorem   Algorithm   preserves    LDP 

  High Dimensional and Nonparametric

Learning via Random Projections

In this section we consider three learning problems under
noninteractive LDP  Mean Estimation and Linear Regression in Highdimensions  as well as Kernel Ridge Regression  Using random projection techniques  we are able to
get logarithmic dependence on   in highdimensional settings  and also to get good guarantees for Kernel version 
The  rst problem is considered in statistical settings  as we
need to assume   sparse mean vector  The latter two problems are considered as ERM problems  which can easily be
translated to population risk using uniform convergence 

  Highdimensional Mean Estimation

In this section  we propose   noninteractive LDP mechanism for highdimensional sparse mean estimation problem  By assuming  cid  bounded data points  and  cid  bounded

 Note one can also use the advanced composition mechanism
 Kairouz et al      with   re ned analysis  but the main dependence over   and   will remain nearly the same 

Noninteractive Local DP Learning

population mean  we can get error rates with logarithmic
dependence on    Our results are in sharp contrast with the
lower bound for  cid bounded general mean estimation under standard DP  Bassily et al    as well as the lower
bound for  cid bounded  sparse mean estimation under local DP  Duchi et al    It can be easily seen that our
method extends to mean estimation problem for arbitrary
lowcomplexity constraint set in high dimensions  We state
our results in  cid  setting to keep the arguments clear  Our
problem adopts   statistical estimation setting as follows 
 cid bounded sparse mean estimation Suppose there is
an unknown distribution   supported on      with
 cid ED   cid      The  cid bounded sparse mean estimation
problem requires us to produce an estimator   that makes
 cid    ED   cid  small with high probability 

Algorithm   LDP  cid  Constrained Mean Estimation
Input          xn          
Output  Estimator  

 
Set      cid 
Sample      
for User   do

  cid  and      cid  log  
 cid 
pN        

Collect yi   Gxi   ri 
with ri               log 

 

Ip 

 cid 

end for
for              do

        

Sj  
Let       Sj 

 cid 
            

  Sj

yi 

      jn

 

 cid 

 

end for
Let              
for              do

Let rj   min cid        cid               

 cid 

 

end for
Let      arg minj rj  and        
Solve the following convex program 

 cid   cid 

arg min

 

    cid Gz     cid       log nd 

 

 cid   

 

 

nel  This locally private estimation procedure can be
viewed as   variant of noisy compressed sensing  where
 cid  recovery rate is fundamentally controlled by the Gaussian Mean Width of constraint set  Vershynin   
Though the distribution has bounded support 
the concentration for mean estimation is dimensiondependent 
while dimensionindependent Markov Inequalities hold 
To tackle this problem  we employ Medianof Mean estimator to get exponential tails  Hsu   Sabato   
We  rst give the following bound on the error in projected
space 
Lemma   Let         xn           with     ED   
and supp           Let   and  yi  
   de ned in the
above procedure  For each of group Sj  xed  we have the
following with probability  

 cid cid cid 

 cid 

 cid 

  log nd 

 cid Sj 

yi     

   

 

 cid cid cid   

 Sj 

 cid 

yi Sj

The aggregation step in Algorithm   is   highdimensional
generalization of Medianof Mean estimator used in
heavytailed statistics  The tail properties are guaranteed
in the following lemma 
Lemma    Proposition   in  Hsu   Sabato   
Suppose in metric space       set of points    
                     with     dX              
   
Let   be generated from the following procedure  ri  

min cid      BX                

 cid and     arg min   ri 

 

Then we have 

    dX                 

 

Since the original data are        samples from underlying
distribution  small group with  xed indices should also be
        Therefore          are        Combining
Lemma   and Lemma   we get the following result 
Corollary   The vector   constructed in Algorithm   satis es the following with probability      

 cid    log nd 

 cid   

 cid 

 

 

 

 cid       cid     

In Algorithm   we describe our data collection procedure
and estimation algorithm  We are primarily using two
techniques 
random projection and recovery from lowcomplexity structures  medianof mean estimator to boost
failure probability  The privacy argument is directly implication of Theorem  
Intuitively  adding noise to each entry of mean vector will
result in error rate   linear dependence on    Thus we
adopt the random projection technique to send   compressed version of data vector through the noisy chan 

Then we turn to the recovery of original mean estimator  The primary tool we are using are General    bound
in  Vershynin   
Lemma    Theorem   in  Vershynin    High Probability Version  For unknown vector         Rd  let
pN         Noisy vector     Rp with  cid cid     
     
Let     Gx     By solving the following optimization
problem 

arg min

  cid   cid   cid cid         cid Gx cid      cid     

 

Noninteractive Local DP Learning

 cid 

 cid 
 cid   

 cid   

  

 cid      cid     

log

nd
 

log

 
 

 cid 

 cid  

where  cid cid   denotes the Minkowski functional of    Then
we can get the following with probability      

 cid             log  

 cid 

 

 

 

 cid       cid cid     

where      denotes the Gaussian width of   

By putting these results together we get the bound on estimation loss 
Theorem   Algorithm   outputs   satisfying the following
with probability      

  Sparse Linear Regression

  xT

 cid    yi        

In this section  we consider empirical loss of sparse linear
      yi  where
regression                  
     xi  yi        cid xi cid 
  
De ne      argminw           where    
   cid   cid 
 cid    We want to obtain   vector wpriv    
within noninteractive LDP model  such that the empirical
excess risk   wpriv               has polynomial den 
pendences on log   and  
As in the case of highdimensional mean estimation  directly manipulating in the original high dimensional feature space will introduce large noise  hence we use   subGaussian random matrix     Rm   to project original data
      vectors in Rd  into the low dimensional space      
Rm   rst  then perturb each data in low dimensional space
      Basic Private Vector mechanism given in Algorithm  
which protects local privacy  and send it to the server 
Having obtained private synopsis  the server then reconstruct an unbiased estimator for objective function according to these private synopsis  We subtract   quadratic term
to ensure unbiasedness and project to PSD matrices to preserve convexity  To show good approximation guarantee 
we make use of RIP bounds for random projection  As
the loss function is determined by inner products between
  and data  it could be uniformly preserved in projected
space  which guarantees the accuracy of solution estimated
with local privacy  Apparently  our methods also imply
bounds with general lowcomplexity constraint set that preserves RIP 
Our private learning mechanism is given in Algorithm
  and any random projection matrix can be used here 
The privacy argument directly follows from Private Vector
Mechanism and composition 

 Our methods suits to any radius of   and   

trix     Rd  

zi   Basic Private Vector    xi     
vi   Basic Private Vector  yi     

Algorithm   LDP  cid  Constrained Linear Regression
Input  Personal data        parameter     projection maOutput  Learned classi er wpriv   Rd
  for Each user                 do
 
 
  end for
  Setting           zn        
  wpriv   argminw               where
                    

    ProjS           Im            vn  

               

  vT      

  ln 

 

 

 

 

   
  

 cid cid         cid cid     

In fact  as original data is in    ball  and random projection
preserves norms with high probabilty  hence steps   in
Algorithm   will be executed with very low probability 
Denote the true objective function in low dimensional
  yT        
space             
where            xn            Let    
 
argminw                The following lemma gives the
accuracy of private solution wpriv when reduced into low
dimensional space 
Lemma   Under the assumptions made in this section 
given projection matrix   with high probability over the
randomness of private mechanism  we have

   wpriv                           cid    

 

 cid cid   

 cid 

  

Now  combined with RIP bound for random projection  we
can move on to prove the empirical excess risk of sparse
linear regression 
Theorem   Under the assumption in this section  set    
 

 cid cid    log  

  then with high probability   there is

 cid 

  wpriv             

 cid cid  log  

 cid cid 

  

Note  Talwar et al    assume data is in    ball  while
both  Kasiviswanathan   Jin    and ours assume data
is in    ball  However  in LDP model   Duchi et al   
show it was impossible to obtain polynomial dependences
over log   for  cid  mean estimation problem if data is in   
ball 

  In nite Dimension  Kernel Ridge Regression

Previous method mainly applies to data with  nite dimensional features  However  it is common to use kernel trick
in practice  This brings about new dif culties for LDP
learning  as we could not add noise in the Hilbert space 

Noninteractive Local DP Learning

In this subsection  we take kernel ridge regression as an example to show how to use Random Fourier Features  RFF 
 Rahimi et al    to deal with such cases caused by
shiftinvariant kernels                          Note
our technique also suits to similar problems 
Fix   shiftinvariant kernel    denote the Hilbert space
implicitly de ned as    and the corresponding feature map
as     Rd      Let the Hilbert space corresponding
to the random Fourier feature map be      Rdp  and its
feature map     Rd       where dp is the RFF projection dimension  Given   subset     Rd and data
     xi  yi xi               for any               
de ne loss functions in   and    as follows 

 cid 
 cid cid 
 cid cid      xi    yi
 cid cid cid gT  xi    yi
 cid cid cid 
 cid 

 

 

 cid   cid 

 

 cid   cid 
  

 
 

 
 

 

 

   

 

 

LH       

          

 
  

 
  

where   is the regularization parameter  Denote     
argminf   LH           argming                as the
Lipschitz constant of square loss  which depends on the
bounded norm of features  Kernel ridge regression try to
optimize formula   while after using RFF  we try to solve
formula   in noninteractive LDP model  which can be
easily tackled with similar mechanisms like sparse linear
regression above  Borrow the key result in  Rubinstein
et al     restated in lemma   below  which used RFF
to design private mechanims for SVM in DP model  it becomes easy to prove guarantees for kernel ridge regression
in our setting  see Corollary  
Lemma    Rubinstein et al    Suppose dual
variables with respect to       are    norm bounded
by some       and supx               
            cid    then there is supx            

           cid        cid CG       
 cid cid   

        wpriv LH      cid    

 cid 

 cid 

  

Corollary   Algorithm   satis es    LDP  and by setting dp     

  with high probability  there is

dn 

 cid cid 
 cid cid   

  

 cid cid 

              wpriv   cid    

sup
   

  Learning Smooth Generalized Linear

Model

In this section  we consider learning smooth generalized linear model in noninteractive LDP setting  Noninteractive LDP learning for this problem is essentially dif 
 cult  as it is even hard to obtain an unbiased estimator of

Algorithm   LDP kernel mechanism
Input  Personal data  xi  yi          random feature  
dimension dp  shiftinvariant kernel          
          with Fourier
transform        

 cid    jsT xk   dx  privacy parameter    

Output  Private output  wpriv   Rdp
  Draw        samples               sdp   Rd from      
and               bdp     from the uniform distribution
on    

 
 

 cid cid   

Construct low dimensional random feature  xi   

  xi              cos sT
dp

xi   bdp  

  for                 do
 

 cid   

dp

 cid 
 cid cid   

cos sT

 cid   

 cid dp   Rdp

 

    
zi   Basic Private Vector    xi     
vi   Basic Private Vector  yi     

dp

dp

 
 
  end for
  Setting           zn        
   wpriv   argmin   
                 

    wT         

              where
  vT     

    ProjS           Idp             vn  

 

 

  ln 

 

 

gradient  We resolve this problem using Chebyshev polynomial expansion  which requires additional smoothness
assumptions  Fortunately these assumptions are naturally
satis ed by   broad range of learning tasks 
We will  rst de ne the Smooth GLM loss family with
appropriate assumptions  Our de nition could be shown
with connection to exponential family GLM  which is commonly used in machine learning  We also illustrate our algorithm and guarantees with logistic regression 
De nition    Absolutely Smooth Functions  We say that
an univariate function      is absolutely smooth  if for any
                rx  satis es the following properties 
there exist functions               which are polynomial on   and            kr  such that for any       
there is 

          cid                    are absolutely continuous

on    

   cid cid        cid cid  

 cid                   

De nition    Smooth Generalized Linear Loss  SGLL   
loss function  cid          is called smooth generalized linear loss  if for any given data         cid          is convex
and  smooth with respect to    and there exist absolutely
smooth functions           such that  cid           
 yh xT        xT   
It will be convenient to consider population risk directly 
Now  we adopt standard setting of learning problems 

Noninteractive Local DP Learning

 rxT      yh cid 
   cid   

where each data point        is drawn from some underlying unknown distribution   and  cid   cid 
 cid    Given
the population loss is de ned as
  SGLL  cid         
               cid          For simplicity  instead
of assuming   belongs to      we use the following equivalent notation   cid             yh rxT     
  rxT    and the constraint set for   is       
Denote               cid            rm           
where                cid 
 rxT    Suppose
        cid                  cid 
  where       
      This is   common assumption in stochastic optimization literature  such as  Bubeck et al   
Given any       we hope to design   noninteractive local
DP mechanism with low sample complexity  such that the
 nal output point wpriv satis es   wpriv          cid   
For GLM loss functions  it is easy to see that the stochastic
gradient evaluated on   with data point xi is at the same
direction with xi  So adding isotropic noise to xi provides
 unbiased  information about direction of stochastic gradient  However  the magnitude is   nonlinear function of
wT xi  making it hard for SGD even to converge to population minimizer  This is why we seek to  nd polynomial
approximation of the magnitude of gradients 
To estimate the magnitude of gradients  we use Chebyshev
polynomials to approximate nonlinear univariate function
  rx  where         For brevity of
fi        cid 
notations  we just use       to represent either      or
     Denote the Chebyshev approximation with degree
  as  fp       
   akTk    where Tk    is the kth Chebyshev polynomial  and ak    
    dx
 
is the corresponding coef cient  According to existing results about Chebyshev approximations and some calculations  we have the following lemma 
Lemma   Given any       by setting    
         cid            cid  where   is   constant  we have
  ln  

   cid  

 
     Tk   

 cid   

 

 cid cid cid 

 cid cid cid   fp           
 cid  

 cid   

   cid  

The Chebyshev approximations with degree   for fi   
         are denoted as  fip       
   aikTk     
   cikxk  where cik is the coef cient of term xk  Now

we approximate            and            as follows 

                       rxT           rxT   

  cid 

 

         ky rxT    

  

                           

With these approximations  we state our mechanism in Algorithm   where Basic Private Vector mechanism is given

in Algorithm   Note an important trick in Step   of
Algorithm   is that  we run basic private mechanism  
times  to obtain fresh private copies of the same vector   
which are then used to calculate an unbiased estimation of
            with variance as low as possible       line   in
Algorithm  The LDP property of Algorithm   is given
as follows  The privacy proof directly follows from Basic

Algorithm   LDP SGLD Mechanism   Collection
Input  Personal data        expansion order    privacy paOutput  Private synopsis      zyi  zj                

rameter    
         sent to the server
            

  Setting       

           

          

 

    

       Basic Private Vector       
  for                   do
zyj   Basic Private Vector           
 
  end for
  for                   
 
  end for

zj   Basic Private Vector       

do

 

Vector Mechanism and Composition Theorem 
Theorem   LDP SGLD Mechanism   preserves    
LDP 

Having obtained the private synopsis sent by all uers  now
the server can construct   stochastic inexact gradient oracle
 de ned in De ntion   for any point        as stated in
Algorithm  
De nition    Dvurechensky   Gasnikov    For an objective function               stochastic oracle returns
  turple                such that 

               
               

  cid              cid   cid   

   cid           cid   
 

 cid       cid             

where                     cid            cid 

For any        in the domain  as loss function  cid          is
convex and  smooth with respect to    we can prove the
following lemma 
Lemma   For any       setting       ln   
       
 cid           cid 
then Algorithm   outputs        
stochastic oracle de ned in De nition   where    
  

                

 cid 

 cid 

 

   

Noninteractive Local DP Learning

Algorithm   LDP SGLD Mechanism   Learning
Input  Private synopsis      zy  zj               
  of each user  public coef cients              
        initial point   

Output  Learned classi er wpriv
  for                 do
 
 

  Construct stochastic inexact gradient
  Denote the private synopsis of user   as   above
for abbreviation
Set       
for                 do

tj  cid     
 cid    cid 

 cid 

      wT

  zi 

         kzyj tkrk 

end for
   ws      
  One update via SIGM
Run one iteration of SIGM algorithm with    ws    
and obtain ws 

  

  

 
 
 
 

 

 
 

  end for
  Set wpriv   wn 

Based on above       stochastic oracle  and the algorithm proposed in SIGM paper  Dvurechensky   Gasnikov     omitted here  due to the limitation of space 
our complete learning algorithm is given in Algorithm  
Before proving our sample complexity  we state the basic
convergence result of SIGM algorithm 
Lemma    Dvurechensky   Gasnikov    Assume
  function        suppose constrain set is    is endowed
with         stochastic oracle  then the sequence wk
 corresponds to yk in the original paper  generated by the
SIGM algorithm satis es 

     wk           cid   

 cid 

 cid   

   

 

where expectation is over the randomness of the stochastic
oracle and      argminw        
The accuracy results directly follows from the quality of
inexact stochastic gradient oracle we constructed  and the
convergence result of SIGM 
Theorem   Consider smooth generalized linear loss  For
any setting       by setting      
       
 cid           cid  in Algorithm     if

          ln   

 cid    

 cid cr ln   cid   

     

   ln ln   

 
we can achieve loss guarantee   wpriv          cid   

 

 cid 

 

  
 

 cid cid 

 

As we can see  learning in noninteractive LDP model is
more dif cult than interactive form  especially when loss is

highly nonlinear  we even can not obtain an unbiased estimation either for objective function or gradients  However 
our method shows it possible to learn smooth GLM with
quasipolynomial sample complexity 

  Example  Learning Logistic Regression

 cid 

 cid   

  wT     ln      wT   

  wT   cid   
 cid   

Either from the view of exponential family generalized
linear model or the concrete loss function  it is not dif 
 cult to see logistic loss belongs to SGLL  For example 
in logistic regression   cid            log      ywT     
So we let
   ln      As we know logistic
        
loss is convex and  smooth for some parameter   and the
absolutely smooth property of linear function is obvious 
hence once we prove         ln         is absolutely
smooth  then logistic loss satis es the de nition of SGLL 
Proposition           ln         is absolutely smooth
with           

            

             rk
 

 

 

Hence  we can use private mechanisms   to learn logistic regression 
Theorem   Consider Logistic regression problem with
 cid            log    exp ywT    For any      
 cid cid 
 cid cr ln   cid   
         cid           cid 
by setting      
if      
in
Algorithm     we can achieve   wpriv          cid   

     ln ln   cid    

          ln   

 cid 

    

 

 

  Conclusions
In this paper  we consider how to design ef cient algorithms for common learning and estimation problems under
noninteractive LDP model  In particular  for sparse linear
regression and mean estimation problem  we propose ef 
cient algorithms and prove the polynomial dependence of
excess risk or square error over log   and  
   which is exactly to be expected in high dimensional case  We also
extend our methods to nonparametric case and show good
bounds for Kernel Ridge Regression 
For more dif cult smooth generalized linear loss optimization problems  we use private Chebyshev approximations
to estimate gradients of the objective loss  combined with
existing inexact gradient descent methods to obtain  nal
outputs  The sample complexity of our mechanism is
  where   is the desired
quasipolynomial with respect to  
population excess risk 
An interesting open problem is whether our theoretical
guarantees are optimal  If not  how to improve them while
preserving the ef ciency in noninteractive LDP model 
We think these problems are critical to understand LDP in
the future 

Noninteractive Local DP Learning

Acknowledgments
This work was partially supported by National Basic
Research Program of China   Program   grant no 
 CB  NSFC   We would like to
thank the anonymous reviewers for their valuable comments on our paper 

References
Bassily  Raef and Smith  Adam  Local  private  ef cient
protocols for succinct histograms  In Proceedings of the
FortySeventh Annual ACM on Symposium on Theory of
Computing  pp    ACM   

Bassily  Raef  Smith  Adam  and Thakurta  Abhradeep  Private empirical risk minimization  Ef cient algorithms
and tight error bounds  In Foundations of Computer Science  FOCS    IEEE  th Annual Symposium on 
pp    IEEE   

Bubeck    ebastien et al  Convex optimization  Algorithms
and complexity  Foundations and Trends   cid  in Machine
Learning     

Chaudhuri     and Monteleoni     Privacypreserving logistic regression  In Conference on Neural Information
Processing Systems  British Columbia  Canada  December  pp     

Chaudhuri     Monteleoni     and Sarwate        Differentially private empirical risk minimization  The Journal
of Machine Learning Research     

Duchi  John  Wainwright  Martin    and Jordan  Michael   
Local privacy and minimax bounds  Sharp rates for
probability estimation  In Advances in Neural Information Processing Systems  pp       

Duchi  John  Wainwright  Martin  and Jordan  Michael 
Minimax optimal procedures for locally private estimation  arXiv preprint arXiv   

Duchi  John    Jordan  Michael    and Wainwright  Martin    Local privacy and statistical minimax rates 
In
Foundations of Computer Science  FOCS    IEEE
 th Annual Symposium on  pp    IEEE     

Dvurechensky  Pavel and Gasnikov  Alexander  Stochastic
intermediate gradient method for convex problems with
stochastic inexact oracle  Journal of Optimization Theory and Applications     

Dwork     McSherry     Nissim     and Smith     Calibrating noise to sensitivity in private data analysis  In
Theory of cryptography  pp    Springer  New
York  USA   

Dwork  Cynthia and Roth  Aaron 

The algorithmic
foundations of differential privacy  Foundations and
Trends   cid  in Theoretical Computer Science   
   

Feldman  Vitaly  Guzm an  Crist obal  and Vempala  Santosh  Statistical query algorithms for mean vector estimation and stochastic convex optimization  In Proceedings of the TwentyEighth Annual ACMSIAM Symposium on Discrete Algorithms  pp    Society for
Industrial and Applied Mathematics   

Geng  Quan and Viswanath  Pramod  The optimal mechIn Information Theory
anism in differential privacy 
 ISIT    IEEE International Symposium on  pp 
  IEEE   

Hardt     and Rothblum          multiplicative weights
In
mechanism for privacypreserving data analysis 
IEEE Symposium on Foundations of Computer Science 
pp     

Hardt     Ligett     and Mcsherry       simple and practical algorithm for differentially private data release  In
Advances in Neural Information Processing Systems  pp 
   

Hsu  Daniel and Sabato  Sivan  Loss minimization and parameter estimation with heavy tails  Journal of Machine
Learning Research     

Kairouz  Peter  Oh  Sewoong  and Viswanath  Pramod 
Extremal mechanisms for local differential privacy 
In
Advances in neural information processing systems  pp 
   

Kairouz  Peter  Oh  Sewoong  and Viswanath  Pramod  The
In Procomposition theorem for differential privacy 
ceedings of The  nd International Conference on Machine Learning  pp       

Kairouz  Peter  Oh  Sewoong  and Viswanath  Pramod  SeIn Advances in
cure multiparty differential privacy 
Neural Information Processing Systems  pp   
   

Kairouz  Peter  Bonawitz  Keith  and Ramage  Daniel  Discrete distribution estimation under local privacy  In Proceedings of The  rd International Conference on Machine Learning  pp     

Kasiviswanathan        Lee        Nissim     Raskhodnikova     and Smith     What can we learn privately 
In IEEE Symposium on Foundations of Computer Science  pp     

Kasiviswanathan  Shiva Prasad and Jin  Hongxia  Ef cient
private empirical risk minimization for highdimensional

Noninteractive Local DP Learning

Wang     Jin     Fan     Zhang     Huang     Zhong 
   and Wang     Differentially private data releasing for
smooth queries  Journal of Machine Learning Research 
   

Warner  Stanley    Randomized response    survey technique for eliminating evasive answer bias  Journal of the
American Statistical Association     

Zhang  Jiaqi  Zheng  Kai  Mou  Wenlong  and Wang  Liwei  Ef cient private erm for smooth objectives  arXiv
preprint arXiv   

Zhang  Jun  Zhang  Zhenjie  Xiao  Xiaokui  Yang  Yin  and
Winslett  Marianne  Functional mechanism  regression
analysis under differential privacy  Proceedings of the
VLDB Endowment     

learning  In Proceedings of The  rd International Conference on Machine Learning  pp     

Kasiviswanathan  Shiva Prasad  Lee  Homin    Nissim 
Kobbi  Raskhodnikova  Sofya  and Smith  Adam  What
can we learn privately  SIAM Journal on Computing   
   

Kearns  Michael  Ef cient noisetolerant learning from statistical queries  Journal of the ACM  JACM   
   

Kifer  Daniel  Smith  Adam  and Thakurta  Abhradeep 
Private convex empirical risk minimization and highdimensional regression  Journal of Machine Learning
Research     

Lei     Differentially private mestimators 

In Advances
in Neural Information Processing Systems  pp   
 

Rahimi  Ali  Recht  Benjamin  et al  Random features for
largescale kernel machines  In NIPS  volume   pp   
 

Rubinstein     Bartlett        Huang     and Taft    
Learning in   large function space  Privacypreserving
mechanisms for svm learning  Journal of Privacy and
Con dentiality     

Smith     Privacypreserving statistical estimation with optimal convergence rates  In ACM Symposium on Theory
of Computing  STOC  pp     

Smith  Adam and Thakurta  Abhradeep  Differentially private model selection via stability arguments and the robustness of the lasso    Mach Learn Res Proc Track   
   

Talwar  Kunal  Thakurta  Abhradeep  and Zhang  Li 
Nearly optimal private lasso  In Advances in Neural Information Processing Systems  pp     

Thaler     Ullman     and Vadhan     Faster algorithms
for privately releasing marginals  In International Colloquium on Automata  Languages  and Programming  volume   pp     

Vershynin  Roman  Estimation in high dimensions    geometric perspective  In Sampling theory    renaissance 
pp    Springer   

Wang     Fienberg        and Smola        Privacy for free 
Posterior sampling and stochastic gradient monte carlo 
In International Conference on Machine Learning  pp 
   

