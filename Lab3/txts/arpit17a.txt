  Closer Look at Memorization in Deep Networks

Devansh Arpit       Stanis aw Jastrz ebski     Nicolas Ballas       David Krueger       Emmanuel Bengio  

Maxinder    Kanwal   Tegan Maharaj     Asja Fischer   Aaron Courville       Yoshua Bengio      

Simon LacosteJulien    

Abstract

We examine the role of memorization in deep
learning  drawing connections to capacity  generalization  and adversarial robustness  While
deep networks are capable of memorizing noise
data  our results suggest that they tend to prioritize learning simple patterns  rst 
In our
experiments  we expose qualitative differences
in gradientbased optimization of deep neural
networks  DNNs  on noise vs  real data  We
also demonstrate that for appropriately tuned
explicit regularization       dropout  we can
degrade DNN training performance on noise
datasets without compromising generalization on
real data  Our analysis suggests that the notions
of effective capacity which are dataset independent are unlikely to explain the generalization
performance of deep networks when trained with
gradient based methods because training data itself plays an important role in determining the
degree of memorization 

  Introduction
The traditional view of generalization holds that   model
with suf cient capacity       more parameters than training
examples  will be able to  memorize  each example  over 
 tting the training set and yielding poor generalization to
validation and test sets  Goodfellow et al    Yet deep
neural networks  DNNs  often achieve excellent generalization performance with massively overparameterized
models  This phenomenon is not wellunderstood 

 Equal contribution  Montr al Institute for Learning Algorithms  Canada  Universit  de Montr al  Canada  Jagiellonian
University  Krakow  Poland  McGill University  Canada
 University of California  Berkeley  USA  Polytechnique
Montr al  Canada  University of Bonn  Bonn  Germany
 CIFAR Fellow  CIFAR Senior Fellow  Correspondence to 
 david krueger umontreal ca 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

From   representation learning perspective  the generalization capabilities of DNNs are believed to stem from
their incorporation of good generic priors  see       Bengio et al    Lin   Tegmark   further suggest
that the priors of deep learning are well suited to the physical world  But while the priors of deep learning may help
explain why DNNs learn to ef ciently represent complex
realworld functions  they are not restrictive enough to rule
out memorization 
On the contrary  deep nets are known to be universal approximators  capable of representing arbitrarily complex
functions given suf cient capacity  Cybenko    Hornik
et al    Furthermore  recent work has shown that the
expressiveness of DNNs grows exponentially with depth
 Montufar et al    Poole et al    These works 
however  only examine the representational capacity  that
is  the set of hypotheses   model is capable of expressing
via some value of its parameters 
Because DNN optimization is not wellunderstood  it is unclear which of these hypotheses can actually be reached by
gradientbased training  Bottou    In this sense  optimization and generalization are entwined in DNNs  To account for this  we formalize   notion of the effective capacity  EC  of   learning algorithm    de ned by specifying
both the model and the training procedure      train the
LeNet architecture  LeCun et al    for   epochs using stochastic gradient descent  SGD  with   learning rate
of   as the set of hypotheses which can be reached
by applying that learning algorithm on some dataset  Formally  using setbuilder notation 

EC              such that           

where      represents the set of hypotheses that is reachable by   on   dataset   
One might suspect that DNNs effective capacity is suf 
ciently limited by gradientbased training and early stopping to resolve the apparent paradox between DNNs  excellent generalization and their high representational capacity 
However  the experiments of Zhang et al    suggest
that this is not the case  They demonstrate that DNNs are

  Since   can be stochastic       is   set 

  Closer Look at Memorization in Deep Networks

able to    pure noise without even needing substantially
longer training time  Thus even the effective capacity of
DNNs may be too large  from the point of view of traditional learning theory 
By demonstrating the ability of DNNs to  memorize  random noise  Zhang et al    also raise the question
whether deep networks use similar memorization tactics on
real datasets 
Intuitively    bruteforce memorization approach to  tting data does not capitalize on patterns shared
between training examples or features  the content of what
is memorized is irrelevant    paradigmatic example of
  memorization algorithm is knearest neighbors  Fix  
Hodges Jr    Like Zhang et al    we do not
formally de ne memorization  rather  we investigate this
intuitive notion of memorization by training DNNs to   
random data 

Main Contributions

We operationalize the de nition of  memorization  as the
behavior exhibited by DNNs trained on noise  and conduct
  series of experiments that contrast the learning dynamics
of DNNs on real vs  noise data  Thus  our analysis builds
on the work of Zhang et al    and further investigates
the role of memorization in DNNs 
Our  ndings are summarized as follows 

  There are qualitative differences in DNN optimization
behavior on real data vs  noise  In other words  DNNs
do not just memorize real data  Section  

  DNNs learn simple patterns  rst  before memorizing
 Section  
In other words  DNN optimization is
contentaware  taking advantage of patterns shared by
multiple training examples 

  Regularization techniques can differentially hinder
memorization in DNNs while preserving their ability
to learn about real data  Section  

  Experiment Details
We perform experiments on MNIST  LeCun et al   
and CIFAR   Krizhevsky et al  datasets  We investigate two classes of models   layer multilayer perceptrons  MLPs  with recti er linear units  ReLUs  on MNIST
and convolutional neural networks  CNNs  on CIFAR 
If not stated otherwise  the MLPs have   hidden units
per layer and are trained for   epochs with SGD and
learning rate   The CNNs are   small Alexnetstyle
CNN   as in Zhang et al    and are trained using
 Input   Crop    Conv    BN   ReLU  
MaxPooling    Conv    BN  ReLU  MaxPool 

SGD with momentum  and learning rate of   scheduled to drop by half every   epochs 
Following Zhang et al    in many of our experiments
we replace either  some portion of  the labels  with random
labels  or the inputs  with        Gaussian noise matching
the real dataset   mean and variance  for some fraction of
the training set  We use randX and randY to denote datasets
with   unless speci ed  noisy inputs and labels  respectively 

  Qualitative Differences of DNNs Trained

on Random vs  Real Data

Zhang et al    empirically demonstrated that DNNs
are capable of  tting random data  which implicitly necessitates some high degree of memorization  In this section 
we investigate whether DNNs employ similar memorization strategy when trained on real data  In particular  our
experiments highlight some qualitative differences between
DNNs trained on real data vs  random data  supporting the
fact that DNNs do not use bruteforce memorization to   
real datasets 

  Easy Examples as Evidence of Patterns in Real

Data

  bruteforce memorization approach to  tting data should
apply equally well to different training examples  However  if   network is learning based on patterns in the data 
some examples may    these patterns better than others  We
show that such  easy examples   as well as correspondingly  hard examples  are common in real  but not in
random  datasets  Speci cally  for each setting  real data 
randX  randY  we train an MLP for   single epoch starting from   different random initializations and shuf ings
of the data  We  nd that  for real data  many examples
are consistently classi ed  in correctly after   single epoch 
suggesting that different examples are signi cantly easier
or harder in this sense  For noise data  the difference between examples is much less  indicating that these examples are     more  independently  Results are presented in
Figure  
For randX  apparent differences in dif culty are well modeled as random Binomial noise  For randY  this is not the
case  indicating some use of shared patterns  Visualizing
 rstlevel features learned by   CNN supports this hypothesis  Figure  
ing    Dense    BN   ReLU   Dense    BN
  ReLU   Dense classes    Softmax  Here Crop      crops
height and width from both sides with respective values 

  Closer Look at Memorization in Deep Networks

We  nd that for real data  only   subset of the training set
has high  gx  while for random data   gx is high for virtually
all examples  We also  nd   different behavior when each
example is given   unique class  in this scenario  the network has to learn to identify each example uniquely  yet
still behaves differently when given real data than when
given random data as input 
We visualize  Figure   the spread of  gx as training progresses by computing the Gini coef cient over      The
Gini coef cient  Gini    is   measure of the inequality
among values of   frequency distribution    coef cient of
  means exact equality       all values are the same  while
  coef cient of   means maximal inequality among values 
We observe that  when trained on real data  the network has
  high  gx for   few examples  while on random data the network is sensitive to most examples  The difference between
the random data scenario  where we know the neural network needs to do memorization  and the real data scenario 
where we re trying to understand what happens  leads us to
believe that this measure is indeed sensitive to memorization  Additionally  these results suggest that when being
trained on real data  the neural network probably does not
memorize  or at least not in the same manner it needs to for
random data 
In addition to the different behaviors for real and random
data described above  we also consider   class speci   losst  Lt       xy   
sensitivity   gi           
where Lt        is the term in the crossentropy sum corresponding to class    We observe that the losssensitivity
       class   for training examples of class   is higher when
       but more spread out for real data  see Figure  
An interpretation of this is that for real data there are more
interesting crosscategory patterns that can be learned than
for random data 
Figure   and   were obtained by training   fullyconnected
network with   layers of   units on   downscaled  
  MNIST digits using SGD 

   cid  

  Capacity and Effective Capacity

In this section  we investigate the impact of capacity and
effective capacity on learning of datasets having different
amounts of random input data or random labels 

  EFFECTS OF CAPACITY AND DATASET SIZE ON

VALIDATION PERFORMANCES

In    rst experiment  we study how overall model capacity impacts the validation performances for datasets with
different amounts of noise  On MNIST  we found that the
optimal validation performance requires   higher capacity
model in the presence of noise examples  see Figure  
This trend was consistent for noise inputs on CIFAR  but

Figure   Average  over   experiments  misclassi cation rate
for each of   examples after one epoch of training  This measure of an example   dif culty is much more variable in real data 
We conjecture this is because the easier examples are explained
by some simple patterns  which are reliably learned within the
 rst epoch of training  We include   points samples from  
binomial distribution with       and   equal to the average
estimated   correct  for randX  and note that this curve closely
resembles the randX curve  suggesting that random inputs are all
equally dif cult 

Figure   Filters from  rst layer of network trained on CIFAR 
 left  and randY  right 

  LossSensitivity in Real vs  Random Data

To further investigate the difference between real and fully
random inputs  we propose   proxy measure of memorization via gradients  Since we cannot measure quantitatively
how much each training sample   is memorized  we instead
measure the effect of each sample on the average loss  That
is  we measure the norm of the loss gradient with respect
to   previous example   after   SGD updates  Let Lt be the
loss after   updates  then the sensitivity measure is given by

     cid Lt   cid   
gt

The parameter update from training on   in uences all future Lt indirectly by changing the subsequent updates on
different training examples  We denote the average over gt
 
after   steps as  gx  and refer to it as losssensitivity  Note
that we only report  cid norm results  but that results stay
very similar using  cid norm and in nity norm 
We compute gt
  by unrolling   SGD steps and applying
backpropagation over the unrolled computation graph  as
done by Maclaurin et al    Unlike Maclaurin et al 
  we only use this procedure to compute gt
   and do
not modify the training procedure in any way 

  Closer Look at Memorization in Deep Networks

Figure   Plots of the Gini coef cient of  gx over examples    see section   as training progresses  for    example real dataset
    MNIST  versus random data  On the left    is the normal class label  on the right  there are as many classes as examples  the
network has to learn to map each example to   unique class 

Figure   Plots of perclass gx  see previous  gure  log scale   
cell      represents the average          xy         the losssensitivity of examples of class          training examples of class
   Left is real data  right is random data 

we did not notice any relationship between capacity and
validation performance on random labels on CIFAR 
This result contradicts the intuitions of traditional learning
theory  which suggest that capacity should be restricted  in
order to enforce the learning of  only  the most regular patterns  Given that DNNs can perfectly    the training set in
any case  we hypothesize that that higher capacity allows
the network to    the noise examples in   way that does
not interfere with learning the real data  In contrast  if we
were simply to remove noise examples  yielding   smaller
 clean  dataset    lower capacity model would be able to
achieve optimal performance 

  EFFECTS OF CAPACITY AND DATASET SIZE ON

TRAINING TIME

Our next experiment measures timeto convergence      
how many epochs it takes to reach   training accuracy  Reducing the capacity or increasing the size of the
dataset slows down training as well for real as for noise

Figure   Performance as   function of capacity in  layer MLPs
trained on  noisy versions of  MNIST  For real data  performance
is already very close to maximal with   hidden units  but when
there is noise in the dataset  higher capacity is needed 

data  However  the effect is more severe for datasets containing noise  as our experiments in this section show  see
Figure  
Effective capacity of   DNN can be increased by increasing the representational capacity       adding more hidden
units  or training for longer  Thus  increasing the number of hidden units decreases the number of training iterations needed to    the data  up to some limit  We observe stronger diminishing returns from increasing representational capacity for real data  indicating that this limit
is lower  and   smaller representational capacity is suf 
cient  for real datasets 
Increasing the number of examples  keeping representational capacity  xed  also increases the time needed to
memorize the training set 
In the limit  the representational capacity is simply insuf cient  and memorization is
not feasible  On the other hand  when the relationship between inputs and outputs is meaningful  new examples sim 

  Regularization can also increase timeto convergence  see

section  

 numberofSGDsteps logscale Ginicoe cientof gxdistributionrealdata realdatarandomdata numberofSGDsteps logscale Ginicoe cientof gxdistributionrealdatarandomdata   Closer Look at Memorization in Deep Networks

Figure   Time to convergence as   function of capacity with dataset size  xed to    left  or dataset size with capacity  xed to  
units  right   Noise level  denotes to the proportion of training points whose inputs are replaced by Gaussian noise  Because of the
patterns underlying real data  having more capacity data does not decrease increase training time as much as it does for noise data 

ply give more  possibly redundant  clues as to what the input   output mapping is  Thus  in the limit  an idealized
learner should be able to predict unseen examples perfectly 
absent noise  Our experiments demonstrate that timeto 
convergence is not only longer on noise data  as noted by
Zhang et al    but also  increases substantially as  
function of dataset size  relative to real data  Following the
reasoning above  this suggests that our networks are learning to extract patterns in the data  rather than memorizing 

  DNNs Learn Patterns First
This section aims at studying how the complexity of the hypotheses learned by DNNs evolve during training for real
data vs  noise data  To achieve this goal  we build on the
intuition that the number of different decision regions into
which an input space is partitioned re ects the complexity
of the learned hypothesis  Sokolic et al    This notion
is similar in spirit to the degree to which   function can scatter random labels    higher density of decision boundaries
in the data space allows more samples to be scattered 
Therefore  we estimate the complexity by measuring how
densely points on the data manifold are present around the
model   decision boundaries  Intuitively  if we were to randomly sample points from the data distribution    smaller
fraction of points in the proximity of   decision boundary
suggests that the learned hypothesis is simpler 

  Critical Sample Ratio  CSR 

Here we introduce the notion of   critical sample  which
we use to estimate the density of decision boundaries as
discussed above  Critical samples are   subset of   dataset
such that for each such sample    there exists at least one
adversarial example    in the proximity of    Speci cally 
consider   classi cation network   output vector        

              fk      Rk for   given input sample     Rn
from the data manifold  Formally we call   dataset sample
    critical sample if there exists   point    such that 

fi     cid  arg max

 

arg max
      cid        cid     

 

fj   

 

 

   

where   is    xed box size  As in recent work on adversarial examples  Kurakin et al    the above de nition
depends only on the predicted label arg maxi fi    of   
and not the true label  as in earlier work on adversarial examples  such as Szegedy et al    Goodfellow et al 
 
Following the above argument relating complexity to decision boundaries    higher number of critical samples indicates   more complex hypothesis  Thus  we measure complexity as the critical sample ratio  CSR  that is  the fraction of datapoints in   set     for which we can  nd   critical sample   critical samples
To identify whether   given data point   is   critical samples  we search for an adversarial sample    within   box
of radius    To perform this search  we propose using
Langevin dynamics applied to the fast gradient sign method
 FGSM  Goodfellow et al    as shown in algorithm
  We refer to this method as Langevin adversarial sample
search  LASS  While the FGSM search algorithm can get
stuck at   points with zero gradient  LASS explores the box
more thoroughly  Speci cally    problem with  rst order
gradient search methods  like FGSM  is that there might
exist training points where the gradient is   but with   large
 nd derivative corresponding to   large change in prediction
in the neighborhood  The noise added by the LASS algorithm during the search enables escaping from such points 

 In our experiments  we set             and   is

samples from standard normal distribution 

  Closer Look at Memorization in Deep Networks

    Noise added on classi cation inputs 

    Noise added on classi cation labels 

Figure   Accuracy  left in each pair  solid is train  dotted is validation  and Critical sample ratios  right in each pair  for MNIST 

    Noise added on classi cation inputs 

    Noise added on classi cation labels 

Figure   Accuracy  left in each pair  solid is train  dotted is validation  and Critical sample ratios  right in each pair  for CIFAR 

Algorithm   Langevin Adversarial Sample Search  LASS 
Require      Rn         noise process  
Ensure    
  converged   FALSE
                
  while not converged or max iter reached do
          sign   fk   
 
 

           
for         do

 cid  xi       sign xi   xi 

            

 xi  
end for
if arg maxi        cid  arg maxi       then

 xi

if xi   xi     
otherwise

 

 
 
 
 
 
  end while

end if

converged   TRUE
       

  Critical Samples Throughout Training

We now show that the number of critical samples is much
higher for   deep network  speci cally    CNN  trained on
noise data compared with real data  To do so  we mea 

Figure   Critical sample ratio throughout training on CIFAR 
random input  randX  and random label  randY  datasets 

sure the number of critical samples in the validation set 
throughout training  Results are shown in Figure    

  We also measure the number of critical samples in the training sets  Since we train our models using log loss  training points
are pushed away from the decision boundary even after the network learns to classify them correctly  This leads to an initial rise
and then fall of the number of critical samples in the training sets 
 We use   box size of   which is small enough in    
pixel scale to be unnoticeable by   human evaluator  Different
values for   were tested but did not change results qualitatively

  Closer Look at Memorization in Deep Networks

higher number of critical samples for models trained on
noise data compared with those trained on real data suggests that the learned decision surface is more complex for
noise data  randX and randY  We also observe that the
CSR increases gradually with increasing number of epochs
and then stabilizes  This suggests that the networks learn
gradually more complex hypotheses during training for all
three datasets 
In our next experiment  we evaluate the performance and
critical sample ratio of datasets with   to   of the
training data replaced with either input or label noise  Results for MNIST and CIFAR  are shown in Figures  
and   respectively  For both randX and randY datasets 
the CSR is higher for noisier datasets  re ecting the higher
level of complexity of the learned prediction function  The
 nal and maximum validation accuracies are also both
lower for noisier datasets  indicating that the noise examples interfere somewhat with the networks ability to learn
about the real data 
More signi cantly  for randY datasets  Figures     and
    the network achieves maximum accuracy on the validation set before achieving high accuracy on the training
set  Thus the model  rst learns the simple and general patterns of the real data before  tting the noise  which results in decreasing validation accuracy  Furthermore  as
the model moves from  tting real data to  tting noise  the
CSR greatly increases  indicating the need for more complex hypotheses to explain the noise  Combining this result
with our results from Section   we conclude that real
data examples are easier to    than noise 

  Effect of Regularization on Learning
Here we demonstrate the ability of regularization to degrade training performance on data with random labels 
while maintaining generalization performance on real data 
Zhang et al    argue that explicit regularizations are
not the main explanation of good generalization performance  rather SGD based optimization is largely responsible for it  Our  ndings extend their claim and indicate that
explicit regularizations can substantially limit the speed of
memorization of noise data without signi cantly impacting
learning on real data 
We compare the performance of CNNs trained on CIFAR 
  and randY with the following regularizers  dropout
 with dropout rates in range   input dropout  range  
  input Gaussian noise  with standard deviation in range
  hidden Gaussian noise  range   weight decay
 range   and additionally dropout with adversarial training  with weighting factor in range   and dropout in

and lead to the same conclusions

Figure   Effect of different regularizers on train accuracy  on
noise dataset  vs  validation accuracy  on real dataset  Flatter
curves indicate that memorization  on noise  can be capped without sacri cing generalization  on real data 

rate range   We train   separate model for every
combination of dataset  regularization technique  and regularization parameter 
The results are summarized in Figure   For each combination of dataset and regularization technique  the  nal
training accuracy on randY  xaxis  is plotted against the
best validation accuracy on CIFAR  from amongst the
models trained with different regularization parameters  yaxis  Flat curves indicate that the corresponding regularization technique can reduce memorization when applied
on random labeling  while resulting in the same validation accuracy on the clean validation set  Our results show
that different regularizers target memorization behavior to
different extent   dropout being the most effective  We
 nd that dropout  especially coupled with adversarial training  is best at hindering memorization without reducing the
model   ability to learn  Figure   additionally shows this
effect for selected experiments       selected hyperparameter values  in terms of train loss 

  Related Work
Our work builds on the experiments and challenges the interpretations of Zhang et al    We make heavy use
of their methodology of studying DNN training in the context of noise datasets  Zhang et al    show that DNNs
can perfectly    noise and thus that their generalization
ability cannot be explained through traditional statistical
learning theory       see  Vapnik   Vapnik    Bartlett
et al    We agree with this  nding  but show in addition that the degree of memorization and generalization
in DNNs depends not only on the architecture and training

 We perform adversarial training using critical samples found

by LASS algorithm with default parameters 

  Closer Look at Memorization in Deep Networks

the effect of noise samples on learning dynamics has   long
tradition  Bishop    An    we are the  rst to examine relationships between the fraction of noise samples
and other attributes of the learning algorithm  namely  capacity  training time and dataset size 
Multiple techniques for analyzing the training of DNNs
have been proposed before  including looking at generalization error  trajectory length evolution  Raghu et al 
  analyzing Jacobians associated to different layers  Wang  Saxe et al    or the shape of the loss minima found by SGD  Im et al    Chaudhari et al   
Keskar et al    Instead of measuring the sharpness
of the loss for the learned hypothesis  we investigate the
complexity of the learned hypothesis throughout training
and across different datasets and regularizers  as measured
by the critical sample ratio  Critical samples refer to real
datapoints that have adversarial examples  Szegedy et al 
  Goodfellow et al    nearby  Adversarial examples originally referred to imperceptibly perturbed datapoints that are con dently misclassi ed 
 Miyato et al 
  de ne virtual adversarial examples via changes in
the predictive distribution instead  thus extending the de 
nition to unlabeled datapoints  Kurakin et al    recommend using this de nition when training on adversarial
examples  and it is the de nition we use 
Two contemporary works perform indepth explorations of
topics related to our work  Bojanowski   Joulin  
show that predicting random noise targets can yield state
of the art results in unsupervised learning  corroborating
our  ndings in Section   especially Figure   Koh  
Liang   use in uence functions to measure the impact
on parameter changes during training  as in our Section  
They explore several promising applications for this technique  including generation of adversarial training examples 

  Conclusion
Our empirical exploration demonstrates qualitative differences in DNN optimization on noise vs  real data  all of
which support the claim that DNNs trained with SGDvariants  rst use patterns  not brute force memorization  to
   real data  However  since DNNs have the demonstrated
ability to    noise  it is unclear why they  nd generalizable solutions on real data  we believe that the deep learning priors including distributed and hierarchical representations likely play an important role  Our analysis suggests
that memorization and generalization in DNNs depend on
network architecture and optimization procedure  but also
on the data itself  We hope to encourage future research on
how properties of datasets in uence the behavior of deep
learning algorithms  and suggest   datadependent understanding of DNN capacity as   research goal 

Figure   Training curves for different regularization techniques
on random label  left  and real  right  data  The vertical ordering
of the curves is different for random labels than for real data  indicating differences in the propensity of different regularizers to
slowdown memorization 

procedure  including explicit regularizations  but also on
the training data itself  
Another direction we investigate is the relationship between regularization and memorization 
Zhang et al 
  argue that explicit and implicit regularizers  including SGD  might not explain or limit shattering of random
data 
In this work we show that regularizers  especially
dropout  do control the speed at which DNNs memorize 
This is interesting since dropout is also known to prevent
catastrophic forgetting  Goodfellow et al    and thus
in general it seems to help DNNs retain patterns 
  number of arguments support the idea that SGDbased
learning imparts   regularization effect  especially with  
small batch size  Wilson   Martinez    or   small
number of epochs  Hardt et al    Previous work also
suggests that SGD prioritizes the learning of simple hypothesis  rst  Sjoberg et al    showed that  for linear
models  SGD  rst learns models with small  cid  parameter
norm  More generally  the ef cacy of early stopping shows
that SGD  rst learns simpler models  Yao et al    We
extend these results  showing that DNNs trained with SGD
learn patterns before memorizing  even in the presence of
noise examples 
Various previous works have analyzed explanations for the
generalization power of DNNs  Montavon et al    use
kernel methods to analyze the complexity of deep learning architectures  and  nd that network priors       implemented by the network structure of   CNN or MLP  control the speed of learning at each layer  Neyshabur et al 
  note that the number of parameters does not control the effective capacity of   DNN  and that the reason
for DNNs  generalization is unknown  We supplement this
result by showing how the impact of representational capacity changes with varying noise levels  While exploring

 We conclude the latter part based on experimental  ndings in

sections   and  

  Closer Look at Memorization in Deep Networks

ACKNOWLEDGMENTS

We thank Akram Erraqabi  Jason Jo and Ian Goodfellow
for helpful discussions  SJ was supported by Grant No  DI
  from Ministry of Science and Higher Education  Poland  DA was supported by IVADO  CIFAR and
NSERC  EB was  nancially supported by the Samsung Advanced Institute of Technology  SAIT  MSK and SJ were
supported by MILA during the course of this work  We
acknowledge the computing resources provided by ComputeCanada and CalculQuebec  Experiments were carried
out using Theano  Theano Development Team    and
Keras  Chollet et al   

References
An  Guozhong  The effects of adding noise during backpropagation training on   generalization performance 
Neural computation     

Goodfellow  Ian  Bengio  Yoshua  and Courville  Aaron 
Deep Learning  MIT Press    http www 
deeplearningbook org 

Goodfellow  Ian    Mirza  Mehdi  Xiao  Da  Courville 
Aaron  and Bengio  Yoshua  An empirical investigation
of catastrophic forgetting in gradientbased neural networks  arXiv preprint arXiv   

Goodfellow  Ian    Shlens  Jonathon  and Szegedy  Christian  Explaining and harnessing adversarial examples 
arXiv preprint arXiv   

Hardt  Moritz  Recht  Benjamin  and Singer  Yoram  Train
faster  generalize better  Stability of stochastic gradient
descent  arXiv preprint arXiv   

Hornik  Kurt  Stinchcombe  Maxwell  and White  Halbert 
Multilayer feedforward networks are universal approximators  Neural networks     

Bartlett  Peter    Bousquet  Olivier  Mendelson  Shahar 
et al  Local rademacher complexities  The Annals of
Statistics     

Im  Daniel Jiwoong  Tao  Michael  and Branson  Kristin 
An empirical analysis of deep network loss surfaces 
arXiv preprint arXiv   

Bengio  Yoshua et al  Learning deep architectures for ai 
Foundations and trends  in Machine Learning   
   

Bishop  Chris    Training with noise is equivalent to
tikhonov regularization  Neural computation   
   

Bojanowski     and Joulin     Unsupervised Learning by

Predicting Noise  ArXiv eprints  April  

Bottou    on  Online learning and stochastic approximations  Online learning in neural networks   
 

Chaudhari  Pratik  Choromanska  Anna  Soatto  SteEntropy sgd  Biasing
arXiv preprint

fano  and LeCun  Yann 
gradient descent
arXiv   

into wide valleys 

Chollet  Fran ois et al  Keras  https github com 

fchollet keras   

Cybenko  George  Approximation by superpositions of  
sigmoidal function  Mathematics of Control  Signals 
and Systems  MCSS     

Fix  Evelyn and Hodges Jr  Joseph   

Discriminatory analysisnonparametric discrimination  consistency
properties  Technical report  DTIC Document   

Gini  Corrado  Variabilita   mutabilita 
Royal Statistical Society     

Journal of the

Keskar  Nitish Shirish  Mudigere  Dheevatsa  Nocedal 
Jorge  Smelyanskiy  Mikhail  and Tang  Ping Tak Peter  On largebatch training for deep learning  GenarXiv preprint
eralization gap and sharp minima 
arXiv   

Koh  Pang Wei and Liang  Percy  Understanding blackbox predictions via in uence functions  arXiv preprint
arXiv   

Krizhevsky  Alex  Nair  Vinod  and Hinton  Geoffrey 
Cifar   canadian institute for advanced research  URL http www cs toronto edu 
 kriz cifar html 

Kurakin  Alexey  Goodfellow  Ian  and Bengio  Samy  Adversarial examples in the physical world  arXiv preprint
arXiv   

LeCun  Yann  Cortes  Corinna  and Burges  Christopher JC  The mnist database of handwritten digits   

Lin  Henry   and Tegmark  Max  Why does deep
arXiv preprint

and cheap learning work so well 
arXiv   

Maclaurin  Dougal  Duvenaud  David    and Adams 
Ryan    Gradientbased hyperparameter optimization
In ICML  pp   
through reversible learning 
 

Miyato  Takeru  Maeda  Shinichi  Koyama  Masanori 
Nakae  Ken  and Ishii  Shin  Distributional smoothing
with virtual adversarial training  stat     

  Closer Look at Memorization in Deep Networks

Wang  Shengjie  Analysis of deep neural networks with the

extended data jacobian matrix 

Wilson    Randall and Martinez  Tony    The general inef ciency of batch training for gradient descent learning 
Neural Networks     

Yao  Yuan  Rosasco  Lorenzo  and Caponnetto  Andrea  On
early stopping in gradient descent learning  Constructive
Approximation     

Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht  Benjamin  and Vinyals  Oriol  Understanding deep learning
requires rethinking generalization  International Conference on Learning Representations  ICLR   

Montavon  Gr goire  Braun  Mikio    and   ller  KlausRobert  Kernel analysis of deep networks  Journal of
Machine Learning Research     

Montufar  Guido    Pascanu  Razvan  Cho  Kyunghyun 
and Bengio  Yoshua  On the number of linear regions
of deep neural networks 
In Ghahramani     Welling 
   Cortes     Lawrence        and Weinberger       
 eds  Advances in Neural Information Processing Systems   pp    Curran Associates  Inc   

Neyshabur  Behnam  Tomioka  Ryota  and Srebro  Nathan 
In search of the real inductive bias  On the role of implicit regularization in deep learning  arXiv preprint
arXiv   

Poole  Ben  Lahiri  Subhaneil  Raghu  Maithreyi  SohlDickstein  Jascha  and Ganguli  Surya  Exponential
expressivity in deep neural networks through transient
chaos 
In Lee        Sugiyama     Luxburg       
Guyon     and Garnett      eds  Advances in Neural Information Processing Systems   pp    Curran Associates  Inc   

Raghu  Maithra  Poole  Ben  Kleinberg  Jon  Ganguli 
Surya  and SohlDickstein  Jascha  On the expresarXiv preprint
sive power of deep neural networks 
arXiv   

Saxe  Andrew    McClelland  James    and Ganguli 
Surya  Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks  arXiv preprint
arXiv   

Sjoberg     Sjoeberg     Sj berg     and Ljung     Overtraining  regularization and searching for   minimum 
with application to neural networks  International Journal of Control     

Sokolic  Jure  Giryes  Raja  Sapiro  Guillermo  and Rodrigues  Miguel RD  Robust large margin deep neural
networks  arXiv preprint arXiv   

Szegedy  Christian  Zaremba  Wojciech  Sutskever  Ilya 
Bruna  Joan  Erhan  Dumitru  Goodfellow  Ian    and
Fergus  Rob 
Intriguing properties of neural networks 
CoRR  abs    URL http arxiv 
org abs 

Theano Development Team  and others  Theano    Python
framework for fast computation of mathematical expressions  arXiv eprints  abs  May  

Vapnik  Vladimir Naumovich and Vapnik  Vlamimir  Statistical learning theory  volume   Wiley New York 
 

