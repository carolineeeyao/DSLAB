Orthogonalized ALS    Theoretically Principled Tensor Decomposition

Algorithm for Practical Use

Vatsal Sharan   Gregory Valiant  

Abstract

The popular Alternating Least Squares  ALS  algorithm for tensor decomposition is ef cient and
easy to implement  but often converges to poor
local optima particularly when the weights of
the factors are nonuniform  We propose   modi cation of the ALS approach that is as ef 
cient as standard ALS  but provably recovers
the true factors with random initialization under standard incoherence assumptions on the factors of the tensor  We demonstrate the significant practical superiority of our approach over
traditional ALS for   variety of tasks on synthetic data including tensor factorization on exact  noisy and overcomplete tensors  as well as
tensor completion and for computing word embeddings from   thirdorder word trioccurrence
tensor 

  Introduction
From   theoretical perspective  tensor methods have become an incredibly useful and versatile tool for learning
  wide array of popular models  including topic modeling  Anandkumar et al    mixtures of Gaussians  Ge
et al    community detection  Anandkumar et al 
   
learning graphical models with guarantees via
the method of moments  Anandkumar et al      Chaganty   Liang    and reinforcement learning  Azizzadenesheli et al    The key property of tensors that enables these applications is that tensors have  
unique decomposition  decomposition here refers to the
most commonly used CANDECOMP PARAFAC or CP
decomposition  under mild conditions on the factor matrices  Kruskal    for example  tensors have   unique
decomposition whenever the factor matrices are full rank 
As tensor methods naturally model threeway  or higherorder  relationships  it is not too optimistic to hope that

 Stanford University  USA  Correspondence to  Vatsal Sharan

 vsharan stanford edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

their practical utility will only increase  with the rise of
multimodal measurements       measurements taken by
 Internet of Things  devices  and the numerous practical
applications involving high order dependencies  such as
those encountered in natural language processing or genomic settings 
In fact  we are already seeing exciting
applications of tensor methods for analysis of highorder
spatiotemporal data  Yu   Liu    health data analysis
 Wang et al      and bioinformatics  Colombo   Vlassis    Nevertheless  to truly realize the practical impact that the current theory of tensor methods portends  we
require better algorithms for computing decompositions 
practically ef cient algorithms that are both capable of
scaling to large  and possibly sparse  tensors  and are robust to noise and deviations from the idealized  lowrank 
assumptions 
As tensor decomposition is NPHard in the worstcase
 Hillar   Lim      astad    one cannot hope for
algorithms which always produce the correct factorization 
Despite this worstcase impossibility  accurate decompositions can be ef ciently computed in many practical settings  Early work from the     Leurgans et al   
Harshman    established   simple algorithm for computing the tensor decomposition  in the noiseless setting 
provided that the factor matrices are full rank  This approach  based on an eigendecomposition  is very sensitive
to noise in the tensor  as we also show in our experiments 
and does not scale well for large  sparse tensors 
Since this early work  much progress has been made  Nevertheless  many of the tensor decomposition algorithms
hitherto proposed and employed have strong provable success guarantees but are computationally expensive  though
still polynomial time either requiring an expensive initialization phase  being unable to leverage the sparsity of
the input tensor  or not being ef ciently parallelizable  On
the other hand  there are also approaches which are ef 
cient to implement  but which fail to compute an accurate
decomposition in many natural settings  The Alternating
Least Squares  ALS  algorithm  either with random initialization or more complicated initializations  falls in this latter category and is  by far  the most widely employed decomposition algorithm despite its often poor performance

Orthgonalized ALS for Tensor Decomposition

and propensity for getting stuck in local optima  which we
demonstrate on both synthetic data and real NLP data 
In this paper we propose an alternative decomposition algorithm   Orthogonalized Alternating Least Squares   OrthALS  which has strong theoretical guarantees  and seems to
signi cantly outperform the most commonly used existing
approaches in practice on both real and synthetic data  for  
number of tasks related to tensor decomposition  This algorithm is   simple modi cation of the ALS algorithm to periodically  orthogonalize  the estimates of the factors  Intuitively  this periodic orthogonalization prevents multiple
recovered factors from  chasing after  the same true factors  allowing for the avoidance of local optima and more
rapid convergence to the true factors 
From the practical side  our algorithm enjoys all the bene 
 ts of standard ALS  namely simplicity and computational
ef ciency scalability  particularly for very large yet sparse
tensors  and noise robustness  Additionally  the speed of
convergence and quality of the recovered factors are substantially better than standard ALS  even when ALS is initialized using the more expensive SVD initialization  As
we show  on synthetic lowrank tensors  our algorithm consistently recovers the true factors  while standard ALS often falters in local optima and fails both in recovering the
true factors and in recovering an accurate lowrank approximation to the original tensor  We also applied OrthALS to
  large  tensor of word cooccurrences to compute  word
embeddings  The embedding produced by our OrthALS
algorithm is signi cantly better than that produced by standard ALS  as we quantify via   near   better performance of the resulting word embeddings across standard
NLP datasets that test the ability of the embeddings to answer basic analogy tasks        puppy is to dog as kitten
  and semantic wordsimilarity tasks  Together 
is to
these results support our optimism that with better decomposition algorithms  tensor methods will become an indispensable  widelyused data analysis tool in the near future 
Beyond the practical bene ts of OrthALS  we also consider its theoretical properties  We show that OrthALS
provably recovers all factors under random initialization
for worstcase tensors as long as the tensor satis es an incoherence property  which translates to the factors of the
tensors having small correlation with each other  which is
satis ed by random tensors with rank          where
  is the dimension of the tensor  This requirement that    
     is signi cantly worse than the best known provable recovery guarantees for polynomialtime algorithms

 Word embeddings are vector representations of words  which
can then be used as features for higherlevel machine learning 
Word embeddings have rapidly become the backbone of many
downstream natural language processing tasks  see       Mikolov
et al     

on random tensors the recent work Ma et al    succeeds even in the overcomplete setting with         
Nevertheless  our experiments support our belief that this
shortcoming is more   property of our analysis than the algorithm itself  Additionally  for many practical settings 
particularly natural language tasks  the rank of the recovered tensor is typically signi cantly sublinear in the dimensionality of the space  and the bene ts of an extremely ef 
 cient and simple algorithm might outweigh limitations on
the required rank for provable recovery 
Finally  as   consequence of our analysis technique for
proving convergence of OrthALS  we also improve the
known guarantees for another popular tensor decomposition algorithm the tensor power method  We show that
the tensor power method with random initialization converges to one of the factors with small residual error for
rank          where   is the dimension  We also show that
the convergence rate is quadratic in the dimension  Anandkumar et al      had previously shown local convergence of the tensor power method with   linear convergence rate  and also showed global convergence via   SVDbased initialization scheme  obtaining the  rst guarantees
for the tensor power method in nonorthogonal settings 
Our new results  particularly global convergence from random initialization  provide some deeper insights into the
behavior of this popular algorithm 
The rest of the paper is organized as follows Section  
states the notation  In Section   we discuss related work 
Section   introduces OrthALS  and states the convergence
guarantees  We state our convergence results for the tensor
power method in Section   The experimental results  on
both synthetic data and the NLP tasks are discussed in Section   Proof details have been deferred to the Appendix 
  Notation
We state our algorithm and results for  rd order tensors 
and believe the algorithm and analysis techniques should
extend easily to higher dimensions  Given    rd order tensor     Rd     our task is to decompose the tensor into its
     wiAi   Bi   Ci 
where Ai denotes the ith column of   matrix    Here
wi      Ai  Bi  Ci   Rd and   denotes the tensor prodif           Rd then             Rd     and
uct 
           ijk   aibjck  We will refer to wi as the weight
of the factor  Ai  Bi  Ci  This is also known as CP decomposition  We refer to the dimension of the tensor by  
and denote its rank by    We refer to different dimensions
of   tensor as the modes of the tensor 
We denote      as the mode   matricization of the tensor 
which is the  attening of the tensor along the nth direction
obtained by stacking all the matrix slices together  For example    denotes  attening of   tensor     Rn     to

factor matrices      and       cid 

Orthgonalized ALS for Tensor Decomposition

also de ne                cid 

       mp  matrix  We denote the KhatriRao product of
two matrices   and   as     cid         Ai   Bi  where
 Ai   Bi  denotes the  attening of the matrix Ai   Bi
into   row vector  For any tensor   and vectors          we
      Tijkaibjck  Throughout 
we say                 if                up to polylogarithmic factors 
Though all algorithms in the paper extend to asymmetric
tensors  we prove convergence results under the symmetric setting where            Similar to other works
 Tang   Shah    Anandkumar et al      Ma et al 
  our guarantees depend on the incoherence of the
factor matrices  cmax  de ned to be the maximum correlation in absolute value between any two factors      
  Aj  This serves as   natural assumpcmax   maxi cid    AT
tion to simplify the problem as it is NPHard in the worst
 
case  Also  tensors with randomly drawn factors satisfy
cmax      
  Background and Related Work
We begin the section with   brief discussion of related work
on tensor decomposition  We then review the ALS algorithm and the tensor power method and discuss their basic
properties  Our proposed tensor decomposition algorithm 
OrthALS  builds on these algorithms 
  Related Work on Tensor Decomposition

   and our results hold for such tensors 

Though it is not possible for us to do justice to the substantial body of work on tensor decomposition  we will review
three families of algorithms which are distinct from alternating minimization approaches such as ALS and the tensor power method  Many algorithms have been proposed
for guaranteed decomposition of orthogonal tensors  we
refer the reader to Anandkumar et al      Kolda  
Mayo   Comon et al    Zhang   Golub  
However  obtaining guaranteed recovery of nonorthogonal
tensors using algorithms for orthogonal tensors requires
converting the tensor into an orthogonal form  known as
whitening  which is ill conditioned in high dimensions  Le
et al    Souloumiac    and is computationally
the most expensive step  Huang et al    Another
very interesting line of work on tensor decompositions is
to use simultaneous diagonalization and higher order SVD
 Colombo   Vlassis    Kuleshov et al    De Lathauwer    but these are not as computationally ef cient
as alternating minimization  Recently  there has been in 

 De Lathauwer   prove unique recovery under very general conditions  but their algorithm is quite complex and requires
solving   linear system of size      which is prohibitive for
large tensors  We ran the simultaneous diagonalization algorithm
of Kuleshov et al    on   dimension   rank   tensor  and
the algorithm needed around   minutes to run  whereas OrthALS converges in less than   seconds 

triguing work on provably decomposing random tensors
using the sumof squares approach  Ma et al    Hopkins et al    Tang   Shah    Ge   Ma   
Ma et al    show that   sumof squares based relaxation can decompose highly overcomplete random tensors
of rank up to      Though these results establish the
polynomial learnability of the problem  they are unfortunately not practical 
Very recently  there has been exciting work on scalable tensor decomposition algorithms using ideas such as sketching  Song et al    Wang et al      and contraction
of tensor problems to matrix problems  Shah et al   
Also worth noting are recent approaches to speedup ALS
via sampling and randomized least squares  Battaglino
et al    Cheng et al    Papalexakis et al   
  Alternating Least Squares  ALS 

ALS is the most widely used algorithm for tensor decomposition and has been described as the  workhorse  for tensor
decomposition  Kolda   Bader    The algorithm is
conceptually very simple  if the goal is to recover   rankk
tensor  ALS maintains   rankk decomposition speci ed by
three sets of       dimensional matrices               corresponding to the three modes of the tensor  ALS will iteratively    two of the three modes  say    and     and
then update    by solving   leastsquared regression problem to  nd the best approximation to the underlying tensor
  having factors    and    in the  rst two modes  namely
 Cnew   arg minC cid   cid                cid cid  ALS will then continue to iteratively    two of the three modes  and update
the other mode via solving the associated leastsquares regression problem  These updates continue until some stopping condition is satis ed typically when the squared error of the approximation is no longer decreasing  or when  
 xed number of iterations have elapsed  The factors used in
ALS are either chosen uniformly at random  or via   more
expensive initialization scheme such as SVD based initialization  Anandkumar et al     
In the SVD based
scheme  the factors are initialized to be the singular vectors of   random projection of the tensor onto   matrix 
The main advantages of the ALS approach  which have led
to its widespread use in practice are its conceptual simplicity  noise robustness and computational ef ciency given its
graceful handling of sparse tensors and ease of parallelization  There are several publicly available optimized packages implementing ALS  such as Kossai  et al   
Vervliet et al  Bader et al    Bader   Kolda  
Smith   Karypis  Huang et al    Kang et al   
Despite the advantages  ALS does not have any global
convergence guarantees and can get stuck in local optima
 Comon et al    Kolda   Bader    even under
very realistic settings  For example  consider   setting
where the weights wi for the factors  Ai  Bi  Ci  decay

Orthgonalized ALS for Tensor Decomposition

according to   powerlaw  hence the  rst few factors have
much larger weight than the others  As we show in the experiments  see Fig    ALS fails to recover the lowweight
factors  Intuitively  this is because multiple recovered factors will be chasing after the same high weight factor  leading to   bad local optima 
  Tensor Power Method

The tensor power method is   special case of ALS that only
computes   rank  approximation  The procedure is then
repeated multiple times to recover different factors  The
factors recovered in different iterations of the algorithm are
then clustered to determine the set of unique factors  Different initialization strategies have been proposed for the
tensor power method  Anandkumar et al      showed
that the tensor power method converges locally      
for
  suitably chosen initialization  for random tensors with
rank      They also showed that   SVD based initialization strategy gives good starting points and used this
to prove global convergence for random tensors with rank
     However  the SVD based initialization strategy can
be computationally expensive  and our experiments suggest
that even SVD initialization fails in the setting where the
weights decay according to   powerlaw  see Fig   
In this work  we prove global convergence guarantees with
random initializations for the tensor power method for random and worstcase incoherent tensors  Our results also
demonstrate how  with random initialization  the tensor
power method converges to the factor having the largest
product of weight times the correlation of the factor with
the random initialization vector  This explains the dif culty
of using random initialization to recover factors with small
weight  For example  if one factor has weight less than  
   fraction of the weight of  say  the heaviest    factors 
then with high probability we would require at least     
random initializations to recover this factor  This is because
the correlation between random vectors in high dimensions
is approximately distributed as   Normal random variable
and if    samples are drawn from the standard Normal
distribution  the probability that one particular sample is at
least   factor of   larger than the other    other samples
scales as roughly     
  The Algorithm  Orthogonalized

Alternating Least Squares  OrthALS 

In this section we introduce OrthALS  which combines
the computational bene ts of standard ALS and the provable recovery of the tensor power method  while avoiding
the dif culties faced by both when factors have different
weights  OrthALS is   simple modi cation of standard
ALS that adds an orthogonalization step before each set of
ALS steps  We describe the algorithm below  Note that
steps   are just the solution to the least squares problem

expressed in compact tensor notation  for instance step  
can be equivalently stated as     arg minC cid   cid         
        cid cid  Similarly  step   is the least squares estimate of
the weight wi of each rank  component  Ai    Bi    Ci 
Algorithm   Orthogonalized ALS  OrthALS 
Input  Tensor     Rd      number of iterations   
  Initialize each column of        and      Rd   uni 

formly from the unit sphere

  for           do
Find QR decomposition of     set         Orthogo 
 
nalize    and    analogously 
             cid     
           cid     
 
           cid     
 
Normalize         and store results in           
 
  end for
  Estimate weights  wi        Ai   Bi   Ci         
  return               

To get some intuition for why the orthogonalization makes
sense  let us consider the more intuitive matrix factorization
problem  where the goal is to compute the eigenvectors of
  matrix  Subspace iteration is   straightforward extension
of the matrix power method to recover all eigenvectors at
once  In subspace iteration  the matrix of eigenvector estimates is orthogonalized before each power method step
 by projecting the second eigenvector estimate orthogonal
to the  rst one and so on  because otherwise all the vectors
would converge to the dominant eigenvector  For the case
of tensors  the vectors would not all necessarily converge
to the dominant factor if the initialization is good  but with
high probability   random initialization would drive many
factors towards the larger weight factors  The orthogonalization step is   natural modi cation which forces the estimates to converge to different factors  even if some factors
are much larger than the others  It is worth stressing that
the orthogonalization step does not force the  nal recovered factors to be orthogonal  because the ALS step follows
the orthogonalization step  and in general the factors output will not be orthogonal  which is essential for accurately
recovering the factors 
From   computational perspective  adding the orthogonalization step does not add to the computational cost as the
least squares updates in step   of Algorithm   involve an
extra pseudoinverse term for standard ALS  which evaluates to identity for OrthALS and does not have to be computed  The cost of orthogonalization is        while the
cost of computing the pseudoinverse is also        We
also observe signi cant speedups in terms of the number of
iterations required for convergence for OrthALS as compared to standard ALS in our simulations on random tensors  see the experiments in Section  

Orthgonalized ALS for Tensor Decomposition

Variants of Orthogonalized ALS  Several other modi 
cations to the simple orthogonalization step also seem natural  Particularly for lowdimensional settings  in practice
we found that it is useful to carry out orthogonalization
for   few steps and then continue with standard ALS updates until convergence  we call this variant HybridALS 
HybridALS also gracefully reverts to standard ALS in settings where the factors are highly correlated and orthogonalization is not helpful 
  Performance Guarantees

 cid  
   wiAi   Ai   Ai  Let cmax   maxi cid    AT

We now state the formal guarantees on the performance of
Orthogonalized ALS  The speci   variant of Orthogonalized ALS that our theorems apply to is   slight modi cation
of Algorithm   and differs in that there is   periodic  every
log   steps  rerandomization of the factors for which our
analysis has not yet guaranteed convergence  In our practical implementations  we observe that all factors seem to
converge within this  rst log   steps  and hence the subsequent rerandomization is unnecessary 
Theorem   Consider   ddimensional rank   tensor    
  Aj  be
the incoherence between the true factors and     wmax
wmin
be the ratio of the largest and smallest weight  Assume
 cmax        and the estimates of the factors are initialized randomly from the unit sphere  Provided that  at
the   log     log log   th step of the algorithm the estimates for all but the  rst   factors are rerandomized  then
with high probability the orthogonalized ALS updates converge to the true factors in     log     log log    steps 
and the error at convergence satis es  up to relabelling 
      max   
 cid  Ai    Ai  cid 
   
  max cmax      for all   

max      and      wi

wi

Theorem   immediately gives convergence guarantees for
 
random low rank tensors  For random   dimensional tensors  cmax     
   therefore OrthALS converges
globally with random initialization whenever         
If the tensor has rank much smaller than the dimension 
then our analysis can tolerate signi cantly higher correlation between the factors  In the Appendix  we also prove
Theorem   for the special and easy case of orthogonal tensors  which nevertheless highlights the key proof ideas 
  New Guarantees for the Tensor Power Method

As   consequence of our analysis of the orthogonalized
ALS algorithm  we also prove new guarantees on the tensor power method  As these may be of independent interest
because of the wide use of the tensor power method  we
summarize them in this section  We show   quadratic rate
of convergence  in   log log    steps  with random initialization for random tensors having rank          This
contrasts with the analysis of Anandkumar et al     
who showed   linear rate of convergence    log    steps 

for random tensors  provided an SVD based initialization
 cid  
is employed 
Theorem   Consider   ddimensional rank   tensor    
   wiAi   Ai   Ai with the factors Ai sampled uniformly from the ddimensional sphere  De ne     wmax
wmin
to be the ratio of the largest and smallest weight  Assume          and     polylog    If the initialization
     Rd is chosen uniformly from the unit sphere  then with
high probability the tensor power method updates converge
to one of the true factors  say    in   log log    steps 
and the error at convergence satis es  cid            cid   
 
 
   
   Also  the estimate of the weight     satis es
       
       

  

  

 cid  
   wiAi   Ai   Ai  Let cmax   maxi cid    AT

Theorem   provides guarantees for random tensors  but it
is natural to ask if there are deterministic conditions on the
tensors which guarantee global convergence of the tensor
power method  Our analysis also allows us to obtain   clean
characterization for global convergence of the tensor power
method updates for worstcase tensors in terms of the incoherence of the factor matrix 
Theorem   Consider   ddimensional rank   tensor    
  Aj 
be the ratio of the largest and smalland     wmax
wmin
est weight  and assume  cmax       
If the initialization      Rd is chosen uniformly from the unit
sphere  then with high probability the tensor power method
updates converge to one of the true factors  say    in
  log     log log    steps  and the error at convergence
max      and
satis es  cid            cid 
       

      max cmax     

        max   

  

  Experiments
We compare the performance of OrthALS  standard ALS
 with random and SVD initialization  the tensor power
method  and the classical eigendecomposition approach 
through experiments on low rank tensor recovery in   few
different parameter regimes  on   overcomplete tensor decomposition task and   tensor completion task  We also
compare the factorization of OrthALS and standard ALS
on   large realworld tensor of word trioccurrence based
on the   billion word English Wikipedia corpus 
  Experiments on Random Tensors
Recovering low rank tensors  We explore the abilities
of OrthALS  standard ALS  and the tensor power method
 TPM  to recover   low rank  rank    tensor that has been
constructed by independently drawing each of the   factors
independently and uniformly at random from the   dimensional unit spherical shell  We consider several different

 MATLAB  Python and   code for OrthALS and Hybridhttp web stanford edu 

is

ALS
 vsharan orthals html

available

at

Orthgonalized ALS for Tensor Decomposition

combinations of the dimension     and rank     We also
consider both the setting where all of the factors are equally
weighted  as well as the practically relevant setting where
the factor weights decay geometrically  and consider the
setting where independent Gaussian noise has been added
to the lowrank tensor 
In addition to random initialization for standard ALS and
the TPM  we also explore SVD based initialization  Anandkumar et al      where the factors are initialized via
SVD of   projection of the tensor onto   matrix  We also
test the classical technique for tensor decomposition via simultaneous diagonalization  Leurgans et al    Harshman     also known as Jennrich   algorithm  we refer
to it as SimDiag  which  rst performs two random projections of the tensor  and then recovers the factors by an
eigenvalue decomposition of the projected matrices  This
gives guaranteed recovery when the tensors are noiseless
and factors are linearly independent  but is extremely unstable to perturbations 
We evaluate the performance in two respects    the ability
of the algorithms to recover   lowrank tensor that is close
to the input tensor  and   the ability of the algorithms to
recover accurate approximations of many of the true factors  Fig    depicts the performance via the  rst metric 
We evaluate the performance in terms of the discrepancy
between the input lowrank tensor  and the lowrank tensor
recovered by the algorithms  quanti ed via the ratio of the
Frobenius norm of the residual  to the Frobenius norm of
the actual tensor   cid       cid  
  where    is the recovered tensor 
 cid   cid  
Since the true tensor has rank    the inability of an algorithm to drive this error to zero indicates the presence of
local optima  Fig    depicts the performance of OrthALS 
standard ALS with random initialization and the hybrid algorithm that performs OrthALS for the  rst  ve iterations
before reverting to standard ALS  HybridALS  Tests are
conducted in both the setting where factor weights are uniform  as well as   geometric spacing  where the ratio of the
largest factor weight to the smallest is   Fig    shows
that Hybrid ALS and OrthALS have much faster convergence and  nd   signi cantly better    than standard ALS 
Fig    quanti es the performance of the algorithms in terms
of the number of the original factors that the algorithms
accurately recover  We use standard ALS  OrthALS  Algorithm   HybridALS  TPM with random initialization
 TPM  ALS with SVD initialization  ALSSVD  TPM
with SVD initialization  TPMSVD  and the simultaneous
diagonalization approach  SimDiag  We run TPM and
SVDTPM with   different initializations and  nd   rank
      decomposition for ALS  ALSSVD  OrthALS 
HybridALS and SimDiag  We repeat the experiment  by
sampling   new tensor    times  We perform this evaluation in both the setting where we receive an actual low 

rank tensor as input  as well as the setting where each entry Tijk of the lowrank tensor has been perturbed by independent Gaussian noise of standard deviation equal to
 Tijk  We can see that OrthALS and HybridALS perform signi cantly better than the other algorithms and are
able to recover all factors in the noiseless case even when
the weights are highly skewed  Note that the reason the
HybridALS and OrthALS fail to recover all factors in the
noisy case when the weights are highly skewed is that the
magnitude of the noise essentially swamps the contribution
from the smallest weight factors 
Recovering overcomplete tensors  Overcomplete tensors are tensors with rank higher than the dimension  and
have found numerous theoretical applications in learning
latent variable models  Anandkumar et al    Even
though orthogonalization cannot be directly applied to the
setting where the rank is more than the dimension  as the
factors can no longer be orthogonalized  we explore   de 
 ation based approach in this setting  Given   tensor   with
dimension       and rank        we  nd   rank   decomposition    of     subtract    from     and then compute  
rank   decomposition of    to recover the next set of   factors  We repeat this process to recover subsequent factors 
After every set of   factors has been estimated  we also re 
 ne the factor estimates of all factors estimated so far by
running an additional ALS step using the current estimates
of the extracted factors as the initialization  Fig     plots
the number of factors recovered when this de ation based
approach is applied to   dimension       tensor with  
mild power low distribution on weights  We can see that
HybridALS is successful at recovering tensors even in the
overcomplete setup  and gives an improvement over ALS 
Tensor completion  We also test the utility of orthogonalization on   tensor completion task  where the goal is to
recover   large missing fraction of the entries  Fig     suggests HybridALS gives considerable improvements over
standard ALS  Further examining the utility of orthogonalization in this important setting  in theory and practice 
would be an interesting direction 
  Learning Word Embeddings via Tensor

Factorization

  word embedding is   vector representation of words
which preserves some of the syntactic and semantic relationships in the language  Current methods for learning
word embeddings implicitly  Mikolov et al      Levy
  Goldberg    or explicitly  Pennington et al   
factorize some matrix derived from the matrix of word cooccurrences    where Mij denotes how often word   appears with word    We explore tensor methods for learning
word embeddings  and contrast the performance of standard ALS and Orthogonalized ALS on standard tasks 

Orthgonalized ALS for Tensor Decomposition

               

uniform weights

               

wmax
wmin

   

               

uniform weights

               

wmax
wmin

   

Figure   Plot of the normalized discrepancy between the recovered rank   tensor    and the true tensor      cid       cid  
  as   function of the
 cid   cid  
iteration  In all settings  the OrthALS and the hybrid algorithm drive this discrepancy nearly to zero  with the performance of OrthALS
improving for the higher dimensional cases  whereas standard ALS algorithm has slower convergence and gets stuck in bad local optima 

    Noiseless case  ratio of weights equals wmax
wmin

    Noisy case  ratio of weights equals wmax
wmin

  the ratio of the maximum factor
Figure   Average number of factors recovered by different algorithms for different values of wmax
wmin
weight to minimum factor weight  with the weights spaced geometrically  along with error bars for the standard deviation in the number
of factors recovered  across independent trials  The true rank       and the dimension       We say   factor  Ai  Bi  Ci  of the
tensor   is successfully recovered if there exists at least one recovered factor    Aj   Bj   Cj  with correlation at least   in all modes 
OrthALS and HybridALS recover all factors in almost all settings  whereas ALS and the tensor power method struggle when the
weights are skewed  even with the more expensive SVD based initialization 

    No  of factors recovered for   dimension      
tensor with varying rank  The weights of the factors
are geometrically spaced and the ratio of the weights
between every pair of consecutive factors is  

    Average error on the missing entries for tensor
completion where each entry is sampled with probability    on     different runs with each setting of   
The tensor has dimension       and rank      

Figure   Experiments on overcomplete tensors and tensor completion  Even though our theoretical guarantees do not apply to these
settings  we see that orthogonalization leads to signi cantly better performance over standard ALS 

 IterationNormalized Error  Hybrid ALSOrth ALSALS IterationNormalized Error  Hybrid ALSOrth ALSALS IterationNormalized Error  Hybrid ALSOrth ALSALS IterationNormalized Error  Hybrid ALSOrth ALSALS Ratio of weightsNo of factors recovered  Hybrid ALS Orth ALS Sim DiagALS SVDALSTPM SVDTPM Ratio of weightsNo of factors recovered  Hybrid ALS Orth ALSALS SVDALSTPM SVDTPMSim Diag RankNo  of factors recovered  RankALSHybrid ALS Sampling probability    Normalized MSEALSHybridALSOrthgonalized ALS for Tensor Decomposition

Methodology  We used the English Wikipedia as our corpus  with   billion words  We constructed   word cooccurrence tensor   of the   most frequent words 
where the entry Tijk denotes the number of times the words
     and   appear in   sliding window of length   across the
corpus  We consider two different window lengths       
and       Before factoring the tensor  we apply the
nonlinear elementwise scaling         log       to the
tensor  This scaling is known to perform well in practice
for cooccurrence matrices  Pennington et al    and
makes some intuitive sense in light of the Zip an distribution of word frequencies  Following the application of this
elementwise nonlinearity  we recover   rank   approximation of the tensor using OrthALS or ALS 
We concatenate the  three  recovered factor matrices into
one matrix and normalize the rows  The ith row of this
matrix is then the embedding for the ith word  We test the
quality of these embeddings on two tasks aimed at measuring the syntactic and semantic structure captured by these
word embeddings 
We also evaluated the performance of matrix SVD based
methods on the task  For this  we built the cooccurrence
matrix   with   sliding window of length   over the corpus  We applied the same nonlinear elementwise scaling
and performed   rank   SVD  and set the word embeddings to be the singular vectors after row normalization 
It is worth highlighting some implementation details for
our experiments  as they indicate the practical ef ciency
and scalability inherited by OrthALS from standard ALS 
Our experiments were run on   cluster with   cores and
  GB of RAM memory per core  Most of the runtime
was spent in reading the tensor  the runtime for OrthALS
was around   minutes  with   minutes spent in reading
the tensor  the runtime for standard ALS was around  
minutes because it took longer to converge  Since storing
  dense representation of the   tensor is too expensive  we use an optimized ALS solver for
sparse tensors  Smith   Karypis    which also has an
ef cient parallel implementation 

Evaluation  Similarity and Analogy Tasks  We evaluated the quality of the recovered word embeddings produced by the various methods via their performance on
two different NLP tasks for which standard  humanlabeled
data exists  estimating the similarity between   pair of
words  and completing word analogies 
The word similarity tasks  Bruni et al    Finkelstein
et al    contain word pairs along with human assigned
similarity scores  and the objective is to maximize the correlation between the similarity in the embeddings of the
two words  according to   similarity metric such as the dot
product  and human judged similarity 

Algorithm

Similarity tasks Analogy tasks

Standard ALS       
Standard ALS       

OrthALS       
OrthALS       

Matrix methods       
Matrix methods       

 
 
 
 
 
 

 
 
 
 
 
 

Table   Results for word analogy and word similarity tasks for
different window lengths   over which the cooccurrences are
counted  The embeddings recovered by OrthALS are significantly better than those recovered by standard ALS  Despite
this  embeddings derived from word cooccurrences using matrix SVD still outperform the tensor embeddings  and we are unsure whether this is due to the relative sparsity of the tensor  suboptimal elementwise scaling       the         log    function
applied to the counts  or something more fundamental 

The word analogy tasks  Mikolov et al        present
questions of the form    is to    as   is to
         Paris
  We  nd the answer to   
is to France as Rome is to
is to    as   is to    by  nding the word whose embedding
is the closest to wa    wa   wb in cosine similarity  where
wa denotes the embedding of the word   

Results  The performances are summarized in the Table
  The use of OrthALS rather than standard ALS leads to
signi cant improvement in the quality of the embeddings
as judged by the similarity and analogy tasks  However 
the matrix SVD method still outperforms the tensor based
methods  We believe that it is possible that better tensor
based approaches       using better renormalization  additional data  or some other tensor rather than the symmetric trioccurrence tensor  or   combination of tensor and
matrix based methods can actually improve the quality of
word embeddings  and is an interesting research direction 
Alternatively  it is possible that natural language does not
contain suf ciently rich higherorder dependencies among
words that appear close together  beyond the cooccurrence
structure  to truly leverage the power of tensor methods 
Or  perhaps  the two tasks we evaluated on similarity and
analogy tasks do not require this higher order 
In any
case  investigating these possibilities seems worthwhile 
  Conclusion
Our results suggest the theoretical and practical bene ts of
Orthogonalized ALS  versus standard ALS  An interesting
direction for future work would be to more thoroughly examine the practical and theoretical utility of orthogonalization for other tensorrelated tasks  such as tensor completion  Additionally  its seems worthwhile to investigate
Orthogonalized ALS or Hybrid ALS in more applicationspeci   domains  such as natural language processing 

Orthgonalized ALS for Tensor Decomposition

References
Anandkumar  Animashree  Liu  Yikai  Hsu  Daniel    Foster  Dean    and Kakade  Sham      spectral algorithm
for latent dirichlet allocation  In Advances in Neural Information Processing Systems  pp     

Anandkumar  Animashree  Ge  Rong  Hsu  Daniel  and
Kakade  Sham      tensor approach to learning mixed
membership community models  The Journal of Machine Learning Research       

Anandkumar  Animashree  Ge  Rong  Hsu  Daniel 
Kakade  Sham    and Telgarsky  Matus  Tensor decompositions for learning latent variable models  Journal of
Machine Learning Research       

Anandkumar  Animashree  Ge  Rong  and Janzamin  Matensor decomposiarXiv preprint

jid 
tion via alternating rank  updates 
arXiv     

Guaranteed nonorthogonal

Anandkumar  Animashree  Ge  Rong  and Janzamin  MaLearning overcomplete latent variable models
In Proceedings of The  th

jid 
through tensor methods 
Conference on Learning Theory  pp     

Azizzadenesheli  Kamyar  Lazaric  Alessandro 

and
Anandkumar  Animashree  Reinforcement learning of
POMDPs using spectral methods  In  th Annual Conference on Learning Theory  pp     

Bader  Brett    and Kolda  Tamara    Ef cient MATLAB
computations with sparse and factored tensors  SIAM
Journal on Scienti   Computing    December  

Bader  Brett    Kolda  Tamara    et al  Matlab tensor

toolbox version   Available online  January  

Battaglino  Casey  Ballard  Grey  and Kolda  Tamara     
practical randomized CP tensor decomposition  arXiv
preprint arXiv   

Bruni  Elia  Boleda  Gemma  Baroni  Marco  and Tran 
NamKhanh  Distributional semantics in technicolor  In
Proceedings of the  th Annual Meeting of the Association for Computational Linguistics   

Chaganty  Arun Tejasvi and Liang  Percy  Estimating
latentvariable graphical models using moments and
likelihoods  In ICML  pp     

Cheng  Dehua  Peng  Richard  Liu  Yan  and Perros 
Ioakeim  SPALS  Fast alternating least squares via implicit leverage scores sampling  In Advances In Neural
Information Processing Systems  pp     

Colombo  Nicolo and Vlassis  Nikos  FastMotif  spectral
sequence motif discovery  Bioinformatics     

Colombo  Nicolo and Vlassis  Nikos  Tensor decomposition via joint matrix schur decomposition  In Proceedings of The  rd International Conference on Machine
Learning  pp     

Comon  Pierre  Luciani  Xavier 

and De Almeida 
Andr   LF  Tensor decompositions  alternating least
squares and other tales  Journal of chemometrics   
   

De Lathauwer  Lieven    link between the canonical decomposition in multilinear algebra and simultaneous matrix diagonalization  SIAM journal on Matrix Analysis
and Applications     

Duembgen  Lutz  Bounding standard gaussian tail proba 

bilities  arXiv preprint arXiv   

Finkelstein  Lev  Gabrilovich  Evgeniy  Matias  Yossi 
Rivlin  Ehud  Solan  Zach  Wolfman  Gadi  and Ruppin 
Eytan  Placing search in context  The concept revisited 
In Proceedings of the  th international conference on
World Wide Web  pp    ACM   

Ge  Rong and Ma  Tengyu  Decomposing overcomplete
 rd order tensors using sumof squares algorithms  Approximation  Randomization  and Combinatorial Optimization  Algorithms and Techniques  pp     

Ge  Rong  Huang  Qingqing  and Kakade  Sham    Learning mixtures of gaussians in high dimensions  In Proceedings of the FortySeventh Annual ACM on Symposium on Theory of Computing  pp    ACM   

Harshman  Richard    Foundations of the parafac procedure  Models and conditions for an  explanatory  multimodal factor analysis   

  astad  Johan  Tensor rank is NPComplete  Journal of

Algorithms     

Hillar  Christopher   and Lim  LekHeng  Most tensor
problems are NPHard  Journal of the ACM     

Hopkins  Samuel    Schramm  Tselil  Shi  Jonathan  and
Fast spectral algorithms from sumSteurer  David 
tensor decomposition and planted
ofsquares proofs 
sparse vectors  In Proceedings of the  th Annual ACM
SIGACT Symposium on Theory of Computing   

Huang  Furong  Niranjan  UN  Hakeem  Mohammad Umar  and Anandkumar  Animashree  Fast detection of overlapping communities via online tensor methods  arXiv preprint arXiv   

Huang  Furong  Matusevych  Sergiy  Anandkumar  Anima 
Karampatziakis  Nikos  and Mineiro  Paul  Distributed
latent dirichlet allocation via tensor factorization 
In
NIPS Optimization Workshop   

Orthgonalized ALS for Tensor Decomposition

Kang     Papalexakis  Evangelos  Harpale  Abhay  and
Faloutsos  Christos  Gigatensor  scaling tensor analysis
up by   timesalgorithms and discoveries  In Proceedings of the  th ACM SIGKDD international conference
on Knowledge discovery and data mining  ACM   

Papalexakis  Evangelos    Faloutsos  Christos 

and
Sidiropoulos  Nicholas    Parcube  Sparse parallelizable tensor decompositions  In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases  pp    Springer   

Kolda  Tamara   and Bader  Brett    Tensor decomposi 

tions and applications  SIAM review     

Kolda  Tamara   and Mayo  Jackson    Shifted power
method for computing tensor eigenpairs  SIAM Journal
on Matrix Analysis and Applications     

Kossai  Jean  Panagakis  Yannis  and Pantic  Maja  TenarXiv preprint

sorly  Tensor learning in python 
arXiv   

Kruskal  Joseph    Threeway arrays  rank and uniqueness
of trilinear decompositions  with application to arithmetic complexity and statistics  Linear algebra and its
applications     

Kuleshov  Volodymyr  Chaganty  Arun Tejasvi  and Liang 
Percy  Tensor factorization via matrix factorization  In
AISTATS   

Le  Quoc    Karpenko  Alexandre  Ngiam  Jiquan  and Ng 
ICA with reconstruction cost for ef cient
Andrew   
In Advances in Neural
overcomplete feature learning 
Information Processing Systems  pp     

Leurgans  SE  Ross  RT  and Abel  RB    decomposition
for threeway arrays  SIAM Journal on Matrix Analysis
and Applications     

Levy  Omer and Goldberg  Yoav  Neural word embedding
as implicit matrix factorization  In Advances in Neural
Information Processing Systems  pp     

Jonathan 

Ma  Tengyu  Shi 

and Steurer  David 
Polynomialtime tensor decompositions with sumof 
squares  In Foundations of Computer Science  FOCS 
  IEEE  th Annual Symposium on   

Mikolov  Tomas  Chen  Kai  Corrado  Greg  and Dean  Jeffrey  Ef cient estimation of word representations in vector space  arXiv preprint arXiv     

Mikolov  Tomas  Sutskever  Ilya  Chen  Kai  Corrado 
Greg    and Dean  Jeff  Distributed representations of
In Adwords and phrases and their compositionality 
vances in Neural Information Processing Systems  pp 
     

Mikolov  Tomas  Yih  Wentau  and Zweig  Geoffrey  Linguistic regularities in continuous space word representations  In HLTNAACL  pp       

Pennington 

Jeffrey  Socher  Richard  and Manning 
Christopher    Glove  Global vectors for word repreIn Empirical Methods in Natural Language
sentation 
Processing  EMNLP  pp     

Shah  Parikshit  Rao  Nikhil  and Tang  Gongguo  Sparse
and lowrank tensor decomposition  In Advances in Neural Information Processing Systems   

Smith  Shaden and Karypis  George  SPLATT  The Sur 

prisingly ParalleL spArse Tensor Toolkit 

Smith  Shaden and Karypis  George  DMS  Distributed
sparse tensor factorization with alternating least squares 
Technical report   

Song  Zhao  Woodruff  David  and Zhang  Huan  Sublinear
time orthogonal tensor decomposition  In Advances in
Neural Information Processing Systems   

Souloumiac  Antoine 

Is nonJoint diagonalization 
In  rd
orthogonal always preferable to orthogonal 
IEEE International Workshop on Computational Advances in MultiSensor Adaptive Processing   

Tang  Gongguo and Shah  Parikshit  Guaranteed tensor deIn Proceedings of
composition    moment approach 
the  nd International Conference on Machine Learning  ICML  pp     

Vervliet     Debals     Sorber     Van Barel     and
De Lathauwer     Tensorlab   Mar    Available online 

Wang  Yichen  Chen  Robert  Ghosh  Joydeep  Denny 
Joshua    Kho  Abel  Chen  You  Malin  Bradley    and
Sun  Jimeng  Rubik  Knowledge guided tensor factorization and completion for health data analytics  In Proceedings of the  th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pp 
  ACM     

Wang  Yining  Tung  HsiaoYu  Smola  Alexander    and
Anandkumar  Anima  Fast and guaranteed tensor deIn Advances in Neural Incomposition via sketching 
formation Processing Systems  pp       

Yu  Rose and Liu  Yan  Learning from multiway data  SimIn Proceedings of
ple and ef cient tensor regression 
the  nd International Conference on Machine Learning  ICML  pp     

Zhang  Tong and Golub  Gene    Rankone approximation
to high order tensors  SIAM Journal on Matrix Analysis
and Applications     

