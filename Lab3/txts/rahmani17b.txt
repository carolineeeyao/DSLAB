Innovation Pursuit    New Approach to the Subspace Clustering Problem

Mostafa Rahmani   George Atia  

Abstract

This paper presents   new scalable approach 
termed Innovation Pursuit  iPursuit  to the problem of subspace clustering 
iPursuit rests on  
new geometrical idea whereby each subspace is
identi ed based on its novelty with respect to the
other subspaces  The subspaces are identi ed
consecutively by solving   series of simple linear optimization problems  each searching for  
direction of innovation in the span of the data   
detailed mathematical analysis is provided establishing suf cient conditions for the proposed approach to correctly cluster the data points  Moreover  the proposed direction search approach can
be integrated with spectral clustering to yield
  new variant of spectralclustering based algorithms  Remarkably  the proposed approach can
provably yield exact clustering even when the
subspaces have signi cant intersections  The numerical simulations demonstrate that iPursuit can
often outperform the stateof theart subspace
clustering algorithms   more so for subspaces
with signi cant intersections   along with substantial reductions in computational complexity 

  Introduction
Principal Component Analysis  PCA  is   popular and ef 
 cient procedure to approximate the data with   single low
dimensional subspace  Lerman et al    Nonetheless 
in numerous contemporary applications the data points
may originate from multiple independent sources  in which
case   union of subspaces can better model the data  Vidal 
  The problem of subspace clustering is concerned
with learning these lowdimensional subspaces and clustering the data points to their respective subspaces  generally
without prior knowledge about the number of subspaces
and their dimensions  nor the membership of the data points

 University of Central Florida  Orlando  Florida  USA  Corre 

spondence to  Mostafa Rahmani  mostafa knights ucf edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

to these subspaces  Subspace clustering naturally arises
in many machine learning and data analysis problems  including computer vision       motion segmentation  Vidal
et al    face clustering  Ho et al    image processing  Yang et al    and system identi cation  Vidal
et al    Numerous approaches for subspace clustering
have been studied in prior work  including statisticalbased
approaches  Yang et al    Rao et al    spectral clustering  Soltanolkotabi et al    Von Luxburg 
  Dyer et al    Elhamifar   Vidal    Heckel
    olcskei    Liu et al    Chen   Lerman   
the algebraicgeometric approach  Vidal et al    and
iterative methods  Bradley   Mangasarian    We refer the reader to  Vidal    for   comprehensive survey
on the topic 
This paper aims to advance the stateof theart research on
subspace clustering on several fronts  First  the proposed
approach   termed iPursuit   rests on   novel geometrical
idea whereby one subspace is identi ed at   time based on
its novelty with respect to         the other subspaces  Second  the proposed method is   provable and scalable subspace clustering algorithm   the computational complexity of iPursuit only scales linearly in the number of subspaces and quadratically in their dimensions       Section
  In contrast to the spectralclustering based algorithms
such as  Dyer et al    Elhamifar   Vidal    Liu
et al    which need to solve an    
   dimensional optimization problem to build the similarity matrix  where
   is the number of data points  the proposed method
requires solving few   dimensional linear optimization
problems  This feature makes iPursuit remarkably faster
than the stateof theart algorithms  Third  innovation pursuit in the data span enables superior performance when the
subspaces have considerable intersections in comparison to
the stateof theart subspace clustering algorithms 

  Notation and de nitions
Given   matrix     cid   cid  denotes its spectral norm  For  
vector     cid   cid  denotes its  cid norm and  cid   cid  its  cid norm 
Given matrices  Ai  
   with equal number of rows  we
use the union symbol   to de ne the matrix   
 
  An  as the concatenation of the matrices
      
 Ai  
   For   matrix    we overload the set member 

Ai

  

Submission and Formatting Instructions for ICML  

for            and cid  

that the given data matrix follows the following data model 
Data Model   The data matrix     RM    can
be represented as           DN     where   is an arbitrary permutation matrix  The columns of Di   RM ni
lie in Si  where Si is an ridimensional linear subspace 
   ni      De ne Vi as an
orthonormal basis for Si 
In addition  de ne   as the
space spanned by the data          
Si  Moreover 
it is assumed that every subspace in the set of subspaces
 Si  
   has an innovation over the other subspaces 
to say that  for            the subspace Si does not
Sk   In addition  the columns of   are
completely lie in
normalized       each column has an  cid norm equal to one 

  
  
  cid  

  

  

  iPursuit  Basic idea

iPursuit is   multistep algorithm that identi es one subspace at   time  In each step  the data is clustered into two
subspaces  One subspace is the identi ed subspace and the
other one is the direct sum of the other subspaces  The
data points of the identi ed subspace are removed and the
algorithm is applied to the remaining data to  nd the next
subspace  Accordingly  each step of the algorithm can be
interpreted as   subspace clustering problem with two subspaces  Therefore  for ease of exposition we  rst investigate the twosubspace scenario then extend the result to
multiple  more than two  subspaces  Thus  in this subsection  it is assumed that the data follows Data model   with
     
To gain some intuition  we  rst consider   simple example before stating our main result  Suppose that    and
   are not orthogonal and assume that         The
nonorthogonality of    and    is not   requirement  but
is merely used herein to simplify the exposition of the basic idea underlying the proposed approach  De ne    as
the optimal point of the following optimization problem

  

 cid cT   cid 

       and  cid   cid     

subject to

min
 
where  cid cid  is the  cid norm  The  rst constraint limits the
search space to the span of the data  and the equality constraint  cid   cid      is used to avoid the trivial        solution 
Assume that the columns of    and    are uniformly distributed in    and    respectively  Accordingly  with high
probability  whp  the data is not aligned along any speci  
direction in    and   
The  cid norm minimization problem   searches for   nonzero vector in   that is orthogonal to the maximum number
of data points  Since    has to lie in    we claim that the
optimal point of   lies in            whp given the assumption that the number of data points in    is greater

Figure   The subspace            is the innovation subspace
of    over    The subspace            is orthogonal to   

  

Gi

 cid 

ship operator by using the notation       to signify that
  is   column of      collection of subspaces  Gi  
   is
   dim Gi 
said to be independent if dim
where   denotes the direct sum operator and dim Gi  is

the dimension of Gi  Given   vector     cid cid    is the vector

 cid  

 cid    

of absolute values of the elements of    For   real number
   sgn    denotes the sign of    The complement of   set
  is denoted Lc  For any positive integer    the index set
             is denoted    
Consider two subspaces    and    such that     cid    
and     cid     This means that each of the subspaces   
and    carries some innovation        the other  As such 
corresponding to each subspace we de ne an innovation
subspace capturing its novelty  innovation         the other
subspaces  de ned formally as follows 
De nition   Assume that    and    are two orthonormal bases for    and    respectively  We de ne the innovation subspace of    over    denoted           
words             is the complement of    in the subspace        
Similarly  we can also de ne            as the innovation subspace of    over    Fig    illustrates   scenario
in which the data lies in   union of   twodimensional and
  onedimensional subspace  Note that the innovation subspace of    over    is orthogonal to    and is the complement of    in        

as the subspace spanned by  cid       VT

 cid     In other

 

  Proposed Approach
In this section  the main geometrical idea underlying iPursuit is  rst presented  This idea is based on   nonconvex
 cid norm minimization problem searching for   direction
of innovation in the span of the data  Then  we provide  
convex relaxation to   linear optimization problem  whose
solution is shown to yield the correct subspaces under mild
suf cient conditions  Due to space limitations  the proofs
of all the theoretical results are deferred to an extended version of this paper  Rahmani   Atia    It is assumed

Submission and Formatting Instructions for ICML  

than the number of data points in    To clarify  consider
the following cases 
   If         then it would not be orthogonal to the majority of the data points in    given that the columns of   
are uniformly distributed in    In addition  it cannot be
orthogonal to most of the data points in    since    and
   are not orthogonal  Since the optimal vector should be
orthogonal to the maximum number of data points  it is
highly likely that     cid     Similarly  it is highly unlikely
that the optimal point lies in    II  If                
then according to De nition   it is orthogonal to the data
points in    However  since it was assumed that        
the cost function of   can be decreased if    lies in
            which is orthogonal to    III  If    does
not lie in any of the subspaces                  and
           then it is neither orthogonal to    nor to   
Now  since the data points are distributed uniformly in the
subspaces  we see that    would not be orthogonal to the
maximum number of data points 
Hence  it is highly likely that    lies in           
It
follows that the columns of   corresponding to the nonzero elements of        lie in    The following lemma
ensures that these columns span   
Lemma   The columns of   corresponding to the nonzero elements of        span    if the following conditions are satis ed                         ii     cannot
follow Data model   with       that is  the data points in
   do not lie in the union of lower dimensional subspaces
within    each with innovation        the other subspaces 

It is important to note that the conditions of Lemma   are
by no means restrictive  Speci cally  if the requirement  ii 
of Lemma   is not satis ed  then the problem can be viewed
as   subspace clustering problem with more than two subspaces  In Section   we will investigate the clustering
problem with more than two subspaces 
Remark   At   high level  the innovation search problem    nds the most sparse vector in the row space of
   Interestingly   nding the most sparse vector in   linear subspace has bearing on  and has been effectively used
in  other machine learning problems  including dictionary
learning and spectral estimation  Qu et al   

  Convex relaxation

The  cid norm is known to provide an ef cient convex relaxation of the  cid norm  Thus  we relax the nonconvex cost
function and rewrite   as

 cid cT   cid 

subject to

       and  cid   cid       

 

min

  

Since the feasible set of   is nonconvex  we further substitute the equality constraint with   linear constraint to

consider the following convex program

 IP  min

  

 cid cT   cid 

     

       and

 cT      

 

 IP  is the core program of iPursuit to  nd   direction of
innovation  The vector   is   unit  cid norm vector that
should not be orthogonal to    In Section   we present  
methodology to obtain   good choice for the vector   from
the given data  However  our investigations have shown
that iPursuit performs well generally even when   is chosen as   random vector in   

  Segmentation of two subspaces  Performance

guarantees

Suppose that   follows Data model   with           
the data lies in   union of two subspaces  In order to show
that the optimal point of  IP  yields correct clustering  it
suf ces to show that the optimal point lies in           
given that condition  ii  of Lemma   is satis ed for     or
lies in            given that the condition is satis ed for
   The following theorem provides suf cient conditions
for the optimal point of   to lie in            provided
that

 cid cT   cid   

inf

        

cT   

inf

        

cT   

 cid cT   cid 

 

If the inequality in   is reversed  then similar suf cient
conditions can be established for the optimal point of  
to lie in            Hence  assumption   does not
lead to any loss of generality  The subspaces           
and            are orthogonal to    and    respectively 
Thus    is equivalent to

inf

          

cT   

 cid cT   cid   

inf

        

cT   

 cid cT   cid 

 

Henceforth   innovation subspace  refers to           
whenever the twosubspace scenario is considered and  
is satis ed  Theorem   stated next provides suf cient
conditions for the optimal point of  IP  in   to lie in
           These conditions are characterized in terms
of the optimal solution to an oracle optimization problem
 OP  wherein the feasible set of  IP  is replaced by the innovation subspace  De ne    as the optimal point of the
following optimization problem
 cid cT   cid 

 OP 

  

min
                and

subject to

 

 cT      

Theorem   establishes that    is the optimal point of  
Before we state the theorem  we de ne the index set     
  di     di      with cardinality     
           cT
    and   complement set Lc
  comprising the indices of
the columns of    orthogonal to   

Submission and Formatting Instructions for ICML  

sgn cT

oracle  OP  in   and de ne    cid  di   

Theorem   Suppose the data matrix   follows Data
model   with       Also  assume that condition   and
the requirement of Lemma   for    are satis ed  condition  ii  of Lemma   Let    be the optimal point of the
  di di 
Also  let    denote an orthonormal basis for           
and assume that   is   unit  cid norm vector in   that is not
orthogonal to            If
 
inf
   
 
 cid cid 
 cid qT   cid 
 cid qT   cid 

 cid cid     cid VT
 cid 
 cid cid   di

 cid 
 cid 
    cid 

 cid 
 cid cid cid 

 cid 
 cid 

 cid cid   di

 cid cid      

 cid cid      

   cid VT

    cid 

di   

  and

  Lc
 

inf
   
 cid cid 

di   

 cid 

 
then                 is the optimal point of  IP  in  
and iPursuit clusters the data correctly 

In what follows  we provide   detailed discussion of the
signi cance of the suf cient conditions   of Theorem  
which reveal some interesting facts about the properties of
iPursuit 

di Di

 cid cid   

 cid 

 cid cid uT di

  Data distribution  The LHS of   is known as the permeance statistic  an ef cient measure of how well the data
points are distributed in   subspace  Lerman et al   
As such  the suf cient conditions   imply that the distribution and the number of the data points within the subspaces are important performance factors for iPursuit  For
  set of data points Di in   subspace Si  the permeance
statistic is de ned as   Di Si    inf
  Si
 cid   cid 
From this de nition  we see that the permeance statistic is
fairly small if   set of data points are aligned along   given
direction       not well distribued  In addition  having   
on the RHS reveals that the distribution of the data points
within    also matters since    cannot be simultaneously
orthogonal to   large number of columns of    if the data
does not align along speci   directions  Hence  according
to   iPursuit yields correct clustering if the data is well
distributed within the subspaces 
  The coherency of   with            For the optimal
point of  IP  to lie in            the vector   should not
be too coherent with    This can be seen by observing that
if   has   small projection on              in which case
it would be more coherent with      the Euclidean norm
of any feasible point of   lying in            will have
to be large to satisfy the equality constraint in  IP  Such
points are less likely to be optimal in the sense of attaining
the minimum of the objective function in   The aforementioned intuition is af rmed by the analysis 
Indeed 
the factor  cid qT   cid cid qT   cid  in the suf cient conditions of
Theorem   and Lemma   indicates that the coherency of  

 

  

  cid qT   cid 
  cid qT   cid       

with the innovation subspace is an important performance
factor for iPursuit  The coherence property could have  
more serious effect on the performance of the algorithm for
nonindependent subspaces  especially when the dimension
of their intersection is signi cant  For instance  consider
the scenario where the vector   is chosen randomly from
   and de ne   as the dimension of the intersection of   
and    It follows that            has dimension        
Thus 
  Therefore    randomly chosen
vector   is likely to have   small projection on the innovation subspace when   is large  As such  in dealing with
subspaces with signi cant intersection  it may not be favorable to choose the vector   at random  In Section   and
section   we develop   simple technique to learn   good
choice for   from the given data  It makes iPursuit remarkably powerful in dealing with subspaces with intersection
as shown in the numerical results section 
Remark   We conjecture that if the ratios  ni ri  
   are
suf ciently large  the optimal point of   always lies in an
innovation subspace  our investigations have shown that
the optimal point of  IP  always lies in an innovation subspace provided that   is suf ciently coherent with the innovation subspace regardless how large the intersections
between the subspaces are  This is particularly compelling
if the subspaces are too close  in which case spectralclustering based methods can seldom construct   correct
similarity matrix  which leads to large clustering errors 
By contrast  in such cases iPursuit can yield accurate clustering provided that the constraint vector is suf ciently coherent with the innovation subspace 
In Section   we
present   method to identify   coherent constraint vector 

Now  we demonstrate that the suf cient conditions   are
not restrictive  The following lemma simpli es the conditions when the data points are randomly distributed in the
subspaces  In this setting  we show that the conditions are
naturally satis ed 
Lemma   Assume that the distribution of the columns of
   in    and    in    is uniformly random and consider
the same setup of Theorem   If

 cid 

 

 

 cid 
     cid   

 

            

 cid    

 cid 

      

            

    log   

     

 

  

  
  

 cid   

       

 
   

   cid VT

      
 

 cid cid   

 cid    
 cid 
    cid 
 
 cid 
    cid 
probability at least     exp cid    
 cid    

  for all                

 
   cid VT

 cid qT   cid 
 cid qT   cid 

       
 

   

  
  

 cid 

exp

  

 
 

then the optimal point of   lies in            with

Submission and Formatting Instructions for ICML  

When the data points are not aligned along any speci  
directions     can only be simultaneously orthogonal to  
small number of columns of    Thus     will be much
smaller than    The LHS of   has order    and the
         which is much smaller than   
RHS has order
Therefore  the suf cient conditions are naturally satis ed
when the data is well distributed within the subspaces 

 

  Clustering multiple subspaces

Suppose that   follows Data Model   with       where
      Similar to   without loss of generality assume
that

 cid cT Dm cid   

 cid cT Dj cid 

 

    cid Sm    

inf

 cid 

Sk

  
  cid  
cT qm 

    cid Sj    

inf

 cid 

Sk

  
  cid  
cT qm 

Sk         cid Sm     

for all                 where qm is   unit  cid norm in
  Sk  Given   we expect the optimal point of  IP 
  
with     qm to lie in the innovation subspace of Sm over
  
  
  cid  
identi ed  Accordingly  in this step the clustering problem separates  Dm Sm  and  
Si  Theorem
  can be used to derive suf cient conditions for the optimal

 cid  in which case Sm will be

point to lie in   cid Sm     

 cid  by substituting      

  

  

Di  

Sk

Sk

  
  cid  

  

  

Si  Conwith  Dm Sm  and       with  
sequently  we can also establish suf cient conditions for
exact subspace segmentation  Due to space limitations  we
defer the analysis to  Rahmani   Atia   

Di  

  

  

  

  

  
  cid  

 

is

equivalent

optimization
 cid aT UT   cid 

problem  
subject to aT UT      

  Complexity analysis
De ne   as an orthonormal basis for   
Thus 
to
the
Furmin
ther  de ne     UT    This optimization problem can be
ef ciently solved using the Alternating Direction Method
of Multipliers  ADMM   Boyd et al    Due to space
constraints  we defer the details of the iterative solver
to  Rahmani   Atia   
The complexity of the
initialization step of the solver is      plus the complexity of obtaining    Obtaining an appropriate   has
       complexity by applying the clustering algorithm
to   random subset of the rows of    with the rank of
sampled rows equal to    In addition  the complexity of
each iteration of the solver is   rM  Thus  the overall
complexity is less than                since the
number of data points remaining keeps decreasing over
the iterations  In most cases     cid     hence the overall
complexity is roughly          

Table   Run time of different algorithms              
 ri 

       

        ni 
LRR
SSC
  
   
   
 
   
 
   
       
 
             

iPursuit
   
   
   
   

SSCOMP

   
   
   
     

Remark   The proposed method brings about substantial speedups over existing algorithms due to the following 
   unlike existing multistep algorithms  such as RANSAC 
which have exponential complexity in the number and dimension of subspaces  the complexity of iPursuit is linear
in the number of subspaces and quadratic in their dimension  In addition  while iPursuit has linear complexity in
   spectralclustering based algorithms have complexity
     
      for their spectral clustering step plus the complexity of obtaining the similarity matrix  ii  more importantly  the solver of the proposed optimization problem has
  rM  complexity per iteration  while the other operations   whose complexity are        and        sit
outside of the iterative solver  This feature makes the proposed method notably faster than most of the existing algorithms which solve highdimensional optimization problems  For instance  solving the optimization problem of the
SSC algorithm has roughly      
    rM  complexity per
iteration  Elhamifar   Vidal   
For instance  suppose        the data lies in   union of
three  dimensional subspaces and ni      Table  
compares the running time of the algorithms  One can observe that iPursuit is remarkably faster  More running time
comparisons are available in  Rahmani   Atia   

  How to choose the vector   

Our investigations have shown that iPursuit performs very
well when the subspaces are independent or have small intersections even if   is chosen randomly  However  in the
more challenging scenarios in which the dimensions of the
intersections between the subspaces are signi cant  randomly choosing the vector   could be unfavorable since
the dimension of the innovation subspace decreases as the
dimension of the intersection increases  This motivates
the methodology described next that aims to search for  
 good  vector    Consider the optimization problem

  

min

subject to

 cid qT   cid 

       and

 cid   cid       
which searches for   nonzero vector in   with small projections on the columns of    It is straightforward to show
that the optimal point of   is the singular vector corresponding to the least nonzero singular value of    When
the subspaces are close to each other  it is not hard to see

Submission and Formatting Instructions for ICML  

that the least singular vector is highly coherent with the innovation subspace  thus can be   good candidate for the
vector    For subspaces with remarkable intersections  this
choice of   brings about substantial improvement in performance compared to using   randomly generated    cf 
Section   Clearly  when the data is noisy  we utilize the
least dominant singular vecor  In addition  when the singular values of the noisy data decay rapidly  it may be hard
to accurately estimate the rank of    which may lead to an
unfavorable use of   singular vector corresponding to noise
as the constraint vector  Alternatively  we can choose the
data point closest to the least dominant singular vector as
our vector    This technique makes the proposed method
robust to the presence of noise  cf  Section  

  Noisy data

In the presence of additive noise  we model the data as
De           where De is the given noisy data matrix 
  is the clean data which follows Data model   and   represents the noise component  The rank of   is equal to
   Thus  the singular values of De can be divided into two
subsets  the dominant singular values  the  rst   singular
values  and the small singular values  or the singular values corresponding to the noise component  Estimating the
number of dominant singular values is   fairly wellstudied
topic  Stoica   Selen   
Consider the optimization problem  IP  using De      

 cid cT De cid 

     span De  and  cT        
Clearly  the optimal point of   is very close to the subspace spanned by the singular vectors corresponding to the
small singular values  Thus  if ce denotes the optimal soe De will be fairly
lution of   then all the elements of cT
small and the subspaces cannot be distinguished  However 
the span of the dominant singular vectors is approximately
equal to    Accordingly  we propose the following approximation to  IP 
 cid cT De cid 

     span    and  cT        

    

min

  

    

min

  

where   is an orthonormal basis for the span of the dominant singular vectors  The  rst constraint of   forces
the optimal point to lie in span    which serves as  
good approximation to span    For instance  consider
           where the columns of        
lie in    dimensional subspace    and the columns of
        lie in another  dimensional subspace   
De ne ce and cr as the optimal points of   and   rer De  with the maxspectively  Fig    shows  cT
  De  and  cT
imum element scaled to one  Clearly  cT
  De can be used to
correctly cluster the data  In addition  when   is low rank 
the subspace constraint in   can  lter out   remarkable
portion of the noise component 

Figure   The left plot shows the output of   while the right
plot shows the output of iPursuit when its search domain is restricted to the subspace spanned by the dominant singular vectors
as per  

When the data is noisy and the singular values of   decay rapidly  it may be hard to accurately estimate    If the
dimension is incorrectly estimated    may contain some
singular vectors corresponding to the noise component 
wherefore the optimal point of   could end up lying
close to   noise singular vector  In the sequel  we present
two effective techniques to effectively avoid this undesirable scenario 

  Using   data point as   constraint vector    singular vector corresponding to the noise component is nearly
orthogonal to the entire data       has small projection on
all the data points  Thus  if the optimal vector is forced to
have strong projection on   data point  it will be unlikely for
the optimal direction to be close to   noise singular vector 
Thus  we modify   as follows

 cid cT De cid 

    

     span    and  cT dek        

min

  

where dek is the kth column of De  The modi ed constraint in   ensures that the optimal point is not orthogonal to dek  If dek lies in the subspace Si  the optimal point
of   will lie in the innovation subspace corresponding
to Si whp 
In order to determine   good data point for
the constraint vector  we leverage the principle presented
in section   Speci cally  we use the data point that is
closest to the least dominant singular vector rather than the
least dominant singular vector itself 

  Sparse representation of the optimal point  When  
is low rank          cid  min       any direction in the
span of the data   including the optimal direction sought by
iPursuit   can be represented as   sparse combination of the
data points  For such settings  we propose the alternative
optimization problem

 cid aT QT De cid     cid   cid 

min
   
subject to     QT De   and aT QT dek      

 

where   is   regularization parameter  Forcing   sparse
representation in   for the optimal direction averts   solution that lies in close proximity with the small singular
vectors  which are normally obtained through linear combinations of   large number of data points  This alternative

Submission and Formatting Instructions for ICML  

formulation is particularly useful when the dimension of
the data cannot be accurately estimated  When   is not
  low rank matrix  we can set   equal to zero  The table
of Algorithm   details the proposed method for noisy data
along with the used notation and de nitions 

  ERROR PROPAGATION

If    or ci  and the threshold co in Algorithm   are chosen appropriately  the algorithm exhibits strong robustness
in the presence of noise  Nonetheless  if the data is too
noisy  an error incurred in one step of the algorithm may
propagate and unfavorably affect the performance in subsequent steps  Two types of error could occur  The  rst
type is that some data points are erroneously included in
   or    An example is when Sm is the subspace to be
identi ed in   given step of the algorithm       the optimal
point of   lies in the innovation subspace corresponding
to Sm  but few data points from the other subspaces are erroneously included in    The second type of error is that
some of the data points remain unidenti ed  For instance 
Sm is to be identi ed in   given iteration  yet not all the data
points belonging to Sm are identi ed  In an extended version of this work  Rahmani   Atia    we discuss the
two main sources of error and present some techniques to
effectively neutralize their impact on subsequent iterations 
In addition    brief discussion about handling the presence
of outliers is provided 

  Subspace clustering using direction search and

spectral clustering

In  Rahmani   Atia    we showed that the direction search optimization problem   can be utilized to
 nd   neighborhood set for the kth data point  We leveraged this feature to propose   new spectralclustering 
based subspace segmentation algorithm  dubbed Direction
search based Subspace Clustering  DSC   Rahmani   Atia 
  We showed that DSC often outperforms the existing spectralclustering based methods particularly for hard
scenarios involving high levels of noise and close subspaces  and notably improves the stateof theart result for
the challenging problem of face segmentation using subspace clustering 

  Numerical Simulations
  The coherency parameter

In this experiment  we examine the impact of the coherency
of   with the innovation subspace  Here  it is assumed that
the data follows Data Model   with       and       
The dimension of the subspaces is equal to   and the dimension of their intersection varies between   to   Each
subspace contains   data points distributed uniformly at

       cid      cid       VT

random within the subspace  Let cr    cid qT   cid cid qT   cid 
Thus  cr captures the coherency of   with the innovation
subspace  De ne     and     as orthonormal bases for the
identi ed subspaces    trial is considered successful if
 cid       VT
       cid          
The left plot of Fig    shows the phase transition in the
plane of cr and    where   is the dimension of the intersection of the two subspaces  In this  gure  white designates exact identi cation of the subspaces with probability
almost equal to one  As shown  the probability of correct
clustering increases if cr is increased  Remarkably  the left
plot of Fig    shows that when cr is suf ciently large  the
algorithm yields exact segmentation even when      

Figure   Left  Phase transition plot in the plane of the coherency
parameter and the dimension of intersection  Right  The clustering error of iPursuit  SSC and LRR versus the dimension of the
intersection 

  Clustering data in union of multiple subspaces

In this simulation  we consider the subspace clustering
problem with    dimensional subspaces  Si 
   and
       Each subspace contains   data points and the
distribution of the data within the subspaces is uniformly
random  We compare the performance of the proposed
approach to the stateof theart sparse subspace clustering
 SSC   Elhamifar   Vidal    algorithm and low rank
representation  LRR  based clustering  Liu et al   
The number of replicates used in spectral clustering for
SSC and LRR is equal to   De ne the clustering error
as the ratio of misclassi ed points to the total number of
data points  The right plot of Fig    shows the clustering
error versus the dimension of the intersection  The dimension of intersection varies between   and   Each point in
the plot is obtained by averaging over   independent runs 
iPursuit is shown to yield the best performance 

  Noisy data

In this section  we study the performance of the proposed
approach  SSC  LRR  SCC  Chen   Lerman    TSC
 Heckel     olcskei    and SSCOMP  Dyer et al 

Submission and Formatting Instructions for ICML  

Table   CE   of algorithms on Hopkins  dataset  Mean   Median 

 

     
     

SSC

     

LRR
     

       

     

iPursuit

SSCOMP

TSC

  ats

     
     

       
     

     
     

     
     

SCC
     

     

Thus  the problem here is to cluster data lying in two or
three subspaces  Table   shows the clustering error  in percentage  for iPursuit  SSC  LRR  TSC  SSCOMP and   
 ats  We use the results reported in  Elhamifar   Vidal 
  Heckel     olcskei    Vidal    Park et al 
  For SSCOMP and TSC  the number of parameters
for motion segmentation are equal to   and   One can
observe that iPursuit yields competitive results comparable
to SSC  SCC  and LRR and outperforms TSC  SSCOMP
and   ats 

Algorithm   Innovation pursuit  iPursuit  for noisy data
Initialization Set      and    as integers greater than   and set
ci and co as positive real numbers less than  
While The number of identi ed subspaces is less than    or the
number of the columns of De is less than    
  Obtaining the basis for the remaining Data  Construct   as
the orthonormal matrix formed by the dominant singular vectors
of De 
  Choosing the vector    Set     the column of De closest to
the last column of   
  Solve   and de ne      Qa  where    is the optimal point
of   and de ne     

 cid cid DT
    cid cid 
 cid cid DT
    cid cid 

max 

 

  Finding   basis for the identi ed subspace  Construct    as
the matrix consisting of the columns of De corresponding to the
elements of    that are greater than ci  Alternatively  construct
   using the columns of De corresponding to the   largest elements of    De ne    as an orthonormal basis for the dominant
left singular vectors of   
  Finding   basis for the rest of the data 
De ne the vector    whose entries are equal to the  cid norm of the
columns of        FT
   De  Scale    as         max   
Construct    as the columns of De corresponding to the elements
of    greater than co  De ne    as an orthonormal basis for the
dominant left singular vectors of of   
  Find the data point belonging to the identi ed subspace 
Assign dei to the identi ed subspace if  cid FT
  dei cid 
  Remove the data points belonging to the identi ed subspace  Update De by removing the columns corresponding to
the identi ed subspace 
End While

  dei cid     cid FT

Acknowledgment  This work was supported by NSF
CAREER Award CCF  and NSF Grant CCF 
 

Figure   Performance of the algorithms versus the dimension of
intersection for different noise levels 

  with different noise levels  and varying dimensions
of the intersection between the subspaces  which gives rise
to both low rank and high rank data matrices  It is assumed
that   follows Data model   with              
      and  ri 
       The dimension of the intersection between the subspaces varies from   to   Thus  the
rank of   ranges from   to   The Noisy data is modeled as De          with the elements of   sampled independently from   zero mean Gaussian distribution  Fig 
  shows the performance of the different algorithms versus the dimension of the intersection for    
equal
to       and   One can observe that even
with       iPursuit signi cantly outperforms the other
algorithms  In addition  when the data is very noisy      
      it yields better performance when the dimension of the intersection is large  SSC  LRR  and SSCOMP
yield   better performance for lower dimension of intersection  This is explained by the fact that the rank of the data
is high when the dimension of the intersection is low  and
the subspace projection operation QT De may not always
 lter out the additive noise effectively 

 cid   cid  
 cid   cid  

  Real data

We apply iPursuit to the problem of motion segmentation
using the Hopkins   Tron   Vidal    dataset  which
contains video sequences of   or   motions 
In motion
segmentation  each motion corresponds to one subspace 

Submission and Formatting Instructions for ICML  

References
Boyd  Stephen  Parikh  Neal  Chu  Eric  Peleato  Borja  and
Eckstein  Jonathan  Distributed optimization and statistical learning via the alternating direction method of multipliers  Foundations and Trends  in Machine Learning 
   

Bradley  Paul   and Mangasarian  Olvi    kplane clustering  Journal of Global Optimization     

Chen  Guangliang and Lerman  Gilad  Spectral curvature
clustering  scc  International Journal of Computer Vision     

Dyer  Eva    Sankaranarayanan  Aswin    and Baraniuk 
Richard    Greedy feature selection for subspace clustering  The Journal of Machine Learning Research   
   

Elhamifar  Ehsan and Vidal  Rene  Sparse subspace clustering  Algorithm  theory  and applications  Pattern Analysis and Machine Intelligence  IEEE Transactions on   
   

Heckel  Reinhard and   olcskei  Helmut  Robust subarXiv preprint

space clustering via thresholding 
arXiv   

Ho  Jason  Yang  MingHsuan  Lim  Jongwoo  Lee  KuangChih  and Kriegman  David  Clustering appearances of
In Proobjects under varying illumination conditions 
ceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition  CVPR  volume    

Rahmani  Mostafa and Atia  George  Innovation pursuit 
  new approach to subspace clustering  arXiv preprint
arXiv   

Rahmani  Mostafa and Atia  George    direction search
and spectral clustering based approach to subspace clustering  arXiv preprint   

Rao  Shankar  Tron  Roberto  Vidal  Rene  and Ma  Yi 
Motion segmentation in the presence of outlying  incomplete  or corrupted trajectories  Pattern Analysis and Machine Intelligence  IEEE Transactions on   
   

Soltanolkotabi  Mahdi  Candes  Emmanuel    et al    geometric analysis of subspace clustering with outliers  The
Annals of Statistics     

Stoica  Petre and Selen  Yngve  Modelorder selection   
review of information criterion rules  Signal Processing
Magazine  IEEE     

Tron  Roberto and Vidal  Ren      benchmark for the comparison of    motion segmentation algorithms  In IEEE
Conference on Computer Vision and Pattern Recognition
 CVPR  pp     

Vidal  Ren    Soatto  Stefano  Ma  Yi  and Sastry  Shankar 
An algebraic geometric approach to the identi cation of
  class of linear hybrid systems  In Proceedings of the
 nd IEEE Conference on Decision and Control  CDC 
volume   pp     

Vidal  Rene  Ma  Yi  and Sastry  Shankar  Generalized
principal component analysis  GPCA  Pattern Analysis and Machine Intelligence  IEEE Transactions on   
   

Lerman  Gilad  McCoy  Michael    Tropp  Joel    and
Zhang  Teng  Robust computation of linear models by
convex relaxation  Foundations of Computational Mathematics     

Vidal  Ren    Tron  Roberto  and Hartley  Richard  Multiframe motion segmentation with missing data using
powerfactorization and GPCA  International Journal of
Computer Vision     

Liu  Guangcan  Lin  Zhouchen  Yan  Shuicheng  Sun  Ju 
Yu  Yong  and Ma  Yi  Robust recovery of subspace
structures by lowrank representation  Pattern Analysis
and Machine Intelligence  IEEE Transactions on   
   

Park  Dohyung  Caramanis  Constantine  and Sanghavi 
In Advances in
Sujay  Greedy subspace clustering 
Neural Information Processing Systems  pp   
 

Qu  Qing  Sun  Ju  and Wright  John  Finding   sparse vector in   subspace  Linear sparsity using alternating directions  In Advances in Neural Information Processing
Systems  pp     

Vidal  Rene  Subspace clustering  IEEE Signal Processing

Magazine     

Von Luxburg  Ulrike    tutorial on spectral clustering 

Statistics and computing     

Yang  Allen    Rao  Shankar    and Ma  Yi  Robust
statistical estimation and segmentation of multiple subIn Computer Vision and Pattern Recognition
spaces 
Workshop    CVPRW  Conference on  pp   
IEEE   

Yang  Allen    Wright 

John  Ma  Yi  and Sastry 
  Shankar  Unsupervised segmentation of natural images via lossy data compression  Computer Vision and
Image Understanding     

