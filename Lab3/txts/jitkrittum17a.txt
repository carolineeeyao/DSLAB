An Adaptive Test of Independence with Analytic Kernel Embeddings

Wittawat Jitkrittum   Zolt   Szab    Arthur Gretton  

Abstract

  new computationally ef cient dependence measure  and an adaptive statistical test of independence 
are proposed  The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals  evaluated at
   nite set of locations  features  These features are
chosen so as to maximize   lower bound on the test
power  resulting in   test that is dataef cient  and
that runs in linear time  with respect to the sample
size    The optimized features can be interpreted
as evidence to reject the null hypothesis  indicating
regions in the joint domain where the joint distribution and the product of the marginals differ most 
Consistency of the independence test is established 
for an appropriate choice of features  In realworld
benchmarks  independence tests using the optimized
features perform comparably to the stateof theart
quadratictime HSIC test  and outperform competing
     and     log    tests 

  Introduction
We consider the design of adaptive  nonparametric statistical
tests of dependence  that is  tests of whether   joint distribution Pxy factorizes into the product of marginals PxPy
with the null hypothesis that        and   are independent  While classical tests of dependence  such as Pearson  
correlation and Kendall     are able to detect monotonic
relations between univariate variables  more modern tests
can address complex interactions  for instance changes in
variance of   with the value of     Key to many recent
tests is to examine covariance or correlation between data
features  These interactions become signi cantly harder to
detect  and the features are more dif cult to design  when
the data reside in high dimensions 

Zolt   Szab   ORCID ID    Arthur Gretton   ORCID ID     Gatsby Unit  University College London  UK   CMAP   cole Polytechnique  France 
Correspondence to  Wittawat Jitkrittum  wittawatj gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

  basic nonlinear dependence measure is the HilbertSchmidt Independence Criterion  HSIC  which is the
HilbertSchmidt norm of the covariance operator between
feature mappings of the random variables  Gretton et al 
    Each random variable   and   is mapped
to   respective reproducing kernel Hilbert space Hk and
Hl  For suf ciently rich mappings  the covariance operator
norm is zero if and only if the variables are independent   
second basic nonlinear dependence measure is the smoothed
difference between the characteristic function of the joint
distribution  and that of the product of marginals  When
  particular smoothing function is used  the statistic corresponds to the covariance between distances of   and   variable pairs  Feuerverger    Sz kely et al    Sz kely
  Rizzo    yielding   simple test statistic based on
pairwise distances  It has been shown by Sejdinovic et al 
  that the distance covariance  and its generalization
to semimetrics  is an instance of HSIC for an appropriate
choice of kernels    disadvantage of these feature covariance statistics  however  is that they require quadratic time
to compute  besides in the special case of the distance covariance with univariate realvalued variables  where Huo  
Sz kely   achieve an     log    cost  Moreover  the
feature covariance statistics have intractable null distributions  and either   permutation approach or the solution of
an expensive eigenvalue problem       Zhang et al    is
required for consistent estimation of the quantiles  Several
approaches were proposed by Zhang et al    to obtain
faster tests along the lines of HSIC  These include computing HSIC on  nitedimensional feature mappings chosen as
random Fourier features  RFFs   Rahimi   Recht   
  blockaveraged statistic  and   Nystr   approximation
to the statistic  Key to each of these approaches is   more
ef cient computation of the statistic and its threshold under
the null distribution  for RFFs  the null distribution is  
 nite weighted sum of   variables  for the blockaveraged
statistic  the null distribution is asymptotically normal  for
Nystr    either   permutation approach is employed  or
the spectrum of the Nystr   approximation to the kernel
matrix is used in approximating the null distribution  Each
of these methods costs signi cantly less than the      cost
of the full HSIC  the cost is linear in    but also depends
quadratically on the number of features retained    potential disadvantage of the Nystr   and Fourier approaches is
that the features are not optimized to maximize test power 

An Adaptive Test of Independence with Analytic Kernel Embeddings

but are chosen randomly  The block statistic performs worse
than both  due to the large variance of the statistic under the
null  which can be mitigated by observing more data 
In addition to feature covariances  correlation measures have
also been developed in in nite dimensional feature spaces 
in particular  Bach   Jordan   Fukumizu et al   
proposed statistics on the correlation operator in   reproducing kernel Hilbert space  While convergence has been
established for certain of these statistics  their computational cost is high at      and test thresholds have relied
on permutation    number of much faster approaches to
testing based on feature correlations have been proposed 
however  For instance  Dauxois   Nkiet   compute
statistics of the correlation between  nite sets of basis functions  chosen for instance to be step functions or low order
Bsplines  The cost of this approach is      This idea
was extended by LopezPaz et al    who computed
the canonical correlation between  nite sets of basis functions chosen as random Fourier features  in addition  they
performed   copula transform on the inputs  with   total
cost of     log    Finally  space partitioning approaches
have also been proposed  based on statistics such as the
KL divergence  however these apply only to univariate variables  Heller et al    or to multivariate variables of
low dimension  Gretton   Gy       that said  these
tests have other advantages of theoretical interest  notably
distributionindependent test thresholds 
The approach we take is most closely related to HSIC on  
 nite set of features  Our simplest test statistic  the Finite
Set Independence Criterion  FSIC  is an average of covariances of analytic functions       features  de ned on each
of   and       normalized version of the statistic  NFSIC 
yields   distributionindependent asymptotic test threshold 
We show that our test is consistent  despite    nite number
of analytic features being used  via   generalization of arguments in Chwialkowski et al    As in recent work
on twosample testing by Jitkrittum et al    our test
is adaptive in the sense that we choose our features on  
heldout validation set to optimize   lower bound on the
test power  The design of features for independence testing
turns out to be quite different to the case of twosample
testing  however  the task is to  nd correlated feature pairs
on the respective marginal domains  rather than attempting
to  nd   single  highdimensional feature representation on
the tensor product of the marginals  as we would need to
do if we were comparing distributions Pxy and Qxy  While
the use of coupled feature pairs on the marginals entails  
smaller feature space dimension  it introduces signi cant
complications in the proof of the lower bound  compared
with the twosample case  We demonstrate the performance
of our tests on several challenging arti cial and realworld
datasets  including detection of dependence between music
and its year of appearance  and between videos and captions 

In these experiments  we outperform competing linear and
    log    time tests 
  Independence Criteria and Statistical Tests
We introduce two test statistics   rst  the Finite Set Independence Criterion  FSIC  which builds on the principle
that dependence can be measured in terms of the covariance between data features  Next  we propose   normalized
version of this statistic  NFSIC  with   simpler asymptotic
distribution when Pxy   PxPy  We show how to select
features for the latter statistic to maximize   lower bound
on the power of its corresponding statistical test 

  The Finite Set Independence Criterion

We begin by recalling the HilbertSchmidt Independence
Criterion  HSIC  as proposed in Gretton et al    since
our unnormalized statistic is built along similar lines  Consider two random variables         Rdx and        
Rdy  Denote by Pxy the joint distribution between   and    
Px and Py are the marginal distributions of   and     Let  
denote the tensor product  such that               cid      cid  Assume that               and               are positive
de nite kernels associated with reproducing kernel Hilbert
spaces  RKHS  Hk and Hl  respectively  Let  cid     cid HS be the
norm on the space of Hl   Hk HilbertSchmidt operators 
Then  HSIC between   and   is de ned as

HSIC         cid cid xy          

 cid cid 

HS

          cid   cid          cid        cid 
  ExE
           Ex cid        cid Ey cid        cid   

  cid          cid EyEy cid        cid 

 
where Ex   Ex Px  Ey   Ey Py  Exy         Pxy 
Hl to Hk   xy    cid 
and   cid  is an independent copy of    The mean embedding of
Pxy belongs to the space of HilbertSchmidt operators from
 cid 
       dPx      Hk and       cid 
                dPxy        
HS Hl Hk  and the marginal mean embeddings are     
       dPy     
Hl  Smola et al    Gretton et al    Theorem  
show that if the kernels   and   are universal  Steinwart
  Christmann    on compact domains   and    then
HSIC            if and only if   and   are independent 
Given   joint sample Zn    xi  yi  
     Pxy  an empirical estimator of HSIC can be computed in      time
by replacing the population expectations in   with their
corresponding empirical expectations based on Zn 
We now propose our new lineartime dependence measure  the Finite Set Independence Criterion  FSIC  Let
    Rdx and     Rdy be open sets 
Let
                       The idea is to see  xy        
Exy                        Ex         and        
Ey         as smooth functions  and consider   new dis 

An Adaptive Test of Independence with Analytic Kernel Embeddings

tance between  xy and      instead of   HilbertSchmidt
distance as in HSIC  Gretton et al    The new measure is given by the average of squared differences between  xy and       evaluated at   random test locations
VJ    vi  wi  

FSIC         

 xy vi  wi       vi   wi 

           
  cid 
 
  cid 
 

  

 
 

  

  vi  wi   

 
   cid   cid 
 

 

where

           xy                  
  Exy                  Ex        Ey        

  covxy                

 

                       vJ   wJ  cid  and  vi  wi  
  
are realizations from an absolutely continuous distribution
 wrt the Lebesgue measure 
Our  rst result in Proposition   states that FSIC       
almost surely de nes   dependence measure for the random
variables   and     provided that the product kernel on
the joint space       is characteristic and analytic  see
De nition  
De nition    Analytic kernels  Chwialkowski et al   
Let   be an open set in Rd    positive de nite kernel
              is said to be analytic on its domain      
if for all                         is an analytic function on
   
Assumption    The kernels               and
              are bounded by Bk and Bl respectively
 supx   cid          cid    Bk  supy   cid          cid    Bl    and
the product kernel            cid    cid           cid        cid  is
characteristic  Sriperumbudur et al    De nition  
and analytic  De nition   on                  
Proposition    FSIC is   dependence measure  Assume
that assumption   holds  and that
locations
   are drawn from an absolutely conVJ    vi  wi  
tinuous distribution   Then   almost surely  it holds that
FSIC           
   cid   cid      if and only if   and   are
independent 

the test

Proof  Since   is characteristic  the mean embedding map
        cid                   is injective  Sriperumbudur et al    Section   where   is   probability
distribution on        Since   is analytic  by Lemma  
 Appendix   xy and      are analytic functions  Thus 
Lemma    Appendix  setting         guarantees that
FSIC              Pxy   PxPy     and   are
independent almost surely 

FSIC uses  xy as   proxy for Pxy  and      as   proxy for
PxPy  Proposition   states that  to detect the dependence
between   and     it is suf cient to evaluate the difference
of the population joint embedding  xy and the embedding
of the product of the marginal distributions      at    
nite number of locations  de ned by VJ  The intuitive explanation of this property is as follows  If Pxy   PxPy 
then             everywhere  and FSIC           
for any VJ 
If Pxy  cid  PxPy  then   will not be   zero
function  since the mean embedding map is injective  requires the product kernel to be characteristic  Using the
same argument as in Chwialkowski et al    since  
and   are analytic    is also analytic  and the set of roots
Ru                        has Lebesgue measure zero 
Thus  it is suf cient to draw        from an absolutely continuous distribution to have          Ru  almost surely 
and hence FSIC            We note that   characteristic
kernel which is not analytic may produce   such that Ru has
  positive Lebesgue measure  In this case  there is   positive
probability that          Ru  resulting in   potential failure
to detect the dependence 
The next proposition shows that Gaussian kernels   and  
yield   product kernel which is characteristic and analytic  in
other words  this is an example when Assumption   holds 
exp cid 
Proposition  
kernels
exp cid 
characteristic
is
 
 
kernels on Rdx   Rdx and Rdy   Rdy
respectively 
Then 
for positive de nite matrices   and   
           cid    cid           cid        cid  is characteristic and analytic on  Rdx   Rdy      Rdx   Rdy  
Proof  sketch  The main idea is to use the fact that   Gaussian kernel is analytic  and   product of Gaussian kernels is
  Gaussian kernel on the pair of variables  See the full proof
in Appendix   

       cid cid         cid cid 
       cid cid         cid cid 

   product
and analytic  Let        cid 

of Gaussian
       cid 
Gaussian

and
be

observe

estimators

Plugin Estimator Assume
that we
 
 cid  
      
joint sample Zn    xi  yi  
  Pxy  Uni 
 cid 
 cid  
biased
of  xy      
and           
are
and
 cid          
  cid     xi      yj    
respectively    straightforward empirical estimator of
FSIC  is then given by

     xi      yi    
  

 
 
 
    

 xy      

 

 

 cid FSIC Zn   

 
 

   vi  wi 

            xy          cid          

 

  

 cid 
where                cid    cid 
    cid                  cid    

       

   

 

      xi  yi   xj  yj 

 

 

 

 
            
For conciseness  we

  cid 

An Adaptive Test of Independence with Analytic Kernel Embeddings
de ne                   uJ  cid 
  RJ where  ui      vi  wi 
so that  cid FSIC Zn     
     cid     
 cid FSIC  can be ef ciently computed in   dx  dy Jn  time

which is linear in    see   which does not have nested
double sums  assuming that the runtime complexity of
evaluating         is   dx  and that of         is   dy 
Since FSIC satis es FSIC                      in
principle its empirical estimator can be used as   test statistic
for an independence test proposing   null hypothesis     
   and   are independent  against an alternative        
and   are dependent  The null distribution       distribution of the test statistic assuming that    is true  is challenging to obtain  however  and depends on the unknown Pxy 
This prompts us to consider   normalized version of FSIC
whose asymptotic null distribution takes   more convenient
form  We  rst derive the asymptotic distribution of    in
Proposition   which we use to derive the normalized test
statistic in Theorem   As   shorthand  we write           
           covz is covariance Vz stands for variance 
Proposition    Asymptotic distribution of     De ne
                    tJ  cid          
           
Ex cid     cid     and         
            Ey cid     cid    
Let      ij    RJ   be the positive semide nite
matrix with entries  ij   covz   ti     tj   
Exy      vi      wi      vj      wj   ti   tj  Then 
locations
under both    and   
            tJ  for which   is full rank  and    
Vz htj         for                  it holds that       
    
Proof  For    xed             tJ     is   onesample secondorder multivariate Ustatistic with   Ustatistic kernel ht 
Thus  by Lehmann   Theorem   and Kowalski   Tu   Section   Theorem   it follows di 
        where we note that

rectly that            
Exy                         

for any  xed test

       

Recall from Proposition   that       holds almost surely under    The asymptotic normality described in Proposition
  implies that   cid FSIC     
     cid     converges in distribution
to   sum of   dependent weighted   random variables 
The dependence comes from the fact that the coordinates
             uJ of    all depend on the sample Zn  This null distribution is not analytically tractable  and requires   large
number of simulations to compute the rejection threshold
   for   given signi cance value  

  Normalized FSIC and Adaptive Test

For the purpose of an independence test  we will consider
  normalized variant of  cid FSIC  which we call  cid NFSIC 
whose tractable asymptotic null distribution is     the

chisquared distribution with   degrees of freedom  We
then show that the independence test de ned by  cid NFSIC  is
consistent  These results are given in Theorem  
Theorem    Independence test based on  cid NFSIC  is consistent  Let   be   consistent estimate of   based on the joint
sample Zn  where   is de ned in Proposition   Assume
that VJ    vi  wi  
       where   is absolutely continuous wrt the Lebesgue measure  The  cid NFSIC  statistic is
   where        is  

de ned as          cid cid       nI

 cid 

regularization parameter  Assume that

  Assumption   holds 

    is invertible  almost surely 

  limn        

Then  for any      and VJ satisfying the assumptions 

 

  Under      

      as      

  Under    for any        limn    cid      

   
 almost surely  That is  the independence test based on
 cid NFSIC  is consistent 

 cid 

Proof  sketch    Under        cid       nI     asymptotically follows     because      is asymptotically normally distributed  see Proposition   Claim   builds on
the result in Proposition   stating that    cid    under    it
follows using the convergence of    to    The full proof can
be found in Appendix   

Theorem   states that if    holds  the statistic can be arbitrarily large as   increases  allowing    to be rejected for
any  xed threshold  Asymptotically the test threshold    is
given by the      quantile of     and is independent
of    The assumption on the consistency of   is required
to obtain the asymptotic chisquared distribution  The regularization parameter    is to ensure that        nI  can
be stably computed  In practice     requires no tuning  and
can be set to be   very small constant  We emphasize that  
need not increase with   for test consistency 
The next proposition states that the computational complexity of the  cid NFSIC  estimator is linear in both the input
dimension and sample size  and that it can be expressed
in terms of the      ij       vi  xj    RJ       
 Lij       wi  yj    RJ   matrices  In contrast to typical
kernel methods    large Gram matrix of size       is not
needed to compute  cid NFSIC 
Proposition    An empirical estimator of  cid NFSIC  Let
                cid 
  Rn  Denote by   the elementwise
matrix product  Then 

An Adaptive Test of Independence with Analytic Kernel Embeddings

 

    

             
    consistent estimator for   is      cid 

              

  where
               cid 
                 cid 
 ub                                     
    cid cid       nI

Assume that the complexity of the kernel evaluation is linear in the input dimension  Then the test statistic     
   can be computed in               

       ub cid 

 cid 

   

 dx   dy Jn  time 

Proof  sketch  Claim   for    is straightforward  The expression for   in claim   follows directly from the asymptotic covariance expression in Proposition   The consistency of   can be obtained by noting that the  nite sample
bound for   cid     cid        decreases as   increases  This
is implicitly shown in Appendix    and its following
sections 

Although the dependency of the estimator on   is cubic  we
empirically observe that only   small value of   is required
 see Section   The number of test locations   relates to
the number of regions in       of pxy and pxpy that differ
 see Figure  
Theorem   asserts the consistency of the test for any test
locations VJ drawn from an absolutely continuous distribution  In practice  VJ can be further optimized to increase
the test power for    xed sample size  Our  nal theoretical
result gives   lower bound on the test power of  cid NFSIC      
the probability of correctly rejecting    We will use this
lower bound as the objective function to determine VJ and
the kernel parameters  Let  cid     cid   be the Frobenius norm 
Theorem      lower bound on the test power  Let
NFSIC               nu cid    Let   be   kernel
class for      be   kernel class for    and   be   collection
with each element being   set of   locations  Assume that

exist

  There

such
Bk

and Bl

 nite Bk
supk   supx   cid           cid 
supl   supy   cid           cid    Bl 
       supk   supl   supVJ    cid cid      
 cid 
Then  for any               VJ      and         the test

power satis es   cid      

       where

that
and

 

              

             cid   cid       
     

nn   

                 

 cid cid  is the  oor function     
depending on only Bk and Bl         

   

 

            is   constant
 JB      BkBl 

                                      
      
           and               Moreover  for suf ciently
large  xed         is increasing in    

 
 

 

    

    

    

      

      

 cid     cid 

 cid 
 cid 

the proof

 cid 
 cid 

 cid 
 cid 

   cid wi   wj cid 

   cid VJ
      for all    cid    cid  If we

 cid 
in Appendix    To put
We provide
assume that    
Theorem   into perspective 
 cid 
     
   
  Kg
        cid  exp
 
 
        and    
for some      
       
 cid     cid 
     
   
        cid  exp
  Lg
 
for some      
        are Gaussian kernel
       
classes  Then  in Theorem       Bk   Bl    
and        The assumption        is   technical condition to guarantee that
the test power lower
bound is  nite for all   de ned by the feasible sets
     and    Let    
   cid vi cid cid wi cid   
  and  cid vi   vj cid 
set     Kg     Lg  and         for some         then
       as Kg Lg  and     are compact  In practice  these
conditions do not necessarily create restrictions as they
almost always hold implicitly  We show in Appendix   that
the objective function used to choose VJ will discourage
any two locations to be in the same neighborhood 
Parameter Tuning Let   be the collection of all tuning
parameters of the test  If     Kg and     Lg       Gaussian kernels  then      
   VJ  The test power
lower bound      in Theorem   is   function of     
nu cid   which is the population counterpart of the test
statistic     As in FSIC  it can be shown that        if
and only if   are   are independent  from Proposition  
According to Theorem   for   suf ciently large    the test
power lower bound is increasing in     One can therefore
think of       function of   as representing how easily the
test rejects    given   problem Pxy  The higher the     the
greater the lower bound on the test power  and thus the more
likely it is that the test will reject    when it is false 
In light of this reasoning  we propose to set   by maximizing the lower bound on the test power      set   to
    arg max       Assume that   is suf ciently large
so that     cid       is an increasing function  Then 
arg max         arg max      That this procedure is
also valid under    can be seen as follows  Under   
    arg max    will be arbitrary  Since Theorem   guarantees that   
      as       for any   the asymptotic
null distribution does not change by using   In practice 
   is   population quantity which is unknown  We propose
dividing the sample Zn into two disjoint sets  training and
test sets  The training set is used to compute     an estimate
of     to optimize for   and the test set is used for the actual independence test with the optimized   The splitting
is to guarantee the independence of   and the test sample
to avoid over tting 

 

An Adaptive Test of Independence with Analytic Kernel Embeddings

     xy      

     cid          

     cid      
Figure   Illustration of  cid NFSIC 

    Statistic         

     

 xy     cid         

To better understand the behaviour of  cid NFSIC  we visualize  xy        cid           and        as   function of
one test location        on   simple toy problem  In this
problem             where           is an independent noise variable  As we consider only one location
              is   scalar  The statistic can be written
as       
  These components are
shown in Figure   where we use Gaussian kernels for both
  and     and the horizontal and vertical axes correspond
to       and        respectively 
Intuitively              xy          cid           captures
the difference of the joint distribution and the product of
the marginals as   function of        Squaring         
and dividing it by the variance shown in Figure    gives the
statistic  also the parameter tuning objective  shown in Figure     The latter  gure illustrates that the parameter tuning
objective function can be nonconvex  nonconvexity arises
since there are multiple ways to detect the difference between the joint distribution and the product of the marginals 
In this case  the lower left and upper right regions equally
indicate the largest difference    convex objective would
not be able to capture this phenomenon 

  Experiments
In this section  we empirically study the performance of
the proposed method on both toy  Section   and real
problems  Section   We are interested in challenging problems requiring   large number of samples  where
  quadratictime test might be computationally infeasible  Our goal is not to outperform   quadratictime test
with   lineartime test uniformly over all testing problems 
We will  nd  however  that our test does outperform the
quadratictime test in some cases  Code is available at
https github com wittawatj fsictest 
We compare the proposed NFSIC with optimization  NFSICopt  to  ve multivariate nonparametric tests  The  cid NFSIC 
test without optimization  NFSICmed  acts as   baseline 
allowing the effect of parameter optimization to be clearly

seen  For pedagogical reason  we consider the original HSIC
test of Gretton et al    denoted by QHSIC  which is  
quadratictime test  Nystr   HSIC  NyHSIC  uses   Nystr   approximation to the kernel matrices of   and   when
computing the HSIC statistic  FHSIC is another variant of
HSIC in which   random Fourier feature approximation
 Rahimi   Recht    to the kernel is used  NyHSIC and
FHSIC are studied in Zhang et al    and can be computed in      with quadratic dependency on the number
of inducing points in NyHSIC  and quadratic dependency
on the number of random features in FHSIC  Finally  the
Randomized Dependence Coef cient  RDC  proposed in
LopezPaz et al    is also considered  The RDC can be
seen as the primal form  with random Fourier features  of
the kernel canonical correlation analysis of Bach   Jordan
  on copulatransformed data  We consider RDC as  
lineartime test even though preprocessing by an empirical
copula transform costs   dx   dy   log   
We use Gaussian kernel classes Kg and Lg for both
  and   in all the methods  Except NFSICopt  all
other tests use full sample to conduct the independence
test  where the Gaussian widths    and    are set according to the widely used median heuristic          
median  cid xi   xj cid                   and    is set in
the same way using  yi  
   The   locations for NFSICmed are randomly drawn from the standard multivariate
normal distribution in each trial  For   sample of size   
NFSICopt uses half the sample for parameter tuning  and
the other disjoint half for the test  We permute the sample
  times in RDC  and HSIC to simulate from the null
distribution and compute the test threshold  The null distributions for FHSIC and NyHSIC are given by    nite sum of
weighted   random variables given in Eq    of Zhang
et al    Unless stated otherwise  we set the test threshold of the two NFSIC tests to be the      quantile of
    To provide   fair comparison  we set       use  
inducing points in NyHSIC  and   random Fourier features
in FHSIC and RDC 
Optimization of NFSICopt The parameters of NFSICopt
are         and   locations of size  dx   dy    We treat all
the parameters as   long vector in   dx dy   and use gradient ascent to optimize     We observe that initializing
VJ by randomly picking   points from the training sample
yields good performance  The regularization parameter   
in NFSIC is  xed to   small value  and is not optimized  It is
worth emphasizing that the complexity of the optimization
procedure is still lineartime 

 We use   permutation test for RDC  following the authors  implementation  https github com lopezpaz 
randomized dependence coefficient  referred commit    ac   

 Our claim on linear runtime  with respect to    is for the
gradient ascent procedure to  nd   local optimum for   We do not

An Adaptive Test of Independence with Analytic Kernel Embeddings

    SG      

    SG      

    Sin

    GSign

Figure       Runtime      Probability of rejecting    as problem parameters vary  Fix      

Since FSIC  NyHFSIC and RDC rely on    nitedimensional kernel approximation  these tests are consistent
only if both the number of features increases with    By
constrast  the proposed NFSIC requires only   to go to in 
 nity to achieve consistency        can be  xed  We refer
the reader to Appendix   for   brief investigation of the test
power vs  increasing    The test power does not necessarily
monotonically increase with   

  Toy Problems

We consider three toy problems 
  Same Gaussian  SG  The two variables are independently drawn from the standard multivariate normal distribution              Idx   and         Idy   where Id
is the       identity matrix  This problem represents   case
in which    holds 
  Sinusoid  Sin  Let pxy be the probability density of Pxy 
In the Sinusoid problem  the dependency of   and   is characterized by           pxy             sin    sin   
where the domains of            and   is the frequency of the sinusoid  As the frequency   increases  the
drawn sample becomes more similar to   sample drawn
from Uniform    That is  the higher   the harder
to detect the dependency between   and     This problem
was studied in Sejdinovic et al    Plots of the density
for   few values of   are shown in Figures   and   in the
appendix  The main characteristic of interest in this problem
is the local change in the density function 
 cid dx
  Gaussian Sign  GSign 
In this problem     
   sgn Xi  where         Idx   sgn  is the
   
sign function  and           serves as   source of noise 
The full interaction of                 Xdx   is what makes
the problem challenging  That is    is dependent on   
yet it is independent of any proper subset of             Xd 
Thus  simultaneous consideration of all the coordinates of
  is required to successfully detect the dependency 
We          and vary the problem parameters  Each
problem is repeated for   trials  and the sample is redrawn
each time  The signi cance level   is set to   The re 

claim   linear runtime to  nd   global optimum 

sults are shown in Figure   It can be seen that in the SG
problem  Figure     where    holds  all the tests achieve
roughly correct typeI errors at       In particular 
we point out that NFSICopt   rejection rate is well controlled as the sample used for testing and the sample used
for parameter tuning are independent  The rejection rate
would have been much higher had we done the optimization
and testing on the same sample       over tting  In the
Sin problem  NFSICopt achieves high test power for all
considered                 highlighting its strength in detecting local changes in the joint density  The performance of
NFSICmed is signi cantly lower than that of NFSICopt 
This phenomenon clearly emphasizes the importance of the
optimization to place the locations at the relevant regions in
      RDC has   remarkably high performance in both Sin
and GSign  Figure         despite no parameter tuning  The
ability to simultaneously consider interacting features of
NFSICopt is indicated by its superior test power in GSign 
especially at the challenging settings of dx      
NFSIC vs  QHSIC  We observe that NFSICopt outperforms the quadratictime QHSIC in these two problems 
QHSIC is de ned as the RKHS norm of the witness function    see   Intuitively  one can think of the RKHS
norm as taking into account all the locations        By
contrast  the proposed NFSIC evaluates the witness function
at   locations  If the differences in pxy and pxpy are local
      Sin problem  or there are interacting features      
GSign problem  then only small regions in the space of
        are relevant in detecting the difference of pxy and
pxpy  In these cases  pinpointing exact test locations by
the optimization of NFSIC performs well  On the other
hand  taking into account all possible test locations as done
implicitly in QHSIC also integrates over regions where the
difference between pxy and pxpy is small  resulting in  
weaker indication of dependence  Whether QHSIC is better
than NFSIC depends heavily on the problem  and there is
no one best answer  If the difference between pxy and pxpy
is large only in localized regions  then the proposed linear
time statistic has an advantage  If the difference is spatially
diffuse  then QHSIC has an advantage  No existing work
has proposed   procedure to optimally tune kernel parameters for QHSIC  by contrast  NFSIC has   clearly de ned
objective for parameter tuning 

 dxanddy Time   dxanddy TypeIerror in sin   sin   Testpower dx Testpower dx TestpowerNFSICoptNFSIC medQHSICNyHSICFHSICRDCAn Adaptive Test of Independence with Analytic Kernel Embeddings

    SG  dx   dy    

    SG  dx   dy    

    Sin       

    GSign  dx    

Figure       Runtime      Probability of rejecting    as   increases in the toy problems 

To investigate the sample ef ciency of all the tests  we   
dx   dy     in SG        in Sin  dx     in GSign  and
increase    Figure   shows the results  The quadratic dependency on   in QHSIC makes it infeasible both in terms of
memory and runtime to consider   larger than    Figure     By constrast  although not the most timeef cient 
NFSICopt has the highest sampleef ciency for GSign  and
for Sin in the lowsample regime  signi cantly outperforming QHSIC  Despite the small additional overhead from the
optimization  we are yet able to conduct an accurate test
with       dx   dy     in less than   seconds 
We observe in Figure    that the two NFSIC variants have
correct typeI errors across all sample sizes  We recall from
Theorem   that the NFSIC test with random test locations
will asymptotically reject    if it is false    demonstration
of this property is given in Figure     where the test power
of NFSICmed eventually reaches   with   higher than  

  Real Problems

We now examine the performance of our proposed test on
real problems 
Million Song Data  MSD  We consider   subset of the
Million Song Data   BertinMahieux et al    in which
each song     out of   is represented by   features 
of which   features are timbre average  over all segments 
of the song  and   features are timbre covariance  Most of
the songs are western commercial tracks from   to  
The goal is to detect the dependency between each song and
its year of release      We set       and repeat for
  trials where the full sample is randomly subsampled
to   points in each trial  Other settings are the same as
in the toy problems  To make sure that the typeI error
is correct  we use the permutation approach in the NFSIC
tests to compute the threshold  Figure    shows the test
powers as   increases from   to   To simulate the
case where    holds in the problem  we permute the sample
to break the dependency of   and     The results are shown
in Figure   in the appendix 
Evidently  NFSICopt has the highest test power among all

 Million Song Data subset  https archive ics 

uci edu ml datasets YearPredictionMSD 

    MSD problem 

    Videos   Captions problem 
Figure   Probability of rejecting    as   increases in the
two real problems       

the lineartime tests for all the sample sizes  Its test power
is second to only QHSIC  We recall that NFSICopt uses
half of the sample for parameter tuning  Thus  at      
the actual sample for testing is   which is relatively
small  The fact that there is   vast power gain from  
 NFSICmed  to    NFSICopt  at       suggests that
the optimization procedure can perform well even at   lower
sample sizes 
Videos and Captions Our last problem is based on the
VideoStory    dataset  Habibian et al    The
dataset contains   Youtube videos     of an average length of roughly one minute  and their corresponding
text captions      uploaded by the users  Each video is
represented as   dx     dimensional Fisher vector encoding of motion boundary histograms  MBH  descriptors
of Wang   Schmid   Each caption is represented
as   bag of words with each feature being the frequency
of one word  After  ltering only words which occur in at
least six video captions  we obtain dy     words  We
examine the test powers as   increases from   to  
The results are given in Figure   The problem is suf 
ciently challenging that all lineartime tests achieve   low
power at       QHSIC performs exceptionally well
on this problem  achieving   maximum power throughout 
NFSICopt has the highest sample ef ciency among the
lineartime tests  showing that the optimization procedure is
also practical in   high dimensional setting 

 VideoStory   dataset  https ivi fnwi uva nl 

isis mediamill datasets videostory php 

 Samplesizen Time   Samplesizen TypeIerror Samplesizen Testpower Samplesizen Testpower dx TestpowerNFSICoptNFSIC medQHSICNyHSICFHSICRDC Samplesizen TypeIerrorNFSIC optNFSICmedQHSICNyHSICFHSICRDC Samplesizen Testpower Samplesizen TestpowerAn Adaptive Test of Independence with Analytic Kernel Embeddings

Acknowledgement

We thank the Gatsby Charitable Foundation for the  nancial
support  The major part of this work was carried out while
Zolt   Szab  was   research associate at the Gatsby Computational Neuroscience Unit  University College London 

References
Anderson  Theodore    An Introduction to Multivariate

Statistical Analysis  Wiley   

Bach  Francis    and Jordan  Michael    Kernel independent component analysis  Journal of Machine Learning
Research     

BertinMahieux  Thierry  Ellis  Daniel      Whitman 
Brian  and Lamere  Paul  The million song dataset  In
International Conference on Music Information Retrieval
 ISMIR   

Chwialkowski  Kacper    Ramdas  Aaditya  Sejdinovic 
Dino  and Gretton  Arthur  Fast TwoSample Testing
with Analytic Representations of Probability Measures 
In Advances in Neural Information Processing Systems
 NIPS  pp     

Dauxois  Jacques and Nkiet  Guy Martial  Nonlinear canonical analysis and independence tests  The Annals of Statistics     

Feuerverger  Andrey    consistent test for bivariate dependence  International Statistical Review   
 

Fukumizu  Kenji  Gretton  Arthur  Sun  Xiaohai  and
Sch lkopf  Bernhard  Kernel measures of conditional
dependence  In Advances in Neural Information Processing Systems  NIPS  pp     

Gretton  Arthur and Gy      szl  Consistent nonparametric tests of independence  Journal of Machine Learning
Research     

Gretton  Arthur  Bousquet  Olivier  Smola  Alex  and
Sch lkopf  Bernhard  Measuring Statistical Dependence
with HilbertSchmidt Norms  In Algorithmic Learning
Theory  ALT  pp     

Gretton  Arthur  Fukumizu  Kenji  Teo  Choon    Song 
Le  Sch lkopf  Bernhard  and Smola  Alex      Kernel
Statistical Test of Independence  In Advances in Neural
Information Processing Systems  NIPS  pp   
 

Habibian  Amirhossein  Mensink  Thomas  and Snoek 
Cees GM  Videostory    new multimedia embedding
for fewexample recognition and translation of events  In

ACM International Conference on Multimedia  pp   
 

Heller  Ruth  Heller  Yair  Kaufman  Shachar  Brill  Barak 
and Gor ne  Malka  Consistent distributionfree ksample and independence tests for univariate random
variables  Journal of Machine Learning Research   
   

Huo  Xiaoming and Sz kely    bor    Fast computing
for distance covariance  Technometrics   
 

Jitkrittum  Wittawat  Szab  Zolt    Chwialkowski  Kacper 
and Gretton  Arthur  Interpretable Distribution Features
with Maximum Testing Power  In Advances in Neural
Information Processing Systems  NIPS  pp   
 

Kowalski  Jeanne and Tu  Xin    Modern Applied   

Statistics  John Wiley   Sons   

Lehmann  Eric    Elements of LargeSample Theory 

Springer Science   Business Media   

LopezPaz  David  Hennig  Philipp  and Sch lkopf  Bernhard  The Randomized Dependence Coef cient  In Advances in Neural Information Processing Systems  NIPS 
pp     

Rahimi  Ali and Recht  Benjamin  Random features for
largescale kernel machines  In Advances in Neural Information Processing Systems  NIPS  pp   
 

Sejdinovic  Dino  Sriperumbudur  Bharath  Gretton  Arthur 
and Fukumizu  Kenji  Equivalence of distancebased and
RKHSbased statistics in hypothesis testing  The Annals
of Statistics     

Ser ing  Robert    Approximation Theorems of Mathemati 

cal Statistics  John Wiley   Sons   

Smola  Alex  Gretton  Arthur  Song  Le  and Sch lkopf 
Bernhard    Hilbert space embedding for distributions 
In International Conference on Algorithmic Learning
Theory  ALT  pp     

Sriperumbudur  Bharath    Gretton  Arthur  Fukumizu 
Kenji  Sch lkopf  Bernhard  and Lanckriet  Gert      
Hilbert Space Embeddings and Metrics on Probability
Measures  Journal of Machine Learning Research   
   

Steinwart  Ingo and Christmann  Andreas  Support vector

machines  Springer Science   Business Media   

An Adaptive Test of Independence with Analytic Kernel Embeddings

Sz kely    bor    and Rizzo  Maria    Brownian distance
covariance  The Annals of Applied Statistics   
   

Sz kely    bor    Rizzo  Maria    and Bakirov  Nail   
Measuring and testing dependence by correlation of distances  The Annals of Statistics     

van der Vaart  Aad  Asymptotic Statistics  Cambridge Uni 

versity Press   

Wang  Heng and Schmid  Cordelia  Action recognition with
improved trajectories  In IEEE International Conference
on Computer Vision  ICCV  pp     

Zhang  Kun  Peters  Jonas  Janzing  Dominik  and
Sch lkopf  Bernhard  Kernelbased conditional independence test and application in causal discovery  In Conference on Uncertainty in Arti cial Intelligence  UAI  pp 
   

Zhang  Qinyi  Filippi  Sarah  Gretton  Arthur  and Sejdinovic  Dino  LargeScale Kernel Methods for Independence Testing  Statistics and Computing  pp     

