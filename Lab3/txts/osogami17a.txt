Bidirectional Learning for Timeseries Models with Hidden Units

Takayuki Osogami   Hiroshi Kajino   Taro Sekiyama  

Abstract

Hidden units can play essential roles in modeling
timeseries having longterm dependency or nonlinearity but make it dif cult to learn associated
parameters  Here we propose   way to learn such
  timeseries model by training   backward model
for the timereversed timeseries  where the backward model has   common set of parameters as
the original  forward  model  Our key observation
is that only   subset of the parameters is hard to
learn  and that subset is complementary between
the forward model and the backward model  By
training both of the two models  we can effectively learn the values of the parameters that are
hard to learn if only either of the two models is
trained  We apply bidirectional learning to   dynamic Boltzmann machine extended with hidden
units  Numerical experiments with synthetic and
real datasets clearly demonstrate advantages of
bidirectional learning 

  Introduction
Learning from timeseries data is of paramount importance
for prediction  anomaly detection  classi cation  and other
critical tasks that appear in business and society  Various
models of timeseries have been studied in the literature to
better learn from timeseries data  These include vector autoregressive  VAR  models    utkepohl    hidden Markov
models  HMM   Baum   Petrie    and recurrent neural
networks  RNN   Rumelhart et al    including long
short term memory  LSTM   Hochreiter   Schmidhuber 
  and echo state networks  ESN   Jaeger   Haas   
With these models of timeseries  one seeks to learn the
relation between past values and future values 
In some of these models of timeseries  hidden units  or
latent variables  play essential roles in taking into account
long term dependency or nonlinearity in timeseries  Hid 

 IBM Research   Tokyo  Tokyo  Japan  Correspondence to 

Takayuki Osogami  osogami jp ibm com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

den units  however  make it dif cult to learn the parameters
of those models  For example  the BaumWelch algorithm
 Baum   Petrie    learns the parameters of an HMM
by iteration of expectation and maximization       an EM
algorithm  An RNN  including LSTM  is trained via back
propagation through time  Rumelhart et al    These
algorithms are timeconsuming and do not necessarily  nd
optimal values of the parameters 
This dif culty in learning   model with hidden units partly
stems from the fact that the values of hidden units can only
be reliably estimated after observing future values of target
timeseries  It then requires iteration or back propagation to
learn the relation between the hidden values and preceding
values  An ESN  on the other hand  gives up learning the
hardto learn parameters between hidden values and preceding  visible or hidden  values and set those parameters
randomly  The ESN only learns the relation between visible
values and preceding  hidden  values  Jaeger   Haas   
We study   model of timeseries whose parameters can be
represented as   matrix   or   set of such matrices  An
element Mi   of the matrix may  for example  represent the
weight between   past value of   unit   and   future value of
  unit    In   VAR model  each matrix corresponds to the
coef cients for   particular lag  In an RNN    matrix corresponds to the weight between hidden units  We consider  
situation where it is hard to estimate an appropriate value
of Mi   for some of the units        in particular    may
denote the set of hidden units 
We propose   method of training   timeseries model with
hidden units in   bidirectional manner  one from the past
and the other from the future  From   timeseries model
with parameter    we construct   backward model  whose
parameters are represented by the transposed matrix   cid 
or   set of such transposed matrices  The       th element
of   cid  is Mj    which represents the weight between  
preceding value of   unit   and   succeeding value of  
unit    Our key idea is to let the preceding value represent  
future value and the succeeding value represent   past value
in the backward model  Then an intuitive meaning of Mj  
in the backward model matches that in the original  forward 
model  We use   common matrix   both in the forward
model and in the backward model  Namely  the parameters
are shared between the two models 

Bidirectional Learning for Timeseries Models with Hidden Units

The two models have an identical structure and trained in an
identical manner with   stochastic gradient method  Bottou 
  except that we train the forward model using timeseries in   standard  forward  manner and the backward
model using the timeseries from the future to the past  We
alternately train the forward model to learn   and the backward model to learn   cid  An advantage of our bidirectional
training is that the elements of   that are hard to estimate
differ between when we train the forward model and when
we train the backward model  For the forward model  it
is hard to estimate Mi   for                  For the
backward model  it is hard to estimate Mj   for            
MH       cid    Although MH   is hard to estimate
for both models  the other elements of   can be reliably
estimated in either of the two models  This idea of bidirectional training for learning timeseries models with hidden
units constitutes our  rst contribution 
Here  we extend   Dynamic Boltzmann Machine  DyBM 
to incorporate hidden units and apply bidirectional training
to learn its parameters  The DyBM has been proposed by
Osogami   Otsuka       and subsequently studied by
Dasgupta et al    Dasgupta   Osogami   Kajino   The prior work on the DyBM does not consider
hidden units but instead uses various features of past values
to capture the long term dependency in timeseries  For
example  Dasgupta   Osogami   use an ESN to create nonlinear features of past values  In our context  these
features are fed into visible units of   DyBM  In particular  what features of past values are used is determined
randomly without learning  Our analysis of the DyBM with
hidden units illuminates the dif culty of learning some of
its parameters  With bidirectional learning  we seek to learn
the weight from the past visible values to the future hidden
values  which corresponds to learning what features of past
values are effective for prediction  The DyBM with hidden
units and its analysis constitute our second contribution 
We validate the effectiveness of bidirectional training and
the hidden units in DyBMs through numerical experiments
using synthetic and real timeseries  We will show that the
DyBM with hidden units can be trained more effectively
with bidirectional training and reduces the predictive error
by up to    

  Related Work

Our bidirectional training is related to but different from
bidirectional recurrent neural networks  BRNN   Schuster
  Paliwal    Baldi et al    including bidirectional
LSTM  Graves   Schmidhuber    Chen   Chaudhari 
  Similar to our bidirectional training    BRNN trains
both   forward model and   backward model  These two
models  however  do not share parameters  contrary to our
bidirectional training  In fact  motivation and purpose of the

BRNN are quite different from ours  The BRNN uses both
the sequence from the past and the sequence from the future
to better estimate missing values  The BRNN thus needs
both of the two sequences for learning and for prediction 
On the other hand  our bidirectional training uses both  
forward sequence and   backward sequence at the time
of learning  but the trained model uses only the sequence
from the past to predict future unseen values  Namely  our
bidirectional training is used to learn   model for predicting
future values from past values  while the BRNN is used for
estimating missing values from past and future values 
Our bidirectional training is also related to but different
from Forward Backward Lasso Granger  FBLG  by Cheng
et al    In FBLG  forward and backward VAR models
are estimated with Lasso  and an averaged model is used
to infer the Granger causality  The VAR models in FBLG
do not have hidden units  while our bidirectional training is
motivated by the need for training timeseries models with
hidden units  Winkler et al    also study   backward
VAR in the context of the Granger causality but do not
consider hidden units 
Another related work is structure learning of   VAR model
 Bahadori et al    or   simpler linear dynamical system  Jalali   Sanghavi    that takes into account the
existence of unobserved  or latent  variables  However 
their goal is to reliably estimate the structure of the relation
between observable variables by taking into account the unobserved variables  This is in contrast to the purpose of our
bidirectional training  which aims at learning the relation
between visible units and hidden units 

  DyBM with Hidden Units
We study   particularly structured Boltzmann machine for
timeseries  see Figure   Corresponding to   segment
of   timeseries of length       the Boltzmann machine
in Figure   has       layers in the horizontal  temporal 
direction  Each layer corresponds to   time    for        
    Each layer has two parts  hidden and visible  The visible
part      at the  th layer represents the values of the timeseries at time       The hidden part      represents the
values of hidden units at time       Here  units within
each layer do not have connections to each other  We let
                    and de ne      analogously 
The Boltzmann machine in Figure   has bias parameter  
and weight parameter              Let              
be the parameters connected to visible units       from the
units in the past       or      for        and           
The energy of this Boltzmann machine is given as follows 

                    
                                   

 

Bidirectional Learning for Timeseries Models with Hidden Units

Figure      forward  DyBM with hidden units 

Figure     backward DyBM 

where we de ne
               

     cid          cid 

     cid             cid 

 

 

 

     cid        

and de ne                 from   by letting    
                and            
Similar to the DyBM in Osogami   Otsuka     we
study the case where          and other matrices have the following parametric forms for       
                         
                         

 
 
where   is   decay rate satisfying           Then  in the
limit of       the energy in   can be represented as
follows  and                 has an analogous limit
shown in   of the supplementary material 
               

     cid          cid 

     cid             cid 

     cid        

 

 

     cid                cid          

 
where     is referred to as an eligibility trace in Osogami
  Otsuka     and de ned as follows  here  we de ne
an eligibility trace     for the hidden part analogously 

 cid 

 cid 

     

             

       

 

  

  

The energy in   gives the conditional probability distribution over      given      and      For binaryvalued
timeseries  we have
                 

exp                  

 
 

for any binary vector      where   is the normalization
factor for the probabilities to sum up to one  Due to the
structure in Figure   the values in             
      are
conditionally independent of each other given      and
     so that we can represent
                 

              

 cid 

        

  

             is dewhere the conditional probability pi     
 ned with the energy associated with unit    see Osogami  
Otsuka     For realvalued timeseries  one can de ne
             with   Gaussian
the conditional density pi     
distribution whose mean is given from the energy associated
with unit    Dasgupta   Osogami    Osogami   
Conditional distributions can be de ned analogously for     
 see   and   in the supplementary material 

  Training   DyBM with Hidden Units
Here  we derive   learning rule for   We will also see that
  cannot be trained in an analogous manner 
Our DyBM with binary hidden units gives the probability
of   timeseries             cid    by

      

      

                

 

           cid 

   denotes the summation over all of the possible

values of hidden units from time      cid  to        and

                

 

  cid 

where                  is de ned analogously to  
  and provided in   of the supplementary material  and
we arbitrarily de ne          and           for      cid 

 cid 

  cid 

  

  cid 

where cid 

 Bidirectional Learning for Timeseries Models with Hidden Units

We seek to maximize the log likelihood of   given   by
maximizing   lower bound given by Jensen   inequality 

  cid 
 cid    cid 

  cid 

      

                

       log

                

  cid 

  cid 

log                 

log     

 cid cid 

  

  log

 cid 
 cid 
  cid 

 

  

  

 

      

 cid 

 cid 
 cid 

 

 

 

  cid 

           log                 

 

Now we take the gradient of      with respect to  
     

 
            log                 

 cid 

  cid 

 

  cid 

     

where
           
   

  cid 
  cid 

  cid 

                

  log                 

  cid 

  cid 

 

      cid     cid       cid 

  cid 

  cid cid 

  cid 

     

      

  cid 

 

            

  log                   

Plugging   into the righthand side of   we obtain
     

  cid 

 

           log                 

  cid 

 cid 

  cid 

     

  cid 

where the summation with respect to       is over all of the
possible values of       for           and

               cid 

                

 

The gradient of the lower bound with respect to   is 
     

 
           log                 

 cid 

  cid 

 

  cid 

     

The righthand side of   is   summation of expected
gradients  which suggests   method of stochastic gradient  Namely  at each step    we sample      according to
                and update   on the basis of

This learning rule is equivalent to the one for the model
where all of the units are visible  except that the values for
the hidden units are given by sampled values 
Therefore  the learning rule for   follows directly from Osogami   Otsuka    

                   cid     cid 

                             cid     cid cid 
                             cid     cid cid 
                          cid     cid cid 
                          cid     cid cid 

 
 
 
 
 
for            where  cid     cid  denotes the expected values
of      with respect to    in  

  log                

where

 

  log                 

 

  cid 

Similar to   the expression of   suggests   method
of stochastic gradient  at each time    we sample     
according to                 and update   on
the basis of the following stochastic gradient 
log                 Gt 

 

Gt      cid 

  cid 

  log                

 

Computation of   involves mainly two interrelated inef 
 ciencies  First  although   can be approximately computed using sampled hidden values       in the same way as
  the samples cannot be reused after updating   because
it was sampled from the distribution with the previous parameter  Second  since each summand of Gt  is dependent
on   Gt  also has to be recomputed after each update 
Thus  the computational complexity of   grows linearly
with respect to the length of the timeseries            cid  in
contrast to   whose complexity is independent of that
length  One could approximately compute   recursively 
Gt     Gt          log                

 

Bidirectional Learning for Timeseries Models with Hidden Units

 

  

 

where         is   discount factor  The recursive update
rule with       puts exponentially small weight      on
  log                 computed with an old value of
           cid     This recursively computed Gt is related to
the momentum in gradient descent  Qian    See the
supplementary material for speci   learning rules suggested
by  
Observe in   that       consists of the products
of log                 and   log                
Without
for
the dependency on
log                
the parameter   is updated
in   way that      is more likely to be generated      
the learning rule would be equivalent to that for visible
units  Such an update rule is undesirable  because     
has been sampled and is not necessarily what we want to
sample again  The dependency on log                
suggests that   is updated by   large amount if the sampled
     happens to make the future values       for       
Intuitively  weighting   log                
likely 
by log                 for       is inevitable  because
whether the particular values of hidden units are good for
the purpose of predicting future values will only be known
after seeing future values 

  Learning with Reversed Timeseries
Because the stochastic gradient for   requires approximations that are not needed for   we might not be able to
learn appropriate values of   as effectively as   This motivates us to consider   backward DyBM in Figure   which
has   common set of parameters      as the forward
DyBM in Figure   but de nes the conditional distribution
for timeseries from the future  Speci cally  the energy of
the backward DyBM is represented analogously to   with
the superscript      replaced by      where we de ne

                     cid          cid 

     cid   cid     

 

     cid   cid     

 

    cid 

 

and                 is de ned from   by letting
                    and             Similar
to the forward DyBM  we assume that the weight has the
parametric form of   and let      
Namely  the backward DyBM is obtained from the forward
DyBM by the following changes        cid        cid 
      cid  and       cid  The other difference between  
and   is the sign of   but this is because the backward
DyBM deals with timeseries from the future 
Because the backward DyBM has the structure that is equivalent to that of the forward DyBM  it can be trained in

the same manner as the forward DyBM but using timeseries from the future  Speci cally   cid       cid    cid     in
the backward DyBM is optimized analogously to   in the
forward DyBM  Likewise   cid       cid    cid  is optimized
analogously to   Recall that   and  cid  are relatively hard to
optimize  and   and  cid  are relatively easy to optimize 
Our key observation is that the parameter    which is in
  and is relatively hard to optimize in the forward DyBM 
is in  cid  and is relatively easy to optimize in the backward
DyBM  By training both the forward DyBM and the backward DyBM  we expect to effectively  nd appropriate values
of   and  cid 
Consider   stochastic process      whose distribution is
given by   forward DyBM  We remark that the distribution
of the stochastic process      that is de ned by reversing
     is generally different from what the corresponding
backward DyBM gives unless the DyBMs have no hidden
units  The exact distribution of      needs to be given by
marginalizing out the past values       succeeding values
for the backward process  Despite this discrepancy  we
expect that bidirectional training is effective because of
intuitive correspondence between the forward DyBM and
the backward DyBM  In particular     
    in both DyBMs
represent the strength of the correlation between the past
value of unit   and the future value of unit    where the
time is separated by     recommendation is  however  to
perform backward training more moderately than forward
training  We will show an example of   speci   procedure
in the next section 

  Numerical Experiments
We now demonstrate the effectiveness of bidirectional training through numerical experiments in two settings  The
purpose of the  rst setting is to study whether bidirectional
learning of the DyBM with hidden units can indeed learn to
predict what cannot be done without bidirectional learning
or hidden units  We use   synthetic dataset that is designed
speci cally for this purpose  In the second setting  we study
the effectiveness of bidirectional learning and hidden units
on real datasets  We use the two datasets that have been
used in Dasgupta   Osogami   as well as     dimensional timeseries  which is substantially larger than the
eight or lower dimensional timeseries that are used in the
other experiments  The experiments are carried out with  
Python implementation on workstations having   GB
memory and   GHz CPU 

  Speci   Learning Algorithms to Evaluate

For each dataset  we train   DyBM with or without hidden
units  Because all of the datasets are real valued  visible
units are Gaussian and give predictions by   with      omit 

Bidirectional Learning for Timeseries Models with Hidden Units

Algorithm   Speci   steps of bidirectional learning evaluated in experiments

    The total number of iterations
   The number of iterations of bidirectional learning
    The relative frequency of forward learning
for       to   do

if        and   mod           then

Backward learning to update          

else

Forward learning to update          

end if
end for

ted  see   in the supplementary material  To reduce the
variability in the experiments  we use the expected value
for the output of   hidden unit instead of sampling   binary
value  By the law of large numbers  the use of expected
value corresponds to having an in nitely many binary hidden units that are conditionally independent and identically
distributed         given the internal state of the DyBM  See
also  Sutskever et al    Sutskever   Hinton    for
the related use of the expected values for hidden units 
  DyBM with hidden unit is trained bidirectionally or only
with forward learning    DyBM without hidden units is
trained only with forward learning  While bidirectional
learning has several design choices  here we evaluate the
speci   algorithm shown in Algorithm   In particular  we
perform bidirectional learning for the  rst    iterations 
where the backward training is apply every       steps  For
the rest of        iterations  we perform forward learning
only  Throughout we set       Note that   is  xed with
its initial values throughout learning  Here we do not update
  in forward learning and   in backward learning  This is
partly because the learning rule of   has no effect
when we use the expected values in hidden units 
Before applying Algorithm   the bias   is initialized to
zero  and the weight              is initialized with       
normal random variable with mean   and standard deviation
of    Hinton    except the following two changes 
First  we set the mean of    as an identity matrix for real
datasets  because using the previous value for prediction
is clearly bene cial for these datasets  Second  we use the
small standard deviation of   for the large dataset of
  dimensions for faster convergence  The learning rate is
adjusted according to AdaGrad  Duchi et al    where
the the initial learning rate is optimized as we discuss in the
following 
Throughout the experiments  we set the decay rate of the
eligibility traces in   to zero               and
             The delay   and the number of hidden
units are varied for each dataset 

  Synthetic Data

We  rst demonstrate the effectiveness of our bidirectional
training in   synthetic setting of learning   onedimensional
noisy sawtooth wave  which is generated according to

 cid 

 cid   

 

      

 
 

      for              

 

where   is the period of the noisy sawtooth wave  and    is
an        normal random variable  whose mean is  xed at  
and standard deviation at   The noisy sawtooth wave has
large discontinuity at the end of each period  which makes
hidden units essential for learning 
Here we train   DyBM with one hidden unit or no hidden
units  Throughout  the delay is set       and the learning rate is initialized to       Bidirectional learning
is continued for          iterations  where   is varied
depending on    In each iteration  we use one period of
the noisy sawtooth wave and update the parameters with
stochastic gradients 
Figure     and     show how learning progresses over iterations  The period of the noisy sawtooth wave is       in
    and       in     Training is continued for        
iterations for       and         iterations for      
In every       iterations  we let the DyBM predict the
onestep ahead value for each of the   steps of one period
during forward learning  We evaluate the root mean squared
error  RMSE  of onestep ahead predictions against true
values  For clarity  the RMSE curves are smoothed with  
Gaussian  lter with window size of  
In the  gure  the solid curves show the results with the bidirectionally trained DyBMs  Bidirectional  As   baseline 
we also train the DyBM only with forward training and
show the results with dashed curves  Baseline  The dotted
curves show the results with the DyBM with no hidden units
 No hidden  The comparison suggests that Bidirectional
can substantially  by   factor of   improve the predictive
accuracy over Baseline  Notice also that the hidden unit can
hurt the predictive accuracy without bidirectional training 
No hidden often exhibits lower RMSE than Baseline 
In Figure     the reduction of the RMSE accelerates at
   iterations  after which bidirectional learning is no longer
performed  This suggests that  after learning appropriate
values of    namely  what features should be used for prediction  via bidirectional learning  it is better to optimally
learn           given the learned values of    Namely 
although bidirectional learning help learn appropriate values
of    it does not necessarily optimize all of the parameters
of the DyBM  Recall also the discussion at the end of Section   that the backward DyBM is not exactly the same as
the timereversed DyBM 
Figure     and     show the values predicted by bidirection 

Bidirectional Learning for Timeseries Models with Hidden Units

    RMSE       

    Prediction       

    RMSE       

    Prediction       

Figure   Experimental results with noisy sawtooth waves  Bidirectional is   bidirectionally trained DyBM with one hidden unit  Baseline
is   DyBM with hidden units trained only with forward learning  and No hidden is   bidirectionally trained DyBM without hidden units 
In     and     the RMSE of onestep ahead prediction with respect to training data is plotted against the number of iterations of training 
In     and     the predicted values after training is plotted for two periods together with corresponding target values  red curves 

ally trained DyBMs  black curves  and the corresponding
target values  red curves  For clarity  we use   noiseless
sawtooth wave as the target by letting        in   Observe that the bidirectionally trained DyBM well predicts
the next value of the sawtooth wave  substantially better
than the baseline  or the DyBM with no hidden unit  In particular  the bidirectionally trained DyBM can well predict
the sharp drop at the end of each period  This is in contrast
to the baseline  whose prediction is rather smoothed out
over the period 

  Real Data

Next  we demonstrate the effectiveness of bidirectional training on three real datasets  including the two datasets used
in Dasgupta   Osogami   The  rst dataset is the
monthly sunspot number  which we will refer to as Sunspot 
This timeseries has one dimension and   steps  corresponding to January   to December   The second dataset is the weekly retail gasoline and diesel prices 
which we will refer to as Price  This timeseries has eight
dimensions  corresponding to eight locations in the US  and
  steps  corresponding to April  th    to September
 th    Following Dasgupta   Osogami   the
 rst     of each timeseries is used for training  and the
remaining     is used for test  See Dasgupta   Osogami
  for further details about the  rst two datasets  The
third dataset is the NOAA Global Surface Temperature 
which consists of   real valued timeseries of   steps
                 with   dimensions  We use the  rst
    of the timeseries for training and the remaining    
for test 
We normalize the values of each dataset in   way that the

 https datamarket com data set   
 https www eia gov dnav pet pet pri 

gnd   epm pte dpgal   htm

    of Air Temperature

from
https www esrl noaa gov psd data gridded 
data noaaglobaltemp html

 air mon anom nc 

values in   training data are in   for each dimension 
Notice that this normalization differs from that in  Dasgupta
  Osogami    where the values in training or test data
are in   for each dimension  However  the use of test
data for normalization is less appropriate  This difference
in normalization has no effect on the Price dataset  but the
results on the Sunspot dataset need to be renormalized to be
compared against those in  Dasgupta   Osogami   
Here we train   DyBM with four hidden units or no hidden
units  In each iteration  we use the whole training data  except the  rst   steps for forward learning and the last   steps
for backward learning  which are used only to update the
internal state of the DyBM  once and update the parameters
with stochastic gradients 
We  nd that the speed of convergence is sensitive to the
initial learning rate  Here  we choose the initial learning
rate from             Speci cally  we choose   
with the smallest   such that the training RMSE after the
initial     iterations is smaller than that with    
Because the training RMSE tends to decrease with   up to
  point and then increases with    we usually choose the
initial learning rate that minimizes the training RMSE after
those initial iterations 
Figure   shows the RMSE of onestep ahead prediction with
respect to the test data after every       iterations of training  We show the results where the delay is set       for
Sunspot        for Price  and       for Temperature  However  we have also run experiments with         for
Sunspot and         for Price and have found that these
do not improve the accuracy for any of the three methods 
For the large dataset of Temperature  we have been able
to perform limited experiments due to its relatively heavy
computational requirements  Again  we compare Bidirectional against Baseline and No hidden  However  we now
vary                  so that each  gure has three solid
curves  The range of the vertical axis is chosen in   way
that the upper limit corresponds to the RMSE with   na ve

 Number of iterations Training RMSEBidirectionalNo hiddenBaseline Steps Target or predicted valueBidirectionalNo hiddenTargetBaseline Number of iterations Training RMSEBidirectionalNo hiddenBaseline Steps Target or predicted valueBidirectionalNo hiddenTargetBaselineBidirectional Learning for Timeseries Models with Hidden Units

    Sunspot dataset       

    Price dataset       

    Temperature dataset       

Figure   Experimental results with real datasets  Bidirectional is   bidirectionally trained DyBM with four hidden units  Baseline is
  DyBM with four hidden units trained only with forward learning  and No hidden is   DyBM with no hidden units trained only with
forward learning  The RMSE of onestep ahead prediction with respect to test data is plotted against the number of iterations of training 

prediction of using the preceding values as prediction 
For the Sunspot dataset     we  nd that Bidirectional does
not improve upon Baseline  although having hidden units
 Bidirectional and Baseline  can make the RMSE lower than
No hidden  This means that the randomly set values of  
is effective  and bidirectional learning does not  nd better
values of    After   iterations  however  Bidirectional
with          or          achieves the RMSE that is
essentially indistinguishable from that with Baseline  The
best RMSE achieved by the Baseline is   which corresponds to   when the dataset is normalized in the way
of  Dasgupta   Osogami    and is lower than  
reported in  Dasgupta   Osogami   
For Price     and Temperature     Bidirectional improves
upon Baseline particularly when the bidirectional learning
is stopped after        iterations  Bidirectional reduces the
RMSE more slowly than Baseline or No hidden but eventually outperforms the others  and the reduction of the RMSE
can be accelerated by stopping the bidirectional training after        iterations  In particular  the best RMSE achieved
by Bidirectional with          is   which is lower
than   reported in  Dasgupta   Osogami    In
addition  while Baseline and No hidden  namely  forward
learning only  starts over tting to training data and increases
the RMSE with respect to the test data after some iterations 
bidirectional training appears to avoid such over   

  Conclusion
We have proposed bidirectional training for timeseries models with hidden units  Namely  we consider two models
that have   common set of parameters  where one model is
trained with forward timeseries and the other with backward timeseries  Our key idea is that some of the parameters that are dif cult to learn in one model can be effectively
learned in the other model  Numerical experiments suggest that bidirectional training has the additional effect of

avoiding over   to training data 
The DyBM with hidden units analyzed in Sections   is
new  and its analysis has two highlights  which have led
to proposing bidirectional training  The  rst highlight is
that the learning rule for    hiddento visible weight  in
  becomes equivalent to those for    visibleto 
visible weight  in   when the lower bound   is
maximized  The second highlight is that we cannot learn
the weight to hidden units            in the same way as
the weight to visible units               due to the form
of the gradient in  
Although we have demonstrated the effectiveness of bidirectional training in speci   cases  its capabilities are not fully
explored  Bidirectional training has many design choices
that need further study  For example  one might want to use
the gradients in   possibly with the approximation
in   to update        in forward learning and       
in backward learning  It would also be interesting to apply
bidirectional training to other timeseries models having
parameters that represent the dependency between hidden
values at one time and visible values at another time  In
addition to DyBM and VAR with hidden units  these include
Spiking Boltzmann Machine  Hinton   Brown    and
Conditional Restricted Boltzmann Machine  Taylor et al 
  Because bidirectional training is largely complementary to other techniques for learning timeseries  it would
be interesting to investigate how bidirectional training improves performance when it is combined with these other
techniques  We expect that this work opens up   line of research on more effective methods of bidirectional training 

Acknowledgments
This work was supported by JST CREST Grant Number
JPMJCR  Japan 

 Number of iterations Test RMSENo hiddenBaselineBidirectional Number of iterations Test RMSENo hiddenBaselineBidirectional Number of iterations Test RMSENo hiddenBaselineBidirectionalBidirectional Learning for Timeseries Models with Hidden Units

References
Bahadori        Liu     and Xing        Fast structure learning in generalized stochastic processes with latent factors 
In Proceedings of the  th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining 
pp     

Baldi     Brunak     Frasconi     Soda     and Pollastri 
   Exploiting the past and the future in protein secondary
structure prediction  Bioinformatics   
 

Baum        and Petrie     Statistical inference for probabilistic functions of  nite state Markov chains  The
Annals of Mathematical Statistics     

Bottou     Online learning and stochastic approximations 
In Saad      ed  OnLine Learning in Neural Networks 
chapter   pp    Cambridge University Press   

Chen     and Chaudhari     Protein secondary structure
prediction with bidirectional LSTM networks  In International Joint Conference on Neural Networks  Postconference Workshop on Computational Intelligence Approaches for the Analysis of Biodata  CIBIO  August
 

Cheng     Bahadori        and Liu     FBLG    simple
and effective approach for temporal dependence discovery from time series data  In Proceedings of the  th
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining  pp     

Dasgupta     and Osogami     Nonlinear dynamic Boltzmann machines for timeseries prediction  In The  st
AAAI Conference on Arti cial Intelligence  AAAI  pp 
  January  

Dasgupta     Yoshizumi     and Osogami     Regularized dynamic Boltzmann machine with delay pruning for
unsupervised learning of temporal sequences  In Proceedings of the  rd International Conference on Pattern
Recognition   

Duchi     Hazan     and Singer     Adaptive subgradient
methods for online learning and stochastic optimization 
Journal of Machine Learning Research   
 

Graves     and Schmidhuber     Framewise phoneme classi cation with bidirectional LSTM networks and other
neural network architectures  Neural Networks   
   

Hinton          practical guide to training restricted Boltzmann machines  In Montavon     Orr        and   uller 

      eds  Neural Networks  Tricks of the Trade  chapter   pp    SpringerVerlag Berlin Heidelberg 
 

Hinton        and Brown        Spiking Boltzmann machines  In Advances in Neural Information Processing
Systems   pp    November  

Hochreiter     and Schmidhuber     Long shortterm memory 

Neural Computation     

Jaeger     and Haas     Harnessing nonlinearity  Predicting
chaotic systems and saving energy in wireless communication  Science     

Jalali     and Sanghavi     Learning the dependence graph
of time series with latent factors  In Proceedings of the
 th International Conference on Machine Learning  pp 
   

Kajino       functional dynamic Boltzmann machine  In
Proceedings of the  th International Joint Conference
on Arti cial Intelligence  August  

  utkepohl     New Introduction to Multiple Time Series

Analysis  SpringerVerlag Berlin Heidelberg   

Osogami    

Learning binary or realvalued timeseries via spiketiming dependent plasticity  CoRR 
abs    URL http arxiv org 
abs 

Osogami     and Otsuka     Seven neurons memorizing
sequences of alphabetical images via spiketiming dependent plasticity  Scienti   Reports       

Osogami     and Otsuka     Learning dynamic Boltzmann
machines with spiketiming dependent plasticity  Technical Report RT  IBM Research     

Qian     On the momentum term in gradient descent learning algorithms  Neural Networks  The Of cial Journal of
the International Neural Network Society   
 

Rumelhart        Hinton        and Williams        LearnIn
ing internal representations by error propagation 
Rumelhart        McClelland        and PDP Research
Group  eds  Parallel Distributed Processing  Explorations in the Microstructure of Cognition  vol    Foundations  chapter   MIT Press   

Schuster     and Paliwal        Bidirectional recurrent neural networks  IEEE Transactions on Signal Processing 
   

Sutskever     and Hinton        Learning multilevel distributed representations for highdimensional sequences 
In Proceedings of the Eleventh International Conference

Bidirectional Learning for Timeseries Models with Hidden Units

on Arti cial Intelligence and Statistics  AISTATS  volume   pp    Journal of Machine Learning Research   Proceedings Track   

Sutskever     Hinton        and Taylor        The recurrent
In Advances
temporal restricted Boltzmann machine 
in Neural Information Processing Systems   pp   
  December  

Taylor        Hinton        and Roweis        Modeling
human motion using binary latent variables  In Advances
in Neural Information Processing Systems   pp   
   

Winkler     Panknin     Bartz       uller       and Haufe 
   Validity of time reversal for testing granger causality 
IEEE Transactions on Signal Processing   
   

