Decoupled Neural Interfaces using Synthetic Gradients

    

hA  

    

 

SB

MA  

 

    

    

    

 
 

SB

MA  

    

hA  

 

 

fi 

   

fi 

  

fi

   

  

   

 

 

   

Max Jaderberg   Wojciech Marian Czarnecki   Simon Osindero   Oriol Vinyals   Alex Graves   David Silver  

Koray Kavukcuoglu  

Abstract

Training directed neural networks typically requires forwardpropagating data through   computation graph  followed by backpropagating error signal  to produce weight updates  All layers  or more generally  modules  of the network
are therefore locked  in the sense that they must
wait for the remainder of the network to execute
forwards and propagate error backwards before
they can be updated  In this work we break this
constraint by decoupling modules by introducing   model of the future computation of the network graph  These models predict what the result of the modelled subgraph will produce using
only local information  In particular we focus on
modelling error gradients  by using the modelled
synthetic gradient in place of true backpropagated error gradients we decouple subgraphs 
and can update them independently and asynchronously      we realise decoupled neural interfaces  We show results for feedforward models  where every layer is trained asynchronously 
recurrent neural networks  RNNs  where predicting one   future gradient extends the time over
which the RNN can effectively model  and also
  hierarchical RNN system with ticking at different timescales  Finally  we demonstrate that in
addition to predicting gradients  the same framework can be used to predict inputs  resulting in
models which are decoupled in both the forward
and backwards pass   amounting to independent
networks which colearn such that they can be
composed into   single functioning corporation 

 DeepMind  London  UK  Correspondence to  Max Jaderberg

 jaderberg google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Legend 

Forward connection  
update locked

Forward connection  
not update locked

Error gradient

Synthetic error  
gradient

  

fB
 

SB

MB

 

hA
fA

   

Figure   General communication protocol between   and    After receiving the message hA from      can use its model of   
MB  to send back synthetic gradients    which are trained to approximate real error gradients     Note that   does not need to
wait for any extra computation after itself to get the correct error gradients  hence decoupling the backward computation  The
feedback model MB can also be conditioned on any privileged information or context     available during training such as   label 

  Introduction
Each layer  or module  in   directed neural network can be
considered   computation step  that transforms its incoming data  These modules are connected via directed edges 
creating   forward processing graph which de nes the  ow
of data from the network inputs  through each module  producing network outputs  De ning   loss on outputs allows
errors to be generated  and propagated back through the
network graph to provide   signal to update each module 
This process results in several forms of locking  namely 
    Forward Locking   no module can process its incoming data before the previous nodes in the directed forward
graph have executed   ii  Update Locking   no module can
be updated before all dependent modules have executed in
forwards mode  also  in many creditassignment algorithms
 including backpropagation  Rumelhart et al    we
have  iii  Backwards Locking   no module can be updated
before all dependent modules have executed in both forwards mode and backwards mode 
Forwards  update  and backwards locking constrain us to
running and updating neural networks in   sequential  synchronous manner  Though seemingly benign when training

Decoupled Neural Interfaces using Synthetic Gradients

simple feedforward nets  this poses problems when thinking about creating systems of networks acting in multiple
environments at different and possibly irregular or asynchronous timescales  For example  in complex systems
comprised of multiple asynchronous cooperative modules
 or agents  it is undesirable and potentially unfeasible that
all networks are update locked  Another example is   distributed model  where part of the model is shared and used
by many downstream clients   all clients must be fully executed and pass error gradients back to the shared model
before the model can update  meaning the system trains
as fast as the slowest client  The possibility to parallelise
training of currently sequential systems could hugely speed
up computation time 
The goal of this work is to remove update locking for neural
networks  This is achieved by removing backpropagation 
To update weights    of module   we drastically approximate the function implied by backpropagation 

  
  

  fBprop hi  xi  yi          

 hi
  

   fBprop hi 

 hi
  

where   are activations    are inputs    is supervision  and
  is the overall loss to minimise  This leaves dependency
only on hi   the information local to module   
The premise of this method is based on   simple protocol for learnt communication  allowing neural network
modules to interact and be trained without update locking 
While the communication protocol is general with respect
to the means of generating   training signal  here we focus on   speci   implementation for networks trained with
gradient descent   we replace   standard neural interface   
connection between two modules in   neural network  with
  Decoupled Neural Interface  DNI  Most simply  when  
module         layer  sends   message  activations  to another module  there is an associated model which produces
  predicted error gradient with respect to the message immediately  The predicted gradient is   function of the message alone  there is no dependence on downstream events 
states or losses  The sender can then immediately use these
synthetic gradients to get an update  without incurring any
delay  And by removing updateand backwards locking
in this way  we can train networks without   synchronous
backward pass  We also show preliminary results that extend this idea to also remove forward locking   resulting in
networks whose modules can also be trained without   synchronous forward pass  When applied to RNNs we show
that using synthetic gradients allows RNNs to model much
greater time horizons than the limit imposed by truncating backpropagation through time  BPTT  We also show
that using synthetic gradients to decouple   system of two
RNNs running at different timescales can greatly increase

training speed of the faster RNN 
Our synthetic gradient model
is most analogous to  
value function which is used for gradient ascent  Baxter   Bartlett    or critics for training neural networks  Schmidhuber    Most other works that aim
to remove backpropagation do so with the goal of performing biologically plausible credit assignment  but this
doesn   eliminate update locking between layers       target propagation  Lee et al    Bengio    removes
the reliance on passing gradients between layers  by instead generating target activations which should be  tted
to  However these targets must still be generated sequentially  propagating backwards through the network and layers are therefore still updateand backwardslocked  Other
algorithms remove the backwards locking by allowing loss
or rewards to be broadcast directly to each layer        REINFORCE  Williams     considering all activations are
actions  Kickback  Balduzzi et al    and Policy Gradient Coagent Networks  Thomas      but still remain
update locked since they require rewards to be generated
by an output  or   global critic  While RealTime Recurrent Learning  Williams   Zipser    or approximations
such as  Ollivier   Charpiat    Tallec   Ollivier   
may seem   promising way to remove update locking  these
methods require maintaining the full  or approximate  gradient of the current state with respect to the parameters 
This is inherently not scalable and also requires the optimiser to have global knowledge of the network state  In
contrast  by framing the interaction between layers as   local communication problem with DNI  we remove the need
for global knowledge of the learning system  Other works
such as  Taylor et al    CarreiraPerpin an   Wang 
  allow training of layers in parallel without backpropagation  but in practice are not scalable to more complex
and generic network architectures 

  Decoupled Neural Interfaces
We begin by describing the highlevel communication protocol that is used to allow asynchronously learning agents
to communicate 
As shown in Fig    Sender   sends   message hA to Receiver      has   model MB of the utility of the message hA      model of utility MB is used to predict the
feedback  an error signal      MB hA  sB     based on
the message hA  the current state of    sB  and potentially
any other information     that this module is privy to during training such as the label or context  The feedback   
is sent back to   which allows   to be updated immediately  In time    can fully evaluate the true utility    of the
message received from    and so     utility model can be
updated to    the true utility  reducing the disparity between
   and    

Decoupled Neural Interfaces using Synthetic Gradients

Figure       An RNN trained with truncated BPTT using DNI to
communicate over time  Every timestep   recurrent core takes
input and produces   hidden state ht and output yt which affects
  loss Lt  The core is unrolled for   steps  in this  gure    
  Gradients cannot propagate across the boundaries of BPTT 
which limits the time dependency the RNN can learn to model 
However  the recurrent core includes   synthetic gradient model
which produces synthetic gradients    which can be used at the
boundaries of BPTT to enable the last set of unrolled cores to
communicate with the future ones      In addition  as an auxiliary
task  the network can also be asked to do future synthetic gradient
prediction  an extra output      is computed every timestep  and
is trained to minimise            Tk 

This protocol allows   to send messages to   in   way that
  and   are update decoupled     does not have to wait
for   to evaluate the true utility before it can be updated  
and   can still learn to send messages of high utility to   
We can apply this protocol to neural networks communicating  resulting in what we call Decoupled Neural Interfaces  DNI  For neural networks  the feedback error signal
   can take different forms       gradients can be used as
the error signal to work with backpropagation  target messages as the error signal to work with target propagation  or
even   value  cumulative discounted future reward  to incorporate into   reinforcement learning framework  However  as   clear and easily analysable set of  rst steps into
this important and mostly unexplored domain  we concentrate our empirical study on differentiable networks trained
with backpropagation and gradientbased updates  Therefore  we focus on producing error gradients as the feedback
   which we dub synthetic gradients 

Notation To facilitate our exposition  it   useful to introduce some notation  Without loss of generality  consider
neural networks as   graph of function operations     nite
chain graph in the case of   feedforward models  an in 
nite chain in the case of recurrent ones  and more generally
  directed acyclic graph  The forward execution of the network graph has   natural ordering due to the input dependencies of each functional node  We denote the function
corresponding to step   in   graph execution as fi and the
composition of functions       the forward graph  from step
  to step   inclusive as    
    We denote the loss associated
with layer     of the chain as Li 

  Synthetic Gradient for Recurrent Networks
We begin by describing how our method of using synthetic
gradients applies in the case of recurrent networks  in some
ways this is simpler to reason about than feedforward networks or more general graphs 
An RNN applied to in nite stream prediction can be
viewed as an in nitely unrolled recurrent core module  
with parameters   such that the forward graph is     
 fi    where fi        and the core module propagates an output yi and state hi based on some input xi 
yi  hi   fi xi  hi 
At   particular point

in time   we wish to minimise

           Of course  one cannot compute an update of the
form               

  due to the in nite future time
   
Instead  generally one considers   tractable

dependency 
time horizon  

     

   
 

      

       

       

  TX    
  TX    

   
 

   
 

   

        

   
 hT

 

 hT
 

 

    

 hT
 

 

    

and as in truncated BPTT  calculatesPt  

  with back 
   
propagation and approximates the remaining terms  beyond
        by using        This limits the time horizon over
which updates to   can be learnt  effectively limiting the
amount of temporal dependency an RNN can learn  The
approximation that        is clearly naive  and by using
an appropriately learned approximation we can hope to do
better  Treating the connection between recurrent cores at
time       as   Decoupled Neural Interface we can approximate      with      MT  hT       learned approximation
of the future loss gradients   as shown and described in
Fig       
This amounts to taking the in nitely unrolled RNN as the
full neural network      and chunking it into an in nite
number of subnetworks where the recurrent core is unrolled for   steps  giving       
  Inserting DNI between
two adjacent subnetworks       
allows
the recurrent network to learn to communicate to its future
self  without being update locked to its future self  From
the view of the synthetic gradient model  the RNN is predicting its own error gradients 
The synthetic gradient model      MT  hT   is trained
to predict the true gradients by minimising   distance
          to the target gradient      in practice we  nd
   distance to work well  The target gradient is ideally the
  but as this is
not   tractable target to obtain  we can use   target gradient
that is itself bootstrapped from   synthetic gradient and then
backpropagated and mixed with   number of steps of true

true gradient of future loss         

and       

   
 hT

   

 

 

hA  

 

MB

 

hA  

 

Decoupled Neural Interfaces using Synthetic Gradients

 

 

 

update rule

fi 

   

fi 

  

fi

   

  

   

 

fi 

fi 

fi

Mi 

  

fi 

fi 

fi

Mi 

   
Mi 

  

 

   

 
   

 

   

Figure         section of   vanilla feedforward neural network
        Incorporating one synthetic gradient model for the outF  
  and    
put of layer    This results in two subnetworks    
  
which can be updated independently      Incorporating multiple
synthetic gradient models after every layer results in   independently updated layers 

      

   
 hT

    
 hT

    

gradient              

  This bootstrapping is exactly analogous to bootstrapping value functions in reinforcement learning and allows temporal credit
assignment to propagate beyond the boundary of truncated
BPTT 
This training scheme can be implemented very ef ciently
by exploiting the recurrent nature of the network  as shown
in Fig    in the Supplementary Material 
In Sect   
we show results on sequenceto sequence tasks and language modelling  where using synthetic gradients extends
the time dependency the RNN can learn 

Auxiliary Tasks We also propose an extension to aid
learning of synthetic gradient models for RNNs  which is to
introduce another auxiliary task from the RNN  described
in Fig        This extra prediction problem is designed to
promote coupling over the maximum time span possible 
requiring the recurrent core to explicitly model short term
and long term synthetic gradients  helping propagate gradient information backwards in time  This is also shown to
further increase performance in Sect   

  Synthetic Gradient for FeedForward Networks
As another
illustration of DNIs  we now consider
feedforward networks consisting of   layers fi     
             each taking an input hi  and producing an
output hi   fi hi  where        is the input data  The
forward execution graph of the full network can be denoted
as as    
De ne the loss imposed on the output of the network as
    LN  Each layer fi has parameters    that can be
trained jointly to minimise   hN   with   gradientbased

      section of which is illustrated in Fig       

              

 hi
  

      

  
 hi

where   is the learning rate and   
is computed with back 
 hi
propagation  The reliance on    means that the update to
layer   can only occur after the remainder of the network 
        
    the subnetwork of layers between layer      
and layer   inclusive  has executed   full forward pass 
generated the loss   hN   then backpropagated the gradient through every successor layer in reverse order  Layer  
is therefore update locked to    
  
   we can use
To remove the update locking of layer   to    
the communication protocol described previously  Layer
  sends hi to layer       which has   communication
model Mi  that produces   synthetic error gradient     
Mi hi  as shown in Fig        which can be used immediately to update layer   and all the other layers in    

 

              

 hi
  

                  

To train the parameters of the synthetic gradient model
Mi  we simply wait for the true error gradient    to be
computed  after   full forwards and backwards execution
of    
   and    the synthetic gradient to the true gradients
by minimising        ik 
 
Furthermore  for   feedforward network  we can use synthetic gradients as communication feedback to decouple
every layer in the network  as shown in Fig        The
execution of this process is illustrated in Fig    in the Supplementary Material 
In this case  since the target error
gradient    is produced by backpropagating     through
layer          is not the true error gradient  but an estimate
bootstrapped from synthetic gradient models later in the
network  Surprisingly  this does not cause errors to compound and learning remains stable even with many layers 
as shown in Sect   
Additionally  if any supervision or context   is available
at the time of synthetic gradient computation  the synthetic gradient model can take this as an extra input      
Mi hi    
This process allows   layer to be updated as soon as   forward pass of that layer has been executed  This paves the
way for subparts or layers of networks to be trained in an
asynchronous manner  something we show in Sect   

  Arbitrary Network Graphs
Although we have explicitly described the application of
DNIs for communication between layers in feedforward
networks  and between recurrent cores in recurrent networks  there is nothing to restrict the use of DNIs for arbitrary network graphs  The same procedure can be applied

Decoupled Neural Interfaces using Synthetic Gradients

to any network or collection of networks  any number of
times  An example is in Sect    where we show communication between two RNNs  which tick at different rates 
where the communication can be learnt by using synthetic
gradients 

  Mixing Real   Synthetic Gradients
In this paper we focus on the use of synthetic gradients to
replace real backpropagated gradients in order to achieve
update unlocking  However  synthetic gradients could also
be used to augment real gradients  Mixing real and synthetic gradients results in BP   an algorithm anolgous to
     for reinforcement learning  Sutton   Barto   
This can be seen as   generalized view of synthetic gradients  with the algorithms given in this section for update unlocked RNNs and feedforward networks being speci   instantiations of BP   This generalised view is discussed
further in Sect    in the Supplementary Material 

  Experiments
In this section we perform empirical expositions of the use
of DNIs and synthetic gradients   rst by applying them to
RNNs in Sect    showing that synthetic gradients extend
the temporal correlations an RNN can learn  Secondly  in
Sect    we show how   hierarchical  twotimescale system of networks can be jointly trained using synthetic gradients to propagate error signals between networks  Finally  we demonstrate the ability of DNIs to allow asynchronous updating of layers   feedforward network in
Sect    More experiments can be found in Sect    in
the Supplementary Material 

  Recurrent Neural Networks
Here we show the application of DNIs to recurrent neural
networks as discussed in Sect    We test our models on
the Copy task  Repeat Copy task  as well as characterlevel
language modelling 
For all experiments we use an LSTM  Hochreiter  
Schmidhuber    of the form in  Graves    whose
output is used for the task at hand  and additionally as input to the synthetic gradient model  which is shared over
all timesteps  The LSTM is unrolled for   timesteps after
which backpropagation through time  BPTT  is performed 
We also look at incorporating an auxiliary task which predicts the output of the synthetic gradient model   steps in
the future as explained in Sect    The implementation
details of the RNN models are given in Sect     in the
Supplementary Material 

Copy and Repeat Copy We  rst look at two synthetic
tasks   Copy and Repeat Copy tasks from  Graves et al 

  Copy involves reading in   sequence of   characters and after   stop character is encountered  must repeat
the sequence of   characters in order and produce    nal
stop character  Repeat Copy must also read   sequence of
  characters  but after the stop character  reads the number     which indicates the number of times it is required
to copy the sequence  before outputting    nal stop character  Each sequence of reading and copying is an episode 
of length Ttask         for Copy and Ttask           for
Repeat Copy 
While normally the RNN would be unrolled for the length
of the episode before BPTT is performed      Ttask  we
wish to test the length of time the RNN is able to model
with and without DNI bridging the BPTT limit  We therefore train the RNN with truncated BPTT             
with and without DNI  where the RNN is applied continuously and across episode boundaries  For each problem 
once the RNN has solved   task with   particular episode
length  averaging below   bits error  the task is made
harder by extending   for Copy and Repeat Copy  and also
  for Repeat Copy 
Table   gives the results by reporting the largest Ttask that
is successfully solved by the model  The RNNs without
DNI generally perform as expected  with longer BPTT resulting in being able to model longer time dependencies 
However  by introducing DNI we can extend the time dependency that is able to be modelled by an RNN  The additional computational complexity is negligible but we require an additional recurrent core to be stored in memory
 this is illustrated in Fig    in the Supplementary Material  Because we can model larger time dependencies with
  smaller     our models become more dataef cient  learning faster and having to see less data samples to solve  
task  Furthermore  when we include the extra task of predicting the synthetic gradient that will be produced   steps
in the future  DNI   Aux  the RNNs with DNI are able
to model even larger time dependencies  For example with
            performing BPTT across only three timesteps 
on the Repeat Copy task  the DNI enabled RNN goes from
being able to model   timesteps to   timesteps when using future synthetic gradient prediction as well  This is in
contrast to without using DNI at all  where the RNN can
only model   timesteps 

Language Modelling We also applied our DNIenabled
RNNs to the task of characterlevel language modelling 
using the Penn Treebank dataset  Marcus et al    We
use an LSTM with   units  which at every timestep
reads   character and must predict the next character in
the sequence  We train with BPTT with and without DNI 
as well as when using future synthetic gradient prediction
 DNI   Aux  with               as well as strong
baselines with         We measure error in bits per

Decoupled Neural Interfaces using Synthetic Gradients

     
 
Copy
Repeat Copy
 

 
 
 

BPTT

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

DNI

 
 
 

 
 
 

 
 
 

 
 
 

DNI   Aux
 
 
 
 
 
 

 
 
 

 
 
 

Penn Treebank                                  

Table   Results for applying DNI to RNNs  Copy and Repeat Copy task performance is reported as the maximum sequence length that
was successfully modelled  higher is better  and Penn Treebank results are reported in terms of test set bits per character  lower is better 
at the point of lowest validation error  No learning rate decreases were performed during training 

Figure   Left  The task progression during training for the Repeat Copy task  All models were trained for    iterations  but the
varying unroll length   results in different quantities of data consumed  The xaxis shows the number of samples consumed by the
model  and the yaxis the time dependency level solved by the model   step changes in the time dependency indicate that   particular
time dependency is deemed solved  DNI Aux refers to DNI with the additional future synthetic gradient prediction auxiliary task  Right 
Test error in bits per character  BPC  for Penn Treebank character modelling  We train the RNNs with different BPTT unroll lengths
with DNI  solid lines  and without DNI  dashed lines  Early stopping is performed based on the validation set  Bracketed numbers give
 nal test set BPC 

character  BPC  as in  Graves    perform early stopping based on validation set error  and for simplicity do
not perform any learning rate decay  For full experimental details please refer to Sect     in the Supplementary
Material 
The results are given in Table   Interestingly  with BPTT
over only two timesteps        an LSTM can get surprisingly good accuracy at next character prediction  As expected  increasing   results in increased accuracy of prediction  When adding DNI  we see an increase in speed
of learning  learning curves can be found in Fig     Right 
and Fig    in the Supplementary Material  and models
reaching greater accuracy  lower BPC  than their counterparts without DNI  As seen with the Copy and Repeat Copy
task  future synthetic gradient prediction further increases
the ability of the LSTM to model long range temporal dependencies   an LSTM unrolled   timesteps with DNI and
future synthetic gradient prediction gives the same BPC as
  vanilla LSTM unrolled   steps  only needs   of the
data and is   faster in wall clock time to reach  BPC 
Although we report results only with LSTMs  we have

found DNI to work similarly for vanilla RNNs and Leaky
RNNs  Ollivier   Charpiat   

  MultiNetwork System
In this section  we explore the use of DNI for communication between arbitrary graphs of networks  As   simple
proofof concept  we look at   system of two RNNs  Network   and Network    where Network   is executed at  
slower rate than Network    and must use communication
from Network   to complete its task  The experimental
setup is illustrated and described in Fig        Full experimental details can be found in Sect     in the Supplementary Material 
First  we test this system trained endto end  with full backpropagation through all connections  which requires the
joint Network ANetwork   system to be unrolled for    
timesteps before   single weight update to both Network  
and Network    as the communication between Network
  to Network   causes Network   to be update locked to
Network    We the train the same system but using synthetic gradients to create   learnable bridge between Net 

  

 

 

 

count odd     

count       

Decoupled Neural Interfaces using Synthetic Gradients

count   

 

 

 

 

  

  

 

count odd 

 

  

   

 

 

count odd 

  

   

Figure       System of two RNNs communicating with DNI  Network   sees   datastream of MNIST digits and every   steps must
output the number of odd digits seen  Network   runs every   steps  takes   message from Network   as input and must output the
number of    seen over the last     timesteps  Here is   depiction where           The test error over the course of training Network
  and Network   with       Grey shows when the twonetwork system is treated as   single graph and trained with backpropagation
endto end  with an update every     timesteps  The blue curves are trained where Network   and Network   are decoupled  with
DNI  blue  and without DNI  red  When not decoupled  grey  Network   can only be updated every     steps as it is update locked
to Network    so trains slower than if the networks are decoupled  blue and red  Without using DNI  red  Network   receives no
feedback from Network   as to how to process the data stream and send   message  so Network   performs poorly  Using synthetic
gradient feedback allows Network   to learn to communicate with Network    resulting in similar  nal performance to the endto end
learnt system  results remain stable after    steps 

work   and Network    thus decoupling Network   from
Network    This allows Network   to be updated   times
more frequently  by using synthetic gradients in place of
true gradients from Network   
Fig        shows the results for       Looking at the test
error during learning of Network    Fig        Top  it is
clear that being decoupled and therefore updated more frequently allows Network   to learn much quicker than when
being locked to Network    reaching  nal performance in
under half the number of steps  Network   also trains faster
with DNI  most likely due to the increased speed in learning of Network    and reaches   similar  nal accuracy as
with full backpropagation  Fig        Bottom  When the
networks are decoupled but DNI is not used       no gradient is received by Network   from Network    Network
  receives no feedback from Network    so cannot shape
its representations and send   suitable message  meaning
Network   cannot solve the problem 

  FeedForward Networks
In this section we apply DNIs to feedforward networks in
order to allow asynchronous or sporadic training of layers 
as might be required in   distributed training setup  As explained in Sect    making layers decoupled by introducing synthetic gradients allows the layers to communicate
with each other without being update locked 

Asynchronous Updates To demonstrate the gains by decoupling layers given by DNI  we perform an experiment
on   four layer FCN model on MNIST  where the backwards pass and update for every layer occurs in random
order and only with some probability pupdate         layer is
only updated after its forward pass pupdate of the time  This
completely breaks backpropagation  as for example the  rst
layer would only receive error gradients with probability
update and even then  the system would be constrained to be
  
synchronous  However  with DNI bridging the communication gap between each layer  the stochasticity of   layer  
update does not mean the layer below cannot update  as
it uses synthetic gradients rather than backpropagated gradients  We ran   experiments with different values of
pupdate uniformly sampled between   and   The results are
shown in Fig     Left  for DNI with and without conditioning on the labels  With pupdate     the network can still
train to   accuracy  Incredibly  when the DNI is conditioned on the labels of the data    reasonable assumption
if training in   distributed fashion  the network trains perfectly with only   chance of an update  albeit just slower 

Complete Unlock As   drastic extension  we look at
making feedforward networks completely asynchronous 
by removing forward locking as well  In this scenario  every layer has   synthetic gradient model  but also   synthetic input model   given the data  the synthetic input

Decoupled Neural Interfaces using Synthetic Gradients

  

  

  

  

  

  

  

  

  

  

 

Figure   Completely unlocked feedforward network training allowing forward and update decoupling of layers 

Update Decoupled

Forwards and Update Decoupled

fi 

Mi 

 
 

DNI

cDNI

DNI

cDNI
Mi 

  

fi

Figure   Left  Four layer FCNs trained on MNIST using DNI between every layer  however each layer is trained stochastically  
after every forward pass    layer only does   backwards pass with probability pupdate  Population test errors are shown after different
numbers of iterations  turquoise is at the end of training after    iterations  The purple diamond shows the result when performing
regular backpropagation  requiring   synchronous backwards pass and therefore pupdate     When using cDNIs however  with only  
probability of   layer being updated the network can train effectively  Right  The same setup as previously described however we also
use   synthetic input model before every layer  which allows the network to also be forwards decoupled  Now every layer is trained
completely asynchronously  where with probability     pupdate   layer does not do   forward pass or backwards pass   effectively the
layer is  busy  and cannot be touched at all 

 

model produces an approximation of what the input to the
layer will be  This is illustrated in Fig    Every layer
can now be trained independently  with the synthetic gradient and input models trained to regress targets produced
by neighbouring layers  The results on MNIST are shown
in Fig     Right  and at least in this simple scenario  the
completely asynchronous collection of layers train independently  but colearn to reach   accuracy  only slightly
slower  More details are given in the Supplementary Material 

  Discussion   Conclusion
In this work we introduced   method  DNI using synthetic gradients  which allows decoupled communication
between components  such that they can be independently
updated  We demonstrated signi cant gains from the increased time horizon that DNIenabled RNNs are able to
model  as well as faster convergence  We also demonstrated the application to   multinetwork system    communicating pair of fastand slowticking RNNs can be decoupled  greatly accelarating learning  Finally  we showed

that the method can be used facilitate distributed training
by enabling us to completely decouple all the layers of  
feedforward net   thus allowing them to be trained asynchronously  nonsequentially  and sporadically 
It should be noted that while this paper introduces and
shows empirical justi cation for the ef cacy of DNIs and
synthetic gradients  the work of Czarnecki et al   
delves deeper into the analysis and theoretical understanding of DNIs and synthetic gradients  con rming the convergence properties of these methods and modelling impacts
of using synthetic gradients 
To our knowledge this is the  rst time that neural net modules have been decoupled  and the update locking has been
broken  This important result opens up exciting avenues
of exploration   including improving the foundations laid
out here  and application to modular  decoupled  and asynchronous model architectures 

References
Balduzzi     Vanchinathan     and Buhmann     Kickback cuts backprop   redtape  Biologically plausible

Decoupled Neural Interfaces using Synthetic Gradients

Taylor     Burmeister     Xu     Singh     Patel     and
Goldstein     Training neural networks without gradients    scalable admm approach  ICML   

Thomas        Policy gradient coagent networks  In Advances in Neural Information Processing Systems  pp 
   

Williams        Simple statistical gradientfollowing algorithms for connectionist reinforcement learning  Machine learning     

Williams        and Zipser       learning algorithm for continually running fully recurrent neural networks  Neural
computation     

credit assignment in neural networks  arXiv preprint
arXiv   

Baxter     and Bartlett        Direct gradientbased reinforcement learning  In Circuits and Systems    Proceedings  ISCAS   Geneva  The   IEEE International Symposium on  volume   pp    IEEE 
 

Bengio     How autoencoders could provide credit assignment in deep networks via target propagation  arXiv
preprint arXiv   

CarreiraPerpin an      and Wang     Distributed optimization of deeply nested systems  In AISTATS  pp   
   

Czarnecki       Swirszcz     Jaderberg     Osindero    
Vinyals     and Kavukcuoglu     Understanding synthetic gradients and decoupled neural interfaces  arXiv
preprint   

Graves     Generating sequences with recurrent neural net 

works  arXiv preprint arXiv   

Graves     Wayne     and Danihelka     Neural turing

machines  arXiv preprint arXiv   

Hochreiter     and Schmidhuber     Long shortterm mem 

ory  Neural computation     

Lee     Zhang     Fischer     and Bengio     Difference
target propagation  In Machine Learning and Knowledge
Discovery in Databases  pp    Springer   

Marcus        Marcinkiewicz        and Santorini    
Building   large annotated corpus of english  The penn
treebank  Computational linguistics   
 

Ollivier     and Charpiat    

works online without backtracking 
arXiv   

Training recurrent netarXiv preprint

Rumelhart        Hinton        and Williams        Learning representations by backpropagating errors  Nature 
   

Schmidhuber    urgen  Networks adjusting networks 

In
Proceedings ofDistributed Adaptive Neural Information
Processing  St  Augustin  Citeseer   

Sutton      and Barto       Reinforcement learning  An

introduction   

Tallec     and Ollivier     Unbiased online recurrent opti 

mization  arXiv preprint arXiv   

