Beyond Filters  Compact Feature Map for Portable Deep Model

Yunhe Wang   Chang Xu   Chao Xu   Dacheng Tao  

Abstract

Convolutional neural networks  CNNs  have
shown extraordinary performance in   number of
applications  but they are usually of heavy design
for the accuracy reason  Beyond compressing the
 lters in CNNs  this paper focuses on the redundancy in the feature maps derived from the large
number of  lters in   layer  We propose to extract intrinsic representation of the feature maps
and preserve the discriminability of the features 
Circulant matrix is employed to formulate the
feature map transformation  which only requires
    log    computation complexity to embed  
ddimensional feature map  The  lter is then recon gured to establish the mapping from original input to the new compact feature map  and
the resulting network can preserve intrinsic information of the original network with signi cantly
fewer parameters  which not only decreases the
online memory for launching CNN but also accelerates the computation speed  Experiments on
benchmark image datasets demonstrate the superiority of the proposed algorithm over stateof 
theart methods 

  Introduction
Tremendous power of convolutional neural networks
 CNNs  have been well demonstrated in   wide variety
of computer vision applications  from image classi cation  Simonyan   Zisserman    and object detection  Ren et al    to image segmentation  Long et al 
  Meanwhile  there is   recent consensus that there are

 Key Laboratory of Machine Perception  MOE  and Cooperative Medianet Innovation Center  School of EECS  Peking
 UBTech Sydney
University  Beijing        China 
AI Institute  School of IT  FEIT  The University of Sydney  Darlington  NSW   Australia 
Correspondence
to  Yunhe Wang  wangyunhe pku edu cn  Chang Xu
   xu sydney edu au  Chao Xu  xuchao cis pku edu cn 
Dacheng Tao  dacheng tao sydney edu au 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

signi cant redundancy in most of existing convolutional
neural networks  For instance  the ResNet   He et al 
  with some   convolutional layers needs over  MB
memory for storage and over       times of  oating
number multiplications for calculating each image  and after discarding more than   of its weights  the network
still works as usual  Wang et al   
Admittedly    heavy neural network is extremely dif cult
to train and to use in mobile terminal apps due to the limited memory and computational resource  Lots of methods
have been developed to reduce the amount of parameters in
CNNs  Liu et al    to obtain   considerable compression ratio   Han et al    discarded subtle weights in
  pretrained network and constructed   sparse neural network with less computational complexity  Subsequently 
 Wang et al    further studied the redundancy on all
weights and their underlying connections in the DCT frequency domain  which achieves higher compression and
speedup ratios   Wen et al    excavated subtle connections in different aspects and  Figurnov et al    re 
 ned the conventional convolution neurons as locally connection on the input data in order to reduce the computational cost  In addition  there are   variety of techniques for
compressing convolution  lters       pruning  Han et al 
  Hu et al    Li et al    quantization and binarization  Arora et al    Rastegari et al    Courbariaux   Bengio    matrix approximation  Cheng
et al    and matrix decomposition  Denton et al 
 
Although these methods obtained promising performance
to reduce the storage of convolution  lters  the memory usage introduced by  lters is still huge in the stage of online
inference  This is because except convolution  lters  we
have to store feature maps  output data  of different layers
for the subsequent processes       over  MB memory is
required for storing feature maps of one single image when
running   ResNet   He et al    without batch normalization layers  and   batch consisting of   instances
consumes some  GB GPU memory  However  existing
compression methods tend to directly compress the  lters
in one step and rarely consider the signi cant demand of
feature maps on the storage and computational cost 
In   CNN  the size of   convolution  lter is usually much

Beyond Filters  Compact Feature Map for Portable Deep Model

smaller than the number of  lters in   convolutional layer 
Given   convolutional layer with    lters of size      
any   patch in the input data will be mapped into    
dimensional space         The size of the space
to describe this small patch has broken up more than  
folds  which leads to the redundancy of the feature map 
We are therefore motivated to discover the intrinsic representations of the redundant feature maps via dimensionality
reduction  Circulant matrix is employed to formulate the
feature map transformation considering its low space complexity and high mapping speed  Based on the obtained
compact feature maps  we reformulate the convolution  lters to establish its connections with the input data  In summary  the proposed approach makes the following contributions 
  We propose to excavate the intrinsic information and
decrease the redundancy in feature maps derived from
  large number of  lters in each layer  and then the network architecture is upgraded to produce   new compact network with fewer  lters but the similar discriminativeness 
  We devise to learn   circulant matrix for projection
which is exactly   diagonal matrix in the Fourier domain and thus yields   high speed for training and low
complexity for mapping 
  Experiments demonstrate that  compared to the original heavy network  the learned portable counterpart
network achieves   comparable accuracy  but has signi cantly lower memory usage and computational
cost 

  Feature Map Dimensionality Reduction
Here we will  rst introduce some preliminaries of CNNs
and then develop feature map dimensionality reduction
method 
For   convolutional layer   in   pretrained convolution
neural network   whose input data and output feature
maps are     RH     and     RH cid    cid    respectively  where          cid     cid  are widths and heights of  
and       is the channel number       the number of convolution  lters in the previous layer  and   is the number of
convolution  lters in this layer  These convolution  lters
can be denoted as   tensor           Rs        where   
and    are the width and the height of  lters  respectively 
Taking the neural network as   powerful feature extractor 
the convolutional layer then becomes   mapping from the
patch     Rs    to feature map     Rd 
Generally    is much larger than                       
and       in the second layer in ResNet  and there are
even some layers with       Admittedly  using such
  long ddimensional vector to represent           area is
heavy and redundant  To decrease the storage and com 

putation cost of the feature map  we attempt to discover
the compact representations of the feature maps  Many
sophisticated methods such as locally linear embedding
 LLE   Roweis   Saul    principle component analysis  PCA   Wold et al    Pan   Jiashi    isometric feature mapping  Isomap   Tenenbaum et al   
locality preserving projection  LLP   He   Niyogi   
and other excellent dimensionality methods  Pan et al 
  Xu et al    can be applied for dimensionality
reduction  Lowdimensional representations produced by
these methods can inherit intrinsic information of original
highdimensional data  so that the performance of the transformed features can be maintained  and enhanced in some
cases  We thus proceed to develop an exclusive feature
map dimensionality reduction method for the deep network
compression problem 
Dividing   into       cid       cid  patches and vectorizing them  we have      vec      vec xq    Rsc   
Accordingly  we reformulate feature maps and convolution  lters as      vec      vec Yd    Rq   and
     vec      vec Fd    Rsc    respectively  Thus 
the conventional convolution operation of the given layer  
can be rewritten as 

    XT   

 

where the ith column in   corresponds to the convolution
responses of all patches through the ith  lter in    Note
that  when we consider the entire dataset    should be additionally multiplied by the number of samples which is an
extreme large number  The most compact representation of
  CNN should have no correlation between feature maps
derived from different convolution  lters  In other words 
feature maps from different  lters should be independent
of each other as far as possible  The independence  or redundancy  in   can be measured by

       YT           
   

 
where        is the Frobenius norm for matrices    is  
full one matrix    is the elementwise product  and   is
an identity matrix  Denote the reduced feature maps as
           Rq      where        and   can be
either   linear  Wold et al    or nonlinear transformation  Roweis   Saul    However  since there are
numerous training samples in real word image datasets
      ImageNet  Russakovsky et al    computational
complexities of the nonlinear transformation will be great 
We thus use the linear transformation instead           
      YP     where            is the projection matrix 
An optimal transformation should generate the new representations which occupy more information of the original
input and have less internal correlation  Based on the measurement in Fcn    the optimal projection   can be solved

Beyond Filters  Compact Feature Map for Portable Deep Model

by minimizing the following function 

   YT YP       

             diag   

 

min
   

where     diag    is   diagonal matrix  whose functionality is equal to the identity matrix in Fcn   
Deep neural network enjoys great popularity due to its excellent capability of learning effective features for examples  An optimal projection should thus not only remove
redundancy between feature maps  but also preserve the
discriminability of these features  If images from different
categories are well separated from each other in the feature
space  classi ers will more easily accomplish the classi cation task  To maintain the accuracy of the original network
and its representation capability  we propose to preserve
the distances between feature maps and form the following
objective function for seeking the compact feature maps 

   YT YP       

                  

min
   

    

     YP         diag   

 

where            Rq   and Dij is the Euclidean distance between the ith column and the jth column of   

  Optimal Feature Map Learning
The above section has proposed   feature map dimensionality reduction model  However  calculating the distance
matrix        is inef cient and memory consuming
since the column length of   corresponds to the number
of training samples and is rather large in practice  For example  there are over   images in the ILSVRC 
dataset and there are up to    lters of   network layer 
The size of   will be larger than     which is inconvenient for distance calculation  In this section  we will propose an alternative feature map dimensionality reduction
approach  which consists of two steps  distance preservation and sparse approximation 
In fact  distances between feature maps can be easily preserved if   is orthogonal                  where   is
an identity matrix with size        For any two feature maps       Rd  we have               and
               
  We thus reformulate
Fcn    as

            

   YT YP       
   

min

 

         diag              

 

 
 

min
  

where          cid 

stage  we propose to use   sparse matrix to approximate
the representation generated by    
       YP   

         

 

     Yi  and  Yi is the ith column in
          is  cid norm which is   widely used regularization  Nie et al    Liu et al    for removing useless
 cid   ui 
columns in    and the closed form solution of Fcn    is

 Yi  

 ui  ui 
 

if      ui 
otherwise

 

and Pi

where ui   YP  
is the ith column in    
 
Zero columns in    can be discarded to achieve the lowdimensional representations 
By combining Fcn    and Fcn    we can obtain   uni ed
model as follow

min
   

   YT YP       

       
         diag              

 

where       is the  cid  norm to make   sparse  so that some
small valued columns in      YP   will be discarded   
is   weight parameter which controls the sparsity of    and
implicitly in uence the resulting dimensionality of the new
feature map of this layer 
Considering there are       variables in   and   can be
up to   Fcn    cannot be ef ciently optimized           
Since each frequency coef cient corresponds to   Fourier
base with different textures  circulant matrices have complex internal structures and strong diversity thus can be utilized for approximating huge matrices  Cheng et al   
We therefore propose using   circulant matrix  Gray   
Henriques et al      to formulate     which then
only has   variables in the Fourier frequency domain  We
propose the following model to learn the projection for generating the compact lowdimensional feature maps 

min
   

   YT YP       

               
         circ        diag   

       
 
where   is the weight for relaxing the equality constrain
in Fcn    and     circ    is   circulant matrix  For the
     vector           pd     we can refer to it as the base
sample  and the cyclic shift operator can be de ned as 

   

 

The orthogonal transformation   learned by Fcn    is able
to extract the intrinsic representation and preserve distances
between feature maps  but the dimensionality has not been
reduced since   is   square matrix  Hence at the second

circ     

 

  
  
 
pd 
pd

pd
  

  

pd 

     
pd

  
 
     

  

 
 
  

  
  
 
pd
  

Beyond Filters  Compact Feature Map for Portable Deep Model

Given the fact that all circulant matrices are made diagonal
by the discrete Fourier transform  DFT  Bracewell   
  and     can be expressed as

   

 
 

SH diag            

 
 

  diag    SH  

 

where   is the DFT transform matrix which is constant 
the DFT is   linear transform  and SH     dI     is the
frequency representation of        

            Sp 

 

and its inverse discrete Fourier transform  IDFT  is    
        
  SH     In following illustrations  we will use
  hat   to denote the DFT frequency representations 
Since any two bases in   are orthogonal thus it can hold the
Euclidean distance between any two vectors  For any two
ddimensional samples in   we have

 SyT
    SyT

   
   

 SyT

   

   

   
 
        
 

 
 
 
 

 

In addition  the most elegant property of the circulant matrix is that the projection in the original domain is equivalent to vector element wise product in the Fourier domain  Oppenheim et al    which is bene cial for signi cantly decreasing the computational complexity      

    yT                   
  yP                     

 

where   denotes the element wise product  Since DFT and
IDFT can be ef ciently computed in     log    with the
fast Fourier transform  FFT   Bracewell    the projection for generating lowdimensional feature maps is only
    log    compared with the      complexity of the
original dense matrix multiplication  Considering the ef 
 cient computation over circulant matrix in the frequency
domain  we propose to use the frequency optimizing approach to obtain the optimal feature maps representation
     YP     We optimize Fcn    by alternatively  xing  
and    and leave the optimization details in the supplementary materials for the limited page length 
Given the optimal    and    the transformation for reducing
the dimensionality of feature maps can be written as

   

 
 

diag   SH      

 

where  ci     if ci     and  ci     otherwise  the
transformation   in Fcn    is   row sparse matrix  and the
rows with all zeros can be discarded to reformulate   compact transformation matrix      with    rows according to    

Therefore the feature map matrix   can be transformed as
     YP  
  This projection is exactly   linear transform 
  
if we only take one convolutional layer into consideration 
     the input data matrix   is  xed  we can explicitly include the  lter matrix   into the dimensionality reduction
procedure      

     YP  

     XFT    

        FT  

 

Hence  we can also directly reduce the number of convolution  lters after obtaining the optimal projection matrix      
Fixing the  lter size as              we can reconstruct convolutional layers with smaller  lters      Rs        Based
on the above analysis  we have the following proposition 
Proposition   Given   convolutional layer   with    lters       its feature map dimensionality is    For the ddimensional feature of any sample through    the proposed
method for solving its lowdimensional embedding has
space complexity      and time complexity     log   

  CNN Layer Reconstruction
Section   proposes an effective approach for learning compact feature maps of   given convolutional layer  In the online inference  it is impossible to  rst calculate the original
highdimensional feature maps  and then project them into
the lowdimensional space  To conserve the online computation resource  we thus aim to establish the mapping directly from the input data to the compact feature map 
The dimensionality of the feature map for the ith convolutional layer Li has been reduced by Fcn    and the number of convolution  lters of Li has also been reduced from
  to     where     cid     For the following convolutional layer
     the size of the input data    has becomes         
and we have      Rs       which leads original  lters can no
longer be used for calculating  Thus  we propose minimizing the following function for reconstructing convolution
 lters of this layer 

        XT    

         
   

min

  

 

where    is the compact feature maps of      after applying Fcn    and   is   weight parameter for balancing the
two terms  Note that the second term can be regarded as
  weight decay regularization in the training of neural networks  Krizhevsky et al    Fcn    can be ef ciently
solved by the following closed form solution 

           XT             

 

where   is an identity matrix  However  when the scale of
the dataset is enormous  we cannot construct the two huge
matrices    and    through all instances in the dataset  The

Beyond Filters  Compact Feature Map for Portable Deep Model

Algorithm   CNN Layer Reconstruction Method 
Input    pretrained convolutional neural network   learned
through   dataset   with   convolutional layers      Lk 
weight parameter   learning rate  
network       Yk          

  Calculate feature maps of each layer by using the original
  for       to       do
 
 
  end for
  Keep feature maps of the kth layer   Yk   Yk 
  Construct   new network    according to          Yk  and
initialize convolution  lters          Fk  by random values
from the standard normal distribution 
Randomly select   batch Xj from    
for       to   do

Learn the projection Pi by solving Fcn   
Calculate new feature maps   Yi   YiP  
   

Generate input data  Xi of Li exploiting     
Estimate the new  lter matrix  Fcn   
 Fi    Fi        Fi   Fi 
Convert  Fi into  lter data and  ll it in     

 
 
  until Convergence 
Output  The new convolutional neural network     

end for

  repeat
 
 
 
 

minibatch strategy is adopted for updating    iteratively 
The loss function of    can be directly formulated as

         Tr   FT  XT       

   Tr   FT           Tr   FT    

 

and the gradient of        is

       

    

        XT                     

 

By using stochastic gradient descent  SGD     can be updated as

           

 

 

       

    

where   is the learning rate 
It is worth mentioning that input data of the  rst layer of
the original network   and feature map of the last layer
 closely related to the number of classi cation labels  are
kept unchanged  As for other intermediate convolutional
layers and fully connected layers  we can generate compact
feature maps    from the original feature maps    Then 
calculate the input data    using the compressed network
   and estimate the  lter matrix     The detailed  lters updating procedure can be found in Alg   
According to Proposition   for   ddimensional feature 
the complexity of the proposed feature map dimensionality
reduction method with the help of circulant matrix is only
    log    Compared to      of other traditional linear
projection methods  the proposed scheme is of great bene  

for conducting experiments on large scale datasets  Moreover  since we only need to store   ddimensional vector
for one layer  the proposed method also have an obvious
advantage on the space complexity for learning   portable
version of neural networks with   larger number of layers
      ResNet  He et al   
Discussion  There are some works investigating the intrinsic information of feature maps in the original neural
network to learn   new thinner and deeper neural network 
 Hinton et al     rst built   thinner neural network and
then made its feature map of the fully connected layer similar to that of the original pretrained networks  thus enhanced the accuracy of the new network   Romero et al 
  further extended this work to   general model which
minimizes the difference between the feature map of an arbitrary layer in the smaller network and the feature map of
  given layer in the original network  yields   thinner and
deeper network with some accuracy decline 
The difference between these methods and the proposed
method is twofold 
    These methods rebuild   new
student network with less parameters while the proposed
method outputs   compact CNN based on the original network itself  which inherits the welldesigned architectures 
 ii  The performance of the newly learned student network
will be declined  since it is only in uenced by the information from one or several layers of the teacher network 
By contrast  the proposed method excavates redundancy in
feature maps of every layer and preserves the distances between examples to guarantee the accuracy of the CNN 

  Analysis on Compression Performance
  novel dimensionality reduction method for learning  
portable neural network has been proposed in Alg    Compared with the original heavy network     the new network
   has the same depth but less convolution  lters per layer 
In this section  we will further analyze the memory usage
and computation cost of    and calculate the compression
ratio and speedup ratio theoretically 
Speedup ratio  Consider the ith convolutional layer Li in
the original network   with its output feature map and convolution  lters are Yi   RH cid 
   ci di 
respectively  We only discuss square  lters and the conclusion can be straightforwardly extended to nonsquare  lters
as well  Wherein  ci   di  is the channel number of  lters in Li and         RGB color images  The feature
map and convolution  lters in the corresponding layer  Li
in the learned compact network are Yi   RH cid 
    di and
    Rs 
   ci   di  respectively  Generally  weights and feature maps are stored in  bit  oating values whose multiplications are much more expensive than additions  Complexities of other auxiliary layers       pooling  Relu  etc 

  di and     Rs 

     cid 

     cid 

Beyond Filters  Compact Feature Map for Portable Deep Model

    Accuracy of the reconstructed model 

    Model accuracy after  netuning 

    Compression and speedup ratios 

Figure   The performance of the proposed CNN compression method with different  

have been discarded since they only account for   subtle
proportion of the overall complexity  Hence  considering
the major multiplications  the speedup ratio of the compact
network for this layer compared with the original network
is

 

 

di di
 di
 di 

rs  

  di diH cid 
  
 diH cid 
 di 
  
 

iW  cid 
iW  cid 

 

 

 

It is obvious that the speedup ratio for one convolutional
layer relates to numbers of  lters in this layer and the previous layer  Thus  if we only keep    lters per layer  the
speedup ratio will be increased to  
Compression ratio  The online memory can be divided
into two major parts  feature maps of different layers and
 lters of convolutional layers  Although we can remove
feature maps of   layer after it has already been used for
calculating the following layer  the the memory allocation
and release will increase the time consumption  Moreover  if   layer is connected with several other convolutional layers  we have to store feature maps of previous
layers when doing online inference       the second convolutional layer and the  fth convolutional layer will be
combined in ResNet   He et al    and we have to
preserve these feature maps before merging them  For  
given convolutional layer Li  the compression ratio of the
proposed method is

rc  

  di di   diH cid 
  
 di    diH cid 
 di 
  
 

iW  cid 
iW  cid 

 

 

 

 

which is simultaneously affected by the current layer and
the pervious layer  Meanwhile  the memory for storing feature maps of other layers  such as pooling layers and Relu
layers  will be reduced  We will further illustrate the detailed compression ratio and speedup ratio of the proposed
method in the following section experimentally 

  Experiments
Baselines and Models  Several effective approaches for
compressing deep neural networks were selected for comparison  SVD  Denton et al    XNORNet  Rastegari

et al    Pruning  Han et al    Perforation  Figurnov et al    and CNNpack  Wang et al    and
we denoted the proposed method as RedCNN  The evaluation was conducted on the MNIST and ILSVRC 
datasets  Russakovsky et al    We  rst tested the
performance of the proposed method and analyzed impacts
of parameters on the MNIST dataset using LeNet  LeCun
et al    then compared the proposed method with two
benchmark CNNs  VGGNet   Simonyan   Zisserman 
  and ResNet   He et al    on the ILSVRC
  dataset  Russakovsky et al    which has more
than   million nature images  All methods were implemented using MatConvNet  Vedaldi   Lenc    and
ran on NVIDIA    cards  Filters and data in CNNs were
stored and calculated as   bit  oatingpoint values 
Impact of parameters  The hyperparameter   in the proposed reconstruction method  Alg    controls the weight
decay regularization and makes weights in new convolution  lters not too large  We set   as   empirically 
In addition  the proposed dimensionality method
 Fcn    has two hyperparameters         and   We  rst
tested their impact on the network accuracy by conducting
an experiment using   LeNet for classifying the MNIST
dataset  Vedaldi   Lenc    where the network has four
convolutional layers of size                
        and         respectively  and its
accuracy is     is used for enforcing the projection
matrix   to be orthogonal and is set to be   experimentally    is directly related to the sparsity of     and it effects
compression and speedup ratios of the proposed method 
Although   larger   will produce   smaller network  it also
leads to   larger distortion on the Euclidean distances between samples  To have   clear illustration  we reported the
compression performance by ranging different   as shown
in Fig   
From Fig        we found that the compact network reconstructed by using Alg    can also hold   considerable accuracy         when       which demonstrates that
the proposed method can preserve the intrinsic information
of the original CNN  Moreover  the accuracy decline can
be rebounded   when       after  netuning as

 Accuracy   Accuracy    ratio compressionspeedupBeyond Filters  Compact Feature Map for Portable Deep Model

Table   Compression statistics for VGG  Net 

Layer
conv   
conv   
conv   
conv   
conv   
conv   
conv   
conv   
conv   
conv   
conv   
conv   
conv   

fc 
fc 
fc 
Total

Memory
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB

 MB
 MB

Mult 
rc
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
       

rs
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

shown in Fig        However    network that was directly
trained with such   small architecture can only achieve  
  accuracy  In addition  although the impact of   is
sensitive but monotonous    larger   enhances compression and speedup ratios simultaneously  but decreases the
accuracy of CNNs as well  The value of   can be easily adjusted according to the demand and restrictions of devices 
In our experiments  we set       which provides the
best tradeoff between compression performance and accuracy       rc     rs     with   accuracy 
In this case  the layers in the compact convolution network
is of the size       and
  respectively  The resulting compact network
only occupies around  KB memory  The MATLAB  le
of the compressed network and the demo code can be found
in https github com YunheWang RedCNN 
Deep Neural Networks Compression on ISLVRC 
We next employed the proposed RedCNN for CNN compression on the ImageNet ILSVRC  dataset  Russakovsky et al    which contains over    training images and    validation images  We evaluated the
compression performance on three widely used models 
AlexNet  Krizhevsky et al    which has more than
   parameters and   top  accuracy of   VGGNet 
   Simonyan   Zisserman    which has over   
parameters and   top  accuracy of   and ResNet 
   He et al    which has more than   layers with
  convolutional layers  and   top  accuracy of   It
is worth mentioning that there are considerable  lters in
the ResNet  and thus the network has less redundancy
and it is hard for further compression  We  rst begin our
experiment with the AlexNet dataset  and the detailed experimental results were shown in Tab   

Layer
conv 
conv 
conv 
conv 
conv 
fc 
fc 
fc 
Total

Memory
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB
 MB

Mult 
rc
 
 
   
   
 
 
 
 
 
 
   
   
   

rs
 
 
 
 
 
 
 
 
 

From Tab    we found that the proposed method achieved
    compression ratio and     speedup ratio for
AlexNet  Then  we reported the compression result of
VGGNet  in Tab   
It can be seen from Tab    we obtained     compression ratio and     speedup ratio on VGGNet  In
addition  the compression ratio and the speedup ratio on
ResNet  are   and   respectively  Note that
the compression ratio we reported here is calculated by

Table   Compression statistics for AlexNet 

rc   

Fcn    which contains both convolution  lters and feature maps  More compression results of these three CNN
models can be found in the supplementary material 
Comparison with stateof theart methods  Tab    details the compression results of the proposed method and
several stateof theart methods on three benchmark deep
CNN models  Since comparison methods do not change
the number of  lters of the original neural network  feature
map compression ratios of these methods are both equal to
  Thus  we reported the compression ratio of  lters rc 
and feature maps rc  separately for   fair comparison  For
  convolutional neural network with layers  its compression
ratios is calculated as

 cid  
 cid  
   diH cid 
 diH cid 

  

iW  cid 
iW  cid 

 

 

 

 

 cid  
 cid  

     
     
 

  di di
 di 
 di

 

rc   

Tab    also shows the cost of various models for processing
one image       storage of  lters  memory usage of feature
maps  and multiplications for calculating convolutions  It
is obvious that feature maps accounting   considerable proportion of memory usage of the whole network  and the
proposed RedCNN can provide signi cant compression ratios rc  on every network  Although we can remove the
feature map of   layer after calculation for saving memory 
frequently allocating and releasing is also time consuming 
It can be seen from Tab    RedCNN clearly achieves the
best performance in terms of both the speedup ratio  rs 
and the feature map compression ratio  rc 
In addition  convolution  lter compression ratios of the proposed
method is lower than those of pruning  Han et al   
and CNNpack  Wang et al    These two comparison methods employed quantization approaches       onedimensional kmeans clustering  and thus  bit  oating
values can be converted into about  bit values without af 

Beyond Filters  Compact Feature Map for Portable Deep Model

Table   An overall comparison of stateof theart methods for deep neural network compression and speedup  where rc  is the compression ratio of convolution  lters  rc  is the compression ratio of feature maps  and rs is the speedup ratio 

Model
AlexNet

Filters   MB 
Maps  MB 
Multiplications
     
VGGNet 
Filters   MB 
Maps   MB 
Multiplications
     
ResNet 
Filters   MB 
Maps   MB 
Multiplications
     

Evaluation Original

rc 
rc 
rs

top  err
top  err

rc 
rc 
rs

top  err
top  err

rc 
rc 
rs

top  err
top  err

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

SVD
XNOR
 
 
 
 
 
 
   
   

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

Pruning
 
 
 

 
 
 
 
 
 
 

 
 
 
 
 

Perforation

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

CNNpack

 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

RedCNN
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

fecting the accuracy of the original network  If we adopt
this similar strategy  the convolution  lter compression ratio rc  of the proposed scheme can be further multiplied  
factor of around        we can obtain an almost  
 lter compression ratio on ResNet  model  which is superior to all the other comparison methods  However   bit
 or other unconventional format  value cannot be directly
used in generic devices       GPU cards  mobile phones 
and thus we did not try them in the experiments of this
paper 
In summary  the proposed RedCNN can achieve
considerable compression and speedup ratios  which can
make existing deep models portable 

Table   Runtime of the proposed deep neural network compression algorithm on different models per image 

time
LeNet
AlexNet

VGGNet 
ResNet 

Original
  ms
  ms
  ms
  ms

RedCNN speedup
 
  ms
 
  ms
 
  ms
 
  ms

Runtime 
In fact  most of comparison methods cannot
signi cantly accelerate the deep network for various additional operations  For example   Han et al    needs
to decode the CSR data before testing  which slows down
the online inference and will not achieve the comparable
compression and speedup ratios with those of the proposed algorithm in practice  Since the proposed compression method directly recon gures the network into   more
compact form  and does not require other additional support for realizing the network speedup  the runtime of the
compressed models for processing images will be reduced
signi cantly  To explicitly demonstrate the superiority of
the proposed method  we compared runtimes for recognizing images by benchmark CNN models before and after
applying the proposed method  and showed the experimental results in Tab   
We found that runtimes of these models after compression

were signi cantly reduced  The results are extremely encouraging       the compressed ResNet can recognize over
  images per second  This ef ciency can also be inherited into the  netuning process  therefore  the compressed
networks can be quickly adjusted when applied them to  
new dataset  In addition  the practical speedup ratios of
runtimes were slightly lower than the corresponding theoretical speedup ratios rs due to the costs incurred by data
transmission  pooling  padding  etc  Note that  the runtime
reported here is   bit higher than that in  Vedaldi   Lenc 
  due to different con gurations and hardware environments 

  Conclusions and Discussions
Compression methods for learning portable CNNs are urgently required so that neural networks can be used on mobile devices  Besides convolution  lters  the storage of feature maps also accounts for   larger proportion of the online memory usage  we thus no longer search useless connections or weights of  lters  In this paper  we present  
feature map dimensionality reduction method by excavating and removing redundancy in feature maps generated by
different  lters  Although the portable network learned by
our approach has signi cantly fewer parameters  its feature maps can also preserve intrinsic information of the
original network  Experiments conducted on benchmark
datasets and models show that the proposed method can
achieve considerable compression ratio and speedup ratios simultaneously without affecting the classi cation accuracy of the original CNN  In addition  the compressed
network generated by exploiting the proposed method is
still   regular CNN with  bit  oat values which does not
have any decoding or other procedures for online inference 

Acknowledgements

We thank supports of NSFC   and  BAF    and
ARC Projects  FT  DP  LP 

Beyond Filters  Compact Feature Map for Portable Deep Model

References
Arora  Sanjeev  Bhaskara  Aditya  Ge  Rong  and Ma  Tengyu 
Provable bounds for learning some deep representations 
ICML   

Bracewell  Ron  The fourier transform and iis applications  New

York     

Cheng  Yu  Yu  Felix    Feris  Rogerio    Kumar  Sanjiv  Choudhary  Alok  and Chang  ShiFu  An exploration of parameter
redundancy in deep networks with circulant projections 
In
CVPR   

Courbariaux  Matthieu and Bengio  Yoshua  Binarynet  Training
deep neural networks with weights and activations constrained
to    or  arXiv preprint arXiv   

Denton  Emily    Zaremba  Wojciech  Bruna  Joan  LeCun  Yann 
and Fergus  Rob  Exploiting linear structure within convolutional networks for ef cient evaluation  In NIPS   

Figurnov  Michael  Vetrov  Dmitry  and Kohli  Pushmeet  Perforatedcnns  Acceleration through elimination of redundant convolutions  NIPS   

Gray  Robert    Toeplitz and circulant matrices    review  now

publishers inc   

Han  Song  Pool  Jeff  Tran  John  and Dally  William  Learning
both weights and connections for ef cient neural network  In
NIPS   

Han  Song  Mao  Huizi  and Dally  William    Deep compression  Compressing deep neural networks with pruning  trained
quantization and huffman coding   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian 
Deep residual learning for image recognition  arXiv preprint
arXiv   

He  Xiaofei and Niyogi  Partha  Locality preserving projections 

In NIPS   

Henriques  Joao    Martins  Pedro  Caseiro  Rui    and Batista 
Jorge  Fast training of pose detectors in the fourier domain  In
NIPS   

Henriques  Jo ao    Caseiro  Rui  Martins  Pedro  and Batista 
Jorge  Highspeed tracking with kernelized correlation  lters 
IEEE TPAMI     

Hinton  Geoffrey  Vinyals  Oriol  and Dean  Jeff  DistillarXiv preprint

ing the knowledge in   neural network 
arXiv   

Hu  Hengyuan  Peng  Rui  Tai  YuWing  and Tang  ChiKeung  Network trimming    datadriven neuron pruning
approach towards ef cient deep architectures  arXiv preprint
arXiv   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey    Imagenet classi cation with deep convolutional neural networks 
In NIPS   

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document recognition  Proceedings of the IEEE     

Li  Hao  Kadav  Asim  Durdanovic  Igor  Samet  Hanan  and
Graf  Hans Peter  Pruning  lters for ef cient convnets  arXiv
preprint arXiv   

Liu  Baoyuan  Wang  Min  Foroosh  Hassan  Tappen  Marshall 
and Pensky  Marianna  Sparse convolutional neural networks 
In CVPR   

Liu  Guangcan  Lin  Zhouchen  and Yu  Yong  Robust subspace

segmentation by lowrank representation  In ICML   

Long  Jonathan  Shelhamer  Evan  and Darrell  Trevor  Fully
convolutional networks for semantic segmentation  In CVPR 
 

Nie  Feiping  Huang  Heng  Cai  Xiao  and Ding  Chris    Ef 
 cient and robust feature selection via joint  cid  norms minimization  In NIPS   

Oppenheim  Alan    Schafer  Ronald    and Buck  John   
Discretetime signal processing  Prentice Hall Upper Saddle
River   

Pan  Zhou and Jiashi  Feng  Outlierrobust tensor pca  In CVPR 

 

Pan  Zhou  Zhouchen  Lin  and Chao  Zhang 

Integrated lowrank based discriminative feature learning for recognition 
IEEE TNNLS     

Rastegari  Mohammad  Ordonez  Vicente  Redmon  Joseph  and
Farhadi  Ali  Xnornet  Imagenet classi cation using binary
convolutional neural networks  ECCV   

Ren  Shaoqing  He  Kaiming  Girshick  Ross  and Sun  Jian 
Faster rcnn  Towards realtime object detection with region
proposal networks  In NIPS   

Romero  Adriana  Ballas  Nicolas  Kahou  Samira Ebrahimi 
Chassang  Antoine  Gatta  Carlo  and Bengio  Yoshua  Fitnets 
Hints for thin deep nets  In ICLR   

Roweis  Sam   and Saul  Lawrence    Nonlinear dimensionality
reduction by locally linear embedding  Science   
   

Russakovsky  Olga  Deng  Jia  Su  Hao  Krause  Jonathan 
Satheesh  Sanjeev  Ma  Sean  Huang  Zhiheng  Karpathy  Andrej  Khosla  Aditya  Bernstein  Michael  et al 
Imagenet
IJCV   
large scale visual recognition challenge 
   

Simonyan  Karen and Zisserman  Andrew  Very deep convolutional networks for largescale image recognition  ICLR   
Tenenbaum  Joshua    De Silva  Vin  and Langford  John     
global geometric framework for nonlinear dimensionality reduction  science     

Vedaldi  Andrea and Lenc  Karel  Matconvnet  Convolutional
neural networks for matlab  In Proceedings of the  rd Annual
ACM Conference on Multimedia Conference   

Wang  Yunhe  Xu  Chang  You  Shan  Tao  Dacheng  and Xu 
Chao  Cnnpack  Packing convolutional neural networks in the
frequency domain  In NIPS   

Wen  Wei  Wu  Chunpeng  Wang  Yandan  Chen  Yiran  and Li 
Hai  Learning structured sparsity in deep neural networks  In
NIPS   

Wold  Svante  Esbensen  Kim  and Geladi  Paul  Principal component analysis  Chemometrics and intelligent laboratory systems     

Xu  Chang  Tao  Dacheng  Xu  Chao  and Rui  Yong  Largemargin weakly supervised dimensionality reduction  In ICML 
 

