Coclustering through Optimal Transport

Charlotte Laclau   Ievgen Redko   Basarab Matei   Youn es Bennani   Vincent Brault  

Abstract

In this paper  we present   novel method for
coclustering  an unsupervised learning approach
that aims at discovering homogeneous groups of
data instances and features by grouping them simultaneously  The proposed method uses the entropy regularized optimal transport between empirical measures de ned on data instances and
features in order to obtain an estimated joint
probability density function represented by the
optimal coupling matrix  This matrix is further factorized to obtain the induced row and
columns partitions using multiscale representations approach  To justify our method theoretically  we show how the solution of the regularized optimal transport can be seen from the variational inference perspective thus motivating its
use for coclustering  The algorithm derived for
the proposed method and its kernelized version
based on the notion of GromovWasserstein distance are fast  accurate and can determine automatically the number of both row and column
clusters  These features are vividly demonstrated
through extensive experimental evaluations 

  Introduction
Cluster analysis aims to gather data instances into groups 
called clusters  where instances within one group are similar among themselves while instances in different groups
are as dissimilar as possible  Clustering methods have become more and more popular recently due to their ability
to provide new insights into unlabeled data that may be
dif cult or even impossible to capture for   human being 

 CNRS  LIPN  Universit   Paris     Sorbonne Paris Cit   
France  CNRS UMR     INSERM    Univ  Lyon  
INSA Lyon     Villeurbanne  France  CNRS  LJK  Univ 
GrenobleAlpes  France  Correspondence to  Charlotte Laclau
 charlotte laclauc univgrenoble alpes fr 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

 The  rst author of this paper is now   postdoc in CNRS 

LIG  Univ  GrenobleAlpes  France

Clustering methods  however  do not take into account the
possible existing relationships between the features that describe the data instances  For example  one may consider
  data matrix extracted from text corpus where each document is described by the terms appearing in it 
In this
case  clustering documents may bene   from the knowledge about the correlation that exists between different
terms revealing their probability of appearing in the same
documents  This idea is the cornerstone of coclustering
 Hartigan    Mirkin    where the goal is to perform clustering of both data points and features simultaneously  The obtained latent structure of data is composed of blocks usually called coclusters  Applications of
coclustering include but are not limited to recommendation systems  George   Merugu    Deodhar   Ghosh 
  Xu et al    gene expression analysis  Cheng
et al    Hanisch et al    and text mining  Dhillon
et al      Wang et al    As   result  these methods
are of an increasing interest to the data mining community 
Coclustering methods are often distinguished into probabilistic methods        Dhillon et al      Banerjee
et al    Nadif   Govaert    Wang et al   
Shan   Banerjee    and metric based        Rocci
  Vichi    Ding et al    methods  Probabilistic
methods usually make an assumption that data was generated as   mixture of probability density functions where
each one of them corresponds to one cocluster  The goal
then is to estimate the parameters of the underlying distributions and the posterior probabilities of each cocluster
given the data  Metric based approaches proceed in   different way and rely on introducing and optimizing   criterion commonly taking into account intraand interblock
variances  This criterion  in its turn  is de ned using some
proper metric function that describes the geometry of data
in the most precise way possible  Both metric and probabilistic approaches are known to have their own advantages
and limitations  despite being quite ef cient in modeling
the data distribution  probabilistic methods are computationally demanding and hardly scalable  metric methods
are less computationally demanding but present the need to
choose the  right  distance that uncovers the underlying latent coclusters  structure based on available data  Furthermore  the vast majority of coclustering methods require
the number of coclusters to be set in advance  This is usu 

Coclustering through Optimal Transport

ally done using the computationally expensive exhaustive
search over   large number of possible pairs of row and
column clusters as in  Keribin et al    Wyse   Friel 
  Wyse et al   
In this paper  we address the existing issues of coclustering
methods described above by proposing   principally new
approach that ef ciently solves the coclustering problem
from both qualitative and computational points of view
and allows the automatic detection of the number of coclusters  We pose the coclustering problem as the task of
transporting the empirical measure de ned on the data instances to the empirical measure de ned on the data features  The intuition behind this process is very natural to
coclustering and consists in capturing the associations between instances and features of the data matrix  The solution of optimal transportation problem is given by   doublystochastic coupling matrix which can be considered as the
approximated joint probability distribution of the original
data  Furthermore  the coupling matrix can be factorized
into three terms where one of them re ects the posterior
distribution of data given coclusters while two others represent the approximated distributions of data instances and
features  We use these approximated distributions to obtain
the  nal partitions  We also derive   kernelized version of
our method that contrary to the original case  is based on
an optimal transportation metric de ned on the space of
dissimilarity functions 
The main novelty of our work is twofold  To the best
of our knowledge  the proposed approach is    rst attempt to apply entropy regularized optimal transport for coclustering and to give its solution   coclustering interpretation  While Wasserstein distance has already been adapted
to design clustering algorithms  Cuturi   Doucet    Irpino et al    our idea is to concentrate our attention on
the solution of the optimal transport given by the coupling
matrix and not to minimize the quantization error with respect to         Wasserstein distance  We also note that
using entropy regularization leads to   very ef cient algorithm that can be easily parallelized  Cuturi    Second  we show that under some plausible assumptions the
density estimation procedure appearing from the use of the
optimal transport results in the variational inference problem with the minimization of the reversed KullbackLeibler
divergence  The important implications of this difference
       other existing methods are explained in Section  
The rest of this paper is organized as follows  In Section
  we brie   present the discrete version of the optimal
transportation problem and its entropy regularized version 
Section   proceeds with the description of the proposed approach  its theoretical analysis and algorithmic implementation  In Section   we evaluate our approach on synthetic
and realworld data sets and show that it is accurate and

substantially more ef cient than the other stateof theart
methods  Last section concludes the paper and gives   couple of hints for possible future research 

  Background and notations
In this section  we present the formalization of the MongeKantorovich  Kantorovich    optimization problem
and its entropy regularized version 

  Optimal transport

 cid NS

transportation theory was  rst

Optimal
introduced in
 Monge    to study the problem of resource allocation 
Assuming that we have   set of factories and   set of mines 
the goal of optimal transportation is to move the ore from
mines to factories in an optimal way       by minimizing
the overall transport cost 
More formally  given two empirical probability measures 
de ned as
      
NS
uniformly weighted sums of Dirac with mass at locations
supported on two point sets XS    xS
   and
XT    xT
   the MongeKantorovich problem
consists in  nding   probabilistic coupling   de ned as  
joint probability measure over XS   XT with marginals
   and    that minimizes the cost of transport        some
metric     Xs   Xt     

 cid NT
    xT
    Rd NS

    Rd NT

and       
NT

    xS

 

 

min

       

 cid     cid  

 

where  cid cid   is the Frobenius dot product            
    RNS NT
                   is   set of doubly
stochastic matrices and   is   dissimilarity matrix      
    de ning the energy needed to move  
Mij     xS
probability mass from xS
    This problem admits  
 
unique solution   and de nes   metric on the space of
probability measures  called the Wasserstein distance  as
follows 

    xT

to xT

            

min

       

 cid     cid    

The Wasserstein distance has been successfully used in various applications  for instance  computer vision  Rubner
et al    texture analysis  Rabin et al    tomographic reconstruction     Abraham   Carlier    domain adaptation  Courty et al    metric learning  Cuturi   Avis    and clustering  Cuturi   Doucet   
Irpino et al    This latter application is of   particular
interest as Wasserstein distance is known to be   very ef 
cient metric due to its capability of taking into account the

 Due space limitation  we present only the discrete version
of optimal transport  For more details on the general continuous
case and the convergence of empirical measures  we refer the interested reader to the excellent monograph by  Villani   

Coclustering through Optimal Transport

geometry of data through the pairwise distances between
samples  The success of algorithms based on this distance
is also due to  Cuturi    who introduced an entropy regularized version of optimal transport that can be optimized
ef ciently using matrix scaling algorithm  We present this
regularization below 

  Entropic regularization

The idea of using entropic regularization dates back to
 Schr odinger    In  Cuturi    it found its application to the optimal transportation problem through the
following objective function 

  

Second term       cid NS  NT

 cid     cid      
 

       

min

   

     log      in this
equation allows to obtain smoother and more numerically
stable solutions compared to the original case and converges to it at the exponential rate  Benamou et al   
Another advantage of entropic regularization is that it allows to solve optimal transportation problem ef ciently using SinkhornKnopp matrix scaling algorithm  Sinkhorn  
Knopp   
In the next section  we explain the main underlying idea
of our approach that consists in associating data instances
with features through regularized optimal transport 

  Coclustering through optimal transport
In this section we show how the coclustering problem can
be casted in   principally new way and then solved using
the ideas from the optimal transportation theory 

  Problem setup

   and  yc  

Let us denote by   and   two random variables taking values in the sets  xr  
   respectively 
where subscripts   and   correspond to rows  instances  and
columns  features  Similar to  Dhillon et al      we
assume that the joint probability distribution between  
and   denoted by          is estimated from the data matrix     Rn    We further assume that   and   consist
of instances that are distributed        probability measures
       supported on        where      Rd and      Rn 
respectively 
The problem of coclustering consists in jointly grouping the set of features and the set of instances into homogeneous blocks by  nding two assignment functions
Cr and Cc that map as follows  Cr               xn   
             xg  Cc               yd                 ym  where  
and   denote the number of row and columns clusters  and
discrete random variables    and    represent the partitions
induced by   and               Cr      and      Cc    

 cid  

To use discrete optimal transport  we also de ne two empirical measures    and    based on   and   as follows 
    yi  We are now
      
 
ready to present our method 

    xi and       

 

 cid  

  Proposed approach

The main underlying idea of our approach is to use the optimal transportation presented above to  nd   probabilistic coupling of the empirical measures de ned based on
rows and columns of   given data matrix  More formally 
for some  xed       we solve the coclustering problem
through the following optimization procedure 
    argmin     cid     cid      
 
 

  

 

where the matrix   is computed using the Euclidean distance       Mij    cid xi   yj cid  The elements of the resulting matrix  
  provides us with the weights of associations
between instances and features  similar instances and features correspond to higher values in  
  Our intuition is to
use these weights to identify the most similar sets of rows
and columns that should be grouped together to form coclusters 
Following  Benamou et al    this optimization problem can be equivalently rewritten in the following way 

min

 cid     cid      
 

    

min

     
where         is the Gibbs kernel 
Finally  we can rewrite the last expression as follows 

     

 
 

KL cid 

min

     

KL cid    min

   KL cid 

where             is the intersection of closed convex subsets given by          Rd          and
         Rd             The solution of the
entropy regularized optimal transport can be obtained using SinkhornKnopp algorithm and has the following form
 Benamou et al   

 
    diag diag 

 

where   and   are the scaling coef cients of the Gibbs
kernel  
In what follows  we show that under some plausible assumptions  we can interpret these two vectors as approximated rows and columns probability density functions 

  Connection to variational inference

In order to justify our approach from the theoretical point
of view  we  rst explain how the obtained solution  

Coclustering through Optimal Transport

can be used for coclustering  As mentioned in  Dhillon
et al      and later in  Banerjee et al   
the
coclustering can be seen as   density estimation problem where the goal is to approximate the real density
         by   simpler one depending on the obtained
coclustering in   way that it preserves the loss in the
mutual information given by                        where
         dxdy is the mutual information  This quantity is further shown to be equal to the
KullbackLeibler divergence between the original distribution          and          where the latter has the following form 

          cid 

XY         log       

                              

From this point  one may instantly see that the solution of
the optimal transport problem   has   very similar form
as it also represents the joint probability distribution that
approximates the original probability distribution        
given by the Gibbs measure   and also factorizes into three
terms  The most important difference  however  lies in the
asymmetry of the KL divergence  while  Dhillon et al 
    and  Banerjee et al    concentrate on minimizing KL         cid          our idea is different and
consists in minimizing KL         cid          This approach is known in the literature as the variational inference
 Bishop    and exhibits   totally different behaviour
compared to the minimization of KL         cid         
As shown by  Bishop    in variational inference the
estimated distribution          concentrates on the modes
of data and remains compact  while the minimizer of
KL         cid          tends to cover the whole surface of
the original density and to overestimate its support  As   
  and    and    represent the observed and unobserved
variables  respectively  the natural goal is to try to estimate the distribution                   of the data given the
obtained coclusters by the simpler variational distribution
         However  as the maximisation of                  
is computationally impossible  it is common to introduce  

free distribution    on the parameters  cid   and cid   in order

to obtain the following decomposition 
log                        KL            cid                  
where the lower bound
               

               

         log

  xd  

 cid 

 cid 

      

      

        

is maximized when the KL divergence is minimized 
Now  if we assume that                   follows the Gibbs
distribution                                     we can consider the original formulation of the regularized optimal
transport as the variational inference problem 
KL            cid                     min

KL cid 

min

 

 

where the optimal coupling   equals to the estimated joint
probability            
At this point  we know that the coupling matrix can be seen
as an approximation to the original unknown posterior density function but the question how one can use it to obtain
the clustering of rows and columns has not been answered
yet 
In order to solve the variational inference problem 
it is usually assumed that the variables        are independent and thus the variational distribution          factorizes
as                     This assumption  however  goes
against the whole idea of coclustering that relies on the existence of   deep connection between these two variables 
To this end  we propose to consider the factorization of
         that has the following form

                                

This particular form follows the idea of structured stochastic variational inference proposed in  Hoffman   Blei 
  where   term depicting the conditional distribution
between hidden and observed variables is added to the fully
factorized traditional setting presented above  As stated in
 Hoffman   Blei    this term allows arbitrary dependencies between observed and hidden variables which can
increase the  delity of the approximation 
Following  Bishop    the optimal estimated densities
     and      are controlled by the direction of the smallest variance of      and      respectively  Furthermore 
     and      are proportional to the joint densities        
and                             and                Bearing in mind the equivalence between  
  and          this
brings us to the following important conclusions    the
matrices diag  and diag  can be seen as the approximated densities             and             vectors   and
  represent the approximated densities      and       obtained by summing   and   out of           and            
respectively 
According to  Laird    the nonparametric estimate of
the mixing distribution is   piecewise step function where
the number of steps depend on the number of components
in the mixture  In the cluster analysis  we can assume that
     and       consist of   and   components  respectively  Then  our goal is to detect these steps based on the
estimates given by   and   to obtain the desired partitions 

  Kernelized version and GromovWasserstein

distance

In this part  we introduce the kernelized version of our
method and compare it to the original formulation of our
algorithm  In order to proceed  we  rst de ne two similarity matrices Kr   Rn   and Kc   Rd   associated
to empirical measures        thus forming metricmeasure

Coclustering through Optimal Transport

spaces as in    emoli    Matrices Kr and Kc are de 
 ned by calculating the pairwise distances or similarities
between rows and columns  respectively  without the restriction of them being positive or calculated based on  
proper distance function satisfying the triangle inequality 
The entropic GromovWasserstein discrepancy in this case
is de ned as follows  Peyr   et al   

GW Kr  Kc            min

         

 Kr Kc        

  min

          

  Kri     Kck                 

 cid 

       

where   is   coupling matrix between two similarity matrices and                is an arbitrary lostfunction 
usually the quadraticloss or KullbackLeibler divergence 
Based on this de nition  one may de ne the problem of the
entropic GromovWasserstein barycenters for similarity or
distance matrices Kr and Kc as follows 

     Ki           

 

 cid 

min

     

      

to one      cid 

where   is the computed barycenter and              
   are the coupling matrices that align it with Kr and
Kc  respectively     are the weighting coef cients summing
              that determine our interest in
more accurate alignment between Kr and   or Kc and   
The intuition behind this optimization procedure for coclustering with respect to original formulation given in  
is the following  while in   we align rows with columns
directly  in   our goal is to do it via an intermediate representation given by the barycenter   that is optimally
aligned with both Kr and Kc  In this case  we obtain the
solutions    and    that  similar to   can be decomposed
as follows 
 
    diag   rdiag     
    diag   cdiag   
where        Mr and        Mc are Gibbs kernels calculated between the barycenter and row and column similarity matrices using any arbitrary lossfunction   as explained before  Finally  based on the analysis presented
above  we further use vectors    and    to derive row and
column partitions 

  Detecting the number of clusters

In order to detect the steps  or jumps  in the approximated
marginals  we propose to adapt   procedure introduced
in  Matei   Meignen    for multiscale denoising of
piecewise smooth signals  This method is of particular interest for us as it determines the signi cant jumps in the
vectors   and   without knowing their number and location  nor   speci   threshold to decide the signi cance

of   jump  As the proposed procedure deals with nondecreasing functions  we  rst sort the values of   and  
in the ascending order  Since the procedure is identical for
both vectors  we only describe it for the vector  
We consider that the elements     
   of   are the local
   
averages of   piecewise continuous function    
      on the intervals    
                   de ned by
the uniform subdivision of step    of the interval    
    dt                      The
More precisely    
detection strategy is based on the following cost function 
    de ned for each interval 
      
Therefore  we get the list of the interval suspicious to contain   jump for the subdivision of order   as follows 

       cid  

      cid 

   
 

       

       

Ln            argmaxiF In

   

This detection should be re ned in order to get only significant jumps in our vector   To this end we use the multiscale representation of   as in  Harten    and we perform this detection on each scale  On the  rst scale  we get
  coarse version of   by averaging 

   

   

 
 

  

       

                        

Now  by considering the coarse version of   we obtain  
second list Ln  of suspicious intervals as before  After
that  these two lists merge in the list Ljumps as follows   
jump will be considered in the interval    
    if the
interval     
is also detected as suspicious at the coarse
scale  This procedure is iterated  log     times and   jump
is observed if   chain of detection exists from  ne to coarse
scales  Finally  the number of clusters is obtained by    
 Ljumps     

   or    

 

  Algorithmic implementation

We now brie   summarize the main steps of both CCOT
and CCOTGW methods and discuss their peculiarities with
respect to each other  The pseudocode of both approaches
in Matlab are presented in Algorithm   and Algorithm  
respectively 

CCOT First step of our algorithm consists in calculating
the cost matrix   and using it to obtain the optimal coupling matrix  
  by applying the regularized optimal transport  In order to calculate    row and column instances
should both lie in   space of the same dimension  This condition  however  is veri ed only if the matrix   is squared
which occurs rarely in the realworld applications  To overcome this issue  we  rst subsample the original data set  
in   way that allows us to equalize the number of rows and
columns and operate with two sets of the same dimension 
If we assume that       then this new reduced data set is
denoted by     Rd    We repeat the sampling procedure
until every individual is picked at least once 

Coclustering through Optimal Transport

Algorithm   Coclustering through Optimal Transport
with GromovWasserstein barycenters  CCOTGW 
Input

      data matrix      regularization parameter          
weights for barycenter calculation
Output  Cr  Cc   partition matrices for rows and columns        
Kr   pdist      
Kc   pdist ZT  ZT 
         
jumps  Cr       jump detection sort   
    
jumps  Cc       jump detection sort   
    

      gw barycenter Kr  Kc           

number of row and column clusters

     

  Experimental evaluations
In this section  we provide empirical evaluation for the proposed algorithms 

  Synthetic data
Simulation setting We simulate data following the generative process of the Gaussian Latent Block Models  for
details see  Govaert   Nadif    and we consider four
scenarios with different number of coclusters  degree of
separation and size  Table   and Figure   present the characteristics of theta simulated data sets and their visualization showing the different coclustering structures 

Table   Size         number of coclusters         degree of
overlapping   for wellseparated and   for illseparated coclusters  and the proportions of coclusters for simulated data sets 

Data set
  
  
  
  

     

     
     
     
     

      Overlapping
     
     
     
     

 
 
 
 

Proportions

Equal
Unequal
Equal
Unequal

The next step is to perform for each               ns the jump
detection on the sorted vectors    and    to obtain two lists
of the jumps locations    
jumps and to de ne the
number of row and column clusters   and    By using
them  we obtain the resulting row partition 

jumps and    

 

   

  xr   

jumps 

         
     
      
     
    
jumps             

jumps                
jumps    

jumps   
jumps 

The partition for columns    
  yc  is obtained in the same
way  Finally  we apply the majority vote over all samples
partitions to obtain Cr and Cc  Regarding complexity  both
SinkhornKnopp algorithm used to solve the regularized
optimal transport  Knight    and the proposed jump
detection techniques are known to converge at the linear
rate multiplied by the number of samples         nsd  On
the other hand  the calculation of modes of the clustering
obtained on the generated samples for both features and
data instances has the complexity   ns      In the end 
the complexity of the whole algorithm is   ns       
We also note that in the realworld applications  we usually deal with scenarios where    cid     big data  or
   cid     small  data  thus reducing the overall complexity to   nsn  and   nsd  respectively  This makes our
approach even more computationally attractive 

Algorithm   Coclustering through Optimal Transport
 CCOT 
Input

      data matrix      regularization parameter  ns   number of sampling
Output  Cr  Cc   partition matrices for rows and columns        

number of row and column clusters

         size   
for       to ns do

Di   datasample      
Mi   pdist Di  DT
  
            optimal transport Mi   
        jump detection sort   
    
jumps  Ci
        jump detection sort   
    
jumps  Ci
Cr   mode Ci
  
Cc   mode Ci
  

CCOTGW As
it can be seen from Algorithm  
CCOTGW allows to overcome the important disadvantage
of CCOT that consists in the need to perform sampling to
cluster all data objects  On the other hand  the computational complexity of CCOT is only   nsd  while for
CCOTGW it scales as              We also note that
CCOTGW offers   great  exibility in terms of the possible data representation used at its input  One may easily
consider using any arbitrary kernel function to calculate
similarity matrices or even learn them beforehand using
multiplekernel learning approaches 

      

      

      

Figure         and    reorganized        the true partitions 

We use several stateof theart coclustering algorithms as
baselines including ITCC  Dhillon et al      Double
KMeans  DKM   Rocci   Vichi    Orthogonal Nonnegative Matrix TriFactorizations  ONTMF   Ding et al 
  the Gaussian Latent Block Models  GLBM   Nadif
  Govaert    Govaert   Nadif    and Residual
Bayesian CoClustering  RBC   Shan   Banerjee   

Coclustering through Optimal Transport

Table   Mean   standarddeviation  of the coclustering error  CCE  obtained for all con gurations    indicates that the algorithm
cannot  nd   partition with the requested number of coclusters  Pvalues obtained using the nonparametric test of Wilcoxon  Wilcoxon 
  that imply signi cant differences are printed in bold  signi cance level of  

Data set

  
  
  
  

Kmeans
     
     
     

 

NMF

     
     

 
 

DKM

     
     
     
     

TriNMF
     
     

 
 

GLBM

Algorithms
     
     
     
     

ITCC

     
     
     
     

RBC

     
     
     

 

CCOT

     
     
     
     

CCOTGW
     
     
     
     

We also report the results of Kmeans and NMF  run on
both modes of the data matrix  as clustering baseline  To
assess the performance of all compared methods  we compute the coclustering error  CCE   Patrikainen   Meila 
  de ned as follows 
CCE                                                 

where    and    are the partitions of instances and variables
estimated by the algorithm    and   are the true partitions
and           resp           denotes the error rate       the
proportion of misclassi ed instances  resp  features 
For all con gurations  we generate   data sets and compute the mean and standard deviation of the CCE over all
sets  As all the approaches we compared with are very
sensitive to the initialization  we run them   times with
random initializations and retain the best result according
to the corresponding criterion  RBC is initialized with Kmeans  Regarding CCOT we set ns to   for all con 
 gurations except    which has the same number of rows
and columns  and therefore does not require any sampling 
For CCOTGW  we use Gaussian kernels for both rows and
columns with   computed as the mean of all pairwise Euclidean distances between vectors  Kar   Jain    Finally  we let both CCOT and CCOTGW detect automatically the number of coclusters  while for all other algorithms we set the number of clusters to its true value 

Coclustering performance We report the mean  and
standard deviation  of coclustering errors obtained in Table   Based on these results  we observe that on    which
has   clear block structure  all algorithms perform equally
well  however CCOTGW gives the best results  closely followed by CCOT and Kmeans  Regarding       and
   which have more complicated structure than    both
CCOT and CCOTGW signi cantly outperform all other algorithms and this difference is all the more important on   
and    where some of the compared algorithms are unable
to  nd   partition with the desired number of clusters 
Furthermore  we argued that one of the strengths of our
method is its ability to detect automatically the number
of coclusters by applying   jump detection algorithm on
  and   From Figure   one can observe that the plots

of these vectors  obtained with CCOT  with their elements
sorted in the ascending order reveal clear steps that correspond to the correct number of clusters and also illustrate
their proportions and the degree of overlapping  The same
observation is valid for CCOTGW  Both approaches correctly identi ed the number of clusters in most cases and
CCOT is slightly more accurate than CCOTGW when the
proportions of coclusters are unbalanced 

   

Data Set

CCOT

  
  
  
  

 

 
 
 
 

 
 
 
 
 

   

   

CCOTGW
 
 
 
 
 
 
 
 
 
 

Figure   Vectors       and       obtained with CCOT on   
    Number of times CCOT and CCOTGW correctly detect the
number of coclusters    and    over   trials 

To summarize  CCOT and CCOTGW outperform all the
other baselines for the considered data structures and
present two important advantages    they do not suffer
from the initialization issues    they are able to detect automatically the number coclusters 

  MovieLens
Data and setting MOVIELENS   
is   popular
benchmark data set that consists of usermovie ratings  on
  scale of one to  ve  collected from   movie recommendation service gathering   ratings from   users on
  movies  In the context of coclustering  our goal is
to  nd homogeneous subgroups of users and  lms in order
to further recommend previously unseen movies that were

 https grouplens org datasets movielens   

    Instances Variables Coclustering through Optimal Transport

highly rated by the users from the same group 
We set
the regularization parameters for CCOT and
CCOTGW using the crossvalidation  the number of samplings for CCOT is set to    as the dimensions of the data
set are quite balanced  the weights for the barycenter in
CCOTGW are set to        

Results
In what follows we only present  gures and results obtained by CCOTGW as both algorithms return the
same number of blocks and the partitions are almost identical  with   normalized mutual information between partitions above   CCOTGW automatically detects   structure consisting of       blocks  that corresponds to   user
clusters and   movie clusters  From Figure   one can
observe that the users and the movies are almost equally
distributed across clusters  except for two user and three
movie clusters which have   larger size than others 

   

   

Figure   Distribution of the number of     users and     movies
across the clusters obtained with CCOTGW 

Figure   shows the original data set as well as   summarized version where each block is represented by its mean
rating value  the lighter the block  the higher the ratings 
revealing   structure into homogeneous groups  One can
observe that the  rst movie cluster consists of  lms for
which all users agree on giving high ratings  most popular movies  while the last movie cluster consists of the
movies with very low ratings  We also report the   best
rated movies in those two clusters in Table   One can
easily see that popular movies  such that both Star Wars
episodes are in    while    is composed of movies that
were less critically acclaimed 

Table   Top   of movies in clusters    and   

  

  

Star Wars  

The Lion King  
Return of the Jedi  

Contact  

Raiders of the lost ark  

Amytiville    New Generation  
Amytiville  It   About Time  

Ninjas  High Noon at Mega Mountain  

Sudden Manhattan  

Dream Man  

We can make similar observations for the interpretation of
user clusters  For instance  the last two user clusters include
users that tend to give less good ratings to movies than the
average population  Also  we note that block     cor 

   

   

Figure       MOVIELENS matrix      the matrix summarized by
the mean rating   ratings are excluded  for each block obtained
with CCOTGW  Darker shades indicate lower values 

responds to users who liked movies from    better than
the rest of the users  These observations are also very similar to the results reported by  Banerjee et al    where
the authors proposed   detailed study of         blocks
structure for this data set  Additional results can be found
in the Supplementary material 

  Conclusions and future perspectives
In this paper we presented   novel approach for coclustering based on the entropy regularized optimal transport  Our method is principally different from other coclustering methods and consists in  nding   probabilistic
coupling of the empirical measures de ned based on the
data instances and features  We showed how this procedure can be seen as the variational inference problem and
that the inferred distribution can be used to obtain the row
and feature partitions  The resulting algorithm is not only
more accurate than other stateof theart methods but also
fast and capable of automatically detecting the number of
coclusters  We also presented an extended version of our
algorithm that makes use of the optimal transportation distance de ned on similarity matrices associated to the rows 
and columns  empirical measures 
In the future  our work can be continued in multiple directions  First  we would like to extend our method in order to
deal with the online setting where the goal is to classify  
new previously unseen observation without the need to do
the coclustering of the data set that includes it  This can
be done using   recent approach proposed in  Perrot et al 
  that allows to update the learned coupling matrix using the outof sample observations without recomputing it
using all the data  We believe that this extension will make
our algorithm attractive for the exploitation in realtime industrial recommendation systems due to its computational
ef ciency  We would also like to study the generalization
properties of our algorithm in   spirit similar to the results obtained in  Maurer   Pontil    This latter work
presents   rare case where the generalization bounds are derived for some famous unsupervised learning algorithms 

 User clustersNumber of users Movie clustersNumber of moviesCoclustering through Optimal Transport

Acknowledgements
This work has been supported by the ANR project
COCLICO  ANR MONU 

References
Banerjee  Arindam  Dhillon  Inderjit  Ghosh  Joydeep 
Merugu  Srujana  and Modha  Dharmendra   
 
generalized maximum entropy approach to bregman
Journal of
coclustering and matrix approximation 
Machine Learning Research    December
 

Benamou  JeanDavid  Carlier  Guillaume  Cuturi  Marco 
Nenna  Luca  and Peyr    Gabriel  Iterative Bregman Projections for Regularized Transportation Problems  SIAM
Journal on Scienti   Computing       
 

Bishop  Christopher    Pattern Recognition and Machine
Learning  Information Science and Statistics  SpringerVerlag New York  Inc   

Cheng  KinOn  Law  NgaiFong  Siu  WanChi  and Liew 
Alan    Identi cation of coherent patterns in gene expression data using an ef cient biclustering algorithm
and parallel coordinate visualization  BMC Bioinformatics     

Courty  Nicolas  Flamary    emi  and Tuia  Devis  Domain
In Pro 

adaptation with regularized optimal transport 
ceedings ECML PKDD   pp     

Cuturi  Marco  Sinkhorn distances  Lightspeed compuIn Proceedings NIPS  pp 

tation of optimal transport 
   

Ding     Li     Peng     and Park     Orthogonal nonnegative matrix trifactorizations for clustering  In Proceedings ACM SIGKDD  pp     

George     and Merugu       scalable collaborative  ltering framework based on coclustering  In Proceedings
ICDM  pp     

Govaert     and Nadif     Coclustering  John Wiley  

Sons   

Hanisch  Daniel  Zien  Alexander  Zimmer  Ralf  and
Lengauer  Thomas  Coclustering of biological networks and gene expression data  BMC Bioinformatics 
 suppl    

Harten  Amiram  Eno schemes with subcell resolution 

Journal of Computational Physics     

Hartigan        Direct Clustering of   Data Matrix  Journal
of the American Statistical Association   
   

Hoffman  Matthew    and Blei  David    Stochastic strucIn Proceedings AISTATS 

tured variational inference 
volume   pp     

   Abraham     Abraham     Bergounioux and Carlier    
Tomographic reconstruction from   few views    multimarginal optimal transport approach  Applied Mathematics and Optimization  pp     

Irpino  Antonio  Verde  Rosanna  and De Carvalho  Francisco de      Dynamic clustering of histogram data
based on adaptive squared wasserstein distances  Expert
Systems with Applications     

Cuturi  Marco and Avis  David  Ground metric learning 
Journal of Machine Learning Research   
 

Kantorovich  Leonid  On the translocation of masses  In
      Doklady  Acad  Sci  URSS      volume  
pp     

Cuturi  Marco and Doucet  Arnaud  Fast computation of
wasserstein barycenters  In Proceedings ICML  pp   
   

Deodhar     and Ghosh     Scoal    framework for simultaneous coclustering and learning from complex data 
ACM Transactions on Knowledge Discovery from Data 
   

Dhillon  Inderjit    Mallela  Subramanyam  and Kumar 
Rahul    divisive information theoretic feature clustering algorithm for text classi cation  Journal of Machine
Learning Research       

Kar  Purushottam and Jain  Prateek  Similaritybased learnIn NIPS  pp   

ing via data driven embeddings 
   

Keribin  Christine  Brault  Vincent  Celeux  Gilles  and Govaert    erard  Estimation and selection for the latent
block model on categorical data  Statistics and Computing     

Knight  Philip    The sinkhornknopp algorithm  Convergence and applications  SIAM Journal on Matrix Analysis and Applications    March  

Dhillon  Inderjit    Mallela  Subramanyam  and Modha 
Dharmendra    Informationtheoretic coclustering  In
Proceedings ACM SIGKDD  pp       

Laird     Nonparametric maximum likelihood estimation
of   mixing distribution  Journal of the American Statistical Association     

Coclustering through Optimal Transport

Shan  Hanhuai and Banerjee  Arindam  Residual bayesian
coclustering for matrix approximation  In Proceedings
of the SIAM International Conference on Data Mining 
SDM   April     May     Columbus  Ohio 
USA  pp     

Sinkhorn  Richard and Knopp  Paul  Concerning nonnegative matrices and doubly stochastic matrices  Paci  
Journal of Mathematics     

Villani    edric 

old and
new  Grundlehren der mathematischen Wissenschaften 
Springer  Berlin   

Optimal

transport

 

Wang  Pu  Domeniconi  Carlotta  and Laskey  Kathryn  Latent dirichlet bayesian coclustering  Machine Learning
and Knowledge Discovery in Databases  pp   
 

Wilcoxon     Individual Comparisons by Ranking Meth 

ods  Biometrics Bulletin    December  

Wyse  Jason and Friel  Nial  Block clustering with collapsed latent block models  Statistics and Computing 
   

Wyse  Jason  Friel  Nial  and Latouche  Pierre 

Inferring structure in bipartite networks using the latent block
model and exact ICL  ArXiv eprints   

Xu  Bin  Bu  Jiajun  Chen  Chun  and Cai  Deng  An exploration of improving collaborative recommender systems
via useritem subgroups  In Proceedings WWW  pp   
   

Matei  Basarab and Meignen  Sylvain  Nonlinear cellaverage multiscale signal representations  Application
to signal denoising  Signal Processing   
 

Maurer  Augusto and Pontil  Massimiliano     dimensional
coding schemes in hilbert spaces  IEEE Trans  Information Theory     

  emoli  Facundo  Gromovwasserstein distances and the
metric approach to object matching  Foundations of
Computational Mathematics     

Mirkin  Boris Grigorievitch  Mathematical classi cation
and clustering  Nonconvex optimization and its applications  Kluwer academic publ  Dordrecht  Boston  London   

Monge  Gaspard    emoire sur la th eorie des   eblais et des
remblais  Histoire de   Acad emie Royale des Sciences 
pp     

Nadif     and Govaert     Algorithms for modelbased
block gaussian clustering  In DMIN  the   International Conference on Data Mining   

Patrikainen     and Meila     Comparing subspace clusterings  IEEE Transactions on Knowledge and Data Engineering    July  

Perrot  Micha el  Courty  Nicolas  Flamary    emi  and
Habrard  Amaury  Mapping estimation for discrete optimal transport  In NIPS  pp     

Peyr    Gabriel  Cuturi  Marco  and Solomon  Justin 
Gromovwasserstein averaging of kernel and distance
matrices  In Proceedings of the  nd International Conference on Machine Learning  ICML   New York
City  NY  USA  June     pp     

Rabin  Julien  Peyr    Gabriel  Delon  Julie  and Bernot 
Marc  Wasserstein barycenter and its application to texIn Proceedings SSVM  volume   pp 
ture mixing 
   

Rocci     and Vichi     Twomode multipartitioning 
Computational Statistics and Data Analysis   
   

Rubner  Yossi  Tomasi  Carlo  and Guibas  Leonidas    The
earth mover   distance as   metric for image retrieval  International Journal on Computer Vision   
 

Schr odinger    

Uber die umkehrung der naturgesetze  Sitzungsberichte Preuss  Akad  Wiss  Berlin  Phys 
Math     

