Tensor Balancing on Statistical Manifold

Mahito Sugiyama     Hiroyuki Nakahara   Koji Tsuda      

Abstract

We solve tensor balancing  rescaling an Nth order nonnegative tensor by multiplying   tensors of order    cid    so that every  ber sums to
one  This generalizes   fundamental process of
matrix balancing used to compare matrices in  
wide range of applications from biology to economics  We present an ef cient balancing algorithm with quadratic convergence using Newton   method and show in numerical experiments
that the proposed algorithm is several orders of
magnitude faster than existing ones  To theoretically prove the correctness of the algorithm 
we model tensors as probability distributions in
  statistical manifold and realize tensor balancing as projection onto   submanifold  The key to
our algorithm is that the gradient of the manifold 
used as   Jacobian matrix in Newton   method 
can be analytically obtained using the   obius inversion formula  the essential of combinatorial
mathematics  Our model is not limited to tensor balancing  but has   wide applicability as it
includes various statistical and machine learning
models such as weighted DAGs and Boltzmann
machines 

  Introduction
Matrix balancing is the problem of rescaling   given square
nonnegative matrix     Rn cid   cid 
to   doubly stochastic matrix RAS  where every row and column sums to one  by
multiplying two diagonal matrices   and    This is  
fundamental process for analyzing and comparing matrices in   wide range of applications  including inputoutput
analysis in economics  called the RAS approach  Parikh 
  Miller   Blair    Lahr   de Mesnard   
seat assignments in elections  Balinski    Akartunal   

 National Institute of Informatics  JST PRESTO  RIKEN
Brain Science Institute  Graduate School of Frontier Sciences 
The University of Tokyo  RIKEN AIP  NIMS  Correspondence
to  Mahito Sugiyama  mahito nii ac jp 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure  Overview of our approach 

Knight    HiC data analysis  Rao et al    Wu  
Michor    the Sudoku puzzle  Moon et al    and
the optimal transportation problem  Cuturi    Frogner
et al    Solomon et al    An excellent review of
this theory and its applications is given by Idel  
The standard matrix balancing algorithm is the SinkhornKnopp algorithm  Sinkhorn    Sinkhorn   Knopp 
  Marshall   Olkin    Knight      special
case of Bregman   balancing method  Lamond   Stewart 
  that iterates rescaling of each row and column until
convergence  The algorithm is widely used in the above
applications due to its simple implementation and theoretically guaranteed convergence  However  the algorithm
converges linearly  Soules    which is prohibitively
slow for recently emerging large and sparse matrices  Although Livne   Golub   and Knight   Ruiz  
tried to achieve faster convergence by approximating each
step of Newton   method  the exact Newton   method with
quadratic convergence has not been intensively studied yet 
Another open problem is tensor balancing  which is   generalization of balancing from matrices to higherorder multidimentional arrays  or tensors  The task is to rescale an
Nth order nonnegative tensor to   multistochastic tensor 
in which every  ber sums to one  by multiplying     cid   th
order   tensors  There are some results about mathematical properties of multistochastic tensors  Cui et al   
Chang et al    Ahmed et al    However  there
is no result for tensor balancing algorithms with guaranteed convergence that transforms   given tensor to   multistochastic tensor until now 

Every  cid bersums to  Given tensor AMultistochastic tensor   Submanifold  Probabilitydistribution PStatistical manifold  dually  cid at Riemannian manifold ProjectionTensor balancingProjecteddistribution   Projecteddistribution   Tensor Balancing on Statistical Manifold

Here we show that Newton   method with quadratic convergence can be applied to tensor balancing while avoiding solving   linear system on the full tensor  Our strategy is to realize matrix and tensor balancing as projection onto   dually  at Riemmanian submanifold  Figure  
which is   statistical manifold and known to be the essential structure for probability distributions in information
geometry  Amari    Using   partially ordered outcome space  we generalize the loglinear model  Agresti 
  used to model the higherorder combinations of binary variables  Amari    Ganmor et al    Nakahara   Amari    Nakahara et al    which allows
us to model tensors as probability distributions in the statistical manifold  The remarkable property of our model is
that the gradient of the manifold can be analytically computed using the   obius inversion formula  Rota    the
heart of combinatorial mathematics  Ito    which enables us to directly obtain the Jacobian matrix in Newton  
method  Moreover  we show that     cid     entries for the
size nN of   tensor are invariant with respect to one of the
two coordinate systems of the statistical manifold  Thus
the number of equations in Newton   method is   nN cid 
The remainder of this paper is organized as follows  We
begin with   lowlevel description of our matrix balancing
algorithm in Section   and demonstrate its ef ciency in numerical experiments in Section   To guarantee the correctness of the algorithm and extend it to tensor balancing  we
provide theoretical analysis in Section   In Section   we
introduce   generalized loglinear model associated with  
partial order structured outcome space  followed by introducing the dually  at Riemannian structure in Section  
In Section   we show how to use Newton   method to
compute projection of   probability distribution onto   submanifold  Finally  we formulate the matrix and tensor balancing problem in Section   and summarize our contributions in Section  

  The Matrix Balancing Algorithm
Given   nonnegative square matrix      aij    Rn cid   cid    the
task of matrix balancing is to  nd        Rn that satisfy

 RAS     

 RAS        

 
where     diag    and     diag    The balanced matrix
 
  RAS is called doubly stochastic  in which each entry
 
 
ij   aijrisj and all the rows and columns sum to one 
 
The most popular algorithm is the SinkhornKnopp algorithm  which repeats updating   and   as      As  and
     AT    We denote by                    ng hereafter 
In our algorithm  instead of directly updating   and    we
update two parameters  cid  and  cid  de ned as

 

 

 

 

log pij  

  cid  

  cid  

 cid     

 cid ij  

  cid  

  cid  

pi   

 

Figure  Matrix balancing with two parameters  cid  and  cid 

 

 

ij aij so that

for each            where we normalized entries as pij  
ij pij     We assume for simplicaij 
ity that each entry is strictly larger than zero  The assumption will be removed in Section  
The key to our approach is that we update  cid   
ij with      
or       by Newton   method at each iteration              
while  xing  cid ij with          so that  cid   
ij satis es the following condition  Figure  
         cid         
 cid   

         cid         
 cid   

Note that the rows and columns sum not to   but to    due
to the normalization  The update formula is described as

 

 cid   
 

 

 

 cid   
  
 cid   
 

 cid   
  

 cid   

   

 

 cid   
 
 
 cid   
  
 cid   
 
 
 cid   
  

 

 cid 

 cid   
  
 cid   
 

 cid   
  

 cid      cid        

 cid   
 

 

 

 cid      cid        
 cid      cid        

 

 cid      cid        

   

where   is the Jacobian matrix given as

  ij       

 cid   
ij
 cid   
    

   cid maxfi     maxfj     cid   cid ij cid       

ij

 

  we can compute     

which is derived from our theoretical result in Theorem  
Since   is      cid cid   cid  matrix  the time complexity
of each update is      which is needed to compute the
inverse of   
After updating to  cid   
and  cid   
by Equation   Since this update does not ensure the
condition
as
and recompute     
 cid   
 
and  cid   
By iterating the above update process in Equation   until
convergence       aij  with aij   npij becomes doubly
stochastic 

 
for each           

    we again update  cid   

ij     
 cid  log

ij
   cid   

ij     

ij

 

 

ij

ij

ij

ij

  Numerical Experiments
We evaluate the ef ciency of our algorithm compared to the
two prominent balancing methods  the standard SinkhornKnopp algorithm  Sinkhorn    and the stateof theart

  cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid   cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid MatrixConstraints for balancingInvariantTensor Balancing on Statistical Manifold

Figure Results on Hessenberg matrices  The BNEWT algorithm  green  failed to converge for    cid   

Figure Results on Trefethen matrices  The BNEWT algorithm
 green  failed to converge for    cid   

is clearly the fastest  It is three to  ve orders of magnitude
faster than the standard SinkhornKnopp algorithm  plotted
in red  Although the BNEWT algorithm  plotted in green 
is competitive if   is small  it suddenly fails to converge
whenever    cid    which is consistent with results in the
original paper  Knight   Ruiz    where there is no result for the setting    cid    on the same matrix  Moreover 
our method converges around   to   steps  which is about
three and seven orders of magnitude smaller than BNEWT
and SinkhornKnopp  respectively  at      
To see the behavior of the rate of convergence in detail  we
plot the convergence graph in Figure   for       where
we observe the slow convergence rate of the SinkhornKnopp algorithm and unstable convergence of the BNEWT
algorithm  which contrasts with our quick convergence 
Trefethen Matrix  Next  we collected   set of Trefethen
matrices from   collection website  which are nonnegative diagonal matrices with primes  Results are plotted in
Figure   where we observe the same trend as before  Our
algorithm is the fastest and about four orders of magnitude
faster than the SinkhornKnopp algorithm  Note that larger
matrices with       do not have total support  which
is the necessary condition for matrix balancing  Knight  
Ruiz    while the BNEWT algorithm fails to converge
if       or      

  Theoretical Analysis
In the following  we provide theoretical support to our algorithm by formulating the problem as   projection within
  statistical manifold  in which   matrix corresponds to an
element  that is    probability distribution  in the manifold 
We show that   balanced matrix forms   submanifold and
matrix balancing is projection of   given distribution onto
the submanifold  where the Jacobian matrix in Equation  
is derived from the gradient of the manifold 

 http www cise ufl edu research sparse 

matrices 

Figure  Convergence graph on   

 

    

 

 cid     

algorithm BNEWT  Knight   Ruiz    which uses
Newton   methodlike iterations with conjugate gradients 
All experiments were conducted on Amazon Linux AMI
release   with   single core of   GHz Intel Xeon
CPU       and   GB of memory  All methods
were implemented in    with the Eigen library and
compiled with gcc   We have carefully implemented
BNEWT by directly translating the MATLAB code provided in  Knight   Ruiz    into    with the Eigen
library for fair comparison  and used the default parame 
 
ters  We measured the residual of   matrix  
ij  by
    cid    where each enthe squared norm   
 
try  
ij is obtained as npij in our algorithm  and ran each
of three algorithms until the residual is below the tolerance
threshold  
Hessenberg Matrix  The  rst set of experiments used  
Hessenberg matrix  which has been   standard benchmark
for matrix balancing  Parlett   Landis    Knight  
Ruiz    Each entry of an    cid    Hessenberg matrix
Hn    hij  is given as hij     if        cid    and hij    
otherwise  We varied the size   from   to     and
measured running time  in seconds  and the number of iterations of each method 
Results are plotted in Figure   Our balancing algorithm
with the Newton   method  plotted in blue in the  gures 

 cid 

 An implementation of algorithms for matrices and third
https github com 

order
mahitosugiyama newtonbalancing

is available at 

tensors

nNumber of iterations nRunning time  sec cid cid cid cid cid cid cid cid Newton  proposed SinkhornBNEWTNumber of iterationsResidual cid cid cid cid SinkhornNewtonBNEWTnNumber of iterations nRunning time  sec cid cid cid cid cid cid cid cid Newton  proposed SinkhornBNEWTTensor Balancing on Statistical Manifold

 

  probability vector is treated as   mapping            
             where every entry      is assuch that
sumed to be strictly larger than zero 
Using the zeta and the   obius functions  let us introduce
two mappings  cid          and  cid          as
 
 

From the   obius inversion formula  we have

 cid       log     

 cid            

 cid     

 cid     

    

  cid  

   

 

 

log       

 cid      cid     

 cid   

  cid  

 
 
 
 

   

   

 

 

      

   

 cid      cid   

They are generalization of the loglinear model  Agresti 
 
  that gives the probability      of an ndimensional
binary vector                 xn        gn as

 

 

 cid ixi  

 cid ijxixj  

 cid ijkxixjxk

log       

 

   

   cid cid cid     cid nx          xn  cid    

     

where  cid     cid           cid    is   parameter vector    is  
normalizer  and  cid     cid           cid    represents the expectation of variable combinations such that

 cid       xi    Pr xi    
 cid ij     xixj    Pr xi   xj                 
 cid                xn    Pr       cid cid cid    xn    

They coincide with Equations   and   when we let
       with                  ng  each       as the set
of indices of   of    and the order  cid  as the inclusion relationship  that is     cid    if and only if    cid     Nakahara
et al    have pointed out that  cid  can be computed from
  using the inclusionexclusion principle in the loglinear
model  We exploit this combinatorial property of the loglinear model using the   obius inversion formula on posets
and extend the loglinear model from the power set    to
any kind of posets    cid  Sugiyama et al    studied  
relevant loglinear model  but the relationship with   obius
inversion formula has not been analyzed yet 

  Formulation

We introduce our loglinear probabilistic model  where the
outcome space is   partially ordered set  or   poset  Gierz
et al    We prepare basic notations and the key mathematical tool for posets  the   obius inversion formula  followed by formulating the loglinear model 

     OBIUS INVERSION
  poset    cid  the set of elements   and   partial order
 cid  on    is   fundamental structured space in computer
science    partial order  cid  is   relation between elements in   that satis es the following three properties 
For all                   cid     re exivity       cid    
   cid             antisymmetry  and      cid    
   cid         cid     transitivity  In what follows    is always  nite and includes the least element  bottom        
that is     cid    for all        We denote         by   
Rota   introduced the   obius inversion formula on
posets by generalizing the inclusionexclusion principle 
Let  cid       cid            be the zeta function de ned as

 

 cid        

   
 cid 

 

The   obius function  cid      cid       satis es  cid cid       which
is inductively de ned for all      with    cid    as

 cid        

  cid      cid      

From the de nition  it follows that

if       
if       
otherwise 

if    cid    
otherwise 

 
 

 
 

   

 
 

  cid   cid  

 cid      cid        

 cid          cid xy 

 

 cid      cid        

 cid          cid xy

  cid   cid  

   

with the Kronecker delta  cid  such that  cid xy     if       and
 cid xy     otherwise  Then for any functions       and   with
the domain   such that

      

 cid             

     

 
 

   

      

 cid             

   

     

  is uniquely recovered with the   obius function 

 

       

   

 cid          

       

   

  cid  

 
 
 

  cid  

 cid          

  Dually Flat Riemannian Manifold

This is called the   obius inversion formula and is at the
heart of enumerative combinatorics  Ito   

  LOGLINEAR MODEL ON POSETS
We consider   probability vector   on    cid  that gives  
discrete probability distribution with the outcome space   

We theoretically analyze our loglinear model introduced in
Equations     and show that they form dual coordinate
systems on   dually  at manifold  which has been mainly
studied in the area of information geometry  Amari   
Nakahara   Amari    Amari      Moreover 
we show that the Riemannian metric and connection of our
model can be analytically computed in closed forms 

Tensor Balancing on Statistical Manifold

 

   cid     

          

   

     log

    
    

 

In the following  we denote by  cid  the function  cid  or  cid  and by
  the gradient operator with respect to        nf        
    cid          cid    for        and denote by   the
set of probability distributions speci ed by probability vectors  which forms   statistical manifold  We use uppercase
letters                for points  distributions  in   and their
lowercase letters                for the corresponding probability vectors treated as mappings  We write  cid   and  cid   if they
are connected with   by Equations   and   respectively 
and abbreviate subscripts if there is no ambiguity 

  DUALLY FLAT STRUCTURE
We show that   has the dually  at Riemannian structure
induced by two functions  cid  and  cid  in Equation   and  
We de ne   cid  as

  cid     cid cid     cid  log   

 
which corresponds to the normalizer of    It is   convex
function since we have

  cid    log

exp

 
   cid    cid     cid    cid  We apply the Leg 

   cid  

   

 cid   

from log       
endre transformation to   cid  given as

 

  

 

   
 

 cid   

 cid 
    

 

 

 

   cid 

 cid    max

 cid 

Then  cid  coincides with the negative entropy 
Theorem    Legendre dual 

     log     

 

  

    

  cid  
     

Proof  From Equation   we have

 

 cid 

 cid   

 
 cid       log  

   

 

    
Thus it holds that

 

 cid   cid    cid 

 

   

 cid 

     cid  log  
 
 

 
     log  

   

Hence it is maximized with         

 cid 

 

 

 cid   cid    cid 
 
   

   

 cid   

   cid  

 
       log  

 
 

    

 
   

 

 
 

 
 

Proof  They can be directly derived from our de nitions
 Equations   and   as

        cid   

  cid  

   cid 
 cid   

 cid 
 cid   

 

 

 

    exp
 

 cid   

  cid   exp

   cid    cid   

   cid    cid   

 

 cid cid   cid    cid 
 

   cid   

 

 

Moreover  we can con rm the orthogonality of  cid  and  cid  as

 

  log     

  log     

 cid   

 cid   

 cid      cid          cid xy 

 

   

The last equation holds from Equation   hence the
  obius inversion directly leads to the orthogonality 
The Bregman divergence is known to be the canonical divergence  Amari    Section   to measure the difference between two distributions   and   on   dually  at
manifold  which is de ned as

             cid        cid     cid   cid    cid   

In our case  since we have  cid     
and  cid    cid   cid   cid      
and Equation   it is given as

         log     
         log      from Theorem  

 

 

 

which coincides with the Kullback Leibler divergence  KL
divergence  from   to                DKL        

  RIEMANNIAN STRUCTURE
Next we analyze the Riemannian structure on   and show
that the   obius inversion formula enables us to compute
the Riemannian metric of   
Theorem    Riemannian metric  The manifold       cid 
is   Riemannian manifold with the Riemannian metric   cid 
such that for all          

 
 cid      cid            cid   cid   cid   

 

if  cid     cid 

if  cid     cid 

 

 
 

   

   

gxy cid   

   

 

 cid      cid          

 cid 

 

   

Proof  Since the Riemannian metric is de ned as
  cid     cid 

  cid       cid 

Since they are connected with each other by the Legendre
transformation  they form   dual coordinate system    cid 
and  cid  of    Amari    Section   which coincides with  cid  and  cid  as follows 
Theorem    dual coordinate system 

   cid     cid   cid     cid 

 

when  cid     cid  we have

gxy cid   

 

 cid   cid   

 

 

 

 cid   

   

  cid   

 cid   

 cid   

 

   

   cid  

 cid       exp

 cid     cid    cid 

  

 

   

 

Tensor Balancing on Statistical Manifold

 cid      cid            cid  jSj cid   cid   

 cid   

 

 

  cid  
 cid 

 
 

 cid   

 cid   

When  cid     cid  it follows that

 cid   cid   

 

 

 

 

 cid   

gxy cid   

 

 

 cid      cid          

   

 cid       log

  cid  

 cid      cid   

  

 
 

 

 

Since   cid  coincides with the Fisher information matrix 

 

 cid   

 

 cid   

log     

log     

 

 cid   

 

 cid   

log     

  gxy cid 

log     

  gxy cid 

 

Then the Riemannian  Levi Chivita  connection  cid cid  with
respect to  cid  which is de ned as

 

 
 

 cid xyz cid   

 gyz cid 
 cid   

 gxz cid 
 cid   

 cid   gxy cid 
 cid   
for all              can be analytically obtained 
Theorem    Riemannian connection  The Riemannian
connection  cid cid  on the manifold       cid  is given in the
 
following for all             
 cid        cid   cid   
    

 
 
 
 
 cid        cid   cid   
 cid        cid   cid   

   

 
 

 cid xyz cid   

 

 
 

   

 

 cid   
 

 cid      cid      cid          

if  cid     cid 
 cid  if  cid     cid 

Proof  Connections  cid xyz cid  and  cid xyz cid  can be obtained
by directly computing  gyz cid cid    and  gyz cid cid   
respectively 

  The Projection Algorithm

Projection of   distribution onto   submanifold is essential  several machine learning algorithms are known to be
formulated as projection of   distribution empirically estimated from data onto   submanifold that is speci ed by the
target model  Amari    Here we de ne projection of
distributions on posets and show that Newton   method can
be applied to perform projection as the Jacobian matrix can
be analytically computed 

  DEFINITION
Let   cid  be   submanifold of   such that
  cid    fP        cid          cid         dom cid    

speci ed by   function  cid  with dom cid   cid     Projection
of       onto   cid  called mprojection  which is de ned
as the distribution   cid      cid  such that

 cid   cid         cid   
 cid   cid         cid      

if     dom cid 
if          dom cid 

is the minimizer of the KL divergence from   to   cid 

  cid    argmin
    cid 

DKL      

The dually  at structure with the coordinate systems  cid  and
 cid  guarantees that the projected distribution   cid  always exists and is unique  Amari    Theorem   Moreover 
the Pythagorean theorem holds in the dually  at manifold 
that is  for any       cid  we have

DKL         DKL      cid    DKL   cid    

We can switch  cid  and  cid  in the submanifold   cid  by changing DKL       to DKL        where the projected distribution   cid  of   is given as

 

 

 cid   cid         cid      
 cid   cid         cid   

if          dom cid 
if     dom cid 

This projection is called eprojection 
Example    Boltzmann machine  Given   Boltzmann machine represented as an undirected graph            with
  vertex set   and an edge set    cid  ffi  jg              
The set of probability distributions that can be modeled by
  Boltzmann machine   coincides with the submanifold
      fP        cid           if jxj     or     Eg 

with          Let    be an empirical distribution estimated from   given dataset  The learned model is the mprojection of the empirical distribution    onto      where
the resulting distribution   cid  is given as

 

 cid   cid         
 cid   cid         cid        

if jxj     or       
if jxj     or       

  COMPUTATION

Here we show how to compute projection of   given probability distribution  We show that Newton   method can be
used to ef ciently compute the projected distribution   cid  by
iteratively updating    
 cid          until
converging to   cid 

 cid      as    

 cid       

 cid       

Let us start with the mprojection with initializing    
 cid   
    for all     dom cid 
    In each iteration    we update  cid   
  cid 
       cid       for all          dom cid 
while  xing  cid   
  cid 
 
which is possible from the orthogonality of  cid  and  cid  Using
Newton   method   cid   

    should satisfy

 

 

 

 

  cid 

 cid   
  cid 

     cid   cid   

  cid 

   

   

     cid   cid   

 cid   
  cid 

 
Jxy
  dom cid 

Tensor Balancing on Statistical Manifold

Jxy  

for every     dom cid  where Jxy is an entry of the
jdom cid    cid  jdom cid   Jacobian matrix   and given as
 cid 

 

   

 

 cid      cid          

 cid     

   

   
from Theorem   Therefore  we have the update formula
for all     dom cid  as
       cid   
  cid 

 
     cid   cid   

 

     cid 

 cid   
  cid 

 cid   
  cid 

 

 

 cid   
  cid 
 cid   
  cid 

 cid 
 
xy
  dom cid 

    for     dom cid  while    
       cid       for all          dom cid  To ensure
      we add   to dom cid  and  cid      We

In eprojection  update  cid   
  cid 
ing  cid   
  cid 
 cid   
  cid 
update  cid   
  cid 
       cid   
  cid 

     cid 

 cid   
  cid 

    at each step   as

 cid 
xy

     cid   cid   

 cid   
  cid 

 

 

 

 

 

xy  

 cid   
  cid 
 cid   
  cid 

   

   

 cid      cid          

 cid     
   cid   
  cid 

  cid 

 cid  jSj cid   
   
 
  as it is not
   cid      

   

In this case  we also need to update  cid   
  cid 
guaranteed to be  xed  Let us de ne

   
 
 cid 

          

 cid     

exp

  dom cid 

exp

   

 
 

 
  dom cid 

 

   

 
 
 

Since we have

    
 cid 

     

exp

exp

 cid   
  cid 
 cid   
  cid 

it follows that
 cid   
  cid 
   cid  log

 
 
   cid   cid   

  cid 

exp

 cid   
  cid 

 

 
 

The time complexity of each iteration is   jdom cid   
which is required to compute the inverse of the Jacobian
matrix 
Global convergence of the projection algorithm is always
guaranteed by the convexity of   submanifold   cid  de ned
in Equation   Since   cid  is always convex with respect
to the  cid  and  cid coordinates  it is straightforward to see that
our eprojection is an instance of the Bregman algorithm
onto   convex region  which is well known to always converge to the global solution  Censor   Lent   

  Balancing Matrices and Tensors
Now we are ready to solve the problem of matrix and tensor
balancing as projection on   dually  at manifold 

 cid   
  cid 
 cid   
  cid 

 
 
 
   

 
 
 

 

    

   
 cid 

   

 

   

 

   
 cid 

 

  Matrix Balancing
Recall that the task of matrix balancing is to  nd        Rn
that satisfy  RAS      and  RAS         with    
diag    and     diag    for   given nonnegative square
matrix      aij    Rn cid   cid   
Let us de ne   as

                         and aij      

            cid                  cid    and    cid    

 cid       minf                  im       

 
 
where we remove zero entries from the outcome space   as
our formulation cannot treat zero probability  and give each
probability as           aij 
ij aij  The partial order
 cid  of   is naturally introduced as
 
resulting in         In addition  we de ne  cid     for
each         and           such that
where the minimum is with respect to the order  cid  If  cid    
does not exist  we just remove the entire kth row if      
or kth column if       from    Then we switch rows and
columns of   so that the condition
 
is satis ed for each            which is possible for any
matrices  Since we have
 
 cid cid       cid   cid cid       
          
 
          
if the condition   is satis ed  the probability distribution
is balanced if for all         and          

 cid    cid   cid    cid   cid cid cid   cid   cid    

   
 

if      
if      

 cid cid       

  cid   

 

 

Therefore  we obtain the following result 
Matrix balancing as eprojection 
Given   matrix     Rn cid   with its normalized probability distribution       such that           aij 
ij aij 
De ne the poset    cid  by Equations   and   and let
  cid  be the submanifold of   such that
  cid    fP        cid          cid    for all     dom cid   
where the function  cid  is given as

 

dom cid      cid                           gg 

 cid cid       

  cid   

 

 

 cid   cid         cid      
 cid   cid         cid   

Matrix balancing is the eprojection of   onto the submanifold   cid  that is  the balanced matrix  RAS   is the
distribution   cid  such that

 

if          dom cid 
if     dom cid 
 

 

 

  

which is unique and always exists in    thanks to its dually
 at structure  Moreover  two balancing vectors   and   are

 
 cid   cid   cid       cid   cid    cid     

exp
for every         and     rn 

  

 

ij aij 

ri
ai

if      
if      

 

Tensor Balancing on Statistical Manifold

  Tensor Balancing

Next  we generalize our approach from matrices to tensors 
For an Nth order tensor      ai   iN     Rn cid   cid cid cid cid cid nN
and   vector     Rnm  the mmode product of   and   is
de ned as

    cid       im cid im iN

 

ai   iN bim  

nm 

im 

We de ne tensor balancing as follows  Given   tensor    
Rn cid   cid cid cid cid cid nN with       cid cid cid    nN       nd     cid   
order tensors               RN such that

   cid           Rn cid cid cid cid cid nm cid cid nm cid cid cid cid cid nN  

 

 

for all              
 
 
im   
 
    iN
    iN of the balanced tensor  
entry  
 
    iN

  ai   iN

 

Rm

 

 
    where each
  is given as

  im cid im iN

 

     

  that satis es Equation   is called multiA tensor  
stochastic  Cui et al    Note that this is exactly the
same as the matrix balancing problem if      
It is straightforward to extend matrix balancing to tensor
balancing as eprojection onto   submanifold  Given   tensor     Rn cid   cid cid cid cid cid nN with its normalized probability
distribution   such that

 

       ai   iN

aj   jN

 

    jN

 

 

for all                    iN   The objective is to obtain
im    cid            iN      nN cid  for all
  cid  such that
         and            iN       In the same way as matrix
balancing  we de ne   as

 
   
with removing zero entries and the partial order  cid  as
              iN    cid                jN               im  cid  jm 
In addition  we introduce  cid     as

 cid cid  ai   iN

               iN         

   

 cid       minf                    iN         im       

and require the condition in Equation  
Tensor balancing as eprojection 
Given   tensor     Rn cid   cid cid cid cid cid nN with its normalized
probability distribution       given in Equation   The
submanifold   cid  of multistochastic tensors is given as
  cid    fP        cid          cid    for all     dom cid   
where the domain of the function  cid  is given as
dom cid       cid                         

and each value is described using the zeta function as

 

 cid cid       

    

 cid cid       cid     

 

nN cid   

 

Tensor balancing is the eprojection of   onto the submanifold   cid  that is  the multistochastic tensor is the distribution   cid  such that

 

 cid   cid         cid      
 cid   cid         cid   

if          dom cid 
if     dom cid 

which is unique and always exists in    thanks to its dually
 at structure  Moreover  each balancing tensor Rm is

  

Rm

  im cid im iN

   

im 

   

  

  exp

 cid   cid   cid       cid   cid    cid     

 

for every          and        nN cid 
to recover   multistochastic tensor 
Our result means that the eprojection algorithm based on
Newton   method proposed in Section   converges to the
unique balanced tensor whenever   cid      holds 

aj jN
 

  jN

  Conclusion
In this paper  we have solved the open problem of tensor
balancing and presented an ef cient balancing algorithm
using Newton   method  Our algorithm quadratically converges  while the popular SinkhornKnopp algorithm linearly converges  We have examined the ef ciency of our
algorithm in numerical experiments on matrix balancing
and showed that the proposed algorithm is several orders
of magnitude faster than the existing approaches 
We have analyzed theories behind the algorithm  and
proved that balancing is eprojection in   special type of
  statistical manifold  in particular    dually  at Riemannian manifold studied in information geometry  Our key
 nding is that the gradient of the manifold  equivalent to
Riemannian metric or the Fisher information matrix  can be
analytically obtained using the   obius inversion formula 
Our information geometric formulation can model several
machine learning applications such as statistical analysis
on   DAG structure  Thus  we can perform ef cient learning as projection using information of the gradient of manifolds by reformulating such models  which we will study
in future work 

Acknowledgements
The authors sincerely thank Marco Cuturi for his valuable comments 
This work was supported by JSPS
KAKENHI Grant Numbers JP    JP   
 MS  JP  and JP     HN  The research
of      was supported by JST CREST JPMJCR 
RIKEN PostK  KAKENHI Nanostructure and KAKENHI
JP   

Tensor Balancing on Statistical Manifold

References
Agresti     Categorical data analysis  Wiley    edition 

 

Ahmed     De Loera     and Hemmecke     Polyhedral
Cones of Magic Cubes and Squares  volume   of Algorithms and Combinatorics  pp    Springer   

Akartunal     and Knight        Network models and
biproportional rounding for fair seat allocations in the
UK elections  Annals of Operations Research  pp   
 

the National Academy of Sciences   
 

Gierz     Hofmann        Keimel     Lawson        Mislove     and Scott        Continuous Lattices and Domains  Cambridge University Press   

Idel       review of matrix scaling and sinkhorn   normal
form for matrices and positive maps  arXiv 
 

Ito      ed  Encyclopedic Dictionary of Mathematics  The

MIT Press    edition   

Amari    

Information geometry on hierarchy of probability distributions  IEEE Transactions on Information
Theory     

Knight        The Sinkhorn Knopp algorithm  Convergence and applications  SIAM Journal on Matrix Analysis and Applications     

Amari     Information geometry and its applications  ConIn Nielsen    
vex function and dually  at manifold 
 ed  Emerging Trends in Visual Computing  LIX Fall
Colloquium  ETVC   Revised Invited Papers  pp 
  Springer   

Amari    

Information geometry of positive measures
and positivede nite matrices  Decomposable dually  at
structure  Entropy     

Amari    

Information Geometry and Its Applications 

Springer   

Balinski     Fair majority voting  or how to eliminate gerrymandering  American Mathematical Monthly   
   

Censor     and Lent     An iterative rowaction method for
interval convex programming  Journal of Optimization
Theory and Applications     

Chang     Paksoy        and Zhang     Polytopes of
stochastic tensors  Annals of Functional Analysis   
   

Cui       Li     and Ng        Birkhoff von Neumann
theorem for multistochastic tensors  SIAM Journal on
Matrix Analysis and Applications     

Cuturi     Sinkhorn distances  Lightspeed computation of
In Advances in Neural Information

optimal transport 
Processing Systems   pp     

Frogner     Zhang     Mobahi     Araya     and Poggio        Learning with   Wasserstein loss  In Advances
in Neural Information Processing Systems   pp   
   

Ganmor     Segev     and Schneidman     Sparse loworder interaction network underlies   highly correlated
and learnable neural population code  Proceedings of

Knight        and Ruiz       fast algorithm for matrix
balancing  IMA Journal of Numerical Analysis   
   

Lahr     and de Mesnard     Biproportional techniques
in inputoutput analysis  Table updating and structural
analysis  Economic Systems Research   
 

Lamond     and Stewart        Bregman   balancing
method  Transportation Research Part    Methodological     

Livne        and Golub        Scaling by binormalization 

Numerical Algorithms     

Marshall        and Olkin     Scaling of matrices to achieve
speci ed row and column sums  Numerische Mathematik     

Miller        and Blair        InputOutput Analysis  Foundations and Extensions  Cambridge University Press   
edition   

Moon        Gunther        and Kupin        Sinkhorn
solves sudoku  IEEE Transactions on Information Theory     

Nakahara     and Amari     Informationgeometric measure for neural spikes  Neural Computation   
   

Nakahara     Nishimura     Inoue     Hori     and
Amari     Gene interaction in DNA microarray data is
decomposed by information geometric measure  Bioinformatics     

Nakahara     Amari     and Richmond          comparison of descriptive models of   single spike train by
informationgeometric measure  Neural computation   
   

Tensor Balancing on Statistical Manifold

Parikh     Forecasts of inputoutput matrices using the
       method  The Review of Economics and Statistics 
   

Parlett        and Landis        Methods for scaling to doubly stochastic form  Linear Algebra and its Applications 
   

Rao           Huntley        Durand        Stamenova 
      Bochkov        Robinson        Sanborn       
Machol     Omer        Lander        and Aiden       
     map of the human genome at kilobase resolution
reveals principles of chromatin looping  Cell   
   

Rota       On the foundations of combinatorial theory   
Theory of   obius functions     Wahrseheinlichkeitstheorie     

Sinkhorn       relationship between arbitrary positive matrices and doubly stochastic matrices  The Annals of
Mathematical Statistics       

Sinkhorn     and Knopp     Concerning nonnegative matrices and doubly stochastic matrices  Paci   Journal of
Mathematics     

Solomon     de Goes     Peyr       Cuturi     Butscher 
   Nguyen     Du     and Guibas     Convolutional
Wasserstein distances  Ef cient optimal transportation
on geometric domains  ACM Transactions on Graphics 
   

Soules        The rate of convergence of sinkhorn balancing  Linear Algebra and its Applications   
 

Sugiyama     Nakahara     and Tsuda     Information
In   IEEE Indecomposition on structured space 
ternational Symposium on Information Theory  pp   
  July  

Wu       and Michor       computational strategy to adjust
for copy number in tumor HiC data  Bioinformatics   
   

