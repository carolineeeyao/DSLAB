No Spurious Local Minima in Nonconvex Low Rank Problems 

  Uni ed Geometric Analysis

Rong Ge   Chi Jin   Yi Zheng  

Abstract

In this paper we develop   new framework that
captures the common landscape underlying the
common nonconvex lowrank matrix problems
including matrix sensing  matrix completion and
robust PCA  In particular  we show for all above
problems  including asymmetric cases    all local minima are also globally optimal    no highorder saddle points exists  These results explain
why simple algorithms such as stochastic gradient descent have global converge  and ef ciently
optimize these nonconvex objective functions in
practice  Our framework connects and simpli es
the existing analyses on optimization landscapes
for matrix sensing and symmetric matrix completion  The framework naturally leads to new
results for asymmetric matrix completion and robust PCA 

  Introduction
Nonconvex optimization is one of the most powerful tools
in machine learning  Many popular approaches  from traditional ones such as matrix factorization  Hotelling   
to modern deep learning  Bengio    rely on optimizing nonconvex functions  In practice  these functions are
optimized using simple algorithms such as alternating minimization or gradient descent  Why such simple algorithms
work is still   mystery for many important problems 
One way to understand the success of nonconvex optimization is to study the optimization landscape  for the objective function  where are the possible locations of global
optima  local optima and saddle points  Recently    line
of works showed that several natural problems including
tensor decomposition  Ge et al    dictionary learning  Sun et al      matrix sensing  Bhojanapalli et al 

Authors listed alphabetically   Duke University  Durham NC
 UC Berkeley  Berkeley CA  Correspondence to  Rong Ge
 rongge cs duke edu  Chi Jin  chijin cs berkeley edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Park et al    and matrix completion  Ge et al 
  have wellbehaved optimization landscape  all local
optima are also globally optimal  Combined with recent
results       Ge et al    Carmon et al    Agarwal et al    Jin et al    that are guaranteed to
 nd   local minimum for many nonconvex functions  such
problems can be ef ciently solved by basic optimization
algorithms such as stochastic gradient descent 
In this paper we focus on optimization problems that look
for low rank matrices using partial or corrupted observations  Such problems are studied extensively  Fazel 
  Rennie   Srebro    Cand es   Recht    and
has many applications in recommendation systems  Koren 
  see survey by Davenport   Romberg   These
optimization problems can be formalized as follows 

min

  Rd   
    

     

rank        

 

Here   is an         matrix and   is   convex function
of    The nonconvexity of this problem stems from the
low rank constraint  Several interesting problems  such
as matrix sensing  Recht et al    matrix completion
 Cand es   Recht    and robust PCA  Cand es et al 
  can all be framed as optimization problems of this
form see Section  
In practice  Burer   Monteiro   heuristic is often
used   replace   with an explicit low rank representation
    UV cid  where     Rd   and     Rd    The new
optimization problem becomes

min

  Rd     Rd  

   UV cid           

 

Here         is    optional  regularizer  Despite the objective being nonconvex  for all the problems mentioned
above  simple iterative updates from random or even arbitrary initial point  nd the optimal solution in practice  It is
then natural to ask  Can we characterize the similarities
between the optimization landscape of these problems 
We show this is indeed possible 
Theorem    informal  The objective function of matrix
sensing  matrix completion and robust PCA have similar

No Spurious Local Minima in Nonconvex Low Rank Problems

optimization landscape  In particular  for all these problems    all local minima are also globally optimal    any
saddle point has at least one strictly negative eigenvalue in
its Hessian 

More precise theorem statements appear in Section   Note
that there were several cases  matrix sensing  Bhojanapalli
et al    Park et al    symmetric matrix completion  Ge et al    where similar results on the optimization landscape were known  However the techniques
in previous works are tailored to the speci   problems and
hard to generalize  Our framework captures and simpli 
 es all these previous results  and also gives new results on
asymmetric matrix completion and robust PCA 
The key observation in our analysis is that for matrix
sensing  matrix completion  and robust PCA  when  xing
sparse estimate  function    in Equation   is   quadratic
function over the matrix    Hence the Hessian   of   with
respect to   is   constant  More importantly  the Hessian
  in all above problems has similar properties  that it approximately preserves norm  similar to the RIP properties
used in matrix sensing  Recht et al    which allows
their optimization landscapes to be characterized in   uni 
 ed way  Speci cally  our framework gives principled way
of de ning   direction of improvement for all points that are
not globally optimal 
Another crucial property of our framework is the interaction between the regularizer and the Hessian    Intuitively 
the regularizer makes sure the solution is in   nice region  
      set of incoherent matrices for matrix completion  and
only within   the Hessian has the norm preserving property  On the other hand  regularizer should not be too large
to severely distort the landscape  This interaction is crucial
for matrix completion  and is also very useful in handling
noise and perturbations  In Section   we discuss ideas required to apply this framework to matrix sensing  matrix
completion and robust PCA 
Using this framework  we also give   way to reduce
asymmetric matrix problems to symmetric PSD problems
 where the desired matrix is of the form UU cid  See Section   for more details 
In addition to the results of no spurious local minima 
our framework also implies that any saddle point has at
least one strictly negative eigenvalue in its Hessian  Formally  we proved all above problems satisfy   robust version of this claim   strict saddle property  see De nition   which is one of crucial suf cient conditions to admit ef cient optimization algorithms  and thus following
corollary 
Corollary    informal  For matrix sensing  matrix completion and robust PCA  simple local search algorithms can
 nd the desired low rank matrix UV cid      cid  from an ar 

bitrary starting point in polynomial time with high probability 

Several algorithms  including many variants of gradient descent Ge et al    Carmon et al    Agarwal et al 
  Jin et al    are known to converge to   local
optimum for strictsaddle functions  and hence can be applied to the problems discussed in this paper  There are
some technicalities in the exact guarantees  which we defer
to supplementary material 
For simplicity  we present most results in the noiseless setting  but our results can also be generalized to handle noise 
See supplementary material for details 

  Related Works

The landscape of low rank matrix problems have recently
received   lot of attention  Ge et al    showed symmetric matrix completion has no spurious local minimum 
At the same time  Bhojanapalli et al    proved similar result for symmetric matrix sensing  Park et al   
extended the matrix sensing result to asymmetric case  All
of these works guarantee global convergence to the correct
solution 
There has been   lot of work on the local convergence
analysis for various algorithms and problems  For matrix sensing or matrix completion  the works  Keshavan
et al        Hardt   Wootters    Hardt    Jain
et al    Chen   Wainwright    Sun   Luo   
Zhao et al    Zheng   Lafferty    Tu et al   
showed that given   good enough initialization  many simple local search algorithms  including gradient descent and
alternating least squares  succeed  Particularly  several
works       Sun   Luo   Zheng   Lafferty  
accomplished this by showing   geometric property which
is very similar to strong convexity holds in the neighborhood of optimal solution  For robust PCA  there are also
many analysis for local convergence  Lin et al    Netrapalli et al    Yi et al    Zhang et al   
Several works also try to unify the analysis for similar problems  Bhojanapalli et al    gave   framework for local
analysis for these low rank problems  Belkin et al   
showed   framework of learning basis functions  which
generalizes tensor decompositions  Their techniques imply
the optimization landscape for all such problems are very
similar  For problems looking for   symmetric PSD matrix 
Li   Tang   showed for objective similar to    but in
the symmetric setting  restricted smoothness strong convexity on the function   suf ces for local analysis  However  their framework does not address the interaction between regularizer and the function    hence cannot be directly applied to problems such as matrix completion or
robust PCA 

No Spurious Local Minima in Nonconvex Low Rank Problems

Organization We will  rst introduce notations and basic optimality conditions in Section   Then Section   introduces the problems and our results  For simplicity  we
present our framework for the symmetric case in Section  
and brie   discuss how to reduce asymmetric problem to
symmetric problem in Section   For clean presentation 
many proofs are deferred to supplementary material 

  Preliminaries
In this section we introduce notations and basic optimality
conditions 

  Notations

product  and for matrices we use  cid      cid   cid 

We use bold letters for matrices and vectors  For   vector  
we use  cid   cid  to denote its  cid  norm  For   matrix   we use
 cid   cid  to denote its spectral norm  and  cid   cid   to denote its
Frobenius norm  For vectors we use  cid      cid  to denote inneri   MijNij
to denote the trace of MN cid  We will always use   cid  to
denote the optimal low rank solution  Further  we use  cid 
 
to denote its largest singular value   cid 
  to denote its rth
singular value and  cid     cid 
  be the condition number 
We use    to denote the gradient and    to denote its
Hessian  Since function   can often be applied to both  
 as in   and       as in   we use        to denote
gradient with respect to   and           to denote gradient with respect to       Similar notation is used for Hessian  The Hessian        is   crucial object in our framework  It can be interpreted as   linear operator on matrices 
This linear operator can be viewed as               matrix

 cid  matrix in the symmetric case  that ap 

 or cid   

 cid   cid   

 cid 

plies to the vectorized version of matrices  We use the notation           to denote the quadratic form  cid       cid 
Similarly  the Hessian of objective   is   linear operator
on   pair of matrices       which we usually denote as
         

 

 

  Optimality Conditions
Local Optimality Suppose we are optimizing   function
      with no constraints on    In order for   point   to be
  local minimum  it must satisfy the  rst and second order
necessary conditions  That is  we must have           
and         cid   
De nition    Optimality Condition  Suppose   is   local
minimum of       then we have

                   cid   

saddle property  which is   quantitative version of the optimality conditions  and can lead to ef cient algorithms to
 nd local minima 
De nition   We say function     is      strict saddle  That is  for any    at least one of followings holds 

   cid      cid     
   min          
    is  close to    cid    the set of local minima 

Intuitively  this de nition says for any point    it either violates one of the optimality conditions signi cantly  rst
two cases  or is close to   local minima  Note that   and  
are often closely related  For   function with strictsaddle
property  it is possible to ef ciently  nd   point near   local
minimum 

Local vs  Global However  of course  nding   local minimum is not suf cient in many case  In this paper we are
also going to prove that all local minima are also globally
optimal  and they correspond to the desired solutions 

  Low Rank Problems and Our Results
In this section we introduce matrix sensing  matrix completion and robust PCA  For each problem we give the results
obtained by our framework  The proof ideas are illustrated
later in Sections   and  

  Matrix Sensing

Matrix sensing  Recht et al    is   generalization of
compressed sensing  Candes et al   
In the matrix
sensing problem  there is an unknown low rank matrix
  cid    Rd    We make linear observations on this malet         Am   Rd    be   sensing matritrix 
ces  the algorithm is given  Ai   and the corresponding
bi    cid Ai    cid cid  The goal is now to  nd the unknown matrix   cid  In order to  nd   cid  we need to solve the following nonconvex optimization problem

min

  Rd     rank    

       

 
  

 cid    Ai cid    bi 

  cid 

  

We can transform this constraint problem to an unconstraint problem by expressing   as     UV cid  where
    Rd   and     Rd    We also need an additional
regularizer  common for all asymmetric problems 

Intuitively  if one of these conditions is violated  then it
is possible to  nd   direction that decreases the function
value   Ge et al    characterized the following strict 

min
   

 
  

 cid UV cid  Ai cid  bi   

 cid   cid      cid   cid 
   

 
 

 

  cid 

  

No Spurious Local Minima in Nonconvex Low Rank Problems

The regularizer has been widely used in previous works
 Zheng   Lafferty    Park et al    In Section   we
show how this regularizer can be viewed as   way to deal
with the additional invariants in asymmetric case  and reduce the asymmetric case to the symmetric case    crucial
concept in standard sensing literature is Restrict Isometry
Property  RIP  which is de ned as follows 
De nition     group of sensing matrices       Am 
satis es the      RIP condition  if for every matrix   of
rank at most   

     cid   cid 

     
 

 cid Ai    cid         cid   cid 
   

  cid 

  

 cid  
  cid Ai cid  approxiIntuitively  RIP says operator  
 
mately perserve norms for all low rank matrices  When
the sensing matrices are chosen to be        matrices with
independent Gaussian entries  if                for large
   
enough constant    the sensing matrices satisfy the      
RIP condition  Candes   Plan    Using our framework we can show 
Theorem   When measurements  Ai  satisfy      
   
RIP  for matrix sensing objective   we have   all local minima satisfy UV cid      cid    the function is
   cid 

 strict saddle 

        
 cid 
 

This in particular says   no spurious local minima existsl 
  whenever at some point        so that the gradient is
small and the Hessian does not have signi cant negative
eigenvalue  then the distance to global optimal  see De nition   and De nition   is guaranteed to be small  Such  
point can be found ef ciently  see supplementary material 

  Matrix Completion

Matrix completion is   popular technique in recommendation systems and collaborative  ltering  Koren    Rennie   Srebro    In this problem  again we have an
unknown low rank matrix   cid  We observe each entry
of the matrix   cid  independently with probability    Let
            be   set of observed entries  For any matrix
   we use    to denote the matrix whose entries outside
of   are set to   That is           Mi   if           
and            otherwise  We further use  cid   cid  to denote  cid   cid     Matrix completion can be viewed as   special case of matrix sensing  where the sensing matrices only
have one nonzero entry  However such matrices do not satisfy the RIP condition 
In order to solve matrix completion  we try to optimize the
following 

min

  Rd     rank    

 cid       cid cid 
 

 
  

  wellknown problem in matrix completion is that when
the true matrix   cid  is very sparse  then we are very likely
to observe only   entries  and has no chance to learn the
other entries of   cid  To avoid this case  previous works
have assumed following incoherence condition 
De nition     rank   matrix     Rd    is  
incoherent  if for the rankr SVD XDY cid  of    we have
for all                

    cid   cid     

 cid   cid 

    cid   cid     

 cid   cid 

We assume the unknown optimal low rank matrix   cid  is
 incoherent 
In the nonconvex program  we try to make sure the
decomposition UV cid  is also incoherent by adding  
regularizer            
   
  Here         are param 
 
eters that we choose later        max      Using this
regularizer  we can now transform the objective function to
the unconstraint form

 cid   
  cid   cid 

 cid   
  cid   cid 

    cid     

    cid     

min
   

 

 cid UV cid      cid cid 

 
  
 cid   cid       cid   cid 
 
 

 

           

 

min        choose  

Using the framework  we can show following 
Theorem   Let     max       when sample rate    
         cid 
     cid  log  
 
and         
   cid    With probability at
least      poly    for Objective Function   we have  
all local minima satisfy UV cid      cid    The objective is
 strict saddle for polynomially small  
   cid 

   cid            

         cid 

   

  

  

 

 

        
 cid 
 

  Robust PCA

Robust PCA  Cand es et al    is   generalization to
the standard Principled Component Analysis 
In Robust
PCA  we are given an observation matrix Mo  which is an
true underlying matrix   cid  corrupted by   sparse noise   cid 
 Mo     cid      cid  In some sense the goal is to decompose the matrix   into these two components  There are
many models on how many entries can be perturbed  and
how they are distributed  In this paper we work in the setting where   cid  is  incoherent  and the rows columns of
  cid  can have at most  fraction nonzero entries 
In order to express robust PCA as an optimization problem 
we need constraints on both   and   
 cid         Mo cid 
   

 

min
     rank           is sparse 

 
 

No Spurious Local Minima in Nonconvex Low Rank Problems

 

There can be several ways to specify the sparsity of    In
this paper we restrict attention to the set    which is the set
of matrices that have at most  fraction nonzero entries in
each column row  and entries have absolute value at most
     cid 
 
    
Assuming the true sparse matrix   cid  is in    Note that the
in nite norm requirement on   cid  is without loss of generality  because by incoherence   cid  cannot have entries with
absolute value more than    cid 
 
  Any entry larger than that
    
is obviously in the support of   cid  and can be truncated 
In objective function  we allow   to be   times denser  in
   where   is   parameter we choose later  Now the
constraint optimization problem can be tranformed to the
unconstraint problem

  Framework for Symmetric Positive De nite

Problems

In this section we describe our framework in the simpler
setting where the desired matrix is positive semide nite  In
particular  suppose the true matrix   cid  we are looking for
can be written as   cid      cid   cid cid  where   cid    Rd    For
objective functions that is quadratic over    we denote its
Hessian as   and we can write the objective as

min

  Rd  

sym  rank    

       cid               cid 

 
 

 

We call this objective function       Via BurerMonteiro
factorization  the corresponding unconstraint optimization
problem  with regularization   can be written as

 

min

  Rn  

 
 

 UU cid   cid         UU cid   cid       

min
   

          

 cid   cid       cid   cid 
   

 
 

           min
    

 cid UV cid        Mo cid 
   

 
 

Of course  we can also think of this as   joint minimization
problem of          However we choose to present it this
way in order to allow extension of the strictsaddle condition  Since          is not twicedifferetiable            
it does not admit Hessian matrix  so we use the following
generalized version of strictsaddle
De nition   We say function     is      pseudo
strict saddle if for any    at least one of followings holds 

   cid      cid     
   gx  so that     gx            gx           

 min gx       

    is  close to    cid    the set of local minima 

Note that in this de nition  the upperbound in   can be
viewed as similar to the idea of subgradient  For functions with nondifferentiable points  subgradient is de ned
so that it still offers   lowerbound for the function  In our
case this is very similar   although Hessian is not de ned 
we can use   smooth function that upperbounds the current
function  upperbound is required for minimization  In the
case of robust PCA the upperbound is obtained by    xed
   Using this formalization we can prove
Theorem   There is an absolute constant       if    
   and           cid     
  holds  for objective function
Eq  we have   all local minima satis es UV cid      cid 
  objective function is    cid 
 pseudo strict
saddle for polynomially small  

 
        
 cid 
 

 cid 

In this section  we also denote       as objective function
with respect to parameter    abuse the notation of      
previously de ned over   

Direction of Improvement The optimality condition
 De nition   implies if the gradient is nonzero  or if we
can  nd   negative direction of the Hessian  that is   direction    so that   cid            then the point is not
  local minimum    common technique in characterizing
the optimization landscape is therefore trying to explicitly
 nd this negative direction  We call this the direction of
improvement  Different works  Bhojanapalli et al   
Ge et al    have chosen very different directions of
improvement 
In our framework  we show it suf ces to choose   single
direction   as the direction of improvement  Intuitively 
this direction should bring us close to the true solution   cid 
from the current point    Due to rotational symmetry   
and UR behave the same for the objective if   is   rotation
matrix  we need to carefully de ne the difference between
  and   cid 
De nition   Given matrices      cid    Rd    de ne their
difference           cid    where     Rr   is chosen as
    argminZ cid   ZZ cid    cid       cid   cid 
   
Note that this de nition tries to  align    and   cid  before
taking their difference  and therefore is invariant under rotations  In particular  this de nition has the nice property
that as long as     UU cid  is close to   cid      cid   cid cid 
we have   is small  we defer the proof to Appendix 
Lemma   Given matrices      cid    Rd    let     UU cid 
and   cid      cid   cid cid  and let   be de ned as in De 
     cid       cid cid 
nition   then we have  cid cid cid 
    and
  cid cid 
 cid       cid cid 
   
 cid 

   

 
 
 

 

No Spurious Local Minima in Nonconvex Low Rank Problems

Now we can state the main Lemma 
Lemma    Main  For the objective   let   be de ned as
in De nition   and     UU cid  Then  for any     Rd   
we have

                  cid         cid 
         cid               cid 
   cid        cid                     cid       cid 
 

To see why this lemma is useful  let us look at the simplest
case where          and   is identity  In this case  if
gradient is zero  by Eq   

                  cid cid 

     cid       cid cid 
By Lemma   this is no more than  cid       cid cid 
    Therefore  all stationary point with    cid     must be saddle
points  and we immediately conclude all local minimum
satis es UU cid      cid 

 

Interaction with Regularizer For problems such as matrix completion  the Hessian   does not preserve the norm
for all low rank matrices  In these cases we need to use
additional regularizer  In particular  conceptually we need
the following steps 

  Show that the regularizer   ensures for any   such

that                  for some set   

  Show that whenever        the Hessian operator  
behaves similarly as identity  for some       we have 
 cid         cid          cid              cid   
   cid cid 
   

  Show that the regularizer does not contribute   large
positive term to                This means
we show an upperbound for  cid        cid       
             cid       cid 

Interestingly  these steps are not just useful for handling
regularizers  Any deviation to the original model  such as
noise  or if the optimal matrix is not exactly low rank  can
be viewed as an additional  regularizer  function     
and argued in the same framework  See supplementary material for more details 

  Matrix Sensing

Matrix sensing is the ideal setting for this framework  For
symmetric matrix sensing  the objective function is

min

  Rd  

 
  

 cid Ai  UU cid cid    bi 

 

  cid 

  

Recall that matrices  Ai                are known sensing matrices  and bi    cid Ai    cid cid  is the result of ith observation  The intended solution is the unknown low rank
matrix   cid      cid   cid cid  For any low rank matrix    the
Hessian operator satis es

  cid 

           

 cid Ai    cid 

  

Therefore if the sensing matrices satisfy the RIP property
 De nition   the Hessian operator is close to identity for
all low rank matrices  In the symmetric case there is no
regularizer  so the landscape for symmetric matrix sensing
follows immediately from our main Lemma  
Theorem   When measurement  Ai  satis es      
   
RIP  for matrix sensing objective   we have   all local minima   satisfy UU cid      cid    the function is
   cid 

 strict saddle 

        
 cid 
 

Proof  For point   with small gradient
 cid      cid       by        RIP property 
                      cid cid cid 

 

satisfying

         cid       cid cid 
           cid       cid cid 
     cid 
     cid cid  

  cid cid 

     cid cid  
     cid cid  

inequality is due to Lemma   that
The second last
     cid       cid cid 
 cid cid cid 
   and last inequality is due to
  and second part of Lemma   This means if  
      
is not close to   cid  that is  if  cid cid      
  we have    
              cid 
 
     
 cid 
 
strict saddle property  Take       we know all stationary
points with  cid cid    cid    are saddle points  This means all
local minima are global minima  satisfying UU cid      cid 
which  nishes the proof 

   This proves    cid 

  cid cid 

 cid 
 

  Matrix Completion

For matrix completion  we need to ensure the incoherence
condition  De nition   In order to do that  we add   regularizer      that penalize the objective function when
some row of   is too large  We choose the same regui cid Ui cid   
 

larizer as  Ge et al            cid  

The objective is then

min

  Rd  

 
  

 cid   cid    UU cid cid 

        

 

Using our framework  we  rst need to show that the regularizer ensures all rows of   are small  step  
Lemma   There exists an absolute constant    when sample rate         
    and    

  log             cid 

 

No Spurious Local Minima in Nonconvex Low Rank Problems

   cid    we have for any points   with  cid      cid      
   
for polynomially small   with probability at least    
 poly   

 cid   cid 

    cid     

max

 

 

 cid     cid cid 

 

 cid 

This is   slightly stronger version of Lemma   in  Ge
et al    Next we show under this regularizer  we can
still select the direction   and the  rst part of Equation  
is signi cantly negative when   is large  step  
Lemma   When sample rate          cid  log  
  by
choosing          cid 
   cid    with probability at least      poly    for all   with  cid      cid      
for polynomially small   we have

    and        

 

 

 cid 

 

       

 cid     cid             cid     cid 

  cid cid 

 

This lemma follows from several standard concentration inequalities  and is made possible because of the incoherence
bound we proved in the previous lemma 
Finally we show the additional regularizer related term in
Equation   is bounded  step  
Lemma   By choosing          cid 
  cid 

    and    

    we have 
                 cid       cid     cid 

  cid cid 

 

 

 
 

Combining these three lemmas  it is easy to see
Theorem   When sample rate          cid  log  
  by
choosing          cid 
   cid    Then with
probability at least      poly    for matrix completion
objective   we have   all local minima satisfy UU cid   
 strict saddle for
  cid    the function is    cid 
polynomially small  

    and        

        
 cid 
 

 

 

Notice that our proof is different from  Ge et al    as
we focus on the direction   for both  rst and second order
conditions while they need to select different directions for
the Hessian  The framework allowed us to get   simpler
proof  generalize to asymmetric case and also improved the
dependencies on rank 

  Robust PCA

In the robust PCA problem  for any given matrix   the
objective function try to  nd the optimal sparse perturbation   
In the symmetric PSD case  recall we observe
Mo     cid     cid  we de ne the set    to be the set of matrices whose rows columns have at most  fraction nonzero
entries  and entries are bounded by      cid 
    Note the projection onto set    be computed in polynomial time  using
  max  ow algorithm 

 

We assume   cid       the objective can be written as

min
 

      wheref       min
    

 
 

 cid UU cid        Mo cid 
   
 

Here   is   slack parameter that we choose later 
Note that now the objective function       is not quadratic 
so we cannot use the framework directly  However  if we
      then fS       
  is   quadratic
function with Hessian equal to identity  We can still apply
our framework to this function  In this case  since the Hessian is identity for all matrices  we can skip the  rst step 
The problem becomes   matrix factorization problem 

 cid UU cid        Mo cid 

min

  Rd  

 
 

 cid     UU cid cid 
   

 

The difference here is that the matrix    which is   cid   
  cid       is not equal to   cid  and is in general not low rank 
We can use the framework to analyze this problem  and
treat the residue       cid  as the  regularizer      
Lemma   Let     Rd   be   symmetric PSD matrix 
and matrix factorization objective to be 
         cid UU cid      cid 

 

where               then   all local minima satis es UU cid    Pr     best rankr approximation    objective is    cid 

 strict saddle 

        
 cid 
 

To deal with the case   not  xed  but as minimizer
of Eq  we let     cid  be the best
rank rapproximation of   cid      cid       The next lemma shows
when   is close to    up to some rotation    will actually
be already close to   cid  up to some rotation 
Lemma   There is an absolute constant    assume       
and           cid     
    Let     cid  be the best rank rapproximation of   cid     cid    where   is the minimizer as
in Eq  Assume minR cid   RR cid    cid         cid      
 
Let   be de ned as in De nition   then  cid cid       
 cid 
for polynomially small  

The proof of Lemma   is inspired by Yi et al   
and uses the property of the optimally chosen sparse set
   Combining these two lemmas we get our main result 
Theorem   There is an absolute constant    if        and
       cid     
  holds  for objective function Eq  we
have   all local minima satis es UU cid      cid    objective function is    cid 
 pseudo strict saddle
for polynomially small  

 
        
 cid 
 

 cid 

  Handling Asymmetric Matrices
In this section we show how to reduce problems on asymmetric matrices to problems on symmetric PSD matrices 

No Spurious Local Minima in Nonconvex Low Rank Problems

Let   cid      cid   cid cid  and     UV cid  and objective function 

                  cid                cid 

 

 
 

 cid   cid       cid   cid 

           

 

Note this is   scaled version of objectives introduced in
Sec   multiplied by   and scaling will not change the
property of local minima  global minima and saddle points 
We view the problem as if it is trying to  nd             
matrix  whose  rst    rows are equal to    and last    rows
are equal to   
De nition   Suppose   cid  is the optimal solution  and
its SVD is   cid   cid   cid cid  Let   cid      cid   cid   
      cid   
        UV cid  is the current point  we reduce the
  cid   cid   
problem into   symmetric case using following notations 

   

    cid   

      WW cid    cid      cid   cid cid 
 
Further    is de ned to be the difference between   and
  cid  up to rotation as in De nition  

  cid 

 cid  

 cid 

 

 cid   cid 

 cid 

We will also transform the Hessian operators to operate on
             matrices  In particular  de ne Hessian    
such that for all   we have 

                       
             cid   cid       cid   cid 

 

Now  let                and we can rewrite the objective function       as

       cid                 cid                     
 
 
 
We know    perserves the norm of low rank matrices   
To reduce asymmetric problems to symmetric problem  intuitively  we also hope    to approximately preserve the
norm of    However this is impossible as by de nition 
   only acts on    which is the offdiagonal blocks of   
We can expect            to be close to the norm of
UV cid  but for all matrices      with the same UV cid  the
matrix   can have very different norms  The easiest example is to consider     diag    and     diag   
while UV cid      no matter what   is  the norm of   is
of order   and can change drastically  The regularizer
is exactly there to handle this case  the Hessian   of the
regularizer will be related to the norm of the diagonal components  therefore allowing the full Hessian            
to still be approximately identity 
Now we can formalize the reduction as the following main
Lemma 

Lemma   For the objective   let        cid  be de ned
as in De nition   Then  for any              we have

                  cid         cid 
         cid               cid     cid        cid 
                   cid       cid 

 
where              Further  if    satis es         
         cid   cid 
  for some matrix     UV cid  let  
and   be de ned as in   then                
 cid   cid 
   

Intuitively  this lemma shows the same direction of improvement works as before  and the regularizer is exactly
what it requires to maintain the normpreserving property
of the Hessian 
The proofs are deferred to supplementary material 

 cid   cid       cid   cid 

  Conclusions
In this paper we give   framework that explains the recent
success in understanding optimization landscape for low
rank matrix problems  Our framework connects and simpli es the existing proofs  and generalizes to new settings
such as asymmetric matrix completion and robust PCA 
The key observation is when the Hessian operator preserves
the norm of certain matrices  one can use the same directions of improvement to prove similar optimization landF is
scape  We show the regularizer  
exactly what it requires to maintain this norm preserving
property in the asymmetric case Our analysis also allows
the interaction between regularizer and Hessian to handle
dif cult settings such as 
For low rank matrix problems  there are generalizations
such as weighted matrix factorization Li et al    and
 bit matrix sensing Davenport et al    where the Hessian operator may behave differently as the settings we can
analyze  How to characterize the optimization landscape in
these settings is still an open problem 
In order to get general ways of understanding optimization
landscapes for more generally  there are still many open
problems  In particular  how can we decide whether two
problems are similar enough to share the same optimization
landscape    minimum requirement is that the nonconvex
problem should have the same symmetry structure   the set
of equivalent global optimum should be the same  In this
work  we show if the problems come from convex objective
functions with similar Hessian properties  then they have
the same optimization landscape  We hope this serves as  
 rst step towards general tools for understanding optimization landscape for groups of problems 

No Spurious Local Minima in Nonconvex Low Rank Problems

References
Agarwal  Naman  AllenZhu  Zeyuan  Bullins  Brian 
Hazan  Elad  and Ma  Tengyu  Finding approximate local minima for nonconvex optimization in linear time 
arXiv preprint arXiv   

Belkin  Mikhail  Rademacher  Luis  and Voss  James  Basis learning as an algorithmic primitive  arXiv preprint
arXiv   

Bengio  Yoshua  Learning deep architectures for AI  Foundations and trends   cid  in Machine Learning   
 

Bhojanapalli  Srinadh  Kyrillidis  Anastasios  and Sanghavi  Sujay  Dropping convexity for faster semide nite
optimization  arXiv   

Bhojanapalli  Srinadh  Neyshabur  Behnam  and Srebro 
Nathan  Global optimality of local search for low
rank matrix recovery  arXiv preprint arXiv 
 

Burer  Samuel and Monteiro  Renato DC    nonlinear programming algorithm for solving semide nite programs
via lowrank factorization  Mathematical Programming 
   

Candes  Emmanuel   and Plan  Yaniv  Tight oracle inequalities for lowrank matrix recovery from   minimal number of noisy random measurements  IEEE Transactions
on Information Theory     

Cand es  Emmanuel   and Recht  Benjamin  Exact matrix completion via convex optimization  Foundations
of Computational mathematics     

Candes  Emmanuel    Romberg  Justin    and Tao  Terence  Stable signal recovery from incomplete and inaccurate measurements  Communications on pure and
applied mathematics     

Cand es  Emmanuel    Li  Xiaodong  Ma  Yi  and Wright 
John  Robust principal component analysis  Journal of
the ACM  JACM     

Carmon  Yair  Duchi  John    Hinder  Oliver  and Sidford 
Aaron  Accelerated methods for nonconvex optimization  arXiv preprint arXiv   

Chen  Yudong and Wainwright  Martin   

Fast lowrank estimation by projected gradient descent  General
statistical and algorithmic guarantees  arXiv preprint
arXiv   

Davenport  Mark    Plan  Yaniv  van den Berg  Ewout  and
Wootters  Mary   bit matrix completion  Information
and Inference     

Fazel  Maryam  Matrix rank minimization with applications  PhD thesis  PhD thesis  Stanford University   

Ge  Rong  Huang  Furong  Jin  Chi  and Yuan  Yang  Escaping from saddle points online stochastic gradient
for tensor decomposition  arXiv   

Ge  Rong  Lee  Jason    and Ma  Tengyu  Matrix completion has no spurious local minimum  In Advances in
Neural Information Processing Systems  pp   
 

Hardt  Moritz  Understanding alternating minimization for

matrix completion  In FOCS   IEEE   

Hardt  Moritz and Wootters  Mary  Fast matrix completion
without the condition number  In COLT   pp   
   

Hotelling  Harold  Analysis of   complex of statistical variables into principal components  Journal of educational
psychology     

Jain  Prateek  Netrapalli  Praneeth  and Sanghavi  Sujay  Lowrank matrix completion using alternating minimization  In Proceedings of the forty fth annual ACM
symposium on Theory of computing  pp    ACM 
 

Jin  Chi  Ge  Rong  Netrapalli  Praneeth  Kakade  Sham   
and Jordan  Michael    How to escape saddle points ef 
ciently  arXiv preprint arXiv   

Keshavan  Raghunandan    Montanari  Andrea  and Oh 
InSewoong  Matrix completion from   few entries 
formation Theory  IEEE Transactions on   
     

Keshavan  Raghunandan    Montanari  Andrea  and Oh 
Sewoong  Matrix completion from noisy entries  The
Journal of Machine Learning Research   
   

Koren  Yehuda  The bellkor solution to the net ix grand

prize  Net ix prize documentation     

Li  Qiuwei and Tang  Gongguo  The nonconvex geometry
of lowrank matrix optimizations with general objective
functions  arXiv preprint arXiv   

Davenport  Mark   and Romberg  Justin  An overview of
lowrank matrix recovery from incomplete observations 
IEEE Journal of Selected Topics in Signal Processing 
   

Li  Yuanzhi  Liang  Yingyu  and Risteski  Andrej  Recovery guarantee of weighted lowrank approximaarXiv preprint
tion via alternating minimization 
arXiv   

No Spurious Local Minima in Nonconvex Low Rank Problems

Lin  Zhouchen  Chen  Minming  and Ma  Yi  The augmented lagrange multiplier method for exact recovarXiv preprint
ery of corrupted lowrank matrices 
arXiv   

Zhao  Tuo  Wang  Zhaoran  and Liu  Han    nonconvex
optimization framework for low rank matrix estimation 
In Advances in Neural Information Processing Systems 
pp     

Zheng  Qinqing and Lafferty  John  Convergence analysis
for rectangular matrix completion using burermonteiro
arXiv preprint
factorization and gradient descent 
arXiv   

Nesterov  Yurii and Polyak  Boris    Cubic regularization
of Newton method and its global performance  Mathematical Programming     

Netrapalli  Praneeth  Niranjan  UN  Sanghavi  Sujay 
Anandkumar  Animashree  and Jain  Prateek  Nonconvex robust pca  In Advances in Neural Information
Processing Systems  pp     

Park  Dohyung  Kyrillidis  Anastasios  Caramanis  Constantine  and Sanghavi  Sujay 
Nonsquare matrix
sensing without spurious local minima via the burermonteiro approach  arXiv preprint arXiv 
 

Recht  Benjamin  Fazel  Maryam  and Parrilo  Pablo   
Guaranteed minimumrank solutions of linear matrix
equations via nuclear norm minimization  SIAM review 
   

Rennie  Jasson DM and Srebro  Nathan  Fast maximum
margin matrix factorization for collaborative prediction 
In Proceedings of the  nd international conference on
Machine learning  pp    ACM   

Sun  Ju  Qu  Qing  and Wright  John  Complete dictionary
recovery over the sphere    Overview and the geometric
picture  arXiv     

Sun  Ju  Qu  Qing  and Wright  John  When are nonconvex
problems not scary  arXiv preprint arXiv 
   

Sun  Ruoyu and Luo  ZhiQuan  Guaranteed matrix comIn Foundations
pletion via nonconvex factorization 
of Computer Science  FOCS    IEEE  th Annual
Symposium on  pp    IEEE   

Tu  Stephen  Boczar  Ross  Soltanolkotabi  Mahdi  and
Recht  Benjamin  Lowrank solutions of linear maarXiv preprint
trix equations via procrustes  ow 
arXiv   

Yi  Xinyang  Park  Dohyung  Chen  Yudong  and Caramanis  Constantine  Fast algorithms for robust pca via graIn Advances in neural information prodient descent 
cessing systems  pp     

Zhang  Xiao  Wang  Lingxiao  and Gu  Quanquan    nonconvex free lunch for lowrank plus sparse matrix recovery  arXiv preprint arXiv   

