SelfPaced Cotraining

Fan Ma   Deyu Meng     Qi Xie   Zina Li   Xuanyi Dong  

Abstract

Cotraining is   wellknown semisupervised
learning approach which trains classi ers on two
different views and exchanges labels of unlabeled instances in an iterative way  During cotraining process  labels of unlabeled instances in
the training pool are very likely to be false especially in the initial training rounds  while the
standard cotraining algorithm utilizes    draw
without replacement  manner and does not remove these false labeled instances from training 
This issue not only tends to degenerate its performance but also hampers its fundamental theory  Besides  there is no optimization model to
explain what objective   cotraining process optimizes  To these issues  in this study we design  
new cotraining algorithm named selfpaced cotraining  SPaCo  with    draw with replacement 
learning mode  The rationality of SPaCo can be
proved under theoretical assumptions utilized in
traditional cotraining research  and furthermore 
the algorithm exactly complies with the alternative optimization process for an optimization
model of selfpaced curriculum learning  which
can be  nely explained in robust learning manner  Experimental results substantiate the superiority of the proposed method as compared with
current stateof theart cotraining methods 

  Introduction
Semisupervised learning  SSL  aims to implement learning on both labeled and unlabeled data through fully
considering the supervised knowledge delivered by labeled data and unsupervised data structure under unlabeled
ones  Zhu    Cotraining  Blum   Mitchell    is
one of the most classical and well known SSL approaches
that train classi ers on two views and exchanges labels of

 Xi an Jiaotong University  Xi an  China  University of Technology Sydney  Sydney  Australia  Correspondence to  Deyu
Meng  dymeng xjtu edu cn 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

unlabeled instances in an iterative way  In the recent years 
cotraining has been attracting much attention attributed
to both its wide applications  like web classi cation and
visual detection  Xu et al    and rational theoretical supports  Blum   Mitchell    Balcan et al   
Wang   Zhou     
However  there are still some limitations existing in current cotraining investigation  Speci cally  although there
are multiple theoretical results to support the rationality
of the cotraining regimes  most of them require   strong
preassumption that the false pseudolabeled instances can
be correctly recognized during training by classi ers or
pseudo labels of unlabeled instances predicted in each iteration are of   high con dence extent  Based on such
highcon dence assumption  most of current cotraining
algorithms add pseudolabeled samples into training without replacement  Nevertheless  in most real cases such assumption is too subjective to be satis ed  especially in the
early learning stage of   cotraining algorithm  The learned
classi ers might be not able to con dently distinguish certain samples  or precisely pseudoannotate them with an
expected accuracy requirement  This not only inclines
to degenerate the performance of cotraining since those
false pseudolabeled involved in training have no chance
to be recti ed in the latter training process on account of
such  draw without replacement  manner  but also might
make the basic assumption under theoretical support of cotraining incorrect 
Another critical issue in most of current cotraining methods is on their absence of an optimization model that can
measure the performance and explain the intrinsic mechanism under the cotraining implementation  As formally
de ned in  Mitchell    the performance measure is
one of the necessary elements for   machine learning
method  It is thus meaningful to explore whether there exists such an optimization model  which can  nely interpret
the cotraining implementation as the process of solving
this model  Such model also should be helpful in revealing
more insights underlying cotraining 
To address the aforementioned issues    new cotraining
method  called selfpaced cotraining  SPaCo  is proposed
in this study  The method differs from the previous cotraining regimes mainly in two aspects  Firstly  it utilizes  

SelfPaced Cotraining

 draw with replacement  learning manner  In the method 
an unlabeled instance having been added into the training
pool is likely to be removed if classi ers in later training
rounds identify it as   lowcon dence annotated one  Besides  the pseudo label of an unlabeled instance has chance
to be recti ed based on the prediction knowledge obtained
by classi ers in later training rounds  Secondly  the SPaCo
method employs   serial mode to update the classi ers of
two views in cotraining implementation instead of the parallel mode commonly adopted by previous methods  Under
such amelioration  the new method can be proved to still
guarantee the theoretical effectiveness under  expansion
assumption in the traditional cotraining theory  while more
importantly  such series implementation exactly complies
with the alternative optimization algorithm on solving  
concise optimization model  Besides  it is substantiated
that the new method can attain evidently better performance beyond current stateof theart cotraining methods
on average of our experiments  which further veri es the
rationality of the proposed SPaCo algorithm 
In summary  this work makes the following contributions 
    new cotraining method  called SPaCo  is proposed
by adopting the  draw with replacement  learning
manner  Similar to traditional cotraining strategies 
the theoretical soundness of the new method can also
be proved under the commonly speci ed  expansion
assumption for pseudolabel con dence 

  The implementation process of the new method exactly complies with an alternative updating algorithm for   underlying optimization model  This
model corresponds to   selfpaced curriculum learning paradigm  which helps reveal more  easyto hard 
insights under the cotraining implementation 

  Through this derived model  the effectiveness of the
proposed SPaCo method can be  nely interpreted as 
the method implements robust learning regimes in
both views under the regularization that the robust loss
forms in two views are closely related  This understanding provides   natural explanation for the effectiveness mechanism under such cotraining strategy 
without any requirement of subjective assumptions for
pseudolabel con dences 

  Experimental results on multiple text classi cation
and person reID data substantiate the superiority of
the SPaCo method as compared with current stateof 
theart cotraining methods 

The rest of the paper is organized as follows  We  rst
brie   introduce related work in Section   and then present
the proposed SPaCo algorithm and its underlying model in
Section   More theoretical explanations on its insights are

also provided in this part  We then present the experimental
results and  nally give   conclusion remark 

  Related Work
  Cotraining methods

The traditional cotrainig method  Blum   Mitchell   
builds classi ers for different views and exchanges predictions of highcon dence unlabeled data to augment the
training set in two views in each training round  Afterwards  multiple advancements have been developed  These
cotraining variants can be roughly categorized into two
paradigms  One paradigm is to follow the iterative training process of cotraining but to label unlabeled samples
using different methods with certain con dence criterion in
each iteration  Goldman   Zhou    Brefeld   Scheffer      Zhou et al    Li   Zhou    Zhang
  Zhou    The other is to embed extra information
from other views as   regularization term into the learning
objective  Sindhwani et al    Sindhwani   Rosenberg 
  Ye et al    and turn the semisupervised multiview problem into   new optimization problem 

  Other methods related to colearning

Recently  there are multiple other methods proposed aiming to simultaneously learn classi ers on two different
views or multiviews  While different from cotraining
approaches  these methods directly pseudolabel all unlabeled samples and involve them into the training process 
  typical approach along this line is CoEM  Nigam  
Ghani    which iteratively updates labels of unlabeled
data based on the posterior class probability calculated by
naive Bayesian learners  and updates the classi ers on all
of them  Several other methods directly encode the unknown labels of unlabeled data and classi er parameters
on two views into   model  and simultaneously calculate all
these variables through solving this model  Typical methods of this category include CoMR  Sindhwani   Rosenberg    which deduces   coregularization kernel by
exploiting two Reproducing Kernel Hilbert Spaces de ned
over the same input space  and RANC  Ye et al   
which assumes predictions for unlabeled data under different views are consistent with each other and enforces an af 
 xed rank constraint on optimization function of each view 

  Cotraining theory development

The rationality of cotraining is supported by   series of related theoretical analyses        Blum   Mitchell   
showed that class on two views is learnable in the PAC
model with classi cation noise when the features of two
views are independent given the class  To further relax
the assumption for cotraining   Abney    provided  

SelfPaced Cotraining

weaker viewindependence condition that brings about the
success of cotraining  Afterwards   Balcan et al   
introduced the  expansion assumption  which is   con 
dence assumption on pseudo labeled positive samples  further relaxing the condition of guaranteeing the effectiveness of   cotraining strategy  Later   Wang   Zhou   
made   new analysis of cotraining on label propagation
strategy designed for cotraining 
for current coDespite providing theoretical support
training methods  all these theories include some subjective
assumptions like the independence between classi ers of
different views and the con dence extent of pseudolabels
of unlabeled samples obtained by the algorithm  These assumptions  however  are not only hard to be justi ed in real
applications  but also not very intuitive to be easily understood by common cotraining users  which might possibly
keep it from being more extensively utilized in practice 

  Selfpaced learning

 Bengio et al    proposed   learning paradigm called
curriculum learning CL  in which   model is learned by
gradually including samples from easy to complex in training so as to increase the entropy of training samples  Afterwards  selfpaced learning  Kumar et al    is proposed to embed curriculum design as   regularization term
into the learning objective  Due to its generality  the SPL
theory has been widely applied to various tasks  such as
object tracking  Supancic   Ramanan    image classi cation  Jiang et al    and multimedia event detection  Jiang et al        Especially  the SPL paradigm
has been integrated into the system developed by CMU Informedia team  and achieved the leading performance in
challenging TRECVID MED MER competition organized
by NIST in    Yu et al   
Let   yi    xi     denote the loss function which calculates the cost between the ground truth label yi and the estimated one   xi     Here   represents the model parameter inside the decision function    The SPL model considers   weighted loss term for all samples and   general
selfpaced regularizer with respect to sample weights  expressed as 

min

     

           

 viL yi    xi       vi   

 
where   is the age parameter for controlling the learning space  and         represents the selfpaced regularizer
 SPregularizer brie    By jointly learning the model parameter   and the latent weight   using alternative optimization strategy with gradually increasing age parameter 
more samples can be automatically included into training
from easy to complex in   purely selfpaced way 
The recent development of SPL includes that  Jiang et al 

  cid 

  

  improved SPL as   more effective selfpaced curriculum learning  SPCL  regime by embedding useful
loss prior knowledge into the model and analyzed that
this regime is analogous to rational
instructorstudent 
collaborative learning mode of human teaching  And multiple researches  Zhang et al    Zhao et al    Pi
et al    showed that SPL worked well when dealing
with real data  Besides   Meng   Zhao    Ma et al 
  proved that the optimization problem of SPL solved
by the alternative optimization algorithm is equivalent to  
robust loss minimization problem solved by   majorizationminimization algorithm  This work  rst reveals the insightful understanding of the robust learning mechanism under
SPL 

  Selfpaced Cotraining
In this section  we introduce the details of our proposed
framework  SelfPaced Cotraining  SPaCo  model  We
 rst present the mathematical form of this model  and then
introduce the alternative optimization algorithm for solving
this model  It is then evident that the algorithm  nely complies with   cotraining strategy in    draw with replacement  mode and   series implementation manner  Some
properties of the algorithm will be detailedly analyzed afterwards  We then provide the theoretical support under
 expansion assumption  and show that the effectiveness of
the algorithm can be proved under the conventional routine
of cotraining  Finally    new interpretation on the mechanism of this method will be presented from the viewpoint
of selfpaced learning 

  SPaCo Model

We  rst present the following SPaCo model  which extends
the selfpaced learning optimization model   to two view
scenarios  by introducing importance weights of two views
                        together with the correv 
      
sponding hard selfpaced regularizer              as proposed in  Jiang et al     

           

    yk         

  yi          

 

        

 
 

     

 

     

    yk          

                  
   

  

    

         
where   and   denote the number of labeled and unlabeled instances  respectively      
is the ith sample     
           under jth view          and yi is the common label of     
  denotes the weight of

for every        

 

 

min
   
    yk     
           

 cid 
  cid 
 cid 

  

  

 

    cid 

 cid 

  

SelfPaced Cotraining

    
  where                         is an udimensional
vector preserving all the weights of unlabeled instances under jth view where its kth element is     
          represents
parameters of jth classi er trained on jth view      is the
age parameter controlling the training scale in each iteration with respect to jth view  and   is the parameter adjusting in uence from the other view when one view is going
to add more training samples 
The above SPaCo model actually corresponds to the sum
of SPL model under two views plus   regularization term
        This inner product encodes the relationship of
 sample easiness degree  between two views  The new coregularizer delivers the basic assumption under cotraining
that different views share common knowledge of pseudolabeled sample con dence  an unlabeled sample is likely
to be labeled correctly or wrongly simultaneously for both
views  and thus this inner product enforces the weight penalizing the loss of one view similar to that of the other
view  This  nely accords to the idea of SPCL and complies with an instructorstudent collaborative learning manner under   speci   cotraining curriculum 

  AOS algorithm for solving  

The alternative optimization strategy  AOS  can be readily adopted to solve this SPaCo model  The optimization
process are shown as follows 
Initialization  The  rst step is to initialize parameters of
model     and    are zero vectors in Ru    and  
are initialized with small values to allow   few unlabeled
instances into training for the  rst iteration    is set as  
speci   value in the whole training process  Two classi ers
are simultaneously trained on labeled samples to get initial
losses of both labeled and unlabeled instances 
Update     
         The physical meaning of this
step is to prepare con dent unlabeled instances  with nonzero     
values  for the training on the jth view  This is
known as the process of picking con dent instances from
one view in the traditional cotraining algorithm  By calculating the derivative of Eq    with respect to     
  we
can get

 

 

 

  

     

 

      

 

             
   

 

Then we can get the closedform updating equation for
   

 

 cid 

as follows 
    

 

 

      
  otherwise 

 

         vj
  

 

 For the ease of description  we only present the optimization
process under one view since parameters under the other view are
optimized in the same way 

In the  rst iteration  all the     
    are zeros according to the
initialization  Thus unlabeled instances are selected only
from the    th view  In other words  unlabeled instances
with loss values less than     will be seen as con dent
instances 
Update     
    The goal of this step is to formally de ne
which unlabeled instances will be feeded into the training
pool of   th  view  The optimization process for     
is the
 
same as previous step  but unlabeled instances selected in
this step will be directly employed for training in jth view 
From Eq 
  we can easily observe that con dent instances from the other view  picked in the previous step 
possess higher chance than other instances to be selected
for training 
Update      This step aims to train   classi er by virtue
of the labeled and pseudolabeled samples in the training
pool of the jth view  By setting the loss as wellknown
hinge loss  we can directly choose SVM to train the expected classi er  In this case  Eq    degenerates to the
standard SVM optimization problem as 

min
    

 
 

       

    

   

      
    
   

 

  

    

 

      yt        

                      This
where     
problem can be readily solved by any offthe shelf SVM
toolbox  Jiang et al      For the cross entropy loss 
we can employ deep learning network to train the expected
classi er  and thus our model is not constrained within one
single classi cation algorithm 
Update yk  The next step is to update pseudolabels of
training samples by solving the following minimization
subproblem 

  cid 

    cid 

 cid 

  

yk   argmin

yk

    
    yk        

        

 

It is easy to prove that the global optimum of the above
problem can be obtained by directly setting the pseudolabel yi of   training sample as the weighted sum of prediction value under two classi ers 
Once pseudolabels of training samples are refreshed 
    are enlarged to allow more instances with lager
loss values into the training pool in the next iteration  Then
we repeat the above optimization process with respect to
each variable under different views until there is no more
available unlabeled instances or the preset largest iteration
number is reached 
The entire process of this alternative optimization algorithm is summarized in Algorithm   It can be seen that

SelfPaced Cotraining

such optimization process exactly corresponds to the traditional cotraing algorithm with some reasonable adjustments  Assisted by this model  such   cotraining algorithm
possesses all of the necessary elements   formal machine
learning method should have 

Algorithm   Alternative Optimization Algorithm for Solving SPaCo Model
  Input  samples   

        

        

     labels

       

     yl  parameters       and max round 

  Prepare con dent instances from

  Output       
  Initialize           and  
  Update      
  training round    
  while not converge   training round   max round do
for       to   do
 
Update     
 
      th view for training on jth view
Update     
training pool of jth view based on     
 
     
Update      Train   classi er  SVM for instance  on training pool of jth view
Update yk  Find optimal pseudo label for each of
selected unlabeled instances by solving Eq   
Augment    

    Add unlabeled instances into the
and

 

 

 

 

 

 
end for
 
  end while
  Return      

From Algorithm   we can easily observe that it has   very
similar implementation scheme as traditional cotraining
methods  Speci cally  it also iteratively trains classi ers on
two views by exchanging labels of unlabeled instances  Yet
beyond that  the proposed algorithm complies with an optimization implementation on   underlying selfpaced learning model  This model thus tends to provide some novel insightful understandings on the intrinsic effectiveness mechanism under the cotraining approach  which will be analyzed in Sec   

  Algorithm analysis

The standard cotraining method  Blum   Mitchell   
requires to simultaneously train classi ers of both views 
and then select highly con dent samples to label for each
view and feed them into the training pool of the other view 
The proposed SPaCo algorithm  as listed in Algorithm  
mainly differs from traditional cotraining methods in the
following threefold aspects 
First  instead of  draw without replacement  mode as conventional  the SPaCo algorithm utilizes    draw with re 

placement  manner  The algorithm does not consistently
keep the previously selected training pool unchanged 
while   con dence sample in the pool has certain chance
to be thrown out from it when the loss value of   sample is
larger than   preset threshold       Note that this con 
dence threshold is larger than that   set for samples not
in the training pool  implying that we still more prefer to
keep the samples in the pool than those not in it  Also 
when we set   as an extremely large value  then it is easy
to deduce that the algorithm will degenerate to   traditional
 draw without replacement  method since the loss values
of any samples in the training pool will be smaller than the
threshold and will thus be de nitely selected in the next
round 
Second  instead of the parallel training way as conventional  the new algorithm uses   serial manner for training
the classi ers of two views  This not only will make this algorithm fully comply with the alternative updating strategy
for solving an optimization model  but also leads to better
performance than traditional parallel model methods  see
experiment part  This might possibly be due to the fact
that the serial way can better guarantee the reliability of
added highcon dence pseudolabel samples based on the
updated while not the nonupdated classi ers as in parallel
way 
Third  when updating the training pool in one view  besides feeding into highcon dence samples justi ed by the
other view  the new algorithm might add into the pool   few
highcon dence samples which obtain very small loss values calculated on the current view  This is expected to make
the algorithm use more reliable highcon dence knowledge
from the predicted knowledge by current classi ers 

  Learnable theory of SPaCo under  expansion

assumption

Similar to the theoretical support for traditional cotraining
methods  we want to prove that the SPaCo algorithm is  
PAC learning algorithm under the certain  expansion condition as utilized in  Balcan et al    First we give the
de nition of the  expansion condition 

De nition    Balcan et al    Let     denote the positive region and    denote the distribution over     and
Xi         is the training data set in the ith view  For
        and         the    is  expanding if the following inequality holds 

               min                         

 
where            denotes the probability of examples for
being con dent in both views  and            denotes
the probability of examples for being con dent in only one
view 

SelfPaced Cotraining

We can then prove the following theorem for the proposed
SPaCo algorithm  The proof is presented in supplementary
material 

Theorem   Let    in and    in be the desired accuracy
and con dence parameters  Suppose that the serial  
expanding condition is satis ed in each training round  and
then we can achieve the error rate    in with probability  
   
   in by running the SPaCo for         
   
pinit  
rounds  each time running algorithm    and algorithm   
with accuracy and con dence parameters set to    in
and
   in
     respectively 
Therefore  the rationality of the new algorithm can also be
supported by the traditional theoretical means 

  log  
   in

 

 

  Corobust loss interpretation for SPaCo rationality

Based on the SPaCo model   underlying Algorithm  
we can get some new insights underlying the cotraining
regimes 
 Meng   Zhao    has proved that the optimization
problem   of SPL is closely related to   robust loss minimization problem  Such understanding can be utilized in
this study to present   new understanding for the effectiveness insight underlying this cotraining strategy  Specifically  in the SPaCo model   there is   separate SPL
objective function for each view  which means that there
implicitly exists   robust loss for training the classi er of
each view on pseudolabeled samples  However  such robust losses for different views are not independent while
closely related to each other since   sample should be labeled correctly or wrongly for any view of data representation  Thus in SPaCo model   the cotraining curriculum
regularization actually encodes such relationship between
robustness of different views  That is  through consistently
exchanging pseudolabels justi ed in different views  the
robust loss functions of both views are enforced to be related by such regularization term  This guarantees   sound
learning manner for the cotraining process 
Note that such   explanation for the effectiveness of the
SPaCo algorithm can be easily understood and requires
no subjective assumptions on pseudolabel con dences or
twoview independence  It is thus expected to facilitate  
better extension of cotraining paradigms to general users 

  Experimental results
To validate the performance of the proposed SPaCo
method  we  rst employ six text classi cation data sets derived from three realworld domains  where each data set
is associated with two naturally partitioned or arti cially
generated views  Besides  we also apply our method to the
person reidenti cation task  which is   popular research

Table   Statistics of Utilized Data Sets

Attributes

view  

Data set Number of
examples
 
course
ads 
 
 
ads 
 
ads 
 
NG 
NG 
 

view  
 
 
 
 
 
 

Postive
proportion
 
 
 
 

 
 

 
 
 
 
 
 

topic in the  eld of computer vision 

  Text Classi cation Experiments
Datasets  We employ the following six data sets  all having
been used for testing in the previous cotraining literatures 

Course data  This data set contains   home pages collected from web sites of Computer Science departments of
Cornell University  These pages are manually labeled as
course or noncourse  each with   pagebased view  words
appearing in the page itself  and   linkbased view  words
appearing in hyperlinks pointing to it  Among all homepages  course homepages   correspond to positive examples while all others are negative examples 
Advertisement data  This data set contains advertising images in web pages  Each image is described from multiple
views  such as image properties  image caption  words occurring in the image source   url  words occurring in the
af liated web page   url and words occurring in the image
anchor   url  By using the words of different areas  we create data sets named ads  ads  and ads 
Newsgroup data  This data set is related to   newsgroups postings from the MiniNewsgroup data  Each
group consists of   postings randomly drawn from the
  postings in the original  Newsgroup data  The  
chosen newsgropus are divided into four groups  and we
create NG  and NG  data sets based on   partition strategy  Zhang   Zhou   
Each utilized data set contains two classes  Table   summarizes the statistics of these data sets  For each data set 
  of the data are retained as test examples while the rest
are used as training examples       including both labeled
and unlabeled examples  Three experiments are conducted
on these six data sets with different number of labeled in 
 Data available at http www cs cmu edu afs cs project theo 

 www data 

 Advertisements

 Data available at https archive ics uci edu ml datasets Internet

 Data available at http www cs cmu edu afs cs project theo 

 www  naivebayes mini newsgroup tar gz 

SelfPaced Cotraining

Table   Accuracy comparison of   competing methods on   testing data sets  Each result is averaged from   trials with independently
sampled labeled samples  The best result in each series is highlighted in bold 

  

  

  

course
ng 
ng 
ad 
ad 
ad 
average
course
ng 
ng 
ad 
ad 
ad 
average
course
ng 
ng 
ad 
ad 
ad 
average

SelfTrain
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Cotrain
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

CoEM CoMR
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Cotrade
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

RANC
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

SPaCo
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

stances  To simulate realworld cases where labeled samples are rarely available  only   small number of instances
are randomly selected as labeled data  Among the training samples  we choose    positive and        negative 
          labeled instances for Course     positive and
       negative labeled examples for Advertisement  and
        positive and        negative labeled examples for
Newsgroup  based on their different data size  On each data
set  three series of experiments are implemented       for
          and each experiment series contains   trials  with independently sampled labeled samples 
Experiment setting  The performance of SPaCo is compared with six current semisupervised learning algorithms  including  SelfTrain  Scudder    the most
conventional SSL method  the cotraining method  Blum
  Mitchell   
the most conventional cotraining
method  CoTrade  Zhang   Zhou    one stateof theart cotraining method  CoEM  Nigam   Ghani   
CoMR  Sindhwani   Rosenberg    and RANC  Ye
et al    representing the stateof theart for solving
twoview colearning problem  SVM is employed as base
classi er in text classi cation task for all the iterative training methods 
For SelfTrain  two classi ers expand their training pool
by selecting the  most con dent  samples they thought
by themselves in each training round  The standard co 

training and Cotrade select the  most con dent  samples
justi ed by the other view  To avoid introducing too much
noise  each classi er of SelfTrain and standard cotraining
only selects    ositive     egative  for the course data    
   for the ads data  and       for the newsgroup data 
These three algorithms terminate when no more examples
are available for training  Instead of augmenting training
pool step by step  CoEM estimates class probability in every training round and employs these pseudo labels to reestimate class probability after each iteration 
For CoMR  each data set adpots an uni ed representation by merging the two views  and the regularization parameters     varied on   gird of values
            where the results from
optimal con guration are reported  RANC embeds the rank
constraints into the optimized function of multiview learning object  Ye et al    and proposes two different ways
to solve the problem  The ADMM method  Boyd et al 
  is adopted in this paper to get the solution 
For our SPaCo algorithm  instead of tuning   directly  we
increase the number of nonzero element of      in each
training round  Besides  to judge unlabeled instances based
on two views  we easily set   as   throughout all our experiments  Its setting actually is not sensitive to the  nal
performance of our algorithm 
Experimental results  The average accuracy over all  

SelfPaced Cotraining

trails obtained by each competing method on each data set
is shown in Table   From the table  we can easily observe
that the proposed SPaCo method can attain the best   out
of   or the second best   out of   performance among
all competing methods  In average  SPaCo acquires an evident better performance than other competing methods under different sizes of initialized labeled examples  This veri es the superiority of the proposed method on these cotraining problems 

  Person Reidenti cation Experiments

The person reidenti cation  reID  task is usually viewed
as an image retrieval problem  aiming to match pedestrians
from the gallery  Zheng et al    Speci cally  given  
personof interest  query    person reID method aims to
determine whether the person has been observed by cameras 
Dataset  We employ the Market  set in our experiment  This data set contains   detected person bounding boxes of   identities  Zheng et al    Images
of each identity is captured by six cameras at most  and two
cameras at least  According to the data set setting  training
set contains   cropped images of   identities and
testing set contains   cropped images of   identities  They are directly detected by Deformable Part Model
 DPM  instead of handdrawn bboxs  which is closer to the
realistic setting  Each identity may have multiple images
under each camera  We use the provided  xed training and
test set  under both the singlequery and multiquery evaluation settings 
In the experiments    instances of training data with
their labels are chose with labels  the rest of data are treated
as unlabeled instances  Instead of directly selecting labeled
samples from the whole data  we randomly sample  
labeled samples for each class  We implemented the experiments two times under two randomly sampled training
data  and their average is reported as the  nal result 
Experiment setting  Like the stateof art Person reID
model proposed by  Zheng et al    three different
deep learning networks  including Alexnet  Googlenet  and
Vggnet  respectively  are used to generate multiview features for Market  data set  The employed model is  
new siamese network that simultaneously computes identi 
 cation loss and veri cation loss  Zheng et al    We
treat this new loss as the optimized loss in our model  and
thus the reID task can be well handled using the SPaCo
algorithm  Two combinations  Alexnet with Googlenet and
Googlenet with Vggnet  are adopted in our experiments 
Over all experiments  parameters of each model are set following the training setting as  Zheng et al   
Via using cotraining and selftrain algorithms as compar 

Table   Rank  accuracy comparison of   competing methods on
Market  data set

Vggnet   Googlenet
Method
Final
View  View 
 
 
 
Base
 
 
SelfTrain 
Cotrain  
 
 
SPaCo            

Alexnet   Googlenet
Final
 
 
 

View  View 
 
 
 
 
 
 

ison methods  the effectiveness of SPaCo method is validated  The training process on reID task is the same as the
process on text classi cation task  Cotrade is not adopted
to reID task since it can only handle two class problems 
CoMR and RANC are also not included since they are not
trained in an iterative way  and hard to be applied to the
reID task  For the SPaCo algorithm  in every iteration 
the number of selected unlabeled instances is ranged from
  to   The lower bound is to guarantee suf cient
instances for each class and the higher bound is to avoid
introducting too many noisy samples 
Experimental results  From Table   it is seen that rank 
accuracies of all methods are improved since more samples are used for training  When Alexnet and Googlenet
networks are adopted  SPaCo achieves the highest rank 
  accuracy not only on  nal result  but also on two view
results  respectively 
Speci cally  our model achieves
  rank  accuracy  evidently better than other competing methods  For the Vggnet and Googlenet view experiment  our SPaCo algorithm achieves     rank 
accuracy  which is also the best among all competing methods 

  Conclusion and Future Work
We have proposed an improved cotraining algorithm
which trains the classi ers under two views of data in  
serial way and alternatively updates the pseudo labels of
unlabeled data to improve the performance of classi ers in
each training round  We represent the algorithm with an
selfpaced cotraining  SPaCo  model  and the optimized
strategy for solving this model is consistent with the training process of an improved cotraining algorithm  Experimental results verify the advantage of SPaCo beyond current cotraining methods 
Research directions in our future work include designing
more selfpaced regularizers for SPaCo considering their
different capacities on noise data  Besides  since there are
many multiview other than two view data sets in practice 
we need to develop   more general SPaCo regimes to deal
with multiview tasks 

SelfPaced Cotraining

Acknowledgements
This research was supported by the China NSFC project
under Grant No 
     
  Macau Science and Technology Development
Funds under Grant No   AFJ from the Macau
Special Administrative Region of the People   Republic of
China  and the National Grand Fundamental Research  
Program of China under Grant No   CB 

References
Abney  Steven  Bootstrapping  In Proceedings of the  th
Annual Meeting on Association for Computational Linguistics  pp    Association for Computational
Linguistics   

Balcan  MariaFlorina  Blum  Avrim  and Yang  Ke  Cotraining and expansion  Towards bridging theory and
practice  In Advances in neural information processing
systems  pp     

Bengio  Yoshua  Louradour    er ome  Collobert  Ronan  and
In Proceedings
Weston  Jason  Curriculum learning 
of the  th annual international conference on machine
learning  pp    ACM   

Blum  Avrim and Mitchell  Tom  Combining labeled and
unlabeled data with cotraining  In Proceedings of the
eleventh annual conference on Computational learning
theory  pp    ACM   

Boyd  Stephen  Parikh  Neal  Chu  Eric  Peleato  Borja  and
Eckstein  Jonathan  Distributed optimization and statistical learning via the alternating direction method of multipliers  Foundations and Trends   cid  in Machine Learning     

Brefeld  Ulf and Scheffer  Tobias  Coem support vector
learning  In Proceedings of the twenty rst international
conference on Machine learning  pp    ACM   

Brefeld  Ulf and Scheffer  Tobias  Semisupervised learning for structured output variables  In Proceedings of the
 rd international conference on Machine learning  pp 
  ACM   

Goldman  Sally and Zhou  Yan  Enhancing supervised
In ICML  pp   

learning with unlabeled data 
 

Jiang  Lu  Meng  Deyu  Mitamura  Teruko  and Hauptmann  Alexander    Easy samples  rst  Selfpaced
reranking for zeroexample multimedia search  In Proceedings of the  nd ACM international conference on
Multimedia  pp    ACM     

Jiang  Lu  Meng  Deyu  Yu  ShoouI  Lan  Zhenzhong 
Shan  Shiguang  and Hauptmann  Alexander  Selfpaced
learning with diversity  In Advances in Neural Information Processing Systems  pp       

Jiang  Lu  Meng  Deyu  Zhao  Qian  Shan  Shiguang  and
Hauptmann  Alexander    Selfpaced curriculum learning  In AAAI  volume   pp     

Kumar    Pawan  Packer  Benjamin  and Koller  Daphne 
In AdSelf paced learning for latent variable models 
vances in Neural Information Processing Systems  pp 
   

Li  Ming and Zhou  ZhiHua  Improve computeraided diagnosis with machine learning techniques using undiagIEEE Transactions on Systems  Man 
nosed samples 
and CyberneticsPart    Systems and Humans   
   

Ma  Zilu  Liu  Shiqi  and Meng  Deyu  On convergence property of implicit selfpaced objective  CoRR 
abs    URL http arxiv org 
abs 

Meng  Deyu and Zhao  Qian  What objective does selfpaced learning indeed optimize  In arXiv 
 

Mitchell  Tom  Machine Learning  McGraw Hill   

Nigam  Kamal and Ghani  Rayid  Analyzing the effectiveness and applicability of cotraining  In Proceedings of
the ninth international conference on Information and
knowledge management  pp    ACM   

Pi  Te  Li  Xi  Zhang  Zhongfei  Meng  Deyu  Wu  Fei 
Xiao  Jun  and Zhuang  Yueting  Selfpaced boost learning for classi cation  In IJCAI   

Scudder     Probability of error of some adaptive patternrecognition machines  IEEE Transactions on Information Theory     

Sindhwani  Vikas and Rosenberg  David    An rkhs for
multiview learning and manifold coregularization  In
Proceedings of the  th international conference on Machine learning  pp    ACM   

Sindhwani  Vikas  Niyogi  Partha  and Belkin  Mikhail   
coregularization approach to semisupervised learning
In Proceedings of ICML workwith multiple views 
shop on learning with multiple views  pp    Citeseer   

Supancic  James   and Ramanan  Deva  Selfpaced learning for longterm tracking  In Proceedings of the IEEE
conference on computer vision and pattern recognition 
pp     

SelfPaced Cotraining

arti cial intelligence  volume   pp    Menlo Park 
CA  Cambridge  MA  London  AAAI Press  MIT Press 
   

Zhu  Xiaojin  Semisupervised learning  In Encyclopedia

of machine learning  pp    Springer   

Wang  Wei and Zhou  ZhiHua    new analysis of cotraining  In Proceedings of the  th international conference on machine learning  ICML  pp   
 

Wang  Wei and Zhou  ZhiHua  Cotraining with insuf 

cient views  In ACML  pp     

Xu  Qian  Hu  Derek Hao  Xue  Hong  Yu  Weichuan  and
Yang  Qiang  Semisupervised protein subcellular localization  BMC bioinformatics     

Ye  HanJia  Zhan  DeChuan  Miao  Yuan  Jiang  Yuan 
and Zhou  ZhiHua  Rank consistency based multiview
In Proceedlearning    privacypreserving approach 
ings of the  th ACM International on Conference on Information and Knowledge Management  pp   
ACM   

Yu     Jiang     Mao     Chang        Du        Gan 
   Lan        Xu        Li        Cai     Kumar 
   Miao     Martin     Wolfe     Xu        Li    
Lin     Ma        Yang     Meng        Shan       
Sahin        Burger     Metze     Singh     Raj    
Mitamura     Stern     and Hauptmann     CmuinformediaTRECVID   In TRECVID   

Zhang  Dingwen  Meng  Deyu  and Han  Junwei  Cosaliency detection via   selfpaced multipleinstance
IEEE Transactions on Pattern
learning framework 
Analysis and Machine Intelligence   

Zhang  MinLing and Zhou  ZhiHua  Cotrade  con dent
cotraining with data editing  IEEE Transactions on Systems  Man  and Cybernetics  Part    Cybernetics   
   

Zhao  Qian  Meng  Deyu  Jiang  Lu  Xie  Qi  Xu  Zongben 
and Hauptmann  Alexander    Selfpaced learning for
matrix factorization  In AAAI  pp     

Zheng     Yang     and Hauptmann        Person Reidenti cation  Past  Present and Future  ArXiv eprints 
October  

Zheng  Liang  Shen  Liyue  Tian  Lu  Wang  Shengjin 
Wang  Jingdong  and Tian  Qi  Scalable person reidenti cation    benchmark  In Proceedings of the IEEE
International Conference on Computer Vision  pp   
   

Zheng  Zhedong  Zheng  Liang  and Yang  Yi    discriminatively learned CNN embedding for person reidenti cation  CoRR  abs   

Zhou  ZhiHua  Zhan  DeChuan  and Yang  Qiang  Semisupervised learning with very few labeled training exIn Proceedings of the national conference on
amples 

