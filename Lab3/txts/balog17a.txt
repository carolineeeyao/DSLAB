Lost Relatives of the Gumbel Trick

Matej Balog     Nilesh Tripuraneni   Zoubin Ghahramani     Adrian Weller    

Abstract

The Gumbel trick is   method to sample from  
discrete probability distribution  or to estimate its
normalizing partition function  The method relies on repeatedly applying   random perturbation to the distribution in   particular way  each
time solving for the most likely con guration 
We derive an entire family of related methods 
of which the Gumbel trick is one member  and
show that the new methods have superior properties in several settings with minimal additional
computational cost  In particular  for the Gumbel trick to yield computational bene ts for discrete graphical models  Gumbel perturbations on
all con gurations are typically replaced with socalled lowrank perturbations  We show how  
subfamily of our new methods adapts to this setting  proving new upper and lower bounds on the
log partition function and deriving   family of sequential samplers for the Gibbs distribution  Finally  we balance the discussion by showing how
the simpler analytical form of the Gumbel trick
enables additional theoretical results 

  Introduction
In this work we are concerned with the fundamental problem of sampling from   discrete probability distribution and
evaluating its normalizing constant    probability distribution   on   discrete sample space   is provided in terms
of its potential function          corresponding to logunnormalized probabilities via              
where the normalizing constant   is the partition function 
In this context    is the Gibbs distribution on   associated
with the potential function   The challenges of sampling
from such   discrete probability distribution and estimating
the partition function are fundamental problems with ubiq 

 University of Cambridge  UK  MPIIS    ubingen  Germany
 UC Berkeley  USA  Uber AI Labs  USA  Alan Turing Institute 
UK  Correspondence to  Matej Balog  rst last gmail com 
Code  https github com matejbalog gumbelrelatives 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

uitous applications in machine learning  classical statistics
and statistical physics  see       Lauritzen   
Perturband MAP methods  Papandreou   Yuille   
constitute   class of randomized algorithms for estimating
partition functions and sampling from Gibbs distributions 
which operate by randomly perturbing the corresponding
potential functions and employing maximum   posteriori
 MAP  solvers on the perturbed models to  nd   maximum
probability con guration  This MAP problem is NPhard
in general  however  substantial research effort has led to
the development of solvers which can ef ciently compute
or estimate the MAP solution on many problems that occur
in practice       Boykov et al    Kolmogorov   
Darbon    Evaluating the partition function is   harder
problem  containing for instance  Phard counting problems  The general aim of perturband MAP methods is
to reduce the problem of partition function evaluation  or
the problem of sampling from the Gibbs distribution  to repeated instances of the MAP problem  where each instance
is on   different random perturbation of the original model 
The Gumbel trick  Papandreou   Yuille    relies on
adding Gumbeldistributed noise to each con guration  
potential     We derive   wider family of perturband 
MAP methods that can be seen as perturbing the model in
different ways   in particular using the Weibull and Fr echet
distributions alongside the Gumbel  We show that the new
methods can be implemented with essentially no additional
computational cost by simply averaging existing Gumbel
MAP perturbations in different spaces  and that they can
lead to more accurate estimators of the partition function 
Evaluating or perturbing each con guration   potential
with        Gumbel noise can be computationally expensive 
One way to mitigate this is to cleverly prune computation
in regions where the maximum perturbed potential is unlikely to be found  Maddison et al    Chen   Ghahramani    Another approach exploits the product structure of the sample space in discrete graphical models  replacing        Gumbel noise with    lowrank  approximation  Hazan   Jaakkola   Hazan et al    showed
that from such an approximation  upper and lower bounds
on the partition function and   sequential sampler for the
Gibbs distribution can still be recovered  We show that  
subfamily of our new methods  consisting of Fr echet  Exponential and Weibull tricks  can also be used with low 

Lost Relatives of the Gumbel Trick

rank perturbations  and use these tricks to derive new upper
and lower bounds on the partition function  and to construct
new sequential samplers for the Gibbs distribution 
Our main contributions are as follows 
    family of tricks that can be implemented by simply
averaging Gumbel perturbations in different spaces  and
which can lead to more accurate or more sample ef 
cient estimators of    Section  

  New upper and lower bounds on the partition function of
  discrete graphical model computable using lowrank
perturbations  and   corresponding family of sequential
samplers for the Gibbs distribution  Section  

  Discussion of advantages of the simpler analytical form
of the Gumbel trick including new links between the errors of estimating    sampling  and entropy estimation
using lowrank Gumbel perturbations  Section  

Background and Related work The idea of perturbing
the potential function of   discrete graphical model in order to sample from its associated Gibbs distribution was introduced by Papandreou   Yuille   inspired by their
previous work on reducing the sampling problem for Gaussian Markov random  elds to the problem of  nding the
mean  using independent local perturbations of each Gaussian factor  Papandreou   Yuille    Tarlow et al 
  extended this perturband MAP approach to sampling  in particular by considering more general structured
prediction problems  Hazan   Jaakkola   pointed out
that MAP perturbations are useful not only for sampling the
Gibbs distribution  considering the argmax of the perturbed
model  but also for bounding and approximating the partition function  by considering the value of the max 
Afterwards  Hazan et al    derived new lower bounds
on the partition function and proposed   new sampler for
the Gibbs distribution that samples variables of   discrete
graphical model sequentially  using expected values of lowrank MAP perturbations to construct the conditional probabilities  Due to the lowrank approximation  this algorithm
has the option to reject   sample  Orabona et al   
and Hazan et al    subsequently derived measure concentration results for the Gumbel distribution that can be
used to control the rejection probability  Maji et al   
derived an uncertainty measure from random MAP perturbations  using it within   Bayesian active learning framework for interactive image boundary annotation 
Perturband MAP was famously generalized to continuous
spaces by Maddison et al    replacing the Gumbel
distribution with   Gumbel process and calling the resulting
algorithm    sampling  Maddison   cast this work
into   uni ed framework together with adaptive rejection
sampling techniques  based on the notion of exponential
races  This recent view generally brings together perturb 

andMAP and acceptreject samplers  exploiting the connection between the Gumbel distribution and competing
exponential clocks that we also discuss in Section  
Inspired by    sampling  Kim et al    proposed an exact sampler for discrete graphical models based on lazilyinstantiated random perturbations  which uses linear programming relaxations to prune the optimization space  Further recent applications of perturband MAP include structured prediction in computer vision  Bertasius et al   
and turning the discrete sampling problem into an optimization task that can be cast as   multiarmed bandit problem  Chen   Ghahramani    see Section   below 
In addition to perturband MAP methods  we are aware of
three other approaches to estimate the partition function
of   discrete graphical model via MAP solver calls  The
WISH method  weightedintegrals andsums byhashing 
Ermon et al    relies on repeated MAP inference calls
applied to the model after subjecting it to random hash constraints  The FrankWolfe method may be applied by iteratively updating marginals using   constrained MAP solver
and line search  Belanger et al    Krishnan et al 
  Weller   Jebara     instead use just one MAP
call over   discretized mesh of marginals to approximate
the Bethe partition function  which itself is an estimate
 which often performs well  of the true partition function 

  Relatives of the Gumbel Trick
In this section  we review the Gumbel trick and state the
mechanism by which it can be generalized into an entire
family of tricks  We show how these tricks can equivalently
be viewed as averaging standard Gumbel perturbations in
different spaces  instantiate several examples  and compare
the various tricks  properties 

Notation Throughout this paper  let   be    nite sample
space of size         Let           be an unnormalized mass function over   and let    Px         be
its normalizing partition function  Write              
for the normalized version of     and       ln       for the
logunnormalized probabilities       the potential function 
We write Exp  for the exponential distribution with rate
 inverse mean    and Gumbel  for the Gumbel distribution with location   and scale   The latter has mean       
where       is the EulerMascheroni constant 
  The Gumbel Trick
Similarly to the connection between the Gumbel trick and
the Poisson process established by Maddison   we
introduce the Gumbel trick for discrete probability distributions using   simple and elegant construction via competing exponential clocks  Consider   independent clocks 

Table   New tricks for constructing unbiased estimators of different transformations       of the partition function 

Lost Relatives of the Gumbel Trick

Trick

Gumbel
Exponential
Weibull   
Fr echet   
Pareto
Tail  

Mean      

    
  ln      
ln  
 
 
 
      
     
                
   for      
ex
  tZ
 

 

     

started simultaneously  such that the jth clock rings after
  random time Tj   Exp    Then it is easy to show that
  the time until some clock rings has Exp PN
       distribution  and   the probability of the jth clock ringing
 rst is proportional to its rate     These properties are also
widely used in survival analysis  Cox   Oakes   
Consider   competing exponential clocks  Tx       indexed by elements of     with respective rates           
Property   of competing exponential clocks tells us that

min

    Tx  Exp   

 

Property   says that the random variable argminx Tx  taking values in     is distributed according to   

argmin

     Tx    

 

The Gumbel trick is obtained by applying the function
         ln       to the equalities in distribution  
and   When   is applied to an Exp  random variable  the result follows the Gumbel     ln   distribution  which can also be represented as ln       where
       Gumbel   
    Gumbel    De ning       
and noting that   is strictly decreasing  applying the function   to equalities in distribution   and   we obtain 

max

              Gumbel     ln   
                

argmax

 

 

where we have recalled that       ln      ln       The
distribution Gumbel     ln    has mean ln    and thus
the log partition function can be estimated by averaging
samples  Hazan   Jaakkola   

  Constructing New Tricks
Given the equality in distribution   we can treat the problem of estimating the partition function   as   parameter
estimation problem for the exponential distribution  Applying the function          ln       as in the Gumbel
trick to obtain   Gumbel     ln    random variable  and

Variance of      
 
 
 
  
 
 
      for      
 
  tZ      tZ 

  
 

  

for      

 

Asymptotic var  of   
 
     
   
 

   
   

         
         

  

 

   
   tZ  
  

estimating its mean to obtain an unbiased estimator of ln   
is just one way of inferring information about   
We consider applying different functions   to   particularly those functions   that transform the exponential
distribution to another distribution with known mean  As
the original exponential distribution has rate    the transformed distribution will have mean       where   will in
general no longer be the logarithm function  Since we often
are interested in estimating various transformations      
of    this provides us   with   collection of unbiased estimators from which to choose  Moreover  further transforming these estimators yields   collection of  biased  estimators for other transformations of    including   itself 
Example    Weibull tricks  For any     applying
the function           to an Exp  random variable
yields   random variable with the Weibull    distribution with scale   and shape   which has mean
      and can be also represented as     
      
where     Weibull    De ning          
Weibull    and noting that   is increasing  applying
  to the equality in distribution   gives

min

            Weibull     

 

Estimating the mean of Weibull      yields an unbiased estimator of        The special case      
corresponds to the identity function           we call the
resulting trick the Exponential trick 

Table   lists several examples of tricks derived this way 
As Example   shows  these tricks may not involve additive perturbation of the potential function     the Weibull
tricks multiplicatively perturb exponentiated unnormalized
probabilities     with Weibull noise  As models of interest are often speci ed in terms of potential functions  to be
able to reuse existing MAP solvers in   blackbox manner
with the new tricks  we seek an equivalent formulation in
terms of the potential function  The following Proposition
shows that by not passing the function   through the minimization in equation   the new tricks can be equivalently
formulated as averaging additive Gumbel perturbations of
the potential function in different spaces 

Lost Relatives of the Gumbel Trick

Figure   Analytically computed MSE and variance of Gumbel
and Exponential trick estimators of    left  and ln    right  The
MSEs are dominated by the variance  so the dashed and solid lines
mostly overlap  See Section   for details 

Figure   MSE of estimators of    left  and ln    right  stemming from Fr echet    
      Gumbel       and Weibull
tricks     See Section   for details 

Proposition   For any function           such that
        ET Exp         exists  we have
               
                exp  max
       Gumbel   
where       
Proof  As maxx          Gumbel     ln   
we have     exp maxx            Exp    and the
result follows by the assumption relating   and   

Proposition   shows that the new tricks can be implemented
by solving the same MAP problems maxx      as
in the Gumbel trick  and then merely passing the solutions
through the function           exp    before averaging
them to approximate the expectation 

  Comparing Tricks
  ASYMPTOTIC EFFICIENCY
The Delta method  Casella   Berger    is   simple
technique for assessing the asymptotic variance of estimators that are obtained by   differentiable transformation of an estimator with known variance  The last column in Table   lists asymptotic variances of corresponding tricks when unbiased estimators of       are passed
through the function    to yield  biased  but consistent
and nonnegative  estimators of   itself  It is interesting
to examine the constants that multiply    in some of the
obtained asymptotic variance expressions for the different
tricks  For example  it can be shown using Gurland   ratio  Gurland    that this constant is at least   for the
Weibull and Fr echet tricks  which is precisely the value
achieved by the Exponential trick  which corresponds to
      Moreover  the Gumbel trick constant   can be
shown to be the limit as       of the Weibull and Fr echet
trick constants  In particular  the constant of the Exponential trick is strictly better than that of the standard Gumbel
trick            This motivates us to compare the
Gumbel and Exponential tricks in more detail 

MPM

  MEAN SQUARED ERROR  MSE 
For estimators     their MSE       var       bias     is
  commonly used comparison metric  When the Gumbel or
Exponential tricks are used to estimate either   or ln    the
biases  variances  and MSEs of the estimators can be computed analytically using standard methods  Appendix   
For example  the unbiased estimator of ln   from the Gumbel trick can be turned into   consistent nonnegative estimator of   by exponentiation      exp   
   Xm 
       Gumbel     ln    are obtained
where            XM
using equation   The bias and variance of   can be
computed using independence and the moment generating
functions of the Xm    see Appendix   for details 
Perhaps surprisingly  all estimator properties only depend
on the true value of   and not on the structure of the model
 distribution    since the estimators rely only on        samples of   Gumbel     ln    random variable  Figure  
shows the analytically computed estimator variances and
MSEs  For estimating   itself  left  the Exponential trick
outperforms the Gumbel trick in terms of MSE for all sample sizes        for         both estimators have
in nite variance and MSE  The ratio of MSEs quickly approaches   and in this regime the Exponential trick requires           fewer samples than the Gumbel
trick to reach the same MSE  Also  for estimating ln   
 Figure   right  the Exponential trick provides   lower
MSE estimator for sample sizes       only for      
the Gumbel trick provides   better estimator 
Note that as biases are available analytically  the estimators can be easily debiased  by subtracting their bias  One
then obtain estimators with MSEs equal to the variances of
the original estimators  shown dashed in Figure   The Exponential trick would then always outperform the Gumbel
trick when estimating ln    even with sample size      
For Weibull tricks with       and Fr echet tricks  we estimated the biases and variances of estimators of   and ln  
by constructing         estimators in each case and
evaluating their bias and variance  Figure   shows the results for varying   and several sample sizes    We plot the

Lost Relatives of the Gumbel Trick

analytically computed value for the Gumbel trick at      
as we observe that the Weibull trick interpolates between
the Gumbel trick and the Exponential trick as   increases
from   to   We note that the minimum MSE estimator is
obtained by choosing   value of   that is close to       
the Exponential trick  This agrees with the  nding from
Section   that       is optimal as      
  Bayesian Perspective
  Bayesian approach exposes two choices when constructing estimators of    or of its transformations      

Unary perturbations provide the upper bound ln          
on the log partition function  Hazan   Jaakkola    can
be used to construct   sequential sampler for the Gibbs distribution  Hazan et al    and  if the perturbations are
scaled down by   factor of      lower bound on ln   can
also be recovered  Hazan et al    In this section we
show that   subfamily of tricks introduced in Section  
consisting of Fr echet and Weibull  and Exponential  tricks 
is applicable in the lowrank perturbation setting and use
them to derive new families of upper and lower bounds
on ln   and sequential samplers for the Gibbs distribution 
Please note full proofs are deferred to Appendix   and   

    choice of prior distribution      encoding prior
beliefs about the value of   before any observations 
    choice of how to summarize the posterior distribu 

tion pM               XM   given   samples 

Taking the Jeffrey   prior           an improper prior
that it is invariant under reparametrization  observing  
samples            XM

       Exp    yields the posterior 
   Xm 

dom variable  the posterior mean is

pM               XM     ZM   ZPM
Recognizing the density of   Gamma   PM
    
MXm 
               XM    
coinciding with the Exponential trick estimator of   

PM

   Xm

 

 

Xm 

 

   Xm  ran 

  Lowrank Perturbations
One way of exploiting perturband MAP to yield computational savings is to replace independent perturbations
of each con guration   potential with an approximation 
Such approximations are available      in discrete graphical
models  where the sampling space   has   product space
structure              with Xi the state space of
the ith variable 
De nition      Hazan   Jaakkola    The sumunary
perturbation MAP value is the random variable

    max

  Xn     

nXi 

   xi   

where    xi    xi                         Gumbel   
This de nition involves    Xn         Gumbel random variables  rather than      With       this coincides
with fullrank perturbations and     Gumbel   ln   
For       the distribution of   is not available analytically 
One can similarly de ne the pairwise  or higherorder  perturbations  where independent Gumbel noise is added to
each pairwise  or higherorder  potential 

  Upper Bounds on the Partition Function
The following family of upper bounds on ln   can be derived from the Fr echet and Weibull tricks 
Proposition   For any             the upper
bound ln        holds with
ln         

Proof   Sketch  By induction on    with the induction step
provided by our Clamping Lemma  Lemma   below 

      

  nc  

ln      

 
 

 

To evaluate these bounds in practice          is estimated
using samples of    Corollary   of Hazan et al    can
 pn 
be used to show that var       is  nite for      
and so then the estimation is wellbehaved 
  natural question is how these new bounds relate to the
Gumbel trick upper bound ln           by Hazan  
Jaakkola   The following result aims to answers this 
Proposition   The limit of    as       exists and
equals                 the Gumbel trick upper bound 
The question remains  When is it advantageous to use  
value       to obtain   tighter bound on ln   than the
Gumbel trick bound  The next result can provide guidance 
Proposition   The function    is differentiable at    
  and the derivative equals

 

    

 

 

  

 

    var      

While the variance of   is generally not tractable  in practice one obtains samples from   to estimate the expectation
in    and these samples can be reused to assess var    
Interestingly  var     equals    for both the uniform
distribution and the distribution concentrated on   single
con guration  and in our empirical investigations always
var          Then the derivative at   is nonnegative
and Fr echet tricks provide tighter bounds on ln    However  as    is estimated with samples  the question of

Lost Relatives of the Gumbel Trick

estimator variance arises  We investigate the tradeoff between tightness of the bound ln        and the variance
incurred in estimating    empirically in Section  
  Clamping
Consider the partial sumunary perturbation MAP values 
where the values of the  rst    variables have been  xed 
and only the rest are perturbed 

xj  xn 

     

nXi  

 

   xi 

Uj            xj    max

The following lemma involving the Uj   serves three purposes      it provides the induction step for Proposition  
 II  it shows that clamping never hurts partition function
estimation with Fr echet and Weibull tricks  and  III  it will
be used to show that   sequential sampler constructed in
Section   below is wellde ned 
Lemma    Clamping Lemma  For any                 
and             xj             the following inequality holds with any            
  he      ln           Uj   
Xxj Xj
    he      ln          Uji 
Proof  This follows directly from the Fr echet trick    
    or the Weibull trick     and representing the
Fr echet resp  Weibull random variables in terms of Gumbel
random variables  See Appendix    for more details 

Corollary   Clamping never hurts ln   estimation using
any of the Fr echet or Weibull upper bounds   
Proof  Applying the function     ln    to both sides of
the Clamping Lemma   with       the righthand side
equals    while the lefthand side is the estimate of ln  
after clamping variable   

This was shown previously in restricted settings  Hazan
et al    Zhao et al    Similar results showing
that clamping improves partition function estimation have
been obtained for the mean  eld and TRW approximations  Weller   Domke    and in certain settings for
the Bethe approximation  Weller   Jebara      and LFIELD  Zhao et al   

  Sequential Sampling
Hazan et al    derived   sequential sampling procedure for the Gibbs distribution by exploiting the    GumIn the same spirit  one
bel trick upper bound on ln   

can derive sequential sampling procedures from the Fr echet
and Weibull tricks  leading to the following algorithm 

Algorithm   Sequential sampler for Gibbs distribution
Input              potential function   on  
Output    sample   from the Gibbs distribution       
  for       to   do
for xj      do
 
pj xj       
pj reject       Pxj Xj
xj   sample according to pj
if xj   reject then
RESTART  goto  

    Uj   xj  
    Uj    xj 

 
 
 
 

pj xj 

 

 

This algorithm is wellde ned if pj reject      for all   
which can be shown by canceling terms in the Clamping
Lemma   We discuss correctness in Appendix    As for
the Gumbel sequential sampler of Hazan et al    the
expected number of restarts  and hence the running time 
only depend on the quality of the upper bound      
ln    and not on the ordering of variables 

  Lower Bounds on the Partition Function
Similarly as in the Gumbel trick case  Hazan et al   
one can derive lower bounds on ln   by perturbing an arbitrary subset   of variables 
Proposition   Let             be   product space
and     potential function on     Let          
For any subset                  of the variables            xn
we have ln    
ln Ehe  maxx      xS     
where xS    xi          and    xS    Gumbel   
independently for each setting of xS 

ln      

 
 

   

 

 

By averaging   such lower bounds corresponding to singleton sets         together  we obtain   lower bound on ln  
that involves the averageunary perturbation MAP value

 
 

    max

nXi 

   xi   

        
Corollary   For any             we have the
lower bound ln        where
 
 
  

ln    exp        

        

ln      

 

Again            can be de ned by continuity  where
       ln   is the Gumbel trick lower bound by Hazan
et al   

Lost Relatives of the Gumbel Trick

  Advantages of the Gumbel Trick
We have seen how the Gumbel trick can be embedded into
  continuous family of tricks  consisting of Fr echet  Exponential  and Weibull tricks  We showed that the new tricks
can provide more ef cient estimators of the partition function in the fullrank perturbation setting  Section   and
in the lowrank perturbation setting lead to sequential samplers and new bounds on ln    which can be also more ef 
 cient  as we investigate in Section   To balance the
discussion of merits of different tricks  in this section we
brie   highlight advantages of the Gumbel trick that stem
from its simpler analytical form 
First  by consulting Table   we see that the function       
  ln      has the property that the variance of the resulting
estimator  of ln    does not depend on the value of    the
function   is   variance stabilizing transformation for the
Exponential distribution 
Second  exploiting the fact that the logarithm function leads
to additive perturbations  Maji et al    showed that the
entropy of    the con guration with maximum potential
after sumunary perturbation in the sense of De nition  
               
We extend this result to show how the errors of bounding
ln    sampling  and entropy estimation are related 
Proposition   Writing   for the Gibbs distribution and
                    for the entropy bound  we have
             
 
 

 
Third  the additive character of the Gumbel perturbations
can also be used to derive   new result relating the error of
the lower bound    and of sampling    as the con guration achieving the maximum averageunary perturbation
value    instead of sampling from the Gibbs distribution   
Proposition   Writing   for the Gibbs distribution 

can be bounded as              Pn

  KL        
 
 

sampling error

      ln   

error in entropy estimation

error in ln   bound

  

  

  

 

 

ln       

error in ln   bound

 

  

  KL        
 
 

sampling error

  

 

   

Remark  While we knew from Hazan et al    that
ln            this is   stronger result showing that
the size of the gap is an upper bound on the KL divergence
between the approximate sampling distribution of    and
the Gibbs distribution   

Proofs of the new results appear in Appendix    and   
Fourth  viewed as   function of the Gumbel perturbations
  the random variable   has   bounded gradient  allowing
earlier measure concentration results  Orabona et al   
Hazan et al    Proving similar measure concentration
results for the expectations         appearing in    for
      may be more challenging 

  Experiments
We conducted experiments with the following aims 

  To show that the higher ef ciency of the Exponential
trick in the fullrank perturbation setting is useful in
practice  we compared it to the Gumbel trick in   
sampling  Maddison et al     Section   and in
the largescale discrete sampling setting of Chen  
Ghahramani    Section  

  To show that nonzero values of   can lead to better estimators of ln   in the lowrank perturbation setting as well  we compare the Fr echet and Weibull trick
bounds    to the Gumbel trick bound    on  
common discrete graphical model with different coupling strengths  see Section  

     Sampling
   sampling  Maddison et al    is   sampling algorithm for continuous distributions that perturbs the logunnormalized density   with   continuous generalization
of the Gumbel trick  called the Gumbel process  and uses
  variant of    search to  nd the location of the maximum of the perturbed   Returning the location yields an
exact sample from the original distribution  as in the discrete Gumbel trick  Moreover  the corresponding maximum value also has the Gumbel     ln    distribution
 Maddison et al    Our analysis in Section   tells
us that the Exponential trick yields an estimator with lower
MSE than the Gumbel trick  we brie   veri ed this on
the Robust Bayesian Regression experiment of Maddison
et al    We constructed estimators of ln   from the
Gumbel and Exponential tricks  debiased version  see Section   and assessed their variances by constructing
each estimator       times and looking at the sample variance  Figure    shows that the Exponential trick
requires up to   fewer samples to reach   given MSE 

  Scalable Partition Function Estimation
Chen   Ghahramani   considered sampling from  
discrete distribution of the form            QS
   fs   
when the number of factors   is large relative to the sample space size     Computing        Gumbel perturbations
    for each      is then relatively cheap compared to
evaluating all potentials             PS
   ln fs   
Chen   Ghahramani   observed that each  perturbed 
potential can be estimated by subsampling the factors  and
potentials that appear unlikely to yield the MAP value can
be pruned off from the search early on  The authors formalized the problem as   Multiarmed bandit problem with
   nite reward population and derived approximate algorithms for ef ciently  nding the maximum perturbed potential with   probabilistic guarantee 

Lost Relatives of the Gumbel Trick

   

   

Figure       Sample size   required to reach   given MSE using
Gumbel and Exponential trick estimators of ln    using samples
from    sampling  see Section   on   Robust Bayesian Regression task  The Exponential trick is more ef cient  requiring
up to   fewer samples to reach   given MSE      MSE of ln  
estimators for different values of   using       samples
from the approximate MAP algorithm discussed in Section  
with different error bounds   For small   the Exponential trick
is close to optimal  matching the analysis of Section   For
larger   the Weibull trick interpolation between the Gumbel and
Exponential tricks can provide an estimator with lower MSE 

While Chen   Ghahramani   considered sampling 
by modifying their procedure to return the value of the
maximum perturbed potential rather than the argmax  cf
equations   and   we can estimate the partition function instead  However  the approximate algorithm only
guarantees to  nd the MAP con guration with   probability       Figure    shows the results of running the
RacingNormal algorithm of Chen   Ghahramani  
on the synthetic dataset considered by the authors with the
 very hard  noise setting       For low error bounds  
the Exponential trick remained close to optimal  but for  
larger error bound the Weibull trick interpolation between
the Gumbel and Exponential tricks proved useful to provide
an estimator with lower MSE 

  Lowrank Perturbation Bounds on ln  
Hazan   Jaakkola   evaluated tightness of the Gumbel trick upper bound      ln   on       binary spin
glass models  We show one can obtain more accurate estimates of ln   on such models by choosing       To
account for the fact that in practice an expectation in   
is replaced with   sample average  we treat    as an estimator of ln   with asymptotic bias equal to the bound gap
      ln    and estimate its MSE 
Figure   shows the MSEs of    as estimators of ln   on
             binary pairwise grid models with unary
potentials sampled uniformly from     and pairwise potentials from       attractive models  or from       
 mixed models  for varying coupling strengths    We replaced the expectations in      with sample averages of
size       using libDAI  Mooij    to solve the
MAP problems yielding these samples  We constructed
each estimator   times to assess its variance 

Figure   MSEs of    as estimators of ln   on       attractive  left  middle  and mixed  right  spin glass model with different coupling strengths    see Section   We also show the
percentage of samples saved by using the best   in place of the
Gumbel trick estimator    assuming the asymptotic regime 
For this we only considered    pn      where
variance is provably  nite  see Section   The MAP problems
were solved using the exact junction tree algorithm  JCT  left and
right  or approximate belief propagation  BP  middle 
In all
cases  when coupling is very low    close to   is optimal  This
also holds for BP when coupling is high  In other regimes  upper
bounds for the Fr echet trick           provide more accurate
estimators 

  Discussion
By casting partition function evaluation as   parameter estimation problem for the exponential distribution  we derived
  family of methods of which the Gumbel trick is   special
case  These methods can be equivalently seen as   perturbing models using different distributions  or as   averaging standard Gumbel perturbations in different spaces 
allowing implementations with little additional cost 
We showed that in the fullrank perturbation setting  the
new Exponential trick provides an estimator with lower
MSE  or instead allows using up to   fewer samples than
the Gumbel trick estimator to reach the same MSE 
In the lowrank perturbation setting  we used our Fr echet 
Exponential and Weibull tricks to derive new bounds on
ln   and sequential samplers for the Gibbs distribution  and
showed that these can also behave better than the corresponding Gumbel trick results  However  the optimal trick
to use  as speci ed by   depends on the model  sample
size  and MAP solver used  if approximate  Since in practice the dominant computational cost is carried by solving
repeated instances of the MAP problem  one can try and assess different values of   on the problem at hand  That said 
we believe that investigating when different tricks yield
better results is an interesting avenue for future work 
Finally  we balanced the discussion by pointing out that the
Gumbel trick has   simpler analytical form which can be
exploited to derive more interesting theoretical statements
in the lowrank perturbation setting  Beyond existing results  we derived new connections between errors of different procedures using lowrank Gumbel perturbations 

Lost Relatives of the Gumbel Trick

Maddison       Poisson process model for Monte Carlo 

In
Hazan     Papandreou     and Tarlow      eds  Perturbation 
Optimization  and Statistics  MIT Press   

Maddison     Tarlow     and Minka        sampling  In NIPS 

 

Maji     Hazan     and Jaakkola     Active boundary annotation

using random MAP perturbations  In AISTATS   

Mooij    

libDAI    free and open source    library for discrete approximate inference in graphical models  Journal of
Machine Learning Research     

Orabona     Hazan     Sarwate     and Jaakkola     On measure
concentration of random maximum aposteriori perturbations 
In ICML   

Papandreou     and Yuille     Gaussian sampling by local pertur 

bations  In NIPS   

Papandreou     and Yuille     Perturband MAP random  elds 
Using discrete optimization to learn and sample from energy
models  In Proc  IEEE Int  Conf  on Computer Vision  ICCV 
pp    November  

Tarlow     Adams     and Zemel     Randomized optimum

models for structured prediction  In AISTATS   

Wainwright     and Jordan     Graphical Models  Exponential Families  and Variational Inference  Found  Trends Mach 
Learn    January  

Weller     and Domke     Clamping improves TRW and mean

 eld approximations  In AISTATS   

Weller     and Jebara     Approximating the Bethe partition func 

tion  In UAI     

Weller     and Jebara     Clamping variables and approximate

inference  In NIPS     

Wright     and Nocedal     Numerical optimization  Springer

Science     

Zhao     Djolonga     Tschiatschek     and Krause     Variable
clamping for optimizationbased inference  In NIPS Workshop
on Advances in Approximate Bayesian Inference  December
 

Acknowledgements
The authors thank Tamir Hazan for helpful discussions 
and Mark Rowland  Maria Lomeli  and the anonymous
reviewers for helpful comments  AW acknowledges support by the Alan Turing Institute under EPSRC grant
EP    and by the Leverhulme Trust via the CFI 

References
Belanger     Sheldon     and McCallum     Marginal inference
In NIPS Workshop on Greedy

in MRFs using FrankWolfe 
Optimization  FrankWolfe and Friends   

Bertasius     Liu     Torresani     and Shi     Local Perturb 

andMAP for Structured Prediction  In AISTATS   

Boykov     Veksler     and Zabih     Fast approximate energy
IEEE Transactions on pattern

minimization via graph cuts 
analysis and machine intelligence     

Casella     and Berger    

Statistical inference  volume  

Duxbury Paci   Grove  CA   

Chen     and Ghahramani     Scalable discrete sampling as  

multiarmed bandit problem  In ICML   

Cox     and Oakes     Analysis of survival data  volume   CRC

Press   

Darbon     Global optimization for  rst order Markov random
 elds with submodular priors  Discrete Applied Mathematics 
       

Ermon     Sabharwal     and Selman     Taming the curse of
dimensionality  Discrete integration by hashing and optimization  In ICML   

Gurland     An inequality satis ed by the Gamma function  Scan 

dinavian Actuarial Journal     

Hazan     and Jaakkola     On the partition function and random

maximum aposteriori perturbations  In ICML   

Hazan     Maji     and Jaakkola     On sampling from the Gibbs
distribution with random maximum aposteriori perturbations 
In NIPS   

Hazan     Orabona     Sarwate     Maji     and Jaakkola 
   High dimensional inference with random maximum aposteriori perturbations  CoRR  abs   

Kim     Sabharwal     and Ermon     Exact sampling with integer linear programs and random perturbations  In AAAI  pp 
   

Kolmogorov     Convergent treereweighted message passing for
IEEE transactions on pattern analysis

energy minimization 
and machine intelligence     

Krishnan  Rahul    LacosteJulien  Simon  and Sontag  David 

Barrier FrankWolfe for Marginal Inference  In NIPS   

Lauritzen     Graphical models  Oxford statistical science series 

Clarendon Press  Oxford    Autre tirage    

