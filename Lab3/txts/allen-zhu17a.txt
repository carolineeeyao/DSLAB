Natasha  Faster NonConvex Stochastic Optimization

via Strongly NonConvex Parameter

Zeyuan AllenZhu  

Abstract

Given   nonconvex function       that is an average of   smooth functions  we design stochastic  rstorder methods to  nd its approximate
stationary points  The performance of our new
methods depend on the smallest  negative  eigenvalue   of the Hessian  This parameter  
captures how strongly nonconvex       is  and
is analogous to the strong convexity parameter
for convex optimization  At least in theory  our
methods outperform known results for   range of
parameter   and can also be used to  nd approximate local minima  Our result implies an interesting dichotomy  there exists   threshold   so
that the  currently  fastest methods for      
and for       have different behaviors  the former scales with    and the latter scales with
  

  Introduction
We study the problem of composite nonconvex minimization 

 cid 

 cid 

  cid 

  

 
 

fi   

                           

min
  Rd
 
where each fi    is nonconvex but smooth  and   is
proper convex  possibly nonsmooth  but relatively simple 
We are interested in  nding   point   that is an approximate
local minimum of      
  The  nitesum structure          

   fi    arises
prominently in largescale machine learning tasks  In
particular  when minimizing loss over   training set 
each example   corresponds to one loss function fi 
in the summation  This  nitesum structure allows one
to perform stochastic gradient descent with respect to  

 cid  

 

Future version of this paper shall be found at http 
arxiv org abs   Microsoft Research  Correspondence to  Zeyuan AllenZhu  zeyuan csail mit edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

random  fi   

  The socalled proximal term     adds more generality to the model  For instance  if     is the indicator
function of   convex set  then problem   becomes
constraint minimization  if        cid   cid  then we can
allow problem   to perform feature selection 
In
general      has to be   simple function where the
 cid       cid 
projection operation arg minx       
is ef ciently computable  At    rst reading of this paper  one can assume         for simplicity 

Many nonconvex machine learning problems fall into
problem   Most notably  training deep neural networks
and classi cations with sigmoid loss correspond to  
where neither fi    or       is convex  However  our understanding to this challenging nonconvex problem is very
limited 
  Strongly NonConvex Optimization
Let   be the smoothness parameter for each fi    meaning
all the eigenvalues of  fi    lie in       
We denote by          the strongnonconvexity parameter
of          
 

   fi    meaning that

 cid  

all the eigenvalues of        lie in     

We emphasize that parameter   is analogous to the strongconvexity parameter   for convex optimization  where all
the eigenvalues of        lie in      for some      
We wish to  nd an  approximate stationary point        
critical point  of       that is

  point   satisfying  cid     cid     

where      is the socalled gradient mapping of        see
Section   for   formal de nition 
In the special case of
      gradient mapping      is the same as gradient
       so   satis es  cid      cid     
Since     is  strongly nonconvex  any  approximate stationary point is automatically also an    approximate
local minimum   meaning that the Hessian of the output
point         cid     is approximately positive semide 
nite  PSD 

 This de nition also applies to functions       that are not

twice differentiable  see Section   for details 

Natasha  Faster NonConvex Stochastic Optimization Via Strongly NonConvex Parameter

  Motivations and Remarks
  We focus on strongly nonconvex optimization because
introducing this parameter   allows us to perform  
more re ned study of nonconvex optimization 
If  
equals   then Lstrongly nonconvex optimization is
equivalent to the general nonconvex optimization 

  We focus only on  nding stationary points as opposed to local minima  because in   recent study  
see Appendix    researchers have shown that  nding
   approximate local minima reduces to  nding  
approximate stationary points in an   strongly nonconvex function 

  Parameter   is often not constant and can be much
smaller than    For instance  secondorder methods of 
 approximate local minima  Nesterov 
ten  nd  
  and this corresponds to    

 

 

 

Figure   Comparison to prior works

function  Under mild assumption       this approach
   nds an  approximate stationary point in gradient

complexity  cid   cid      

 cid 

 

  

 

  Known Results
Despite the widespread use of nonconvex models in machine learning and related  elds  our understanding to nonconvex optimization is still very limited  Until recently 
nearly all research papers have been mostly focusing on either       or       
  If      

the accelerated SVRG method  ShalevShwartz    AllenZhu   Yuan     nds   satisfying                   in gradient complexity

 cid   cid       cid   cid  This result is irrelevant to this

paper because       is simply convex 

  If        the SVRG method  AllenZhu   Hazan 
   nds an  approximate stationary point of      
in gradient complexity           

  If        gradient descent  nds an  approximate sta 

tionary point in gradient complexity   nL 

  If        stochastic gradient descent  nds an  approx 

stationary point in gradient complexity     

Throughout this paper  we refer to gradient complexity
as the total number of stochastic gradient computations
 fi    and proximal computations     Prox     
arg miny       
 cid       cid 
Very recently 
it was observed by two independent
groups  Agarwal et al    Carmon et al     
although implicitly  see Section   that for solving the
 strongly nonconvex problem  one can repeatedly regularize       to make it  strongly convex  and then apply
the accelerated SVRG method to minimize this regularized

 We use  cid   to hide polylogarithmic factors in        

 Some authors also refer to them as incremental  rstorder oracle  IFO  and proximal oracle  PO  calls  In most machine learning applications  each IFO and PO call can be implemented to run
in time      where   is the dimension of the model  or even in
time      if   is the average sparsity of the data vectors 

We call this method repeatSVRG in this paper  Unfortunately  repeatSVRG is even slower than the vanilla SVRG
for       by   factor    see Figure  
Remark on SGD  Stochastic gradient descent  SGD  has
  slower convergence rate       in terms of   than other
cited  rstorder methods       in terms of   see for
instance  Ghadimi   Lan    However  the complexity
of SGD does not depend on   and thus is incomparable to
gradient descent  SVRG  or repeatSVRG  This is one of
the main motivations to study how to reduce the complexity
of nonSGD methods  especially in terms of   
  Our New Results
In this paper  we identify an interesting dichotomy with respect to the spectrum of the nonconvexity parameter    
     In particular  we showed that if       
   then
our new method Natasha  nds an  approximate stationary point of       in gradient complexity
    

 cid 

 cid 

 

 

  log

 

 
 

 

 

In other words  together with repeatSVRG  we have improved the gradient complexity for  stringly nonconvex
optimization to 

  

    

 

 
and the  rst term in the min is smaller if       
the second term is smaller if       

  and
   We illustrate our

 
 

 

 cid 

 cid  

min

 cid    

 

 cid cid 

 In practice  there are examples in nonconvex empirical risk
minimization  AllenZhu   Hazan    and in training neural
networks  AllenZhu   Hazan    Reddi et al    where
SVRG can outperform SGD  Of course  for deep learning tasks 
SGD remains to be the best practical method of choice 

 We remark here that this is under mild assumptions for   being suf ciently small  For instance  the result of  Agarwal et al 
  Carmon et al    requires       In our result  the
term   log  

  disappears when         

complexity  logscale repeatSVRGNatashaSVRG gradient descent Natasha  Faster NonConvex Stochastic Optimization Via Strongly NonConvex Parameter

      

performance improvement in Figure   Our result matches
that of SVRG for        and has   much simpler analysis 
Additional Results  One can take   step further and ask
what if each function fi    is  cid   cid smooth for parameters  cid   cid      meaning that all the eigenvalues of  fi   
lie in  cid   cid 
We show that   variant of our method  which we call
Natashafull  solves this more re ned problem of   with

total gradient complexity   cid   log  

 cid  as

      cid cid 

long as  cid cid 
Remark   In applications   cid  and  cid  can be of very different magnitudes  The most in uential example is  nding
the leading eigenvector of   symmetric matrix  Using the
socalled shiftand invert reduction  Garber et al   
computing the leading eigenvector reduces to the convex version of problem   where each fi    is    
smooth for    cid    Other examples include all the applications that are built on shiftand invert  including high
rank SVD PCA  AllenZhu   Li    canonical component analysis  AllenZhu   Li      online matrix learning  AllenZhu   Li      and approximate local minima algorithms  Agarwal et al    Carmon et al   

 

MiniBatch  Our result generalizes trivially to the minibatch stochastic setting  where in each iteration one computes  fi    for   random choices of index         and average them  The stated gradient complexities of Natasha
and Natashafull can be adjusted so that the factor    is
replaced with     

  Our Techniques
Let us  rst recall the main idea behind stochastic variancereduced methods  such as SVRG  Johnson   Zhang   
The SVRG method divides iterations into epochs  each of

length    It maintains   snapshot point cid   for each epoch 
and computes the full gradient     cid    only for snapshots 
estimator cid     fi xt fi cid        cid    which satis es
Ei cid        xt  and performs proximal update xt   
 cid xt    cid cid  for some learning rate    Recall that
if       then we would have xt    xt    cid 

Then  in each iteration   at point xt  SVRG de nes gradient

Prox 

In nearly all
the aforementioned results for nonconvex optimization  researchers have either directly applied
SVRG  AllenZhu   Hazan     for the case       
or repeatedly applied SVRG  Agarwal et al    Carmon
et al     for general          This puts some limitation in the algorithmic design  because SVRG requires
each epoch to be of length exactly   

 The epoch length of SVRG is always    or   constant multiple of   in practice  because this ensures the computation of

 cid  is of amortized gradient complexity    The periteration

complexity of SVRG is thus the same as the traditional stochastic

  zt    

  zt    

      Then  we

theory suggests the choice        

equivalent to replacing       with its regularized version

same way as SVRG  However  we do not apply compute

  In our base algorithm Natasha  we divide each epoch

epochs  We provide pseudocode in Algorithm   and illustrate it in Figure  

Our New Idea 
In this paper  we propose Natasha and
Natashafull  two methods that are no longer blackbox reductions to SVRG  Both of them still divide iterations into

epochs of length    and compute gradient estimators  cid  the
xt    cid  directly 
into   subepochs  each with   starting vector cid    Our
replace the use of  cid  with  cid     xt  cid    This is
         cid   cid   cid  where the center cid   varies across subWe view this additional term  xt  cid    as   type of
the vector   bit in the backward direction towards cid   
dates zt    Prox zt    cid  with respect to   dif 
 cid  
and compute gradient estimators  cid  at points xt  We
 cid   as another type
ing towards cid    The technique of computing gradients at

  In our full algorithm Natashafull  we add one more ingredient on top of Natasha  That is  we perform upferent sequence  zt  and then de ne xt    

provide pseudocode in Algorithm   in the appendix 
We view this averaging xt    
of retraction  which stabilizes the algorithm by mov 

retraction  which stabilizes the algorithm by moving

points xt but moving   different sequence of points zt is
related to the Katyusha momentum recently developed
for convex optimization  AllenZhu   

  Other Related Work
Methods based on variancereduced stochastic gradients
were  rst introduced for convex optimization  The  rst
such method is SAG by Schmidt et al  Schmidt et al 
  The two most popular choices for gradient estimators are the SVRGlike one we adopted in this paper  independently introduced by  Johnson   Zhang    Zhang
et al    and the SAGAlike one introduced by  Defazio et al   
In nearly all applications  the results
proven for SVRGlike estimators and SAGAlike estimators are simply exchangeable  therefore  the results of this
paper naturally generalize to SAGAlike estimators 
The  rst  nonconvex use  of variance reduction is by
ShalevShwartz  ShalevShwartz    who assumes that
each fi    is nonconvex but their average       is still convex  This result has been slightly improved to several more
re ned settings  AllenZhu   Yuan    The  rst truly
nonconvex use of variance reduction       for       being
also nonconvex  is independently by  AllenZhu   Hazan 
  and  Reddi et al    Firstorder methods only

gradient descent  SGD 

Natasha  Faster NonConvex Stochastic Optimization Via Strongly NonConvex Parameter

Figure   One full epoch of Natasha  The   iterations are divided into   subepochs  each consisting of         steps 

 

 

 nd stationary points  unless there is extra assumption on
the randomness of the data  and converge no faster than
 
When the secondorder Hessian information is used  one
can    nd local minima instead of stationary points  and
  improve the   rate to   The  rst such result is by cubicregularized Newton   method  Nesterov 
  however  its periteration complexity is very high 
Very recently  two independent groups of authors tackled
this problem from   somewhat similar viewpoint  Carmon
et al    Agarwal et al    if the computation of
same order of the computation of gradients  fi    then
 approximate local minimum in
one can obtain    
      

Hessianvector multiplications      cid fi   cid    is on the
 cid  if we use bigO to
gradient complexity  cid   cid   

also hide dependencies on the smoothness parameters 
Other related papers include Ge et al   Ge et al   
where the authors showed that   noiseinjected version of
SGD converges to local minima instead of critical points 
as long as the underlying function is  strictsaddle  Their
theoretical running time is   large polynomial in the dimension  Lee et al 
 Lee et al    showed that gradient
descent  starting from   random point  almost surely converges to   local minimum if the function is  strictsaddle 
The rate of convergence required is somewhat unknown 
  Preliminaries
Throughout this paper  we denote by  cid     cid  the Euclidean
norm  We use          to denote that   is generated from
                     uniformly at random  We denote by
       the full gradient of function   if it is differentiable 
and        any subgradient if   is only Lipschitz continuous at point    We let    be any minimizer of      
Recall some de nitions on strong convexity  SC  strongly
nonconvexity  and smoothness 
De nition   For   function     Rd     

   lot of interesting problems satisfy this property  including

training neural nets 

    is  strongly convex if         Rd  it satis es

                 cid             cid   

 cid       cid   
    is  strongly nonconvex if         Rd  it satis es
 cid       cid   

                 cid             cid     
 

    is  cid   cid smooth if         Rd  it satis es

 
 

         cid             cid     cid 

   cid       cid         

           cid             cid     cid 
 

 cid       cid   

    is Lsmooth if it is       smooth 
The  cid   cid smoothness parameters were introduced
in  AllenZhu   Yuan    to tackle the convex setting
of problem   The notion of strong nonconvexity is
also known as  almost convexity  Carmon et al   
or  lower smoothness  AllenZhu   Yuan    We refrain from using the name  almost convexity  because it
coincides with several other nonequivalent de nitions in
optimization literatures 
De nition   Given   parameter       the gradient
mapping of     in   at point   is

 cid       cid cid 

      

 cid     cid         cid     

 
 

 cid       cid cid 

where   cid    arg miny
In particular  if       then              
The following theorem for the SVRG method can be found
for instance in  AllenZhu   Yuan    which is built on
top of the results  ShalevShwartz    Lin et al   
Frostig et al   
Theorem    SVRG  Let               
   gi   
 
be  strongly convex  then the SVRG method  nds   point
  satisfying                

  with gradient complexity   cid       
  with gradient complexity   cid      cid cid 

gi  is Lsmooth  for       or

gi  is  cid   cid smooth  for  cid   cid     

 cid  
 cid  if each
 cid  if each

    log  

 

    log  

 

 regularized by    regularized by    regularized by      next   we

 

 

      with its regularized version                   cid    

makes sure that when performing update xt   xt  we

to be   random one from             xm 
Full Method 
In Natashafull  see full version  we also
divide each full epoch into   subepochs  In each subepoch

each subepoch    we start with   point     cid    and replace
 cid   cid  Then  in each iteration   of the subepoch    we
  compute gradient estimator  cid  with respect to     xt 
 cid       cid cid    cid   
 cid     xt cid cid  with learning rate  
  perform update xt    arg miny
Effectively  the introduction of the regularizer  cid    cid   cid 
also move   bit towards point cid         retraction by regularization  Finally  when the subepoch is done  we de ne cid  
   we start with   point          cid   and de ne         
         cid    cid   cid  However  this time in each iteration   
  compute gradient estimator  cid  with respect to     xt 
 cid       cid cid    cid   
 cid     zt cid cid  with learning rate   and
  perform update zt    arg miny
Effectively  the regularizer  cid   cid   cid  makes sure that when
performing updates  we move   bit towards point cid        
 cid   also helps us move towards point cid  
Finally  when the subepoch is over  we de ne cid   to be  

retraction by regularization  at the same time  the choice
xt     
      retraction by the socalled  Katyusha momentum 
random one from the set             xm  and move to the
next subepoch 
    Suf cient Stopping Criterion
In this section  we present   suf cient condition for  nding
approximate stationary points in    strongly nonconvex
function  Lemma   below states that  if we regularize the

approximate saddlepoint for      

original function and de ne                 cid    cid   cid 
for an arbitrary point cid    then the minimizer of      is an
Lemma   Suppose                 cid    cid   cid  for some
given point cid    and let    be the minimizer of      If we
 cid  we have the gradient
then for every      cid 

minimize      and obtain   point   satisfying

                 

  choose xt     

  zt     

  zt     

 cid   

 cid   cid       cid   cid  and  cid   cid       cid cid cid 

If one performs acceleration  the running times become

  RepeatSVRG
We recall the idea behind   simple algorithm  that we call
repeatSVRG  which  nds the  approximate stationary
points for problem   when       is  strongly nonconvex  The algorithm is divided into stages  In each stage   
consider   modi ed function Ft               cid    xt cid 
It is easy to see that Ft    is  strongly convex  so one can
apply the accelerated SVRG method to minimize Ft   
Let xt  be any suf ciently accurate approximate minimizer of Ft   
Section   that xt  is an
Now  one can prove      
  cid xt   xt cid approximate stationary point for      
Therefore  if  cid xt   xt cid      we can stop the algorithm
because we have already found an   approximate staIf  cid xt   xt cid        then it must sattionary point 
isfy that    xt       xt     cid xt   xt cid     
          
    stages  Therefore  the total gradient complexity is
  multiplied with the complexity of accelerated SVRG

but this cannot happen for more than       cid   
in each stage  which is  cid         cid    according to

Theorem  
Remark   The complexity of repeatSVRG can be inferred from  Agarwal et al    Carmon et al    but
is not explicitly stated  For instance  the paper  Carmon
et al    does not allow       to have   nonsmooth
proximal term     and applies accelerated gradient descent instead of accelerated SVRG 

  Our Algorithms
We introduce two variants of our algorithms    the base
method Natasha targets on the simple regime when      
and each fi    are both Lsmooth  and   the full method
Natashafull targets on the more re ned regime when      
is Lsmooth but each fi    is  cid   cid smooth 
Both methods follow the general idea of variancereduced
in each innermost iteration 
stochastic gradient descent 

they compute   gradient estimator  cid  that is of the form
 cid        cid   fi cid   fi    and satis es Ei     cid   
       Here   cid   is   snapshot point that is changed once
for computing  cid  is    periteration 

every   iterations       for each different                    cid 
in the pseudocode  and we call it   full epoch for every
distinct    Notice that the amortized gradient complexity

Base Method 
In Natasha  see Algorithm   as illustrated by Figure   we divide each full epoch into   subepochs                       each of length          In
 Since the accelerated SVRG method has   linear convergence
rate for strongly convex functions  the complexity to  nd such
xt  only depends logarithmically on this accuracy 

Natasha  Faster NonConvex Stochastic Optimization Via Strongly NonConvex Parameter

mapping

 

max   

 cid     cid     cid     cid   cid      cid cid   

Notice that when         this lemma is trivial  and can
be found for instance in  Carmon et al    The main

 The idea for this second kind of retraction  and the idea of
having the updates on   sequence zt but computing gradients at
points xt  is largely motivated by our recent work on the Katyusha
momentum and the Katyusha acceleration  AllenZhu   

Natasha  Faster NonConvex Stochastic Optimization Via Strongly NonConvex Parameter

        cid   

 
  subepoch count         epoch count    cid  learning rate      

                

   cid      

Algorithm   Natasha  
Input  starting vector  
Output  vector xout 
  for       to    cid  do
 
 
 
 
 
 
 
 
 
 
  end for

for       to       do

for       to       do

 cid    cid            cid   
    cid           cid   
 cid     fi xt     fi cid           xt  cid   
 cid       random choice from                xm 

      random choice from       

 cid       

xt    arg miny Rd

end for

end for

 cid     xt cid     cid cid    cid cid 

   cid       random vector in   
  xout   an approximate minimizer of                 cid    cid   cid  using SVRG 

  return xout 

 cid     cid  full epochs
 cid    subepochs in each epoch
 cid    iterations in each subepoch

 cid  Ei cid     cid          cid    cid   cid cid cid cid xt

 cid  for practitioners  choose the average

 cid  for practitioners  choose the last

 cid  it suf ces to run SVRG for     log  

    iterations 

technical dif culty arises in order to deal with      cid   
The proof is included in the full version 
  Base Method  Analysis for One Full Epoch
In this section  we consider problem   where each fi   
is Lsmooth and       is  strongly nonconvex  We use
our base method Natasha to minimize       and analyze
its behavior for one full epoch in this section  We assume
      without loss of generality  because any Lsmooth
function is also Lstrongly nonconvex 
Notations  We introduce the following notations for analysis purpose only 

  Let cid xs be the vector cid   at the beginning of subepoch   
  be the vector xt in subepoch   
  Let                   cid    cid xs cid                  
  be the index         in subepoch   at iteration   
 cid    cid xs cid  and xs    arg minx       
  Let cid     xs
   fi cid      cid   xt 
 cid    where     is
  Let  cid    xs
       fi cid          cid    where

       fi xs
       fi xs

  Let xs
  Let is

   

   
    is

We obviously have that        and        are  strongly
convex  and        is       smooth 
  Variance Upper Bound
The following lemma gives an upper bound on the variance

of the gradient estimator  cid     xs
 cid cid cid     xs
   cid xs cid    pL cid   
    cid cid xk  cid xk cid   

Lemma   We have Eis
pL cid xs

   cid cid   

           xs

   

 

 

 

  

Eis

Ei     

Proof  We have

  Ei     
   Ei     
   pEi     

          xs

           xs

 cid cid cid    xs
   cid cid    Eis
   cid cid 
 cid cid cid     xs
          cid   cid cid cid cid 
 cid cid cid cid fi xs
       fi cid   cid   cid    xs
       fi cid   cid cid cid 
 cid cid cid fi xs
       fi cid xs cid cid cid 
 cid cid cid fi xs
 cid cid cid fi cid xk     fi cid xk cid cid cid 
    cid   
   cid xs cid    pL cid   
    cid cid xk  cid xk cid   

   pL cid xs
Above  inequality   is because for any random vector    
  is because cid     cid   and for any   vectors               ap  
Rd  it holds that   cid     cid      cid cid cid   cid  inequality
Rd  it holds that  cid     ap cid      cid   cid     cid ap cid 
and inequality   is because each fi  is Lsmooth   cid 
  Analysis for One SubEpoch
The following inequality is classically known as the  regret inequality  for mirror descent  AllenZhu   Orecchia 
  and its proof is classical  see full version 
Fact  
     cid xs
    cid 
 cid xs

       cid     xs
   cid xs

 cid cid     xs

          

for every     Rd 

    cid 
 

  xs

    xs

  cid 

 

The following lemma is our main contribution for the base
method Natasha 
Lemma   As long as      

      we have

  cid cid     cid xs        xs cid cid 
    cid      cid xs        xs 

 pL cid    cid 

 cid cid xk cid xk cid cid cid 

 

   

  

Natasha  Faster NonConvex Stochastic Optimization Via Strongly NonConvex Parameter

    xs
       xs

Proof  We  rst compute that
                  xs
       cid     xs
            xs

     xs
    xs
        

               xs
 cid xs

     

        
    xs

  cid 

  cid   

 

 cid xs

    xs

  cid 

    cid     xs

    xs
   cid     xs

  cid   

     xs
    xs

     
 
      cid     xs

          

 
Above  inequality   uses the fact that      is       
smooth  and inequality   uses the convexity of      Now 
we take expectation with respect to is
  on both sides of  
and derive that 

  cid          

 

Eis
   Eis

 
   Eis

   Eis

  Eis

 

 

 

     

 cid     xs
 cid cid cid     xs
 cid cid cid     xs
   cid xs
 cid 
 cid cid cid     xs
 cid 

 

 

 

 pL cid xs
      cid 
 

 cid xs

 

       cid 

           xs
 cid xs
           xs

    xs

    xs

    xs
  cid     xs
    xs
    xs
       
 cid xs

 

 cid   

 

           xs

   cid cid   
  cid 
   cid xs cid     pL 
 cid 
   cid xs
 cid     xs

       cid 

 

  

  cid     cid cid     xs
 cid 

        
 cid xs
  cid   

 cid cid xs

      cid 
 
     xs

  cid cid 
   cid xs
 cid cid xk  cid xk cid 

      cid 
 

 

 

 

 

 

   

    xs

  xs

    cid cid   
 cid   cid 

 cid   cid     

 cid     xs

    implies
    xs

     xs
    xs
       xs

 cid cid     xs
 cid cid cid     xs

           xs
  cid     cid     xs
    xs

Above  inequality   is follows from   together with
the fact that Eis

      cid cid 
  cid cid cid     xs
Eis
  Eis
inequality   uses Fact   inequality   uses      
together with Young   inequality  cid      cid     
and inequality   uses Lemma  
Finally  choosing     xs  to be the  unique  minimizer of
                and telescoping inequality   for
                      we have
          xs cid cid 
 cid 
  cid 

   cid xs cid 
 cid cid xk  cid xk cid cid cid 
  cid 
 cid cid xk  cid xk cid cid cid 
    cid      cid xs        xs 
   pmL cid    cid 
Above  the second inequality uses the fact that cid xs  is chom  uniformly at random  as well as

  cid    cid 
    cid cid xs

sen from  xs
the  strong convexity of     

    xs cid 
 

 pL cid xs

          xs

   pL 

 

  

  

  

  

 

 

    xs

       cid 

 cid 

       cid 

 

 

Dividing both sides by   and rearranging the terms  using
  cid cid     cid xs        xs cid cid 
      we have
    cid      cid xs        xs 

   pL cid    cid 

 cid cid xk  cid xk cid cid cid 

   cid 

   

  

  Analysis for One Full Epoch
One can telescope Lemma   for an entire epoch and arrive at the following lemma  see full version 
Lemma  
have

           

If      

       we

  cid cid     cid xs        xs cid cid       cid 

   and      
 cid 
   cid         cid xp 

 

  cid 

  

  Base Method  Final Theorem
We are now ready to state and prove our main convergence
theorem for Natasha 
Theorem   Suppose in   each fi    is Lsmooth
and       is  strongly nonconvex for        Then  if
       our base

     cid  and        
 cid       
 cid         

  
method Natasha outputs   point xout satisfying
  cid   xout cid     

            cid   
for every      cid 

 cid  In other words  to obtain

         

   cid  

 

 

max   

  cid   xout cid      we need gradient complexity
       

     

       

  log

 

 

 

 cid 

 cid 

 

 
 

 

In the above theorem  we have assumed       without
loss of generality because any Lsmooth function is also
     
Lstrongly nonconvex  Also  we have assumed   
and if this inequality does not hold  then one should apply
repeatSVRG for   faster running time  see Figure  

      cid     
Proof of Theorem   We choose      cid   
 cid xp of the previous epoch equals cid    of the next epoch  we

     and      
      so we can apply
Lemma   If we telescope Lemma   for the entire algorithm  which has    cid  full epochs  and use the fact that

conclude that if we choose   random epoch and   random
subepoch    we will have

       

      

 

      cid xs        xs     

 

         

pT  cid       

By the  strong convexity of      we have   cid cid xs  
xs cid     
Now                    cid   cid xs cid  satis es the assumption

of      in Lemma   If we use the SVRG method  see
Theorem   to minimize the convex function        we

 
pT  cid       

       

Natasha  Faster NonConvex Stochastic Optimization Via Strongly NonConvex Parameter

get an output xout satisfying     xout        xs      in

We can therefore apply Lemma   and conclude that this
output xout satis es

gradient complexity   cid       
 cid         
 cid   
 cid       

  cid   xout cid     

pT  cid 

    log  

 

   

   cid  

 cid        log  

   

 

 cid         

       
 

         

 cid 

In other words  we obtain   cid   xout cid      using

   cid      

         

 

       

 

       

 cid 

computations of the stochastic gradients  Here  the additive
term   is because    cid     
    the gradient complexFinally  adding this with     log  
ity for the application of SVRG in the last line of Natasha 
we  nish the proof of the total gradient complexity   cid 

  Full Method  Final Theorem
We analyze and state the main theorems for our full method
Natashafull in the full version of this paper 
  Conclusion
Stochastic gradient descent and gradient descent  including
alternating minimization  have become the canonical methods for solving nonconvex machine learning tasks  However  can we design new nonconvex methods to run even
faster than SGD or GD 
This present paper tries to tackle this general question  by
providing   new Natasha method which is intrinsically different from GD or SGD  It runs faster than GD and SVRGbased methods at least in theory  We hope that this could
be   nonnegligible step towards our better understanding
of nonconvex optimization 
Finally  our results give rise to an interesting dichotomy in
 
the bestknown complexity of  rstorder nonconvex opti 
 
mization  the complexity scales with    for       
 
   It remains open to investiand with    for       
gate whether this dichotomy is intrinsic  or we can design
  more ef cient algorithm that outperforms both 
References
Agarwal  Naman  AllenZhu  Zeyuan  Bullins  Brian 
Hazan  Elad  and Ma  Tengyu  Finding Approximate Local Minima for Nonconvex Optimization in Linear Time 
In STOC   

AllenZhu  Zeyuan  Katyusha  The First Direct Accelera 

tion of Stochastic Gradient Methods  In STOC   

AllenZhu  Zeyuan and Hazan  Elad  Variance Reduction

for Faster NonConvex Optimization  In NIPS   

AllenZhu  Zeyuan and Li  Yuanzhi  LazySVD  Even
Faster SVD Decomposition Yet Without Agonizing
Pain  In NIPS   

AllenZhu  Zeyuan and Li  Yuanzhi  Doubly Accelerated
Methods for Faster CCA and Generalized Eigendecomposition  In Proceedings of the  th International Conference on Machine Learning  ICML      

AllenZhu  Zeyuan and Li  Yuanzhi  Follow the Compressed Leader  Faster Online Learning of Eigenvectors
In Proceedings of the  th Interand Faster MMWU 
national Conference on Machine Learning  ICML  
   

AllenZhu  Zeyuan and Orecchia  Lorenzo  Linear Coupling  An Ultimate Uni cation of Gradient and Mirror
Descent  In Proceedings of the  th Innovations in Theoretical Computer Science  ITCS    

AllenZhu  Zeyuan and Yuan  Yang 

Improved SVRG
for NonStrongly Convex or Sumof NonConvex Objectives  In ICML   

Carmon  Yair  Duchi  John    Hinder  Oliver  and Sidford 
Aaron  Accelerated Methods for NonConvex Optimization  ArXiv eprints  abs  November  

Defazio  Aaron  Bach  Francis  and LacosteJulien  Simon 
SAGA    Fast Incremental Gradient Method With Support for NonStrongly Convex Composite Objectives 
In NIPS    URL http arxiv org abs 
 

Frostig  Roy  Ge  Rong  Kakade  Sham    and Sidford 
Aaron  Unregularizing  approximate proximal point
and faster stochastic algorithms for empirical risk minimization  In ICML  volume   pp      URL
http arxiv org abs 

Garber  Dan  Hazan  Elad  Jin  Chi  Kakade  Sham   
Musco  Cameron  Netrapalli  Praneeth  and Sidford 
Aaron  Robust shiftand invert preconditioning  Faster
and more sample ef cient algorithms for eigenvector
computation  In ICML   

Ge  Rong  Huang  Furong  Jin  Chi  and Yuan  Yang  Escaping from saddle points online stochastic gradient
for tensor decomposition  In Proceedings of the  th Annual Conference on Learning Theory  COLT    

for

nonconvex

gradient methods

Ghadimi  Saeed and Lan  Guanghui 

and stochastic programming 

Accelernonlin 
ated
Mathematiear
cal Programming  pp   
ISSN
   
doi 
 
URL
http arxiv org abs 
 http link springer com 
  

feb  

Natasha  Faster NonConvex Stochastic Optimization Via Strongly NonConvex Parameter

Johnson  Rie and Zhang  Tong  Accelerating stochastic gradient descent using predictive variance reduction 
In Advances in Neural Information Processing Systems 
NIPS   pp     

Lee  Jason    Simchowitz  Max  Jordan  Michael    and
Recht  Benjamin  Gradient descent only converges to
minimizers  In Proceedings of the  th Conference on
Learning Theory  COLT   New York  USA  June  
    pp     

Lin  Hongzhou  Mairal  Julien  and Harchaoui  Zaid 
  Universal Catalyst
for FirstOrder Optimization 
In NIPS    URL http arxiv org pdf 
   pdf 

Nesterov  Yurii  Accelerating the cubic regularization of
newton   method on convex problems  Mathematical
Programming     

Reddi  Sashank    Hefny  Ahmed  Sra  Suvrit  Poczos 
Barnabas  and Smola  Alex  Stochastic variance reduction for nonconvex optimization  ArXiv eprints 
abs  March  

Schmidt  Mark  Le Roux  Nicolas  and Bach  Francis  Minimizing  nite sums with the stochastic average gradient  arXiv preprint arXiv  pp     
URL http arxiv org abs  Preliminary version appeared in NIPS  

ShalevShwartz  Shai  SDCA without Duality  Regulariza 

tion  and Individual Convexity  In ICML   

Zhang  Lijun  Mahdavi  Mehrdad  and Jin  Rong  Linear
convergence with condition number independent access
of full gradients  In Advances in Neural Information Processing Systems  pp     

