Understanding the Representation and Computation of Multilayer

Perceptrons    Case Study in Speech Recognition

Tasha Nagamine   Nima Mesgarani  

Abstract

Despite the recent success of deep learning  the
nature of the transformations they apply to the
input features remains poorly understood  This
study provides an empirical framework to study
the encoding properties of node activations in
various layers of the network  and to construct
the exact function applied to each data point in
the form of   linear transform  These methods are
used to discern and quantify properties of feedforward neural networks trained to map acoustic
features to phoneme labels  We show   selective and nonlinear warping of the feature space 
achieved by forming prototypical functions to account for the possible variation of each class 
This study provides   joint framework where the
properties of node activations and the functions
implemented by the network can be linked together 

  Introduction
In recent years  deep learning has achieved remarkable
performance on   variety of tasks in machine learning
    Andor   Collins       Silver   Hassabis   
   He   Sun    including automatic speech recognition        Dahl   Acero       Hinton   Kingsbury         Mohamed   Hinton       Xiong
  Zweig    Despite these successes  our understanding of deep neural networks  DNNs  and the nature of their
computation and representations lags far behind these performance gains  This has motivated   number of recent
studies aimed at better understanding the computational
principles of deep learning in the hope of gaining intuitions
that may lead to improved models 
It has been established that networks with at least one
 Columbia University  New York  NY  USA  Correspondence

to  Nima Mesgarani  nima ee columbia edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

hidden layer are universal approximators  Cybenko   
   Hornik   White    Several recent theoretical studies have proven that deeper architectures are able to more
ef ciently solve problems than shallow models  given  
limited number of parameters  Eldan   Shamir    Lin
  Tegmark    Other studies have focused on networks
with recti ed linear units  and have shown that deeper networks are able to learn more complex functions by splitting
the input space into exponentially more linear response regions than equivalent shallow models     Montufar   Bengio       Pascanu   Bengio    Finally  recent successes using deep residual networks and subsequent analyses show the effectiveness of extremely deep representations in supervised learning tasks     He   Sun   
   Veit   Belongie   
At the same time  there is   growing body of empirical studies that aim to understand the behavior of neural networks
by developing mathematical methods and techniques for
visualization  Zeiler   Fergus       Yosinski   Lipson 
  as well as studies of instability in the face of adversarial examples     Szegedy   Fergus       Nguyen  
Clune    and contraction and separation properties of
these models  Mallat    In the  eld of speech recognition  several studies explored the representations of speech
learned by feedforward networks used in acoustic modeling       Mohamed   Penn       Nagamine   Mesgarani      Finally  an architectureindependent
method was proposed to summarize the complexity of the
parameters learned in feedforward networks     Wang  
Aslan   
In this study  we aim to bridge the gap between these theoretical and empirical studies by providing methods for analyzing both the properties of activations in each layer of
the network and estimating the exact linear function that is
applied to each data point  We discern and quantify properties of the network representation and computation to determine   what aspects of the feature space are encoded
in different layers  internal representation  and   how the
network transforms the feature space to achieve categorization  network function  Our method thus provides  
joint framework in which the properties of node activations
are directly linked to the properties of the function that is

Visualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

learned  which stands in contrast to previous studies that
focus on   speci   property of the network  These methods are easily extensible to other applications for MLPs  as
well as other feedforward network architectures 

  Methods
  Deep neural network acoustic models

Neural network acoustic models for phone recognition
were trained on the Wall Street Journal speech corpus
    Garofolo   Pallett      The corpus includes
approximately   hours of read speech training data  We
report test accuracies using the eval  set  Input features to
all models were extracted using the Kaldi speech recognition toolkit     Povey   Vesely    and consisted of  
dimensional logMel  lter bank coef cients   ms window    ms frame rate  with applied mean and variance normalization  spliced over   context frames  Models were
trained on HMM state targets consisting of monophones
with one state per phone   output states  corresponding
to   phonemes and one silence state  All alignments for
training were obtained using the WSJ    recipe in Kaldi 
To ensure the generality of our  ndings  we explored two
network architectures using   and   nodes per hidden layer with  ve hidden layers  In this work  we analyze
models using recti ed linear units  ReLUs  because it has
been shown that ReLU networks are faster to train  are less
sensitive to initialization than sigmoid units  and improve
performance in speech recognition tasks        Dahl  
Hinton          Maas   Ng          Zeiler   Hinton    For the networks with   nodes per hidden
layer  we adopted batch normalization  Ioffe   Szegedy 
  and dropout     Srivastava   Salakhutdinov   
as regularization methods  with     dropout rate on input
and hidden layers  All models were trained using Theano
    Bergstra   Bengio    Hyperparameters were determined using grid search  Networks with   and  
nodes per hidden layer were trained with   and   epochs
of backpropagation  respectively  Because the goal of this
study is to examine the transformation from features to
phone posteriors of MLP acoustic models  not language
models  we report performance using framewise classi 
 cation accuracy rather than word error rate 

  Analyzing the learned network functions

To analyze and compare neural networks  we extended the
methods used in     Wang   Aslan    to construct an
extended data Jacobian matrix  EDJM  for the nodes in  
neural network  The basic concept underlying the EDJM
is that   neural network with layers of subsequent nonlinear transformations can be mathematically reproduced by
 nding   perdata point linear system mapping input to out 

Table   Framewise accuracy of acoustic models on WSJ dataset 

LAYERS NODES

 
 

 
 

DEV 

TR 
EVAL 
     
     

put  More formally  consider   dataset   consisting of inputs           xN   and outputs           yN   For
  deep neural network model with   layers and arbitrary
nonlinear function   mapping inputs xn to predicted output
 yn                  xn  the data Jacobian matrix  DJM  of any output unit can be constructed
by  nding the gradient of the node with respect to each of
the inputs using the following equation 

DJM   xi   

 

 

   yi
   

   yi
 xi

 cid   

 cid 

 

 

 hl 

 

   
 
 xi

 

 

   
   
   

 cid   

 

 

 

 cid   

 

   xi 

 xi

 

Here    cid 
  represents the activation of layer  cid  for data point
   while   indicates dependence on network parameters 
Now assuming   linear output function and recti ed linear
units  ReLU  with   nonlinearity of the form  ReLU      
max      where   is the weighted input to the node  this
equation can be written 

DJM   xi       

 

   
    

    

 

 

   
 

   
    

    

 

 

    
IN 

 
This method can be easily extended to any node in any hidden layer  cid      by replacing   with  cid  in Eq    Intuitively 
this is equivalent to  nding the unique linear mapping function    for   network node for each individual data point 
Using the piecewise linear property of the recti ed linear
function  the previous equation can be easily simpli ed because taking the gradient with respect to any node activation is equivalent to setting rows of the weight matrix to  
for zeroactivation data points 
   cid 
 

if   cid 
otherwise 

 cid   xi          

 cid          

         

 cid 

    cid 

 

Thus  for   feedforward ReLU network   nding the DJM
for point   for   given node is mathematically equivalent to
setting selected rows of each weight matrix to   and doing
  simple matrix multiplication 
   

 cid 
 cid cid 
   cid         
 cid      
 cid cid 
 cid 

 yi      

          

 

      

 

INxi

INxi

 

 

  DJM   xi   

Calculating the EDJM for the whole dataset over every node in   layer results in   tensor of dimension
     din   dout  where for each data point            
we have constructed   linear map from input to output  This
method  outlined in Eq    is applicable to any network

Visualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

Figure   For   single data point xi  given   network with ReLUs and   linear output  the sampledependent linear transform
SLT xi  can be found by setting rows of the weight matrices to
zero according to Eq   

with   nonlinearity that is differentiable with respect to its
inputs       sigmoid and hyperbolic tangent  in which the
gradients can be interpreted as linear approximations to the
network function for   given data point  For clarity  for
the rest of this paper we will refer to the EDJM of individual nodes as the sampledependent linear transform  SLT 
The SLT for   given dataset has    xed dimensionality independent of network architecture  providing   useful tool
for comparing different networks 

  Quantifying complexity with SVD

In   neural network  both activations and the SLT of nodes
can be written as   twodimensional matrix of the form
     mi      RN     In the case of activations   cid  in
layer  cid    is the number of data points in dataset    and  
is dimensionality of the layer  In the case of the SLT  we
perform decompositions on the SLT of individual nodes 
where   is the dimension of the inputs  Given      matrix  we can then perform matrix factorization using singular value decomposition  SVD  of the form         
The singular values  sorted by decreasing order  of the diagonal of   de ne the weights given to the orthonormal
basis functions de ned by   and    Because the  rst dimension of the activation matrix is de ned over data points 
the singular values of SLT cid 
  can serve thus as   metric of
the complexity  diversity  of the learned network function 
Consider the case where rank SLT cid 
       This means
that for node   in layer  cid  the linear system from input to
output is the same for all data points and the function for
this node is linear  At the other extreme    uniform distribution of singular values suggests that the function learned
for each data point is drastically different  Thus  the relative
values of the SVD spectra for SLT matrices can serve as  
metric of nonlinearity for the system  Similarly  the distribution of singular values of   cid  indicate how uniformly the
nodes within   layer respond to different data points 
In general  the matrices   cid  and SLT cid 
  are fullrank  Thus 
we quantify the shape of the distribution of the SVD
spectrum by normalizing by its maximum value  where
max       and computing the area under the curve
 AUC  We can compute this score for the  rst   singular
values   of the spectrum using the following equation 

Figure   Unsupervised hierarchical clustering of activations in
each hidden layer across classes  rows  and nodes  columns   
nodes layer  for hidden layers             and           Number
of clusters of nodes  columns  in each hidden layer based on   distance cutoff criterion        correlation distance  for networks
with   and   nodes layer 

AUC  

 

      

max  

  

  

  cid 

         

 

Because the AUC is computed after normalizing the spectrum so that its maximum value is   intuitively  this metric
quanti es the nonuniformity of the singular values of the
spectrum  Larger values signify greater uniformity in the
distribution of singular values and thus   higher degree of
complexity 

  Results  Analysis of node activations
We begin by considering the network activations    which
can be interpreted as nonlinearly transformed feature representations of the network inputs  To characterize this representation  we studied both the properties of the activations
of individual nodes  local encoding  and the population of
nodes in each layer  global encoding 

  Visualizing the global feature encoding

In   supervised learning task  hidden layers of   neural network extract representations with meaningful  classbased
distinctions  To study these distinctions  for each hidden
layer  we grouped node activations to the WSJ eval  set
by their corresponding label  To quantify the overall pattern of selectivity in hidden layer  cid  with   nodes  we compute the average activation hm for every node   for each
distinct label            This results in   vector of
dimension        for each node that characterizes its aver 

xiyi   SLT xi  xiWIN       OUT time    SLT xi frequencySLT xi    WIN xi      xi         xi OUT Visualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

Table   Classi cation accuracy using linear discriminant analysis
 LDA  trained on the WSJ dev  set 

LAYER
FEATS
HL 
HL 
HL 
HL 
HL 

NNODES    

NNODES    

DEV 

EVAL 

DEV 
EVAL 
       
       
       
       
       
       

age response to each class  which we call   class selectivity
vector  CSV  where each element is calculated as follows 

Nk cid 

 
Nk

hm  xi yi     

  

CSVm     

 
Concatenating the CSVs into   matrix of dimension     
    summarizes the overall selectivity of   hidden layer 
where rows correspond to classes and columns correspond
to individual nodes  Examining this matrix reveals patterns in both the individual node and population coding of
classes within   layer  To visualize these selectivity patterns  we performed an unsupervised hierarchical clustering on columns  nodes  based on the similarity of their
CSVs  and rows  phonemes  based on the similarity of their
overall activation pattern over nodes in   layer  UPGMA 
correlation distance  in hidden layers     and   of the
  node layer network  Fig   AC  For visualization
purposes  the dendrogram showing clusters of nodes was
truncated at   set cutoff distance of  
Examining these vectors reveals nodes with various types
of response pro le  including selectivity to both individual
classes and groups of classes with shared features  For example  each layer has   group of nodes dedicated to encoding only silence  by far the most common label in the training set  The remaining classes  corresponding to speech
sounds  tend to be encoded jointly  with groups of nodes
encoding phoneme classes are predominantly organized by
phonetic features such as manner and place of articulation 
However  other nodes exhibit selectivity patterns not easily
explained by acoustic or phonetic feature distinctions 
We observe that in deeper layers  individual nodes tend
to become less broadly selective to classes  evidenced by
sparser selectivity patterns in their CSVs  The result is that
in deeper representations  the average activations of nodes
to individual classes tend to become differentiated from one
another  This is quanti ed in the top dendrograms showing
the clustering of nodes in Fig   AC  where we observe
that the use of   distance cutoff criterion  correlation distance        results in   larger number of branches in
each subsequent hidden layer  This holds true for networks
with   nodes and   nodes per layer  Fig      This
shows that neural networks are successful at using progressive hidden layer representations to decorrelate their inputs

Figure       Explained variance ratio  EVR  for LDA model
trained on features and activations from hidden layers      SVD
spectra for LDAtransformed activations  separated by class and
averaged      AUC for spectra in     and     for networks with
  and   nodes layer 

in order to extract useful features for classi cation 

  Deep representations are more classdiscriminable

Next  we wanted to directly examine the discriminability
of classes in each layer of the network  Measuring the distance between classes directly from the activations  however  may produced biased results  Between networks and
even within networks  activations in different layers suffer from the problem that they   may have different dimensionalities  or   are drawn from different distributions
      layers may exhibit differing amounts of sparsity  or
contain  dead  nodes 
To remedy this  we applied linear discriminant analysis
 LDA  as   supervised dimensionalityreduction technique
 SVD solver  on input features and hidden layer activations  Doing so allows us to project hidden representations

of   layer   cid  onto   set of       orthonormal bases  cid   cid 

that maximally retain class discriminability and are more
easily comparable between layers and networks  Due to
the large size of the training set  we used the WSJ dev  to
train the LDA models  noting that the development set was
not used in the training of any of the neural network models  Table   shows the classi cation accuracy using LDA
on input features and hidden layer activations for networks
with   and   nodes layer  We observe two main patterns  First  for both networks  representations of classes
are more separable in each subsequent hidden layer  Second  given layers of equal depth  class discriminability is
greater for the larger network 
By examining the ratio of explained variance of each LDA
model  we can see that more dimensions are needed to explain    xed percentage of the variance in deeper layers
 Fig      meaning that in deeper layers  the representation contains more meaningfully discriminant dimensions 
However  we also wanted to investigate properties of the

singular value order singular value order featsHL HL HL HL HL ABexplained variance ratioSVD spectraCnormalizedsingular valuenormalizedsingular valuefeatsHL HL HL HL HL Visualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

  simple linear classi er present   less dif cult problem to
the neural network than those that are incorrect and thus
linearly inseparable  We denote the LDAtransformed activations of hidden layers  cid  to correct and incorrect groups as
INC  inseparable  respectively  We
also consider the whole dataset  which we simply denote

 cid   cid 
COR  separable  and  cid   cid 
as  cid   cid  Furthermore  we  lter examples in each of these

groups by considering only data points that are classi ed
correctly at the output layer 
To determine if the feature transformation that takes place
between hidden layers is applied nonuniformly across examples  we examined the relative location in the representational space of the hidden layers for three example classes
when they are easily confusable         and when they
are seldom confused         in the input features  confusion matrix shown in Fig      Fig   AB visualizes the

Euclidean distance between data points in  cid   cid  using mul 

tidimensional scaling  MDS   Kruskal   Wish    for
classes         and         Individual class label is shown
by color  while labeled points shown in black represent the
centroid of each class  The distances between classes for
each hidden layer are overlaid at the right  Comparison of
the relative distances between the classes in Fig     versus
  shows that the network nonlinearities expand the space
between the more confusable classes         more than for
classes that are more linearly separable        
While this analysis shows that the hidden layer repre 

warped more nonlinearly  we also investigated whether this
nonuniform and selective nonlinear transformation occurs
within each class as well  Fig     tracks the relative loINC as they propagate
through the network  Compared to Fig      we see that
there is   rapid nonlinear increase in the relative distance
between these more dif cult examples of the same class 

sentations  cid   cid  between more overlapping categories are
cation of data points contained in  cid   cid 
which occurs more strongly than for the examples in  cid   cid 
put features cid    of cid   cid 

This observation is quanti ed in Fig      where we show
the relative expansion  increase in distance as compared to
the distance between examples in the LDAtransformed inINC for both         and        
With the intuition gained from considering   subset of
classes  we next quantify this  nding for all classes in Fig 
   for networks with both   and   nodes layer  However  instead of focusing on the relative expansion between
three classes  we consider the expansion of each class to
its most frequently confused class at the output layer  Our
results show that over all classes  linearly inseparable examples undergo   greater degree of expansion than linearly
separable ones  The observed nonuniform  nonlinear  and
focused stretching of the feature space illustrates the power
of   multilayer neural network to nonlinearly expand speci   parts of the feature space that are critical for discrim 

COR and cid   cid 

Figure   Visualization of activations  cid   on the WSJ eval  set in

hidden layers   and     nodes layer  using tSNE  For visualization  data points were aggregated using Kmeans clustering 

representation within   class  We did so by separating the

transformed activations  cid   cid  into separate matrices for each

class    then performing SVD on each matrix individually
in each layer  Figure    shows the resultant SVD spectra  averaged over all classes 
In deeper layers  on average the spectra require fewer singular values to reach the
same ratio of explained variance  indicating that class separability is accompanied by   normalization of activations
within classes  to represent   single class  fewer orthogonal
bases are required  These results are quanti ed in Figure
   using the AUC score from Eq    For the LDA models  explained variance ratio  EVR  we observe that the
AUC score tends to increase with layer depth  while the
LDAtransformed activations of individual classes exhibit
smaller AUC scores in deeper layers  Taken together  these
results suggest that representations learned in hidden layers
of   neural network become increasingly discriminative  in
part through normalization of withinclass variability 
This normalization is visualized in Fig    for   network
with   nodes layer  where tSNE  van der Maaten  

Hinton    is used to project  cid   cid  for  cid        into   

 Euclidean distance  To reduce the dimensionality of the
problem  we used Kmeans clustering to cluster data points
of the same class  choosing the number of clusters for each
class as Nk
    where Nk is the number of data points of class
  in the WSJ eval  set  The color and symbol  international phonetic alphabet  of each cluster indicate class
membership  the size of each point is proportional to the
number of data points in the cluster as determined by Kmeans  We observe   tighter clustering of classes in deeper
layers  echoing the quanti cation in Fig   

  Representations are transformed nonuniformly

In this section  we show that the normalization shown in
the previous section is greater for data points that are  dif 
 cult  to classify  We accomplish this by  rst dividing
the WSJ eval  set into two groups based on whether the
data points were classi ed correctly or incorrectly using the
LDA model trained on the input features  This is based on
the intuition that examples that are classi ed correctly in

Visualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

and         three similar classes          and     and     inseparable examples of          in  cid   cid 

Figure   First two MDS dimensions of LDAtransformed features and hidden layer activations for     three dissimilar classes         
INC  For visualization purposes  data points
were aggregated using Kmeans clustering  the size of each point is proportional to the number of examples in that cluster  Centroids for
each class are shown in black text      Confusion matrix for LDA classi er trained on input features for classes in  AC   EF  Average
relative expansion of centroids of linearly separable and inseparable  LDA  input features  data points  relative to input features for    
classes          left  and             all classes for   neural network with   nodes layer  left  and   nodes layer  right 

ination of overlapping categories  while at the same time
applying more linear transformations to the parts of the feature space which are less overlapping 

  Results  Analysis of the network function
The function of   ReLU network can be interpreted as   collection of sampledependent linear transformations  SLTs 
of the input features  In this section  we directly characterize the properties of this function and examine the contribution of different layers to the resulting computation of
the network  Because the size of the SLT for   layer of  
neural network is very large  we restrict the analyses in this
section to the network with   nodes layer 

  Networks learn clusters of functions

We start by looking at the sampledependent linear transforms  SLTs  mapping inputs to hidden and output layer
representations of the neural networks  Fig     visualizes the SLT for the output node for class      SLTout
   
for all correctly classi ed data points with label     in the
WSJ eval  set  sorted by clustering  UPGMA  Euclidean
distance  Here  columns represent data points  and rows
are the linear weights  templates  applied to each sample to
map it to the output node  We see that while the network

can potentially learn   different template for each sample 
the SLTs tend to cluster in groups  with several broad clusters separated by dashed black lines  revealing similarities
between linear transforms that are applied to subsets of data
points  The distance between each SLT for class     is visualized using MDS in Fig      while the average linear
mapping function for each cluster is shown in Fig     

  Deeper layers encode more diverse functions

The number of distinct SLTs that are applied to data
points from the same class shows the diversity of templates learned for that class  This diversity of templates can
also be interpreted as the complexity of the function that
is learned for   given class  With this intuition  to quantitatively analyze   network we perform SVD on the SLT
of node   in layer  cid  and keep the singular value spectrum
contained in the diagonal of the matrix  cid 
   normalized by
the maximum value 
Because we are interested in the transformations occurring
in deep networks at each layer  we applied this analysis on
each of the individual nodes of the output and hidden layers of the network  For each node  if we retain the vector
of singular values and take their average  we see that given
the same dataset  deeper layers also encode more complex
functions     top left  In this analysis we neglect the  rst

Visualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

Finally  we examined the SLTs for more dif cult examples
in the dataset  Recall from section   that the representational space was transformed more for linearly inseparable data points than for separable ones  We replicated
this result in the SLT space by performing an SVD for individual nodes by splitting the data points into the analogous linearly separable and inseparable groups SLT cid 
and SLT cid 
INC based on classi cation accuracy from the LDA
model trained on input features for each hidden layer  cid 
We then calculated the score for the difference between
the spectra  cid 
  COR  averaged over nodes    using
area under the curve  AUC  The resulting positive values
shown in Fig     show that the diversity of SLTs learned
for the dif cult examples is consistently greater that the
easier ones  Therefore  the nonuniform warping of the feature space is achieved by an increase in the number of SLTs
that are learned  resulting in division of the feature space
into   greater number of linear subregions to accommodate these more subtle class distinctions 

  INC    cid 

COR

  Visualizing the SLT from input to output

We showed in the previous section that
the functions
learned in subsequent layers of deep neural networks are
more complex  Here  we seek to visualize the learned functions that de ne the separate classes  Because we are interested in the features important for class distinctions  for the
remainder of this section we consider the subset of data
points in the eval  set that are classi ed correctly at the
output layer of the network  For each output node  we calculate the SLT which is   matrix of dimension  Nk      
where Nk is the number of data points belonging to class
   To visualize the learned linear templates at the output
layer  we  nd the centroid over data points Nk and compute the mean of the SLT and corresponding input features
over the   samples closest to the centroid  Results for selected classes are shown in Fig      where the average input features  left  outlined in red  and corresponding mean
SLT  right  outlined in blue  are presented sideby side 
  standard MLP trained for supervised learning must normalize sources of variability in the feature space in order to
perform   successful classi cation  In   phoneme recognition task  there are two main sources of variability within
  class  The  rst source comes from differences in pronunciations of phones  or allophonic variations of the same
phoneme  Ladefoged   Johson    The second source
of variability comes from the fact that the network must
learn that timeshifted variants of the same inputs belong
to the same class  In the previous section  we showed that
the network equivalent sampledependent linear systems
are quite diverse at the output layer  evidenced by the large
AUC score of the SVD spectra of output nodes  To visualize this diversity of templates  which uncover the possible
variations of features for each class  we used unsupervised

Figure       SLTs of correctly classi ed data points with the label     clustered by similarity 
    Pairwise distances between
individual SLTs in     projected into    using MDS  Color indicates cluster assignment      Average SLT for clusters de ned in
   

hidden layer because the SLT of each node is bimodal  contains only values of    
  or   for   data point  meaning that
rank SLT 
       In deeper layers of the network  the
AUC score from Eq    is larger  indicating that more orthogonal bases are needed to explain    xed percentage of
the variance  For   comparison of deep vs  shallow networks with the same number of parameters  see     Wang
  Aslan   
While this analysis shows that on average individual nodes
encode more diverse SLTs in deep layers  we also wanted
to con rm that this resulted in   more complex population
coding of classes  To do this  we performed   similar analysis  this time doing one SVD for each class on the matrix
containing the SLT of all nodes in   layer to instances of
that class  reshaped to size  Nk           In this way 
we perform SVD on all the nodes in   layer in response
to each classes  Fig     shows the resultant score calculated from SVD on the SLT  averaged over classes  plotted against the scores from Fig     calculated similarly
from the activations  We observe   strong negative correlation  indicating that the increased normalization that occurs
within classes in the activations of deeper layers is due to
an increase in the diversity of SLTs that create more linear
response regions in the network 

Figure       SVD spectra of SLTs for hidden layers computed
over individual nodes  NODES  and input dimensions  INPUTS 
averaged  Below are histograms of AUC scores over nodes input
dimensions for the average spectra      AUC score from SVD
spectra calculated from activations vs  SLT      Area under the
curve  AUC  for  cid 
  COR calculated from SLT of nodes 

  INC    cid 

 data points with label      MDS  MDS  Bfreq time freq    time featuresCAHL HL HL HL HL singular value order INPUTSlayersdeeperoutHL HL HL HL singular value order NODESlayersdeeper layerHL HL HL HL HL AUC AUCactivation AUCSLT AUCAUCBCVisualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

Figure       Features  red  and corresponding SLT  blue  averaged over the WSJ eval  set for selected classes      Unsupervised
hierarchical clustering for SLT  red  shown with corresponding features  blue  Number of data points in each cluster is shown below
the features and SLT      Comparison of average SLT for selected output nodes  columns  to data points of selected classes  rows 

hierarchical clustering  UPGMA  Euclidean distance  The
clustering of SLTs for one example class  phonemes      
is shown in Figure     In this  gure  the mean SLT of each
group is outlined in blue and the corresponding average
features are outlined in red  below each feature SLT pair
are the number of data points in that cluster 
This clustering analysis shows that the network learns to
emphasize different feature dimensions depending on what
variation of each class is presented to the network  For example  consider the cluster of     highlighted and starred
in purple in Fig      This cluster represents    apped allophone of      such as in the American English pronunciation
of  water  The average SLT for this variation of     shows
absence of power in high frequency bands  compare to the
average features at the top  This cluster has   vastly different spectral pro le than the cluster of     shown above  and
we can see that the corresponding mean SLT for this cluster
emphasizes different parts of the feature space  This analysis thus provides   data driven method to discovering the
variations present in phonemes  which has been the subject
of linguistic debate for decades  Stevens   
In the previous two analyses  we considered the mapping
of data points of each class to the corresponding nodes in
the output  However  it is equally important to form functions that provide evidence against   data point belonging
to other classes  Because one can  nd the SLTs for each
sample to all output nodes  this method makes it possible
to also investigate the resolution of confusable features of
similar classes  Fig     examines these properties through
the visualization of the output nodes corresponding to two
frequently confused class pairs     and         and     and
the average SLT of these nodes to both classes  What we
observe is that for easily confused pairs  there are shared
features  shown in black  but also distinguishing features
between the classes  shown in red  For example  the main
difference between phonemes     and     are the low frequencies  red arrow  which are negatively weighted for
the     node and positively weighted for the     node  This
is consistent with the input features for     and     in Fig 
    where we see the voiced characteristics of     give the
inputs energy in low frequencies 

  Discussion
In this study  we introduce novel techniques for jointly analyzing the internal representation and the transformations
of features that are used by   multilayer perceptron  Using networks trained to map acoustic features of speech to
phoneme categories  we determined the encoding properties of each phoneme by the individual and population of
nodes in hidden layers of the network  We found   progressive nonuniform and nonlinear transformation of the
feature space which increases the discriminant dimensions
of overlapping instances of different classes 
To study how the network achieves categorization of its inputs  we proposed   method by which the exact function
that is applied to every data point can be constructed in
the form of   linear transformation of the input features
 sampledependent linear transform  or SLT  We found
that while the network can potentially learn unique functions for every data point  instead the network applies similar functions to clusters of data points  These clusters of
shared functions exemplify the prototypical variations of
each class  suggesting   computational strategy to explicitly model variability  More generally  analyzing the properties of these functions provides   datadriven feature interpretation method  which can be used for feature selection  discovering dependencies between features  or analyzing the variability of each class 
Overall  our study provides   novel and intuitive account
of how deep neural networks perform classi cation  and
provides   qualitative and quantitative method for comparison of networks with various sizes and architectures  Although the results here are presented in   phoneme recognition task  these methods are easily extensible to other applications trained on alternative datasets or architectures      
very deep networks  bottlenecks  convolutional networks 
or any other network where the activation function is differentiable with respect to the inputs  Moreover  analyzing
the SLTs allows one to investigate the encoding properties
of misclassi ed data points  which can provide intuitions
that aid in devising improved feature extraction and classi 
 cation techniques 

Visualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

References
      Maas        Hannun and Ng        Recti er nonlinearities improve neural network acoustic models  In
International Conference on Machine Learning  Atlanta 
GA   

   Nguyen     Yosinkski and Clune     Deep neural networks are easily fooled  High con dence predictions for
unrecognizable images  In CVPR  Boston  MA   

     Mohamed        Dahl and Hinton     Acoustic modeling using deep belief networks  IEEE Transactions on
Audio  Speech  and Language Processing   
 

     Mohamed     Hinton and Penn     Understanding
how deep belief networks perform acoustic modelling 
In ICASSP  Kyoto  Japan   

   Veit        Wilber and Belongie     Residual networks
behave like ensembles of relatively shallow networks 
In Advances in Neural Information Processing Systems 
Barcelona  Spain   

   Szegedy     Zaremba     Sutskever    Bruna    Erhan
Intriguing properties of

   Goodfellow and Fergus    
neural networks  arXiv   

Cybenko     Approximation by superpositions of   sigmoidal function  Mathematics of Control  Signals  and
Systems     

   Andor     Alberti     Weiss    Severyn    Presta   
Ganchev    Petrov and Collins     Globally normalized
arXiv 
transitionbased neural networks 
 

   Povey     Ghoshal     Boulianne    Burget    Glembek    Goel    Hannemann    Motlicek    Qian   
Schwarz    Silovsky    Stemmer and Vesely     The
In IEEE Workshop
kaldi speech recognition toolkit 
on Automatic Speech Recognition and Understanding
 ASRU  Waikoloa  HI   

   Silver     Huang        Maddison    Guez    Sifre   
van den Driessche    Schrittwieser    Antonoglou    Panneershelvam    Lanctot    Dieleman    Grewe    Nham
   Kalchbrenner    Sutskever    Lillicrap    Leach   
Kavukcuoglu    Graepel and Hassabis     Mastering the
game of go with deep neural networks and tree search 
Nature     

Eldan     and Shamir     The power of depth for feedfor 

ward neural networks  arXiv   

      Dahl     Yu     Deng and Acero     Contextdependent pretrained deep neural networks for largeIEEE Transactions on
vocabulary speech recognition 

Audio  Speech  and Language Processing   
 

      Dahl        Sainath and Hinton       

Improving
deep neural networks for lvcsr using recti ed linear units
In ICASSP  pp    Vancouver 
and dropout 
Canada   

   Hinton     Deng     Yu       Dahl    Mohamed   
Jaitly    Senior    Vanhoucke    Nguyen       Sainath
and Kingsbury     Deep neural networks for acoustic
modeling in speech recognition  the shared views of four
research groups  IEEE Signal Processing Magazine  pp 
   

   Montufar     Pascanu     Cho and Bengio     On the
number of linear regions of deep neural networks 
In
Neural Information Processing Systems  pp    Montreal  Canada   

Ioffe     and Szegedy     Batch normalization  Accelerating deep network training by reducing internal covariate
shift  arXiv   

   Bergstra     Breuleux        Bastien    Lamblin    Pascanu    Desjardins    Turian    WardeFarley and Bengio     Theano    cpu and gpu math compiler in python 
In Proceedings of the Python for Scienti   Computing
Conference  SciPy  Austin  TX   

   Garofolo     Graff     Paul and Pallett     CSRI  WSJ 
Complete  Linguistic Data Consortium  Philadelphia 
 

   Garofolo     Graff     Paul and Pallett     CSRLinguistic Data Consortium 

II  WSJ  Complete 
Philadelphia   

   Yosinski     Clune     Nguyen    Fuchs and Lipson    
Understanding neural networks through deep visualization  arXiv   

   He     Zhang     Ren and Sun     Deep residual learning for image recognition  In The IEEE Conference on
Computer Vision and Pattern Recognition  CVPR  pp 
  Las Vegas  USA   

   Hornik     Stinchcombe and White     Multilayer feedforward networks are universal approximators  Neural
Networks     

Kruskal  Joseph    and Wish  Myron  Multidimensional

Scaling  Sage Publications  Newbury Park   

Ladefoged     and Johson       Course in Phonetics 

Wadsworth Publishing  Boston  MA   

Lin        and Tegmark     Why does deep and cheap

learning work so well  arXiv   

Visualizing and Understanding Multilayer Perceptron Models    Case Study in Speech Processing

      Zeiler     Ranzato     Monga    Mao    Yang     
Le    Nguyen    Senior    Vanhoucke    Dean and Hinton        On recti ed linear units for speech processing 
In ICASSP  pp    Vancouver  Canada   

Mallat     Understanding deep convolutional networks 

Phil  Trans     Soc     

   Srivastava        Hinton     Krizhevsky    Sutskever and
Salakhutdinov     Dropout    simple way to prevent
neural networks from over tting  Journal of Marchine
Learning Research     

   Pascanu     Montufar and Bengio     On the number
of response regions of deep feedforward networks with
piecewise linear activations  arXiv     

   Wang     Mohamed     Caruana    Bilmes    Plilipose
   Richardson    Geras    Urban and Aslan     Analysis of deep neural networks with the extended data jacobian matrix  In International Conference on Machine
Learning  New York  NY   

Stevens        Acoustic Phonetics  The MIT Press   

   Nagamine        Seltzer and Mesgarani     Exploring
how deep neural networks form phonemic categories 
In INTERSPEECH  pp    Dresden  Germany 
 

   Nagamine        Seltzer and Mesgarani     On the
role of nonlinear transformations in deep neural network
acoustic models  In INTERSPEECH  pp    San
Francisco  CA   

van der Maaten         and Hinton       Visualizing
highdimensional data using tsne  Journal of Marchine
Learning Research     

   Xiong     Droppo     Huang    Seide    Seltzer    Stolcke    Yu and Zweig     The microsoft   conversational speech recognition system  arXiv 
 

Zeiler        and Fergus     Visualizing and understanding

convolutional networks  arXiv   

