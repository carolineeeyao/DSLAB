Consistent OnLine OffPolicy Evaluation

Assaf Hallak   Shie Mannor  

Abstract

The problem of online offpolicy evaluation
 OPE  has been actively studied in the last decade
due to its importance both as   standalone problem and as   module in   policy improvement
scheme  However  most Temporal Difference
 TD  based solutions ignore the discrepancy between the stationary distribution of the behavior
and target policies and its effect on the convergence limit when function approximation is applied 
In this paper we propose the Consistent
OffPolicy Temporal Difference  COPTD   
algorithm that addresses this issue and reduces
this bias at some computational expense  We
show that COPTD    can be designed to converge to the same value that would have been obtained by using onpolicy TD  with the target
policy  Subsequently  the proposed scheme leads
to   related and promising heuristic we call logCOP TD    Both algorithms have favorable
empirical results to the current state of the art online OPE algorithms  Finally  our formulation
sheds some new light on the recently proposed
Emphatic TD learning 

  Introduction
Reinforcement Learning  RL  techniques were successfully applied in  elds such as robotics  games  marketing
and more  Kober et al    AlRawi et al    Barrett et al    We consider the problem of offpolicy
evaluation  OPE    assessing the performance of   complex strategy without applying it  An OPE formulation is
often considered in domains with limited sampling capability  For example  marketing and recommender systems
 Theocharous and Hallak    Theocharous et al   
directly relate policies to revenue    more extreme example is drug administration  as there are only few patients in

 The Technion  Haifa 
Correspondence
Assaf Hallak  ifogph gmail com  Shie Mannor

Israel 

to 
 shie ee technion ac il 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

the testing population  and suboptimal policies can have
life threatening effects  Hochberg et al    OPE can
also be useful as   module for policy optimization in   policy improvement scheme  Thomas et al     
In this paper  we consider the OPE problem in an online
setup where each new sample is immediately used to update our current value estimate of some previously unseen
policy  We propose and analyze   new algorithm called
COPTD  for estimating the value of the target policy 
COPTD  has the following properties 

  Easy to understand and implement online 

  Allows closing the gap to consistency such that the
limit point is the same that would have been obtained
by onpolicy learning with the target policy 

  Empirically comparable to stateof the art algorithms 

Our algorithm resembles  Sutton et al     Emphatic
TD that was extended by  Hallak et al    to the general parametric form ETD  We clarify the connection between the algorithms and compare them empirically 
Finally  we introduce an additional related heuristic called
LogCOP TD  and motivate it 

  Notations and Background
We consider the standard discounted Markov Decision Process  MDP  formulation  Bertsekas and Tsitsiklis   
with   single long trajectory  Let                  
be an MDP where   is the  nite state space and   is the
 nite action space  The parameter   sets the transition
probabilities Pr   cid       given the previous state       and
action        where the  rst state is determined by the
distribution   The parameter   sets the reward distribution         obtained by taking action   in state   and   is
the discount factor specifying the exponential reduction in
reward with time  The process advances as follows 
  state    is sampled according to the distribution    
Then  at each time step   starting from       the agent
draws an action at according to the stochastic behavior policy    st    reward rt
 
    st  at  is accumulated by the
agent  and the next state st  is sampled using the transition probability Pr   cid st  at 

Consistent OnLine OffPolicy Evaluation

 cid   cid 

 cid 
 cid cid cid        

The expected discounted accumulated reward starting from
  speci   state and choosing an action by some policy   is
called the value function  which is also known to satisfy the
Bellman equation in   vector form 

          

 trt

     

 
            

  
 
               and        cid 

 
where     
 
    Pr   cid        are the policy induced reward vector
and transition probability matrix respectively     is called
the Bellman operator  The problem of estimating      
from samples is called policy evaluation  If the target policy   is different than the behavior policy   which generated the samples  the problem is called offpolicy evaluation  OPE  The TD   Sutton    algorithm is   standard solution to online onpolicy evaluation  Each time
step the temporal difference error updates the current value
function estimate  such that eventually the stochastic approximation process will converge to the true value function  The standard form of TD  is given by 

  cid 

    

  st  

 irt         Vt st   

  

     

  

  st

 nR   

 

st

 cid 

 cid 

  
 Vt st     Vt st      

  

  st

   Vt st 

 

 cid 

 

 cid 

 cid    cid 

where    is the step size  The value     
  st is an estimate
of the current state      st  looking forward   steps  and
  st is an exponentially weighted average of all of these
  
estimates going forward till in nity  Notice that Equation  
does not specify an online implementation since     
  st depends on future observations  however there exists   compact online implementation using eligibility traces  Bertsekas and Tsitsiklis   for online TD  and Sutton
et al    Sutton et al    for offpolicy TD 
The underlying operator of TD  is given by 

           
   

  

 iP  
                  

  

  

             

 

 

   contraction  Bertsekas   

and is    
We denote by      the stationary distribution over states
induced by taking the policy   and mark      diag   
Since we are concerned with the behavior at in nite horizon  we assume            In addition  we assume that
the MDP is ergodic for the two speci ed policies     so
                           and that the OPE problem
is proper                        

 cid 

When the state space is too large to hold         linear
function approximation scheme is used           cid 
     
where   is the optimized weight vector and     is the feature vector of state   composed of   features  We denote by
    the projection to the subspace spanned by the features
with respect to the   weighted norm  and by     RS   the
matrix whose lines consist of the feature vectors for each
state and assume its columns are linearly independent 
TD  can be adjusted to  nd the  xed point of        
 
 Sutton and Barto   

  cid 

  

    

  st  

 irt        cid 

   st   

     

  

  st

           

 nR   

st

 

   st cid   st 

   cid 

  st

  

 cid 
 cid   
  cid 

  

Finally  we de ne OPErelated quantities 

 
 

  

 at st 
 at st 

    
 

 
 

     

     

 
 

    
    

 

we call    the covariate shift ratio  as denoted under different settings by  Hachiya et al   
We summarize the assumptions used in the proofs 

  For both policies the induced Markov chain is ergodic 

  The  rst state    is distributed according to the sta 

tionary distribution of the behavior policy     
  The problem is proper                       
  The feature matrix   has full rank   

Assumption   is commonly used for convergence theorems
as it veri es the value function is well de ned on all states
regardless of the initial sampled state  Assumption   can be
relaxed since we are concerned with the longterm properties of the algorithm past its mixing time   we require it for
clarity of the proofs  Assumption   is required so the importance sampling ratios will be well de ned  Assumption
  guarantees the optimal   is unique which greatly simpli 
 es the proofs 

  Previous Work
We can roughly categorize previous OPE algorithms to
two main families  Gradient based methods that perform
stochastic gradient descent on error terms they want to minimize  These include GTD  Sutton et al      GTD 

Consistent OnLine OffPolicy Evaluation

to see that the stationary distributions are given by 

      

 

      

 cid       

 cid  

     

 cid  

 cid       

     

 

TDC  Sutton et al      and HTD  White and White 
  The main disadvantages of gradient based methods
are     they usually update an additional error correcting
term  which means another timestep parameter needs to
be controlled  and     they rely on estimating nontrivial
terms  an estimate that tends to converge slowly  The other
family uses importance sampling  IS  methods that correct
the gains between onpolicy and offpolicy updates using
the ISratios       Among these are full IS  Precup et al 
  and ETD   Sutton et al    These methods are characterized by the biasvariance tradeoff they resort to   navigating between biased convergent values  or
even divergent  and very slow convergence stemming from
the high variance of IS correcting factors  the    products 
There are also   few algorithms that fall between the two 
for example TOGTD  van Hasselt et al    and WISTD   Mahmood and Sutton   
  comparison of these algorithms in terms of convergence
rate  synergy with function approximation and more is
available in  White and White    Geist and Scherrer 
  We focus in this paper on the limit point of the
convergence  For most of the aforementioned algorithms 
the process was shown to converge almost surely to the
 xed point of the projected Bellman operator  dT  where
  is some stationary distribution  usually    however the
  in question was never     as we would have obtained
from running onpolicy TD with the target policy  also
see  Kolter    for relevant discussion  The algorithm
achieving the closest result is ETD  which replaced  
   where   tradesoff some of the
process  variance with the bias in the limit point  Hence 
our main contribution is   consistent algorithm which can
converge to the same value that would have been obtained
by running an onpolicy scheme with the same policy 

with    cid         cid 

 cid 

 

  Motivation
Here we provide   motivating example showing that even
in simple cases with  close  behavior and target policies 
the two induced stationary distributions can differ greatly 
Choosing   speci   linear parameterization further emphasizes the difference between applying onpolicy TD with
the target policy  and applying inconsistent offpolicy TD 
Assume   chain MDP with numbered states        
where from each state   you can either move left to state
      or right to state       If you ve reached the beginning or the end of the chain  states   or     then taking  
step further does not affect your location  Assume the behavior policy moves left with probability       while the
target policy moves right with probability       It is easy

 Except full IS  however its variance is too high to be applica 

ble in practice 

For instance  if we have   length   chain with    
  for the rightmost state we have           
           Let   set the reward to be   for
the right half of the chain  so the target policy is better
since it spends more time in the right half  The value of
the target policy in the edges of the chain for       is
               
Now what happens if we try to approximate the value function using one constant feature         The  xed point
of        is       while the  xed point of       
is           substantial difference  The reason for
this difference lies in the emphasis each projection puts on
the states  according to     the important states are in the
left half of the chain   these with low value function  and
therefore the value estimation of all states is low  However 
according to     the important states are concentrated on
the right part of the chain since the target policy will visit
these more often  Hence  the estimation error is emphasized on the right part of the chain and the value estimation
is higher  When we wish to estimate the value of the target
policy  we want to know what will happen if we deploy it
instead of the behavior policy  thus taking the  xed point of
       better represents the offpolicy evaluation solution 

  cid 

  

  COPTD   
Most offpolicy algorithms multiply the TD summand of
TD  with some value that depends on the history and the
current state  For example  full ISTD by  Precup et al 
  examines the ratio between the probabilities of the
trajectory under both policies 

                   st  at 
                   st  at 

 

       

    

 

In problems with   long horizon  or these that start from the
stationary distribution  we suggest using the timeinvariant
covariate shift    multiplied by the current     The intuition is the following  We would prefer using the probabilities ratio given in Equation   but it has very high variance 
and after many time steps we might as well look at the stationary distribution ratio instead  This direction leads us to
the following update equations 

     

 cid rt    cid 
Lemma   If the    satisfy cid 

          st  

   st     st cid   st 
         cid 

 
     
  converges almost

    

then the process described by Eq 
surely to the  xed point of           

Consistent OnLine OffPolicy Evaluation

The proof follows the ODE method  Kushner and Yin 
  similarly to Tsitsiklis and Van Roy    see the
appendix for more details 
Since       is generally unknown  it is estimated using an
additional stochastic approximation process  In order to do
so  we note the following Lemma 

Lemma   Let  cid   be an unbiased estimate of     and for

   cid   st    

 

    Then 

every                   de ne   
 
   st

  

 cid  

 cid 

     st 

For any state st there are       such quantities   
    
where we propose to weight them similarly to TD 

  

 cid 

         
 

      

 

 

  

Note that       unlike       is restricted to   close set
since its   weighted linear combination is equal to   and
all of its entries are nonnegative  We denote this   
weighted simplex by     and let     be the  nonlinear 
projection to this set with respect to the Euclidean norm
    can be calculated ef ciently   Chen and Ye   
Now  we can devise   TD algorithm which estimates    and
uses it to  nd   which we call COPTD     Consistent
OffPolicy TD 

Algorithm   COPTD  Input     cid   

                      

 

               

Observe st  at  rt  st 
Update normalization terms 

  Init           
  for           do
 
 
     st       st     
 
 
 
 

  
   

   cid 
  
 

     
     weighted average 

  
       
Update   
Ft      Ft    est 
Update   project by      TD error 

 cid     st 
 cid cid         

   cid    
 cid   cid cid   cid 
 cid 
 cid             
  est
              cid     st     st 
   st     st 

Offpolicy TD 
     rt    cid 

    

 

 

 
 
 
 
  end for

Similarly to the Bellman operator for TDlearning  we de 
 ne the underlying COPoperator   and its   extension 

     cid 

        
      
              
     cid 

           cid 

       

The following Lemma may give some intuition on the convergence of the    estimation process 
Lemma   Under the ergodicity assumption  denote the
eigenvalues of    by                   Then     is
   
  maxi cid 
       contraction in the   norm on the
orthogonal subspace to     and    is    xed point of    

The technical proof is given in the appendix 

Theorem   If the step sizes satisfy cid 
 cid 
  cid    
 cid      for some constant   and every  
   
      and
and    then after applying COPTD     cid     converges

        cid 

  
      
   st

          

to    almost surely  and    converges to the  xed point of
      

       

    

  
 

Notice that COPTD    given in Alg    is infeasible in
problems with large state spaces since           Like
TD  we can introduce linear function approximation 
represent          cid 
      where   is   weight vector and     is the offpolicy feature vector and adjust

the algorithm accordingly  For  cid   to still be contained in
         cid noted as
 cid  In practice  the latter
requirement can be approximated   cid 
 cid 
rates nonzero  cid similarly to ETD cid 

the set     we pose the requirement on the feature vectors        Rk
the simplex projection        
       
   cid 
   st      resulting in an extension of the previ 
 
ously applied    estimation  step   in COPTD    We
provide the full details in Algorithm   which also incorpo 

  and cid 

      cid 

      cid 

 

      

            

Algorithm   COPTD  with Function Approximation 
Input     
  Init           
  for           do
 
 
 
 
 
 
 

Observe st  at  rt  st 
Update normalization terms 
  
       
                 st 
Update   
     weighted average 
Ft      Ft     st 
Update   project by      TD error 
     cid 
  
  
 
           
Offpolicy TD 

 cid 
 cid       

   st cid 

   st 

 cid  Ft

        

   

    

 

 
 
  Mt            cid 
 
 
 
  end for

et       et   Mt st 
     rt    cid 
              tet

   st 
   st     st 

Theorem   If the step sizes satisfy cid 
 cid 
  cid    

 cid      for some constant   and every      

        cid 

   
      and

          

  
      
   st

       

    

  
 

Consistent OnLine OffPolicy Evaluation

 

 

almost surely 
     

   cid     converges to the  xed

then after applying COPTD    with function approximation satisfying       Rk
point of           denoted by  COP
and if    converges it is to the  xed point of    COP
where   is   coordinatewise product of vectors 
The proof is given in the appendix and also follows the
ODE method  Notice that   theorem is only given for    
  convergence results for general   should follow the work
by Yu  
  possible criticism on COPTD  is that it is not actually consistent  since in order to be consistent the original
state space has to be small  in which case every offpolicy
algorithm is consistent as well  Still  the dependence on
another set of features allows to tradeoff accuracy with
computational power in estimating    and subsequently    
Moreover  smart feature selection may further reduce this
gap  and COPTD    is still the  rst algorithm addressing this issue  We conclude with linking the error in     
estimate with the difference in the resulting   which suggests that   well estimated    results in consistency 
Corollary   Let           If           COP
       
    then the  xed point of COPTD  with function
approximation  COP satis es the following  where  cid     cid  is
the    induced norm 

 cid     COP cid   

   cid cid cid Rmax        cid cid cid COP cid cid   

 cid   

where       cid           and   sets the  xed point
of the operator          

  Relation to ETD   

Recently  Sutton et al    had suggested an algorithm
for offpolicy evaluation called Emphatic TD  Their algorithm was later on extended by Hallak et al    and renamed ETD    which was shown to perform extremely
well empirically by White and White   ETD   
can be represented as 

 cid 

Ft        

  
            tFt  

    
   

 cid rt    cid 

   st     st cid   

 

As mentioned before  ETD    converges to the  xed
   Yu    where        Ft st        
point of       
      Error bounds can be achieved by showing that
the operator       
  is   contraction under certain requirements on   and that the variance of Ft is directly related to
  as well  Hallak et al     and thus affects the convergence rate of the process 
When comparing ETD   form to COPTD   
instead of spending memory and time resources on  

state featuredependent Ft  ETD  uses   onevariable
approximation  The resulting Ft is in fact   onestep esti 

mate of     starting from  cid           see Equations    

up to   minor difference    ETD
     COPTD
following our logic adds bias to the estimate  
Unlike ETD    COPTD   effectiveness depends
on the available resources  The number of features    
can be adjusted accordingly to provide the most affordable
approximation  The added cost is  netuning another stepsize  though    effect is less prominent 

     which

 

 

  The Logarithm Approach for Handling

Long Products

We now present   heuristic algorithm which works similarly to COPTD    Before presenting the algorithm 
we explain the motivation behind it 

  Statistical Interpretation of TD 

st    Each Rn

Konidaris et al    suggested   statistical interpretation of TD  They show that under several assumptions
st is the maximum likelihood estithe TD  estimate   
st is an unbiased
mator of    st  given Rn
st are inestimator of    st    The random variables Rn
dependent and speci cally uncorrelated    The random
variables Rn
st are jointly normally distributed  and   The
variance of each Rn
Under Assumptions   the maximum likelihood estimator
of       given its previous estimate can be represented as  
linear convex combination of Rn

st is proportional to    

st with weights 

 cid 
 cid 

 cid 

 cid 

Var

    
st

 cid cid 
 cid cid   

 cid 

wn  

Var

    

st

  

Subsequently  in Konidaris et al    Assumption   was
relaxed and instead   closed form approximation of the
variance was proposed  In   followup paper by Thomas
et al      the second assumption was also removed
 cid cov Rst  en
and the weights were instead given as  wn  
 cid cov Rst    
where the covariance matrix can be estimated from the
data  or otherwise learned through some parametric form 
While both the approximated variance and learned covariance matrix solutions improve performance on several
benchmarks  the  rst uses   rather crude approximation 
and the second solution is both statedependent and based
on noisy estimates of the covariance matrix  In addition 
there aren   ef cient online implementations since all past

 We have conducted several experiments with an altered ETD
and indeed obtained better results compared with the original 
these experiments are outside the scope of the paper 

Consistent OnLine OffPolicy Evaluation

weights should be recalculated to match   new sample 
Still  the suggested statistical justi cation is   valuable tool
in assessing the similar role of   in ETD   

  Variance Weighted   
 

As was shown by Konidaris et al    we can use statedependent weights instead of   exponents to obtain better estimates  The second moments are given explicitly as
    cid   

follows    cid 
 cid 
         

 cid 
     st
           cid      

     est
  st 

 cid    

  where

 cid 

  

  cid 

 

 

These can be estimated for each state separately  Notice
that the variances increase exponentially depending on the
largest eigenvalue of     as Assumption   dictates  but this
is merely an asymptotic behavior and may be relevant only
when the weights are already negligible  Hence  implementing this solution online should not be   problem with
the varying weights  as generally only the  rst few of these
are nonzero  While this solution is impractical in problems with large state spaces parameterizing or approximating these variances  similarly to Thomas et al     
could improve performance in speci   applications 

  LogCOP TD   

Assumption   in the previous section is that the sampled
    are normally distributed  For on polestimators         
icy TD  this assumption might seem not too harsh as the
estimators      represent growing sums of random variables  However  in our case the estimators   
  are growing
products of random variables  To correct this issue we can
de ne new estimators using   logarithm on each   
   

log    st    log

 cid cid 

 cid cid  st

  

 cid 

 cid cid   st   
  log  cid   st     

 

  cid 
  cid 

     

     

   log    st   

 

cannot expect the estimated value to converge  so we propose using an arti cial one  log  We can incorporate function approximation for this formulation as well  Unlike
COPTD    we can choose the features and weights as
we wish with no restriction  besides the linear constraint
on the resulting    through the weight vector   This
can be approximately enforced by normalizing   using
   st   which should equal   if we
 
 
were exactly correct  We call the resulting algorithm LogCOP TD 

 cid 
  exp cid 

 
   
 

     

Algorithm   LogCOP TD  with Function Approximation  Input   
  Init                         
  for           do
 
 
 

            log     

Observe st  at  rt  st 
Update normalization terms 
  
 
 st          exp cid 
Update log  
Ft    logFt      
Update   project by log     TD error 
    Ft
  
  
  
 
 
    
             
Offpolicy TD 

   st 
     weighted average 
  log st 
   st 
   st 

 cid 
   st cid       

  Mt             exp cid cid 

 cid    

 
 
 
 

   cid 

 
 

  

et       et   Mt st 
     rt    cid 
              tet

   st     st 

 
 
 
  end for

  Using the Original Features

An interesting phenomenon occurs when the behavior and
target policies employ   feature based Boltzmann distribu 

tion for choosing the actions          exp cid cid 
and         exp cid cid 

    cid 
    cid  where   constant feature is

added to remove the  possibly different  normalizing constant  Thus  log               cid st  and LogCOP TD  obtains   parametric form that depends on
the original features instead of   different set 

This approximation is crude   we could add terms reducing the error through Taylor expansion  but these would
be complicated to deal with  Hence  we can relate to this
method mainly as   wellmotivated heuristic 
Notice that this formulation resembles the standard MDP
formulation  only with the corresponding  reward  terms
log    going backward instead of forward  and no discount factor  Unfortunately  without   discount factor we

 The covariances can be expressed analytically as well  for

clarity we drop this immediate result 

  Approximation Hardness

As we propose to use linear function approximation for
      and log       one cannot help but wonder how hard
it is to approximate these quantities  especially compared
to the value function  The comparison between       and
      is problematic for several reasons 

  The ultimate goal is estimating       approximation

errors in       are second order terms 

  The value function       depends on the policy 

Consistent OnLine OffPolicy Evaluation

induced reward function and transition probability
matrix  while       depends on the stationary distributions induced by both policies  Since each depends
on at least one distinct factor   we can expect different
setups to result in varied approximation hardness  For
example  if the reward function has   poor approximation then so will       while extremely different
behavior and target policies can cause       to behave
erratically 

  Subsequently  the choice of features for approximating       and       can differ signi cantly depending on the problem at hand 

If we would still like to compare       and       we
could think of extreme examples 

  When                 when          then

         

  In the chain MDP example in Section   we saw that
      is an exponential function of the location in the
chain  Setting reward in one end to   will result in an
exponential form for       as well  Subsequently  in
the chain MDP example approximating log       is
easier than       as we obtain   linear function of the
position  This is not the general case 

  Experiments
We have performed   types of experiments  Our  rst batch
of experiments  Figure   demonstrates the accuracy of predicting    by both COPTD    and LogCOP TD   
We show two types of setups in which visualization of   
is relatively clear   the chain MDP example mentioned in
Section   and the mountain car domain  Sutton and Barto 
  in which the state is determined by only two continuous variables   the car   position and speed  The parameters   and   exhibited low sensitivity in these tasks so
they were simply set to   we show the estimated    after
  iterations  For the chain MDP  top two plots  notice
the logarithmic scale  we  rst approximate    without any
function approximation  topleft  and we can see COPTD
manages to converge to the correct value while LogCOP 
TD is much less exact  When we use linear feature space
 constant parameter and position  LogCOP TD captures
the true behavior of    much better as expected  The two
lower plots show the error  in color  in    estimated for the
mountain car with   pure exploration behavior policy vs 
  target policy oriented at moving right  The zaxis is the
same for both plots and it describes   much more accurate
estimate of    obtained through simulations  The features
used were local state aggregation  We can see that both
algorithms succeed similarly on the positionspeed pairs
which are sampled often due to the behavior policy and the

Figure   Estimation quality of COPTD and LogCOP TD in the
chain MDP  top  and mountain car  bottom  problems  The chain
MDP plots differ by the function approximation and the shading
re ects one standard deviation over   trajectories  The mountain
car plots compare COPTD with LogCOP TD where the zaxis
is the same  true     with the colors specifying the error 

mountain  When looking at more rarely observed states 
the estimate becomes worse for both algorithms  though
LogCOP TD seems to be better performing on the spike
at position    
Next we test the sensitivity of COPTD    and LogCOP TD  to the parameters   and  log  Figure   on
two distinct toy examples   the chain MDP introduced before but with only   states with the positionlinear features  and   random MDP with   states    actions and  
 bit binary feature vector along with   free parameter  this
compact representation was suggested by White and White
  to approximate real world problems  The policies
on the chain MDP were taken as described before  and on
the random MDP   state independent   probability to choose an action by the behavior target policy  As
we can see  larger values of   cause noisier estimations in
the random MDP for COPTD    but has little effect in
other venues  As for  log   we can see that if it is too large or
too small the error behaves suboptimally  as expected for
the crude approximation of Equation   In conclusion  unlike ETD    Log COPTD    are much less effected
by   though  log should be tuned to improve results 
Our  nal experiment  Figure   compares our algorithms
to ETD    and GTD    over   setups  chain MDP
with   states with right half rewards   with linear features      action random MDP with   states and binary
features  acrobot   actions  and cartpole balancing   actions   Sutton and Barto    with reset at success and
state aggregation to   states 
In all problems we used
the same features for    and       estimation       
constant step size   for the TD process and results were
averaged over   trajectories  other parameters     other
step sizes   log  were swiped over to  nd the best ones  To

 State dChain MDP     states  no func  approx      dCOP dLogCOP State dChain MDP     states  linear func  approx      dCOP dLogCOP  PositionMountain car    estimation   COPSpeed     PositionMountain car    estimation   logCOPSpeed    Consistent OnLine OffPolicy Evaluation

Figure   The effect of    log on COPTD  and LogCOP 
TD  the yaxis is      estimation sum of squared errors
 SSE  over all states 

Figure   Error over time of several online offpolicy algorithms 

reduce  gure clutter we have not included standard deviations though the noisy averages still re ect the variance in
the process  Our method of comparison on the  rst   setups
estimates the value function using the suggested algorithm 
and  nds the    weighted average of the error between  
and the onpolicy  xed point      

 cid 

 cid 

 cid 

 cid            cid 

  

 

    

     cid   

 

 

where   is the optimal   obtained by onpolicy TD using
the target policy  On the latter continuous state problems
we applied online TD on   different trajectory following
the target policy  used the resulting   value as ground truth
and taken the sum of squared errors with respect to it  The
behavior and target policies for the chain MDP and random
MDP are as speci ed before  For the acrobot problem the
behavior policy is uniform over the   actions and the target
policy chooses between these with probabilities    
   
For the cartpole the action space is divided to   actions
from   to   equally  the behavior policy chooses among
these uniformly while the target policy is   times more
prone to choosing   positive action than   negative one 
The experiments show that COPTD    and LogCOP 
TD    have comparable performance to ETD   
where at least one is better in every setup  The advantage
in the new algorithms is especially seen in the chain MDP
corresponding to   large discrepancy between the stationary distribution of the behavior and target policy  GTD 
is consistently worse on the tested setups  this might be due
to the large difference between the chosen behavior and target policies which affects GTD  the most 

     

     

  Conclusion
Research on offpolicy evaluation has  ourished in the last
decade  While   plethora of algorithms were suggested so
far  ETD    by Hallak et al    has perhaps the simplest formulation and theoretical properties  Unfortunately 
ETD    does not converge to the same point achieved by
online TD when linear function approximation is applied 
We address this issue with COPTD  and proved it
can achieve consistency when used with   correct set of
features  or at least allow tradingoff some of the bias by
adding or removing features  Despite requiring   new set
of features and calibrating an additional update function 
COPTD   performance does not depend as much on
  as ETD  and shows promising empirical results 
We offer   connection to the statistical interpretation of
TD  that motivates our entire formulation  This interpretation leads to two additional approaches      weight the
  using estimated variances instead of   exponents and
  
    approximating log    instead of     both approaches
deserve consideration when facing   real application 

  Acknowledgments
This Research was supported in part by the Israel Science Foundation  grant No    and by the European Research Council under the European Union   Seventh Framework Programme  FP  ERC Grant
Agreement   

    SSE  COP TD  sweep states Chain MDP    Random MDP   states                          SSE  Log COP TD   sweep                           TimeSSE  Log COP TD log sweep    Time   log    log    log    log    log       ErrorChain MDP  ETDGTDCOP TDLog COP TD    Random MDP    ErrorTimeAcrobot    TimeCart poleConsistent OnLine OffPolicy Evaluation

References
Hasan AA AlRawi  Ming Ann Ng  and KokLim Alvin
Yau  Application of reinforcement learning to routing in
distributed wireless networks    review  Arti cial Intelligence Review     

Enda Barrett  Enda Howley  and Jim Duggan  Applying
reinforcement learning towards automating resource allocation and application scalability in the cloud  Concurrency and Computation  Practice and Experience   
   

   Bertsekas  Dynamic Programming and Optimal Con 

trol  Vol II  Athena Scienti     th edition   

   Bertsekas and    Tsitsiklis  NeuroDynamic Program 

ming  Athena Scienti     

   Bertsekas and    Yu  Projected equation methods for
approximate solution of large linear systems  Journal
of Computational and Applied Mathematics   
   

Shalabh Bhatnagar  Vivek   Borkar  and LA Prashanth 
Adaptive feature pursuit  Online adaptation of features
in reinforcement learning  Reinforcement Learning and
Approximate Dynamic Programming for Feedback Control  pages    

Shalabh Bhatnagar  Vivek   Borkar  and KJ Prabuchandran  Feature search in the grassmanian in online reinforcement learning  IEEE Journal of Selected Topics
in Signal Processing     

Wendelin   ohmer  Steffen Gr unew alder  Yun Shen  Marek
Musial  and Klaus Obermayer  Construction of approximation spaces for reinforcement learning  Journal of
Machine Learning Research     

Justin   Boyan  Leastsquares temporal difference learn 

ing  In ICML  pages    

Steven   Bradtke and Andrew   Barto  Linear leastsquares
algorithms for temporal difference learning  Machine
learning     

Yunmei Chen and Xiaojing Ye  Projection onto   simplex 

arXiv preprint arXiv   

Christoph Dann  Gerhard Neumann  and Jan Peters  Policy
evaluation with temporal differences    survey and comparison  Journal of Machine Learning Research   
   

Dotan Di Castro and Shie Mannor  Adaptive bases for reinforcement learning  Machine Learning and Knowledge
Discovery in Databases  pages    

Amir   Farahmand  Mohammad Ghavamzadeh  Shie
Mannor  and Csaba Szepesv ari  Regularized policy iteration  In Advances in Neural Information Processing
Systems  pages    

Clement Gehring  Yangchen Pan  and Martha White  Incremental truncated lstd  arXiv preprint arXiv 
 

Matthieu Geist and Bruno Scherrer    penalized projected
bellman residual  In European Workshop on Reinforcement Learning  pages   Springer   

Matthieu Geist and Bruno Scherrer  Offpolicy learning
with eligibility traces    survey  The Journal of Machine
Learning Research     

Matthieu Geist  Bruno Scherrer  Alessandro Lazaric  and
Mohammad Ghavamzadeh    dantzig selector approach to temporal difference learning  arXiv preprint
arXiv   

Mohammad Ghavamzadeh  Alessandro Lazaric  Odalric
Maillard  and   emi Munos  Lstd with random projecIn Advances in Neural Information Processing
tions 
Systems  pages    

Sertan Girgin and Philippe Preux  Basis expansion in natural actor critic methods  In European Workshop on Reinforcement Learning  pages   Springer   

Arash Givchi and Maziar Palhang  Offpolicy temporal difference learning with distribution adaptation in fast mixing chains  Soft Computing  pages    

Hirotaka Hachiya and Masashi Sugiyama  Feature selection for reinforcement learning  Evaluating implicit
statereward dependency via conditional mutual information  Machine Learning and Knowledge Discovery
in Databases  pages    

Hirotaka Hachiya  Masashi Sugiyama  and Naonori Ueda 
Importanceweighted leastsquares probabilistic classi 
 er for covariate shift adaptation with application to human activity recognition  Neurocomputing   
 

Assaf Hallak  Aviv Tamar  Remi Munos  and Shie
Mannor  Generalized emphatic temporal difference
arXiv preprint
learning  Biasvariance analysis 
arXiv   

Irit Hochberg  Guy Feraru  Mark Kozdoba  Shie Mannor 
Moshe Tennenholtz  and Elad YomTov  Encouraging
physical activity in patients with diabetes through automatic personalized feedback via reinforcement learning
improves glycemic control  Diabetes care     
    

Consistent OnLine OffPolicy Evaluation

Matthew   Hoffman  Alessandro Lazaric  Mohammad
Ghavamzadeh  and   emi Munos  Regularized least
squares temporal difference learning with nested    and
In European Workshop on Reinforcel  penalization 
ment Learning  pages   Springer   

Jeff Johns and Sridhar Mahadevan  Constructing basis
functions from directed graphs for value function apIn Proceedings of the  th international
proximation 
conference on Machine learning  pages   ACM 
 

Jeffrey Johns  Christopher PainterWake eld  and Ronald
Parr  Linear complementarity for regularized policy
evaluation and improvement  In Advances in neural information processing systems  pages    

Takafumi Kanamori  Shohei Hido  and Masashi Sugiyama 
  leastsquares approach to direct importance estimation  Journal of Machine Learning Research   Jul 
   

Jens Kober    Andrew Bagnell  and Jan Peters  Reinforcement learning in robotics    survey  The International
Journal of Robotics Research  page  
 

  Zico Kolter  The  xed points of offpolicy TD  In NIPS 

 

  Zico Kolter and Andrew   Ng  Regularization and feature
selection in leastsquares temporal difference learning 
In Proceedings of the  th annual international conference on machine learning  pages   ACM   

DeRong Liu  HongLiang Li  and Ding Wang  Feature selection and feature learning for highdimensional batch
reinforcement learning    survey  International Journal
of Automation and Computing     

Manuel Loth  Manuel Davy  and Philippe Preux  Sparse
In Approxitemporal difference learning using lasso 
mate Dynamic Programming and Reinforcement Learning    ADPRL   IEEE International Symposium
on  pages   IEEE   

Sridhar Mahadevan  Samuel meets amarel  Automating
value function approximation using global state space
analysis  In AAAI  volume   pages    

Sridhar Mahadevan and Bo Liu  Sparse qlearning with
mirror descent  arXiv preprint arXiv   

Sridhar Mahadevan and Mauro Maggioni  Protovalue
functions    laplacian framework for learning representation and control in markov decision processes  Journal of Machine Learning Research   Oct 
 

Sridhar Mahadevan et al  Learning representation and control in markov decision processes  New frontiers  Foundations and Trends   cid  in Machine Learning   
   

  Rupam Mahmood and Richard   Sutton  Offpolicy
learning based on weighted importance sampling with
linear computational complexity  In Conference on Uncertainty in Arti cial Intelligence   

  Rupam Mahmood  Hado   van Hasselt  and Richard  
Sutton  Weighted importance sampling for offpolicy
In Adlearning with linear function approximation 
vances in Neural Information Processing Systems  pages
   

Ishai Menache  Shie Mannor  and Nahum Shimkin  Basis
function adaptation in temporal difference reinforcement
learning  Annals of Operations Research   
   

Reevaluating complex backups
In Advances

George Konidaris  Scott Niekum  and Philip   Thomas 
in
in
Information Processing Systems   pages
URL

Tdgamma 
temporal difference learning 
Neural
  Curran Associates 
http papers nips cc paper td 
gammare evaluatingcomplex backupsin temporaldifference learning 
pdf 

Inc   

Mark Kroon and Shimon Whiteson  Automatic feature selection for modelbased reinforcement learning in facIn Machine Learning and Applications 
tored mdps 
  ICMLA  International Conference on  pages
  IEEE   

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei   Rusu  Joel Veness  Marc   Bellemare  Alex
Graves  Martin Riedmiller  Andreas   Fidjeland  Georg
Ostrovski  et al  Humanlevel control through deep reinforcement learning  Nature     

Harold Kushner and   George Yin  Stochastic approximation and recursive algorithms and applications  volume   Springer Science   Business Media   

Christopher PainterWake eld and Ronald Parr  Greedy
arXiv

algorithms for sparse reinforcement learning 
preprint arXiv   

Bo Liu  Sridhar Mahadevan  and Ji Liu  Regularized offpolicy tdlearning  In Advances in Neural Information
Processing Systems  pages    

Ronald Parr  Christopher PainterWake eld  Lihong Li 
and Michael Littman  Analyzing feature generation
In Proceedings of
for valuefunction approximation 

Consistent OnLine OffPolicy Evaluation

the  th international conference on Machine learning 
pages   ACM   

Ronald Parr  Lihong Li  Gavin Taylor  Christopher PainterWake eld  and Michael   Littman  An analysis of linear
models  linear valuefunction approximation  and feature selection for reinforcement learning  In Proceedings
of the  th international conference on Machine learning  pages   ACM   

Marek Petrik  An analysis of laplacian methods for value
function approximation in mdps  In IJCAI  pages  
   

Marek Petrik  Gavin Taylor  Ron Parr  and Shlomo Zilberstein  Feature selection using regularization in approximate linear programs for markov decision processes 
arXiv preprint arXiv   

Doina Precup  Richard   Sutton  and Sanjoy Dasgupta 
Offpolicy temporaldifference learning with function
approximation  In ICML   

Zhiwei Qin  Weichang Li  and Firdaus Janoos  Sparse reinforcement learning via convex optimization  In Proceedings of the  st International Conference on Machine
Learning  ICML  pages    

Zeev Schuss and Vivek   Borkar  Stochastic approxima 

tion    dynamical systems viewpoint   

William   Smart  Explicit manifold representations for
valuefunction approximation in reinforcement learning 
In ISAIM   

Yi Sun  Mark Ring    urgen Schmidhuber  and Faustino  
Incremental basis construction from tempoGomez 
In Proceedings of the  th Interral difference error 
national Conference on Machine Learning  ICML 
pages    

      Sutton and    Barto  Reinforcement learning  An

introduction  Cambridge Univ Press   

      Sutton        Mahmood  and   White  An emphatic approach to the problem of offpolicy temporaldifference learning  arXiv   

Rich Sutton  Ashique   Mahmood  Doina Precup  and
Hado   Hasselt    new    lambda  with interim forward view and monte carlo equivalence  In Proceedings
of the  st International Conference on Machine Learning  ICML  pages    

Richard   Sutton  Hamid   Maei  and Csaba Szepesv ari 
  convergent      temporaldifference algorithm for
offpolicy learning with linear function approximation 
In Advances in neural information processing systems 
pages      

Richard   Sutton  Hamid Reza Maei  Doina Precup  Shalabh Bhatnagar  David Silver  Csaba Szepesv ari  and Eric
Wiewiora  Fast gradientdescent methods for temporaldifference learning with linear function approximation 
In Proceedings of the  th Annual International Conference on Machine Learning  pages   ACM 
   

Georgios Theocharous and Assaf Hallak  Lifetime value
marketing using reinforcement learning  RLDM  
page    

Georgios Theocharous  Philip   Thomas  and Mohammad
Ghavamzadeh  Personalized ad recommendation systems for lifetime value optimization with guarantees 
In Proceedings of the TwentyFourth International Joint
Conference on Arti cial Intelligence  IJCAI   

Philip Thomas  Georgios Theocharous  and Mohammad
Ghavamzadeh  High con dence policy improvement 
In Proceedings of the  nd International Conference on
Machine Learning  ICML  pages      

Information Processing

Systems

 

Philip   Thomas  Scott Niekum  Georgios Theocharous 
Policy evaluation usin NeuIn Advances
pages
Curran
   
http papers nips cc paper 

and George Konidaris 
ing the omegareturn 
ral
 
URL
 policyevaluation usingthe return 
pdf 

Associates 

Inc 

John   Tsitsiklis and Benjamin Van Roy  An analysis of
temporaldifference learning with function approximation  Automatic Control  IEEE Transactions on   
   

Hado van Hasselt    Rupam Mahmood  and Richard   Sutton  Offpolicy td   with   true online equivalence  In
Proceedings of the  th Conference on Uncertainty in
Arti cial Intelligence  Quebec City  Canada   

Jian Wang  Zhenhua Huang  and Xin Xu    novel approach for constructing basis functions in approximate
In Adapdynamic programming for feedback control 
tive Dynamic Programming And Reinforcement Learning  ADPRL    IEEE Symposium on  pages  
IEEE   

Richard   Sutton  Learning to predict by the methods
of temporal differences  Machine learning   
 

Adam White and Martha White 

Investigating practical  linear temporal difference learning  arXiv preprint
arXiv   

Consistent OnLine OffPolicy Evaluation

Dean   Wookey and George   Konidaris  Regularized feature selection in reinforcement learning  Machine Learning     

Dean Stephen Wookey  Representation discovery using  
 xed basis in reinforcement learning  PhD thesis  University of the Witwatersrand South Africa   

   Yu  On convergence of emphatic temporaldifference

learning  In COLT   

Huizhen Yu  Convergence of least squares temporal difference methods under general conditions  In Proceedings
of the  th International Conference on Machine Learning  ICML  pages    

Tom Zahavy  Nir BenZrihem  and Shie Mannor  Graying the black box  Understanding dqns  arXiv preprint
arXiv   

