Composing Tree Graphical Models with Persistent Homology Features

for Clustering MixedType Data

Xiuyan Ni   Novi Quadrianto     Yusu Wang   Chao Chen  

Abstract

Clustering data with both continuous and discrete attributes is   challenging task  Existing
methods often lack   principled probabilistic formulation 
In this paper  we propose   clustering method based on   treestructured graphical model to describe the generation process of
mixedtype data  Our treestructured model factorizes into   product of pairwise interactions 
and thus localizes the interaction between feature
variables of different types  To provide   robust
clustering method based on the treemodel  we
adopt   topographical view and compute peaks of
the density function and their attractive basins for
clustering  Furthermore  we leverage the theory
from topology data analysis to adaptively merge
trivial peaks into large ones in order to achieve
meaningful clusterings  Our method outperforms
stateof theart methods on mixedtype data 

  Introduction

Clustering is one of the most widely used techniques
in data analysis  Xu   Wunsch    Jain    Despite   rich literature on pure continuous data or pure categorical data  the clustering problem remains challenging
for mixedtype data       data with both types of attributes
 Everitt et al    Mixedtype data are ubiquitous in
real world domains       social science  biomedicine and
 nance  where categorical attributes often describe demographic information or questionnaire responses  and continuous attributes often correspond to quantitative measurements  However  only   very limited number of clustering
methods have been proposed for such data  Everitt et al 
  Huang    The major challenge is the lack of  
good geometric intuition of data on the mixedtype domain 

 City University of New York  CUNY  New York  USA
 University of Sussex  Falmer  United Kingdom  National Research University Higher School of Economics  Moscow  Russia  Ohio State University  Columbus  USA  Correspondence to 
Chao Chen  chao chen cchen gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

such intuition is the important basis of many successful
geometric clustering methods       kmeans  MacQueen
et al    Ward   method  Ward Jr    DBSCAN
 Ester et al    to name   few  In practice  what usually being done is to convert mixedtype data to either pure
continuous or pure categorical domain  and subsequently
use existing geometric clustering methods    metric for directly dealing with mixedtype data is also available  based
on Gower   coef cient   The uptake of geometric
clustering methods is mostly driven by their lightweight
computational requirements  However  these methods lack
  well justi ed underlying probabilistic model  are sensitive to the choice of underlying metric  and do not give  
principled answer to the fundamental question of required
number of clusters for the data at hand 

In this paper  we propose   probabilistic clustering
method for mixedtype data  which admits at least four
attractive properties  First  our probabilistic method goes
beyond the widelyadopted class conditional independence
assumption of feature variables       as in the latent class
model  McCutcheon    Second  our method is based
on the global topographical features       peaks and mountains  of the density function  rather than the distances between data points  The argument for topographical features
is to sidestep   premature speci cation of the metric space
in which our mixedtype data will achieve the best grouping  Third  our method is able to utilize   persistent homology theory to automatically determine the number of clusters in the data  Fourth  the proposed method can be easily
parallelized to achieve   competitive running time with respect to many lightweight geometric clustering methods 

From the modeling perspective  we compose tree
graphical models with topographical features to achieve  
probabilistic mixedtype clustering model  Graphical models provide   way of factorizing   joint probability distribution into   product of local interactions  These local
interactions capture dependency among feature variables 
While   Bayesian network or   Markov random  eld can
be built with   set of nodes representing each feature variable  The graph structure and parameter estimation can be
computationally expensive  By constraining the graph to
be   tree  the structure and parameter can be learned ef 
ciently  Other than computational bene ts  treestructured

Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

graphical models also provide   modeling elegance  with  
tree structure  we have   factorization that explicitly corresponds to empirical univariate and bivariate marginal distributions  For the bivariate distributions  we can then
adapt the product kernel density estimation  Scott   
to capture interaction between continuouscontinuous variables  between categoricalcategorical variables  and between categoricalcontinuous variables 

Having modeled the data generation process via   tree
graphical model  we are left with  nding   robust approach
for assigning each data point to its cluster  To achieve
this  we adopt   topological perspective  namely  we view
  probability distribution as   terrain function  called the
density landscape  and capture its topographical features
as the basis for de ning clusters  The topographical features include modes  peaks  and their attractive basins  For
highdimension and sparse data  it is natural to have many
modes  To avoid oversegmentation of the data and generation of many clusters with only few members  we employ  
persistent homology theory  Edelsbrunner   Harer   
to measure the saliency of all modes and merge the trivial ones  Our principled method for clustering mixedtype
data respects the underlying topographical features of the
density landscape and achieves competitive performance
on real data 

  Related Work

Clustering has been extensively studied in machine
learning and data mining  Many comprehensive surveys
have been produced detailing the landscape of clustering
problems and models  Here we will review related work
in the context of geometric versus probabilistic clustering
methods for mixedtype data and clustering methods that
rely on topographical features such as modes and their attractive basins 
Geometric clustering methods   straightforward approach for mixedtype data clustering is to map them into
either pure continuous or pure categorical domains before
applying   standard clustering method    metric based
on Gower   coef cient  Gower    has been proposed
for mixedtype data  which rescales the difference in all
dimensions  continuous or categorical  and take the average  One can apply any distance based method using these
metrics  However  all these methods are heuristic  there is
no good justi cation for the underlying geometric intuition
of these methods on such   counterintuitive metric space 
despite some successful stories in practice  For example  KPrototypes algorithm  Huang    uses   weighted
sum of the Euclidean distance and Hamming distance and
adopts the KMeans method  Faber    which iteratively  nds the mean of each cluster and reassociates data
to different clusters  When the data is pure categorical 
the method is called KModes  Huang    Chiu et
al    proposed   hierarchical clustering method  in

which distance between clusters are measured using their
loglikelihood  which treat continuous and categorical domain separately 
Probabilistic clustering methods Graphical models have
been applied to clustering before  Zhang   proposed  
latent tree model         Bayesian tree whose leaf nodes correspond to all observed dimensions and internal nodes are
latent variables determining different clusters  Such tree
structure can be learned using ef cient algorithms  Chen
et al    Liu et al    However  this method is only
restricted to categorical data  Lee   Hastie   proposed   loopy graphical model to model mixedtype data 
Their model reduces to   discrete Markov random  eld
when all attributes are categorical  and   Gaussian graphical model when all attributes are continuous  Parameters are learned using pseudolikelihood estimation  Besag 
  and edges are selected using group sparsity penalties
 Yuan   Lin    Huang   Zhang    However  an
ef cient inference model is missing in order to apply such
model to clustering 
Clustering by modeseeking The density landscape has
been exploited before to extract global properties of the
data and to achieve better clustering quality  Modeseeking
methods       associating data to modes representing clusters  have been proposed before in continuous domain
 Cheng    Comaniciu   Meer      But such
methods rely on   kernel density estimation  which suffers
from the curse of dimensionality and thus do not scale to
high dimensions  Wasserman    chap    Chen  
Quadrianto   proposed   modeseeking method for
categorical data clustering  However  their method tends
to produce trivial modes clusters and thus oversegments
the data  mainly due to the lack   principled way to merge
modes into clusters of proper size 
Persistent homology for merging clusters In recent
years  novel approaches have been proposed to merge
modes clusters based on the topographical landscape of the
density function  Chazal et al    used topological persistence to guide the merging of data into clusters  Their
method  although theoretically sound  relies on   knearest
neighbor graph of the data and   given density function 
       kernel density estimation  Silverman    or   distance from measure  Chazal et al    This method
assumes that the data is   high quality sample of the domain and the knearest neighbor graph faithfully captures
the topographical characteristics of the distribution  However  this condition is often too strong to assume in practice 
where most datasets are relatively sparse 
In this paper 
we propose to start with modeseeking  and leverage these
modes and the gradient paths as   more accurate account of
the density landscape  Our idea proves to be   better solution and   good complement to the theoretical tool  We also
refer to other topological and geometrical studies into the

Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

global structures of hierarchical clustering  Eldridge et al 
  Carlsson     emoli   

  Background

  probabilistic graphical model  Koller   Friedman 
  consists of   set of interdependent random variables
                XD    potential function    and   graph
          Each element in the node set   represents
one random variable from    The edges represents the
dependence relations between pairs of variables  There
are two different kinds of variables in our setting  continuous ones and discrete ones variables  For simpli cation  we assume each discrete variable takes discrete values
Xi                    In this paper  we use discrete and
categorical interchangeably and focus on nonordinal discrete variables  although ordinal discrete variables are of
interest in practice as well  In our setting  only Hamming
distance can be used for discrete variables 

  value assignment

to all random variables    
            xD  is called   con guration    potential function           assigns to each con guration   real value 
which is inversely proportional to the logarithm of the probability distribution         exp            where   is
the logpartition function  In this paper  we focus on tree
structured graphical models  represented by          
For   tree model  the probability and potential of   con guration can be factorized into   product  Bach   Jordan 
 

 cid 

      

 cid 

   

      

  xi  xj 
  xi   xj 

  xk 

 

where   xi  xj  is the bivariate marginal density of the variable Xi and Xj  and   xk  is the univariate marginal density of the variable Xk 

When the true distribution can be represented by   tree 
we can use the algorithm by Chow   Liu   to reconstruct the tree model  First  we compute the mutual information between all pairs of variables 

 cid 

  Iij  

  xi  xj  log

  xi  xj 
  xi   xj 

dxidxj 

xi xj

using empirical univariate and bivariate marginals  The integral is replaced by sum when Xi and Xj have discrete
values  Next  we compute the maximum spanning tree of
  complete graph with   nodes  using the mutual information as edge weights  The computed tree is the desired tree
model with the optimal KLdivergence from the true tree
distribution  Liu et al    More details of the selection
of the models for univariate and bivariate densities will be
given in Section  

  Method

Our method  rst estimates the underlying probabilistic
density function from given data  We choose treemodels
as they strike   elegant balance between computational ef 
 ciency and  exibility of the model  Next  we propose
to cluster data based on the density landscape  associating data with modes peaks of the density  and merge them
based on advanced persistent homology theory  First  we
formalize the de nition of modes in the mixedtype domain  Then we present algorithms for modesseeking  Section   and for modesmerging  Section  

We  rst formalize what   mode is in   Ddimensional
mixedtype data domain  Our de nition is not restricted to
the underlying model  Denote by Id and Ic the index sets
of discreteand continuousvalued random variables  Denote by distH       cid  the Hamming distance between   and
  cid  within the discrete dimensions  and distL      cid  the   
distance within the continuous dimensions  We call   discrete neighborhood of   with radius       as all elements
with no more than   Hamming distance and zero Euclidean
distance from    formally 

           cid    distd      cid 
   

        distc      cid 

     

Similarly  we de ne   continuous neighborhood of   with
radius       as

   
           cid    distd      cid 

        distc      cid 

     

Given   probability density function         mode is  
local maximum in both the continuous neighborhood and
discrete neighborhood  formally 
De nition    Modes    point       is   mode if and only
if there exists positive numbers       and       such that
      and         
             cid  for any   cid       
    cid  for any   cid       
It suf ces to use the smallest positive integer for the discrete neighborhood        In this paper  we focus on  
treestructured graphical model  Next  we describe our tree
model in details within the mixedtype setting 

     

  Instantiating the Tree Model

We formalize the univariate and bivariate marginal densities   xi  and   xi  xj  in the tree model  Eq    We
assume   set of   data          yN  is given  For
discrete dimensions  we use Multinoulli distribution with
Dirichlet prior               Id 

  cid 
  cid yn

    xi cid 

  xi   

Nxi    
     

  with Nxi  

  xi  xj   

Nxi xj    
        

with Nxi xj  

  cid 
  cid yn

    xi   yn

    xj cid 

Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

For continuous variables  we use onedimensional kernel
density estimation for univariate density  and product kernel  Scott    for univariate and bivariate marginal density  Formally          Ic 

neighborhood    
      with       The two procedures
have to be taken alternatively in order to continue increasing the probability until   mode is reached 

Our algorithm starts at each data     iteratively walks to
  nearby point with bigger probability until convergence 
The  nal position is the mode of interest and will be associated with the data     For ease of computation  we use the
potential function       instead of the probability density
function 

           cid 

log   xi  xj   cid 

    di  log   xi 

      

   

 
in which di is the degree of node   in the tree  It is easy to
verify that               Therefore  modes of      are
the local minima of       following the same de nition in
Def    We follow the aforementioned iterative procedure 
except at each step  we  nd   nearby point with smaller
potential 

At each step of the algorithm  we  rst update all discrete variables until no better elements exist within the discrete neighborhood    
      with       Next  we update
all continuous variables using gradient descent  until the
gradient of   at continuous dimensions  cf becomes zero 
Our main algorithm is summarized in Alg   

repeat

function   
to each data  ci             

Algorithm   ModeSeeking Algorithm
  Input  Data      si                potential
  Output    set of modes     mode indices associated
       
  for       to   do
    si
 
repeat
 
 
 
 
 
 
 
 
 
 
 
 
  end for

until   converges
repeat
         cf
until   converges

until   converges
if       then
           

end if
ci   the index of   in  

    argminz    

           

Here   is the stepsize  The best neighbor within Hamming distance one  argminz    
            can be computed
using dynamic programming  This can be achieved by
directly adapting the algorithm by  Chen   Quadrianto 
 

It remains to compute the gradient of   in the contin 

  xi   

 
 

  xi  xj   

    xi  and

  

Kh    yn

  cid 
  cid 
 cid Kh   yn
 cid 
 cid    

 
 

  

    xi  Kh    yn

    xj cid   

 

   exp

We use   onedimensional Gaussian kernel  denoted as
Kh       
Following standard nonparametric statistics literature  Fan   Gijbels    Tsybakov    the kernel bandwidths for univariate and bivariate density are chosen as

   

 

hti    min

 
   

       
  
 

  

  

   

            

 cid 

 cid 

      

   and   

where  
   are the standard deviation  the
  and   sample quantiles of Xi  respectively  The
variable   is the order of the kernel  Fan   Gijbels   
and is set to   by default 

The choice of   product kernel is justi ed by two reasons  First    product kernel reduces to the product of onedimensional kernels  which are more reliable that   direct
   kernel density estimation  Second  the product kernel
proves to be convenient to be adopt to bivariate densities
for variables with mixedtype as follows  For   mixedtype
pair of variables   Xi  Xj      Ic      Id  we take the
limit of     to zero in the product kernel formula  Equation
  The  rst kernel becomes the Diracdelta function 
leading to the following bivariate marginal

  cid 

  

 cid cid yn
    xj cid  Kh   yn

    xi cid   

  xi  xj   

 
 

Building the tree model  Using these empirical univariate
and bivariate marginal densities  we estimate all pairwise
mutual information  and then compute the tree       using
the ChowLiu algorithm  Plugging the univariate and bivariate marginal densities into Eq    we have the complete density distribution  the tree model  Next  we present
our algorithm for  nding the modes over the density landscape of the computed model 

  ModeSeeking Algorithm

Our algorithm assigns each data to   mode via   gradient ascent procedure  For   mixeddomain    gradient is not
well de ned  Following the de nition of modes  Def   
we formulate   gradient step as an optimization within either the continuous neighborhood    
      or the discrete

Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

uous domain   cf  For each continuous variable     Ic 
relevant terms in the energy function  Eq    can be divided into three groups  the univariate term  the bivariate
terms with   continuous neighbor      Ic  and the bivariate terms with   discrete neighbor      Id  Treating them
differently  the partial derivative 

 cid  
 cid  
 cid  
 cid  
 cid  

   Kh    yn

   Kh   yn

   Kh   yn

   xi
  
  

    xi  yn
    xi 
   Kh   yn
    xi  Kh    yn
    xj  yn
    xj 
    xi  Kh    yn
    xk cid  yn
    xi cid yn
   xi
  
  
    xi 

   Kh   yn

 cid  

   xi
  
  

 

  Ic      

   Kh    yn

      di 

      
 xi

   cid 
   cid 

  Id      

persistence threshold  

Algorithm   Merging Data Using Topological Persistence

  Output  Clusters  
       

  Input   cid      cid   cid    density function      cid       
  Sort elements in  cid   according to the density function
values  so that   vi      vi   vi  vi     cid   
  for       to  cid    do
nbd    vj    vi  vj     cid           

  neighbors of vi with smaller indices  bigger   
if nbd     then

create   new cluster      vi 
birth        vi 
           
Cnbd   all clusters containing nodes in nbd
cmax   argmaxc Cnbd birth   
for all     Cnbd and    cid  cmax do

persistence      birth        vi 
if persistence        then

  merge   into cmax
cmax   cmax    
        

end if
end for
  assign vi to cmax
cmax   cmax    vi 

else

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end if
 
  end for

  Merging Clusters Using Topological Persistence

The modes computed in Alg    provide   clustering of
the data  However  in practice  the data is often relatively
sparse  In such cases  the method tends to produce   large

Figure   Left  the density landscape with three modes  red  The
  axis is the density function  The persistence  length of the blue
vertical bars        measure their saliency  The function values of the ends of these bars are the birth and death times         
and    for the left mode  The global maxima has in nite persistence        Right  an illustration of the merging hierarchy
of clusters  when           The second mode is merged
into the third one when the  rst mode remains separated  The
 rst mode remains separated as its persistence is bigger than the
threshold  Two clusters remain after merging  Although in this
example  the domain is pure continous  we believe that the intuition carries to the mixed domain 

number of modes  and thus oversegments the data into
small clusters  There are ways to merge these small clusters  Ward Jr    Day   Edelsbrunner    But they
rely on   distance metric to measure similarities between
clusters  Instead  we propose   principled approach that is
only based on the density landscape       the topographical
features such as peaks  ridges  valleys  Our method is built
on the theory of persistent homology  We focus on zerodimensional topological structures in this paper  although
the theory is much more general 
Persistence of modes  We estimate the saliency of   peak
 mode  using its  relative height  namely  the difference
between its height and the level at which its basin of attraction meets the one of another higher mode  Formally  we
 lter the domain using   function value threshold   from
  to   As   decreases  we monitor the topological changes of the progressively growing superlevel set 
                         that is  the domain whose
probability density value is no smaller than    Each mode
attributes to the birth of   new connected component in the
superlevel set and the component is killed when it meets
another component created by   higher mode  The density value of the creating mode and the density value of the
point at which the two components meet  called   saddle 
are called the birth and death times  and their difference 
called the persistence  measures the saliency of this mode 
See Figure   for an illustration 

The merging of connected components as we decrease
the threshold   provides   natural way to merge modes 
when two connected components meet  we merge them if
one of them has     persistence  Figure   This gives us  
principled way to merge modes  Based on the convergence
of treemodel estimation  Liu et al    and the stability

        Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

of persistent homology  CohenSteiner et al    this
method is guaranteed to be robust to noise and    perturbation of the density function 
Samplebased persistence computation  Given   dense
uniform sampling of the whole domain     we can trust
these samples will describe the density landscape faithfully 
In practice  however    uniform sampling will have exponential size to the dimension  Chazal et al    used the
knearest neighbor graph of the input data     assuming
they are good samples from the density function  However 
in practice  the data is often relatively sparse and cannot
represent the landscape well enough to produce   high quality modemerging hierarchy  In fact  it is very likely that the
modes are not included in the data and thus the birth time
 as well as the persistence  will be underestimated  See
Figure  left  for an illustration 

In this paper  we propose to compute persistence based
on all points we encountered during the modeseeking procedure  In Algorithm   we collect the point   computed
after each iteration  after line   The gradient step also
provides   natural edge connecting these points  This tree
structured graph give us   highquality description of the attractive basin of each mode  This provides us   wellsuited
underlying graph describing the density landscape  See
Figure  right  Finally  to ensure the graph is fully connected  and the space between modes are well described 
we add edges  green edges  connecting points from neighboring attractive basins  as well as the lowest point along
these edges  green markers  Note that this is the only time
when the distance metric plays   role in our model  We use
  sum of the Hamming distance and Euclidean distance 

Algorithm  Given   graph  cid      cid   cid    in which each

node is assigned   probability density  we compute the
persistencebased merge tree as follows  Sort all nodes in
decreasing order of their density function values  Add them
into the superlevel set oneby one  To add   node vi  we
check whether it is adjacent to any nodes that have been
included  If not  vi  which must be   mode itself  creates  
new connected component with the birth time   vi  If vi is
connected to multiple existing connected components  we
keep the one with the earliest birth time  cmax  and merge
some others into cmax  In particular  for each other adjacent connected component  we check whether its life length
so far is less than   The ones with     life length will be
merged into cmax  We add vi into the connected component cmax See Figure   for an illustration  See Alg    for
the pseudocode 

  Experiments

We compare our methods with existing clustering
methods on several real world mixedtype datasets from
UCI repository  Lichman    Contraceptive Method
Choice dataset  CMC  Credit Approval dataset  CRX 

German Credit Approval  German  and Statlog Heart Disease dataset  Heart  See the table below for more details 
All datasets have   to   of the features being discrete 

 
 
 
 

 
 
 
 

Table   Datasets

  of clusters

 
 
 
 

  of samples Dimension

Data
CMC
Heart
CRX
German
Our method can be straightforwardly parallelized  We
run the modeseeking for all data points  the forloop in
Alg    in parallel  On average  the modeseeking of  
single data takes   gradient ascent steps and   seconds 
On   cluster with   cores  our program  nishes within  
minutes for any of the datasets  If running in   sequential
manner  the time will be linear to the dataset size  After
all data are processed  we collect all relevant points and
run   persistencebased merging sequentially  This step
takes less than   seconds for any of the datasets  The
persistencebased merging depends on   threshold  
It
is hard to select   universal one due to the large variation among datasets 
Instead  we choose the   for each
dataset so that the desired the nubmer of clusters remain after merging  This is   fair comparison  all clustering methods we compare with use an oracle number of clusters  We
empirically set the parameter   to one  Using   bigger  
hurts the performance as it would try to  smooth  the landscape in the categorical domain 

All methods can be grouped into  ve different groups 
based on the underlying domain and the approach  The  rst
group assumes   continuous domain and an Euclidean metric  We project the mixedtype data into the continuous domain and directly apply such methods  including kmeans
 Faber    Af nity Propagation  Frey   Dueck   
Mean Shift  Cheng    Comaniciu   Meer     
Spectral Clustering  Kamvar et al    Ward   algorithm  Ward Jr    Agglomerative clustering  Day  
Edelsbrunner    and DBSCAN  Ester et al   

The second group are methods designed for pure categorical domain       KModes  Huang    ROCK
 Guha et al    mixture of multinoulli  latent class
analysis   McCutcheon    We convert mixedtype
data into categorical data by thresholding continuous values at the median  We also include Af nity Propagation  Spectral Clustering and DBSCAN in this group  these
methods can be applied to any distance metrics  We compute pairwise Hamming distance between data as the input
of these three methods 

For the third group  we use these three methods  but using   distance matrix based on Gower   coef cient  Gower 
  which was designed speci cally for mixeddomain 
The fourth group uses   simply sum of the Euclidean distance  restricted to continuous dimensions  and Hamming
distance  restricted to categorical dimensions    good rep 

Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

Figure   Comparison of the topological methods   Chazal et al         ours  The data is sampled from   mixture of three   
Gaussian functions  Left  knearest neighbor graph with       on the original data  red markers  plotted on the density function  Next
to it is the merging tree of clusters  Since the three components are disconnected  we cannot accurately estimate   proper merging time
between them  We also consider the middle and upperright mode equally salient  as we underestimates the birth time of the upperright
mode using the probability of one of its data point  Right  our method using the gradient steps as edges  blue  and explicit edges
connecting adjacent attractive basins  green  as well as more points collected during the procedure  Capturing the actual modes gives
us an accurate estimation of the birth time of each basin  furthermore  the lowest point along the green edge  purple marker  gives us
  good estimation of the saddle points and thus the death time of each connected component  In the illustration  we use       so all
three modes merged into   single cluster  The goal is to show that our method better captures the merging tree compared with  Chazal
et al    The domain is    But we believe that the intuition carries to the highdimensional mixed domain 

methods  For all methods requiring random initializations 
we run each one for   times and take the average performance  When necessary  we provide   true number of
clusters as an oracle  The cells with     correspond to
the cases when the program crashes  It is most likely because the Gower   coef cient and Hamming distance does
not give us   wellconditioned distance matrix for the spectral clustering method 

Discussion  Our method outperforms most methods from
all other four groups  using different types of metrics  We
also observe that   few methods based on pure categorical domain are quite competitive  Similarly  Kprototype 
  popular tool for mixedtype data  has good performance
on some data  Outperforming other topological methods
 modes only and persistence only  demonstrate the signi 
cance of our contribution 

Our current experiments assume the correct number of
clusters is given  It is possible to prove that with suf cient
samples and the correct threshold   the persistencebased
clustering can  nd the correct number of cluster and the
right clustering for most data points in   sense similar to
the elegant result in  Chazal et al      closely related
theoretical result is in  Eldridge et al    which shows
that the hierarchical clustering tree constructed by   similar
merging procedure is consistent for points sampled from  
nice density distribution over RD 

Figure   Adding   new node vi into the superlevel set  Component    is merged into cmax as its life length       birth     
  vi  is smaller than   The  rst component    remains separated
after vi as its life length    is bigger than  

resentative in such group is KPrototypes  Huang   
We again applied the three methods  Af nity  Spectral and
DBSCAN  on this new metric 

In the last group  we compare our method and   few
other topological methods  We compare to the method using only modes for clustering  This is essentially an adaptation of  Chen   Quadrianto    to the mixedtype domain  We also compare to  Chazal et al    by computing the persistence on the knearest neighbor graph  using
our treemodel as the underlying density estimation  Finally  we also show the result of our method 

The results are listed in Table   We use the Adjusted
Mutual Information  AMI   Vinh et al    and Adjusted
Rand Score  ARS   Hubert   Arabie    to evaluate all

vip       cmaxc   Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

Table   Results on Real Datasets

Adjusted Random Score  ARS 

CRX

German

Adjusted Mutual Information  AMI 
CMC Heart
German

CRX

 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 

 
 
 
 

 
 
 
 
 
 
 

 
 
 
 
   
 

 
   
 

 
   
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 

 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 

 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 

 
 
 
 

 
 
 

CMC

Agglomerative

 
 
 
 
 
 
 

KMeans
Af nity
MeanShift
Spectral
Ward

Heart
Continuous Domain  Euclidean Metric 
 
 
 
 
 
 
 
Categorical Domain  Hamming Metric 
 
 
 
 
 
 

 
 
 
 
   
 

Af nity
Spectral
DBSCAN

Kmodes
ROCK

Mixture  LCA 

DBSCAN

 
 
 
 
 
 
 

 
 
 
 
 
 

Mixed Domain  Gower   Coef cient 

 
   
 

 
 
 

Af nity
Spectral
DBSCAN

 
 
 
Summed Distance Metric  Euclidean   Hamming 
 
 
 
 

Af nity
Spectral
DBSCAN
  Prototype

 
 
 
 

 
   
 
 

Topological Methods

Modes Only

Persistence Only

Ours  Modes   Pers 

 
 
 

 
 
 

 
 
 

 
 
 

  Conclusions

In this paper  we propose   probabilistic clustering
method for mixedtype data  We design   treestructured
graphical model for the mixedtype domain  We also develop methods based on   topographical view of the density landscape  We design algorithms to capture modes of
the density landscape and merge trivial modes based on the
theory of persistent homology 

Acknowledgments  XN and CC have been partly funded
by the grant PSCCUNY     NQ has been partly
funded by the Russian Academic Excellence Project  
  YW has been partly supported by the grant NSF
DMS  The authors gratefully acknowledge use
of the services and facilities of CUNY Queens Colleges
Center for Computational Infrastructure for the Sciences
 CCIS 

References
Bach  Francis   and Jordan  Michael    Beyond independent components  trees and clusters  The Journal of Machine Learning Research     

Besag  Julian  Statistical analysis of nonlattice data  The

statistician  pp     

Carlsson  Gunnar and   emoli  Facundo  Characterization  stability and convergence of hierarchical clustering
methods  Journal of Machine Learning Research   
   

Chazal  Fr ed eric  CohenSteiner  David  and   erigot 
Quentin  Geometric inference for measures based on
distance functions  Foundations of Computational Mathematics     

Chazal  Fr ed eric  Guibas  Leonidas    Oudot  Steve    and
Skraba  Primoz  Persistencebased clustering in Rieman 

Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

nian manifolds  Journal of the ACM  JACM   
 

Chen  Chao and Quadrianto  Novi  Clustering high dimensional categorical data via topographical features  In International Conference on Machine Learning  ICML 
 

Chen  Tao  Zhang  Nevin    Liu  Tengfei  Poon  Kin Man 
and Wang  Yi  Modelbased multidimensional clustering
of categorical data  Arti cial Intelligence   
   

Cheng  Yizong  Mean shift  mode seeking  and clustering 
IEEE Transactions on Pattern Analysis and Machine Intelligence     

Chiu  Tom  Fang  DongPing  Chen  John  Wang  Yao  and
Jeris  Christopher    robust and scalable clustering algorithm for mixed type attributes in large database environment  In Proceedings of the seventh ACM SIGKDD international conference on knowledge discovery and data
mining  pp    ACM   

Chow    and Liu     Approximating discrete probability
distributions with dependence trees  IEEE Transactions
on Information Theory     

CohenSteiner  David  Edelsbrunner  Herbert  and Harer 
John  Stability of persistence diagrams  Discrete   Computational Geometry     

Comaniciu     and Meer     Mean shift    robust approach
toward feature space analysis  Pattern Analysis and Machine Intelligence  IEEE Transactions on  PAMI   
     

Comaniciu  Dorin and Meer  Peter  Mean shift    robust
approach toward feature space analysis  Pattern Analysis
and Machine Intelligence  IEEE Transactions on   
     

Day  William HE and Edelsbrunner  Herbert  Ef cient algorithms for agglomerative hierarchical clustering methods  Journal of classi cation     

Edelsbrunner  Herbert and Harer  John  Computational

Topology  an Introduction  AMS   

Eldridge  Justin  Belkin  Mikhail  and Wang  Yusu  Beyond hartigan consistency  Merge distortion metric for
hierarchical clustering  In COLT  pp     

Everitt  Brian    Landau  Sabine  Leese  Morven  and
Stahl  Daniel  Cluster Analysis  th Edition  Wiley 
 

Faber  Vance  Clustering and the continuous kmeans algo 

rithm  Los Alamos Science     

Fan  Jianqing and Gijbels  Irene  Local polynomial modelling and its applications  monographs on statistics and
applied probability   volume   CRC Press   

Frey  Brendan   and Dueck  Delbert  Clustering by passing
messages between data points  Science   
   

Gower  John      general coef cient of similarity and

some of its properties  Biometrics  pp     

Guha  Sudipto  Rastogi  Rajeev  and Shim  Kyuseok 
ROCK    robust clustering algorithm for categorical attributes  In International Conference on Data Engineering  ICDE  pp     

Huang  Junzhou and Zhang  Tong  The bene   of group
sparsity  The Annals of Statistics   
 

Huang  Zhexue    fast clustering algorithm to cluster very
large categorical data sets in data mining  In DMKD  pp 
   

Huang  Zhexue  Extensions to the kmeans algorithm for
clustering large data sets with categorical values  Data
mining and knowledge discovery     

Hubert  Lawrence and Arabie  Phipps  Comparing parti 

tions  Journal of classi cation     

Jain  Anil    Data clustering    years beyond kmeans 

Pattern recognition letters     

Kamvar  Sepandar    Klein  Dan  and Manning  Christopher    Spectral learning  In Proceedings of the  th
international joint conference on Arti cial intelligence 
pp    Morgan Kaufmann Publishers Inc   

Koller  Daphne and Friedman  Nir  Probabilistic graphical

models  principles and techniques  MIT press   

Lee  Jason   and Hastie  Trevor    Learning the structure
of mixed graphical models  Journal of Computational
and Graphical Statistics     

Lichman     UCI machine learning repository    URL

http archive ics uci edu ml 

Ester  Martin  Kriegel  HansPeter  Sander    org  Xu  Xiaowei  et al    densitybased algorithm for discovering
In Kdd 
clusters in large spatial databases with noise 
volume   pp     

Liu  Han  Xu  Min  Gu  Haijie  Gupta  Anupam  Lafferty 
John  and Wasserman  Larry  Forest density estimation  Journal of Machine Learning Research   
   

Composing Tree Graphical Models with Persistent Homology Features for Clustering MixedType Data

Liu  TengFei  Zhang  Nevin    Chen  Peixian  Liu 
April Hua  Poon  Leonard KM  and Wang  Yi  Greedy
learning of latent tree models for multidimensional clustering  Machine learning     

MacQueen  James et al  Some methods for classi cation
In Proceedand analysis of multivariate observations 
ings of the  fth Berkeley symposium on mathematical
statistics and probability  volume   pp    Oakland  CA  USA   

McCutcheon  Allan    Latent class analysis  Number  

Sage   

Scott  David    Multivariate density estimation  theory 

practice  and visualization  John Wiley   Sons   

Silverman  Bernard    Density estimation for statistics and

data analysis  volume   CRC press   

Tsybakov  Alexandre   

Introduction to nonparametric
estimation  revised and extended from the   french
original  translated by vladimir zaiats   

Vinh  Nguyen Xuan  Epps  Julien  and Bailey  James  Information theoretic measures for clusterings comparison  Variants  properties  normalization and correction
for chance  Journal of Machine Learning Research   
 Oct   

Ward Jr  Joe    Hierarchical grouping to optimize an objective function  Journal of the American statistical association     

Wasserman  Larry  All of statistics    concise course in statistical inference  Springer Science   Business Media 
 

Xu  Rui and Wunsch  Donald  Survey of clustering algoIEEE Transactions on neural networks   

rithms 
   

Yuan  Ming and Lin  Yi  Model selection and estimation in
regression with grouped variables  Journal of the Royal
Statistical Society  Series    Statistical Methodology 
   

Zhang  Nevin    Hierarchical latent class models for cluster
analysis  The Journal of Machine Learning Research   
   

