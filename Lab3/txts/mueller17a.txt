Sequence to Better Sequence  Continuous Revision of Combinatorial Structures

Jonas Mueller   David Gifford   Tommi Jaakkola  

Abstract

We present   model that  after learning on observations of  sequence  outcome  pairs  can be
ef ciently used to revise   new sequence in order
to improve its associated outcome  Our framework requires neither example improvements 
nor additional evaluation of outcomes for proposed revisions  To avoid combinatorialsearch
over sequence elements  we specify   generative
model with continuous latent factors  which is
learned via joint approximate inference using  
recurrent variational autoencoder  VAE  and an
outcomepredicting neural network module  Under this model  gradient methods can be used to
ef ciently optimize the continuous latent factors
with respect to inferred outcomes  By appropriately constraining this optimization and using the
VAE decoder to generate   revised sequence  we
ensure the revision is fundamentally similar to
the original sequence  is associated with better
outcomes  and looks natural  These desiderata
are proven to hold with high probability under
our approach  which is empirically demonstrated
for revising natural language sentences 

those which appear realistic  For example    random sequence of words will almost never form   coherent sentence
that reads naturally  and   random aminoacid sequence is
highly unlikely to specify   biologically active protein 
In this work  we consider applications where each sequence
  is associated with   corresponding outcome       
For example    news article title or Twitter post can be
associated with the number of shares it subsequently received online  or the aminoacid sequence of   synthetic
protein can be associated with its clinical ef cacy  We operate under the standard supervised learning setting  assumiid  pXY
ing availability of   dataset Dn   tpxi  yiqun
  
of sequenceoutcome pairs  The marginal distribution
pX is assumed as   generative model of the natural sequences  and may be concentrated in   small subspace of
    Throughout this paper    denotes both density and distribution functions depending on the referenced variable 
After  tting models to Dn  we are presented   new sequence         with unknown outcome  and our goal is to
quickly identify   revised version that is expected to have
superior outcome  Formally  we seek the revised sequence 
 

ErY       xs

     argmax
xPCx 

Introduction
The success of recurrent neural network  RNN  models
in complex tasks like machine translation and audio synthesis has inspired immense interest in learning from sequence data  Eck   Schmidhuber    Graves   
Sutskever et al    Karpathy    Comprised of elements st      which are typically symbols from   discrete
vocabulary    sequence     ps          sTq     has length  
which can vary between different instances  Sentences are
  popular example of such data  where each sj is   word
from the language  In many domains  only   tiny fraction
of    the set of possible sequences over   given vocabulary  represents sequences likely to be found in nature  ie 

 MIT Computer Science   Arti cial Intelligence Laboratory 
Correspondence to     Mueller  jonasmueller csail mit edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Here  we want the set Cx  of feasible revisions to ensure
that    remains natural and is merely   minor revision of
   Under   generative modeling perspective  these two
goals are formalized as the following desiderata  pXpx   is
not too small  and    and    share similar underlying latent
characteristics  When revising   sentence for example  it is
imperative that the revision reads naturally  has reasonable
likelihood under the distribution of realistic sentences  and
retains the semantics of the original 
This optimization is dif cult because the constraintset and
objective may be highly complex and are both unknown
 must be learned from data  For many types of sequence
such as sentences  standard distance measures applied directly in the space of   or    eg  Levenshtein distance or
TFIDF similarity  are inadequate to capture meaningful
similarities  even though these can be faithfully re ected by
  simple metric over an appropriately learned space of continuous latent factors  Mueller   Thyagarajan    In
this work  we introduce   generativemodeling framework
which transforms   into   simpler differentiable optimiza 

Sequence to Better Sequence  Continuous Revision of Combinatorial Structures

tion by leveraging continuousvalued latent representations
learned using neural networks  After the generative model
has been     our proposed procedure can ef ciently revise
any new sequence in   manner that satis es the aforementioned desiderata  with high probability 

Related Work
Unlike imitation learning  our setting does not require
availability of improved versions of   particular sequence 
This prevents direct application of   sequenceto sequence
model  Sutskever et al    Similar to our approach 
  omezBombarelli et al    also utilize latent autoencoder representations in order to propose novel chemical
structures via Bayesian optimization  However  unlike sequential bandit reinforcementlearning settings  our learner
sees no outcomes outside of the training data  neither for
the new sequence it is asked to revise  nor for any of its
proposed revisions of said sequence  Mueller et al   
Our methods only require an easilyassembled dataset of
sequenceoutcome pairs and are thus widely applicable 
Combinatorial structures are often optimized via complex
search heuristics such as genetic programming  Zaefferer
et al    However  search relies on evaluating isolated changes in each iteration  whereas good revisions of
  sequence are often made over   larger context  ie  altering   phrase in   sentence  From the vast number of
possibilities  such revisions are unlikely to be found by
searchprocedures  and it is generally observed that such
methods are outperformed by gradientbased optimization
in highdimensional continuous settings  Unlike combinatorial search  our framework leverages gradients in order to
ef ciently  nd good revisions at test time  Simonyan et al 
  and Nguyen et al    also proposed gradientbased optimization of inputs with respect to neural predictions  but work in this vein has been focused on conditional
generation  rather than revision  and is primarily restricted
to the continuous image domain  Nguyen et al   

Methods
To identify good revisions  we  rst map our stochastic combinatorial optimization problem into   continuous space
where the objective and constraints exhibit   simpler form 
We assume the data are generated by the probabilistic
graphical model in Figure     Here  latent factors     Rd
specify    continuous  con guration of the generative process for       both sequences and outcomes  and we
adopt the prior pZ   Np  Iq  Relationships between these
variables are summarized by the maps         which we
parameterize using three neural networks          trained
to enable ef cient approximate inference under this model 
The  rst step of our framework is to    this model to Dn

    Graphical Model

 

 

 

 

 

 

    Revision Procedure
  

         

 

  

  

 

 
 rzF

  

 rzF

     

  

 
 rzF

  

 

  

 

  

Figure       Assumed graphical model  shaded nodes indicate
observed variables  dashed arrows are learned neural network
mappings      Procedure for revising   given    to produce   
with superior expected outcome 

by learning the parameters of these inference networks  the
encoder    the decoder    and the outcomepredictor    
  good model that facilitates highquality revision under
our framework will possess the following properties   
  can ef ciently be inferred from   and this relationship
obeys   smooth functional form    the map   produces
  realistic sequence   given any   with reasonable prior
probability    the distribution of natural sequences is geometrically simple in the latent Zspace  We explicitly encourage   by choosing   as   fairly simple feedforward
network    by de ning   as the mostlikely   given   
and   by endowing   with our simple Np  Iq prior 
Another characteristic desired of our Zrepresentations is
that they encode meaningful sequencefeatures such that
two fundamentally similar sequences are likely to have
been generated from neighboring zvalues  Applied to image data  VAE models similar to ours have been found to
learn latent representations that disentangle salient characteristics such as scale  rotation  and other independent
visual concepts  Higgins et al    The latent representations of recurrent architectures trained on text  similar
to the models used here  have also been shown to encode
meaningful semantics  with   strong correlation between
distances in the latent space and humanjudged similarity
between texts  Mueller   Thyagarajan    By exploiting such simpli ed geometry    basic shift in the latent
vector space may be able to produce higherquality revisions than attempts to directly manipulate the combinatorial space of sequence elements 
After  tting   model with these desirable qualities  our
strategy to revise   given sequence        is outlined
in Figure     First  we compute its latent representation
     Epx   using   trained encoding map  As the latent
representations   are continuous  we can employ ef cient
gradientbased optimization to  nd   nearby local optimum
   of Fpzq  within   simple constraintset around    de 
 ned later on  To    we subsequently apply   simple decoding map    de ned with respect to our learned model 
in order to obtain our revised sequence    Under our

Sequence to Better Sequence  Continuous Revision of Combinatorial Structures

assumed model  the optimization in latent representationspace attempts to identify   generative con guration which
produces large values of    as inferred via     The subsequent decoding step seeks the most likely sequence produced by the optimized setting of the latent factors 

Variational Autoencoder
For approximate inference in the      relationship  we
leverage the variational autoencoder  VAE  model of
Kingma   Welling   In our VAE    generative model
of sequences is speci ed by our prior over the latent values
  combined with   likelihood function pDpx   zq which our
decoder network   outputs in order to evaluate the likelihood of any sequence   given     Rd  Given any sequence
   our encoder network   outputs   variational approximation qEpz   xq of the true posterior over the latentvalues
ppz   xq  pDpx   zqpZpzq  As advocated by Kingma  
Welling   and Bowman et al    we employ the
variational family qEpz   xq   Np            with diagonal covariance  Our revision methodology employs the
encoding procedure Epxq        which maps   sequence
to the maximum   posteriori  MAP  con guration of the
latent values    as estimated by the encoder network   
The parameters of      are learned using stochastic
variational inference to maximize   lower bound for the
marginal likelihood of each observation in the training data 

log pXpxq    Lrecpxq   Lpripxq 
Lrecpxq    EqEpz xq rlog pDpx   zqs
Lpripxq   KLpqEpz   xq  pZq

 

De ning        diagp   xq  the priorenforcing KullbackLeibler divergence has   differentiable closed form expression when qE  pZ are diagonal Gaussian distributions  The
reconstruction term Lrec  ie  negative loglikelihood under
the decoder model  is ef ciently approximated using just
one MonteCarlo sample     qEpz   xq  To optimize the
variational lower bound over our data Dn with respect to
the parameters of neural networks       we use stochastic gradients of   obtained via backpropagation and the
reparameterization trick of Kingma   Welling  
Throughout  our encoder decoder models      are recurrent neural networks  RNN  RNNs adapt standard feedforward neural networks for sequence data     ps          sTq 
where at each timestep                Tu     xed size hiddenstate vector ht   Rd is updated based on the next element
in the input sequence  To produce the approximate posterior for   given    our encoder network   appends the
following additional layers to the  nal RNN hiddenstate
 parameterized by       Wv        bv 
         hT        Rd
       expp                ReLUpWvhT   bvq  

The  squared  elements of        Rd form the diagonal of
our approximateposterior covariance       Since Lpri is
minimized at          and Lrec is likely to worsen with
additional variance in encodings  as our posterior approximation is unimodal  we simply do not consider      values that exceed   in our variational family  This restriction
results in more stable training and also encourages the encoder and decoder to coevolve such that the true posterior
is likely closer to unimodal with variance    
To evaluate the likelihood of   sequence  RNN   computes
not only its hidden state ht  but also the additional output 

     softmaxpW ht      

 

At each position       estimates ppst              st   by relying on ht to summarize the sequence history  By the
factorization pps          sTq     
   ppst   st              
we have pDpx   zq   
    trsts  which is calculated by
specifying an initial hiddenstate        and feeding
    ps          sTq into    From   given latent con guration
   our revisions are produced by decoding   sequence via
the mostlikely observation  which we denote as the map 

Dpzq   argmax
xPX

pDpx   zq

 

While the mostlikely decoding in   is itself   combinatorial problem  beam search can exploit the sequentialfactorization of ppx   zq to ef ciently  nd   good approximate solution  Wiseman   Rush    Sutskever et al 
  For      Dpzq       this decoding strategy seeks
to ensure neither pXpx   nor ppz       is too small 
Compositional Prediction of Outcomes
In addition to the VAE component  we      compositional outcomeprediction model which uses   standard
feed forward neural network   to implement the map
    Rd      It is assumed that Fpzq   ErY       zs under our generative model  Rather than integrating over  
to compute ErY       xs   FpzqqEpz   xqdz  we employ the  rstorder Taylor approximation FpEpxqq  where
the approximationerror shrinks the more closely   resembles an af ne transformation  To ensure this approximateinference step accurately estimates the conditional expectation  we jointly train   and   with the loss 

Lmsepx  yq   ry   FpEpxqqs 

 

If the architecture of networks      is speci ed with suf 
 cient capacity to capture the underlying conditional relationship  then we should have FpEpxqq   ErY       xs
after properly learning the network parameters from   suf 
 ciently large dataset  even   is   nonlinear map 

Sequence to Better Sequence  Continuous Revision of Combinatorial Structures

Enforcing Invariance
In theory  it is possible that some dimensions of   pertain
solely to the outcome   and do not have any effect on the
decoded sequence Dpzq  Happening to learn this sort of
latent representation would be troubling  since subsequent
optimization of the inferred   with respect to   might not
actually lead to   superior revised sequence  To mitigate
this issue  we carefully ensure the dimensionality   of our
latent   does not signi cantly exceed the bottleneck capacity needed to produce accurate outcomepredictions and
VAE reconstructions  Gupta et al    We explicitly
suppress this undesirable scenario by adding the following
loss to guide training of our neural networks 

 

Linv   Ez pZ Fpzq   FpEpDpzqqq 

When optimizing neural network parameters with respect
to this loss  we treat the parameters of   and the lefthand
Fpzq term as  xed  solely backpropagating MonteCarlo
estimated gradients into        Driving Linv toward   ensures our outcomepredictions remain invariant to variation introduced by the encodingdecoding process  and this
term also serves as   practical regularizer to enforce additional smoothness in our learned functions 

Joint Training
The parameters of all components of this model  qE  pD 
and     are learned jointly in an endto end fashion  Training is done via stochastic gradient descent applied to minimize the following objective over the examples in Dn 
  Linv
 

Lpx  yq   Lrec    priLpri    mse

  Lmse    inv

 

 

where  
  denotes the  empirical  variance of the outcomes 
and the       are constants chosen to balance the relative
weight of each goal so that the overall framework produces
maximally useful revisions  By setting  mse    inv    
at  rst  we can optionally leverage   separate large corpus of unlabeled examples to initially train only the VAE
component of our architecture  as in the unsupervised pretraining strategy used successfully by Kiros et al   
Erhan et al   
In practice  we found the following training strategy to
work well  in which numerous minibatch stochastic gradient updates  typically   epochs  are applied within
every one of these steps 
Step   Begin with  inv    pri     so Lrec and Lmse
are the only training objectives  We found that regardless
of the precise value speci ed for  mse  both Lrec and Lmse
were often driven to their lowest possible values during this
joint optimization  veri ed by training individually against
each objective 

Step   Grow  pri from   to   following the sigmoid annealing schedule proposed by Bowman et al    which
is needed to ensure the variational sequence to sequence
model does not simply ignore the encodings    note that
the formal variational lower bound is attained at  pri    
Step   Gradually increase  inv linearly until Linv becomes
small on average across our MonteCarlo samples     pZ 
Here  pD is treated as constant with respect to Linv  and
each minibatch used in stochastic gradient descent is chosen to contain the same number of MonteCarlo samples
for estimating Linv as  sequence  outcome  pairs 
Proposing Revisions
While the aforementioned training procedure is computationally intensive  once learned  our neural networks can
be leveraged for ef cient inference  Given userspeci ed
constant       and   tobe revised sequence    we propose the revision    output by the following procedure 

Fpzq

 beam search 

 gradient ascent 

REVISE Algorithm
Input  sequence          constant             
  
Output  revised sequence       
  Use   to compute qEpz      
  De ne Cx        Rd   qEpz          
  Find      argmax
zPCx 
  Return      Dpz  
Intuitively  the levelset constraint Cx    Rd ensures that
   the latent con guration from which we decode    is
likely similar to the latent characteristics responsible for the
generation of    Assuming    and    share similar latent
factors implies these sequences are fundamentally similar
according to the generative model  Note that      Epx  
is always   feasible solution of the latentfactor optimization over     Cx   for any allowed value of   Furthermore  this constrained optimization is easy under our Gaussian approximateposterior  since Cx  forms   simple ellipsoid centered around Epx   
To  nd    in Step   of the REVISE procedure  we use gradient ascent initialized at     Epx    which can quickly
reach   local maximum if   is parameterized by   simple
feedforward network  Starting the search at Epx   makes
most sense for unimodal posterior approximations like our
Gaussian qE  To ensure all iterates remain in the feasible
region Cx  we instead take gradient steps with respect to  
penalized objective Fpzq       Jpzq where 
    pz   Epx qq 
Jpzq   log     pz   Epx qqT  
      logrp qd      
 
and           is gradually decreased toward   to en 

Sequence to Better Sequence  Continuous Revision of Combinatorial Structures

sure the optimization can approach the boundary of Cx  In
terms of resulting revision quality  we found this log barrier
method outperformed other standard  rstorder techniques
for constrained optimization such as the projected gradient
and FrankeWolfe algorithms 
In principle  our revision method can operate on the latent
representations of   traditional deterministic autoencoder
for sequences  such as the seq seq models of Sutskever
et al    and Cho et al    However  the VAE
offers numerous practical advantages  some of which are
highlighted by Bowman et al    in the context of generating morecoherent sentences  The posterior uncertainty
of the VAE encourages the network to smoothly spread the
training examples across the support of the latent distribution  In contrast  central regions of the latent space under  
traditional autoencoder can contain holes  to which no examples are mapped  and it is not straightforward to avoid
these in our optimization of    Furthermore  we introduce
an adaptive variant of our decoder in     which is designed
to avoid poor revisions in cases where the initial sequence
is already not reconstructed properly  DpEpx qq     
Theoretical Properties of Revision
Here  we theoretically characterize properties of revisions
obtained via our REVISE procedure  all proofs are relegated to     in the Supplementary Material  Our results imply that in an ideal setting where our neural network inference approximations are exact  the revisions proposed by our method are guaranteed to satisfy our previously stated desiderata     is associated with an expected
outcomeincrease     appears natural  has nontrivial probability under pX whenever    is   natural sequence  and
   is likely to share similar latent characteristics as   
 since    is the most likely observation generated from
   and qEpz            by design  Although exact
approximations are unrealistic in practice  our theory precisely quanti es the expected degradation in the quality of
proposed revisions that accompanies   decline in either the
accuracy of our approximate inference techniques or the
marginal likelihood of the original sequence to revise 
Theorems   and   below ensure that for an initial sequence
   drawn from the natural distribution  the likelihood of the
revised sequence    output by our REVISE procedure under pX has lower bound determined by the userparameter
  and the probability of the original sequence pXpx   
Thus  when revising   sequence    which looks natural
 has substantial probability under pX  our procedure is
highly likely to produce   revised sequence    which also
looks natural  The strength of this guarantee can be precisely controlled by choosing   appropriately large in applications where this property is critical 
In each high probability statement  our bounds assume the

distribution de ned by Hoffman   Johnson   as 

ppz   xq       qEpz   xq whenever qEpz   xq    

initial tobe revised sequence    stems from the natural
distribution pX  and each result holds for any  xed constant       We  rst introduce the following assumptions 
    For             there exists           such that 
   With probability          over     pX 
ii  PrpZ   BR   qq       PrprZ   BR   qq
where     Np  Iq  and rZ   qZ  the average encoding
 
qZpzq   Ex pXrqEpz   xqs
BRp     tz   Rd         Ru denotes the Euclidean ball
centered around   with radius   de ned here as 
 
    maxtR     
with        logr      qd  
     maxtrR      rR         
   log   
 
    There exists        depends on   such that with
ppz           
probability          over      pX 
This means the latent posterior is bounded at        as
de ned in REVISE  where both depend upon the initial tobe revised sequence   
Theorem   For any           and     imply 

pXpx      

    pXpx  
with probability          over      pX 
Condition     forms   generalization of absolute continuity  and is required since little can be guaranteed about
our inference procedures if the variational posterior is too
inaccurate  Equality holds in     with probability   if the
variational distributions qE exactly represent the true posterior       as the variational approximations become more
accurate over the measure pX  In practice  minimization
of the reverse KL divergence  Lpri  used in our VAE formulation ensures that qEpz   xq is small wherever the true
posterior ppz   xq takes small values  Blei et al   
While the bound in Theorem   has particularly simple
form  this result hinges on assumption     One can show
for example that the inequality in     is satis ed if the
posteriors ppz       are Lipschitz continuous functions of
  at     sharing one Lipschitz constant over all possible
   In general however      heavily depends on both the
data distribution pX and decoder model pD  Therefore  we
provide   similar lower bound guarantee on the likelihood
of our revision    under pX  which instead only relies on
weaker assumption     below 
    There exists       such that for each        
pDpx   zq is   LLipschitz function of   over BR     

Sequence to Better Sequence  Continuous Revision of Combinatorial Structures

Here    depends on    through    and we assume      
without loss of generality      is guaranteed to hold in the
setting where we only consider sequences of  nite length
      This is because the probability output by our decoder
model  pDpx   zq  is differentiable with bounded gradients over all     BRp   under any sequenceto sequence
RNN architecture which can be properly trained using gradient methods  Since BR       Rd is   closed interval 
pDpx   zq must be Lipschitz continuous over this set  for  
given value of    We can simply de ne   to be the largest
Lipschitz constant over the      possible choices of      
      size of the vocabulary  In the next theorem below 
userspeci ed constant       is de ned in REVISE  and   
    all depend on  
Theorem   For any       if     and     hold  then
with probability          over      pX 

pXpx     Ce  
        pXpx     
where constant        
        
    

pd    qd
pd    qd 

Ld

Our  nal result  Theorem   ensures that our optimization
of    with respect to   is tied to the expected outcomes at
     Dpz    so that large improvements in the optimization objective  Fpz     FpEpx qq imply that our revision
procedure likely produces large expected improvements in
the outcome  ErY             ErY            For this
result  we make the following assumptions 
   
PrpX   Kq         where we de ne 

there exists       such that

For any      

    tx                pXpx       

 

as the subset of sequences whose improved versions produced by our REVISE procedure remain natural with likelihood     Note that either Theorem   or    with the corresponding assumptions  ensures that one can suitably de ne
  such that     is satis ed  by considering   suf ciently
large  nite subset of    
    For any       there exists  mse     such that
PrpX   Emseq         where we de ne 
Emse  tx        FpEpxqq   ErY      xs     mseu  
    For any       there exists  inv     such that 
 Fpzq   FpEpDpzqqq     inv
where   is de ned in   and depends on  
Here   mse and  inv quantify the approximation error of our
neural networks for predicting expected outcomes and ensuring encodingdecoding invariance with respect to    

for all     BRp     Rd

Standard learning theory implies both  mse   inv will be
driven toward   if we use neural networks with suf cient
capacity to substantially reduce Lmse and Linv over   large
training set 
Theorem   For any       if conditions            
and     hold  then with probability            
          Fpz     FpEpx qq          

 

where

      ErY             ErY          
     inv    mse

Here     inv are de ned in terms of   as speci ed in    
    and  mse is de ned in terms of   as speci ed in    

Experiments
All of our RNNs employ the Gated Recurrent Unit  GRU 
of Cho et al    which contains   simple gating mechanism to effectively learn longrange dependencies across
  sequence  Throughout    is   simple feedforward network with   hidden layer and tanh activations  note that the
popular ReLU activation is inappropriate for   since it has
zero gradient over half its domain  Decoding with respect
to pD is simply done entirely greedily  ie    beamsearch of
size   to demonstrate our approach is not reliant on search
heuristics      contains additional details for each analysis 
Simulation Study
To study our methods in   setting where all aspects of performance can be quanti ed  we construct   natural distribution pX over sequences of lengths   whose elements
stem from the vocabulary     tA                Ju  Each sequence is generated via the probabilistic grammar of Table
   For each sequence  the associated outcome   is simply the number of times   appears in the sequence    completely deterministic relationship  Since   often follows  
and is almost always followed by   under pX    procedure
to generate natural revisions cannot simply insert substitute
  symbols at random positions 
Table   compares various methods for proposing revisions 
Letting    denote the standard deviation of outcomes
in Dn  we evaluate each proposed    using   rescaled
version of the actual underlying outcomeimprovement 
   px      
  pErY             ErY         sq  Except where sample size is explicitly listed  all models were
trained using          sequence  outcome  pairs sampled from the generative grammar  Wherever appropriate 
the different methods all make use of the same neural network components with latent dimension       Other
than   all hyperparameters of each revision method described below were chosen so that over   revisions  the
Levenshtein  edit  distance dpx          on average 

Model
log      
     
     
log      
ADAPTIVE
 inv    pri    
SEARCH

Sequence to Better Sequence  Continuous Revision of Combinatorial Structures
   px  
   
   
   
   
   
   
   

  log pXpx  
   
   
   
   
   
   
   

Model
log      
log      
ADAPTIVE
 inv    pri    
SEARCH

dpx     
   
   
   
   
   
   
   

   px  
   
   
   
   
   

 Lpx  
   
   
   
   
   

dpx     
   
   
   
   
   

Table   Results for revisions    produced by different methods
in our simulation study  averaged over the same test set of  
starting sequences      pX  with   standard deviation shown
and the best results in bold 

All three results above the line in Table   are based on the
full model described in our joint training procedure  with
new sequences proposed via our REVISE algorithm  using
the setting log       In the latter two results  this
model was only trained on   smaller subset of the data  We
also generated revisions via this same procedure with the
more conservative choice log       ADAPTIVE denotes
the same approach  with log       this time using
the adaptive decoding Dx  introduced in     which is intended to slightly bias revisions toward    The model with
 inv    pri     is   similar method using   deterministic
sequenceto sequence autoencoder rather than our probabilistic VAE formulation  no variational posterior approximation or invarianceenforcing  where the latent encodings
are still jointly trained to predict outcomes via     Under
this model    revision is proposed by starting at Epx   in
the latent space  taking    unconstrained  gradient steps
with respect to     and  nally applying   to the resulting   
The above methods form an ablation study of the various
components in our framework  SEARCH is   different combinatorial approach where we randomly generate   revisions by performing   random edits in     each individual
edit is randomly selected as one of  substitution  insertion 
deletion  or no change 
In this approach  we separately
learn   languagemodel RNN   on our training sequences
 Mikolov et al    Sharing the same GRU architecture as our decoder model    directly estimates the likelihood of any given sequence under pX  Of the randomly
generated revisions  we only retain those sequences   for
which Lpxq    
    Lpx    in this case  those which are not
estimated to be     times less likely than the original
sequence    under pX  Finally  we score each remaining candidate  including    using the outcomeprediction
model FpEpxqq  and the best is chosen as   
Table   shows that our probabilistic VAE formulation
outperforms the alternative approaches  both in terms of
outcomeimprovement achieved as well as ensuring revi 

Table   Results for revised beerreview sentences    produced
by different methods  average   standard deviation reported over
the same heldout set of   initial sentences    The third column employs the de nition  Lpx     log Lpx     log Lpx   

sions follow pX  For comparison    log pXpx   had an average value of    over these   starting sequences 
and changing one randomlyselected symbol in each sequence to   results in an average negative logprobability
of   Thus  all of our revision methods clearly account
for pX to some degree  We  nd that all components used
in our REVISION procedure are useful in achieving superior
revisions  While individual standard deviations seem large 
nearly all average differences in    or   log pX values
produced by different methods are statistically signi cant
considering they are over   revisions 
From Supplementary Figure    it is clear that   controls how conservative the changes proposed by our REVISE procedure tend to be  in terms of both   log pXpx  
and the edit distance dpx       The red curve in Figure
    suggests that our theoretical lower bounds for pXpx  
are overly stringent in practice  although only the averagecase is depicted in the  gure  The relationship between
log pXpx   and log pXpx    see Figure      is best   by
  line of slope   indicating that the linear dependence
on pXpx   in the Theorem   bound for pXpx   is reasonably accurate  Figure     shows that the magnitude
of changes in the latent space  arising from zoptimization
during our REVISE procedure  only exhibits   weak correlation with the edit distance between the resulting revision
and the original sequence  This implies that    xed shift in
different directions in the latent space can produce drastically different degrees of change in the sequence space  To
ensure   highquality revision  it is thus crucial to carefully
treat the  variational  posterior landscape when performing
manipulations of   

Improving Sentence Positivity
Next  we apply our model to    reviews from BeerAdvocate  McAuley et al    Each beer review is parsed
into separate sentences  and each sentence is treated as an
individual sequence of words  In order to evaluate methods using an outcome that can be obtained for any proposed revision  we choose           as the VADER sentiment compound score of   given sentence  Hutto   Gilbert 

Sequence to Better Sequence  Continuous Revision of Combinatorial Structures
dpx     

Model
  
log      
ADAPTIVE
log      
 inv    pri    
SEARCH

Sentence
this smells pretty bad 
smells pretty delightful 
smells pretty delightful 
  liked this smells pretty 
pretty this smells bad 
wow this smells pretty bad 

 

   px    Lpx  
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

Table   Example of   heldout beer review     in bold  revised to improve the VADER sentiment  Underneath the original sentence 
we show the revision produced by each different method along with the true  rescaled  outcome improvement      change in estimated
marginal likelihood     and edit distance dpx       Table    contains additional examples 

  Steps Decoded Sentence
  
 
 
 
 
  
  
 
 
 
 
  

where are you  henry 
where are you  henry 
where are you  royal 
where art thou now 
which cannot come  you of thee 
where art thou  keeper 
you are both the same size 
you are both the same 
you are both wretched 
you are both the king 
you are both these are very 
you are both wretched men 

Table   Decoding from latent   con gurations encountered at
the indicated number of  unconstrained  gradient steps from
Epx    for the model trained to distinguish sentences from Shakespeare vs  contemporary authors  Shown  rst and last are    and
the    returned by our REVISION procedure  constrained with
log       Table    contains additional examples 

  VADER is   complex rulebased sentiment analysis
tool which jointly estimates polarity and intensity of English text  and larger VADER scores correspond to text that
humans  nd more positive with high  delity 
We applied all aforementioned approaches to produce revisions for   heldout set of   test sentences  As pX
underlying these sentences is unknown  we report estimates
thereof obtained from   RNN languagemodel   learned on
the sentences in Dn  Table   demonstrates that our VAE approach achieves the greatest outcomeimprovement  Moreover  Tables   and    show that our probabilisticallyconstrained VAE revision approach produces much more
coherent sentences than the other strategies 

Revising Modern Text in the Language of Shakespeare
For our  nal application  we assemble   dataset of   
short sentences which are either from Shakespeare or  
more contemporary source  details in     In this training data  each sentence is labeled with outcome      

if it was authored by Shakespeare and       otherwise
 these values are chosen to avoid the  at region of the sigmoid output layer used in network     When applied in
this domain  our REVISE procedure thus attempts to alter
  sentence so that the author is increasingly expected to be
Shakespeare rather than   more contemporary source 
Tables   and    show revisions  of heldout sentences 
proposed by our REVISE procedure with adaptive decoding  see     together with sentences generated by applying the adaptive decoder at various points along an unconstrained gradientascent path in latent   space  following
gradients of     Since the data lack similar versions of  
sentence written in both contemporary and Shakespearean
language  this revision task is an ambitious application of
our ideas  Without observing   continuous spectrum of outcomes or leveraging speciallydesigned style transfer features  Gatys et al    our REVISE procedure has to
alter the underlying semantics in order to nontrivially increase the expected outcome of the revised sentence under
    Nevertheless  we  nd that many of the revised sentences
look realistic and resemble text written by Shakespeare 
Furthermore  these examples demonstrate how the probabilistic constraint in our REVISE optimization prevents the
revisiongenerating latent   con gurations from straying
into regions where decodings begin to look very unnatural 

Discussion
This paper presents an ef cient method for optimizing discrete sequences when both the objective and constraints are
stochastically estimated  Leveraging   latentvariable generative model  our procedure does not require any examples
of revisions in order to propose naturallooking sequences
with improved outcomes  These characteristics are proven
to hold with high probability in   theoretical analysis of
VAE behavior under our controlled latentvariable manipulations  However  ensuring semantic similarity in textrevisions remains dif cult for this approach  and might be
improved via superior VAE models or utilizing additional
similarity labels to shape the latent geometry 

Sequence to Better Sequence  Continuous Revision of Combinatorial Structures

References
Blei        Kucukelbir     and McAuliffe        Variational inference    review for statisticians  Journal of
the American Statistical Association   

Bowman        Vilnis     Vinyals     Dai        Jozefowicz     and Bengio     Generating sentences from  
continuous space  Conference on Computational Natural Language Learning   

Cho     van Merrienboer     Gulcehre     Bahdanau 
   Bougares     Schwenk     and Bengio     Learning phrase representations using rnn encoderdecoder for
statistical machine translation  Empirical Methods on
Natural Language Processing   

Karpathy     The unreasonable effectiveness of recurrent
neural networks  Andrej Karpathy blog    URL
karpathy github io 

Kingma        and Welling     Autoencoding variational
bayes  International Conference on Learning Representations   

Kiros     Zhu     Salakhutdinov     Zemel        Torralba     Urtasun     and Fidler     Skipthought vectors  Advances in Neural Information Processing Systems   

McAuley     Leskovec     and Jurafsky     Learning attitudes and attributes from multiaspect reviews  IEEE
International Conference on Data Mining   

Eck     and Schmidhuber        rst look at music composition using lstm recurrent neural networks  IDSIA Technical Report   

Mikolov     Kara at     Burget     Cernocky     and
Khudanpur     Recurrent neural network based language
model  Interspeech   

Erhan     Bengio     Courville     Manzagol     Vincent     and Bengio     Why does unsupervised pretraining help deep learning  Journal of Machine Learning Research     

Gatys        Ecker        and Bethge    

Image style
transfer using convolutional neural networks  Computer
Vision and Pattern Recognition   

  omezBombarelli     Duvenaud     Hern andezLobato 
      AguileraIparraguirre    
  Hirzel     Adams 
      and AspuruGuzik     Automatic chemical design using   datadriven continuous representation of
molecules  arXiv   

Graves     Generating sequences with recurrent neural net 

works  arXiv   

Gupta     Banchs        and Rosso     Squeezing bottlenecks  Exploring the limits of autoencoder semantic
representation capabilities  Neurocomputing   
   

Higgins     Matthey     Glorot     Pal     Uria    
Blundell     Mohamed     and Lerchner     Early visual concept learning with unsupervised deep learning 
arXiv   

Hoffman        and Johnson        Elbo surgery  yet
another way to carve up the variational evidence lower
bound  NIPS Workshop on Advances in Approximate
Bayesian Inference   

Hutto       and Gilbert     Vader    parsimonious rulebased model for sentiment analysis of social media text 
Eighth International Conference on Weblogs and Social
Media   

Mueller     and Thyagarajan     Siamese recurrent architectures for learning sentence similarity  Proc  AAAI
Conference on Arti cial Intelligence   

Mueller     Reshef        Du     and Jaakkola     Learning optimal interventions  Arti cial Intelligence and
Statistics   

Nguyen     Yosinski     and Clune     Deep neural networks are easily fooled  High con dence predictions for
unrecognizable images  Computer Vision and Pattern
Recognition   

Nguyen     Dosovitskiy     Yosinski     Brox     and
Clune     Synthesizing the preferred inputs for neurons in
neural networks via deep generator networks  Advances
in Neural Information Processing Systems   

Simonyan     Vedaldi     and Zisserman     Deep inside
convolutional networks  Visualising image classi cation
models and saliency maps  ICLR Workshop Proceedings 
 

Sutskever     Vinyals     and Le       Sequence to sequence learning with neural networks  Advances in Neural Information Processing Systems   

Wiseman     and Rush        Sequenceto sequence learning as beamsearch optimization  Empirical Methods in
Natural Language Processing   

Zaefferer     Stork     Friese     Fischbach     Naujoks 
   and BartzBeielstein     Ef cient global optimization
for combinatorial problems  Genetic and Evolutionary
Computation Conference   

