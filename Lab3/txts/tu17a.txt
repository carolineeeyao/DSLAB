Breaking Locality Accelerates Block GaussSeidel

Stephen Tu   Shivaram Venkataraman   Ashia    Wilson   Alex Gittens   Michael    Jordan   Benjamin Recht  

Abstract

Recent work by Nesterov and Stich  
showed that momentum can be used to accelerate the rate of convergence for block GaussSeidel in the setting where    xed partitioning of the coordinates is chosen ahead of time 
We show that this setting is too restrictive  constructing instances where breaking locality by
running nonaccelerated GaussSeidel with randomly sampled coordinates substantially outperforms accelerated GaussSeidel with any  xed
partitioning  Motivated by this  nding  we analyze the accelerated block GaussSeidel algorithm in the random coordinate sampling setting  Our analysis captures the bene   of acceleration with   new datadependent parameter which is well behaved when the matrix subblocks are wellconditioned  Empirically  we
show that accelerated GaussSeidel with random
coordinate sampling provides speedups for large
scale machine learning tasks when compared to
nonaccelerated GaussSeidel and the classical
conjugategradient algorithm 

  Introduction

The randomized GaussSeidel method is   commonly used
iterative algorithm to compute the solution of an       linear system Ax     by updating   single coordinate at  
time in   randomized order  While this approach is known
to converge linearly to the true solution when   is positive
de nite  see       Leventhal   Lewis    in practice
it is often more ef cient to update   small block of coordinates at   time due to the effects of cache locality 

In extending randomized GaussSeidel to the block setting 
  natural question that arises is how one should sample the
next block  At one extreme    xed partition of the coordi 

 UC Berkeley  Berkeley  California  USA  Rensselaer Polytechnic Institute  Troy  New York  USA  Correspondence to 
Stephen Tu  stephent berkeley edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

nates is chosen ahead of time  The algorithm is restricted
to randomly selecting blocks from this  xed partitioning 
thus favoring data locality  At the other extreme we break
locality by sampling   new set of random coordinates to
form   block at every iteration 

Theoretically  the  xed partition case is well understood
both for GaussSeidel  Qu et al    Gower   Richt arik 
  and its Nesterov accelerated variant  Nesterov  
Stich    More speci cally  at most   
part log 
iterations of GaussSeidel are suf cient to reach   solution
with at most   error  where  part is   quantity which measures how well the   matrix is preconditioned by the block
diagonal matrix containing the subblocks corresponding
to the  xed partitioning  When acceleration is used  Nesterov and Stich   show that the rate improves to

  cid    

   

part log cid  where   is the partition size 

For the random coordinate selection model  the existing
literature is less complete  While it is known  Qu et al 
  Gower   Richt arik    that the iteration complexity with random coordinate section is   
rand log 
for an   error solution   rand is another instance dependent
quantity which is not directly comparable to  part  Hence
it is not obvious how much better  if at all  one expects
random coordinate selection to perform compared to  xed
partitioning 

Our  rst contribution in this paper is to show that  when
compared to the random coordinate selection model  the
 xed partition model can perform very poorly in terms of
iteration complexity to reach   prespeci ed error  Speci 
cally  we present   family of instances  similar to the matrices recently studied by Lee and Wright   where nonaccelerated GaussSeidel with random coordinate selection
performs arbitrarily faster than both nonaccelerated and
even accelerated GaussSeidel  using any  xed partition 
Our result thus shows the importance of the sampling strategy and that acceleration cannot make up for   poor choice
of sampling distribution 

This  nding motivates us to further study the bene ts of
acceleration under the random coordinate selection model 
Interestingly 
the bene ts are more nuanced under this
model  We show that acceleration improves the rate from
  

rand log  to   cid   

rand log cid  where   is

Breaking Locality Accelerates Block GaussSeidel

  new instance dependent quantity that satis es      
rand 
We derive   bound on   which suggests that if the subblocks of   are all well conditioned  then acceleration can
provide substantial speedups  We note that this is merely  
suf cient condition  and our experiments suggest that our
bound is conservative 

In the process of deriving our results  we also develop  
general proof framework for randomized accelerated methods based on Wilson et al    which avoids the use of
estimate sequences in favor of an explicit Lyapunov function  Using our proof framework we are able to recover
recent results  Nesterov   Stich    AllenZhu et al 
  on accelerated coordinate descent  Furthermore  our
proof framework allows us to immediately transfer our results on GaussSeidel over to the randomized accelerated
Kaczmarz algorithm  extending   recent result by Liu and
Wright   on updating   single constraint at   time to
the block case 

Finally  we empirically demonstrate that despite its theoretical nuances  accelerated GaussSeidel using random coordinate selection can provide signi cant speedups in practical applications over GaussSeidel with  xed partition
sampling  as well as the classical conjugategradient  CG 
algorithm  As an example  for   kernel ridge regression
 KRR  task in machine learning on the augmented CIFAR 
  dataset          acceleration with random coordinate sampling performs up to   faster than acceleration with    xed partitioning to reach an error tolerance of
  with the gap substantially widening for smaller error
tolerances  Furthermore  it performs over   faster than
conjugategradient on the same task 

  Background

We assume that we are given an       matrix   which is
positive de nite  and an   dimensional response vector   
We also    an integer   which denotes   block size  Under
the assumption of   being positive de nite  the function
         
  xTAx   xTb is strongly convex and smooth  Recent analysis of GaussSeidel  Gower   Richt arik   
proceeds by noting the connection between GaussSeidel
and  block  coordinate descent on     This is the point of
view we will take in this paper 

  Existing rates for randomized block GaussSeidel

We  rst describe the sketching framework of  Qu et al 
  Gower   Richt arik    and show how it yields
rates on GaussSeidel when blocks are chosen via    xed
partition or randomly at every iteration  While we will only
focus on the special case when the sketch matrix represents
column sampling  the sketching framework allows us to
provide   uni ed analysis of both cases 

To be more precise  let   be   distribution over Rn    and
let Sk     be drawn iid from   
If we perform block
coordinate descent by minimizing   along the range of Sk 
then the randomized block GaussSeidel update is given by

xk    xk   Sk    

  ASk    

   Axk       

 

Column sampling  Every index set         with      
  induces   sketching matrix         eJ    eJ   
where ei denotes the ith standard basis vector in Rn 
and           is any ordering of the elements of    
By equipping different probability measures on     one
can easily describe  xed partition sampling as well as
random coordinate sampling  and many other sampling
schemes  The former puts uniform mass on the index sets

     Jn    whereas the latter puts uniform mass on all cid  
  cid 

index sets of size    Furthermore  in the sketching framework there is no limitation to use   uniform distribution 
nor is there any limitation to use    xed   for every iteration  For this paper  however  we will restrict our attention
to these cases 

Existing rates  Under the assumptions stated above   Qu
et al    Gower   Richt arik    show that for every
      the sequence   satis es

  kxk     kA           kx      kA  

 

where      min   PA    The expectation in   is
taken with respect to the randomness of         and the
expectation in the de nition of   is taken with respect to
       Under both  xed partitioning and random coordinate selection        is guaranteed  see       Gower  
Richt arik    Lemma   Thus    achieves   linear
rate of convergence to the true solution  with the rate governed by the   quantity shown above 

We now specialize   to  xed partitioning and random coordinate sampling  and provide some intuition for why we
expect the latter to outperform the former in terms of iteration complexity  We  rst consider the case when the
sampling distribution corresponds to  xed partitioning  Assume for notational convenience that the  xed partitioning
corresponds to placing the  rst   coordinates in the  rst
partition    the next   coordinates in the second partition
   and so on  Here       part corresponds to   measure
of how close the product of   with the inverse of the block
diagonal is to the identity matrix  de ned as

 min cid     blkdiag cid   

  

      

Jn   cid cid   

 

 part  

 
 

Above  AJi denotes the       matrix corresponding to the
submatrix of   indexed by the ith partition    loose lower
bound on  part is

 part  

 
 

 min   

max        max AJi  

 

 

Breaking Locality Accelerates Block GaussSeidel

On the other hand  in the random coordinate case  Qu et
al    derive   lower bound on      rand as

 rand  

 

  cid         

max     Aii

 min     cid 

 

 

where                 Using the lower bounds
  and   we can upper bound the iteration complexity of  xed partition GaussSeidel Npart
by

 

 

 min   

 min   

max     Aii

max        max AJi  

  cid   
nate GaussSeidel Nrand as   cid   

log cid  and random coordilog cid 

Comparing the bound on Npart to the bound on Nrand 
it is not unreasonable to expect that random coordinate
sampling has better iteration complexity than  xed partition sampling in certain cases 
In Section   we verify
this by constructing instances   such that  xed partition
GaussSeidel
takes arbitrarily more iterations to reach
  prespeci ed error tolerance compared with random
coordinate GaussSeidel 

    

de ned as            
   The family   exhibits
 
  crucial property that  TA       for every    
  permutation matrix   Lee and Wright   recently
exploited this invariance to illustrate the behavior of cyclic
versus randomized permutations in coordinate descent 

We explore the behavior of GaussSeidel as the matrices
   become illconditioned  To do this  we consider   particular parameterization which holds the minimum eigenvalue equal to one and sends the maximum eigenvalue to
in nity via the subfamily     Our  rst proposition characterizes the behavior of GaussSeidel with  xed
partitions on this subfamily 

Proposition   Fix       and positive integers        
such that     pk  Let  Ji  
   be any partition of       
with  Ji       and denote Si   Rn   as the column selector for partition Ji  Suppose     Rn   takes on value Si
with probability     For every        we have that

 part  

 

 

 

      

  Accelerated rates for  xed partition GaussSeidel

Based on the interpretation of GaussSeidel as block coordinate descent on the function     we can use Theorem   of
Nesterov and Stich   to recover   procedure and   rate
for accelerating   in the  xed partition case  the speci  
details are discussed in the full version of the paper  Tu
et al    We will refer to this procedure as ACDM 

  

 part cid 

The convergence guarantee of the ACDM procedure is that
for all      
  kxk     kA       cid      
Above       kx      kA and  part is the same quantity de ned in   Comparing   to the nonaccelerated
GaussSeidel rate given in   we see that acceleration improves the iteration complexity to reach   solution with  
error from   
as discussed in Section  

part log  to   cid    

part log cid 

    

   

 

 

  Results

We now present the main results of the paper  Proofs can
be found in the full version  Tu et al    of this paper 

  Fixed partition vs random coordinate sampling

Our  rst result is to construct instances where GaussSeidel
with  xed partition sampling runs arbitrarily slower than
random coordinate sampling  even if acceleration is used 
Consider the family of       positive de nite matrices  
given by                           with   

Next  we perform   similar calculation under the random
column sampling model 

Proposition   Fix       and integers      such that
           Suppose each column of     Rn   is chosen uniformly at random from       en  without replacement  For every        we have that

 rand  

 

      

 

       

             

 

 

The differences between   and   are striking  Let us
assume that   is much larger than    Then  we have that
 part     for   whereas  rand     
   for   That
is   part can be made arbitrarily smaller than  rand as  
grows 

Our next proposition states that the rate of GaussSeidel
from   is tight orderwise in that for any instance there
always exists   starting point which saturates the bound 
Proposition   Let   be an       positive de nite
matrix  and let   be   random matrix such that    
 min   PA        Let    denote the solution to
Ax      There exists   starting point      Rn such that
the sequence   satis es for all      

  kxk     kA         kkx      kA  

 

From   we see that GaussSeidel using random coordinates computes   solution xk satisfying   kxk  
  kA        in at most         
  log  iterations  On
the other hand  Proposition   states that for any  xed partition  there exists an input    such that       log 
iterations are required to reach the same   error tolerance 

Breaking Locality Accelerates Block GaussSeidel

Furthermore  the situation does not improve even if use
ACDM from  Nesterov   Stich    Proposition  
which we describe later  implies that for any  xed partition

    log cid 
there exists an input    such that      cid    
iterations are required for ACDM to reach   error  Hence
as       the gap between random coordinate and  xed
partitioning can be made arbitrarily large  These  ndings
are numerically veri ed in Section  

    Lyapunov analysis of accelerated GaussSeidel

and Kaczmarz

Motivated by our  ndings  our goal is to understand the
behavior of accelerated GaussSeidel under random coordinate sampling  In order to do this  we establish   general
framework from which the behavior of accelerated GaussSeidel with random coordinate sampling follows immediately  along with rates for accelerated randomized Kaczmarz  Liu   Wright    and the accelerated coordinate
descent methods of  Nesterov   Stich    and  AllenZhu et al   

For conciseness  we describe   simpler version of our
framework which is still able to capture both the GaussSeidel and Kaczmarz results  deferring the general version
to the full version of the paper  Our general result requires
  bit more notation  but follows the same line of reasoning 
Let   be   random       positive semide nite matrix 
Put          and suppose that   exists and is positive
de nite  Furthermore  let     Rn     be strongly convex
and smooth  and let   denote the strong convexity constant
of          the   kG  norm 
Consider the following sequence  xk  yk  zk    de ned
by the recurrence

xk   

 

     

yk  

 

     

zk  

yk    xk    Hk    xk   
 
zk    zk      xk    zk   
 

Hk    xk   

   

   

   

where         are independent realizations of   and
  is   parameter to be chosen  Following  Wilson et al 
  we construct   candidate Lyapunov function Vk for
the sequence   de ned as

Vk      yk        

 
  kzk       

    

 

The following theorem demonstrates that Vk is indeed  
Lyapunov function for  xk  yk  zk 
Theorem   Let         be as de ned above  Suppose
further that   has  Lipschitz gradients       
the   kG 
norm  and for every  xed     Rn 
                  

 
          
   

 

holds for         where                      Set   in
  as       with

     max cid Eh   HG   cid   

Then for every       we have

  Vk           kV   

We now proceed to specialize Theorem   to both the
GaussSeidel and Kaczmarz settings 

  ACCELERATED GAUSSSEIDEL

Let     Rn   denote   random sketching matrix  As suggested in Section   we set          
  xTAx   xTb and put
        TAS      Note that           TAS      is
positive de nite iff  min   PA        and is hence satis ed for both  xed partition and random coordinate sampling       Section   Next  the fact that   is  Lipschitz
       the   kG  norm and the condition   are standard
calculations  All the hypotheses of Theorem   are thus
satis ed  and the conclusion is Theorem   which characterizes the rate of convergence for accelerated GaussSeidel
 Algorithm  

Algorithm   Accelerated randomized block GaussSeidel 

     Rn         Rn               

Require      Rn              Rn  sketching matrices
 Sk   
  Set      
  Set             
  for               do
  yk    
 
  Hk   Sk    
 
 
  end for
  Return yT  

yk    xk    Hk Axk      
zk    zk      xk    zk     

  Hk Axk      

  ASk    
   

xk     

  zk 

Theorem   Let   be an       positive de nite matrix
and     Rn  Let      Rn denote the unique vector satisfying Ax       Suppose each Sk              is an
independent copy of   random matrix     Rn    Put
     min   PA    and suppose the distribution of  
satis es       Invoke Algorithm   with   and   where

     max cid Eh   HG   cid   

 

with         TAS     and          Then with    

   for all      
  kyk     kA   

         kx      kA  

 

Note that in the setting of Theorem   by the de nition
of   and   it is always the case that       Therefore 

Breaking Locality Accelerates Block GaussSeidel

      and      
   min ATA  Hence  the above theorem states that the iteration complexity to reach   error is   cid 

log cid  which matches Theo 

rem   of  Liu   Wright    orderwise  However 
Theorem   applies in general for any sketching matrix 

  min ATA 

  Specializing accelerated GaussSeidel to random

coordinate sampling

We now instantiate Theorem   to random coordinate
sampling  The   quantity which appears in Theorem  
is identical to the quantity appearing in the rate   of nonaccelerated GaussSeidel  That is  the iteration complex 

ity to reach tolerance   is   cid   

rand log cid  and the

only new term here is   In order to provide   more intuitive interpretation of the   quantity  we present an upper
bound on   in terms of an effective block condition number de ned as follows  Given an index set         de 
 ne the effective block condition number of   matrix   as
           maxi   Aii
 min AJ     Note that             AJ   always  The following lemma gives upper and lower bounds
on the   quantity 
Lemma   Let   be an      positive de nite matrix and
let   satisfy            We have that
     cid         cid   
     
where           maxJ                  is de ned
in   and the distribution of   corresponds to uniformly
selecting   coordinates without replacement 

   cid       
     

 cid   

 
       

 

Lemma   states that if the       subblocks of   are
wellconditioned as de ned by the effective block condition number          then the speedup of accelerated
GaussSeidel with random coordinate selection over its
nonaccelerate counterpart parallels the case of  xed partitioning sampling       the rate described in   versus the
rate in   This is   reasonable condition  since very illconditioned subblocks will lead to numerical instabilities
in solving the subproblems when implementing GaussSeidel  On the other hand  we note that Lemma   provides merely   suf cient condition for speedups from acceleration  and is conservative  Our numerically experiments in Section    suggest that in many cases the
  parameter behaves closer to the lower bound     than
Lemma   suggests 

the iteration complexity of acceleration is at least as good
as the iteration complexity without acceleration 

We conclude our discussion of GaussSeidel by describing the analogue of Proposition   for Algorithm   which
shows that our analysis in Theorem   is tight orderwise 
The following proposition applies to ACDM as well  we
show in the full version of the paper how ACDM can be
viewed as   special case of Algorithm  

Proposition   Under the setting of Theorem   there

exists starting positions         Rn such that the iterates
 yk  zk    produced by Algorithm   satisfy
  kyk     kA   kzk     kA           kky      kA  

  ACCELERATED KACZMARZ

The argument for Theorem   can be slightly modi ed to
yield   result for randomized accelerated Kaczmarz in the
sketching framework  for the case of   consistent overdetermined linear system 
Speci cally  suppose we are given an     matrix   which
has full column rank  and          Our goal is to recover
the unique    satisfying Ax       To do this  we apply
  similar line of reasoning as  Lee   Sidford    We
set          
  and     PATS  where   again
is our random sketching matrix  At  rst  it appears our
choice of   is problematic since we do not have access to
  and      but   quick calculation shows that          
   TA     Ax      Hence  with rk   Axk      the sequence   simpli es to

 kx       

 

 

zk  

     

xk   
yk  
yk    xk        
zk    zk      xk    zk   

     
       

  rk   

   

   

 
 

    

       

  rk       

The remainder of the argument proceeds nearly identically 
and leads to the following theorem 
Theorem   Let   be an       matrix with full colSuppose each Sk     
umn rank  and     Ax 
        is an independent copy of   random sketching matrix     Rm    Put     PATS and    
     The sequence   with      min   PATS     
 max cid   cid   HG cid cid  and        satis es
for all      

  kyk         

         kx          

 

We can now combine Theorem   with   to derive the
following upper bound on the iteration complexity of accelerated GaussSeidel with random coordinates as

Specialized to the setting of  Liu   Wright    where
each row of   has unit norm and is sampled uniformly
at every iteration  it can be shown  Section    that

Nrand acc       

ps max     Aii

 min   

        log   

Breaking Locality Accelerates Block GaussSeidel

Illustrative example  We conclude our results by illustrating our bounds on   simple example  Consider the subfamily           with

     An            

 

  

  simple calculation yields that             
      
   cid      
  cid 
and hence Lemma   states that        
Furthermore  by   similar calculation to Proposition  
 rand  
         Assuming for simplicity that    
     and         Theorem   states that at most
     
log  iterations are suf cient for an  accurate
  
solution  On the other hand  without acceleration   states
that      
   log  iterations are suf cient and Proposition   shows there exists   starting position for which it is
necessary  Hence  as either   grows large or   tends to zero 
the bene ts of acceleration become more pronounced 

  Related Work

We split the related work into two broad categories of interest      work related to coordinate descent  CD  methods
on convex functions and     randomized solvers designed
for solving consistent linear systems 

When   is positive de nite  GaussSeidel can be interpreted as an instance of coordinate descent on   strongly
convex quadratic function  We therefore review related
work on both nonaccelerated and accelerated coordinate
descent  focusing on the randomized setting instead of the
more classical cyclic order or GaussSouthwell rule for selecting the next coordinate  See  Tseng   Yun    for
  discussion on nonrandom selection rules   Nutini et al 
  for   comparison of random selection versus GaussSouthwell  and  Nutini et al    for ef cient implementations of GaussSouthwell 

Nesterov   original paper in    rst considered randomized CD on convex functions  assuming   partitioning
of coordinates  xed ahead of time  The analysis included
both nonaccelerated and accelerated variants for convex
functions  This work sparked   resurgence of interest in CD
methods for large problems  Most relevant to our paper are
extensions to the block setting  Richt arik   Tak       
handling arbitrary sampling distributions  Qu   Richt arik 
      Fountoulakis   Tappenden    and second order updates for quadratic functions  Qu et al   

For accelerated CD  Lee and Sidford   generalize the
analysis of Nesterov   While the analysis of  Lee  
Sidford    was limited to selecting   single coordinate
at   time  several follow on works  Qu   Richt arik     
Lin et al    Lu   Xiao    Fercoq   Richt arik 
  generalize to block and nonsmooth settings  More
recently  both AllenZhu et al    and Nesterov and

Stich   independently improve the results of  Lee  
Sidford    by using   different nonuniform sampling
distribution  One of the most notable aspects of the analysis
in  AllenZhu et al    is   departure from the  probabilistic  estimate sequence framework of Nesterov  Instead 
the authors construct   valid Lyapunov function for coordinate descent  although they do not explicitly mention this 
In our work  we make this Lyapunov point of view explicit  The constants in our acceleration updates arise from
  particular discretization and Lyapunov function outlined
from Wilson et al    Using this framework makes our
proof particularly transparent  and allows us to recover results for strongly convex functions from  AllenZhu et al 
  and  Nesterov   Stich    as   special case 

From the numerical analysis side both the GaussSeidel and
Kaczmarz algorithm are classical methods  Strohmer and
Vershynin   were the  rst to prove   linear rate of
convergence for randomized Kaczmarz  and Leventhal and
Lewis   provide   similar kind of analysis for randomized GaussSeidel  Both of these were in the single
constraint coordinate setting  The block setting was later
analyzed by Needell and Tropp   More recently 
Gower and Richt arik   provide   uni ed analysis for
both randomized block GaussSeidel and Kaczmarz in the
sketching framework  We adopt this framework in this paper  Finally  Liu and Wright   provide an accelerated
analysis of randomized Kaczmarz once again in the single
constraint setting and we extend this to the block setting 

  Experiments

In this section we experimentally validate our theoretical
results on how our accelerated algorithms can improve convergence rates  Our experiments use   combination of
synthetic matrices and matrices from large scale machine
learning tasks 

Setup  We run all our experiments on     socket Intel Xeon
CPU    machine with   cores per socket and  TB of
DRAM  We implement all our algorithms in Python using
numpy  and use the Intel MKL library with   OpenMP
threads for numerical operations  We report errors as relA  Finally  we use the

ative errors       kxk       
best values of   and   found by tuning each experiment 

  kx   

We implement  xed partitioning by creating random blocks
of coordinates at the beginning of the experiment and cache
the corresponding matrix blocks to improve performance 
For random coordinate sampling  we select   new block of
coordinates at each iteration 

For our  xed partition experiments  we restrict our
attention to uniform sampling  While Gower and
Richt arik   propose   nonuniform scheme based on
Tr   TAS  for translationinvariant kernels this reduces to

Breaking Locality Accelerates Block GaussSeidel

Figure   Experiments comparing  xed partitions versus random
coordinate sampling for the example from Section   with    
  coordinates  block size      

Figure   Experiments comparing  xed partitions versus uniform
random sampling for CIFAR  augmented matrix while running
kernel ridge regression  The matrix has       coordinates
and we set block size to      

uniform sampling  Furthermore  as the kernel block Lipschitz constants were also roughly the same  other nonuniform schemes  AllenZhu et al    also reduce to
nearly uniform sampling 

  Fixed partitioning vs random coordinate sampling

Our  rst set of experiments numerically verify the separation between  xed partitioning sampling versus random
coordinate sampling 

Figure   shows the progress per iteration on solving
         with the    de ned in Section   Here
we set                   and           
Figure   veri es our analytical  ndings in Section   that
the  xed partition scheme is substantially worse than uniform sampling on this instance  It also shows that in this
case  acceleration provides little bene   in the case of random coordinate sampling  This is because both   and  
are orderwise      and hence the rate for accelerated and
nonaccelerated coordinate descent coincide  However we
note that this only applies for matrices where   is as large
as it can be            that is instances for which GaussSeidel is already converging at the optimal rate  see  Gower
  Richt arik    Lemma  

  Kernel ridge regression

We next evaluate how  xed partitioning and random coordinate sampling affects the performance of GaussSeidel
on large scale machine learning tasks  We use the popular image classi cation dataset CIFAR  and evaluate  
kernel ridge regression  KRR  task with   Gaussian kernel  Speci cally  given   labeled dataset  xi  yi  
   we
solve the linear system              with Kij  
exp kxi   xjk 
  where         are tunable parameters  The key property of KRR is that the kernel matrix  
is positive semide nite  and hence Algorithm   applies 

Figure   Comparing conjugate gradient with accelerated and unaccelerated GaussSeidel methods for CIFAR  augmented matrix while running kernel ridge regression  The matrix has    
  coordinates and we set block size to      

For the CIFAR  dataset  we augment the dataset  to include  ve re ections  translations perimage and then apply standard preprocessing steps used in image classi cation  Coates   Ng    Sparks et al    We  nally
apply   Gaussian kernel on our preprocessed images and
the resulting kernel matrix has       coordinates 
We also include experiments on   smaller MNIST kernel
matrix        in Section   

Results from running   iterations of random coordinate
sampling and  xed partitioning algorithms are shown in
Figure   Comparing convergence across iterations  similar to previous section  we see that unaccelerated GaussSeidel with random coordinate sampling is better than accelerated GaussSeidel with  xed partitioning  However
we also see that using acceleration with random sampling
can further improve the convergence rates  especially to
achieve errors of   or lower 

 Similar to https github com akrizhevsky cudaconvnet 

 Iteration kxk       kx   AId RankOne     GSFixedPartitionGSAccFixedPartitionGSRandomCoordinatesGS AccRandomCoordinates Iteration kxk       kx   ACIFAR KRR       kGSFixedPartitionGSAccFixedPartitionGSRandomCoordinatesGS AccRandomCoordinates Time   kxk       kx   ACIFAR KRR       kGSFixedPartitionGSAccFixedPartitionGSRandomCoordinatesGS AccRandomCoordinatesConjugateGradientBreaking Locality Accelerates Block GaussSeidel

CG  In terms of iteration complexity  standard results state
that CG takes at most    log  iterations to reach
an   error solution  where   denotes the condition number of    On the other hand  GaussSeidel takes at most
    
In the
case of any  normalized  kernel matrix associated with  
translationinvariant kernel such as the Gaussian kernel  we
have max     Aii     and hence generally speaking
    is much lower than  

      log  where       max     Aii

 min   

 

Figure   The effect of block size on the accelerated GaussSeidel
method  For the MNIST dataset  preprocessed using random features  we see that block size of       works best 

We also compare the convergence with respect to running
time in Figure   Fixed partitioning has better performance
in practice random access is expensive in multicore systems  However  we see that this speedup in implementation
comes at   substantial cost in terms of convergence rate 
For example in the case of CIFAR  using  xed parti 

tions leads to an error of     after around   sec 

onds  In comparison we see that random coordinate sampling achieves   similar error in around   seconds and
is thus   faster  We also note that this speedup increases
for lower error tolerances 

  Comparing GaussSeidel to ConjugateGradient

We also compared GaussSeidel with random coordinate
sampling to the classical conjugategradient  CG  algorithm  CG is an important baseline to compare with  as it
is the defacto standard iterative algorithm for solving linear systems in the numerical analysis community  While
we report the results of CG without preconditioning  we
remark that the performance using   standard banded preconditioner was not any better  However  for KRR specifically  there have been recent efforts  Avron et al   
Rudi et al    to develop better preconditioners  and we
leave   more thorough comparison for future work  The
results of our experiment are shown in Figure   We note
that GaussSeidel both with and without acceleration outperform CG  As an example  we note that to reach error   on CIFAR  CG takes roughly   seconds 
compared to less than   seconds for accelerated GaussSeidel  which is     improvement 
To understand this performance difference  we recall that
our matrices   are fully dense  and hence each iteration
of CG takes      On the other hand  each iteration of
both nonaccelerated and accelerated GaussSeidel takes
  np       Hence  as long as          the time
per iteration of GaussSeidel is orderwise no worse than

  Effect of block size

We next analyze the importance of the block size   for the
accelerated GaussSeidel method  As the values of   and  
change for each setting of    we use   smaller MNIST matrix for this experiment  We apply   random feature transformation  Rahimi   Recht    to generate an      
matrix   with       features  We then use       TF
and       TY as inputs to the algorithm  Figure   shows
the wall clock time to converge to   error as we vary
the block size from       to      

Increasing the blocksize improves the amount of progress
that is made per iteration but the time taken per iteration
increases as       Line   Algorithm   However  using
ef cient BLAS  primitives usually affords   speedup from
systems techniques like cache blocking  We see the effects
of this in Figure   where using       performs better
than using       We also see that these bene ts reduce
for much larger block sizes and thus       is slower 

  Conclusion

In this paper  we extended the accelerated block GaussSeidel algorithm beyond  xed partition sampling  Our
analysis introduced   new datadependent parameter  
which governs the speedup of acceleration  Specializing our theory to random coordinate sampling  we derived
an upper bound on   which shows that well conditioned
blocks are   suf cient condition to ensure speedup  Experimentally  we showed that random coordinate sampling is
readily accelerated beyond what our bound suggests 

The most obvious question remains to derive   sharper
bound on the   constant from Theorem   Another interesting question is whether or not the iteration complexity
of random coordinate sampling is always bounded above
by the iteration complexity with  xed coordinate sampling 

We also plan to study an implementation of accelerated
GaussSeidel in   distributed setting  Tu et al    The
main challenges here are in determining how to sample
coordinates without signi cant communication overheads 
and to ef ciently estimate   and   To do this  we wish to
explore other sampling schemes such as shuf ing the coordinates at the end of every epoch  Recht         

 Time   to errorp           MNISTRandomFeatures   Breaking Locality Accelerates Block GaussSeidel

Acknowledgements

We thank Ross Boczar for assisting us with Mathematica support for noncommutative algebras  Orianna DeMasi for providing useful feedback on earlier drafts of
this manuscript  and the anonymous reviewers for their
helpful feedback  ACW is supported by an NSF Graduate Research Fellowship  BR is generously supported by
ONR awards    and   
NSF award CCF  the DARPA Fundamental Limits of Learning  Fun LoL  Program    Sloan Research Fellowship  and   Google Research Award  This research is
supported in part by DHS Award HSHQDC 
NSF CISE Expeditions Award CCF  DOE Award
SN  DESC  and DARPA XData Award
FA  and gifts from Amazon Web Services 
Google  IBM  SAP  The Thomas and Stacey Siebel Foundation  Apple Inc  Arimo  Blue Goji  Bosch  Cisco  Cray 
Cloudera  Ericsson  Facebook  Fujitsu  HP  Huawei  Intel  Microsoft  Mitre  Pivotal  Samsung  Schlumberger 
Splunk  State Farm and VMware 

References

AllenZhu  Zeyuan  Richt arik  Peter  Qu  Zheng  and Yuan 
Yang  Even Faster Accelerated Coordinate Descent Using NonUniform Sampling  In ICML   

Avron  Haim  Clarkson  Kenneth    and Woodruff 
David    Faster Kernel Ridge Regression Using Sketching and Preconditioning  arXiv     

Coates  Adam and Ng  Andrew    Learning Feature Representations with KMeans  In Neural Networks  Tricks
of the Trade  Springer   

Fercoq  Olivier and Richt arik  Peter  Accelerated  Parallel 
and Proximal Coordinate Descent  SIAM    Optim   
   

Fountoulakis  Kimon and Tappenden  Rachael    Flexible
Coordinate Descent Method  arXiv     

Gower  Robert    and Richt arik  Peter  Randomized Iterative Methods for Linear Systems  SIAM Journal on
Matrix Analysis and Applications     

Conditioning  Mathematics of Operations Research   
   

Lin  Qihang  Lu  Zhaosong  and Xiao  Lin  An Accelerated

Proximal Coordinate Gradient Method  In NIPS   

Liu  Ji and Wright  Stephen    An Accelerated Randomized
Kaczmarz Algorithm  Mathematics of Computation   
   

Lu  Zhaosong and Xiao  Lin  On the Complexity Analysis of Randomized BlockCoordinate Descent Methods 
Mathematical Programming     

Needell  Deanna and Tropp  Joel    Paved with Good Intentions  Analysis of   Randomized Block Kaczmarz
Method  Linear Algebra and its Applications     

Nesterov  Yurii  Ef ciency of Coordinate Descent Methods
on HugeScale Optimization Problems  SIAM    Optim 
   

Nesterov  Yurii and Stich  Sebastian  Ef ciency of Accelerated Coordinate Descent Method on Structured
Optimization Problems  Technical report  Universit  
catholique de Louvain  CORE Discussion Papers   

Nutini  Julie  Schmidt  Mark  Laradji  Issam    Friedlander  Michael  and Koepke  Hoyt  Coordinate Descent
Converges Faster with the GaussSouthwell Rule Than
Random Selection  In ICML   

Nutini  Julie  Sepehry  Behrooz  Laradji  Issam  Schmidt 
Mark  Koepke  Hoyt  and Virani  Alim  Convergence
Rates for Greedy Kaczmarz Algorithms  and Faster
Randomized Kaczmarz Rules Using the Orthogonality
Graph  In UAI   

Qu  Zheng and Richt arik  Peter  Coordinate Descent
with Arbitrary Sampling    Algorithms and Complexity 
arXiv       

Qu  Zheng and Richt arik  Peter  Coordinate Descent with
Arbitrary Sampling II  Expected Separable Overapproximation  arXiv       

Lee  ChingPei and Wright  Stephen    Random Permutations Fix   Worst Case for Cyclic Coordinate Descent 
arXiv     

Qu  Zheng  Richt arik  Peter  and Zhang  Tong  Randomized Dual Coordinate Ascent with Arbitrary Sampling 
In NIPS   

Lee  Yin Tat and Sidford  Aaron  Ef cient Accelerated
Coordinate Descent Methods and Faster Algorithms for
Solving Linear Systems  In FOCS   

Qu  Zheng  Richt arik  Peter  Tak      Martin  and Fercoq 
Olivier  SDNA  Stochastic Dual Newton Ascent for Empirical Risk Minimization  In ICML   

Leventhal  Dennis and Lewis  Adrian    Randomized
Methods for Linear Constraints  Convergence Rates and

Rahimi  Ali and Recht  Benjamin  Random Features for

LargeScale Kernel Machines  In NIPS   

Breaking Locality Accelerates Block GaussSeidel

Recht  Benjamin and      Christopher  Parallel Stochastic Gradient Algorithms for LargeScale Matrix Completion  Mathematical Programming Computation   
   

Richt arik  Peter and Tak      Martin  Iteration Complexity
of Randomized BlockCoordinate Descent Methods for
Minimizing   Composite Function  Mathematical Programming     

Rudi  Alessandro  Carratino  Luigi  and Rosasco  Lorenzo 
FALKON  An Optimal Large Scale Kernel Method 
arXiv     

Sparks  Evan    Venkataraman  Shivaram  Kaftan  Tomer 
Franklin  Michael  and Recht  Benjamin  KeystoneML 
Optimizing Pipelines for LargeScale Advanced Analytics  In ICDE   

Strohmer  Thomas and Vershynin  Roman    Randomized Kaczmarz Algorithm with Exponential Convergence  Journal of Fourier Analysis and Applications   
   

Tseng  Paul and Yun  Sangwoon    Coordinate Gradient
Descent Method for Nonsmooth Separable Minimization  Mathematical Programming     

Tu  Stephen  Roelofs  Rebecca  Venkataraman  Shivaram 
and Recht  Benjamin  Large Scale Kernel Learning
using Block Coordinate Descent  arXiv   
 

Tu  Stephen  Venkataraman  Shivaram  Wilson  Ashia   
Gittens  Alex  Jordan  Michael    and Recht  Benjamin  Breaking Locality Accelerates Block GaussSeidel  arXiv     

Wilson  Ashia    Recht  Benjamin  and Jordan  Michael   
  Lyapunov Analysis of Momentum Methods in Optimization  arXiv     

