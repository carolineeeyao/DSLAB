Learning Gradient Descent  Better Generalization and Longer Horizons

Kaifeng Lv     Shunhua Jiang     Jian Li  

Abstract

  Existing Work

Training deep neural networks is   highly nontrivial task  involving carefully selecting appropriate training algorithms  scheduling step sizes
and tuning other hyperparameters  Trying different combinations can be quite laborintensive
and time consuming  Recently  researchers have
tried to use deep learning algorithms to exploit
the landscape of the loss function of the training
problem of interest  and learn how to optimize
over it in an automatic way 
In this paper  we
propose   new learningto learn model and some
useful and practical tricks  Our optimizer outperforms generic  handcrafted optimization algorithms and stateof theart learningto learn optimizers by DeepMind in many tasks  We demonstrate the effectiveness of our algorithms on  
number of tasks  including deep MLPs  CNNs 
and simple LSTMs 

  Introduction
Training   neural network can be viewed as solving an optimization problem for   highly nonconvex loss function 
Gradientbased algorithms are by far the most widely used
algorithms for training neural networks  such as basic SGD 
Adagrad  RMSprop  Adam  etc  For   particular neural
network  it is unclear   priori which one is the best optimization algorithm  and how to set up the hyperparameters  such as learning rates  It usually takes   lot of time
and experienced hands to identify the best optimization algorithm together with best hyperparameters  and possibly
some other tricks are necessary to make the network work 

 Equal contribution  The research is supported in part
by the National Basic Research Program of China grants
 CB   CBA   CBA  and the
National NSFC grants  
Interdisciplinary Information Sciences  Tsinghua University  Beijing  China 
Correspondence to  Kaifeng Lv    eaking com  Shunhua Jiang  linda com  Jian Li
 lijian mail tsinghua edu cn 

 Institute for

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

To address the above issue    promising approach is to
use machine learning algorithms to replace the hardcoded
optimization algorithms  and hopefully  the learning algorithm is capable of learning   good strategy  from experience  to explore the landscape of the loss function and
adaptively choose good descent steps  In   high level  the
idea can be categorized under the umbrella of learningto 
learn  or metalearning    broad area known to learning
community for more than two decades 
Using deep learning for training deep neural networks was
initiated in   recent paper  Andrychowicz et al    The
authors proposed an optimizer using coordinatewise Long
Short Term Memory  LSTM   Hochreiter   Schmidhuber 
  that takes the gradients of the optimizee as input and
outputs the updates for each optimizee parameters  We
call this optimizer DMoptimizer throughout this paper  and
we use the term optimizee to refer to the loss function of
the neural network being optimized  The authors showed
that DMoptimizer outperforms traditional optimization algorithms in solving the task on which it is trained  and it
also generalizes well to the same type of tasks 
In one
of their experiments  they trained DMoptimizer to minimize the average loss of    step training process of  
 hiddenlayer Multilayer Perceptron  MLP  with sigmoid
as the activation function  and the optimizer was shown to
have generalization ability to some extent  it also performs
well on such MLP with one more hidden layer or double
hidden neurons  However  there are still some limitations 

  If the activation function of the MLP is changed from
sigmoid to ReLU in the test phase  DMoptimizer performs poorly to train such MLP  In other words  their
algorithms fail to generalize to different activations 

  Even though the authors showed that DMoptimizer
performs well to train the optimizee for   descent
steps  the loss increases dramatically for much longer
horizons  In other words  their algorithms fail to handle   relatively large number of descent steps 

  Our Contributions

In this paper  we propose two new training tricks and   new
model to improve the results of training   recurrent neural

Learning Gradient Descent  Better Generalization and Longer Horizons

network  RNN  to optimize the loss functions of realworld
neural networks 
The most effective trick is Random Scaling  which is used
when training the RNN optimizer to improve its generalization ability by randomly scaling the parameters of the
optimizee  The other trick is to combine the loss function
of the optimizee with other simple convex functions  which
helps to accelerate the training process  With the help of
our new training tricks  our new model  called RNNprop 
achieves notable improvements upon previous work after
being trained on   simple  hiddenlayer MLP 

  It can train optimizees for longer horizons 

In particular  when RNNprop is only trained to minimize
the  nal loss of    step training process  in testing
phase it can successfully train optimizees for several
thousand steps 

  It can generalize to   variety of neural networks
including much deeper MLPs  CNNs  and simple
LSTMs  On these tasks it achieves better or at least
comparable performance with traditional optimization
algorithms 

  Other Related Work
  Learning to Learn

The notion of learning to learn or metalearning has been
used to address the concept of learning metaknowledge
about the learning process for years  However  there is
no agreement on the exact de nition of metalearning  and
various concepts have been developed by different authors
 Thrun   Pratt    Vilalta   Drissi    Brazdil et al 
 
In this paper  we view the training process of   neural network as an optimization problem  and we use an RNN as
an optimizer to train other neural networks  The usage of
another neural network to direct the training of neural networks has been put forward by Naik and Mammone  
In their early work  Cotter and Younger     argued that RNNs can be used to model adaptive optimization algorithms  Prokhorov et al    This idea was further developed in  Younger et al    Hochreiter et al 
  and gradient descent is used to train an RNN optimizer on convex problems  Recently  as shown in Section
  Andrychowicz et al    proposed   more general
optimizer model using LSTM to learn gradient descent  and
our work directly follows their work  In another recent paper  Chen et al    an RNN is used to take current
position and value as input and outputs the next position 
and it works well for blackbox optimization and simple
RL tasks 

From   reinforcement learning perspective  the optimizer
can be viewed as   policy which takes the current state
as input and output the next action  Schmidhuber et al 
  Two recent papers  Daniel et al    Hansen 
  trained adaptive controllers to adjust the hyperparameters  learning rate  of traditional optimization algorithms from this perspective  Their method can be regarded
as hyperparameter optimization  More general methods
have been introduced in  Li   Malik    Wang et al 
  which also take the RL perspective and train   neural
network to model   policy 

  Traditional Optimization Algorithms

  great number of optimization algorithms have been proposed to improve the performance of vanilla gradient descent  including Momentum Tseng    Adagrad Duchi
et al    Adadelta Zeiler    RMSprop Tieleman
  Hinton    Adam Kingma   Ba    The update
rules of several common optimization algorithms are listed
in Table  

Table   Traditional optimization algorithms    are the parameters
of   neural network and   represents the gradient         
are the hyperparameters of an optimization algorithm  All vector
operations are coordinatewise 

Name

Update Rule
      gt

SGD
Momentum mt    mt         gt 

Adagrad

Adadelta

RMSprop

Adam

       mt
   
Gt   Gt      
       gtG
 
 
vt    vt           
   
      gtv
 
    
  
Dt    Dt           
vt    vt           
   
       gtv
mt    mt         gt 
vt    vt           
   
 mt   mt      
 
 vt   vt      
 
        mt  
 
 

 
 

  Rethinking of Optimization Problems
  Problem Formalization

We are interested in  nding an optimizer that undertakes
the optimization tasks for different optimizees  An optimizee is   function     to be minimized  In the case when
the optimizee is stochastic  that is  the value of     de 

Learning Gradient Descent  Better Generalization and Longer Horizons

pends on the sample   selected from   dataset    the goal
of an optimizer is to minimize

 cid 

   

 
   

fd 

 

over the variables  
When optimizing an optimizee on   dataset    the behavior
of an optimizer can be summarized by the following loop 
For each step 
  Given the current parameters    and   sample dt     
perform forward and backward propagation to compute the function value yt   fdt    and the gradient
gt    fdt   

  Based on the current state ht  of the optimizer  and
the gradient gt  the optimizer produces the new state
ht  and proposes an increment    

  Update the parameters by setting               

In the initialization phase     is produced by the optimizer 
and   is generated according to the initialization rule of
the given optimizee  At the end of the loop  we take    as
the  nal optimizee parameters 

  Some Insight into Adaptivity

Table   summaries optimization algorithms that are most
commonly used when training neural networks  All of
these optimization algorithms have some degree of adaptivity  that is  they are able to adjust the effective step size
    when training 
We can divide these algorithms into two classes  The  rst
class includes SGD and Momentum  as they determine the
effective step size by the absolute size of gradients  The
second class includes Adagrad  Adadelta  RMSprop  and
Adam  These algorithms maintain the sum or the moving
average of past gradients   
    which can be seen as  with
  little abuse of terminology  the second raw moment  or
uncentered variance  Then  these algorithms produce the
effective step size only by the relative size of the gradient 
namely  the gradient divided by the square root of the second moment coordinatewise 
In   training process  as the parameters gradually approach
to   local minimum    smaller effective step size is required for   more careful local optimization  To obtain
such smaller effective step size  these two classes of algorithms have two different mechanisms  For the  rst class 
if we take the full gradient  the effective step size automatically gets smaller when approaching to   local minimum 
However  since we use stochastic gradient descent  the effective step size may not be small enough  even if   is not

far from   local minimum  For the second class    smaller
effective step size       of each coordinate   is mainly induced by   relatively smaller partial derivative comparing
with past partial derivatives  When approaching to   local minimum  the gradient may  uctuate due to stochastic
nature  Algorithms of the second class can decrease the
effective step size of each coordinate in accordance with
the  uctuation amplitude of that coordinate         coordinate with larger uncentered variance yields smaller effective step size  Thus  the algorithms of the second class are
able to further decrease effective step size for the coordinates with more uncertainty  and they are more robust than
those of the  rst class 
To get more insight into the difference between these two
classes of algorithms  we consider what happens if we scale
the optimizee by   factor         let        cf   Ideally  the scaling should not affect the behaviors of the algorithms  However  for the algorithms of the  rst class  since
               the effective step size is also scaled by   
Hence  the behaviors of the algorithms change completely 
But for the algorithms of the second class  they behave the
same on      and     since the scale factor   is canceled
out  Thus the algorithms of the second class are more robust with respect to scaling 
The above observation  albeit very simple  is   key inspiration for our new model  On the one hand  we use some
training tricks so that our model can be exposed to functions with different scales at the training stage  On the other
hand  we take relative gradients as input so that our optimizer belongs to the second class  In the following section 
we introduce our training tricks and new model in details 

  Methods
Our RNN optimizer operates coordinatewise on parameters   which follows directly from  Andrychowicz et al 
  The RNN optimizer handles the gradients coordinatewise and maintains hidden states for every coordinate
respectively  The parameters of the RNN itself are shared
between different coordinates  In this way  the RNN optimizer can train optimizees with any number of parameters 

  Random Scaling

We propose   training trick  called Random Scaling  to prevent over tting when training our model  Before introducing our ideas  consider what happens if we train an RNN
optimizer to minimize        cid cid 
  with initial parame 
       is the optimal polter   Clearly              
icy since the lowest point can be reached in just one step 
However  if the RNN optimizer learns to follow this rule
exactly  testing this RNN optimizer on the same function
with different   might produce   modest or even bad result 

Learning Gradient Descent  Better Generalization and Longer Horizons

The method to solve this issue is rather simple  We randomly pick     for every iteration when training our RNN
optimizer  Notice that we can also pick   random number to
scale all the parameters to achieve the same goal  To further
generalize this idea  we design our training trick  Random
Scaling  which coordinatewise randomly scales the parameters of the objective function in the training stage 
In more details  for each iteration of training the optimizer
on   loss function     with initial parameter   we  rst
randomly pick   vector   of the same dimension as   where
each coordinate of   is sampled independently from   distribution    Then  we train our model on   new optimizee

fc         

 
with initial parameter    where all the multiplication
and inversion operations are performed coordinatewise  In
this way  the RNN optimizer is forced to learn an adaptive
policy to determine the best effective step size  rather than
to learn the best effective step size itself of   particular task 

  Combination with Convex Functions

Now we introduce another training trick 
It is clear that
we should train our RNN optimizer on optimizees implemented with neural networks  However  due to nonconvex
and stochastic nature of neural networks  it may be hard for
an RNN to learn the basic idea of gradient descent 
Our idea is loosely inspired by the proximal algorithms  see
      Parikh   Boyd    To make training easier  we
combine the original optimizee function   with an ndim
convex function   to get   new optimizee function  

                   

 

For every iteration of training RNN optimizer  we generate
  random vector   in ndim vector space  and the function
  is de ned as

  cid 

  

      

 
 

 xi   vi 

 

where the initial value of   is also generated randomly 
Without this trick  the RNN optimizer wanders around aimlessly on the nonconvex loss surface of function   in the
beginning stage of training  After we combine the optimizee with function    since   has the good property of convexity  our RNN optimizer soon learns some basic knowledge of gradient descent from these additional optimizee
coordinates  This knowledge is shared with other coordinates because the RNN optimizer processes its input coordinatewise  In this way  we can accelerate the training
process of the RNN optimizer  As the training continues 
the RNN optimizer further learns   better method with gradient decent as   baseline 

We can apply Random Scaling on the function   as well to
make the behavior of the RNN optimizer more robust 

  RNNprop Model

Aside from the above two tricks  we also design   new
model RNNprop as shown in Figure   All the operations in
our model are coordinatewise  following the idea of DMoptimizer idea in  Andrychowicz et al   
The main difference between RNNprop and DMoptimizer
is the input  The input  mt and  gt are de ned as follows 

 
 mt    mt  
 
 
 gt   gt  
 

 

 

 
 

where  mt   vt are de ned the same way as Adam in Table
  This change of the input has three advantages  First 
this input contains no information about the absolute size of
gradients  so our algorithm belongs to the second class automatically and hence is more robust  Second  this manipulation of gradients can be seen as   kind of normalization
so that the input values are bounded by   constant  which
is somewhat easier for   neural network to learn  Lastly  if
our model outputs   constant times  mt  it reduces to Adam 
Similarly  if our model outputs   constant times  gt  then
it reduces to RMSprop  Hence  the hope is that by further optimizing the parameters of RNNprop  it is capable
of achieving better performance than Adam and RMSprop
with  xed learning rate 
The input is preprocessed by   fullyconnected layer with
ELU  Exponential Linear Unit  as the activation function
 Clevert et al    before being handled by the RNN 
The central part of our model is the RNN  which is   twolayer coordinatewise LSTM that is same as DMoptimizer 
The RNN outputs   single vector xout  and the increment
is taken as

       tanh xout 

 

This formula can be viewed as   variation of gradient clipping so that all effective step sizes are bounded by the preset parameter   In all our experiments  we just set   large
enough value      

 mt  gt

Preprocessing

 

RNN

  

Figure   The structure of our model RNNprop 

Learning Gradient Descent  Better Generalization and Longer Horizons

Figure   Performance on the base MLP  Left  RNNprop achieves comparable performance when allowed to run for   steps  Right 
RNNprop continues to decrease the loss even for   steps  but the performance is slightly worse than some traditional algorithms 

  Experiments
We trained two RNN optimizers  one to reproduce DMoptimizer in  Andrychowicz et al    the other to implement RNNprop with our new training tricks  Their performances were compared in   number of experiments   
We use the same optimizee as in  Andrychowicz et al 
  to train these two optimizers  which is the crossentropy loss of   simple MLP on the MNIST dataset  For
convenience  we address this MLP as the base MLP  It has
one hidden layer of   hidden units and uses sigmoid as
activation function  The value of     is computed using
  minibatch of   random pictures  For each iteration
during training  the optimizers are allowed to run for  
steps  Optimizers are trained using truncated Backpropagation Trough Time  BPTT  We split the   steps into  
periods of   steps  In each period  we initialize the initial parameter   and initial hidden state    from the last
period or generate them if it is the  rst period  Adam is
used to minimize the loss       
   wtf     We
trained DMoptimizer using the loss with wt     for all  
as in  Andrychowicz et al    For RNNprop we set
wT     and wt     for other    In this way  the optimizer
is not strictly required to produce   low loss at each step  so
it can be more  exible  We also notice that this loss results
in slightly better performance 
The structure of our model RNNprop is shown in Section
  The RNN is   twolayer LSTM whose hidden state
size is   To avoid division by zero  in actual experiments
we add another term       and the input is changed to
 
 

 mt    mt   
 gt   gt   

     
     

 cid  

 

The parameters   and   for computing mt and gt are

 Our code can be found at https github com 

vfleaking rnnprop 

    ci    

simply set to   In preprocessing  the input is mapped to
   dim vector for each coordinate 
When training RNNprop  we  rst apply Random Scaling
to the optimizee function   and the convex function   respectively  where   is de ned as Equation   and then we
combine them together as introduced in Section   We set
the dimension of the convex function   to be       and
generate the vectors   and   from      uniformly randomly  To generate each coordinate of the vector   in Random Scaling  we  rst generate   number   from       
uniformly randomly  and then take exp    as the value of
that coordinate  where exp is the natural exponential function  This implementation is aimed to produce   of differ 
      Pr   
ent order of magnitude       Pr   
ci     We also tried other transformations including
using uniform distribution  scaling the entire function directly  randomly dropping some coordinates  etc  This version of Random Scaling is selected after comprehensive
comparison  In the experiments we set       for the function   and       for the function   
We save all the parameters of the RNN optimizers every
  iterations when training  For DMoptimizer  we select the saved optimizer with the best performance on the
validation task  same as in  Andrychowicz et al   
Since RNNprop tends not to over   to the training task because of the Random Scaling method  we simply select the
saved optimizer with lowest average train loss  which is
the moving average of the losses of the past   iterations with decay factor   The selected optimizers are
then tested on other different tasks  Their performances
are compared with the best traditional optimization algorithms whose learning rates are carefully chosen and other
hyperparameters are set to the default values in Tensor ow
 Abadi et al    All the initial optimizee parameters
used in the experiments are generated independently from
the Gaussian distribution      

Learning Gradient Descent  Better Generalization and Longer Horizons

All  gures shown in this section were plotted after running
the optimization process multiple times with random initial values and data  We removed the outliers with exceedingly large loss value when plotting the loss curves  No loss
value of RNNprop was removed when plotting the  gures 

  Generalization to More Steps

We  rst test optimizers on the task used in the training
stage  which is to optimize the base MLP for   steps 
Both DMoptimizer and RNNprop outperform all traditional optimization algorithms  DMoptimizer has better
performance possibly because of over tting  We then test
optimizers to run for more steps on the base MLP  The left
plot of Figure   indicates that RNNprop can achieve comparable performance with traditional algorithms for  
steps while DMoptimizer fails 
We also test the optimizers for much more steps   
steps  as shown in the right plot of Figure  
It is clear
that DMoptimizer loses the ability to decrease the loss after about   steps and its loss begins to increase dramatically  RNNprop  on the other hand  is able to decrease
the loss continuously  though it slows down gradually and
traditional algorithms overtake it  The main reason is that
RNNprop is trained to run for only   steps  and  
step training process may be signi cantly different from
 step training process  Additionally  traditional optimization algorithms are able to achieve good performance
on both tasks because we explicitly adjusted their learning
rates to adapt to these tasks 
Figure   shows how the  nal loss after   steps changes
when using different learning rates  For example  Adam
can outperform RNNprop only if its learning rate lies in
the narrow interval from   to  
For other optimizees  RNNprop shows similar ability to
train for longer horizons  Due to space constraints  we do
not discuss them in details 

  Generalization to Different Activation Functions

We test the optimizers on the base MLP with different activation functions  As shown in Figure   if the activation
function is changed from sigmoid to ReLU  RNNprop can
still achieve better performance than traditional algorithms
while DMoptimizer fails  For other activations  RNNprop
also generalizes well as shown in Table  

  Generalization to Deeper MLP

In deep neural networks  different layers may have different
optimal learning rates  but traditional algorithms only have
one global learning rate for all the parameters  Our RNN
optimizer can achieve better performance bene ted from

Figure   The  nal loss of different algorithms on the base MLP
after   steps  The colorful solid curves show how the  nal
losses of traditional algorithms after   steps change with different learning rates  and the horizontal dash line shows the  nal
loss of RNNprop  We compute the  nal loss by freezing the  nal
parameters of the optimizee and compute the average loss using
all the data encountered during optimization process 

Table   Performance on the base MLP with different activations 
The numbers in table were computed after running the optimization processes for   times 

Activation Adam DMoptimizer

RNNprop

sigmoid

ReLU

ELU

tanh

 

 

 

 

 

 

 

 

 

 

 

 

its more adaptive behavior 
We tested the optimizers on deeper MLPs  More hidden
layers are added to the base MLP  all of which have   hidden units and use sigmoid as activation function  As shown
in Figure   RNNprop can always outstrip traditional algorithms until the MLP becomes too deep and none of them
can decrease its loss in   steps  Figure   shows the loss
curves on the MLP with   hidden layers as an example 

  Generalization to Different Structures

  CNN

The CNN optimizees are the crossentropy losses of convolutional neural networks  CNN  with similar structure
as VGGNet  Simonyan   Zisserman    on dataset
MNIST or dataset CIFAR  All convolutional layers use
   lters and the window of each maxpooling layer is of
size     with stride   We use   to denote   convolutional
layer    to denote   maxpooling layer and   to denote  
fullyconnected layer  Three CNNs are used in the experiments  CNN with structure cc pf on MNIST  CNN

Learning Gradient Descent  Better Generalization and Longer Horizons

Figure   RNNprop slightly outperforms traditional algorithms on
the base MLP with activation replaced with ReLU 

Figure   RNNprop signi cantly outperforms traditional algorithms on the base MLP with   hidden layers 

with structure cc pc cp ff on MNIST and CNN
with structure cc pf on CIFAR 
The results are shown in Figure   RNNprop can outperform traditional algorithms on CNN with structure
cc pf on dataset MNIST  On the other two CNNs 
only the best traditional algorithm outperforms RNNprop 
Even though  Andrychowicz et al    showed that
DMoptimizer that is trained on CNNs can train CNNs
faster than traditional algorithms 
in our experiments
DMoptimizer fails to train any of the CNNs when the training set is  xed to the base MLP 

  LSTM

The optimizers are also tested on the mean squared loss
of an LSTM with hidden state size   on   simple task 
given   sequence                 with additive noise  the
LSTM needs to predict the value of     Here        
  sin       When generating the dataset  we uniformly
randomly choose                        
      and we draw the noise from the Gaussian distribution      

Figure   Performance on the base MLP with different number of
hidden layers  Among all traditional algorithms we only list the
performance of Adam since it achieves lowest loss 

Table   Performance on the task with different settings of LSTM 
We list the  nal loss of RNNprop and best traditional optimization
algorithms on the task with  layer LSTM and on the task with
smaller noise  The numbers in table were computed after running
the optimization processes for   times 

Experiment Adam Adagrad DMoptimizer RNNprop

Default

  Layers

Small Noise

 

 

 

 

 

 

 

 

 

 

 

 

Even though the task is completely different from the task
that is used for training  RNNprop still has comparable or
even better performance than traditional algorithms  which
may be due to the fact the structure inside LSTM is similar
to that of the base MLP with sigmoid in between 
We also adjust the settings of the task  As shown in Figure   RNNprop still achieve good results when we use
  smaller noise from the distribution       or use  
twolayer LSTM instead of onelayer 

  Control Experiment

To assess the effectiveness of each contribution separately 
we also trained three more RNN optimizers  DMoptimizer
trained with the two tricks and two RNNprop  each trained
with one of the two tricks respectively 
Recall that the trick of combining with convex function
aims to accelerate the training of RNN optimizers  We test
the performance of RNNprop whose own parameters are
trained for different numbers of iterations  with or without
this trick  The result is shown in Table   With this trick
RNN optimizer can achieve   good result with fewer iterations of training 

Learning Gradient Descent  Better Generalization and Longer Horizons

Figure   Performance on different CNNs  Left  The CNN has   convolutional layer    pooling layer and   fullyconnected layer and is
on dataset MNIST  Center  The CNN has   convolutional layer    pooling layer and   fullyconnected layer and is on dataset MNIST 
Right  The CNN has   convolutional layer    pooling layer and   fullyconnected layer and is on dataset CIFAR 

Figure   Performance on   sequence prediction problem implemented by LSTM 

To assess the other two contributions  we select the trained
optimizers in the same way as RNNprop  In Figure   we
test their performances on the base MLP with activation
replaced with ReLU for   steps  From the  gure  we
conclude that Random Scaling is the most effective trick 

Table   Comparison between RNNprop with or without combination with convex functions  We test them on the base MLP and
the base MLP with activation replaced with ReLU  The  nd column shows the number of iterations used to train optimizers  The
 th column shows the  nal loss produced by RNNprop with all
training tricks  while the last column shows the  nal loss produced
by RNNprop trained without combination with convex functions 

Optimizee

 Iter Adam RNNprop No CC

Base MLP

ReLU

  
  
  

  
  
  

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

Figure   Comparison between RNNprop trained with two tricks 
RNNprop trained without Random Scaling  DMoptimizer 
DMoptimizer trained with two tricks  All optimizers are tested
on the base MLP with activation replaced with ReLU for  
steps 

  Conclusion
In this paper  we present   new learningto learn model
with several useful tricks  We show that our new optimizer has better generalization ability than the stateof art
learningto learn optimizers  After trained using   simple MLP  our new optimizer achieves better or comparable
performance with traditional optimization algorithms when
training more complex neural networks or when training
for longer horizons 
We believe it is possible to further improve the generalization ability of our optimizer  Indeed  on some tasks in our
experiments  our optimizer did not outperform the best traditional optimization algorithms  in particular when training for much longer horizon or when training neural networks on different datasets  In the future  we aim to further develop   more generic optimizer with more elaborate
designing  so that it can achieve better performance on  
wider range of tasks that are analogous with the optimizee
used in training 

Learning Gradient Descent  Better Generalization and Longer Horizons

References
Abadi     Agarwal     Barham     Brevdo     Chen    
Citro     Corrado       Davis     Dean     Devin 
   et al  Tensor ow  Largescale machine learning
on heterogeneous distributed systems  arXiv preprint
arXiv   

Andrychowicz     Denil       omez     Hoffman       
Pfau     Schaul     and de Freitas     Learning to
learn by gradient descent by gradient descent 
In Lee 
      Sugiyama     Luxburg        Guyon     and
Garnett      eds  Advances in Neural Information Processing Systems   pp    Curran Associates 
Inc   

Brazdil     Carrier       Soares     and Vilalta     Metalearning  Applications to Data Mining  Springer Publishing Company  Incorporated    edition    ISBN
   

Chen     Hoffman       Colmenarejo       Denil    
Lillicrap       and de Freitas     Learning to learn
for global optimization of black box functions  arXiv
preprint arXiv   

Clevert       Unterthiner     and Hochreiter     Fast
and accurate deep network learning by exponential linear units  elus  arXiv preprint arXiv   

Li     and Malik     Learning to optimize  In International

Conference on Learning Representations   

Naik       and Mammone       Metaneural networks that
learn by learning  In International Joint Conference on
Neural Networks  volume   pp    IEEE   

Parikh     and Boyd     Proximal algorithms  Foundations

and Trends  in Optimization     

Prokhorov       Feldkarnp       and Tyukin       Adaptive behavior with  xed weights in rnn  an overview  In
International Joint Conference on Neural Networks  pp 
   

Schmidhuber     Zhao     and Wiering     Simple PrinIstituto Dalle Molle Di Studi

ciples of Metalearning 
Sull Intelligenza Arti ciale   

Simonyan     and Zisserman     Very deep convolutional
networks for largescale image recognition  In International Conference on Learning Representations   

Thrun     and Pratt     Learning to Learn  Springer US 

 

Tieleman     and Hinton     Lecture  rmsprop  Divide
the gradient by   running average of its recent magnitude  COURSERA  Neural Networks for Machine Learning     

Cotter       and Conwell       Fixedweight networks can
learn  In IJCNN International Joint Conference on Neural Networks  pp     

Tseng     An incremental gradient  projection  method
with momentum term and adaptive stepsize rule  SIAM
Journal on Optimization     

Daniel     Taylor     and Nowozin     Learning step size
controllers for robust neural network training  In AAAI
Conference on Arti cial Intelligence   

Duchi     Hazan     and Singer     Adaptive subgradient
methods for online learning and stochastic optimization 
Journal of Machine Learning Research   Jul 
   

Hansen     Using deep qlearning to control optimization hyperparameters  arXiv preprint arXiv 
 

Hochreiter     and Schmidhuber     Long shortterm mem 

ory  Neural computation     

Hochreiter     Younger     and Conwell     Learning to
learn using gradient descent  International Conference
on Arti cial Neural Networks  pp     

Kingma     and Ba     Adam    method for stochastic
optimization  In International Conference on Learning
Representations   

Vilalta     and Drissi       perspective view and survey
of metalearning  Arti cial Intelligence Review   
   

Wang       Kurthnelson     Tirumala     Soyer    
Leibo       Munos     Blundell     Kumaran     and
Botvinick     Learning to reinforcement learn  arXiv
preprint arXiv   

Younger       Conwell       and Cotter       Fixedweight online learning  IEEE Transactions on Neural
Networks     

Younger       Hochreiter     and Conwell       MetaIn International Joint
learning with backpropagation 
Conference on Neural Networks  volume   IEEE   

Zeiler       Adadelta  an adaptive learning rate method 

arXiv preprint arXiv   

