Bayesian Boolean Matrix Factorisation

Tammo Rukat   Chris    Holmes     Michalis    Titsias   Christopher Yau  

Abstract

Boolean matrix factorisation aims to decompose
  binary data matrix into an approximate Boolean
product of two low rank  binary matrices  one
containing meaningful patterns  the other quantifying how the observations can be expressed as
  combination of these patterns  We introduce
the OrMachine    probabilistic generative model
for Boolean matrix factorisation and derive  
Metropolised Gibbs sampler that facilitates ef 
cient parallel posterior inference  On real world
and simulated data  our method outperforms all
currently existing approaches for Boolean matrix
factorisation and completion  This is the  rst
method to provide full posterior inference for
Boolean Matrix factorisation which is relevant
in applications       for controlling false positive rates in collaborative  ltering and  crucially 
improves the interpretability of the inferred patterns  The proposed algorithm scales to large
datasets as we demonstrate by analysing single
cell gene expression data in   million mouse
brain cells across   thousand genes on commodity hardware 

 cid 

 

Figure   The observed images are   digits from   to   as they are
traditionally represented in calculators  The data is factorised into
matrices of rank   which is not suf cient for full errorfree reconstruction  Every digit  except   can be constructed by Boolean
combination of the inferred codes  The OrMachine infers   posterior mean probability of   for using code       in constructing     Note that there exist other equally valid solutions to this
problem with   latent dimensions  The pixels represent posterior
means  Codes and observations are arranged to   images for
interpretation 

larger than zero are set to one      

  cid 

xnd  

znl   uld  

 

  Introduction
Boolean matrix factorisation  BooMF  can infer
interpretable decompositions of   binary data matrix
           into   pair of lowrank  binary matrices
           and             The data generating
process is based on the Boolean product    special case of
matrix product between binary matrices where all values

 Department of Statistics  University of Oxford  UK  Nuf eld
Department of Medicine  University of Oxford  UK  Department
of Informatics  Athens University of Economics and Business 
Greece  Centre for Computational Biology  Institute of Cancer
and Genomic Sciences  University of Birmingham  UK  Correspondence to  Tammo Rukat  tammo rukat stats ox ac uk 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  

Here    and   encode the Boolean disjunction and conjunction  respectively  BooMF provides   framework for learning from binary data where the inferred codes   provide
  basis and the indicator variables   encode the presence
or absence of these codes  This representation is illustrated
in the calculator digits example in Fig    We can think of
BooMF as binary factor analysis or as clustering with joint
assignments  where each observation is assigned to   subset
of   cluster centroids or codes  The Ldimensional indicators provide   compact representation of which codes are
allocated to each observation  As stated in Eq      feature xnd takes   value of one if it equals one in any of the
assigned codes 
BooMF has many realworld applications ranging from
topic modelling  Blei    to collaborating  ltering  Su

                      observationofsizeD               Codesu     LatentrepresentationszThe OrMachine

  The OrMachine
  Model Formulation

  Khoshgoftaar    and computer vision    azaroGredilla et al    In this paper  we introduce the OrMachine    Bayesian approach to BooMF  and    the model
using   fast and scalable Metropolised Gibbs sampling algorithm  On simulated and realworld data  our method is
shown to signi cantly outperform the current stateof theart message passing approaches for learning BooMF models  Moreover  we consider   challenging application in
the analysis of highthroughput single cell genomics data 
BooMF is used to identify latent gene signatures  codes 
that correspond to key cellular pathways or biological processes from large gene expression datasets consisting of
  million cells across   thousand genes  Genes are expressed if one or more relevant biological processes are active    property which is naturally modelled by the Boolean
OR operation  We also introduce   multilayered extensions of Bayesian BooMF that can capture hierarchical dependencies in the latent representations 

  Related Work
There has been   sustained interest in BooMF and related
methods of which we will give   brief review  The Discrete
Basis Problem  Miettinen et al    provides   greedy
heuristic algorithm to solve BooMF without recourse to
an underlying probabilistic model  It is based on association rule mining  Agrawal et al    and has more recently been extended to automatically select the optimal
dimensionality of the latent space based on the minimum
description length principle  Miettinen   Vreeken   
In contrast  multi assignment clustering for Boolean data
 Streich et al    leverages on   probabilistic model for
BooMF  adding   further global noise source to the generative process  Point estimates are inferred by deterministic annealing  Similarly  Wood et al    develop  
probabilistic model to infer hidden causes  In contrast to
the Boolean OR  the likelihood of an observation increases
with the number of active hidden codes  They use an Indian
Buffet process prior over the latent space and   Gibbs sampler to infer the distribution over the unbounded number of
hidden causes    more expressive model for matrix factorisation with binary latent variables is introcued by  Meeds
et al    who combine binary interactions with continous weights    similar approach to ours is the work by Ravanbakhsh et al    The authors tackle BooMF using
  probabilistic graphical model and derive   message passing algorithm to perform MAP inference  Their method is
shown to have stateof theart performance for BooMF and
completion  It therefore serves us as baseline benchmark
in these tasks  The message passing approach has recently
been employed by   azaroGredilla et al    in   hierarchical network combined with pooling layers to infer the
building blocks of binary images 

The OrMachine is   probabilistic generative model for
Boolean matrix factorisation    matrix of   binary observations xn        is generated from   discrete mixture
of   binary codes ul         Binary latent variables znl
denote whether or not code   is used in generating   particular observation xn  The probability for   data point xnd
to be one is greater than   if the corresponding codes and
latent variables in at least one latent dimension both equal
one  conversely  if there exists no dimension where codes
and latent variables both equal one  the probability for the
data point to be one is less than   The exact magnitude of
this probability is inferred from the data and  for later notational convenience  is parametrised as the logistic sigmoid
of   global dispersion parameter            with
       Next  we give   full description of the likelihood
and prior distributions used in the OrM 
The likelihood function is factorised across the   observations and   features with each factor given by

  xnd          

 cid 
 cid 

   

 xnd

 
 

 cid 

if    min  uT
if   cid  min  uT

  zn 
  zn 
    znluld 

 cid 

     

 

 cid cid 

 

 

 
Tilde denotes the           mapping so that for
any binary variable                     The expression inside the parentheses of Eq    encodes the OR
operation and evaluates to   if znl   uld     for at least
one    and to   otherwise  The dispersion parameter controls the noise in the generative process       as       all
probabilities tend to   or   and the model describes   deterministic Boolean matrix product  Note that the likelihood
can be computed ef ciently from Eq    as we describe in
detail in the next section  We further assume independent
Bernoulli priors for all variables uld and znl  Such priors
allow us to promote denseness or sparsity in codes and latent variables  Notice that the designation of   as codes
and   as latent variables is not necessary since these matrices appear in   symmetric manner  If we transpose the
matrix of observations    then codes and latent variables
merely swap roles 
Finally  we do not place   prior on the dispersion parameter
  but maximise it using an EMtype algorithm described
below 

  Fast Posterior Inference

The full joint distribution of all data and random variables
is given by                                    

The OrMachine

The full conditional for znl  and analogous for uld  is
  znl     

 

 znl

 xnduld

 znl cid ul cid   logit   znl 

 

 

 cid 

 cid 

 cid 

 

 cid 

  cid cid  

We give   detailed derivation in the supplement  Notice
that the independent Bernoulli prior enters the expression
as additive term inside the sigmoid function that vanishes
for the uninformative Bernoulli prior         
The form of Eq    allows for computationally ef cient
evaluation of the conditionals  The underlying principle is
that once certain conditions are met  the result of the full
conditional is known without considering the remainder of
  variable   Markov blanket  For instance  when computing updates for znl  terms in the sum over   necessarily
evaluate to zero if one of the following conditions is met 
    uld     or  ii  znl cid ul cid       for some   cid   cid     This leads
to Algorithm   for fast evaluation of the conditionals 

Algorithm   Computation of the full conditional of znl

accumulator    
for   in             do

if uld     then

continue  next iteration over   

end if
for   cid  in             do

if   cid   cid    and znl cid      and ul cid       then

continue  next iteration over   

end if
end for
accumulator   accumulator    xnd
end for
  znl             znl   accumulator 

To infer the posterior distribution over all variables uld
and znl we could iteratively sample from the above conditionals using standard Gibbs sampling 
In practice we
use   modi cation of this procedure which is referred to
as Metropolised Gibbs sampler and was proposed by Liu
  We always propose to  ip the current state  leading
to   Hastings acceptance probability of             
This is guaranteed to yield lower variance Monte Carlo estimates  Peskun   
After every sweep through all variables  the dispersion
parameter   is updated to maximise the likelihood akin
to the Mstep of   Monte Carlo EM algorithm  Specifically  given the current values of the codes   and
latent variables   we can compute how many observations xnd are correctly predicted by the model  as
  This allows us to

 cid 
xnd      cid 

   cid 

    znluld 

 cid 

 

   

 

Algorithm   Sampling from the OrMachine

for   in           maxiters do

for   in              in parallel  do

for   in             do

Compute   znl    following Algorithm  
Flip znl with probability    znl   

end for

end for
for   in              in parallel  do

for   in             do

Compute   uld    following Algorithm  
Flip uld with probability    uld   

end for

end for
Set   to its MLE according to Eq   

end for

rewrite the likelihood as           which can be
subsequently maximised with respect to   to yield the update

   

 
   

 

 

The alternation between sampling         and updating the
dispersion parameter is carried out until convergence  see
Algorithm   for all steps of this procedure 

  Dealing with Missing Data

We can handle unobserved data  by marginalising the likelihood over the missing observations  More precisely  if
     Xobs  Xmis  is the decomposition of the full matrix into the observed part Xobs and the missing part Xmis 
after marginalisation  the initial likelihood             
simpli es to   Xobs          Then    na ve implementation could be based on indexing the observed components
inside matrix   and modifying the inference procedure
so that the posterior conditionals of znl and uld involve
only sums over observed elements    simpler  equivalent
implementation  which we follow in our experiments  is
to represent the data as  xnd         where missing
observations are encoded as zeros  each contributing the
constant factor       to the full likelihood  so that
                   Xobs          where   is   constant  Thus  the missing values do not contribute to the
posterior over   and   which is also clear from the form
of the full conditionals in Eq    that depend on   sum
weighted by xnds  For the update of the dispersion parameter in Eq    we need to subtract the number of all
missing observations in the denominator  The dispersion
now indicates the fraction of correct prediction in the observed data  Following this inference procedure  we can
impute missing data based on   Monte Carlo estimate of

The OrMachine

train multilayer OrMachines with various depths and layer
widths  iterating through the individual layers during  
iterations of burnin  We then draw   samples from each
consecutive layer with the remaining layers held  xed to
their MAP estimate  In order to enforce distributed representations  we choose independent Bernoulli sparsity priors
for the codes    uld          for each layer  respectively  Superior performance in reconstructing the unobserved data is achieved by    hidden layer architecture
with hidden layers of size                      This
 layer model reduces the reconstruction error from  
to   compared to the singlelayer model with width
      Maximum likelihood estimates of the dispersion
for the three layers are           The  rst layer
infers the seven bars that compose all digits  the following
layer infers dominant groupings of these bars  and so on 
In Fig    we plot the probabilities that each prototype induces in the observation layer  They are given by the means
of the posterior predictive as described in the the previous
section  conditioned on the onehot activations of zn with
  znl     Alongside  we depict the average posterior mean of the corresponding representations
for each digit in the training data  This example illustrates
that the multilayer OrMachine infers interpretable higherorder correlations and is able to exploit them to achieve
signi cant improvements in missing data imputation 

znl       and cid 

  Practical Implementation and Speed

The algorithm is implemented in Python with the core
sampling routines in compiled Cython  Code is avaialble on GitHub  Binary data is represented as    
with missing data encoded as   This economical representation of data and variables as integer types simpli es
computations considerably  Algorithm   is implemented
in parallel across the observations                    and
conversely updates for uld are implemented in parallel
across all features                    The computation
time scales linearly in each dimension    single sweep
through highresolution calculator digits toy dataset with
            data points and       latent dimensions takes approximately   second on   desktop computer 
  single sweep through the approximately       data
points presented in the biological example in Section  
with       latent dimensions takes approximately   minutes executed on   computing cores  For all examples
presented here   iterations suf ce for the algorithm to
converge to    local  posterior mode 

  Experiments on Simulated Data
In this section  we probe the performance of the OrMachine  OrM  at random matrix factorisation and completion

 https github com TammoR OrMachine 

Figure   An OrMachine with   hidden layers is trained to reconstruct   calculator digits with   of observations missing 
The rows depict increasingly abstract layers of the model  Shown
are the latent prototypes fed forward to the data layer  Variables
are arranged to   images for interpretation  The right sides
show the corresponding posterior means for representations of the
partially observed input digits 

the predictive distribution of some unobserved xnd as

  cid 

  

 
 

  xnd               

 

where each             is   posterior sample    much
faster approximation of the predictive distribution is obtained by   xnd             where we simply plug the posterior mean estimates for         into the predictive distribution  For the simulated data in Section   we  nd both
methods to perform equally well and therefore follow the
second  faster approach for all remaining experiments 

  MultiLayer OrMachine

BooMF learns patterns of correlation in the data  In analogy to multilayer neural networks  we can build   hierarchy of correlations by applying another layer of factorisation to the factor matrix    This is reminiscent of the idea
of deep exponential families  as introduced by Ranganath
et al    The ability to learn features at different levels of abstraction is commonly cited as an explanation for
the success that deep neural networks have across many domains of application  Lin   Tegmark    Bengio et al 
 
In the present setting  with stochasticity at every
step of the generative process and posterior inference  we
are able to infer meaningful and interpretable hierarchies of
abstraction 
To give an example  we determine the optimal multilayer
architecture for representing the calculator digit toy dataset
as introduced in Fig    We observe   digits and consider
  of the data points randomly as unobserved  We then

The OrMachine

Figure   Illustration of the matrix factorisation task for         matrix of rank   The posterior means estimate the probability of
each data point to take   value of one  MAP estimates are computed by rounding to the closest integer 

tasks  Message passing  MP  has been shown to compare
favourably with other stateof theart methods for BooMF
that we introduced in Section    Ravanbakhsh et al   
It therefore is the focus of our comparison  The following settings for MP and the OrM are used throughout our
experiments  unless mentioned otherwise  For MP  we use
the Python implementation provided by the authors  We
also proceed with their choice of hyperparameters  as experimentation with different learning rates and maximum
number of iterations did not lead to any improvements  For
both methods  we set the priors      and      to the factor matrices  expected value based on the density of the
product matrix in an EmpiricalBayes fashion  The only
exception is MP in the matrix completion task  where uniform priors  as used by Ravanbakhsh et al    lead to
slightly better performance  For the OrM  we initialise the
parameters uniformly at random and draw   iterations after   samples of burnin  Note that around   sampling steps are usually suf cient for convergence 

  Random Matrix Factorisation
We generate   quadratic matrix            of rank
  by taking the Boolean product of two random      
factor matrices  The Boolean product   of two rank  
binary matrices that are sampled        from   Bernoulli
distribution with parameter   has an expected value of
                    Since we generally prefer   to
be neither sparse nor dense  we    its expected density to
  unless stated otherwise  This ensures that   simple bias
toward zeroes or ones in either method is not met with reward  Bits in the data are  ipped at random with probabilities ranging from   to   Factor matrices of the
correct underlying dimension are inferred and the data is
reconstructed from the inferred factorisation  An example
of the task is shown in Fig   
Results for the reconstruction error  de ned as the fraction
of misclassi ed data points  are depicted in Fig    All experiments were repeated   times with error bars denoting
standard deviations  The OrM outperforms MP under all
conditions  except when both methods infer equally errorfree reconstructions  Fig     top  reproduces the experi 

Figure   Comparison of OrMachine and message passing for
BooMF for random matrices of different size  rank and density 
Compare to Fig    in Ravanbakhsh et al   

mental settings of Fig    in Ravanbakhsh et al    We
 nd that the OrMachine enables virtually perfect reconstruction of         matrix of rank       for up
to   bit  ip probability  Notably  MP performs worse
for smaller noise levels 
It was hypothesised by Ravanbakhsh et al    that symmetry breaking at higher noise
levels helps message passage to converge to   better solution  Fig     middle  demonstrates the consistently improved performance of the OrMachine for   more challenging example of       matrices of rank   The
reconstruction performance of both methods is similar for
lower noise levels  while the OrMachine consistently out 

The OrMachine

Table   Collaborative  ltering performance for MovieLens   
and    dataset  Given are the percentages of correctly reconstructed unobserved data as means from   random repetitions 
Compare to Table   in Ravanbakhsh et al    who also provide comparison to other stateof the art methods  Their results for
message passing were independently reproduced  The multilayer
OrMachine has two hidden layers of size   and   respectively 

OBSERVED PERCENT  OF AVAILABLE RATINGS
 
 

       

 

  
ORM  
MP
 
MULTI
LAYER
ORM
  
ORM  
MP
 
MULTI
LAYER
ORM

 

 
 

 
 

 
 

 
 

 

 

 

 

 
 

 
 

 
 

 
 

 

 

 

 

 
 

 

 
 

 

its reconstruction provides further useful information about
the missing data  For instance  this information can be used
to control for false positives or false negatives  simply by
setting   threshold for the posterior mean 

  Experiments on RealWorld Data
  MovieLens Matrix Completion

We investigate the OrMachine   performance for collaborative  ltering on   realworld dataset  The MovieLens  
dataset  contains   integer  lm ratings from   to   from
  users for    lms         of the possible ratings
are available  Similarly  the MovieLens    dataset contains   users and    lms  Following Ravanbakhsh
et al    we binarise the data taking the global mean
as threshold  We observe only   fraction of the available
data  varying from   to   and reconstruct the remaining available data following the procedure in Section  
with       latent dimensions  Reconstruction accuracies
are given as fractions of correctly reconstructed unobserved
ratings in Table   The given values are means from   randomly initialised runs of each algorithm  The corresponding standard deviations are always smaller than   The
OrMachine is more accurate than message passing in all
cases  except for the    dataset with   available ratings  The OrMachine   advantage is particularly signi 
cant if only little data is observed 
Increasing the latent
dimension   to values of   or   yields no consistent improvement  while   further increase is met with diminishing

 The MovieLens dataset is available online  https 

grouplens org datasets movielens 

Figure   Matrix completion performance for simulated low rank
matrices  top  and kernel density estimate of the distribution of
posterior means for inferred matrix entries  bottom 

performs MP for larger noise levels  For biased data with
  xnd      in Fig     bottom  we observe   similar pattern with   larger performance gap for higher noise levels 
Even for   bit  ipprobability of   the OrMachine retains   reconstruction error of approximately   which
is achieved by levering the bias in the data 
Fig     middle  also shows the reconstruction error on the
observed data  indicating that MP over ts the data more
than the OrM for larger noise levels  This may contribute
to the improved performance of the OrMachine 

  Random Matrix Completion

We further investigate the problem of matrix completion
or collaborative  ltering  where bits of the data matrix are
unobserved and reconstructed from the inferred factor matrices  Following the procedure outlined in Section   we
generate random matrices of rank   and size      
We only observe   random subset of the data  ranging from
  and   The missing data is reconstructed from
the inferred factor matrices  As shown in Fig    the OrMachine outperforms message passing throughout  The plot
indicates means and standard deviations from   repetitions of each experiment 
Notably  the OrMachine does not only provide   MAP estimate  but also an estimate of the posterior probability for
each unobserved data point xnd  Fig     bottom  shows
an estimate of the density of the posterior means for the
correctly and incorrectly completed data points  The distribution of incorrect predictions peaks around   probability
of   indicating that the OrMachine   uncertainty about

The OrMachine

Figure   ROC curve for MovieLens    data  adjusting the
threshold for when   prediction is considered   like    of the
available data were observed and used for inference with an OrM
of size       Predictions were tested on the remaining  

Figure   Hierarchy in the latent features of calculator digits  The
arrows indicate the split of codes into more distributed codes of
lower density  They are inferred as latent variables in an OrMachine  with codes from the model of size   as data and codes from
model with of size    as  xed codes 

returns  We achieve the best withinsample performance
for   twolayer OrMachine with different architectures performing best for different amounts of observed data  An
OrMachine with two hidden layers of sizes   and   respectively yields the best average performance  As indicated in
Table   it provides better results throughout but exceeds
the performance of the shallow OrMachine rarely by more
than   This indicates that there is not much higher order
structure in the data  which is unsurprising given the sparsity of the observations and the low dimensionality of the
 rst hidden layer 
We illustrate   further advantage of full posterior inference
for collaborative  ltering  We can choose   threshold for
how likely we want   certain prediction to take   certain
value and trade off false with true positives    corresponding ROC curve for the MovieLens    dataset  where  
of the available data was observed  is shown in Fig   

  Explorative Analysis of Single Cell Gene

Expression Pro les

Singlecell RNA expression analysis is   revolutionary experimental technique that facilitates the measurement of
gene expression on the level of   single cell  Blainey  
Quake    In recent years this has led to the discovery of new cell types and to   better understanding of tissue
heterogeneity  Trapnell    The latter is particularly
relevant in cancer research where it helps to understand the
cellular composition of   tumour and its relationship to disease progression and treatment  Patel et al    Here
we apply the OrMachine to binarised gene expression pro 
 les of about   million cells for about   thousand genes
per cell  Cell specimens were obtained from cortex  hip 

pocampus and subventricular zone of     embryonic day
  mice  the data is publicly available  Only   of the
data points are nonzero  We set all nonzero expression
levels to one  retaining the essential information of whether
or not   particular gene is expressed  We remove genes
that are expressed in fewer than   of cells with roughly
  thousand genes remaining  This leaves us with approximately     data points  We apply the OrMachine for
latent dimensions                 The algorithm converges
to   posterior mode after   iteration  taking roughly an
hour on    core desktop computer and   minutes on
  cluster with   cores  We draw   samples and discard
the  rst   as burnin 
Factorisations with different latent dimensionality form hierarchies of representations  where features that appear together in codes for lower dimensions are progressively split
apart when moving to   higher dimensional latent space 
We illustrate this approach to analysing the inferred factorisations on calculator digits in Fig    Each row corresponds to an independently trained OrMachine with the dimensionality   increasing from   to   We observe denser
patterns dividing up consecutively until only the seven constituent bars remain  This is   form of hierarchical clustering that  in contrast to traditional methods  does not impose any hierarchical structure on the model  We perform
the same analysis on the single cell gene expression data
with the results for both  gene patterns and specimen patterns shown in Fig    This Figure should be interpreted
in analogy to Fig    Furthermore  we run   gene set enrichment analysis for the genes that are unique to each inferred code  looking for associated biological states  This

 https support xgenomics com

The OrMachine

Figure   Hierarchy in the latent representations of genes     and specimens     under variation of the latent dimensionality  Rows with
the same dimensionality                in the top and bottom box correspond to the same OrMachine factorisation  Rows with different
dimensionality are trained independently  Importantly  the ordering of genes specimens is identical for all subplots  Each subplot    
describes set of expressed genes  limited to the approximately    out of    analysed genes that are used in at least one code  Subplots in
    describe representations of cell specimens in terms of which of the gene sets they express  See legend in each box for more details 

is done using the Enrichr analysis tool  Chen et al   
and   mouse gene atlas  Su et al    Biological states
are denoted above each code  together with the logarithm
to base   of their adjusted pvalue  Increasing the latent
dimensionality leads to   more distributed representation
with subtler  biologically plausible patterns  The columns
in Fig    are ordered to emphasise the hierarchical structure 
     in the  rst column for       and second column for
        gene set with signi cant overlap to two biological
processes  olfactory bulb and hippocampus  splits into two
gene sets each corresponding to one of the two processes 
In the assignment of cells to these sets  Fig      this is
associated with an increase in posterior uncertainty as to
which cell expresses this property  The signi cance levels
of the associated biological processes drop from pvalues
on the order of   to pvalues on the order of   In addition  typical genes for each of the biological states are annotated  LopezBendito et al    Zheng et al    Demyanenko et al    Upadhya et al    Raman et al 
  This examples illustrates the OrMachine   ability to
scale posterior inference to massive datasets  It enables the

discovery of readily interpretable patterns  representations
and hierarchies  all of which are biologically plausible 

  Conclusion
We have developed the OrMachine    probabilistic model
for Boolean matrix factorisation  The extremely ef cient
Metropolised Gibbs sampler outperforms stateof theart
methods in matrix factorisation and completion  It is the
 rst method that infers posterior distributions for Boolean
matrix factorisation    property which is highly relevant
in practical applications where full uncertainty quanti cation matters  Despite full posterior inference  the proposed
method scales to very large datasets  We have shown that
tens of billions of data points can be handled on commodity hardware  The OrMachine can readily accommodate
missing data and prior knowledge  Layers of OrMachines
can be stacked  akin to deep belief networks  inferring representations at different levels of abstraction  This leads
to improved reconstruction performance on simulated and
real world data 

The OrMachine

References
Agrawal  Rakesh  Srikant  Ramakrishnan  et al  Fast algoIn Proc   th Int 
rithms for mining association rules 
Conf  Very Large Data Bases  VLDB  volume   pp 
   

Bengio     Courville     and Vincent     Representation
IEEE translearning    review and new perspectives 
actions on pattern analysis and machine intelligence   
    doi   tpami 

Blainey  Paul    and Quake  Stephen    Dissecting genomic diversity  one cell at   time  Nat  Methods   
  Jan  

Blei  David    Probabilistic topic models  Communidoi   

cations of the ACM     
 

Chen  Edward    Tan  Christopher    Kou  Yan  Duan 
Qiaonan  Wang  Zichen  Meirelles  Gabriela  Clark 
Neil    and Ma ayan  Avi  Enrichr 
Interactive and
collaborative html  gene list enrichment analysis tool 
BMC Bioinformatics      doi   
 

Demyanenko        Siesser        Wright        Brennaman        Bartsch     Schachner     and Maness 
         and chl  cooperate in thalamocortical axon
targeting  Cerebral Cortex      doi 
 cercor bhq 

  azaroGredilla  Miguel  Liu  Yi  Phoenix    Scott  and
George  Dileep  Hierarchical compositional feature
learning  arXiv preprint arXiv   

Lin  Henry    and Tegmark  Max  Why does deep
arXiv preprint

and cheap learning work so well 
arXiv   

Liu     Miscellanea  peskun   theorem and   modi ed
discretestate gibbs sampler  Biometrika   
    doi   biomet 

LopezBendito     Flames     Ma     Fouquet    
Meglio     Di  Chedotal     TessierLavigne     and
Marin     Robo  and robo  cooperate to control the
guidance of major axonal tracts in the mammalian foreJournal of Neuroscience   
brain 
  doi   jneurosci 

Miettinen  Pauli and Vreeken  Jilles  Mdl bmf  Minimum
description length for boolean matrix factorization  ACM
Trans  Knowl  Discov  Data    October
  ISSN   doi   

Miettinen  Pauli  Mielik ainen  Taneli  Gionis  Aristides 
Das  Gautam  and Mannila  Heikki  The Discrete Basis Problem  pp   
Springer Berlin Heidelberg   
ISBN   doi   
   

Patel        Tirosh     Trombetta        Shalek        Gillespie        Wakimoto     Cahill        Nahed       
Curry        Martuza        Louis        RozenblattRosen     Suva        Regev     and Bernstein       
Singlecell rnaseq highlights intratumoral heterogeneity in primary glioblastoma  Science   
    doi   science 

Peskun  Peter    Optimum montecarlo sampling using

markov chains  Biometrika     

Raman  Arjun    Pitts  Matthew    Seyedali  Ali 
Hashimoto  Ann    Bellinger  Frederick    and Berry 
Marla    Selenoprotein   expression and regulation in
mouse brain and neurons  Brain and Behavior   
    doi   brb 

Ranganath  Rajesh  Tang  Linpeng  Charlin  Laurent  and
Blei  David    Deep exponential families  In AISTATS 
 

Ravanbakhsh  Siamak    oczos  Barnab as  and Greiner 
Russell  Boolean matrix factorization and noisy comIn Proceedings of The
pletion via message passing 
 rd International Conference on Machine Learning 
volume   of JMLR    CP   

Streich  Andreas    Frank  Mario  Basin  David  and Buhmann  Joachim    Multiassignment clustering for
boolean data  In Proceedings of the  th Annual International Conference on Machine Learning   ICML  
    doi   

Su        Wiltshire     Batalov     Lapp     Ching 
      Block     Zhang     Soden     Hayakawa    
Kreiman     Cooke        Walker        and Hogenesch          gene atlas of the mouse and human
proteinencoding transcriptomes  Proceedings of the National Academy of Sciences     
doi   pnas 

Meeds  Edward  Ghahramani  Zoubin  Neal  Radford   
and Roweis  Sam    Modeling dyadic data with binary
latent factors  Advances in neural information processing systems     

Su  Xiaoyuan and Khoshgoftaar  Taghi      survey of
collaborative  ltering techniques  Adv  in Artif  Intell 
  January   ISSN   doi   
 

The OrMachine

Trapnell  Cole  De ning cell types and states with singlecell genomics  Genome Research   
  doi   gr 

Upadhya  Sudarshan    Smith  Thuy    Brennan  Peter    Mychaleckyj  Josyf    and Hegde  Ashok   
Expression pro ling reveals differential gene induction underlying speci   and nonspeci   memory for
pheromones in mice  Neurochemistry International   
    doi     neuint 

Wood  Frank  Grif ths  Thomas  and Ghahramani  Zoubin 
  nonparametric bayesian method for inferring hidden
causes  arXiv preprint arXiv   

Zheng  Yue  Cheng  XiaoRui  Zhou  WenXia  and Zhang 
YongXiang  Gene expression patterns of hippocampus and cerebral cortex of senescenceaccelerated mouse
treated with huanglian jiedu decoction  Neuroscience
Letters      doi     neulet 
 

