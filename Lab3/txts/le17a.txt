Coordinated MultiAgent Imitation Learning

Hoang    Le   Yisong Yue   Peter Carr   Patrick Lucey  

Abstract

We study the problem of imitation learning from
demonstrations of multiple coordinating agents 
One key challenge in this setting is that learning   good model of coordination can be dif cult 
since coordination is often implicit in the demonstrations and must be inferred as   latent variable  We propose   joint approach that simultaneously learns   latent coordination model along
with the individual policies 
In particular  our
method integrates unsupervised structure learning with conventional imitation learning  We illustrate the power of our approach on   dif cult
problem of learning multiple policies for  negrained behavior modeling in team sports  where
different players occupy different roles in the coordinated team strategy  We show that having  
coordination model to infer the roles of players
yields substantially improved imitation loss compared to conventional baselines 

  Introduction
The areas of multiagent planning and control have witnessed   recent wave of strong interest due to the practical
desire to deal with complex realworld problems  such as
smartgrid control  autonomous vehicles planning  managing teams of robots for emergency response  among others 
From the learning perspective   cooperative  multiagent
learning is not   new area of research  Stone   Veloso 
  Panait   Luke    However  compared to the
progress in conventional supervised learning and singleagent reinforcement learning  the successes of multiagent
learning have remained relatively modest  Most notably 
multiagent learning suffers from extremely high dimensionality of both the state and actions spaces  as well as
relative lack of data sources and experimental testbeds 

 California Institute of Technology  Pasadena  CA  Disney
Research  Pittsburgh  PA  STATS LLC  Chicago  IL  Correspondence to  Hoang    Le  hmle caltech edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Our motivating example of learning coordinating behavior policies for team sports from tracking data  Red is the
attacking team  blue is the defending team  and yellow is the ball 

The growing availability of data sources for coordinated multiagent behavior  such as sports tracking data
 Bialkowski et al    now enables the possibility of
learning multiagent policies from demonstrations  also
known as multiagent imitation learning  One particularly
interesting aspect of domains such as team sports is that the
agents must coordinate  For example  in the professional
soccer setting depicted in Figure   different players must
coordinate to assume different roles       defend left  eld 
However  the roles and role assignment mechanism are unobserved from the demonstrations  Furthermore  the role
for   player may change during the same play sequence  In
the control community  this issue is known as  indexfree 
multiagent control  Kingston   Egerstedt   
Motivated by these challenges  we study the problem of
imitation learning for multiple coordinating agents from
demonstrations  Many realistic multiagent settings require
coordination among collaborative agents to achieve some
common goal  Guestrin et al    Kok et al    Beyond team sports  other examples include learning policies
for game AI  controlling teams of multiple robots  or modeling collective animal behavior  As discussed above  we
are interested in settings where agents have access to the
outcome of actions from other agents  but the coordination
mechanism is neither clearly de ned nor observed  which
makes the full state only partially observable 
We propose   semisupervised learning framework that integrates and builds upon conventional imitation learning
and unsupervised  or latent  structure learning  The latent

Coordinated MultiAgent Imitation Learning

structure model encodes   coordination mechanism  which
approximates the implicit coordination in the demonstration data  In order to make learning tractable  we develop
an alternating optimization method that enables integrated
and ef cient training of both individual policies and the
latent structure model  For learning individual policies 
we extend reductionbased singleagent imitation learning approaches into multiagent domain  utilizing powerful
blackbox supervised techniques such as deep learning as
base routines  For latent structure learning  we develop  
stochastic variational inference approach 
We demonstrate the effectiveness of our method in two settings  The  rst is   synthetic experiment based on the popular predatorprey game  The second is   challenging task
of learning multiple policies for team defense in professional soccer  using   large training set  of play sequences
illustrated by Figure   We show that learning   good latent structure to encode implicit coordination yields signi 
cantly superior imitation performance compared to conventional baselines  To the best of our knowledge  this is the
 rst time an imitation learning approach has been applied to
jointly learn cooperative multiagent policies at large scale 

  Problem Formulation
In coordinated multiagent imitation learning  we have  
agents acting in coordination to achieve   common goal  or
sequence of goals  Training data   consists of multiple
demonstrations of   agents  Importantly  we assume the
identity  or indexing  of the   experts may change from
one demonstration to another  Each  unstructured  set of
demonstrations is denoted by     tU          UKu  where
Uk   tut kuT
   is the sequence of actions by agent   at
time    Note that each set of demonstrations can have varying sequence length    Let     tctuT
   be the context
associated with each demonstration sequence 
Policy Learning  Our ultimate goal is to learn    largely 
decentralized policy  but for clarity we  rst present the
problem of learning   fully centralized multiagent policy  Following the notation of  Ross et al    let
 cid   cid sq    cid   denote the joint policy that maps the joint
state   cid     rs          sKs  of all   agents into   actions
 cid     ra          aKs  The goal is to minimize imitation loss 

Limitation     cid     cid    cid   cid   cid sqqs  

where   cid  denotes the distribution of states experienced by
joint policy  cid  and  cid  is the imitation loss de ned over the
demonstrations       squared loss for deterministic policies  or cross entropy for stochastic policies 
The decentralized setting decomposes the joint policy  cid   
 Data at http www stats com datascience 
and see video result at http hoangminhle github io

            Ks into   policies  each tailored to   speci  
agent index or  role  The loss function is then 
  cid   kpskqqs  

Limitation     

Es    

  

BlackBox Policy Classes  In order to leverage powerful
blackbox policy classes such as random forests and deep
learning  we take   learning reduction approach to training
 cid  One consequence is that the state space representation
    rs          sKs must be consistently indexed       agent
  in one instance must correspond to agent   in another instance  This requirement applies for both centralized and
decentralized policy learning  and is often implicitly assumed in prior work on multiagent learning    highly related issue arises in distributed control of indexfree coordinating robots       to maintain   de ned formation  Kloder
  Hutchinson    Kingston   Egerstedt   
Motivating example  Soccer Domain  Consider the task of
imitating professional soccer players  where training data
includes play sequences from different teams and games 
Context   corresponds to the behavior of the opposing
team and the ball  The data includes multiple sequences
of Kset of trajectories     tU             UKu  where the
actual identity of player generating Uk may change from
one demonstration to the next 
One important challenge for blackbox policy learning is
constructing an indexing mechanism over the agents to
yield   consistent state representation  For example  the
same index should correspond to the  left defender  in all
instances  Otherwise  the inputs to the policy will be inconsistent  making learning dif cult if not impossible  Note
that barring extensive annotations or some heuristic rulebased de nitions  it is unnatural to quantitatively de ne
what makes   player  left defender  In addition  even if
we had   way to de ne who the  left defender  is  he may
not stay in the same role during the same sequence 
Rolebased Indexing  We address indexfree policy learning via role learning and rolebased index assignment  To
motivate our notion of role  let    rst consider the simplest indexing mechanism  one could equate role to agent
identity  However  the data often comes from various sequences  with heterogeneous identities and teams of agents 
Thus instead of learning identityspeci   policies  it is
more natural and dataef cient to learn   policy per role 
However    key challenge in learning policies directly is
that the roles are unde ned  unobserved  and could change
dynamically within the same sequence  We thus view learning the coordination  via role assignment  as an unsupervised structured prediction problem 

 It is straightforward to extend our formulation to settings
where multiple agents can occupy the same role  and where not
all roles are occupied across all execution sequences 

Coordinated MultiAgent Imitation Learning

  

Algorithm   Coordinated MultiAgent Imitation Learning
Input  Multiple unstructured trajectory sets     tU          UKu

with Uk   tut kuT

   and context     tctuT

Input  Graphical model   with global local parameters   and   
Input  Initialized policies                    
Input  Step size sequence                  
  repeat
 
 
 

Rollout              to obtain pA          pAK
 Alternatively  Ak   pAk with prob   for      

rA          AKs   AssigntU          UK qp  zqu
            Ks   LearnrA          AK   Cs

  Ak   pAk   

qp  zq   LearnStructuretA          AK         nu

 
  until No improvement on validation set
output   policies               

We propose an alternating optimization approach to solving
  summarized in Figure   The main idea is to integrate
imitation learning with unsupervised structure learning by
taking turns to     optimize for imitation policies while  xing   structured model  minimizing imitation loss  and  ii 
retrain the latent structure model and reassign roles while
 xing the learning policies  maximizing role assignment
entropy  The alternating nature allows us to circumvent
the circular dependency between policy learning and latent
structure learning  Furthermore  for     we develop   stable
multiagent learning reduction approach 

  Approach Outline

Algorithm   outlines our framework  We assume the latent
structure model for computing role assignments is formulated as   graphical model  The multiagent policy training
procedure Learn utilizes   reduction approach  and can
leverage powerful offthe shelf supervised learning tools
such as deep neural networks  Hochreiter   Schmidhuber 
  The structure learning LearnStructure and
role assignment Assign components are based on graphical model training and inference  For ef cient training  we
employ alternating stochastic optimization  Hoffman et al 
  Johnson   Willsky    Beal    on the same
minibatches  Note that batch training can be deployed
similarly  as illustrated by one of our experiments 
We interleave the three components described above into
  complete learning algorithm  Given an initially unstructured set of training data  an initialized set of policies  and
prior parameters of the structure model  Algorithm   performs alternating structure optimization on each minibatch
 size   in Algorithm  

  Line   Role assignment is performed on trajectories
tA          AKu by running inference procedure  Algorithm   The result is an ordered set rA          AKs 
where trajectory Ak corresponds to policy    

  Line   Each policy    is updated using joint multiagent training on the ordered set rA          AK  Cs

Figure   Alternating stochastic optimization training scheme for
our semisupervised structure regularization model 
Coordination via Structured Role Assignment  Instead
of handcrafting the de nition of roles  we learn the roles
in an unsupervised fashion  without attaching any semantic
labels to the roles  At the same time  role transition should
obey certain structural regularity  due to coordination  This
motivates using graphical models to represent the coordination structure 
Coordinated Policy Learning  We formulate the indexing
mechanism as an assignment function   which maps the
unstructured set   and some probabilistic structured model
  to an indexed set of trajectory   rearranged from        

    tU    UKu       rA    AKs  

where the set tA    AKu   tU    UKu  We view   as
  latent variable model that infers the role assignments for
each set of demonstrations  Thus    drives the indexing
mechanism   so that state vectors can be consistently constructed to facilitate optimizing for the imitation loss 
We employ entropy regularization  augmenting the imitation loss with some low entropy penalty  Grandvalet et al 
  Dudik et al    yielding our overall objective 
  cid   kpskqq   Ds HpA Dq  

  

min

Esk    

     

  

where both imitation loss and entropy are measured with
respect to the state distribution induced by the policies  and
  is training data  This objective can also be seen as maximizing the mutual information between latent structure and
observed trajectories  Krause et al   

  Learning Approach
Optimizing   is challenging for two reasons  First  beyond the challenges inherited from singleagent settings 
multiagent imitation learning must account for multiple simultaneously learning agents  which is known to
cause nonstationarity for multiagent reinforcement learning  Busoniu et al    Second  the latent role assignment model  which forms the basis for coordination  depends on the actions of the learning policies  which in turn
depend on the structured role assignment 

Coordinated MultiAgent Imitation Learning

 Algorithm   The updated models are executed to
yield   rolledout set of trajectories  which replace the
previous set of trajectories tAku 

  Line   Parameters of latent structured model are updated from the rolledout trajectories  Algorithm  

The algorithm optionally includes   mixing step on line
  where the rolledout trajectories may replace the training trajectories with increasing probability approaching  
which is similar to scheduled sampling  Bengio et al 
  and may help stabilize learning in the early phase
of the algorithm  In our main experiment  we do not notice
  performance gain using this option 

  Joint MultiAgent Imitation Learning

    where   

In this section we describe the Learn procedure for multiagent imitation learning in Line   of Algorithm   As
background  for single agent imitation learning  reductionbased methods operate by iteratively collecting   new data
set Dn at each round   of training  consisting of stateaction pairs pst    
  is some optimal or demonstrated action given state st    new policy can be formed
by     combining   new policy from this data set Dn with
previously learned policy    Daum   III et al    or  ii 
learning   new policy   directly from the data set formed by
aggregating           Dn  Ross et al    Other variants
exist although we do not discuss them here 
The intuition behind the iterative reduction approach is to
prevent   mismatch in training and prediction distributions
due to sequential cascading errors  also called covariateshift  The main idea is to use the learned policy   own
predictions in the construction of subsequent states  thus
simulating the testtime performance during training  This
mechanism enables the agent to learn   policy that is robust to its own mistakes  Reductionbased methods also
accommodate any blackbox supervised training subroutine  We focus on using expressive function classes such
as Long ShortTerm Memory networks  LSTM   Hochreiter   Schmidhuber    as the policy class 
Algorithm   outlines the Learn procedure for stable
multiagent imitation learning  Assume we are given
consistently indexed demonstrations     rA          AKs 
where each Ak   tat kuT
   corresponds action of policy     Let the corresponding expert action be   
     To
lighten the notation  we denote the peragent state vector
by st      kprat          at            at    ctsq 

 Note that conventional training of LSTMs does not address
the cascading error problem  While LSTMs are very good at
sequenceto sequence prediction tasks  they cannot naturally deal
with the drifting of input state distribution drift caused by action
output feedback in dynamical systems  Bengio et al   
structed as st       kpra      tsq           kpra         tsqs

 Generally  state vector st   of policy    at time   can be con 

Algorithm   Joint MultiAgent Imitation Learning
LearnpA             AK  Cq
Input  Ordered actions Ak   tat kuT
       context tctuT
  
Input  Initialized policies             
Input  base routine TrainpS  Aq mapping state to actions
  Set increasing prediction horizon                Tu
  for                        do
 
 
 

Rollout  at        kp st   kq   agent  
Crossupdate for each policy                Ku
 st          pr at             at               at       ct isq

for                       do

 
 

end for
Policy update for all agent  
     Trainpt st        

   
    kuj
  end for
output   updated policies               

Algorithm   employs   rollout horizon    which divides
the entire trajectory into     segments  The following happens for every segment 

  Iteratively perform rollout at each time step   for all

  policies  line   to obtain actions tpai ku 
  Each policy simultaneously updates its statepsi    ustween predictedpai   versus expert action   

  At the end of the current segment  all policies are updated using the error signal from the deviation bei    for all  

ing the prediction from all other policies  line  

along the subsegment  line  

After policy updates  the training moves on to the next jlength subsegment  using the freshly updated policies for
subsequent rollouts  The iteration proceeds until the end
of the sequence is reached  In the outer loop the rollout
horizon   is incremented 
Two key insights behind our approach are 

  In addition to the trainingprediction mismatch issue
in singleagent learning  each agent   prediction must
also be robust to imperfect predictions from other
agents  This nonstationarity issue also arises in multiagent reinforcement learning  Busoniu et al   
when agents learn simultaneously  We perform joint
training by crossupdating each agent   state using
previous predictions from other agents 

  Many singleagent imitation learning algorithms assume the presence of   dynamic oracle to provide onestep corrections   
  along the rollout trajectories  In
practice  dynamic oracle feedback is very expensive
to obtain and some recent work have attempted to relax this requirement  Le et al    Ho   Ermon 
  Without dynamic oracles  the rolledout trajectory can deviate signi cantly from demonstrated trajectories when the prediction horizon   is large      
leading to training instability  Thus   is gradually increased to allow for slowly learning to make good sequential predictions over longer horizons 

Coordinated MultiAgent Imitation Learning

For ef cient training  we focus on stochastic optimization  which can invoke base routine Train multiple times
and thus naturally accommodates varying    Note that the
batchtraining alternatives to Algorithm   can also employ
similar training schemes  with similar theoretical guarantees lifted to the multiagent case  The Appendix shows
how to use DAgger  Ross et al    for Algorithm  
which we used for our synthetic experiment 

  Coordination Structure Learning

The coordination mechanism is based on   latent structured
model that governs the role assignment  The training and
inference procedures seek to address two main issues 

  LearnStructure  unsupervised learning   proba 

bilistic role assignment model   

  Assign  how   informs the indexing mechanism so
that unstructured trajectories can be mapped to structured trajectories amenable to Algorithm  

Given an arbitrarily ordered set of trajectories    
tU          UK  Cu  let the coordination mechanism underlying each such   be governed by   true unknown model
   with global parameters   We suppress the agent policy
subscript and consider   generic featurized trajectory xt  
rut  cts     Let the latent role sequence for the same agent
be           At any time    each agent is acting according
to   latent role zt   Categoricalt             Ku  which
are the local parameters to the structured model 
Ideally  role and index asignment can be obtained by calculating the true posterior ppz        which is often intractable  We instead aim to approximate ppz       by  
simpler distribution   via techniques from stochastic variational inference  Hoffman et al    which allows for
ef cient stochastic training on minibatches that can naturally integrate with our imitation learning subroutine 
In variational inference  posterior approximation is often
cast as optimizing over   simpler model class    via searching for parameters   and   that maximize the evidence
lower bound  ELBO    
Lpqpz   qq   Eq rln ppz    xqs   Eq rln qpz   qs   ln ppxq
Maximizing   is equivalent to  nding       to minimize
the KL divergence KLpqpz   xq ppz   xqq  We focus on
the structured mean eld variational family  which factorizes   as qpz       qpzqqp    This factorization breaks the
dependency between   and    but not between single latent
states zt  unlike variational inference for       data  Kingma
  Welling   

  TRAINING TO LEARN MODEL PARAMETERS

The procedure to learn the parameter of our structured
model is summarized in Algorithm   Parameter learning

Algorithm   Structure Learning
LearnStructure tU          UK            qp  zq
Input  Xk   txt kuT
     trut    ctsu           tXkuK
  
  Local update  compute qpzq via messagepassing while    

Graphical model parameters   stepsize  

ing    See Appendix for derivations 
                  prior   bJEqpzq rtpz  xqsq

  Global parameter update  via natural gradient ascent
output Updated model qp  zq   qp qqpzq
proceeds via alternating updates over the factors qp   and
qpzq  while keeping other factor  xed  Stochastic variational inference performs such updates ef ciently in minibatches  We slightly abuse notations and overload   for the
natural parameters of global parameter   in the exponential
family  Assuming the usual conjugacy in the exponential
family  the stochastic natural gradient takes   convenient
form  line   of Algo   and derivation in Appendix  where
tpz  xq is the vector of suf cient statistics    is   vector of
scaling factors adjusting for the relative size of the minibatches  Here the global update assumes optimal local update qpzq has been computed 
Fixing the global parameters  the local updates are based
on messagepassing over the underlying graphical model 
The exact mathematical derivation depends on the speci  
graph structure  The simplest scenario is to assume independence among zt    which resembles naive Bayes  In our
experiments  we instead focus on Hidden Markov Models
to capture  rstorder dependencies in role transitions over
play sequences  In that case  line   of Algorithm   resembles running the forwardbackward algorithm to compute
the update qpzq  The forwardbackward algorithm in the
local update step takes OpK  Tq time for   chain of length
  and   hidden states  For completeness  derivation of
parameter learning for HMMs is included in the Appendix 

  INFERENCE FOR ROLE AND INDEX ASSIGNMENT

We can compute two types of inference on   learned   
Role inference  Compute the most likely role sequence
                 KuT        using Viterbi  or dynamic
tzt kuT
programmingbased forward message passing for graph
structures  This most likely role sequence for agent   
which is the lowdimensional representation of the coordination mechanism  can be used to augment the contextual
feature tctuT
Rolebased Index Assignment Transform the unstructured set   into an ordered set of trajectories   to facilitate
the imitation learning step  This is the more important task
for the overall approach  The intuitive goal of an indexing mechanism is to facilitate consistent agent trajectory to
policy mapping  Assume for notational convenience that
we want index   assigned to an unique agent who is most
likely assuming role     Our inference technique rests on the

  for each agent   policy training 

Coordinated MultiAgent Imitation Learning

  

    tUkuK

Algorithm   MultiAgent Role Assignment
Assign tU          UK qu   rA          AKs
Input  Approximate inference model    Unordered trajectories
  Calculate cost matrix     RK   per equation  
      MinCostAssignmentpMq
output Ak   UApkq                   
wellknown Linear Assignment Problem  Papadimitriou  
Steiglitz    which is solved optimally via the KuhnMunkres algorithm  Speci cally  construct the cost matrix
  as 
           
    

 
  
 
qpxt   zt      kq
  

qptxt ku zt      kq

 
 
ln qptxt ku zt      kq

ln qpxt   zt      kq

 

 

    

 

 

 
 

  
 

  

where                                      is the Hadamard
product  and matrices       take advantage of the
Markov property of the graphical model  Now solving
the linear assignment problem for cost matrix    we obtain the matching   from role    to index    such that the
total cost per agent is minimized  From here  we rearrange the unordered set tU          UKu to the ordered sequence rA          AKs   rUAp            UApKqs according to
the minimum cost mapping 
To see why this index assignment procedure results in an
increased entropy in the original objective   notice that 
Pp kqqpApAkq    kq log qpApAkq    kq
  

HpA Dq       

   
     
 

   

Mp    kq 

where we assume each latent role    has equal probability 
The RHS increases from the linear assignment and consequent role assignment procedure  Our inference procedure
to perform role assignment is summarized in Algorithm  

  Experiments
We present empirical results from two settings  The  rst is
  synthetic setting based on predatorprey  where the goal
is to imitate   coordinating team of predators  The second
is   largescale imitation learning setting from player trajectores in professional soccer games  where the goal is to
imitate defensive team play 

  PredatorPrey Domain
Setting  The predatorprey problem  also frequently called
the Pursuit Domain  Benda    is   popular setting for

Figure  

multiagent reinforcement learning  The traditional setup
is with four predators and one prey  positioned on   grid
board  At each time step  each agent has  ve moves 
        or no move  The
world is toroidal  the agents
can move off one end of
the board and come back
on the other end  Agents
make move simultaneously 
but two agents cannot occupy the same position  and
collisions are avoided by assigning   random move priority
to the agents at each time step  The predators can capture
the prey only if the prey is surrounded by all four predators  The goal of the predators is to capture the prey as fast
as possible  which necessarily requires coordination 
Data  The demonstration data is collected from   game
instances  where four experts  indexed   to   are prescribed
the consistent and coordinated role as illustrated in the capture state of Figure   In other words  agent   would attempt to capture the prey on the right hand side  which allows for one  xed role for each expert throughout the game 
However  the particular role assignment is hidden from the
imitation learning task  Each expert is then exhaustively
trained using Value Iteration  Sutton   Barto    in the
reinforcement learning setting  with the reward of   if the
agent is in the position next to the prey according to its de 
 ned role  and   otherwise    separate set of   games
was collected for evaluation    game is terminated after
  time steps if the predators fail to capture the prey  In
the test set  the experts fail to capture the prey in   of the
games  and on average take   steps to capture the prey 
Experiment Setup  For this experiment  we use the batch
version of Algorithm    see appendix  to learn to imitate
the experts using only demonstrations  Each policy is represented by   random forest of   trees  and were trained
over   iterations  The expert correction for each rolledout
state is collected via Value Iteration  The experts thus act as
dynamic oracles  which result in   multiagent training setting analogous to DAgger  Ross et al    We compare
two versions of multiagent imitation learning 

  Coordinated Training  We use our algorithm  with
the latent structure model represented by   discrete
Hidden Markov Model with binomial emission  We
use Algorithm   to maximize the role consistency of
the dynamic oracles across different games 

  Unstructured Training  An arbitrary role is assigned
to each dynamic oracle for each game       the agent
index is meaningless 

In both versions  training was done using the same data aggregation scheme and batch training was conducted using
the same random forests con guration 

Coordinated MultiAgent Imitation Learning

Figure   Comparing performance in PredatorPrey between our
approach and unstructured multiagent imitation learning  as  
function of the number of training rounds  Our approach demonstrates both signi cantly lower failure rates as well as lower average time to success  for successful trials 
Results  Figure   compares the test performance of our
method versus unstructured multiagent imitation learning 
Our method quickly approaches expert performance  average   steps with   failure rate in the last iteration 
whereas unstructured multiagent imitation learning performance did not improve beyond the  rst iteration  average
  steps with   failure rate  Note that we even gave the
unstructured baseline some advantage over our method  by
forcing the prey to select the moves last after all predators
make decisions  effectively making the prey slower  Without this advantage  the unstructured policies fail to capture
the prey almost   of the time  Also  if the same restriction is applied to the policies obtained from our method 
performance would be on par with the experts   success rate  with similar number of steps taken 

  Multiagent Imitation Learning for Soccer
Setting  Soccer is   popular domain for multiagent learning  RoboCup  the robotic and simulation soccer platform 
is perhaps the most popular testbed for multiagent reinforcement learning research to date  Stone    The
success of MARL has been limited  however  due to the extremely high dimensionality of the problem  In this experiment  we aim to learn multiagent policies for team soccer
defense  based on tracking data from reallife professional
soccer  Bialkowski et al   
Data  We use the tracking data from   games of real
professional soccer from   recent European league  The
data was chunked into sequences with one team attacking
and the other defending  Our goal is to learn up to   policies for team defense   players per team  minus the goal
keeper  The training data consists of   sets of trajectories     tA                where Ak   tat kuT
   is the
sequence of positions of one defensive player  and   is the

Figure   Experimental results on soccer domain  We see that using coordination substantially improves the imitation loss  and
that the decentralized policy is comparable to the centralized 
context consisting of opponents and the ball  Overall  there
are about   million frames at   frames per second  The
average sequence length is   steps  and the maximum is
 
Experiment Setup  Each policy is represented by   recurrent neural network structure  LSTM  with two hidden layers of   units each  As LSTMs generally require
 xedlength input sequences  we further chunk each trajectory into subsequences of length   with overlapping
window of   time steps  The joint multiagent imitation
learning procedure follows Algorithm   closely  In this setup  without access to dynamic oracles for imitation learning in the style of SEARN  Daum   III et al    and
DAgger  Ross et al    we gradually increase the horizon of the rolledout trajectories from   to   steps lookahead  Empirically  this has the effect of stabilizing the
policy networks early in training  and limits the cascading
errors caused by rollingout to longer horizons 
The structured model component is learned via stochastic
variational inference on   continuous HMM  where the perstate emission distribution is   mixture of Gaussians  Training and inference operate on the same minibatches used
for joint policy learning 
We compare against two variations  The  rst employs centralized policy that aggregates the state vectors of all decentralized learner and produces the actions for all players 
       multitask policy  The centralized approach generally
requires more model parameters  but is potentially much
more accurate  The second variation is to not employ joint
multiagent training  we modify Algorithm   to not crossupdate states between agents  and each role is trained conditioned on the ground truth of the other agents 
Results  Figure   shows the results  Our coordinated
learning approach substantially outperforms conventional
imitation learning without structured coordination  The
imitation loss measures average distance of rollouts and
ground truth in meters  note the typical size of soccer  eld

Coordinated MultiAgent Imitation Learning

Figure   Result of   coordinated imitation policies  corresponding with Figure   White is the rolledout imitation policies 
is       meters  As expected  average loss increases
with longer sequences  due to cascading errors  However 
this error scales sublinearly with the length of the horizon  even though the policies were trained on sequences
of length   Note also that the performance difference between decentralized and centralized policies is insigni cant
compared to the gap between coordinated and unstructured
policies  further highlighting the bene ts of structured coordination in multiagent settings  The loss of   single network  nonjoint training scheme is very large and thus omitted from Figure    see the appendix 
Visualizations 
Imitation loss  of course  is not   full
re ection of the quality of the learned policies  Unlike
predatorprey  the longterm reward signal is not available 
so we rely on visual inspection as part of evaluation  Figure   overlays policy prediction on top of the actual game
sequence from Figure   Additional test examples are included in our supplemental video   We note that learned
policies are qualitatively similar to the ground truth demonstrations  and can be useful for applications such as counterfactual replay analysis  Le et al    Figure   displays the Gaussian components of the underlying HMM 
The components correspond to the dominant modes of the
roles assigned  Unlike the predatorprey domain  roles can
be switched during   sequence of play  See the appendix
for more details on role swap frequency 

  Other Related Work
The problem of multiagent imitation learning has not been
widely considered  perhaps with the exception of  Chernova   Veloso    which focused on very different applications and technical challenges       learning   model
of   joint task by collecting samples from direct interaction
with teleoperating human teachers  The actual learning algorithm there requires the learner to collect enough data
points from human teachers for con dent classi cation of

 Watch video at http hoangminhle github io

Figure   Components of role distributions  corresponding to  
popular formation arrangement in professional soccer
task  It is not clear how well the proposed method would
translate to other domains 
Indexfree policy learning is generally dif cult for blackbox machine learning techniques  Some recent work has
called attention to the importance of order to learning when
input or output are sets  Vinyals et al    motivated by
classic algorithmic and geometric problems such as learning to sort   set of numbers  or  nding convex hull for  
set of points  where no clear indexing mechanism exists 
Other permutation invariant approaches include those for
standard classi cation  Shivaswamy   Jebara   

  Limitations and Future Work
In principle  the training and inference of the latent structure model can accommodate different types of graphical
models  However  the exact procedure varies depending
on the graph structure  It would be interesting to  nd domains that can bene   from more general graphical models  Another possible direction is to develop fully endto end differentiable training methods that can accommodate our indexfree policy learning formulation  especially
deep learningbased method that could provide computational speedup compared to traditional graphical model inference  One potential issue with the endto end approach
is the need to depart from   learningreductions style approach 
Although we addressed learning from demonstrations in
this paper  the proposed framework can also be employed
for generative modeling  or more ef cient structured exploration for reinforcement learning  Along that line  our proposed method could serve as   useful component of general
reinforcement learning  especially in multiagent settings
where traditional explorationbased approaches such as Qlearning prove computationally intractable 
Acknowledgement  This work was funded in part by NSF
Awards       JPL PDF IAMS   
Bloomberg Data Science Research Grant  and   gift from
Northrop Grumman 

Coordinated MultiAgent Imitation Learning

References
Beal  Matthew James  Variational algorithms for approximate
Bayesian inference  University of London United Kingdom 
 

Benda  Miroslav  On optimal cooperation of knowledge sources 

Technical Report BCSG   

Bengio  Samy  Vinyals  Oriol  Jaitly  Navdeep  and Shazeer 
Noam  Scheduled sampling for sequence prediction with reIn Advances in Neural Information
current neural networks 
Processing Systems  pp     

Bialkowski  Alina  Lucey  Patrick  Carr  Peter  Yue  Yisong  and
Matthews  Iain   win at home and draw away  Automatic formation analysis highlighting the differences in home and away
In MIT Sloan Sports Analytics Conference
team behaviors 
 SSAC   

Blei  David    Kucukelbir  Alp  and McAuliffe  Jon    Variational inference    review for statisticians  Journal of the
American Statistical Association   justaccepted   

Busoniu  Lucian  Babuska  Robert  and De Schutter  Bart    comprehensive survey of multiagent reinforcement learning  IEEE
Transactions on Systems Man and Cybernetics Part   Applications and Reviews     

Chernova  Sonia and Veloso  Manuela  Multiagent collaborative
task learning through imitation  In Proceedings of the fourth
International Symposium on Imitation in Animals and Artifacts  pp     

Kingma  Diederik   and Welling  Max  Autoencoding variational

bayes  arXiv preprint arXiv   

Kingston  Peter and Egerstedt  Magnus  Indexfree multiagent
IFAC Proceedings Volumes 

systems  An eulerian approach 
   

Kloder  Stephen and Hutchinson  Seth 

permutationinvariant multirobot formations 
tions on Robotics     

Path planning for
IEEE Transac 

Kok  Jelle    Spaan  Matthijs TJ  Vlassis  Nikos  et al  Multirobot
In Proceedings
decision making using coordination graphs 
of the  th International Conference on Advanced Robotics 
ICAR  volume   pp     

Krause  Andreas  Perona  Pietro  and Gomes  Ryan    Discriminative clustering by regularized information maximization  In
Advances in neural information processing systems  pp   
   

Le  Hoang  Kang  Andrew  Yue  Yisong  and Carr  Peter  Smooth
imitation learning for online sequence prediction  In Proceedings of The  rd International Conference on Machine Learning  pp     

Le  Hoang    Carr  Peter  Yue  Yisong  and Lucey  Patrick  Datadriven ghosting using deep imitation learning  In MIT Sloan
Sports Analytics Conference  SSAC   

Panait  Liviu and Luke  Sean  Cooperative multiagent learning 
The state of the art  Autonomous agents and multiagent systems     

Daum   III  Hal  Langford  John  and Marcu  Daniel  Searchbased structured prediction  Machine learning   
 

Papadimitriou  Christos   and Steiglitz  Kenneth  Combinatorial
optimization  algorithms and complexity  Courier Corporation 
 

Dudik  Miroslav  Phillips  Steven    and Schapire  Robert    Performance guarantees for regularized maximum entropy density estimation  In International Conference on Computational
Learning Theory  pp    Springer   

Ross  Stephane  Gordon  Geoff  and Bagnell     Andrew    reduction of imitation learning and structured prediction to noregret
In Conference on Arti cial Intelligence and
online learning 
Statistics  AISTATS   

Grandvalet  Yves  Bengio  Yoshua  et al  Semisupervised learning by entropy minimization  In NIPS  volume   pp   
   

Shivaswamy  Pannagadatta   and Jebara  Tony  Permutation invariant svms  In Proceedings of the  rd international conference on Machine learning  pp    ACM   

Guestrin  Carlos  Lagoudakis  Michail  and Parr  Ronald  Coordinated reinforcement learning  In ICML  volume   pp   
   

Ho  Jonathan and Ermon  Stefano  Generative adversarial imitation learning  In Advances in Neural Information Processing
Systems  pp     

Hochreiter  Sepp and Schmidhuber    urgen  Long shortterm

memory  Neural computation     

Hoffman  Matt and Blei  David  Structured stochastic variational

inference  CoRR abs   

Hoffman  Matthew    Blei  David    Wang  Chong  and Paisley  John William  Stochastic variational inference  Journal of
Machine Learning Research     

Johnson  Matthew James and Willsky  Alan    Stochastic variational inference for bayesian time series models  In ICML  pp 
   

Stone  Peter  What   hot at RoboCup  In The AAAI Conference

on Arti cial Intelligence  AAAI   

Stone  Peter and Veloso  Manuela  Multiagent systems    survey
from   machine learning perspective  Autonomous Robots   
   

Sutton  Richard   and Barto  Andrew    Reinforcement learning 

An introduction  volume   MIT press Cambridge   

Vinyals  Oriol  Bengio  Samy  and Kudlur  Manjunath  Order matters  Sequence to sequence for sets  arXiv preprint
arXiv   

