  Divergence Bound for Hybrids of MCMC and Variational Inference and an

Application to Langevin Dynamics and SGVI

Justin Domke  

Abstract

Two popular classes of methods for approximate inference are Markov chain Monte Carlo
 MCMC  and variational
inference  MCMC
tends to be accurate if run for   long enough time 
while variational inference tends to give better
approximations at shorter time horizons  However  the amount of time needed for MCMC to
exceed the performance of variational methods
can be quite high  motivating more  negrained
tradeoffs  This paper derives   distribution over
variational parameters  designed to minimize  
bound on the divergence between the resulting
marginal distribution and the target  and gives an
example of how to sample from this distribution
in   way that interpolates between the behavior
of existing methods based on Langevin dynamics and stochastic gradient variational inference
 SGVI 

  Introduction
One of the simplest Markov chain Monte Carlo  MCMC 
methods is Langevin dynamics  Grenander   Miller   
Robert   Casella    Sec    where one repeats gradient steps with injected Gaussian noise  Namely  one iterates

 

       

 

 rz log          

where   is sampled from   standard Gaussian distribution
and   is   stepsize that may decay over time  To get exact samples    MetropolisHastings rejection step should
be used  However  as the stepsize   becomes smaller 
the acceptance probability goes to one  and so this can be
disregarded  In the Bayesian inference setting    denotes
unknown parameters and      represents   posterior over

 College of Computing and Information Sciences  University of Massachusetts  Amherst  USA  Correspondence to  Justin
Domke  domke cs umass edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

them  Computing      thus requires   full pass over the
dataset  The idea of Stochastic Gradient Langevin Dynamics  Welling   Teh    is to get an unbiased estimate
of the gradient of log      by approximating the posterior
using   minibatch  Since the gradient term is multiplied by
  the variance of its error has order   while the variance
of    has order   Thus  with   small step   the injected
noise dominates  meaning one can might expect these to
have   stationary distribution close to the target     
The idea of variational inference is to posit   simple family of distributions qw    and then optimize   to minimize
the KLdivergence from qw    to the target     
 In  
Bayesian setting  this is equivalent to maximizing the evidence lower bound  If one were to use plain gradient ascent to perform this optimization  this leads to the iterate
of

       

 
 rwEqw   log        log qw   

 

where the gradient step   is divided by two for convenience 
and may again decay over time  In practice  doing gradient
ascent like this has two dif culties  First  in the Bayesian
inference setting  computing log      requires   full pass
over the data  This can be approximated without bias using
  minibatch  The second dif culty stems from the expectation of log      with respect to qw    in Eq    The gradient
of this expectation can be approximated in different ways
using samples from qw     Section   Since this gives
unbiased estimates of the gradient  stochastic optimization
methods  such as stochastic gradient descent  will converge
to   local optima when the stepsize becomes small 
Intuitively  both MCMC and variational methods can be
thought of as trying to  nd   highprobability region of
log      with different strategies to encourage entropy to
get coverage of the distribution  In MCMC  entropy is created by randomness in the Markov chain  while in variational methods the KLdivergence directly measures the
entropy of the variational distribution  It is natural to think
that hybrid methods might employ fractions of these two
strategies  This paper does so by de ning   distribution
over the parameters   of the approximating family       
This distribution      is designed to minimize   bound on
the KLdivergence of the resulting marginal distribution

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

     

     

     

     

     

Figure   Inference on toy distributions  Colored contours show the target density      Sampled weights   are drawn equally from
      iterations and pictured as ellipsoids at one standard deviation  Fewer   are shown for smaller   to avoid visual clutter 

     to the target     
Intuitively  when de ning     
there are two ways to encourage entropy  namely to give
higher density to vectors   where        has high entropy
or to encourage entropy over   itself 
Given   distribution         and   target distribution     
this paper uses two bounds on the marginal divergence
Dtrue   KL      kp      First  one can upperbound by
the conditional divergence      KL         kp   
Second  one can  adjoin  some distribution        to
     and bound Dtrue by the joint divergence     
KL         kp       This paper takes   convex combination of these two bounds       uses the family of bounds
                     For any   between   and   the
distribution      is derived that minimizes this bound for
 xed   and    xed approximating family         Theorem
  Then  one can sample from this distribution over   by
 stochastic  Langevin dynamics  This turns out to lead to
the iterate

 

       

 rw Eqw  log              log qw   
    log         
where      is related to the adjoining distribution       
and may be thought of as    base measure    over the space
of    Of course  exactly computing this gradient is intractable in general  but the same strategies used for variational inference can be used to ef ciently compute an un 

 

 To see why   is necessary  consider   reparameterization of
   In order for the marginal distribution      to stay the same  the
density of the base measure must be transformed corresponding
to the reparameterization  Put another way  without      the
results of inference would depend on the parameterization of   
even with an arbitrarily small stepsize 

biased estimate 
It can be shown that as       the solution concentrates
in the optima of the variational optimization  and thus the
sampling algorithm reduces to gradient ascent on the variational objective  Meanwhile  as will be discussed in Sections   and   it is best to make     function of   such that
  with high entropy have less density when   approaches
  When this is true  then as       only   indexing
highly concentrated distributions over   retain density  and
so sampling from      is essentially just   reparameterization of sampling from      and the algorithm reduces to
Langevin dynamics 
Thus  for the extreme cases of       and       this algorithm reduces to variational inference and Langevin dynamics  respectively  Informally  at these extremes  the algorithm is  fast but approximate  and  slow but accurate 
In the intermediate range  the algorithm exhibits new behavior with    negrained tradeoff between speed and accuracy  Thus  this approach gives   smooth continuum of
algorithms with different priorities of speed and accuracy 

  Notation
This paper uses three notational conventions that are
not universal  First  the conditional probability       
will alternatively be written as qw    when convenient 
Second        indicates that   and   have the same
expected value or  if   is   constant  that   is an unbiased
estimator of    Finally  upper and lowercases indicate
what terms are random variables in   KLdivergence  So 
KL        kp        Eq      log              
is
the
      
for
   xed    while KL         kp        

divergence

      

between

and

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Eq      log                
tional divergence 

is   standard condi 

  Information Theoretic Results
This section derives   few results from an information theoretic viewpoint  without any particular regard for form of
the target distribution  or how one might sample from it 
Proofs are given in the appendix 

  Preliminaries
Let      be the target distribution  For simplicity    is
treated here as   continuous variable  although the results
in this section remain true if it is discrete  There is some
 xed set of conditional distributions        which may
also be written as qw    In principle  one might like to
know how to set      such that the resulting marginal distribution over    is close to      as measured by the KLdivergence 

Dtrue   KL      kp      Eq    log

    
    

 

 

This quantity is dif cult to control directly  since the
marginal      typically cannot be evaluated in closed form 
One bound comes from the conditional KLdivergence  as
stated in the following Lemma 
Lemma   The divergence from      to      is

KL      kp      KL         kp   

 Iq      

 

  

  

 

 
where      Eq      log              is the conditional divergence and Iq   KL          kq         denotes mutual information under   

  second bound comes from adjoining some distribution
       to      Then  the following lemma is essentially
just   restatement of the chain rule for the KLdivergence
 Cover   Thomas    Thm   
Lemma   The KLdivergence between      and      can
be written as

KL     kp      KL         kp      

 

  

  

  KL        kp       

 

 

Since the mutual information in Eq    and the conditional
divergence on the right of Eq    are both nonnegative 
both    and    are upperbounds on Dtrue  Note that if
              then         and so Lemma   follows
from Lemma  

  Divergence Bound and its Minimizer
This paper is based on convex combination of    and     
parameterized by some         namely
                  

 

Since    and    are upperbounds  so is    The following theorem gives the distribution      to minimize   
Theorem   For  xed values of   and        the distribution      that minimizes    is

 

exp     

       log        KL        kp     

       exp          
    logZw
      KL        kp     
Moreover  at    the objective value is         
Since    is
divergence    must be nonpositive 
also be seen directly 
 

upperbound

an

on

the KLcan
This
since it can be written as

 

and the inner diver 

logRw      exp KL        kp       

      KL        kp   
gences as well as       are nonnegative 
To understand the connection of this bound to variational
inference and MCMC  one can look at behavior where    
  or where       For the former case  one obtains   result
closely related to the solution to the variational inference
problem 
Remark   In the limit where       the divergence bound
at the optimal    becomes

lim
 

     inf
 

KL        kp     

 

Similarly  when            concentrates at the minimizer    of this divergence   Formally  with multiple global
optima with the same divergence       will concentrate
equally around all minimizers 
For the case where         it is easy to see that

     exp  KL        kp       

       logZw
 
which will be zero if      is only supported on   where the
divergence between        and        is zero  This may
initially seem like   strange condition  given that      results from both the target distribution      and the adjoined
distribution        However  the next section gives more
speci cs 

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI
  Speci   Form for       
The results in the previous section were written without
without considering the speci   form of        This paper
takes the approach of de ning

and the goal is to set   so that             Since  
must be integrated out  the entropy of   under   is typically intractable  but can be bounded by augmenting   with
some distribution        and then optimizing the joint KLdivergence over   and    similarly to Lemma   Note two
differences  First  here the distribution over   is mathematically derived to minimize the bound  while previous
work numerically optimizes   at runtime  Second  previous work also optimizes parameters of the distribution
       to further improve the bound  while here it is left
 xed  for each   for simplicity  Sec    Future work
might explore optimizing        at runtime in the current
paper   framework 

        

          

rz

 

 

where      is   possibly improper distribution over   and

rz   Rw            is   normalizer  Note that        is

still wellde ned as long as the integral de ning rz exists 
Here  we assume for convenience that rz is   constant  not
depending on    Enforcing rz to be constant essentially
means choosing      in such   way that it doesn    favor 
any   over any other since if             then      is
uniform over   
Lemma   If                    rz and rz is   constant  then the solution in Thm    holds with

       log        log rz
  Eqw    log              log qw   

 

This gives   distribution over      that can be written in
various equivalent ways  such as

qw   

            exp                   
      exp         KL  qw   kp   
 log              log qw   
      exp   
where         Eqw   log qw    is the entropy of qw
and        Eqw   log     
Here       plays   role similar to   base density in an
exponential family 
If one simply used          then
       may not be wellde ned  Further  without   base
density  the marginal      would depend on the particular
parameterization of    To see this  take single parameter
   and imagine reparameterizing so the negative reals are
 squashed  by   factor of two  The base density must increase by   factor of two for the negative reals to compensate in order to leave the marginal      unchanged under
the reparameterization 

  Latent variables in variational inference
Adjoining        to      to de ne    above is strongly
related to auxiliary random variables  Agakov   Barber 
  explored in variational inference to increase the representative power of         by including latent stochastic
transition operators  Salimans et al    random Gaussian process mappings  Tran et al    or hierarchical
variables  Ranganath et al   
Imagine doing variational inference with         where now   is auxiliary

  Bayesian Inference
This section considers algorithms to sample from the distribution de ned by Eq    Probabilistic inference can
be used in various settings  but for concreteness the rest of
this paper will focus on Bayesian inference  To    notation 
suppose   set of   inputs xi with corresponding outputs yi 
 In   generative setting  xi would be empty  Then  if   is
  vector of parameters  the posterior distribution over   is 
up to   constant

NXi 

log        log       

log   yi xi         

 

where      is the prior  and   yi xi     is the likelihood
for the ith datum  The goal of probabilistic inference is to
be able to evaluate expectations with respect to     

  Langevin Dynamics
The goal of MCMC methods is to obtain samples from
the target distribution      Langevin dynamics sample by
an extremely simple process of repeating gradient steps of
log    with injected Gaussian noise  Speci cally  the iterate is

       

 
where   is sampled from   standard Multivariate Gaussian
distribution and   is   stepsize that may decay over time  If
Langevin dynamics are used as   proposal for   MetropolisHastings sampler  it can be shown that correct acceptance
ratio is

 

   log          

exp              

 

 
 

 
 
  krs     
  krs       
           rs      rs   

 

where        log      up to   constant factor  and    is
the proposed point from Eq    It is easy to see that as  
becomes small  this acceptance ratio will go to one  and so
one can disregard the acceptance step with some bias in the
results determined by the step size 

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

 

Langevin dynamics explore the space of   through   random walk  and thus are likely to be slower than alternatives
such as Hamiltonian Monte Carlo when used as an exact
method  Neal    Section   The main practical advantage of Langevin dynamics comes from the case where
the number of data   is large  In that case  one can use  
minibatch of   elements and approximate   log      as
  Xi minibatchr log   yi xi    
  log          log     

 
Thus  Stochastic Gradient Langevin Dynamics  avoid   full
pass through the dataset in each iteration  and so can scale
to large datasets  Though Eq    is an unbiased estimate of
the gradient of log      this still adds   bias to the stationary distribution of the the chain   See  Mandt et al   
for an analysis  In the limit of   small stepsize  the variance of the noise due to stochastic estimation of the gradient of log      will be of order   while the variance of the
injected noise is of order   meaning the latter dominates
 Welling   Teh    Teh et al   

  Stochastic Gradient Variational Inference
The goal of variational inference is to maximize
       Eqw   log        log qw   

 
equivalent
to minimizing the KL divergence between
qw    and      If it were possible to exactly compute   
this could be maximized  to   local optima  by   simple
gradient iteration like

       

 
 rwL   

 

While in some cases with speci     and    one can derive exact updates of    Ghahramani   Beal    in general one cannot exactly evaluate the expectation over   in
   One line of approach to this  Ranganath et al   
Salimans   Knowles    is to write the gradient as
rL   Eqw   log qw      log       log qw    and
estimate this by drawing samples from qw    This experimentally seems to result in gradients with large variance  but this can be reduced by two strategies  Firstly  one
can use control variates  based on either Taylor expansion
 Paisley et al    or the fact that the expected value of
rw log qw    is zero  Secondly  since log      is often  
sum of terms de ned on subsets of variables  one can RaoBlackwellize by integrating out other variables  This approach only needs to be able to compute log      no other
access  even to the gradient of    is needed 

  REPARAMETERIZATION TRICK
Another line of approach to variational inference is based
on the  reparameterization trick   Kingma   Welling 

  Rezende et al    Titsias     azaroGredilla 
  known in the stochastic approximation literature as
   pathwise  derivative  Kushner   Yin    Sec   
Suppose that there is   deterministic mapping zr   from parameters   and random numbers    from some  xed standard distribution  such that zr     qw    Then  we could
equivalently write

       ER log   zR          

 
where         Eqw    log qw    denotes the entropy 
The advantage of Eq    this is that the expectation is not
  function of    and so if computing the gradient of    the
gradient moves inside the expectation  Then  provided zr  
is differentiable with respect to    an unbiased estimate of
the gradient of   is available as

rwL      rw log   zr      rwH   

 

If      cannot be integrated in closedform an alternative
estimator based on

       ER log   zR      log qw zR   

 

is possible  and in some circumstances this can even have
lower variance  Roeder et al   
Computing the gradient of log   zr    with respect to  
amounts to computing the derivative of log      with respect to   multiplied by the Jacobian dzT
    dw  For simple distributions like Gaussians zr   this is easy to do analytically  Section   Alternatively  this can all be done
ef ciently by automatic differentiation   Kucukelbir et al 
  the approach used here 
As with Langevin dynamics  with   large dataset  log     
can be approximated by taking   sum over   minibatch  The
most obious approach to this would be to choose   single
vector   and use it throughout the minibatch  However 
variance can often be reduced by the  local reparameterization trick   Kingma et al    in which   different
random vector ri is drawn for each datum in the minibatch 
Intuitively  the motivation is that if   and   are two data in
the minibatch then log   yi xi  zr    should be less correlated with log   yj xj  zr    when   different random vector    is used for the second datum  consider the limiting
case where all data are identical  Then  one can write the
approximation as

rwL     

 

  Xi minibatch rw log   zri   

  Nrw log   yi xi  zri      rw log qw zri   

Note that the approximation on the righthand side depends
both on the choice of the minibatch and on the random vectors ri  one chosen per element of the minibatch 

 

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI
  Hybrid Dynamics
An algorithm that interpolates between the methods in the
previous section results from applying Langevin dynamics
to the distribution over   de ned by Thm    If the adjoining distribution        is chosen as in in Sec    with
some      that depends on   then de ne

         log     

  Eqw   log              log       

 

which is   times the form of      derived in   with the
constant term of log rz dropped  Applying Langevin Dynamics  to this results in the iteration

       

 

 rL       

 

where again   is   stepsize that may decrease over time and
  is sampled from   standard Gaussian distribution 
This paper uses   closedform for the entropy       
 Eqw    log qw    and so uses the gradient estimator

rL         log             rH   

  Eqw   log              log       

 

  Speci cs For Gaussians
The following experiments use the simplest common variational distribution for qw    namely   fullyfactorized
Gaussian distribution  To make   unconstrained  let    
    where   is   vector of mean components and the
standard deviation of the ith dimension is          To
sample from this distribution given   vector    one can
simply take zr               where   is elementwise
product  The entropy of qw    is  up to constant factors
      Pi    ln  
an improper prior       Qi           that is uniform

over   with   Gaussian prior on   with    xed variance of
  and   mean    Since this is uniform over   rz is   constant  The value    was calculated to numerically optimize
the divergence bound    for   onedimensional standard
Gaussian      Since         at the solution  Theorem   for any given   and       can be evaluated by
symbolically integrating out   and then numerically integrating over   The values used in these experiments are
below  with linear interpolation used for any other  

It remains to set the base density  These experiments used

 

   
 
                        

 

 

 

 

 

 

 

 

 For some stepsize   the Langevin dynamics would immediately be          rs        
If we de ne
      then this results in the given form since  rs     
 rs       rL   

Figure   Performance of the hybrid algorithm with various values
of   on the toy mixture of three Gaussians from Fig   

Theoretically  it   easy to show that as          is minimized  with   minimum of zero  by        meaning that after one iteration        and henceforth 
zr       Thus  Eq    is equivalent to Langevin dynamics
as in Eq    However  using                prior centered on   standard deviation of       is practically
equivalent and is more useful to guide linear interpolation 
Meanwhile  when       the algorithm is independent of
     though the optimal value of        is correct for
small  
The above choice of      is quite arbitrary  It is possible that other choices could be better  though performance
was surprisingly insensitive in practice  With large datasets
log      dominates log      just as likelihoods dominate
Bayesian priors  Calling on inspiration from recent work
in variational inference  it might also be useful to optimize
       at runtime  Section  
  Experiments
There does not appear to be   single standard performance measure to evaluate approximate inference algorithms  For Bayesian inference  testset accuracy or likelihood are sometimes used  but these measures have high
variance due to the dataset  and con ate evaluation of the
inference method with evaluation of the model it is running
on   An inference method with poor coverage of      can
have higher accuracy than   perfect sampler due to model
misspeci cation or dataset peculiarities 
For   more  negrained measure of inference performance 
this paper uses the Maximum Mean Discrepancy  MMD 
measure  Gretton et al      MMD is essentially the empirical difference of the means of two samples in some feature space  We  rst draw   sample from
     by exhaustively running   traditional MCMC algorithm  Stan  Team    based on   variant of Hamil 

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

     

     

     

     

     

     

Stan

Figure   Samples on ionosphere after    top row     middle row  or    bottom row  iterations  One sample   is drawn from
qw    at each iteration and projected onto the  rst two principal components  computed on Stan samples  Each plot uses the stepsize
  with the lowest MMD at that time horizon  The same sequence of random numbers is for all inference methods   More in appendix 

tonian Monte Carlo  Hoffman   Gelman    for  
large number of iterations  and compare this to the results
of approximate inference  Since we have large samples 
and want to compute the online performance at all iterations     nitedimensional feature space must be used to
avoid the quadratic complexity of computing MMD using an arbitrary kernel  For twodimensional problems  it
was bene cial to use random Fourier features  Rahimi  
Recht    to approximate the radialbasis function kernel           exp kx   yk  For higher dimensions 
the original feature space was suf ciently discriminative 
Approximate inference gives   sequence of vectors wt  For
each timestep    set of   samples At are drawn from
    wt  Then  the MMD between      At and the
sample from exhaustive MCMC can be computed for all  
in linear time    disadvantage of this approach is the need
for exhaustive MCMC for comparison  which by de nition
eludes the largescale cases that motivate Langevin dynamics and stochastic variational inference  However this gives
  more  negrained view of performance 

  Toy Distributions
For    rst demonstration  the algorithm was applied to  
set of toy  dimensional distributions  with various values
of     few results are shown in Fig    with more in

the appendix  While the algorithm displays the expected
tradeoffs between speed and accuracy for different    Fig 
  the results fairly uninteresting as all curves cross near
the same time MMD point where        variational inference  and        MCMC  meet  So  for any given amount
of time  either variational inference or MCMC are nearoptimal  and so intermediate   do not much expand the
 Pareto frontier  on these problems  although one might
prefer an intermediate   since the crossover horizon is not
known in advance 

  Bayesian Logistic Regression
Next  the algorithm was applied to binary logistic regression with several standard datasets  using   minibatch size
of     different random vector   for each element in the
minibatch  and   standard multivariate Laplace distribution
as   prior  Simply picking   single value or schedule for
the step   is unsatisfactory  as the best schedule for   given
problem  length of time  and value of   varies  To give
  fair comparison  inference was run for with   range of
constant   varying by factors of two from    to    
This spans the range from large enough to often diverge
to below optimal even after   iterations  Then  for each
time  the best value of   was retrospectively chosen  with
performance averaged over repetitions before this choice 
It is likely that decaying stepsize schedules would perform

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Figure   Iterations vs MMD for inference with various values of   averaged over  ve runs  each using the same random number
sequence for all   For each   iteration and dataset  the graphs show the MMD for the best stepsize    averaged over the   runs
before computing the best  Since    nite range of   varying by factors of two is used  this can result in visible  kinks  at iterations
where   different  smaller    exceeds the old one  From left to right       australian  ionosphere  and sonar 

better  but this was not pursued since it would reduce transparency of the results 
The mean errors after   repetitions are shown in Fig   
while samples are compared to the ground truth for ionosphere in Fig    This con rms the basic ideas of the algorithm  namely that it smoothly interpolates between the
two previous algorithms without introducing any new behavior  The main experimental  nding is that there is usually   large range of time horizons for which an intermediate setting of   performs better than either of the previous
existing algorithms  This suggests that  say  someone who
has   time budget slightly larger than that needed for variational inference could bene   from running the algorithm
with   small value of   for   slightly larger time  This is in
contrast to the toy twodimensional examples where for almost all time horizons either       or       was optimal 

  Discussion
This paper has two contributions  First    distribution over
variational parameters is derived to minimize   bound on
the marginal divergence to   target distribution  Secondly 
this is used to derive an algorithm that interpolates between Langevin dynamics and stochastic variational inference  The Bayesian logistic regression experiments show
that the intermediate values of this algorithm are useful in
the sense that there are several orders of magnitude of time
horizons where an intermediate algorithm performs better
than either Langevin dynamics or variational inference 
Many improvements to stochastic Langevin dynamics and
variational inference have been proposed beyond the simple algorithms used here  For Langevin dynamics  Welling
  Teh   and Li et al    suggest adding   preconditioner  Patterson   Teh   propose Riemannian
Langevin Dynamics  and Dubey et al    propose to
make use of the  nite sum structure in   Bayesian posterior
     by incorporating techniques for incremental optimization  For stochastic variational inference  Hoffman et al 

  propose to use the natural gradient  Mandt   Blei
  propose using smoothed gradients  and Sakaya  
Klami   propose   strategy of reusing previous gradient computations  It would be interesting to consider using
these ideas with the hybrid algorithm  which could give insight into the relationship between the two different methods 
There is other work based on combining the advantages
of variational inference and MCMC  Stochastic Gradient Fisher Scoring  Ahn et al    interpolates between Langevin dynamics and   Gaussian approximation
of the target  However  this approximation is based on the
Bayesian central limit theorem and so will not be the same
in general as the optimal variational distribution  even when
  Gaussian is used as the variational family  Another line of
research is that of interpreting the path of   MCMC algorithm as   variational distribution  and then  tting parameters to tighten   variational bound  Salimans et al   
Rezende   Mohammed    While motivationally quite
similar  the contribution of this paper is orthogonal  in that
one could conceivably use these same ideas to de ne the
variational family qw    used in this paper  This would result in   somewhat unusual method that runs MCMC over
the parameters of variational family of distributions that are
themselves de ned in terms of    different  MCMC algorithm 

References
Agakov  Felix    and Barber  David  An auxiliary varia 

tional method  In NIPS     

Ahn  Sungjin  Balan  Anoop Korattikara  and Welling 
Max  Bayesian posterior sampling via stochastic gradient  sher scoring  In ICML     

Cover  Thomas    and Thomas  Joy    Elements of Infor 

mation Theory  WileyInterscience       

Dubey  Kumar Avinava  Reddi  Sashank    Williamson 

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Sinead      oczos  Barnab as  Smola  Alexander    and
Xing  Eric    Variance reduction in stochastic gradient
langevin dynamics  In NIPS     

Neal  Radford  MCMC using Hamiltonian dynamics  volume Handbook of Markov Chain Monte Carlo  pp   
  Chapman   Hall   CRC Press     

Ghahramani  Zoubin and Beal  Matthew    Propagation
In NIPS 

algorithms for variational bayesian learning 
   

Paisley  John William  Blei  David    and Jordan 
Michael    Variational bayesian inference with stochastic
search  In ICML     

Grenander  Ulf and Miller  Michael  Representations of
knowledge in complex systems        Statist  Soc   
     

Patterson  Sam and Teh  Yee Whye  Stochastic gradient riemannian langevin dynamics on the probability simplex 
In NIPS     

Gretton  Arthur  Borgwardt  Karsten    Rasch  Malte   
Sch olkopf  Bernhard  and Smola  Alexander      kernel
method for the twosample problem  In NIPS     

Gretton  Arthur  Borgwardt  Karsten    Rasch  Malte   
Sch olkopf  Bernhard  and Smola  Alexander      kernel
twosample test  Journal of Machine Learning Research 
     

Hoffman  Matthew    and Gelman  Andrew  The nou turn
sampler  adaptively setting path lengths in hamiltonian
monte carlo  Journal of Machine Learning Research   
     

Hoffman  Matthew    Blei  David    Wang  Chong 
and Paisley  John William  Stochastic variational inference  Journal of Machine Learning Research   
     

Kingma  Diederik   and Welling  Max  Stochastic gradient
vb and the variational autoencoder  In ICLR     

Kingma  Diederik    Salimans  Tim  and Welling  Max 
Variational dropout and the local reparameterization
trick  In NIPS     

Kucukelbir  Alp  Tran  Dustin  Ranganath  Rajesh  Gelman  Andrew  and Blei  David    Automatic differentiation variational inference  Journal of Machine Learning
Research       

Kushner  Harold and Yin  George 

Stochastic Approximation and Recursive Algorithms and Applications 
SpringerVerlag     

Li  Chunyuan  Chen  Changyou  Carlson  David    and
Carin  Lawrence  Preconditioned stochastic gradient
langevin dynamics for deep neural networks  In AAAI 
   

Mandt  Stephan and Blei  David    Smoothed gradients

for stochastic variational inference  In NIPS     

Mandt  Stephan  Hoffman  Matthew    and Blei  David   
  variational analysis of stochastic gradient algorithms 
In ICML     

Rahimi  Ali and Recht  Benjamin  Random features for

largescale kernel machines  In NIPS     

Ranganath  Rajesh  Gerrish  Sean  and Blei  David   
Black box variational inference  In AISTATS     

Ranganath  Rajesh  Tran  Dustin  and Blei  David    Hier 

archical variational models  In ICML     

Rezende  Danilo Jimenez and Mohammed  Shakir  Variational inference with normalizing  ows  In ICML   
 

Rezende  Danilo Jimenez  Mohamed  Shakir  and Wierstra 
Daan  Stochastic backpropagation and variational inference in deep latent gaussian models 
In International
Conference on Machine Learning     

Robert  Christian and Casella  George  Monte Carlo Sta 

tistical Methods  SpringerVerlag    edition     

Roeder  Geoffrey  Wu  Yuhuai  and Duvenaud  David 
Sticking the landing    simple reducedvariance gradient for advi  NIPS workshop in Advances in Approximate Bayesian Inference     

Sakaya  Joseph and Klami  Arto  Reusing gradient computations in automatic variational inference  Technical
report    NIPS workshop on approximate Bayesian
Inference   

Salimans  Tim and Knowles  David    On using control
variates with stochastic approximation for variational
bayes and its connection to stochastic linear regression 
In ICLR     

Salimans  Tim  Kingma  Diedrik  and Welling  Max 
Markov chain monte carlo and variational inference 
Bridging the gap  In ICML       

Team  Stan Development  Stan Modeling Language Users

Guide and Reference Manual    edition     

Teh  Yee Whye  Thiery  Alexandre    and Vollmer  Sebastian    Consistency and  uctuations for stochastic gradient langevin dynamics     Mach  Learn  Res   
     

  Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Titsias  Michalis    and   azaroGredilla  Miguel  Doubly
stochastic variational bayes for nonconjugate inference 
In ICML     

Tran  Dustin  Ranganath  Rajesh  and Blei  David    The

variational gaussian process  In ICLR     

Welling  Max and Teh  Yee Whye  Bayesian learning via
stochastic gradient langevin dynamics  In ICML   
     

