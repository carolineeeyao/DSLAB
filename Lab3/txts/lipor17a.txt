Leveraging Union of Subspace Structure to Improve Constrained Clustering

John Lipor   Laura Balzano  

Abstract

Many clustering problems in computer vision and
other contexts are also classi cation problems 
where each cluster shares   meaningful label  Subspace clustering algorithms in particular are often
applied to problems that    this description  for
example with face images or handwritten digits 
While it is straightforward to request human input on these datasets  our goal is to reduce this
input as much as possible  We present   pairwiseconstrained clustering algorithm that actively selects queries based on the unionof subspaces
model  The central step of the algorithm is in
querying points of minimum margin between estimated subspaces  analogous to classi er margin 
these lie near the decision boundary  We prove
that points lying near the intersection of subspaces
are points with low margin  Our procedure can be
used after any subspace clustering algorithm that
outputs an af nity matrix  We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the stateof theart active query algorithms on datasets with
subspace structure and is competitive on other
datasets 

  Introduction
The union of subspaces  UoS  model  in which data vectors
lie near one of several subspaces  has been used actively in
the computer vision community on datasets ranging from
images of objects under various lighting conditions  Basri
  Jacobs    to visual surveillance tasks  Oliver et al 
  The recent textbook  Vidal et al    includes  
number of useful applications for this model  including lossy
image compression  clustering of face images under different lighting conditions  and video segmentation  Subspace
clustering algorithms utilize the UoS model to cluster data

 Department of Electrical and Computer Engineering  University Michigan  Ann Arbor  MI  USA  Correspondence to  John
Lipor  lipor umich edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

vectors and estimate the underlying subspaces  achieving excellent performance on   variety of real datasets  However 
as we will show in Section   even oracle UoS classi ers
do not achieve perfect clustering on these datasets  While
current algorithms for subspace clustering are unsupervised 
in many cases   human could provide relevant information
in the form of pairwise constraints between points       answering whether two images are of the same person or
whether two objects are the same 
The incorporation of pairwise constraints into clustering algorithms is known as pairwiseconstrained clustering  PCC 
PCC algorithms use supervision in the form of mustlink
and cannotlink constraints by ensuring that points with
mustlink constraints are clustered together and points with
cannotlink constraints are clustered apart  In  Davidson
et al    the authors investigate the phenomenon that
incorporating poorlychosen constraints can lead to an increase in clustering error  rather than   decrease as one
would expect from additional label information  This is
because points constrained to be in the same cluster that
are otherwise dissimilar can confound the constrained clustering algorithm  For this reason  researchers have turned
to active query selection methods  in which constraints are
intelligently selected based on   number of heuristics  These
algorithms perform well across   number of datasets but do
not take advantage of any known structure in the data  In the
case where data lie on   union of subspaces  one would hope
that knowledge of the underlying geometry could give hints
as to which points are likely to be clustered incorrectly 

Let    cid xi   RD cid  

  

be   set of data points lying near
  union of   linear subspaces of the ambient space  We
denote the subspaces by  Sk  
   each having dimension
dk  An example union of subspaces is shown in Fig   
where                    The goal of subspace clustering algorithms has traditionally been to cluster the points
in   according to their nearest subspace without any supervised input  We turn this around and ask whether this
model is useful for active clustering  where we request  
very small number of intelligently selected labels    key observation when considering data wellmodeled by   union of
subspaces is that uncertain points will be ones lying equally
distant to multiple subspaces  Using   novel de nition of
margin tailored for the union of subspaces model  we incorporate this observation into an active subspace clustering

Leveraging Union of Subspace Structure to Improve Constrained Clustering

 Park et al    Many recent algorithms exist with both
strong theoretical guarantees and empirical performance 
and   full review of all approaches is beyond the scope
of this work  However  the core element of all recent algorithms lies in the formation of the af nity matrix  after
which spectral clustering is performed to obtain label estimates  In SSC  the af nity matrix is formed via   series
of  cid penalized regressions  LRR uses   similar cost function but penalizes the nuclear norm instead of the  cid  TSC
thresholds the spherical distance between points  and GSC
works by successively  greedily  building subspaces from
points likely to lie in the same subspace  Of these methods 
variants of SSC achieve the best overall performance on
benchmark datasets and has the strongest theoretical guarantees  which were introduced in  Elhamifar   Vidal   
and strengthened in numerous recent works  Soltanolkotabi
  Candes      Wang   Xu    Wang et al 
  While the development of ef cient algorithms with
stronger guarantees has received   great deal of attention 
very little attention has been paid to the question of what to
do about data that cannot be correctly clustered  Thus  when
reducing clustering error to zero  or near zero  is   priority 
users must look beyond unsupervised subspace clustering
algorithms to alternative methods  One such method is
to request some supervised input in the form of pairwise
constraints  leading to the study of pairwiseconstrained
clustering  PCC 
PCC algorithms work by incorporating mustlink and
cannotlink constraints between points  where points with
mustlink constraints are forced  or encouraged in the case
of spectral clustering  to be clustered together  and points
with cannotlink constraints are forced to be in separate
clusters  In many cases  these constraints can be provided
by   human labeler  For example  in  Biswas   Jacobs 
  the authors perform experiments where comparisons
between human faces are provided by users of Amazon
Mechanical Turk with an error rate of   Similarly  for
subspace clustering datasets such as Yale   and MNIST   
human could easily answer questions such as   Are these
two faces the same person  and  Are these two images
the same number  An early example of PCC is found
in  Wagstaff et al    where the authors modify the
Kmeans cost function to incorporate such constraints  In
 Basu et al    the authors utilize active methods to initialize Kmeans in an intelligent  EXPLORE  phase  during
which neighborhoods of mustlinked points are built up  After this phase  new points are queried against representatives
from each neighborhood until   mustlink is obtained   
similar explore phase is used in  Mallapragada et al   
after which   minmax approach is used to select the most
uncertain sample  Early work on constrained spectral clustering appears in  Xu et al    Wang   Davidson   
in which spectral clustering is improved by examining the

Figure   Example union of       subspaces of dimensions     
         and       

algorithm 
Our contributions are as follows  We introduce   novel algorithm for pairwise constrained clustering that leverages
UoS structure in the data    key step in our algorithm is
choosing points of minimum margin       those lying near
  decision boundary between subspaces  We de ne   notion of margin for the UoS model and provide theoretical
insight as to why points of minimum margin are likely to be
misclustered by unsupervised algorithms  We show through
extensive experimental results that when the data lie near
  union of subspaces  our method drastically outperforms
existing PCC algorithms  requiring far fewer queries to
achieve perfect clustering  Our datasets range in dimension
from   number of data points from   and
number of subspaces from   On ten MNIST digits
with   modest number of queries  we get   classi cation
error with only   pairwise queries compared to about  
error for current stateof theart PCC algorithms and  
for unsupervised algorithms  We also achieve   classi 
 cation error on the full Yale  COIL  and USPS datasets
with   small fraction of the number of queries needed by
competing algorithms  In datasets where we do not expect
subspace structure  our algorithm still achieves competitive
performance  Further  our algorithm is agnostic to the input subspace clustering algorithm and can therefore take
advantage of any future algorithmic advances for subspace
clustering 

  Related Work
  survey of recently developed subspace clustering algorithms can be found in  Vidal    and the textbook  Vidal et al   
In these and more recent work  clustering algorithms that employ spectral methods achieve
the best performance on most datasets  Notable examples of such algorithms include Sparse Subspace Clustering
 SSC   Elhamifar   Vidal    and its extensions  You
et al        LowRank Representation  LRR   Liu et al 
  Thresholded Subspace Clustering  TSC   Heckel  
  olcskei    and Greedy Subspace Clustering  GSC 

      Leveraging Union of Subspace Structure to Improve Constrained Clustering

eigenvectors of the af nity matrix in order to determine the
most informative points  However  these methods are limited to the case of two clusters and therefore impractical in
many cases 
More recently  the authors in  Xiong et al    Biswas
  Jacobs    improve constrained clustering by modeling which points will be most informative given the current clustering  with stateof theart results achieved on numerous datasets by the algorithm in  Xiong et al   
referred to as Uncertainty Reducing Active Spectral Clustering  URASC  URASC works by maintaining   set of
certain sets  whereby points in the same certain set are mustlinked and points in different certain sets are cannotlinked 
  test point xT is selected via an uncertaintyreduction
model motivated by matrix perturbation theory  after which
queries are presented in an intelligent manner until xT is
either matched with an existing certain set or placed in its
own new certain set  In practice  Xiong    the certain
sets are initialized using the EXPLORE algorithm of  Basu
et al   
While we are certainly not the  rst to consider actively selecting labels to improve clustering performance  to the best
of our knowledge we are the  rst to do so with structured
clusters  Structure within and between data clusters is often
leveraged for unsupervised clustering  Wright et al   
and that structure is also leveraged for adaptive sampling of
the structured signals themselves       see previous work
on sparse  Haupt et al    Indyk et al    structured
sparse  Soni   Haupt    and low rank signals  Krishnamurthy   Singh    This paper emphasizes the power
of that structure for reducing the number of required labels
in an active learning algorithm as opposed to reducing the
number of samples of the signal itself  and points to exciting open questions regarding the tradeoff between signal
measurements and query requirements in semisupervised
clustering 

  UoSBased PairwiseConstrained

Clustering

Recall that    cid xi   RD cid  

  

is   set of data points lying
on   union of   subspaces  Sk  
   each having dimension
   In this work  we assume all subspaces have the same
dimension  but it is possible to extend our algorithm to deal
with nonuniform dimensions  The goal is to cluster the data
points according to this generative model       assigning
each data point to its  unknown  subspace  In this section
we describe our algorithm  which actively selects pairwise
constraints in order to improve clustering accuracy  The key
step is choosing an informative query test point  which we
do using   novel notion of minimum subspace margin 
Denote the true clustering of   point       by      Let

  

 cid  

 cid     xi 

the output of   clustering algorithm  such as SSC  be an
af nity similarity matrix   and   set of label estimates
  These are the inputs to our algorithm  The
highlevel operation of our algorithm is as follows  To initialize  we build   set of certain sets   using an EXPLORElike
algorithm similar to that of  Basu et al    Certain sets
are in some sense equivalent to labels in that points within
  certain set belong to the same cluster and points across
certain sets belong to different clusters  Following this  the
following steps are repeated until   maximum number of
queries has been made 

  Spectral Clustering  Obtain label estimates via spec 

tral clustering 

  PCA on each cluster  Obtain   lowdimensional subspace estimate from points currently sharing the same
estimated cluster label 

  Select Test Point  Obtain   test point xT using subspace margin with respect to the just estimated subspaces 

  Assign xT to Certain Set  Query the human to compare the test point with representatives from certain
sets until   mustlink is found or all certain sets have
been queried  in which case the test point becomes its
own certain set 

  Impute Label Information  Certain sets are used to
impute mustlink and cannotlink values in the af nity
matrix 

We refer to our algorithm as SUPERPAC  SUbsPace clustERing with Pairwise Active Constraints    diagram of the
algorithm is given in Fig    and we outline each of these
steps below and provide pseudocode in Algorithm  

  Sample Selection via Margin

Minmargin points have been studied extensively in active
learning  intuitively  these are points that lie near the decision boundary of the current classi er  In  Settles   
the author notes that actively querying points of minimum
margin  as opposed to maximum entropy or minimum con 
 dence  is an appropriate choice for reducing classi cation error  In  Wang   Singh    the authors present  
marginbased binary classi cation algorithm that achieves
an optimal rate of convergence  within   logarithmic factor 
In this section  we de ne   novel notion of margin for
the UoS model and provide theoretical
insight as to
why points of minimum margin are likely to be misdist   Sk    miny Sk  cid       cid   cid cid     UkU  
    cid cid 
clustered 
For   subspace Sk with orthonormal basis Uk  let the distance of   point to that subspace be
  Let
     arg mink    dist   Sk  be the index of the closest
subspace  where                Then the subspace

Leveraging Union of Subspace Structure to Improve Constrained Clustering

Figure   Diagram of SUPERPAC algorithm for pairwise constrained clustering 

Then

   

and

     cid       
     cid          dist           
     cid          dist     

     cid       

 

         

with probability at least               where   is an

absolute constant 

Figure   Illustration of subspace margin  The blue and red lines
are the generative subspaces  with corresponding disjoint decision
regions  The yellowgreen color shows the region within some
margin of the decision boundary  given by the dotted lines 

margin of   point       is the ratio of closest and second
closest subspaces  de ned as

          max

  cid       

dist    Sk   
dist    Sj 

 

 

The point of minimum margin is then de ned as
arg minx       Note that the fraction is   value in    
where the   value of   implies that the point   is equidistant
to its two closest subspaces  This notion is illustrated in
Figure   where the yellowgreen color shows the region
within some margin of the decision boundary 
In the following theorem  we show that points lying near
the intersection of subspaces are included among those of
minimum margin with high probability  This method of
point selection is then motivated by the fact that the dif cult
points to cluster are those lying near the intersection of
subspaces   Further  theory for SSC   shows
that problematic points are those having large inner product
with some or all directions in other subspaces  Subspace
margin captures exactly this phenomenon 
Theorem   Consider two ddimensional subspaces    and
   Let            where        and          ID 
De ne

         

dist     
dist     

 

The proof is given in the supplementary material  Note that
if dist        dist      then           In this case 
Thm    states that under the given noise model  points with
small residual to the incorrect subspace       points near the
intersection of subspaces  will have small margin  These are
exactly the points for which supervised label information
will be most bene cial 
The statement of Thm    allows us to quantify exactly how
near   point must be to the intersection of two subspaces to
be considered   point of minimum margin  Let        
       be the   principal angles  between    and    If
the subspaces are very far apart   
   sin    is near
 
  and if they are very close  
   sin    is near zero 
 
Note that  for any       

 cid  

 cid  

sin    dist        sin     

that is  there are bounds on dist      depending on the
relationship of the two subspaces  We also know that if   is
drawn using isotropic Gaussian weights from   basis for   
then

  cid dist     cid   

 
 

  cid 

  

sin     

Given this  we might imagine that margin of the noisy points
is   useful indicator of points near the intersection in   scei  sin    is not 
nario where sin  is small but  
 
 See  Golub   Loan    for   de nition of principal angles 

 cid  

 Select Test PointxT   argminx    Form Unsupervised Affinity MatrixAAssign xT to Certain Set  actively query human xTZ     NoNoYesImpute Label InformationSpectral ClusteringRun PCA on Individual Clusters Leveraging Union of Subspace Structure to Improve Constrained Clustering

     when the subspaces have an intersection but are distant
in other directions  With this in mind we state the following
corollary  whose proof can be found in the supplementary
material 
Corollary   Suppose         is such that
  cid 

 cid 

 cid 

sin   

 

dist        sin     

 
 

  

for some small       that is     is close to the intersection
of    and    Let    be   random point in    generated as
         where    is   basis for    and          
  Id 
We observe yi   xi   ni  where ni                
If there exists       such that

   

 
   

 
 

and

 

 cid 

sin   

 
 

 cid 
         

  cid 

  

 

 
 

sin     

 

that is  the average angle is suf ciently larger than the
smallest angle  then

                       

   

ds          

   

     

where     is de ned as in Thm      is an absolute constant 
and      
 

   sin   

 cid  

  cid             cid 

We make some remarks  rst to connect our results to other
subspace distances that are often used  Perhaps the most
intuitive form of subspace distance between that spanned by
   and    is  
    if the two subspaces
are the same  the projection onto the orthogonal complement is zero  if they are orthogonal  we get the norm of   
alone  giving   distance of   This is equal to the more visuF   another common distance 
ally symmetric      
Further we note that  by the de nition of principal angles
 Golub   Loan   

    cid 

  cid    

 
  cid    

    cid 

       

 
 

   

cos     

 
 

sin     

  cid 

  

  cid 

  

From Equation   we see that the size of   determines
how close         is to    if          is as close to   
as possible  For example  if       the two subspaces
intersect  and       implies that              Equation
  captures the gap between average principal angle and
the smallest principal angle  We conclude that if this gap
is large enough and   is small enough so that    is close to
   then the observed    will have smaller margin than the
average point in    even when observed with noise 

Algorithm   SUPERPAC

Input                     xN  data     number of
clusters     subspace dimension     af nity matrix  maxQueries  maximum number of pairwise comparisons
Estimate Labels       SPECTRALCLUSTERING     
Initialize Certain Sets  Initialize           Znc 
and numQueries via UOSEXPLORE in supplementary
material 
while numQueries   maxQueries do

PCA on Each Cluster  Solve

Sk   min
  RD  

      xi  

 cid xi       cid xi cid   

 cid 

Obtain Test Point  select xT   arg minx      
Assign xT to Certain Set 

Sort       Znc  in order of most likely mustlink  via subspace residual for xT   query xT against
representatives from Zk until mustlink constraint is
found or     nc  If no mustlink constraint is found 
set           Znc xT  and increment nc 
Impute Constraints  Set Aij   Aji     for  xi  xj 
in the same certain set and Aij   Aji     for  xi  xj 
in different certain sets  do not impute for points absent
from certain sets 
Estimate Labels 
ING     

     SPECTRALCLUSTER 

end while

For another perspective  consider that in the noiseless case 
for            the condition dist        dist     
is enough to guarantee that    lies nearer to    Under the
given additive noise model  yi   xi   ni for         the
gap between dist      and dist      must be larger
by some factor depending on the noise level  After two
applications of Thm    and rearranging terms  we have that
          with high probability if
 dist        dist                   
where                 value near   for small
  Equation   shows that the gap dist       
dist      must grow  approximately linearly  with the
noise level   The relationship of this gap to the subspace
distances is quanti ed by Corollary   plugging sin 
from Equation   into Equation   and rearranging yields
  statement of the form in Equation  

  Pairwise Constrained Clustering with SUPERPAC

We now describe SUPERPAC in more detail  our algorithm
for PCC when data lie near   union of subspaces  given in
Algorithm   The algorithm begins by initializing   set of
disjoint certain sets  an optional process described in the

Leveraging Union of Subspace Structure to Improve Constrained Clustering

Figure   Misclassi cation rate for Yale   and MNIST datasets with many pairwise comparisons  Leftto right  Yale          input
from SSC  Yale          input from SSC  MNIST        input from TSC  MNIST        input from TSC 

 cid cid 

  xT

 cid 
 cid 
residual cid cid xT   UkU  
                   xk 

supplementary material due to space constraints  Next our
algorithm assigns the points most likely to be misclassi ed
to certain sets by presenting   series of pairwise comparisons  Finally  we impute values onto the af nity matrix for
all points in the certain sets and perform spectral clustering 
The process is then repeated until the maximum number of
pairwise comparisons has been reached 
Let xT be the test point chosen as the minmargin point 
Our goal is to assign xT to   certain set using as the fewest
number of queries possible  For each certain set Zk  the
representative xk is chosen as the maximummargin point
within the set  Next  for each    we let Uk be the ddimensional PCA estimate of the matrix whose columns
are the points
  We then query
our test point xT against the representatives xk in order of
 smallest  rst  If   mustlink
constraint is found  we place xT in the corresponding certain set  Otherwise  we place xT in its own certain set and
update the number of certain sets  Pseudocode for the complete algorithm is given in Algorithm   As   technical note 
we  rst normalize the input af nity matrix   so that the
maximum value is   For mustlink constraints  we impute
  value of   in the af nity matrix  while for cannotlink
constraints we impute     The approach of imputing values
in the af nity matrix is common in the literature but does
not strictly enforce the constraints  Further  we found in
our experiments that imputing the maximum value in the
af nity matrix resulted in unstable results  Thus  users must
be careful to not only choose the correct constraints as noted
in  Basu et al    but to incorporate these constraints in
  way that allows for robust clustering 
SUPERPAC can be thought of as an extension of ideas from
PCC literature  Basu et al    Biswas   Jacobs   
Xiong et al    to leverage prior knowledge about the
underlying geometry of the data  For datasets such as Yale  
and MNIST  the strong subspace structure makes Euclidean
distance   poor proxy for similarity between points in the
same cluster  leading to the superior performance of our
algorithm demonstrated in the following sections  This

structure does not exist in all datasets  in which case we
do not expect our algorithm to outperform current PCC
algorithms  The reader will note we made   choice to order
the certain sets according to the UoS model  this is similar
to the choice in  Xiong et al    to query according to
similarity  where our notion of similarity here is based on
subspace distances  We found this resulted in signi cant
performance bene ts  matching our intuition that points are
clustered based on their nearest subspace  In contrast to
 Biswas   Jacobs    Xiong et al    where the test
point is chosen according to   global improvement metric 
we choose test points according to their classi cation margin 
In our experiments  we found subspace margin to be   strong
indicator of which points are misclassi ed  meaning that our
algorithm rapidly corrects the errors that occur as   result of
unsupervised subspace clustering 
Finally  note that the use of certain sets relies on the assumption that the pairwise queries are answered correctly an
assumption that is common in the literature  Basu et al 
  Mallapragada et al    Xiong et al    However  in  Xiong et al    the authors demonstrate that
an algorithm based on certain sets still yields signi cant improvements under   small error rate  The study of robustly
incorporating noisy pairwise comparisons is an interesting
topic for further study 

  Experimental Results
We compare the performance of our method and the nonparametric version of the URASC algorithm  URASCN 
over   variety of datasets  Note that while numerous PCC
algorithms exist  URASC achieves both the best empirical results and computational complexity on   variety of
datasets  We also compared with the methods from  Basu
et al    and  Biswas   Jacobs    but found both
to perform signi canly worse than URASC on all datasets
considered  with   far greater computational cost in the case

 In our experiments  the parametric version of URASC was
found to be numerically unstable and did not have signi cantly
different performance from URASCN in the best cases 

 Yale       Yale       MNIST       MNIST       number of pairwise comparisonsmissclassification  SUPERPACURASCNRandomOracle UoSLeveraging Union of Subspace Structure to Improve Constrained Clustering

Dataset

Yale

MNIST
COIL 
COIL 

USPS

 

 
 

 
 
 

 

 

 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

Table   Datasets used for experiments with relevant parameters 
   total number of samples     number of clusters     ambient
dimension     estimated subspace dimension 

of size       of each of   objects  The COIL  dataset
 Nene et al      contains   objects  distinct from the
COIL  objects  of the same size and with the same number of images of each object  For both datasets  we use
subspace dimension       Finally  we apply our algorithm
to the USPS dataset provided by  Cai et al    which
contains   total images of handwritten digits   of size
      with roughly even label distribution  We again use
subspace dimension      

Input Subspace Clustering Algorithms   major
strength of our algorithm is that it is agnostic to the initial
subspace clustering algorithm used to generate the input
af nity matrix  To demonstrate this fact  we apply our
algorithm with an input af nity matrix obtained from  
variety of subspace clustering methods  summarized in
Table   Note that some recent algorithms are not included
in the simulations here  However  the simulations show
that our algorithm works well with any initial clustering 
and hence we expect similar results as new algorithms are
developed 

Experimental Results Fig    shows the clustering error
versus the number of pairwise comparisons for the Yale and
MNIST datasets  The input af nity matrix is obtained by
running SSC for the Yale datset and by running TSC for
the MNIST dataset  The  gure clearly demonstrates the
bene ts of leveraging UoS structure in constrained clustering in all cases  SUPERPAC requires roughly half the
number of queries needed by URASC to achieve perfect
clustering  For the Yale dataset with       roughly  Kd
queries are required to surpass oracle performance  and for
      roughly  Kd queries are required  Note that for the
Yale dataset  the clustering error increases using URASC 
This is due to the previously mentioned fact that imputing
the wrong constraints can lead to worse clustering performance  For suf ciently many queries  the error decreases
as expected  Fig    shows the misclassi cation rate versus
number of points for all       subjects of the Yale databse 
with the input af nity matrix taken from SSCOMP  You
et al      We space out the markers for clearer plots 
In this case  URASC performs roughly the same as random
query selection  while SUPERPAC performs signi cantly

Figure   Misclassi cation rate versus number of pairwise comparisons for extended Yale face database   with       subjects 
Input af nity matrix is taken from SSCOMP 

of  Biswas   Jacobs    We use   maximum query
budget of    for UOSEXPLORE and EXPLORE  For completeness  we also compare to random constraints  in which
queries are chosen uniformly at random from the set of
unqueried pairs 
Finally  we compare against the oracle PCA classi er  which
we now de ne  Let Uk be the ddimensional PCA estimate
of the points whose true label           Then the oracle
label is  Co      arg mink   
  This allows us to quantitatively capture the idea that  because the
true classes are not perfectly lowrank  some points would
not be clustered with the lowrank approximation of their
own true cluster  In our experiments  we also compared
with oracle robust PCA  Candes et al    implemented
via the augmented Lagrange multiplier method  Lin et al 
  but did not  nd any improvement in classi cation
error 

 cid cid     UkU  
    cid cid 

Datasets We consider  ve datasets commonly used as
benchmarks in the subspace clustering literature  with  
summary of the datasets and their relevant parameters are
given in Table   The Yale   dataset consists of   images
of size       of each of   different subjects under
  variety of lighting conditions  For values of   less than
  we follow the methodology of  Zhang et al    and
perform clustering on   randomly selected subsets of size
   We choose       as is common in the literature  Elhamifar   Vidal    Heckel     olcskei    The MNIST
handwritten digit database test dataset consists of  
centered       pixel images of handwritten digits  
We follow   similar methodology to the previous section
and select   random subsets of size    using subspace
dimension       as in  Heckel     olcskei    The
COIL  dataset  Nene et al      consists of   images

 The validity of the UoS assumption for two of these datasets
is investigated in  Elhamifar   Vidal    Heckel     olcskei 
 

 number of pairwise comparisons missclassification  Yale       SUPERPACURASCNRandomOracle UoSLeveraging Union of Subspace Structure to Improve Constrained Clustering

Figure   Misclassi cation rate versus number of pairwise comparisons for COIL         and COIL         databases 
Input af nity matrix is taken from EnSC  Rightmost plot shows proposed smoothing heuristic 

Figure   Misclassi cation rate versus number of pairwise comparisons for USPS dataset with       digits    total samples 
Input af nity matrix is taken from EnSC  URASC did not complete
after   hours of run time 

Figure   Misclassi cation rate for Sonar dataset from  Xiong et al 
  where there is not reason to believe the clusters have subspace structure  We are still very competitive with stateof theart 

better 
Fig    demonstrates the continued superiority of our algorithm in the case where UoS structure exists  In the case
of COIL  the clustering is sometimes unstable  alternating between roughly   and   clustering error for both
active algorithms  This further demonstrates the observed
phenomenon that spectral clustering is sensitive to small
perturbations  To avoid this issue  we kept track of the
Ksubspaces cost function  see  Bradley   Mangasarian 
  and ensured the cost decreased at every iteration  We
refer to this added heuristic as SUPERPACS in the  gure 
The incorporation of this heuristic into our algorithm is  
topic for further study 
Fig    shows the resulting error on the USPS dataset  again
indicating the superiority of our method  Note that   is
large for this dataset  making spectral clustering computationally burdensome  Further  the computational complexity
of URASC is dependent on    As   result  URASC did not
complete   queries in   hours of run time when using
  cores  so we compare to the result after completing only
  queries  Finally  in Fig    we demonstrate that even
on data without natural subspace structure  SUPERPAC
performs competitively with URASC 

  Conclusion
We have presented   method of selecting and incorporating
pairwise constraints into subspace clustering that considers
the underlying geometric structure of the problem  The
union of subspaces model is often used in computer vision
applications where it is possible to request input from human labelers in the form of pairwise constraints  We showed
that labeling is often necessary for subspace classi ers to
achieve   clustering error near zero  additionally  these constraints can be chosen intelligently to improve the clustering
procedure overall and allow for perfect clustering with  
modest number of requests for human input 
Developing techniques for handling noisy query responses
will allow extension to undersampled or compressed data 
One may assume that compressed data would be harder to
distinguish  leading to noisier query responses  Finally  we
saw that for datasets with different types of cluster structure  the structure assumptions of each algorithm had direct
impact on performance  in the future we plan to additionally develop techniques for learning from unlabeled data
whether the union of subspace model or   standard clustering approach is more appropriate 

 COIL number of pairwise comparisons COIL COIL  Smoothingmissclassification  SUPERPACURASCNRandomSUPERPAC SOracle UoS number of pairwise comparisons missclassification  USPSSUPERPACURASCNRandomOracle UoS number of pairwise comparisons missclassification  SonarSUPERPACURASCNRandomLeveraging Union of Subspace Structure to Improve Constrained Clustering

Acknowledgements
This work was supported by NSF   GRFP
and US ARO Grant   NF 

Krishnamurthy  Akshay and Singh  Aarti  Lowrank matrix
and tensor completion via adaptive sampling  In Advances
in Neural Information Processing Systems  pp   
 

References
Basri     and Jacobs     Lambertian re ectance and linear
subspaces  IEEE TPAMI    February  

Lin  Zhouchen  Chen  Minming  Wu  Leqin  and Ma  Yi 
Linearized alternating direction method with adaptive
penalty for lowrank representation  In Advances in Neural Information Processing Systems   

Basu  Sugato  Banerjee  Arindam  and Mooney  Raymond   
Active semisupervision for pairwise constrained clustering  In Proc  SIAM Int  Conf  on Data Mining   

Biswas  Arjit and Jacobs  David  Active image clustering
with pairwise constraints from humans  International
Journal on Computer Vision     

Bradley  Paul    and Mangasarian  Olvi    kPlane cluster 

ing  Journal of Global Optimization     

Cai  Deng  He  Xiaofei  Han  Jiawei  and Huang  Thomas   
Graph regularized nonnegative matrix factorization for
data representation  IEEE Transactions on Pattern Analysis and Machine Intelligence     

Candes  Emmanuel    Li  Xiadong  Ma  Yi  and Wright 
John  Robust principal component analysis  Journal of
the ACM    May  

Davidson  Ian  Wagstaff  Kiri    and Basu  Sugato  Measuring constraintset utility for partitional clustering algorithms  In Proc  European Conf  on Machine Learning
and Prinicpals and Practice of Knowledge Discovery in
Databases   

Elhamifar  Ehsan and Vidal  Renee  Sparse subspace clustering  Algorithm  theory  and applications  IEEE Trans 
on Pattern Analysis and Machine Intelligence   
  November  

Golub  Gene and Loan  Charles Van  Matrix Computations 

Johns Hopkins University Press   

Haupt  Jarvis  Castro  Rui    and Nowak  Robert  Distilled
sensing  Adaptive sampling for sparse detection and estimation  IEEE Transactions on Information Theory   
   

Heckel  Reinhard and   olcskei  Helmut  Robust subspace
clustering via thresholding  IEEE Trans  Inf  Theory   
   

Indyk  Piotr  Price  Eric  and Woodruff  David    On the
power of adaptivity in sparse recovery  In Foundations
of Computer Science  FOCS    IEEE  nd Annual
Symposium on  pp    IEEE   

Liu  Guangcan  Lin  Zhouchen  and Yu  Yong  Robust
subspace segmentation by lowrank representation  In
Proceedings of the  th international conference on machine learning  ICML  pp     

Mallapragada  Pavan Kumar  Jin  Rong  and Jain  Anil   
Active query selection for semisupervised clustering  In
Proc  Int  Conf  on Pattern Recognition   

Nene        Nayar        and Murase     Columbia object
image library  COIL  Technical report  Columbia
University     

Nene        Nayar        and Murase     Columbia object
image library  COIL  Technical report  Columbia
University     

Oliver       Rosario     and Pentland         bayesian
computer vision system for modeling human interactions 
IEEE Transactions on Pattern Analysis and Machine Intelligence     

Park  Dohyung  Caramanis  Constantine  and Sanghavi  Sujay  Greedy subspace clustering  In Advances in Neural
Information Processing Systems  pp     

Settles  Burr  Active Learning  Morgan   Claypool   

Soltanolkotabi  Mahdi and Candes  Emmanuel      Geometric Analysis of Subspace Clustering with Outliers  The
Annals of Statistics     

Soltanolkotabi  Mahdi and Candes  Emmanuel    Robust
Subspace Clustering  The Annals of Statistics   
   

Soni  Akshay and Haupt  Jarvis  On the fundamental limits
of recovering tree sparse vectors from noisy linear measurements  IEEE Transactions on Information Theory 
   

Vidal  Rene  Sastry     Shankar  and Ma  Yi  Generalized
Principal Component Analysis  SpringerVerlag   

Vidal  Renee  Subspace clustering  IEEE Signal Processing

Magazine    March  

Leveraging Union of Subspace Structure to Improve Constrained Clustering

Wagstaff  Kiri  Cardie  Claire  Rogers  Seth  and Schroedl 
Stefan  Constrained Kmeans clustering with background
knowledge  In Proc  Int  Conf  on Machine Learning 
 

Wang  Xiang and Davidson  Ian  Active spectral clustering 

In Proc   th Int  Conf  on Data Mining   

Wang     and Singh     Noiseadaptive marginbased active
learning for multidimensional data and lower bounds
In Proc  AAAI Conference on
under tsybakov noise 
Arti cial Intellgence   

Wang  Yining  Wang  YuXiang  and Singh  Aarti  Graph
connectivity in noisy sparse subspace clustering  In Proceedings of The  th International Conference on Arti 
cial Intelligence and Statistics   

Wang  YuXiang and Xu  Huan  Noisy sparse subspace
In Proceedings of The  th International

clustering 
Conference on Machine Learning  pp     

Wright  John  Yang  Allen    Ganesh  Arvind  Sastry 
  Shankar  and Ma  Yi  Robust face recognition via
sparse representation  IEEE transactions on pattern analysis and machine intelligence     

Xiong  Caiming    personal correspondence 

Xiong  Caiming  Johnson  David    and Corso  Jason   
Active clustering with modelbased uncertainty reduction  IEEE Trans  Pattern Anal  Mach  Intelligence   
Accepted for publication 

Xu  Qianjun  desJardins  Marie  and Wagstaff  Kiri    Active constrained clustering by examining spectral eigenvectors  In Proc   th Int  Conf  on Discovery Science 
 

You  Chong  Li  ChunGuang  Robinson  Daniel    and
Vidal  Rene  Oracle based active set algorithm for scalable elastic net subspace clustering  In Proc  IEEE International Conference on Computer Vision and Pattern
Recognition     

You  Chong  Robinson  Daniel    and Vidal  Rene  Scalable sparse subspace clustering by orthogonal matching
pursuit  In Proc  IEEE International Conference on Computer Vision and Pattern Recognition     

Zhang  Teng  Szlam  Arthur  Wang  Yi  and Lerman  Gilad 
Hybrid linear modeling via local best    ats  International Journal of Computer Vision     

