Random Fourier Features for Kernel Ridge Regression 

Approximation Bounds and Statistical Guarantees

Haim Avron   Michael Kapralov   Cameron Musco  

Christopher Musco   Ameya Velingker   Amir Zandieh  

Abstract

Random Fourier features is one of the most popular techniques for scaling up kernel methods 
such as kernel ridge regression  However  despite impressive empirical results  the statistical
properties of random Fourier features are still not
well understood  In this paper we take steps toward  lling this gap  Speci cally  we approach
random Fourier features from   spectral matrix
approximation point of view  give tight bounds
on the number of Fourier features required to
achieve   spectral approximation  and show how
spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression 

  Introduction
Kernel methods constitute   powerful paradigm for devising nonparametric modeling techniques for   wide range
of problems in machine learning  One of the most elementary is Kernel Ridge Regression  KRR  Given training data
                xn  yn         where    Rd is an input
domain and      is an output domain    positive de nite
kernel function             and   regularization parameter     the response for   given input   is estimated
as 

  xj     

        

nXj 

where              is the solution of the equation

      In      

 

 Equal contribution  School of Mathematical Sciences  Tel
Aviv University  Israel  School of Computer and Communication Sciences  EPFL  Switzerland  Computer Science and
Arti cial Intelligence Laboratory  MIT  USA  Correspondence
to  Haim Avron  haimav post tau ac il  Michael Kapralov
 michael kapralov ep ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

In the above      Rn   is the kernel matrix or Gram
matrix de ned by Kij     xi  xj  and           yn   is
the vector of responses  The KRR estimator can be derived
by minimizing   regularized square loss objective function
over   hypothesis space de ned by the reproducing kernel
Hilbert space associated with    however  the details
are not important for this paper 
While simple  KRR is   powerful technique that is well
understood statistically and capable of achieving impressive empirical results  Nevertheless  the method has  
key weakness  computing the KRR estimator can be prohibitively expensive for large datasets  Solving   generally requires     time and     memory  Thus  the design of scalable methods for KRR  and other kernel based
methods  has been the focus of intensive research in recent
years  Zhang et al    Alaoui   Mahoney    Musco
  Musco    Avron et al   
One of the most popular approaches to scaling up kernel
based methods is random Fourier features sampling  originally proposed by Rahimi   Recht   For shiftinvariant kernels      
the Gaussian kernel  Rahimi  
Recht   presented   distribution   on functions from
  to Cs    is   parameter  such that for every        Rd

                     

The idea is to sample   from   and use           
      as   surrogate kernel  The resulting approximate KRR estimator can be computed in   ns  time and
  ns  memory  see   for details  giving substantial
computational savings if       
This approach naturally raises the question  how large
should   be to ensure   high quality estimator  Or  using
the exact KRR estimator as   natural baseline  how large
should   be for the random Fourier features estimator to be
almost as good as the exact KRR estimator  Answering
this question can help us determine when random Fourier
features can be useful  whether the method needs to be improved  and how to go about improving it 
The original random Fourier features analysis  Rahimi
  Recht    bounds the pointwise distance between

Random Fourier Features for Kernel Ridge Regression

   and      for other approaches for analyzing random Fourier features  see   However  the bounds do
not naturally lead to an answer to the aforementioned question  In contrast  spectral approximation bounds on the entire kernel matrix       of the form

        In          In             In     

naturally have statistical and algorithmic implications  Indeed  in   we show that when   holds we can bound the
excess risk introduced by the random Fourier features estimator when compared to the KRR estimator  We also show
that       In can be used as an effective preconditioner for
the solution of   This motivates the study of how large  
should be as   function of   for   to hold 
In this paper we rigorously analyze the relation between
the number of random Fourier features and the spectral approximation bound   Our main results are the following 
  We give an upper bound on the number of random features needed to achieve    Theorem   This bound  in
conjunction with the results in   positively shows that
random Fourier features can give guarantees for KRR
under reasonable assumptions 

tight for the Gaussian kernel  Theorem  

  We give   lower bound showing that our upper bound is
  We show that the upper bound can be improved dramatically by modifying the sampling distribution used
in classical random Fourier features   Our sampling
distribution is based on an appropriately de ned leverage function of the kernel  closely related to socalled
leverage scores frequently encountered in the analysis of
sampling based methods for linear regression  Unfortunately  it is unclear how to ef ciently sample using the
leverage function 

  To address the lack of an ef cient way to sample using the leverage function  we propose   novel  easyto 
sample distribution for the Gaussian kernel which approximates the true leverage function distribution and allows random Fourier features to achieve   signi cantly
improved upper bound  Theorem   The bound has an
exponential dependence on the data dimension  so it is
only applicable to low dimensional datasets  Nevertheless  it demonstrate that classic random Fourier features
can be improved for spectral approximation and motivates further study  As an application  our improved
understanding of the leverage function yields   novel
asymptotic bound on the statistical dimension of Gaussian kernel matrices over bounded datasets  which may
be of independent interest  Corollary  

  Preliminaries
  Setup and Notation
The complex conjugate of       is denoted by    For  
vector   or   matrix       or    denotes the Hermitian
transpose  The       identity matrix is denoted Il  We use
the convention that vectors are columnvectors 
  Hermitian matrix   is positive semide nite  PSD  if
  Ax     for every vector    It is positive de nite  PD  if
  Ax     for every vector       For any two Hermitian
matrices   and   of the same size        means that
      is PSD 
We use          Rd       to denote the space of
complexvalued squareintegrable functions with respect to
some measure        is   Hilbert space equipped
with the inner product

hf  giL      ZRd
  ZRd

       

           

In the above     is the density associated with  
We denote the training set by                 xn  yn   
Rd      Note that   denotes the number of
    
training examples  and   their dimension  We denote the
kernel  which is   function from     to    by    We
denote the kernel matrix by    with Kij     xi  xj 
The associated reproducing kernel Hilbert space  RKHS 
is denoted by Hk  and the associated inner product by
 Hk  Some results are stated for the Gaussian kernel
  for some bandwidth pak         exp kx   zk 
rameter  
We use        to denote the ridge regularization parameter  While for brevity we omit the   subscript  the
choice of regularization parameter generally depends on   
Typically         and           See Caponnetto
  De Vito   and Bach   for discussion on the
asymptotic behavior of     noting that in our notation    is
scaled by an   factor as compared to those works  As the
ratio between   and   will be an important quantity in our
bounds  we denote it as        
The statistical dimension or effective degrees of freedom is
denoted by        Tr      In   

  Random Fourier Features
  CLASSICAL RANDOM FOURIER FEATURES
Random Fourier features  Rahimi   Recht    is an
approach to scaling up kernel methods for shiftinvariant
kernels    shiftinvariant kernel is   kernel of the form
                   where    is   positive de nite func 

Random Fourier Features for Kernel Ridge Regression

tion  we abuse notation by using   to denote both the kernel
and the de ning positive de nite function 
The underlying observation behind random Fourier features is   simple consequence of Bochner   Theorem  for
every shiftinvariant kernel for which        there is  
probability measure     and   corresponding probability
density function pk  both on Rd  such that

          ZRd
  ZRd

              

          pk     

 

In other words  the inverse Fourier transform of the kernel
   is   probability density function  pk  For simplicity
we typically drop the   subscript  writing         and
     pk  with the associated kernel function clear from
context 
If              are drawn according to    and we de ne
       ps      
     then it is not

            

hard to see that

                    

The idea of the Random Fourier features method is then to
de ne

                  

 
 

sXl 

     

       

 

as   substitute kernel 
Now suppose that     Cn   is the matrix whose jth row
is  xj  and let      ZZ     is the kernel matrix corresponding to     The resulting random Fourier features
KRR estimator is         Pn
   xj      where   is the
solution of         In         Typically        and we can
represent      more ef ciently as 

  

             

where

            Is    

We can compute   in   ns  time  making random
Fourier features computationally attractive if       

  MODIFIED RANDOM FOURIER FEATURES
While it seems to be   natural choice  there is no fundamental reason that we must sample the frequencies             
using the Fourier transform density function    In fact 
our results show that it is advantageous to use   different
sampling distribution based on the kernel leverage function
 de ned later 

Let    be any probability density function whose support
includes that of    If we sample              using   
and de ne
    

            

ps     

     

     

     

  

    

 

we still have                    We refer to this
method as modi ed random Fourier features and remark
that it can be viewed as   form of importance sampling 

  ADDITIONAL NOTATIONS AND IDENTITIES
Now that we have de ned  modi ed  random Fourier features  we can introduce some additional notation and identities that shall prove useful in the rest of the paper 
The        entry of   is given by

Zjl  

 
ps

  ixT

   lpp       

Let     Rd   Cn be de ned by

        ixT

     

 

Note that column   of   from the previous section is exactly

    pp            So we have 

ZZ   

        

 
 

sXl 

    
    

Finally  by   we have    ZZ      since

   ZRd

        ZRd

          

  Related Work
Rahimi   Recht    original analysis of random
Fourier features bounded the pointwise distance between
   and       In followup work  they give learning
rate bounds for   broad class of estimators using random
Fourier features  However  their results do not apply to
classic KRR  Rahimi   Recht    Their main bound
becomes relevant only when the number of sampled features is on order of the training set size 
Rudi et al    prove generalization properties for KRR
with random features  under somewhat dif cult to verify
technical assumptions  some of which can be seen as constraining the leverage function distribution that we study 
They leave open improving their bounds via   more re 
 ned sampling approach  Bach   analyzes random
Fourier features from   function approximation point of
view  He de nes   similar leverage function distribution
to the one that we consider  but leaves open establishing

Random Fourier Features for Kernel Ridge Regression

bounds on and effectively sampling from this distribution 
both of which we address in this work  Finally  Tropp
  analyzes the distance between the kernel matrix and
its approximation in terms of the spectral norm  kK   Kk 
which can be   signi cantly weaker error metric than  
Outside of work on random Fourier features  risk in ation bounds for approximate KRR and leverage score sampling have been used to analyze and improve the Nystr om
method for kernel approximation  Bach    Alaoui  
Mahoney    Rudi et al    Musco   Musco   
We apply   number of techniques from this line of work 
Spectral approximation bounds  such as   are quite popular in the sketching literature  see Woodruff   Most
closely related to our work is analysis of spectral approximation bounds without regularization             for the
polynomial kernel  Avron et al    Improved bounds
with regularization  still for the polynomial kernel  were
recently proved by Avron et al   

  Spectral Bounds and Statistical Guarantees
Given   feature transformation  like random Fourier features  how do we analyze it and relate its use to nonapproximate methods    common approach  taken for
example in the original paper on random Fourier features  Rahimi   Recht    is to bound the difference
between the true kernel    and the approximate kernel
    However  it is unclear how such bounds translate
to downstream guarantees on statistical learning methods 
such as KRR  In this paper we advocate and focus on spectral approximation bounds on the regularized kernel matrix 
speci cally  bounds of the form
   In    ZZ   In      In   
for some      
De nition   We say that   matrix   is    spectral approximation of another matrix    if             
       
Remark   When       bounds of the form of  
can be viewed as   lowdistortion subspace embedding
bounds 
Indeed  when       it follows from   that
Sp                 xn       can be embedded with
 distortion in Sp              xn    Rs 
The main mathematical question we seek to address in this
paper is  when using random Fourier features  how large
should   be in order to guarantee that ZZ     In is    
spectral approximation of      In  To motivate this question  in the following two subsections we show that such
bounds can be used to derive risk in ation bounds for approximate kernel ridge regression  We also show that such
bounds can be used to analyze the use of ZZ     In as  
preconditioner for      In 

While this paper focuses on KRR for conciseness  we remark that in the sketching literature  spectral approximation bounds also form the basis for analyzing sketching
based methods for tasks like lowrank approximation  kmeans and more  In the kernel setting  such bounds where
analyzed  without regularization  for the polynomial kernel  Avron et al    Cohen et al    recently
showed that   along with   trace condition on ZZ   which
holds for all sampling approaches we consider  yields   so
called  projectioncost preservation  condition for the kernel approximation  With   chosen appropriately  this condition ensures that ZZ  can be used in place of   for approximately solving kernel kmeans clustering and for certain versions of kernel PCA and kernel CCA  See Musco  
Musco   for details  where this analysis is carried out
for the Nystr om method 

  Risk Bounds
One way to analyze estimators is via risk bounds  several recent papers on approximate KRR employ such an
analysis  Bach    Alaoui   Mahoney    Musco  
Musco    In particular  these papers consider the  xed
design setting and seek to bound the expected insample
predication error of the KRR estimator     viewing it as an
empirical estimate of the statistical risk  More speci cally 
the underlying assumption is that yi satis es

yi      xi      

 
for some             The      are       noise terms 
distributed as normal variables with variance  
  The empirical risk of an estimator    which can be viewed as  
measure of the quality of the estimator  is

              

 

nXj 

    xi       xi 

 note that   itself might be   function of    
Let     Rn be the vector whose jth entry is    xj  It is
quite straightforward to show that for the KRR estimator   
we have  Bach    Alaoui   Mahoney   

                     In  

    

 Tr        In   

Since           In               In   and
Tr        In    Tr        In        
we de ne
bRK                  In       
and note that           bRK     The  rst term in the above
expressions for         and bRK     is frequently referred to

as the bias term  while the second is the variance term 

     

Random Fourier Features for Kernel Ridge Regression

Lemma   Suppose that   holds  and let     Rn be the
vector whose jth entry is    xj  Let    be the KRR estimator  and let    be KRR estimator obtained using some
other kernel     whose kernel matrix is     Suppose that
      In is    spectral approximation to      In for
some       and that kKk      The following bound
holds 

               bRK      

 

       

rank     

 

   
 
 

The proof appears in the supplementary material  Appendix   
In short  Lemma   bounds the risk of the approximate
KRR estimator as   function of both the risk upper bound

bRK       and an additive term which is small if the rank of
rank      and or   is small  In particular  it is instructive
 rank     
to compare the additive term    
to the variance term   
         Since approximation
   is only useful computationally if rank          we
should expect the additive term in   to also approach   an
generally be small when   is large 
Remark   An approximation    is only useful computationally if rank          so    gives   signi cantly compressed approximation to the original kernel matrix  Ideally we should have rank           as       and so
the additive term in   will also approach   and generally
be small when   is large 

  Random Features Preconditioning
Suppose we choose to solve       In      using
an iterative method       CG  In this case  we can apply ZZ     In as   preconditioner  Using standard analysis of Krylovsubspace iterative methods it is immediate that if ZZ     In is    spectral approximation of
     In then the number of iterations until convergence

is              Thus  if ZZ     In is  say   
 spectral approximation of   In  then the number of
iterations is bounded by   constant  The preconditioner can
be ef ciently applied  after preprocessing  via the Woodbury formula  giving cost per iteration  if        of     
The overall cost of computing the KRR estimator is therefore   ns       Thus  as long as          this approach
gives an advantage over direct methods which cost     
For small   it also beats nonpreconditioned iterative meth 

ods cost          We reach again the question that

was poised earlier  how big should   be so that ZZ     In
is    spectral approximation of      In 
See Cutajar et al    and Avron et al    for more
details and discussion on random features preconditioning 

  Ridge Leverage Function Sampling and

Random Fourier Features

In this section we present upper bounds on the number of random Fourier features needed to guarantee that
ZZ     In is    spectral approximation to      In  Our
bounds are applicable to any shiftinvariant kernel  and  
wide range of feature sampling distributions  and  in particular  for classical random Fourier features 
Our analysis is based on relating the sampling density to an
appropriately de ned ridge leverage function  This function is   continuous generalization of the popular leverage scores  Mahoney   Drineas    and ridge leverage
scores  Alaoui   Mahoney    Cohen et al    used
in the analysis of linear methods  Bach   de ned the
leverage function of the integral operator given by the kernel function and the data distribution  For our purposes   
more appropriate de nition is with respect to    xed input
dataset 
De nition   For given            xn and shiftinvariant kernel    de ne the ridge leverage function as
                   

In the above    is the kernel matrix and    is the distribution associated with   
Proposition  

                    

ZRd

          

The  simple  proof of the proposition is given in the supplementary material  Appendix   
Recall that we denote the ratio    which appears frequently in our analysis  by         As discussed  theoretical bounds generally set        as   function of   
so           However we remark that in practice  it may
frequently be the case that   is very small and        
Corollary   For any             
For any shiftinvariant kernel with             and
            as kx   zk            the Gaussian kernel  if we allow points to be arbitrarily spread out  the kernel matrix converges to the identity matrix  and   In   
        if       so the above bound is tight 
However  this requires datasets of increasingly large diameter  as   grows  In contrast  the usual assumption in statistical learning is that the data is sampled from   bounded
domain     In   we show via   leverage function upper
bound that for the important Gaussian kernel  for bounded
datasets we have            

Random Fourier Features for Kernel Ridge Regression

In the matrix sketching literature it is well known that spectral approximation bounds similar to   can be constructed
by sampling columns relative to upper bounds on the leverage scores  In the following  we generalize this for the case
of sampling Fourier features from   continuous domain 
Lemma   Let     Rd     be   measurable function such
that         for all     Rd  and furthermore assume
that

    ZRd

     

is  nite  Denote                Let       and
        Assume that kKk      Suppose we take
      ln      samples              from the
     
distribution associated with the density      and the construct the matrix   according to   with          Then
ZZ     In is  spectral approximation of      In with
probability of at least      
The proof is based on matrix concentration inequalities 
and appears in the supplementary material  Appendix   
Lemma   shows that if we could sample using the ridge
leverage function 
then        log      samples
suf ce for spectral approximation of    for    xed   and
failure probability  While there is no straightforward way
to perform this sampling  we can consider how well the
classic random Fourier features sampling distribution approximates the leverage function  obtaining   bound on its
performance  the proof is in Appendix   as well 
Theorem   Let       and         Assume that
kKk      If we use      
      ln      random Fourier features       sampled according to    then
ZZ     In is  spectral approximation of      In with
probability of at least      
Theorem   establishes that if      log    and   is  xed 
     random Fourier features suf ce for spectral approximation  and so the method can provably speed up KRR 
Nevertheless  the bound depends on    instead of     
as is possible with true leverage function sampling  see
Lemma   This gap arises from our use of the simple 
often loose  ridge leverage function upper bound given by
Proposition  
Unfortunately  as the next section shows  the bound in
Theorem   cannot be improved since the classic random
Fourier features sampling distribution can be far enough
from the ridge leverage distribution that     features
may be needed even when            

  Lower Bound
Our lower bound shows that the upper bound of Theorem
  on the number of samples required by classic random
Fourier features to obtain   spectral approximation to    

 In is essentially best possible  The full proof is given in
the supplementary material  Appendix   
Theorem   Consider the Gaussian kernel with    
   Suppose       is
   so         
an odd integer    satis es  
    and   satis es
       
  log           
  Then  there exists  
 plog   
dataset of   points  xj  
            such that if   random Fourier features       sampled according to    are
used for some       
  then with probability at least  
there exists   vector     Rn such that

 

        In   

   ZZ     In 

 

 
 

Furthermore  for the said dataset we have             
poly  log   

Thus  the number of samples   required for ZZ     In to
be    spectral approximation to      In for   bounded
dataset of points must either depend exponentially on the
radius of the point set  or at least linearly on    and there is
an asymptotic gap between what is achieved with classical
random Fourier features and what is achieved by modi ed
random Fourier features using leverage function sampling 
We note that the above lower bound is proven for   onedimensional point set  which makes it only stronger  even
at low dimensions  and for the common Gaussian kernel 
there is   large gap between the performance of classic random Fourier features and leverage function sampling 
The bound applies for datasets bounded on the range

       for      log     As we will see in  

the key idea behind the proof is to show that for such  
dataset  the ridge leverage function is large on   range of
low frequencies 
In contrast  the classic random Fourier
features distribution is very small at the edges of this frequency range  and so signi cantly undersamples some frequencies and does not achieve spectral approximation 
We remark that it would be preferable if Theorem   applied to bounded datasets       with    xed  as the usual
assumption in statistical learning theory is that data is sampled from   bounded domain  However  our current techniques are unable to address this scenario  Nevertheless 
our analysis allows   to grow very slowly with   and we
conjecture that the upper bound is tight even for bounded
domains 

  Improved Sampling  Gaussian Kernel 
Contrasting with the lower bound of Theorem   we now
give   modi ed Fourier feature sampling distribution that
does perform well for the Gaussian kernel on bounded input sets  Furthermore  unlike the true ridge leverage function  this distribution is simple and ef cient to sample from 

Random Fourier Features for Kernel Ridge Regression

To reduce clutter  we state the result for    xed bandwidth
      This is without loss of generality since we
can rescale the points and adjust the bounding interval 
Our modi ed distribution essentially corrects the classic
distribution by  capping  the probability of sampling low
frequencies near the origin  This allows it to allocate more
samples to higher frequencies  which are undersampled by
classical random Fourier features  For simplicity  we focus on the onedimensional setting  Our results extend to
higher dimensions  albeit with an exponential in the dimension loss 
De nition    Improved Fourier Feature Distribution for the
Gaussian Kernel  De ne the function

    

    

function  pR           

   plog   
        max      log    
Let       RR       and de ne the probability density
Note that  pR  is just the uniform distribution for low
 plog    and the classic
frequencies with  
Fourier features distribution  appropriately scaled  outside
this range  As we show in       upper bounds the true
ridge leverage function   for all   Hence  simply applying Lemma  
Theorem   For any integer   and parameter        
   
consider the one dimensional Gaussian kernel with    
   and any dataset of   points
   so         
            with any radius       If we sample
 xj  
       ln      random Fourier features acs    
cording to  pR  and construct   according to   then
with probability at least       ZZ     In is  spectral
approximation of   In for any       and        
Furthermore          Rplog      log     and  pR 
can be sampled from in    time 

Theorem   represents   possibly exponential improvement over the bound obtainable by classic random Fourier
features  For     log    our modi ed distribution requires   Rplog    samples  as compared to the lower
bound of   

  given by Theorem  

  Bounding the Ridge Leverage Function
We conclude by discussing our approach to bounding the
ridge leverage function of the Gaussian kernel  which leads
to Theorems   and   The key idea is to reformulate the
leverage function as the solution of two dual optimization
problems  By exhibiting suitable test functions for these
optimization problems  we are able to give both upper and
lower bounds on the ridge leverage function  and correspondingly on the sampling performance of classic and
modi ed Fourier feature sampling 

  PrimalDual Characterization
In this section we prove two alternative characterizations of
the ridge leverage function  one as   minimization  and the
other as   maximization  These characterization are useful
for bounding the leverage function  as we exhibit in the next
subsection for the Gaussian kernel 
De ne the operator            Cn by
      

 

    ZRd

The following two lemmas constitute the main result of this
subsection  The proofs can be found in the supplementary
material  Appendix   
Lemma   The ridge leverage function can alternatively
be de ned as follows 

      

    min

      pp     

    
 
Lemma   The ridge leverage function can alternatively
be de ned as follows 

    kyk 

    max
 Cn

    

        

            

 

 

Similar results are well known for the  nite dimensional
case  Here we extend these results to an in nite dimensional case  Lemma   allows us to upper bound the leverage function at any point     Rd by exhibiting   carefully constructed function    and upper bounding the ratio in   while Lemma   allows us to lower bound it in
  similar fashion 

  Leverage Function  the Gaussian Case
In this section we prove nearly matching bounds on the
leverage score function for the onedimensional Gaussian
kernel on bounded datasets  For simplicity of presentation
we focus on the onedimensional setting  Our results extend to higher dimensions  albeit with an exponential in the
dimension loss in the gap between upper and lower bounds 
Our bounds are parameterized by the width of the point
set  which we denote by    To reduce clutter  we present
all results for  xed       This is without loss of
generality since we can rescale the points  All the proofs
appear in the supplementary material  Appendices     
Theorem   Consider the one dimensional Gaussian kernel with       For any integer   and parameter
    and any radius       if      xn         
       
for every    plog   

      max      log      

Random Fourier Features for Kernel Ridge Regression

         

Theorem   Consider the one dimensional Gaussian kernel with       For any integer       any param 
  and every radius   log          
eter  
  there exist      xn          such that for
 
 plog   
every      plog     plog    we have 
     

 

      Rn 

   

  

 

The last two theorems lead to   tight bound on the statistical
dimension matrices corresponding to bounded points sets 
Corollary   Consider the Gaussian kernel with    
  For any integer   and parameter        
   
and any       if      xn          then we have 
           max      log    plog       
    Rplog      log    
Furthermore  if   log          
 plog   
exists   set of points            xn          such that 

there

 

      Rplog     

The bounds above match up to constant
  log             
   

they match up to   plog    factor 

factors if
  For any   log      

 plog   

 

 

  Theorems   and   Proof Outline
Lemma   allows us to bound   simply by exhibiting any    which makes the cost function small  One
simple attempt might be     
            where  
is the Dirac delta function  This choice zeros out the  rst
term  However the delta function is not square integrable 
    
       so the lemma cannot be used  Another
 
trivial attempt is        which zeros out the second
term and recovers the trivial bound          Nevertheless    smarter test functions    can yield improved
bounds  yielding results on the leverage score function that
are parameterized by the diameter of the point set 
At   high level  our approach is to replace the spike function at   with    soft spike  whose Fourier transform still
looks approximately like   cosine wave on        yet is
still square integrable  The smaller   is  the more spread
out this function will be able to be  and hence the smaller
its   norm  and the better the leverage score bound   
natural candidate for    soft spike  is   Gaussian of appropriate variance  but this choice does not suf ce to obtain
tight bounds  due to two dif culties  First  for the upper
bound   simple Gaussian does not result in   function that
is close enough to   pure frequency in time domain  rst

 
 
 
 
 
 
 

 

 
 
 
 
 
 
 

 

 

 

 

 

 

 

sinc and gaussian components
dampened test function with lower energy

smoothed approximation to target function
smoothed approximation to target frequency

 

 
 
 
 
 
 
 
 
 
 

 
 
 

 

 

 

 

frequency    

 
 

 
 

 
 

 

 
 

 
 

 
 

 
  

Figure    Soft spike  function   and its Fourier transform    
which is approximately   pure cosine wave on       

term of the objective function in Lemma   unless we settle for an upper bound of       poly     as opposed
to the tight      on the leverage score density function 
Second  the lower bound on the leverage score function
resulting from using   Gaussian pulse would only be of
the form    plog    leading to   weak lower bound
on the statistical dimension  namely     as opposed to
     plog    thereby missing entirely the effect of the

regularization parameter   on the statistical dimension 
The remedy to the issues above turns out to be the convolution of    modulated  Gaussian with   rectangular pulse
in time domain  product of   shifted Gaussian with the sinc
function in frequency domain  Speci cally  our bounds
are based on variants of    attened Gaussian spike function

                    sinc          

 

for some             and       
It turns out that with   proper setting of parameters  where
one should think of   as large      
the spike   is rather
narrow  the function        satis es

    

      dt 

                 exp            

    

 

 

 

 

 

    

    

    

haveR     

An illustration of this function in   is given in Fig     left 
and the function    in Fig     right  Note that if the parameter   is chosen to be large  then for   not too large we
      dt      
the second multiplier is essentially constant        at as  
function of    hence the term  attened Gaussian spike 
This means that        is essentially the kernel density
evaluated at   times   pure harmonic term exp      
which is exactly what one needs to minimize the  rst term

      dt      

on the rhs of   in Lemma   up to   factor ofpp   

see Appendix    One can also see that setting   to be not too
large results in   good function to use in the maximization
problem in   in Lemma     see Appendix    Obtaining tight bounds and in particular achieving the right dependence on plog    requires several modi cations to the
function   above  but the intuition we just described works 

Random Fourier Features for Kernel Ridge Regression

Acknowledgements
The authors thank Arturs Backurs helpful discussions at
early stages of this project  Haim Avron acknowledges
the support from the XDATA program of the Defense Advanced Research Projects Agency  DARPA  administered
through Air Force Research Laboratory contract FA 
    and an IBM Faculty Award  Cameron Musco
acknowledges the support by NSF Graduate Research Fellowship  AFOSR grant FA  and the NSF
Center for Science of Information 

References
Alaoui  Ahmed El and Mahoney  Michael    Fast randomized kernel ridge regression with statistical guarantees 
In Neural Information Processing Systems  NIPS   

Avron  Haim  Nguyen  Huy  and Woodruff  David  Subspace embeddings for the polynomial kernel  In Neural
Information Processing Systems  NIPS   

Avron  Haim  Clarkson  Kenneth    and Woodruff 
David    Faster kernel ridge regression using sketching and preconditioning  CoRR  abs   
URL http arxiv org abs 

Bach  Francis 

On the equivalence between kernel quadrature rules and random feature expansions 
Journal of Machine Learning Research   
 
URL http jmlr org papers   
 html 

Bach  Francis   

Sharp analysis of lowrank kernel
matrix approximations 
In Conference on Learning
Theory  COLT    URL http jmlr org 
proceedings papers   Bach html 

Optimal

Caponnetto     and De Vito    

rates
for the regularized leastsquares algorithm 
Foundations of Computational Mathematics   
   
 
   URL http dx doi org 
   

ISSN  

doi 

Cohen  Michael    Musco  Cameron  and Musco  ChristoInput sparsity time lowrank approximation via
pher 
ridge leverage score sampling 
In Proceedings of the
TwentyEighth Annual ACMSIAM Symposium on Discrete Algorithms  SODA   pp    Philadelphia  PA  USA    Society for Industrial and Applied Mathematics  URL http dl acm org 
citation cfm id 

Cutajar  Kurt  Osborne  Michael  Cunningham  John  and
Filippone  Maurizio  Preconditioning kernel matrices  In
International Conference on Machine Learning  ICML 

  URL http jmlr org proceedings 
papers   cutajar html 

Feller  William 

An introduction to probability theory and its applications  Volume   Wiley series in
probability and mathematical statistics  John Wiley  
sons  New York  Chichester  Brisbane   
ISBN
  URL http opac inria fr 
record   

Mahoney  Michael    and Drineas  Petros 
improved data

CUR
for
analymatrix decompositions
the National Academy of
sis 
Proceedings of
Sciences     
 
pnas  URL http www pnas org 
content abstract 

doi 

Musco  Cameron and Musco  Christopher  Recursive sampling for the Nystr om method  CoRR  abs 
 
URL http arxiv org abs 
 

Ogawa  Hidemitsu  An operator pseudoinversion lemma 
SIAM Journal on Applied Mathematics   
   
doi    URL http 
 dx doi org 

Rahimi     and Recht     Random features for largescale
kernel machines  In Neural Information Processing Systems  NIPS   

Rahimi  Ali and Recht  Benjamin  Weighted sums of random kitchen sinks  Replacing minimization with randomization in learning  In Neural Information Processing Systems  NIPS   

Rudi  Alessandro  Camoriano  Raffaello  and Rosasco 
Lorenzo  Less is more  Nystr om computational regularization  In Neural Information Processing Systems
 NIPS   

Rudi  Alessandro  Camoriano  Raffaello  and Rosasco 
Lorenzo  Generalization properties of learning with random features  ArXiv eprints  feb  

Tropp  Joel    An introduction to matrix concentration inequalities  Foundations and Trends in Machine
Learning     
ISSN   doi 
  URL http dx doi org 
 

Woodruff  David    Sketching as   tool for numerical linear
algebra  Found  Trends Theor  Comput  Sci   
  October   URL http dx doi org 
 

Zhang  Yuchen  Duchi  John  and Wainwright  Martin  Divide and conquer kernel ridge regression    distributed

Random Fourier Features for Kernel Ridge Regression

algorithm with minimax optimal rates     Mach  Learn 
Res    January  
ISSN  
 
URL http dl acm org citation 
cfm id 

