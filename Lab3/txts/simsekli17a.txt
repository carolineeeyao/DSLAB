Fractional Langevin Monte Carlo  Exploring   evy Driven Stochastic

Differential Equations for Markov Chain Monte Carlo

Umut   ims ekli  

Abstract

Along with the recent advances in scalable
Markov Chain Monte Carlo methods  sampling
techniques that are based on Langevin diffusions have started receiving increasing attention 
These so called Langevin Monte Carlo  LMC 
methods are based on diffusions driven by  
Brownian motion  which gives rise to Gaussian
proposal distributions in the resulting algorithms 
Even though these approaches have proven successful in many applications  their performance
can be limited by the lighttailed nature of the
Gaussian proposals 
In this study  we extend
classical LMC and develop   novel Fractional
LMC  FLMC  framework that is based on   family of heavytailed distributions  called  stable
  evy distributions  As opposed to classical approaches 
the proposed approach can possess
large jumps while targeting the correct distribution  which would be bene cial for ef cient exploration of the state space  We develop novel
computational methods that can scale up to largescale problems and we provide formal convergence analysis of the proposed scheme  Our experiments support our theory  FLMC can provide superior performance in multimodal settings  improved convergence rates  and robustness to algorithm parameters 

  Introduction
Markov Chain Monte Carlo  MCMC  techniques that are
based on continuous diffusions have become increasingly
popular due to their success in largescale Bayesian machine learning  In these techniques  the goal is to generate
samples from   target distribution   by forming   contin 

 LTCI    el ecom ParisTech  Universit   ParisSaclay 
  Paris  France  Correspondence to  Umut   ims ekli
 umut simsekli telecomparistech fr 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

uous diffusion which has   as   stationary distribution  In
practice    is usually known up to   normalization constant 
                 exp       for     RD  where
  is called the unnormalized target density and   is called
the potential energy function 
Originated in statistical physics  Rossky et al   
Langevin Monte Carlo  LMC  is constructed upon the
Langevin diffusion that is de ned by the following stochastic differential equation  SDE   Roberts   Stramer   

 

dXt       Xt dt  

 dBt 

 

where Bt denotes the standard Ddimensional Brownian
motion  Under certain regularity conditions on    the solution process  Xt    can be shown to be ergodic with
   Roberts   Stramer    which allows us to generate samples from   by simulating the continuoustime process   in discretetime  This approach paves the way
for the celebrated Unadjusted Langevin Algorithm  ULA 
 Roberts   Stramer    that is given as follows 

 Xn     Xn           Xn   cid   Bn 

 

where   denotes the iterations       is   sequence of
stepsizes  and  Bn   is   sequence of independent and
identicallydistributed        
standard Gaussian random
variables  Convergence properties of ULA have been studied in  Lamberton   Pages   
In   statistical physics context  Xt often represents the position of   particle  at time    that is under the in uence of  
random force  In this case  the Langevin equation   is motivated by the hypothesis that this random force is the sum
of many        random  pulses  whose variance is assumed
to be  nite  Yanovsky et al    Then  by the central
limit theorem  CLT  the sum of these pulses converges to
  Gaussian random variable  which justify the choice of the
Brownian motion in the Langevin equation  
  natural question arises if we relax the  nite variance assumption and allow the random pulses to have in nite variance  In such circumstances  the  usual  CLT would not
hold  however  one can still show that the sum of these
pulses converges to   broader class of heavytailed distributions called  stable  or   evystable  distributions    evy 

Fractional Langevin Monte Carlo

We support our theoretical results by several synthetic and
real experiments  Our experiments show that the proposed
approach forms   viable alternative to classical LMC with
additional bene ts  such as providing superior performance
in multimodal settings  higher convergence rates  and robustness to algorithm parameters  The proposed approach
also opens up several interesting future directions  as we
will point out in Section  

  Technical Background
Stable distributions  Stable distributions are heavytailed
distributions  They are the limiting distributions in the generalized central limit theorem  the properly scaled sum of
       random variables  which do not necessarily have  
nite variance  will converge to an  stable random variable
 Samorodnitsky   Taqqu    In this study  we are interested in the centered symmetric  stable       distribution  which is   special case of  stable distributions 
The     distribution can be seen as   heavytailed generalization of the centered Gaussian distribution  The probability density function  pdf  of an     distribution cannot be written in closedform except for certain special
cases  however  the characteristic function of the distribution can be written as follows            
  exp        exp  Here          is called
the characteristic exponent and determines the tail thickness of the distribution  as   gets smaller      becomes
heaviertailed  The parameter        is called the scale
parameter and measures the spread of   around  
As an important special case of      we obtain the Gaussian distribution              for       In Figure   we illustrate the  approximately computed  pdf of
the symmetric  stable distribution for different values of
  As can be clearly observed from the  gure  the tails of
the distribution vanish quickly when             Gaussian 
whereas the tails get thicker as we decrease  
An important property of the  stable distributions is that
their moments can only be de ned up to the order       
           if and only if       for         implying that   has in nite variance for    cid    Moreover 
even though the pdf of     does not admit an analytical
form  it is straightforward to draw random samples from
stable distributions  Chambers et al    where ef cient
implementations are readily available in public software libraries such as the GNU Scienti   Library  gnu org 
software gsl 
SDEs driven by symmetric stable   evy processes   In
this study  we are interested in SDEs driven by symmetric
 stable   evy processes  which are de ned as follows 

dXt     Xt   dt   dL 
   

 

Figure   Top  probability density functions of       Bottom 
illustration of  stable   evy motions 

  Since the law of the random force is nonGaussian
in this case  the Brownian motion would not be appropriate
in   and it needs to be replaced with the  stable   evy
motion  which will be described in Section  
As opposed to the Brownian motion  which is almost surely
continuous  the   evy motion can contain discontinuities
that are often referred to as  jumps  Due to these jumps 
the SDEs that are driven by   evy motions are also called
anomalous diffusions  It has been noticed that this heavytailed nature of the   evy processes can be more appropriate for modeling natural phenomena that might incur
large variations    situation often encountered in statistical physics  Eliazar   Klafter     nance  Mandelbrot 
  and signal processing  Kuruoglu   
Despite the fact that   evydriven SDEs have been studied
in more general Monte Carlo contexts       for  nancial
simulations   Konakov   Menozzi    Mikulevi cius  
Zhang    surprisingly  their use in MCMC has been
left widely unexplored 
In the statistical physics literature  Ditlevsen   considered     evydriven SDE with
  doublewell potential and investigated its waitingtimes 
In   similar context  Eliazar   Klafter   developed
an approximate technique based on Tauberian theorems for
targeting     evydriven system to   prespeci ed distribution  where they required the target distribution to be exactly evaluated  Whilst being relevant  the applicability and
the impact of these approaches are rather limited in the domain of machine learning 
In this study  we explore the use of   evydriven SDEs
within MCMC  Encouraged by earlier studies that illustrate
the bene ts of using heavytailed distributions in MCMC
      improved convergence rates   Stramer   Tweedie 
  Jarner   Roberts    we aim at investigating the
potential bene ts of the usage of the   evy motions in LMC 
in lieu of the classical Brownian motion  We extend classical LMC and develop   novel Fractional LMC framework 
which targets the correct distribution even in the presence
of the jumps induced by the   evy motion  We then develop novel computational methods that can scale up to
largescale problems and provide formal theoretical analysis of the convergence behavior of the proposed approach 

             Fractional Langevin Monte Carlo

where   is called the drift and is chosen as   function of
  in our context  and Xt  will be de ned in the sequel 
  denotes the Ddimensional  stable   evy motion
Here    
with independent components       each component of   
 
forms an independent scalar  stable   evy motion  which
is de ned as follows for          Duan   
   
 ii 

      almost surely 
  
For               tN   the increments    
  are independent                  
  
distribution             for       
  has stochastically continuous sample paths      
continuous in probability  for all       and      
    

            as       

 iii  The difference    

      

    and   

    have the same

      

tn 

 iv    

 

tn

 

Due to the stochastic continuity property   stable   evy
motions can have   countable number of discontinuities 
which are often referred to as  jumps  As illustrated in
Figure    bottom  the size of these jumps becomes larger
as   get smaller  since     becomes heavier tailed  As  
consequence  the sample paths of these processes are continuous from the right and they have left limits at every time
 Duan    Xt  hence denotes the left limit of Xt at
time    Therefore  these processes are called   adl ag       the
French acronym for  continue    droite  limite    gauche 
Similarly to the symmetric  stable distributions  the symmetric  stable   evy motions   
  coincide with   scaled
Brownian motion
 Bt when       This can be simply
      
veri ed by observing that the difference   
  follows
  Gaussian distribution            and   
  becomes almost surely continuous everywhere 
Riesz potentials and fractional differentiation 
Fractional calculus aims to generalize differentiation  and integration  to fractional orders  Herrmann    The canonical example of fractional differentiation can be given as
the halfderivative operator  which coincides with the  rstorder derivative when applied twice to any function 
In this study  we are interested in fractional Riesz derivatives  Riesz    which are closely related to  stable
distributions  The fractional Riesz derivative directly generalizes the secondorder differentiation to fractional orders and it is   nonlocal operator  In the one dimensional
case  it is de ned by the following identity 
         cid         

 
where   denotes the Fourier transform and       
        Here        is the order of the differentiation 
for         we obtain the Riesz potentials  which will
be our main source of interest  and for       we obtain the
 For          corresponds to fractional integration  However  we follow the fractional calculus literature and still refer to
it as fractional differentiation 

usual secondorder differentiation up to   sign difference 
                       dx  Note that         does not
coincide with  rstorder differentiation in general 

  Fractional Langevin Monte Carlo
In this section  we present our main results and construct
the proposed Fractional LMC framework step by step  We
 rst develop     evydriven SDE that targets the correct distribution and analyze the weak convergence properties of
its Euler discretization  Afterwards  we develop numerical
methods for approximate simulation of the proposed SDE
and present formal analysis of the approximation error of
the numerical schemes and the weak error analysis of the
corresponding Euler discretizations 
In the rest of this paper  we restrict   to be in     in order
the mean of the process to exist  Besides  in all our analyses we focus on the scalar case        for simplicity 
however  all our results can be extended for       All the
proofs are provided in the supplementary document 

  Invariant measure and weak convergence analysis

Our  rst goal is to  nd   drift   in such   way that the
Markov process  Xt    that is     adl ag solution of the
SDE in   would have the target distribution   as an invariant distribution  In the following theorem  we present
our  rst main result 
Theorem   Consider the SDE   where   is de ned as 

        cid cid       cid   

 
Here        cid     xU     and    is de ned in  
Then    is an invariant measure of the Markov process
 Xt    that is     adl ag solution of the SDE given in  
Furthermore  if   is Lipschitz continuous  then   is the
unique invariant measure of the process  Xt   

The Lipschitz continuity of  xU is   standard condition in
LMC for ensuring the uniqueness of the invariant measure 
albeit it is often violated in practical applications  In our
context  we need   to be Lipschitz continuous for uniqueness    condition which cannot be easily veri ed for    cid   
Here  it is also worth noting that when       we ob 
 
tain the classical Langevin diffusion   as lim    
   
 Bt and lim            xU    
Theorem   suggests that if we could generate continuous
sample paths from  Xt    then we could use them as
samples drawn from   However  this is not possible since
the drift   does not admit an analytical form in general  and
even if it could be computed exactly  we still could not simulate the SDE   exactly as it is   continuoustime process 
For now  let us assume that we can exactly compute the
drift   and focus on simulating the SDE by considering

Fractional Langevin Monte Carlo

its EulerMaruyama discretization  Duan    Panloup 
  which is given as follows 

 Xn     Xn          Xn       

    

  

 

ple averages  given as          cid   

where                 denotes the timesteps    is the total number of timesteps       iterations       is   sequence of stepsizes  and    
    is   sequence of       
   
standard symmetric  stable random variables          
     We can clearly observe that this discretization
schema is   fractional generalization of ULA given in  
where it coincides with ULA when      
The EulerMaruyama scheme in   lets us approximately
compute the expectation of   test function   under the tar 

get density             cid   cid      dX  by using samwhere HN    cid  

    ng   Xn 
       Even though the convergence
properties of the estimators obtained via ULA have been
wellestablished  Roberts   Stramer    Durmus  
Moulines    it is not clear whether the estimator       
converges to the true expectation     for    cid   
For the convergence analysis  we make use of relatively
recent results from the applied probability literature  Panloup    In order to establish the convergence of our
estimator  we need certain conditions to be satis ed  First 
we have   rather standard assumption on the stepsizes 
   limn        

limN  HN    

 cid  

HN

     the stepsizes are required to be decreasing and their
sum is required to diverge  Secondly  we need   more technical Lyapunov condition in order to ensure the stochastic
process to be meanreverting 
   Let           
  be   function in    satisfying
lim               xV      
  for some      
xV is bounded  There exists               and
and  
       such that       CV   and   xV              
where   is de ned in  

 

Under these conditions  we present the following corollary to Theorem   and  Panloup    Theorem   where
we establish the weak convergence of the EulerMaruyama
scheme de ned in  
Corollary   Assume that   is Lipschitz continuous and
the conditions    and    hold  If the test function    

  cid       cid  with         then the following holds 

               
lim

almost surely 

This corollary shows that under certain regularity and Lyapunov conditions  the EulerMaruyama scheme in   still
weakly converges for    cid    as long as the drift can be
computed exactly  Note that we consider the Lipschitz condition for ensuring the uniqueness of the invariant measure 

however  this is not   crucial assumption as one can show
that every weak limit of the sequence      is an invariant
probability for the SDE in  

  Numerical approximation

Even though Corollary   ensures the weak convergence of
the Euler scheme  its practical implication is somewhat limited since the Riesz derivatives cannot be computed exactly
in general  In this section  we develop and analyze numerical methods for approximately computing the drift   
In  Ortigueira    it has been shown that for    
    the Riesz derivative    of   function       can
be de ned as the limit of the fractional centered difference
operator  
hf     where

   given as            limh   
hf      cid     
 

and      cid           
       
  By using the above de nition  we can rewrite our drift

 cid 
     kf      kh 
    cid    where      is
as          cid limh   
           
        cid   

de ned in Theorem  
We now propose our  rst numerical scheme for approximating the drift   by following    elik   Duman  

          bh         cid cid 

 

 

 

where  
operator  de ned as follows 

    is the truncated fractional central difference

 cid  

  Kf      cid     
 

  kf      kh 

 

   

Here  we merely replaced the Riesz derivative with the central difference operator where we  xed   and truncated the
in nite summation in order the numerical scheme to be
computationally tractable  We provide   numerically stable implementation of   in the supplementary document 
The scheme in   provides us   practical way for approximately computing the drift  However  for  xed   and   
this approach would yield   certain approximation error
and therefore Corollary   would no longer hold if we replace   by    in   Throughout this section  we analyze
this approximation error and the weak error of the Euler
scheme with the approximate drift 
We  rst analyze the approximation error of our numerical
scheme in   Since     is constant for   given    we
focus on            
  Kf    Here  we  rst need  
technical regularity condition on   
               and all derivatives up to order three
belong to     
We need an additional assumption on    which ensures the
tails of the target distribution   vanish suf ciently quickly 

Fractional Langevin Monte Carlo

          kh      exp      for some       and
        for some       

Now  we present our second main result 
Theorem   Assume that the conditions    and    hold 
Then  for         the following bound holds 

 cid cid           

  Kf   cid cid      cid       hK cid 

 

as   goes to zero 

Theorem   shows that the error induced by our numerical
approximation scheme is bounded and can be made arbitrarily small by decreasing   and increasing    We can
also observe that for  xed    the optimal         
The hidden constant in the right hand side of   is allowed to depend on   and let it be denoted as      In
order to ease the analysis  in the rest of the paper we will
assume that supx            so that Theorem   can
be directly used for bounding the error          for any   
Note that this   mild assumption and holds trivially when
  belongs to   bounded domain       the setting in  Wang
et al   
We now consider the following EulerMaruyama discretization of   with the approximate drift 

 bh     Xn       

    

  

 bh           xU    

 HN  cid  

 Xn     Xn      
 
where the corresponding estimator is de ned as          cid 
    ng   Xn  Even in this approximate Eulerscheme  we still obtain ULA as   special case of   as
we have lim 
As opposed to               does not converge to    
due to the error induced by the numerical approximation 
However  fortunately  the weak error of this Euler scheme
can still be bounded  as we show in the following theorem 
For this result  we need an additional ergodicity condition 
   The SDE   and dXt    bh   Xt   dt   dL 
  are geometrically ergodic with their unique invariant measures 
Theorem   Assume that the conditions      and   hold 
   holds for     and   is chosen in such   way that    holds
for any    Finally  assume that  xg  is bounded  Then  the
following bound holds almost surely 

 cid cid      limN        cid cid      cid       hK cid 

 

This theorem shows that the weak error of the discretization
in   is dominated by the numerical error induced by   
and can be made arbitrarily small by tuning   and   

 Proving the ergodicity of the SDEs in    is beyond the scope
of this study  more information can be found in  Masuda   

  Multidimensional case

Even though we have focused on the scalar case          
  so far  we can generalize the presented results to vector
processes by using the same proof strategies since the comt are independent  For       the drift turns
ponents of   
out to be   multidimensional generalization of   and has
the following form   for                 

             

xd

   xd        

 

where      denotes the   th component of   vector   and
  
xd denotes the partial fractional Riesz derivative along
the direction xd  Ortigueira et al    With this de nition of the multidimensional drift  similar to the scalar case 
we obtain the classical Langevin equation as   special case
of   since lim                  for      
In applications  we can approximate   by applying the
same numerical technique presented in   to each dimension    However  for large    this approach would be impractical since it would require the fractional derivatives to
be computed   times at each iteration 
In this section  we propose   second scheme for approximating the fractional Riesz derivatives  The current approach is   computationally more ef cient variant of the
 rst numerical scheme presented in   and it is given as
follows                  where         
      for        In other words  we approximate
   
the fractional derivatives by using only the  rst term of the
centered difference operator de ned in   When all the
partial fractional derivatives in   are approximated with
this approach  the multidimensional drift greatly simpli es
and has the following form   for      

                  cid          

 
where     cid        We  nally consider   discretization of   where the drift is approximated by  
and ultimately propose the Fractional Langevin Algorithm
 FLA  de ned as follows 

 Xn     Xn             Xn     

    

  

 

Similar to the previous discretization schemes given in
  and   FLA generalizes ULA as well 
since
                 Besides  FLA has the exlim 
act same computational complexity as ULA  since it only
requires to compute    and generate    
   Another interesting observation is that    increases as   decreases 

 While extending our results to       the independence of
the components of   
  turns out to be   crucial requirement since
the spectral measure of the corresponding multivariate stable distribution becomes discrete  Nolan    Our results cannot be
directly extended to SDEs that are driven by other multivariate
stable processes  such as isotropic stable processes  Nolan   

Fractional Langevin Monte Carlo

implying that FLA tends to increase the  weight  of the gradient as the driving process becomes heaviertailed 
We now present our last theoretical result where we analyze
the approximation error of the simpli ed scheme for    
  and present it as   corollary to Theorems   and  
Corollary   Assume that the conditions    and   hold 
Let Kx      be   value that satis es    for   given   
and let            be   function de ned as 
  kf     kh 

 cid cid cid 
 cid cid cid      cid             Kx cid 
         cid cid      cid             Kx   cid 

      cid cid cid cid cid 
  cid Kx Kx cid 
 cid cid cid                
 cid cid      lim
and         cid   HN  cid  

   
Then  for           the following bound holds 

Furthermore  if    holds     and   hold for     and  xg  is
bounded  then  the following bound holds 

almost surely  where      arg maxx             Kx 

    ng   Xn 

      

 

Here  the term      plays   similar role as the parameter  
in   This corollary shows that the approximation quality of   may vary depending on the particular   where
       is evaluated  and depending on the values of Kx
and         might even provide more accurate approximations than    does  As   result  we observe that the weak
error is dominated by the largest numerical error induced
by     On the other hand  even if    would have   higher
approximation error when compared to     we would expect
that the scheme in   to be better behaved than   since
it is less prone to numerical instability 

  Largescale Bayesian posterior sampling

In Bayesian machine learning  the target distribution   is
often chosen as the Bayesian posterior               
where      Yi NY
   is   set of observed        data
points  This choice of the target distribution imposes
the following form on the potential energy         
   log   Yi      log      where   Yi    is the

 cid NY

likelihood function and      is the prior distribution 
In large scale applications  NY becomes very large and
therefore computing    at each iteration can be computationally inhibitive 
Inspired by the Stochastic Gradient
Langevin Dynamics  SGLD  algorithm  Welling   Teh 
  which extends ULA to largescale settings  we extend FLA by replacing the exact gradients    in   with
an unbiased estimator  given as follows 
   Un        log       

  log   Yi   

 cid 

NY
  

   

Figure   Top  the doublewell potential  Middle  the empirical
distribution obtained via ULA  corresponds to FLA with      
Bottom  the empirical distribution obtained via FLA      

where                NY   is   random data subsample that
is drawn with replacement at iteration   and     cid  NY
denotes the number of elements in   We call the resulting
algorithm Stochastic Gradient FLA  SGFLA  Note that
SGFLA coincides with SGLD when       We leave the
convergence analysis of SGFLA as   future work 

  Experiments
The doublewell potential  We conduct our  rst set of
experiments on   synthetic setting where we consider the
doublewell potential  de ned as follows 

                                 

We illustrate the doublewell potential in Figure    top 
It can be observed that the potential contains two wellseparated modes with different heights  which makes the
problem challenging 
In our  rst experiment  we consider our  rst discretization
scheme presented in   where we approximate the true
drift    by     Here  we use decreasing stepsizes that are
determined as              where we          
and        In each experiment  we generate      
samples by using   and estimate the mean of the target
distribution   by using the sample average  For each   we
run this scheme for different values of   and    repeat each
experiment   times  and monitor the bias  where the ground
truth is obtained via   numerical integrator 
We  rst          and monitor the bias for increasing
values of    Here  we de ne the notion of an optimal  
as the smallest    for whose larger values the performance
improvement becomes negligible  As we can observe from
Figure    top  the bias is gracefully degrading for increasing    where the optimal   depends on the choice of  
We also observe that  modest values of   seem suf cient
for obtaining accurate results  especially for small  

       Frequency   FrequencyFractional Langevin Monte Carlo

Table   Evaluation of the accuracy of    

                             

 

 

 

 

 

 

Figure   Evaluation of the scheme in   on the estimation of
the mean of the doublewell potential  Top  the average bias vs
  for    xed    Bottom  the average bias vs   for    xed   

Figure   The average bias obtained via FLA for different values
of   Note that FLA corresponds to ULA when      

In our second experiment  we          and monitor the
bias for different values of    The results are illustrated in
Figure    bottom  We observe that the results support our
theory  for very small values of    the term  hK  in the
bound of Theorem   dominates since   is  xed  Therefore  we observe   drop in the bias as we increase   up to
  certain point  and then the bias gradually increases along
with    The results show that the performance becomes
more sensitive to the value of    as   becomes smaller 
Even though the results in Figure   are promising  in practical applications we would not be able to use the scheme
in   due to computational issues  In our next experiment 
we aim to assess the approximation error of our second approximation scheme    given in   However  the error
              cannot be measured in   straightforward
manner  since   cannot be computed exactly and the error
itself depends on the particular point   
Here  we develop an intuitive accuracy criterion for getting
better insight into this error  where the aim is to compute
the value of   for which  bh        and         would
yield similar approximation errors on average  For   given
  and  xed    we  rst choose   large enough    cid      
and compute   cid       cid   bh   cid        as our reference for
imation error            cid   bh            cid      and
the error induced by the ultimate approximation scheme 
         cid              cid      We then  nd the value
of   for which           and         are the closest 
        arg minK                      We  nally
evaluate   on   different points  xi  
   and use the average of these values as the measure of accuracy of     de ned
    xi    Intuitively  this value is

       Then  for    cid     cid cid  we compute the approx 

as         cid  

expected to be large when    yields   low error 
In order to assess the accuracy of    in the doublewell problem  we compute   for different values of   where we
            cid      and choose  xi   as      
evenlyspaced points from the interval     The results
are given in Table   The results show that  despite its sim 

plicity     is able to provide reasonably accurate estimates
for    We observe that for         becomes  
which is even larger than the optimal   for       as
shown in Figure   Therefore  it is promising to use    in real
applications since it yields suf ciently accurate approximations with less computational requirements and does not require additional tuning for   and   
In our last experiment on the doublewell potential  we
evaluate the ultimately proposed approach FLA on estimation of the mean of the target distribution  Similarly to the
previous experiments  we run FLA for different values of
  where we try several values for the hyperparameters   
and    and report the best results for each   In each experiment  we generate       samples by using  
and repeat the procedure   times  We  rst illustrate two
typical empirical distributions obtained via FLA and ULA
in Figure    middle  bottom  It can be clearly observed
that ULA can locate only one of the modes  whereas FLA
is able to locate both of the modes  thanks to the jumps
of the  stable processes  This circumstance also re ects
in the average bias  as illustrated in Figure   The results
show that for       the average bias is around   implying
that the algorithm concentrates on either one of the modes
at each trial  whereas we observe that the bias rapidly decreases as we decrease   The best performance is achieved
when       Finally we note that these results are also
in line with the bestperforming results given in Figure  
Matrix factorization 
In our second set of experiments  we switch to   largescale Bayesian machine learning context  We explore the use of SGFLA on   largescale link prediction application where we consider the following probabilistic matrix factorization model  Gemulla
et al    Salakhutdinov   Mnih    Ail  
where     RI   is the observed data matrix with possible missing entries  and     RI   and     RL   are
the latent factor matrices  The aim in this application is to
predict the missing values of   by using   lowrank approximation  Recently  SGLD has been proven successful
on similar models  Ahn et al      ims ekli et al   

      Blj         Yij          cid cid 

  AilBlj   cid 

   Bias   Bias BiasFLAFractional Langevin Monte Carlo

Figure   The performance of SGFLA on   link prediction application  Top  ML    middle  ML    bottom  ML   

Durmus et al      ims ekli et al   
In this set of experiments  we apply SGFLA on the three
MovieLens movie ratings datasets  grouplens org 
MovieLens  Million  ML     Million  ML    and
 Million  ML    The ML   dataset contains   million nonzero entries  where        movies  and
       users  The ML   dataset contains   million nonzero entries  where       and      
Finally  the ML   dataset contains   million ratings 
where       and       In our experiments 
we randomly select   of the data as the test set and use
the remaining data for generating the samples  The rank
  is set to   for all datasets  In all experiments  we use
decreasing stepsizes  where we           and try
several values for    and report the best results  We set
     NY   where NY denotes the number of nonzero
entries in   given dataset 
Figure   shows the root mean squarederrors  RMSE  that
are obtained on the three test sets  In all these experiments 
we observe that the rate of convergence of SGFLA increases as we decrease   from         SGLD  to   In
the case when       the jumps induced by the stableL evy motion becomes very large and the performance starts
degrading  These results show that SGFLA can be considered as   viable alternative to SGLD in large scale settings
and it can provide improved performance over SGLD via
minor algorithmic modi cations  which come with the expense of tuning an additional parameter  
Sigmoid Belief Networks  In our last set of experiments 
we investigate the use of SGFLA on Sigmoid Belief Networks  SBN   Gan et al    which have been investigated in recent Stochastic Gradient MCMC studies  Chen
et al    We make use of the software provided in
 Chen et al    and employ the identical experimental
setup described therein  the binary observed data are assumed to be generated from   single binary hidden layer
with sigmoid activations  The overall model is applied on

Figure   The test loglikelihoods obtained via SGFLA on SBNs 
as   function of stepsize  

the MNIST dataset  which contains    binary images  of
size       corresponding to different digits 
In our experiments  we use an SBN with   hidden units 
We use   training set of    images and    images for
testing  set the size of the data subsample        and
run SGFLA for   iterations for training  Finally  we estimate the test likelihoods by using an annealed importance
sampler  Salakhutdinov   Murray   
As opposed to our previous experiments  we use constant
stepsizes in these experiments              for all    and
investigate the performance of SGFLA on SBNs for different values of   and   The results are illustrated in Figure   We can observe that for small values of   SGFLA
yields similar test likelihoods for all values of   However 
as we increase the step size  we observe that the test likelihood of SGLD       starts to diverge  whereas SGFLA
becomes more robust to large step sizes as   gets smaller 
When       the test likelihood stays almost constant for
increasing values of   We do not observe an improvement
in the performance for      

  Conclusion and Future Directions
In this study  we explored the use of   evydriven SDEs
within MCMC and presented   novel FLMC framework 
We  rst showed that FLMC targets the correct distribution
and then developed novel and scalable computational methods for practical applications  We provided formal analysis
of the convergence properties and the approximation quality of the proposed numerical schemes  We supported our
theory with several experiments  which showed that FLMC
brings various bene ts  such as providing superior performance in multimodal settings  higher convergence rates 
and robustness to algorithm parameters 
The proposed framework opens up several interesting future directions 
    the use of FLMC in simulated annealing for global optimization  Chen et al    where
the jumps might bring further advantages  ii  extension of
FLMC to  stablelike  processes  Bass    where   can
depend on    iii  incorporation of the local geometry for
faster convergence  Patterson   Teh    Li et al   
  ims ekli et al       iv  the use of SGFLA in Bayesian
model selection    ims ekli et al     

 Iterations    Test RMSE Iterations    Test RMSE Iterations    Test RMSE Stepsize Test Loglikelihood Fractional Langevin Monte Carlo

Acknowledgments
The author would like to thank to Alain Durmus for his
helps on the proofs  and to Roland Badeau     Taylan
Cemgil  and Ga el Richard for fruitful discussions  The author would also like to thank to Changyou Chen for sharing
the code used in the experiments conducted on SBNs  This
work is partly supported by the French National Research
Agency  ANR  as   part of the FBIMATRIX project  ANR 
 CE  and the EDISON    project  ANR 
CORD 

References
Ahn     Korattikara     Liu     Rajan     and Welling 
   Largescale distributed Bayesian matrix factorization
using stochastic gradient MCMC  In KDD   

Duan     An Introduction to Stochastic Dynamics  Cam 

bridge University Press  New York   

Durmus     and Moulines     Nonasymptotic convergence
analysis for the unadjusted Langevin algorithm  arXiv
preprint arXiv   

Durmus       ims ekli     Moulines     Badeau     and
Richard     Stochastic gradient RichardsonRomberg
Markov Chain Monte Carlo  In NIPS   

Eliazar     and Klafter       evydriven Langevin systems 
Targeted stochasticity  Journal of statistical physics   
   

Gan     Henao     Carlson        and Carin     Learning
deep sigmoid belief networks with data augmentation  In
AISTATS  volume   pp     

Bass        Uniqueness in law for pure jump Markov processes  Probability Theory and Related Fields   
   

Gemulla     Nijkamp        Haas     and Sismanis 
Largescale matrix factorization with distributed

  
stochastic gradient descent  In ACM SIGKDD   

   elik     and Duman     Crank Nicolson method for the
fractional diffusion equation with the Riesz fractional
derivative  Journal of Computational Physics   
   

Chambers        Mallows        and Stuck         
method for simulating stable random variables  Journal of the american statistical association   
   

Chen     Ding     and Carin     On the convergence of
stochastic gradient MCMC algorithms with highorder
integrators  In Advances in Neural Information Processing Systems  pp     

Chen     Carlson     Gan     Li     and Carin    
Bridging the gap between stochastic gradient MCMC
and stochastic optimization  In AISTATS   

  ims ekli     Badeau     Cemgil        and Richard 
   Stochastic quasiNewton Langevin Monte Carlo  In
ICML     

  ims ekli     Badeau     Richard     and Cemgil 
     
ef 
cient Bayesian model selection via stochastic gradient
MCMC  In ICASSP     

Stochastic thermodynamic integration 

  ims ekli     Durmus     Badeau     Richard    
Moulines     and Cemgil        Parallelized stochastic gradient Markov Chain Monte Carlo algorithms for
nonnegative matrix factorization  In ICASSP   

Ditlevsen        Anomalous jumping in   doublewell po 

tential  Physical Review       

Herrmann     Fractional calculus  an introduction for

physicists  World Scienti     

Jarner        and Roberts        Convergence of heavytailed Monte Carlo Markov Chain algorithms  Scandinavian Journal of Statistics     

Konakov     and Menozzi     Weak error for stable driven
stochastic differential equations  Expansion of the densities  Journal of Theoretical Probability   
 

Kuruoglu        Signal processing in  stable noise environments    least lpnorm approach  PhD thesis  University of Cambridge   

Lamberton     and Pages     Recursive computation of the
invariant distribution of   diffusion  the case of   weakly
mean reverting drift  Stochastics and dynamics   
   

  evy     Th eorie de   addition des variables al eatoires 

GauthiersVillars  Paris   

Li     Chen     Carlson     and Carin     Preconditioned
stochastic gradient Langevin dynamics for deep neural
networks  In AAAI Conference on Arti cial Intelligence 
 

Mandelbrot       

Fractals and Scaling in Finance 
Discontinuity  Concentration  Risk  Selecta Volume   
Springer Science   Business Media   

Masuda     Ergodicity and exponential  mixing bounds
for multidimensional diffusions with jumps  Stochastic
processes and their applications     

Fractional Langevin Monte Carlo

Mikulevi cius     and Zhang     On the rate of convergence
of weak Euler approximation for nondegenerate SDEs
driven by   evy processes  Stochastic Processes and their
Applications     

Stramer     and Tweedie        Langevintype models ii  selftargeting candidates for MCMC algorithms 
Methodology and Computing in Applied Probability   
   

Wang        Fienberg        and Smola        Privacy for
free  Posterior sampling and stochastic gradient Monte
Carlo  In ICML  pp     

Welling     and Teh        Bayesian learning via stochastic gradient Langevin dynamics  In International Conference on Machine Learning  pp     

Yanovsky        Chechkin        Schertzer     and Tur 
        evy anomalous diffusion and fractional Fokker 
Planck equation  Physica    Statistical Mechanics and
its Applications     

Nolan        An overview of multivariate stable distribu 

tions  Technical Report   

Nolan        Multivariate elliptically contoured stable distributions  theory and estimation  Computational Statistics     

Ortigueira        Riesz potential operators and inverses
via fractional centred derivatives  International Journal
of Mathematics and Mathematical Sciences     

Ortigueira        LalegKirati        and Machado    
      Riesz potential versus fractional Laplacian  Journal of Statistical Mechanics     

Panloup     Recursive computation of the invariant measure
of   stochastic differential equation driven by     evy process  The Annals of Applied Probability   
 

Patterson     and Teh        Stochastic gradient Riemannian Langevin dynamics on the probability simplex 
In Advances in Neural Information Processing Systems 
 

Riesz       int egrale de RiemannLiouville et le probl eme

de Cauchy  Acta mathematica     

Roberts        and Stramer     Langevin Diffusions
and MetropolisHastings Algorithms  Methodology and
Computing in Applied Probability    December   ISSN  

Rossky        Doll        and Friedman        Brownian
dynamics as smart Monte Carlo simulation  The Journal
of Chemical Physics     

Salakhutdinov     and Mnih     Bayesian probabilistic matrix factorization using Markov Chain Monte Carlo  In
ICML  pp     

Salakhutdinov     and Murray     On the quantitative analIn ICML  pp   

ysis of deep belief networks 
 

Samorodnitsky     and Taqqu        Stable nonGaussian
random processes  stochastic models with in nite variance  volume   CRC press   

  ims ekli     Koptagel       uldas     Cemgil       
 Oztoprak     and Birbil         Parallel stochastic gradient Markov Chain Monte Carlo for matrix factorisation
models  arXiv preprint arXiv   

