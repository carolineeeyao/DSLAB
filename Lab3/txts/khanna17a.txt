On Approximation Guarantees for Greedy Low Rank Optimization

Rajiv Khanna   Ethan    Elenberg   Alexandros    Dimakis   Joydeep Ghosh   Sahand Negahban  

Abstract

We provide new approximation guarantees for
greedy low rank matrix estimation under standard
assumptions of restricted strong convexity and
smoothness  Our novel analysis also uncovers
previously unknown connections between the low
rank estimation and combinatorial optimization 
so much so that our bounds are reminiscent of corresponding approximation bounds in submodular
maximization  Additionally  we also provide statistical recovery guarantees  Finally  we present
empirical comparison of greedy estimation with
established baselines on two important realworld
problems 

  Introduction
Low rank matrix optimization stands as   major tool in
modern dimensionality reduction and unsupervised learning  The singular value decomposition can be used when
the optimization objective is rotationally invariant to the
parameters  However  if we wish to optimize over more
complex  nonconvex objectives we must choose to either
rely on convex relaxations  Recht et al    Negahban
  Wainwright    Rohde   Tsybakov    or directly
optimize over the nonconvex space  Park et al    Jain
et al    Chen   Wainwright    Lee   Bresler   
Jain et al   
More concretely  in the low rank matrix optimization problem  we wish to solve

arg max

 

 

     rank      

 

Rather than perform the computationally intractable optimization above researchers have studied convex relaxations
of the form

arg max

 

     nuc 

 UT Austin  Yale University  Correspondence to  Rajiv Khanna

 rajivak utexas edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

Unfortunately  the above optimization can be computationally taxing  General purpose solvers for the above optimization problem that rely on semide nite programming  SDP 
require       computation  which is prohibitive  Gradient descent techniques require          computational cost for an epsilon accurate solution  This improvement is sizable in comparison to SDP solvers  Unfortunately 
it is still prohibitive for large scale matrix estimation 
An alternate vein of research has focused on directly optimizing the nonconvex problem   To that end  authors
have seen recent theoretical success in studying the convergence properties of

arg max

  Rn     Rd  

 UVT  

Solving the problem above automatically forces the solution
to be low rank  and recent results have shown promising
behavior 
Another approach is to optimize   incrementally via rank
one updates to the current estimate  ShalevShwartz et al 
  Wang et al    This approach has also been studied in more general contexts such as boosting  Buhlmann
  Yu    coordinate descent  Jaggi    Jaggi  
Sulovsk      and incremental atomic norm optimization  Gribonval   Vandergheynst    Barron et al   
Khanna et al    Rao et al    Dudik et al   
Locatello et al   

  Set Function Optimization and Coordinate

Descent

In this paper  we interpret low rank matrix estimation as
  set optimization problem over an in nite set of atoms 
Speci cally  we wish to optimize

arg max

   Xk  

   kXi 

 iXi   

where the set of atoms   is the set of all rank one matrices
with unit operator norm  This settings is analogous to that
taken in the results studying atomic norm optimization  coordinate descent via the total variation norm  and FrankWolfe
style algorithms for atomic optimization  This formulation
allows us to connect the problem of low rank matrix estimation to that of submodular set function optimization  which

On Approximation Guarantees for Greedy Low Rank Optimization

we discuss in the sequel  Before proceeding we discuss
related work and an informal statement of our main result 

  Informal Result and Related Work
Our result demonstrates an exponential decrease in the
amount of error incurred by greedily adding rank one matrices to the low rank matrix approximation 
Theorem    Approximation Guarantee  Informal  If we let
   be our estimate of the rank   matrix   at iteration   
then for some universal constant   related to the restricted
condition number of the problem we have

              exp ck       

Note that after   iterations the matrix    will be at most
rank   

Related work  There has been   wide array of studies
looking at the computational and statistical bene ts of rank
one updates to estimating   low rank matrix  At its most
basic  the singular value decomposition will keep adding
rank one approximations through de ation steps  This
work can be generally segmented into two sets of results
  the ones that present sublinear rates of convergence and
those that obtain linear rates  Interestingly  parallel lines of
work have also demonstrated similar convergence bounds
for more general atomic or dictionary element approximations  Buhlmann   Yu    Gribonval   Vandergheynst 
  Barron et al    Khanna et al    For space
constraints  we will summarize these results into two categories rather than explicitly state the results for each individual paper 
If we de ne the atomic norm of   matrix     Rn   written
as    nuc to be the sum of the singular values of that
matrix  then the bounds establishing sublinear convergence
behave as

           

 

nuc

 

where we take   to be the best rank   solution  What we
then see is convergence towards the optimal bound  However  we expect our statistical error to behave as          
where   is the number of samples that we have received
from our statistical model and   is rank    Negahban  
Wainwright    Rohde   Tsybakov    We can take
 nuc      which would then imply that we would need
  to behave as          However  that would then imply
that the rank of our matrix should grow linearly in the number of observations in order to achieve the same statistical
error bounds  The above error bounds are  fast  If we
consider   model that yields slow error bounds  then we
    In that case 
     which looks better  but

expect the error to behave like  nucq    
we can take      nucq  

still requires signi cant growth in   as   function of    To
overcome the above points  some authors have aimed to
study similar greedy algorithms that then enjoy exponential
rates of convergence as we show in our paper  These results
share the most similarities with our own and behave as

             

This result decays exponentially  However  when one looks
at the behavior of   it will typically act as exp  min     
for an       matrix  As   result  we would need to choose  
of the order of the dimensionality of the problem in order to
begin to see gains  In contrast  for our result listed above  if
we seek to only compare to the best rank   solution  then the
gamma we  nd is     exp     Of course  if we wish
to  nd   solution with full rank  then the bounds we stated
above match the existing bounds 
In order to establish our results we rely on   notion introduced in the statistical community called restricted strong
convexity  This assumption has connections to ideas such as
the restricted isometry property  restricted eigenvalue condition  and incoherence  Negahban   Wainwright    In
the work by ShalevShwartz  Gonen  and Shamir    
form of strong convexity condition is imposed over matrices 
Under that setting  the authors demonstrate that

         

  

 

 

where   is the rank of   In contrast  our bound behaves as

                exp      

Our contributions  We improve upon the linear rates of
convergence for low rank approximation using rank one
updates by connecting the coordinate descent problem to
that of submodular optimization  We present this result in
the sequel along with the algorithmic consequences  We
demonstrate the good performance of these rank one updates
in the experimental section 

  Background
We begin by  xing some notation  We represent sets using
sans script fonts            Vectors are represented using
lower case bold letters            and matrices are represented using upper case bold letters            Nonbold
face letters are used for scalars              and function
names          The transpose of   vector or   matrix is
represented by           De ne                      For
singleton sets  we write               Size of   set   is
denoted by         is used for matrix inner product 
Our goal is to analyze greedy algorithms for low rank estimation  Consider the classic greedy algorithm that picks up

On Approximation Guarantees for Greedy Low Rank Optimization

the next element myopically      given the solution set built
so far  the algorithm picks the next element as the one which
maximizes the gain obtained by adding the said element into
the solution set  Approximation guarantees for the greedy
algorithm readily imply for the class of functions de ned as
follows 
De nition     set function               is submodular
if for all           

                                     

Submodular set functions are well studied and have many
desirable properties that allow for ef cient minimization and
maximization with approximation guarantees  Our low rank
estimation problem also falls under the purview of another
class of functions called monotone functions    function is
called monotone if and only if               for all       
For the speci   case of maximizing monotone submodular
set functions  it is known that the greedy algorithm run for
  iterations is guaranteed to return   solution that is within
        of the optimum set of size    Nemhauser et al 
  Without further assumptions or knowledge of the
function  no other polynomial time algorithm can provide  
better approximation guarantee unless   NP  Feige   
More recently  the aforementioned greedy approximation
guarantee has been extended to   larger class of functions
called weakly submodular functions  Elenberg et al   
Khanna et al    Central to the notion of weak submodularity is   quantity called the submodularity ratio 
De nition    Submodularity Ratio  Das   Kempe   
Let            be two disjoint sets  and                The
submodularity ratio of   with respect to   is given by

       Pj                     

                 

 

 

The submodularity ratio of   set   with respect to an integer
  is given by

       min

        
       

     

 

It is easy to show that     is submodular if and only if
         for all sets   and    However  an approximation guarantee is obtained when                 Das  
Kempe    Elenberg et al    The subset of monotone functions which have                 are called
weakly submodular functions in the sense that even though
the function is not submodular  it still provides   provable
bound for greedy selections 
Also vital to our analysis is the notion of restricted strong
concavity and smoothness  Negahban et al    Loh  
Wainwright   

De nition    Low Rank Restricted Strong Concavity  RSC 
Restricted Smoothness  RSM    function     Rn      
is said to be restricted strong concave with parameter   
and restricted smooth with parameter    if for all       
    Rn   
 

                hr        Xi

  
  kY   Xk 

  
  kY   Xk 
   

   

Remark   If   function   has restricted strong concavity
parameter    then its negative   has restricted strong
convexity parameter    We choose to use the nomenclature
of concavity for ease of exposition in terms of relationship
to submodular maximization  Further  note that we de ne
RSC RSM conditions on the space of matrices rather than
vectors  on   domain   constrained by rank rather than
sparsity  It is straightforward to see that if       then
        and        
  Setup
In this section  we delineate our setup of low rank estimation 
In order to connect to the weak submodular maximization
framework more easily  we operate in the setting of maximization of   concave matrix variate function under   low
rank constraint  This is equivalent to minimizing   convex
matrix variate function under the low rank constraint as considered by ShalevShwartz et al    or under nuclear
norm constraint or regularization as considered by Jaggi
  Sulovsk     The goal is to maximize   function
    Rn       

 

max

rank    

   

Instead of using   convex relaxation of   our approach
is to enforce the rank constraint directly by adding rank  
matrices greedily until   is of rank    The rank   matrices
to be added are obtained as outer product of vectors from the
given vector sets   and    While our results hold for general
vector sets     assuming an oracle access to subroutines
GreedySel and OMPSel  to be detailed later  for the rest
of the paper we focus on the case of norm   balls         
Rn     kxk      and          Rd     kxk     
The problem   can be interpreted in the context of sparsity assuming   and   are enumerable  For example  by
the SVD theorem  it is known that we can rewrite   as
Pk
    iuiv     where     ui    and vi      By enumerating   and   under    nite precision representation
of real values  one can rethink of the optimization   as
 nding   sparse solution for the in nite dimensional vector
   ShalevShwartz et al    Dudik et al    We
can also optimize over support sets  similar to the classical
setting of support selection for sparse vectors  For   speci ed support set   consisting of vectors from   and    let

On Approximation Guarantees for Greedy Low Rank Optimization

UL and VL be the matrices formed by stacking the chosen
elements of   and   respectively  We de ne the following
set function to maximize   given   

        max

        

     HVL     

 

We will denote the optimizing matrix for   support set   as
     In other words  letting  HL be the argmax obtained
 HLVL  Thus  the low rank matrix
in   then           
estimation problem   can be reinterpreted as the following
equivalent combinatorial optimization problem 

     

max
    

 

  Algorithms
Our greedy algorithm  illustrated in Algorithm   builds the
support set incrementally   adding rank   matrices one at
  time such that at iteration   for           the size of the
chosen support set  and hence rank of the current iterate  is
   We assume access to   subroutine GreedySel for the
greedy selection  Step   This subroutine solves an inner
optimization problem by calling   subroutine GreedySel
which returns an atom   from the candidate support set that
ensures

   SG

             SG

where

         SG

             SG

    

     arg max
       SG
  

   SG

             SG

  

In words  the subroutine GreedySel ensures that the gain
in     obtained by using the selected atom is within    
    multiplicative approximation to the atom with the
best possible gain in     The hyperparameter   governs
  tradeoff allowing   compromise in myopic gain for  
possibly quicker selection 
The greedy selection requires  tting and scoring every candidate support  which is often prohibitively expensive  An
alternative is to choose the next atom by using the linear
maximization oracle used by FrankWolfe  Jaggi    or
Matching Pursuit algorithms  Gribonval   Vandergheynst 
  Locatello et al    This step replaces Step   of
Algorithm   as illustrated in Algorithm   Let     SO
  
be the set constructed by the algorithm at iteration       
The linear oracle OMPSel returns an atom   for iteration  
ensuring
hr      usv        

  hr      uv   
The linear problem OMPSel can be considerably faster that
GreedySel  OMPSel reduces to  nding the left and right

          SO

max

singular vectors of        corresponding to its largest
singular value  If   is the number of nonzero entries in
       then this takes     
We note that Algorithm   is the same as considered
by ShalevShwartz et al    as GECO  Greedy Ef 
cient Component Optimization  However  as we shall see 
our analysis provides stronger bounds than their Theorem  

   log     log    time 

Algorithm   GREEDY           
  Input  vector sets       sparsity parameter    subroutine hyperparameter  
  SG
     
  for               do
 
    GreedySel   
SG
    SG
 
  end for
  return SG

       
      SG

       SG
   

Algorithm   GECO           
same as Algorithm   except
      OMPSel   
Remark   We note that Step   of Algorithms   and   requires solving the RHS of   which is   matrix variate
problem of size    at iteration    This re tting is equivalent
to the  fullycorrective  versions of FrankWolfe Matching
Pursuit algorithms  Locatello et al    LacosteJulien  
Jaggi    which  intuitively speaking  extract out all the
information         from the chosen set of atoms  thereby
ensuring that the next rank   atom chosen has row and
column space orthogonal to the previously chosen atoms 
Thus the constrained maximization on the orthogonal complement of SO
in GreedySel 
 
need not be explicitly enforced  but is still shown for clarity 

in subroutine OMPSel  SG
 

  Analysis
In this section  we prove that low rank matrix optimization
over the rank one atoms satis es weak submodularity  We
explicitly delineate some notation and assumptions  With
slight abuse of notation  we assume   is mistrongly concave and Mismooth over pairs of matrices of rank    For
       note that mi   mj and Mi   Mj  Additionally  let
           rank            and assume   is
   smooth over   It is easy to see         
First we prove that if the low rank RSC holds  De nition  
then the submodularity ratio  De nition   is lowerbounded
by the inverse condition number 
Theorem   Let   be   set of   rank   atoms and   be
  set of   rank   atoms where we sequentially orthogonalize the atoms against   
If   is mistrongly con 

On Approximation Guarantees for Greedy Low Rank Optimization

cave over matrices of rank    and    smooth over the set
           rank            then
       Pa                    

                 

 

mr  
   

 

The proof of Theorem   is structured around individually
obtaining   lower bound for the numerator and an upper
bound for the denominator of the submodularity ratio by
exploiting the concavity and convexity conditions 

  Greedy Improvement
Bounding the submodularity ratio is crucial to obtaining
approximation guarantees for Algorithm  
Theorem   Let     SG
  be the greedy solution set obtained by running Algorithm   for   iterations  and let   
be an optimal support set of size    Let   be mi strongly
concave on the set of matrices with rank less than or equal
to    and     smooth on the set of matrices in the set  
Then 

         

 

ec           

 

ec       

where            

 

  and        mr  
   

 

   

The proof technique for the  rst inequality of Theorem  
relies on lower bounding the progress made in each iteration
of Algorithm   Intuitively  it exploits weak submodularity
to make sure that each iteration makes enough progress  and
then applies an induction argument for   iterations  We also
emphasize that the bounds in Theorem   are for normalized
set function      which means           more detailed
proof is presented in the appendix 
The bounds obtained in Theorem   are similar to the one
obtained in submodular maximization of monotone normalized functions  Nemhauser et al    In fact  our result
can be reinterpreted as an extension to previous results  The
greedy algorithm for submodular maximization assumes  
nite ground sets  We extend this for in nite ground sets  We
can do this  for matrices  as long as we have an implementation of the oracle GreedySel  Once the choice is made
by the oracle  standard analysis holds 
Remark   Theorem   provides the approximation guarantees for running the greedy selection algorithm up to  
iterations to obtain   rank   matrix iterate visa vis the
best rank   approximation  For        and       we get
an approximation bound            which is reminiscent
of the greedy bound of         under the framework of
submodularity  Note that our analysis can not be used to establish classical submodularity  However  establishing weak
submodularity that lower bounds   is suf cient to provide
slightly weaker than classical submodularity guarantees 

   log  

Remark   Theorem   implies that to obtain   approximation guarantee in the worst case  running Algorithm  
for     rM
          log   iterations suf ces  This
is useful when the application allows   tradeoff  compromising on the low rank constraint   little to achieve tighter
approximation guarantees 
Remark   Das   Kempe   considered the special
case of greedily maximizing    statistic for linear regression  which corresponds to classical sparsity in vectors 
They also obtain   bound of         where   is the
submodularity ratio for their respective setup  This was
generalized by Elenberg et al    to general concave
functions under sparsity constraints  Our analysis is for the
low rank constraint  as opposed to sparsity in vectors that
was considered by them 

  GECO Improvement
In this section  we obtain approximation guarantees for
Algorithm   The greedy search over the in nitely many
candidate atoms is infeasible  especially when       Thus
while Algorithm   establishes interesting theoretical connections with submodularity  it is not practical in general 
To obtain   tractable and practically useful algorithm  the
greedy search is replaced by   Frank Wolfe or Matching
Pursuit style linear optimization which can be easily implemented as  nding the top singular vectors of the gradient at
iteration    In this section  we show that despite the speedup 
we lose very little in terms of approximation guarantees  In
fact  if the approximation factor   in OMPSel  is   we get
the same bounds as those obtained for the greedy algorithm 
Theorem   Let     SO
  be the greedy solution set obtained using Algorithm   for   iterations  and let    be the
optimum size   support set  Let   be mr   strongly concave on the set of matrices with rank less than or equal to
        and     smooth on the set of matrices with rank in
the set   Then 

         

 

ec       

where          mr  
   

 

   

The proof of Theorem   follows along the lines of Theorem   The central idea is similar   to exploit the RSC
conditions to make sure that each iteration makes suf cient
progress  and then provide an induction argument for   iterations  Unlike the greedy algorithm  however  using the
submodularity ratio is no longer required  Note that the
bound obtained in Theorem   is similar to Theorem   except the exponent on the approximation factor  
Remark   Our proof technique for Theorem   can be applied for classical sparsity to improve the bounds obtained
by Elenberg et al    for OMP for support selection

On Approximation Guarantees for Greedy Low Rank Optimization

under RSC  and by Das   Kempe   for    statistic 
If              their bounds involve terms of the form
        in the exponent  as opposed to our bounds which
only has     in the exponent 
Remark   Similar to the greedy algorithm  to achieve  
tighter approximation to best rank   solution  one can relax
the low rank constraint   little by running the algorithm for
      greedy iterations  The result obtained by our Theorem   can be compared to the bound obtained by  ShalevShwartz et al     Theorem   for the same algorithm 
For an   multiplicative approximation  Theorem   implies we
need         log   On the other hand  ShalevShwartz
et al    obtain an additive approximation bound with
         which is an exponential improvement 

  Recovery Guarantees
While understanding approximation guarantees are useful 
providing parameter recovery bounds can further help us
understand the practical utility of greedy algorithms  In this
section  we present   general theorem that provides us with
recovery bounds of the true underlying low rank structure 
Theorem   Suppose that an algorithm achieves the approximation guarantee 

   Sk    Cr kf    
  

where Sk is the set of size   at iteration   of the algorithm 
  be the optimal solution for rcardinality constrained
  
maximization of     and Cr   be the corresponding approximation ratio guaranteed by the algorithm  Recall that
we represent by US  VS the matrices formed by stacking
the vectors represented by the support set   chosen from
    respectively                Then under mk   RSC  with
Br       HVS for any     Rr    we have
           kr Br   

kB Sk    Brk 

 

   

  
    Cr   

mk  

 

 Br     

Theorem   can be applied for Br       
    which is the
argmax for maximizing   under the low rank constraint 
It is general in the sense that it can be applied for getting
recovery bounds from approximation guarantees for any
algorithm  and hence is applicable for both Algorithms  
and  
Statistical recovery guarantees can be obtained from Theorem   for speci   choice of   and statistical model  Consider the case of low rank matrix estimation from noisy
linear measurements  Let Xi   Rm    for         be
generated so that each entry of Xi is       We observe
yi   hXi         where   is low rank  and say    

nky   

      Let          and let     Rm      Rn
be the linear operator so that      hXi      Our corresponding function is now        
  For this
function  using arguments by Negahban et al    we
know kr BS 
                
with high probability  It is also straightforward to apply
  and
their results to bound mk      
       which gives explicit bounds as per Theorem   for
Algorithms     for the considered function and the design
matrix 

          log  

and  BS 

 

     

    log  

 

  Experiments
In this section  we empirically evaluate the proposed algorithms 

  Clustering under Stochastic Block Model
First  we test empirically the performance of GECO  Algorithm   with       for   clustering task  We are provided
with   graph with nodes and the respective edges between
the nodes  The observed graph is assumed to have been
noisily generated from   true underlying clustering  The
goal is to recover the underlying clustering structure from
the noisy graph provided to us  Our greedy framework is applicable because the adjacency matrix of the true clustering
is low rank  We compare performance of Algorithm   on
simulated data against standard baselines of spectral clustering which are commonly used for this task  We begin by
describing   generative model for creating edges between
nodes given the ground truth 
The Stochastic Block Model is   model to generate random
graphs  It takes its input the set of   nodes  and   partition
of     which form   set of disjoint clusters  and returns
the graph with nodes and the generated edges  The model
has two additional parameters  the generative probabilities
         pair of nodes within the same cluster have an
edge between them with probability    while   pair of nodes
belonging to different clusters have an edge between them
with probability    For simplicity we assume           
The model then iterates over each pair of nodes  For each
such pair that belongs to same cluster  it samples an edge as
Bernoulli    otherwise as Bernoulli       This provides
us with       adjacency matrix 
We compare against two versions of spectral clustering 
which is   standard technique applied to  nd communities
in   graph  The method takes as input the       adjacency
matrix    which is       matrix with an entry Aij    
if there is an edge between node   and    and is   otherwise  From the adjacency matrix  the graph Laplacian  
is constructed  The Laplacian may be unnormalized  in
which case it is simply            where   is the diagonal matrix of degrees of nodes    normalized Laplacian is

On Approximation Guarantees for Greedy Low Rank Optimization

computed as Lnorm     LD  After calculating the
Laplacian  the algorithm solves for bottom   eigenvectors
of the Laplacian  and then apply kmeans clustering on the
rows of the thus obtained eigenvector matrix  We refer to
the works of Shi   Malik   Ng et al    for the
speci   details of clustering algorithms using unnormalized
and normalized graph Laplacian respectively 
We use our greedy algorithm to cluster the graph by optimizing   logistic PCA objective function  which is   special case of the exponential family PCA  Collins et al 
  For   given matrix    each entry Xij is assumed
to be independently drawn with likelihood proportional to
exph ij  Xiji     ij  where   is the true underlying
parameter  and    is the partition function corresponding
to   generalized linear model  GLM  It is easy to see we
can apply our framework of greedy selection by de ning
  as the loglikelihood 

       Xi  Xi  

log   ij 

where   is the true parameter matrix of   and   that generates   realization of    Since the true   is low rank  we get
the low rank constrained optimization problem 

max

rank  

 

where   is   hyperparameter suggesting the true number
of clusters  Note that lack of knowledge of true value of  
is not more restrictive than spectral clustering algorithms
which typically also require the true value of    Having
cast the clustering problem in the same form as   we
can apply our greedy selection algorithm as opposed to the
more costly alternating minimizing algorithms suggested
by Collins et al   
We generate the data as follows  For       nodes  and
 xed number of cluster       we vary the within cluster
edge generation probability   from   to   in increments of   and use the Stochastic Block model to generate   noisy graph with each    Note that smaller   implies
that the sampled graph will be more noisy and likely to be
more different than the underlying clustering 
We compare against the spectral clustering algorithm using
unnormalized Laplacian of Shi   Malik   which we
label  Spectral unnorm    for           and the
spectral clustering algorithm using normalized Laplacian
of Ng et al    which we label  Spectral norm   
for           We use Algorithm   which we label
 Greedy    for           For each of these models 
the referred   is the supplied hyperparameter  We report the
least squares error of the output from each model to the true
underlying    generalization error  and to the instantiation
used for training    reconstruction error 

Figure   Greedy Logistic PCA vs spectral clustering baselines
averaged over   runs  Top  Robust performance of greedy logistic
PCA for generalizing over varying values of   across different
values of    spectral clustering algorithms are more sensitive to
knowing true value of   Bottom  Strong performance of greedy
logisitic PCA even with small value of       for reconstructing
the given cluster matrix 

Figure   shows that the greedy logistic PCA performs well
in not only recreating the given noisy matrix  reconstruction  but also captures the true low rank structure better
 generalization  Further  note that providing the true hyperparameter   is vital for spectral clustering algorithms 
while on the other hand greedy is less sensitive to    This is
very useful in practice as   is typically not known  Spectral
clustering algorithms typically select   by computing an
SVD and rerunning kmeans for different values of    In
addition to being more robust  our greedy algorithm does
not need to be rerun for different values of     it produces
solutions incrementally 

 cluster probability Generlization error cluster prRbability ecRnstructiRn errRrSSectral unnorm SSectral unnorm SSectral unnorm SSectral norm SSectral norm SSectral norm GreeGy GreeGy GreeGy On Approximation Guarantees for Greedy Low Rank Optimization

Table   Empirical study of binomial based greedy factorization
shows competitive performance of word embeddings of common
words across tasks and datasets 

   MEN MSR

GOOGLE

  QUERIES

SVD
PPMI
SGNS
GREEDY

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

  times  which yields   vocabulary of   Note that
since we keep only the most common words  several queries
from the datasets are invalid because we do not have embeddings for words appearing in them  However  we do include
them by assigning invalid queries   value of   and reporting
the overall average over the entire dataset 
Table   shows the empirical evaluation  SVD and PPMI
are the models proposed by Levy   Goldberg  
while SGNS is the skipgram with negative sampling model
of Mikolov et al      We run each of these for
            and report the best results  This shows
that alternative factorizations such as our application of binomial PCA can be more consistent and competitive with
other embedding methods 
Conclusion  We have connected the problem of greedy
low rank matrix estimation to that of submodular optimization  Through that connection we have provided improved
exponential rates of convergence for the algorithm  An interesting area of future study will be to connect these ideas
to general atoms or dictionary elements 

Acknowledgements
We thank the anonymous reviewers for their helpful feedback  Research supported by William Hartwig Fellowship 
NSF Grants CCF        
  IIS   and ARO YIP   NF 

  Word Embeddings
Algorithms for embedding text into   vector space yield
representations that can be quite bene cial in many applications       features for sentiment analysis  Mikolov et al 
    proposed   contextbased embedding called skipgram or word vec  The context of   word can be de ned
as   set of words before  around  or after the respective
word  Their model strives to  nd an embedding of each
word so that the representation predicts the embedding of
each context word around it  Levy   Goldberg   subsequently showed that the word embedding model proposed
by Mikolov et al      can be reinterpreted as matrix factorization of the PMI matrix constructed as follows    word
  is in context of   if it lies within the respective window of
   The PMI matrix is then calculated as

PMIw     log         
          

In practice the probabilities                   are replaced
by their empirical counterparts  Further  note that         is
  if words   and   do not coexist in the same context  which
yields   for PMI  Levy   Goldberg   suggest
using an alternative  PPMIw     max PMIw      They
also suggest variations of PMI hyper parameterized by  
which corresponds to the number of negative samples in the
training of the original skip gram model 
We employ the binomial PCA model on the normalized
count matrix  instead of the PMI  in   manner similar to
the clustering approach in Section   The normalized
count matrix is calculated simply as       
       without taking
logarithms  This gives us   probability matrix which has
each entry between   and   and which can be factorized
under the binomial model greedily as per Algorithm  
We empirically study the embeddings obtained by binomial
factorization on two tasks   word similarity and analogies 
For word similarity  we use the    dataset  Finkelstein
et al    and the MEN data  Bruni et al    Both
these datasets contain words with human assigned similarity
scores  We evaluate the embeddings by their cosine similarity  and measuring the correlation with the available human
ratings  The fraction of correctly answered queries are returned as the metric  For the analogy task  we use the Microsoft Research  MSR  syntactic analogies  Mikolov et al 
    and the Google mixed analogies dataset  Mikolov
et al      For completing analogy          the prediction is calculated as arg maxx
  To compute
accuracy  we use the multiplication similarity metric as used
by Levy   Goldberg   To train the word embeddings 
we use the   news crawl dataset  We  lter out stop
words  nonASCII characters  and words occurring less than

cos      cos     

cos     

 http www statmt org wmt 
trainingmonolingual newscrawl

On Approximation Guarantees for Greedy Low Rank Optimization

References
Barron  Andrew    Cohen  Albert  Dahmen  Wolfgang 
and DeVore  Ronald    Approximation and learning by
greedy algorithms  The Annals of Statistics   
Feb  

Bruni  Elia  Boleda  Gemma  Baroni  Marco  and Tran 
NamKhanh  Distributional semantics in technicolor  In
Proceedings of the  th Annual Meeting of the Association for Computational Linguistics  Long Papers   Volume   ACL   pp    Stroudsburg  PA  USA 
  Association for Computational Linguistics 

Buhlmann  Peter and Yu  Bin  Boosting  Wiley Interdisciplinary Reviews  Computational Statistics   
Dec   ISSN  

Chen  Yudong and Wainwright  Martin    Fast lowrank estimation by projected gradient descent  General statistical
and algorithmic guarantees  arXiv  abs   
URL http arxiv org abs 

Collins  Michael  Dasgupta  Sanjoy  and Schapire  Robert   
  generalization of principal component analysis to the
exponential family  In Advances in Neural Information
Processing Systems  MIT Press   

Das  Abhimanyu and Kempe  David  Submodular meets
Spectral  Greedy Algorithms for Subset Selection  Sparse
Approximation and Dictionary Selection  In ICML  February  

Dudik  Miro  Harchaoui  Zaid  and Malick  Jerome  Lifted
coordinate descent for learning with tracenorm regularization  In AISTATS   

Elenberg  Ethan    Khanna  Rajiv  Dimakis  Alexandros   
and Negahban  Sahand  Restricted Strong Convexity
Implies Weak Submodularity  Proc  NIPS Workshop on
Learning in High Dimensions with Structure  December
 

Feige  Uriel    threshold of ln   for approximating set cover 

Journal of the ACM  JACM     

Finkelstein  Lev  Gabrilovich  Evgeniy  Matias  Yossi 
Rivlin  Ehud  Solan  Zach  Wolfman  Gadi  and Ruppin 
Eytan  Placing search in context  the concept revisited 
pp     

Jaggi  Martin and Sulovsk    Marek    simple algorithm
for nuclear norm regularized problems  In Frnkranz  Johannes and Joachims  Thorsten  eds  Proceedings of
the  th International Conference on Machine Learning
 ICML  pp    Omnipress   

Jain  Prateek  Netrapalli  Praneeth  and Sanghavi  Sujay 
Lowrank matrix completion using alternating minimization  In Symposium on Theory of Computing Conference 
STOC  Palo Alto  CA  USA  June     pp   
   

Jain  Prateek  Tewari  Ambuj  and Kar  Purushottam  On
iterative hard thresholding methods for highdimensional
mestimation  In Advances in Neural Information Processing Systems   Annual Conference on Neural Information Processing Systems   December    
Montreal  Quebec  Canada  pp     

Khanna  Rajiv  Tschannen  Michael  and Jaggi  Martin  Pursuits in Structured NonConvex Matrix Factorizations 
arXiv       

Khanna  Rajiv  Elenberg  Ethan    Dimakis  Alexandros   
Neghaban  Sahand  and Ghosh  Joydeep  Scalable Greedy
Support Selection via Weak Submodularity  AISTATS 
 

LacosteJulien  Simon and Jaggi  Martin  On the Global Linear Convergence of FrankWolfe Optimization Variants 
In NIPS   pp     

Lee  Kiryung and Bresler  Yoram  Corrections to  admira 
Atomic decomposition for minimum rank approximation  IEEE Trans  Information Theory   
 

Levy  Omer and Goldberg  Yoav  Neural word embedding as
implicit matrix factorization  In Ghahramani     Welling 
   Cortes     Lawrence        and Weinberger       
 eds  Advances in Neural Information Processing Systems   pp    Curran Associates  Inc   

Locatello  Francesco  Khanna  Rajiv  Tschannen  Michael 
and Jaggi  Martin    uni ed optimization view on generalized matching pursuit and frankwolfe  In Proc  International Conference on Arti cial Intelligence and Statistics
 AISTATS   

Gribonval    emi and Vandergheynst     On the exponential
convergence of matching pursuits in quasiincoherent
dictionaries  IEEE Trans  Inform  Theory   
 

Loh  PoLing and Wainwright  Martin    Regularized mestimators with nonconvexity  Statistical and algorithmic
theory for local optima     Mach  Learn  Res   
  January   ISSN  

Jaggi  Martin  Revisiting FrankWolfe  ProjectionFree
In ICML  pp   

Sparse Convex Optimization 
 

Mikolov  Tomas  Chen  Kai  Corrado  Greg  and Dean  Jeffrey  Ef cient estimation of word representations in vector
space  CoRR  abs     

On Approximation Guarantees for Greedy Low Rank Optimization

equations via nuclear norm minimization  SIAM Review 
  Jan   ISSN  

Rohde  Angelika and Tsybakov  Alexandre    Estimation
of highdimensional lowrank matrices  The Annals of
Statistics    Apr   ISSN   doi 
 aos  URL http dx doi org 
 aos 

ShalevShwartz     Gonen     and Shamir     Largescale
convex minimization with   lowrank constraint  In ICML 
 

Shi  Jianbo and Malik  Jitendra  Normalized cuts and image
segmentation  IEEE Trans  Pattern Anal  Mach  Intell 
  August   ISSN  

Wang  Zheng  Lai  MingJun  Lu  Zhaosong  Fan  Wei 
Davulcu  Hasan  and Ye  Jieping  Orthogonal rankone
matrix pursuit for low rank matrix completion  SIAM
Journal on Scienti   Computing        Jan
  ISSN   doi    URL
http dx doi org 

Mikolov  Tomas  Sutskever  Ilya  Chen  Kai  Corrado 
Greg    and Dean  Jeff  Distributed representations of
words and phrases and their compositionality  In Burges 
         Bottou     Welling     Ghahramani     and
Weinberger         eds  Advances in Neural Information Processing Systems   pp    Curran Associates  Inc     

Mikolov  Tomas  Yih  Scott Wentau  and Zweig  Geoffrey  Linguistic regularities in continuous space word
representations  In Proceedings of the   Conference
of the North American Chapter of the Association for
Computational Linguistics  Human Language Technologies  NAACLHLT  Association for Computational
Linguistics  May    

Negahban  Sahand and Wainwright  Martin    Estimation of
 near  lowrank matrices with noise and highdimensional
scaling  The Annals of Statistics     
ISSN  

Negahban  Sahand and Wainwright  Martin    Restricted
strong convexity and weighted matrix completion  Optimal bounds with noise  JMLR     

Negahban  Sahand  Ravikumar  Pradeep  Yu  Bin  and
Wainwright  Martin      Uni ed Framework for HighDimensional Analysis of MEstimators with Decomposable Regularizers  Statistica Sinica     
ISSN   doi   STS 

Nemhauser  George    Wolsey  Laurence    and Fisher  Marshall    An analysis of approximations for maximizing
submodular set functionsi  Mathematical Programming 
   

Ng  Andrew    Jordan  Michael    and Weiss  Yair  On
spectral clustering  Analysis and an algorithm  In ADVANCES IN NEURAL INFORMATION PROCESSING
SYSTEMS  pp    MIT Press   

Park  Dohyung  Kyrillidis  Anastasios  Bhojanapalli  Srinadh  Caramanis  Constantine  and Sanghavi  Sujay  Provable nonconvex projected gradient descent for   class
of constrained matrix optimization problems  arXiv 
abs    URL http arxiv org 
abs 

Rao  Nikhil  Shah  Parikshit  and Wright  Stephen  Forward backward greedy algorithms for atomic norm regIEEE Transactions on Signal Processing 
ularization 
  Nov  
ISSN   doi 
 tsp  URL http dx doi 
org tsp 

Recht  Benjamin  Fazel  Maryam  and Parrilo  Pablo   
Guaranteed minimumrank solutions of linear matrix

