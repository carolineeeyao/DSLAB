Maximum Selection and Ranking under Noisy Comparisons

Moein Falahatgar   Alon Orlitsky   Venkatadheeraj Pichapati   Ananda Theertha Suresh  

Abstract

We consider    PAC maximumselection and
ranking using pairwise comparisons for general
probabilistic models whose comparison probabilities satisfy strong stochastic transitivity and
stochastic triangle inequality  Modifying the
popular knockout
tournament  we propose  
simple maximumselection algorithm that uses

    
    log  

  comparisons  optimal up to  

constant factor  We then derive   general framework that uses noisy binary search to speed up
many ranking algorithms  and combine it with
merge sort to obtain   ranking algorithm that uses
  

    
  log   log log    comparisons for      

optimal up to    log log    factor 

  Introduction
  Background
Maximum selection and sorting using pairwise comparisons are computerscience staples taught in most introductory classes and used in many applications  In fact  sorting 
also known as ranking  was once claimed to utilize   of
all computer cycles        Mukherjee   
In many applications  the pairwise comparisons produce
only random outcomes  In sports  tournaments rank teams
based on pairwise matches whose outcomes are probabilistic in nature  For example  Microsoft   TrueSkill  Herbrich
et al    software matches and ranks thousands of Xbox
gamers based on individual game results  And in online advertising  out of   myriad of possible ads  each web page
may display only   few  and   user will typically select at
most one  Based on these random comparisons  ad companies such as Google  Microsoft  or Yahoo  rank the ads  appeal  Radlinski   Joachims    Radlinski et al   
These and related applications have brought about   resur 

 University of California  San Diego  Google Research 
Venkatadheeraj Pichapati  dheera 

to 

Correspondence
jpv ucsd edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

gence of interest in maximum selection and ranking using noisy comparisons 
Several probabilistic models
were considered 
including the popular BradleyTerry 
Luce  Bradley   Terry    and its PlackettLuce  PL 
generalization  Plackett    Luce    Yet even for
such speci   models  the number of pairwise comparisons
needed  or sample complexity  of maximum selection and
ranking was known only to within   log   factor  We consider   signi cantly broader class of models and yet propose algorithms that are optimal up to   constant factor for
maximum selection and up to  log log    for ranking 

  Notation
Noiseless comparison assumes an unknown underlying
ranking                of the elements in             such
that if two elements are compared  the higherranked one
is selected  Similarly for noisy comparisons  we assume
an unknown ranking of the elements  but now if two elements   and   are compared    is chosen with some unknown probability         and   is chosen with probability                       where if   is higherranked  then
  Repeated comparisons are independent of
           
each other 
  re ect the additional probability
Let                       
by which   is preferable to    Note that                    
and              if                      can also be seen as
  measure of dissimilarity between   and    Following  Yue
  Joachims    we assume that two natural properties  satis ed for example by the PL model  hold whenever
                   Strong Stochastic Transitivity  SST 
           max                  and Stochastic Triangle Inequality  STI                                
Two types of algorithms have been proposed for maximum selection and ranking under noisy comparisons  nonadaptive or of ine  Rajkumar   Agarwal    Negahban
et al      Jang et al    where the comparison pairs are chosen in advance  and adaptive or online
where the comparison pairs are selected sequentially based
on previous comparison results  We focus on the latter 
We specify the desired output via the    PAC
paradigm  Yue   Joachims    Sz or enyi et al   
that requires the output to likely closely approximate the
intended outcome  Speci cally  given         with prob 

Maximum Selection and Ranking under Noisy Comparisons

     

ability         maximum selection must output an  
maximum element   such that for all               
     
Similarly  with probability         the ranking algorithm
must output an  ranking                such that whenever                        
  Outline
In Section   we review past work and summarize our
contributions 
In Section   we describe and analyze our
maximumselection algorithm 
In Section   we propose
and evaluate the ranking algorithm  In Section   we experimentally compare our algorithms with existing ones  In
Section   we mention some future directions 

  Old and new results
  Related work
Several researchers studied algorithms that with probabil 
 Feige
ity        nd the exact maximum and ranking 
et al    considered   simple model where the elements are ranked  and              whenever            
 BusaFekete et al      considered comparison probabilities         satisfying the Mallows model  Mallows 
  And  Urvoy et al    BusaFekete et al     
Heckel et al    considered general comparison probabilities  without an underlying ranking assumption  and derived rankings based on Copelandand Bordacounts  and
randomwalk procedures  As expected  when the comparison probabilities approach half  the above algorithms require arbitrarily many comparisons 
To achieve  nite complexity even with nearhalf comparison probabilities  researchers adopted the PAC paradigm 
For the PAC model with SST and STI constraints   Yue  
Joachims    derived   maximumselection algorithm

  and used it to bound

with sample complexity     
the regret of the problem   duelingbandits variant  Related
results appeared in  Syrgkanis et al   
For the PL
model   Sz or enyi et al    derived   PAC ranking algorithm with sample complexity     
Deterministic adversarial versions of the problem were
considered by  Ajtai et al    and by  Acharya et al 
      who were motivated by density estimation  Acharya et al     

  log   log  

  log  

   

  New results
We consider    PAC adaptive maximum selection and
ranking using pairwise comparisons under SST and STI
constraints  Note that when      
  or            for
maximum selection and             for ranking  any
output is correct  We show for        
  and any   

complexity 

   log    log  

    log  

  Maximumselection algorithm with sample complexity     
  optimal up to   constant factor 
  sample
  Ranking algorithm with     
  General framework that converts any ranking algorithm with sample complexity     
 
   log     log  
into   ranking algorithm that for      
  has sample
  log   log log     
complexity     
  Using the above framework    ranking algorithm with
sample complexity     
  log   log log    for      
  
  lower bound on the sample complex 
  An    

ity of any PAC ranking algorithm  matching our algorithm   sample complexity up to    log log    factor 

  log  

  Maximum selection
  Algorithm outline
We propose   simple maximumselection algorithm based
on Knockout tournaments  Knockout tournaments are used
to  nd   maximum element under nonnoisy comparisons 
Knockout tournament of   elements runs in dlog ne rounds
where in each round it randomly pairs the remaining elements and proceeds the winners to next round 
Our

    log  
    
 
    

KNOCKOUT
uses
and      memory
 Yue   Joachims    uses
  comparisons and      memory to  nd an

 maximum  Hence we get log nfactor improvement in
the number of comparisons and also we use linear memory
compared to quadratic memory  From  Zhou   Chen 
  it can be inferred that the best PAC maximum selec 

given
in
comparisons

to  nd an  maximum 

algorithm 

  log  

tion algorithm requires    

    log  

  comparisons 

hence up to constant factor  KNOCKOUT is optimal 
 Yue   Joachims    Sz or enyi et al    eliminate elements one by one until only  maximums are remaining 
Since they potentially need       eliminations  in order
to appply union bound they had to ensure that each eliminated element is not an  maximum          requiring
  log    comparisons for each eliminated element and
hence   superlinear sample complexity     log   
In contrast  KNOCKOUT eliminates elements in log  
rounds  Since in Knockout tournaments  number of elements decrease exponentially with each round  we afford
to endure more error in the initial rounds and less error
in the latter rounds by repeating comparison between each
pair more times in latter rounds  Speci cally  let bi be the
highestranked element  according to the unobserved underlying ranking  at the beginning of round    KNOCKOUT
makes sure that           
        bi  bi       by repeating

Maximum Selection and Ranking under Noisy Comparisons

  

 
 

  

       

times  Choosing        

comparison between each pair in round   for     
 
log   
    with       we make sure
  and by
that comparison complexity is     
    log  
union bound and STI                     bdlog ne   
Pdlog ne 
For         relaxed notion of SST  called  stochastic
transitivity  Yue   Joachims    requires that if       
            then max                                 Our
results apply to this general notion of  stochastic transitivity and the analysis of KNOCKOUT is presented under
this model  KNOCKOUT uses      

      log  

  com 

parisons 
Remark  
 Yue   Joachims    considered   different de nition of  maximum as an element   that is at most
  dissimilar to true maximum      for   with          
             Note that this de nition is less restrictive than
ours  hence requires fewer comparisons  Under this de 
  comnition   Yue   Joachims    used      
  comtion of KNOCKOUT shows that      

parisons suf ce  Hence we also get   signi cant improvement in the exponent of  

parisons to  nd an  maximum whereas   simple modi ca 

      log  

  log  

To simplify the analysis  we assume that   is   power of
  otherwise we can add  dlog ne     dummy elements that
lose to every original element with probability   Note that
all  maximums will still be from the original set 

  Algorithm
We start with   subroutine COMPARE that compares two
elements  It compares two elements      and maintains empirical probability  pi    proxy for         It also maintains  
con dence value                 pi                           
COMPARE stops if it is con dent about the winner or if it
reaches its comparison budget    It outputs the element
with more wins breaking ties randomly 

          wi    

Algorithm   COMPRARE
Input  element    element    bias   con dence  
Initialize   pi    
  while    pi    

       

        
  log  
         and       
           

    Compare   and    if   wins wi   wi    
   log    
   
               pi   wi

if  pi    

  Output     else Output    

We show that COMPARE        outputs the correct winner
if the elements are well seperated 

Lemma   If              then

    COMPARE                   

Note that instead of using  xed number of comparisons 
COMPARE stops the comparisons adaptively if it is con 
 dent about the winner  If            COMPARE stops
much before comparison budget  
  and hence works
better in practice 
Now we present the subroutine KNOCKOUTROUND that
we use in main algorithm KNOCKOUT 

  log  

  KNOCKOUTROUND
KNOCKOUTROUND takes   set   and outputs   set of size
    It randomly pairs elements  compares each pair using COMPARE  and returns the set of winners  We will later
show that maximum element in the output set will be comparable to maximum element in the input set 

Algorithm   KNOCKOUTROUND
Input  Set    bias   con dence  
Initialize  Set      
  Pair elements in   randomly 

  for every pair       

Add COMPARE           to   

Output   

Note that comparisons between each pair can be handled
by   different processor and hence this algorithm can be
easily parallelized 
  can have several maximum elements  Comparison probabilities corresponding to all maximum elements will be
essentially same because of STI  We de ne max    to be
the maximum element with the least index  namely 

max    def    min                         
Lemma   KNOCKOUTROUND        uses     log  
comparisons and with probability        

   max    max KNOCKOUTROUND           

 

  KNOCKOUT
Now we present the main algorithm KNOCKOUT  KNOCKOUT takes an input set   and runs log   rounds of
KNOCKOUTROUND halving the size of   at the end of
each round  Recall that KNOCKOUTROUND makes sure
that maximum element in the output set is comparable to

Maximum Selection and Ranking under Noisy Comparisons

maximum element in the input set  Using this  KNOCKOUT makes sure that the output element is comparable to
maximum element in the input set 
Since the size of   gets halved after each round  KNOCKOUT compares each pair more times in the latter rounds 
Hence the bias between maximum element in input set and
maximum element in output set is small in latter rounds 

Algorithm   KNOCKOUT
Input  Set    bias   con dence   stochasticity  
Initialize            set of all elements           
while        
      KNOCKOUTROUND   

   

       

  

           

Output  the unique element in   

 

 

KNOCKOUT       

      log  

  comparisons and with proba 

Note that KNOCKOUT uses only memory of set   and
hence      memory suf ces 
Theorem   shows that KNOCKOUT outputs an  maximum
with probability         It also bounds the number of
comparisons used by the algorithm 
Theorem
uses
      
bility at least       outputs an  maximum 
  Ranking
We propose   ranking algorithm that with probability at
  uses      log   log log   
  comparisons and outleast    
puts an  ranking 
Notice that we use only       log  
    comparisons for      
 Sz or enyi et al    uses     log   
where as
comparisons even for constant error probability   Furthermore  Sz or enyi et al    provided these guarantees
only under PlackettLuce model which is more restrictive
compared to ours  Also  their algorithm uses      memory compared to      memory requirement of ours 
Our main algorithm BINARYSEARCH RANKING assumes
the existence of   ranking algorithm RANKx that with
probability at least       uses     

parisons and outputs an  ranking for any         and
some       We also present   RANKx algorithm with
     
Observe that we need RANKx algorithm to work for any
model that satis es SST and STI   Sz or enyi et al   
showed that their algorithm works for PlackettLuce model
but not for more general model  So we present   RANKx

  com 

   log     log  

 

 

 

algorithm that works for general model 
The main algorithm BINARYSEARCH RANKING randomly selects
 log     elements  anchors  and rank them
using RANKx   The algorithm has then effectively created
 log     bins  each between two successively ranked
anchors  Then for each element  the algorithm identi es
the bin it belongs to using   noisy binary search algorithm 
The algorithm then ranks the elements within each bin using RANKx  
We  rst present MERGERANK    RANK  algorithm 

 

log  

  Merge Ranking
We present   simple ranking algorithm MERGERANK that

  comparisons       memory and
uses      log   
with probability         outputs an  ranking  Thus
MERGERANK is   RANKx algorithm for      
Similar to Merge Sort  MERGERANK divides the elements
into two sets of equal size  ranks them separately and combines the sorted sets  Due to the noisy nature of comparisons  MERGERANK compares two elements      suf 
 cient times  so that the comparison output is correct with
log    Put differently 
high probability when         
MERGERANK is same as the typical Merge Sort  except it
uses COMPARE as the comparison function  Due to lack of
space  MERGERANK is presented in Appendix   
Let   de ne the error of an ordered set   as the maximum
distance between two wrongly ordered items in    namely 

 

err    def  max

       

            

We show that when we merge two ordered sets  the error of
log   more than the
the resulting ordered set will be at most
maximum of errors of individual ordered sets 
Observe that MERGERANK is   recursive algorithm and
the error of   singleton set is   Two singleton sets each
containing   unique element from the input set merge to
form   set with two elements with an error at most
log   
then two sets with two elements merge to form   set with
four elements with an error of at most
log   and henceforth 
Thus the error of the output ordered set is bounded by  
Lemma   shows that MERGERANK can output an  
ranking of   with probability         It also bounds the
number of comparisons used by the algorithm 
Lemma
 

  MERGERANK   
log       comparisons and with probability
    log    
        outputs an  ranking  Hence  MERGERANK is  
RANK  algorithm 

   

log    

takes

 

 

 

 

 

 

Now we present our main ranking algorithm 

Maximum Selection and Ranking under Noisy Comparisons

  BINARYSEARCH RANKING
We  rst sketch the algorithm outline below  We then provide   proof outline 

 

 

 

  ALGORITHM OUTLINE
Our algorithm is stated in BINARYSEARCH RANKING  It
can be summarized in three major parts 
Creating anchors 
 Steps   to   BINARYSEARCH 
RANKING  rst selects   set    of
 log     random elements
 anchors  and ranks them using RANKx   At the end of
 log     ranked anchors  Equivalently 
this part  there are
 log         bins  each bin between
the algorithm creates
two successively ranked anchors 
Coarse ranking   Step   After forming the bins  the algorithm uses   random walk on   binary search tree  to  nd
which bin each element belongs to  INTERVALBINARY 
SEARCH is similar to the noisy binary search algorithm
in  Feige et al    It builds   binary search tree with the
bins as the leaves and it does   random walk over this tree 
Due to lack of space the algorithm INTERVALBINARY 
SEARCH is presented in Appendix   but more intuition is
given later in this section 
Ranking within each bin 
 Step   For each bin  we
show that the number of elements far from both anchors
is bounded  The algorithm checks elements inside   bin
whether they are close to any of the bin   anchors  For
the elements that are close to anchors  the algorithm ranks
them close to the anchor  And for the elements that are
away from both anchors the algorithm ranks them using
RANKx and outputs the resulting ranking 

 

 

   Remove these elements from   

 log   xk    

Algorithm   BINARYSEARCH RANKING
Input  Set    bias  
Initialize              and So     Sj    
Cj     and Bj     for         
 log   xk random elements from
  Form   set    withj
  Rank    using RANKx       
  
                 SS    Add dummy element   at
the end of    such that                  SS   
  for       

  Add dummy element   at the beginning of    such that

        INTERVALBINARY SEARCH        
    Insert   in Sk 

 

   if COMPARE           log   

  for       toj
 log   xk    
    for     Sj 
   
        insert   in Cj 
       
if
COMPARE       
    log         
       
    Rank Bj using RANKx Bj     
  

iii  else insert   in Bj 

then insert   in Cj 

ii  else

    Append      Cj  Bj in order at the end of So 

     

 
 

Output  So

 

  ANALYSIS OF BINARYSEARCH RANKING
Creating anchors In Step   of the algorithm we select
  log     random elements  Since these are chosen uniformly random  they lie nearly uniformly in the set    This
intuition is formalized in the next lemma 
Lemma   Consider   set   of   elements  If we select
 log     elements uniformly randomly from   and build an
ordered set                                   then with
probability        
                                        log     
In Step   we use RANKx to rank    Lemma   shows the
guarantee of ranking   
Lemma   After Step   of
RANKING with probability        
At the end of Step   we have
 log         bins  each between two successively ranked anchors  Each bin has   left

the BINARYSEARCH 
        is  ranked 

     for any     and all   

 

anchor and   right anchor   We say that an element belongs
to   bin if it wins over the bin   left anchor with probability
  and wins over the bin   right anchor with probabil 
   
ity    
  Notice that some elements might win over   
with probability    
  and thus not belong to any bin  So in
Step   we add   dummy element   at the beginning of   
where   loses to every element in SS    with probability
end of    where every element in SS    loses to   with

probability  
Coarse Ranking Note that      and         are respectively the left and right anchors of the bin Si 

  For similar reasons we add   dummy element   to the

Algorithm   COMPARE 
Input  element    element    number of comparisons   

  Compare   and   for   times and return the fraction

of times   wins over   

Maximum Selection and Ranking under Noisy Comparisons

 

      for some      

Since    is  ranked and the comparisons are noisy  it is
hard to  nd   bin Si for an element   such that            
  and                 
  We call   bin Si    nearly
correct bin for an element   if              
      and
                
In Step   for each element we  nd an  nearly correct bin
using INTERVALBINARY SEARCH   Next we describe an
outline of INTERVALBINARY SEARCH 
INTERVALBINARY SEARCH  rst builds   binary search
tree of intervals  see Appendix    as follows  the root node
is the entire interval between the  rst and the last elements
in    Each nonleaf node interval   has two children corresponding to the left and right halves of    The leaves of the
tree are the bins between two successively ranked anchors 
To  nd an  nearly correct bin for an element    the algorithm starts at the root of the binary search tree and at every
nonleaf node corresponding to interval    it checks if   belongs to   or not by comparing   with     left and right
anchors  If   loses to left anchor or wins against the right
anchor  the algorithm backtracks to current node   parent 
If   wins against     left anchor and loses to its right one 
the algorithm checks if   belongs to the left or right child
by comparing   with the middle element of   and moves
accordingly 
When at   leaf node  the algorithm checks if   belongs to
the bin by maintaining   counter  If   wins against the bin  
left anchor and loses to the bin   right anchor  it increases
the counter by one or otherwise it decreases the counter by
one  If the counter is less than   the algorithm backtracks
to the bin   parent  By repeating each comparison several
times  the algorithm makes   correct decision with proba 
 
bility    
Note that there could be several  nearly correct bins for
  and even though at each step the algorithm moves in the
direction of one of them  it could end up moving in   loop
and never reaching one of them  We thus run the algorithm
for   log   steps and terminate 
If the algorithm is at   leaf node by   log   steps and the
counter is more than   log   we show that the leaf node bin
is    nearly correct bin for   and the algorithm outputs
the leaf node  If not  the algorithm puts in   set   all the
anchors visited so far and orders   according to   
We select   log   steps to ensure that if there is only one
nearly correct bin  then the algorithm outputs that bin     
     Also we do not want too many steps so as to
       
bound the size of   
By doing   simple binary search in   using BINARYSEARCH  see Appendix    we  nd an anchor      
such that             Since INTERVALBINARY 

      and  

SEARCH ran for at most   log   steps    can have at most
  log   elements and hence BINARYSEARCH can search
effectively by repeating each comparison   log    times
to maintain high con dence  Next paragraph explains how
BINARYSEARCH  nds such an element   
BINARYSEARCH  rst compares   with the middle elIf the fraction of
ement   of   for   log    times 
wins for   is between  
      and  
      then       
           and hence BINARYSEARCH outputs   
If the fraction of wins for   is less than  
      then       
             and hence it eliminates all elements to the
right of   in    If the fraction of wins for   is more than
      then                     and hence it eliminates
 
all elements to the left of   in    It continues this process
until it  nds an element   such that the fraction of wins for
  is between  
     
INTERVALBINARY 
In next Lemma  we show that
SEARCH achieves to  nd    nearly correct bin for every
element 
Lemma   For any element        Step   of
BINARYSEARCH RANKING places   in bin Sl such that
               and                   with probability        
    
Ranking within each bin Once we have identi ed the bins 
we rank the elements inside each bin  By Lemma   inside
each bin all elements are close to the bin   anchors except
at most  log      of them 
The algorithm  nds the elements close to anchors in Step
   by comparing each element in the bin with the bin  
anchors  If an element in bin Sj is close to bin   anchors
     or           the algorithm moves it to the set Cj
or Cj  accordingly and if it is far away from both  the algorithm moves it to the set Bj  The following two lemmas
state that this separating process happens accurately with
high probability  The proofs of these results follow from
the Chernoff bound and hence omitted 
Lemma   At the end of Step     for all         Cj 
               with probability        
    
Lemma   At the end of Step     for all         Bj 
min                             with probability
    
       
Combining Lemmas     and   next lemma shows that the
size of Bj is bounded for all   
Lemma   At the end of Step      Bj   log      for
all    with probability        
    
Since all the elements in Cj are already close to an anchor  they need not be ranked  By Lemma   with probability        
   the number of elements in Bj is at most
 log      We use RANKx to rank each Bj and output
the  nal ranking 

Maximum Selection and Ranking under Noisy Comparisons

Lemma   shows that all Bj   are  ranked at the end of
Step     Proof follows from properties of RANKx and
union bound 
Lemma   At the end of Step     all Bjs are  ranked
with probability        
    
Combining the above set of results yields our main result 
Theorem   Given access
to RANKx  BINARYSEARCH RANKING with probability        
   uses
     log   log log    

  comparisons and outputs an  

ranking 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

KNOCKOUT
MalllowsMPI
AR
BTMPAC

  

  

  

Number of elements

uses

 

 

BINARYSEARCH RANKING

Using MERGERANK as   RANKx algorithm with      
leads to the following corollary 
Corollary

  comparisons and outputs an  

     log   log log   
ranking with probability        
  
Using PALPACAMPRR  Sz or enyi et al    as  
RANKx algorithm with       leads to the following corollary over PL model 
Corollary   Over PL model  BINARYSEARCH 
RANKING with
uses

     

It is well known that to rank   set of   values under the
noiseless setting     log    comparisons are necessary 

probability  
  comparisons and outputs an  ranking 
  samples

     log   log log  
We show that under the noisy model     
are necessary to output an  ranking and hence our algorithm is nearoptimal 
Theorem   For      
  there exists   noisy model
       
that satis es SST and STI such that to output an  ranking
with probability            

  comparisons are

necessary 

  log  

 

 

  log  

  Experiments
We compare the performance of our algorithms with that
of others over simulated data  Similar to  Yue   Joachims 
  we consider the stochastic model where          
          Note that this model satis es both SST and STI 
We  nd  maximum with error probability       Observe that       is the only  maximum  We compare
the sample complexity of KNOCKOUT with that of BTMPAC  Yue   Joachims    MallowsMPI  BusaFekete
et al      and AR  Heckel et al    BTMPAC is
an    PAC algorithm for the same model considered in
this paper  MallowsMPI  nds   Condorcet winner which
exists under our general model  AR  nds the maximum according to Borda scores  We also tried PLPAC  Sz or enyi
et al    developed originally for PL model but the algorithm could not meet guarantees of       under this

Figure   Comparison of sample complexity for small input sizes 
with       and      

 

 
 
 
 
 
 
 
 
 
 

 

 
 
 
 
 

 

 

 

 

 

 

KNOCKOUT

MallowsMPI

AR

     

     

     

Number of elements

     

Figure   Comparison of sample complexity for large input size 
with       and      

model and hence omitted  Note that in all the experiments
the reported numbers are averaged over   runs 
In Figure   we compare the sample complexity of algorithms when there are     and   elements  Our algorithm outperforms all the others  BTMPAC performs
much worse in comparison to others because of high constants in the algorithm  Further BTMPAC allows comparing an element with itself since the main objective in
 Yue   Joachims    is to reduce the regret  We exclude
BTMPAC for further experiments with higher number of
elements 
In Figure   we compare the algorithms when there are  
    and   elements  Our algorithm outperforms
others for higher number of elements too  Performance of
AR gets worse as the number of elements increases since
Borda scores of the elements get closer to each other and
hence AR takes more comparisons to eliminate an element 
Notice that number of comparisons is in logarithmic scale
and hence the performance of MallowsMPI appears to be
close to that of ours 
As noted in  Sz or enyi et al    sample complexity of
MallowsMPI gets worse as          gets close to   To

Maximum Selection and Ranking under Noisy Comparisons

 

 

 

 

KNOCKOUT

MallowsMPI

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 
 

 

 

 

 

 

 

Number of elements

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

Figure   Sample complexity of KNOCKOUT and MallowsMPI
for different values of     with       and      

 

 

 

 

 
 
 
 
 
 
 
 
 
 

 

 
 
 
 
 

 
 

MallowsMPI
KNOCKOUT

 

 

 

 

 

Figure   Sample complexity of KNOCKOUT and MallowsMPI
under Mallows model for various values of  

show the pronounced effect  we use the stochastic model
                                               
where        and the number of elements is   Here too
we  nd  maximum with       Note that       is
the only  maximum in this stochastic model  In Figure   we compare the algorithms for different values of
        and   As discussed above  the performance of MallowsMPI gets much worse whereas our
algorithm   performance stays unchanged  The reason is
that MallowsMPI  nds the Condorcet winner using successive elimination technique and as    gets closer to  
MallowsMPI takes more comparisons for each elimination  Our algorithm tries to  nd an alternative which defeats Condorcet winner with probability         and
hence for alternatives that are very close to each other  our
algorithm declares either one of them as winner after comparing them for certain number of times 
Next we evaluate KNOCKOUT on Mallows model which
does not satisfy STI  Mallows is   parametric model which
is speci ed by single parameter   As in  BusaFekete
et al      we consider       elements and various
values for                   and  
Here again we seek to  nd  maximum with      

Figure   Sample complexity of MERGERANK for different  

As we can see in Figure   sample complexity of KNOCKOUT and MallowsMPI is essentially same under small values of   but KNOCKOUT outperforms MallowsMPI as  
gets close to   since comparison probabilities grow closer
to   Surprisingly  for all values of   except for  
KNOCKOUT returned Condorcet winner in all runs  For
      KNOCKOUT returned second best element in  
runs out of   Note that           and hence
KNOCKOUT still outputed    maximum  Even though
we could not show theoretical guarantees of KNOCKOUT
under Mallows model  our simulations suggest that it can
perform well even under this model 
For the stochastic model                     we run
our MERGERANK algorithm to  nd an  ranking with
      Figure   shows that sample complexity does not
increase   lot with decreasing   We attribute this to the
subroutine COMPARE that  nds the winner faster when the
elements are more dissimilar 
Some more experiments are provided in Appendix   

  Conclusion
We studied maximum selection and ranking using noisy
comparisons for broad comparison models satisfying SST
and STI  For maximum selection we presented   simple
algorithm with linear  hence optimal  sample complexity 
For ranking we presented   framework that improves the
performance of many ranking algorithms and applied it to
merge ranking to derive   nearoptimal algorithm 
We conducted several experiments showing that our algorithms perform well and outperform existing algorithms
on simulated data 
The maximumselection experiments suggest that our algorithm performs well even without STI  It would be of interest to extend our theoretical guarantees to this case  For
ranking  it would be interesting to close the  log log    ratio between the upperand lowercomplexity bounds 

Maximum Selection and Ranking under Noisy Comparisons

Negahban  Sahand  Oh  Sewoong  and Shah  Devavrat  Iterative
ranking from pairwise comparisons  In NIPS  pp   
 

Negahban  Sahand  Oh  Sewoong  and Shah  Devavrat  Rank centrality  Ranking from pairwise comparisons  Operations Research   

Plackett  Robin    The analysis of permutations  Applied Statis 

tics  pp     

Radlinski  Filip and Joachims  Thorsten  Active exploration for
In Proceedings of

learning rankings from clickthrough data 
the  th ACM SIGKDD  pp    ACM   

Radlinski  Filip  Kurup  Madhu  and Joachims  Thorsten  How
does clickthrough data re ect retrieval quality 
In Proceedings of the  th ACM conference on Information and knowledge management  pp    ACM   

Rajkumar  Arun and Agarwal  Shivani    statistical convergence
perspective of algorithms for rank aggregation from pairwise
data  In Proc  of the ICML  pp     

Syrgkanis  Vasilis  Krishnamurthy  Akshay  and Schapire 
Robert    Ef cient algorithms for adversarial contextual learning  arXiv preprint arXiv   

Sz or enyi  Bal azs  BusaFekete    obert  Paul  Adil 

and
  ullermeier  Eyke  Online rank elicitation for plackettluce 
  dueling bandits approach  In NIPS  pp     

Urvoy  Tanguy  Clerot  Fabrice    eraud  Raphael  and Naamane 
In

Sami  Generic exploration and karmed voting bandits 
Proc  of the ICML  pp     

Yue  Yisong and Joachims  Thorsten  Beat the mean bandit  In

Proc  of the ICML  pp     

Zhou  Yuan and Chen  Xi  Optimal pac multiple arm identi cation

with applications to crowdsourcing   

  Acknowledgements
We thank Yi Hao and Vaishakh Ravindrakumar for very
helpful discussions and suggestions  and NSF for supporting this work through grants CIF  and CIF 
 

References
Acharya  Jayadev  Jafarpour  Ashkan  Orlitsky  Alon  and Suresh 
Ananda Theertha  Sorting with adversarial comparators and
application to density estimation 
In ISIT  pp   
IEEE     

Acharya  Jayadev  Jafarpour  Ashkan  Orlitsky  Alon  and Suresh 
Ananda Theertha  Nearoptimal sample estimators for spherical gaussian mixtures  NIPS     

Acharya  Jayadev  Falahatgar  Moein  Jafarpour  Ashkan  Orlitsky  Alon  and Suresh  Ananda Theertha  Maximum selection
and sorting with adversarial comparators and an application to
density estimation  arXiv preprint arXiv   

Ajtai  Mikl os  Feldman  Vitaly  Hassidim  Avinatan  and Nelson  Jelani  Sorting and selection with imprecise comparisons 
ACM Transactions on Algorithms  TALG     

Bradley  Ralph Allan and Terry  Milton    Rank analysis of incomplete block designs     the method of paired comparisons 
Biometrika     

BusaFekete    obert    ullermeier  Eyke  and Sz or enyi  Bal azs 
Preferencebased rank elicitation using statistical models  The
case of mallows  In Proc  of the ICML  pp       

BusaFekete    obert  Sz or enyi  Bal azs  and   ullermeier  Eyke 
Pac rank elicitation through adaptive sampling of stochastic
pairwise preferences  In AAAI     

Feige  Uriel  Raghavan  Prabhakar  Peleg  David  and Upfal  Eli 
Computing with noisy information  SIAM Journal on Computing     

Heckel  Reinhard  Shah  Nihar    Ramchandran  Kannan  and
Wainwright  Martin    Active ranking from pairwise comparisons and when parametric assumptions don   help  arXiv
preprint arXiv   

Herbrich  Ralf  Minka  Tom  and Graepel  Thore  Trueskill   
bayesian skill rating system 
In Proceedings of the  th International Conference on Neural Information Processing Systems  pp    MIT Press   

Jang  Minje  Kim  Sunghyun  Suh  Changho  and Oh  Sewoong 
Topk ranking from pairwise comparisons  When spectral
ranking is optimal  arXiv preprint arXiv   

Luce    Duncan  Individual choice behavior    theoretical anal 

ysis  Courier Corporation   

Mallows  Colin    Nonnull ranking models     Biometrika   

   

Mukherjee  Sudipta  Data structures using      problems and

solutions  McGraw Hill Education   

