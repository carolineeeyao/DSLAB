CountBasedExplorationwithNeuralDensityModelsGeorgOstrovski MarcG Bellemare   aronvandenOord   emiMunos AbstractBellemareetal introducedthenotionofapseudocount derivedfromadensitymodel togeneralizecountbasedexplorationtonon tabularreinforcementlearning ThispseudocountwasusedtogenerateanexplorationbonusforaDQNagentandcombinedwithamixedMonteCarloupdatewassuf cienttoachievestateoftheartontheAtari gameMontezuma sRevenge Weconsidertwoquestionsleftopenbytheirwork First howimportantisthequalityofthedensitymodelforexploration Second whatroledoestheMonteCarloupdateplayinexploration Weanswerthe rstquestionbydemonstratingtheuseofPixelCNN anadvancedneuraldensitymodelforimages tosupplyapseudo count Inparticular weexaminetheintrinsicdif cultiesinadaptingBellemareetal sapproachwhenassumptionsaboutthemodelareviolated Theresultisamorepracticalandgeneralalgorithmrequiringnospecialapparatus WecombinePixelCNNpseudocountswithdifferentagentarchitecturestodramaticallyimprovethestateoftheartonseveralhardAtarigames Onesurprising ndingisthatthemixedMonteCarloupdateisapowerfulfacilitatorofexplorationinthesparsestofsettings includingMontezuma sRevenge IntroductionExplorationistheprocessbywhichanagentlearnsaboutitsenvironment Inthereinforcementlearningframework thisinvolvesreducingtheagent suncertaintyabouttheenvironment stransitiondynamicsandattainablerewards Fromatheoreticalperspective explorationisnowwellunderstood     Strehl Littman Jakschetal Osbandetal andBayesianmethodshave DeepMind London UK Correspondenceto GeorgOstrovski ostrovski google com Proceedingsofthe thInternationalConferenceonMachineLearning Sydney Australia PMLR Copyright bytheauthor   beensuccessfullydemonstratedinanumberofsettings Deisenroth Rasmussen Guezetal Ontheotherhand practicalalgorithmsforthegeneralcaseremainscarce fullyBayesianapproachesareusuallyintractableinlargestatespaces andthecountbasedmethodtypicaloftheoreticalresultsisnotapplicableinthepresenceofvaluefunctionapproximation Recently Bellemareetal proposedthenotionofpseudocountasareasonablegeneralizationofthetabu larsettingconsideredinthetheoryliterature Thepseudocountisde nedintermsofadensitymodel trainedonthesequenceofstatesexperiencedbyanagent           where     canbethoughtofasatotalpseudocountcom putedfromthemodel srecodingprobability   theprobabilityofxcomputedimmediatelyaftertrainingonx AsapracticalapplicationtheauthorsusedthepseudocountsderivedfromthesimpleCTSdensitymodel Bellemareetal toincentivizeexplorationinAtari agents OneofthemainoutcomesoftheirworkwassubstantialempiricalprogressontheinfamouslyhardgameMONTEZUMA SREVENGE Theirmethodcriticallyhingedonseveralassumptionsregardingthedensitymodel themodelshouldbelearningpositive     theprobabilityassignedtoastatexshouldincreasewithtraining itshouldbetrainedonline usingeachsampleexactlyonce and theeffectivemodelstepsizeshoulddecayatarateofn PartoftheirempiricalsuccessalsoreliedonamixedMonteCarlo QLearningupdaterule whichpermittedfastpropagationoftheexplorationbonuses Inthispaper wesetouttoanswerseveralresearchquestionsrelatedtothesemodellingchoicesandassumptions Towhatextentdoesabetterdensitymodelgiverisetobetterexploration Cantheabovemodellingassumptionsberelaxedwithoutsacri cingexplorationperformance WhatroledoesthemixedMonteCarloupdateplayinsuccessfullyincentivizingexploration CountBasedExplorationwithNeuralDensityModelsInparticular weexploretheuseofPixelCNN vandenOordetal     astateof theartneuraldensitymodel Weexaminethechallengesposedbythisapproach Modelchoice Performingtwoevaluationsandonemodelupdateateachagentstep tocompute   and   canbeprohibitivelyexpensive Thisrequiresthedesignofasimpli ed yetsuf cientlyexpressiveandaccurate PixelCNNarchitecture Modeltraining ACTSmodelcannaturallybetrainedfromsequentiallypresented correlateddatasamples Traininganeuralmodelinthisonlinefashionrequiresmorecarefulattentiontotheoptimizationproceduretopreventover ttingandcatastrophicforgetting French Modeluse Thetheoryofpseudocountsrequirestheden sitymodel srateoflearningtodecayovertime Optimizationofaneuralmodel however imposesconstraintsonthestepsizeregimewhichcannotbeviolatedwithoutdeterio ratingeffectivenessandstabilityoftraining Theconceptofintrinsicmotivationhasmadearecentresurgenceinreinforcementlearningresearch ingreatpartduetoadissatisfactionwith greedyandBoltzmannpolicies Ofnote Tangetal maintainanapproximatecountbymeansofhashtablesoverfeatures whichinthepseudocountframeworkcorrespondstoahash baseddensitymodel Houthooftetal usedasecondorderTaylorapproximationofthepredictiongaintodriveexplo rationincontinuouscontrol Asresearchmovestowardsevermorecomplexenvironments weexpectthetrendtowardsmoreintrinsicallymotivatedsolutionstocontinue Background PseudoCountandPredictionGainHerewebrie yintroducenotationandresults referringthereaderto Bellemareetal fortechnicaldetails Let beadensitymodelona nitespaceX and     theprobabilityassignedbythemodeltoxafterbeingtrainedonasequenceofstatesx xn Assume     forallx   Therecodingprobability     isthentheprobabilitythemodelwouldassigntoxifitweretrainedonthatsamexonemoretime Wecall learningpositiveif         forallx xn     Thepredictiongain PG of isPGn   log     log     Alearningpositive impliesPGn   forallx   Forlearningpositive wede nethepseudocountas Nn                   derivedfrompostulatingthatasingleobservationofx Xshouldleadtoaunitincreaseinpseudocount     Nn         Nn     where nisthepseudocounttotal Thepseudocountgen eralizestheusualstatevisitationcountfunctionNn   Undercertainassumptionson   pseudocountsgrowapproximatelylinearlywithrealcounts Crucially thepseudocountcanbeapproximatedusingthepredictiongainofthedensitymodel Nn   cid ePGn   cid Itsmainuseistode neanexplorationbonus Weconsiderareinforcementlearning RL agentinteractingwithanenvironmentthatprovidesobservationsandextrinsicrewards seeSutton Barto forathoroughexpositionoftheRLframework Totherewardatstepnweaddthebonusr   Nn   whichincentivizestheagenttotrytoreexperiencesur prisingsituations Quantitiesrelatedtopredictiongainhavebeenusedforsimilarpurposesintheintrinsicmotivationliterature Lopesetal wheretheymeasureanagent slearningprogress Oudeyeretal Althoughthepseudo countbonusisclosetothepredictiongain itisasymptoticallymoreconservativeandsupportedbystrongertheoreticalguarantees DensityModelsforImagesTheCTSdensitymodel Bellemareetal isbasedonthenamesakealgorithm ContextTreeSwitching Venessetal aBayesianvariableorderMarkovmodel Initssimplestform themodeltakesasinputa DimageandassignstoitaprobabilityaccordingtotheproductoflocationdependentL shaped lters wherethepredictionofeach lterisgivenbyaCTSalgorithmtrainedonpastimages InBellemareetal thismodelwasappliedto bitgreyscale downsampledAtari frames Fig TheCTSmodelpresentsadvantagesintermsofsimplicityandperformancebutislimitedinexpressiveness scalability anddataef ciency Inrecentyears neuralgenerativemodelsforimageshaveachievedimpressivesuccessesintheirabilitytogeneratediverseimagesinvariousdomains Kingma Welling Rezendeetal Gregoretal Goodfellowetal Inparticular vandenOordetal     introducedPixelCNN afullyconvolutionalneuralnetworkcomposedofresidualblockswithmultiplica tivegatingunits whichmodelspixelprobabilitiesconditionalonpreviouspixels intheusualtoplefttobottom rightrasterscanorder byusingmaskedconvolution lCount BasedExplorationwithNeuralDensityModelsFigure Atariframepreprocessing Bellemareetal ters Thismodelachievedstateof theartmodellingperfor manceonstandarddatasets pairedwiththecomputationalef ciencyofaconvolutionalfeedforwardnetwork MultiStepRLMethodsAdistinguishingfeatureofreinforcementlearningisthattheagent learnsonthebasisofinterimestimates Sutton Forexample theQLearningupdateruleisQ                 maxa                   linkingtherewardrandnextstatevaluefunctionQ     tothecurrentstatevaluefunctionQ     Thisparticularformisthestochasticupdaterulewithstepsize andinvolvestheTD error Intheapproximatereinforcementlearningsetting suchaswhenQ     isrepresentedbyaneuralnetwork thisupdateisconvertedintoalosstobeminimized mostcommonlythesquaredloss     Itiswellknownthatbetterperformance bothintermsoflearningef ciencyandapproximationerror isattainedbymultistepmethods Sutton Tsitsiklis vanRoy Thesemethodsinterpolatebetweenonestepmeth ods QLearning andtheMonteCarloupdateQ           Xt tr xt at         MC     wherex       isasamplepaththroughtheenvironmentbeginningin     ToachievetheirsuccessonthehardestAtari games Bellemareetal usedthemixedMonteCarloupdate MMC                 MC     with Thischoicewasmadefor computationalandimplementationalsimplicity andisaparticularlycoarsemulti stepmethod AbettermultistepmethodistherecentRetrace algorithm Munosetal Retrace usesaproductoftruncatedimportancesamplingratiosc   toreplace withtheerrorterm RETRACE     Xt   tYs cs xt at effectivelymixinginTDerrorsfromallfuturetimesteps Munosetal showedthatRetrace issafe doesnotdivergewhentrainedondatafromanarbitrarybehaviourpol icy andef cient makesthemostofmultistepreturns UsingPixelCNNforExplorationAsmentionedintheIntroduction thetheoryofusingdensitymodelsforexplorationmakesseveralassumptionsthattranslateintoconcreterequirementsforanimplementation   Thedensitymodelshouldbetrainedcompletelyonline     exactlyonceoneachstateexperiencedbytheagent inthegivensequentialorder   Thepredictiongain PG shoulddecayataraten toensurethatpseudocountsgrowapproximatelylin earlywithrealcounts   Thedensitymodelshouldbelearningpositive Simultaneously apartlycompetingsetofrequirementsareposedbythepracticalitiesoftraininganeuraldensitymodelandusingitaspartofanRLagent   Forstability ef ciency andtoavoidcatastrophicforgettinginthecontextofadriftingdatadistribution itisadvantageoustotrainaneuralmodelinminibatches drawnrandomlyfromadiversedataset   Foreffectivetraining acertainoptimizationregime       xedlearningrateschedule hastobefollowed   Thedensitymodelmustbecomputationallylightweight toallowcomputingthePG twomodelevaluationsandoneupdate aspartofeverytrainingstepofanRLagent WeinvestigatehowtobestresolvethesetensionsinthecontextoftheArcadeLearningEnvironment Bellemareetal asuiteofbenchmarkAtari games DesigningaSuitableDensityModelDrivenby   andaimingforanagentwithcomputationalperformancecomparabletoDQN wedesignaslimvariantofthePixelCNNnetwork Itscoreisastackof gatedresidualblockswith featuremaps comparedto residualblockswith featuremapsinvanillaPixelCNN AswasdonewiththeCTSmodel imagesaredownsampledto andquantizedto bitgreyscale SeeAppendixAfortechnicaldetails CountBasedExplorationwithNeuralDensityModelsFigure Left PixelCNNloglossonFREEWAY whentrainedonline onarandompermutation singleuseofeachframe oronrandomlydrawnsamples withreplacement potentiallyusingsameframemultipletimes fromthestatesequence Tosimulatetheeffectofnonstationarity theagent spolicychangesevery Kupdates Alltrainingmethodsshowqualitativelysimilarlearningprogressandstability Middle PixelCNNloglossover rst KtrainingframesonPONG Verticaldashedlinesindicateepisodeends Thecoincidinglossspikesarethedensitymodel   surprise uponobservingthedistinctivegreenframethatsometimesoccursattheepisodestart Right DQNPixelCNNtrainingperformanceonMONTEZUMA SREVENGEaswevarylearningrateandPGdecayschedules Figure Modellossaveragedover Kframes after Mtrainingframes forconstant   andn learningrateschedules Thesmallestlossisachievedbyaconstantlearningrateof TrainingtheDensityModelInsteadofusingrandomizedminibatches wetrainthedensitymodelcompletelyonlineonthesequenceofexperiencedstates Empiricallywefoundthatwithminortuningofoptimizationhyperparameterswecouldtrainthemodelasrobustlyonatemporallycorrelatedsequenceofstatesasonasequencewithrandomizedorder Fig left Besidessatisfyingthetheoreticalrequirement   completelyonlinetrainingofthedensitymodelhastheadvan tagethat     sothatthemodelupdateperformedforcomputingthePGneednotbereverted Anothermoresubtlereasonforavoidingminibatchup datesofthedensitymodel despite   isapracticaloptimizationissue The necessarilyonline computationofthePGinvolvesamodelupdateandhencetheuseofanoptimizer Advancedoptimizersusedwithdeepneuralnetworks liketheRMSPropoptimizer Tieleman Hinton usedinthiswork arestateful trackingrunningaveragesofe   meanandvarianceofthemodelparameters Ifthemodelisadditionallytrainedfromminibatches thetwostreamsofupdatesmayshowdifferentstatisticalchar TheCTSmodelallowsqueryingthePGcheaply withoutincurringanactualupdateofmodelparameters acteristics     differentgradientmagnitudes invalidatingtheassumptionsunderlyingtheoptimizationalgorithmandleadingtoslowerorunstabletraining Todetermineasuitableonlinelearningrateschedule wetrainthemodelonasequenceof Mframesofexperienceofarandompolicyagent Wecomparethelossachievedbytrainingproceduresfollowingconstantordecayinglearningrateschedules seeFig Thelowest naltraininglossisachievedbyaconstantlearningrateof oradecayinglearningrateof   Wesettledourchoiceontheconstantlearningratescheduleasitshowedgreaterrobustnesswithrespecttothechoiceofinitiallearningrate PixelCNNrapidlylearnsasensibledistributionoverstatespace Fig left showsthemodel slossdecayingasitlearnstoexploitimageregularities Spikesinitslossfunctionquicklystarttocorrespondtovisuallymeaningfulevents suchasthestartsofepisodes Fig middle Avideoofearlydensitymodeltrainingisprovidedinhttp youtu be   iaa   eyE Figure Samplesafter Ksteps Left CTS right PixelCNN ComputingthePseudoCountFromtheprevioussectionweobtainaparticularlearningrateschedulethatcannotbearbitrarilymodi edwithoutdeterioratingthemodel strainingperformanceorstability ToachievetherequiredPGdecay   weinsteadreplacePGnbycn PGnwithasuitablydecayingsequencecn CountBasedExplorationwithNeuralDensityModelsInexperimentscomparingactualagentperformanceweempiricallydeterminedthatinfacttheconstantlearningrate pairedwithaPGdecaycn     obtainsthebestexplorationresultsonhardexplorationgameslikeMONTEZUMA SREVENGE seeFig right We ndthemodeltoberobustacross ordersofmagnitudeforthevalueofc andinformallydeterminec tobeasensiblecon gurationforachievinggoodresultsonabroadrangeofAtari games seealsoSection Regarding   itishardtoensurelearningpositivenessforadeepneuralmodel andanegativePGcanoccurwhenevertheoptimizer overshoots alocallossminimum Asaworkaround wethresholdthePGvalueat Tosummarize thecomputedpseudocountis Nn   cid exp cid     PGn   cid cid ExplorationinAtari GamesHavingdescribedourpseudocountfriendlyadaptationofPixelCNN wenowstudyitsperformanceonAtarigames Tothisendweaugmenttheenvironmentrewardwithapseudocountexplorationbonus yieldingthecombinedrewardr     Nn   Asusualforneuralnetworkbasedagents weensurethetotalrewardliesin byclippinglargervalues DQNwithPixelCNNExplorationBonusOur rstsetofexperimentsprovidesthePixelCNNexplorationbonustoaDQNagent Mnihetal Ateachagentstep thedensitymodelreceivesasingleframe withwhichitsimultaneouslyupdatesitsparametersandoutputsthePG WerefertothisagentasDQNPixelCNN TheDQNCTSagentwecompareagainstisderivedfromtheonein Bellemareetal Forbettercomparability itistrainedinthesameonlinefashionasDQNPixelCNN     thePGiscomputedwheneverwetrainthedensitymodel Bycontrast theoriginalDQNCTSqueriedthePGattheendofeachepisode Unlessstatedotherwise wealwaysusethemixedMonteCarloupdate MMC fortheintrinsicallymotivatedagents butregularQLearningforthebaselineDQN Fig showstrainingcurvesofDQNcomparedtoDQN UnlikeBellemareetal weuseregularQLearninginsteadofDoubleQ Learning vanHasseltetal asourearlyexperimentsshowednosigni cantadvantageofDoubleDQNwiththePixelCNNbasedexplorationreward TheuseofMMCinareplaybasedagentposesaminorcom plication astheMCreturnisnotavailableforreplayuntiltheendofanepisode Forsimplicity inourimplementationwedisregardthisdetailandsettheMCreturnto fortransitionsfromthemostrecentepisode Figure DQN DQNCTSandDQN PixelCNNonhardexplorationgames top andeasierones bottom Figure Improvements in ofAUC ofDQNPixelCNNandDQN CTSoverDQNin Atarigames Annotationsindicatethenumberofhardexplorationgameswithpositive right andnegative left improvement respectively CTSandDQNPixelCNN OnthefamousMONTEZUMA SREVENGE bothintrinsicallymotivatedagentsvastlyoutperformthebaselineDQN Onotherhardexplorationgames PRIVATEEYE orVENTURE appendixFig DQNPixelCNNachievesstateoftheartresults substantiallyoutperformingDQNandDQN CTS Theothertwogamesshown ASTEROIDS BERZERK poseeasierexplorationproblems wheretherewardbonusshouldnotprovidelargeimprovementsandmayhaveanegativeeffectbyskewingtherewardlandscape Here DQNPixelCNNbe havesmoregracefullyandstilloutperformsDQNCTS Wehypothesizethisisduetoaqualitativedifferencebetweenthemodels seeSection OverallPixelCNNprovidestheDQNagentwithalargeradvantagethanCTS andoftenacceleratesorstabilizestrainingevenwhennotaffectingpeakperformance OutofCountBasedExplorationwithNeuralDensityModels Atarigames DQNPixelCNNoutperformsDQN CTSin gamesbymaximumachievedscore and byAUC methodologyinAppendixB SeeFig forahighlevelcomparison appendixFig forfulltraininggraphs Thegreatestgainsfromusingeitherexplorationbonusareobservedingamescategorizedashardexplorationgamesinthe taxonomyofexploration in Bellemareetal reproducedinAppendixD speci callyinthemostchallengingsparserewardgames     MONTEZUMA SREVENGE PRIVATEEYE VENTURE AMultiStepRLAgentwithPixelCNNEmpiricalpractitionersknowthattechniquesbene cialforoneagentarchitectureoftencanbedetrimentalforadifferentalgorithm TodemonstratethewideapplicabilityofthePixelCNNexplorationbonus wealsoevaluateitwiththemorerecentReactoragent Gruslysetal Thisreplaybasedactor criticagentrepresentsitspolicyandvaluefunctionbyarecurrentneuralnetworkand crucially usesthemultistepRetrace algorithmforpolicyevaluation replacingtheMMCweuseinDQNPixelCNN Toreduceimpactoncomputationalef ciencyofthisagent wesubsampleintrinsicrewards weperformupdatesofthePixelCNNmodelandcomputetherewardbonuson randomlychosen ofallsteps leavingtheagent srewardunchangedonothersteps WeusethesamePGdecayscheduleof   withnthenumberofmodelupdates Figure Reactor ReactorPixelCNNandDQN DQNPixelCNNtrainingperformance averagedover seeds TrainingcurvesfortheReactor ReactorPixelCNNagentcomparedtoDQN DQNPixelCNNareshowninFig ThebaselineReactoragentissuperiortotheDQNagent obtaininghigherscoresandlearningfasterinabout outof games ItisfurtherimprovedonalargefractionofgamesbythePixelCNNexplorationreward seeFig fulltraininggraphsinappendixFig Theeffectoftheexplorationbonusisratheruniform yieldingimprovementsonabroadrangeofgames Inparticu Theexactagentvariantisreferredtoas LOO with lar ReactorPixelCNNenjoysbettersampleef ciency intermsofareaunderthecurve AUC thanvanillaReactor Wehypothesizethat likeotherpolicygradientalgorithms ReactorgenerallysuffersfromweakerexplorationthanitsvaluebasedcounterpartDQN Thisaspectismuchhelpedbytheexplorationbonus boostingtheagent ssampleef ciencyinmanyenvironments Figure Improvements in ofAUC ofReactorPixelCNNoverReactorin Atarigames However onhardexplorationgameswithsparserewards Reactorseemsunabletomakefulluseoftheexplorationbonus Webelievethisisbecause inverysparsesettings thepropagationofrewardinformationacrosslonghorizonsbecomescrucial TheMMCtakesoneextremeofthisview directlylearningfromtheobservedreturns TheRetrace algorithm ontheotherhand hasaneffectivehorizonwhichdependson and critically thetruncatedimportancesamplingratio Thisratioresultsinthediscardingoftrajectorieswhichareoff policy     unlikelyunderthecurrentpolicy WehypothesizethattheverygoaloftheRetrace algorithmtolearncautiouslyiswhatpreventsitfromtakingfulladvantageoftheexplorationbonus QualityoftheDensityModelPixelCNNcanbeexpectedtobemoreexpressiveandaccuratethanthelessadvancedCTSmodel andindeed samplesgeneratedaftertrainingaresomewhathigherquality Fig However wearenotusingthegenerativefunctionofthemodelswhencomputinganexplorationbonus andabettergenerativemodeldoesnotnecessarilygiverisetobetterprobabilityestimates Theisetal Figure PGonMONTEZUMA SREVENGE logscale CountBasedExplorationwithNeuralDensityModelsInFig wecomparethePGproducedbythetwomodelsthroughout Ktrainingsteps PixelCNNconsistentlyproducesPGslowerthanCTS Moreimportantly itsPGsaresmoother exhibitinglessvariancebetweensuccessivestates whileshowingmorepronouncedpeaksatcertaininfrequentevents Thisyieldsarewardbonusthatislessharmfulineasyexplorationgames whileprovidingastrongsignalinthecaseofnovelorrareevents AnotherdistinguishingfeatureofPixelCNNisitsnondecayingstep size TheperstepPGnevercompletelyvan ishes asthemodeltracksthemostrecentdata Thisprovidesanunexpectedbene   theagentremainsmildlysurprisedbysigni cantstatechanges     switchingroomsinMONTEZUMA SREVENGE Thesepersistentrewardsactasmilestonesthattheagentlearnstoreturnto ThisisillustratedinFig depictingtheintrinsicrewardoverthecourseofanepisode Theagentroutinelyrevisitstherighthandsideofthetorchroom notbecauseitleadstorewardbutjustto takeinthesights Avideooftheepisodeisprovidedathttp youtu be tOUPKPoQ Figure IntrinsicrewardinMONTEZUMA SREVENGE Lastly PixelCNN sconvolutionalnatureisexpectedtobebene cialforitssampleef ciency InAppendixCwecomparetoaconvolutionalCTSandcon rmthatthisexplainspart butnotallofPixelCNN sadvantageovervanillaCTS ImportanceoftheMonteCarloReturnLikeforDQNCTS thesuccessofDQNPixelCNNhingesontheuseofthemixedMonteCarloupdate Thetransientandvanishingnatureoftheexplorationrewardsrequiresthelearningalgorithmtolatchontotheserapidly TheMMCservesthisendasasimplemultistepmethod helpingtopropagaterewardinformationfaster Anadditionalbene tliesinthefactthattheMonteCarloreturnhelpsbridginglonghorizonsinenvironmentswhererewardsarefarapartandencounteredrarely Ontheotherhand itis AnotheragentvideoonthegamePRIVATEEYEcanbefoundathttp youtu be kNyFygeUa   Figure Top gameswhereMMCcompletelyexplainstheimproved decreasedperformanceofDQNPixelCNNcomparedtoDQN Bottomleft MMCandPixelCNNshowadditivebene ts Bottomright hardexploration sparserewardgame onlycombiningMMCandPixelCNNbonusachievestrainingprogress importanttonotethattheMonteCarloreturn sonpolicynatureincreasesvarianceinthelearningalgorithm andcanpreventthealgorithm sconvergencetotheoptimalpolicywhentrainingoffpolicy Itcanthereforebeexpectedtoadverselyaffecttrainingperformanceinsomegames TodistilltheeffectoftheMMConperformance wecompareallfourcombinationsofDQNwith withoutPixelCNNexplorationbonusandwith withoutMMC Fig showstheperformanceofthesefouragentvariants graphsforallgamesareshowninFig Thesegameswerepickedtoillustrateseveralcommonlyoccurringcases MMCspeedsuptrainingandimproves nalperformancesigni cantly examples BANKHEIST TIMEPILOT Inthesegames MMCaloneexplainsmostoralloftheimprovementofDQNPixelCNNoverDQN MMChurtsperformance examples MS PACMAN BREAKOUT Heretoo MMCaloneexplainsmostofthedifferencebetweenDQNPixelCNNandDQN MMCandPixelCNNrewardbonushaveacompoundingeffect example         Mostimportantly thesituationisratherdifferentwhenwerestrictourattentiontothehardestexplorationgameswithsparserewards HerethebaselineDQNagentfailstomakeanytrainingprogress andneitherMonteCarloreturnnortheexplorationbonusaloneprovideanysigni cantbene   Theircombinationhowevergrantstheagentrapidtrainingprogressandallowsittoachievehighperformance Oneeffectoftheexplorationbonusinthesegamesistoprovideadenserrewardlandscape enablingtheagenttolearnmeaningfulpolicies Duetothetransientnatureoftheexplorationbonus theagentneedstobeabletolearnfromthisrewardsignalfasterthanregularonestepmeth odsallow andMMCprovestobeaneffectivesolution CountBasedExplorationwithNeuralDensityModelsFigure DQNPixelCNN hardexplorationgames differentPGscalesc   PGn   seedseach PushingtheLimitsofIntrinsicMotivationInthissectionweexploretheideaofa maximallycurious agent whoserewardfunctionisdominatedbytheexplorationbonus ForthatweincreasethePGscale previouslychosenconservativelytoavoidadverseeffectsoneasyexplorationgames Fig showsDQNPixelCNNperformanceonthehardestexplorationgameswhenthePGscaleisincreasedby ordersofmagnitude Thealgorithmseemsfairlyrobustacrossawiderangeofscales themaineffectofincreasingthisparameteristotradeoffexploration seekingmaximalreward withexploitation optimizingthecurrentpolicy Asexpected ahigherPGscaletranslatestostrongerexploration severalrunsobtainrecordpeakscores inGRAVITAR inMONTEZUMA SREVENGE inPRIVATEEYE inVENTURE surpassingthestateoftheartbyasubstantialmargin forpreviouslypublishedresults seeAppendixD Aggressivescalingspeedsuptheagent sexplorationandachievespeakperformancerapidly butcanalsodeteriorateitsstabilityandlongtermperfor mance Notethatinpractice becauseofthenondecayingstep sizethePGdoesnotvanish Afterrewardclipping anoverlyin atedexplorationbonuscanthereforebecomeessentiallyconstant nolongerprovidingausefulintrinsicmotivationsignaltotheagent Anotherwayofcreatinganentirelycuriositydrivenagentistoignoretheenvironmentrewardaltogetherandtrainbasedontheexplorationrewardonly seeFig Remarkably thecuriositysignalaloneissuf cienttotrainahighperformingagent measuredbyenvironmentreward Itisworthnotingthatagentswithexplorationbonusseemto neverstopexploring fordifferentseeds theagentsmakelearningprogressatverydifferenttimesduringtraining aqualitativedifferencetovanillaDQN ConclusionWedemonstratedtheuseofPixelCNNforexplorationandshowedthatitsgreateraccuracyandexpressivenesstranslateintoamoreusefulexplorationbonusthanthatobtainedfrompreviousmodels WhilethecurrenttheoryofpseudoFigure DQNPixelCNNtrainedfromintrinsicrewardonly seedsforeachcon guration countsputsstringentrequirementsonthedensitymodel wehaveshownthatPixelCNNcanbeusedinasimplerandmoregeneralsetup andcanbetrainedcompletelyonline Italsoprovestobewidelycompatiblewithbothvaluefunctionandpolicy basedRLalgorithms InadditiontopushingthestateoftheartonthehardestexplorationproblemsamongtheAtari games PixelCNNimprovesspeedoflearningandstabilityofbaselineRLagentsacrossawiderangeofgames Thequalityofitsrewardbonusisevidencedbythefactthatonsparserewardgames thissignalalonesuf cestolearntoachievesigni cantscores creatingatrulyintrinsicallymotivatedagent OuranalysisalsorevealstheimportanceoftheMonteCarloreturnforeffectiveexploration Thecomparisonwithmoresophisticatedbut xedhorizonmulti stepmethodsshowsthatitssigni canceliesbothinfasterlearninginthecontextofausefulbuttransientrewardfunction aswellasbridgingrewardgapsinenvironmentswhereextrinsicandintrinsicrewardsare orquicklybecome extremelysparse CountBasedExplorationwithNeuralDensityModelsAcknowledgementsTheauthorsthankTomSchaul OlivierPietquin IanOsband SriramSrinivasan TejasKulkarni AlexGraves CharlesBlundell andShimonWhitesonforinvaluablefeedbackontheideaspresentedhere andAudrunasGruslysespeciallyforprovidingtheReactoragent ReferencesBellemare Marc Veness Joel andTalvitie Erik Skipcontexttreeswitching ProceedingsoftheInternationalConferenceonMachineLearning Bellemare MarcG Naddaf Yavar Veness Joel andBowling Michael Thearcadelearningenvironment Anevaluationplatformforgeneralagents JournalofArti cialIntelligenceResearch Bellemare MarcG Srinivasan Sriram Ostrovski Georg Schaul Tom Saxton David andMunos   emi Unifyingcount basedexplorationandintrinsicmotivation AdvancesinNeuralInformationProcessingSystems Deisenroth MarcP andRasmussen CarlE PILCO Amodelbasedanddata ef cientapproachtopolicysearch InProceedingsoftheInternationalConferenceonMachineLearning French RobertM Catastrophicforgettinginconnectionistnetworks Trendsincognitivesciences Goodfellow Ian PougetAbadie Jean Mirza Mehdi Xu Bing WardeFarley David Ozair Sherjil Courville Aaron andBengio Yoshua Generativeadversarialnets InAdvancesinNeuralInformationProcessingSystems Gregor Karol Danihelka Ivo Graves Alex Rezende Danilo andWierstra Daan Draw Arecurrentneuralnetworkforimagegeneration InProceedingsoftheInternationalConferenceonMachineLearning Gruslys Audrunas Azar MohammadGheshlaghi Bellemare MarcG andMunos   emi TheReactor Asampleef cientactorcriticarchitecture arXivpreprintarXiv Guez Arthur Silver David andDayan Peter Ef cientbayesadaptivereinforcementlearningusingsample basedsearch InAdvancesinNeuralInformationProcessingSystems Houthooft Rein Chen Xi Duan Yan Schulman John DeTurck Filip andAbbeel Pieter Variationalinformationmaximizingexploration InAdvancesinNeuralInformationProcessingSystems NIPS Jaksch Thomas Ortner Ronald andAuer Peter Nearoptimalregretboundsforreinforcementlearning JournalofMachineLearningResearch Kingma DiederikP andWelling Max Autoencodingvariationalbayes InProceedingsoftheInternationalConferenceonLearningRepresentations Lopes Manuel Lang Tobias Toussaint Marc andOudeyer PierreYves Explorationinmodelbasedre inforcementlearningbyempiricallyestimatinglearningprogress InAdvancesinNeuralInformationProcessingSystems Mnih Volodymyr Kavukcuoglu Koray Silver David Rusu AndreiA Veness Joel Bellemare MarcG Graves Alex Riedmiller Martin Fidjeland AndreasK Ostrovski Georg etal Humanlevelcontrolthroughdeepreinforcementlearning Nature Munos   emi Stepleton Tom Harutyunyan Anna andBellemare MarcG Safeandef cientoffpolicyrein forcementlearning InAdvancesinNeuralInformationProcessingSystems Osband Ian vanRoy Benjamin andWen Zheng Generalizationandexplorationviarandomizedvaluefunc tions InProceedingsoftheInternationalConferenceonMachineLearning Oudeyer PierreYves Kaplan Fr ed eric andHafner VerenaV Intrinsicmotivationsystemsforautonomousmentaldevelopment IEEETransactionsonEvolutionaryComputation Rezende DaniloJimenez Mohamed Shakir andWierstra Daan Stochasticbackpropagationandapproximateinferenceindeepgenerativemodels InProceedingsofTheInternationalConferenceonMachineLearning Strehl AlexanderL andLittman MichaelL AnanalysisofmodelbasedintervalestimationforMarkovdecisionprocesses JournalofComputerandSystemSciences Sutton RichardS Generalizationinreinforcementlearning Successfulexamplesusingsparsecoarsecoding InAdvancesinNeuralInformationProcessingSystems Sutton RichardS andBarto AndrewG Reinforcementlearning Anintroduction MITPress Tang Haoran Houthooft Rein Foote Davis Stooke Adam Chen Xi Duan Yan Schulman John DeTurck CountBasedExplorationwithNeuralDensityModelsFilip andAbbeel Pieter Exploration Astudyofcountbasedexplorationfordeepreinforcementlearn ing arXivpreprintarXiv Theis Lucas vandenOord   aron andBethge Matthias Anoteontheevaluationofgenerativemodels InProceedingsoftheInternationalConferenceonLearningRepresentations Tieleman TijmenandHinton Geoffrey RMSProp dividethegradientbyarunningaverageofitsrecentmagnitude COURSERA Lecture ofNeuralNetworksforMachineLearning Tsitsiklis JohnN andvanRoy Benjamin Ananalysisoftemporaldifferencelearningwithfunctionapproxima tion IEEETransactionsonAutomaticControl vandenOord Aaron Kalchbrenner Nal Espeholt Lasse Vinyals Oriol Graves Alex etal ConditionalimagegenerationwithPixelCNNdecoders InAdvancesinNeuralInformationProcessingSystems   vandenOord Aaron Kalchbrenner Nal andKavukcuoglu Koray Pixelrecurrentneuralnetworks InProceedingsoftheInternationalConferenceonMachineLearning   vanHasselt Hado Guez Arthur andSilver David DeepreinforcementlearningwithDoubleQlearning InProceedingsoftheAAAIConferenceonArti cialIntelligence Veness Joel Ng KeeSiong Hutter Marcus andBowling MichaelH Contexttreeswitching InProceedingsoftheDataCompressionConference Wang Ziyu Schaul Tom Hessel Matteo vanHasselt Hado Lanctot Marc anddeFreitas Nando Duelingnetworkarchitecturesfordeepreinforcementlearning InProceedingsofThe rdInternationalConferenceonMachineLearning pp 