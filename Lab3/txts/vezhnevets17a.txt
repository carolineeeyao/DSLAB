FeUdal Networks for Hierarchical Reinforcement Learning

Alexander Sasha Vezhnevets   Simon Osindero   Tom Schaul   Nicolas Heess   Max Jaderberg   David Silver  

Koray Kavukcuoglu  

Abstract

We introduce FeUdal Networks  FuNs    novel
architecture for hierarchical reinforcement learning  Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton  and gains power and ef cacy by decoupling endto end learning across multiple levels
  allowing it to utilise different resolutions of
time  Our framework employs   Manager module and   Worker module  The Manager operates
at   lower temporal resolution and sets abstract
goals which are conveyed to and enacted by the
Worker  The Worker generates primitive actions
at every tick of the environment  The decoupled
structure of FuN conveys several bene ts   in addition to facilitating very long timescale credit
assignment it also encourages the emergence of
subpolicies associated with different goals set
by the Manager  These properties allow FuN to
dramatically outperform   strong baseline agent
on tasks that involve longterm credit assignment
or memorisation 

  Introduction
Deep reinforcement learning has recently enjoyed successes in many domains  Mnih et al    Schulman
et al    Levine et al    Mnih et al    Lillicrap
et al    Nevertheless  longterm credit assignment remains   major challenge for these methods  especially in
environments with sparse reward signals  such as the infamous Montezuma   Revenge ATARI game  It is symptomatic that the standard approach on the ATARI benchmark suite  Bellemare et al    is to use an actionrepeat heuristic  where each action translates into several
 usually   consecutive actions in the environment  Yet another dimension of complexity is seen in nonMarkovian
environments that require memory   these are particularly

 DeepMind  London  United Kingdom  Correspondence to 

Alexander Sasha Vezhnevets  vezhnick google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

challenging  since the agent has to learn which parts of experience to store for later  using only   sparse reward signal 
The framework we propose takes inspiration from feudal
reinforcement learning  FRL  introduced by Dayan   Hinton   where levels of hierarchy within an agent communicate via explicit goals  Some key insights from FRL
are that goals can be generated in   topdown fashion  and
that goal setting can be decoupled from goal achievement 
  level in the hierarchy communicates to the level below
it what must be achieved  but does not specify how to do
so  Making higher levels reason at   lower temporal resolution naturally structures the agents behaviour into temporally extended subpolicies 
The architecture explored in this work is   fullydifferentiable neural network with two levels of hierarchy
 though there are obvious generalisations to deeper hierarchies  The top level  the Manager  sets goals at   lower
temporal resolution in   latent statespace that is itself
learnt by the Manager  The lower level  the Worker  operates at   higher temporal resolution and produces primitive
actions  conditioned on the goals it receives from the Manager  The Worker is motivated to follow the goals by an
intrinsic reward  However  signi cantly  no gradients are
propagated between Worker and Manager  the Manager receives its learning signal from the environment alone  In
other words  the Manager learns to select latent goals that
maximise extrinsic reward 
The key contributions of our proposal are      consistent 
endto end differentiable model that embodies and generalizes the principles of FRL      novel  approximate transition policy gradient update for training the Manager  which
exploits the semantic meaning of the goals it produces   
The use of goals that are directional rather than absolute in
nature      novel RNN design for the Manager     dilated LSTM   which extends the longevity of the recurrent
state memories and allows gradients to  ow through large
hops in time  enabling effective backpropagation through
hundreds of steps 
Our ablative analysis  Section   con rms that transitional policy gradient and directional goals are crucial
for best performance  Our experiments on   selection of
ATARI games  including the infamous Montezuma   re 

FeUdal Networks for Hierarchical Reinforcement Learning

venge  and on several memory tasks in the    DeepMind
Lab environment  Beattie et al    show that FuN signi cantly improves longterm credit assignment and memorisation 
  Related Work
Building hierarchical agents is   long standing topic in reinforcement learning  Sutton et al    Precup   
Dayan   Hinton    Dietterich    Boutilier et al 
  Dayan    Kaelbling    Parr   Russell   
Precup et al      Schmidhuber    Sutton 
  Wiering   Schmidhuber    Vezhnevets et al 
  Bacon et al    The options framework  Sutton et al    Precup    is   popular formulation for
considering the problem with   two level hierarchy  The
bottom level   an option   is   subpolicy with   termination condition  which takes in environment observations
and outputs actions until the termination condition is met 
An agent picks an option using its policyover options  the
top level  and subsequently follows it until termination  at
which point the policyover options is queried again and
the process continues  Options are typically learned using
subgoals and  pseudorewards  that are provided explicitly  Sutton et al    Dietterich    Dayan   Hinton 
  For   simple  tabular case  Wiering   Schmidhuber    Schaul et al    each state can be used as  
subgoal  Given the options    policyover options can be
learned using standard techniques by treating options as actions  Recently  Tessler et al    Kulkarni et al   
have demonstrated that combining deep learning with prede ned subgoals delivers promising results in challenging
environments like Minecraft and Atari  however subgoal
discovery was not addressed 
  recent work of  Bacon et al    shows the possibility of learning options jointly with   policyover options
in an endto end fashion by extending the policy gradient theorem to options  When options are learnt endto 
end  they tend to degenerate to one of two trivial solutions 
    only one active option that solves the whole task   ii 
  policyover options that changes options at every step 
micromanaging the behaviour  Consequently  regularisers  Bacon et al    Vezhnevets et al    are usually
introduced to steer the solution towards multiple options of
extended length  This is believed to provide an inductive
bias towards reusable temporal abstractions and to help
generalisation 
  key difference between our approach and the options
framework is that in our proposal the top level produces
  meaningful and explicit goal for the bottom level to
achieve  Subgoals emerge as directions in the latent statespace and are naturally diverse  We also achieve significantly better scores on ATARI than OptionCritic  section  

Figure   The schematic illustration of FuN  section  

There has also been   signi cant progress in nonhierarchical deep RL methods by using auxiliary losses and
rewards   Bellemare et al      have signi cantly advanced the stateof theart on Montezuma   Revenge by using pseudocount based auxiliary rewards for exploration 
which stimulate agents to explore new parts of the state
space  The recently proposed UNREAL agent  Jaderberg
et al    also demonstrates   strong improvement by using unsupervised auxiliary tasks to help re ne its internal
representations  We note that these bene ts are orthogonal
to those provided by FuN  and that both approaches could
be combined with FuN for even greater effect 
  The model
What is FuN  FuN is   modular neuralnetwork consisting of two modules   the Worker and the Manager  The
Manager internally computes   latent state representation
st and outputs   goal vector gt  The Worker produces actions conditioned on external observation  its own state  and
the Managers goal  The Manager and the Worker share
  perceptual module which takes an observation from the
environment xt and computes   shared intermediate representation zt  The Manager   goals gt are trained using an
approximate transition policy gradient  This is   particularly ef cient form of policy gradient training that exploits
the knowledge that the Worker   behaviour will ultimately
align with the goal directions it has been set  The Worker
is then trained via intrinsic reward to produce actions that
cause these goal directions to be achieved  Figure    illustrates the overall design and the following equations describe the forward dynamics of our network 

zt   fpercept xt  st   fMspace zt 
   gt    gt gt 
  cid 
     gt   fMrnn st  hM
hM

     

wt    

gi 

hW   Ut   fWrnn zt  hW

        SoftMax Utwt 

 

 

 

 

where both the Manager and the Worker are recurrent  Here
hM and hW correspond to the internal states of the Man 

   Mrnnf  perceptf  Wrnn cid wt Rkx Ut     xkXzt RdatNo gradient cid cid rt cid cos St cSt gt Policy gradientk      st Rdf  MspacextWorkerManageractiongoal RtVMt   cid cid cos st cst gt Transition policy gradientgt RdPolicy gradientTransition policy gradientFeUdal Networks for Hierarchical Reinforcement Learning

ager and the Worker respectively    linear transform  
maps   goal gt into an embedding vector wt   Rk  which is
then combined via product with matrix Ut  Workers output 
to produce policy     vector of probabilities over primitive
actions  The next section provides the details on goal embedding and the following sections   describes how
FuN is trained 
  Goal embedding
The goal   modulates the policy via   multiplicative
interaction in   low dimensional goalembedding space
Rk         The Worker  rst produces an embedding
vector for every action  represented by rows of matrix
           eq    To incorporate goals from the Manager  the last   goals are  rst pooled by summation and then
embedded into   vector     Rk using   linear projection
   eq    The projection   is linear  with no biases  and
is learnt with gradients coming from the Worker   actions 
The embedding matrix   is then combined with the goal
embedding   via   matrixvector product  eq    Since
  has no biases it can never produce   constant nonzero
vector   which is the only way the setup could ignore the
Manager   input  This makes sure that the goal output by
the Manager always in uences the  nal policy  Notice how 
due to pooling of goals over several timesteps  the conditioning from the Manager varies smoothly 
  Learning
We consider   standard reinforcement learning setup  At
each step    the agent receives an observation xt from the
environment and selects an action at from    nite set of
possible actions  The environment responds with   new observation xt  and   scalar reward rt  The process continues until the terminal state is reached  after which it restarts 
The goal of the agent is to maximise the discounted return
    krt    with         The agent   behaviour is de ned by its actionselection policy   FuN
produces   distribution over possible actions    stochastic
policy  as de ned in eq   
The conventional wisdom would be to train the whole architecture monolithically through gradient descent on either the policy directly or via TDlearning  Notice  that
since FuN is fully differentiable we could train it endto 
end using   policy gradient algorithm operating on the actions taken by the Worker  The outputs   of the Manager
would be trained by gradients coming from the Worker 
This  however would deprive Manager   goals   of any semantic meaning  making them just internal latent variables
of the model  We propose instead to independently train
Manager to predict advantageous directions  transitions  in
state space and to intrinsically reward the Worker to follow
these directions  If the Worker can ful   the goal of moving in these directions  as it is rewarded for doing  then we
ought to end up taking advantageous trajectories through

Rt    cid 

statespace  We formalise this in the following update rule
for the Manager 

   dcos st     st  gt 

 

 gt   AM
    Rt      

 

  cid 

 

where AM
 xt    is the Manager   advantage function  computed using   value function estimate    
 xt    from the internal critic  dcos     
     is the cosine similarity between two vectors 
Note  the dependence of   on   is ignored when computing
 dcos   this avoids trivial solutions  Notice that now gt
acquires   semantic meaning as an advantageous direction
in the latent state space at   horizon    which de nes the
temporal resolution of the Manager 
The intrinsic reward that encourages the Worker to follow
the goals is de ned as 

rI
      

dcos st   st    gt   

 

  

We use directions because it is more feasible for the Worker
to be able to reliably cause directional shifts in the latent
state than it is to assume that the Worker can take us to
 potentially  arbitrary new absolute locations  It also gives
  degree of invariance to the goals and allows for structural
generalisation   the same directional subgoal   can invoke
  subpolicy that is valid and useful in   large part of the
latent state space       evade an enemy  swim up for air  etc 
We compare absolute against directional goals empirically
in section  
The original feudal reinforcement learning formulation
of Dayan   Hinton   advocated completely concealing the reward from the environment from lower levels of
hierarchy  In practice we take   softer approach by adding
an intrinsic reward for following the goals  but retaining the
environment reward as well  The Worker is then trained to
maximise   weighted sum Rt    RI
    where   is   hyperparameter that regulates the in uence of the intrinsic reward  The Workers policy   can be trained to maximise
intrinsic reward by using any offthe shelf deep reinforcement learning algorithm  Here we use an advantage actor
critic  Mnih et al   
     AD

 
   
The advantage function estimator AD
   xt    is calculated using an internal critic  which esV  
timates the value functions for both rewards 
Note that the Worker and Manager can potentially have different discount factors   for computing the return  This allows  for instance  the Worker to be more greedy and focus
on immediate rewards while the Manager can consider  
longterm perspective 

    log  at xt   

     Rt    RI

FeUdal Networks for Hierarchical Reinforcement Learning

  Transition Policy Gradients
We now motivate our proposed update rule for the Manager
as   novel form of policy gradient with respect to   model
of the Worker   behaviour  Consider   highlevel policy
ot    st    that selects among subpolicies  possibly
from   continuous set  where we assume for now that these
subpolicies are  xed duration behaviours  lasting for  
steps  Corresponding to each subpolicy is   transition distribution    st   st  ot  that describes the distribution of
states that we end up at the end of the subpolicy  given the
start state and the subpolicy enacted  The highlevel policy
can be composed with the transition distribution to give  
 transition policy        st   st      st   st   st    describing the distribution over end states given start states 
It is valid to refer to this as   policy because the original
MDP is isomorphic to   new MDP with policy      and
transition function st           st        the state always
transitions to the end state picked by the transition policy 
As   result  we can apply the policy gradient theorem to the
transition policy        so as to  nd the performance gradient with respect to the policy parameters 
    

       Rt      st  log   st   st   st   
 
In general  the Worker may follow   complex trajectory   
naive application of policy gradients requires the agent to
learn from samples of these trajectories  But if we know
where these trajectories are likely to end up  by modelling
the transitions  then we can skip directly over the Worker  
behaviour and instead follow the policy gradient of the predicted transition  FuN assumes   particular form for the
transition model  that the direction in statespace  st   st 
follows   von MisesFisher distribution  Speci cally  if the
mean direction of the von MisesFisher distribution is given
by   ot   which for compactness we write as gt  we would
have   st   st  ot    edcos st   st gt  If this functional
form were indeed correct  then we see that our proposed
update heuristic for the Manager  eqn  is in fact the proper
form for the transition policy gradient arrived at in eqn 
Note that the Worker   intrinsic reward  eqn    is based
on the loglikelihood of state trajectory  Through that the
FuN architecture actively encourages the functional form
of the transition model to hold true  Because the Worker is
learning to achieve the Manager   direction  its transitions
should  over time  closely follow   distribution around this
direction  and hence our approximation for transition policy gradients should hold reasonably well 
  Architecture details
This section provides the particular details of the model as
described in section   The perceptual module fpercept is
  convolutional network  CNN  followed by   fully connected layer  The CNN has    rst layer with          

ters of stride   followed by   layer with with        lters of stride   The fully connected layer has   hidden
units  Each convolutional and fullyconnected layer is followed by   recti er nonlinearity  The state space which
the Manager implicitly models in formulating its goals is
computed via fMspace  which is another fully connected
layer followed by   recti er nonlinearity  The dimensionality of the embedding vectors     is set as       To encourage exploration in transition policy  at every step with
  small probability   we emit   random goal sampled from
  univariate Gaussian 
The Worker   recurrent network fWrnn is   standard
LSTM  Hochreiter   Schmidhuber    For the Manager   recurrent network  fMrnn  we propose   novel design   the dilated LSTM  which is introduced in the next
section  Both fMrnn and fWrnn have   hidden units 
  Dilated LSTM
We propose   novel RNN architecture for the Manager 
which operates at lower temporal resolution than the data
stream  The main contribution here is the inductive bias towards slowly varying outputs  which have very longterm
temporal dependencies  We de ne   dilated LSTM analogously to dilated convolutional networks  Yu   Koltun 
  For   dilation radius   let the full state of the network be      hi  
it is composed of   separate groups of substates or  cores  At time   the network is governed by the following equations   ht  
  gt  
    LSTM  where   denotes the modulo opLSTM st   ht  
eration and allows us to indicate which group of cores is
currently being updated  We make the parameters of the
LSTM network  LSTM explicit to stress that the same set
of parameters governs the update for each of the   groups
within the dLSTM 
At each time step only the corresponding part of the state is
updated and the output is pooled across the previous   outputs  This allows the   groups of cores inside the dLSTM
to preserve the memories for long periods  yet the dLSTM
as   whole is still able to process and learn from every input experience  and is also able to update its output at every step  This idea is similar to clockwork RNNs  Koutn  
et al    however there the top level  ticks  at    xed 
slow pace  whereas the dLSTM observes all the available
training data instead  In the experiments we set      
and this was also used as the predictions horizon    
  Experiments
The goal of our experiments is to demonstrate that FuN
learns nontrivial  helpful  and interpretable subpolicies

       

 

 This is substantially the same CNN as in  Mnih et al   
  the only difference is that in the preprocessing stage we
retain all colour channels 

FeUdal Networks for Hierarchical Reinforcement Learning

   

   

Figure      Learning curve on Montezuma   Revenge    This is   visualisation of subgoals learnt by FuN in the  rst room  For each time
step we compute the latent state st and the corresponding goal gt  We then  nd   future state for which cos st   st  gt  is maximized 
The plot corresponds to the number of past states for which   frame maximizes the goal        the taller the bar  the more frequently that
state was   maximizer of the goal for some previous state  Notice that FuN has learnt   semantically meaningful subgoals   the tall bars
in the plot       consistent goals  correspond to interpretably useful waypoints in Montezuma 
and subgoals  and also to validate components of the architecture  We start by describing technical details of the experimental setup and then present results on Montezuma  
revenge   an infamously hard ATARI game   in section  
Section   presents results on more ATARI games and
extensively compares FuN to LSTM baseline with different discount factors and BPTT lengths  In section   we
present results on   set of visual memorisation tasks in
   environment  Section   presents an ablation study of
FuN  validating our design choices 
Baseline  Our main baseline is   recurrent LSTM network on top of   representation learned by   CNN  The
LSTM  Hochreiter   Schmidhuber    architecture is
  widely used recurrent network and it was demonstrated
to perform very well on   suite of reinforcement learning problems  Mnih et al    LSTM uses   hidden units  and its inputs are the feature representation of
an observation and the previous action of the agent  Action
probabilities and the value function estimate are regressed
from its hidden state  All the methods the same CNN architecture  input preprocessing  and an action repeat of  
Optimisation  We use the     method  Mnih et al 
  for all reinforcement
It
was shown to achieve stateof theart results on several challenging benchmarks  Mnih et al    We
cut the trajectory and run backpropagation through time
 BPTT   Mozer    after   forward passes of   network or if   terminal signal is received  For FuN    
  for LSTM  unless otherwise stated        We
discuss different choice of   for LSTM in section  
The optimization process runs   asynchronous threads using shared RMSProp  There are   hyperparameters in

FuN and   in the LSTM baselines  For each method 
we ran   experiments  each using randomly sampled
hyperparameters 
Learning rate and entropy penalty
were sampled from   LogUniform    interval
for LSTM  For FuN the learning rate was sampled from
LogUniform    to account for higher gradients due to longer BPTT unrolls  The learning rate was
linearly annealed from   sampled value to half the initial
rate for all agents  To explore intrinsic motivation in FuN 
we sample its weight     Uniform    We de ne  
training epoch as one million observations  When reporting learning curves  we plot the average episode score of
the top   agents  according to the  nal score  against the
training epochs  For all ATARI experiments we clip the
reward to     interval
  Montezuma   revenge
Montezuma   revenge is one of the hardest games available through the ALE  Bellemare et al    The game
is infamous for challenging agents with lethal traps and
sparse rewards  We had to broaden and intensify our
hyperparameter search for the LSTM baseline to see any
progress at all for that model  We have experimented with
many different hyperparameter con gurations for LSTM
baseline  for instance expanding learning rate search to
LogUniform    and we report on the con guration that worked best  We use   small discount   for
LSTM  for FuN we use   in Worker and   in Manager  Figure    analyses the subgoals learnt by FuN in
the  rst room  They turn out to be meaningful milestones 
which bridge the agents progress to its  rst extrinsic reward   picking up the key  Interestingly  two of the learnt
subgoals correspond to roughly the same locations as the
ones handcrafted in  Kulkarni et al     ladder and
key  but here they are learnt by the agent itself  Figure   

learning experiments 

 This choice means that FuN and the LSTM baseline to have

roughly the same number of total parameters 

 Training epochs Scoremontezuma revengeFuN   LSTM   Time stepGoal countstart FeUdal Networks for Hierarchical Reinforcement Learning

Figure   ATARI training curves  Epochs corresponds to   million training steps of an agent  The value is the average per episode score
of top   agents  according to the  nal score  We used two different discount factors   and  

Figure   FuN in water maze  top down view  The left plot visualises FuN trajectories during one episode  The  rst trajectory
 green  performs   search for the target in different locations 
while subsequent ones  other colours  perform searches along  
circle of    xed radius matched to that of the target  always  nding
the target  The right plot visualises different learnt subpolicies 
produced by sampling   random   and  xing it for   steps  Each
colour corresponds to   different    the black circle represents the
starting location 

plots the learning curves  Notice how FuN starts learning
much earlier and achieves much higher scores 
It takes
    epochs for LSTM to reach the score   which
corresponds to solving the  rst room  take the key  open
  door  it stagnates at that score until about   epochs 
when it starts exploring further  FuN solves the  rst room
in less than   epochs and immediately moves on to explore further  eventually visiting several other rooms and
scoring up to   points 
  ATARI
Experiments in this section validate that the capabilities of
FuN go beyond what standard tools for longterm credit
assignment   discount factors and BPTT unroll length  
can provide for   baseline LSTM agent  We use two discounts   and   for both FuN and LSTM agents 
 For the experiments on FuN only the discount for the
Manager changes  while the Worker   discount is  xed at

Figure   Training curves for memory tasks on Labyrinth 

  For the LSTM we explore BPTT of   and  
while for FuN we use   BPTT unroll of   For LSTM
with BPTT   we search for learning rate in the interval LogUniform    as for FuN  We use   diverse set of ATARI games  some of which involve longterm credit assignment and some which are more reactive 
Figure   plots the learning curves    few categories
emerge  On Ms  Pacman  Amidar  and Gravitar FuN with
  low Manager discount of   strongly outperforms all
other methods  All of these games are known to require
longterm reasoning to play well  Enduro stands out as all
the LSTM agents completely fail at it 
In this game the
agent controls   racing car and scores points for overtaking other racers  this requires accelerating and steering for
signi cant amount of time before the  rst reward is experienced  Frostbite is   hard game  Vezhnevets et al   
Lake et al    that requires both longterm credit assignment and good exploration  The bestperforming frost 

 Scorems pacman amidar gravitarFuN   FuN   LSTM   LSTM   LSTM    BPTT enduro frostbite Training epochs Scorespace invaders Training epochs hero Training epochs seaquest Training epochs alien Training epochs breakoutGoal ScorenonmatchFuNLSTM TmazeFuNLSTM Training epochs Tmaze FuNLSTM Training epochs Water mazeFuNLSTMScoreFeUdal Networks for Hierarchical Reinforcement Learning

bite agent is FuN with   Manager discount  which outperforms the rest by   factor of   On Hero and Space
Invaders all agents perform equally well  On Seaquest
and Breakout  the baseline LSTM with   more aggressive
discount of   is the best  This suggests that in these
games longterm credit assignment is not important and
the agent is better off optimising more immediate rewards
in   greedy fashion  Alien is the only game where using different discounts doesn   meaningfully in uence the
agents performance  here we see the baseline LSTM outperforms our FuN model  although both still achieve   satisfactory scores  We provide qualitative analysis of subpolicies learnt on Seaquest in supplementary material 
Note how using an unroll for BPTT  in the baseline
LSTM signi cantly hurts its performance  hence we do not
explore longer unrolls  while FuN performs very well with
BPTT of   thanks to its ability to leverage the dLSTM 
Being able to train   recurrent network over very long sequences could be an enabling tool for many memory related
task  as we demonstrate in section  

Optioncritic architecture
 Bacon et al    is  to
the best of our knowledge  the only other endto end trainable system with subpolicies  The experimental results for
OptionCritic on   ATARI  Bacon et al    games show
scores similar those from    at DQN  Mnih et al   
baseline agent  Notice that our baseline  Mnih et al   
is much stronger than DQN  We also ran FuN on the same
games as OptionCritic  Asterix  Ms  Pacman  Seaquest
and Zaxxon  and after   epochs it achieves   similar
score on Seaquest  doubles it on Ms  Pacman  more than
triples it on Zaxxon and gets more than    improvement
on Asterix  see supplementary material for plots 

Figure   Ablative analysis

  Memory in Labyrinth
DeepMind Lab  Beattie et al    is    rstperson   
game platform extended from OpenArena  It     visually
complex    environment with agent actions corresponding
to movement and orientation  We use   different levels that
test longterm credit assignment and visual memory 

Water maze
is   reproduction of the Morris water maze
experiment  Morris    from the behavioural science literature  An agent is dropped into   circular pool of water with   concealed platform at unknown random location 
The agent can move around and upon stepping on the platform it receives   reward and the trial restarts  The platform
remains in the same location for the rest of the episode 
while agent starts each trial at   random location  The walls
of the pool are decorated with visual cues to assist localisation 

Tmaze
is another classic animal cognition test  The
agent spawns in   small Tshaped maze  Two objects with
randomly chosen shape and colour are spawned at the left
and right  baiting  locations  One of them is assigned   reward of   and the other   reward of   When the agent
collects one of the objects  it receives the reward and is respawned at the beginning of the Tmaze  The objects are
also reinstantiated in the same locations and with the same
rewards on the respawn event  The agent should remember which object gives the positive reward across respawns
and collect it as many times as possible within the  xed
time given for the episode  Tmaze  is   modi cation of
Tmaze  where at each trial the length of corridors can vary 
adding additional dimension of complexity 

Nonmatch is   visual memorisation task  Each trial begins in small room with an out of reach object being displayed in one of two display pods  There is   pad in the
middle  which upon touching  the agent is rewarded with  
point  and is teleported to   second room which has two objects in it  one of which matches the object in the previous
room  Collecting the matching object gives   reward of  
points  collecting the non matching object gives   reward of
  points  Once either is collected  the agent is teleported
back to the  rst room  with the same object being shown 
For all agents we include reward as   part of the observation  Figure   plots the learning curves  FuN consitently
outperforms the LSTM baseline   it learns faster and also
reaches   higher  nal reward  We analyse the FuN agent  
behaviour in more detail in Figure     It demonstrates that
FuN learns meaningful subpolicies  which are then ef 
ciently integrated with memory to produce rewarding behaviour  Interestingly  the LSTM agent doesn   appear to
use its memory for water maze task at all  always circling
the maze at the roughly the same radius 

 Training epochs gravitar Training epochs enduro Scorems pacman amidarFuNNon feudal FuNPure feudal FuNManager via PGAbsolute goalsScoreFeUdal Networks for Hierarchical Reinforcement Learning

Figure   Action repeat transfer

  Ablative analysis
This section empirically validates the main innovations of
this paper  transition policy gradient for training the Manager  relative rather than absolute goals  intrinsic motivation for the Worker  First we consider    nonFeudal  FuN  
it has exactly the same network architecture as FuN  but the
Managers output   is trained with gradients coming directly
from the Worker and no intrinsic reward is used  much like
in OptionCritic architecture  Bacon et al    Second 
  is learnt using   standard policy gradient approach with
the Manager emitting the mean of   Gaussian distribution
from which goals are sampled  as if the Manager were solving   continuous control problem  Schulman et al   
Mnih et al    Lillicrap et al    Third  we explore   variant of FuN in which   speci es absolute  rather
than relative directional  goals  and the Worker   intrinsic
reward is adjusted accordingly  but otherwise everything is
the same  The experiments  Figure   reveal that  although
alternatives do work to some degree their performance is
signi cantly inferior  We also evaluate   purely feudal version of FuN   in which the Worker is trained from the intrinsic reward alone  This ablation performs better than
other  but still inferior to the full FuN approach  It shows
that allowing the Worker to experience the external reward
is bene cial 
  ATARI action repeat transfer
One of the advantages of FuN is the clear separation of duties between Manager and Worker  The Manager learns  
transition policy  while the Worker learns to operate primitive actions to enact these transitions  This transition policy
is invariant to the underlying embodiment of the agent   the
way its primitive actions translate into state space transitions  Potentially  the transition policy can be transferred
between agents with different embodiment       
robot
models with different bodies or different operational frequency  We provide evidence towards that possibility by
transferring policies across agents with different action repeat on ATARI  Action repeat is   heuristic used in all successful agents  Mnih et al      Bellemare et al 
    Vezhnevets et al    It enables better exploration  eases credit assignment  and saves computation by
repeating an action chosen by the agent several     times 

To perform transfer  we initialise the FuN system with parameters extracted from an agent trained with action repeat of   and then make the following adjustments     
we accordingly adjust the discounts for all rewards   ii 
we increase the dilation of the dLSTM by   factor of  
 iii  we increase the Manager   goal horizon   by   factor of    These modi cations adapt all the  hardwired 
but explicitly temporally sensitive aspects of the agent 
We then train this agent without action repeat  As   baseline we use an LSTM agent transferred in   similar way
 with adjusted discounts  as well as FuN and LSTM agents
trained without action repeat from scratch  Figure   shows
the corresponding learning curves  The transferred FuN
agent  green curve  signi cantly outperforms every other
method  Furthermore it shows positive transfer on each environment  whereas LSTM only shows positive transfer on
Ms  Pacman 
  Discussion and future work
How to create agents that can learn to decompose their behaviour into meaningful primitives and then reuse them to
more ef ciently acquire new behaviours is   long standing
research question  The solution to this question may be an
important stepping stone towards agents with general intelligence and competence  This paper introduced FeUdal
Networks    novel architecture that formulates subgoals as
directions in latent state space  which  if followed  translate
into   meaningful behavioural primitives  FuN clearly separates the module that discovers and sets subgoals from
the module that generates the behaviour through primitive
actions  This creates   natural hierarchy that is stable and
allows both modules to learn in complementary ways  Our
experiments clearly demonstrate that this makes longterm
credit assignment and memorisation more tractable  This
also opens many avenues for further research  for instance 
deeper hierarchies can be constructed by setting goals at
multiple time scales  scaling agents to truly large environments with sparse rewards and partial observability  The
modular structure of FuN is also lends itself to transfer and
multitask learning   learnt behavioural primitives can be
reused to acquire new complex skills  or alternatively the
transitional policies of the Manager can be transferred to
agents with different embodiment 

 ScorefrostbiteFuNFuN transferLSTM transferLSTM ms pacman amidar space invadersFeUdal Networks for Hierarchical Reinforcement Learning

Acknowledgements
We thank Daan Wierstra  Olivier Pietquin  Tejas Kulkarni  Alex Graves  Oriol Vinyals  Joseph Modayil and Vlad
Mnih for many helpful discussions  suggestions and comments on the paper 

References
Bacon  PierreLuc  Precup  Doina  and Harb  Jean  The

optioncritic architecture  In AAAI   

Beattie  Charles  Leibo  Joel    Teplyashin  Denis  Ward 
Tom  Wainwright  Marcus    uttler  Heinrich  Lefrancq 
Andrew  Green  Simon  Vald es    ctor  Sadik  Amir 
Schrittwieser  Julian  Anderson  Keith  York  Sarah 
Cant  Max  Cain  Adam  Bolton  Adrian  Gaffney 
Stephen  King  Helen  Hassabis  Demis  Legg  Shane 
arXiv preprint
and Petersen  Stig  Deepmind lab 
arXiv   

Bellemare  Marc  Srinivasan  Sriram  Ostrovski  Georg 
Schaul  Tom  Saxton  David  and Munos  Remi  Unifying countbased exploration and intrinsic motivation 
In NIPS     

Bellemare  Marc    Naddaf  Yavar  Veness  Joel  and
Bowling  Michael  The arcade learning environment 
An evaluation platform for general agents  Journal of
Arti cial Intelligence Research   

Bellemare  Marc    Ostrovski  Georg  Guez  Arthur 
Thomas  Philip    and Munos    emi  Increasing the action gap  New operators for reinforcement learning  In
Proceedings of the AAAI Conference on Arti cial Intelligence     

Boutilier  Craig  Brafman  Ronen    and Geib  Christopher 
Prioritized goal decomposition of markov decision processes  Toward   synthesis of classical and decision theoretic planning  In IJCAI   

Dayan  Peter  Improving generalization for temporal difference learning  The successor representation  Neural
Computation   

Jaderberg  Max  Mnih  Volodymyr  Czarnecki  Wojciech Marian  Schaul  Tom  Leibo  Joel    Silver 
David  and Kavukcuoglu  Koray  Reinforcement learning with unsupervised auxiliary tasks  arXiv preprint
arXiv   

Kaelbling  Leslie Pack  Hierarchical learning in stochastic

domains  Preliminary results  In ICML   

Koutn    Jan  Greff  Klaus  Gomez  Faustino  and Schmid 

huber    urgen    clockwork rnn  In ICML   

Kulkarni  Tejas    Narasimhan  Karthik    Saeedi  Ardavan  and Tenenbaum  Joshua    Hierarchical deep reinforcement learning  Integrating temporal abstraction and
intrinsic motivation  arXiv preprint arXiv 
 

Lake  Brenden    Ullman  Tomer    Tenenbaum 
Joshua    and Gershman  Samuel    Building machines that learn and think like people  arXiv preprint
arXiv   

Levine  Sergey  Finn  Chelsea  Darrell  Trevor  and Abbeel 
Pieter  Endto end training of deep visuomotor policies 
arXiv preprint arXiv   

Lillicrap  Timothy    Hunt  Jonathan    Pritzel  Alexander 
Heess  Nicolas  Erez  Tom  Tassa  Yuval  Silver  David 
and Wierstra  Daan  Continuous control with deep reinforcement learning  arXiv preprint arXiv 
 

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David 
Rusu  Andrei    Veness  Joel  Bellemare  Marc   
Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas   
Ostrovski  Georg  Petersen  Stig  Beattie  Charles  Sadik 
Amir  Antonoglou  Ioannis  King  Helen  Kumaran 
Dharshan  Wierstra  Daan  Legg  Shane  and Hassabis 
Demis  Humanlevel control through deep reinforcement
learning  Nature       

Mnih  Volodymyr  Badia  Adria Puigdomenech  Mirza 
Mehdi  Graves  Alex  Lillicrap  Timothy    Harley  Tim 
Silver  David  and Kavukcuoglu  Koray  Asynchronous
methods for deep reinforcement learning  ICML   

Dayan  Peter and Hinton  Geoffrey    Feudal reinforcement learning  In NIPS  Morgan Kaufmann Publishers 
 

Morris  Richard GM  Spatial localization does not require
the presence of local cues  Learning and motivation   
   

Dietterich  Thomas    Hierarchical reinforcement learning
with the maxq value function decomposition     Artif 
Intell  Res JAIR   

Mozer  Michael      focused backpropagation algorithm for temporal pattern recognition  Complex systems   

Hochreiter  Sepp and Schmidhuber    urgen  Long short 

term memory  Neural computation   

Parr  Ronald and Russell  Stuart  Reinforcement learning

with hierarchies of machines  NIPS   

FeUdal Networks for Hierarchical Reinforcement Learning

Precup  Doina 

Temporal abstraction in reinforcement
learning  PhD thesis  University of Massachusetts   

Precup  Doina  Sutton  Richard    and Singh  Satinder   
Planning with closedloop macro actions  Technical report   

Precup  Doina  Sutton  Richard    and Singh  Satinder 
Theoretical results on reinforcement learning with temporally abstract options  In European Conference on Machine Learning  ECML  Springer   

Schaul  Tom  Horgan  Dan  Gregor  Karol  and Silver 
David  Universal value function approximators  ICML 
 

Schmidhuber    urgen  Neural sequence chunkers  Techni 

cal report   

Schulman  John  Levine  Sergey  Moritz  Philipp  Jordan 
Michael    and Abbeel  Pieter  Trust region policy optimization  In ICML   

Schulman  John  Moritz  Philipp  Levine  Sergey  Jordan 
Michael  and Abbeel  Pieter  Highdimensional continuous control using generalized advantage estimation 
ICLR   

Sutton  Richard    Td models  Modeling the world at  

mixture of time scales  In ICML   

Sutton  Richard    Precup  Doina  and Singh  Satinder  Between mdps and semimdps    framework for temporal
abstraction in reinforcement learning  Arti cial intelligence   

Tessler  Chen  Givony  Shahar  Zahavy  Tom  Mankowitz 
Daniel    and Mannor  Shie    deep hierarchical approach to lifelong learning in minecraft  arXiv preprint
arXiv   

Vezhnevets  Alexander  Mnih  Volodymyr  Osindero  Simon  Graves  Alex  Vinyals  Oriol  Agapiou  John  and
kavukcuoglu  koray  Strategic attentive writer for learning macroactions  In NIPS   

Wiering  Marco and Schmidhuber    urgen  Hqlearning 

Adaptive Behavior   

Yu  Fisher and Koltun  Vladlen  Multiscale context aggre 

gation by dilated convolutions  ICLR   

