Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

Christos Louizos     Max Welling    

Abstract

We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in   variational
setting for Bayesian neural networks  We show
that through this interpretation it is both ef cient
and straightforward to improve the approximation by employing normalizing  ows  Rezende
  Mohamed    while still allowing for local reparametrizations  Kingma et al    and
  tractable lower bound  Ranganath et al   
Maal   et al   
In experiments we show
that with this new approximation we can signi cantly improve upon classical mean  eld for
Bayesian neural networks on both predictive accuracy as well as predictive uncertainty 

  Introduction
Neural networks have been the driving force behind the
success of deep learning applications  Given enough training data they are able to robustly model inputoutput relationships and as   result provide high predictive accuracy 
However  they do have some drawbacks  In the absence of
enough data they tend to over   considerably  this restricts
them from being applied in scenarios were labeled data are
scarce       in medical applications such as MRI classi cation  Even more importantly  deep neural networks trained
with maximum likelihood or MAP procedures tend to be
overcon dent and as   result do not provide accurate con 
dence intervals  particularly for inputs that are far from the
training data distribution    simple example can be seen at
Figure     the predictive distribution becomes overly overcon dent       assigns   high softmax probability  towards
the wrong class for things it hasn   seen before       an
MNIST   rotated by   degrees  This in effect makes them
unsuitable for applications where decisions are made      

 University of Amsterdam  Netherlands  TNO Intelligent Imaging  Netherlands  Canadian Institute For Advanced
Research  CIFAR  Correspondence to 
Christos Louizos
   louizos uva nl 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

when   doctor determines the disease of   patient based on
the output of such   network 
  principled approach to address both of the aforementioned shortcomings is through   Bayesian inference procedure  Under this framework instead of doing   point estimate for the network parameters we infer   posterior distribution  These distributions capture the parameter uncertainty of the network  and by subsequently integrating over
them we can obtain better uncertainties about the predictions of the model  We can see that this is indeed the case
at Figure     the con dence of the network for the unseen
digits is drastically reduced when we are using   Bayesian
model  thus resulting into more realistic predictive distributions  Obtaining the posterior distributions is however
no easy task  as the nonlinear nature of neural networks
makes the problem intractable  For this reason approximations have to be made 
Many works have considered the task of approximate
Bayesian inference for neural networks using either
Markov Chain Monte Carlo  MCMC  with Hamiltonian
Dynamics  Neal    distilling SGD with Langevin
Dynamics  Welling   Teh    Korattikara et al 
  or deterministic techniques such as the Laplace
Approximation  MacKay    Expectation Propagation  Hern andezLobato   Adams    Hern andezLobato et al    and variational inference  Graves 
  Blundell et al    Kingma et al    Gal  
Ghahramani      Louizos   Welling   
In this paper we will also tackle the problem of Bayesian
inference in neural networks  We will adopt   stochastic
gradient variational inference  Kingma   Welling   
Rezende et al    procedure in order to estimate the
posterior distribution over the weight matrices of the network  Arguably one of the most important ingredients of
variational inference is the  exibility of the approximate
posterior distribution  it determines how well we are able
to capture the true posterior distribution and thus the true
uncertainty of our models  In Section   we will show how
we can produce very  exible distributions in an ef cient
way by employing auxiliary random variables  Agakov  
Barber    Salimans et al    Ranganath et al   
Maal   et al    and normalizing  ows  Rezende  
Mohamed   
In Section   we will discuss related

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

    LeNet with weight decay

    LeNet with multiplicative formalizing  ows

Figure   Predictive distribution for   continuously rotated version of     from MNIST  Each colour corresponds to   different class
and the height of the bar denotes the probability assigned to that particular class by the network  Visualization inspired by  Gal  
Ghahramani     

work  whereas in Section   we will evaluate and discuss
the proposed framework  Finally we will conclude with
Section   where we will provide some  nal thoughts along
with promising directions for future research 

  Multiplicative normalizing  ows
  Variational inference for Bayesian Neural

Networks

Let   be   dataset consisting of input output pairs
                xn  yn  and let     denote the weight
matrices of   layers  Assuming that   Wi    Wi  are
the prior and approximate posterior over the parameters of
the   th layer we can derive the following lower bound on
the marginal loglikelihood of the dataset   using variational Bayes  Peterson    Hinton   Van Camp   
Graves    Blundell et al    Kingma et al   
Gal   Ghahramani      Louizos   Welling   

 cid  log            
  log          log       cid 

     Eq     

 
where          denotes the training data distribution and  
the parameters of the variational posterior  For continuous    distributions that allow for the reparametrization
trick  Kingma   Welling    or stochastic backpropagation  Rezende et al    we can reparametrize the random sampling from    of the lower bound in terms of
noise variables   and deterministic functions      

 cid  log             

  log           log        cid 

 
This reparametrization allow us to treat approximate parameter posterior inference as   straightforward optimiza 

    Ep 

tion problem that can be optimized with offthe shelf
 stochastic  gradient ascent techniques 

  Improving the variational approximation

For Bayesian neural networks the most common family for
the approximate posterior is that of mean  eld with independent Gaussian distributions for each weight  Despite
the fact that this leads to   straightforward lower bound
for optimization  the approximation capability is quite limiting  it corresponds to just   unimodal  bump  on the
very high dimensional space of the parameters of the neural network  There have been attempts to improve upon
this approximation with works such as  Gal   Ghahramani      with mixtures of delta peaks and  Louizos  
Welling    with matrix Gaussians that allow for nontrivial covariances among the weights  Nevertheless  both
of the aforementioned methods are still  in   sense  limited 
the true parameter posterior is more complex than delta
peaks or correlated Gaussians 
There has been   lot of recent work on ways to improve
the posterior approximation in latent variable models with
normalizing  ows  Rezende   Mohamed    and auxiliary random variables  Agakov   Barber    Salimans
et al    Ranganath et al    Maal   et al   
being the most prominent  Brie      normalizing  ow is
constructed by introducing parametrized bijective transformations  with easy to compute Jacobians  to random variables with simple initial densities  By subsequently optimizing the parameters of the  ow according to the lower
bound they can signi cantly improve the posterior approximation  Auxiliary random variables instead construct more
 exible distributions by introducing latent variables in the

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

posterior itself  thus de ning the approximate posterior as
  mixture of simple distributions 
Nevertheless  applying these ideas to the parameters in  
neural network has not yet been explored  While it is
straightforward to apply normalizing  ows to   sample of
the weight matrix from      this quickly becomes very
expensive  for example with planar  ows  Rezende   Mohamed    we will need two extra matrices for each step
of the  ow  Furthermore  by utilizing this procedure we
also lose the bene ts of local reparametrizations  Kingma
et al    Louizos   Welling    which are possible
with Gaussian approximate posteriors 
In order to simultaneously maintain the bene ts of local reparametrizations and increase the  exibility of the
approximate posteriors in   Bayesian neural network we
will rely on auxiliary random variables  Agakov   Barber    Salimans et al      Ranganath et al 
  Maal   et al    more speci cally we will exploit the well known  multiplicative noise  concept       as
in  Gaussian  Dropout  Srivastava et al    in neural
networks and we will parametrize the approximate posterior with the following process 

 

                   

distribution          cid            dz  with   being  

where now the approximate posterior becomes   compound

vector of random variables distributed according to the
mixing density      To allow for local reparametrizations we will parametrize the conditional distribution for
the weights to be   fully factorized Gaussian  Therefore we
assume the following form for the fully connected layers 

        

   zi ij   

ij 

 

Din cid 

Dout cid 

  

  

Dh cid 

Dw cid 

Df cid 

where Din  Dout is the input and output dimensionality 
and the following form for the kernels in convolutional networks 

        

   zk ijk   

ijk 

 

  

  

  

where Dh  Dw  Df are the height  width and number of
 lters for each kernel  Note that we did not let   affect the
variance of the Gaussian approximation  in   pilot study we
found that this parametrization was prone to local optima
due to large variance gradients  an effect also observed with
the multiplicative parametrization of the Gaussian posterior  Kingma et al    Molchanov et al    We
have now reduced the problem of increasing the  exibility
of the approximate posterior over the weights   to that of
increasing the  exibility of the mixing density      Since

  is of much lower dimension  compared to    it is now
straightforward to apply normalizing  ows to      in this
way we can signi cantly enhance our approximation and
allow for      multimodality and nonlinear dependencies
between the elements of the weight matrix  This will in
turn better capture the properties of the true posterior distribution  thus leading to better performance and predictive
uncertainties  We will coin the term multiplicative normalizing  ows  MNFs  for this family of approximate posteriors  Algorithms     describe the forward pass using local
reparametrizations for fully connected and convolutional
layers with this type of approximate posterior 

Algorithm   Forward propagation for each fully connected
layer    Mw     are the means and variances of each
layer    is   minibatch of activations and NF  is the normalizing  ow described at eq    For the  rst layer we have
that       where   is the minibatch of inputs 
Require     Mw    
           
  ZTf   NF   
  Mh       cid  ZTf  Mw
  Vh      
           
 
  return Mh  

Vh  cid   

Algorithm   Forward propagation for each convolutional
layer    Nf are the number of convolutional  lters    is
the convolution operator and we assume the  batch  height 
width  feature maps  convention 
Require     Mw    
           
  zTf   NF   
  Mh        Mw  cid  reshape zTf       Df  
  Vh          
           
 
  return Mh  

Vh  cid   

For the normalizing  ow of      we will use the masked
RealNVP  Dinh et al    using the numerically stable updates introduced in Inverse Autoregressive Flow
 IAF   Kingma et al   

    Bern 
        

    tanh       cid  zt 
         

zt       cid  zt        cid   zt  cid             cid   
 

 cid cid cid cid   zt 

 zt

 cid cid cid cid            log  

log

where  cid  corresponds to elementwise multiplication   

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

is the sigmoid function  and           are linear mappings  We resampled the mask   every time in order to
avoid   speci   splitting over the dimensions of    For the
starting point of the  ow      we used   simple fully factorized Gaussian and we will refer to the  nal iterate as
zTf  

  Bounding the entropy

Unfortunately  parametrizing the posterior distribution as
eq    makes the lower bound intractable as generally we do
not have   closed form density function for      This
makes the calculation of the entropy   Eq   log     
challenging  Fortunately we can make the lower bound
tractable again by further lower bounding the entropy in
terms of an auxiliary distribution         Agakov   Barber    Salimans et al      Ranganath et al 
  Maal   et al    This can be seen as if we are
performing variational inference on the augmented probability space                that maintains the same true
posterior distribution         as we can always marginalize out        to obtain the original model  The lower
bound in this case becomes 
       Eq         

 cid  log                 

  log          log           

  log              log       cid 

 
where   are the parameters of the auxiliary distribution   
This bound is looser than the previous bound  however the
extra  exibility of      can compensate and allow for  
tighter bound  Furthermore  the tightness of the bound also
depends on the ability of        to approximate the  auxiliary  posterior distribution                    
  Therefore  to allow for    exible        we will follow  Ranganath et al    and we will parametrize it with inverse
normalizing  ows as follows 

    

  zTb     

       
   

Dz cid 

  

out 

      

where for fully connected layers we have that 

    cid      tanh cT   cid   cid    
 cid 
 cid cid      tanh cT   cid   cid    
 cid   cid   DhDw 
    cid tanh mat          
 cid cid tanh mat          
 cid 
 cid   cid   DhDw 

and for convolutional 

      

out 

 

        

 

 exp   

 

 

 

 

 

 

where         are trainable vectors that have the same
dimensionality as    Dz    corresponds to   vector of
      corresponds to the outer product and mat  corresponds to the matricization  operator  The zTb variable
corresponds to the fully factorized variable that is transformed by   normalizing  ow to zTf or else the variable obtained by the inverse normalizing  ow  zTb   NF zTf  
We will parametrize this inverse directly with the procedure described at eq    Notice that we can employ local
reparametrizations also in eq    so as to avoid
sampling the  potentially big  matrix    With the standard
normal prior and the fully factorized Gaussian posterior of
eq    the KLdivergence between the prior and the posterior
can be computed as follows 

  KL           

where each of the terms corresponds to 

  Eq   zT  KL     zTf      
  log   zTf      log   zTf  
 cid 

  KL     zTf        
       

  log  

       
 

        
Tfi

 

 
 

   

 

log   zTf      log   zTb     
Tf cid 

log   zTf     log       

log

  

Tf  Tb cid 
 cid cid cid cid   zt 

 zt

  Tf

log

 cid cid cid cid 

 

 cid cid cid cid   zt 

 zt

 cid cid cid cid 

 

 

It should be noted that this bound is   generalization of the
bound proposed by  Gal   Ghahramani      We can
arrive at the bound of  Gal   Ghahramani      if we
trivially parametrize the auxiliary model              
 which provides   less tight bound  Ranganath et al   
use   standard normal prior for      Bernoulli      with
probability of success   and then let the variance of our
conditional Gaussian        go to zero  This will result
into the lower bound being in nite due to the log of the
variances  nevertheless since we are not optimizing over  
we can simply disregard those terms  After   little bit of algebra we can show that the only term that will remain in the
KLdivergence between      and      will be the expectation of the trace of the square of the mean matrix      
Eq     
  with
      being the dropout rate 
We also found that in general it is bene cial to  constrain 
the standard deviations  ij of the conditional Gaussian posterior        during the forward pass for the computation

  tr diag        diag         

 cid   cid 

 Converting the multidimensional tensor to   matrix 
 The matrix that has            ij

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

of the likelihood to   lower than the true range          
instead of the     we have with   standard normal prior 
This results into   small bias and   looser lower bound 
however it helps in avoiding bad local minima in the variational objective  This is akin to the free bits objective described at  Kingma et al   

  Related work
Approximate inference for Bayesian neural networks has
been pioneered by  MacKay    and  Neal   
Laplace approximation  MacKay    provides   deterministic approximation to the posterior that is easy to obtain  it is   Gaussian centered at the MAP estimate of the
parameters with   covariance determined by the inverse of
the Hessian of the loglikelihood  Despite the fact that it
is straightforward to implement  its scalability is limited
unless approximations are made  which generally reduces
performance  Hamiltonian Monte Carlo  Neal    is
so far the golden standard for approximate Bayesian inference  nevertheless it is also not scalable to large networks and datasets due to the fact that we have to explicitly
store the samples from the posterior  Furthermore as it is
an MCMC method  assessing convergence is non trivial 
Nevertheless there is interesting work that tries to improve
upon those issues with stochastic gradient MCMC  Chen
et al  and distillation methods  Korattikara et al   
Deterministic methods
for approximate inference in
Bayesian neural networks have recently attained much attention  One of the  rst applications of variational inference in neural networks was in  Peterson    and  Hinton   Van Camp    More recently  Graves   
proposed   practical method for variational inference in
this setting with   simple  but biased  estimator for  
fully factorized posterior distribution   Blundell et al 
  improved upon this work with the unbiased estimator from  Kingma   Welling    and   scale mixture prior   Hern andezLobato   Adams    proposed
to use Expectation Propagation  Minka    with fully
factorized posteriors and showed good results on regression
tasks   Kingma et al    showed how Gaussian dropout
can be interpreted as performing approximate inference
with loguniform priors  multiplicative Gaussian posteriors and local reparametrizations  thus allowing straightforward learning of the dropout rates  Similarly  Gal  
Ghahramani      showed interesting connections between Bernoulli Dropout  Srivastava et al    networks
and approximate Bayesian inference in deep Gaussian Processes  Damianou   Lawrence    thus allowing the
extraction of uncertainties in   principled way 
Similarly  Louizos   Welling    arrived at the same result through structured posterior approximations via matrix Gaussians and local reparametrizations  Kingma et al 

 
It should also be mentioned that uncertainty estimation
in neural networks can also be performed without the
Bayesian paradigm  frequentist methods such as Bootstrap  Osband et al    and ensembles  Lakshminarayanan et al    have shown that in certain scenarios
they can provide reasonable con dence intervals 

of

the

coded

experiments were

  Experiments
All
in Tensor 
 ow  Abadi et al    and optimization was done
with Adam  Kingma   Ba    using the default hyperparameters  We used the LeNet    LeCun et al   
convolutional architecture with ReLU  Nair   Hinton 
  nonlinearities  The means   of the conditional
Gaussian        were initialized with the scheme proposed in  He et al    whereas the log of the variances
were initialized by sampling from       Unless
explicitly mentioned otherwise we use  ows of length two
for      and        with   hidden units for each step
of the  ow of      and   hidden units for each step of
the  ow of        We used   posterior samples to
estimate the predictive distribution for all of the models
during testing and   posterior sample during training 

Table   Models considered in this paper  Dropout corresponds
to the model used in  Gal   Ghahramani      Deep Ensemble to the model used in  Lakshminarayanan et al    FFG to
the Bayesian neural network employed in  Blundell et al   
FFLU to the Bayesian neural network used in  Kingma et al 
  Molchanov et al    with the additive parametrization
of  Molchanov et al    and MNFG corresponds to the proposed variational approximation 
It should be noted that Deep
Ensembles use adversarial training  Goodfellow et al   
Name
  
Dropout
   Ensem 
FFG
FFLU
MNFG

fully factorized additive Gaussian
fully factorized additive Gaussian
multiplicative normalizing  ows

Prior
      
      
      
      

mixture of zero and delta peaks

Posterior
delta peak

log       

 

mixture of peaks

  Predictive performance and uncertainty
MNIST We trained on MNIST LeNet architectures using
the priors and posteriors described at Table   We trained
Dropout with the way described at  Gal   Ghahramani 
    using   for the dropout rate and for Deep Ensembles  Lakshminarayanan et al    we used   members and       for the adversarial example generation 
For the models with the Gaussian prior we constrained the
standard deviation of the conditional posterior to be    

 The version from Caffe 

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

during the forward pass  The classi cation performance of
each model can be seen at Table   while our overall focus is not classi cation accuracy per se  we see that with
the MNF posteriors we improve upon mean  eld reaching
similar accuracies with Deep Ensembles 

notMNIST To evaluate the predictive uncertainties of
each model we performed the task described at  Lakshminarayanan et al    we estimated the entropy of the
predictive distributions on notMNIST  from the LeNet architectures trained on MNIST  Since we apriori know that
none of the notMNIST classes correspond to   trained class
 since they are letters and not digits  the ideal predictive
distribution is uniform over the MNIST digits         maximum entropy distribution  Contrary to  Lakshminarayanan
et al    we do not plot the histogram of the entropies
across the images but we instead use the empirical CDF 
which we think is more informative  Curves that are closer
to the bottom right part of the plot are preferable  as it denotes that the probability of observing   high con dence
prediction is low  At Figure   we show the empirical CDF
over the range of possible entropies      for all of the
models 

ability to allow for uncertainty in the predictions  The sparsity levels  are     for the two convolutional layers and     for the two fully connected  Similar
effects would probably be also observed if we optimized
the dropout rates for Dropout  The only source of randomness in the neural network is from the Bernoulli random
variables          By employing the Central Limit Theorem  we can express the distribution of the activations as  
Gaussian  Wang   Manning    with variance affected
by the variance of the Bernoulli                 The
maximum variance of the Bernoulli      is when      
therefore any tuning of the Dropout rate will result into  
decrease in the variance of the      and therefore   decrease
in the variance of the Gaussian at the hidden units  This will
subsequently lead into less predictive variance and more
con dence 
Finally  whereas it was shown at  Lakshminarayanan et al 
  that Deep Ensembles provide good uncertainty estimates  better than Dropout  on this task using fully connected networks  this result did not seem to apply for the
LeNet architecture we considered  We hypothesize that
they are sensitive to the hyperparameters       adversarial
noise  number of members in the ensemble  and it requires
more tuning in order to improve upon Dropout on this architecture 

CIFAR   We performed   similar experiment on CIFAR
  To arti cially create the  unobserved class  scenario 
we hid   of the labels  dog  frog  horse  ship  truck  and
trained on the rest  airplane  automobile  bird  cat  deer 
For this task we used the larger LeNet architecture  described at  Gal   Ghahramani      For the models
with the Gaussian prior we similarly constrained the standard deviation during the forward pass to be     For Deep
Ensembles we used  ve members with       for the adversarial example generation  The predictive performance on
these  ve classes can be seen in Table   with Dropout and
MNFs achieving the overall better accuracies  We subsequently measured the entropy of the predictive distribution
on the classes that were hidden  with the resulting empirical
CDFs visualized in Figure  
We similarly observe that the network with just weight decay was the most overcon dent  Furthermore  Deep Ensembles and Dropout had similar uncertainties  with Deep
Ensembles having lower accuracy on the observed classes 
The networks with the Gaussian priors also had similar uncertainty with the network with the log uniform prior  nevertheless the MNF posterior had much better accuracy on
 Computed by pruning weights where log     log    

   Molchanov et al   

 Assuming that the network is wide enough 
   lters at each convolutional layer and   hidden units

for the fully connected layer 

Figure   Empirical CDF for the entropy of the predictive distributions on notMNIST 

It is clear from the plot that the uncertainty estimates from
MNFs are better than the other approaches  since the probability of   low entropy prediction is overall lower  The
network trained with just weight decay was  as expected 
the most overcon dent with an almost zero median entropy while Dropout seems to be in the middle ground  The
Bayesian neural net with the loguniform prior also showed
overcon dence in this task  we hypothesize that this is due
to the induced sparsity  Molchanov et al    which results into the pruning of almost all irrelevant sources of
variation in the parameters thus not providing enough vari 

 Can be found at http yaroslavvb blogspot 

co uk notmnistdataset html

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

sarial examples at least it  knows that it doesn   know 

Figure   Empirical CDF for the entropy of the predictive distributions on the   hidden classes from CIFAR  

the observed classes  The sparsity levels for the network
with the loguniform prior now were     for the
convolutional layers and     for the fully connected  Overall  the network with the MNF posteriors seem
to provide the better tradeoff in uncertainty and accuracy
on the observed classes 

Table   Test errors   with the LeNet architecture on MNIST
and the  rst  ve classes of CIFAR  
Dataset
MNIST
CIFAR  

   Dropout   Ensem  FFG FFLU MNFG
 
 

 
 

 
 

 
 

 
 

 
 

Figure   Accuracy  solid  vs entropy  dashed  as   function of
the adversarial perturbation   on MNIST 

CIFAR We performed the same experiment also on the
 ve class subset of CIFAR   The results can be seen
in Figure   Here we however observe   different picture 
compared to MNIST  since all of the methods experienced
overcon dence  We hypothesize that adversarial examples
are harder to escape and be uncertain about in this dataset 
due to the higher dimensionality  and therefore further investigation is needed 

  Accuracy and uncertainty on adversarial examples

We also measure how robust our models and uncertainties are against adversarial examples  Szegedy et al   
Goodfellow et al    by generating examples using the
fast sign method  Goodfellow et al    for each of the
previously trained architectures using Cleverhans  Papernot et al    For this task we do not include Deep
Ensembles as they are trained on adversarial examples 

MNIST On this scenario we observe interesting results
if we plot the change in accuracy and entropy by varying
the magnitude of the adversarial perturbation  The resulting plot can be seen in Figure   Overall Dropout seems
to have better accuracies on adversarial examples  nevertheless  those come at an  overcon dent  price since the
entropy of the predictive distributions is quite low thus resulting into predictions that have  on average  above  
probability for the dominant class  This is in contrast with
MNFs  while the accuracy almost immediately drops close
to random  the uncertainty simultaneously increases to almost maximum entropy  This implies that the predictive
distribution is more or less uniform over those examples 
So despite the fact that our model cannot overcome adver 

Figure   Accuracy  solid  vs entropy  dashed  as   function of the
adversarial perturbation   on CIFAR    on the  rst   classes 

  Regression on toy dataset

For the  nal experiment we visualize the predictive distributions obtained with the different models on the toy regression task introduced at  Hern andezLobato   Adams 
  We generated   training inputs from      and
then obtained the corresponding targets via           
where           We  xed the likelihood noise to its
true value and then  tted   Dropout network with      

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

    Dropout      
Figure   Predictive distributions for the toy dataset  Blue areas correspond to   standard deviations around the mean 

    Dropout learned  

    FFLU

    MNFG

for the hidden layer  an FFLU network and an MNFG  We
also  tted   Dropout network where we also learned the
dropout probability   of the hidden layer according to the
bound described at section    which is equivalent to the
one described at  Gal   Ghahramani      using REINFORCE  Williams    and   global baseline  Mnih
  Gregor    The resulting predictive distributions can
be seen at Figure  
As we can observe  MNF posteriors provide more realistic
predictive distributions  closer to the true posterior  which
can be seen at  Hern andezLobato   Adams    and
with the network being more uncertain on areas where we
do not observed any data  The uncertainties obtained by
Dropout with  xed       did not diverge as much in
those areas but overall they were better compared to the
uncertainties obtained with FFLU  We could probably attribute the latter to the sparsi cation of the network since
  and   of the parameters were pruned for each layer
respectively 
Interestingly the uncertainties obtained with the network
with the learned Dropout probability were the most  over 
 tted  This might suggest that Dropout uncertainty is
probably not   good posterior approximation since by optimizing the dropout rates we do not seem to move closer to
the true posterior predictive distribution  This is in contrast
with MNFs  they are  exible enough to allow for optimizing all of their parameters in   way that does better approximate the true posterior distribution  This result also empirically veri es the claim we previously made  by learning the
dropout rates the entropy of the posterior predictive will decrease thus resulting into more overcon dent predictions 

  Conclusion
We introduce multiplicative normalizing  ows  MNFs 
  family of approximate posteriors for the parameters of
  variational Bayesian neural network  We have shown
that through this approximation we can signi cantly improve upon mean  eld on both predictive performance as

 No Dropout was used for the input layer since it is  

dimensional 

well as predictive uncertainty  We compared our uncertainty on notMNIST and CIFAR with Dropout  Srivastava
et al    Gal   Ghahramani      and Deep Ensembles  Lakshminarayanan et al    using convolutional
architectures and found that MNFs achieve more realistic
uncertainties while providing predictive capabilities on par
with Dropout  We suspect that the predictive capabilities
of MNFs can be further improved through more appropriate optimizers that avoid the bad local minima in the variational objective  Finally  we also highlighted limitations
of Dropout approximations and empirically showed that
MNFs can overcome them 
There are   couple of promising directions for future research  One avenue would be to explore how much
can MNFs sparsify and compress neural networks under
either sparsity inducing priors  such as the loguniform
prior  Kingma et al    Molchanov et al    or empirical priors  Ullrich et al    Another promising direction is that of designing better priors for Bayesian neural
networks  For example  Neal    has identi ed limitations of Gaussian priors and proposes alternative priors
such as the Cauchy  Furthermore  the prior over the parameters also affects the type of uncertainty we get in our
predictions  for instance we observed in our experiments  
signi cant difference in uncertainty between Gaussian and
loguniform priors  Since different problems require different types of uncertainty it makes sense to choose the prior
accordingly       use an informative prior so as to alleviate
adversarial examples 

Acknowledgements
We would like to thank Klamer Schutte  Matthias Reisser
and Karen Ullrich for valuable feedback  This research is
supported by TNO  NWO and Google 

References
Abadi  Mart    Agarwal  Ashish  Barham  Paul  Brevdo 
Eugene  Chen  Zhifeng  Citro  Craig  Corrado  Greg   
Davis  Andy  Dean  Jeffrey  Devin  Matthieu  et al  Tensor ow  Largescale machine learning on heterogeneous

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

distributed systems  arXiv preprint arXiv 
 

Richard   
arXiv preprint arXiv   

Blackbox  divergence minimization 

Agakov  Felix   and Barber  David  An auxiliary variaIn International Conference on Neural

tional method 
Information Processing  pp    Springer   

Blundell  Charles  Cornebise  Julien  Kavukcuoglu  Koray 
and Wierstra  Daan  Weight uncertainty in neural networks  Proceedings of the  nd International Conference on Machine Learning  ICML   Lille  France 
  July    

Chen  Tianqi  Fox  Emily    and Guestrin  Carlos  Stochas 

tic gradient hamiltonian monte carlo 

Damianou  Andreas    and Lawrence  Neil    Deep gaussian processes  In Proceedings of the Sixteenth International Conference on Arti cial Intelligence and Statistics  AISTATS   Scottsdale  AZ  USA  April     May
    pp     

Dinh  Laurent  SohlDickstein  Jascha  and Bengio  Samy 
arXiv preprint

Density estimation using real nvp 
arXiv   

Gal  Yarin and Ghahramani  Zoubin  Bayesian convolutional neural networks with bernoulli approximate variarXiv preprint arXiv 
ational inference 
   

Gal  Yarin and Ghahramani  Zoubin  Dropout as   bayesian
approximation  Representing model uncertainty in deep
learning  arXiv preprint arXiv     

Goodfellow  Ian    Shlens  Jonathon  and Szegedy  Christian  Explaining and harnessing adversarial examples 
arXiv preprint arXiv   

Graves  Alex  Practical variational inference for neural netIn Advances in Neural Information Processing

works 
Systems  pp     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Delving deep into recti ers  Surpassing humanIn Prolevel performance on imagenet classi cation 
ceedings of the IEEE International Conference on Computer Vision  pp     

Hern andezLobato 

Jos   Miguel and Adams  Ryan 
Probabilistic backpropagation for scalable learning of
In Proceedings of the  nd
bayesian neural networks 
International Conference on Machine Learning  ICML
  Lille  France    July   pp   
 

Hern andezLobato 

Li 

Yingzhen 
Hern andezLobato  Daniel  Bui  Thang  and Turner 

Jos   Miguel 

Hinton  Geoffrey   and Van Camp  Drew  Keeping the
neural networks simple by minimizing the description
length of the weights  In Proceedings of the sixth annual
conference on Computational learning theory  pp   
ACM   

Kingma  Diederik and Ba  Jimmy  Adam    method for
International Conference on

stochastic optimization 
Learning Representations  ICLR  San Diego   

Kingma  Diederik   and Welling  Max  Autoencoding
International Conference on Learn 

variational bayes 
ing Representations  ICLR   

Kingma  Diederik    Salimans  Tim  and Welling  Max 
Variational dropout and the local reparametrization trick 
Advances in Neural Information Processing Systems 
 

Kingma  Diederik    Salimans  Tim  and Welling  Max  Improving variational inference with inverse autoregressive
 ow  arXiv preprint arXiv   

Korattikara  Anoop  Rathod  Vivek  Murphy  Kevin  and
Welling  Max  Bayesian dark knowledge  arXiv preprint
arXiv   

Lakshminarayanan  Balaji  Pritzel  Alexander  and Blundell  Charles  Simple and scalable predictive uncertainty estimation using deep ensembles  arXiv preprint
arXiv   

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  Proceedings of the IEEE   
   

Louizos  Christos and Welling  Max  Structured and ef 
cient variational deep learning with matrix gaussian posteriors  arXiv preprint arXiv   

Maal    Lars    nderby  Casper Kaae    nderby 
  ren Kaae  and Winther  Ole  Auxiliary deep generative models  arXiv preprint arXiv   

MacKay  David JC    practical bayesian framework for
backpropagation networks  Neural computation   
   

Minka  Thomas    Expectation propagation for approxIn Proceedings of the Sevimate bayesian inference 
enteenth conference on Uncertainty in arti cial intelligence  pp    Morgan Kaufmann Publishers Inc 
 

Multiplicative Normalizing Flows for Variational Bayesian Neural Networks

Szegedy  Christian  Zaremba  Wojciech  Sutskever  Ilya 
Bruna  Joan  Erhan  Dumitru  Goodfellow  Ian  and Fergus  Rob  Intriguing properties of neural networks  arXiv
preprint arXiv   

Ullrich  Karen  Meeds  Edward  and Welling  Max  Soft
weightsharing for neural network compression  arXiv
preprint arXiv   

Wang  Sida and Manning  Christopher  Fast dropout trainIn Proceedings of The  th International Confer 

ing 
ence on Machine Learning  pp     

Welling  Max and Teh  Yee    Bayesian learning via
stochastic gradient langevin dynamics  In Proceedings
of the  th International Conference on Machine Learning  ICML  pp     

Williams  Ronald    Simple statistical gradientfollowing
learning 

algorithms for connectionist reinforcement
Machine learning     

Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht  Benjamin  and Vinyals  Oriol  Understanding deep learning requires rethinking generalization  arXiv preprint
arXiv   

Mnih  Andriy and Gregor  Karol  Neural variational inference and learning in belief networks  arXiv preprint
arXiv   

Molchanov     Ashukha     and Vetrov     Variational
Dropout Sparsi es Deep Neural Networks  ArXiv eprints  January  

Nair  Vinod and Hinton  Geoffrey    Recti ed linear units
improve restricted boltzmann machines  In Proceedings
of the  th International Conference on Machine Learning  ICML  pp     

Neal  Radford    Bayesian learning for neural networks 

PhD thesis  Citeseer   

Osband  Ian  Blundell  Charles  Pritzel  Alexander  and
Van Roy  Benjamin  Deep exploration via bootstrapped
dqn  arXiv preprint arXiv   

Papernot  Nicolas  Goodfellow  Ian  Sheatsley  Ryan  Feinman  Reuben  and McDaniel  Patrick  cleverhans   
an adversarial machine learning library  arXiv preprint
arXiv   

Peterson  Carsten    mean  eld theory learning algorithm
for neural networks  Complex systems   
 

Ranganath  Rajesh  Tran  Dustin  and Blei  David   
arXiv preprint

Hierarchical variational models 
arXiv   

Rezende  Danilo Jimenez and Mohamed  Shakir  Variational inference with normalizing  ows  arXiv preprint
arXiv   

Rezende  Danilo Jimenez  Mohamed  Shakir  and Wierstra 
Daan  Stochastic backpropagation and approximate inIn Proceedings of
ference in deep generative models 
the  th International Conference on Machine Learning  ICML   Beijing  China    June   pp 
   

Salimans  Tim  Knowles  David    et al  Fixedform variational posterior approximation through stochastic linear
regression  Bayesian Analysis     

Salimans  Tim  Kingma  Diederik    Welling  Max  et al 
Markov chain monte carlo and variational inference 
Bridging the gap  In International Conference on Machine Learning  pp     

Srivastava  Nitish  Hinton  Geoffrey  Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan  Dropout 
  simple way to prevent neural networks from over tting  The Journal of Machine Learning Research   
   

