Sparse   GroupSparse Dirty Models  Statistical Guarantees without

Unreasonable Conditions and   Case for NonConvexity

Eunho Yang     Aur elie    Lozano  

Abstract

Imposing sparse   groupsparse superposition
structures in highdimensional parameter estimation is known to provide  exible regularization
that is more realistic for many realworld problems  For example  such   superposition enables partiallyshared support sets in multitask
learning  thereby striking the right balance between parameter overlap across tasks and task
speci city  Existing theoretical results on estimation consistency  however  are problematic as
they require too stringent an assumption  the incoherence between sparse and groupsparse superposed components  In this paper  we  ll the
gap between the practical success and suboptimal analysis of sparse   groupsparse models 
by providing the  rst consistency results that do
not require unrealistic assumptions  We also
study nonconvex counterparts of sparse   groupsparse models  Interestingly  we show that these
are guaranteed to recover the true support set
under much milder conditions and with smaller
sample size than convex models  which might be
critical in practical applications as illustrated by
our experiments 

  Introduction
We consider highdimensional statistical models where the
ambient dimension   is much larger than the number of
observations    Under such highdimensional scaling  it
is still possible to obtain consistent estimators by imposing lowdimensional structural constraints upon the statistical models  such as sparsity      
in compressed sens 

 School of Computing  KAIST  Daejeon  South Korea  AItrics  Seoul  South Korea  IBM      Watson Research Center  Yorktown Heights  NY  USA  Correspondence
to  Eunho Yang  eunhoy kaist ac kr  Aurelie    Lozano
 aclozano us ibm com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ing  Baraniuk    and Lasso  Tibshirani    lowrank structure  Recht et al    Negahban   Wainwright 
  sparse graphical model structure  Friedman et al 
  Ravikumar et al    and sparse additive structure for nonparametric models  Ravikumar et al     
widely used approach to structured learning is via speci  
regularization functions  For instance   cid regularization
is employed for sparse models  Tibshirani     cid cid  
norms for group sparsity  Yuan   Lin    and nuclear
norm for lowrank matrixstructured models  Cand es  
Tao    Much attention has been devoted to the study
of these structured norms and their theoretical properties 
Such    clean  regularization approach  however  might be
too stringent in practice  For instance in linear regression 
  blend of elementwise sparsity and groupsparsity might
be more appropriate than   purely sparse or purely groupsparse solution  In multitask learning  while some parameters might be shared across tasks  others might only be relevant to   subset of tasks or   single task  To overcome
this limitation    line of work on socalled dirty models
has emerged  which addresses this caveat by  mixing and
matching  different structures  One basic approach consists in decomposing the model parameters as   sum of
two components  each penalized separately  one component captures the common structure across tasks and the
other taskspeci   characteristics  Jalali et al    Gong
et al    For instance the dirty model in Jalali et al 
  employs  cid  and  cid  regularizers to the two components  Chandrasekaran et al    consider the problem of recovering unknown lowrank and sparse matrices 
given the sum of their sum  with application such as optical imaging systems  Robust principal component analysis
and related extensions  Cand es et al    Agarwal et al 
  Hsu et al    estimate   covariance matrix that is
the sum of   lowrank matrix and   structured       sparse 
column sparse  matrix 
  general framework for studying dirty models was recently proposed in Yang   Ravikumar   which
bridges and extends several analyses for speci   pairs
of superposition structures and speci   statistical models       Jalali et al    Chandrasekaran et al   
Cand es et al    Agarwal et al    Hsu et al 

Sparse   Group Sparse Dirty Models

  Speci cally  this framework applies to   general
class of Mestimators employing   socalled hybrid regularization function  which is the in mal convolution of
weighted regularization functions  one for each structural
component  This formulation is equivalent to an Mestimator that combines   loss function applied to the sum
of multiple parameter vectors  one per structural component  and   weighted sum of regularization functions  one
per parameter vector 
For the sparse   group sparse decomposition  however
existing analyses are highly problematic  The key weakness is that they require some form of structural incoherence condition which captures the interaction between the
different structured components  While such   structural
incoherence is   reasonable assumption for     
sparse
  low rank superposition  it is what too stringent for the
sparse group sparse case because the two structures are
completely coherent for this case  This yields   key motivating question for this paper  Under the sparse   group
sparse setting  can we bypass structural incoherence conditions and yet obtain tight error bounds 

In this paper we provide   positive answer by developing
  novel proof technique  Prior analyses require  local  restricted strong convexity conditions  RSC  one condition
for the sparse component and one for the group sparse
component  The use of structural incoherence between
sparse and group sparse components in then needed to
show  global  RSC for the vector concatenating sparse and
group sparse components  To avoid the need for structural
incoherence  we use RSC in the summed space directly
 namely for the summed sparse   groupsparse structure 
However  this brings in   new issue  in this case  the dirty
regularizer for the parameter vector is not decomposable 
To circumvent this issue  our key ingredient is to introduce
 surrogate  sparse and group sparse components depending on our estimators such that    their sum equals the sum
of the true parameter components and ii  corresponding error vectors are decomposable even though the regularizer
itself is not decomposable  Using the decomposability of
error vectors  we are then able to show  cid  consistency for
general loss functions 
As an additional key contribution of this paper  we consider the extension of sparse group sparse dirty models to
nonconvex regularizers  and show their  cid  consistency 
Interestingly  these models are guaranteed to recover the
true support set under much milder conditions and with
smaller sample size than convex models 
In particular 
our  cid  consistency results require neither incoherence in
the loss function nor structural incoherence between sparse
and group sparse parameters  We illustrate the practical
impact of this superior theoretical results with simulation
experiments 

The remainder of this paper is organized as follows  In Section   we review sparse groupsparse dirty models with
convex penalties and introduce their nonconvex counterparts 
In Section   we discuss the incoherence assumption required by prior analyses and explain why such an
assumption is unreasonable  Section   introduces the key
ingredient of our novel proof technique  Section   presents
the convergence bounds for models with convex penalties 
Those for nonconvex penalties are stated in Section   Finally  simulation experiments are provided in Section   to
illustrate the remarkable practical advantage of nonconvex
penalties  agreeing with their superior convergence rates 

  Sparse   GroupSparse Dirty Models 

Setup and Formulations

Consider   data collection                 Zn  where each
element is drawn independently from distribution    and  
loss function                 where         measures
the goodness of    of parameter       to the given data
collection    Typically     Rp  parameters are vectors 
or Rp    parameters are matrices  Assume there are some
known groups                 Gq  that partition the parameter index set  Gi   Gj     and   
We aim at
unique minimizer of
argmin 

  Gg               
recovering parameter   which is
 

EZ       in cases where

the population risk 

the
 

         

 
where   is   sparse component and   is   groupsparse
component obeying the group structure    For that purpose 
we focus on regularized Mestimators under   dirty learning setting that combines sparsity and groupsparsity  We
consider both convex and nonconvex regularizers as follows 

  Dirty models with convex regularizers

We focus on regularized Mestimators of the form
             cid cid     cid cid   

minimize

 

 

function        

 cid cid      cid  

where the loss
is possibly nonconvex  Here  given known parameter groups    
               Gq 
the group regularizer is de ned as
    cid Gt cid   for       where  Gt denotes
the parameter subset in group Gt  The constant   determines how the elements within each group are combined 
We provide examples for the popular settings of linear regression and inverse covariance estimation 

Linear regression  Consider the standard linear model
           where     Rn is the observation vector   

Sparse   Group Sparse Dirty Models

 cid       cid     cid 

is the true regression parameter which is the sum of sparse
  and group sparse       Rn   is the design matrix 
and     Rn is the observation noise  The  dirty  regularized least squares solves

 
  

minimize
 Rp

     cid cid     cid cid  
 
where groups are de ned within    single  parameter vector
space via   The formulation can be seamlessly extended
to cover the dirty multitask learning setting of Jalali et al 
 

  cid 

  

 
  

 cid cid             cid       

 cid cid cid 

 

 

 cid cid     cid cid 

minimize
 Rp  

where we have   related tasks in columns        Rp   
and the groups can be de ned across tasks in rows       for
predictor                      belong to the same group 
Here          indicates kth column of matrix input

     

Graphical Model Estimation  Another key example is
  modi ed graphical Lasso where the goal is to estimate
the structure of the underlying graphs representing conditional independences across variables  Assume that there
are some known set of edge groups and that the true parameter   has only   small number of active edge groups
plus some individual edges  To recover   we solve

trace cid       cid cid    log det       

 

minimize
    cid 

where  cid  is the sample covariance matrix and regulariz 

 cid   cid     cid   cid  

ers are applied to offdiagonal entries of   and    As
done in   for the linear model  the formulation   can be
seamlessly extended to the multitask setting where we wish
to estimate multiple precision matrices jointly  encouraging similar structure while allowing for some discrepancy
across them  This estimator is discussed in Hara   Washio
 

Equivalent Program  As shown in Yang   Ravikumar
  the formulation   can be rewritten as 

         cid cid 

 

 cid 

minimize

 
where  cid cid  is the in mal convolution of two regularizers
 cid cid    inf
   
It is known that  cid     cid  is   norm and its dual is de ned
    max cid cid cid cid      where     
as  cid cid 
        so that  cid     cid    is the dual norm of  cid     cid    see
Yang   Ravikumar   for details 

 cid cid     cid cid              

 cid 

 

  Dirty models with nonconvex regularizers

 cid cid      

 cid 

In this paper  we introduce and study estimators of the form

          

 

minimize

 
Here   is any regularizer inducing sparsity beyond the
 cid norm  note that the notation encapsulates the regularization parameter   itself within the regularizer  satisfying
the following conditions  Loh   Wainwright   

            and is symmetric 
For      
    is nondecreasing but        is nonincreasing
in    Besides        is differentiable for    cid    with
limt   cid 
     is convex for
 
some      
    There exists some scalar       such that
 cid 

        and is          

        when      

 

Following the notation of Loh   Wainwright   we
call    amenable if it satis es     and    
amenable if it additionally satis es    
The popular nonconvex regularizers SCAD  Fan   Li    and
MCP  Zhang    are both    amenable  Loh  
Wainwright   
The regularizer       nonconvex counterpart of the
group regularizer  cid     cid   employed in   where we use
  instead of  cid     cid  over groups 
           

where       cid   cid           cid Gq cid   cid  Example of
nonconvex regularizers include the GroupSCAD and
GroupMCP penalties where SCAD and MCP penalties are
respectively used on the norm of each group 
Remarkably  the proof techniques developed in this paper
make it possible to provide not only  cid error bounds under
milder conditions than prior work on convex problem  
but also support set recovery guarantees for nonconvex
one   In fact we shall see that dirty models with nonconvex regularizers   enjoy strictly better statistical guarantees than their convex counterpart   with practical consequences 

  Structural Incoherence  essential in prior

work  yet an unreasonable assumption

As our starting point  we focus on the case of convex
dirty models in   or equivalently in     key ingredient for showing statistical guarantees of regularized Mestimators is the decomposability of regularizer  Negahban
et al    However  considering the form of regularizer
in   it is not obvious to  nd the model space and its orthogonal complement with which we could directly derive

Sparse   Group Sparse Dirty Models

error bounds with optimal rates  To circumvent this problem  Yang   Ravikumar   utilize the decomposability
of each component separately  but this requires restricted
strong convexity  RSC  to hold jointly for all component
parameters  In order to have the  joint  RSC property from
 local  RSC with respect to each individual component 
Yang   Ravikumar   assume   structural incoherence
condition  Even if the loss function is strongly convex with
respect to each component  such incoherence across components is essential for the joint RSC due to the linearity
across components  To see this more clearly  suppose we
have the function    for        which is obviously strongly
convex  If we assume  however  that   is the sum of two
components           then one can immediately see that
        is not strongly convex jointly in   and   because
  and   are completely coherent in this one dimensional
example 
The problem is that the structural incoherence condition for
the  cid   cid   setting is way too restrictive because the sparse
and the groupsparse structures essentially share the same
model and its orthogonal spaces  In order to see this  we
consider the popular linear model setting   for example 
Let     and    be the support set of true sparse  groupsparse  component and sc be its complement  Furthern   cid   sc    denotes the projection of the sammore     
ple covariance onto sc     coordinate space  jth coordinate becomes zero if     sc      Projections on other
spaces are de ned similarly  Then  the structural incoherence condition for joint RSC can be reduced as  for all
          sc         bc   sc  bc 
    cid       

 
where  max  is the maximum singular value of   matrix    is the curvature of  restricted  eigenvalue condition  and   is some  xed constant  Informally  this condition requires the maximum singular value of sample covariance  modulo the projection onto the true model and
its orthogonal space  to be smaller than its minimum singular value  Note that for linear models  the curvature parameter of the eigenvalue condition is related to the minimum singular value of the sample covariance  This condition can be easily shown to fail in many cases  For instance consider the popular setting where the design matrix   is   set of samples from Gaussian ensemble with
covariance   and the true parameter is the sum of group
sparse     single nonzero component as depicted in Figure   Then  the incoherence condition in   implies
    cid   ij     min  which can be easily
maxi      
violated in many natural setting of   because the minimum
eigenvalue of   is smaller than the maximum element of  

 cid      

 cid   

 max

 Note that the sparse   group sparse setting is outstanding 
The structural incohence assumption makes sense in other dirty
models settings       sparse   low rank dirty models 

Figure   Example illustrating why the incoherence condition required by previous work fails to hold 

by the Rayleigh quotient 
This naturally leads to the following question 
Can we provide tight error bounds for the problem   not
requiring the joint RSC across individual structures and
hence bypassing the incoherence condition 

  Our key strategy  Constructing surrogate
components that are always decomposable 
In order to address the above question  our key proof technique is to establish the decomposability between two components of error vectors  by making the target components
dependent of our estimation  Consider arbitrary target parameter   such that           Note that we do not
impose additional constraints on de ning the sparse component   and the group sparse component   hence the
possible combination of     is not unique  As we
will see later  we provide estimation error bounds that depend on the selection of    more precisely on the
sparsity level of   and the group sparsity level of   In
that sense our theorems provide sets of estimation bounds 
However  it is important to note that we still do not need to
worry about the identi ability between structures  because
we only care about the  cid  and  cid  error rates of the  nal
or  summed  estimator  we do not recover  nor care about 
the individual components 

Suppose we compute cid  from the program   where  cid  and
 cid  are minimizing its dirty regularizer   Then  rather than
directly deriving error bounds of cid      from  cid      and
 cid      which are not decomposable  we introduce an ad 

ditional set of vectors      and   from the following rules 

  If  

      then       cid    and     

      then            

     
     cid   
   cid    and  
   cid    then       cid   and       

   cid   

  If  

 

  If  

    is de ned as the sum of   and  

Sparse   Group Sparse Dirty Models

     

     

     

     cid 

     cid 

     cid     

     cid     

     

     

vectors based on surrogates are decomposable  see text for details 

Figure   Example of constructing surrogate target parameters given                  cid  and      cid  via transformation     Error
sity pattern and values from   depending on cid  The same
             cid cid 

Illustrative example  Figure   describes an example 
consider         matrix parameter with   known groups in
rows  Suppose     the target parameter is given by      ii 
we de ne     and     as the sparse and group sparse components of   and  iii  the minimizer of program   are com 

By construction    is same as   but   has different spar 

holds for   as well  We denote the above transformation as

     cid     

     cid     

It turns out that the error vectors computed based on the
surrogate   and   are always decomposable as described
in the following proposition  and the consequence of this
decomposability plays   key role for showing     cid error
bounds without incoherence condition and ii  support set
recovery guarantee for nonconvex  cid     cid   dirty regularizers  with faster estimation rates than for convex dirty regularizers 

Proposition   Consider any local optimum  cid  of conT    cid cid  Then  the error vectors for individual
components       cid      and      cid      are decomposable in the sense that cid cid      
 cid cid              for all
   and the overall  cid  error  cid cid     cid  is lower bounded as

vex or nonconvex dirty models  and corresponding    

follows 

 cid cid     cid 

     cid cid 

     cid cid 
   

 
Moreover  let      supp   the support set of       
supp    supp    and     supp    supp 
Similarly  we also de ne      supp  and     
supp    Then  by construction of                 and
             However  it is always guaranteed that

puted as in     and     respectively for  cid  and  cid  Then     

and     show the error vectors for sparse and groupsparse
components  which are not decomposable   does not
hold for     and     On the other hand  for   in     and  
in     derived from     we can verify that surrogate error
vectors  shown in     and     are decomposable  at every

position   cid      and  cid      are sign consistent  or at least

one of them is zero 

  Statistical Guarantees of Models with

Convex Regularizers

Throughout our analysis  we assume that the loss function
   is twice differentiable and and satis es the restricted
strong convexity condition
 RSC  For any vector       Rp  the loss function   
satis es

 cid               

 cid 

 

     cid cid 
   
 cid cid     cid cid   

for all  cid cid         
for all  cid cid         

 cid   cid cid 

             

and              

 
where     represents the projection of   onto the   
coordinate space  that is         is    if        and  
otherwise 

RSC of the loss is also used to guarantee  cid consistency
 Negahban et al    Loh   Wainwright    or  cid 
consistency  Loh   Wainwright    of  clean  structurally constrained problems       problems with   single

 Theta       hat   hat Del Gam   bar   bar Del bar Gam barSparse   Group Sparse Dirty Models

structure  Note that there are slight variations in the definition of RSC conditions in the literature  Here we adopt
the form with tolerance terms in Loh   Wainwright  
  to allow for   wide class of non quadratic and or
nonconvex loss functions  We will show that RSC with
tolerance in dirty norm holds with high probability under
the popular setting of Gaussian ensembles  as an example 
For the analysis  we consider   slight modi cation of the
program  

      cid cid 

minimize
 cid cid  

 
where   is possibly nonconvex  but satis es  RSC  The
additional constraint  cid cid      also involves the dirty norm
  but with   different parameter vector   This constraint
is   safety radius commonly used for analyzing nonconvex
problems to ensure that the global minimum exists  see     
Loh   Wainwright     In practice  we can disregard this additional constraint 
Theorem   Consider the dirty model
for problem
  where   
satis 
 es
the restricted strong convexity  RSC  Suppose
that   is feasible and the regularization parameters
are set so that      cid   cid  and    
 cid   cid   
Suppose furthermore that    

 cid  Then  any lomin cid   
cal optimum cid  of   is guaranteed to be  cid  consistent 
sG cid 

 
where   is the number of nonzero elements in   and sG is
the number of nonzero groups in  

 cid cid cid     cid cid     

max cid 

  max     
 

is possibly nonconvex but

   
 

     

 

 

 

 

 

 

error bound in  

Remarks  The
scales with
          sG  at the same rate as previous analysis  Yang  
Ravikumar    for the sparse plus group sparse setting 
which required   much stringent incoherence condition 
as we already discussed 
It is also instructive to note
that Theorem   holds for any combination of    
such that           but different views of    
constructing   give different bounds depending on
sparsity group sparsity levels of             and sG 
In this sense  Theorem   provides   set of  cid  estimation
upper bounds 

Linear model and modi ed graphical Lasso 
In the following corollaries  we apply Theorem   to the linear model
  and the modi ed graphical Lasso problem   and derive their corresponding  cid  estimation bounds 
Corollary   Consider the linear model   Assume that
    each row Xi of the observation matrix   is independently sampled from        ii    is  group  column normalized by scaling as in  Negahban et al    and  iii 

 cid 

 

  is independent subGaussian with parameter   Now
suppose that in   we set        where   is the parameter for the group norm both for  cid cid  and  cid cid    constant

   only depends on   and            cid log    
and          cid      cid log      for   groups and
local optimum cid  satis es
 cid cid 
 cid cid     cid     

maximum group size    maxg    Gg  Suppose that
  is feasible to program   with these settings  Then
with probability at least        exp     
          any

 cid  sGm

 
where   is some constant depending on  
Corollary   Consider the modi ed graphical Lasso   to
estimate inverse covariance   Suppose we set the pa 
    maxg   cid cid cid      
rameters of   as           maxi cid  
     
  where       is the spectral norm of the mathis problem  Then  any local optimum cid  satis es
trix and       In addition  assume that   is feasible for
 cid cid     cid           max cid 

 cid cid cid ij    
 cid cid 
 cid cid    and    
sG cid   

 cid 

sG log  

  log  

     

max

 

 

 

 

 

 

ij

 

 

 

 

Remark  Since  cid cid  scales with  
  for the speci ed
values of   the constraint  cid cid      gets milder as   increases  It is also important to note that this constraint is
no more stringent than those of nonconvex analyses with
  single regularizer  Loh   Wainwright      their
constraints can be written as  cid cid       since    cid 
which directly implies  cid cid      since  cid cid     cid cid  by
the de nition of  cid     cid 

 cid log     for linear models for example  in our notation 

  Statistical Guarantees of Models with

Nonconvex Penalties

  natural extension of   is to incorporate nonconvex
regularizers that have some advantages such as unbiasedness  For that purpose  in this section we consider the following formulation

minimize
 cid cid  

         

 cid cid      

 

 cid cid           

where        inf  
  While the  cid  analysis in Theorem   can be extended
to nonconvex regularizers following proof techniques recently developed in Loh   Wainwright   using nonconvex unbiased regularizers has no bene   in terms of
asymptotic convergence rates of  cid  estimation errors  Instead  we here investigate the  cid norm bound and related
support set recovery guarantees where nonconvex unbiased regularizers help  To derive  cid  bounds  we use the

Sparse   Group Sparse Dirty Models

 

   
 

primaldual witness method described in the supplementary materials 
Theorem   Consider the dirty program with nonconvex penalties in   under  RSC  Suppose      
  max   
for some

     cid  max     
   cid  the strict dual feasibility
 cid  of   is supported by    recall     supp   

of primaldual witness holds  Then  any stationary point
supp  if the number of samples is large enough to
sG      for some constant   desatisfy max 
pending only on     and  

     
     

Also suppose that

    

 

 

 

 

 

 

    

primal dual witness 

enough sample size    the program   has   unique

As in Theorem   the decomposability in   and   with
respect to the surrogates   and   plays   crucial role in
establishing the support set recovery guarantee of any local
optimum in Theorem  
Based on Theorem   we can derive the  cid  bounds following the standard steps in  Loh   Wainwright   
Corollary   Suppose the assumptions in Theorem   hold 
Then 
  If  

sG cid  holds for large
 cid  max 
     
stationary point cid  speci ed by the construction of the
     cid      cid     cid dt  it holds
  Letting  cid      cid   
that  cid cid     cid     cid cid cid cid QU  
 cid cid   
 cid cid cid cid cid cid cid  where       denotes  
min   cid cid cid cid cid cid cid cid QU  
 cid cid   
min    cid cid cid cid QU  
if   is    amenable  and the minij  is lower 
 cid cid cid cid cid cid cid      max   
min   cid cid cid cid cid cid cid cid QU  
reduced to tighter bound as  cid cid     cid   
 cid cid cid cid QU  
 cid cid 
 cid    

mum absolute value  
bounded by  

matrix induced norm  maximum absolute row sum 

 cid    

 cid    

the error bound in the statement   is

min   minj  

  Moreover 

Then 

Multitask highdimensional
linear regression  We
consider the multitask highdimensional linear regression 
as   concrete example of using nonconvex dirty regularizers  This is the counterpart of model   which uses convex
dirty regularizer  In the following corollary  we analyze the
sparsistency of dirty multitask linear regression with nonconvex regularizers 

minimize

 Rp ms   cid cid  

  cid 

  

 
  

 cid cid               
 cid cid      

 cid cid 
        
 cid cid           
 

where        inf  
  Now  we derive   corollary for this particular nonconvex dirty model 

Corollary   Consider the multitask regression model 
Suppose that for each task  design matrix       is   zeromean Gaussian ensemble and is column normalized      
is independent subGaussian with parameter   Now suppose we set parameters of   as         constant  only

depends on   and         cid log pm       
  cid log       log     and   is feasible to program
 cid  with probability at least        exp    log pm   
   exp cid    log       log  cid   which is approaching to  
  supp cid    supp 

  with these settings  Then  for any local optimum

for some positive constants        

UkUk

 cid   

is bounded as follows 

  if additionally the regularizer   is    amenable
with      min   the minimum eigenvalue of  
and Cmin   mink    min

 cid      then
supp cid    supp  and the elementwise difference
 cid 
 cid     max   max
min    

 cid             
 cid    log pm 

nCmin
  min   

   

nCmin

  log pm 

provided that  
maxk      

UkUk

      max   

In order to highlight

Remark 
the bene   of using
   amenable regularizers  we brie   compare the result of Corollary   with that of  cid     cid   case in
 Jalali et al   
in  Jalali
et al    requires the incoherence on    speci cally 

Not only the result

 cid cid cid      but it also
 cid   
  term in  cid     max bound 

maxj     cid  

 cid cid   

  Uk

  Uk
 
  
Cmin

  
has an additional
Moreover 
to zero more slowly     cid 

the required   and   there can converge
and    cid 

log pm 

 

 
 
  

  log pm 

 
 
  

 

    log   

sm   log   

 

  Experiments
To illustrate the practical consequences of the superior statistical guarantees of models with nonconvex penalties 
we perform experiments on both simulated and realworld
data and compare convex and nonconvex dirty models for
sparse   groupsparse structures 

Simulated data  We consider multitask regression problems with       tasks and       variables for settings
of parameters     sG                  with
respectively less   more support overlap across tasks  recall
  and sG are the number of nonzero elements in   and
the number of nonzero groups in   respectively  The

Sparse   Group Sparse Dirty Models

Figure    cid error for comparison methods for varying sample size    Left  less sharing across tasks  Right  more sharing across tasks

rows of the design matrices   are sampled        from  
zeromean Gaussian distribution with correlation of   between feature pairs  For each set of parameters     sG 
we generate   instances of the problem where for each
instance the nonzero entries of the true model parameter matrix are        zeromean Gaussian to agree with  
and sG  Gaussian error with standard deviation of   is
added to each observation  For varying sample size   we
measure the  cid  error of parameters estimated by     convex dirty model  Jalali et al     ii  nonconvex dirty
model with SCAD   GroupSCAD penalty  and  iii  nonconvex dirty model with MCP   GroupMCP penalty  We
also evaluate the following baselines  Lasso  MCP  SCAD 
GroupLasso  GroupMCP and GroupSCAD   The regularization parameters of each method are tuned via  folds
crossvalidation  The results are presented in Figure    To
avoid cluttering the graphs  we do not display standard errors as these are much lower than the gaps between the
pertinent groups of methods  and we only display the best
group of baselines for each setting  As can be seen from
the  gure  dirty models with nonconvex penalties enjoy
superior performance over their counterparts with convex
penalties as   function of the sample size 
In terms of
computational cost   Group  coordinate descent steps for
 group  lasso   group  MCP and  group  SCAD all have
simple closedform expressions  Huang et al    similarly for proximalbased approaches  We noticed that for
  wide range of     nonconvex procedures took less
time and converged faster  See supplements  As future
work it would be interesting to study their theoretical numerical convergence rates 

Real data analysis  We consider the problem of predicting biological activities of molecules given features
extracted from their chemical structures  We analyze three biological activity datasets from the  molec 

 Our theorem on  cid  consistency requires the sample size to
be larger than the maximum of two terms  which precludes from
presenting graphs with curve alignment across    by rescaling the
xaxis with   control parameter as in Jalali et al   

ular activity challenge   http www kaggle com 
  MerckActivity  Speci cally we consider multitask
regression with three tasks corresponding to predicting the
raw value   log IC  of three different types of biological activities    binding to cannabinoid receptor    inhibition of dipeptidyl peptidase   and  time dependent    
inhibitions  For each task we used       observations
with       molecular features  We consider   random
data splits into training and validation sets  using   of the
data for tranining and   for validation  and report the average    over these random splits  As shown in table 
dirty models outperformed  clean  models suggesting the
importance to strike   balanc   between task speci city and
sharing for this data  Nonconvex dirty models achieved
the best    which illustrate their capability as   valuable
tool for highdimensional data analysis 

Table   Average    for comparison methods on molecular activity data

Method
Lasso
SCAD
MCP
GLasso
GSCAD
GMCP
Convex DM  Lasso GLasso 
Nonconvex DM  SCAD GSCAD 
Nonconvex DM  MCP GMCP 

  

     
     
     
     
     
     
     
     
     

  Concluding Remarks
This paper  nally resolved the outstanding case of sparse  
groupsparse dirty models with convex penalties  we provided the  rst satisfactory consistency results that do not
require implausible assumptions  thereby fully justifying
their practical success  In addition we proposed and studied dirty models with nonconvex penalties and showed that
they enjoy superior theoretical guarantees that translate into
signi cant practical impact  An interesting direction for
future work is to investigate whether our proof technique
might be applicable to other dirty models and beyond 

   perrorLassoSCADMCPConvex DM  Lasso GLasso Non convex DM  SCAD   SCAD Non convex DM  MCP   MCP   perrorGLassoGSCADGMCPConvex DM  Lasso GLasso Non convex DM  SCAD   SCAD Non convex DM  MCP   MCP Sparse   Group Sparse Dirty Models

Acknowledgments
     acknowledges
the support of MSIP NRF  National Research Foundation of Korea  via NRF 
       
for
Information   Communications Technology Promotion of Korea  via ICT     program  
 

and MSIP IITP  Institute

References
Agarwal     Negahban     and Wainwright        Noisy
matrix decomposition via convex relaxation  Optimal
rates in high dimensions  The Annals of Statistics   
   

Baraniuk     Compressive sensing  IEEE Signal Process 

ing Magazine     

Cand es        and Tao     The power of convex relaxation 
Information Theory 

Nearoptimal matrix completion 
IEEE Transactions on     

Cand es        Li     Ma     and Wright     Robust principal component analysis  Journal of the ACM   
May  

Jalali     Ravikumar     Sanghavi     and Ruan      
dirty model for multitask learning  In Neur  Info  Proc 
Sys   NIPS     

Loh     and Wainwright        Support recovery without
incoherence    case for nonconvex regularization  Arxiv
preprint arXiv   

Loh     and Wainwright        Regularized mestimators
with nonconvexity  Statistical and algorithmic theory for
local optima  Journal of Machine Learning Research
 JMLR     

Negahban     and Wainwright        Estimation of  near 
lowrank matrices with noise and highdimensional scaling  In Inter  Conf  on Machine learning  ICML   

Negahban     Ravikumar     Wainwright        and Yu 
     uni ed framework for highdimensional analysis
of Mestimators with decomposable regularizers  Statistical Science     

Raskutti     Wainwright        and Yu     Restricted
eigenvalue properties for correlated gaussian designs 
Journal of Machine Learning Research  JMLR   
   

Chandrasekaran     Sanghavi     Parrilo        and Willsky        Ranksparsity incoherence for matrix decomposition  SIAM Journal on Optimization   
   

Ravikumar     Wainwright        Raskutti     and Yu 
   Model selection in gaussian graphical models  Highdimensional consistency of  cid regularized mle  In Neur 
Info  Proc  Sys   NIPS     

Fan     and Li     Variable selection via nonconcave penalized likelihood and its oracle properties  Jour  Amer 
Stat  Ass    December  

Friedman     Hastie     and Tibshirani     Sparse inverse
covariance estimation with the graphical Lasso  Biostatistics   

Gong     Ye     and Zhang     Multistage multitask feature learning  In Pereira     Burges           Bottou    
and Weinberger         eds  Advances in Neural Information Processing Systems   pp     

Hara     and Washio     Learning   common substructure
of multiple graphical gaussian models  Neural Networks 
   

Hsu     Kakade        and Zhang     Robust matrix decomposition with sparse corruptions  Information Theory  IEEE Transactions on     

Huang     Breheny     and Ma       selective review of group selection in highdimensional models  Statist  Sci        doi   
 STS  URL http dx doi org 
 STS 

Ravikumar     Lafferty     Liu     and Wasserman    
Sparse additive models  Journal of the Royal Statistical Society  Series    Statistical Methodology   JRSSB 
   

Recht     Fazel     and Parrilo        Guaranteed
minimumrank solutions of linear matrix equations via
nuclear norm minimization  Allerton Conference  
Allerton House  Illinois   

Tibshirani     Regression shrinkage and selection via the
lasso  Journal of the Royal Statistical Society  Series   
   

Wainwright        Sharp thresholds for highdimensional
and noisy sparsity recovery using  cid constrained
quadratic programming  Lasso  IEEE Trans  Information Theory    May  

Yang     and Ravikumar     Dirty statistical models 

Neur  Info  Proc  Sys   NIPS     

In

Yang     Ravikumar     Allen        and Liu     Graphical models via univariate exponential family distributions  Journal of Machine Learning Research  JMLR 
   

Sparse   Group Sparse Dirty Models

Yuan     and Lin     Model selection and estimation in
regression with grouped variables  Journal of the Royal
Statistical Society       

Zhang        Nearly unbiased variable selection under minimax concave penalty  Annals of Statistics   
   

