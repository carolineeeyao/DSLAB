Neural Networks and Rational Functions

Matus Telgarsky  

Abstract

Neural networks and rational functions ef 
ciently approximate each other 
In more detail  it is shown here that for any ReLU network 
there exists   rational function of degree   poly log  which is  close  and similarly for any rational function there exists  
ReLU network of size   poly log  which
is  close  By contrast  polynomials need degree  poly  to approximate even   single
ReLU  When converting   ReLU network to  
rational function as above  the hidden constants
depend exponentially on the number of layers 
which is shown to be tight  in other words    compositional representation can be bene cial even
for rational functions 

  Overview
Signi cant effort has been invested in characterizing the
functions that can be ef ciently approximated by neural
networks  The goal of the present work is to characterize
neural networks more  nely by  nding   class of functions
which is not only wellapproximated by neural networks 
but also wellapproximates neural networks 
The function class investigated here is the class of rational
functions  functions represented as the ratio of two polynomials  where the denominator is   strictly positive polynomial  For simplicity  the neural networks are taken to
always use ReLU activation         max     for   review of neural networks and their terminology  the reader
is directed to Section   For the sake of brevity    network
with ReLU activations is simply called   ReLU network 

  Main results

The main theorem here states that ReLU networks and rational functions approximate each other well in the sense

 University of Illinois  UrbanaChampaign  work completed
while visiting the Simons Institute  Correspondence to  your
friend  mjt illinois edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Rational  polynomial  and ReLU network    to  spike 
  function which is    along     and   elsewhere 

that  approximating one class with the other requires  
representation whose size is polynomial in ln      rather
than being polynomial in  
Theorem  

  Let         and nonnegative inte 
           and
ger   be given  Let    
                 be polynomials of degree     
each with     monomials  Then there exists   function
              representable as   ReLU network of
size  number of nodes 

   ln     

  min

such that

 cid 
srk ln sr     sdk  ln dsr    cid cid 
 cid cid cid cid             

 cid cid cid cid     

    

sup

   

 

  cid 

  Let         be given  Consider   ReLU network
             with at most   nodes in each
of at most   layers  where each node computes    cid 
     cid        where the pair         possibly distinct
across nodes  satis es  cid   cid            Then there exists   rational function              with degree
 maximum degree of numerator and denominator 

  cid 

ln   kmk cid 
 cid cid             cid cid     

sup

   

such that

 spikeratpolynetNeural networks and rational functions

Perhaps the main wrinkle is the appearance of mk when
approximating neural networks by rational functions  The
following theorem shows that this dependence is tight 
Theorem   Let any integer       be given  There exists   function           computed by   ReLU network
with    layers  each with     nodes  such that any rational function           with       total terms in the
numerator and denominator must satisfy

              dx    
 

 

 

 cid 

 cid 

Note that this statement implies the desired dif culty of approximation  since   gap in the above integral     distance
implies   gap in the earlier uniform distance     and
furthermore an rdegree rational function necessarily has
         total terms in its numerator and denominator 
As    nal piece of the story  note that the conversion between rational functions and ReLU networks is more seamless if instead one converts to rational networks  meaning
neural networks where each activation function is   rational
function 
Lemma   Let   ReLU network             
be given as in Theorem   meaning   has at most   layers
and each node computes    cid       cid        where where the
pair         possibly distinct across nodes  satis es  cid   cid   
        Then there exists   rational function   of degree
  ln    so that replacing each    in   with   yields  
function              with

                 

sup

   

Combining Theorem   and Lemma   yields an intriguing corollary 
Corollary   For every       there exists   function
          computed by   rational network with     
layers and      total nodes  each node invoking   rational
activation of degree      such that every rational function
          with less than     total terms in the numerator
and denominator satis es

              dx    
 

 

 

Figure   Polynomial and rational    to the threshold function 

  Auxiliary results

The  rst thing to stress is that Theorem   is impossible
with polynomials  namely  while it is true that ReLU networks can ef ciently approximate polynomials  Yarotsky 
  Safran   Shamir    Liang   Srikant    on
the other hand polynomials require degree  poly 
rather than   poly ln 
to approximate   single
ReLU  or equivalently the absolute value function  Petrushev   Popov    Chapter   Page  
Another point of interest is the depth needed when converting   rational function to   ReLU network  Theorem  
is impossible if the depth is   ln  speci cally  it is
impossible to approximate the degree   rational function
   cid     with size   ln  but depth   ln 
Proposition   Set             the reciprocal map  For
any       and ReLU network           with   layers and
        nodes 

              dx    

 

Lastly  the implementation of division in   ReLU network
requires   few steps  arguably the most interesting being
   continuous switch statement  which computes reciprocals differently based on the magnitude of the input  The
ability to compute switch statements appears to be   fairly
foundational operation available to neural networks and rational functions  Petrushev   Popov    Theorem  
but is not available to polynomials  since otherwise they
could approximate the ReLU 

  Related work

 cid 

The hardto approximate function   is   rational network
which has   description of size      Despite this  attempting to approximate it with   rational function of the
usual form requires   description of size     Said another way  even for rational functions  there is   bene   to
  neural network representation 

The results of the present work follow   long line of work
on the representation power of neural networks and related functions  The ability of ReLU networks to    continuous functions was no doubt proved many times  but
it appears the earliest reference is to Lebesgue  Newman 
  Page   though of course results of this type are usu 

 threshratpolyNeural networks and rational functions

ally given much more contemporary attribution  Cybenko 
  More recently  it has been shown that certain function classes only admit succinct representations with many
layers  Telgarsky    This has been followed by proofs
showing the possibility for   depth   function to require exponentially many nodes when rewritten with   layers  Eldan   Shamir    There are also   variety of other
result giving the ability of ReLU networks to approximate
various function classes  Cohen et al    Poggio et al 
 
Most recently    variety of works pointed out neural networks can approximate polynomials  and thus smooth
functions essentially by Taylor   theorem  Yarotsky   
Safran   Shamir    Liang   Srikant    This
somewhat motivates this present work  since polynomials
can not in turn approximate neural networks with   dependence   poly log  they require degree   even
for   single ReLU 
Rational functions are extensively studied in the classical
approximation theory literature  Lorentz et al    Petrushev   Popov    This literature draws close connections between rational functions and splines  piecewise
polynomial functions    connection which has been used
in the machine learning literature to draw further connections to neural networks  Williamson   Bartlett    It
is in this approximation theory literature that one can  nd
the following astonishing fact  not only is it possible to approximate the absolute value function  and thus the ReLU 
over     to accuracy       with   rational function of
degree   ln   Newman    but moreover the optimal rate is known  Petrushev   Popov    Zolotarev 
  These results form the basis of those results here
which show that rational functions can approximate ReLU
networks   Approximation theory results also provide other
functions  and types of neural networks  which rational
functions can approximate well  but the present work will
stick to the ReLU for simplicity 
An ICML reviewer revealed prior work which was embarrassingly overlooked by the author 
it has been known 
since decades ago  Beame et al    that neural networks using threshold nonlinearities       the map    cid 
       can approximate division  and moreover the
proof is similar to the proof of part   of Theorem  
Moreover  other work on threshold networks invoked
Newman polynomials to prove lower bound about linear
threshold networks  Paturi   Saks    Together this
suggests that not only the connections between rational
functions and neural networks are tight  and somewhat
known unsurprising  but also that threshold networks and
ReLU networks have perhaps more similarities than what is
suggested by the differing VC dimension bounds  approximation results  and algorithmic results  Goel et al   

Figure   Newman polynomials of degree      

  Further notation

Here is   brief description of the sorts of neural networks
used in this work  Neural networks represent computation
as   directed graph  where nodes consume the outputs of
their parents  apply   computation to them  and pass the
resulting value onward 
In the present work  nodes take
their parents  outputs   and compute      cid        where
  is   vector    is   scalar  and         max     another popular choice of nonlineary is the sigmoid    cid 
    exp    The graphs in the present work are
acyclic and connected with   single node lacking children
designated as the univariate output  but the literature contains many variations on all of these choices 
As stated previously    rational function     Rd     is
ratio of two polynomials  Following conventions in the approximation theory literature  Lorentz et al    the denominator polynomial will always be strictly positive  The
degree of   rational function is the maximum of the degrees
of its numerator and denominator 

  Approximating ReLU networks with

rational functions

This section will develop the proofs of part   of Theorem   Theorem   Lemma   and Corollary  

  Newman polynomials

The starting point is   seminal result in the theory of rational functions  Zolotarev    Newman    there
exists   rational function of degree   ln  which can
approximate the absolute value function along     to
accuracy       This in turn gives   way to approximate
the ReLU  since

        max      

       

 

 

 

The construction here uses the Newman polynomials  New 

 Neural networks and rational functions

man    given an integer    de ne

Nr     

 
     exp   

  

  cid 

  

The Newman polynomials       and    are depicted
in Figure   Typical polynomials in approximation theory 
for instance the Chebyshev polynomials  have very active
oscillations  in comparison  the Newman polynomials look
  little funny  lying close to   over     and quickly increasing monotonically over     The seminal result of
Newman   is that

 cid  Nr      Nr   

Nr      Nr   

 cid cid cid cid cid cid      exp 

 cid cid cid cid cid       

sup
   

  

Thanks to this bound and eq    it follows that the ReLU
can be approximated to accuracy       by rational functions of degree   ln 
 Some basics on Newman polynomials  as needed in the
present work  can be found in Appendix   

  Proof of Lemma  

Now that   single ReLU can be easily converted to   rational function  the next task is to replace every ReLU in
  ReLU network with   rational function  and compute
the approximation error  This is precisely the statement of
Lemma  
The proof of Lemma   is an induction on layers  with
full details relegated to the appendix  The key computation  however  is as follows  Let      denote   rational
approximation to     Fix   layer       and let      denote the multivalued mapping computed by layer    and
let HR    denote the mapping obtained by replacing each
   in   with    Fix any node in layer       and let
   cid       cid           denote its output as   function of
the input  Then

 cid cid cid     cid                 cid HR        
 cid cid cid 
 cid cid cid     cid                  cid HR        
 cid cid cid 
 cid 
 cid 
 cid cid cid     cid HR               cid HR        
 cid cid cid 
 cid cid 
 cid 
 cid 
 cid cid cid     cid   cid cid        HR   cid 

For the  rst term   note since    is  Lipschitz and by
  older   inequality that

   cid cid cid   cid        HR   

 

 cid cid 

 

 

 

meaning this term has been reduced to the inductive hypothesis since  cid   cid      For the second term   if

Figure   Polynomial and rational    to  

  cid HR        can be shown to lie in      which is
another easy induction  then   is just the error between  
and    on the same input 

  Proof of part   of Theorem  

It is now easy to  nd   rational function that approximates
  neural network  and to then bound its size  The  rst step 
via Lemma   is to replace each    with   rational function   of low degree  this last bit using Newman polynomials  The second step is to inductively collapse the network
into   single rational function  The reason for the dependence on the number of nodes   is that  unlike polynomials  summing rational functions involves an increase in
degree 

    
    

 

    
    

 

                   

        

 

  Proof of Theorem  

The  nal interesting bit is to show that the dependence on
ml in part   of Theorem    where   is the number of
nodes and   is the number of layers  is tight 
Recall the  triangle function 

  

     

       
              
 

otherwise 

The kfold composition    is   piecewise af ne function
with     regularly spaced peaks  Telgarsky    This
function was demonstrated to be inapproximable by shallow networks of subexponential size  and now it can be
shown to be   hard case for rational approximation as well 
Consider the horizontal line through       The function    will cross this line    times  Now consider   rational function                  The set of points where
          corresponds to points where              

 ratpolyNeural networks and rational functions

  poor estimate for the number of zeros is simply the degree of       however  since   is univariate    stronger tool
becomes available  by Descartes  rule of signs  the number
of zeros in       is upper bounded by the number of
terms in        

  Approximating rational functions with

ReLU networks

This section will develop the proof of part   of Theorem   as well as the tightness result in Proposition  

  Proving part   of Theorem  

Figure   Polynomial and rational    to    

To establish part   of Theorem   the  rst step is to approximate polynomials with ReLU networks  and the second is to then approximate the division operation 
The representation of polynomials will be based upon constructions due to Yarotsky   The starting point is the
following approximation of the squaring function 
Lemma    Yarotsky    Let any       be given 
There exists             represented as   ReLU
network with   ln  nodes and layers  such that
supx                  and        
Yarotsky   proof is beautiful and deserves mention  The
approximation of    is the function fk  de ned as

fk            cid 

     

  

 

  

where   is the triangle map from Section   For every
   fk is   convex  piecewiseaf ne interpolation between
points along the graph of    going from   to       does
not adjust any of these interpolation points  but adds   new
set of      interpolation points 
Once squaring is in place  multiplication comes via the polarization identity xy                    
Lemma    Yarotsky    Let any       and      
be given  There exists                       represented by   ReLU network with   ln    nodes and
layers  with

           xy     

sup

    

and             if       or      

Next  it follows that ReLU networks can ef ciently approximate exponentiation thanks to repeated squaring 
Lemma   Let         and positive integer   be given 
There exists               represented by   ReLU
network with   ln    nodes and layers  with

 cid cid        xy cid cid     

sup

    

sup

    

 

 cid cid cid cid         
 cid cid cid cid     
 cid 

This proof relies on two tricks  The  rst is to observe  for
        that

 

Let  

size   cid min sr ln sr  sd ln dsr cid  which satis es

With multiplication and exponentiation    representation
result for polynomials follows 
Lemma   Let         be given 
 
           denote   polynomial with    
monomials  each with degree     and scalar coef 
 cient within    
Then there exists   function
           computed by   network of
 
supx                   
The remainder of the proof now focuses on the division operation  Since multiplication has been handled  it suf ces
to compute   single reciprocal 
Lemma   Let         and nonnegative integer   be
given  There exists   ReLU network                  
of size      ln  and depth      ln  such that

 
 

 

 

          

 

        

  

Thanks to the earlier development of exponentiation  truncating this summation gives an expression easily approximate by   neural network as follows 
Lemma   Let           and       be given  Then there
exists   ReLU network           with   ln   
layers and        ln    nodes satisfying

 cid cid cid cid         

 

 cid cid cid cid     

sup
      

Unfortunately  Lemma   differs from the desired statement Lemma   inverting inputs lying within       requires     ln  nodes rather than      ln 

 ReLUratpolyNeural networks and rational functions

To obtain   good estimate with only   ln  terms of
the summation  it is necessary for the input to be   bounded
below by   positive constant  not depending on    This
leads to the second trick  which was also used by Beame
et al   
Consider  for positive constant       the expression

 
 

 

 

        cx 

   

    cx   

 cid 

  

If   is small  choosing   larger   will cause this summation
to converge more quickly  Thus  to compute    accurately
over   wide range of inputs  the solution here is to multiplex approximations of the truncated sum for many choices
of    In order to only rely on the value of one of them  it
is possible to encode   large  switch  style statement in  
neural network  Notably  rational functions can also representat switch statements  Petrushev   Popov    Theorem   however polynomials can not  otherwise they
could approximate the ReLU more ef ciently  seeing as it
is   switch statement of      degree   polynomial  and     
degree   polynomial 
Lemma   Let             reals              
an   an  and   function         an      be given 
Moreover  suppose for                  there exists   ReLU
network gi         of size   mi and depth   ki with
gi        along  ai  ai  and

sup

 gi            

Then there exists   function           computed by  

  ai ai 

ReLU network of size   cid   ln     cid 
  cid ln      maxi ki

 cid  satisfying

 cid  and depth

  mi

                 

sup

    an 

  Proof of Proposition  

It remains to show that shallow networks have   hard time
approximating the reciprocal map    cid     
This proof uses the same scheme as various proofs in  Telgarsky    which was also followed in more recent
works  Yarotsky    Safran   Shamir    the idea
is to  rst upper bound the number of af ne pieces in ReLU
networks of   certain size  and then to point out that each
linear segment must make substantial error on   curved
function  namely    
The proof is fairly brute force  and thus relegated to the
appendices 

  Summary of  gures
Throughout this work    number of  gures were presented
to show not only the astonishing approximation properties

Figure   Polynomial and rational    to  

of rational functions  but also the higher  delity approximation achieved by both ReLU networks and rational functions as compared with polynomials  Of course  this is only
  qualitative demonstration  but still lends some intuition 
In all these demonstrations  rational functions and polynomials have degree   unless otherwise marked  ReLU networks have two hidden layers each with   nodes  This is
not exactly apples to apples       the rational function has
twice as many parameters as the polynomial  but still reasonable as most of the approximation literature  xes polynomial and rational degrees in comparisons 
Figure   shows the ability of all three classes to approximate   truncated reciprocal  Both rational functions and
ReLU networks have the ability to form  switch statements  that let them approximate different functions on different intervals with low complexity  Petrushev   Popov 
  Theorem   Polynomials lack this ability  they can
not even approximate the ReLU well  despite it being low
degree polynomials on two separate intervals 
Figure   shows that rational functions can    the threshold
function errily well  the particular rational function used
here is based on using Newman polynomials to approximate            Newman   
Figure   shows Newman polynomials          As discussed in the text  they are unlike orthogonal polynomials 
and are used in all rational function approximations except
Figure   which used   least squares    
Figure   shows that rational functions  via the Newman
polynomials       very well  whereas polynomials have
trouble  These errors degrade sharply after recursing 
namely when approximating   as in Figure  
Figure   shows how polynomials and rational functions   
the ReLU  where the ReLU representation  based on Newman polynomials  is the one used in the proofs here  Despite the apparent slow convergence of polynomials in this
regime  the polynomial    is still quite respectable 

 ratpolyNeural networks and rational functions

Liang  Shiyu and Srikant     Why deep neural networks

for function approximation  In ICLR   

Lorentz        Golitschek  Manfred von  and Makovoz 
Yuly  Constructive approximation   advanced problems 
Springer   

Newman        Rational approximation to     Michigan

Math          

Paturi  Ramamohan and Saks  Michael    Approximating
Inf  Comput 

threshold circuits by rational functions 
   

Petrushev        Penco Petrov and Popov  Vasil    Rational
approximation of real functions  Encyclopedia of mathematics and its applications  Cambridge University Press 
 

Poggio  Tomaso  Mhaskar  Hrushikesh  Rosasco  Lorenzo 
Miranda  Brando  and Liao  Qianli  Why and when can
deep   but not shallow   networks avoid the curse of
dimensionality    review    arXiv 
 cs LG 

Safran  Itay and Shamir  Ohad  Depth separation in relu
networks for approximating smooth nonlinear functions    arXiv   cs LG 

Telgarsky  Matus  Representation bene ts of deep feedforward networks    arXiv   
 cs LG 

Telgarsky  Matus  Bene ts of depth in neural networks  In

COLT   

Williamson  Robert    and Bartlett  Peter    Splines  ratio 

nal functions and neural networks  In NIPS   

Yarotsky  Dmitry  Error bounds for approximations with
  arXiv 

deep relu networks 
 cs LG 

Zolotarev       Application of elliptic functions to the problem of the functions of the least and most deviation from
zero  Transactions Russian Acad  Scai  pp     

  Open problems
There are many next steps for this and related results 

  Can rational functions  or some other approximating
class  be used to more tightly bound the generalization properties of neural networks  Notably  the VC
dimension of sigmoid networks uses   conversion to
polynomials  Anthony   Bartlett   

  Can rational functions  or some other approximating
class  be used to design algorithms for training neural
networks  It does not seem easy to design reasonable
algorithms for minimization over rational functions 
if this is fundamental and moreover in contrast with
neural networks  it suggests an algorithmic bene   of
neural networks 

  Can rational functions  or some other approximating
class  give   suf ciently re ned complexity estimate
of neural networks which can then be turned into  
regularization scheme for neural networks 

Acknowledgements
The author thanks Adam Klivans and Suvrit Sra for stimulating conversations  Adam Klivans and the author both
thank Almare Gelato Italiano  in downtown Berkeley  for
necessitating further stimulating conversations  but now on
the topic of health and exercise  Lastly  the author thanks
the University of Illinois  UrbanaChampaign  and the Simons Institute in Berkeley  for  nancial support during this
work 

References
Anthony  Martin and Bartlett  Peter    Neural Network
Learning  Theoretical Foundations  Cambridge University Press   

Beame  Paul  Cook  Stephen    and Hoover     James 
Log depth circuits for division and related problems 
SIAM Journal on Computing     

Cohen  Nadav  Sharir  Or  and Shashua  Amnon  On the
expressive power of deep learning    tensor analysis 
  COLT 

Cybenko  George  Approximation by superpositions of  
sigmoidal function  Mathematics of Control  Signals and
Systems     

Eldan  Ronen and Shamir  Ohad  The power of depth for

feedforward neural networks  In COLT   

Goel  Surbhi  Kanade  Varun  Klivans  Adam  and Thaler 
Justin  Reliably learning the relu in polynomial time  In
COLT   

