GSOS  GaussSeidel Operator Splitting Algorithm for
MultiTerm Nonsmooth Convex Composite Optimization

Li Shen   Wei Liu   Ganzhao Yuan   Shiqian Ma  

Abstract

In this paper  we propose   fast GaussSeidel
Operator Splitting  GSOS  algorithm for addressing multiterm nonsmooth convex composite optimization  which has wide applications in
machine learning  signal processing and statistics  The proposed GSOS algorithm inherits the advantage of the GaussSeidel technique to accelerate the optimization procedure  and leverages the
operator splitting technique to reduce the computational complexity  In addition  we develop
  new technique to establish the global convergence of the GSOS algorithm  To be speci    we
 rst reformulate the iterations of GSOS as   twostep iterations algorithm by employing the tool of
operator optimization theory  Subsequently  we
establish the convergence of GSOS based on the
twostep iterations algorithm reformulation  At
last  we apply the proposed GSOS algorithm to
solve overlapping group Lasso and graphguided
fused Lasso problems  Numerical experiments
show that our proposed GSOS algorithm is superior to the stateof theart algorithms in terms of
both ef ciency and effectiveness 

  Introduction
In this paper  we focus on the multiterm nonsmooth convex composite optimization

  cid 

  

 

gi   

min
           
where   is   linear space  gi
          is
  proper  lower semicontinuous convex function for all            and             is   continuous
 Tencent AI Lab  China  Sun Yatsen University  China
 The Chinese University of Hong Kong  China  Correspondence to  Li Shen  mathshenli gmail com  Wei Liu  wliu ee columbia edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

differentiable convex function with its gradient satisfying
the inequality that

 cid cid               cid cid   cid                      cid   

 
 

 cid  

regularization term cid  

The above multiterm nonsmooth convex composite optimization problem   covers   large class of applications
in machine learning such as simultaneous lowrank and sparsity  Richard et al    Zhou et al    overlapping group Lasso  Zhao et al    Jacob et al   
Mairal et al    graphguided fused Lasso  Chen et al 
  Kim   Xing    graphguided logistic regression  Chen et al    Zhong   Kwok    variational image restoration  Combettes   Pesquet    Dup  
et al    Pustelnik et al    and other types of structure regularization paradigms  Teo et al      By
introducing the multiterm nonsmooth regularization term
   gi    such as structured sparsity  Huang et al   
Bach et al    Bach    and nonnegativity  Chen  
Plemmons    Xu   Yin    more prior information can be included to enhance the accuracy of regularization models  However  due to the multiterm nonsmooth
   gi    the optimization problem
  is too complicated to be solved even for small    For
      some existing popular  rstorder optimization
methods are accelerated proximal gradient method  Beck
  Teboulle    Nesterov    smoothing accelerated proximal gradient method  Nesterov        three
operator splitting method  Davis   Yin    and some
primaldual operator splitting methods such as majorized
alternating direction method of multiplier  ADMM   Cui
et al    Lin et al    fast proximity method  Li  
Zhang    and so on 
On the other hand  when       there also exist some algorithms for solving problem     directly method for
  is smoothing accelerated proximal gradient  SAPG 
proposed by Nesterov  Nesterov        Then  Yu  Yu 
  proposed   new approximation method called PAAPG for handling   by combining the proximal average
approximation technique and Nesterov   acceleration technique  which has been enhanced very recently by Shen
et al   Shen et al    Their proposed method called
APAAPG adopts an adaptive stepsize strategy  However 

GSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization

the above mentioned methods SAPG  PAAPG and its enhanced version APAAPG all need   strict restriction on the
nonsmooth functions  gi    that each gi    must be Lipschitz continuous  In addition  some primaldual parallel
splitting methods  BricenoArias et al    Combettes
  Pesquet      Condat           generalized from traditional operator splitting  such as forward
backward splitting method  Chen   Rockafellar    and
Douglas Rachford splitting method  Eckstein   Bertsekas 
  can also solve the multiterm nonsmooth convex
composite optimization problem   Different from prior work  Raguet et al   Raguet et al    proposed an
ef cient primal operator splitting method called generalized forward backward splitting method using the classic
forward backward splitting technique  which has shown
the superiority over numerous existing primaldual splitting
methods  Monteiro   Svaiter    Combettes   Pesquet 
  Chambolle   Pock    in dealing with variational image restoration problems  All the above mentioned
methods for problem   with       share   common
feature that they all split the nonsmooth composite term
   gi    in the Jacobi iteration manner       parallelly 
This is one of the main differences between existing splitting methods and our proposed method in this paper 

 cid  
To split the nonsmooth composite term cid  

   gi    more
ef ciently  we propose   novel operator splitting algorithm
to solve problem   by harnessing the advantage of GaussSeidel iterations 
the computation of the proximal
mapping of the current function gi    uses the proximal
mappings of gj    for all       which have already
been computed ahead  In addition  to further improve the
algorithm   ef ciency  we leverage the overrelaxation
acceleration technique  What   more  we provide   new
strategy that the overrelaxation stepsize can be determined
adaptively  ensuring   larger value to accelerate the algorithm  The most important is that the convergence of our
proposed GSOS algorithm is established by   newly developed analysis technique  In detail  given an invertible linear
operator    we  rst argue that the optimal solution set
    of problem   can be recovered
 
This is ful lled through adopting the tool of operator
optimization theory 
in which the composite operator
SR            NV is generalized from the de nition of
the composite monotone operator       in  Eckstein
  Bertsekas    Next  by unitizing the de nition of
the  enlargement of maximal monotone  Burachik et al 
    Burachik   Svaiter    Svaiter   
we establish   key property for SR            NV  
 
that
Based on
this observation  we equivalently reformulate the GSOS
algorithm as   twostep iterations algorithm  Then  the

    cid  
by the zero point set cid   SR            NV

gph cid SR            NV
gph cid     SR            NV  cid 

 cid 

    gi 

    

 cid 

is 

global convergence of the proposed GSOS algorithm is
easily established based on this reformulation 
The closest algorithm to our proposed GSOS algorithm
is the generalized forward backward splitting method proposed by Raguet et al   Raguet et al    By carefully
selecting the scaling matrix   in the forthcoming GSOS
algorithm  it is easy to check that GSOS covers the generalized forward backward splitting method as   special case 
Another highly related algorithm to our proposed GSOS algorithm is the matrix splitting method  Luo   Tseng   
Yuan et al    Choosing the scaling matrix   suitably 
the proposed GSOS algorithm can inherit the advantage of
the matrix splitting technique which has shown the ef ciency in  Yuan et al    for coping with   special class of
coordinate separable composite optimization problems 
The rest of this paper is organized as follows  In Section  
we  rst give the de nitions of some useful notations which
can make the paper much more readable  We also establish
some lemmas and propositions based on monotone operator theory  Bauschke   Combettes    which are the
key to the convergence of the GSOS algorithm  In Section
  we present the proposed GSOS algorithm and then analyze its convergence and iteration complexity  In Section
  we conduct numerical experiments on overlapping group
Lasso and graphguided fused Lasso problems to evaluate
the ef cacy of the GSOS algorithm  Finally  we draw conclusions in Section  

  Preliminaries and Notations

   Xi be the product space of Xi with Xi    
for all              Let   be   linear space and   
be its complementary space with the following de nitions

 cid      cid           cid 

yi    cid 

Let    cid  
   cid                  yn
 cid  IX

IX  cid 

 

Let IX         be the identity map and EY  
      be   block linear operator de ned as EY  
  Let           be   linear operator de ned as Ay    
   yi  Hence  its
adjoint operator            is de ned as        
nEY   
Let             be block lower triangular linear invertible operators satisfying         and         cid   
Moreover    is de ned as

 cid  

       
 

nE 

  
 
Hn 
Hn 

 
 

 

 
 
  Hn   
 

 
 
Hn   Hn  

 

 

 

   

where Hi           is   linear operator for all         
       It is worthwhile to emphasize that Hi   is also
possible to be   lower triangular linear operator satisfying

GSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization
     cid    Next  we abuse the notation  cid     cid   which

   cid Ax Ax cid  In addition  if   further satis es

with      
inequalities   it holds that

Hi       
is induced by the inner product  cid   cid  satisfying

 cid     cid      cid cid   cid   cid cid   cid 

 cid 

 cid 

 

      

 

 cid     cid     cid      

 

 

 

In addition  we de ne the generalized proximal mapping of
  proper  lower semicontinuous convex function gi    with
respect to the invertible linear operator Hi   
De nition   For   given    the proximal mapping denoted
    of   proper  lower semicontinuous conby ProxH 
    gi
vex function gi with respect to an invertible linear operator
     cid    is de ned to be the zero
Hi   satisfying Hi       
point of the following inclusion equation
     gi    Hi        

 
Moreover  if Hi   is symmetric  it can be reformulated as
the following convex minimization

ProxHi igi      arg min

    gi     

 cid       cid Hi  

 

 
 

Next  we recall the de nition of  enlargement of monotone
operators  Burachik et al      Burachik   Svaiter 
  Svaiter    which is an effective tool for establishing the convergence of the proposed GSOS algorithm 

De nition   Given   maximal monotone operator    
       the      enlargement of   is de ned as the

set        cid          cid              cid      for all    
            cid 
isfying inequality   There exits    cid     cid   cid   cid  LI such

Recall that       is   gradient Lipschitz convex function satthat the following two inequalities hold for any      cid     

 

 cid       cid cid cid 
 cid       cid cid 
 

 
 
 
 

 

 

             cid     cid      cid        cid cid   
             cid     cid      cid        cid cid   

Actually  when       is   quadratic function  it holds    cid 

directly in inequalities   and   The following lemma
establishes the property of the enlargement of the composite operator          with   satisfying inequalities  
  or   which is an essential ingredient for reformulating
the GSOS algorithm as   twostep iterations algorithm 

Proposition   Assume that   is   gradient Lipschitz continuous convex function satisfying inequality   For any
           it holds that

                                 

 

                                 

 

with      

 cid Ax    Ax cid 

 cid 

 

Remark   Two comments are made for Proposition  

  This proposition gives two types of estimations for   in

 cid        cid  in   and   When   is   quadratic

function  it is easy to check that
   
 

 cid Ax    Ax cid 

 cid 

 
 

due to  cid       cid  LI  When   is   general gradient

Lipschitz continuous function  we do not know which
estimation for   is tighter in   and  

 cid Ax    Ax cid 

  The second part of this proposition can be regarded as
an intensi ed version of Lemma   in  Svaiter   
for   speci ed composite operator              The
 rst part of the proposition coincides with the results
by applying Lemma   in  Svaiter    for     
       

Next  we generalize the notation        in  Eckstein  
Bertsekas    for   given       and two maximal monotone operators      as SR      for   given invertible linear operator   de ned as

 cid 
gphSR       
 

 

      Ry                     

                               

 cid 
 
By  Eckstein   Bertsekas    we know that        is
maximal monotone if    and    are both maximal monotone  However  its generalized operator SR      is not
monotone unless the invertible linear operator   reduces
to be   constant  Very interesting  it can be shown that its
composition with             SR      is maximal monotone for any invertible linear operator   
Lemma   For any given invertible linear operator    operator    SR      is maximal monotone if    and   
are both maximal monotone operators 
Setting                             NV  we obtain
gph cid SR          NV
SR            NV   which is de ned as
 cid 

 
     Ry                               

 cid 

 

     NV                          

 cid 

 

GSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization

By Lemma   we know that    SR            NV
is maximal monotone due to the maximal monotonicity
of                  and NV  Hence  given   constant       the enlargement    SR            NV  
In addition  based on the de nition of
is well de ned 
SR      again  we set                         or
                       and      NV in  
Then we have the de nition of SR         NV or
SR         NV for any given invertible linear opgph cid SR         NV
erator   and constant       as follows
 cid 
gph cid SR         NV
 cid 
 cid 

 
     Ry                    

 
     Ry                      

     NV                          

 cid 

 cid 

 

 

 

     NV                          

 cid 

 

In the proposition below  we will establish the relationships among the above mentioned three operators SR         NV   SR         NV and
   SR            NV  
Proposition   Given   constant       and an invertible
linear operator    it holds that

 cid 
gph cid SR            NV
  gph cid SR            NV
 cid 
  gph cid     SR            NV  cid 
the optimal solution set     cid  
lem   and  cid   SR            NV
 cid 
through cid   SR            NV
 cid 
 cid 
     cid   SR            NV
that cid 

Lemma   Let
isfy         and   satisfy  
 

In the following  we establish the relationship between
    of prob 
  which
means that we can recover the solution of problem  

operators   and   satDenote
It holds

   cid   TY   EY cid   TY   cid cid 

 cid 

  cid 

    

    gi 

linear

 

 gi

  

  GSOS Algorithm
In this section  we  rst propose the GaussSeidel operator
splitting algorithm for solving the multiterm nonsmooth
convex composite problem   Then  based on the preliminaries in Section   we establish the convergence and
iteration complexity of the GSOS algorithm 

Algorithm   GSOS Algorithm

 cid  where   and   are

Parameters  Choose           linear operator  
satisfying   and   starting point         Set      

 cid  and      cid       
 cid       
xk   EY cid   TY HEY cid   TY Hzk 
 cid   
     cid  
     cid   

  ProxH 
    gi
   xk

de ned via equations     and     respectively 
for               do
for             do

 cid 
   Hi   xk

yk
 
       

   Hi jyk
   

 cid  

    zk

     

 

 

 

 

as     and  adap 

end for
set  adap 
set           adap 
zk    zk          yk   xk 

return     cid   TY   EY cid   TY   zK 

as    
         adap 
 

end for

 

 

 

In Algorithm   parameters      adap 

   adap 

are de 

    max

    max

 ned as 

 adap 
 

 adap 
 

 cid 
 cid 

 

 

              LA    cid   
               

 cid 
 cid 
   cid        cid   

 

        cid   xk   yk cid 
 cid xk   yk cid     
 cid 
 cid   xk   yk cid 
 cid xk   yk cid     

     

 

 

     

   

   

   

Remark   We make some comments on GSOS below 

     Hij

notations   and EY 

 cid cid  
EY cid cid  
 cid  
  For the updating step of xk  we obtain xk  
    Hijzk
 cid cid  
 cid cid  
 cid  
  by using the
to compute the inverse of  cid  
Similarly  we have     
      
     Hij
    Hence  we need
     Hi    However  if
Hi   is   lower triangular matrix operator  xk and   
can be obtained easily 

jizk

  

  

  By the de nitions of ProxH 
    gi

solve the following inclusion equation

and yk  we need to

    Hi iyk
 cid cid  
     gi yk
Gk
   
     cid   
   Hi   xk
   Hi jyk

 cid  
      
   xk

     
where Gk
       
 
easy to choose   suitable Hi   such that the solution
of the above inclusion equation has   closed form 

 cid  Usually  it is

    zk

   

 

 

GSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization

     is the overrelaxation stepsize for accelerating the
the computations of  adap 
are time consuming  we can set     

GSOS algorithm 
and  adap 
max       

If

 

 

  When   is   diagonal matrix       Hi       and Hi    
aiI with some nonnegative constant ai  and the over
relaxation stepsize    is  xed to   smaller region  the
GSOS algorithm reduces to the generalized forward
backward splitting method in  Raguet et al   

tion de ned as         cid  

In the following  we reformulate the GSOS algorithm as
  twostep iterations algorithm by utilizing monotone optimization theory established in Section   which is the key
to the convergence of the GSOS algorithm 
Proposition   Let             be the funci  gi xi  Assume that the
sequences  xk  yk  and zk are generated by Algorithm
  with         Let vk      xk   yk  and
zk   yk       zk   xk  Then  for all       
there exists        such that the iterations in Algorithm
  can be reformulated as the following twostep iterations
algorithm 

  vk      SR            NV    zk 

   cid   vk cid       cid   vk   zk   zk cid   

      cid zk   zk cid   

and zk    zk            vk 
Remark   Based on Proposition   the GSOS algorithm
can be regarded as an inexact overrelaxed metric proximal
point algorithm for the composite inclusion

   

   

       SR          NV    

By Proposition   and Lemma   we can establish the convergence of the GSOS algorithm based on the relationship
between the two zero point sets    
 gi  and  

  cid 

  

Theorem   Let  xk  yk  zk  be the sequence generated
by Algorithm   We have 
    for any         SR          NV   it

holds that
 cid zk      cid       cid zk     cid   

 
             cid xk   yk cid   
to
zero
   SR          NV  
to   point belonging to
         the optimal solution set

belonging

point

to

 

 ii  zk

point
and   

converges
set
converges
    gi 

    cid  

of problem  

Theorem   indicates that  cid xk   yk cid  approaching to zero
implies the convergence of the GSOS algorithm 
In the
theorem below  we measure the convergence rates of two
sequences  cid xk   yk cid  and  cid        cid 
Theorem   Let zk be the sequence generated by the GSOS
algorithm  Then  there exists              such that

 cid xi   yi cid      cid   

 cid   cid cid         cid cid      cid   

 cid 

 

 

Due to the space limit  all proofs of the propositions  lemmas and theorems are placed into the supplementary material 
  Experiments
In this section  we apply the proposed algorithm to the
overlapping group Lasso  Zhao et al    Jacob et al 
  Mairal et al    and graphguided fused Lasso
problems  Chen et al    Kim   Xing    which
can be formulated as

min

 
 

 cid Sx     cid   

gi   

 

  cid 

  

 cid   
    
    

 

For overlapping group Lasso problem   gi     
   cid xGi cid  and   denotes the number of groups  For graphguided Lasso problem   gi       ij cid xi xj cid  and  
denotes the number of edges in the graph edge set   
We describe the detailed techniques in the experimental implementation for   Given      
  and   positive de nite
operator   satisfying    cid  ST    we set

 

Hi    

                
                

  

 
it easy to check that            DA  

   Diag cid EYD cid   cid    Due to the smooth term in overlapHence 
   
speci      we obtain cid  
ping group Lasso   is quadratic  the two estimations  
and  adap 
in     and     are preferred to be used  By
 cid  
 cid  
 cid  
     Hi          
  and
 cid cid  
ther imply xk  cid cid  
 cid  
    Hi jzk
        zk
    which furi   Hi jzk
     Hi  
 cid  
   
 cid  
it holds that  cid  
        zk
    aK   Moreover  by the positive de niteness
 cid  
      
of Hi   and   
   
           zk
 cid  
 
    Hence  we attain     
  
      zk
    aK   In addition  by the de nition of    we
reformulate the estimation     for    as the following
form 

     

  izk

   

  

  

  

 

 

 cid 
    EY cid         ST   cid   
        cid     cid Diag EYD   cid   

 

 cid 

 

    max

GSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization

  and the positive de niteness of      suf 
Due to      
cient condition satisfying the constraint in the above set is

 cid         ST    cid         cid  Hence  we have an alter 
    max cid             ST    cid         cid   

native estimation for   as

 

Similarly  the adaptive stepsize estimation     is reformulated as

 cid cid cid cid    cid 

  

 cid cid cid cid 

 xk   yk
   

 adap
 

     

  cid 

   

 

  cid 

  

   

ST  

 

 

 xk   yk

   THij xk   yk
   

Therefore  the GSOS algorithm can be speci ed as the following form for solving problem  

Algorithm   GSOS Algorithm for Solving Problem  
Parameters  Choose         positive de nite operators   and Hi   satisfying   and   starting point
for             do

        Set   as   and     cid       

 cid 

 cid  
      zk
 cid   
     cid  
       
xk  
for             do
  ST  Sxk       cid   
    gi

  ProxH 

yk
 
 

 

end for
set        
for             do

     adap

 

   Hi jyk
  where  adap

 

  zk

           yk

    xk 

   cid 

zk 
 
end for

end for
return     

 cid  
    zN
       

 

   Hi   xk   zk

     

is de ned via  

  PAAPG  Yu    Proximal Average approximated
Accelerated Proximal Gradient  PAAPG  algorithm
 Yu    is   primal  rstorder method  which utilizes the proximal average technique  Bauschke et al 
  to separate the multiterm nonsmooth function
in   It has been shown to outperform the smoothing accelerated proximal gradient method  Nesterov 
     

  APAAPG  Shen et al    An enhanced version of PAAPG  which incorporates the Adaptive
Proximal Average approximation technique with the
Accelerated Proximal Gradient  APAAPG  method
to improve the ef ciency of the optimization procedure 

It is worthwhile to emphasize that PAAPG and APAAPG
algorithms can only be applied to   speci   class of problems   in which the multiterm nonsmooth regularization
is Lipschitz continuous  Since the nonsmooth regularization terms in overlapping group Lasso and graphguided
fused Lasso are all exactly Lipschitz continuous  the two
ef cient solvers PGAPG  Yu    and its enhanced version APAAPG  Shen et al    are also compared with
the GSOS algorithm to illustrate the ef cacy of GSOS  In
the implementation  the approximation parameter for PAAPG is set as       

  Overlapping Group Lasso

In this subsection  we apply the proposed GSOS algorithm
to the overlapping group Lasso problem  which takes the
following formal de nition 

min

 
 

 cid Sx     cid     

   cid xGi cid 

 

  cid 

  

In this paper  we compare the proposed GSOS algorithm
with four stateof theart algorithms below 

  GFB  Raguet et al    Generalized Forward
Backward  GFB  splitting algorithm is   primal  rstorder operator splitting algorithm for solving   proposed by Raguet et al   Raguet et al    which
has been shown to outperform other competing algorithms such as  Monteiro   Svaiter    Combettes
  Pesquet    Chambolle   Pock    for variational image restoration 

  PDM  Condat       rstorder PrimalDual splitting Method  PDM   Condat    for solving jointly
the primal and dual formulations of largescale convex
minimization problems involving Lipschitz  proximal
and linear composite terms 

 cid Gj  cid    for some       xGi   Rd is  

overlapping groups  Gi          satisfying cid  

where     Rn   is the sampling matrix    is the noisy
observation vector           GK  denotes the set of
   Gi  
       and Gi
duplication of   with       Gi        is the weight
for the ith group  and   is the regularization parameter
controlling group sparsity 
During the implementation of Algorithm   we need to calculate the generalized proximal mapping of  cid xGi cid  in the
    By the positive de niteness of Hi   
updating step of yk
the calculation of yk
  in Algorithm   is equivalent to solving the following problem 

yk
    arg min

  cid   

where bk     
   Hi jyk

   

 

 

 cid     bk cid Hi  

 
 cid cid  
 
 cid  In the proposition below  given    diagj  Hi   xk   zk

     cid xGi cid 
       

  ST  Sxk  

GSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization

Figure   Objective value vs  iteration on overlapping group Lasso 

Figure   Objective value vs  iteration on overlapping group Lasso 

onal positive de nite operatpr Hi   and group    we solve

     arg min

 
When Hi   is identity matrix      has the closedform
solution

 

 cid       cid Hi  

   cid xG cid 

 
 

 cid    

    

 cid       cid cG cid cG   cid cG cid      

  
ci 

      
else 

else 

where   

   

 

Proposition   Let  Hi     be the subdiagonal matrix of
Hi   with the index set    and    be the optimal solution
of the onedimensional optimization problem

 cid 

cG cid      

 

 

 cid   
 cid 

 

 cid cG cid Hi        tI cid 
cG  cid        Hi     cid 

ci 

min
  

    

Hence  the optimal solution of   has the following form

cG 

      
else 

 

Like  Chen et al    Yu    the entries of sampling
matrix     Rn   are sampled from an        normal distribution  and     Rd with xj      exp    and
           Let   be the noise sampled from the standard normal distribution  and the noisy observation satis es
   for
    Sx     In addition  we set       and       
each group Gi and the groups  Gi  are overlapped by  
elements  that is

 cid            

          
GK              

 cid 

 

 

The sampling size and the number of groups        are
chosen from the following set
        

 cid     

 cid 

 

   
   

   
   

   

To further reduce the computations  in Algorithm   we set
Hi      cid ST   cid   and the overrelaxation stepsize    as   in
  Hence  the compared  ve solvers GSOS  GFB  PDM 
PAAPG and APAAPG have the same computational cost
in each iteration  To be fair  all the compared algorithms
start with the same initial point  The following six pictures in Figures   and   display the comparisons of the  ve
solvers for   variety of        It is apparent that our proposed GSOS algorithm shows great superiorities over the
other four solvers  The primaldual solver PDM is slightly
faster than the primal solver GFB  PAAPG is the slowest
algorithm  because the prespeci ed proximal average approximation precision is        which leads to   very
small stepsize  Also  APAAPG is much faster than the
other four solvers at the  rst   iterations  However  it is
slowed down since the stepsize used in APAPG becomes
smaller and smaller as the iterations go on 

  GraphGuided Fused Lasso

In this subsection  we perform experiments on graphguided fused Lasso which is formulated as

min

 
 

 cid Sx     cid     

 ij xi   xj 

 

 cid 

      

where  ij     is the weight for the fused term  cid xi   xj cid 
for all               is the given graph edge set  and   is

           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APGGSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization

Figure   Objective value vs  iteration on graphguided fused Lasso 

Figure   Objective value vs  iteration on graphguided fused Lasso 

the regularization parameter 
In the implementation of Algorithm   for tackling graphguided fused Lasso   we need to solve the following
optimization in the updating step of yk 

    

     arg min

 
 

 cid       cid Hi  

   xi   xj 

ll  
ll  

bl     
bl 

      
      
   cid       

where   is de ned as

 

 
where Hi   is   diagonal positive de nite matrix  and   and
  are given constants  Let hii and hjj be the ith and jth
diagonal elements of Hi    respectively 
Proposition   The optimal solution of   takes the following closedform 

  bl     
 cid cid cid     
  bi bj
 cid cid cid     
 cid             

In the implementation  we use the similar parameter settings of      as above  The dimension parameter pair       
is chosen from the following set
        
and the parameter          Similarly  all the compared algorithms start with the same initial point  The following six pictures in Figures   and   display the comparisons of the  ve solvers for six kinds of choices of        It

           

sign  bi   bj   

bi bj
 
ii   
bi bj
 
ii   

 
jj

 
jj

 cid cid cid 
 cid cid cid 

 

 

   

 
ii   

 
jj

 

 

is obvious that the other four solvers GFB  PDM  APAPG
and APAAPG are not as ef cient as the proposed GSOS
algorithm  which demonstrates that the GaussSeidel technique is very useful for addressing nonsmooth optimization  It is worthwhile to point out that the primal solver
GFB is faster than the primaldual solver PDM on graphguided fused Lasso  One possible reason is that the number
of nonsmooth terms is too large  which will lead to   large
quantity of dual variables introduced in PDM and hence
slow down the updating of primal variables 

  Conclusions
In this paper  we proposed   novel  rstorder algorithm
called GSOS for addressing multiterm nonsmooth convex composite optimization  This algorithm inherits the
advantages of the GaussSeidel technique and the operator splitting technique  therefore being largely accelerated  We found that the GSOS algorithm includes the generalized forward backward splitting method  Raguet et al 
  as   special case  In addition  we developed   new
technique to establish the global convergence and iteration
complexity of the GSOS algorithm  Last  we applied the
proposed GSOS algorithm to solve overlapping group Lasso and graphguided fused Lasso problems  and compared
it against several stateof theart algorithms  The experimental results show the great superiority of the GSOS algorithm in terms of both ef ciency and effectiveness 

 

Acknowledgements
Yuan is supported by NSFChina  

 

 cid 

           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APG           GSOSPDMGFBAPA APGPA APGGSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization

References
Bach        Structured sparsityinducing norms through
submodular functions  Advances in Neural Information
Processing Systems  pp     

Bach        Jenatton     Mairal     and Obozinski    
Structured sparsity through convex optimization  Statistical Science   pgs     

Chen     Lin     Kim     Carbonell        and Xing       
Smoothing proximal gradient method for general structured sparse regression  The Annals of Applied Statistics 
   

Combettes        and Pesquet          douglas rachford
splitting approach to nonsmooth convex variational signal recovery  IEEE Journal of Selected Topics in Signal
Processing     

Bauschke        and Combettes        Convex Analysis and
Monotone Operator Theory in Hilbert Space  Springer
New York   

Combettes       and Pesquet          proximal decomposition method for solving convex variational inverse
problems  Inverse problems     

Bauschke        Goebel     Lucet     and Wang     The
proximal average  basic theory  SIAM Journal on Optimization     

Beck     and Teboulle       fast iterative shrinkagethresholding algorithm for
inverse problems 
SIAM journal on imaging sciences     

linear

BricenoArias        Combettes        Pesquet        and
Pustelnik     Proximal algorithms for multicomponent image recovery problems  Journal of Mathematical
Imaging and Vision     

Burachik        and Svaiter         enlargements of maximal monotone operators in banach spaces  SetValued
Analysis     

Burachik        Iusem        and Svaiter        Enlargement of monotone operators with applications to variational inequalities  SetValued and Variational Analysis 
   

Burachik        Sagastiz abal        and Svaiter         
enlargements of maximal monotone operators  Theory
and applications  In Reformulation  nonsmooth  piecewise smooth  semismooth and smoothing methods  pp 
  Springer   

Chambolle     and Pock        rstorder primaldual algorithm for convex problems with applications to imaging  Journal of Mathematical Imaging and Vision   
   

Chen     and Plemmons        Nonnegativity constraints in

numerical analysis   

Chen           and Rockafellar        Convergence rates
in forward backward splitting  SIAM Journal on Optimization     

Chen     Lin     Kim     Carbonell        and Xing 
      An ef cient proximal gradient method for general
structured sparse learning  stat     

Combettes        and Pesquet        Proximal splitting
methods in signal processing  In Fixedpoint algorithms for inverse problems in science and engineering  pp 
  Springer   

Combettes        and Pesquet        Primaldual splitting
algorithm for solving inclusions with mixtures of composite  lipschitzian  and parallelsum type monotone operators  SetValued and variational analysis   
   

Condat       primaldual splitting method for convex optimization involving lipschitzian  proximable and linear
composite terms  Journal of Optimization Theory and
Applications     

Cui     Li     Sun        and Toh        On the convergence properties of   majorized alternating direction
method of multipliers for linearly constrained convex optimization problems with coupled objective functions 
Journal of Optimization Theory   Applications   
   

Davis     and Yin       threeoperator splitting scheme
and its optimization applications  Mathematics   
   

Dup          Fadili        and Starck          proximal
iteration for deconvolving poisson noisy images using sparse representations  IEEE Transactions on Image Processing     

Eckstein     and Bertsekas        On the douglas rachford
splitting method and the proximal point algorithm for
maximal monotone operators  Mathematical Programming     

Huang     Zhang     and Metaxas     Learning with structured sparsity  Journal of Machine Learning Research 
 Nov   

Jacob     Obozinski     and Vert        Group lasso with
In International Conference
overlap and graph lasso 
on Machine Learning  ICML   Montreal  Quebec 
Canada  June  pp     

GSOS  GaussSeidel Operator Splitting Algorithm for MultiTerm Nonsmooth Convex Composite Optimization

Kim     and Xing        Statistical estimation of correlated genome associations to   quantitative trait network 
PLoS Genet       

Svaiter          family of enlargements of maximal
monotone operators  SetValued Analysis   
 

Li     and Zhang     Fast proximitygradient algorithms
for structured convex optimization problems   Applied
  Computational Harmonic Analysis   
 

Svaiter          class of fej er convergent algorithms  approximate resolvents and the hybrid proximalextragradient method  Journal of Optimization Theory
and Applications     

Teo        Smola     Vishwanathan     and Le       
  scalable modular convex solver for regularized risk
minimization  In Proceedings of the  th ACM SIGKDD international conference on Knowledge discovery and
data mining  pp    ACM   

Teo        Vishwanthan     Smola        and Le       
Bundle methods for regularized risk minimization  Journal of Machine Learning Research   Jan 
 

             splitting algorithm for dual monotone inclusions involving cocoercive operators  Advances in Computational Mathematics     

Xu     and Yin       block coordinate descent method for
regularized multiconvex optimization with applications
to nonnegative tensor factorization and completion  Siam
Journal on Imaging Sciences     

Yu     Better approximation and faster algorithm using the
proximal average  Advances in Neural Information Processing Systems  pp     

Yuan     Zheng        and Ghanem       matrix splitting method for composite function minimization  arXiv
preprint arXiv   

Zhao     Rocha     and Yu     The composite absolute
penalties family for grouped and hierarchical variable selection  The Annals of Statistics  pp     

Zhong     and Kwok           Accelerated stochastic gradient method for composite regularization  In AISTATS 
pp     

Zhou     Zha     and Song     Learning social infectivity in sparse lowrank networks using multidimensional
hawkes processes  In AISTATS  volume   pp   
 

Lin     Liu     and Su     Linearized alternating direction method with adaptive penalty for lowrank representation  Advances in Neural Information Processing
Systems  pp     

Luo        and Tseng     On the convergence of   matrix splitting algorithm for the symmetric monotone linear complementarity problem  SIAM Journal on Control
and Optimization     

Mairal     Jenatton     Bach        and Obozinski       
Network  ow algorithms for structured sparsity  In Advances in Neural Information Processing Systems  pp 
   

Monteiro           and Svaiter        Iterationcomplexity
of blockdecomposition algorithms and the alternating
direction method of multipliers  SIAM Journal on Optimization     

Nesterov     Excessive gap technique in nonsmooth convex minimization  SIAM Journal on Optimization   
     

Nesterov     Smooth minimization of nonsmooth functions  Mathematical programming   
   

Nesterov     Gradient methods for minimizing composite
functions  Mathematical Programming   
   

Pustelnik     Chaux     and Pesquet        Parallel proximal algorithm for image restoration using hybrid regularization  IEEE Transactions on Image Processing   
   

Raguet     Fadili     and Peyr         generalized forwardbackward splitting  SIAM Journal on Imaging Sciences 
   

Richard     Savalle        and Vayatis     Estimation
of simultaneously sparse and low rank matrices  arXiv
preprint arXiv   

Shen     Liu     Huang     Jiang        and Ma     Adaptive proximal average approximation for composite convex minimization  In AAAI   

