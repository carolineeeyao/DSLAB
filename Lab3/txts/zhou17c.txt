When can MultiSite Datasets be Pooled for Regression 

Hypothesis Tests   cid consistency and Neuroscience Applications

Hao Henry Zhou   Yilin Zhang   Vamsi    Ithapu   Sterling    Johnson     Grace Wahba   Vikas Singh  

Abstract

Many studies in biomedical and health sciences
involve small sample sizes due to logistic or  
nancial constraints  Often  identifying weak  but
scienti cally interesting  associations between
  set of predictors and   response necessitates
pooling datasets from multiple diverse labs or
groups  While there is   rich literature in statistical machine learning to address distributional
shifts and inference in multisite datasets  it is
less clear when such pooling is guaranteed to
help  and when it does not    independent of the
inference algorithms we use  In this paper  we
present   hypothesis test to answer this question 
both for classical and high dimensional linear regression  We precisely identify regimes where
pooling datasets across multiple sites is sensible  and how such policy decisions can be made
via simple checks executable on each site before
any data transfer ever happens  With   focus on
Alzheimer   disease studies  we present empirical results showing that in regimes suggested by
our analysis  pooling   local dataset with data
from an international study improves power 

  Introduction
In the last two decades  statistical machine learning algorithms for processing massive datasets have been intensively studied for   widerange of applications in computer
vision  biology  chemistry and healthcare  Murdoch   Detsky    Tarca et al    While the challenges posed
by large scale datasets are compelling  one is often faced
with   fairly distinct set of technical issues for studies in biological and health sciences  For instance    sizable portion

 University of WisconsinMadison  William    Middleton Memorial Veteran   Affairs Hospital 
Correspondence
to  Hao Zhou  hzhou stat wisc edu  Vikas Singh  vsingh biostat wisc edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

of scienti   research is carried out by small or mediumsized groups  Fortin   Currie    supported by modest
budgets  Lauer    Hence  there are logistic nancial
constraints on the number of experiments and or number
of participants within   trial  leading to small size datasets 
While the analysis may be suf ciently powered to evaluate the primary hypothesis of the study experiment  interesting followup scienti   questions  often more nuanced 
come up during the course of the project  These tasks may
be underpowered for the sample sizes available  This necessitates efforts to identify similar datasets elsewhere so
that the combined sample size of the  pooled  dataset is
enough to determine signi cant associations between   response and   set of predictors       within linear regression 
Motivating Application 
In genomics  funding agencies
have invested effort into standardizing curating data collection across large international projects  ENCODE Project
Consortium   
In other disciplines  such as in the
study of neurological disorders  heterogeneity in disease
etiology  variations in scanners and or acquisition tools
make standardization more dif cult  For Alzheimer   disease  AD    motivation of this work  efforts such as ADNI
 Weiner et al    provide   variety of clinical  imaging
and cognitive tests data for   older adults  However 
the research focus has now moved to the early stages of
disease   as early as late middle age   where treatments are
expected to be more effective  But     the statistical signal at this stage is weak and dif cult to demonstrate without large sample sizes and     such  preclinical  participants are not well represented  even in large ADNI sized
studies  Hence  there is   concerted effort in general for
smaller standalone projects  focused on   speci   disease
stage  that can be retrospectively pooled for analysis towards addressing   challenging scienti   hypothesis  Jahanshad et al    Unfortunately  acquisition protocols
for various measures across sites are usually different and
data are heterogeneous  These issues raise   fundamental
technical question  When is it meaningful to pool datasets
for estimating   simple statistical model       linear regression  When can we guarantee improvements in statistical
power  and when are such pooling efforts not worth it  Can
we give   hypothesis test and obtain pvalues to inform our
policies decisions  While related problems have been stud 

When can MultiSite Datasets be Pooled for Regression 

ied in machine learning from an algorithm design perspective  even simple hypothesis tests which can be deployed
by   researcher in practice  are currently unavailable  Our
goal is to remove this signi cant limitation 
Putting our development in context  The realization that
 similar  datasets from multiple sites can be pooled to potentially improve statistical power is not new  With varying
empirical success  models tailored to perform regression
in multisite studies  Group     Haase et al   
 Klunk et al    have been proposed  where due to operational reasons  recruitment and data acquisition are distributed over multiple sites  or even countries  When the
pooling is being performed retrospectively       after the
data has been collected  resolving sitespeci   confounds 
such as distributional shifts or biases in measurements  is
essential before estimation inference of   statistical model 
We will not develop new algorithms for estimating the distributional mismatch or for performing multisite regression   rather  our primary goal is to identify the regimes
 and give easily computable checks  where this regression
task on   pooled dataset is statistically meaningful  assuming that good preprocessing schemes are available  We
will present   rigorous yet simple to implement hypothesis
test  analyze its behavior  and show extensive experimental
evidence  for an important scienti   problem  The practitioner is free to use his her preferred procedure for the
 before step   estimating the distributional shifts 
Contributions     Our main result is   hypothesis test to
evaluate whether pooling data across multiple sites for regression  before or after correcting for sitespeci   distributional shifts  can improve the estimation  mean squared
error  of the relevant coef cients  while permitting an in 
 uence from   set of confounding variables     We derive analogous results in the highdimensional setting by
leveraging   different set of analysis techniques  Using
an existing sparse multitask Lasso model  we show how
the utility of pooling can be evaluated even when the support set of the features  predictors  is not exactly the same
across sites using ideas broadly related to high dimensional
simultaneous inference  Dezeure et al    We show
 cid consistency rate  which supports the use of sparse multitask Lasso when sparsity patterns are not totally identical 
   On an important scienti   problem of analyzing early
Alzheimer   disease  AD  individuals  we provide compelling experimental results showing consistent acceptance
rate and statistical power  Via   publicly available software package  this will facilitate many multisite regression
analysis efforts in the short to medium term future 

  Related Work
Metaanalysis approaches  If datasets at multiple different sites cannot be shared or pooled  the task of deriving

meaningful scienti   conclusions from results of multiple
independently conducted analyses generally falls under the
umbrella term of  meta analysis  The literature provides
various strategies to cumulate the general  ndings from
analyses on different datasets  But even experts believe
that  minor violations of assumptions can lead to misleading scienti   conclusions  Greco et al    and substantial personal judgment  and expertise  is needed to conduct
them  It is widely accepted that when the ability to pool the
data is an option  simpler schemes may perform better 
Domain adaptation shift  Separately 
the idea of addressing  shift  within datasets has been rigorously studied within statistical machine learning  see  Patel et al 
  Li    For example  domain adaptation  including dataset and covariate shift  seeks to align  the distributions of  multiple datasets to enable followup processing  BenDavid   Schuller    Typically  such algorithms assume   bias in the sampling process  and adopt reweighting as the solution  Huang et al    Gong et al 
  Alternatively    family of such methods assume
that sites  or datasets  differ due to feature distortions      
calibration error  which are resolved  in general  by minimizing some distance measure between appropriate distributions  Baktashmotlagh et al    Pan et al   
Long et al    Ganin et al    In general  these approaches have nice theoretical properties  BenDavid et al 
  Cortes   Mohri    Zhou et al    However 
it is important to note that the domain adaptation literature
focuses on the algorithm itself   to resolve the distributional sitewise differences  It does not address the issue
of whether pooling the datasets  after applying the calculated adaptation       transformation  is bene cial  Our
goal in this work is to assess whether multiple datasets can
be pooled   either before or usually after applying the best
domain adaptation methods   for improving our estimation of the relevant coef cients within linear regression  We
propose   hypothesis test to directly address this question 
The highdimensional case  High dimensional scenarios  in general  involve predicting   response       cognitive score  from high dimensional predictors such as image
scans  or derived features  and genetic data  which in general  entails Lassotype formulations unlike the classical regression models  Putting multitask representation learning
 Maurer et al    Ando   Zhang    Maurer et al 
  together with   sparsity regularizer  we get the multitask Lasso model  Liu et al    Kim   Xing   
Although this seems like   suitable model  Chen et al 
  it assumes that the multiple tasks  sites here  have an
identical active set of predictors  Instead  we  nd that the
sparse multitask Lasso  Lee et al    roughly    multitask version of sparse group Lasso  Simon et al    Lee
et al    is   better starting point  There is no theoretical
analysis in  Simon et al    although    cid consistency

When can MultiSite Datasets be Pooled for Regression 

for sparse group lasso is derived in  Chatterjee et al   
using   general proof procedure for Mestimators  it does
not take into account the speci   sparse group Lasso properties  This makes the result noninformative for sparse
group Lasso  much less  sparse multitask Lasso  Specifically  as we will see shortly  in sparse multitask Lasso 
the joint effects of two penalties induces   special type of
asymmetric structure  We show   new result  in the style
of Lasso  Meinshausen   Yu    Liu   Zhang     
for  cid  convergence rate for this model  It matches known
results for Lasso group Lasso  and identi es regimes where
the sparse multitask  multisite  setting is advantageous 
Simultaneous High dimensional Inference 
Simultaneous high dimensional inference models such as multi
samplesplitting and debiased Lasso is an active research
topic in statistics  Dezeure et al    Multi samplesplitting use half of the dataset for variable selection and
the rest for calculating pvalues  Debiased Lasso chooses
one feature as   response and the others as predictors to
estimate   Lasso model 
this procedure is repeated for
each feature  Estimators from Debiased Lasso asymptotically follow the multinormal distribution  Dezeure et al 
  and using BonferroniHolm adjustment produces
simultaneous pvalues  Such ideas together with the  cid 
convergence results for sparse multitask Lasso  will help
extend our analysis to the high dimensional setting 

  Hypothesis Test for MultiSite Regression
We  rst describe   simple setting where one seeks to apply standard linear regression to data pooled from multiple
sites  For presentation purposes  we will deal with variable selection issues later  Within this setup  we will introduce our main result     hypothesis test to evaluate
statistical power improvements       mean squared error 
when running   regression model on   pooled dataset  We
will see that the proposed test is transparent to the use of
adaptation algorithms  if any  to preprocess the multisite
data  more details in appendix  In later sections  we will
present convergence analysis and extensions to the large
  setting  Matrices  vectors scalars  are upper case  and
lower case   cid cid  is the nuclear norm 
We  rst introduce the singlesite regression model  Let
    Rn   and     Rn  denote the feature matrix of
predictors and the response vector respectively  If   corresponds to the coef cient vector       predictor weights 
then the regression model is

min

 

 cid       cid 

 

 
 

 

where            and                  if   is
the true coef cient vector from which   is generated  The
meansquared error  MSE  and  cid consistency of regres 

    tr cid       cid   

sion is wellknown  The meansquared error  MSE  of  
is   cid       cid 
If   denotes the
number of sites  then one may  rst apply   domain adaptation scheme to account for the distributional shifts between the   different predictors  Xi  
   and then run  
regression model  If the underlying  concept        predictors and responses relationship  can be assumed to be the
same across the different sites  then it is reasonable to impose the same   for all sites  For instance  the in uence of
CSF protein measurements on cognitive scores of an individual may be invariant to demographics  Nonetheless  if
the distributional mismatch correction is imperfect  we may
de ne             where                  as the residual difference between the sitespeci   coef cients and the
true shared coef cient vector  in the ideal case  we have
       In the multisite setting  we can write

   cid yi   Xi cid 
   

 

 

where for each site   we have yi   Xi    Xi        and
          
       Here     is   weighting parameter
   
for each site  if such information is available 
Our main goal is to test if the combined regression improves the estimation for   single site  We can pose this
question in terms of improvements in the mean squared error  MSE  Hence           using site   as the reference 
we set       in   and consider      

  cid 

  

min

 

  cid 

 cid        cid 

   

   cid yi   Xi cid 
   

 

 

min

 

 

 

 

  
Clearly  when the sample size is
not large enough 
the multisite
formulation in   may reduce
variance signi cantly  because of
the averaging effect in the objective function  while increasing the
bias by   little  This reduces the
Mean Squared Error  MSE  see
Figure   Note that while traditionally  the unbiasedness property was desirable  an extensive
body of literature on ridge regression suggests that the quantity of
interest should really be   cid     
 cid 
   Hoerl   Kennard    James   Stein   
These ideas are nicely studied within papers devoted to the
 biasvariance  tradeoff  Similar to these results  we will
focus on the mean squared error because the asymptotic
consistency properties that come with an unbiased estimator are not meaningful here anyway   the key reason we
want to pool datasets in the  rst place is because of small
sample sizes  We now provide   result showing how the
tuning parameters              can be chosen 

Figure     and  
are  st and  nd site
coef cients 
After
combination     bias
increases but variance
reduces  resulting in  
smaller MSE 

When can MultiSite Datasets be Pooled for Regression 

Theorem         
  

achieves the smallest variance in  

Remarks  This result follows from observing that the each
site   contribution is inversely proportional to sitespeci  
noise level      We will show that this choice of  is also
leads to   simple mechanism to setup   hypothesis test 

  Sharing all   

In the speci cation above  the estimates of    across all  
sites are restricted to be the same  Without this constraint 
  is equivalent to  tting   regression separately on each
site  So    natural question is whether this constraint improves estimation  To evaluate whether MSE is reduced 
we  rst need to quantify the change in the bias and variance of   compared to   To do so  we introduce   few
notations  Let ni be the sample size of site    and let   
denote the regression estimate from   speci   site    We
have               We de ne the length kp vector   
     similarly for        We use
as        
   for the sample covariance matrix of the data  predictors 
from the site   and               for the covariance
    and
matrix of     where Gii            ni   
Gij         whenever    cid    
Let the difference in bias and variance between the single
site model in   and the multisite model in   be Bias 
and   ar  respectively  Let   
   
         
Lemma   For model   we have

   cid  

  We have 

        

   and   

   ni   
 

 

Remarks  This is our main test result  Although    is typically unknown  it can be easily replaced using its sitespeci   estimation  Theorem   implies that we can conduct   noncentral   distribution test based on the statistic  Also    shows that the noncentral   distribution 
which the test statistics will follow  has   noncentral parameter smaller than   when the suf cient condition   
holds  Meanwhile  in obtaining the  surprisingly simple 
suf cient condition    no other arbitrary assumption is
needed except the application of CauchySchwartz inequality  From   practical perspective  Theorem   implies that
the sites  in fact  do not even need to share the full dataset
to assess whether pooling will be useful  Instead  the test
only requires very highlevel information such as        
   and ni for all participating sites   which can be transferred very cheaply with no additional cost of data storage 
or privacy implications  The following result deals with the
special case where we have two participating sites 

Corollary   For the case where we have two participating sites  the condition   from Theorem   reduces to

                     

 

     
 

 

Remarks  The left side above relates to the Mahalanobis
distance between     with covariance        
  implying that the test statistic is   type of  
     
 
normalized metric between the two regression models 

  Sharing   subset of   

 

 cid Bias cid 
 cid   cid 
  ar     
 

 

    

 cid cid cid     

   cid    
       
   
              
   

    

 cid cid cid   

      

   cid 

 

 

Remarks  The above result bounds the increase in bias and
the reduction in variance  see discussion of Figure   Since
our goal is to test MSE reduction   in principle  we can use
bootstrapping to calculate MSE approximately  This procedure has   signi cant computational footprint  Instead   
 from   onestep CauchySchwartz inequality  gives   suf 
 cient condition for MSE reduction as shown below 

Theorem      Model   has smaller MSE of   than
model   whenever

      cid   cid 

     
 

 

   Further  we have the following test statistic 

 cid cid cid cid cid      

 

 cid cid cid cid cid 

 

 cid cid cid cid cid    

 

 cid cid cid cid 

 

 cid 

 

 

   

    

where  cid   cid  is called    condition value 

In numerous pooling scenarios  we are faced with certain
systemic differences in the way predictors and responses
associate across sites  For example  socioeconomic status may  or may not  have   signi cant association with  
health outcome  response  depending on the country of the
study       due to insurance coverage policies  Unlike in
Section   we now relax the restriction that all coef cients
are the same across sites  see Fig    The model in   will
now include another design matrix of predictors     Rn  
and corresponding coef cients    for each site   

  cid 

min
 

   cid yi   Xi    Zi   cid 
   
yi   Xi    Xi     Zi 

  

 

             

 

 

Our goal is still to evaluate whether the MSE of   reduces 
We do not take into account the MSE change in   because
they correspond to sitespeci   variables  For estimation 
  can  rst be computed from   Treating it as    xed entity now     can be computed using yi and Zi on each site
independently  Clearly  if   is close to the  true    it will
also enable   better estimation of sitespeci   variables  It
turns out that  if  is are replaced by the conditional covariance  the analysis from Section   still holds for this case 

When can MultiSite Datasets be Pooled for Regression 

Speci cally  let  abi be the sample covariance matrix between features   and   from some site    We have 

Theorem   Analysis in Section   holds for   in   by
replacing    with       xxi    xzi   zzi   zxi
Remarks  The test now allows evaluating power improvements focused only on the subset of coef cients that is
shared and permits sitespeci   confounds  For example 
we can test which subset of parameters might bene   from
parameter estimation on pooled data from multiple sites 

  Pooling in High Dimensional Regression
We now describe our analysis of pooling multisite data in
the highdimensional setting where    cid     The challenge
here is that variable section has to be    rst order concern 
In classical regression   cid  consistency properties are well
known and so our focus in Section   was devoted to deriving suf cient conditions for the hypothesis test  In other
words  imposing the same   across sites works in   because we understand its consistency  In contrast  here  one
cannot enforce   shared   for all sites before the active set
of predictors within each site are selected   directly imposing the same   leads to   loss of  cid consistency  making
followup analysis problematic  Therefore  once   suitable
model for highdimensional multisite regression is chosen 
the  rst requirement is to characterize its consistency 
We start with the multitask Lasso    special case of group
Lasso   Liu et al    where the authors show that the
strategy selects better explanatory features compared to
separately  tting Lasso on each site  But this algorithm underperforms when the sparsity pattern of the predictors is
not identical across sites  so we use   recent variant called
sparse multitask Lasso  Lee et al      essentially substituting  sites  for  tasks  The sparse multisite Lasso in
   cid    setting    is the number of predictors  is given as

  cid 

     

arg min

  cid 

 

 cid yi   Xi   cid 
 
 cid   cid         

  

 

  cid 

       

 cid   cid 

 

 

       

  

  

where   is the Lasso regularization parameter  Here     

 

  

  

 

  

  

 

  

  

 

Figure     and   in uence the response     After adjustment 
   and    may be close requiring same   However     and   
may differ   lot  and we need different   and  

Rk   is   matrix where the ith row gives the coef cients
from ith site    sites in total  Also     with subscript denotes the ith row  site  of    we use    with superscript
to give the jth column  coef cients  of    The hyperparameter         balances the two penalties  and will
be used shortly    larger   weighs the  cid  penalty more and
  smaller   puts more weight on the grouping  Similar to  
Lassobased regularization parameter    here will produce
  solution path  to select coef cients  for   given   We
 rst address the consistency behavior of the sparse multisite Lasso in   which was not known in the literature 

   cid  consistency
Our analysis of   is related to known results for Lasso
 Meinshausen   Yu    and the group Lasso  Liu  
Zhang      Recall that            Xk are the data mai ni  and
trices from   sites  We de ne      maxk
       DIAG    
  Xk  where DIAG      
corresponds to constructing   blockdiagonal matrix with
  and   as blocks on the diagonal  We require the following useful properties of    cid cid  denotes  cid norm 
De nition   The msparse minimal and maximal eigenvalues of    denoted by  min    and  max    are

          

min

 cid cid cid   cid 

     
    

and

max

 cid cid cid   cid 

     
    

 

We call   feature  active  if its coef cient is nonzero 
Now  each site may have different active features 
let
sh   kp be the sum of the number of active features over
all sites  Similarly  sp is the cardinality of the union of features that are active in at least one site  sh   ksp  sp     
Recall that when    cid    we add the Lasso penalty to the
multisite Lasso penalty  When the sparsity patterns are assumed to be similar across all sites    is small  In contrast 
to encourage sitespeci   sparsity patterns  we may set  
to be large  We now analyze these cases independently 
Theorem   Let           Assume there exist constants      min    max     such that
 cid cid 

    min
lim inf

 cid 

 cid 
 max sp   min    cid 

   

sp

 

     

ni  kp     max 

lim sup
  

Then  for      cid   log kp  there exists   constant      

such that  with probability converging to   for      

  

   min

 

 cid          cid 

 
 

         log kp 

sp    cid sh      is the noise level 

 

  

 

 
where         

When can MultiSite Datasets be Pooled for Regression 

Remarks  The above result agrees with known results for
multitask Lasso  Liu et al    Liu   Zhang     
when the sparsity patterns are the same across sites  The
simplest way to interpret Theorem   is via the ratio    
  Here        when the sparsity patterns are the same
sh
sp
across sites  As   decreases  the sparsity patterns across
sites start to differ  in turn  the sparse multisite Lasso
from   will provide stronger consistency compared to
the multisite Lasso  which corresponds to       In other
words  whenever we expect sitespeci   active features  the
 cid  consistency of   will improve as one includes an additional  cid penalty together with multisite Lasso 
 
Observe that for the nonsparse     we can verify that
  cid   cid  have the same scale  On the other
 cid   cid  and
hand  for sparse      cid   cid  has the same scale as  cid   cid 
 
  penalization  see appendix  Unlike Thei    with no
orem   where the sparsity patterns across sites are similar  due to this scaling issue  the parameters   and   need
to be  corrected  for the setting where sparsity patterns
have little overlap  We denote these corrected versions by
   
Theorem   Let           Assume there exist constants      min    max     such that
 cid cid 

and          

 
 
 

     

 

  

     

   min

    min
lim inf

 cid 

 cid 
 max sh   min    cid 

   

sh

 

 

ni  kp     max 

lim sup
  

Then  for      cid   log kp  there exists       such that 
with           cid sp      cid sh    instead of    

with probability converging to   for       we have  

  

Remarks  This result agrees with known results for Lasso
 Meinshausen   Yu    when the sparsity patterns are
completely different across sites 
In this case         is
large  the sparse multisite Lasso has stronger consistency
compared to Lasso       The sparse multisite Lasso is
preferable as     sh
increases  Note that although   and
sp
  are used for the results instead of   and   in practice 
one can simply scale the chosen    appropriately       with
      we see that       corresponds to      

Performing hypothesis tests  Theorems   and  
show consistency of sparse multisite Lasso estimation 
Hence  if the hyperparameters   and   are known  we
can estimate the coef cients    This variable selection
phase can be followed by   hypothesis test  similar to Theorem   from Section   The only remaining issue is the
choice of   The existing methods show that joint crossvalidation for   and   performs unsatisfactorily and instead
use   heuristic  set it to   when it is known that sparsity

patterns are similar across sites and   otherwise  Simon
et al    Below  instead of    xed   we provide  
datadriven alternative that works well in practice 

Choosing   using simultaneous inference  Our results
in Thm     and Thm    resp  seem to suggest that
increasing  and decreasing resp    will always improve
consistency  however  this ends up requiring stronger msparsity conditions  We now describe   procedure to choose
  First  recall that an active feature corresponds to   variable with nonzero coef cient  We call   feature  siteactive  if it is active at   site  an  alwaysactive  feature
is active at all   sites  The proposed solution involves three
steps    First  we apply simultaneous inference  like multi
samplesplitting or debiased Lasso  using all features at
each of the   sites with FWER control  This step yields
 siteactive  features for each site  and therefore  gives the
set of alwaysactive features and the sparsity patterns   
Then  each site runs   Lasso and chooses      based on
crossvalidation  We then set  multi site to be the minimum among the best    from each site  Using  multi site 
we can vary   to    various sparse multisite Lasso models   each run will select some number of alwaysactive
features  We plot   versus the number of alwaysactive
features    Finally  based on the sparsity patterns from
the siteactive set  we estimate whether the sparsity patterns across sites are similar or different       share few active features  Then  based on the plot from step   if the
sparsity patterns from the siteactive sets are different  similar  across sites  then the smallest  largest  value of   that
selects the minimum  maximum  number of alwaysactive
features is chosen  The appendix includes details 

  Experiments
Our experiments are twofold  First we perform simulations evaluating the hypothesis test from   and sparse
multisite Lasso from   We then evaluate pooling two
Alzheimer   disease  AD  datasets from different studies
to evaluate improvements in power  and checking whether
the proposed tests provide insights into the regimes when
pooling is bene cial for regression  and will yield tangible
statistical bene ts in investigating scienti   hypotheses 
Power and Type   Error of Theorem   The  rst set
of simulations evaluate the setting from Section   where
the coef cients are same across two different sites  The
inputs for the two sites are set as       Rn   
      with              where   is identity and  
is         matrix of     The true coef cients are given by
            and            where     is multivariate uniform  and the noise corresponds to            
and             for the two sites respectively  With
this design  the responses are set as             and
            Using        and        the shared

When can MultiSite Datasets be Pooled for Regression 

  are estimated  The simulation is repeated   times with
  different sample sizes         with                 for
each repetition  Fig      shows the MSE of twosite  blue
bars  and   baseline singlesite  red bars  model computed
using the corresponding    on site   Although both MSEs
decrease as   increases  the twosites model consistently
produces smaller MSE   with large gains for small sample
sizes  leftend of Fig      Fig      shows the acceptance rates of our proposed hypothesis test  from   and
  with   signi cance level  The purple solid line is
the suf cient condition from Theorem   while the dotted line is where the MSE of the baseline singlesite model
starts to decrease below that of the twosite model  The
trend in Fig      implies that as   increases  the test tends
to reject pooling the multisite data with power     Further  the type   error is wellcontrolled to the left of the
solid line  and is low between the two lines  See appendix
for additional details about Figs       
Power and Type   Error of Theorem   The second set of simulations evaluate the confounding variables setup from Section   Similar to Section   here
we have                       with    
    and
  are the same as before 
               and
               are the coef cients for    and    respectively  The new responses    and    will have the extra
terms    and    respectively  Fig        shows the
results  All the observations from Fig        hold here as
well  For small    MSE of twosite model is much smaller
than baseline  and as sample size increases this difference
reduces  The test accepts with high probability for small   
and as sample size increases it rejects with high power  The
regimes of low type   error and high power in Fig      are
similar to those from Fig     

 cid           

   

 cid 

   

         

  Sparse multisites Lasso  cid consistency
We now use   sites with       samples each and    
  features to test the sparse multisite model from  
We set the design matrices Xi                         
with      Ip      Ep    We consider the two cases
 sparsity patterns shared not shared  separately 
Few sparsity patterns shared 
  shared features and  
sitespeci   features  out of the   are set to be active in
  sites  Each shared feature is sampled from       for
the  rst two sites and       for the rest  All the sitespeci   features are         The noise           
and the responses are yi   Xi         Fig      shows
the  fold cross validation error as   changes       solution path  for different   settings  including the value
from our proposed selection procedure  from Section  
Lasso       group Lasso       and arbitrary values          as suggested by  Simon et al   

Our chosen        the blue curve in Fig      has
the smallest error  across all     thereby implying   better  cid  consistency  Table   in the appendix includes more
details  including       discovers more alwaysactive
features  while preserving the ratio of correctly discovered
active features to all the discovered ones 

Most sparsity patterns shared  Unlike the earlier case 
here we set   shared and   sitespeci   features  both
        to be active among all   features  The result 
shown in Fig      is similar to Fig      The proposed
choice of       competes favorably with alternate
choices while preserving the correctly discovered number
of alwaysactive features  Unlike the previous case  the ratio of correctly discovered active features to all discovered
ones increases here  see appendix 

  Combining AD datasets from multiple sites

We now evaluate whether two AD datasets acquired at different sites   an Alzheimer   Disease Neuroimage Initiative
 ADNI  dataset and   local dataset from Wisconsin ADRC
 ADlocal  can be combined  appendix has dataset details 
The sample sizes are   and   respectively  Cerebrospinal  uid  CSF  protein levels are the inputs  and the
response is hippocampus volume  Using   agematched
samples from each dataset  we  rst perform domain adaptation  using   maximum mean discrepancy objective as
  measure of distance between the two marginals  and
then transform CSF proteins from ADlocal to match with
ADNI  The transformed data is then used to evaluate
whether adding ADlocal data to ADNI will improve the
regression performed on the ADNI data  This is done by
training   regression model on the  transformed  ADlocal
and   subset of ADNI data  and then testing the resulting
model on the remaining ADNI samples  We use two baseline models each of which are trained using   ADNI data
alone  and nontransformed ADlocal  with ADNI subset 
Fig        show the resulting mean prediction error  MPE 
scaled by the estimated noise level in ADNI responses  and
the corresponding acceptance rate  with signi cance level
  respectively  The xaxis in Fig        represents
the size of ADNI subset used for training  As expected 
the MPE reduces as this subset size increases  Most importantly  pooling after transformation  green bars  seems
to be the most bene cial in terms of MPE reduction  As
shown in Fig      to the left of purple line where the
subset size is smaller than ADlocal datasize  pooling the
datasets improves estimation  This is the small sample size
regime which necessitates pooling efforts in general  As
the dataset size increases  to the right of xaxis in Fig     
the resulting MPE for the pooled model is close to what we
will achieve using the ADNI data by itself 
Since pooling after transformation is at least as good as us 

When can MultiSite Datasets be Pooled for Regression 

   

   

   

   

   

   

Figure            MSE and the acceptance rate  Sec         MSE of   and   and the acceptance rate  Sec   using   bootstrap
repetitions  Solid line in       is when the condition from Theorem   is   Dotted line is when MSE of singlesite and multisite models
are the same        error path when sparsity patterns are dissimilar across sites      The regime where sparsity patters are similar 

   

   

Figure         MPE for the pooled regression model after before transformations  green red  compared to baseline  blue  plotted against
training subset size of ADNI  xaxis is number fraction of ADNI labeled samples used in training  apart from ADlocal        show the
acceptance rates for       Unlike in         restricts same training data size for ADNI and ADlocal 

   

   

ing ADNI data alone  our hypothesis test accepts the combination with high rate     see Fig      The test
rejects the pooling strategy with high power for combining before domain adaptation  see Fig      as one would
expect  This rejection power increases rapidly as sample
size increases  see red curve in Fig      The results in
Fig        show the setting where one cannot change the
dataset sizes at the sites      the training set uses an equal
number of labeled samples from both the ADNI and ADlocal  xaxis in Fig      and the testing set always corresponds to   of ADNI data  This is   more interesting
scenario for   practitioner compared to Fig        because
in Fig        we use same sample sizes for both datasets 
The trends in Fig        are the same as Fig       
  Conclusions
We present   hypothesis test to answer whether pooling
multiple datasets acquired from different sites is guaran 

teed to increase statistical power for regression models 
For both standard and high dimensional linear regression 
we identify regimes where such pooling is sensible  and
show how such policy decisions can be made via simple
checks executable on each site before any data transfer ever
happens  We also show empirical results by combining
two Alzheimer   disease datasets in the context of different
regimes proposed by our analysis  and see that the regression    improves as suggested by the theory  The code is
available at https github com hzhoustat ICML 

Acknowledgments  This work is supported by NIH
grants    AG     EB  UW CPCP
AI     AG  and NSF awards DMS
  CAREER   and CCF   The
authors are grateful for partial support from UW ADRC
AG  UW ICTR  UL RR  and funding from
  UWMadison DZNE collaboration initiative 

 Sample Size  log  scale Square Root of MSEMSESingle siteTwo sites Sample Size  log  scale Square Root of MSEMSEb  single siteb  two sitesg   single siteg   two sites lSquare Root of Cross Validation Errora      group lasso        fixed choice        fixed choice        our method        lasso Sufficient ConditionSame MSE Sample Size  log  scale Acceptance RateSufficient ConditionSame MSE Sample Size  log  scale Acceptance Rate lSquare Root of Cross Validation Errora      group lasso        fixed choice        our method        fixed choice        lasso Same sample sizes Sample size of labeled ADNISquare root of MPEMethodlocal  before    ADNIlocal  after    ADNIADNIPower increases Sample size of labeled ADNIAcceptance RateMethodlocal  before    ADNIlocal  after    ADNI Sample sizeSquare Root of MPEMethodlocal  before    ADNIlocal  after    ADNIADNIPower increases Sample sizeAcceptance RateMethodlocal  before    ADNIlocal  after    ADNIWhen can MultiSite Datasets be Pooled for Regression 

References
Ando  Rie Kubota and Zhang  Tong    framework for
learning predictive structures from multiple tasks and
unlabeled data  Journal of Machine Learning Research 
 Nov   

Baktashmotlagh  Mahsa  Harandi  Mehrtash    Lovell 
Brian    and Salzmann  Mathieu  Unsupervised domain
adaptation by domain invariant projection  In Proceedings of the IEEE International Conference on Computer
Vision  pp     

BenDavid  Shai and Schuller  Reba  Exploiting task relatedness for multiple task learning  In Learning Theory
and Kernel Machines  pp    Springer   

BenDavid  Shai  Blitzer  John  Crammer  Koby  Kulesza 
Alex  Pereira  Fernando  and Vaughan  Jennifer Wortman    theory of learning from different domains  Machine learning     

Chatterjee  Soumyadeep  Steinhaeuser  Karsten  Banerjee 
Arindam  Chatterjee  Snigdhansu  and Ganguly  Auroop 
Sparse group lasso  Consistency and climate applicaIn Proceedings of the   SIAM International
tions 
Conference on Data Mining  pp    SIAM   

Chen  Xi  He  Jinghui  Lawrence  Rick  and Carbonell 
Jaime    Adaptive multitask sparse learning with an application to fmri study  In Proceedings of the   SIAM
International Conference on Data Mining  pp   
SIAM   

Cortes  Corinna and Mohri  Mehryar  Domain adaptation in
regression  In International Conference on Algorithmic
Learning Theory  pp    Springer   

Dezeure  Ruben    uhlmann  Peter  Meier  Lukas  Meinshausen  Nicolai  et al  Highdimensional inference 
Con dence intervals  pvalues and rsoftware hdi  Statistical Science     

Dezeure  Ruben    uhlmann  Peter  and Zhang  CunHui 
Highdimensional simultaneous inference with the bootstrap  arXiv preprint arXiv   

ENCODE Project Consortium  The encode  encyclopedia
of dna elements  project  Science   
 

Fortin  JeanMichel and Currie  David    Big science vs 
little science  how scienti   impact scales with funding 
PloS one       

Ganin  Yaroslav  Ustinova  Evgeniya  Ajakan  Hana  Germain  Pascal  Larochelle  Hugo  Laviolette  Franc ois 
Marchand  Mario  and Lempitsky  Victor  Domainadversarial training of neural networks  Journal of Machine Learning Research     

Gong  Boqing  Grauman  Kristen  and Sha  Fei  Connecting the dots with landmarks  Discriminatively learning domaininvariant features for unsupervised domain
adaptation  In ICML   pp     

Greco     Zangrillo     BiondiZoccai     and Landoni    
Metaanalysis  pitfalls and hints  Heart Lung Vessel   
   

Group  Glioma Metaanalysis Trialists GMT  Chemotherapy in adult highgrade glioma    systematic review
and metaanalysis of individual patient data from  
randomised trials  The Lancet   
 

Haase  Michael  Bellomo  Rinaldo  Devarajan  Prasad 
Schlattmann  Peter  HaaseFielitz  Anja  and Group 
NGAL Metaanalysis Investigator  Accuracy of neutrophil gelatinaseassociated lipocalin  ngal  in diagnosis and prognosis in acute kidney injury    systematic
review and metaanalysis  American Journal of Kidney
Diseases     

Hoerl  Arthur   and Kennard  Robert    Ridge regression 
Biased estimation for nonorthogonal problems  Technometrics     

Huang  Jiayuan  Smola  Alexander    Gretton  Arthur 
Borgwardt  Karsten    Sch olkopf  Bernhard  et al  Correcting sample selection bias by unlabeled data  Advances in neural information processing systems   
   

Jahanshad  Neda  Kochunov  Peter    Sprooten  Emma 
Mandl  Ren      Nichols  Thomas    Almasy  Laura 
Blangero  John  Brouwer  Rachel    Curran  Joanne   
de Zubicaray  Greig    et al  Multisite genetic analysis
of diffusion images and voxelwise heritability analysis 
  pilot project of the enigma dti working group  Neuroimage     

James  William and Stein  Charles 

Estimation with
In Proceedings of the fourth Berkeley
quadratic loss 
symposium on mathematical statistics and probability 
volume   pp     

Kim  Seyoung and Xing  Eric    Treeguided group lasso
for multitask regression with structured sparsity   

Klunk  William    Koeppe  Robert    Price  Julie   
Benzinger  Tammie    Devous  Michael    Jagust 
William    Johnson  Keith    Mathis  Chester    Minhas  Davneet  Pontecorvo  Michael    et al  The centiloid project  standardizing quantitative amyloid plaque
estimation by pet  Alzheimer     Dementia   
 

When can MultiSite Datasets be Pooled for Regression 

Lauer  Mike 

Fy  by the numbers 
recent

quick look at
https nexus od nih gov all 
 fy bythe numbers 

trends 

 

and  
URL

Simon  Noah  Friedman  Jerome  Hastie  Trevor  and TibJournal of
shirani  Robert    sparsegroup lasso 
Computational and Graphical Statistics   
 

Lee  Seunghak  Zhu  Jun  and Xing  Eric    Adaptive
multitask lasso  with application to eqtl detection 
In
Advances in neural information processing systems  pp 
   

Tarca  Adi    Carey  Vincent    Chen  Xuewen  Romero 
Roberto  and Dr aghici  Sorin  Machine learning and its
applications to biology  PLoS Comput Biol     
 

Weiner  Michael    Veitch  Dallas    Aisen  Paul    Beckett  Laurel    Cairns  Nigel    Cedarbaum  Jesse  Donohue  Michael    Green  Robert    Harvey  Danielle 
Jack  Clifford    et al 
Impact of the alzheimer   disease neuroimaging initiative    to   Alzheimer  
  Dementia     

Zhou  Hao    Ravi  Sathya    Ithapu  Vamsi    Johnson 
Sterling    Wahba  Grace  and Singh  Vikas  Hypothesis testing in unsupervised domain adaptation with applications in alzheimer   disease  In Advances in Neural
Information Processing Systems  pp     

Li  Qi  Literature survey  domain adaptation algorithms
for natural language processing  Department of Computer Science The Graduate Center  The City University
of New York  pp     

Liu  Han and Zhang  Jian  Estimation consistency of the
group lasso and its applications  In AISTATS  pp   
     

Liu  Han and Zhang  Jian  Estimation consistency of the
group lasso and its applications  In AISTATS  pp   
     

Liu  Han  Palatucci  Mark  and Zhang  Jian  Blockwise coordinate descent procedures for the multitask lasso  with
applications to neural semantic basis discovery  In Proceedings of the  th Annual International Conference on
Machine Learning  pp    ACM   

Long  Mingsheng  Cao  Yue  Wang  Jianmin  and Jordan 
Michael    Learning transferable features with deep
adaptation networks  In ICML  pp     

Maurer  Andreas  Pontil  Massimiliano  and RomeraParedes  Bernardino  Sparse coding for multitask and
transfer learning  In ICML   pp     

Maurer  Andreas  Pontil  Massimiliano  and RomeraParedes  Bernardino  The bene   of multitask representation learning  Journal of Machine Learning Research 
   

Meinshausen  Nicolai and Yu  Bin  Lassotype recovery
of sparse representations for highdimensional data  The
Annals of Statistics  pp     

Murdoch  Travis   and Detsky  Allan    The inevitable
application of big data to health care  Jama   
   

Pan  Sinno Jialin  Tsang  Ivor    Kwok  James    and
Yang  Qiang  Domain adaptation via transfer component analysis  IEEE Transactions on Neural Networks 
   

Patel  Vishal    Gopalan  Raghuraman  Li  Ruonan  and
Chellappa  Rama  Visual domain adaptation    survey
of recent advances  IEEE signal processing magazine 
   

