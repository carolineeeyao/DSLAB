Nearly Optimal Robust Matrix Completion

Yeshwanth Cherapanamjeri   Kartik Gupta   Prateek Jain  

Abstract

In this paper  we consider the problem of Robust Matrix Completion  RMC  where the goal
is to recover   lowrank matrix by observing
  small number of its entries out of which  
few can be arbitrarily corrupted  We propose  
simple projected gradient descentbased method
to estimate the lowrank matrix that alternately
performs   projected gradient descent step and
cleans up   few of the corrupted entries using hardthresholding  Our algorithm solves
RMC using nearly optimal number of observations while tolerating   nearly optimal number
of corruptions  Our result also implies significant improvement over the existing time complexity bounds for the lowrank matrix completion problem  Finally  an application of our result to the robust PCA problem  lowrank sparse
matrix separation  leads to nearly linear time  in
matrix dimensions  algorithm for the same  existing stateof theart methods require quadratic
time  Our empirical results corroborate our theoretical results and show that even for moderate
sized problems  our method for robust PCA is an
order of magnitude faster than the existing methods 

  Introduction
In this paper  we study the Robust Matrix Completion
 RMC  problem where the goal is to recover an underlying
lowrank matrix by observing   small number of sparsely
corrupted entries  Formally 

RMC 

Find rankr matrix      Rm  

using   and          

 
where               is the set of observed entries
 throughout the paper we assume that           denotes the sparse corruptions of the observed entries      

 Microsoft Research India  Correspondence to  Prateek Jain

 prajain microsoft com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Supp        and sampling operator      Rm    
Rm   is de ned as 

 cid 

     ij  

Aij 
 

if           
otherwise 

 

RMC is an important problem with several applications 
It is used to model recommendation systems with outliers
and to perform PCA under gross outliers as well as erasures  Jalali et al   
In addition  it is   strict generalization of the following two fundamental problems in
machine learning 

Matrix Completion  MC  Matrix completion is the task
of completing   low rank matrix given   subset of entries of the matrix  It is widely used in recommender systems and also  nds applications in system identi cation
and global positioning  Note that it is   special case of
the RMC problem where the matrix        Stateof theart result for MC uses nuclear norm minimization and requires      nr  log    under standard  incoherence
assumption  see Section   But it requires        time
in general  The best sample complexity result for   nonconvex iterative method  with at most logarithmic dependence on the condition number of    achieve exact recovery when      nr  log    and needs      computational steps 

Robust PCA  RPCA  RPCA aims to decompose  
sparsely corrupted low rank matrix into its low rank and
sparse components  It corresponds to another special case
of RMC where the whole matrix is observed  Stateof 
theart results for RPCA shows exact recovery of   rankr 
 incoherent     see Assumption   Section   if at most
fraction of the entries in each row column of
     
   are corrupted  Hsu et al    Netrapalli et al   
Moreover  StNcRPCA algorithm  Netrapalli et al   
solves the problem in time   mnr 

 cid   

 cid 

  

Therefore  an ef cient solution to the RMC problem implies ef cient solutions to both the MC and RPCA problems 
In this work  we attempt to answer the following
open question  assuming       
Can RMC be solved exactly by using       rn log   
     fraction of the observed
observations out of which     

Nearly Optimal Robust Matrix Completion

entries in each row column are corrupted 

Note that both    for uniformly random   and   values mentioned in the question above denote the information theoretic limits  Hence  the goal is to solve RMC for
nearlyoptimal number of samples and nearlyoptimal fraction of corruptions 

The existing stateof theart results for RMC with opti 
   fraction of corrupted entries  either require
mal      
at least   constant fraction of the entries of    to be observed  Chen et al    Cand es et al    or require
restrictive assumptions like the support of corruptions being uniformly random  Li     Klopp et al    also
considers RMC problem but studies the noisy setting and
does not provide exact recovery bounds  Moreover  most
of the existing methods for RMC use convex relaxation for
both lowrank and sparse components  and in general exhibit large time complexity        
Under standard assumptions on         and for    
     we answer the above question in af rmative albeit
with   which is       ignoring log factors  larger than
the optimal sample complexity  see Theorem   In particular  we propose   simple projected gradient  PGD  style
method for RMC that alternately cleans up corrupted entries by hardthresholding and performs   projected gradient descent update onto the space of low rank matrices  our
method   computational complexity is also nearly optimal
                      Our algorithm is based on
projected gradient descent for estimating    and alternating projection on the set of sparse matrices for estimating
   Note that projection is onto nonconvex sets of lowrank matrices  for    and sparse matrices  for    hence
standard convex analysis techniques cannot be used for our
algorithm 

Several recent results  Jain   Netrapalli    Netrapalli
et al    Jain et al    Hardt   Wootters    Blumensath    show that under certain assumptions  projection onto nonconvex sets indeed lead to provable algorithms with fast convergence to the global optima  However  as explained in Section   RMC presents   unique
set of challenges as we have to perform error analysis with
the errors arising due to missing entries as well as sparse
corruptions  both of which interact among themselves as
well  In contrast  MC and RPCA only handle noise arising from one of the two sources  To overcome this challenge  we perform error analysis by segregating the effects
due to random sampling and sparse corruptions and leverage their unique structure to obtain our result  See proof
of Lemma   Another consequence of our careful error analysis is improved results for the MC as well as the
RPCA problem 

Our empirical results on synthetic data demonstrates effectiveness of our method  We also apply our method to the
foreground background separation problem and  nd that
our method is an order of magnitude faster than the stateof theart method  StNcRPCA  while achieving similar
accuracy 

In   concurrent and independent work   Yi et al   
also studied the RMC problem and obtained similar results 
They study an alternating gradient descent style algorithm
while our algorithm is based on lowrank projected gradient descent  Our sample complexity  corruption tolerance
as well as time complexity differ along certain critical parameters     Sample complexity  Our sample complexity
bound is dependent only logarithmically on   the condition number of the matrix     see Table   On the other
hand  result of  Yi et al    depends quadratically on  
which can be signi cantly large  Another difference is that
our sample complexity bound depends logarithmically on
the  nal error    de ned as      cid       cid  which for typical  nite precision computation only introduces an extra
constant factor     Corruption tolerance  Our result allows
the fraction of corrupted entries to be information theoretic
     while the result of  Yi
optimal  up to   constant      
fracet al    allows only  
min
tion of corrupted entries     Time Complexity  As   consequence of the sample complexity bounds  running time
of the method by  Yi et al    depends quintically on  
whereas our algorithm only has   polylogarithmic dependence 

 cid cid 

 
 
  

 cid 

 cid 

    

 

  

    

In summary  this paper   main contributions are 
    RMC  We propose   nearly linear time method that
solves RMC with       nr  log    log   cid   cid  random entries and with optimal fraction of corruptions upto
constant factors         
    Matrix Completion  Our result improves upon the
existing linear time algorithm   sample complexity by an
     factor  and time complexity by      factor  although with an extra   log  cid   cid  factor in both time and
sample complexity 
    RPCA  We present   nearly linear time    nr  algorithm for RPCA under optimal fraction of corruptions 
improving upon   mnr  time complexity of the existing
methods 

Notations  We assume that           cid    and        
                      cid     cid   cid   denotes  cid   norm

of   vector     cid   cid  denotes  cid  norm of     cid   cid   cid   cid    
 cid   cid  denotes the operator  Frobenius  and nuclear norm of
   respectively  by default  cid   cid     cid   cid  Operator    is
given by   operators Pk    and HT     are de ned in
Section         denotes ith singular value of   and  
denotes the ith singular value of   

 

Nearly Optimal Robust Matrix Completion

Paper Organization  We present our main algorithm in
Section   and our main results in Section   We also present
an overview of the proof in Section   Section   presents
our empirical result  Due to lack of space  we present most
of the proofs and useful lemmas in the appendix 

  Algorithm
In this section we present our algorithm for solving the
RMC  Robust Matrix Completion  problem  given   and

      where           cid      Rm    rank        
 cid cid   cid      and        cid    the goal is to recover   

To this end  we focus on solving the following nonconvex
optimization problem 

         arg min

 cid                  cid 

 

   

     rank                 cid   cid      

 

For the above problem  we propose   simple iterative algorithm that combines projected gradient descent  for    with
alternating projections  for    In particular  we maintain
iterates      and      where      is the current lowrank
approximation of    and      is the current sparse approximation of         is computed using gradient descent
step for objective   and then projecting back onto the set
of rank   matrices  That is 

 cid 

 cid 

       Pk

      

 
 

                 

 

 

where Pk    denotes projection of   onto the set of rankk matrices and can be computed ef ciently using SVD of
 
mn       is computed by projecting the residual
      
           onto set of sparse matrices using   hardthresholding operator      

       HT          

 
where HT     Rm     Rm   is the hard thresholding
operator de ned as   HT    ij   Aij if  Aij      and
  otherwise  Intuitively    better estimate of the sparse corruptions for each iteration will reduce the noise of the projected gradient descent step and   better estimate of the low
rank matrix will enable better estimation of the sparse corruptions  Hence  under correct set of assumptions  the algorithm should recover       exactly 
Unfortunately  just the above two simple iterations cannot
handle problems where    has poor condition number  as
the intermediate errors can be signi cantly larger than the
smallest singular values of    making recovery of the corresponding singular vectors challenging  To alleviate this
issue  we propose an algorithm that proceeds in stages  In

the qth stage  we project      onto set of rankkq matrices 
Rank kq is monotonic           Under standard assumptions  we show that we can increase kq in   manner such

that after each stage cid cid          cid cid  decreases by at least  

constant factor  Hence  the number of stages is only logarithmic in the condition number of    See Algorithm  
 PGRMC   for   pseudocode of the algorithm 

We require an upper bound of the  rst singular value for our
algorithm to work  Speci cally  we require        
 
Alternatively  we can also obtain an estimate of  
  by using
the thresholding technique from  Yi et al    although
this requires an estimate of the number of corruptions in
each row and column  We also use   simpli ed version of
Algorithm   from  Hardt   Wootters    to form independent sets of samples for each iteration which is required
for our theoretical analysis  Our algorithm has an  outer
loop   see Line   which sets rank kq of iterates      appropriately  see Line   We then update      and      in
the  inner loop  using     We set threshold for the
hardthresholding operator using singular values of current
gradient descent update  see Line   Note that  we divide
  uniformly into       sets  where   is an upper bound on
the number of outer iterations and   is the number of inner
iterations  This division ensures independence across iterates that is critical to application of standard concentration
bounds  such division is   standard technique in the matrix completion related literature  Jain   Netrapalli   
Hardt   Wootters    Recht    Also    is   tunable
parameter which should be less than one and is smaller for
 easier  problems 

Note that updating      requires                      
computational steps  Computation of      requires computing SVD for projection Pr  which can be computed in
                           time  ignoring log factors 
see  Jain et al    for more details  Hence  the computational complexity of each step of the algorithm is linear
in        assuming                 As we show in
the next section  the algorithm exhibits geometric convergence rate under standard assumptions and hence the overall complexity is still nearly linear in    assuming   is just
  constant 

Rank based Stagewise algorithm  We also provide  
rankbased stagewise algorithm  RRMC  where the outer
loop increments kq by one at each stage       the rank is  
in the qth stage  Our analysis extends for this algorithm as
well  however  its time and sample complexity trades off  
factor of   log  from the complexity of PGRMC
with   factor of    rank of    We provide the detailed
algorithm in Appendix   due to lack of space  see Algorithm  

Nearly Optimal Robust Matrix Completion

Algorithm  cid     PGRMC                  

 

 

       

  Input  Observed entries   Matrix         Rm   
convergence criterion   target rank    incoherence parameter   thresholding parameter   estimate of  rst
singular value  

QT   using algorithm  

             with   set to

        log  nr 
  Partition   into           subsets               
                
        mn       HT     
              
  while  kq        
         
 
kq  
 
for Iteration       to       do
       HT                
               mn                            
       Pkq       
         

 cid cid cid cid 
 cid cid cid cid               kq    
 cid   
 kq        cid   

 kq       

   do

 cid 

 

 

 

 cid 

end for
                                  
            

 
 
 
 
 
 
 

  end while
  Return       

quired Sampling Probability    Number of Sets  

Algorithm                   SplitSamples        
  Input  Random samples with probability       Re 
  Output    independent sets of entries              
    cid               
   cid  be sampled from   with each entry being included

sampled with sampling probability  

independently with probability   cid  

  cid 

qr     

  for       to       do
  pr      
 
  end for
  Initialize        for                 
  for Sample      cid  do
 
 
 
  end for

Draw                  with probability qr
Draw   random subset   of size   from             
Add   to    for      

  Analysis
We now present our analysis for both of our algorithms PGRMC  Algorithm   and RRMC  Algorithm   In general the problem of Robust PCA with Missing Entries  
is harder than the standard Matrix Completion problem and
hence is NPhard  Hardt et al    Hence  we need to

   cid   

impose certain  by now standard  assumptions on     cid   
Rm   is   rankr incoherent matrix        cid cid   cid 
    cid cid   
and   to ensure tractability of the problem 
Assumption   Rank and incoherence of        
     cid cid 
     cid cid   cid 
 cid   
                      where
Assumption   Sparsity of  cid       We assume that at
column of  cid    are nonzero for   small enough constant   
Moreover  we assume that   is independent of  cid    Hence 
       cid    also has       fraction of the entries in ex 

          cid  is the SVD of   
Assumption   Sampling     is obtained by sampling
 
mn  
each entry with probability    

   fraction of the elements in each row and

most      

pectation 

Assumptions     are standard assumptions in the provable matrix completion literature  Cand es   Recht   
Recht    Jain   Netrapalli    while Assumptions
    are standard assumptions in the robust PCA  lowrank sparse matrix recovery  literature  Chandrasekaran
et al    Cand es et al    Hsu et al    Hence 
our setting is   generalization of both the standard and popular problems and as we show later in the section  our result
can be used to meaningfully improve the stateof theart for
both of these problems 
We  rst present our main result for Algorithm   under the
assumptions given above 
  hold respectively  Let                 and let the
number of samples   satisfy 

Theorem   Let Assumptions     and   on     cid    and

           log      log 

 cid     

 cid 

 

 

where   is   global constant  Then  with probability at
least        log  
    at most
  log cid   cid  outer iterations and   log     cid   cid 
 
inner iterations  outputs   matrix    such that 

    Algorithm   with       

 

 cid cid cid         cid cid cid  

   

Note that the number of samples matches information theoretic bound upto     log   log   
  factor  Also  the

number of allowed corruptions in  cid    also matches the

known lower bounds  up to   constant factor  and cannot
be improved upon information theoretically 
We now present our result for the rank based stagewise algorithm  Algorithm  

Theorem   Under Assumptions     and   on     cid    and

  respectively and   satisfying 

           log      log

 cid     

 cid 

 

 

Nearly Optimal Robust Matrix Completion

for   large enough constant    then Algorithm   with   set
        
to   
         log  
   

  outputs   matrix    such that 

 cid cid cid         cid cid cid  

Notice that the sample complexity of Algorithm   has an
additional multiplicative factor of      when compared to
that of Algorithm   but shaves off   factor of   log 
Similarly  computational complexity of Algorithm   also
trades off     log   factor for      factor from the computational complexity of Algorithm  

Result for Matrix Completion  Note that for  cid        the

RMC problem with Assumptions   is exactly the same as
the standard matrix completion problem and hence  we get
the following result as   corollary of Theorem  
Corollary    Matrix Completion  Suppose we observe  
and      where Assumptions   hold for    and  
Also  let            log    log    and    
   Then                log  
    Algorithm   outputs        
 cid         cid     
Table   compares our sample and time complexity bounds
for lowrank MC  Note that our sample complexity is
nearly the same as that of nuclearnorm methods while the
running time of our algorithm is signi cantly better than
the existing results that have at most logarithmic dependence on the condition number of   
Result for Robust PCA  Consider the standard Robust
PCA problem  RPCA  where the goal is to recover   
sample   entries from    where   satis es the assumption required by Theorem   This leads us to the following
corollary 
Corollary    Robust PCA  Suppose we observe    

from         cid    For RPCA as well  we can randomly

      cid    where Assumptions     hold for    and
 cid    Generate               by sampling each en 

try independently with probability             
      log    log    Let        Then      
         log  
    Algorithm   outputs          cid         cid     
Hence  using Theorem   we will still be able to recover   
but using only the sampled entries  Moreover  the running
time of the algorithm is only   nr  log    log 
     we are able to solve RPCA problem in time almost linear in    To the best of our knowledge  the existing stateof theart methods for RPCA require at least        time
to perform the same task  Netrapalli et al    Gu et al 
  Similarly  we don   need to load the entire data matrix in memory  but we can just sample the matrix and work
with the obtained sparse matrix with at most linear number
of entries  Hence  our method signi cantly reduces both
runtime and space complexities  and as demonstrated empirically in Section   can help scale our algorithm to very
large data sets without losing accuracy 

  Proof Outline for Theorem  

We now provide an outline of our proof for Theorem  
and motivate some of our proof techniques  the proof of
Theorem   follows similarly  Recall that we assume that

        cid    and de ne        cid    Similarly  we de 
 ne  cid        HT           Critically           cid     
 see Line   of Algorithm         cid      is the set of iterates
we cannot compute  cid      it is introduced only to simplify

that we  could  obtain if entire   was observed  Note that

our analysis 

We  rst rewrite the projected gradient descent step for
     as described in  

       Pkq

 cid 

 

      cid     cid     
 cid cid 
 cid 
 cid 
 cid 
 cid cid 
 cid 
 cid 
            cid       cid   
 cid cid 
 cid 

  

  

 

  

 cid 
         
 cid 

 

 cid 

 

thresholding to reduce  cid cid       cid     cid  we need to bound

That is       is obtained by rankkq SVD of   perturbed
version of                 As we perform entrywise
 cid       cid  To this end  we use techniques from  Jain
  Netrapalli     Netrapalli et al    that explicitly
model singular vectors of      and argue about the in nity norm error using   Taylor series expansion  However 
in our case  such an error analysis requires analyzing the
following key quantities             

              even  

Aj   max
    

Bj   max
    

              odd  

Cj   max
    

Dj   max
    

 cid   cid 

 cid   cid 

 

 cid   cid   cid   
 cid HH cid cid   
    cid cid HH cid cid cid   
    cid   cid   cid cid   

     cid 
    cid 
 cid 

 cid 

 

 cid   cid 

 cid   cid 

  cid 
   cid 
 

Note that        in the case of standard RPCA which was
analyzed in  Netrapalli et al    while        in the
case of standard MC which was considered in  Jain   Netrapalli   
In contrast  in our case both    and   
are nonzero  Moreover     is dependent on random variable   Hence  for       we will get cross terms between
   and    that will also have dependent random variables
which precludes application of standard Bernsteinstyle tail
bounds  To overcome this issue  we  rst carefully segregate
the errors arising due to the randomness in the sampling

process and the deterministic sparse corruptions in  cid    We
do this by introducing  cid      which is the sparse iterate we

Nearly Optimal Robust Matrix Completion

Table   Comparison of PGRMC and RRMC with Other Matrix Completion Methods

Sample Complexity

Nuclear norm  Recht   

SVP  Jain   Netrapalli   

  cid rn log    cid 
  cid     log    cid 
  cid      log    log    cid 
Alt  Grad  Desc   Sun   Luo      cid nr  max  log       cid 
 cid cid 
 cid 
 cid   
     log      log cid   
 cid 
 cid cid 

RRMC  This Paper 
PGRMC  This Paper 

Alt  Min   Hardt   Wootters   

     log      log

 

 

 
 

 
 

 

Computational Complexity

 cid 
  cid    log  
  cid     log    log   
   cid 
  cid      log    log    cid 
  cid      log cid   
 cid cid 
 cid cid 
 cid 
 cid   
     log      log cid   
 cid 
 cid cid 

 
     log      log

 
 

 
 

 

 

would have obtained had the whole matrix been observed 
This allows us to decompose the error term into the sum of
   and    where    represents the error due to the sparse
corruptions and    represents the error arising from the
randomness in the sampling procedure  We then incorporate this decomposition into   careful combinatorialstyle
argument similar to that of  Erdos et al    Jain   Netrapalli    to bound the above given quantity  That is 
we can provide the following key lemma 
  respectively  Let           cid  be the singular value
decomposition of    Furthermore  suppose that in the tth

Lemma   Let      and  cid    satisfy Assumptions     and
iteration of the qth stage   cid      de ned as HT         
satis es Supp cid        Supp cid    then we have 

 cid   

 

 cid 
   cid   cid 

 cid  

 

max Aa  Ba  Ca  Da     

 cid   

 

  

 cid        cid  log  

             log                 log  
    where
      and    are de ned in   Aa  Ba  Ca  Da are de 
 ned in  

Remark  We would like to note that even for the stani    when        we obtain betdard MC setting 
ter bound than that of  Jain   Netrapalli    as we
can bound maxi  cid eT
     qU cid  directly rather than the
  maxi  cid eT
     quj cid  bound that  Jain   Netraweaker
palli    uses 

 

kq   cid   

Now  using Lemmas   and   and by using   hardthresholding argument we can bound  cid          cid   
 
   
  
   see Lemma   in the qth stage 
kq
Hence  after   log 
   inner  iterations  we can guar 

 cid  

 

antee in the qth stage 

 cid           cid      
 
 
 cid   cid     cid   cid      
 

kq 

 
kq 

Moreover  by using sparsity of  cid    and the special structure

of     See Lemma   we have   cid        cid         
where   is   small constant 
Now  the outer iteration sets the next stage   rank kq  as 
kq                                kq          
   Using the bound on  cid        cid  and Weyl   eigen 
 
value perturbation bound  Lemma   we have   
   
   outer  iterations  Algorithm   converges to an  approximate solution
to   

kq  Hence  after       log 

kq 

kq 

  Experiments
In this section we discuss the performance of Algorithm
  on synthetic data and its use in foreground background
separation  The goal of the section is twofold     to
demonstrate practicality and effectiveness of Algorithm  
for the RMC problem     to show that Algorithm   indeed solves RPCA problem in signi cantly smaller time
than that required by the existing stateof theart algorithm
 StNcRPCA  Netrapalli et al    To this end  we use
synthetic data as well as video datasets where the goal is to
perform foregroundbackground separation  Cand es et al 
 
We implemented our algorithm in MATLAB and the results for the synthetic data set were obtained by averaging
over   runs  We obtained   matlab implementation of StNcRPCA  Netrapalli et al    from the authors of  Netrapalli et al    Note that if the sampling probability
is       then our method is similar to StNcRPCA  the
key difference being how the rank is selected in each stage 

Nearly Optimal Robust Matrix Completion

We also implemented the Alternating Minimzation based
algorithm from  Gu et al    However  we found it
to be an order of magnitude slower than Algorithm   on
the foregroundbackground separation task  For example 
on the escalator video  the algorithm did not converge in
less than   seconds despite discounting for the expensive sorting operation in the truncation step  On the other
hand  our algorithm  nds the foreground in about   seconds 

Parameters  The algorithm has three main parameters   
threshold     incoherence   and   sampling probability             mn  In the experiments on synthetic
 
 
speeds up the recovery while for background extraction

data we observed that keeping      cid cid         cid cid   
keeping      cid cid         cid cid     gives   better quality out 

put  The value of   for real world data sets was  gured
out using cross validation while for the synthetic data the
same value was used as used in data generation  The sampling probability for the synthetic data could be kept as low
as      log     while for the real world data set we
get good results for       We de ne effective sample size as the ratio between the sampling probability and
  Also  rather than splitting samples  we use the entire
set of observed entries to perform our updates  see Algorithm  

Synthetic data  We generate           cid    of two sizes 
coherence      cid    is generated by considering   uni 

where           cid        and    is  
random rank   and rank  respectively  matrix with in 

from           where
formly random subset of size
every entry is       
from the uniform distribution in
  
 
mn   This is the same setup as used in  Cand es
 
 
mn  
 
et al   
Figure       plots recovery error  cid       cid     vs computational time for our PGRMC method  with different
sampling probabilities  as well as the StNcRPCA algorithm  Note that even for very small values of sampling
   we can achieve the same recovery error as when using
signi cantly small values  For example  our method with
      achieve   error  cid       cid     in      while
StNcRPCA method requires      to achieve the same
accuracy  Note that we do not compare against the convex
relaxation based methods like IALM from  Cand es et al 
  as  Netrapalli et al    shows that StNcRPCA
is signi cantly faster than IALM and several other convex
relaxation solvers 

 cid cid cid cid   cid cid cid 

Figure       plots time required to achieve different recovery errors  cid       cid     as the sampling probability   increases  As expected  we observe   linear increase in the
runtime with    Interestingly  for very small values of   
we observe an increase in running time 
In this regime 

   

   

   

   

   

Figure   Performance of PGRMC on synthetic data     
time vs error for various sampling probabilities  time taken
by StNcRPCA     sampling probability vs time for constant error  time taken decreases with decreasing sampling
probability upto an extent and then increases     time vs
rank for constant error    
incoherence vs time for constant error     success probability vs effective sample size
for various matrix sizes

 

 cid      cid 
becomes very large  as   doesn   satisfy the sampling requirements  Hence  the increase in the number of
 cid      cid 
iterations      log
  dominates the decrease in
per iteration time complexity 

  

Figure           plot computation time required by our
method  PGRMC   Algorithm   versus rank and incoherence  respectively  As expected  as these two problem
parameters increase  our method requires more time  Note
that our runtime dependence on rank seems to be linear 
while our results require      time  This hints at the possibility of further improving the computational complexity
analysis of our algorithm 

Figure       plots the effective sample size against success probability  We see that the probability of recovering
the underlying low rank matrix undergoes   rapid phase

Time   log                     StNcRPCASampling probability Time       err      err      err      Rank Time         err      err      err      Time         err      err      err      effective sample size success probability                                 Nearly Optimal Robust Matrix Completion

eral videos  Figure           show one frame each from two
such videos    shopping center video    restaurant video 
Figure           show the extracted background from the
two videos by using our method  PGRMC   Algorithm  
with probability of sampling       Figure          
compare objective function value for different   values 
Clearly  PGRMC can recover the true background with
  as small as   We also observe an order of magnitude speedup       over StNcRPCA  Netrapalli et al 
  We present results on the video Escalator in Appendix  

Conclusion  In this work  we studied the Robust Matrix
Completion problem  For this problem  we provide exact
recovery of the lowrank matrix    using nearly optimal
number of observations as well as nearly optimal fraction
of corruptions in the observed entries  Our RMC result
is based on   simple and ef cient PGD algorithm that has
nearly linear time complexity as well  Our result improves
stateof theart sample and runtime complexities for the
related Matrix Completion as well as Robust PCA problem  For Robust PCA  we provide  rst nearly linear time
algorithm under standard assumptions 

Our sample complexity depends on   the desired accuracy in    Removing this factor will be an interesting
future work  Moreover  improving dependence of sample
complexity on    from    to    also represents an important direction  Finally  similar to foreground background
separation  we would like to explore more applications of
RMC RPCA 

References
Bhatia  Rajendra  Matrix Analysis  Springer   

Blumensath  Thomas  Sampling and reconstructing signals
IEEE Trans  Inforfrom   union of linear subspaces 
mation Theory      doi   
TIT 
URL http dx doi org 
 TIT 

Cand es  Emmanuel    and Recht  Benjamin  Exact matrix completion via convex optimization  Foundations
of Computational Mathematics    December  

Cand es  Emmanuel    Li  Xiaodong  Ma  Yi  and Wright 
John  Robust principal component analysis     ACM   
   

Chandrasekaran  Venkat  Sanghavi  Sujay  Parrilo 
Pablo    and Willsky  Alan    Ranksparsity incoherence for matrix decomposition  SIAM Journal on
Optimization     

   

   

   

   

   

   

Figure   PGRMC on Shopping video        video frame
    an extracted background frame     time vs error for different sampling probabilities  PGRMC takes    while
StNcPCA takes     PGRMC on Restaurant video 
      video frame     an extracted background frame
    time vs error for different sampling probabilities  PGRMC takes    while StNcPCA takes   

transition from   to   when sampling probability crosses
    log      
We also study phase transition for different values of sampling probability    Figure       in Appendix   show  
phase transition phenomenon where beyond       the
probability of recovery is almost   while below it  it is almost  

Foregroundbackground separation  We also applied
our technique to the problem of foregroundbackground
separation  We use the usual method of stacking up the
vectorized video frames to construct   matrix  The background  being static  will form the low rank component
while the foreground is considered to be the noise 

We applied our PGRMC method  with varying    to sev 

Time   log                     StNcRPCATime   log                          StNcRPCANearly Optimal Robust Matrix Completion

Montreal  Quebec  Canada  pp      URL
http papers nips cc paper oniterative hardthresholding methodsfor highdimensional mestimation 

Jalali  Ali  Ravikumar  Pradeep  Vasuki  Vishvas  and
Sanghavi  Sujay  On learning discrete graphical models
In Proceedings of
using groupsparse regularization 
the Fourteenth International Conference on Arti cial
Intelligence and Statistics  AISTATS   Fort Lauderdale  USA  April     pp     
URL
http www jmlr org proceedings 
papers   jalali   jalali   pdf 

Klopp  Olga  Lounici  Karim  and Tsybakov  AlexanarXiv preprint

dre    Robust matrix completion 
arXiv   

Li  Xiaodong  Compressed sensing and matrix completion with constant proportion of corruptions  Constructive Approximation     
ISSN  
  doi      URL http 
 dx doi org   

Netrapalli  Praneeth       Niranjan  Sanghavi  Sujay 
Anandkumar  Animashree  and Jain  Prateek  Nonconvex robust pca 
In Ghahramani     Welling 
   Cortes     Lawrence        and Weinberger 
       eds  Advances in Neural Information Processing Systems   pp    Curran Associates  Inc 
 
URL http papers nips cc paper 
 nonconvex robustpca pdf 

Recht  Benjamin    simpler approach to matrix comJournal of Machine Learning Research 
pletion 
    URL http dl acm org 
citation cfm id 

Sun  Ruoyu and Luo  ZhiQuan  Guaranteed matrix completion via nonconvex factorization  In IEEE  th Annual Symposium on Foundations of Computer Science 
FOCS   Berkeley  CA  USA    October   
pp      doi   FOCS  URL
http dx doi org FOCS 

Yi  Xinyang  Park  Dohyung  Chen  Yudong  and Caramanis  Constantine  Fast algorithms for robust PCA via
gradient descent  CoRR  abs    URL
http arxiv org abs 

Chen  Yudong  Jalali  Ali  Sanghavi  Sujay  and Caramanis  Constantine  Lowrank matrix recovery from errors
and erasures  In   IEEE International Symposium on
Information Theory Proceedings  ISIT   St  Petersburg  Russia  July     August     pp   
  doi   ISIT  URL http 
 dx doi org ISIT 

Erdos    aszl    Knowles  Antti  Yau  HorngTzer  and Yin 
Jun  Spectral statistics of Erdos   enyi graphs    Local
semicircle law  The Annals of Probability     
   

Gu  Quanquan  Wang  Zhaoran Wang  and Liu  Han  Lowrank and sparse structure pursuit via alternating minIn Proceedings of the Nineteenth Internaimization 
tional Conference on Arti cial Intelligence and Statistics  AISTATS     adiz  Spain  May    
  URL http jmlr org proceedings 
papers   gu html 

Hardt  Moritz and Wootters  Mary 

Fast matrix comIn Proceedings
pletion without the condition number 
of The  th Conference on Learning Theory  COLT
  Barcelona  Spain  June     pp   
  URL http jmlr org proceedings 
papers   hardt   html 

Computational

Hardt  Moritz  Meka  Raghu  Raghavendra  Prasad  and
limits for matrix
Weitz  Benjamin 
In Proceedings of The  th Confercompletion 
ence on Learning Theory  COLT   Barcelona 
Spain  June     pp      URL
http jmlr org proceedings papers 
  hardt   html 

Hsu  Daniel  Kakade  Sham    and Zhang  Tong  Robust
matrix decomposition with sparse corruptions  Information Theory  IEEE Transactions on   
 

Jain  Prateek and Netrapalli  Praneeth  Fast exact maIn Proceedings
trix completion with  nite samples 
of The  th Conference on Learning Theory  COLT
  Paris  France  July     pp   
  URL http jmlr org proceedings 
papers   Jain html 

Jain  Prateek  Meka  Raghu  and Dhillon  Inderjit    Guaranteed rank minimization via singular value projection 
In NIPS  pp     

Jain  Prateek  Tewari  Ambuj  and Kar  Purushottam  On iterative hard thresholding methods for highdimensional
mestimation  In Advances in Neural Information Processing Systems   Annual Conference on Neural Information Processing Systems   December    

