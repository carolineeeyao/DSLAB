SARAH    Novel Method for Machine Learning Problems

Using Stochastic Recursive Gradient

Lam    Nguyen   Jie Liu   Katya Scheinberg     Martin Tak      

Abstract

In this paper  we propose   StochAstic Recursive grAdient algoritHm  SARAH  as well as its
practical variant SARAH  as   novel approach
to the  nitesum minimization problems  Different from the vanilla SGD and other modern
stochastic methods such as SVRG    GD  SAG
and SAGA  SARAH admits   simple recursive
framework for updating stochastic gradient estimates  when comparing to SAG SAGA  SARAH
does not require   storage of past gradients  The
linear convergence rate of SARAH is proven under strong convexity assumption  We also prove
  linear convergence rate  in the strongly convex
case  for an inner loop of SARAH  the property
that SVRG does not possess  Numerical experiments demonstrate the ef ciency of our algorithm 

  Introduction
We are interested in solving   problem of the form

       

min
  Rd

 cid 

    

def
 

 
 

   

fi   

 

where each fi         
               is convex with  
Lipschitz continuous gradient  Throughout the paper  we
assume that there exists an optimal solution    of  

def

Industrial

 Department of

and Systems Engineering 
 On leave at The University of
Lehigh University  USA 
Oxford  UK  All authors were supported by NSF Grant
CCF 
Katya Scheinberg was partially supported
by NSF Grants DMS   CCF  and CCF 
 
Correspondence to  Lam    Nguyen  lamnguyen mltd gmail com  Jie Liu  jie liu gmail com 
Katya Scheinberg  katyas lehigh edu  Martin Tak    
 Takac MT gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

def

  log    exp yixT

Problems of this type arise frequently in supervised learning applications  Hastie et al    Given   training set
 xi  yi  
   with xi   Rd  yi      the least squares redef
gression model  for example  is written as   with fi   
 
 cid   cid  where  cid cid  denotes the  cid norm  The
    yi     
 xT
 cid regularized logistic regression for binary classi cation
 cid   cid 
is written with fi   
 yi      
In recent years  many advanced optimization methods have
been developed for problem   While the objective function is smooth and convex  the traditional optimization
methods  such as gradient descent  GD  or Newton method
are often impractical for this problem  when     the number of training samples and hence the number of fi     is
very large  In particular  GD updates iterates as follows

        

wt    wt         wt 

                 

Under strong convexity assumption on   and with appropriate choice of     GD converges at   linear rate in terms of
objective function values    wt  However  when   is large 
computing     wt  at each iteration can be prohibitive 
As an alternative  stochastic gradient descent  SGD  originating from the seminal work of Robbins and Monro in
   Robbins   Monro    has become the method
of choice for solving   At each step  SGD picks an index         uniformly at random  and updates the iterate as
wt    wt      fi wt  which is upto   times cheaper
than an iteration of   full gradient method  The convergence rate of SGD is slower than that of GD  in particular  it
is sublinear in the strongly convex case  The tradeoff  however  is advantageous due to the tremendous periteration
savings and the fact that low accuracy solutions are suf 
cient  This tradeoff has been thoroughly analyzed in  Bottou    Unfortunately  in practice SGD method is often
too slow and its performance is too sensitive to the variance in the sample gradients  fi wt  Use of minibatches
 averaging multiple sample gradients  fi wt  was used
in  ShalevShwartz et al    Cotter et al    Tak    

 We mark here that even though stochastic gradient is referred
to as SG in literature  the term stochastic gradient descent  SGD 
has been widely used in many important works of largescale
learning  including SAG SAGA  SDCA  SVRG and MISO 

SARAH    Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient

Table   Comparisons between different algorithms for strongly
convex functions         is the condition number 

Method

Complexity

      log  

   

GD
SGD
         log  
SVRG
SAG SAGA          log  
SARAH          log  

Fixed

Learning

Rate
 
 
 

Low

Storage

Cost
 
 
 

 

 

 

 

Table   Comparisons between different algorithms for convex
functions 

Method

GD
SGD
SVRG
SAGA
SARAH

loop 

SARAH  one outer

Complexity
     
 
        
  
          

  cid cid 
  cid      cid 

         log 

et al    to reduce the variance and improve convergence rate by constant factors  Using diminishing sequence
    is used to control the variance  ShalevShwartz et al 
  Bottou et al    but the practical convergence
of SGD is known to be very sensitive to the choice of this
sequence  which needs to be handpicked 
Recently    class of more sophisticated algorithms have
emerged  which use the speci    nitesum form of   and
combine some deterministic and stochastic aspects to reduce variance of the steps  The examples of these methods are SAG SAGA  Le Roux et al    Defazio et al 
  SDCA  ShalevShwartz   Zhang    SVRG
 Johnson   Zhang    Xiao   Zhang    DIAG
 Mokhtari et al    MISO  Mairal    and   GD
 Kone cn     Richt arik    all of which enjoy faster convergence rate than that of SGD and use    xed learning rate
parameter   In this paper we introduce   new method in
this category  SARAH  which further improves several aspects of the existing methods  In Table   we summarize
complexity and some other properties of the existing methods and SARAH when applied to strongly convex problems  Although SVRG and SARAH have the same convergence rate  we introduce   practical variant of SARAH that
outperforms SVRG in our experiments 
In addition  theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived  Schmidt et al    Defazio
et al    Reddi et al    AllenZhu   Yuan   
AllenZhu    In Table   we summarize the key complexity results  noting that convergence rate is now sublinear 

Our Contributions 
In this paper  we propose   novel
algorithm which combines some of the good properties of
existing algorithms  such as SAGA and SVRG  while aiming to improve on both of these methods  In particular  our
algorithm does not take steps along   stochastic gradient
direction  but rather along an accumulated direction using
past stochastic gradient information  as in SAGA  and occasional exact gradient information  as in SVRG  We summarize the key properties of the proposed algorithm below 
  Similarly to SVRG  SARAH   iterations are divided
into the outer loop where   full gradient is computed
and the inner loop where only stochastic gradient is
computed  Unlike the case of SVRG  the steps of
the inner loop of SARAH are based on accumulated
stochastic information 
  Like SAG SAGA and SVRG  SARAH has   sublinear rate of convergence for general convex functions 
and   linear rate of convergence for strongly convex
functions 
  SARAH uses   constant learning rate  whose size is
larger than that of SVRG  We analyze and discuss the
optimal choice of the learning rate and the number
of inner loop steps  However  unlike SAG SAGA but
similar to SVRG  SARAH does not require   storage
of   past stochastic gradients 
  We also prove   linear convergence rate  in the
strongly convex case  for the inner loop of SARAH 
the property that SVRG does not possess  We show
that the variance of the steps inside the inner loop goes
to zero  thus SARAH is theoretically more stable and
reliable than SVRG 
  We provide   practical variant of SARAH based on
the convergence properties of the inner loop  where
the simple stable stopping criterion for the inner loop
is used  see Section   for more details  This variant shows how SARAH can be made more stable than
SVRG in practice 

  Stochastic Recursive Gradient Algorithm
Now we are ready to present our SARAH  Algorithm  
The key step of the algorithm is   recursive update of the
stochastic gradient estimate  SARAH update 

vt    fit wt     fit wt    vt 

followed by the iterate update 

wt    wt    vt 

 

 

For comparison  SVRG update can be written in   similar
way as

vt    fit wt     fit        

 

SARAH    Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient

 cid  

Algorithm   SARAH

Parameters  the learning rate       and the inner loop
size   
Initialize     
Iterate 
for               do

 cid  
    fi   

      ws 
      
             
 
Iterate 
for                     do

Sample it uniformly at random from    
vt    fit wt     fit wt    vt 
wt    wt    vt

end for
Set  ws   wt with   chosen uniformly at random from
              

end for

Observe that in SVRG  vt is an unbiased estimator of the
gradient  while it is not true for SARAH  Speci cally   
  vt Ft        wt    wt vt   cid      wt   
where   Ft                     it  is the  algebra generated by                  it                Hence 
SARAH is different from SGD and SVRG type of methods 
however  the following total expectation holds    vt   
     wt  differentiating SARAH from SAG SAGA 
SARAH is similar to SVRG since they both contain outer
loops which require one full gradient evaluation per outer
iteration followed by one full gradient descent step with  
given learning rate  The difference lies in the inner loop 
where SARAH updates the stochastic step direction vt recursively by adding and subtracting component gradients
to and from the previous vt         in   Each inner iteration evaluates   stochastic gradients and hence the total
work per outer iteration is        in terms of the number
of gradient evaluations  Note that due to its nature  without
running the inner loop             SARAH reduces to the
GD algorithm 

  Theoretical Analysis
To proceed with the analysis of the proposed algorithm  we
will make the following common assumptions 
Assumption    Lsmooth  Each fi   Rd              is
Lsmooth       there exists   constant       such that
 cid fi       fi   cid cid      cid       cid cid        cid    Rd 
    Ft    Eit   which is expectation with respect to the
random choice of index it  conditioned on                  it 
 Ft also contains all the information of            wt as well as

           vt 

this assumption implies

Note that
that        
   fi    is also Lsmooth  The following strong con 
 
 
vexity assumption will be made for the appropriate parts of
the analysis  otherwise  it would be dropped 
Assumption     strongly convex  The function    
Rd      is  strongly convex       there exists   constant
      such that       cid    Rd 
             cid          cid          cid     

 cid       cid cid 

Another  stronger  assumption of  strong convexity for  
will also be imposed when required in our analysis  Note
that Assumption    implies Assumption    but not vice
versa 
Assumption     Each function fi   Rd              is
strongly convex with      

Under Assumption     let us de ne the  unique  optimal
solution of   as    Then strong convexity of   implies
that

                  cid      cid       Rd 

 

We note here  for future use  that for strongly convex functions of the form   arising in machine learning applicadef
tions  the condition number is de ned as  
     Furthermore  we should also notice that Assumptions    and
   both cover   wide range of problems         regularized
empirical risk minimization problems with convex losses 
Finally  as   special case of the strong convexity of all fi  
with       we state the general convexity assumption 
which we will use for convergence analysis 
Assumption   Each function fi   Rd              is
convex      

fi      fi   cid     fi   cid          cid 

        

Again  we note that Assumption    implies Assumption  
but Assumption    does not  Hence in our analysis  depending on the result we aim at  we will require Assumption   to hold by itself  or Assumption    and Assumption  
to hold together  or Assumption    to hold by itself  We
will always use Assumption  
Our iteration complexity analysis aims to bound the number of outer iterations    or total number of stochastic
gradient evaluations  which is needed to guarantee that
 cid    wT  cid      In this case we will say that wT is an
 accurate solution  However  as is common practice for
stochastic gradient algorithms  we aim to obtain the bound
on the number of iterations  which is required to guarantee
the bound on the expected squared norm of   gradient      

  cid    wT  cid     

 

SARAH    Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient

for SARAH  SVRG  SGD   SGD with decreasing learning rate  and FISTA  an accelerated version of GD  Beck
  Teboulle    with         where the left plot shows
the trend over multiple outer iterations and the right plot
shows   single outer iteration  We can see that for SVRG 
 cid vt cid  decreases over the outer iterations  while it has an
increasing trend or oscillating trend for each inner loop 
In contrast  SARAH enjoys decreasing trends both in the
outer and the inner loop iterations 
We will now show that the stochastic steps computed by
SARAH converge linearly in the inner loop  We present
two linear convergence results based on our two different
assumptions of  strong convexity  These results substantiate our conclusion that SARAH uses more stable stochastic gradient estimates than SVRG  The following theorem
is our  rst result to demonstrate the linear convergence of
our stochastic recursive step vt 
Theorem     Suppose that Assumptions      and   hold 
Consider vt de ned by   in SARAH  Algorithm   with
 cid 
        Then  for any      
 cid 
      
      

 cid    cid vt cid 
 cid     cid      cid 

  cid vt cid   cid 
 cid 

   cid   
   cid   

This result implies that by choosing          we obtain the linear convergence of  cid vt cid  in expectation with the
rate       Below we show that   better convergence
rate can be obtained under   stronger convexity assumption 
Theorem     Suppose that Assumptions   and    hold 
Consider vt de ned by   in SARAH  Algorithm   with
           Then the following bound holds         

  cid vt cid   cid 
 cid 

  

       
       

  

 cid    cid vt cid 
 cid     cid      cid 

Again  by setting          we derive the linear convergence with the rate of       which is   signi cant
improvement over the result of Theorem     when the problem is severely illconditioned 

  Convergence Analysis

In this section  we derive the general convergence rate results for Algorithm   First  we present two important Lemmas as the foundation of our theory  Then  we proceed to
prove sublinear convergence rate of   single outer iteration
when applied to general convex functions  In the end  we

 In the plots of Figure   since the data for SVRG is noisy  we
smooth it by using moving average  lters with spans   for the
left plot and   for the right one 

Figure     twodimensional example of minw       with      
for SVRG  left  and SARAH  right 

Figure   An example of  cid regularized logistic regression on
rcv  training dataset for SARAH  SVRG  SGD  and FISTA with
multiple outer iterations  left  and   single outer iteration  right 

  Linearly Diminishing StepSize in   Single Inner

Loop

The most important property of the SVRG algorithm is the
variance reduction of the steps  This property holds as the
number of outer iteration grows  but it does not hold  if only
the number of inner iterations increases  In other words  if
we simply run the inner loop for many iterations  without
executing additional outer loops  the variance of the steps
does not reduce in the case of SVRG  while it goes to zero
in the case of SARAH  To illustrate this effect  let us take  
look at Figures   and  
In Figure   we applied one outer loop of SVRG and
SARAH to   sum of   quadratic functions in   twodimensional space  where the optimal solution is at the origin  the black lines and black dots indicate the trajectory of
each algorithm and the red point indicates the  nal iterate 
Initially  both SVRG and SARAH take steps along stochastic gradient directions towards the optimal solution  However  later iterations of SVRG wander randomly around the
origin with large deviation from it  while SARAH follows  
much more stable convergent trajectory  with    nal iterate
falling in   small neighborhood of the optimal solution 
In Figure   the xaxis denotes the number of effective
passes which is equivalent to the number of passes through
all of the data in the dataset  the cost of each pass being
equal to the cost of one full gradient evaluation  and yaxis
represents  cid vt cid  Figure   shows the evolution of  cid vt cid 

  Simple Example with SVRGx     Simple Example with SARAHx   Number of Effective Passes kvtk rcv  Moving Average with Span  SARAHSVRGSGD FISTANumber of Effective Passes kvtk rcv  Moving Average with Span  SARAHSVRGSGD FISTAprove that the algorithm with multiple outer iterations has
linear convergence rate in the strongly convex case 
We begin with proving two useful lemmas that do not require any convexity assumption  The  rst Lemma   bounds
the sum of expected values of  cid    wt cid  The second 
Lemma   bounds   cid    wt    vt cid 
Lemma   Suppose that Assumption   holds  Consider
SARAH  Algorithm   Then  we have

  cid    wt cid     
 

               

 

  cid    wt    vt cid          

  

  

  cid 

 

  cid vt cid 

Lemma   Suppose that Assumption   holds  Consider vt
de ned by   in SARAH  Algorithm   Then for any      

  cid 

  

  cid 

  cid 

    cid 

  

  cid    wj        wj cid 

  

Now we are ready to provide our main theoretical results 

  GENERAL CONVEX CASE

Following from Lemma   we can obtain the following upper bound for   cid    wt    vt cid  for convex functions
fi         
Lemma   Suppose that Assumptions   and   hold  Consider vt de ned as   in SARAH  Algorithm   with    
    Then we have that for any      
  cid    wt    vt cid      
      
    
      

 cid   cid   cid      cid vt cid 

  cid   cid 

 cid 

 

Using the above lemmas  we can state and prove one of our
core theorems as follows 
Theorem   Suppose that Assumptions   and   hold  Consider SARAH  Algorithm   with         Then for any
      we have
  cid      ws cid   

 

       ws         
  cid      ws cid 

      
  
      

 

 cid  

  

 

   
   

  cid    wt cid 

                 cid  

  

  cid    wt    vt cid 
  cid   cid 

 

                     
  

 
Since we are considering one outer iteration  with      
then we have                     ws   since     
 ws  and  ws   wt  where   is picked uniformly at random from                Therefore  the following holds 

  cid      ws cid     

  

  cid    wt cid 

  

 

 

       ws         
  cid      ws cid 
Theorem   in the case when        implies that

   
    
  

 cid  

 

   

       ws         

   LE cid      ws cid 

 cid   

  

By choosing the learning rate    

that
vergence result 
  cid      ws cid 

 cid   
      with   such
           we can derive the following con 
 cid    

       ws             cid      ws cid 
Clearly  this result shows   sublinear convergence rate for
SARAH under general convexity assumption within   single inner loop  with increasing    and consequently  we
have the following result for complexity bound 
 cid   
Corollary   Suppose that Assumptions   and   hold  Consider SARAH  Algorithm   within   single outer iteration
     where           is
with the learning rate    
the total number of iterations  then  cid    wt cid  converges
     and theresublinearly in expectation with   rate of
fore  the total complexity to achieve an  accurate solution
de ned in   is        
We now turn to estimating convergence of SARAH with
multiple outer steps  Simply using Theorem   for each of
the outer steps we have the following result 
Theorem   Suppose that Assumptions   and   hold  Consider SARAH  Algorithm   and de ne

 cid    

SARAH    Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient
Hence  by Lemma   with         we have

  cid    wt    vt cid   

  cid vj   vj cid 

  cid      ws cid   

 

    

 

   

       wk                               

Proof  Since             implies  cid         cid     
then by Lemma   we can write

and     max          Then we have

  cid      ws cid           cid        cid     

 

  cid    wt    vt cid       
  

  cid   cid 

 

where      

  

      

   

  and       

    

 cid 

 cid 

 cid  

SARAH    Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient

Figure   Theoretical comparisons of learning rates  left  and convergence rates  middle and right  with           for SVRG and
SARAH in one inner loop 

Based on Theorem   we have the following total complexity for SARAH in the general convex case 
Corollary   Let us choose              with    
    and        in Theorem   Then  the total
complexity to achieve an  accuracy solution de ned in  
is         log 

  STRONGLY CONVEX CASE

We now turn to the discussion of the linear convergence
rate of SARAH under the strong convexity assumption on
    From Theorem   for any       using property   of
the  strongly convex     we have

  cid      ws cid   

 cid 

 

   
    
  

       ws         
  cid      ws cid 

 cid    cid      ws cid 

 

        
  

and equivalently 

  cid      ws cid         cid      ws cid 

 

def
 

 

        

Let us de ne   
    Then by choosing
  and   such that        and applying   recursively 
we are able to reach the following convergence result 
Theorem   Suppose that Assumptions      and   hold 
Consider SARAH  Algorithm   with the choice of   and  
such that

def
 

  

 

      

 

  
      

   

 

Then  we have

  cid      ws cid         cid        cid 

Remark   Theorem   implies that any        will work
for SARAH  Let us compare our convergence rate to that of
SVRG  The linear rate of SVRG  as presented in  Johnson
  Zhang    is given by

    

         

      

 

We observe that it implies that the learning rate has to
satisfy         which is   tighter restriction than

       required by SARAH  In addition  with the same
values of   and   the rate or convergence of  the outer
iterations  of SARAH is always smaller than that of SVRG 

    

 

 

        
      

 

    

     
 
         

 

 

   

Remark   To further demonstrate the better convergence
properties of SARAH  let us consider following optimization problem

min

  

   

min

  

   

which can be interpreted as the best convergence rates for
different values of    for both SARAH and SVRG  After
simple calculations  we plot both learning rates and the
corresponding theoretical rates of convergence  as shown
in Figure   where the right plot is   zoomin on   part
of the middle plot  The left plot shows that the optimal
learning rate for SARAH is signi cantly larger than that of
SVRG  while the other two plots show signi cant improvement upon outer iteration convergence rates for SARAH
over SVRG 

Based on Theorem   we are able to derive the following
total complexity for SARAH in the strongly convex case 
Corollary   Fix         and let us run SARAH with
        and       for   iterations where
     cid log cid        cid  log cid  then we can derive
an  accuracy solution de ned in   Furthermore  we
can obtain the total complexity of SARAH  to achieve the
 accuracy solution  as          log   

    Practical Variant
While SVRG is an ef cient variancereducing stochastic
gradient method  one of its main drawbacks is the sensitivity of the practical performance with respect to the choice
of    It is know that   should be around    while it
still remains unknown that what the exact best choice is  In
this section  we propose   practical variant of SARAH as

  In practice  when   is large        is often considered as  
regularized Empirical Loss Minimization problem with regularization parameter      

    then         

  Learning Rate Evolutions of Learning Rates   SARAH   SVRG   Convergence Rate Evolutions of Convergence Rates   SARAH   SVRG   Convergence Rate Evolutions of Convergence Rates   SARAH   SVRG SARAH    Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient

SARAH   Algorithm   which provides an automatic and
adaptive choice of the inner loop size    Guided by the linear convergence of the steps in the inner loop  demonstrated
in Figure   we introduce   stopping criterion based on the
values of  cid vt cid  while upperbounding the total number of
steps by   large enough   for robustness  The other modi cation compared to SARAH  Algorithm   is the more
practical choice  ws   wt  where   is the last index of the
particular inner loop  instead of randomly selected intermediate index 

Algorithm   SARAH 

Parameters  the learning rate                 and the
maximum inner loop size   
Initialize     
Iterate 
for               do

 cid  
    fi   

      ws 
      
             
 
     
while  cid vt cid     cid   cid  and       do
Sample it uniformly at random from    
vt    fit wt     fit wt    vt 
wt    wt    vt
         

end while
Set  ws   wt

end for

Different from SARAH  SARAH  provides   possibility of
earlier termination and unnecessary careful choices of   
and it also covers the classical gradient descent when we
set        since the while loop does not proceed  In Figure   we present the numerical performance of SARAH 
with different    on rcv  and news  datasets  The size
of the inner loop provides   tradeoff between the fast sublinear convergence in the inner loop and linear convergence
in the outer loop  From the results  it appears that      
is the optimal choice  With   larger              the
iterates in the inner loop do not provide suf cient reduction  before another full gradient computation is required 
while with       an unnecessary number of inner steps
is performed without gaining substantial progress  Clearly
  is another parameter that requires tuning  however  in our
experiments  the performance of SARAH  has been very
robust with respect to the choices of   and did not vary
much from one data set to another 
Similarly to SVRG   cid vt cid  decreases in the outer iterations
of SARAH  However  unlike SVRG  SARAH  also inherits from SARAH the consistent decrease of  cid vt cid  in
expectation in the inner loops  It is not possible to apply
the same idea of adaptively terminating the inner loop of

Figure   An example of  cid regularized logistic regression on
rcv   left  and news   right  training datasets for SARAH  with
different    on loss residuals              

Table   Summary of datasets used for experiments 

Dataset
covtype
ijcnn 
news 

rcv 

 
 
 

 

 

   train 
 
   
   
 

Sparsity
 
 
 
 

   test 
 
   
   
 

 

 
 
 
 

SVRG based on the reduction in  cid vt cid  as  cid vt cid  may have
side  uctuations as shown in Figure  

  Numerical Experiments
To support
the theoretical analyses and insights  we
present our empirical experiments  comparing SARAH and
SARAH  with the stateof theart  rstorder methods for
 cid regularized logistic regression problems with

fi      log    exp yixT

        

 cid   cid 

on datasets covtype  ijcnn  news  and rcv    For ijcnn 
and rcv  we use the prede ned testing and training sets 
while covtype and news  do not have test data  hence we
randomly split the datasets with   for training and  
for testing  Some statistics of the datasets are summarized
in Table  
The penalty parameter   is set to    as is common practice  Le Roux et al    Note that like SVRG   GD and
SAG SAGA  SARAH also allows an ef cient sparse implementation named  lazy updates   Kone cn   et al   
We conduct and compare numerical results of SARAH
with SVRG  SAG  SGD  and FISTA  SVRG  Johnson  
Zhang    and SAG  Le Roux et al    are classic
modern stochastic methods  SGD  is SGD with decreasing learning rate            where   is the number
of effective passes and   is some initial constant learning
rate  FISTA  Beck   Teboulle    is the Fast Iterative
ShrinkageThresholding Algorithm  wellknown as an ef 
 cient accelerated version of the gradient descent  Even
though for each method  there is   theoretical safe learning
rate  we compare the results for the best learning rates in
hindsight 
Figure   shows numerical results in terms of loss residuals

 All datasets are available at http www csie ntu 

edu tw cjlin libsvmtools datasets 

Number of Effective Passes              rcv Number of Effective Passes              news SARAH    Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient

Figure   Comparisons of loss residuals                top  and test errors  bottom  from different modern stochastic methods on
covtype  ijcnn  news  and rcv 

Table   Summary of best parameters for all the algorithms on
different datasets 

Dataset

covtype
ijcnn 
news 

rcv 

SARAH
     
       
       
       
       

SVRG
     
       
       
       

       

SAG
 
  
  
  
  

SGD 
 
  
  
  
  

FISTA
 
  
  
  
  

 top  and test errors  bottom  on the four datasets  SARAH
is sometimes comparable or   little worse than other methods at the beginning  However  it quickly catches up to or
surpasses all other methods  demonstrating   faster rate of
decrease across all experiments  We observe that on covtype and rcv  SARAH  SVRG and SAG are comparable
with some advantage of SARAH on covtype  On ijcnn 
and news  SARAH and SVRG consistently surpass the
other methods 
In particular  to validate the ef ciency of our practical variant SARAH  we provide an insight into how important
the choices of   and   are for SVRG and SARAH in Table   and Figure   Table   presents the optimal choices
of   and   for each of the algorithm  while Figure  
shows the behaviors of SVRG and SARAH with different
choices of   for covtype and ijcnn  where     denote
the best choices  In Table   the optimal learning rates of
SARAH vary less among different datasets compared to all
the other methods and they approximate the theoretical upper bound for SARAH     on the contrary  for the other
methods the empirical optimal rates can exceed their theoretical limits  SVRG with     SAG with    
FISTA with     This empirical studies suggest that it
is much easier to tune and  nd the ideal learning rate for
SARAH  As observed in Figure   the behaviors of both
SARAH and SVRG are quite sensitive to the choices of   
With improper choices of    the loss residuals can be increased considerably from   to   on both covtype
in   effective passes and ijcnn  in   effective passes for

Figure   Comparisons of loss residuals               for different inner loop sizes with SVRG  top  and SARAH  bottom  on
covtype and ijcnn 

SARAH SVRG 

  Conclusion
We propose   new variance reducing stochastic recursive gradient algorithm SARAH  which combines some of
the properties of well known existing algorithms  such as
SAGA and SVRG  For smooth convex functions  we show
  sublinear convergence rate  while for strongly convex
cases  we prove the linear convergence rate and the computational complexity as those of SVRG and SAG  However 
compared to SVRG  SARAH   convergence rate constant
is smaller and the algorithms is more stable both theoretically and numerically  Additionally  we prove the linear
convergence for inner loops of SARAH which support the
claim of stability  Based on this convergence we derive  
practical version of SARAH  with   simple stopping criterion for the inner loops 

Number of Effective Passes              covtypeSARAH SARAHSVRGSAGSGD FISTANumber of Effective Passes              ijcnn SARAH SARAHSVRGSAGSGD FISTANumber of Effective Passes              news SARAH SARAHSVRGSAGSGD FISTANumber of Effective Passes              rcv SARAH SARAHSVRGSAGSGD FISTANumber of Effective Passes Test Error Rate covtypeSARAH SARAHSVRGSAGSGD FISTANumber of Effective Passes Test Error Rate ijcnn SARAH SARAHSVRGSAGSGD FISTANumber of Effective Passes Test Error Rate news SARAH SARAHSVRGSAGSGD FISTANumber of Effective Passes Test Error Rate rcv SARAH SARAHSVRGSAGSGD FISTANumber of Effective Passes              SVRG covtype               Number of Effective Passes              SVRG ijcnn               Number of Effective Passes              SARAH covtype               Number of Effective Passes              SARAH ijcnn               SARAH    Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient

Mairal  Julien  Optimization with  rstorder surrogate

functions  In ICML  pp     

Mokhtari  Aryan    urb uzbalaban  Mert  and Ribeiro  Alejandro 
  double incremental aggregated gradient
method with linear convergence rate for largescale optimization  Proceedings of IEEE International Conference on Acoustic  Speech and Signal Processing  to appear   

Nesterov  Yurii  Introductory lectures on convex optimization     basic course  Applied optimization  Kluwer
Academic Publ  Boston  Dordrecht  London   
ISBN  

Reddi  Sashank    Hefny  Ahmed  Sra  Suvrit    oczos 
Barnab as  and Smola  Alexander    Stochastic variance
In ICML  pp 
reduction for nonconvex optimization 
   

Robbins  Herbert and Monro  Sutton    stochastic approximation method  The Annals of Mathematical Statistics 
   

Schmidt  Mark  Le Roux  Nicolas  and Bach  Francis  Minimizing  nite sums with the stochastic average gradient 
Mathematical Programming  pp     

ShalevShwartz  Shai and Zhang  Tong  Stochastic dual
coordinate ascent methods for regularized loss  Journal
of Machine Learning Research     

ShalevShwartz  Shai  Singer  Yoram  and Srebro  Nathan 
Pegasos  Primal estimated subgradient solver for SVM 
In ICML  pp     

ShalevShwartz  Shai  Singer  Yoram  Srebro  Nathan  and
Cotter  Andrew  Pegasos  Primal estimated subgradient
solver for SVM  Mathematical Programming   
   

Tak      Martin  Bijral  Avleen Singh  Richt arik  Peter  and
Srebro  Nathan  Minibatch primal and dual methods for
SVMs  In ICML  pp     

Xiao  Lin and Zhang  Tong    proximal stochastic gradient method with progressive variance reduction  SIAM
Journal on Optimization     

Acknowledgements
The authors would like to thank the reviewers for useful
suggestions which helped to improve the exposition in the
paper 

References
AllenZhu  Zeyuan  Katyusha  The First Direct Acceleration of Stochastic Gradient Methods  Proceedings of the
 th Annual ACM on Symposium on Theory of Computing  to appear   

AllenZhu  Zeyuan and Yuan  Yang 

Improved SVRG
for NonStrongly Convex or Sumof NonConvex Objectives  In ICML  pp     

Beck  Amir and Teboulle  Marc    fast iterative shrinkageinverse problems 

thresholding algorithm for
SIAM    Imaging Sciences     

linear

Bottou    eon  Online learning and stochastic approximations  In Saad  David  ed  Online Learning in Neural
Networks  pp    Cambridge University Press  New
York  NY  USA    ISBN  

Bottou    eon  Curtis  Frank    and Nocedal  Jorge  Optimization methods for largescale machine learning 
arXiv   

Cotter  Andrew  Shamir  Ohad  Srebro  Nati  and Sridharan 
Karthik  Better minibatch algorithms via accelerated
gradient methods  In NIPS  pp     

Defazio  Aaron  Bach  Francis  and LacosteJulien  Simon 
SAGA    fast incremental gradient method with support
for nonstrongly convex composite objectives  In NIPS 
pp     

Hastie  Trevor  Tibshirani  Robert  and Friedman  Jerome 
The Elements of Statistical Learning  Data Mining  Inference  and Prediction  Springer Series in Statistics 
 nd edition   

Johnson  Rie and Zhang  Tong  Accelerating stochastic
gradient descent using predictive variance reduction  In
NIPS  pp     

Kone cn    Jakub  Liu  Jie  Richt arik  Peter  and Tak      Martin  Minibatch semistochastic gradient descent in the
IEEE Journal of Selected Topics in
proximal setting 
Signal Processing     

Kone cn    Jakub and Richt arik  Peter  Semistochastic gra 

dient descent methods  arXiv   

Le Roux  Nicolas  Schmidt  Mark  and Bach  Francis   
stochastic gradient method with an exponential converIn NIPS  pp   
gence rate for  nite training sets 
   

