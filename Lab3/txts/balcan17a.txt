Differentially Private Clustering in HighDimensional Euclidean Spaces

MariaFlorina Balcan   Travis Dick   Yingyu Liang   Wenlong Mou   Hongyang Zhang  

Abstract

We study the problem of clustering sensitive
data while preserving the privacy of individuals represented in the dataset  which has broad
applications in practical machine learning and
data analysis tasks  Although the problem has
been widely studied in the context of lowdimensional  discrete spaces  much remains unknown concerning private clustering in highdimensional Euclidean spaces Rd 
In this
work  we give differentially private and ef 
cient algorithms achieving strong guarantees for
kmeans and kmedian clustering when    
 polylog    Our algorithm achieves clustering loss at most log   OPT   poly log         
advancing the stateof theart result of
dOPT 
poly log    dd  kd  We also study the case where
the data points are ssparse and show that the
clustering loss can scale logarithmically with   
     log   OPT   poly log    log          Experiments on both synthetic and real datasets verify the effectiveness of the proposed method 

 

  Introduction
In this work  we consider the problem of clustering sensitive data while preserving the privacy of individuals represented in the dataset  In particular  we consider kmeans
and kmedian clustering under the constraint of differential privacy  which is   popular informationtheoretic notion of privacy  Dwork et al      that roughly requires the output of algorithm to be insensitive to changes
in an individual   data  Clustering is an important building block for many data processing tasks with applications in recommendation systems  McSherry   Mironov 
  database systems  Ester et al    image processing  Zhang et al      Pappas    and data mining  Berkhin    Improved privacypreserving clustering algorithms have the potential to signi cantly improve

 Carnegie Mellon University  Pittsburgh  PA  USA  Princeton
University  Princeton  NJ  USA  Peking University  Beijing  China  Correspondence to  Wenlong Mou  mouwenlong pku edu cn 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

the quality of many different areas of private data analysis 
Formally  we consider the following problem  Given   set
of points            xn in Rd  privately  nd   set of   centers
           zk in Rd that approximately minimize one of the
following clustering objectives 

  cid 
 cid 

 cid xi   zj cid 

 

 

min

 

 cid cid 

  
kmedian objective

 cid 

  cid 
 cid 

 cid xi   zj cid 

or

min

 

 cid cid 

  
kmeans objective

 cid 

Minimizing these objectives exactly is NPhard  Dasgupta 
  so we instead seek to  nd approximate solutions
whose clustering objective is at most     OPT     where
OPT denotes the optimal objective  Unlike the nonprivate
setting  any differentially private clustering algorithm must
have        Bassily et al   
Despite   large amount of work on private clustering  many
fundamental problems remain open  One of the longstanding challenges is to design polynomialtime private
kmeans and kmedian clustering algorithms with small
clustering loss for the highdimensional  bigdata setting 
There is signi cant evidence to indicate that even without
the requirement on privacy  exact optimization of these objective function in Euclidean spaces might be computationally infeasible  Dasgupta    Aloise et al    The
bestknown result in this line of research is         OPT
by the local search algorithm  Kanungo et al    When
the space is discrete  there exists   private algorithm that
provides slightly better loss guarantee     OPT  Gupta
et al    However 
this problem becomes notoriously hard in the context of differential privacy in Euclidean spaces  as one typically needs to preserve privacy
for each point in the much larger Euclidean space  While
there exist algorithms in this setting with clustering loss
polylog      OPT        log      Nock et al    or
 
    OPT   poly dd  kd  log    when   is   small constant  Feldman et al     See Table   the problem is
left unresolved in the bigdata  large    highdimensional
      polylogn  scenarios if we require both   and   to
be as small as polylog    Note that running the algorithm
of Feldman et al    after projecting to   log    dimensional space gives   log   log    error  and the algorithm
of Nock et al    has       additive loss  Moreover 
bruteforce discretization in   log    dimensional space

Differentially Private Clustering in HighDimensional Euclidean Spaces

Table   Comparisons of our clustering loss with the best prior ef cient algorithms  We mark algorithms with extra assumption requirements on datasets with  

Reference

Loss

Space

Private

Guarantee

Kanungo et al   
Nissim et al   
Gupta et al   
Nock et al   
Feldman et al   

Ours

Euclidean
kmeans
Euclidean
kmeans
Discrete
kmedian
kmeans
Euclidean
kmedian Euclidean

Both

Euclidean

 
 
 
 
 

 

does not give   polynomial time algorithm 
Given the dif culty of the general form of private clustering  many positive results in this line of research have
focused on the assumptions that the data points are wellseparated    set of data points are called wellseparated
if all nearoptimal clusterings of data induce similar data
partitions  We can exploit such   structural assumption in
many aspects  For example  these wellseparated datasets
are amenable to slightly perturbed initialization  Ostrovsky
et al    This is the main insight behind prior work
for private kmeans clustering  Nissim et al    Wang
et al    However  these observations and techniques
break down in the general case 
Another interesting setting is when the data is extremely
high dimensional  but each example is ssparse 
In this
case  we might hope to improve the additive error   to
be as small as poly log       log       When the entries of
data points are dense  the dependence of     poly    is
in general inevitable even if the number of centers is   according to the lower bound for private empirical risk minimization  Bassily et al    While some prior works
have explored the possibility of nonprivate clustering in
the context of sparse data  Barger   Feldman    much
remains unknown in the private setting 
  Our Contributions
Our work tackles the problem of private clustering in
highdimensional Euclidean spaces  speci cally in the case
when we have plentiful highdimensional data  We advance
the stateof theart in several aspects 
  We design and analyze   computationally ef cient algorithm for private kmeans clustering with clustering loss
at most log      OPT   poly log           See Corollary  
In contrast to Nock et al    and Nissim
et al    our algorithm achieves small clustering loss
without additional assumptions on data  Furthermore 
our clustering loss bound is also competitive even under
their assumptions 

  We extend our algorithm to the problem of kmedian
clustering  The clustering loss is at most log     

               
 
            
 
 

          poly    log           is the size of space

    polylog         
 

log   

       poly dd  kd  log   

   

    polylog    general      poly       log   

    polylog    ssparse      poly log       log      

OPT   poly log           See Theorem   Our guarantee advances the stateof theart results of Feldman et al 
  in the highdimensional space 

  In the case of ssparse data  we further improve the additive error term   to be at most poly log    log          for
private kmeans clustering  See Corollary   To the best
of knowledge  this is the  rst result concerning the computationally ef cient  differentially private clustering algorithm for highdimensional ssparse data 

  We propose an approach for privately constructing   candidate set of centers with approximation guarantee  See
Theorem   The candidate set can be potentially applied
to other problems and is of independent interest 

  We empirically compare our algorithm with the nonprivate kmeans  algorithm and four strong private baseilnes  Across all datasets  our algorithm is competitive
with kmeans  and signi cantly outperforms the private baselines  especially for large dimensional data 

  Our Techniques

Our algorithm has two main steps  First  we use the
JohnsonLindenstrauss  JL  transform to project the data
into   log   dimensional space and use   novel technique
to privately construct   small set of good candidate centers
in the projected space  Then we apply   discrete clustering
algorithm to privately  nd   good centers from the candidate set  Centers in the original space are recovered by
noisy averaging 
Private Candidate Set  Our algorithm uses   novel private technique that recursively subdivides lowdimensional
Euclidean spaces to construct   set of candidate points containing good centers  The algorithm proceeds in rounds 
recursively subdividing the space into multiple cubes until there are few points in each cube  Finally  the algorithm
outputs the centers of all cubes as   candidate set of centers 
In order to make the above procedure work well  we need
to achieve three goals      The output preserves privacy 
    The algorithm is computationally ef cient       the size

Differentially Private Clustering in HighDimensional Euclidean Spaces

of candidate set is polynomial in   and    and     The candidate set has high    approximation rate  namely  it
contains   subset of size   on which the clustering loss is
at most     OPT     with     small  To achieve goal
    our algorithm randomizes the decision of whether to
subdivide   cube or not  To achieve goal     we make the
probability of empty cube being further divided negligible 
and the size of candidate set is upper bounded by the size of
  simple partition tree  which is poly       For goal     it
suf ces to ensure each of the cluster centers to be captured
within distance at scale of its own radius  As   lot of data
points will be gathered around an optimal center  we can
put candidate centers at each cube containing many points
during partition  Random shift and repetition are used to
avoid the worst cases 
From Candidate Set to Private Clustering  Our private
clustering algorithm then follows the technique of local
swapping on the discrete set of candidate centers  inspired
by the work of Gupta et al    for kmedian clustering 
Their algorithm maintains   set of   centers and greedily
replaces one center with   better one from the candidate
set  However  Gupta et al     algorithm only works
for the kmedian problem where the loss obeys the triangle
inequality  To extend the analysis to the kmeans problem 
we adopt the techniques of Kanungo et al    In particular  we construct   swap pairs of points  take the average
of gains of these swap pairs  and relate it to the optimal
loss OPT  Finally  we recover the centers in the original
highdimensional space privately  Given the centers in the
projected space  the centers in the original space has low
sensitivity  We can thus take the noisy mean of   cluster to
obtain   private center for each cluster 
  Related Work
The problem of private clustering in the Euclidean spaces
was investigated by Blum et al    who proposed   private version of Lloyd iteration  which we refer to as SuLQ
kmeans  However  the algorithm suffers from the absence
of uniform guarantee on the clustering loss  Given the sensitive and nonconvex nature of clustering objective  Nissim et al    and Wang et al    applied the sampleand aggregate framework to address the problem of private
clustering by making strong assumptions on the input data 
When no assumption is made on the structure of data  Feldman et al    provided an informationtheoretic upper
bound OPT   poly       log    for the clustering loss according to the bruteforce discretization of whole space and
the exponential mechanism  However  no computationally ef cient algorithm is available in the highdimensional
Euclidean spaces with clustering loss even close to this
bound  Feldman et al    proposed an ef cient algorithm for the bicriteria approximation in the constantdimensional spaces  but their additive loss term   actu 

ally exponentially depends on    Gupta et al    designed an algorithm with   constantfactor approximation
ratio and poly    log      additive loss term for clustering
in the  nitedata space     but the algorithm does not work
in the Euclidean spaces  Recently  Nock et al    proposed   private version of the kmeans  algorithm  However  the additive loss term   therein is almost as high as
the data size   
  Preliminaries
We de ne some notation and clarify our problem setup 
Notation  We will use capital letters to represent matrices or datasets and lowercase letter to represent vectors
or single data points  For   vector         denotes the ith
entry of    We denote by      the output of an algorithm with input dataset    We will frequently use   to
indicate the dimension of input space    to indicate the dimension of space after projection  and   to indicate the
radius of input data  For vector norms  we will denote by
 cid     cid  the  cid  norm   cid     cid  the number of nonzero entries 
and  cid     cid  the maximum of absolute value among entries 
De ne                cid       cid      We denote by
       the uniform distribution in the pdimensional
cube       For any set     we denote by      the cardinality of the set  We will frequently denote the clustering loss in problem   on the centers         zk by
          zk    sample drawn from onedimensional
is
Laplace distribution with density          
denoted by Lap   
Problem Setup  We use the following de nition of differential privacy 

 cid   

 cid 

   exp

 

De nition    Differential Privacy    randomized algorithm   with output range   is  differentially private if
for any given set       and two datasets     Rd   and
           for any     Rd  we have    Pr       
     Pr                 Pr          

In this paper  we study the problem of private clustering
in highdimensional Euclidean spaces without making any
assumptions on the data structure  Formally  we de ne our
problem as follows 

Problem    Private Clustering in HighDimensional Euclidean Spaces  Suppose      polylog    is the dimension of Euclidean space  Given bounded data points
        xn   Rd as input  how can we ef ciently
output   centers         zk such that
the algorithm
is  differentially private and the clustering loss is at
most polylog      OPT   poly    log        
    In the
case of sparse data where  cid xi cid      for each    can
we improve the clustering loss to polylog      OPT  
poly log    log           

   

Differentially Private Clustering in HighDimensional Euclidean Spaces

         

Algorithm   private partition xi  
input              xn           Rp  parameters
    initial cube         xi  
output Private grid     Rp 
Initialize depth       active set of cubes         and
set      
while     log   and    cid    do

       

       cid cid 

         

for Qi     do

 cid 

 

Qi   center Qi 

Remove Qi from   
Partition Qi evenly in each dimension and obtain   
cubes      
for               do

 cid     
      cid 

 

    
  
to   with probability  
 cid   
  exp cid      
     

Add     
 
where

       

     

  exp cid       otherwise 

 cid     

  log   and      

   
 cid  log  

end for

end for
end while

  Private Candidate Set
In this section  we present an ef cient algorithm that constructs   polynomialsized candidate set of centers privately in the lowdimensional space Rp with dimension
      log    This algorithm will serve as the building block
for the private clustering  Our algorithm works by repeatedly applying   recursive discretization of the space with
random shifts  It is worth noticing that direct extension of
previous methods such as  Matou sek    lead to arbitrarily bad quality  The random shift is thus essential to our
proof  which will be further explained in Appendix 
  Private Discretization Routine
We  rst describe our subroutine of private discretization   
private recursive division procedure  We start with   cube
containing all the data points  privately decide whether to
partition the current cubes based on number of data points
they contain  and stop when there are few points in each
cube  Our algorithm is   variation of the hierarchical partitioning in  Matou sek    while setting appropriate
stopping probabilities preserves privacy for our algorithm
 See Algorithm  
To make the algorithm computationally ef cient  we need
to show that the number of candidate centers generated by
Algorithm   is as small as poly    This is based on the
fact that  by design  no empty cube is subdivided by our

Figure   Constructing candidate set of centers by Algorithm  
Here we recursively divide each active cube into multiple subcubes  Roughly    cube is called active if there are suf cient
points therein  The algorithm outputs the centroid of each active
cube as the candidate set of centers 

algorithm with high probability    cube is called active if
the algorithm will divide the cube into multiple subcubes
in the next round  We have the following theorem on the
size of candidate set 
Theorem   The set   generated by Algorithm   satis es
        log    with probability      

Though we have generated   log   candidate centers  they
are wellaligned and depend only slightly on the data  This
alignment makes it possible for us to perform composition
argument by the number of recursion instead of by number
of points  We have the following theorem on the privacy 
Theorem   Algorithm   preserves  differential privacy 

The following theorem uses tail bounds for the exponential
distribution and the union bound to upper bound the number of points in each cube not subdivided by Algorithm  
Theorem   With probability at least       in Algorithm   when   cube Qi is removed from   and its subdivided cubes are not inserted to    then we have either

 cid  or the edge length of Qi is at

 Qi          cid  log  

 

most  
   

  Private Construction of Candidate Set
We now give an algorithm that constructs   polynomialsized candidate set with  differential privacy by applying
the above procedure of private discretization as   subroutine    good candidate set should contain   potential centers with small clustering loss relative to OPT  We formalize such   criterion as follows 
De nition      Approximate Candidate Set  Given  
set of points              xn    Rp    set of points
    Rp is called an    approximate candidate set of
centers  if          zk     such that the clustering loss
  on these points is at most     OPT    

As an example  the dataset   is itself      approximate
candidate set  although is not private  One may also use an

Active cubeInactive cubeCandidate centersData pointsAlgorithm   candidate xi  
input              xn           Rp  parameters

      

Differentially Private Clustering in HighDimensional Euclidean Spaces
Algorithm   localswap xi  
input Private dataset  xi  
output Clustering centers              zk      

rameters     candidate set   

     Rp with  cid xi cid      pa 

         

   

Uniformly sample   centers        from   and form    
       log  
   
for             do

Choose  cid                        cid  with proba 
 cid 

 cid 

    cid       

    

 

 
bility in proposition to exp
where   cid                     
                         

end for
Choose              with probability in proportion
to exp

 cid 
         

 cid 

 

    

Output      

 

    log  

shown in Theorem   it doesn   stop being divided until
either there re only    
  data points within it  or it  
   Since the center of   cube Ql
edge length is less than  
   we only
can capture points within this cube with factor
need to show the partition tree is activated at   level with
edge length pr 
   
If the ball around   
  is completely contained in Ql  we ve
already capture this center with   log     factor  But actually the ball can be divided into several cubes  making it
hard to activate this cube  That   why we turn to the random shift  Using geometric arguments we can show that 
    
      Ql with constant probability  Several repetitions are then used to boost the probability of success  and
to make it uniformly hold for   centers 
Therefore  we can guarantee that each optimal cluster with

 cid  will be captured  Smaller clus 

size at least  cid    log  
clustering loss goes to the       cid   

ters can be ignored safely  as its contribution to the total
 cid 

 cid  term 

       

 

  log   

 

  From Candidate Set to Private Clustering
In this section  we develop an ef cient algorithm of private clustering based on the candidate set of centers that
we construct in the lowdimensional spaces  Technically 
our approach is   twostep procedure of private discrete
clustering in the lowdimensional space and private recovery in the original highdimensional space  In particular 
the step of private discrete clustering extends the work of
Gupta et al    to the kmeans problem on the candidate set of centers  and the step of private recovery outputs
  centers in the input space 

  Private Discrete Clustering

output Candidate center set   

Initialize      
for                  log  

  do

Sample shift vector           
Let Qv            
        private partition xi  

end for

    

     

    Qv 

   cover of      to construct an        log  
   
 
approximate candidate set with privacy  However  this
bruteforce discretization results in   set of size     
in Rp  which depends on   exponentially even if    
 log    In contrast  our following Algorithm   ef ciently
constructs an    log       kpolylog   approximate
candidate set of size polynomial in   
Since Algorithm   only sees the private data through repeated application of Algorithm   we obtain the following
privacy guarantee using standard composition theorems 
Theorem   Algorithm   preserves  differential privacy 

The remaining key argument is to show the approximation
rate of candidate set constructed by Algorithm   The randomness and repetition is critical for the algorithm  They
make it possible for us to  guess  the position of optimal
centers and avoid the worst cases  Figure   depicts how
each optimal cluster may be captured by   cube of the appropriate scale  provided that the center of the cluster is
not near the boundary of   cube  Our proof techniques are
partly inspired by random grids for near neighbor reporting  Aiger et al    and locality sensitive hashing  Andoni   Indyk    We give   short sketch of our proof
in the following  and readers may refer to the Appendix for
complete proofs 
Theorem   With probability at least       Algorithm  

outputs an cid   log           

   cid appriximate can 

  log    and

 cid     

   We say   
        

with factor    if   cid   

didate set of centers  where        
   
      log  
Proof Sketch  There exists   set of  xed but unknown
      
    
optimal centers   
   and corresponding opti 
        
      
mal clusters   
  is captured by  
    Lr 
 cid xi   ul cid  is the average loss  We
  
   
will show that any optimal center is captured with factor
  log     unless the size of corresponding cluster is too
small 
For each   

   cid       cid    where

it  using Markov Inequality   cid cid     

    we can guarantee the number of points around

 cid cid     

 cid cid   

        

    log  

Consider the tree induced by hierarchical partition  as

  log  

       

 cid 

 cid cid 

    

   

 

 

 

 

Differentially Private Clustering in HighDimensional Euclidean Spaces

In this section  we propose   differentially private kmeans
algorithm in the discrete spaces  Inspired from the previous
work on kmedian problem  Gupta et al    our algorithm builds upon the local swap heuristics for kmeans
clustering  Kanungo et al    In each round  the algorithm maintains   set greedily by replacing one point
therein with   better one outside  See Algorithm   We
 rst prove that such an algorithm is differentially private 
Theorem   Algorithm   preserves  differential privacy 

Proof  The privacy guarantee is straightforward using the
basic composition theorem over   rounds of the algorithm 
and an additional exponential mechanism that selects the
best one  It is easy to verify the sensitivity of loss increments                 is   the privacy guarantee of exponential mechanism in each round follows 

The analysis of clustering loss of Algorithm   is based on  
lower bound on the total gains of   swap pairs  Gupta et al 
  However  for the kmeans problem  the triangle inequality does not hold for the quadratic  cid  loss  To resolve
this issue  we apply the inequality relaxation techniques for
swap pairs developed by Kanungo et al    We have
the following theorem on the clustering loss 
Theorem   With probability at least       the output of
Algorithm   obeys         OPT    
 

 cid    

log      

 cid 

 

 

  Private Recovery of Centers in Original Space

We now propose Algorithm   for approximately recovering   centers in the original highdimensional space  This
algorithm is basically built on Algorithms   and   as subroutines  Algorithm   receives   set of points in the lowdimensional projected space as input  and outputs   small
set of points that contains   centers with good clustering
loss  Algorithm   privately outputs   set of clustering centers from   given candidate set 
The following parallel composition lemma  McSherry 
  guarantees that if we have an  differentially private
algorithm for recovering the center of one cluster  then we
can use it to output the centers of all   centers while still
preserving   differential privacy  This result follows from
the fact that the clusters are disjoint 
Lemma    McSherry   Let            Ck be any partition of the points            xn in Rd and suppose that     
is an   differentially private algorithm that operates on sets
of points in Rd  Outputting                Ck  also preserves   differential privacy 

Now we are ready to prove the privacy of Algorithm  
Theorem   Assume that candidate  xi  
        Algorithm   preserves  differential privacy for  xi  
  
and that given any candidate set of centers   

Algorithm   Private Clustering 
input         xn        parameters       
output Clustering centres         zk   Rd 

 

   
Set dimension       log    number of trials       log  
for           do

Sample              
         yn     

          xn 
    

Sj            argminl  cid yi   ul cid              

      cid 
    candidate cid yi  
         uk    localswap cid yi  
 cid     cid 
sj   max cid Sj    Lap cid    
 cid  
 cid      
 cid 
 cid         

    
     sj 
end for
Choose   from                  with probability in
proportion to exp

      cid 

       

xi   Lap

 cid 

     

xi Sj

 sj

 

 

 

           Algorithm   preserves  
   Then Algorithm   pre 

localswap  xi  
differential privacy for  xi  
serves  differential privacy 
Putting everything together  we have the following theorem
on the clustering loss of Algorithm   The key technique in
our proof is to convert the argument of preservation of pairwise distance in the JL Lemma to the bound on the clustering loss  This is due to   simple observation that the optimal
loss in any cluster only depends on the pairwise distances
among its data points 
Theorem   Assume that candidate  xi  
        Algorithm   outputs an    approximate candidate
    and that with probabilset with probability at least  
    localswap  xi  
           Algorithm  
ity at least  
achieves clustering loss at most cOPTC     where
OPTC is the optimal clustering centers in the candidate
set of centers    Then with probability at least      
the output of Algorithm   has kmeans clustering loss
at most    OPT      cid 
  where
 cid 
      

     cid 
for        

 cid     log   

     

 cid 

 cid 

 cid 

 

 

 

  log  

Theorem   together with Theorems   and   leads to the
following guarantees on the clustering loss of Algorithm  
Corollary   There is an  differential private algorithm
that runs in poly          time  and releases   set of centers            zk such that with probability at least    
 

 cid      log    OPT    

  cid zj  

 cid     

    log   

 cid 

  

 

  Extensions
In this section  we present
gorithms 
dimensional sparse data     private kmedian clustering 
  HighDimensional Sparse Data

two extensions of our ala  private kmeans clustering with high 

Differentially Private Clustering in HighDimensional Euclidean Spaces

Algorithm   Privately Recover Centers for Sparse Dataset 
input Private data set  xi  
     Rd with  cid xi cid   
output     Rd 

 cid xi cid       parameters     accuracy  

Compute      
Initialize                      Rd 
 
for          cid    

 cid  
   xi   Rd 
     cid 
   cid  do

exp cid    

                                Lap

end for

Sample       with probability in proportion to

 cid 

 cid    

  

 

For
the case of highdimensional sparse data where
 cid xi cid       our goal is to improve the additive loss term
  to be as small as poly       log    log    by small modi cations of Algorithm   The steps of discretization routine  construction of candidate set  and clustering in the discrete space all remain the same as in the general case  The
only difference is the step of private recovery of centers
in the highdimensional original space  In the nonsparse
setting  we simply take   noisy mean of points that belong
to cluster   and output the center for cluster    resulting in
    additive loss  However  such   procedure does not
exploit the sparse nature of data points  The challenge is
that   highdimensional vector has too many entries to hide
for differential privacy  usually resulting in large error  To
improve the clustering loss  we force the output vector to
be sparse  by choosing coordinates with large absolute values  while zeroing out others  See Algorithm   Both the
choice of nonzero coordinates and the estimation of their
values need to preserve privacy  for which we use both the
exponential and Laplacian mechanisms  By   composition
argument  we have the privacy of Algorithm  
Theorem   Algorithm   preserves  differential privacy 
The
for highdimensional sparse data  the clustering loss has logarithmic
dependence on the dimension 
Theorem   With probability at least       the out 
  OPT  

put of Algorithm   obeys cid  
 cid      log ds

    cid xi     cid     

following theorem guarantees

 cid 

that

 

 

 

 

The intuition of Theorem   is based on the following observation  If the mean is approximately sparse  we can truncate it safely with small additional loss  If not  the mean
must spread across   large set of entries  so the support of
data vectors must be very different from each other  making the variance large  In both cases  the loss of truncation
can be bounded by the variance of data points  and we can
put such   loss of truncation to the multiplicative factor 
By the privacy argument in Lemma   as well as Theorems
  and   we have the following result 

Corollary   For         xn   Rd with  cid xi cid      and
 cid xi cid       there is an  differentially private algorithm
that runs in poly          time  and releases   set of centers            zk such that with probability at least    
 

 cid      log    OPT  

 cid  sk    log  

  cid zj  

 cid 

 

log   
 

  

 

 

 cid 

 cid 

  log

An important implication of Corollary   is that private clustering for highdimensional sparse data is as easy as private
clustering in   log    dimensions  The approximation factor we can achieve in the highdimensional sparse case is
roughly the same as lowdimensional case 
  kMedian Clustering
We can also easily modify our algorithms to adapt to kmedian problem  Note that Theorem   is independent of
form of loss function  since it is based on capturing the
optimal centers  Therefore  the candidate set constructed
 
in Algorithm   guarantees an
approximation rate  Since the discrete clustering algorithm
proposed by Gupta et al    is designed for kmedian 
it only remains to develop   private recovery procedure in
the original space Rd  According to Lemma     private
 median algorithm suf ces to recover the centers  We
can achieve good approximation rate via logconcave sampling  Bassily et al   
Lemma    Error Bound for Exponential Mechanism  Bassily et al    For         xn   Rd  there is  
polynomialtime algorithm that releases   center   and preserves  differential privacy  such that with probability at
    cid xi     cid    minp  cid xi     cid   

least       we have cid  

          log  
   

     
Incorporating logconcave sampling into the step of private recovery in Algorithm   we derive   private kmedian
algorithm  The privacy guarantee follows directly from
Lemma   and the composition argument  As for the kmedian objective  the optimal clustering loss is no longer
  function of pairwise distances  Fortunately  observe that
the original dataset is    approximate candidate set for
kmedian loss  By this  we have the following guarantee 
Theorem   For kmedian problem 
there is an  
differentially private algorithms that runs in poly         
time  and releases   set of centers            zk such

that with probability at least         cid zj  

 cid   

  log   

   

  

 cid       

  log    OPT    

log   
 

 

 

 cid 

  Experiments
In this section  we present an empirical evaluation of our
proposed clustering algorithm and several strong baselines
on realworld image and synthetic datasets  We compare
against nonprivate kmeans  of Arthur   Vassilvitskii
  SuLQ kmeans of Blum et al    the sample
and aggregate clustering algorithm of Nissim et al   

Differentially Private Clustering in HighDimensional Euclidean Spaces

    Synthetic    

    CIFAR  in      

    MNIST    

    Effect of    MNIST 

Figure   Figures  ac  show the effect of   on the clustering objective and Figure     shows the effect of   on the clustering objective 

the kvariates  algorithm of Nock et al    and the
griding algorithm of Su et al    The kvariates 
algorithm can only run when   is small  and the griding algorithm has time and space complexity exponential in the
dimension  so we are only able to compare against these
two baselines with small   or small    We postpone detailed comparisons against these two algorithms to supplementary material  For all other datasets with higher dimensions and all values of    our algorithm is competitive with
nonprivate kmeans  and is always better than SuLQ and
sample and aggregate  Moreover  in agreement with our
theory  the gap between the performance of our algorithm
and the other private baselines grows drastically as the dimension of the dataset increases 
The implementation of our algorithm projects to   space of
dimension     log    rather than   log    and repeats
the candidate set construction routine only   times  Finally 
we perform   iterations of the SuLQ kmeans algorithm to
further improve the quality of the resulting centers  These
modi cations do not affect the privacy guarantee of the algorithm  but gave improved empirical performance  Our
implementation of the SuLQ kmeans algorithm runs for
  iterations and uses the Gaussian mechanism to approximate the sum of points in each cluster  since this allowed us
to add less noise  The SuLQ algorithm initializes its centers to be   randomly chosen points from the bounding box
of the data  Unless otherwise stated  we set      
Results  We  rst compared our algorithm and all baselines
on   small synthetic dataset in   dimensions with       Su
et al   griding algorithm achieves the best objective  while
sample and aggregate  SuLQ  and our method all perform
comparably  and kvariates  is an order of magnitude
worse  Details of the comparison are given in the supplementary material  The griding algorithm and kvariates 
were not able to run in the rest of our experiments 
Next  we ran the nonprivate kmeans  SuLQ kmeans 
and sample and aggregate algorithms on the following
datasets for each value of   in               more
detailed description is given in the supplementary material 

MNIST  The raw pixels of MNIST  LeCun et al    It
has    examples and   features 
CIFAR     randomly sampled examples from the CIFAR  dataset  Krizhevsky    with   features extracted from layer in   of   Google Inception  Szegedy
et al    network 
Synthetic    synthetic dataset of    samples drawn from
  mixture of   Gaussians in   
Figure    ac  shows the objective values obtained by each
algorithm averaged over   independent runs  The sample
and aggregate algorithm   results have been omitted  since
its objective values are orders of magnitude worse than the
other algorithms  Across all values of   and all datasets  our
algorithm is competitive with nonprivate kmeans  and
always outperforms SuLQ kmeans  As the dimensionality
of the datasets increases  our algorithm remains competitive with kmeans  while SuLQ becomes less competitive for large dimensions 
Finally   gure       shows the effect of the privacy parameter   on the objective values for each algorithm on MNIST
with       Our algorithm is competitive with the nonprivate kmeans algorithm even for small   while the SuLQ
algorithm objective deteriorates quickly 
  Conclusions
In this paper  we propose ef cient algorithms for  private
kmeans and kmedian clustering in Rd that achieves clus 

tering loss at most   cid log    cid  OPT   poly cid       log     
 cid 
 cid  respectively 
loss can be even smaller  namely    cid log    cid  OPT  
 cid  Results of this type advance
poly cid       log    log     

and  
We also study the scenario where the data points
are ssparse and show that
the kmeans clustering

log

 
   

 cid 

 cid 

OPT   poly cid       log     

 

 

 

the stateof theart approaches in the highdimensional Euclidean spaces  Our method of constructing candidate set
can be potentially applied to other problems  which might
be of independent interest more broadly 

   kmeans objective nonprivateoursSuLQ   kmeans objective nonprivateoursSuLQ   kmeans objective nonprivateoursSuLQ kmeans objective nonprivateoursSuLQDifferentially Private Clustering in HighDimensional Euclidean Spaces

Acknowledgments
The authors would like to thank Liwei Wang and Colin
White for helpful discussions  Parts of this work was
done when      was visiting CMU  This work was done
when      was visiting Simons Institute 
This work
was supported in part by grants NSF IIS  NSF
CCF  NSF CCF  NSF CCF   
Sloan Fellowship    Microsoft Research Fellowship  NSF
grants CCF  DMS  Simons Investigator
Award  Simons Collaboration Grant  ONRN 
  and the Chinese MOE Training Plan for TopNotch
Students in Basic Discipline 

References
Aiger  Dror  Kaplan  Haim  and Sharir  Micha  Reporting
neighbors in highdimensional euclidean space  SIAM
Journal on Computing     

Aloise  Daniel  Deshpande  Amit  Hansen  Pierre  and
Popat  Preyas  Nphardness of euclidean sumof squares
clustering  Machine learning     

Andoni  Alexandr and Indyk  Piotr  Nearoptimal hashing
algorithms for approximate nearest neighbor in high dimensions  In IEEE Symposium on Foundations of Computer Science  pp     

Arthur  David and Vassilvitskii  Sergei  kmeans  The
In ACMSIAM Sympo 

advantages of careful seeding 
sium on Discrete Algorithms  pp     

Barger  Artem and Feldman  Dan  kmeans for streaming
and distributed big sparse data  In SIAM International
Conference on Data Mining  pp     

Bassily  Raef  Smith  Adam  and Thakurta  Abhradeep 
Differentially private empirical risk minimization  Ef 
 cient algorithms and tight error bounds  arXiv preprint
arXiv   

Berkhin  Pavel    survey of clustering data mining techniques  In Grouping Multidimensional Data  pp   
 

Blum  Avrim  Dwork  Cynthia  McSherry  Frank  and Nissim  Kobbi  Practical privacy  the SuLQ framework  In
ACM SIGMODSIGACT SIGART Symposium on Principles of Database Systems  pp     

Dasgupta  Sanjoy  The hardness of kmeans clustering  Department of Computer Science and Engineering  University of California  San Diego   

Dwork  Cynthia  McSherry  Frank  Nissim  Kobbi  and
Smith  Adam  Calibrating noise to sensitivity in private
In Theory of Cryptography Conference 
data analysis 
 

Dwork  Cynthia  Roth  Aaron  et al  The algorithmic
foundations of differential privacy  Foundations and
Trends in Theoretical Computer Science   
   

Ester  Martin  Kriegel  HansPeter  Sander    org  Xu  Xiaowei  et al    densitybased algorithm for discovering
In ACM
clusters in large spatial databases with noise 
SIGKDD Conference on Knowledge Discovery and Data
Mining  volume   pp     

Feldman  Dan  Fiat  Amos  Kaplan  Haim  and Nissim 
In ACM Symposium on The 

Kobbi  Private coresets 
ory of Computing  pp     

Gupta  Anupam  Ligett  Katrina  McSherry  Frank  Roth 
Aaron  and Talwar  Kunal  Differentially private comIn ACMSIAM symposium on
binatorial optimization 
Discrete Algorithms  pp     

Kanungo  Tapas  Mount  David    Netanyahu  Nathan   
Piatko  Christine    Silverman  Ruth  and Wu  Angela   
  local search approximation algorithm for kmeans
clustering  In Annual Symposium on Computational Geometry  pp     

Krizhevsky  Alex  Learning multiple layers of features
Technical report  University of

from tiny images 
Toronto   

LeCun     Bottou     Bengio     and Haffner     Gradientbased learning applied to document recognition  In Proceedings of the IEEE   

Matou sek  Jir  On approximate geometric kclustering 
Discrete   Computational Geometry   
 

McSherry  Frank and Mironov  Ilya  Differentially private recommender systems  building privacy into the
net  In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pp     

McSherry  Frank    Privacy integrated queries  an extensible platform for privacypreserving data analysis 
In
ACM SIGMOD International Conference on Management of Data  pp     

Nissim  Kobbi  Raskhodnikova  Sofya  and Smith  Adam 
Smooth sensitivity and sampling in private data analysis 
In ACM Symposium on Theory of Computing  pp   
 

Nock  Richard  Canyasse  Rapha el  Boreli  Roksana  and
Nielsen  Frank  kvariates  more pluses in the kmeans  arXiv preprint arXiv   

Differentially Private Clustering in HighDimensional Euclidean Spaces

Ostrovsky  Rafail  Rabani  Yuval  Schulman  Leonard   
and Swamy  Chaitanya  The effectiveness of Lloydtype
methods for the kmeans problem  Journal of the ACM 
   

Pappas  Thrasyvoulos    An adaptive clustering algorithm
IEEE Transactions on Signal

for image segmentation 
Processing     

Su  Dong  Cao  Jianneng  Li  Ninghui  Bertino  Elisa  and
Jin  Hongxia  Differentially private kmeans clustering 
In Proceedings of the Sixth ACM Conference on Data
and Application Security and Privacy  CODASPY  
 

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 
IEEE Conference on
Going deeper with convolutions 
Computer Vision and Pattern Recognition   

Wang  Yining  Wang  YuXiang  and Singh  Aarti  DifIn Advances in
ferentially private subspace clustering 
Neural Information Processing Systems  pp   
 

Zhang  Hongyang  Lin  Zhouchen  Zhang  Chao  and Gao 
Junbin  Robust latent low rank representation for subspace clustering  Neurocomputing     

Zhang  Hongyang  Lin  Zhouchen  Zhang  Chao  and Gao 
Junbin  Relations among some lowrank subspace recovery models  Neural computation   

