DeletionRobust Submodular Maximization 

Data Summarization with  the Right to be Forgotten 

Baharan Mirzasoleiman   Amin Karbasi   Andreas Krause  

Abstract

How can we summarize   dynamic data stream
when elements selected for the summary can be
deleted at any time  This is an important challenge in online services  where the users generating the data may decide to exercise their right
to restrict the service provider from using  part
of  their data due to privacy concerns  Motivated by this challenge  we introduce the dynamic deletionrobust submodular maximization
problem  We develop the  rst resilient streaming
algorithm  called ROBUSTSTREAMING  with  
constant factor approximation guarantee to the
optimum solution  We evaluate the effectiveness
of our approach on several realworld applications  including summarizing   streams of geocoordinates   streams of images  and   clickstream log data  consisting of   million feature
vectors from   news recommendation task 

  Introduction
Streams of data of massive and increasing volume are generated every second  and demand fast analysis and ef cient
storage  including massive clickstreams  stock market data 
image and video streams  sensor data for environmental or
health monitoring  to name   few  To make ef cient and reliable decisions we usually need to react in realtime to the
data  However  big and fast data makes it dif cult to store 
analyze  or make predictions  Therefore  data summarization   mining and extracting useful information from large
data sets   has become   central topic in machine learning
and information retrieval 
  recent body of research on data summarization relies on
utility scoring functions that are submodular  Intuitively 
submodularity  Krause   Golovin    states that select 

 ETH Zurich  Switzerland  Yale University  New Haven 
USA  Correspondence to  Baharan Mirzasoleiman  baharanm inf ethz ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ing any given data point earlier helps more than selecting it
later  Hence  submodular functions can score both diversity
and representativeness of   subset        the entire dataset 
Thus  many problems in data summarization require maximizing submodular set functions subject to cardinality
 or more complicated hereditary constraints  Numerous examples include exemplarbased clustering  Dueck  
Frey    document  Lin   Bilmes    and corpus
summarization  Sipos et al    recommender systems
 ElArini   Guestrin    search result diversi cation
 Rakesh Agrawal    data subset selection  Wei et al 
  and social networks analysis  Kempe et al   
Classical methods  such as the celebrated greedy algorithm
 Nemhauser et al    or its accelerated versions  Mirzasoleiman et al    Badanidiyuru   Vondr ak   
require random access to the entire data  make multiple
passes  and select elements sequentially in order to produce near optimal solutions  Naturally  such solutions cannot scale to large instances  The limitations of centralized methods inspired the design of streaming algorithms
that are able to gain insights from data as it is being collected  Badanidiyuru et al    Chakrabarti   Kale 
  Chekuri et al    Mirzasoleiman et al   
While extracting useful information from big data in realtime promises many bene ts  the development of more
sophisticated methods for extracting  analyzing and using
personal information has made privacy   major public issue  Various web services rely on the collection and combination of data about individuals from   wide variety of
sources  At the same time  the ability to control the information an individual can reveal about herself in online
applications has become   growing concern 
The  right to be forgotten   with   speci   mandate for
protection in the European Data Protection Regulation
  and concrete guidelines released in   allows
individuals to claim the ownership of their personal information and gives them the authority to their online activities  videos  photos  tweets  etc  As an example  consider   road traf   information system that monitors traf  
speeds  travel times and incidents in real time  It combines
the massive amount of control messages available at the
cellular network with their geocoordinates in order to gen 

DeletionRobust Submodular Maximization

erate the areawide traf   information service  Some consumers  while using the service and providing data  may
not be willing to share information about speci   locations
in order to protect their own privacy  With the right to be
forgotten  an individual can have certain data deleted from
online database records so that third parties       search engines  can no longer trace them  Weber    Note that
the data could be in many forms  including    user   posts
to an online social media     visual data shared by wearable cameras       Google Glass     behavioral patterns or
feedback obtained from clicking on advertisement or news 
In this paper  we propose the  rst framework that offers instantaneous data summarization while preserving the right
of an individual to be forgotten  We cast this problem as
an instance of robust streaming submodular maximization
where the goal is to produce   concise realtime summary
in the face of data deletion requested by users  We develop
ROBUSTSTREAMING    method that for   generic streaming algorithm STREAMINGALG with approximation guarantee   ROBUSTSTREAMING outputs   robust solution 
against any   deletions from the summary at any given
time  while preserving the same approximation guarantee 
To the best of our knowledge  ROBUSTSTREAMING is the
 rst algorithm with such strong theoretical guarantees  Our
experimental results also demonstrate the effectiveness of
ROBUSTSTREAMING on several practical applications 

  Background and Related Work
Several streaming algorithms for submodular maximization have been recently developed  For monotone functions  Gomes   Krause    rst developed   multipass algorithm with   approximation guarantee subject to   cardinality constraint    using      memory  under strong assumptions on the way the data is generated 
Later  Badanidiyuru et al    proposed the  rst single pass streaming algorithm with       approximation
under   cardinality constraint  They made no assumptions on the order of receiving data points  and only require     log    memory  Following the same line of inquiry  Chakrabarti   Kale   developed   single pass
algorithm with    approximation guarantee for handling
more general constraints such as intersections of   matroids  The required memory is unbounded and increases
polylogarithmically with the size of the data  For general
submodular functions  Chekuri et al    presented  
randomized algorithm subject to   broader range of constraints  namely pmatchoids  Their method gives      
      approximation using     log    memory
   is the size of the largest feasible solution  Very recently 
Mirzasoleiman et al    introduced             
      approximation algorithm under   psystem and  
knapsack constraints  using   pk log    memory 

An important requirement  which frequently arises in practice 
is robustness  Krause et al    proposed the
problem of robust submodular observation selection  where
we want to solve max     mini cid  fi    for normalized monotonic fi  Submodular maximization of   robust
against   deletions can be cast as an instance of the above
problem  max     min             The running time 
however  will be exponential in    Recently  Orlin et al 
  developed an algorithm with an asymptotic guar 
 
antee   for deletionrobust submodular maximization
   deletions  The results can be imunder up to       
proved for only   or   deletions 
The aforementioned approaches aim to construct solutions
that are robust against deletions in   batch mode way  without being able to update the solution set after each deletion 
To the best of our knowledge  this is the  rst to address the
general deletionrobust submodular maximization problem
in the streaming setting  We also highlight the fact that our
method does not require    the number of deletions  to be
bounded by    the size of the largest feasible solution 
Very recently  submodular optimization over sliding windows has been considered  where we want to maintain  
solution that considers only the last   items  Epasto et al 
  Jiecao et al    This is in contrast to our setting 
where the guarantee is with respect to all the elements received from the stream  except those that have been deleted 
The sliding window model can be easily incorporated into
our solution to get   robust sliding window streaming algorithm with the possibility of   deletions in the window 

  DeletionRobust Model
We review the static submodular data summarization problem  We then formalize   novel dynamic variant  and constraints on time and memory that algorithms need to obey 

  Static Submodular Data Summarization

In static data summarization  we have   large but  xed
dataset   of size    and we are interested in  nding  
summary that best represents the data  The representativeness of   subset is de ned based on   utility function             where for any       the function
      quanti es how well   represents     We de ne the
marginal gain of adding an element       to   summary
      by                            In many data
summarization applications  the utility function   satis es
submodularity       for all           and           

             

Many data summarization applications can be cast as an
instance of   constrained submodular maximization 

OPT   max

         

 

DeletionRobust Submodular Maximization

where        is   given family of feasible solutions 
We will denote by    the optimal solution           
arg maxA           common type of constraint is   cardinality constraint                                  
Finding    even under cardinality constraint is NPhard 
for many classes of submodular functions  Feige   
However    seminal result by Nemhauser et al    states
that for   nonnegative and monotone submodular function
  simple greedy algorithm that starts with the empty set
       and at each iteration augments the solution with the
element with highest marginal gain  obtains         approximation to the optimum solution  For small  static data 
the centralized greedy algorithm or its accelerated variants
produce nearoptimal solutions  However  such methods
fail to scale to truly large problems 

  Dynamic Data  Additions and Deletions

In dynamic deletionrobust submodular maximization
problem  the data   is generated at   fast pace and in realtime  such that at any point   in time    subset Vt     of
the data has arrived  Naturally  we assume that          
    Vn  with no assumption made on the order or the size
of the datastream  Importantly  we allow data to be deleted
dynamically as well  We use Dt to refer to data deleted by
time    where again               Dn  Without loss
of generality  below we assume that at every time step   exactly one element et     is either added or deleted      
 Dt   Dt     Vt   Vt      We now seek to solve  
dynamic variant of Problem  
   At       It                    Vt  Dt 
OPTt   max
At It
 
Note that in general   feasible solution at time   might not
be   feasible solution at   later time   cid  This is particularly
important in practical situations where   subset of the elements Dt should be removed from the solution  We do not
make any assumptions on the order or the size of the data
stream     but we assume that the total number of deletions
is limited to           Dn      

  Dealing with Limited Time and Memory

In principle  we could solve Problem   by repeatedly   at
every time     solving   static Problem   by restricting the
ground set   to Vt   Dt  This is impractical even for moderate problem sizes  For large problems  we may not even
be able to    Vt into the main memory of the computing
device  space constraints  Moreover  in realtime applications  one needs to make decisions in   timely manner
while the data is continuously arriving  time constraints 
We hence focus on streaming algorithms which may maintain   limited memory Mt   Vt   Dt  and must have an
updated feasible solution  At   At   Mt  At   It  to output at any given time    Ideally  the capacity of the memory

 Mt  should not depend on   and Vt  Whenever   new element is received  the algorithm can choose   to insert it
into its memory  provided that the memory does not exceed   prespeci ed capacity bound    to replace it with
one or   subset of elements in the memory  in the preemptive model  or otherwise   the element gets discarded and
cannot be used later by the algorithm  If the algorithm receives   deletion request for   subset Dt   Vt at time  
 in which case It will be updated to accommodate this request  it has to drop Dt from Mt in addition to updating
At to make sure that the current solution is feasible  all
    Vt that contain an element from Dt are infeasubsets   cid 
    It  To account for such losses  the streamsible         cid 
ing algorithm can only use other elements maintained in its
memory in order to produce   feasible candidate solution 
     At   Mt    Vt   Vt    Mt    Dt  We say that
the streaming algorithm is robust against   deletions  if it
can provide   feasible solution At   It at any given time  
such that    At      OPTt for some constant       Later 
we show how robust streaming algorithms can be obtained
by carefully increasing the memory and running multiple
instances of existing streaming methods simultaneously 

  Example Applications
We now discuss three concrete applications  with their
submodular objective functions    where the size of the
datasets and the nature of the problem often require  
deletionrobust streaming solution 

  Summarizing Clickstream and Geolocation Data

There exists   tremendous opportunity of harnessing prevalent activity logs and sensing resources  For instance  GPS
traces of mobile phones can be used by road traf   information systems  such as Google traf    Traf cSense  Navigon  to monitor travel times and incidents in real time  In
another example  stream of user activity logs is recorded
while users click on various parts of   webpage such as ads
and news while browsing the web  or using social media 
Continuously sharing all collected data is problematic for
several reasons  First  memory and communication constraints may limit the amount of data that can be stored and
transmitted across the network  Second  reasonable privacy
concerns may prohibit continuous tracking of users 
In many such applications  the data can be described in
terms of   kernel matrix   which encodes the similarity
between different data elements  The goal is to select  
small subset  active set  of elements while maintaining  
certain diversity  Very often  the utility function boils down
to the following monotone submodular function  Krause  
Golovin    where       and KS   is the principal submatrix of   indexed by the set   

        log det      KS   

 

DeletionRobust Submodular Maximization

In light of privacy concerns  it is natural to consider participatory models that empower users to decide what portion
of their data could be made available  If   user decides not
to share  or to revoke information about parts of their activity  the monitoring system should be able to update the
summary to comply with users  preferences  Therefore  we
use ROBUSTSTREAMING to identify   robust set of the  
most informative data points by maximizing Eq   

  Summarizing Image Collections

Given   collection of images  one might be interested
in  nding   subset that best summarizes and represents
the collection  This problem has recently been addressed
via submodular maximization  More concretely  Tschiatschek et al    designed several submodular objectives            fl  which quantify different characteristics
that good summaries should have       being representative        commonly reoccurring motives  Each function
either captures coverage  including facility location  sumcoverage  and truncated graph cut  or rewards diversity
 such as clustered facility location  and clustered diversity 
Then  they optimize   weighted combination of such functions

wifi   

fw     

 
where weights are nonnegative       wi     and learned
via   largemargin structured prediction  We use their
learned mixtures of submodular functions in our image
summarization experiments  Now  consider   situation
where   user wants to summarize   large collection of her
photos  If she decides to delete some of the selected photos in the summary  she should be able to update the result without processing the whole collection from scratch 
ROBUSTSTREAMING can be used as an appealing method 

  cid 

  

  RobustStreaming Algorithm
In this section  we  rst elaborate on why naively increasing the solution size does not help  Then  we present our
main algorithm  ROBUSTSTREAMING  for deletionrobust
streaming submodular maximization  Our approach builds
on the following key ideas    simultaneously constructing nonoverlapping solutions  and   appropriately merging solutions upon deleting an element from the memory 

  Increasing the Solution Size Does Not Help

One of the main challenges in designing streaming solutions is to immediately discover whether an element received from the data stream at time   is good enough to be
added to the memory Mt  This decision is usually made
based on the added value or marginal gain of the new element which in turn depends on the previously chosen elements in the memory       Mt  Now  let us consider

the opposite scenario when an element   should be deleted
from the memory at time    Since now we have   smaller
context  submodularity guarantees that the marginal gains
of the elements added to the memory after   was added 
could have only increased if   was not part of the stream
 diminishing returns  Hence  if some elements had large
marginal values to be included in the memory before the
deletion  they still do after the deletion  Based on this intuition    natural idea is to keep   solution of   bigger size 
say      rather than    for at most   deletions  However 
this idea does not work as shown by the following example 

Bad Example  Coverage  Consider   collection of  
subsets                 Bn  where Bi                and  
coverage function              Bi          Suppose we
receive                   and then Bi       for        
from the stream  Streaming algorithms that select elements
according to their marginal gain and are allowed to pick
      elements  will only pick up    upon encounter  as
other elements provide no gain  and return An       after processing the stream  Hence  if    is deleted after the
stream is received  these algorithms return the empty set
An      with    An      An optimal algorithm which
knows that element    will be deleted  however  will return
set An               Bk  with value    An         
Hence  standard streaming algorithms fail arbitrarily badly
even under   single deletion             even when we
allow them to pick sets larger than   
In the following we show how we can solve the above issue
by carefully constructing not one but multiple solutions 

  Building Multiple Solutions

As stated earlier  the existing onepass streaming algorithms for submodular maximization work by identifying
elements with marginal gains above   carefully chosen
threshold  This ensures that any element received from the
stream which is fairly similar to the elements of the solution set is discarded by the algorithm  Since elements are
chosen as diverse as possible  the solution may suffer dramatically in case of   deletion 
One simple idea is to try to  nd    near  duplicates for each
element   in the memory        nd   cid  such that      cid         
and    cid         Orlin et al    This way if we face  
deletions we can still  nd   good solution  The drawback
is that even one duplicate may not exist in the data stream
 see the bad example above  and we may not be able to
recover for the deleted element  Instead  what we will do
is to construct nonoverlapping solutions such that once we
experience   deletion  only one solution gets affected 
In order to be robust against   deletions  we run   cascading chain of   instances of STREAMINGALGs as follows  Let Mt      
denote the con 

               

     

 

 

 

DeletionRobust Submodular Maximization

 

Figure   ROBUSTSTREAMING uses   instances of  
generic STREAMINGALG to construct   nonoverlapping
     
memories at any given time            
               
 
 
Each instance produces   solution     
and the solution returned by ROBUSTSTREAMING is the  rst valid solution
St      

     min             

 cid null 

 

 

 

 

 

tent of their memories at time    When we receive  
new element     Vt from the data stream at time    we
pass it to the  rst instance of STREAMINGALG 
If
STREAMINGALG  discards    the discarded element is
cascaded in the chain and is passed to its successive algorithm       STREAMINGALG 
If   is discarded by
STREAMINGALG  the cascade continues and   is passed
to STREAMINGALG  This process continues until either
  is accepted by one of the instances or discarded for good 
Now  let us consider the case where   is accepted by the
ith instance  SIEVESTREAMING    in the chain  As discussed in Section   STREAMINGALG may choose to discard   set of points     
from its memory before
inserting              
  Note that     
 
is empty  if   is inserted and no element is discarded from
     
  we start   new
cascade from       th instance  STREAMINGALG    
Note that in the worst case  every element of the stream can
go once through the whole chain during the execution of the
algorithm  and thus the processing time for each element
scales linearly by    An important observation is that at any
given time    all the memories    
contain disjoint sets of elements  Next  we show how this data
structure leads to   deletionrobust streaming algorithm 

              
  For every discarded element         

         

         

        

     

 

 

 

 

 

 

 

  Dealing with Deletions

         

Equipped with the above data structure shown in Fig   
we now demonstrate how deletions can be treated  Assume an element ed is being deleted from the memory
of the jth instance of STREAMINGALG    at time        
   ed  As discussed in Section   the
     
solution of the streaming algorithm can suffer dramatically
from   deletion  and we may not be able to restore the quality of the solution by substituting similar elements  Since
there is no guarantee for the quality of the solution after
  deletion  we remove STREAMINGALG     from the chain
by making     
   null and for all the remaining elements in

 

 

 

 

  namely      

         

   ed  we start  
its memory      
new cascade from   th instance  STREAMINGALG   
The key reason why the above algorithm works is that the
guarantee provided by the streaming algorithm is independent of the order of receiving the data elements  Note that
at any point in time  the  rst instance   of the algorithm
 cid  null has processed all the elements from the
with      
stream Vt  not necessarily in the order the stream is originally received  except the ones deleted by time         Dt 
Therefore  we can guarantee that STREAMINGALG     provides us with its inherent  approximation guarantee for
reading Vt   Dt  More precisely         
       OPTt  where
OPTt is the optimum solution for the constrained optimization problem   when we have   deletions 
In case of adversary deletions  there will be one deletion
from the solution of   instances of STREAMINGALG in
the chain  Therefore  having           instances  we will
remain with only one STREAMINGALG that gives us the
desired result  However  as shown later in this section  if
the deletions are         which is often the case in practice 
and we have   deletions in expectation  we need   to be
much smaller than       Finally  note that we do not need
to assume that       where   is the size of the largest
feasible solution  The above idea works for arbitrary      
The pseudocode of ROBUSTSTREAMING is given in AlIt uses           instances of STREAMINgorithm  
GALG as subroutines in order to produce   solutions  We
denote by   
the solutions of the  
STREAMINGALGs at any given time    We assume that
an instance   of STREAMINGALG    receive an input element and produces   solution     
based on the input  It
may also change its memory content      
  and discard  
set     
  Among all the remained solutions       the ones
 
that are not  null  it returns the  rst solution in the chain 
     the one with the lowest index 
Theorem   Let STREAMINGALG be    pass streaming
algorithm that achieves an  approximation guarantee for
the constrained maximization problem   with an update
time of     and   memory of size   when there is no deletion  Then ROBUSTSTREAMING uses           instances of STREAMINGALGs to produce   feasible solution St   It  now It encodes deletions in addition to constraints  such that    St     OPTt as long as no more than
  elements are deleted from the data stream  Moreover 
ROBUSTSTREAMING uses   memory of size rM  and has
worst case update time of           and average update
time of   rT  

            

    

 

 

 

 

 

The proofs can be found in the appendix  In Table   we
combine the result of Theorem   with the existing streaming algorithms that satisfy our requirements 

 Data Streamdiscarded forever     RrR     MrM   SrSDeletionRobust Submodular Maximization

         

          

     
if  Dt   Dt   cid    then

Algorithm   ROBUSTSTREAMING
Input  data stream Vt  deletion set Dt        
Output  solution St at any time   
             
  while  Vt   Vt     Dt   Dt   cid    do
 
 
 
 
 
 
 
 
 
  end while

ed    Dt   Dt 
Delete ed 
et    Vt   Vt 
Add  et 

      min               

St  cid     

end if
         

else

 

 

 cid  null cid 

   STREAMINGALG      

 

      

for       do
       
 cid    and       then

     
 
if     
 
Add           
   

  function Add      
 
 
 
 
 
 
  end function

end if
end for

 

 

if          

  function Delete   
for       to   do
 
 
 
 
 
 
 
 
  end function

then
       
         
    null

    
     
Add           
   
return

end if
end for

Theorem   Assume each element of the stream is deleted
with equal probability               in expectation we
have   deletions from the stream  Then  with probability
    ROBUSTSTREAMING provides an  approximation
as long as

 cid  

log cid cid 

 cid   

     

   

Theorem   shows that for  xed      and      constant number   of STREAMINGALGs is suf cient to support     pn
 expected  deletions independently of   
In contrast  for
adversarial deletions  as analyzed in Theorem   pn    
copies of STREAMINGALG are required  which grows linearly in    Hence  the required dependence of   on   is
much milder for random than adversarial deletions  This is
also veri ed by our experiments in Section  

  Experiments
We address the following questions    How much can
ROBUSTSTREAMING recover and possibly improve the
performance of STREAMINGALG in case of deletions   
How much does the time of deletions affect the performance    To what extent does deleting representative
vs  random data points affect the performance  To this
end  we run ROBUSTSTREAMING on the applications we
described in Section   namely  image collection summarization  summarizing stream of geolocation sensor data 
as well as summarizing   clickstream of size   million 
Throughout this section we consider the following streaming algorithms  SIEVESTREAMING  Badanidiyuru et al 
  STREAMGREEDY  Gomes   Krause    and
STREAMINGGREEDY  Chekuri et al    We allow
all streaming algorithms 
including the nonpreemptive
SIEVESTREAMING  to update their solution after each
deletion  We also consider   stronger variant of SIEVESTREAMING  called EXTSIEVE  that aims to pick      elements to protect for deletions       is allowed the same
memory as ROBUSTSTREAMING  After the deletions  the
remaining solution is pruned to   elements 
To compare the effect of deleting representative elements to
the that of deleting random elements from the stream  we
use two stochastic variants of the greedy algorithm  namely 
STOCHASTICGREEDY  Mirzasoleiman et al    and
RANDOMGREEDY  Buchbinder et al    This way
we introduce randomness into the deletion process in  
principled way  Hence  we have 

STOCHASTICGREEDY  SG  Similar to the the greedy
algorithm  STOCHASTICGREEDY starts with an empty set
and adds one element at each iteration until obtains   solution of size    But in each step it  rst samples   random set
  of size       log  and then adds an element from
  to the solution which maximizes the marginal gain 

RANDOMGREEDY  RG  RANDOMGREEDY iteratively
selects   random element from the top   elements with
the highest marginal gains  until  nds   solution of size   
For each deletion method  the   data points are deleted
either while receiving the data  where the steaming algorithms have the chance to update their solutions by selecting new elements  or after receiving the data  where there
is no chance of updating the solution with new elements 
Finally  the performance of all algorithms are normalized
against the utility obtained by the centralized algorithm
that knows the set of deleted elements in advance 

  Image Collection Summarization

We  rst apply ROBUSTSTREAMING to   collection of
  images from Tschiatschek et al    We used

DeletionRobust Submodular Maximization

Algorithm

Problem

ROBUST   SIEVESTREAMING  BMKK 
ROBUST   fMSM  CK 
ROBUST   STREAMINGGREEDY  CGQ  Nonmon  Subm 
ROBUST   STREAMINGLOCAL SEARCH
 MJK 

Mon  Subm 
Mon  Subm 

Nonmon  Subm 

Constraint
Cardinality
pmatroids
pmatchoid
psystem  
  knapsack

Appr  Fact 
     

 
  

   

    

   
       

Memory

  mk log   

  mk log       

  mk log   

  mpk log    

Table   ROBUSTSTREAMING combined with  pass streaming algorithms can make them robust against   deletions 

the weighted combination of   submodular functions either capturing coverage or rewarding diversity       Section   Here  despite the small size of the dataset  computing the weighted combination of   functions makes
the function evaluation considerably expensive 
Fig     compares the performance of SIEVESTREAMING
with its robust version ROBUSTSTREAMING for       and
solution size    Here  we vary the number   of deletions
from   to   after the whole stream is received  We see
that ROBUSTSTREAMING maintains its performance by
updating the solution after deleting subsets of data points
imposed by different deletion strategies 
It can be seen
that  even for   larger number   of deletions  ROBUSTSTREAMING  run with parameter        is able to return  
solution competitive with the strong centralized benchmark
that knows the deleted elements beforehand  For the image
collection  we were not able to compare the performance of
STREAMGREEDY with its robust version due to the prohibitive running time  Fig     shows an example of an updated image summary returned by ROBUSTSTREAMING
after deleting the  rst image from the summary 

    Images

    Images
Performance of ROBUSTSTREAMING vs
Figure  
SIEVESTREAMING for different deletion strategies  SG 
RG  at the end of stream  on   collection of   images 
Here we          and          performance of ROBUSTSTREAMING and SIEVESTREAMING normalized by the
utility obtained by greedy that knows the deleted elements
beforehand     updated solution returned by ROBUSTSTREAMING after deleting the  rst images in the summary 

  Summarizing   stream of geolocation data
Next we apply ROBUSTSTREAMING to the active set selection objective described in Section   Our dataset con 

       with      

sists of   geolocations  collected during   one hour bike
ride around Zurich  Fatio    For each pair of points  
and   we used the corresponding  latitude  longitude  coordinates to calculate their distance in meters di   and chose
  Gaussian kernel Ki     exp   
Fig     shows the dataset where red and green triangles show   summary of size   found by SIEVESTREAMING  and the updated summary provided by
ROBUSTSTREAMING with       after deleting      
of the datapoints  Fig     and    compare the performance of SIEVESTREAMING with its robust version when
the data is deleted after or during the stream  respectively 
As we see  ROBUSTSTREAMING provides   solution very
close to the hindsight centralized method  Fig     and
   show similar behavior for STREAMGREEDY  Note
that deleting data points via STOCHASTICGREEDY or
RANDOMGREEDY are much more harmful on the quality of the solution provided by STREAMGREEDY  We repeated the same experiment by dividing the map into grids
of length  km  We then considered   partition matroid by
restricting the number of points selected from each grid
to be   The red and green triangles in Fig     are the
summary found by STREAMINGGREEDY and the updated
summary provided by ROBUSTSTREAMING after deleting
the shaded area in the  gure 

  Large scale click through prediction

For our largescale experiment we consider again the active
set selection objective  described in Section   We used
Yahoo  Webscope data set containing   user click
logs for news articles displayed in the Featured Tab of the
Today Module on Yahoo  Front Page during the  rst ten
days in May    Yahoo    For each visit  both the
user and shown articles are associated with   feature vector
of dimension   We take their outer product  resulting in  
feature vector of size  
The goal was to predict the user behavior for each displayed
article based on historical clicks  To do so  we considered
the  rst   of the data  for the  st   days  as our training
set  and the last    for the last   days  as our test set 
We used VowpalWabbit  Langford et al    to train  
linear classi er on the full training set  Since only   of
the data points are clicked  we assign   weight of   to each

Number of deletions Normalized objective value RobustRGRobust SGSieveSGSieve RGExtSieveRGExtSieve SGDeletionRobust Submodular Maximization

    SIEVESTREAMING  at end     STREAMGREEDY  at end

    SIEVESTREAMING  during     STREAMGREEDY  during

  

compares

the

performance

ran ROBUSTSTREAMING on each machine to  nd   summary of size    and merged the results to obtain the  
nal summary of size    We then start deleting the data uniformly at random until we left with only   of the data  and
trained another classi er on the remaining elements from
the summary 
Fig 
of ROBUSTSTREAMING for    xed active set of size        
and       with random selection  randomly selecting
equal numbers of clicked and notclicked vectors  and
using SIEVESTREAMING for selecting equal numbers of
clicked and notclicked data points  The yaxis shows the
improvement in AUC score of the classi er trained on  
summary obtained by different algorithms over random
guessing  AUC  normalized by the AUC score of the
classi er trained on the whole training data  To maximize
fairness  we let other baselines select   subset of    
elements before deletions  Fig     shows the same quantity
for      
It can be seen that   slight increase in the
amount of memory helps boosting the performance for all
the algorithms  However  ROBUSTSTREAMING bene ts
from the additional memory the most  and can almost
recover the performance of the classi er trained on the full
training data  even after   deletion 

    Cardinality constraints

    Matroid constraints

Figure   ROBUSTSTREAMING vs SIEVESTREAMING
and STREAMGREEDY for different deletion strategies
 SG  RG  on geolocation data  We          and      
   and    show the performance of robusti ed SIEVESTREAMING  whereas    and    show performance for robusti ed STREAMGREEDY     and    consider the performance after deletions at the end of the stream  while   
and    consider average performance while deletions happen during the stream     red and green triangles show   set
of size   found by SIEVESTREAMING and the updated
solution found by ROBUSTSTREAMING where   of the
points are deleted     set found by STREAMINGGREEDY 
constrained to pick at most   point per grid cell  matroid
constraint  Here       and we deleted the shaded area 

clicked vector  The AUC score of the trained classi er on
the test set was   We then used ROBUSTSTREAMING
and SIEVESTREAMING to  nd   representative subset of
size   consisting of    clicked and    notclicked examples from the training data  Due to the massive size of
the dataset  we used Spark on   cluster of   quadcore
machines with  GB of memory each  We partitioned the
training data to the machines keeping its original order  We

    Yahoo  Webscope       

    Yahoo  Webscope       

Figure   ROBUSTSTREAMING vs random unbalanced
and balanced selection and SIEVESTREAMING selecting
equal numbers of clicked and notclicked data points  on
  feature vectors from Yahoo  Webscope data 
We            and delete   of the data points 

  Conclusion
We have developed the  rst deletionrobust streaming algorithm   ROBUSTSTREAMING   for constrained submodular maximization  Given any singlepass streaming algorithm STREAMINGALG with  approximation guarantee  ROBUSTSTREAMING outputs   solution that is robust
against   deletions  The returned solution also satis es an
 approximation guarantee        to the solution of the optimum centralized algorithm that knows the set of   deletions in advance  We have demonstrated the effectiveness
of our approach through an extensive set of experiments 

Number of deletions Normalized objective value RobustRGSieve SGRobustSGExtSieve SGExtSieveRGSieve RGNumber of deletions Normalized objective value RobustSGRobust RGExtSieveSGExtSieve RGSieveRGSieve SGNumber of deletions Normalized objective value ExtSieveRGSieve SGSieveRGExtSieve SGRobustRGRobust SGNumber of deletions Normalized objective value RobustRGExtSieve RGExtSieveSGRobust SGSieveRGSieve SGRandomRandEqualExtSieveRobustNormalized AUC improvement RandomRandEqualExtSieveRobustNormalized AUC improvement DeletionRobust Submodular Maximization

Acknowledgements
This research was supported by ERC StG     Microsoft Faculty Fellowship  DARPA Young Faculty Award
   AP  SimonsBerkeley fellowship and an ETH
Fellowship  This work was done in part while Amin Karbasi  and Andreas Krause were visiting the Simons Institute
for the Theory of Computing 

References
Babaei  Mahmoudreza  Mirzasoleiman  Baharan  Jalili 
Mahdi  and Safari  Mohammad Ali  Revenue maximization in social networks through discounting  Social Network Analysis and Mining     

Badanidiyuru  Ashwinkumar and Vondr ak  Jan  Fast algorithms for maximizing submodular functions  In SODA 
 

Badanidiyuru  Ashwinkumar  Mirzasoleiman  Baharan 
Karbasi  Amin  and Krause  Andreas  Streaming submodular maximization  Massive data summarization on
the     In KDD   

Buchbinder  Niv  Feldman  Moran  Naor  Joseph Sef  and
Schwartz  Roy  Submodular maximization with cardinality constraints  In SODA   

Chakrabarti  Amit and Kale  Sagar  Submodular maximization meets streaming  Matchings  matroids  and more 
IPCO   

Chekuri  Chandra  Gupta  Shalmoli  and Quanrud  Kent 
Streaming algorithms for submodular function maximization  In ICALP   

Dueck  Delbert and Frey  Brendan    Nonmetric af nity
In

propagation for unsupervised image categorization 
ICCV   

guidelines 

http data consilium europa 
eu doc document ST INIT en 
pdf 

Jiecao  Chen  Nguyen  Huy    and Zhang  Qin  Submodular optimization over sliding windows    preprint 
https arxiv org abs 

Kempe  David  Kleinberg  Jon  and Tardos   Eva  Maximizing the spread of in uence through   social network  In
KDD   

Krause  Andreas and Golovin  Daniel 

Submodular
In Tractability  Practical
function maximization 
Approaches to Hard Problems  Cambridge University
Press   

Krause  Andreas  McMahon    Brendan  Guestrin  Carlos 
and Gupta  Anupam  Robust submodular observation
selection  Journal of Machine Learning Research   

Langford  John  Li  Lihong  and Strehl  Alex  Vowpal wab 

bit online learning project   

Lin  Hui and Bilmes  Jeff    class of submodular functions

for document summarization  In NAACL HLT   

Mirzasoleiman  Baharan  Badanidiyuru  Ashwinkumar 
Karbasi  Amin  Vondrak  Jan  and Krause  Andreas 
Lazier than lazy greedy  In AAAI   

Mirzasoleiman  Baharan  Jegelka  Stefanie  and Krause 
Andreas  Streaming nonmonotone submodular maximization  Personalized video summarization on the    
  preprint  https arxiv org abs 
 

Nemhauser  George    Wolsey  Laurence    and Fisher 
Marshall    An analysis of approximations for maximizing submodular set functions      Mathematical Programming   

ElArini  Khalid and Guestrin  Carlos  Beyond keyword
search  Discovering relevant scienti cliterature  In KDD 
 

Orlin  James    Schulz  Andreas    and Udwani  Rajan 
Robust monotone submodular function maximization 
IPCO   

Epasto  Alessandro  Lattanzi  Silvio  Vassilvitskii  Sergei 
and Zadimoghaddam  Morteza  Submodular optimization over sliding windows  In WWW   

Rakesh Agrawal  Sreenivas Gollapudi  Alan Halverson
Samuel Ieong  Diversifying search results  In WSDM 
 

Fatio  Philipe  https refind com fphilipe 

topics opendata   

Feige  Uriel    threshold of ln   for approximating set

cover  Journal of the ACM   

Gomes  Ryan and Krause  Andreas  Budgeted nonparamet 

ric learning from data streams  In ICML   

Regulation  European Data Protection  http ec 

europa eu justice dataprotection 
document review com en 
pdf   

Sipos  Ruben  Swaminathan  Adith  Shivaswamy  Pannaga 
and Joachims  Thorsten  Temporal corpus summarization using submodular word coverage  In CIKM   

DeletionRobust Submodular Maximization

Tschiatschek  Sebastian  Iyer  Rishabh    Wei  Haochen 
and Bilmes  Jeff    Learning mixtures of submodular
functions for image collection summarization  In NIPS 
 

Weber    

The right

pandora   box 
issues jipitec   

to be forgotten  More than  
https www jipitec eu 

Wei  Kai  Iyer  Rishabh  and Bilmes  Jeff  Submodularity
In ICML 

in data subset selection and active learning 
 

Yahoo  Yahoo  academic relations       yahoo  front page
today module user click log dataset  version    
URL http Webscope sandbox yahoo com 

