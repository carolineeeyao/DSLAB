Stochastic Convex Optimization 

Faster Local Growth Implies Faster Global Convergence

Yi Xu   Qihang Lin   Tianbao Yang  

Abstract

 

In this paper    new theory is developed for  rstorder stochastic convex optimization  showing
that the global convergence rate is suf ciently
quanti ed by   local growth rate of the objective function in   neighborhood of the optimal
solutions 
In particular  if the objective function       in the  sublevel set grows as fast as
 cid       cid 
  where    represents the closest
optimal solution to   and         quanti es
the local growth rate  the iteration complexity
of  rstorder stochastic optimization for achiev 

ing an  optimal solution can be  cid   

which is optimal at most up to   logarithmic factor  To achieve the faster global convergence 
we develop two different accelerated stochastic subgradient methods by iteratively solving
the original problem approximately in   local region around   historical solution with the size
of the local region gradually decreasing as the
solution approaches the optimal set  Besides
the theoretical improvements  this work also include new contributions towards making the proposed algorithms practical      we present practical variants of accelerated stochastic subgradient methods that can run without the knowledge
of multiplicative growth constant and even the
growth rate    ii  we consider   broad family
of problems in machine learning to demonstrate
that the proposed algorithms enjoy faster convergence than traditional stochastic subgradient
method  For example  when applied to the  cid 
regularized empirical polyhedral loss minimization       hinge loss  absolute loss  the proposed
stochastic methods have   logarithmic iteration
complexity 

 Department of Computer Science  The University of Iowa 
Iowa City  IA   USA  Department of Management Sciences  The University of Iowa  Iowa City  IA   USA  Correspondence to  Tianbao Yang  tianbaoyang uiowa edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Introduction
In this paper  we are interested in solving the following
stochastic optimization problem 

           cid           

min

 

where   is   random variable          is   convex function
of       is the expectation over   and   is   convex domain  We denote by            subgradient of        
Let    denote the optimal set of   and    denote the optimal value 
Traditional stochastic subgradient  SSG  method updates
the solution according to

wt       wt         wt     

 

for                   where    is   sampled value of   at tth
iteration     is   step size and         arg minv    cid    
  cid 
  is   projection operator that projects   point into   
Previous studies have shown that under the following assumptions     cid        cid       ii  there exists        
such that  cid wt     cid      for                   and by set 
 
ting the step size       
in   with   high probability
      we have
 

   cid wT           
where  cid wT    cid  
        cid log  in the worstcase 

   wt     The above convergence implies that in order to obtain an  optimal solution by
      nding     such that               
SSG 
with   high probability       one needs at least    

GB   cid log 

 cid 

 

 

 

 

 cid 

 

It is commonly known that the slow convergence of SSG
is due to the variance in the stochastic subgradient and
the nonsmoothness nature of the problem as well  which
therefore requires   decreasing step size or   very small step
size  Recently  there emerges   stream of studies on various
variance reduction techniques to accelerate stochastic gradient method  Roux et al    Zhang et al    Johnson   Zhang    Xiao   Zhang    Defazio et al 
 This holds if we assume the domain   is bounded such that
maxw      cid       cid      or if assume dist          
and project every solution wt into            

Stochastic Convex Optimization  Faster Local Growth Implies Global Convergence

  However  they all hinge on the smoothness assumption  The proposed algorithms in this work tackle the issue
of variance of stochastic subgradient without the smoothness assumption from another pespective 
The main motivation for addressing this problem is from
  key observation    high probability analysis of the SSG
method shows that the variance term of the stochastic subgradient is accompanied by an upper bound of distance of
intermediate solutions to the target solution  This observation has also been leveraged in previous analysis to design faster convergence for stochastic convex optimization
that use   strong or uniform convexity condition  Hazan  
Kale    Juditsky   Nesterov    or   global growth
condition  Ramdas   Singh    to control the distance
of intermediate solutions to the optimal solution by their
functional residuals  However  we  nd these global assumptions are completely unnecessary  which may not only
restrict their applications to   broad family of problems but
also worsen the convergence rate due to the larger multiplicative growth constant that could be domainsize dependent  In contrast  we develop   new theory only relying on
the local growth condition to control the distance of intermediate solutions to the  optimal solution by their functional residuals but achieving   fast global convergence 
Besides the fundamental difference  the present work also
possesses several unique algorithmic contributions compared with previous similar work on stochastic optimization      we have two different ways to control the distance
of intermediate solutions to the  optimal solution  one by
explicitly imposing   bounded ball constraint and another
one by implicitly regularizing the intermediate solutions 
where the later one could be more ef cient if the projection into the intersection of   bounded ball and the problem domain is complicated   ii  we develop more practical
variants that can be run without knowing the multiplicative
growth constant though under   slightly stringent condition   iii  for problems whose local growth rate is unknown
we still develop an improved convergence result of the proposed algorithms comparing with the SSG method  In addition  the present work will demonstrate the improved results and practicability of the proposed algorithms for many
problems in machine learning  which is lacking in similar
previous work 

  Related Work
The most similar work to the present one is  Ramdas  
Singh    which studied stochastic convex optimization
under   global growth condition  which they called Tsybakov noise condition  One major difference from their result is that we achieve the same order of iteration complexity up to   logarithmic factor under only   local growth condition  As observed later on  the multiplicative growth con 

stant in local growth condition is domainsize independent
that is smaller than that in global growth condition  which
could be domainsize dependent  Besides  the stochastic
optimization algorithm in  Ramdas   Singh    assume
the optimization domain   is bounded  which is removed
in this work 
In addition  they do not address the issue
when the multiplicative constant is unknown and lack study
of applicability for machine learning problems  Juditsky
  Nesterov   presented primaldual subgradient and
stochastic subgradient methods for solving problems under
the uniform convexity assumption  see the de nition under
Observation   As exhibited shortly  the uniform convexity condition covers only   smaller family of problems than
the considered local growth condition  However  when the
problem is uniform convex  the iteration complexity obtained in this work resembles that in  Juditsky   Nesterov 
 
Recently  there emerge   wave of studies that attempt to
improve the convergence of existing algorithms under no
strong convexity assumption by considering certain weaker
conditions than strong convexity  Necoara et al    Liu
et al    Zhang   Yin    Liu   Wright    Gong
  Ye    Karimi et al    Zhang    Qu et al 
  Wang   Lin    Several recent works  Necoara
et al    Karimi et al    Zhang    have uni ed
many of these conditions  implying that they are   kind of
global growth condition with       Unlike the present
work  most of these developments require certain smoothness assumption except  Qu et al   
Luo   Tseng         pioneered the idea of using local error bound condition to show faster convergence of gradient descent  proximal gradient descent  and
many other methods for   family of structured composite problems       the LASSO problem  Many followup
works  Hou et al    Zhou et al    Zhou   So 
  have considered different regularizers        cid  regularizer  nuclear norm regularizer  However  these works
only obtained asymptotically faster       linear  convergence and they hinge on the smoothness on some parts of
the problem  Yang   Lin   Xu et al    have
considered the same local growth condition  aka local error bound condition in their work  for developing faster deterministic algorithms for nonsmooth optimization  However  they did not address the problem of stochastic convex
optimization  which restricts their applicability to largescale problems in machine learning 
Finally  we note that the improved iteration complexity in
this paper does not contradict to the lower bound in  Nemirovsky        Yudin    Nesterov    The bad
examples constructed to derive the lower bound for general nonsmooth optimization do not satisfy the assumptions made in this work  in particular Assumption    

Stochastic Convex Optimization  Faster Local Growth Implies Global Convergence

      

  Preliminaries
Recall the notations    and    that denote the optimal set
of   and the optimal value  respectively  For the optimization problem in   we make the following assumption throughout the paper 
Assumption   For   stochastic optimization problem  
we assume
    there exist        and       such that        
       is   nonempty compact set 
    There exists   constant   such that  cid        cid      
Remark      essentially assumes the availability of   lower
bound of the optimal objective value  which usually holds
for machine learning problems  due to nonnegativeness
of the objective function 
    simply assumes the optimal set is closed and bounded  This is   relaxed condition
in contrast with most previous work that assume the domain   is bounded  Even if   is unbounded  as long as the
function is   proper lowersemicontinuous convex and coercive function de ned on    nite dimensional space     is
nonempty and compact  Bolte et al    Note that any
normregularized loss function minimization problem on
   nite dimensional space in machine learning satisfy this
property      is   standard assumption also made in many
previous stochastic gradientbased methods  By Jensen  
inequality  we also have  cid      cid      
For any        let    denote the closest optimal solution
in    to              arg minv     cid       cid 
  which is
unique  We denote by    the  level set of       and by   
the  sublevel set of       respectively                
                                               
Given    is bounded  it follows from  Rockafellar   
Corollary   that the sublevel set    is bounded for any
      and so as the level set    Let   
  denote the closest
point in the  sublevel set to        

 cid       cid 
 

  
    arg min
 
    
       when         using the
It is easy to show that   
KKT condition  Let                Rd    cid     cid      
denote an Euclidean ball centered at   with   radius   
Denote by dist        minv     cid       cid  the distance between   and the set    by        the projection of   onto the nonempty closed convex set            
 cid      cid    minv        cid   cid 

  Functional Local Growth Rate

We quantify the functional local growth rate by measuring
how fast the functional value increase when moving   point
away from the optimal solution in the  sublevel set 
In
particular    function       has   local growth rate    

    in the  sublevel set    cid    if there exists   constant
      such that 

       

 cid       cid 

              

 
where    is the closest solution in the optimal set   
to    Note that the local growth rate   is at most  
This is due to that       is GLipschitz continuous and
limw     cid       cid 
    if       The inequality
in   can be equivalently written as

 

 cid       cid                

       

 

where       which is called as local error bound condition in  Yang   Lin    In this work  to avoid confusion with earlier work by Luo   Tseng        
who also explored   related but different local error bound
condition  we refer to the inequality in   or   as local
growth condition  LGC  If the function       is assumed
to satisfy   for all        it is referred to as global
growth condition  GGC  Note that since we do not assume
  bounded    the GGC might be ill posed  In the following
discussions  when compared with GGC we simply assume
the domain is bounded 
Below  we present several observations mostly from existing work to clarify the relationship between the LGC  
and previous conditions  and also justify our choice of LGC
that covers   much broader family of functions than previous conditions and induces   smaller multiplicative growth
constant   than that induced by GGC 
Observation   Strong convexity or uniform convexity condition implies LGC with       but not vice versa 
      is said to satisfy   uniform convexity condition on  
with convexity parameters       and   if 
                      cid         

 cid       cid  

 

          

 

If we let               and            we have  
with              Clearly LGC covers   broader
family of functions than uniform convexity 
Observation   The weak strong convexity  Necoara et al 
  essential strong convexity  Liu et al    restricted strong convexity  Zhang   Yin    optimal
strong convexity  Liu   Wright    semistrong convexity  Gong   Ye    and other error bound conditions
considered in several recent work  Karimi et al   
Zhang    imply   GGC on the entire optimization domain   with       for   convex function 
Some of these conditions are also equivalent to the GGC
with       We refer the reader to  Necoara et al 
   Karimi et al    and  Zhang    for more
discussions of these conditions 
The third observation shows that LGC could imply faster
convergence than that induced by GGC 

Stochastic Convex Optimization  Faster Local Growth Implies Global Convergence

   

        max    

 cid     

Observation   The LGC could induce   smaller constant
  in   that is domainsize independent than that induced
by the GGC on the entire optimization domain   
To illustrate this  we consider   function            if      
  and             if              where   speci es the size
of the domain  In the  sublevel set       the LGC  
holds with       and       In order to make the inequality       cf     hold for all            we can
 
  
see that     max    
As   result  GGC induces   larger   that depends on the
domain size 
The next observation shows that LuoTseng   local error
bound condition is closely related to the LGC with    
  To this end  we  rst give the de nition of LuoTseng  
local error bound condition  Let                     
where      is   proper closed function with an open domain containing   and is continuously differentiable with
  locally Lipschitz continuous gradient on any compact set
within dom    and       is   proper closed convex function  Such   function       is said to satisfy LuoTseng  
local error bound if for any       there exists          so
that  cid     cid      cid proxP          cid  whenever
 cid proxP                cid      and               
where proxP       arg minu    
Observation   If                      is de ned above
and satis es the LuoTseng   local error bound condition 
it then implies that there exists   suf ciently small  cid     
and       such that  cid       cid                 for
any           cid 
This observation was established in  Li   Pong    Theorem   Note that the LGC condition with       cid  and
      also implies that  cid      cid              
for any           cid  Nonetheless  LuoTseng   local error bound imposes some smoothness assumption on     
The last observation is that the LGC is equivalent to  
Kurdyka    ojasiewicz inequality  KL  which was proved
in  Bolte et al    Theorem  

 cid       cid 

         

Observation   If       satis es   KL inequality      
 cid           cid      cid      for                    
       with       cs  then LGC   holds  and vice
versa 

The above KL inequality has been established for continuous semialgebraic and subanalytic functions  Attouch
et al    Bolte et al      which cover   broad
family of functions therefore justifying the generality of the
LGC 
Finally  we present   key lemma that can leverage the LGC
to control the distance of intermediate solutions to an  
optimal solution 

Lemma   For any       and       we have

 cid       

 cid    dist   

   

 

              

 

 

       is the closest point in the  sublevel set to
where   
  as de ned in  
 cid   
Remark  In view of LGC  we can see that  cid       
  for any        Yang   Lin  
              
have leveraged this relationship to improve the convergence
of the standard subgradient method  In the sequel  we will
build on this relationship to further develop novel stochastic optimization algorithms with faster convergence in high
probability 

  Main Results
In this section  we will present the proposed accelerated
stochastic subgradient  ASSG  methods and establish their
improved iteration complexity with   high probability  The
key to our development is to control the distance of intermediate solutions to the  optimal solution by their functional residuals that are decreasing as the solutions approach the optimal set  It is this decreasing factor that help
mitigate the nonvanishing variance issue in the stochastic subgradient  To formally illustrate this  we consider the
following stochastic subgradient update 

 
Lemma   Given         apply titerations of   For
any  xed                 and         with   probability at least       the following inequality holds

                                 
 cid 

 cid        cid 

 

  

 GD

 

  log   
   
 

 

 

   cid wt               
where  cid wt  cid  

 

    wt   

 

Remark  The proof of the above lemma follows similarly
as that of Lemma   in  Hazan   Kale    We note that
the last term is due to the variance of the stochastic subgradients 
In fact  due to the nonsmoothness nature of the
problem the variance of the stochastic subgradients cannot
be reduced  we therefore propose to address this issue by
reducing   in light of the inequality in Lemma  
The updates in   can be also understood as approximately
solving the original problem in the neighborhood of    In
light of this  we will also develop   regularized variant of
the proposed method  In the sequel  all omitted proofs can
be found in the supplement 

  Accelerated Stochastic Subgradient Method  the

Constrained variant  ASSGc 

In this subsection  we present the constrained variant of
ASSG that iteratively solves the original problem approx 

Stochastic Convex Optimization  Faster Local Growth Implies Global Convergence

imately in an explicitly constructed local neighborhood of
the recent historical solution  The detailed steps are presented in Algorithm   We refer to this variant as ASSGc 
The algorithm runs in stages and each stage runs   iterations
of updates similar to   Thanks to Lemma   we gradually decrease the radius Dk in   stagewise manner  The
step size keeps the same during each stage and geometrically decreases between stages  We notice that ASSGc is
similar to the EpochGD method by Hazan   Kale  
and the  multistage  ACSA method with domain shrinkage by Ghadimi   Lan   for stochastic strongly convex optimization  However  the difference between ASSG
and EpochGD ACSA lies at the initial radius    and the
number of iterations perstage  which is due to difference
between the strong convexity assumption and Lemma  
The convergence of ASSGc is presented in the theorem
below 
Theorem   Suppose Assumption   holds and       obeys
the LGC   Given         let            
 cid log   
  and   be the smallest integer
such that     max    log      
  Then ASSGc
guarantees that  with   probability          wK        
  As   result  the iteration complexity of ASSGc for
achieving an  optimal solution with   high probability       is       cid log   
   cid  log  provided
          

   cid         

 
 

 

   

Remark  It is notable that the faster local growth rate  
implies the faster global convergence       lower iteration
complexity  In light of the lower bound presented in  Ramdas   Singh    under   GGC  our iteration complexity under the LGC is optimal up to at most   logarithmic
factor  It is worth mentioning that unlike traditional highprobability analysis of SSG that usually requires the domain to be bounded  the convergence analysis of ASSG
does not rely on such   condition  Furthermore  the iteration complexity of ASSG has   better dependence on
the quality of the initial solution or the size of domain if
it is bounded  In particular  if we let     GB assuming
dist           though this is not necessary in practice 
then the iteration complexity of ASSG has only   logarithmic dependence on the distance of the initial solution to the
optimal set  while that of SSG has   quadratic dependence
on this distance  The above theorem requires   target precision   in order to set    In subsection   we alleviate
this requirement to make the algorithm more practical 

wk

   

      

    wk
 

 cid  

    wk 

          wk

           wk Dk wk

Let wk
for                     do

end for
Let wk    
 
Let           and Dk    Dk 

Algorithm   ASSGc              
  Input                  and        
 
  Set        
  for                 do
 
 
 
 
 
 
  end for
  Output  wK
present   regularized variant of ASSG  Before delving into
the details of ASSGr  we  rst present   common strategy
that solves the nonstrongly convex problem   by stochastic strongly convex optimization  The basic idea is from the
classical deterministic proximal point algorithm  Rockafellar    which adds   strongly convex regularizer to the
original problem and solve the resulting proximal problem 
In particular  we construct   new problem

 cid       cid 
 

 
 

    cid                

min

  

   

 cid cid 

     arg min
   

wt         cid 

 
where        is called the regularization reference point 

Let  cid    denote the optimal solution to the above problem
given    It is easy to know  cid       is    
 cid cid       cid 

   strongly convex
function on    We can employ the stochastic subgradient
method suited for strongly convex problems to solve the
above problem  The update is given by

 
  We present   lemma below to bound  cid cid     
     wt         wt         
   wt      and
where   cid 
      
wt cid  and  cid wt     cid  by the above update  which will be
 
Lemma   For any       we have  cid cid      wt cid      
used in the proof of convergence of ASSGr for solving  

and  cid wt     cid       
Remark  The lemma implies that the regularization term
implicitly imposes   constraint on the intermediate solutions to center around the regularization reference point 
which achieves   similar effect as the ball constraint in Algorithm  
Recall that the main iteration of the proximal point algorithm  Rockafellar    is

  Accelerated Stochastic Subgradient Method  the

Regularized variant  ASSGr 

One potential issue of ASSGc is that the projection into the
intersection of the problem domain and an Euclidean ball
might increase the computational cost periteration depending on the problem domain    To address this issue  we

wk   arg min

           

 
  

 cid     wk cid 
 

 

where wk approximately solves the minimization problem
above with    changing with    With the same idea  our

 The factor   in the step size is used for proving the high prob 

ability convergence 

Stochastic Convex Optimization  Faster Local Growth Implies Global Convergence

    wk 

Algorithm   the ASSGr algorithm for solving  
  Input                  and        
 
  for                 do
 
 
 
 
 
 
  end for
  Output  wK

Let wk
for                     do
Let   cid 
           cid 
Let wk

     cid     
 cid  

end for
Let wk    
 

    and          

 cid  wk

     

    wk

     

  wk

   

 

      wk

      
   

regularized variant of ASSG generates wk from stage   by
solving the minimization problem   approximately using   The detailed steps are presented in Algorithm  
which starts from   relatively large value of the parameter
      and gradually decreases   by   constant factor after running   number of   iterations   using the solution
from the previous stage as the new regularization reference
point  Despite of its similarity to the proximal point algorithm  ASSGr incorporates the LGC into the choices of
   and the number of iterations perstage and obtains new
iteration complexity described below 
Theorem   Suppose Assumption   holds and       obeys
the LGC   Given           let            
 cid log   
  and   be the smallest integer such that     max     log  log   log   
 
Then ASSGr guarantees that  with   probability      
   wK           As   result  the iteration complexity
of ASSGr for achieving an  optimal solution with   high
probability       is        log  log 
provided           

   cid         

 

   

  More Practical Variants of ASSG

Readers may have noticed that the presented algorithms require appropriately setting up the initial values of    or  
that depend on unknown   and potentially unknown   This
subsection is devoted to more practical variants of ASSG 
For ease of presentation  we focus on the constrained variant of ASSG 
When   is known  we present the details of   restarting variant of ASSG in Algorithm   to which we refer as RASSG 
The key idea is to use an increasing sequence of   and another level of restarting for ASSG 
Theorem    RASSG with unknown    Let      
   cid  in Algorithm   Suppose
      and      cid log   
is suf ciently large so that there exists        
  
with which     satis es   LGC   on    with        
and the constant    and   
      
  Then

and      max    log cid 

      
 

  Let    

 cid 

GD 

 

 

 

   

 

     
 

             

       
   
    and

   cid  log 

Algorithm   ASSG with Restarting  RASSG
         and        
  Input          
  Set  
  for                   do
 
 

Let       ASSGc         ts      
Let ts    ts      
      
   
 
  end for
  Output      
with at most      cid log cid      calls of ASSGc  Algorithm    nds   solution      such that             
  The total number of iterations of RASSG for obtaining  optimal solution is upper bounded by TS  
  cid log   
Remark  The above theorem requires   slightly stringent
LGC condition on    that is induced by the initial value
of    If the problem satis es the LGC with       we
can give   slightly smaller value for   in order to run Algorithm   If the target precision   is not speci ed  we can
give it   suf ciently small value  cid        the machine precision  that only affects   marginally  The corresponding
iteration complexity for achieving an  optimal solution is
given by   cid log   
 cid   cid  log  The parameter
        is introduced to increase the practical performance of RASSG  which accounts for decrease of the objective gap of the initial solutions for each call of ASSGc 
When   is unknown  we can set       and        with    
  in the LGC   where      maxw    minv     cid    
  cid  is the maximum distance between the points in the  
level set    and the optimal set    The following theorem
states the convergence result 
Theorem    RASSG with unknown   Let          
          and      cid log   
   cid  in Algorithm   Assume
is suf ciently large so that there exists        
  
rendering that   
       and
  Then with
at most      cid log cid      calls of ASSGc  Algorithm    nds   solution      such that               
  The total number of iterations of RASSG for obtaining  optimal solution is upper bounded by TS  
  cid log   
Remark  The Lemma   in the supplement shows that   
 
is   monotonically decreasing function in terms of   which
guarantees the existence of   given   suf ciently large
  
    The iteration complexity of RASSG could be still
better with   smaller factor    than the   in the iteration
complexity of SSG  see   where   is the domain size or
the distance of initial solution to the optimal set 

     max    log cid 

   cid  log 

        

  Let    

 cid 

GD 

   

    
 

 

 

 

 

 

Stochastic Convex Optimization  Faster Local Growth Implies Global Convergence

  Applications in Risk Minimization
In this section  we present some applications of the proposed ASSG to risk minimization in machine learning  Let
 xi  yi                  denote   set of pairs of feature vectors
and labels that follow   distribution    where xi       Rd
and yi      Many machine learning problems end up solving the regularized empirical risk minimization problem 

min
  Rd

       

 
 

 cid   cid xi  yi         

 

  cid 

  

where      is   regularizer    is the regularization parameter and  cid       is   loss function  Below we will present
several examples in machine learning that enjoy faster convergence by the proposed ASSG than by SSG 

  Piecewise Linear Minimization
First  we consider some examples of nonsmooth and nonstrongly convex problems such that ASSG can achieve
linear convergence 
In particular  we consider the problem   with   piecewise linear loss and  cid   cid  or  cid 
regularizers 
Piecewise linear loss includes hinge loss  generalized hinge
loss  absolute loss  and  insensitive loss  For particular
forms of these loss functions  please refer to  Yang et al 
  The epigraph of       de ned by sum of   piecewise linear loss function and an  cid   cid  or  cid  norm regularizer is   polyhedron  According to the polyhedral error
bound condition  Yang   Lin    for any       there
exists   constant           such that dist       
             for any        meaning that the proposed
ASSG has an   log  iteration complexity for solving such family of problems  Formally  we state the result
in the following corollary 
Corollary   Assume the loss function  cid       is piecewise
linear  then the problem in   with  cid   cid  or  cid  norm
regularizer satisfy the LGC in   with       Hence ASSG
can have an iteration complexity of   log  log 
with   high probability      
  Piecewise Convex Quadratic Minimization
Next  we consider some examples of piecewise quadratic
minimization problems in machine learning and show that

ASSG enjoys an iteration complexity of  cid   cid   
 cid 

give an de nition of piecewise convex quadratic functions 
which is from  Li      function      is   real
polynomial if there exists        such that       
        
       
         
    The constant   is called the degree of      continuous function
      is said to be   piecewise convex polynomial if there
exist  nitely many polyhedra            Pm with   
  Pj  
Rd such that the restriction of   on each Pj is   convex

   and        cid  

  where        and   
     

 cid  We  rst

      
            

 cid  

 

 

 

polynomial  Let Fj be the restriction of   on Pj  The degree of   piecewise convex polynomial function   is the
maximum of the degree of each Fj 
If the degree is  
the function is referred to as   piecewise convex quadratic
function  Note that   piecewise convex quadratic function
is not necessarily   smooth function nor   convex function  Li   
For examples of piecewise convex quadratic problems
in machine learning  one can consider the problem  
with   huber loss  squared hinge loss or square loss  and
 cid   cid   cid  or huber norm regularizer  Zadorozhnyi
et al    The Huber function is de ned as  cid     
  which is   piecewise convex quadratic function  The huber loss function  cid        
 cid        has been used for robust regression    Huber

 cid   
regularizer is de ned as       cid  

if        
    otherwise 

    
       

    cid wi 

It has been shown that  Li    if       is convex and
piecewise convex quadratic  then it satis es the LGC  
with       The corollary below summarizes the iteration complexity of ASSG for solving these problems 
Corollary   Assume the loss function  cid       is   convex
and piecewise convex quadratic  then the problem in  
with  cid   cid   cid  or huber norm regularizer satisfy the
LGC in   with       Hence ASSG can have an it 
  with   high probability
     
  Structured composite nonsmooth problems
Next  we present   corollary of our main result regarding
the following structured problem 

eration complexity of  cid    log 

 

       cid    Xw         

 

min
  Rd

 

can have an iteration complexity of  cid    log 

Corollary   Assume      is   strongly convex function on
any compact set and       is polyhedral  then the problem
in   satis es the LGC in   with       Hence ASSG
  with   high
probability      
 cid  
The proof of the  rst part of Corollary   can be found
in  Yang   Lin    One example of      is pnorm
    ui   yi    The
error          where         
local strong convexity of the pnorm error          is
shown in  Goebel   Rockafellar   
Finally  we give an example that satis es the LGC with
intermediate values         We can consider an  cid 
constrained  cid   norm regression  Nyquist   

 

min
 cid   cid  

       cid   
 

   cid 

      yi   

       

Liu   Yang   have shown that the problem above satis es the LGC in   with      
  

  cid 

  

Stochastic Convex Optimization  Faster Local Growth Implies Global Convergence

Figure   Comparison of different algorithms for solving different problems on different datasets 

  Experiments
In this section  we perform some experiments to demonstrate effectiveness of proposed algorithms  We use very
largescale datasets from libsvm website in experiments 
including covtype binary        realsim     
  url        for classi cation  million songs
           df          log  
       for regression  The detailed statistics of these
datasets are shown in the supplement 
We  rst compare ASSG with SSG on three tasks   cid  norm
regularized hinge loss minimization for linear classi cation   cid  norm regularized Huber loss minimization for linear regression  and  cid  norm regularized pnorm robust regression with   loss function  cid   cid xi  yi       cid xi   yi   
The regularization parameter   is set to be   in all tasks
 We also perform the experiments with       and include the results in the supplement  We set       in Huber
loss and       in robust regression  In all experiments 
we use the constrained variant of ASSG       ASSGc  For
fairness  we use the same initial solution with all zero entries for all algorithms  We use   decreasing step size proportional to  
    is the iteration index  in SSG  The
initial step size of SSG is tuned in   wide range to obtain
the fastest convergence  The step size of ASSG in the  rst
stage is also tuned around the best initial step size of SSG 
The value of    in both ASSG and RASSG is set to  
for all problems  In implementing the RASSG  we restart
every   stages with   increased by   factor of     and
  respectively for hinge loss  Huber loss and robust regression  We tune the parameter   among        
We report the results of ASSG with    xed number of iterations perstage   and RASSG with an increasing sequence
of    The results are plotted in Figure    rst    gures 
in which we plot the log difference between the objective
value and the smallest obtained objective value  to which
we refer as objective gap  versus number of iterations  The

 

 gures show that     ASSG can quickly converge to   certain level set determined implicitly by     ii  RASSG converges much faster than SSG to more accurate solutions 
 iii  RASSG can gradually decrease the objective value 
Finally  we compare RASSG with stateof art stochastic
optimization algorithms for solving    nitesum problem
with   smooth piecewise quadratic loss       squared hinge
loss  huber loss  and an  cid  norm regularization  In particular  we compare with SAGA  Defazio et al    and
SVRG   AllenZhu   Yuan    We conduct experiments on two highdimensional datasets url and   
log   and    the regularization parameter        We
also include the results for       in the supplement 
We use       in Huber loss  For RASSG  we start from
       and        then restart it every   stages with
  increased by   factor of   We tune the initial step sizes for
all algorithms in   wide range and set the values of parameters in SVRG  followed by  AllenZhu   Yuan   
We plot the objective versus the CPU time  second  in Figure    last    gures  The results show that RASSG converges faster than other three algorithms for the two tasks 
This is not surprising considering that RASSG  SAGA and

SVRG  suffer from an iteration complexity of  cid   

     and     log      respectively 

  Conclusion
In this paper  we have proposed accelerated stochastic subgradient methods for solving general nonstrongly convex
stochastic optimization under the functional local growth
condition  The proposed methods enjoy   lower iteration
complexity than vanilla stochastic subgradient method and
also   logarithmic dependence on the impact of the initial
solution  We have also made an extension by developing
  more practical variant  Applications in machine learning
have demonstrated the faster convergence of the proposed
methods 

numberofiterations log objectivegap hingeloss norm covtypeSSGASSG   RASSG   numberofiterations log objectivegap hingeloss norm realsimSSGASSG   RASSG   numberofiterations log objectivegap huberloss norm millionsongsSSGASSG   RASSG   numberofiterations log objectivegap huberloss norm     dfSSGASSG   RASSG   numberofiterations log objectivegap robust norm millionsongsSSGASSG   RASSG   numberofiterations log objectivegap robust norm     dfSSGASSG   RASSG   cputime   objective squaredhinge norm urlSSGSAGASVRG RASSGcputime   objective huberloss norm   log pSSGSAGASVRG RASSGStochastic Convex Optimization  Faster Local Growth Implies Global Convergence

Acknowledgement
We thank the anonymous reviewers for their helpful comments     Xu and    Yang are partially supported by National Science Foundation  IIS  IIS    
Yang would like to thank Lijun Zhang for pointing out
 Kakade   Tewari    for his attention 

References
AllenZhu  Zeyuan and Yuan  Yang 

Improved svrg for
nonstrongly convex or sumof nonconvex objectives 
In ICML  pp     

Attouch  Hedy  Bolte    er ome  and Svaiter  Benar Fux 
Convergence of descent methods for semialgebraic and
tame problems  proximal algorithms  forwardbackward
splitting  and regularized gaussseidel methods  Math 
Program     

Bolte    er ome  Daniilidis  Aris  and Lewis  Adrian  The
 ojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems  SIAM    on Optimization     

Bolte    er ome  Nguyen  Trong Phong  Peypouquet  Juan 
and Suter  Bruce  From error bounds to the complexity of
 rstorder descent methods for convex functions  CoRR 
abs   

Defazio  Aaron  Bach  Francis    and LacosteJulien  Simon  SAGA    fast incremental gradient method with
support for nonstrongly convex composite objectives  In
NIPS  pp     

Ghadimi  Saeed and Lan  Guanghui  Optimal stochastic
approximation algorithms for strongly convex stochastic composite optimization  ii  Shrinking procedures and
optimal algorithms  SIAM Journal on Optimization   
   

Goebel     and Rockafellar        Local strong convexity
and local lipschitz continuity of the gradient of convex
functions  Journal of Convex Analysis   

Gong  Pinghua and Ye  Jieping  Linear convergence of
variancereduced projected stochastic gradient without
strong convexity  CoRR  abs   

Hazan  Elad and Kale  Satyen  Beyond the regret minimization barrier  an optimal algorithm for stochastic
In COLT  pp   
stronglyconvex optimization 
 

Hou  Ke  Zhou  Zirui  So  Anthony ManCho  and Luo 
ZhiQuan  On the linear convergence of the proximal
gradient method for trace norm regularization  In NIPS 
pp     

Johnson  Rie and Zhang  Tong  Accelerating stochastic
gradient descent using predictive variance reduction  In
NIPS  pp     

Juditsky  Anatoli and Nesterov  Yuri  Deterministic and
stochastic primaldual subgradient algorithms for uniStoch  Syst   
formly convex minimization 
 

Kakade  Sham    and Tewari  Ambuj  On the generalization ability of online strongly convex programming
algorithms  In NIPS  pp     

Karimi  Hamed  Nutini  Julie  and Schmidt  Mark    Linear convergence of gradient and proximalgradient methods under the polyak ojasiewicz condition  In ECMLPKDD  pp     

Li  Guoyin  Global error bounds for piecewise convex

polynomials  Math  Program     

Li  Guoyin and Pong  Ting Kei  Calculus of the exponent of kurdyka   ojasiewicz inequality and its applications to linear convergence of  rstorder methods  CoRR 
abs   

Liu  Ji and Wright  Stephen    Asynchronous stochastic
coordinate descent  Parallelism and convergence properties  SIAM Journal on Optimization     

Liu  Ji  Wright  Stephen         Christopher  Bittorf  Victor  and Sridhar  Srikrishna  An asynchronous parallel
stochastic coordinate descent algorithm     Mach  Learn 
Res      ISSN  

Liu  Mingrui and Yang  Tianbao  Adaptive accelerated gradient converging methods under holderian error bound
condition  CoRR  abs   

Luo  ZhiQuan and Tseng  Paul  On the convergence
of coordinate descent method for convex differentiable
minization  Journal of Optimization Theory and Applications       

Luo  ZhiQuan and Tseng  Paul  On the linear convergence
of descent methods for convex essenially smooth minization  SIAM Journal on Control and Optimization   
     

Luo  ZhiQuan and Tseng  Paul  Error bounds and convergence analysis of feasible descent methods    general
approach  Annals of Operations Research   
 

Necoara     Nesterov  Yu  and Glineur     Linear convergence of  rst order methods for nonstrongly convex optimization  CoRR  abs   

Stochastic Convex Optimization  Faster Local Growth Implies Global Convergence

Zhang  Hui  New analysis of linear convergence of
gradienttype methods via unifying error bound conditions  CoRR  abs   

Zhang  Hui and Yin  Wotao  Gradient methods for convex minimization  better rates under weaker conditions 
CoRR  abs   

Zhang  Lijun  Mahdavi  Mehrdad  and Jin  Rong  Linear
convergence with condition number independent access
of full gradients  In NIPS  pp     

Zhou  Zirui and So  Anthony ManCho    uni ed approach
to error bounds for structured convex optimization problems  CoRR  abs   

Zhou  Zirui  Zhang  Qi  and So  Anthony ManCho    pnorm regularization  Error bounds and convergence rate
In ICML  pp   
analysis of  rstorder methods 
   

Nemirovsky      Arkadii Semenovich  and Yudin       
Problem complexity and method ef ciency in optimization  WileyInterscience series in discrete mathematics 
Wiley  Chichester  New York    ISBN  
    WileyInterscience publication 

Nesterov  Yurii  Introductory lectures on convex optimization     basic course  Applied optimization  Kluwer
Academic Publ    ISBN  

Nyquist     The optimal lp norm estimator in linear regression models  Communications in Statistics   Theory and
Methods     

Qu  Chao  Xu  Huan  and Ong  Chong Jin  Fast rate analysis of some stochastic optimization algorithms  In ICML 
pp     

Ramdas  Aaditya and Singh  Aarti  Optimal rates for
stochastic convex optimization under tsybakov noise
condition  In ICML  pp     

Rockafellar     Tyrrell  Monotone operators and the proximal point algorithm  SIAM Journal on Control and Optimization     

Rockafellar       Convex Analysis  Princeton mathematical

series  Princeton University Press   

Roux  Nicolas Le  Schmidt  Mark    and Bach  Francis 
  stochastic gradient method with an exponential convergence rate for  nite training sets  In NIPS  pp   
   

Wang  PoWei and Lin  ChihJen  Iteration complexity of
feasible descent methods for convex optimization  Journal of Machine Learning Research   
 

Xiao  Lin and Zhang  Tong    proximal stochastic gradient method with progressive variance reduction  SIAM
Journal on Optimization     

Xu  Yi  Yan  Yan  Lin  Qihang  and Yang  Tianbao  Homotopy smoothing for nonsmooth problems with lower
complexity than    In NIPS  pp     

Yang  Tianbao and Lin  Qihang 

Rsg  Beating sgd
without smoothness and or strong convexity  CoRR 
abs   

Yang  Tianbao  Mahdavi  Mehrdad  Jin  Rong  and Zhu 
Shenghuo  An ef cient primaldual prox method for
nonsmooth optimization  Machine Learning   

Zadorozhnyi  Oleksandr  Benecke  Gunthard  Mandt 
Stephan  Scheffer  Tobias  and Kloft  Marius  Hubernorm regularization for linear prediction models 
In
ECMLPKDD  pp     

