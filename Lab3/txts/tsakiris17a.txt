Hyperplane Clustering via Dual Principal Component Pursuit

Manolis    Tsakiris   Ren   Vidal  

Abstract

Stateof theart methods
for clustering data
drawn from   union of subspaces are based on
sparse and lowrank representation theory and
convex optimization algorithms  Existing results guaranteeing the correctness of such methods require the dimension of the subspaces to be
small relative to the dimension of the ambient
space  When this assumption is violated  as is 
     in the case of hyperplanes  existing methods are either computationally too intensive      
algebraic methods  or lack suf cient theoretical
support       KHyperplanes or RANSAC  In
this paper we provide theoretical and algorithmic
contributions to the problem of clustering data
from   union of hyperplanes  by extending   recent subspace learning method called Dual Principal Component Pursuit  DPCP  to the multihyperplane case  We give theoretical guarantees
under which  the nonconvex  cid  problem associated with DPCP admits   unique global minimizer equal to the normal vector of the most
dominant hyperplane 
Inspired by this insight 
we propose sequential  RANSACstyle  and iterative  KHyperplanes style  hyperplane learning DPCP algorithms  which  via experiments on
synthetic and real data  are shown to outperform
or be competitive to the stateof theart 

  Introduction
  Hypeprlane clustering

Subspace clustering  the problem of clustering data drawn
from   union of linear subspaces  is an important problem
in machine learning  pattern recognition and computer vision  Vidal et al      particular case of this problem is hyperplane clustering  which arises when the data

 Center for Imaging Science  Johns Hopkins University  Baltimore  MD  USA  Correspondence to  Manolis    Tsakiris
   tsakiris jhu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

lie in   union of hyperplanes  as in       projective motion
segmentation  Vidal et al       point cloud analysis
 Sampath   Shan    and hybrid system identi cation
 Vidal et al    Bako    Even though in some ways
hyperplane clustering is simpler than general subspace
clustering  since       the dimensions of the subspaces
are equal and known   priori  modern selfexpressiveness 
based methods  Liu et al    Lu et al    Elhamifar
  Vidal    Wang et al    You et al    in principle do not apply in this case  because they require small
relative subspace dimensions      where      are the dimensions of the subspace and ambient space  respectively 
From   theoretical point of view  one of the most appropriate methods for hyperplane clustering is Algebraic Subspace Clustering  ASC  which gives closedform solutions
by means of factorization or differentiation of polynomials  Vidal et al    However  the main drawback of
ASC is its exponential complexity  which makes it impractical in many settings  Another method that is theoretically justi able for clustering hyperplanes is Spectral Curvature Clustering  SCC   Chen   Lerman    which
computes   Dfold af nity between all Dtuples of points
in the dataset  As in the case of ASC  SCC has combinatorial complexity and becomes cumbersome for large   
On the other hand  the intuitive classical method of KHyperplanes  KH   Bradley   Mangasarian    which
alternates between assigning clusters and  tting   new normal vector to each cluster with PCA  is perhaps the most
practical method for hyperplane clustering  since it is simple to implement and it is robust to noise  However  KH
is sensitive to outliers and is guaranteed to converge only
to   local minimum  hence multiple restarts are in general required  Median KFlats  MKF   Zhang et al   
shares   similar objective function as KH  but uses the  cid 
norm instead of the  cid norm  in an attempt to gain robustness to outliers  The minimization is done via   stochastic
gradient descent scheme  and searches directly for   basis of each subspace  which makes it slower to converge
for hyperplanes  Finally  any single robust subspace learning method suitable for high relative dimensions  such as
RANSAC  Fischler   Bolles    or REAPER  Lerman
et al    can be applied either    in   sequential fashion

 The issue of robustness to noise for ASC has been recently

addressed in Tsakiris   Vidal        

Hyperplane Clustering via Dual Principal Component Pursuit

by  rst learning the most dominant hyperplane  removing
the points lying close to it  learning the second most dominant hyperplane  and so on  or ii  in an iterative fashion  by
assigning points to clusters   tting   hyperplane per cluster 
reassigning the points to new clusters and so on 

  Dual principal component pursuit

Dual Principal Component Pursuit  DPCP   Tsakiris  
Vidal          is an  cid  single subspace learning
method  which aims at recovering the orthogonal complement of the subspace in the presence of outliers  and as such
it is particularly suited for hyperplanes  DPCP searches for
the normal vector to   hyperplane by solving   nonconvex
 cid  minimization problem on the sphere  or   recursion of
linear programming relaxations  and under certain conditions  the normal to the hyperplane is the unique global solution to this nonconvex  cid  problem  as well as the limit
point of the LP recursion  Motivated by the robustness of
DPCP to outliers  one could naively use it for hyperplane
clustering by recovering the normal vector to   hyperplane
one at   time  while treating points from other hyperplanes
as outliers  However  such   scheme is not   priori guaranteed to succeed because the assumptions in the theorems
of correctness of DPCP assume that outliers are uniformly
distributed on the sphere  an assumption which is violated
when the data come from   union of hyperplanes 

  Paper contributions

In this paper we provide   theoretical analysis of the nonconvex  cid  DPCP problem for data drawn from   union
of hyperplanes  We show that as long as the hyperplanes
are suf ciently separated  the dominant hyperplane is suf 
ciently dominant and the points are uniformly distributed
 in   deterministic sense  inside their associated hyperplanes  the normal vector of the dominant hyperplane is the
unique  up to sign  global minimizer of the DPCP problem 
This suggests   DPCPbased sequential hyperplane learning algorithm  which uses DPCP to compute   dominant
hyperplane  then   second dominant hyperplane and so on 
Experiments on synthetic data show that this DPCPbased
algorithm signi cantly improves over similar sequential algorithms  which are based on RANSAC or REAPER  Finally     plane clustering experiments on real    point
clouds show that an iterative  KHstyle  DPCP scheme is
very competitive to RANSAC  which is the predominant
stateof theart method for such applications 

  Preliminaries
  Data model and the hyperplane clustering problem
Consider given   collection                 xN     RD  
of   points of the unit sphere SD  of RD  that lie in  

RD              cid  
vector bi   SD       Hi  cid     RD     cid bi    cid       

union  arrangement    of   hyperplanes           Hn of
   Hi  where each hyperplane Hi
is the set of points of RD that are orthogonal to   normal
                   We assume that the data   lie in general position in    by which we mean two things  First  we
mean that there are no linear relations among the points
other than the ones induced by their membership to the
hyperplanes  In particular  every        points coming
from Hi form   basis for Hi and any   points of   that
come from at least two distinct Hi Hi cid  are linearly independent  Second  we mean that the points   uniquely de 
 ne the hyperplane arrangement    in the sense that   is
the only arrangement of   hyperplanes that contains    
This can be veri ed computationally by checking that there
is only one up to scale homogeneous polynomial of degree
  that  ts the data  see Vidal et al    Tsakiris   Vidal
   for details  We assume that for every         precisely Ni points of     denoted by            
                
 
Ni
   Ni      With that notation 
                     where   is an unknown permutation matrix  indicating that the hyperplane membership of
the points is unknown  Moreover  we assume an ordering
              Nn  and we refer to    as the dominant hyperplane  After these preparations  the problem of
hyperplane clustering can be stated as follows  given the
data      nd the number   of hyperplanes associated to    
  normal vector to each hyperplane  and   clustering of the
       according to hyperplane membership 

belong to Hi  with  cid  

data    cid  

  Review of dual principal component pursuit

Dual Principal Component Pursuit  DPCP   Tsakiris   Vidal      is   robust single subspace learning method 
Given unlabeled data     which consist of inliers in   single subspace   of RD of dimension        together with
outliers to the subspace  DPCP computes   basis for the
orthogonal complement of the inlier subspace    The key
idea of DPCP is to identify   single hyperplane   with normal vector   that is maximal with respect to the data    
Such   maximal hyperplane is de ned by the property that
it must contain   maximal number of points NH from the
dataset       NH cid    NH for any other hyperplane   cid  of
RD  Notice that such   maximal hyperplane can be characterized as   solution to the combinatorial problem

 cid cid cid    cid  

 cid cid cid 

min

 

 cid cid cid    cid  

 cid cid cid 

        cid   

 

is the number of nonzero entries of    cid   
since
which is precisely the number of data points in   that lie
outside the hyperplane de ned by    If   is   hyperplane 
     dimS      and if there are at least dimS      
inliers  it is straightforward to show that   has   unique
up to scale global minimizer  the normal vector to the inlier

Hyperplane Clustering via Dual Principal Component Pursuit

hyperplane  Since   is hard to solve  we relax it to

 cid cid cid    cid  

 cid cid cid 

min

 

      cid   cid     

 

which is still challenging to solve  since it is   nonsmooth
nonconvex optimization problem on the sphere  Problem
  has appeared several times in the literature  Sp ath  
Watson    Spielman et al    Qu et al    Sun
In fact  Sp ath   Watson   proved the
et al   
following fascinating result 
Proposition    Sp ath   Watson    Let   be        
matrix of rank    Then any global minimizer of   must
be orthogonal to    linearly independent columns of    

Proposition   establishes an encouraging property of problem   towards recovering the normal vector of the inlier
hyperplane as its global minimizer  Indeed  it would suf 
 ce that the       linearly independent points of   that
  global minimizer is orthogonal to  be points of the inlier
hyperplane  Even though   priori it is not clear under what
conditions this is the case  Tsakiris   Vidal    provided
an answer  informally stated as follows 

Proposition    Tsakiris   Vidal      Suppose that the
inliers are suf ciently uniformly distributed  in   deterministic sense de ned in Grabner et al    inside the intersection of the inlier hyperplane and the  unit  sphere  and
that the outliers are suf ciently uniformly distributed on the
sphere  Then   has   unique up to sign global minimizer 
equal to the normal vector of the inlier hyperplane 

It was further shown in Tsakiris   Vidal    that under
the conditions of Proposition   and assuming that     is  
unit  cid norm vector suf ciently far from the inlier hyperplane  the recursion of linear programs

 cid cid cid    cid  

 cid cid cid 

nk    argmin
  cid   nk 

will converge in    nite number of iterations to the global
minimizer of  

  Hyperplane clustering via DPCP 
Given the discussion in   the DPCP problem   seems
  natural mechanism towards retrieving the normal vectors
to the hyperplanes associated with   dataset   lying in
  hyperplane arrangement 
Indeed  one may be tempted
to solve problem   for such   dataset with the hope of
obtaining   unique global minimizer  which is orthogonal
to one of the underlying hyperplanes  In this case  points

 Remarkably    was  rst proposed in Sp ath   Watson  
as   means of solving   and it was established that it converges
in    nite number of iterations to   critical point of  

coming from the remaining hyperplanes are treated as outliers  Such an idea would give rise to sequential and iterative DPCP hyperplane clustering algorithms as described
in     suf cient condition for the correctness of this
procedure would be that the global minimizer of the DPCP
problem   be orthogonal to the inlier subspace  However 
the conditions for this to be the case do not immediately
follow from the work of Tsakiris   Vidal     since in
the case of hyperplane clustering the outliers lie in   union
of       hyperplanes  and thus can not be uniformly distributed on the sphere  as the conditions of Tsakiris   Vidal
   require  The rest of the paper is devoted to providing such theoretical guarantees   as well as introducing
DPCPbased hyperplane clustering algorithms  

  Theoretical Contributions
  Theoretical analysis of the continuous problem

As it turns out  one can gain important insights about the
analysis of the DPCP problem   for data in   hyperplane
arrangement  by  rst analyzing   certain continuous problem  To see what that problem is  let  Hi   Hi  SD  and
note  rst that for any     SD  we have

 cid cid cid  

Ni cid 

  

 
Ni

 cid 

 cid cid cid   cid 

 cid 

    
 

    Hi

 

 cid 

 cid cid cid  
 cid cid cid      Hi
 cid cid cid 
 cid cid cid    cid 

   

 

 

where the LHS of   is precisely  
and can be
Ni
viewed as an approximation  cid  via the point set     of
denoting the unithe integral on the RHS of   with    Hi
form measure on  Hi  Letting    be the principal angle between   and bi       the unique angle          such
that cos        
bi  and  Hi   RD   Hi the orthogonal
 cid 
projection onto Hi  we have for any     Hi that

with hi      Hi    and  hi     hi   cid hi   cid  Hence 
 cid 

 cid cid 

 cid 

 cid cid cid  

 cid cid cid      Hi

 cid 

 

 

 cid cid cid  

 cid 
  bx

 cid cid cid      Hi

sin   

 cid 

    Hi

     SD 

sin        sin   

 cid cid 

    Hi

 

  SD 

  being the average height of the unit hemisphere of RD 
We can now view the objective function of  
 

 cid cid cid    cid 

   

 cid cid cid 

  cid 

  

  cid 

  

 

Ni

   

Ni

 cid cid cid  

Ni cid 

  

 

 

 cid cid cid 

 cid cid cid    cid  
   
 cid cid cid 

 cid 

    
 

 

 

 

 cid 
 

 cid 

     

 Hi       Hi   

 cid 
 cid 
  bx   sin    
  bx 

   

 cid 

 

 

 

Hyperplane Clustering via Dual Principal Component Pursuit

as an approximation via   of the function        

 cid cid 

  cid 

  

Ni

    Hi

 cid cid cid  

 cid 

 

 cid cid cid      Hi

 cid   

 

  cid 

  

Ni   sin   

 

In that sense  the continuous counterpart of problem   is

           sin        Nn sin   

 

min
  SD 

 

Note that sin    is the distance between the line
spanned by   and the line spanned by bi  Hence 
  of problem   minimizes
every global minimizer  
the sum of the weighted distances of Span  
  from
Span            Span bn  and thus represents   weighted
geometric median of these lines  Even though medians in
Riemmannian Grassmannian manifolds are an active subject of research  Draper et al    Ghalieh   Hajja 
  we are not aware of any literature that studies  
The advantage of working with   instead of   is that
the global minimizers of   depend solely on the weights
Ni as well as on the geometry of the arrangement  captured
by the principal angles  ij between bi and bj  In contrast 
the global minimizers of the discrete problem   will in
principle also depend on the distribution of the points    
From that perspective  understanding when problem  
has as unique solution     is essential for understanding
the potential of   for hyperplane clustering  Towards that
end  we next provide   series of results pertaining to  
The  rst con guration that we examine is that of two hyperplanes  In that case the weighted geometric median of
the two lines spanned by the normals to the hyperplanes
always corresponds to one of the two normals 

Proposition   Consider an arrangement of two hyperplanes in RD with normal vectors       and weights     
   Then the set    of global minimizers of   satis es 
  If         then           
  If         then         

When         problem   recovers the normal    to
the dominant hyperplane  irrespectively of how separated
the two hyperplanes are  since  according to Proposition
  the principal angle   between       does not play  
role  The continuous problem   is equally favorable in
recovering normal vectors as global minimizers in another
extreme situation  where the arrangement consists of up to
  perfectly separated  orthogonal  hyperplanes 
Proposition   Consider       hyperplanes in RD with
orthogonal normal vectors            bn  and weights     
         Nn  Then the set    of global minimizers of
  can be characterized as follows 

 Recall that    is   principal angle               
 All proofs can be found at Tsakiris   Vidal    

  If          Nn  then                 bn 
  If            cid      cid      Nn  for some  cid   

       then                   cid 

Propositions   and   are not hard to prove  since for two hyperplanes the objective function is strictly concave  while
for orthogonal hyperplanes it is separable  In contrast  the
problem becomes harder for       arbitrary hyperplanes 
Even when       characterizing the global minimizers of
  as   function of the geometry and the weights seems
challenging  Nevertheless  when the three hyperplanes are
equiangular and their weights are equal  the symmetry of
the con guration allows us to analytically characterize the
median as   function of the angle of the arrangement 
Proposition   Consider three hyperplanes of RD  with
  bj   cos         cid    
 cid 
normal vectors                
and              Then the set    of global minimizers
of   satis es the following phase transition 
  If       then             
  If       then             
  If       then       
where                  cid             cid 
Proposition   whose proof uses nontrivial arguments from
spherical and algebraic geometry  is particularly enlightening  since it suggests that the global minimizers of   are
associated to the normals of the underlying arrangement
when the hyperplanes are suf ciently separated  while otherwise they seem to be capturing the median hyperplane of
the arrangement  This is in striking similarity with the results regarding the Fermat point of planar and spherical triangles  Ghalieh   Hajja    However  when the symmetry in Theorem   is removed  our proof technique no
longer applies  and the problem seems even harder  Even
so  one intuitively expects an interplay between the angles
and the weights of the arrangement under which  if the hyperplanes are suf ciently separated and    is suf ciently
dominant  then   should have   unique global minimizer
equal to    Our next theorem formalizes this intuition 
Theorem   Consider       hyperplanes in RD  with
normals            bn of pairwise principal angles  ij and
weights Ni  De ne an                 matrix   with
             entry given by NiNj cos ij               
and maximum eigenvalue  max  If

    

      where

 

Ni sin     

     max      
   

 

 cid 

 cid cid 

  

 cid 

  

   

Hyperplane Clustering via Dual Principal Component Pursuit

 cid cid 
 cid 

  

    min
  cid 

  cid  

 cid 

   

     

  cid        

Ni sin ij   cid 

  

   

      and

 

Ni sin       

 

then problem   admits   unique up to sign global mini 
 
mizer  

     

Let us provide some intuition about the meaning of the
quantities     and   in Theorem   To begin with  the
 rst term in   is precisely equal to       while the second term in   can be shown to be   lower bound on the
objective function    sin        Nn sin    if one
discards hyperplane    Moving on  the quantity    
admits   nice geometric interpretation  cos      is  
lower bound on how small the principal angle of   critical
   cid      from    can be  Interestingly  the larger
point  
   the larger this minimum angle is  which shows that
critical hyperplanes    that are distinct from    must be
suf ciently separated from    Finally  the second term
in   is       while the  rst term is the smallest objective
value that corresponds to     bi        and so   simply
guarantees that            bi         Next  condition
   is close to the rest of the hyperplanes  which leads to
small   while the rest of the hyperplanes are suf ciently
separated   which leads to small   and small   Regardless  one can show that
   Ni  then any global minimizer of   has
    
to be one of the normals  irrespectively of what the angles
 ij are  Finally  condition   is consistent with condition   in that it requires    to be close to Hi        
and Hi Hj to be suf ciently separated for          Once
again    can always be satis ed irrespectively of the  ij 
by choosing    suf ciently large  since only the positive
term in the de nition of   depends on   

      cid      of Theorem   is easier to satisfy when
   Ni  cid      and so if
 cid 

 cid 

 

 

  Theoretical analysis of the discrete problem
We now study  the discrete formulation of DPCP       problem   for the case where                      with    
being Ni points in Hi  as described in   For any        
and     SD  we can write the quantity

as

 cid cid cid    cid 

   

 cid cid cid  

Ni cid 

  

 cid 

    
 

 cid  Ni cid 

 cid cid cid     

  

Sign

 cid 

    
 

    
 

 cid 
 cid 

 cid 
Ni cid 

 

  

 cid 

  Ni  

xi    xi    

 
Ni

Sign

 cid 

    
 

 

    
 

 We emphasize that the interpretation of close and suf ciently

separated is relative to    and              

 More detailed arguments and proofs can be found in Tsakiris

  Vidal    

 cid cid cid 
 cid 

 

 

with xi   being the average point of     with respect to the
orthogonal projection hi      Hi    of   onto Hi  xi  
can be viewed as an approximation to the vector integral

 cid 

This leads us to de ne the maximal approximation error

 cid 

Sign  

    Hi

     max
  SD 

        Hi

 cid cid cid xi       cid hi  

 cid cid cid 

     hi   

 

 

 

as   ranges over the entire unit sphere SD  Intuitively  the
more uniformly distributed the points     are inside  Hi  the
smaller    is  This intuition can be formalized by means of
the spherical cap discrepancy  Grabner et al    Grabner   Tichy    of      given by

 cid cid cid cid cid cid   

Ni

Ni cid 

  

 cid 

IC

    
 

 cid       Hi

 cid cid cid cid cid cid   

   

 

SD        sup
 

In   the supremum is taken over all spherical caps   of
  SD  where   spherical cap is the interthe sphere  Hi
section of SD  with   halfspace of RD  and IC  is the
indicator function of    which takes the value   inside   and
zero otherwise  SD      is   deterministic measure of the
uniformity of the point set      By adjusting an argument
of Harman   one can show that

    

 SD              

 
which con rms that uniformly distributed points     correspond to small     We note here that SD      decreases
with   rate of  Dick    Beck   
   

 cid log Ni  

   

 

 

   
 

 

 cid cid  

To state the main theorem of this section  Theorem   we
need   de nition 
De nition   For   set                 yL    SD  and
positive integer        de ne RY   to be the maximum circumradius among the circumradii of all polytopes
  where            jK are distinct integers in     and the circumradius of   closed
bounded set is the minimum radius among all spheres that
contain the set  We now de ne the quantity of interest as

    ji yji    ji      

 cid 

   

max

  Kn   

 Ki   

RX   Ki 

 

points  Combining this fact with the constraint cid 

We note that it is always the case that RX   Ki   Ki  with
this upper bound achieved when     contains Ki colinear
  Ki  
      in   we get that           and the more uniformly distributed are the points   inside the hyperplanes 
the smaller   is  even though   does not go to zero 

  cid 

  

Hyperplane Clustering via Dual Principal Component Pursuit

Theorem   Let  
                 and suppose that    

 

  be   global minimizer of   with    

  If

    

      where

 cid 
 cid 
          cid    

          

 cid 

 cid 
 cid 

  

       

 iNi

 cid 

 

  and

 

with     as in Theorem   and if

          

             

 cid 

 iNi

 

 

 cid 

 iNi

   

 

 cid 

  

 cid 

then problem   has   unique minimizer    

  of Theorem   with conditions       cid           

Notice the similarity of conditions     

         

of Theorem   In fact             and       which implies that the conditions of Theorem   are strictly stronger
than those of Theorem   This is no surprise since  as we
have already remarked  the global minimizers of   depend
not only on the geometry  ij  and the weights  Ni  of the
hyperplane arrangement  but also on the distribution of the
data points  parameters    and    In contrast though to
condition   of Theorem      now appears in both sides
of condition   of Theorem   which is however harmless  under the assumption    
    is equivalent
to the positivity of   quadratic polynomial in    whose
leading coef cient is positive  and hence   can always
be satis ed for suf ciently large    Another interesting
connection of Theorem   to Theorem   is that assuming
limNi  SD          Theorem   can be seen as   limit
version of Theorem   dividing   and   by    letting
           Nn go to in nity while keeping each ratio Ni   
 xed  recalling that           and noting that in view of
  we have limNi         we see that in the limit we
recover the conditions of Theorem  
Finally    theorem of the same  avor gives conditions under
which   converges in    nite number of iterations to    or
    see Theorem   in Tsakiris   Vidal    

 

  Algorithmic Contributions
  DPCP via iteratively reweighted least squares

Sp ath   Watson   Tsakiris   Vidal    propose
solving the nonconvex problem   by means of the recursion of convex optimization problems   referred to as
DPCPr  This is computationally equivalent to   recursion
of linear programs  which can be solved ef ciently by an
optimized LP solver such as GUROBI  However  these linear programs are in principle not sparse  which may render

the running time of this approach prohibitive for bigdata
applications  To alleviate this issue  we solve   by standard Iteratively Reweighted Least Squares  IRLS  applied
to  cid  minimization problems  Cand es et al    Chartrand   Yin    Daubechies et al    Lerman et al 
  The resulting algorithm  referred to as DPCPIRLS 
is dramatically faster than solving DPCPr by GUROBI 
  MATLAB implementation on   standard MacBook Pro
with   dual core  GHz processor and   total of  GB cache
memory is able to handle   points of    in about
one minute  while in such   regime DPCPr seems  as of
now  inapplicable  Moreover  the performance of DPCPIRLS  investigated in   suggests that DPCPIRLS converges most of the time to   global minimizer of   the
theoretical justi cation of this claim is ongoing research 

  Hyperplane clustering algorithms via DPCP
Sequential Hyperplane Learning  SHL  via DPCP 
Since at its core DPCP is   single subspace learning method
 Tsakiris   Vidal      we may as well use it to learn  
hyperplanes in the same way that RANSAC  Fischler  
Bolles    is used  learn one hyperplane from the entire
dataset  remove the points close to it  then learn   second
hyperplane  remove the points close to it  and so on  The
main weakness of this approach is well known  and consists of its sensitivity to   thresholding parameter  which is
necessary in order to be able to remove points 
To alleviate the need of knowing   good threshold  we propose to replace the process of removing points by   process
of appropriately weighting the points  In particular  suppose we solve the DPCP problem   on the entire dataset
  and obtain   unit  cid norm vector    Now  instead of
removing the points of   that are close to the hyperplane
with normal vector     which would require   threshold
parameter  we weight each and every point xj of   by its
distance
second hyperplane with normal    we apply DPCP on the
weighted dataset
  To compute   third hyperplane  the weight of point xj is chosen as the smallest distance of xj from the already computed two hyperplanes 
     DPCP is now applied to
 
After   hyperplanes have been computed  the clustering
of the points is obtained based on their distances to the  
hypeprlanes  We note here that the theoretical correctness
of this weighted sequential scheme does not follow automatically from the theory presented in this paper  since the
latter applies only to unit  cid norm points  studying DPCP
for weighted points is ongoing research 
Iterative Hyperplane Learning  IHL  via DPCP  Another way to do hyperplane learning and clustering via
DPCP is to modify the classic KHyperplanes  which we

 cid cid cid  from that hyperplane  Then to compute  
 cid cid cid cid  
 cid cid cid cid 
 cid 

 cid cid cid  xj

 cid cid 

 cid cid cid  

 cid cid cid  

mini 

 cid 
  xj

 cid 
  xj

 cid 
  xj

 cid 

xj

Hyperplane Clustering via Dual Principal Component Pursuit

will be referring to as IHLSVD  Bradley   Mangasarian 
  Tseng    Zhang et al     see   by computing the normal vector of each cluster by DPCP  instead
of       SVD  see   for more details  The resulting algorithm  IHLDPCP  minimizes  up to   local minimum 
the sum of the distances of the points to the estimated hyperplane arrangement  which corresponds to replacing the
 cid norm in the objective of IHLSVD with the  cid norm 
precisely as in the case of MKF  Zhang et al   

  Experimental Evaluation
  Experiments using synthetic data
We evaluate SHLDPCP   using synthetic data  and
compare it with similar algorithms  where instead of solving the DPCP problem    either via DPCPr or via DPCPIRLS  one uses REAPER or RANSAC  For fairness 
RANSAC does not remove any points as it sequentially
learns the hyperplanes  rather it selects them randomly using the probability distribution induced by weights de ned
in   similar way as in   Moreover  it is con gured to
run at least as long as DPCPr  which uses   maximum
of   iterations in   while REAPER and DPCPIRLS
use   maximum of   iterations and convergence accuracy   The ambient dimension is set to          
as inspired by major applications where hyperplane arrangements appear             in    point cloud analysis  in homogeneous coordinates  and       in twoview geometry  Cheng et al    For each choice of  
we randomly generate           hyperplanes and sample them as follows  Given    we set         with
Ni      Ni        where         is   parameter
that controls the balancing of the clusters        means
the clusters are perfectly balanced  while smaller values
of   lead to less balanced clusters  We set        for
        see Tsakiris   Vidal     Each cluster is
sampled from   zeromean unitvariance Gaussian distribution with support in the corresponding hyperplane  To
make the experiment more realistic  we corrupt points from
each hyperplane by adding white Gaussian noise of deviation       with support in the direction orthogonal to
the hyperplane  Moreover  we corrupt the dataset by adding
              outliers sampled from   standard
zeromean unitvariance Gaussian distribution supported in
the ambient space  where   is the number of outliers 
The left column of Figure   plots the clustering accuracy
over   independent experiments as   function of the relative dimension         and the number of hyperplanes
   As expected  the performance degrades as either the rel 

 We have compared with methods such as SCC or MKF  however we do not report on these methods since they perform significantly more inferior to RANSAC  REAPER or DPCP 

ative dimension or the number of hyperplanes increases 
There are at least two interesting things to notice  First 
RANSAC is the best method when       irrespectively of
the number of hyperplanes  since for such   low ambient dimension the probability that           randomly selected
points lie in the same hyperplane is very high  Indeed  for
      RANSAC   accuracy ranges from          to
         as opposed to  for       REAPER   or
even DPCPIRLS   and DPCPr   On the other
hand  DPCPr is overall the best method with an   accuracy in the challenging scenario               
as opposed to   for DPCPIRLS    for REAPER and
  for RANSAC  The right column of Figure   plots the
clustering accuracy as   function of   and of the percentage of outliers  for       and additive noise as before 
Evidently  DPCPr and DPCPIRLS are the best methods 
with         clustering accuracy of   and   respectively for       and   outliers  as opposed to   for
RANSAC and   for REAPER 

  Experiments using real kinect data

In this section we explore various hyperplane clustering algorithms using the benchmark dataset NYUdepthV   Silberman et al    This dataset consists of   RGBd
data instances acquired using the Microsoft kinect sensor 
Each instance corresponds to an indoor scene  and consists
of the           RGB data together with depth data
for each pixel  The depth data can be used to reconstruct  
   point cloud associated to the scene  In this experiment
we use such    point clouds to learn plane arrangements
and segment the pixels of the corresponding images based
on their plane membership  This is an important problem
in robotics  where estimating the geometry of   scene is
essential for successful robot navigation 
In such    applications RANSAC is the predominant stateof theart method  since the probability of sampling three
points from the dominant plane is very large  Thus we compare   sequential hyperplane learning RANSAC algorithm
 SHLRANSAC  which uses   threshold      
for removing points  to iterative KHyperplane like algorithms based on SVD  REAPER  RANSAC and DPCPIRLS  to be referred to as IHLSVD  IHLREAPER  and
so on  These algorithms randomly initialize   hyperplanes 
they cluster the points according to their distance to these
hyperplanes  they re ne the hyperplanes by  tting   new
hyperplane at each cluster  reassign points based on the
new hyperplanes  and so on  until the objective function
converges or   iterations are reached  We use   independent restarts  and we control the running time of SHLRANSAC and IHLRANSAC to be not less than that of
IHLDPCP IRLS 
The algorithms do not operate on the raw    data  rather on
standard superpixel representations of the data  where each

Hyperplane Clustering via Dual Principal Component Pursuit

    RANSAC

    RANSAC

    REAPER

    REAPER

    DPCPIRLS

    DPCPIRLS

    DPCPr

    DPCPr

Figure   Sequential hyperplane learning  Left  right  column
shows clustering accuracy  white corresponds to   black to  
as   function of the number of hyperplanes   and the relative dimension      percentage           of outliers 

superpixel is represented by its median    point  weighted
by the size of the superpixel  Moreover  since    planes in
an indoor seen usually do not pass through   common origin  the algorithms work with homogeneous coordinates 
Finally  the algorithms as described so far are purely geometric  in the sense that they do not take into account the
spatial coherence of the    point cloud  nearby points are
likely to lie in the same plane  and so we expect their output segmentation to be spatially incoherent  To associate  
spatially smooth image segmentation to each algorithm  we
use the normal vectors            bn that the algorithm produced to minimize   ConditionalRandom Field  Sutton  
McCallum    energy function              yN    

 cid 
 cid xj   xk cid 

 

 cid 

 
 

 yj  cid  yk 

 

  cid 

  

 cid 

  Nj

  

yj xj     
 cid 

CBj   exp

In   the  rst and second terms are known as unary and

Table   Clustering error in   of    planes from Kinect data
without  CRF  and with  CRF  spatial smoothing 

     

     

method
SHLRANSAC
IHLRANSAC
IHLSVD
IHLREAPER
IHLDPCP IRLS

CRF  CRF 
 
 
 
 
 
 
 
 
 
 

CRF  CRF 
 
 
   
 
 
 
 
 
 

pairwise potentials  yj                is the plane label of
   point xj  which is the variable to optimize over  CBj  
is the length of the common boundary between superpixels
   and   and Nj indexes the neighbors of xj  The parameter   in   is set to the inverse of twice the maximal
rowsum of the pairwise matrix  in order to achieve   balance between unary and pairwise terms  Minimization of
  is done via GraphCuts  Boykov et al   
Since NYUdpethV  does not come with   ground truth annotation based on plane membership  we manually annotated   of the   scenes in the dataset  in which dominant planes such as  oors  walls  ceilings  tables and so
on are present  Table   shows the clustering errors of various algorithms on these   annotated scenes for the identi cation of the  rst   dominant planes of each scene 
where for SHLRANSAC the error is averaged over the
three different choices of   threshold  As expected  the
clustering error increases for all methods as the number of
planes to be identi ed increases  Again as expected  the
performance of all algorithms improves signi cantly if one
includes spatial smoothing  Notice that the best method
is IHLRANSAC  and not the sequential SHLRANSAC 
which seems   rather interesting  nding  On the other hand 
the rest of the methods seem to perform similarly to each
other  with IHLSVD being slightly inferior  since it is less
robust to outliers  and IHLDPCP IRLS being overall the
second best method 

  Conclusions
In this paper we extended the framework of Dual Principal
Component Pursuit  DPCP  to the case of data lying in  
union of hyperplanes  We provided theoretical conditions
under which the normal vector of the dominant hyperplane
is the unique global minimizer of the nonconvex  cid  DPCP
optimization problem  Moreover  we proposed   fast implementation of DPCP  as well as DPCPbased hyperplane
clustering algorithms  which were shown to outperform or
be competitive to stateof theart algorithms 

 If the scene has       annotated planes  then the clustering error is computed only with respect to the  rst   dominant
clusters identi ed by the algorithm 

                                                         Hyperplane Clustering via Dual Principal Component Pursuit

Acknowledgements
This work was supported by NSF grants   and
  The  rst author thanks Dr  Glyn Harman for his
help in deriving equation   and Dr  Bijan Afsari for interesting conversations on geometric medians in nonlinear
manifolds  The authors also thank two of the anonymous
reviewers for providing constructive comments 

References
Bako    

Identi cation of switched linear systems via
sparse optimization  Automatica     

Beck     Sums of distances between points on   sphere an
application of the theory of irregularities of distribution
to discrete geometry  Mathematika     

Boykov     Veksler     and Zabih     Fast approximate
energy minimization via graph cuts  IEEE Transactions
on Pattern Analysis and Machine Intelligence   
   

Bradley        and Mangasarian        kplane clustering  Journal of Global Optimization     
ISSN  

Cand es     Wakin     and Boyd     Enhancing sparsity by
reweighted  cid  minimization  Journal of Fourier Analysis
and Applications     

Chartrand     and Yin    

Iteratively reweighted algorithms for compressive sensing  In   IEEE International Conference on Acoustics  Speech and Signal Processing  pp    IEEE   

Chen     and Lerman     Spectral curvature clustering
International Journal of Computer Vision   

 SCC 
    ISSN  

Cheng     Lopez        Camps     and Sznaier       convex optimization approach to robust fundamental matrix estimation  In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition  pp   
   

Daubechies     DeVore     Fornasier     and   unt urk 
      Iteratively reweighted least squares minimization
for sparse recovery  Communications on Pure and Applied Mathematics     

Dick     Applications of geometric discrepancy in numerical analysis and statistics  Applied Algebra and Number
Theory   

Draper     Kirby     Marks     Marrinan     and Peterson        ag representation for  nite collections of
subspaces of mixed dimensions  Linear Algebra and its
Applications     

Elhamifar     and Vidal     Sparse subspace clustering 
Algorithm  theory  and applications  IEEE Transactions
on Pattern Analysis and Machine Intelligence   
   

Fischler        and Bolles        RANSAC random sample consensus    paradigm for model  tting with applications to image analysis and automated cartography 
Communications of the ACM     

Ghalieh     and Hajja     The fermat point of   spherical
triangle  The Mathematical Gazette   
 

Grabner        and Tichy       Spherical designs  discrepancy and numerical integration  Math  Comp   
    ISSN   doi   
  URL http dx doi org 
 

Grabner        Klinger     and Tichy       Discrepancies of
point sequences on the sphere and numerical integration 
Mathematical Research     

Harman     Variations on the koksmahlawka inequality 

Uniform Distribution Theory     

Lerman     McCoy        Tropp        and Zhang    
Robust computation of linear models by convex relaxation  Foundations of Computational Mathematics   
   

Liu     Lin     Yan     Sun     and Ma     Robust recovery of subspace structures by lowrank representation 
IEEE Transactions on Pattern Analysis and Machine Intelligence    Jan  

Lu  CY  Min     Zhao  ZQ  Zhu     Huang  DS  and
Yan     Robust and ef cient subspace segmentation via
In European Conference on
least squares regression 
Computer Vision   

Qu     Sun     and Wright     Finding   sparse vector in
  subspace  Linear sparsity using alternating directions 
In Advances in Neural Information Processing Systems 
pp     

Sampath     and Shan     Segmentation and reconstruction of polyhedral building roofs from aerial lidar point
clouds  Geoscience and Remote Sensing  IEEE Transactions on     

Silberman     Kohli     Hoiem     and Fergus     Indoor
segmentation and support inference from rgbd images 
In European Conference on Computer Vision   

Sp ath     and Watson       On orthogonal linear  cid  approximation  Numerische Mathematik   
 

Hyperplane Clustering via Dual Principal Component Pursuit

Spielman       Wang     and Wright     Exact recovery
of sparselyused dictionaries  In Proceedings of the   
international joint conference on Arti cial Intelligence 
pp    AAAI Press   

You     Li       Robinson     and Vidal     Oracle
based active set algorithm for scalable elastic net subspace clustering  In IEEE Conference on Computer Vision and Pattern Recognition  pp     

Zhang     Szlam     and Lerman     Median   ats for
hybrid linear modeling with many outliers  In Workshop
on Subspace Methods  pp     

Sun     Qu     and Wright     Complete dictionary recovery using nonconvex optimization  In Proceedings of
the  nd International Conference on Machine Learning
 ICML  pp     

Sutton     and McCallum     An introduction to conditional random  elds for relational learning  volume  
Introduction to statistical relational learning  MIT Press 
 

Tsakiris        and Vidal     Dual principal component

pursuit  arXiv     cs CV     

Tsakiris        and Vidal     Hyperplane clustering via
arXiv 

dual principal component pursuit 
 cs CV     

Tsakiris        and Vidal     Filtrated algebraic subspace
clustering  SIAM Journal on Imaging Sciences   
     

Tsakiris       and Vidal     Dual principal component pursuit  In ICCV Workshop on Robust Subspace Learning
and Computer Vision  pp       

Tsakiris       and Vidal     Filtrated spectral algebraic
subspace clustering  In ICCV Workshop on Robust Subspace Learning and Computer Vision  pp       

Tseng     Nearest   at to   points  Journal of Optimiza 

tion Theory and Applications     

Vidal     Soatto     Ma     and Sastry     An algebraic
geometric approach to the identi cation of   class of linear hybrid systems  In IEEE Conference on Decision and
Control  pp     

Vidal     Ma     and Sastry     Generalized Principal
IEEE Transactions on
Component Analysis  GPCA 
Pattern Analysis and Machine Intelligence   
 

Vidal     Ma     Soatto     and Sastry     Twoview
multibody structure from motion  International Journal
of Computer Vision     

Vidal     Ma     and Sastry     Generalized Principal

Component Analysis  Springer Verlag   

Wang  YX  Xu     and Leng     Provable subspace clustering  When LRR meets SSC  In Neural Information
Processing Systems   

