Approximate Newton Methods and Their Local Convergence

Haishan Ye   Luo Luo   Zhihua Zhang  

Abstract

Many machine learning models are reformulated
as optimization problems  Thus  it is important
to solve   largescale optimization problem in
big data applications  Recently  stochastic second order methods have emerged to attract much
attention for optimization due to their ef ciency
at each iteration  recti ed   weakness in the ordinary Newton method of suffering   high cost in
each iteration while commanding   high convergence rate  However  the convergence properties
of these methods are still not well understood 
There are also several important gaps between
the current convergence theory and the performance in real applications  In this paper  we aim
to  ll these gaps  We propose   unifying framework to analyze local convergence properties of
second order methods  Based on this framework 
our theoretical analysis matches the performance
in real applications 

  Introduction
Mathematical optimization is an importance pillar of machine learning  We consider the following optimization
problem

  cid 

  

min
  Rd

       cid   
 

fi   

 

where the fi    are smooth functions  Many machine
learning models can be expressed as   where each fi
is the loss with respect to        
the ith training sample  There are many examples such as logistic regressions 
smoothed support vector machines  neural networks  and
graphical models 
Many optimization algorithms to solve the problem in  
are based on the following iteration 

               tQtg                       

 Shanghai Jiao Tong University  Shanghai  China  Peking University   Beijing Institute of Big Data Research  Beijing  China 
Correspondence to  Zhihua Zhang  zhzhang gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

where        is the step length  If Qt is the identity matrix
and                   the resulting procedure is called
Gradient Descent  GD  which achieves sublinear convergence for   general smooth convex objective function and
linear convergence for   smoothstrongly convex objective
function  When   is large  the full gradient method is inef 
 cient due to its iteration cost scaling linearly in    Consequently  stochastic gradient descent  SGD  has been   typical alternative  Robbins   Monro    Li et al   
Cotter et al    In order to achieve cheaper cost in each
iteration  such   method constructs an approximate gradient
on   small minibatch of data  However  the convergence
rate can be signi cantly slower than that of the full gradient methods  Nemirovski et al    Thus    great deal
of efforts have been made to devise modi cation to achieve
the convergence rate of the full gradient while keeping low
iteration cost  Johnson   Zhang    Roux et al   
Schmidt et al    Zhang et al   
If Qt is       positive de nite matrix containing the curvature information  this formulation leads us to secondorder
methods  It is well known that second order methods enjoy superior convergence rate in both theory and practice
in contrast to  rstorder methods which only make use of
the gradient information  The standard Newton method 
where Qt                              and
       achieves   quadratic convergence rate for smoothstrongly convex objective functions  However  the Newton method takes   nd       cost per iteration  so it
becomes extremely expensive when   or   is very large 
As   result  one tries to construct an approximation of the
Hessian in which way the update is computationally feasible  and while keeping suf cient second order information  One class of such methods are quasiNewton methods  which are generalizations of the secant methods to  nd
the root of the  rst derivative for multidimensional problems  The celebrated BroydenFletcher GoldfarbShanno
 BFGS  and its limited memory version  LBFGS  are the
most popular and widely used  Nocedal   Wright   
They take   nd      cost per iteration 
Recently  when    cid     socalled subsampled Newton
methods have been proposed  which de ne an approximate
Hessian matrix with   small subset of samples  The most
naive approach is to sample   subset of functions fi randomly  RoostaKhorasani   Mahoney    Byrd et al 

Approximate Newton Methods and Their Local Convergence

  Xu et al    to construct   subsampled Hessian 
Erdogdu   Montanari   proposed   regularized subsampled Newton method called NewSamp  When the Hessian can be written as                      where
     is an available       matrix  Pilanci   Wainwright
  used sketching techniques to approximate the Hessian and proposed   sketch Newton method  Similarly 
Xu et al    proposed to sample rows of      with
nonuniform probability distribution  Agarwal et al   
brought up an algorithm called LiSSA to approximate the
inversion of Hessian directly 
Although the convergence performance of stochastic second order methods has been analyzed  the convergence
properties are still not well understood  There are several
important gaps lying between the convergence theory and
real application 
The  rst gap is the necessity of Lipschitz continuity of
Hessian 
In previous work  to achieve   linearquadratic
convergence rate  stochastic second order methods all assume that        is Lipschitz continuous  However  in
real applications without this assumption  they might also
converge to the optimal point  For example  Erdogdu
  Montanari   used NewSamp to successfully train
smoothedSVM in which the Hessian is not Lipschitz continuous 
The second gap is about the sketched size of sketch Newton methods  To obtain   linear convergence  the sketched
size is      in  Pilanci   Wainwright    and then
is improved to      in  Xu et al    using Gaussian
sketching matrices  where   is the condition number of the
Hessian matrix in question  However  the sketch Newton
empirically performs well even when the Hessian matrix is
illconditioned  Sketched size being several tens of times
or even several times of   can achieve   linear convergence
rate in unconstrained optimization  But the theoretical result of Pilanci   Wainwright   Xu et al    implies that sketched size may be beyond   in illcondition
cases 
The third gap is about the sample size in regularized subsampled Newton methods  In both  Erdogdu   Montanari 
  and  RoostaKhorasani   Mahoney    their theoretical analysis shows that the sample size of regularized
subsampled Newton methods should be set as the same as
the conventional subsampled Newton method  In practice 
however  adding   large regularizer can obviously reduce
the sample size while keeping convergence  Thus  this contradicts the extant theoretical analysis  Erdogdu   Montanari    RoostaKhorasani   Mahoney   
In this paper  we aim to  ll these gaps between the current theory and empirical performance  More speci cally 
we  rst cast these second order methods into an algorith 

mic framework that we call approximate Newton  Then we
propose   general result for analysis of local convergence
properties of second order methods  Based on this framework  we give detailed theoretical analysis which matches
the empirical performance very well  We summarize our
contribution as follows 

  We propose   unifying framework  Theorem   to analyze local convergence properties of second order
methods including stochastic and deterministic versions  The convergence performance of second order
methods can be analyzed easily and systematically in
this framework 

  We prove that the Lipschitz continuity condition of
Hessian is not necessary for achieving linear and superlinear convergence in variants of subsampled Newton  But it is needed to obtain quadratic convergence  This explains the phenomenon that NewSamp
 Erdogdu   Montanari    can be used to train
smoothed SVM in which the Lipschitz continuity condition of Hessian is not satis ed  It also reveals the
reason why previous stochastic second order methods 
such as subsampled Newton  sketch Newton  LiSSA 
etc  all achieve   linearquadratic convergence rate 
  We prove that the sketched size is independent of the
condition number of the Hessian matrix which explains that sketched Newton performs well even when
the Hessian matrix is illconditioned 

  We provide   theoretical guarantee that adding   regularizer is an effective way to reduce the sample size in
subsampled Newton methods while keeping converging  Our theoretical analysis also shows that adding  
regularizer will lead to poor convergence behavior as
the sample size decreases 

  Organization

The remainder of the paper is organized as follows 
In
Section   we present notation and preliminaries  In Section   we present   unifying framework for local convergence analysis of second order methods  In Section   we
analyze the local convergence properties of sketch Newton methods and prove that sketched size is independent of
condition number of the Hessian  In Section   we give the
local convergence behaviors of several variants of subsampled Newton method  Especially  we reveal the relationship
among the sample size  regularizer and convergence rate 
In Section   we derive the local convergence properties
of inexact Newton methods from our framework  In Section   we validate our theoretical results experimentally 
Finally  we conclude our work in Section   All the proofs
are presented in the supplementary metarials 

Approximate Newton Methods and Their Local Convergence

  Notation and Preliminaries
In this section  we introduce the notation and preliminaries
that will be used in this paper 

  Notation
Given   matrix      aij    Rm   of rank  cid  and  
positive integer      cid  its condensed SVD is given as
       kV      where Uk and    
             Uk kV  
contain the left singular vectors of    Vk and     contain
the right singular vectors of    and     diag           cid 
with              cid      are the nonzero singular
values of    We use  max    to denote the largest singular value and  min    to denote the smallest nonzero
singular value  Thus  the condition number of   is de ned
by      cid   max   
If   is positive semide nite  then
 min     
      and the square root of   can be de ned as     
         It also holds that               where      
is the ith largest eigenvalue of     max       max   
and  min       min   
Additionally   cid   cid   cid    is the spectral norm  Given   positive de nite matrix     cid   cid    cid   cid      cid  is called the
Mnorm of    Give square matrices   and   with the same
size  we denote    cid    if       is positive semide nite 
  Randomized sketching matrices

We  rst give an  subspace embedding property which will
be used to sketch Hessian matrices  Then we list two most
popular types of randomized sketching matrices 
De nition       Rs   is said to be an  subspace embedding matrix           xed matrix     Rm   where
       if  cid SAx cid         cid Ax cid             cid Ax cid   
 cid SAx cid         cid Ax cid  for all     Rd 
From the de nition of the  subspace embedding matrix 
we can derive the following property directly 
Lemma       Rs   is an  subspace embedding matrix
       the matrix     Rm   if and only if

     AT    cid  AT ST SA  cid       AT   

Leverage score sketching matrix    leverage score
sketching matrix          Rs              Rm   is de 
 ned by sampling probabilities pi    sampling matrix    
Rm   and   diagonal rescaling matrix     Rs    Specifically  we construct   as follows  For every                 
independently and with replacement  pick an index   from
the set                with probability pi  and set  ji    
and  jk     for    cid    as well as Djj    pis  The sampling probabilities pi are the leverage scores of   de ned
as follows  Let     Rm   be the column orthonormal

basis of    and let vi  denote the ith row of     Then
 cid    cid   cid vi cid   for                 are the leverage scores of
   To achieve an  subspace embedding property          
        log    is suf cient 
Sparse embedding matrix    sparse embedding matrix
    Rs   is such   matrix in each column of which there
is only one nonzero entry uniformly sampled from  
 Clarkson   Woodruff    Hence  it is very ef cient
to compute SA  especially when   is sparse  To achieve
an  subspace embedding property            Rm       
     is suf cient  Meng   Mahoney    Woodruff 
 
Other sketching matrices such as Gaussian Random Projection and Subsampled Randomized Hadamard Transformation as well as their properties can be found in the survey
 Woodruff   

  Assumptions and Notions

In this paper  we focus on the problem described in
Eqn    Moreover  we will make the following two assumptions 

Assumption   The objective function   is  strongly
convex  that is 
 
 cid     cid  for      
                         
Assumption          is LLipschitz continuous  that is 

 cid               cid      cid       cid  for      

Assumptions   and   imply that for any     Rd  we have

    cid          cid  LI 

where   is the identity matrix of appropriate size  With  
little confusion  we de ne    cid   
    In fact    is an upper
bound of condition number of the Hessian matrix       
for any   
Besides  if        is Lipschitz continuous  then we have

 cid               cid       cid       cid 

where        is the Lipschitz constant of       
Throughout this paper  we use notions of linear convergence rate  superlinear convergence rate and quadratic convergence rate  In our paper  the convergence rates we will
use are de ned         cid     cid    where            
and    is the optimal solution to Problem     sequence
of vectors       is said to converge linearly to   limit point
   if for some          

lim sup
  

 cid        cid   
 cid        cid   

   

Approximate Newton Methods and Their Local Convergence

    There exists   suf cient small value              
and             such that when  cid          
 cid      we

have that
 cid 
 cid        cid   
 

 
     

   

 cid         
       cid        cid     

 

   
     

Similarly  superlinear convergence and quadratic convergence are respectively de ned as

 cid        cid   
 cid        cid   

lim sup

  

    lim sup

  

 cid        cid   
 cid        cid 
  

   

We call it the linearquadratic convergence rate if the following condition holds 

 cid        cid       cid        cid     cid        cid 
where          

    

  Approximate Newton Methods and Local

Convergence Analysis

The existing variants of stochastic second order methods
share some important attributes  First  these methods such
as NewSamp  Erdogdu   Montanari    LiSSA  Agarwal et al    subsampled Newton with conjugate gradient  Byrd et al    and subsampled Newton with nonuniformly sampling  Xu et al    all have the same
convergence properties  that is  they have   linearquadratic
convergence rate 
Second  they also enjoy the same algorithm procedure summarized as follows  In each iteration  they  rst construct an
approximate Hessian matrix       such that

            cid            cid            

 

where           Then they solve the following optimization problem

min

 

 
 

pT          pT        

 

approximately or exactly to obtain the direction vector
     Finally  their update equation is given as       
            With this procedure  we regard these stochastic
second order methods as approximate Newton methods 
In the following theorem  we propose   unifying framework which describes the convergence properties of the
second order optimization procedure depicted above 

Theorem   Let Assumptions   and   hold  Suppose that
       exists and is continuous in   neighborhood of  
minimizer          is   positive de nite matrix that satis 
 es Eqn    with           Let      be an approximate
solution of Problem   such that

 cid                    cid   

 
   cid        cid 

 

where           De ne the iteration                

 

 

 

Besides      and     will go to   as      goes to   
    Furthermore  if        is Lipschitz continuous with
parameter     and      satis es

 cid         

 

 cid   

 
   

   

where             then it holds that

 cid 
 cid        cid   
 

   

 
     
 

 

     

 cid         
       cid        cid   
         cid        cid 

       

   
 

    

 cid 

 cid 

     
 

From Theorem   we can  nd some important
insights  First  Theorem   provides suf cient conditions to
get different convergence rates including superliner and
is   constant 
quadratic convergence rates  If
then sequence       converges linearly because     and
    will go to   as   goes to in nity  If we set        
and         such that     and     decrease to  
as   increases  then sequence       will converge superlinearly  Similarly  if         cid        cid           
  cid        cid      and        is Lipschitz continuous 
then sequence       will converge quadratically 
Second  Theorem   makes it clear that the Lipschitz continuity of        is not necessary for linear convergence
and superlinear convergence of stochastic second order
methods including Subsampled Newton method  Sketch
Newton  NewSamp  etc  This reveals the reason why
NewSamp can be used to train the smoothed SVM where
the Lipschitz continuity of the Hessian matrix is not satis ed  The Lipschitz continuity condition is only needed
to get   quadratic convergence or linearquadratic convergence  This explains the phenomena that LiSSA Agarwal
et al    NewSamp  Erdogdu   Montanari   
subsampled Newton with nonuniformly sampling  Xu
et al    Sketched Newton  Pilanci   Wainwright 
  have linearquadratic convergence rate because they
all assume that the Hessian is Lipschitz continuous  In fact 
it is well known that the Lipschitz continuity condition of
       is not necessary to achieve   linear or superlinear
convergence rate for inexact Newton methods 

Approximate Newton Methods and Their Local Convergence

Table   Comparison with previous work

Reference

Pilanci   Wainwright  

Xu et al   

Our result Theorem  

Algorithm   Sketch Newton 
  Input                        
  for               until termination do
 

an

 subspace

Construct
for        and where       
      
               
                 ST SB     
Calculate        argminp
Update                   

 

 
 
  end for

embedding matrix  
the form
calculate

is of
and

 

  pT          pT        

Third  the unifying framework of Theorem   contains not
only stochastic second order methods  but also the deterministic versions  For example  letting                 
and using conjugate gradient to get      we obtain the famous  NewtonCG  method  In fact  different choice of
      and different way to calculate      lead us to different second order methods  In the following sections  we
will use this framework to analyze the local convergence
performance of these second order methods in detail 

  Sketch Newton Method
In this section  we use Theorem   to analyze the local convergence properties of Sketch Newton  Algorithm   We
mainly focus on the case that the Hessian matrix is of the
form

                   

 

where      is an explicitly available       matrix  Our
result can be easily extended to the case that

                          

where      is   positive semide nite matrix related to the
Hessian of regularizer 

Theorem   Let       satisfy the conditions described in
Theorem   Assume the Hessian matrix is given as Eqn   
Let                     and           be
given        cid   is an  subspace embedding matrix       
     with probability at least       and direction vector
     satis es Eqn    Then Algorithm   has the following
convergence properties 

    There exists   suf cient small value              
and             such that when  cid          
 cid     
then each iteration satis es Eqn    with probability
at least      

    If          is also Lipschitz continuous and      
satis es Eqn    then each iteration satis es Eqn   
with probability at least      

 cid     log  
 cid 
 cid 
 cid     log  
 cid    log  
 cid 

Sketched Size
 
 
 

 
 

 
 

 
 

Theorem   directly provides   bound of the sketched size 
Using the leverage score sketching matrix as an example 
  is suf cient  We comthe sketched size  cid        log   
pare our theoretical bound of the sketched size with the
ones of Pilanci   Wainwright   and Xu et al    in
Table   As we can see  our sketched size is much smaller
than the other two  especially when the Hessian matrix is
illconditioned 
Theorem   shows that the sketched size  cid  is independent
on the condition number of the Hessian matrix       
just as shown in Table   This explains the phenomena that
when the Hessian matrix is illconditioned  Sketch Newton
performs well even when the sketched size is only several
times of    For   large condition number  the theoretical
bounds of both Xu et al    and Pilanci   Wainwright
  may be beyond the number of samples    Note that
the theoretical results of  Xu et al    and  Pilanci  
Wainwright    still hold in the constrained optimization problem  However  our result proves the effectiveness
of the sketch Newton method for the unconstrained optimization problem in the illconditioned case 

  The Subsampled Newton method and

Variants

In this section  we apply Theorem   to analyze Subsampled
Newton and regularized subsampled Newton method 
First  we make the assumption that each fi    and      
have the following properties 

     cid fi   cid         
max
 min              

 
Accordingly  if        is illconditioned  then the value
  is large 

 

 

  The Subsampled Newton method

The Subsampled Newton method is depicted in Algorithm   and we now give its local convergence properties
in the following theorem 

Theorem   Let       satisfy the properties described in
Theorem   Assume Eqn    and Eqn    hold and let
                    and           be given     
and       are set as in Algorithm   and the direction vector
     satis es Eqn    Then Algorithm   has the following

Approximate Newton Methods and Their Local Convergence

Algorithm   Subsampled Newton 
  Input                        
  Set the sample size           log   
 
 
  for               until termination do
 

 

 cid 
Select   sample set    of size     and construct        
     fj     
   
Calculate        argminp
Update                   

  pT          pT        

 

 
 
  end for

convergence properties 

    There exists   suf cient small value              
and             such that when  cid          
 cid     
then each iteration satis es Eqn    with probability
at least      

    If          is also Lipschitz continuous and      
satis es Eqn    then each iteration satis es Eqn   
with probability at least      

As we can see  Algorithm   almost has the same convergence properties as Algorithm   except several minor differences  The main difference is the construction manner of
      which should satisfy Eqn    Algorithm   relies on
the assumption that each  cid fi   cid  is upper bounded      
Eqn    holds  while Algorithm   is built on the setting of
the Hessian matrix as in Eqn   

  Regularized Subsampled Newton

In illconditioned cases        
  is large  the subsampled
Newton method in Algorithm   should take   lot of samples because the sample size     depends on  
  quadratically  To overcome this problem  one resorts to   regularized subsampled Newton method  The key idea is to add
   to the original subsampled Hessian just as described
in Algorithm   Erdogdu   Montanari   proposed
NewSamp which is another regularized subsampled Newton method depicted in Algorithm   In the following analysis  we prove that adding   regularizer is an effective way
to reduce the sample size while keeping converging in theory 
We  rst give the theoretical analysis of local convergence
properties of Algorithm  

Theorem   Let       satisfy the properties described in
Theorem   Assume Eqns    and   hold  and let
                    and       be given  Assume   is
  constant such that              
    the subsampled size
    satis es           log   
  and       is constructed

 

as in Algorithm   De ne

    max

 cid       

         

 cid 

 

 

 

     

         

which implies that           Besides  the direction
vector      satis es Eqn    Then Algorithm   has the
following convergence properties 

    There exists   suf cient small value              
and             such that when  cid          
 cid   
  each iteration satis es Eqn    with probability at
least      

    If          is also Lipschitz continuous and      
satis es Eqn    then each iteration satis es Eqn   
with probability at least      

 

In Theorem   the parameter   mainly decides convergence
properties of Algorithm   It is determined by two terms
just as shown in Eqn    These two terms depict the relationship among the sample size  regularizer     and convergence rate 
The  rst term describes the relationship between the regularizer    and sample size  Without loss of generality  we
set       which satis es               Then the
sample size           log   
decreases as   increases 
Hence Theorem   gives   theoretical guarantee that adding
the regularizer    is an effective approach for reducing the
sample size when    is large  Conversely  if we want to
sample   small part of fi    then we should choose   large
  Otherwise    will go to       which means      
     the sequence       does not converge 
Though   large   can reduce the sample size  it is at the
expense of slower convergence rate just as the second term
shows  As we can see 
  goes to   as   increases 
Besides    also has to decrease  Otherwise       
 
may be beyond   which means that Algorithm   will not
converge 
In fact  slower convergence rate via adding   regularizer is
because the sample size becomes small  which implies less
curvature information is obtained  However    small sample size implies low computational cost in each iteration 
Therefore    proper regularizer which balances the cost of
each iteration and convergence rate is the key in the regularized subsampled Newton algorithm 
Next  we give the theoretical analysis of local convergence
properties of NewSamp  Algorithm  

 

Theorem   Let       satisfy the properties described in
Theorem   Assume Eqn    and Eqn    hold and let
          and target rank   be given  Let   be   conr  is the       th
stant such that        

   
    where    
  

Approximate Newton Methods and Their Local Convergence

     

Algorithm   Regularized Subsample Newton 
  Input               regularizer parameter   sample size

  for               until termination do
 

 cid 
Select   sample set    of size     and construct        
     fj           
   
Calculate        argminp
Update                   

  pT          pT        

 

 
 
  end for

Algorithm   NewSamp 
  Input                  sample size    
  for               until termination do
 

     

 cid 
     fj     

Select   sample set    of size     and get      
   
Compute rank       truncated SVD deompostion of      
   
to get Ur  and     Construct              
     
      
Calculate        argminp
Update                   

  pT          pT        

              

 

 

 
 
  end for

eigenvalue of          Set the subsampled size     such
that           log   

  and de ne

 

 cid 

 cid 

    max

 

   
      

 

       
  
           
  

 

 

which implies           Assume the direction vector
     satis es Eqn    Then Algorithm   has the following
convergence properties 

    There exists   suf cient small value              
and             such that when  cid          
 cid   
  each iteration satis es Eqn    with probability at
least      

    If          is also Lipschitz continuous and      
satis es Eqn    then each iteration satis es Eqn   
with probability at least      

Similar to Theorem   parameter   in NewSamp is also determined by two terms  The  rst term reveals the the relationship between the target rank   and sample size  Without
loss of generality  we can set        
   Then the sample size is linear in    
   Hence    small   means that
  small sample size is suf cient  Conversely  if we want
to sample   small portion of fi    then we should choose
  small    Otherwise    will go to    
   which means
           the sequence       does not converge  The
second term shows that   small sample size will lead to  
poor convergence rate  If we set       and       then

 

    Consequently  the convergence
  will be    
rate of NewSamp is almost the same as gradient descent 
Similar to Algorithm     small   means   precise solution
to Problem   and the initial point    being close to the
optimal point   
It is worth pointing out that Theorem   explains the empirical results that NewSamp is applicable in training SVM in
which the Lipschitz continuity condition of        is not
satis ed  Erdogdu   Montanari   
We now conduct comparison between Theorem   and Theorem   We mainly focus on the parameter   in these
two theorems which mainly determines convergence properties of Algorithm   and Algorithm   Speci cally  if we
   
set            
  
   
  
which equals to the second term on the righthand side in
Eqn    Hence  we can regard NewSamp as   special
case of Algorithm   However  NewSamp provides an approach for automatical choice of  
Recall that NewSamp includes another parameter  the target rank    Thus  NewSamp and Algorithm   have the
same number of free parameters  If   is not properly chosen  NewSamp will still have poor performance  Therefore 
Algorithm   is theoretically preferred because NewSamp
needs extra cost to perform SVDs 

   in Eqn    then    

  Inexact Newton Methods
Let                  that is        Then Theorem   depicts the convergence properties of inexact Newton
methods 

Theorem   Let       satisfy the properties described in
Theorem   and      be   direction vector such that

 cid                       cid   

 
   cid        cid 

where           Consider the iteration              
    
    There exists   suf cient small value              
and             such that when  cid          
 cid      then it

holds that

       

 cid        cid           

       cid        cid     
    If        is also Lipschitz continuous with parameter
    and       satis es Eqn    then it holds that
 cid        cid     
       cid        cid     
     
         cid        cid 
 

       

       

    

Approximate Newton Methods and Their Local Convergence

  Empirical Study
In this section  we validate our theoretical results about
sketched size of the sketch Newton  and sample size of
regularized Newton  experimentally  Experiments for validating unnecessity of the Lipschitz continuity condition of
       are given in the supplementary materials 
  Sketched Size of Sketch Newton
Now we validate that our theoretical result that sketched
size is independent of the condition number of the Hessian
in Sketch Newton  To control the condition number of the
Hessian conveniently  we conduct the experiment on least
squares regression which is de ned as

min

 

 
 

 cid Ax     cid 

 

In each iteration  the Hessian matrix is AT    In our experiment    is       matrix  We set the singular values
   of   as           Then the condition number of  
is                 We use different sketch
matrices in Sketch Newton  Algorithm   and set different
values of the sketched size  cid  We report our empirical results in Figure  
From Figure   we can see that Sketch Newton performs
well when the sketch size  cid  is several times of   for all
different sketching matrices  Moreover  the corresponding
algorithms converge linearly  This matches our theory that
the sketched size is independent of the condition number
of the Hessian matrix to achieve   linear convergence rate 
In contrast  the theoretical result of  Xu et al    shows
that the sketched size is  cid                    bigger
than      

    Leverage Score Sampling 

    Sparse Sketching 

Figure   Convergence properties of different sketched sizes

  Sample Size of Regularized Subsampled Newton

We also choose least squares regression de ned in
Eqn    in our experiment to validate the theory that
adding   regularizer is an effective approach to reducing
the sample size while keeping convergence in Subsampled
Newton  Let     Rn   where       and      
Hence Sketch Newton can not be used in this case because
  and   are close to each other  In our experiment  we set
different sample sizes     For each     we choose different

regularizer terms   and different target ranks    We report
our results in Figure  
As we can see  if the sample size     is small  then we
should choose   large   otherwise  the algorithm will diverge  However  if the regularizer term   is too large  then
the algorithm will converge slowly  Increasing the sample
size and choosing   proper regularizer will improve convergence properties obviously  When         it only needs
about   iterations to obtain   precise solution while it
needs about   iterations when         Similarly  if
the sample size     is small  then we should choose   small
target rank  Otherwise NewSamp may diverge  Also  if
the target rank is not chosen properly  NewSamp will have
poor convergence properties  Furthermore  from Figure  
we can see that the two algorithms have similar convergence properties  This validates the theoretical result that
NewSamp provides   method to choose   automatically 
Our empirical analysis matches the theoretical analysis in
Subsection   very well 

    Sample Size        

    Sample Size        

Figure   Convergence properties of Regularized Subsampled
Newton and NewSamp

  Conclusion
In this paper  we have proposed   framework to analyze the
local convergence properties of second order methods including stochastic and deterministic versions  This framework reveals some important convergence properties of the
subsampled Newton method and sketch Newton method 
which are unknown before  The most important thing is
that our analysis lays the theoretical foundation of several
important stochastic second order methods 
We believe that this framework might also provide some
useful insights for developing new subsampled Newtontype algorithms  We would like to address this issue in
future 

 iteration log err Levereage Score Sketching       iteration log err Sparse Sketching       iteration log err   iteration log err         log err iteration      iteration log err         Approximate Newton Methods and Their Local Convergence

Nocedal  Jorge and Wright  Stephen  Numerical optimiza 

tion  Springer Science   Business Media   

Pilanci  Mert and Wainwright  Martin    Newton sketch   
lineartime optimization algorithm with linearquadratic
convergence  arXiv preprint arXiv   

Robbins  Herbert and Monro  Sutton    stochastic approximation method  The annals of mathematical statistics 
pp     

RoostaKhorasani  Farbod and Mahoney  Michael    Subsampled newton methods ii  Local convergence rates 
arXiv preprint arXiv   

Roux  Nicolas    Schmidt  Mark  and Bach  Francis     
stochastic gradient method with an exponential convergence rate for  nite training sets  In Advances in Neural
Information Processing Systems  pp     

Schmidt  Mark  Roux  Nicolas Le  and Bach  Francis  Minimizing  nite sums with the stochastic average gradient 
arXiv preprint arXiv   

Woodruff  David    Sketching as   tool for numerical linear algebra  Foundations and Trends   cid  in Theoretical
Computer Science     

Xu  Peng  Yang  Jiyan  RoostaKhorasani  Farbod      
Christopher  and Mahoney  Michael    Subsampled
In Adnewton methods with nonuniform sampling 
vances in Neural Information Processing Systems  pp 
   

Zhang  Lijun  Mahdavi  Mehrdad  and Jin  Rong  Linear
convergence with condition number independent access
of full gradients  In Advance in Neural Information Processing Systems    NIPS  pp     

Acknowledgements
Ye has been supported by the National Natural Science
Foundation of China  Grant No     
  and   Project   grant of Shanghai Jiao Tong
University 
Luo and Zhang have been supported by
he National Natural Science Foundation of China  No 
  Natural Science Foundation of Shanghai City
 No   ZR  and Microsoft Research Asia Collaborative Research Award 

References
Agarwal  Naman  Bullins  Brian  and Hazan  Elad  Second order stochastic optimization in linear time  arXiv
preprint arXiv   

Byrd  Richard    Chin  Gillian    Neveitt  Will  and Nocedal  Jorge  On the use of stochastic hessian information in optimization methods for machine learning 
SIAM Journal on Optimization     

Clarkson  Kenneth   and Woodruff  David    Low rank
approximation and regression in input sparsity time  In
Proceedings of the forty fth annual ACM symposium on
Theory of computing  pp    ACM   

Cotter  Andrew  Shamir  Ohad  Srebro  Nati  and Sridharan 
Karthik  Better minibatch algorithms via accelerated
In Advances in neural information
gradient methods 
processing systems  pp     

Erdogdu  Murat   and Montanari  Andrea  Convergence
rates of subsampled newton methods  In Advances in
Neural Information Processing Systems  pp   
 

Johnson  Rie and Zhang  Tong  Accelerating stochastic
gradient descent using predictive variance reduction  In
Advances in Neural Information Processing Systems  pp 
   

Li  Mu  Zhang  Tong  Chen  Yuqiang  and Smola  Alexander    Ef cient minibatch training for stochastic optiIn Proceedings of the  th ACM SIGKDD
mization 
international conference on Knowledge discovery and
data mining  pp    ACM   

Meng  Xiangrui and Mahoney  Michael    Lowdistortion
subspace embeddings in inputsparsity time and applications to robust linear regression  In Proceedings of the
forty fth annual ACM symposium on Theory of computing  pp    ACM   

Nemirovski  Arkadi  Juditsky  Anatoli  Lan  Guanghui  and
Shapiro  Alexander  Robust stochastic approximation
approach to stochastic programming  SIAM Journal on
Optimization     

