Rerevisiting Learning on Hypergraphs 

Con dence Interval and Subgradient Method

Chenzi Zhang     Shuguang Hu     Zhihao Gavin Tang   TH  Hubert Chan    

Abstract

We revisit semisupervised learning on hypergraphs 
Same as previous approaches  our
method uses   convex program whose objective
function is not everywhere differentiable  We
exploit the nonuniqueness of the optimal solutions  and consider con dence intervals which
give the exact ranges that unlabeled vertices take
in any optimal solution  Moreover  we give
  much simpler approach for solving the convex program based on the subgradient method 
Our experiments on realworld datasets con rm
that our con dence interval approach on hypergraphs outperforms existing methods  and our
subgradient method gives faster running times
when the number of vertices is much larger than
the number of edges 

  Introduction
Given   dataset  similarity relationship between examples
can be represented by   graph in which each example is
represented by   vertex  While pairwise relationship between two vertices can be represented by an edge in   normal graph    higher order relationship involving multiple
vertices can be captured by   hyperedge  which means that
all the corresponding examples are similar to one another 
Hypergraphs have been used in several learning applications such as clustering of categorical data  Gibson et al 
  multilabel classi cation  Sun et al    Laplacian sparse coding  Gao et al    image classi cation  Yu et al    image retrieval  Huang et al   
mapping users across different social networks  Tan et al 
  and predicting edge labels in hypernode graphs  Ricatte et al   

 Equal contribution  University of Hong Kong  Hong Kong 
 This research was partially supported by the Hong Kong RGC
under the grant   Correspondence to  TH  Hubert Chan
 hubert cs hku hk 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

In this paper  we consider semisupervised learning on
an edgeweighted hypergraph               with   set
   
  of labeled vertices  whose labels are given by   
      The task is to predict the labels of the unlabeled vertices    with the working principle that vertices
contained in   hyperedge       are more similar to one
another if the edge weight we is larger  This problem is
also known as transductive inference and has been studied
in  Zhou et al    and  Hein et al   
However  the methods in  Zhou et al    have been
criticized by  Agarwal et al    because essentially
  hypergraph is converted into   normal graph  For instance  given   hyperedge   containing vertices    either
      clique is added between the vertices in    or  ii    star
is formed by adding   new vertex ve connecting every vertex in   to ve  Then    standard convex program using  
regularization potential function for normal graphs can be
applied  Zhu et al    By choosing appropriate edge
weights  it was shown in  Agarwal et al    that the two
approaches are equivalent to the following convex program
relaxation 

 cid 

 cid 

min  old      

 
 
subject to fu      

we

   
      

fu     
  

       

 fu   fv 
      
 

On the other hand  it was proposed in  Hein et al   
that the following regularization function is more suitable
to capture hyperedge expansion 

 new      

 
 

we    max
   

fu   min
   

fv 

 cid 

   

Indeed  it was shown in  Hein et al    that their approach outperforms  Zhou et al    on several datasets
from the UCI Machine Learning Repository  Lichman 
 
Loss Function  In  Hein et al      squared loss function was added by considering the convex program with objective function  new        cid       cid 
  on           
where       is   parameter to be tuned    
  is given by the
labeled vertices    and for the unlabeled vertices   
     

Rerevisiting Learning on Hypergraphs

  for

Figure   Example

The loss function allows errors in the labeled vertices  and
also ensures that the minimizer is unique  However  as  
result  unlabeled vertices have   tendency to acquire   values close to   This might remove useful information as
illustrated in the following example 
Example  In Figure   vertices          are labeled
as   and       is labeled as   Vertices       
  are unlabeled 
There
are three  undirected  edges 
              and          
each with unit weight 
By choosing      
squared loss function  the unique minimizer gives fx    
 
and fy     Hence  this solution gives no useful information regarding the label for vertex   
On the other hand  if we just use the objective function
 new     with the constraints fL     
   then in an optimal solution  fx    
  but fy could be anywhere in the
con dence interval    
    Hence  in this case  we could
  to predict   for vertex   
use the average value    
Our Contributions  In this paper  we revisit the approach
used in  Hein et al    and consider several extensions
and simpli cations  We summarize our results and give an
outline of the paper as follows 
  Uni ed Framework for Directed Hypergraphs  Inspired also from the recent result on Laplacians for directed normal graphs  Yoshida    we introduce   semisupervised learning framework using directed hypergraphs
that can capture higher order causal relationships  This notion of directed hypergraph was  rst introduced in  Gallo
et al    who considered applications in propositional
logic  analyzing dependency in relational database  and
traf   analysis  On   high level    directed hyperedge  
consists of   tail set Te pointing to   head set He such that
  vertex in Te labeled   implies that   vertex in He is
more likely to be labeled  
 Equivalently in terms of
its contrapositive    vertex in He labeled   implies that  
vertex in Te is more likely to be labeled   In Section  
we formally de ne the model and the corresponding potential function   An additional advantage of our potential
function is that there is no need to tune any parameters 
  Con dence Interval for Unlabeled Vertices  Observe
that the minimizer for our convex program might not be
unique  In Section   we introduce the concept of con 
dence interval for each unlabeled vertex that can be useful
for predicting its label  Furthermore  we provide an algorithm to calculate the con dence interval given an optimal
solution 

  Simpler Subgradient Method  Since the new potential
function is not everywhere differentiable but still convex 
we use the subgradient method  Shor et al    to obtain an estimated minimizer for label prediction  Inspired
by the diffusion processes used for de ning Laplacians in
hypergraphs  Louis    and directed graphs  Yoshida 
  in Section   we de ne   simple Markov operator
that returns   subgradient for   which is used to solve the
underlying convex program  We remark that our framework is very easy to understand  because it is   variation on
the wellknown gradient descent 
In contrast  the primaldual approach in  Hein et al   
considers the convex conjugate of the primal objective and
involves complicated update operations on the primal and
dual variables  The subgradient used in our approach gives
the update direction  and we can actually solve exactly the
same convex program with   much simpler method 
  Experimental Results on RealWorld Datasets 
In
Section   we revisit some datasets in the UCI Machine
Learning Repository  Lichman    and experiments
con rm that our prediction model based on con dence interval gives better accuracy than that in  Hein et al   
Our simpler subgradient method takes more iterations than
the primaldual method  Hein et al    but each iteration is much faster  Experiments show that overall both
methods have similar running times  and the subgradient
method has an advantage when the number of vertices is
much larger than the number of edges 
Moreover  using the DBLP dataset  Ley    our experiments also support that using directed hypergraphs to
capture causal relationships can improve the prediction accuracy  The experiments for directed hypergraphs are described in the full version 

  Preliminaries
We consider an edgeweighted directed hypergraph    
          with vertex set    with          edge set   and
weight function            Each hyperedge      
consists of   tail set Te     and   head set He    
 which are not necessarily disjoint  we use the convention
that the direction is from tail to head  For        we denote
      max     
In our application  each vertex       is supposed to have  
label in     Intuitively  the directed hypergraph attempts to capture the rule that for each edge        if there
is   vertex in Te having label   then it is more likely for
vertices in He to receive label   In terms of its contrapositive  if there is   vertex in He having label   then it
is more likely for vertices in Te to receive label  
We use     RV to denote   vector  where the coordi 

Rerevisiting Learning on Hypergraphs

nates are labeled by vertices in     For         we use
fU   RU to denote the vector restricting   to coordinates
in    In semisupervised learning  we consider   set      
         
of labeled vertices  which have labels   
Typically       cid       and the task is to assign   label in
    to each unlabeled vertex in            using
information from the directed hypergraph   
By relaxing labels to be in the interval     we consider
the following regularization potential function     RV  
  

 cid 

   

      

 
 

we         

  max     Te He fu   fv   

where       
maxu Te fu   minv He fv 
In particular  there is   penalty due to edge   only if some
vertex in Te receives   label larger than that of some vertex
in He  The convexity of   is proved in the full version 
Our approach is to consider the following convex program
to obtain an estimated minimizer            which can
be rounded to an integer solution for labeling all vertices 

min     

subject to fu      

fu     
  

 CP 

      
      

Since the   values for the labeled vertices   are  xed in
 CP  we also view     RN     as   function on the
  values of unlabeled vertices    We use OPT   RV to
denote the set of optimal solutions to  CP 
Trivial Edges  An edge       is trivial if there exist vertices     Te     and     He     such that   
     
      As trivial edges contribute constant towards
and   
the objective function   we shall assume that there are no
trivial edges in the convex program  CP 
Special Cases  Our directed hypergraph model can capture
other graph models as follows 

  Undirected Hypergraphs  For each hyperedge    we
can set Te   He to the corresponding subset of vertices 
  Undirected Normal Graphs  For each edge    
       we can set Te   He      Observe that
 cid 
in this case  the potential function becomes       
       wuv fu   fv  which is differentiable  and
hence   CP  can be solved by standard techniques
like gradient descent 

        is  The following relaxation is considered 
  
min  cid             

 cid 

 CP 

 
 

   fu     
  
subject to fu            

   

Observe that  CP  can also be expressed in the framework
of  CP  We simply consider an augmented hypergraph

 cid   such that all vertices   are treated as unlabeled  and for
each        we add   new vertex cid   with label   
undirected edge    cid    with weight     Then  it follows
for  cid   is exactly the same as  CP 

that the convex program  CP  for the augmented instance

  and   new

Challenges Ahead  We next outline how we resolve
the encountered challenges when we use  CP  for semisupervised learning 

  Unlike the case for normal graphs  the set OPT can
contain more than one optimal solution for  CP  In
Section   we prove some structural properties of the
convex program  and illustrate that each       has
some con dence interval from which we can predict
its label 
  The function   is not everywhere differentiable 
Hence  we use the subgradient method  Shor et al 
  In Section   we give   method to generate  
subgradient  which is inspired by the continuous diffusion processes for hypergraphs  Louis    and directed graphs  Yoshida    and our method can in
fact be viewed as   discretized version 

  Con dence Interval for Semisupervised

Learning

In general    minimizer for  CP  might not be unique 
Hence  we introduce the concept of con dence interval 
De nition    Con dence Interval  For each        
we de ne its con dence interval to be  mu  Mu  where
mu   minf OPT fu and Mu   maxf OPT fu  The con 
 dence intervals induce the lower and the upper con dence
vectors   cid   and  cid     RV   respectively 

In Section   we give the proof of the following lemma 
which states that the con dence vectors  cid   and  cid   are optimal solutions  and so are their convex combinations 

Lemma    Con dence Vectors Give Optimal Solutions  For any         the convex combination    cid    
       cid     OPT is optimal for  CP 

Soft Constraints 
In  Hein et al    each labeled
vertex       can also have some weight        
which can  for instance  indicate how trustworthy the label

Semisupervised Learning via Con dence Interval 
Lemma   suggests what one can do when  CP  has more
than one optimal solution  Speci cally  in Algorithm   the

Rerevisiting Learning on Hypergraphs

for labeled vertices  
RN   RN   either by Algorithm   or  

     cid      cid       OPT can be used for label
average vector  
prediction  We show that the con dence vectors  cid   and
 cid   can be recovered from any optimal solution     OPT 
which in turn can be estimated by the subgradient method
described in Section  
Algorithm   SemiSupervised Learning
  Input  Directed hypergraph               labels   
  Compute  estimated  con dence vectors    cid     cid      
 cid 
  Compute average vector        
  Compute threshold        
  for each       do
 cid fu    
if         then
 
 cid fu    
 
 
 
end if
 
  end for

     cid      cid    
        

else

 

  return  cid fN

FineTuning Parameters 
In view of Lemma   one
could further optimize the choice of         in de ning          cid          cid   in Line   Similarly  one could
pick the threshold   to be the  percentile of the sorted coordinates of       for some choice of         The parameters   and   can be tuned using standard techniques like
crossvalidation  However  to illustrate our concepts  we
keep the description simple without introducing too many
free parameters 

  Properties of Con dence Vectors

We derive some properties of the con dence vectors to
prove Lemma   The full proofs of Lemma   and  
are given in the full version 
Given   feasible solution     RV to  CP  we de ne the
following 
  Se       arg maxu Te fu   Te and Ie      

     Se    maxu Te fu and    Ie    minv He fv 

arg minv He fv   He 
Hence  we have             Se       Ie 
                   

  The set of active edges with respect to   is        

The following lemma states even though   minimizer for
 CP  might not be unique  there are still some structural
properties for any optimal solution 

Lemma    Active Edges in an Optimal Solution  Suppose   and   are optimal solutions to  CP  Then  for all
                      In particular  this implies
that the set of active edges                   in any op 

timal solution is uniquely determined  Hence  for       
we can de ne the corresponding  

          

De nition    Pinned Vertex  An unlabeled vertex   is
pinned in   solution     RV if there exist active edges  
and   cid          such that     Se     Ie cid     in which case
we say that the edges   and   cid  pin the vertex   under   

Lemma    Extending an Active Edge  Suppose
edge
          is active in an optimal solution    If He does
not contain   vertex labeled with  
then there exist
    Ie     and another active edge   cid          such that
the following holds 
    The edges   and   cid  pin   under             Se cid    
    If   is an optimal solution  then Ie       Se cid      

Ie      Se cid    and fu   gu 

An analogous result holds when Te does not contain any
vertex labeled with  
In particular  for any active edge        the extremal
values   Se    maxu Te fu and   Ie    minu He fu
are uniquely determined by any optimal solution   

Corollary    Pinned Vertices  In any optimal solution 
the set of pinned vertices is uniquely determined  We use
   to denote the set of labeled or pinned vertices in an
optimal solution  Then  for each        its value   
  in
any optimal solution is also uniquely determined 
From Corollary   the con dence interval for any       
contains exactly one value  namely the unique value   
  in
any optimal solution  The following lemma gives   characterization of an optimal solution 

Lemma   Characterization of Optimal Solutions  
solution   to  CP  is optimal iff the following conditions
hold 
    For each        fu     
  
    For each active edge        both the maximum
maxu Te fu and the minimum minv He fv are attained by vertices in   
    For each inactive edge        for all     Te and
    He  fu   fv 

Proof  We  rst observe that Corollary   states that the
values of the vertices in    are uniquely determined in any
optimal solution  Hence  any optimal solution must satisfy
the three conditions  We next show that the three conditions
implies that the objective value is optimal 
Once the values for vertices in    are  xed  Lemma  
and condition     implies that the contribution of all active
edges    are determined and are the same as any optimal
solution 

Rerevisiting Learning on Hypergraphs

Finally  condition     implies that edges not in    do
not have any contribution towards the objective function 
Hence  any solution satisfying the three conditions must be
optimal 
Deriving Con dence Vectors  To prove Lemma   we
de ne   procedure that returns   vector  cid         such that
for any optimal     OPT  we have      cid    Moreover 
we shall show that  cid     OPT and hence  cid   is the lower
con dence vector  The argument for the upper con dence
vector  cid   is similar  For the special case of undirected hypergraphs  the procedure can be simpli ed to Algorithm  
in Section  

Lemma    Con dence Vectors are Optimal  Proof of
Lemma   The con dence vectors  cid   and  cid   de ned in
De nition   are optimal solutions to  CP  This implies
that any of their convex combination is also optimal 

Proof  We give   procedure that returns   vector  cid   such
that at any moment during the procedure  the following invariant is maintained  for any     OPT       cid   
The following steps correspond to maintaining the conditions in Lemma  
    for       
    Initialization  For        set mv     
set mv     This satis es the invariant  because for any
    OPT and any        fv     
   
    Preserving Active Edges  For each        set
mv   max mv  maxe     He   Ie  Observe that
Lemma     implies that for any optimal     OPT  any
       and any     He  fv     Ie  Hence  the invariant
is maintained 
    Preserving Inactive Edges  While there is an inactive
edge        such that     Te      He and mu   mv 
set mv   mu  We argue why each such update preserves
the invariant  Consider any optimal     OPT  Before this
update  the invariant holds  Hence  we have mu   fu 
Moreover  Lemma   implies that fu   fv  Therefore 
after setting mv   mu  we still have mv   fv 
Finally  observe that after step     the coordinates of  cid  
can take at most   distinct values  Moreover  after each update in step     one coordinate of  cid   must increase strictly 
Hence  this procedure will terminate 
We next argue that  cid   is an optimal solution by checking
that it satis es the conditions in Lemma  
Condition     Observe that for each        mv is initialized to   
    Afterwards the value mv could only be increased  However  because the invariant holds when the
procedure terminates  it must be the case that mv     
  at
the end 
Condition     The procedure makes sure that at the end of

step     for every active edge        minv He mv can be
attained by some vertex in    Since only mv for       
can be increased in step     it follows that in the end  the
minimum can still be attained by some vertex in   
Next  consider     Te  where        For any optimal
solution    Lemma   implies that fu     Se  Hence 
the invariant implies that mu   fu     Se  Since condition     holds  this means that maxv Te mv can be attained
by some vertex in   
Condition     This is clearly satis ed because of the
whiletermination condition 
Therefore  we have  cid     OPT  as required 
The proof for the upper con dence vector  cid   is similar 
We omit the detailed proof and just give the corresponding
procedure to return  cid   
    Initialization  For        set Mv     
set Mv    
    Preserving Active Edges  For each        set Mv  
min Mv  mine     Te   Se 
    Preserving Inactive Edges  While there is an inactive
edge        such that     Te      He and Mu   Mv  set
Mu   Mv 
The same argument can show that for any optimal    
OPT  we have      cid    Moreover  we also have  cid    
OPT 

    for       

  Computing the Con dence Interval

As mentioned before  the proof of Lemma   implicitly
gives   procedure to compute the con dence vectors from
any optimal solution  For the special case of undirected
hypergraphs    simpli ed version of the procedure is given
in Algorithm  
Alternatively  we can try to solve the convex program  CP  for example using Algorithm   in Section  
from two initial feasible solutions to heuristically estimate
the con dence vectors  In Algorithm   one instance approaches an optimal solution from high   values and the
other from low   values 

  Subgradient Method via Markov Operator
Resolving Ties  Observe that     RN     is differentiable at fN   RN that has distinct coordinates  For the
purpose of computing   subgradient  we assume that there
is some global ordering   on   to resolve ties among coordinates with the same value  In particular  the vertices in  
having label   are the highest  and those in   labeled  
are the lowest  Hence  in this section  we may assume that
any arg max or arg min operator over   subset of vertices

Rerevisiting Learning on Hypergraphs

Algorithm   Con dence Intervals for Undirected Hypergraphs
  Input  Undirected hypergraph               label

  Let   be   solution of  CP  either by Algorithm   or
  For all         set           mv     Mv    

by PDHG method  Hein et al   

vector   

  and tolerance      
   cid                        
  while      cid        cid             cid    do
 cid      cid                      
  for each      cid   do

 
  end while

 

end for

           

    Mp        

    an arbitrary vertex in  
for each vertex       do

 
 
 
 
  end for
  for each vertex       do
  mp        
  end for
  for each edge       such that            do
 
 
 
 
  end for
  for each vertex       do
  mv   mp    Mv   Mp   
  end for
  return vectors    cid     cid    

mp      max mp       Ie 
Mp      min Mp       Se 

for each vertex       do

end for

will return   unique vertex 
We next de ne   Markov operator that is inspired from the
diffusion processes on hypergraphs  Louis    and directed graphs  Yoshida    in the context of de ning
Laplacians  We denote the projection operator      RV  
RN that takes     RV and returns the restricted vector
fN   RN  

Lemma   For          that is feasible in  CP 
the Markov operator Mf given in Algorithm   returns  
subgradient of     RN     at fN  

 Sketch  Observe that if fN   RN has distinct coProof 
ordinates  then   is differentiable at fN   and Mf gives exactly the gradient  which is the only possible subgradient in
this case  Observe that in our subgradient method application  we could imagine that at every iteration  in nitesimal
perturbation is performed on the current solution to ensure
that all coordinates are distinct  and ties are resolved according to our global ordering  

for labeled vertices  

Algorithm   Estimate con dence interval
  Input  Directed hypergraph               labels   
        RN with all entries
        RN with all entries

 

being  
being  

  Construct feasible    
  Construct feasible    
   cid     SGM    
 
   cid     SGM    
 
  return the vectors    cid     cid    

 

 

    RV for  CP 

Algorithm   Markov Operator     RV   RN
  Input  Directed hypergraph               feasible
  Construct symmetric matrix     RV      set      
  for each       such that            do
 
 
  Auv   Auv   we 
 The same is done for Avu because   is symmetric 
 
  end for
  Construct diagonal matrix     RN     set      
  for each       do

    arg maxu Te fu 
    arg minv He fv 

  Wuu  cid 

    Auv 

  end for
  return               

Hence  as the magnitude of the perturbation tends to zero 
if the global ordering   is preserved  then the gradient remains the same  which implies that the gradient is also the
subgradient when the perturbation reaches  
Using the Markov operator   as   subroutine to generate
  subgradient  we have the following subgradient method
 SGM   Shor et al   
Algorithm   Subgradient Method SGM    
  Input  Directed hypergraph               with laL for labeled vertices    initial feasible solution

    RN  

bels   
    RN   step size       
   
       
   Throughout the algorithm       

    
      

  is given by the

labeled vertices 

  has not  stabilized  do

  while Solution      
 

    Mf       RN  
    
  cid cid cid     
         
     
         

      

 

 

 

    

 cid cid cid 

 

 
  end while
  return      

Rerevisiting Learning on Hypergraphs

Stabilizing Condition  Our experiments in Section   suggest that it suf ces to run the solver for   short time  after
which   better feasible solution   does not improve the prediction accuracy 

  Experimental Results
Our experiments are run on   standard PC  In our graphs 
each point refers to   sample mean  and the height of the
vertical bar is the standard error of the mean 

  Undirected Hypergraph  Comparing Accuracy of

Prediction Methods

We show that our treatment of hypergraphs performs better
than the previously best method in  Hein et al   
Hypergraph Model  We use three datasets from the UCI
Machine Learning Repository  Lichman    mushroom  covertype  and covertype  As in  Hein et al 
  each dataset  ts into the hypergraph learning model
in the following way  Each entry in the dataset corresponds
to   vertex  which is labeled either   or   Moreover 
each entry has some categorical attributes  For each attribute and each realized value for that attribute  we form
  unitweight hyperedge containing all the vertices corresponding to entries having that attribute value  To summarize  below are the properties of the resulting hypergraphs 
covertype 
Dataset
        
       
 cid 
 
 
       
 

mushroom covertype 

 
 
 

 
 
 

 
 
 

Semisupervised Learning Framework  We compare
our semisupervised learning framework with that in  Hein
et al    which was previously the best  compared
to  Zhou et al    for instance  Speci cally  we compare the prediction accuracy of the following two prediction algorithms 

  Con dence Interval

 CI  We use hard constraints  CP  and con dence intervals for prediction 
as described in Algorithm   in Section  

  Hein et al  We implement the method described
in  Hein et al    which uses soft constraints  regularized version  plus  fold cross validation to determine the regularization parameter 

Testing Methodology  Since we focus on prediction accuracy  using either subgradient method or PDHG  Hein
et al    for solving the underlying convex programs in
each algorithm produces the same results  For each algorithm candidate  we try different sizes of labeled vertices
   where         ranges from   to   For each size  

of labeled vertices  we randomly pick   vertices from the
dataset to form the set   and treat the rest as unlabeled vertices  we resample if only one label   or   appears
in    For each size    we perform   trials to report the
average error rate together with its standard error 
Results  Our experiment can recover the results reported
in  Hein et al    The test error for the two algorithms
on the three datasets is presented in Figure   which
shows that our CI method consistently has lower test error
than the one in  Hein et al   

  Comparing Running Times of Solvers
Different Solvers  We compare the running times of the
following two convex program solvers 

  Subgradient Method  SG  proposed by us  Empiri 
    gives good
cally  the step size     
performance  For large       grows like  
  and so the
method converges  however  for small    we would
like   larger step size to speed up convergence 

min    

   

 

  PrimalDual Hybrid Gradient

in  Hein et al    We choose          
where   is the maximum degree 

  

 PDHG  proposed
 

Theoretical Analysis  Given   hypergraph with   vertices
and   edges  where the average size of an edge is    each
vertex on average appears in mk
  edges  For SG  we use  
heapbased data structure to maintain the vertices within  
hyperedge  Vertices attaining the maximum and the minimum value within   hyperedge can be retrieved in   
time  and   value update takes   log    time  In each iteration  at most    vertices will have their values updated 
Hence  in each iteration  SG takes time     mk
     log     
      
  log    In the description of PDHG in  Hein et al 
  each iteration takes   mk log    time  Hence  when
   cid     each iteration of SG will be signi cantly faster  although in general  the number of iterations required by the
subgradient method can be larger than that for PDHG 
Testing Methodology 
In each experiment  we consider
the hypergraph from one of the above three datasets  We
pick       vertices at random as the labeled vertices
   and form the corresponding convex program  CP  for
the two solvers  where the initial values for unlabeled vertices are chosen independently to be uniformly at random
from     To compare the performance  we run the
two solvers on the same convex program  and record each
trajectory of the objective value versus the time duration 
According to experience    seconds is good enough for
either solver to reach an almost optimal solution  and we
use the minimum value achieved by the two solvers after
  seconds as an estimate for the true optimal value OPT 
Then  we scan each trajectory  and for each relative gap

Rerevisiting Learning on Hypergraphs

Figure   Prediction Accuracy of CI vs Hein  et al 

Figure   Comparing Running Times of the Two Solvers

Figure   Test Error vs Relative Gap for the Two Solvers

                           we  nd the smallest time
    after which the objective value is at most  OPT away
from the estimate OPT  Each instance of the experiment is
repeated   times  with different sets of labeled vertices 
to obtain an average of those      and their standard error  For each relative gap   we also report the test error
for using   feasible solution that is  OPT away from the
presumed optimal value OPT 
Results  Both solvers have similar performance  As predicted by our theoretical analysis  we see in Figure  
that SG has an advantage when the number   of vertices
is much larger than the number   of edges  which is the
case for the the last dataset covertype  Moreover  in
Figure   we see that achieving   relative gap smaller than
  has almost no effect on improving the prediction accuracy  Hence  we can conclude that for either solver  it
takes roughly   to   seconds to produce   solution for
the underlying convex program that can give good predic 

tion accuracy 

  Directed Hypergraph  More Powerful
DBLP Dataset  We use the DBLP  Ley    dataset 
Each paper is represented by   vertex  We include papers
from year   to   from conferences belonging to the
following research areas to conduct our experiments 

ICML

STOC  FOCS

    papers from machine learning  ML  NIPS 
    papers from theoretical computer science  TCS 
    papers from database  DB  VLDB  SIGMOD
We perform the following prediction tasks      ML   vs
TCS   and     ML   vs DB  
The details of the experiment setup and the results are given
in the full version 

Rerevisiting Learning on Hypergraphs

Tan  Shulong  Guan  Ziyu  Cai  Deng  Qin  Xuzhen  Bu 
Jiajun  and Chen  Chun  Mapping users across networks
In AAAI  volby manifold alignment on hypergraph 
ume   pp     

Yoshida  Yuichi  Nonlinear laplacian for digraphs and its
applications to network analysis  In Proceedings of the
Ninth ACM International Conference on Web Search and
Data Mining  pp    ACM   

Yu  Jun  Tao  Dacheng  and Wang  Meng  Adaptive hypergraph learning and its application in image classi 
cation  IEEE Transactions on Image Processing   
   

Zhou  Dengyong  Huang  Jiayuan  and Sch olkopf  Bernhard  Learning with hypergraphs  Clustering  classi cation  and embedding  In Advances in neural information
processing systems  pp     

Zhu  Xiaojin  Ghahramani  Zoubin  and Lafferty  John   
Semisupervised learning using gaussian  elds and harmonic functions  In ICML  pp    AAAI Press 
 

References
Agarwal  Sameer  Branson  Kristin  and Belongie  Serge 
Higher order learning with graphs  In Proceedings of the
 rd international conference on Machine learning  pp 
  ACM   

Gallo  Giorgio  Longo  Giustino  Pallottino  Stefano  and
Nguyen  Sang  Directed hypergraphs and applications 
Discrete applied mathematics     

Gao  Shenghua  Tsang  Ivor WaiHung  and Chia  LiangTien  Laplacian sparse coding  hypergraph laplacian
sparse coding  and applications  Pattern Analysis and
Machine Intelligence  IEEE Transactions on   
   

Gibson  David  Kleinberg  Jon  and Raghavan  Prabhakar 
Clustering categorical data  An approach based on dynamical systems  Databases     

Hein  Matthias  Setzer  Simon  Jost  Leonardo  and RanThe total variation on
gapuram  Syama Sundar 
In Adhypergraphs learning on hypergraphs revisited 
vances in Neural Information Processing Systems  pp 
   

Huang  Yuchi  Liu  Qingshan  Zhang  Shaoting  and
Image retrieval via probabilisMetaxas  Dimitris   
In Computer Vision and Pattic hypergraph ranking 
tern Recognition  CVPR    IEEE Conference on  pp 
  IEEE   

Ley  Michael  Dblp  some lessons learned  Proceedings of

the VLDB Endowment     

Lichman     UCI machine learning repository    URL

http archive ics uci edu ml 

Louis  Anand  Hypergraph markov operators  eigenvalues
In Proceedings of the
and approximation algorithms 
FortySeventh Annual ACM on Symposium on Theory of
Computing  pp    ACM   

Ricatte  Thomas  Gilleron    emi  and Tommasi  Marc  Hypernode graphs for spectral learning on binary relations
In Joint European Conference on Machine
over sets 
Learning and Knowledge Discovery in Databases  pp 
  Springer   

Shor        Kiwiel  Krzysztof    and Ruszcay nski  Andrzej  Minimization Methods for Nondifferentiable
Functions  SpringerVerlag New York  Inc  New York 
NY  USA    ISBN  

Sun  Liang  Ji  Shuiwang  and Ye  Jieping  Hypergraph
In Prospectral learning for multilabel classi cation 
ceedings of the  th ACM SIGKDD international conference on Knowledge discovery and data mining  pp 
  ACM   

