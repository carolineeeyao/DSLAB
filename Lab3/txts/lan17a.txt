Conditional Accelerated Lazy Stochastic Gradient Descent

Guanghui Lan     Sebastian Pokutta     Yi Zhou     Daniel Zink    

Abstract

In this work we introduce   conditional accelerated lazy stochastic gradient descent algorithm
with optimal number of calls to   stochastic  rstorder oracle and convergence rate     
    improving over the projectionfree  Online FrankWolfe
based stochastic gradient descent of  Hazan and
Kale    with convergence rate     

   

  Introduction
The conditional gradient method  also known as  FrankWolfe algorithm  proposed in  Frank and Wolfe   
gained much popularity in recent years due to its simple
projectionfree scheme and fast practical convergence rates 
We consider the basic convex programming  CP  problem

     min
   

     

 

where     Rn is   closed convex set and           is  
smooth convex function such that       

kf               Lkx   yk             

 

The classic conditional gradient  CG  method solves   iteratively by minimizing   series of linear approximations of  
over the feasible set    More speci cally  given xk     
at the kth iteration  it updates xk according to the following
steps 

  Call

the  rstorder

    xk    xk  and set pk     xk 

 FO  oracle

to compute

  Call the linear optimization  LO  oracle to compute

yk   argminx Xhpk  xi 

 

  Set xk          xk     kyk for some         
 ISyE  Georgia Institute of Tech 
 Equal contribution
nology  Atlanta  GA  Correspondence to  Daniel Zink
 daniel zink gatech edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

Compared to most other  rstorder methods  such as     
gradient descent algorithms and accelerated gradient algorithms  Nesterov      the CG method is computationally cheaper in some cases  since it only requires the
solution of   linear optimization subproblem   rather than
an often costly projection onto the feasible region   
There has been extensive and fruitful research on the general class of linearoptimization based convex programming  LCP  methods  which covers the CG method and
its variants  and their application in machine learning      
 Ahipasaoglu and Todd    Bach et al    Beck and
Teboulle    Cox et al    Clarkson    Freund
and Grigas    Hazan    Harchaoui et al   
Jaggi      Jaggi and Sulovsk    Luss and
Teboulle    Shen et al    Hazan and Kale   
Lan    Lan and Zhou    Braun et al    It
should be noted that even the computational cost for LO
oracle to solve the linear optimization subproblem   is
high for some complex feasible regions  Recently  several
approaches have been considered to address this issue  Jaggi
demonstrated practical speed up for the CG method by approximately solving   in  Jaggi    Braun  Pokutta 
and Zink in  Braun et al    proposed   class of modi ed CG methods  namely the lazy conditional gradient
 LCG  algorithms  which calls   weak separation oracle
rather than solving the linear subproblem   in the classical
CG method  In fact  the weak separation oracle is computationally more ef cient than approximate minimization
used in  Jaggi    at the expense of not providing any
guarantee for function value improvement with respect to
  Furthermore  as shown in  Jaggi    Lan    the
total number of iterations for the LCP methods to  nd an
 solution of           point                            
cannot be smaller than    which is not improvable
even when the objective function   is strongly convex 
Improved complexity results can only be obtained under
stronger assumptions on the LO oracle or the feasible set
 see        Garber and Hazan    Lan    However 
the    bound does not preclude the existence of more
ef cient LCP algorithms for solving   Lan and Zhou in
 Lan and Zhou    proposed   class of conditional gradient sliding methods  CGS  which signi cantly improve
the complexity bounds in terms of the number of gradient
evaluations while maintaining optimal complexity bounds

Conditional Accelerated Lazy Stochastic Gradient Descent

for the LO oracle calls required by the LCP methods 
Inspired by  Braun et al    and  Lan and Zhou   
in this paper we focus on   class of modi ed LCP methods
that require only improving solutions for   certain separation problem rather than solving the linear optimization
subproblem   explicitly through LO oracle calls while
simultaneously minimizing the number of gradient evaluations when performing weak separation over the feasible set
   At  rst these two objectives seem to be incompatible as
 Braun et al    give up the dual guarantee to simplify
the oracle  while the dual guarantee of CG iterations is at the
core of the analysis in  Lan and Zhou    We overcome
this impasse by carefully modifying both techniques 
It should be mentioned that Hazan and Kale in  Hazan
and Kale    proposed the online FrankWolfe  OFW 
algorithm  which obtains    rate of convergence for
stochastic problems  Indeed  if we consider the objective
function                   for stochastic optimization 
the OFW method can be applied to solve   by viewing the
iteratively observed function ft as the current realization of
the true objective function         ft           Without
reevaluating the  sub gradients at the updated points  the
OFW obtains       bound for any  smooth or nonsmooth  objective functions  see Theorem   in  Hazan and
Kale    which implies    rate of convergence
in terms of the number of  sub gradient evaluations for
stochastic optimization  However  we can show that our
proposed algorithm obtains     resp     rate
of convergence for smooth  resp  nonsmooth  stochastic
problems  which is much better than the convergence rate of
the OFW method  We would like to stress that the stochastic
optimization bound in  Hazan and Kale    Theorem  
which gives   guarantee of    requires to reevaluate
all gradients at the current iterate and as such the number
of gradient evaluations required grows quadratically in   
Moreover  Hazan and Luo   proposed two methods for
solving the special case of Problem   of the form

min
   

        min
   

 
 

mXi 

fi   

which allows for   potentially smaller number of SFO evaluations than    the lower bound for the general
problem  The two methods Stochastic VarianceReduced
FrankWolfe  SVRF  and Stochastic VarianceReduced Conditional Gradient Sliding  STORC  are obtained by applying
the variance reduction idea of Johnson and Zhang  
and Mahdavi et al    to the CG method and the Stochastic CGS method respectively  Both algorithms however need
  certain number of exact  or full  gradient evaluations leading to   potentially undesirable dependence on the number
of examples   

Contributions
Our main contributions can be brie   summarized as follows  We consider stochastic smooth optimization  where
we have only access unbiased estimators of the gradients of
  via   stochastic  rstorder  SFO  oracle  By incorporating   modi ed LCG procedure  Braun et al    into  
modi ed CGS method  Lan and Zhou    we obtain  
new conditional accelerated lazy stochastic gradient descent
algorithm  CALSGD  and we show that the number of calls
to the weak separation oracle can be optimally bounded by
   while the optimal bound of    on the total
number of calls to the SFO oracle can be maintained  In
addition  if the exact gradients of   can be accessed by  
FO oracle  the latter bound can be signi cantly improved to
     In order to achieve the above we will present  
modi ed lazy conditional gradient method  and show that
the total number of iterations  or calls to the weak separation
oracle  performed by it can be bounded by    under
  stronger termination criterion       the primaldual gap
function 
We also consider strongly convex and smooth functions and
show that without enforcing any stronger assumptions on the
weak separation oracle or the feasible set    the total number of calls to the FO  resp  SFO  oracle can be optimally
bounded by   log    resp     for variants of the
proposed method to solve deterministic  resp  stochastic 
strongly convex and smooth problems  Furthermore  we
also generalize the proposed algorithms to solve an important class of nonsmooth convex programming problems
with   saddle point structure  By adaptively approximating the original nonsmooth problem via   class of smooth
functions  we are able to show that the deterministic version
of CALSGD can obtain an  solution within    number of linear operator evaluations and    number of
calls to the weak separation oracle  respectively  The former
bound will increase to    for nonsmooth stochastic
optimization 
Finally  we demonstrate practical speed ups of CALSGD
through preliminary numerical experiments for the video
colocalization problem  the structured regression problem
and quadratic optimization over the standard spectrahedron 
an extensive study is beyond the scope of this paper and
left for future work  In all cases we report   substantial
improvements in performance 
In the main body of the paper we focus on the stochastic
smooth case  several other results and their proofs have been
relegated to the Supplementary Material 

  Notation and Terminology
Let     Rn be   convex compact set  and       be the
norm associated with the inner product in Rn  For the sake

Conditional Accelerated Lazy Stochastic Gradient Descent

of simplicity  we often skip the subscript in the norm       
We de ne the diameter of the set   as

DX   DX       max

      kx   yk 

 

For   given norm       we denote its conjugate by ksk   
maxkxk hs  xi  For   linear operator     Rn   Rm  we
use kAk to denote its operator norm de ned as kAk  
maxkxk  kAxk  Let           be   convex function  we
denote its linear approximation at   by

lf                  hf        xi 

 

Clearly  if   satis es   then
        lf           

 
Notice that the constant   in   and   depends on      
Moreover  we say   is smooth with curvature at most    if

  ky   xk             

        lf           

               

 

   In
It is clear that if   is bounded  we have     LD 
the following we also use    to denote the set of strictly
positive reals 

  Conditional Accelerated Lazy Stochastic

Gradient Descent

We now present   new method for stochastic gradient descent that is based on the stochastic conditional gradient
sliding  SCGS  method and the parameterfree lazy conditional gradient  LCG  procedure from Section   which
we refer to as the Conditional Accelerated Lazy Stochastic
Gradient Descent  CALSGD  method 
We consider the stochastic optimization problem 

     min

                     

 

where       is   smooth convex function satisfying  

  The Algorithm
Throughout this section  we assume that there exists  
stochastic  rstorder  SFO  oracle  which for   search point
zk     outputs   stochastic gradient    zk         

      zk         zk 

  kF  zk         zk   

     

 
 

If       the stochastic gradient    zk     is the exact
gradient at point zk          zk         zk 
Our algorithmic framework is inspired by the SCGS method
by  Lan and Zhou    However  instead of applying
the classic CG method to solve the projection subproblem

appearing in the accelerated gradient  AG  method  the
CALSGD method utilizes   modi ed parameterfree LCG
algorithm  see Section   to approximately solve the subproblem      de ned in   and skips the computations
of the stochastic gradient         from time to time when
performing weak separation over the feasible region    The
main advantages of our method are that it does not solve
  traditional projection problem and achieves the optimal
bounds on the number of calls to the SFO and LOsepX
oracles  see Oracle   in Subsection   for solving problem
  To the authors  best knowledge  no such algorithms
have been developed before in the literature  we present the
algorithm below in Algorithm  

Algorithm   Conditional Accelerated Lazy Stochastic Gradient Descent  CALSGD 

Input  Initial point         iteration limit    and weak
separation oracle accuracy      
Let                 and                      
be given and set        
for                   do

     zk      

BkPBk

zk          yk     kxk 
gk    
xk   LCG gk     xk    
yk          yk     kxk 

 
 
 
 

where    zk                     Bk  are stochastic gradients computed by the SFO at zk 
end for
Output  yN 

We hasten to make some observations about the CALSGD
method  Firstly  we apply minibatches to estimate the
gradient at point zk  where the parameter  Bk  denotes the
batch sizes used to compute gk  It can be easily seen from
    and   that

Bk

   

     

  by induction  we have

  gk     zk      and   kgk     zk   
and hence gk is an unbiased estimator of   zk  In fact 
letting SBk  PBk
     zk           zk  from   and
      kSBk       zk    Bk       zk   
 
  kSBkk 
    kSBk   
    kF  zk    Bk       zk   
 
 hSBk     zk    Bk       zk   
    kSBk   
 
    kF  zk    Bk       zk   
 
    kF  zk           zk   
 PBk

    Bk 

Conditional Accelerated Lazy Stochastic Gradient Descent

BkPBk

that gk     zk   
SBk   implies the

       zk           zk     

which together with the fact
 
second relationship in  
Secondly  in view of the SCGS method in  Lan and Zhou 
  xk obtained in   should be an approximate solution to the gradient sliding subproblem

Bk

min

 

  kx   xk      

  Xn        hgk  xi     
such that for some        we have
     xk  xk   xi   hgk      xk   xk  xk   xi      
 
for all        If we solve the subproblem   exactly
             then CALSGD will reduce to the accelerated stochastic approximation method by  Lan     
However  by employing the LCG procedure  see Procedure   in Subsection   we only need to use   weak
separation oracle  but still maintaining the optimal bounds
on stochastic  rstorder oracle as in  Lan      Lan
and Zhou   
Thirdly  observe that the CALSGD method so far is conceptual only as we have not yet speci ed the LCG procedure
and the parameters  Bk          and     We will
come back to this issue after introducing the LCG procedure
and establishing its main convergence properties 

  The Parameterfree Lazy Conditional Gradient

Procedure

The classical CG method is   wellknown projectionfree
algorithm  which requires only the solution of   linear optimization subproblem   rather than the projection over
  per iteration  Therefore  it has computational advantages
over many other  rstorder methods when projection over
  being costly  The LCG procedure presented in this subsection    modi cation of the vanilla LCG method in  Braun
et al    goes several steps further than CG and even
vanilla LCG method  Firstly  it replaces LO oracle by  
weaker separation oracle LOsep  which is no harder than
linear optimization and often much simpler  Secondly  it
uses   stronger termination criterion  the FrankWolfe gap
 cf    than vanilla LCG method  Finally  it maintains
the same order of convergence rate as the CG and the vanilla
LCG method 
We present the LOsep oracle in Oracle   below 

Oracle   Weak Separation Oracle LOsepP           
Input      Rn linear objective        point        accuOutput        vertex with either   cT            

racy        objective value 

or       argmaxy   cT            

Observe that the oracle has two output modes  In particular 
Oracle    rst veri es whether there exists an improving
point       with the required guarantee and if so it outputs this point  which we refer it as   positive call  If no
such point exists the oracle certi es this by providing the
maximizer    which then also provides   new duality gap 
We refer to this case as   negative call  The computational
advantages of this oracle are that it can reuse previously
seen solutions   if they satisfy the improvement condition
and even if LO oracle has to be called  the optimization can
be terminated early once the improvement condition is satis 
 ed  Finally  the parameter   allows to only approximately
satisfy the improvement condition making separation even
easier  in our applications we set the parameter   slightly
larger than  
We present the LCG procedure based on  Braun et al   
below  We adapted the parameterfree version to remove
any dependence on hard to estimate parameters  For any
smooth convex function   we de ne its duality gap as

gap        gap      max

                 

 

Clearly  by convexity the duality gap is an upper bound on
              Given any accuracy parameter       the
LCG procedure solves minx       approximately with
accuracy        it outputs   point              gap       
Procedure   Parameterfree Lazy Conditional Gradients
 LCG  procedure
Input  access to gradients of smooth convex function  
       vertex  LOsepX weak linear separation oracle 
accuracy       duality gap bound  
Output         with bounded duality gap       gap     
 
      maxu                 
  for       to       do
 
 
 
 
 
 
 
 
 
 
  end for

end if
     argmin        ut    tvt 
ut           ut    tvt

vt   LOsepX   ut  xt       
if not   ut    ut   vt        then

     maxn    

    Update    

if         then
return      ut
end if

else

 

The LCG procedure is   parameterfree algorithm  Note
that while line search can be expensive in general  for our
subproblems  function evaluation is very cheap  The algorithm needs only one LO oracle call to estimate the
initial functional value gap at Line   Alternatively  this

Conditional Accelerated Lazy Stochastic Gradient Descent

can be also done approximately via binary search with
LOsep  The algorithm maintains   sequence      that
provides valid upper bounds for the functional value gap
at the current iterate        ut             see Theorem   of  Braun et al    and it halves the value
of    only when the current oracle call is negative  Finally 
our LCG procedure exits at Line   whenever LOsepX returns   negative call and         which ensures that
gap      maxy Xhr         yi    
Theorem   below provides   bound for the total number of iterations  or calls to the LOsepX oracle  that the
LCG procedure requires to generate   point        with
gap       
Theorem   Procedure   returns   point        such that
the duality gap at point    is bounded by        gap       
Furthermore  the total number of iterations    and hence
LOsepX calls  performed by Procedure   is at most

 

     
           

 
           

 

 

         
with      llog  

       log  

   

 

Proof  From the observations above  it is clear that the duality gap at the output point    is bounded by  
Also observe that the procedure calls LOsepX once per
iteration  In order to demonstrate the bound in   we split
the LCG procedure into two phases  and bound the number
of iterations separately for each phase  Let    denote the
curvature of the smooth convex function  
We say Procedure   is in the  rst phase whenever        
In view of Theorem   in  Braun et al    it is clear that
the number of iterations in the  rst phase can be bounded as

      llog  

          

    log  
   

Procedure   enters the second phase when         Again
with the argumentation in Theorem   in  Braun et al 
  we obtain that the total number of positive calls in
this phase can be bounded by    
  if      or by  
if         Moreover  the procedure exits whenever the
current LOsepX oracle call is   negative call  Hence  the
number of iterations in the second phase can be bounded by

 

        

     

      

 
     

Thus  our bound in   can be obtained from the above two
bounds plus one more LO oracle call at Line  

  The Convergence Properties of CALSGD
This subsection is devoted to establishing the main convergence properties of the CALSGD method  Since the
algorithm is stochastic  we will establish the convergence
results for  nding   stochastic  solution         point       
                         We  rst state   simple technical
result from  Lan and Zhou    Lemma   that we will
use 
Lemma   Let wt                     be given  Also
let us denote

Wt  

    wt Wt 

     
     

Suppose that Wt     for all       and that the sequence
      satis es

         wt      Bt 
Then for any            we have

               

     Wk   wl

Wl

     Pk

   

Bi

Wi   

Theorem   describes the main convergence properties of
the CALSGD method  cf  Algorithm   The proof of this
theorem can be found in the Supplementary material   
Theorem   Let    be de ned as follows 

    

     
               

 

Suppose that     and     in the CALSGD algorithm
satisfy
 

      and                

   If

    

          
   

       

 

then under assumptions   and   we have
      yk                
   

    

 

 

kXi       

  

   

 iBi          

 

where    is an arbitrary optimal solution of   and
DX is de ned in  

   If

    

          
   

 
 rather than   is satis ed  then the result in part   
holds by replacing    kD 
  with  kkx         in
the  rst term of the RHS of  

       

Conditional Accelerated Lazy Stochastic Gradient Descent

   Under the assumptions in part    or    the number of
inner iterations performed at the kth outer iterations
is bounded by

bound in   then immediately follows from this observation and the fact that the number of calls to the SFO oracle
is bounded by

 

   
         kD 

 

  

Tk      kD 
with      llog

  
 

 kD 

  

Xm   log   

 
  

   

    kD 
  
     kD 
  
 

 

Now we provide two different sets of parameters
        and  Bk  which lead to optimal complexity bounds on the number of calls to the SFO and
LOsepX oracles 
Corollary   Suppose that           and  Bk 
in the CALSGD method are set to

       

          

         LD 

      

 

and Bk       

           

    

 

and we assume kf     is bounded for any optimal solution    of   Under assumptions   and   we have
      yk             LD 
 
As   consequence  the total number of calls to the SFO and
LOsepX oracles performed by the CALSGD method for
 nding   stochastic  solution of   respectively  can be
bounded by

              

       LD 

 

 

    LD 

 

       

     

 

and

    LD 

 

 

log LD 

    LD 

 

 

    with probability      

 

Proof  It can be easily seen from   that   holds  Also
note that by   we have

    

 

        

 

and hence

    

  

   Lk   

  

 

which implies that   holds  It can also be easily checked
from   and   that
     kLD 

 iBi          kLD 
   

    

  

  

 

 

 

Using the bound in   we obtain   which implies that
the total number of outer iterations   can be bounded by

   under the assumptions   and   The

  

Pk
  pLD 

  Pk

PN
  Bk  PN

  

   

    
 

          

     
 

    

We now provide   good estimation for   
   cf  Line   in
LCG procedure  at the kth outer iteration  In view of the
de nition of   

  and     cf    we have 

         xk  xk    xi   hgk  xk    xi 
  

    

 Bk

 Bk

   

  by Cheby 

Moreover  let Ak   kgk     zk           
shev   inequality and   we obtain 
Prob Ak    kgk   zk 
               
which implies that Prob TN
 Ak        Hence  by
CauchySchwarz and triangle inequalities  we have with
probability      
    hgk     zk  xk    xi   hf zk  xk    xi 
  
  kf zk             kf      DX

  

 Bk

       
    

        LD 

    kf     DX 

 

where the last inequality follows from   and  
   Therefore  it
Note that we always have      kD 
follows from the bound in     and   that the total
number of inner iterations can be bounded by

 

      log   

 
  

          kf     

LDX  

PN
  Tk  PN

 

  

  
 

 kD 
 

   kD 

    log
    
NXk  log      
            
      log    

 

             
which implies that our bound in  

We now provide   slightly improved complexity bound on
the number of calls to the SFO oracle which depends on
the distance from the initial point to the set of optimal solutions  rather than the diameter DX  In order to obtain
this improvement  we need to estimate      kx       
and to    the number of iterations   in advance  This
result will play an important role for the analysis of the
CALSGD method to solve strongly convex problems  see
Supplementary Material   

Conditional Accelerated Lazy Stochastic Gradient Descent

       

Corollary   Suppose that there exists an estimate   
     kx               DX  Also assume that the outer
iteration limit       is given  If
         
and Bk          
      yN              LD 

         

Under assumptions   and  

               

          LD 
     

 

 

    
 

 

As   consequence  the total number of calls to the SFO and
LOsepX oracles performed by the CALSGD method for
 nding   stochastic  solution of   respectively  can be
bounded by

    LD 

 

       

     

 

 

and  

Proof  The proof is similar to Corollary   and hence
details are skipped 

It should be pointed out that the complexity bound for the
number of calls to the LOsep oracle in   is established
with probability       However  the probability parameter
  only appears in the nondominant term 

  Experimental Results
We present preliminary experimental results showing the
performance of CALSGD compared to OFW for stochastic
optimization  As examples we use the video colocalization
problem  which can be solved by quadratic programming
over   path polytope  different structured regression problems  and quadratic programming over the standard spectrahedron  In all cases we use objective functions of the
form kAx   bk  with     Rm           examples over  
feasible region of dimension    For comparability we use  
batch size of   for all algorithms to compute each gradient and the full matrix   for the actual objective function
values  All graphs show the function value using   logscale
on the vertical axis 
In Figure   we compare the performance of three algorithms 
CALSGD  SCGS and OFW  As described above SCGS is
the nonlazy counterpart of CALSGD  In the four graphs
of Figure   we report the objective function value over the
number of iterations  the wall clock time in seconds  the
number of calls to the linear oracle  and the number of
gradient evaluations in that order  In all these measures  our
proposed algorithms outperform OFW by multiple orders
of magnitude  As expected in number of iterations and
number of gradient evaluations both versions CALSGD and
SCGS perform equally well  however in wall clock time and

Figure   Performance of CALSGD and its nonlazy variant SCGS
on   structured regression problem compared to OFW  The feasible region is the  owbased formulation of the convex hull of
Hamiltonian cycles on   nodes and has dimension      

in the number of calls to the linear oracle we observe the
advantage of the weaker LOsep oracle over LO 
In Figure   and   we show the performance of CALSGD
on one video colocalization instance and one semide nite
convex programming instance  Due to space limitations
we only report the function value over the number of iterations and wall clock time in seconds  see Supplementary
Material   for   detailed analysis as well as more examples 

Implementation details  Finally  we provide details of
the implementation of LOsep  In the case of the structured
regression problems and the quadratic optimizations over
the path polytope instances  we used Gurobi as   solver
and included callbacks to terminate whenever the required
improvement  given by     is reached  our approach is
one out of many and other approaches could have been used
equally well  If the solver does not  nd   good enough
solution  it returns with   lower bound on the Wolfe gap 
which we use to update     In the case of convex programming over the feasible region Sn        Rn        
  and tr        we compute   maximal eigenvector of
the gradient  which is   matrix in this case  and use the rank 
  factor of the maximal eigenvector  which is an optimal
point  In this case  there is no early termination 
However  in all cases  we use caching       we store all
previously seen points and check if any of them satis es
the improvement guarantee  If that is the case we do not
call Gurobi or the maximal eigenvector routine  The size of
the cache is very small in all experiments  alternatively one
could use cache strategies such as      kpaging 

 Iterations FunctionvalueCALSGDSCGSOFW Wallclocktime FunctionvalueCALSGDSCGSOFW LPcalls FunctionvalueCALSGDSCGSOFW Gradientevaluations FunctionvalueCALSGDSCGSOFWConditional Accelerated Lazy Stochastic Gradient Descent

Figure   Two small video colocalization instances  On the
left  road paths DC   instance        and    
  On the right  road paths DC   instance     
  and       Observe   signi cant difference in function value of multiple orders of magnitude after   few seconds 

Figure   Performance of CALSGD and OFW on   medium sized
convex programming instance with feasible region Sn       
Rn           tr        with       and      
Similar to the results before in both iterations and wall clock time
our method performs better 

Figure   Performance of CALSGD compared to OFW on   small
video colocalization instance  The dimension of the underlying
path polytope is       the time limit is   seconds  Our
algorithm performs signi cantly better both in number of iterations
as well as in wall clock time 

Figure   Two medium sized video colocalization instances  On
the left  road paths DE   instance        and
      On the right  road paths DE   instance
       and       Similar results as in Figure  

Figure   Structured regression problem over the convex hull of
all Hamiltonian cycles of   graph on   nodes        on the
left and   nodes        on the right  We used   density of
      for   and       On both instances CALSGD
achieves lower values much faster  both in number of iterations as
well as in wall clock time 

 Iterations FunctionvalueCALSGDOFW Iterations FunctionvalueCALSGDOFW Wallclocktime FunctionvalueCALSGDOFW Wallclocktime FunctionvalueCALSGDOFW Iterations FunctionvalueCALSGDOFW Iterations FunctionvalueCALSGDOFW Wallclocktime FunctionvalueCALSGDOFW Wallclocktime FunctionvalueCALSGDOFW Iterations FunctionvalueCALSGDOFW Wallclocktime FunctionvalueCALSGDOFW Iterations FunctionvalueCALSGDOFW Wallclocktime FunctionvalueCALSGDOFW Iterations FunctionvalueCALSGDOFW Iterations FunctionvalueCALSGDOFW Wallclocktime FunctionvalueCALSGDOFW Wallclocktime FunctionvalueCALSGDOFWConditional Accelerated Lazy Stochastic Gradient Descent

Acknowledgements
We would to thank Elad Hazan for providing references 
Research reported in this paper was partially supported by
NSF CAREER award CMMI 

References
   Ahipasaoglu and    Todd    Modi ed FrankWolfe Algorithm for Computing MinimumArea Enclosing Ellipsoidal Cylinders  Theory and Algorithms  Computational
Geometry     

   Bach     LacosteJulien  and    Obozinski  On the equivalence between herding and conditional gradient algorithms  In the  th International Conference on Machine
Learning   

   Beck and    Teboulle    conditional gradient method
with linear rate of convergence for solving convex linear
systems  Math  Methods Oper  Res     
   Braun     Pokutta  and    Zink  Lazifying conditional
gradient algorithms  arXiv preprint arXiv 
 

   Chen     Lan  and    Ouyang  Optimal primaldual
methods for   class of saddle point problems  SIAM
Journal on Optimization     

      Clarkson  Coresets  sparse greedy approximation  and
the frankwolfe algorithm  ACM Trans  Algorithms   
  Sept   

   Cox     Juditsky  and       Nemirovski  Dual subgradient
algorithms for largescale nonsmooth learning problems 
Manuscript  School of ISyE  Georgia Tech  Atlanta  GA 
  USA    submitted to Mathematical Programming  Series   

   Frank and    Wolfe  An algorithm for quadratic programming  Naval Research Logistics Quarterly   
 

      Freund and    Grigas  New Analysis and Results for

the FrankWolfe Method  ArXiv eprints  July  

   Garber and    Hazan    Linearly Convergent Conditional Gradient Algorithm with Applications to Online
and Stochastic Optimization  ArXiv eprints  Jan  
   Ghadimi and    Lan  Optimal stochastic approximation
algorithms for strongly convex stochastic composite optimization       generic algorithmic framework  SIAM
Journal on Optimization     

   Ghadimi and    Lan  Optimal stochastic approximation
algorithms for strongly convex stochastic composite optimization  II  shrinking procedures and optimal algorithms 
SIAM Journal on Optimization     

Gurobi Optimization  Gurobi optimizer reference manual version     URL https www gurobi 
com documentation refman 

   Harchaoui     Juditsky  and       Nemirovski  Conditional gradient algorithms for machine learning  NIPS
OPT workshop   

   Hazan  Sparse approximate solutions to semide nite
programs  In    Laber     Bornstein     Nogueira  and
   Faria  editors  LATIN   Theoretical Informatics  volume   of Lecture Notes in Computer Science 
pages   Springer Berlin Heidelberg    ISBN
 

   Hazan and    Kale  Projectionfree online learning  arXiv

preprint arXiv   

   Hazan and    Luo  Variancereduced and projectionfree stochastic optimization  In Proceedings of The  rd
International Conference on Machine Learning  pages
   

   Jaggi 

Sparse Convex Optimization Methods for
Machine Learning  PhD thesis  ETH   rich   
http dx doi org ethza 

   Jaggi  Revisiting frankwolfe  Projectionfree sparse
convex optimization  In the  th International Conference on Machine Learning   

   Jaggi and    Sulovsk    simple algorithm for nuclear
norm regularized problems  In the  th International
Conference on Machine Learning   

   Johnson and    Zhang  Accelerating stochastic gradient
descent using predictive variance reduction  In Advances
in Neural Information Processing Systems  pages  
   

   Joulin     Tang  and    FeiFei  Ef cient image and
video colocalization with frankwolfe algorithm  In European Conference on Computer Vision  pages  
Springer   

   Lan  Convex optimization under inexact  rstorder information  Ph    dissertation  School of Industrial and
Systems Engineering  Georgia Institute of Technology 
Atlanta  GA   USA   

   Lan  An optimal method for stochastic composite optimization  Mathematical Programming   
 

   Lan  The complexity of largescale convex programming
under   linear optimization oracle  Technical Report 
  Available on http www optimizationonline org 

Conditional Accelerated Lazy Stochastic Gradient Descent

   Lan and    Zhou  Conditional gradient sliding for convex
optimization  OptimizationOnline preprint    

   Luss and    Teboulle  Conditional gradient algorithms
for rank one matrix approximations with   sparsity constraint  SIAM Review     

   Mahdavi     Zhang  and    Jin  Mixed optimization for
smooth functions  In Advances in Neural Information
Processing Systems  pages    

      Nesterov    method for unconstrained convex minimization problem with the rate of convergence     
Doklady AN SSSR     

      Nesterov  Introductory Lectures on Convex Optimization    basic course  Kluwer Academic Publishers  Massachusetts   

      Nesterov  Smooth minimization of nonsmooth functions  Mathematical Programming     

   Shen     Kim     Wang  and    van den Hengel  Positive semide nite metric learning using boostinglike algorithms  Journal of Machine Learning Research   
   

