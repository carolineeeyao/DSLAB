Uniform Deviation Bounds for kMeans Clustering

Olivier Bachem   Mario Lucic      Hamed Hassani   Andreas Krause  

Abstract

Uniform deviation bounds limit the difference between   model   expected loss and its loss on  
random sample uniformly for all models in   learning problem  In this paper  we provide   novel
framework to obtain uniform deviation bounds
for unbounded loss functions  As   result  we
obtain competitive uniform deviation bounds for
kMeans clustering under weak assumptions on
the underlying distribution  If the fourth moment

  com 
  rate 

is bounded  we prove   rate of       
pared to the previously known       

We further show that this rate also depends on the
kurtosis   the normalized fourth moment which
measures the  tailedness  of the distribution  We
also provide improved rates under progressively
stronger assumptions  namely  bounded higher
moments  subgaussianity and bounded support of
the underlying distribution 

  Introduction
Empirical risk minimization        the training of models on
   nite sample drawn       from an underlying distribution
  is   central paradigm in machine learning  The hope is
that models trained on the  nite sample perform provably
well even on previously unseen samples from the underlying distribution  But how many samples   are required
to guarantee   low approximation error   Uniform deviation bounds provide the answer  Informally  they are the
worstcase difference across all possible models between
the empirical loss of   model and its expected loss  As such 
they determine how many samples are required to achieve  
 xed error in terms of the loss function  In this paper  we
consider the popular kMeans clustering problem and provide uniform deviation bounds based on weak assumptions
on the underlying data generating distribution 

 Department of Computer Science  ETH Zurich  Correspon 

dence to  Olivier Bachem  olivier bachem inf ethz ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

Related work  Traditional VapnikChervonenkis theory
provides tools to obtain uniform deviation bounds for binary concept classes such as classi cation using halfspaces
 Vapnik   Chervonenkis    While these results have
been extended to provide uniform deviation bounds for sets
of continuous functions bounded in      Haussler   
Li et al    these results are not readily applied to kMeans clustering as the underlying loss function in kMeans
clustering is continuous and unbounded 
In their seminal work  Pollard   shows that kMeans
clustering is strongly consistent       that the optimal cluster
centers on   random sample converge almost surely to the
optimal centers of the distribution under   weak assumption  This has sparked   long line of research on cluster
stability  BenDavid et al    Rakhlin   Caponnetto 
  Shamir   Tishby      which investigates the
convergence of optimal parameters both asymptotically and
for  nite samples  The vector quantization literature offers
insights into the convergence of empirically optimal quantizers for kMeans in terms of the objective    minimax

has bounded support  Linder et al    Bartlett et al 

rate of       
  is known if the underlying distribution
    better rate of      may be achieved for  

nite support  Antos et al    or under both bounded
support and regularity assumptions  Levrard et al   
BenDavid   provides   uniform convergence result for
center based clustering under   bounded support assumption
and Telgarsky   Dasgupta   prove uniform deviation
bounds for kMeans clustering if the underlying distribution
satis es moment assumptions  see Section  
Empirical risk minimization with fattailed losses has been
studied in Mendelson et al    Mendelson  
Gr unwald   Mehta   and Dinh et al    Dinh et al 
  provide fast learning rates for kMeans clustering
but under stronger assumptions than the ones considered in
this paper  Guarantees similar to uniform deviation bounds
can be obtained using importance sampling in the context
of coreset construction  Bachem et al    Lucic et al 
   
Our contributions  We provide   new framework to obtain
uniform deviation bounds for unbounded loss functions  We
apply it to kMeans clustering and provide uniform devi 

ation bounds with   rate of       

  for  nite samples

Uniform Deviation Bounds for kMeans Clustering

under weak assumptions  In contrast to prior work  our
bounds are all scaleinvariant and hold for any set of   cluster centers  not only for   restricted solution set  We show
that convergence depends on the kurtosis of the underlying distribution  which is the normalized fourth moment
and measures the  tailedness  of   distribution  If bounded
higher moments are available  we provide improved bounds
that depend upon the normalized higher moments and we
sharpen them even further under the stronger assumptions
of subgaussianity and bounded support 

  Problem statement for kMeans
We  rst focus on uniform deviation bounds for kMeans
clustering  and defer the  more technical  framework for
unbounded loss functions to Section   We consider the
ddimensional Euclidean space and de ne
  Qkx   qk 

          min

 

for any     Rd and any  nite set     Rd  Furthermore 
slightly abusing the notation  for any        Rd  we set
 
                   kx   yk 
Statistical kMeans  Let   be any distribution on Rd with
    EP     and     EP            For any set
    Rd of       cluster centers  the expected quantization
error is given by EP         The goal of the statistical
kMeans problem is to  nd   set of   cluster centers such
that the expected quantization error is minimized 
Empirical kMeans  Let   denote    nite set of points in
Rd  The goal of the empirical kMeans problem is to  nd
  set   of   cluster centers in Rd such that the empirical
quantization error        is minimized  where

        

       

 

    Xx  

Empirical risk minimization  In practice  the underlying
data distribution   in statistical learning problems is often
unknown  Instead  one is only able to observe independent
samples from     The empirical risk minimization principle
advocates that  nding   good solution on   random sample Xm also provides provably good solutions to certain
statistical learning problems if enough samples   are used 
Uniform deviation bounds  For kMeans  such   result
may be shown by bounding the deviation between the expected loss and the empirical error      

 Xm      EP          

uniformly for all possible clusterings     Rd    If this
difference is suf ciently small for   given    one may then
solve the empirical kMeans problem on Xm and obtain
provable guarantees on the expected quantization error 

  Uniform deviation bounds for kMeans
  simple approach would be to bound the deviation by an
absolute error        to require that

 

 Xm      EP            

 
 

uniformly for   set of possible solutions  Telgarsky   Dasgupta    However  in this paper  we provide uniform
deviation bounds of   more general form  For any distribution   and   sample of                     points  we
require that with probability at least      
 
   

 Xm      EP          

 EP        
uniformly for all     Rd    The terms on the righthand
side may be interpreted as follows  The  rst term based on
the variance   corresponds to   scaleinvariant  additive
approximation error  The second term is   multiplicative approximation error that allows the guarantee to hold even for
solutions   with   large expected quantization error  Similar
additive error terms were recently explored by Bachem et al 
    in the context of seeding for kMeans 
There are three key reasons why we choose   over  
First    is not scaleinvariant and may thus not hold for
classes of distributions that are equal up to scaling  Second    may not hold for an unbounded solution space      
Rd    Third  we can always rescale   to unit variance and

restrict ourselves to solutions   with EP            
Then    implies   for   suitable transformation of    
Importance of scaleinvariance  If we scale all the points
in   data set   and all possible sets of solutions   by some
    then the empirical quantization error is scaled by
  Similarly  if we consider the random variable    where
        then the expected quantization error is scaled by
  At the same time  the kMeans problem remains the
same  an optimal solution of the scaled problem is simply  
scaled optimal solution of the original problem  Crucially 
however  it is impossible to achieve the guarantee in   for
distributions that are equal up to scaling  Suppose that  
holds for some error tolerance   and sample size   with
probability at least       Consider   distribution   and  
solution     Rd   such that with probability at least  

   Xm      EP          

for some       For    pa  let    be the distribution of
the random variable    where       and let  Xm consist of
  samples from      De ning                  we have
with probability at least  

   Xm           Phd                  

 For example  let   be   nondegenerate multivariate normal

distribution and   consist of   copies of the origin 

Uniform Deviation Bounds for kMeans Clustering

which contradicts   for the distribution    and the solution     Hence    cannot hold for both   and its scaled
transformation     
Unrestricted solution space One way to guarantee scaleinvariance would be require that

 

 Xm      EP            

for all     Rd    However  while   is scaleinvariant  it is
also impossible to achieve for all solutions   as the following example shows  For simplicity  consider the  Means
problem in  dimensional space and let   be   distribution
with zero mean  Let Xm denote   independent samples
from   and denote by   the mean of Xm  For any  nite   
suppose that       with high probability  and consider  
solution   consisting of   single point        We then have

 Xm      EP       
 Xm                     
 Xm                          
 Xm                

 

Since       with high probability  clearly this expression
diverges as       and thus   cannot hold for arbitrary
solutions     Rd    Intuitively  the key issue is that both
the empirical and the statistical error become unbounded as
      Previous approaches such as Telgarsky   Dasgupta
  solve this issue by restricting the solution space from
Rd   to solutions that are no worse than some threshold  In
contrast  we allow the deviation between the empirical and

the expected quantization error to scale with EP        

Arbitrary distributions  Finally  we show that we either
need to impose assumptions on   or equivalently make the
relationship between      and   in   depend on the underlying distribution     Suppose that there exists   sample size
       an error tolerance         and   maximal failure
probability         such that   holds for any distribution
    Let   be the Bernoulli distribution on       with
             for        
      By design  we have       
            and EP                 Furthermore 
with probability at least   the set Xm of   independent
samples from   consists of   copies of   point at one 
Hence    implies that with probability at least      

 Xm    EP           EP       

since     EP        However  with probability at
least   we have  Xm      which would imply       and
thus lead to   contradiction with        

 For example  if   is the standard normal distribution 

  Key results for kMeans
In this section  we present our main results for kMeans and
defer the analysis and proofs to Sections  

  Kurtosis bound
Similar to Telgarsky   Dasgupta   the weakest assumption that we require is that the fourth moment of
       for       is bounded  Our results are based on the
kurtosis of   which we de ne as

     

EP       

 

 

The kurtosis is the normalized fourth moment and is   scaleinvariant measure of the  tailedness  of   distribution  For
example  the normal distribution has   kurtosis of   while
more heavy tailed distributions such as the tStudent distribution or the Pareto distribution have   potentially unbounded kurtosis    natural interpretation of the kurtosis is
provided by Moors   For simplicity  consider   data
set with unit variance  Then  the kurtosis may be restated as
the shifted variance of            

      Var           

This provides   valuable insight into why the kurtosis is relevant for our setting  For simplicity  suppose we would like

to estimate the expected quantization error EP       
by the empirical quantization error  Xm  on    nite
sample Xm  Then  the kurtosis measures the dispersion
of        around its mean EP        and provides  
bound on how many samples are required to achieve an error
of   While this simple example provides the key insight
for the trivial solution       it requires   nontrivial
effort to extend the guarantee in   to hold uniformly for
all solutions     Rd   
With the use of   novel framework to learn unbounded loss
functions  presented in Section   we are able to provide
the following guarantee for kMeans 
Theorem    Kurtosis  Let           and        Let  
be any distribution on Rd with kurtosis         For
 
             log      log
   
let                    xm  be   independent samples from
    Then  with probability at least       for all     Rd  
         EP          
 EP          
 While our random variables       are potentially multivariate  it suf ces to consider the behavior of the univariate random
variable        for the assumptions in this section 
 This is   hypothetical exercise as EP            by de 

sign  However  it provides an insight to the importance of the
kurtosis 

       

   

 

 
 

 

 

Uniform Deviation Bounds for kMeans Clustering

The proof is provided in Section   The number of suf 
cient samples

         

 dk log     log

 

 

is linear in the kurtosis     and the dimensionality    nearlinear in the number of clusters   and  
    and quadratic in
    Intuitively  the bound may be interpreted as follows 
 

     
  samples are required such that the guarantee holds
for   single solution     Rd    Informally    generalization of the Vapnik Chervonenkis dimension for kMeans
clustering may be bounded by   dk log    and measures
the  complexity  of the learning problem  The multiplicative dk log     log  
  term intuitively extends the guarantee
uniformly to all possible     Rd   
Comparison to Telgarsky   Dasgupta   While we
require   bound on the normalized fourth moment       the
kurtosis  Telgarsky   Dasgupta   consider the case
where all unnormalized moments up to the fourth are uniformly bounded by some        

EP              

         

 

sup

 

They provide uniform deviation bounds for all solutions  

          

Telgarsky   Dasgupta   bound this deviation by

such that either            or EP             for some
      To compare our bounds  we consider   data set with
unit variance and restrict ourselves to solutions     Rd  
with an expected quantization error of at most the variance 
     EP                 Consider the deviation
  Rd   EP                EP          
  
      
pm dk log   dm    log
          
   
  we improve it to          
 

The key difference lies in how   scales with the sample
size    While Telgarsky   Dasgupta   show   rate of

  dk log     log

In contrast  our bound in Theorem   implies

         

  Bounded higher moments
The tail behavior of        may be characterized by the
moments of     For        consider the standardized pth
moment of         

 

 Mp   EP          

  

 

 

 

 

 

 
 

   

Theorem   provides an improved uniform deviation bound
if   has bounded higher moments 
Theorem    Moment bound  Let                
and        Let   be any distribution on Rd with  nite pth
order moment bound  Mp     for                 For
    max     
   with
   
   
           Mp
              log      log
 
let                    xm  be   independent samples from
    Then  with probability at least       for all     Rd  
         EP          
 EP          
         Mp
   
   
     
is only of     
   compared to near linear for   kurtosis
   

samples  With higher order moment bounds  it is easier to
achieve high probability results since the dependence on  
 

The proof is provided in Section   Compared to the
previous bound based on the kurtosis  Theorem   requires

   dk log     log

bound  The quantity  Mp
  may be interpreted as   bound on
the kurtosis     based on the higher order moment  Mp since
Hoelder   inequality implies that        Mp
    While the
result only holds for                   it is trivially
extended to        Consider Theorem   with          
  

 

 
 

 

 

 

 

    

     Mp 

          

require that there exists   bound  

Then  for   suf ciently large    is of

and note that by Hoelder   inequality  Mp
Comparison to Telgarsky   Dasgupta   Again 
we consider distributions   that have unit variance and
we restrict ourselves to solutions     Rd   with an
expected quantization error of at most the variance      
EP                 Telgarsky   Dasgupta  
EP              
    
     
   
     
   dk ln  
vuut    Mp
       
 CA 
   dk log     log
  rate for all
  as       we obtain          

While Telgarsky   Dasgupta   only show   rate of

In contrast  we obtain that  for   suf ciently large 

      

higher moment bounds 

   

 
   

  dm    ln

    

 

 

 

 
 

 
 

 
 

 

 

Uniform Deviation Bounds for kMeans Clustering

  Subgaussianity
If the distribution   is subgaussian  then all its moments
 Mp are bounded  By optimizing   in Theorem   we are
able to show the following bound 
Theorem    Subgaussian bound  Let                
and        Let   be any distribution on Rd with    
EP     and

 

 

  

with

abp 

        

for some             Let        
         

pb 
                     exp 
   
               log      log
and           log  
    Let                    xm  be  
independent samples from     Then  with probability at least
      for all     Rd  

   

 EP          
The proof is provided in Section   In    notation 

         EP          
      ab log   

   dk log     log

 

 
 

 

 

 

samples are hence suf cient  This result features   polylogarithmic dependence on  
  compared to the polynomial
dependence for the bounds based on bounded higher moments  The suf cient sample size further scales linearly
with the  scaleinvariant  subgaussianity parameters   and   
For example  if   is   onedimensional normal distribution
of any scale  we would have       and      

    This allows us to obtain Theorem  

  Bounded support
The strongest assumption that we consider is if the support
of   is bounded by   hypersphere in Rd with diameter
      This ensures that almost surely            and
hence         
Theorem    Bounded support  Let                
and        Let   be any distribution on Rd  with    
EP     and     EP            whose support
is contained in   ddimensional hypersphere of diameter
      For
 
             log      log
   
let                    xm  be   independent samples from
    Then  with probability at least       for all     Rd  
 EP          

         EP          

      
 

   

 
 

 

 

 

The proof is provided in Section   Again  the suf cient
sample size scales linearly with the kurtosis bound   
    However  the bound is only logarithmic in  
   

  Framework for unbounded loss functions
To obtain the results presented in Section   we propose  
novel framework to uniformly approximate the expected
values of   set of unbounded functions based on   random
sample  We consider   function family   mapping from
an arbitrary input space   to    and   distribution  
on     We further require   generalization of the VapnikChervonenkis dimension to continuous  unbounded functions    the pseudodimension 
De nition    Haussler   Li et al    The pseudodimension of   set   of functions from   to    denoted by Pdim    is the largest    such there is   sequence            xd  of domain elements from   and   sequence            rd  of reals such that for each            bd   
 above  below  there is an      such that for all
                 we have    xi    ri   bi   above 
Similar to the VC dimension  the pseudodimension measures the cardinality of the largest subset of   that can be
shattered by the function family    Informally  the pseudodimension measures the richness of   and plays   critical
role in providing   uniform approximation guarantee across
all        With this notion  we are able to state the main
result in our framework 
Theorem   Let                 and       Let  
be   family of functions from   to    with Pdim      
      Let           be   function such that       
supf         for all        Let   be any distribution on
  and for

  

           log

 

   

   

let                   be    independent samples from    
Then  if

  xi        

 
 

 

 

       

 

  

and     
 mXi 
EP         
it holds with probability at least       that
   xi    EP           
 

mXi 

 
 

Applying Theorem   to   function family   requires three
steps  First  one needs to bound the pseudodimension of

 The pseudodimension was originally de ned for sets of functions mapping to      Haussler    Li et al    However 
it is trivially extended to unbounded functions mapping to   

Uniform Deviation Bounds for kMeans Clustering

   Second  it is necessary to  nd   function          
such that

            

      and        

Ideally  such   bound should be as tight as possible  Third 
one needs to  nd some       and   sample size

   

such that

EPhs        

  

 

           log
and     

 
  xi        

  

 mXi 

 
 

 

Finding such   bound usually entails examining the tail
behavior of      under     Furthermore  it is evident that

and that assumptions on the distribution   are required  In
Section   we will see that for kMeans   function     

  bound   may only be found if EPhs     is bounded
with EPhs         may be found if the kurtosis of   is

bounded 
We defer the proof of Theorem   to Section   of the Supplementary Materials and provide   short proof sketch that captures the main insight  The proof is based on symmetrization 
the bounding of covering numbers and chaining   common techniques in the empirical process literature  Pollard 
  Li et al    Boucheron et al    Koltchinskii 
  van der Vaart   Wellner    The novelty lies in
considering loss functions     and cover functions    in
Theorem   that are potentially unbounded 

Proof sketch  Our proof is based on   double sampling approach  Let xm  xm              be an additional   independent samples from   and let               be independent random variables uniformly sampled from    
Then  we show that  if EPhs          the probability
of   not holding may be bounded by the probability that
there exists        such that

 
 

mXi 

 

       xi       xi   

 

 

We  rst provide the intuition for   single function      and
then show how we extend it to all        While the function
      is not bounded  for   given sample                   
each    xi  is contained within     xi  Given the sample
                   the random variable        xi       xi   
is bounded in   max    xi    xi    and has zero mean 
Hence  given independent samples                    the
probability of   occurring for   single      can be

bounded using Hoeffding   inequality by

 

   

  exp  
   max    xi    xi   
mPm
    exp  
 mP  

     xi   
least      

  

 

  we have

By   with probability at
 

     xi      and we hence require         log  
   
 mP  
samples to guarantee that   does not hold for   single
     with probability at least      
 
To bound the probability that there exists any      such
that   holds  we show in Lemma    see Section   of the
Supplementary Materials  that  given independent samples
                  

 

 
 

        
mXi 
     Pdim   

      xi       xi   

  
     xi   

   

      

  

   

The key dif culty in proving Lemma   is that the functions
     are not bounded uniformly in     To this end 
we provide in Lemma     novel result that bounds the size
of  packings of   if the functions      are bounded in
expectation  Based on Lemma   we then prove the main
claim of Theorem  

  Analysis for kMeans
In order to apply Theorem   to kMeans clustering  we require   suitable family    an upper bound      and   bound
on EPhs      We provide this in Lemma   and defer
     xi  to the proofs of Theorems  
bounding  
Lemma    kMeans  Let        Let   be any distribution
on Rd with     EP         EP            and
bounded kurtosis     For any     Rd and any     Rd   
de ne

 mP  

fQ     

       
 EP         

 
       

 

as well as the function family      fQ        Rd    

Let

        

      

   

 

We then have

Pdim               log    

fQ          

 

 

Uniform Deviation Bounds for kMeans Clustering

 

     

for any     Rd and     Rd   and

EPhs                

The proof of Lemma   is provided in Section   of the
Supplementary Materials  The de nition of fQ    in   is
motivated as follows  If we use Theorem   to guarantee

 

then this implies

mXi 

   xi    EP                   

 
         EP          

 
 

   

 

 EP        

as is required by Theorems   Lemma   further shows
that         is bounded if and only if the kurtosis of   is
bounded  This is the reason why   bounded kurtosis is the
weakest assumption on   that we require in Section  
We now proceed to prove Theorems   by applying Theoi    xi 
rem   and examining the tail behavior of  

 mP  

  Proof of Theorem    kurtosis bound 
The bound based on the kurtosis follows easily from
Markov   inequality 

Proof  We consider the choice                

By Markov   inequality and linearity of expectation  we
then have by Lemma   that

    

  

 mXi 

  xi        

       

 

 

 
 

 

 

rem   to obtain that for

Furthermore  EPhs          Hence  we may apply Theo 
             log      log
   
   xi                        

       
 

   
it holds with probability at least       that

This implies the main claim and thus concludes the proof 

mXi 

 
 

 

  Proof of Theorem    higher order moment bound 
We prove the result by bounding the higher moments
     xi  using the MarcinkiewiczZygmund inof  
equality and subsequently applying Markov   inequality 

 mP  

Proof  Hoelder   inequality implies

EP       

 

 

EP          

 
 

 

 
 

   Mp

 

   Mp

Hence  by Lemma   we have that EP           
  Since          for all     Rd  we have
        EP        max      EP     
  
  max             Mp
     
    max   Mp

   

      

     

 

 

 

This implies that

 

 

     

EPh        EP       
  
  max   Mp   
  max   Mp   

   
   
   

   Mp 

     

     

 

 

 

 

 

   Mp 

 

  EP          

  

 

 

We apply   variant of the MarcinkiewiczZygmund inequality  Ren   Liang    to the zeromean random variable

 

 

 

 

     

 
  

For       the Markov inequality implies

       EP      to obtain
EP 
 
 mXi   xi    EP     
         
       
EPh        EP       
  
         
       
   Mp 
 
  
 mXi   xi    EP     
         
 up     
   Mp 
 
         
 up     
   Mp 
  max 
max     Mp
         
    
 up  
up         Mp
    
         

    

 
  

     

     

 

 

 

 

 

 

 

 

 

 

 

 

Uniform Deviation Bounds for kMeans Clustering

   we thus have

 

 
  

    this implies

For                 Mp
  
 mXi   xi    EP     
Since      
   
  
 mXi   xi    EP     
   EP                   Mp
  
          Mp

It holds that

 
  

 

            

 

 

 

      

 
 

 

            Mp

 
 

 

 

We set             Mp
 mXi 

    

  

 

   and thus have
  xi        

 
 

 

  with

In combination with EPhs         by Lemma   we may
thus apply Theorem   Since        
           Mp
 
it holds with probability at least       that
 
mXi 

              log      log
   xi                        

This implies the main claim and thus concludes the proof 

 
 

 

  Proof of Theorem    subgaussianity 
Under subgaussianity  all moments of        are bounded 
We show the result by optimizing over   in Theorem  

      

Proof  For                 we have
 
  
         
   du
pb  du 
  exp  

 Mp   EP 
    
    

   

 

 

 

 
 

 

 

 

 

Let         

 

   

 

  which implies du dt    

 
 

 
   

 

    Hence 

 Mp  

ab

 

   

     

 

  tt

 

   dt 

By the de nition of the gamma function and since   is even 
we have

         
   

   

 

 

  tt

   dt      

   
     
Hence  for                 we have
 
 

   

 Mp

 
 

 

 

 

abp 

  bp   
  which implies

 

 
 

log

log  

     

  log  

 
   

           log

Let         
and thus    
   
       We instantiate Theorem   with
the   thorder bound  Mp  of     Since   
   
       the
     
               log      log
   
The main claim  nally holds since                log  
   

minimum sample size is thus
   

abp 

 

 

  Proof of Theorem    bounded support 
Proof  Let             Since the support of   is
bounded  we have          for all     Rd  This implies
that EPhs         and that  
     xi      almost

surely  The result then follows from Theorem  

 mP  

  Conclusion
We have presented   framework to uniformly approximate
the expected value of unbounded functions on   sample 
With this framework we are able to provide theoretical guarantees for empirical risk minimization in kMeans clustering
if the kurtosis of the underlying distribution is bounded  In
particular  we obtain stateof the art bounds on the suf cient
number of samples to achieve   given uniform approximation error  If the underlying distribution ful lls stronger
assumptions  such as bounded higher moments  subgaussianity or bounded support  our analysis yields progressively
better bounds  We conjecture that Theorem   can be applied
to other related problems such as hard and soft Bregman
clustering  likelihood estimation of Gaussian mixture models  as well as nonparametric clustering problems  However 
such results do not follow immediately and require additional arguments beyond the scope of this paper 

Uniform Deviation Bounds for kMeans Clustering

Acknowledgements
This research was partially supported by SNSF NRP  
ERC StG     Google Ph    Fellowship and an IBM
Ph    Fellowship  This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing 

References
Antos  Andr as  Gyor     and Gyorgy  Andras  Individual convergence rates in empirical vector quantizer design  IEEE Transactions on Information Theory     

Bachem  Olivier  Lucic  Mario  and Krause  Andreas  Coresets for
nonparametric estimation   the case of DPmeans  In International Conference on Machine Learning  ICML   

Bachem  Olivier  Lucic  Mario  Hassani     Hamed  and Krause 
Andreas  Fast and provably good seedings for kmeans  In
Advances in Neural Information Processing Systems  NIPS  pp 
   

Bachem  Olivier  Lucic  Mario  and Krause  Andreas  Distributed
and provably good seedings for kmeans in constant rounds  In
To appear in International Conference on Machine Learning
 ICML  pp     

Bartlett  Peter    Linder  Tam as  and Lugosi    abor  The minimax
distortion redundancy in empirical quantizer design 
IEEE
Transactions on Information Theory     

BenDavid  Shai    framework for statistical clustering with
constant time approximation algorithms for kmedian and kmeans clustering  Machine Learning     

BenDavid  Shai  Von Luxburg  Ulrike  and   al    avid    sober
look at clustering stability 
In International Conference on
Computational Learning Theory  COLT  pp    Springer 
 

Boucheron  St ephane  Lugosi    abor  and Massart  Pascal  Concentration inequalities    nonasymptotic theory of independence 
Oxford University Press   

Dinh  Vu    Ho  Lam    Nguyen  Binh  and Nguyen  Duy  Fast
learning rates with heavytailed losses  In Advances in Neural
Information Processing Systems  NIPS  pp     

Gr unwald  Peter   and Mehta  Nishant    Fast rates with un 

bounded losses  arXiv preprint arXiv   

HarPeled  Sariel  Geometric approximation algorithms  volume

  American Mathematical Society Boston   

Haussler  David  Decision theoretic generalizations of the pac
model for neural net and other learning applications  Information and Computation     

Hoeffding  Wassily  Probability inequalities for sums of bounded
random variables  Journal of the American Statistical Association     

Koltchinskii  Vladimir  Oracle Inequalities in Empirical Risk
Minimization and Sparse Recovery Problems  Lecture Notes in
Mathematics  Springer   

Levrard  Cl ement et al  Fast rates for empirical vector quantization 

Electronic Journal of Statistics     

Li  Yi  Long  Philip    and Srinivasan  Aravind  Improved bounds
on the sample complexity of learning  Journal of Computer and
System Sciences     

Linder  Tam as  Lugosi    abor  and Zeger  Kenneth  Rates of convergence in the source coding theorem  in empirical quantizer
design  and in universal lossy source coding  IEEE Transactions
on Information Theory     

Lucic  Mario  Bachem  Olivier  and Krause  Andreas  Strong
coresets for hard and soft bregman clustering with applications
to exponential family mixtures  In International Conference on
Arti cial Intelligence and Statistics  AISTATS  May  

Lucic  Mario  Faulkner  Matthew  Krause  Andreas  and Feldman 
Dan  Training mixture models at scale via coresets  To appear
in Journal of Machine Learning Research  JMLR   

Mendelson  Shahar  Learning without concentration for general

loss functions  arXiv preprint arXiv   

Mendelson  Shahar et al  Learning without concentration 

In
International Conference on Computational Learning Theory
 COLT  pp     

Moors  Johannes      The meaning of kurtosis  Darlington reex 

amined  The American Statistician     

Pollard  David  Strong consistency of kmeans clustering  The

Annals of Statistics     

Pollard  David  Convergence of stochastic processes  Springer

Series in Statistics  Springer   

Rakhlin  Alexander and Caponnetto  Andrea  Stability of kmeans
clustering  Advances in Neural Information Processing Systems
 NIPS     

Ren  YaoFeng and Liang  HanYing  On the best constant in
MarcinkiewiczZygmund inequality  Statistics   Probability
Letters     

Sauer  Norbert  On the density of families of sets  Journal of

Combinatorial Theory  Series       

Shamir  Ohad and Tishby  Naftali  Cluster stability for  nite
samples  In Advances in Neural Information Processing Systems
 NIPS  pp     

Shamir  Ohad and Tishby  Naftali  Model selection and stability
in kmeans clustering  In International Conference on Computational Learning Theory  COLT  pp    Citeseer 
 

Telgarsky  Matus   and Dasgupta  Sanjoy  Momentbased uniform
deviation bounds for kmeans and friends 
In Advances in
Neural Information Processing Systems  NIPS  pp   
 

van der Vaart  Aad   and Wellner  Jon    Weak convergence and
empirical processes  with applications to statistics  Springer
Series in Statistics  Springer New York   

Vapnik  Vladimir   and Chervonenkis  Alexey Ya  On the uniform
convergence of relative frequencies of events to their probabilities  Theory of Probability   Its Applications   
 

