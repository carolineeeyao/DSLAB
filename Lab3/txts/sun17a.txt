SafetyAware Algorithms for Adversarial Contextual Bandit

Wen Sun   Debadeepta Dey   Ashish Kapoor  

Abstract

In this work we study the safe sequential decision
making problem under the setting of adversarial contextual bandits with sequential risk constraints  At each round  nature prepares   context 
  cost for each arm  and additionally   risk for
each arm  The learner leverages the context to
pull an arm and receives the corresponding cost
and risk associated with the pulled arm  In addition to minimizing the cumulative cost  for safety
purposes  the learner needs to make safe decisions such that the average of the cumulative risk
from all pulled arms should not be larger than  
prede ned threshold  To address this problem 
we  rst study online convex programming in the
full information setting where in each round the
learner receives an adversarial convex loss and  
convex constraint  We develop   meta algorithm
leveraging online mirror descent for the full information setting and then extend it to contextual
bandit with sequential risk constraints setting using expert advice  Our algorithms can achieve
nearoptimal regret in terms of minimizing the
total cost  while successfully maintaining   sublinear growth of accumulative risk constraint violation  We support our theoretical results by
demonstrating our algorithm on   simple simulated robotics reactive control task 

  Introduction
The topic of Safe Sequential Decision Making recently has
received   lot of attention  Amodei et al    and  nds its
importance in different applications ranging from robotics 
arti cial intelligence and clinical trials  Designing reactive
controls for mobile robots requires reasoning and making
safe decisions so that robots can minimize the risk of dangerous obstacle collision  In clinical trials the risk of the

 Robotics Institute  Carnegie Mellon University  USA
 Microsoft Research  Redmond  USA  Correspondence to  Wen
Sun  wensun cs cmu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

sideeffect of   new treatment must be taken into consideration for patients  safety  In general these applications will
require the risk  probability of collision  level of sideeffect 
to be less than some safethreshold  In this work we study
safe sequential decision making under the setting of adversarial contextual bandits with sequential risk constraints 
The Contextual Bandits problem  Langford   Zhang   
is   classic framework for studying sequential decision making with rich contextual information  In each round  given
the contextual information  the learner chooses an arm      
  decision  to pull based on the history of the interaction
with the environment  and then receives the reward associated with the pulled arm  For the special case where
contexts and rewards are       sampled from  xed unknown
distributions  there exists an oraclebased computationally
ef cient algorithm  Agarwal et al    that achieves nearoptimal regret rate  For adversarial contexts and rewards 
EXP   Auer et al    and EXP    Beygelzimer et al 
  are stateof theart algorithms  which achieve nearoptimal regret rate  but are not computationally ef cient
in general  Recently    few authors have started to incorporate global constraints into multiarmed bandit and contextual bandits problem where the goal of the learner is to
maximize the reward while satisfying   global constraint
to some degree  Previous work considered special cases
such as single resource budget constraint  Ding et al   
Madani et al    and multiple resources budget constraints  Badanidiyuru et al    Resourceful Contextual
Bandits  Badanidiyuru et al     rst introduced resource
budget constraint to contextual bandits  Later on  the authors in  Agrawal et al    generalize the setting to  
setting with   global convex constraint and   concave objective  Recently  Agrawal   Devanur    introduce  
Upper Con dence Bound  UCB  style algorithm for linear
contextual bandits with knapsack constraints  The settings
considered in these previous work mainly focused on the
stochastic case where contexts and rewards are        and
the single constraint is pre xed       timeindependent 
nonadversarial 
We introduce sequential risk constraints in contextual bandits  in each round  the environment prepares   context   
They can be very computationally ef cient for special cases

of expert class  Beygelzimer et al   

SafetyAware Algorithms for Adversarial Contextual Bandit

cost for each arm  and additionally   risk for each arm  The
learner pulls an arm and receives the cost and risk associated
with the pulled arm  Given   prede ned risk threshold  the
learner ideally needs to make safe decisions such that the
risk is no larger than the safethreshold in every round  while
minimizing the cumulative cost simultaneously  Such adversarial risk functions are common in real world applications 
when   robot is navigating in an unfamiliar environment  risk
      probability of being collision with unseen obstacles 
and reward of taking   particular action may dependent on
the robot   current state and the environment  or the whole
history of states  while the sequential states visited by the
robot are unlikely to be       or even Markovian 
To address the adversarial contextual bandit with sequential
risk constraints problem  we  rst study the problem of online convex programming  OCP  with sequential constraints 
where at each round  the environment prepares   convex loss 
and additionally   convex constraint  The learner wants to
minimize its cumulative loss while satisfying the constraints
as best as possible  The online learning with constraints setting is  rst studied in  Mannor et al    in   twoplayer
game setting  Particularly the authors constructed   twoplayer game where there exists   strategy for the adversary
such that among the strategies of the player that satisfy the
constraints on average  there is no strategy that can achieve
the noregret property in terms of maximizing the player  
reward  Later on  Mahdavi et al    Jenatton et al 
  considered the online convex programming framework where they introduced   prede ned global constraint
and designed algorithms that achieve noregret property
on loss functions while maintaining the accumulative constraint violation grows sublinearly  Though the work in
 Mahdavi et al    Jenatton et al    did not consider
timedependent  adversarial constraints  we  nd that their
online gradient descent  OGD   Zinkevich    based algorithms are actually general enough to handle adversarial
timedependent constraints  We  rst present   family of
online learning algorithms based on Mirror Descent  OMD 
 Beck   Teboulle    Bubeck    which we show
achieves nearoptimal regret rate with respect to loss and
maintains the growth of total constraint violation to be sublinear  With   speci   design of   mirror map  our meta
algorithm reveals   similar algorithm shown in  Mahdavi
et al   
The mirror descent based algorithms in the full information
online learning setting also enables us to derive   Multiplicative Weight  MW  update procedure by choosing negative
entropy as the mirror map  Note that MW based update procedure is important when extending to partial information
contextual bandit setting  The MW based update procedure
To be consistent to classic Online Convex Programming set 

ting  we consider minimizing cost

can ensure the regret is polylogarithmic in the number of experts   instead of polynomial in the number of experts from
using the OGDbased algorithms  Mahdavi et al   
Jenatton et al    Leveraging the MW update procedure developed from the online learning setting  we present
algorithms called EXP    EXP  with Risk Constraints 
and EXP      EXP   with Risk Constraints  EXP  
can achieve near optimal regret in terms of minimizing cost
while ensuring the average of the accumulative risk is no
larger than the prede ned threshold  For EXP      we
further introduce   tradeoff parameter that shows how one
can trade between the risk violation and the regret of cost 
The rest of the paper is organized as follows  We introduce
necessary de nitions and problem setup in Sec    We then
deviate to the full information online learning setting where
we introduce sequential  adversarial convex constraints in
Sec    In Sec    we move to contextual bandits with risk
constraints setting to present and analyze the EXP   and
EXP     algorithm 

  Preliminaries
  De nitions
  function            it is strongly convex with respect
to some norm     if and only if there exists   constant
       such that 

              rR              

 
  kx       

Given   strongly convex function    the Bregman divergence DR         

is de ned as follows 

DR                       rR            
  Online Convex Programming with Constraints
In each round  the learner makes   decision xt        
and then receives   convex loss function     and   convex
constraint in the form of ft      The learner suffers
loss       The work in  Mahdavi et al    considers
  similar setting but with   known  prede ned global constraint  Instead of projecting the decision   back to the
convex set induced by the global constraint      Mahdavi et al    introduces an algorithm that achieves noregret on loss while satisfying the global constrain in  
longterm perspective  Since exactly satisfying adversarial
constraint in every round is impossible  we also consider
constraint satisfaction in   longterm perspective  Formally 
for the sequence of decisions  xt   made by the learner  we
de nePT
   ft xt  as the cumulative constraint violation
and we want to control the growth of the cumulative constraint violation to be sublinear  PT
   ft xt          so
  PT
   ft    we have

that for the longterm constraint  
limT 

   ft xt     

 

  PT

SafetyAware Algorithms for Adversarial Contextual Bandit

We place one assumption on the decision set     we assume
that the decision set   is rich enough such that in hindsight 
we have      that can satisfy all constraints        
    ft                    default conservative  safe
decisions   We compete with the optimal decision      
that minimizes the total loss in hindsight 

     arg min
   

TXt 

     

 

Though one probably would be interested in competing against the best decision from the set of decisions
that satisfy the constraints in average            
    PT
  ft        in general it is impossible to compete against the best decision in    in hindsight       
The following proposition adapts the discrete  player game
from proposition   in  Mannor et al    for the OCP
with adversary constraints and shows the learner is unable
to compete against   
Proposition   There exists   decision set       sequence
of convex loss functions       and   sequence of convex constraints  ft        such that for any sequence of
decisions       xt    if it satis es the longterm contPt
   fi xi      then if competstrain as lim supt 
ing against    the regret grows at least linearly 
          

     tXi 

   xi    min
    

tXi 

lim sup

 

 

The proof of the proposition can be found in Sec    in Appendix  Hence in the rest of the paper  we have to restrict to
   The average regret of loss    and the average constraint
violation Rf are de ned as 

Formally  at every time step    the environment generates  
context st        Kdimensional cost vector ct        
and   risk vector rt         The environment then reveals
the context st to the learner  and the learner then proposes
  probability distribution pt       over all arms  Finally the learner samples an action at       according to
pt and receives the cost and risk associated to the chosen
action  ct at  and rt at   we denote      as the   th element
of vector    The learner ideally wants to make   sequence
of decisions that has low accumulative cost and also satis 
 es the constraint that related to the risk  pT
  rt     where
        is   prede ned threshold 
We address this problem by leveraging experts  advice 
Given the expert set   that consists of   experts     
  
where each expert              gives advice
by mapping from the context   to   probability distribution   over arms  The learner then properly combines
the experts  advice       
         compute the average
PN
           to generate   distribution over all arms 
With risk constraints  distributions over policies in   could
be strictly more powerful than any policy in   itself  We
aim to compete against this more powerful set  Given any
distribution       the mixed policy resulting from  
can be regarded as  sample policy   according to   and then
sample an arm according to       Though we do not place
any statistical assumptions              on the sequence of
cost vectors  ct  and risk vectors  rt  we assume the policy set   is rich enough to satisfy the following assumption 

Assumption   The set of distributions from   whose
mixed policies satisfy all risk constraints in expectation is
nonempty 

ft xt 

           Ei       st rt               

    

 
 

 

TXt 

   xt   

      Rf  

 
 

 

TXt 

TXt 

We will assume the decision set
is bounded as
maxx     DR                     is bounded as
kxk          the loss function is bounded as    
       the constraint is bounded  ft         and
the gradient of the loss and constraint is also bounded as
max krx       krxft             where      
is the dual norm with respect to     de ned for     Note
the setting with   global constraint considered in  Mahdavi
et al    is   special case of our setting  Set ft     
where   is the global constraint  If Rf         by Jensen  
inequality  we have    PT
      xt    
           as      
  Contextual Bandits with Risk Constraints
For contextual bandits with sequential risk constraints  let
    be    nite set of   arms    be the space of contexts 

   xt       PT

Namely we assume that the distribution set   is rich
enough such that there always exists at least one mixed policy that can satisfy all risk constraints in hindsight  Similar
to the full information setting  competing against the set of
mixed policies that satisfy the constraint on average  namely
   Ei       st rt        
is impossible in the partial information setting  Hence we
de ne the best mixed policy in hindsight as 

              PT

     arg min
   

TXt 

Ei       st ct   

 

   generated from some
Given   sequence of decisions  at  
algorithm  we de ne average regret and average constraint

SafetyAware Algorithms for Adversarial Contextual Bandit

violation as 

Rc  

Rr  

 

   TXt 
   TXt 

 

Ei       st ct   

ct at   

TXt 
 rt at     

The goal is to either minimize Rc and Rr in high probability or minimize the expected version  Rc    Rc  and
 Rr    Rr  where the expectation is over the randomness of
the algorithms 

  Online Learning with Sequential

Constraints

The online learning with adversarial sequential constraints
setting is similar to the one considered in  Mannor et al 
  Jenatton et al    except that they only have  
prede ned  xed global constraint  However we  nd that
their algorithms and analysis are general enough to extend
to the online learning with adversarial sequential constraints 
In  Mannor et al    Jenatton et al    the algorithms introduce   Lagrangian dual parameter and perform
online gradient descent on   and online gradient ascent on
the dual parameter  Since in this work we are eventually
interested in reducing the contextual bandit problem to the
full information online learning setting  simply adopting the
OGDbased approaches from  Mannor et al    Jenatton et al    will not give   near optimal regret bound 
Hence  developing the corresponding Multiplicative Weight
 MW  update procedure is essential for   successful reduction from adversarial contextual bandit to full information
online learning setting 

  Algorithm
We use the same saddlepoint convex concave formation
from  Mannor et al    Jenatton et al    to design  
composite loss function as 

Lt                 ft     

 
 

 

 

where        Alg    leverages online mirror descent
 OMD  for updating the    Line   and Line   and online
gradient ascent algorithm for updating    Line   Note
that if we use the square norm kxk  as the regularization
function      in Alg    we reveal the gradient descent
based update procedure from  Mahdavi et al   

Algorithm   OCP with Sequential Constraints via OMD
  Input  Learning rate   mirror map    parameter  

 used in Lt 

  Initialize       and      
  for       to   do
 
 
 

Learner proposes xt 
Receive loss function    and constraint ft 
Set  xt  such that rR xt    rR xt   
 rxLt xt    
Projection  xt    arg minx   DR     xt 
Update       max         Lt xt    

 
 
  end for

the number of rounds   is given and we consider the asymptotic property of Alg    when   is large enough 
The algorithm should be really understood as running two
noregret procedures    Online Mirror Descent on the sequence of loss            with respect to   and   Online
Gradient ascent on the sequence of loss    xt     with
respect to   Instead of digging into the details of Online
Mirror Descent and Online Gradient ascent  our analysis
simply leverages the existing analysis of online mirror descent and online gradient ascent and show how to combine
them to derive the regret bound and constraint violation
bound for Alg   
Theorem   Let    be    strongly convex function 
Set     
    For any convex loss
      convex constraint ft        under the assumption
that        the family of algorithms induced by Alg    have
the following property 

        and        

 

       pT   Rf        

Proof Sketch of Theorem   Since the algorithm runs online mirror descent on the sequence of loss  Lt        
with respect to    using the existing results of online mirror
descent       Theorem   and Eq    from  Bubeck 
  we know that for the computed sequence  xt   

 Lt xt                

TXt 

 

DR      

 

 

 
 

TXt 

krxLt xt      
 

 

  Analysis of Alg   
Throughout our analysis  we assume the regularization function      is  strongly convex  For simplicity  we assume

for any        Also  we know that the algorithm runs
online gradient ascent on the sequence of loss  Lt xt    
with respect to   using the existing analysis of online gradient descent  Zinkevich    we have for the computed

SafetyAware Algorithms for Adversarial Contextual Bandit

sequence of      
TXt 

Lt xt     

Lt xt    

TXt 
TXt   Lt wt    

 

 
 

 
 

 

   

  

 
for any      
Note that for  Lt xt          ft xt          
    Similarly for
   xt     
    
krxLt xt      
 

           
  we also have 
krxLt xt      
     kr   xt   
         
   

 
where we  rst used triangle inequality for krxLt xt      
and then use the inequality of  ab                   
Note that we also assumed that the norm of the gradients are
bounded as max kr   xt   krft xt            
Now sum Inequality   and   together  we get 

       trft xt   
 

       
   

Xt Lt xt               
 Xt

 DR          

 

 

 Xt

   
 

     
   

 

 DR          

 

     

  
 

         

  
 

 

    

   

 

 
   
  
 

Substitute the form of Lt into the above inequality  we have 

Xt

   xt           Xt
  Xt
   

  
 

 

 

 ft xt     tft   
 DR          

 

 

 
   

  
 

         

       

above inequality 

Note that from our setting of   and   we can verify that
  in the

 Xt
           we can remove the termPt  
Without the termPt  
Xt
   xt           
   pDR                     pT  

    let us set       and        we get 

For simplicity we assumed   is large enough to be larger than

    to upper bound the regret on loss

            

 DR      

 

any given constant 

with    pDR                 To upper bound
Pt ft xt  we  rst observe that we can lower bound
PT
      xt  minxPT
                  where   is the
upper bound of   ReplacePt    xt          by     
in Eq    and set      Pt ft xt        here we
assumePt ft xt      otherwise we prove the theorem 

we can show that 

 

TXt 

   
 

ft xt   
            

DR              

  
 

  

 

   ft xt         

The RHS of the above inequality is dominated by the term

           when   approaches to in nity  Hence 
we getPT
  in Alg   
As we can see that if we replace      with kxk 
we reveal   gradient descent based update procedure that is
almost identical to the one in  Mahdavi et al    When
  is restricted to   simplex  to derive the multiplicative
weight update procedure  we replace      with the negative

entropy regularizationPi      ln      and we can achieve

the following update steps for   

xt    exp rxLt xt      
   xt    exp rxLt xt      

 

xt     

Pd

We refer readers to  ShalevShwartz    Bubeck   
for the derivation of the above equation 

  Contextual Bandits With Risk Constraints
When contexts  costs and risks are       sampled from some
unknown distribution  then our problem setting can be regarded as   special case of the setting of contextual bandit
with global objective and constraint  CBwRC  considered
in  Agrawal et al    In  Agrawal et al    the
algorithm also leverages Lagrangian dual variable  The difference is that in       setting the dual parameter is  xed with
respect to the underlying distribution and hence it is possible
to estimate the dual variable  For instance one can uniformly
pull arms with    xed number of rounds at the beginning to
gather information for estimating the dual variable and then
use the estimated dual variable for all remaining rounds 
However in the adversarial setting  this nice trick will fail
since the costs and risks are possibly sampled from   changing distribution  We have to rely on OCP algorithms to keep
updating the dual variable to adapt to adversarial risks and
costs 

  Algorithm
Our algorithm EXP    EXP  with Risk constraints 
 Alg    extends the EXP   Auer et al    algorithm

SafetyAware Algorithms for Adversarial Contextual Bandit

Algorithm   EXP  with Risk Constraints  EXP   
  Input  Policy set  
  Initialize                  and      
  for       to   do
 
 
 
 
 
 

Receive context st 
Query experts to get advice    st         
Set pt  PN
Draw action at randomly from distribution pt 
Receive cost ct at  and risk rt at 
Set the cost vector  ct   RK and the risk vector
 rt   RK as follows  for all        

   wt     st 

 ct     

ct     at     

pt   

 

 rt     

rt     at     

pt   

 

For each expert          set 

 yt         st    ct 

 zt         st    rt 

Compute wt  for      

wt    exp     yt          zt   
     wt    exp     yt          zt     

wt     

Compute    

 

 

 

      max       wT

   zt           

  end for

to carefully incorporating the risk constraints for updating
the probability distribution   over all policies  At each
round  it  rst uses the common trick of importance weighting to form unbiased estimations of cost vector    and risk
vector     Then the algorithm uses the unbiased estimations
of cost vector and risk vector to form unbiased estimations
of the cost       and risk       for each expert    EXP   then
starts behaving differently than EXP  EXP   introduces
  dual variable   and combine the cost and risk together
as Lt        wT  yt    wT  zt        
    We then
use Alg    with the negative entropy regularization as  
black box online learner to update the weight   and the dual
variable  
The proposed algorithm EXP   in general is computationally inef cient since similar to EXP  and EXP    it needs
to maintain   probability distribution over the policy set 
Though there exist computationally ef cient algorithms for
stochastic contextual bandits and hybrid contextual bandits 
we are not aware of any computationally ef cient algorithm
for adversarial contextual bandits  even without risk constraints 

  Analysis of EXP  
We provide   reduction based analysis for EXP   by
 rst reducing EXP   to Alg    with negative entropic
regularization  For the following analysis  let us de ne
yt         st   ct and zt         st   rt  which stand for
the expected cost and risk for policy   at round   
Let us de ne Lt        wT  yt    wT  zt        
   
The multiplicative weight update in Line   can be regarded as running Weighted Majority on the sequence of
loss  Lt          while the update rule for   in Line  
can be regarded as running Online Gradient Ascent on the
sequence of loss  Lt wt      Directly applying the classic
analysis of Weighted Majority  ShalevShwartz    on
the generated sequence of weights  wt   and the classic
analysis of OGD  Zinkevich    on the generated sequence of dual variables       we get the following lemma 
Lemma   With the negative entropyPi      ln      as
the regularization function for      in Alg    running Alg   
on the sequence of linear loss functions         wT  yt and
linear constraint ft      wT  zt         we have 
ln 

 

Lt wt     

Lt         

 
 

 

TXt 

TXt 
TXt   Xi 
   zt           

 

 
 

   wT

wt   yt       

   zt   

 

We defer the proof of the above lemma to Appendix  The
EXP   algorithm has the following property 

Theorem   Set     pln          and    
    Assume        EXP   has the following property 

 Rc     pK ln    
 Rr          ln 

   yt   ct at     

Proof Sketch of Theorem   The proof consists of   combination of the analysis of EXP  and the analysis of Theorem   We defer the full proof in Appendix    We
 rst present several known facts  First  we have wT
   zt  
rt at      and wT
For Eat pt wT
             
It
Eat pt  yt   yt and Eat pt  zt   zt 
also true that Eat ptP    wt   yt        and
Eat ptP    wt   zt        

   zt    we can show that  Eat pt wT

Now take expectation with respect to the sequence of deci 

straightforward

that
It
is

to

show

is

also

   zt 

sions  at   on LHS of Inequality  

  at  

TXt hLt wt                
TXt Ect at     Ert at        yT

 

 

 
 

 

    

  
 

 

Now take the expectation with respect to      aT on the
RHS of inequality   we can get 

  RHS of Inequality    

 
 

 

ln 
 

                  

 
   

 

TXt 

Now we can chain Eq    and   together and use the
same technique that we used in the analysis of Theorem  
Chain Eq    and   together and set   to    and       it
is not hard to show that 

Set    

yT

TXt 

ct at   

   TXt 
   pln              pT   ln 

    

where     pln         
 Pt Ert at           we can get 
 Ert at               
   pln         

TXt 

 

equation  it is easy to verity that 

Substitute     pln          back to the above
TXt 

 Ert at               ln 

 

 

 

  Extension To HighProbability Bounds
The regret bound and constraint violation bound of EXP  
hold in expectation  In this section  we present an algorithm named EXP      which achieves highprobability
regret bound and constraint violation bound  The algorithm EXP      as indicated by its name  is built on the
wellknown EXP   algorithm  In this section for the convenience of analysis  without loss of generality  we are going to
assume that for any cost vector   and risk vector    we have
                             and        

SafetyAware Algorithms for Adversarial Contextual Bandit

         zT

       

 

wt     

The whole framework of the algorithm is similar to the
one of EXP    with only one modi cation  For notation
simplicity  let us de ne  xt       yt          zt    Note that
 xt    is an unbiased estimate of yt       tzt    EXP    
modi es EXP   by replacing the update procedure for
wt  in Line   in Alg    with the following update step 

wt    exp xt       PK
     wt    exp xt       PK

  

   st   

pt   

 

    st   

pt   

  

 

 

  

pt   

pt   

    st   

    st   

where   is   constant that will be de ned in the analysis of EXP      We refer readers to Appendix   for the
full version of EXP      Essentially  similar to EXP  
to
  
is not an unbiased estimation of yt       tzt    anymore  as shown in
Lemma    in Appendix    it enables us to upper bound

and EXP    we add an extra term  PK
 xt    Though  xt       PK
Pt  xt       PK

usingPt yt       tzt    with

high probability 
We show that EXP     has the following performance
guarantees 
Theorem   Assume        For any            
    set       ln 
  and
          we have that with probability at least
     

                  ln 

    st   

pt   

  

   

Rc     pT    ln 
Rr        pK ln 

 

The above theorem introduces   tradeoff between the regret
of cost and the constraint violation  As       we can
see that the regret of cost approaches to the nearoptimal
onepT   ln  but the average risk constrain violation
approaches to   constant  Based on speci   applications 
one may set   speci           to balance the regret and
the constraint violation  For instance  for       one can
show that the cumulative regret is      pK ln  and
is impossible to achieve the regret rate      pln  in

the average constraint violation is        Note that if
one simply runs EXP   proposed in the previous section  it

  high probability statement  As shown in  Auer et al   
for EXP  the cumulative regret on the order of       was
possible 
The dif culty of achieving   high probability statement

with optimal cumulative regret   pT   ln  and cu 

mulative constraint violation rate        is from the Lagrangian dual variable   The variance of  zt is proportional to  pt at  With     the variance of     zt scales as
EXP   becomes the same as EXP  when we set all risks and

  to zeros 

SafetyAware Algorithms for Adversarial Contextual Bandit

    Environment

    Cost

    Risk

Figure       Environment set up and   safe trajectory from the initial position to the goal region  green        The performance of EXP  
under different values of risk thresholds  yaxis is in log scale 

   pt at  As we show in Lemma    in Appendix   
 
   could be as large as   Depending on the value
of        could be large        pT   if   is   constant
and      pT   Hence compared to EXP    the Lagrangian dual variable in EXP     makes it more dif 
cult to control the variance of  xt  which is an unbiased
estimation of yt       tzt    This is exactly where the
tradeoff   comes from  we can tune the magnitude of   to
control the variance of  xt and further control the tradeoff
between regret and risk violation   How to achieve total re 

gret   pT   ln  and cumulative constraint violation

      in high probability is still an open problem 

  Simulation
We test our algorithms on   simple synthetic robotics reactive control task  The    environment  shown in Fig     is
set up as follows  We divide the environment into   cells
and each cell is associated by   waypoint  black dot or black
circle  The black dots are associated with risk   and stands
for dangerous areas while the circle dots are associated with
risk zeros       safe regions  The state of the agent is its
   position and for each state  we compute the RBF feature
  with respect   waypoints and then normalize the feature  
such thatP 
            The risk for feature   is the inner
product between   and the  dimension risk vector  The
reward of action   is max           we simply negate
the reward to fake   cost to feed to our algorithms  where
   stands for the old distance to goal and    is the new
distance to goal after taken action    hence   nonlinear
reward mapping  We designed   actions for the agent  it
can move up  right  left  and down with    xed small constant distance  We design   experts where each expert   
is     dimensional vector  where the element in each dimension belongs to the action set  Left Right  Up  Down 
 namely each expert    suggests an action for every waypoint  Given   feature    the suggestion       is computed

as          Pk            Note that this is similar to the

setting consider in  Beygelzimer et al    Note that the
class of experts can be represented by   depth  tree with  
branches  Hence computational ef cient weighted majority

algorithm implementation could be used here  CesaBianchi
  Lugosi    But in this work  we temporarily lighten
computational burden by paralleling computing  We initialize the weight over all experts uniformly and let the EXP  
algorithm picks decision for the agent  The agent executes
the decision  receives risk and cost  The agent is reset to the
initial position once it reaches the goal region or outside of
the map too much 
Fig     and    show the performance of EXP   under
different values of risk threshold   We observe that EXP  
can ensure that the average risk converges to the threshold
  while keep decreasing the average cost simultaneously 
  lower risk threshold usually results   slower decrease in
the average cost  which is consistent to the theorems since
  smaller risk threshold leads to   smaller    Compare to
the performance of EXP  we see the EXP  can offer faster
decrease rate for cost  as it can compete against the whole
policy class set  but it cannot control risk       it discovers
paths that cut through the middle highrisk cell 

  Conclusion
We study safe sequential decision making problem under
the format of adversarial contextual bandits with adversarial
sequential risk constraints  We provide safetyaware algorithms that can satisfy the longterm risk constraint while
achieve nearoptimal regret in terms of minimizing costs 
The proposed two algorithm  EXP   and EXP      are
built on the existing EXP  and EXP   algorithms  EXP  
achieves nearoptimal regret and satis es the longterm constraint in expectation while EXP     achieves similar theoretical bounds with high probability  with   tradeoff that
can trade the constraint violation for regret and vice versa 
Same as EXP  and EXP    the computational complexity
of   simple implementation of our algorithms per step is
linear with respect to the size of the expert class  However
for expert class that has special structures such as trees  as
we used in our simulation  or graphs  one can usually design
ef cient implementation  We leave the ef cient implementation for real robotics control as   future work 

SafetyAware Algorithms for Adversarial Contextual Bandit

Madani  Omid  Lizotte  Daniel    and Greiner  Russell  The budgeted multiarmed bandit problem  In International Conference
on Computational Learning Theory  pp    Springer 
 

Mahdavi  Mehrdad  Jin  Rong  and Yang  Tianbao  Trading regret for ef ciency  online convex optimization with long term
constraints  The Journal of Machine Learning Research   
   

Mannor  Shie  Tsitsiklis  John    and Yu  Jia Yuan  Online learning
with sample path constraints  The Journal of Machine Learning
Research     

ShalevShwartz  Shai  Online Learning and Online Convex Optimization  Foundations and Trends in Machine Learning   
   

Zinkevich  Martin  Online Convex Programming and Generalized
In nitesimal Gradient Ascent  In International Conference on
Machine Learning  ICML   pp     

Acknowledgement
The research work was done during Wen Sun   internship at
Microsoft Research  Redmond 

References
Agarwal  Alekh  Hsu  Daniel  Kale  Satyen  Langford  John  Li 
Lihong  and Schapire  Robert  Taming the monster    fast
and simple algorithm for contextual bandits  In Proceedings of
The  st International Conference on Machine Learning  pp 
   

Agrawal  Shipra and Devanur  Nikhil    Linear contextual bandits

with knapsacks  arXiv preprint arXiv   

Agrawal  Shipra  Devanur  Nikhil    and Li  Lihong  Contextual
bandits with global constraints and objective  arXiv preprint
arXiv   

Amodei  Dario  Olah  Chris  Steinhardt  Jacob  Christiano  Paul 
Schulman  John  and Man    Dan  Concrete problems in ai safety 
arXiv preprint arXiv   

Auer  Peter  CesaBianchi  Nicolo  Freund  Yoav  and Schapire 
Robert    The nonstochastic multiarmed bandit problem  SIAM
Journal on Computing     

Badanidiyuru  Ashwinkumar  Kleinberg  Robert  and Slivkins 
Aleksandrs  Bandits with knapsacks  In Foundations of Computer Science  FOCS    IEEE  th Annual Symposium on 
pp    IEEE   

Badanidiyuru  Ashwinkumar  Langford  John  and Slivkins  Aleksandrs  Resourceful contextual bandits  In COLT  pp   
   

Beck  Amir and Teboulle  Marc  Mirror descent and nonlinear projected subgradient methods for convex optimization  Operations
Research Letters     

Beygelzimer  Alina  Langford  John  Li  Lihong  Reyzin  Lev 
and Schapire  Robert    Contextual bandit algorithms with
supervised learning guarantees  In AISTATS  pp     
Bubeck    ebastien  Convex optimization  Algorithms and complexity  Foundations and Trends    in Machine Learning   
   

Bubeck    ebastien  CesaBianchi  Nicol    et al  Regret analysis
of stochastic and nonstochastic multiarmed bandit problems 
Foundations and Trends    in Machine Learning   
 

CesaBianchi  Nicolo and Lugosi    abor  Prediction  learning 

and games  Cambridge university press   

Ding  Wenkui  Qin  Tao  Zhang  XuDong  and Liu  TieYan 
Multiarmed bandit with budget constraint and variable costs 
In AAAI   

Jenatton  Rodolphe  Huang  Jim  and Archambeau    edric  Adaptive algorithms for online convex optimization with longterm
constraints  ICML   

Langford  John and Zhang  Tong  The epochgreedy algorithm
for multiarmed bandits with side information  In Advances in
neural information processing systems  pp     

