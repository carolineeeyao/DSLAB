Convexi ed Convolutional Neural Networks

Yuchen Zhang   Percy Liang   Martin    Wainwright  

Abstract

We describe the class of convexi ed convolutional neural networks  CCNNs  which capture
the parameter sharing of convolutional neural
networks in   convex manner  By representing
the nonlinear convolutional  lters as vectors in  
reproducing kernel Hilbert space  the CNN parameters can be represented in terms of   lowrank matrix  and the rank constraint can be relaxed so as to obtain   convex optimization problem  For learning twolayer convolutional neural networks  we prove that the generalization
error obtained by   convexi ed CNN converges
to that of the best possible CNN  For learning
deeper networks  we train CCNNs in   layerwise manner  Empirically  we  nd that CCNNs achieve competitive or better performance
than CNNs trained by backpropagation  SVMs 
fullyconnected neural networks  stacked denoising autoencoders  and other baseline methods 

  Introduction
Convolutional neural networks  CNNs   LeCun et al 
  have proven successful across many tasks including
image classi cation  LeCun et al    Krizhevsky et al 
  face recognition  Lawrence et al    speech
recognition  Hinton et al    text classi cation  Wang
et al    and game playing  Mnih et al    Silver
et al    There are two principal advantages of   CNN
over   fullyconnected neural network      sparsity each
nonlinear convolutional  lter acts only on   local patch of
the input  and  ii  parameter sharing the same  lter is applied to each patch 
However  as with most neural networks  the standard approach to training CNNs is based on solving   nonconvex
optimization problem that is known to be NPhard  Blum

 Stanford University  CA  USA  University of California  Berkeley  CA  USA  Correspondence to  Yuchen Zhang
 zhangyuc cs stanford edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Rivest    In practice  researchers use some  avor
of stochastic gradient method  in which gradients are computed via backpropagation  Bottou    This approach
has two drawbacks      the rate of convergence  which is at
best only to   local optimum  can be slow due to nonconvexity  for instance  see the paper  Fahlman    and
 ii  its statistical properties are very dif cult to understand 
as the actual performance is determined by some combination of the CNN architecture along with the optimization
algorithm 
In this paper  with the goal of addressing these two challenges  we propose   new model class known as convexi 
 ed convolutional neural networks  CCNNs  These models have two desirable features  First  training   CCNN
corresponds to   convex optimization problem  which can
be solved ef ciently and optimally via   projected gradient algorithm  Second  the statistical properties of CCNN
models can be studied in   precise and rigorous manner  We
obtain CCNNs by convexifying twolayer CNNs  doing so
requires overcoming two challenges  First  the activation
function of   CNN is nonlinear  In order to address this
issue  we relax the class of CNN  lters to   reproducing
kernel Hilbert space  RKHS  This approach is inspired by
the paper  Zhang et al      which put forth   relaxation
for fullyconnected neural networks  Second  the parameter sharing induced by CNNs is crucial to its effectiveness
and must be preserved  We show that CNNs with RKHS
 lters can be parametrized by   lowrank matrix  Relaxing
this lowrank constraint to   nuclear norm constraint leads
to our  nal formulation of CCNNs 
On the theoretical front  we prove an oracle inequality on
the generalization error achieved by our class of CCNNs 
showing that it is upper bounded by the best possible performance achievable by   twolayer CNN given in nite
data   quantity to which we refer as the oracle risk plus
  model complexity term that decays to zero polynomially
in the sample size  Our results suggest that the sample complexity for CCNNs is signi cantly lower than that of the
convexi ed fullyconnected neural network  Zhang et al 
    highlighting the importance of parameter sharing 
For models with more than one hidden layer  our theory
does not apply  but we provide encouraging empirical results using   greedy layerwise training heuristic  Finally 
we apply CCNNs to the MNIST handwritten digit dataset

Convexi ed Convolutional Neural Networks

as well as four variation datasets  VariationsMNIST  and
 nd that it achieves stateof theart accuracy 

Related work  With the empirical success of deep neural networks  there has been an increasing interest in understanding its connection to convex optimization  Bengio et al    showed how to formulate neural network
training as   convex optimization problem involving an in 
 nite number of parameters  Aslan et al      propose   method for learning multilayer latentvariable models  They showed that for certain activation functions  the
proposed method is   convex relaxation for learning fullyconnected neural networks 
Past work has studied learning translationinvariant features without backpropagation  Mairal et al    present
convolutional kernel networks  They propose   translationinvariant kernel whose feature mapping can be approximated by   composition of the convolution  nonlinearity
and pooling operators  obtained through unsupervised
learning  However  this method is not equipped with the
optimality guarantees that we provide for CCNNs in this
paper  even for learning one convolutional layer  The ScatNet method  Bruna   Mallat    uses translation and
deformationinvariant  lters constructed by wavelet analysis  however  these  lters are independent of the data  in
contrast to CCNNs  Daniely et al    show that   randomly initialized CNN can extract features as powerful as
kernel methods  but it is not clear how to provably improve
the model from random initialization 

Notation  For any positive integer    we use     as  
shorthand for the discrete set                For   rectangular matrix    let  cid   cid  be its nuclear norm   cid   cid  be
its spectral norm       maximal singular value  and  cid   cid  
be its Frobenius norm  We use  cid    to denote the set
of countable dimensional vectors                    such
 cid      For any vectors         cid   
 cid  uivi and the  cid norm

that  cid 
the inner product  cid      cid     cid 
 cid   cid   cid cid      cid  are well de ned 

 cid    

  Background and problem setup
In this section  we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem 

  Convolutional neural networks
At   high level    twolayer CNN  is   function that maps
an input vector     Rd        an image  to an output vector
in     Rd        classi cation scores for    classes  This
mapping is formed in the following manner 

 Average pooling and multiple channels are also an integral
part of CNNs  but these do not present any new technical challenges  so that we defer these extensions to Section  

  First  we extract   collection of   vectors  zp    

of the full input vector    Each vector zp      Rd  is
referred to as   patch  and these patches may depend on
overlapping components of   
  Second  given some choice of activation function
          and   collection of weight vectors  wj  
in Rd  we de ne the functions

  

  

    

hj         cid 

for each patch     Rd 

 
Each function hj  for         is known as    lter  and
note that the same  lters are applied to each patch this
corresponds to the parameter sharing of   CNN 
  Third  for each patch index           lter index        
and output coordinate         we introduce   coef 
 cient            that governs the contribution of the
 lter hj on patch zp    to output fk    The  nal form
of the CNN is given by                         fd     
where the kth component is given by

fk     

     phj zp   

 

Taking the patch functions  zp  
   and activation function
  as  xed  the parameters of the CNN are the  lter vectors
     wj   Rd            along with the collection of
coef cient vectors            RP                  
We assume that all patch vectors zp      Rd  are contained in the unit  cid ball  This assumption can be satis ed
without loss of generality by normalization  By multiplying   constant       to every patch zp    and multiplying
  to the  lter vectors    this assumption holds without
changing the the output of the network 
Given some positive radii    and    we consider the
model class
Fcnn        

 cid wj cid      

 cid 

  cid 

  cid 

  

  

  of the form     max
    
 cid     cid      

and max

        

 cid 

 

 

When the radii        are clear from context  we adopt
Fcnn as   convenient shorthand 

  Empirical risk minimization 

Given an inputoutput pair        and   CNN    we
let            denote the loss incurred when the output   is predicted via       We assume that the loss
function   is convex and LLipschitz in its  rst argument given any value of its second argument  As  
concrete example  for multiclass classi cation with   
the output vector   takes values in the disclasses 
crete set                     
For example  given
  vector                       fd      Rd  of classi cathe associated multiclass logistic loss for
tion scores 
  pair        is given by           
   fy     

Convexi ed Convolutional Neural Networks

log cid cid   

  cid  exp fy cid   cid 
 cid fcnn   arg min

  cid 

Given   training examples  xi  yi  
compute an empirical risk minimizer 

   we would like to

     xi  yi 

  

  Fcnn

 
Recalling that functions     Fcnn depend on the parameters   and   in   highly nonlinear way   this optimization problem is nonconvex  As mentioned earlier  heuristics based on stochastic gradient methods are used in practice  which makes it challenging to gain   theoretical understanding of their behavior  Thus  in the next section  we
describe   relaxation of the class Fcnn for which empirical
risk minimization is convex 

  Convexifying CNNs
We now turn to the development of the class of convexi ed
CNNs  We begin in Section   by illustrating the procedure for the special case of the linear activation function 
Although the linear case is not of practical interest  it provides intuition for our more general convexi cation procedure  described in Section   which applies to nonlinear
activation functions  In particular  we show how embedding the nonlinear problem into an appropriately chosen reproducing kernel Hilbert space  RKHS  allows us to again
reduce to the linear setting 

  Linear activation functions  low rank relaxations

In order to develop intuition for our approach  let us begin
by considering the simple case of the linear activation function          In this case  the  lter hj when applied to the
patch vector zp    outputs   Euclidean inner product of the
form hj zp       cid zp    wj cid  For each     Rd  we  rst
de ne the       dimensional matrix

      

 

sharing of CNNs  See Figure   for   graphical illustration
of this model structure 
Letting                 Ad  be   concatenation of these
matrices across all    output coordinates  we can then de 
 ne   function       Rd    Rd  of the form

          tr                tr     Ad   

 
Note that these functions have   linear parameterization in
terms of the underlying matrix    Our model class corresponds to   collection of such functions based on imposing certain constraints on the underlying matrix    in
particular  we de ne Fcnn       to be the set of func 
    maxj     cid wj cid      
tions     which satis es 
maxk         cid     cid       and     rank        
This is simply an alternative formulation of our original
class of CNNs de ned in equation   Notice that if the
 lter weights wj are not shared across all patches  then the
constraint     still holds  but constraint     no longer
holds  Thus  the parameter sharing of CNNs is realized by
the lowrank constraint     The matrix   of rank   can
be decomposed as          cid  where both   and   have  
columns  The column space of matrix   contains the convolution parameters  wj  and the row space of   contains
to the output parameters      
The matrices satisfying constraints     and     form  
nonconvex set    standard convex relaxation is based on
the nuclear norm  cid   cid  corresponding to the sum of the
singular values of   
It is straightforward to verify that
any matrix   satisfying the constraints     and     must
have nuclear norm bounded as  cid   cid         
   Consequently  if we de ne the function class
       cid   cid         
then we are guaranteed that Fccnn   Fcnn 
We propose to minimize the empirical risk   over Fccnn
instead of Fcnn  doing so de nes   convex optimization
problem over this richer class of functions

Fccnn  

 cid 

 cid 

 cid 

 

 

  

 

 cid fccnn   arg min

    Fccnn

  cid 

  

      xi  yi 

 

In Section   we describe iterative algorithms that can be
used to solve this convex problem in the more general setting of nonlinear activation functions 

  Nonlinear activations  RKHS  lters

For nonlinear activation functions   we relax the class of
CNN  lters to   reproducing kernel Hilbert space  RKHS 
As we will show  this relaxation allows us to reduce the
problem to the linear activation case 
Let     Rd    Rd      be   positive semide nite kernel
function  For particular choices of kernels       the Gaus 

      cid 

 

zP    cid 

   

  cid 

  cid 

  

  cid 
 cid 

  

 cid    cid 

also

de ne

We
vector
                             cid  With this notation  we
can rewrite equation   for the kth output as

   dimensional

the

fk     

       cid zp    wj cid   

 cid 
  jZ   wj

 cid cid 

  

  tr     Ak 

 

  tr

    

wj cid 

   

  

dimensional matrix Ak  cid  

where in the  nal step  we have de ned the         
     Observe that fk
now depends linearly on the matrix parameter Ak  Moreover  the matrix Ak has rank at most    due to the parameter

   wj cid 

Convexi ed Convolutional Neural Networks

Figure   The kth output of   CNN fk        can be expressed as the product between   matrix        RP    whose rows are
features of the input patches and   rankr matrix Ak   Rd     which is made up of the  lter weights  wj  and coef cients  ak     
as illustrated  Due to the parameter sharing intrinsic to CNNs  the matrix Ak inherits   low rank structure  which can be encouraged via
convex relaxation using the nuclear norm 

sian RBF kernel  and   suf ciently smooth activation function   we are able to show that the  lter        cid   cid      cid 
is contained in the RKHS induced by the kernel function   
See Section   for the choice of the kernel function and the
activation function  Let      zp xi                    
be the set of patches in the training dataset  The representer theorem then implies that for any patch zp xi       the
function value can be represented by

  zp xi   

   cid   cid      

ci cid   cid   zp xi  zp cid xi cid   

 cid 

for some coef cients  ci cid   cid   cid   cid       Filters of the
form   are members of the RKHS  because they are linear combinations of basis functions    cid       zp cid xi cid 
Such  lters are parametrized by    nite set of coef cients
 ci cid   cid   cid   cid       which can be estimated via empirical
risk minimization 
Let     RnP nP be the symmetric kernel matrix  where
with rows and columns indexed by the examplepatch index pair                     The entry at row        and
column    cid    cid  of matrix   is equal to   zp xi  zp cid xi cid 
So as to avoid rederiving everything in the kernelized setting  we perform   reduction to the linear setting of Section   Consider   factorization     QQ cid  of the kernel
matrix  where     RnP    one example is the Cholesky
factorization with     nP   We can interpret each row
         Rm as   feature vector in place of the original
zp xi    Rd  and rewrite equation   as
  zp xi     cid          cid  where    

 cid 

ci cid   cid     cid   cid 

   cid   cid 

In order to learn the  lter    it suf ces to learn the mdimensional vector    To do this  de ne patch matrices
  xi    RP   for each         so that its pth row
is        Then the problem reduces to learning   linear
 lter with coef cient vector    Carrying out all of Sec 

tion   solving the ERM gives us   parameter matrix
    Rm      The only difference is that  cid norm constraint     needs to be adapted to the norm of the RKHS 
See Appendix   for details 
At test time  given   new input     Rd  we can compute  
patch matrix        RP   as follows 
  The pth row of this matrix is the feature vector for
patch    which is equal to     zp      Rm  where
for any patch    the vector      is de ned as   nP  
dimensional vector whose       th coordinate is equal
to      zp xi  We note that if   is an instance xi in
the training set  then the vector     zp    is exactly
equal to        Thus the mapping      applies to both
training and testing 

  We can then compute the predictor fk      tr     Ak 
via equation   Note that we do not explicitly need
to compute the  lter values hj zp    to compute the
output under the CCNN 

Retrieving  lters  When we learn multilayer CCNNs
 Section   we need to compute the  lters hj explicitly
in order to form the inputs to the next layer  Recall from
Section   that the column space of matrix   corresponds
to parameters of the convolutional layer  and the row space
of   corresponds to parameters of the output layer  Thus 
once we obtain the parameter matrix    we compute   rank 

  approximation      cid   cid    cid  Then set the jth  lter hj to
where  cid Uj   Rm is the jth column of matrix  cid    and
matrix  cid    cid  encodes parameters of the output layer  thus

       represents the feature vector for patch    The

for any patch     Rd 

the mapping

   cid   cid cid Uj        cid 

 

 If   is   patch in the training set  namely     zp xi  then we

have equation                

fk   tr cid         zP   Pd   wrd   lters     rrP patches cid AkConvexi ed Convolutional Neural Networks

   kernel function    regularization

Algorithm   Learning twolayer CCNNs
Input  Data  xi  yi  
parameter       number of  lters   
  Construct   kernel matrix     RnP nP such that the entry
at column        and row    cid    cid  is equal to
  zp xi  zp cid   xi cid    Compute   factorization     QQ cid  or
an approximation     QQ cid  where     RnP   
  For each xi  construct patch matrix   xi    RP   whose
pth row is the       th row of    where    is de ned in
Section  

  Solve the following optimization problem to obtain   matrix

 

 cid   cid  

  cid cid tr   xi            tr   xi Ad   cid  yi
 cid       
  Compute   rankr approximation  cid   cid    cid     cid   where
 cid     Rm   and  cid     RP     
Output  predictor  cid fccnn     cid tr     cid            tr     cid Ad   cid 
and the convolutional layer output         cid   cid     cid 

 cid 

  

 

 cid      cid             cid Ad   
 cid     argmin
  cid 

 cid      where

doesn   appear in the  lter expression  
It is important to note that the  lter retrieval is not unique  because
the rankr approximation of the matrix   is not unique 
The heuristic we suggest is to form the singular value de 

composition           cid  then de ne  cid   to be the  rst  
    Rd  the resulting output is         cid   cid     cid   

columns of   
When we apply all of the    lters to all patches of an input
this is an       matrix whose element at row   and column
  is equal to hj zp   

  Algorithm

The algorithm for learning   twolayer CCNN is summarized in Algorithm   it is   formalization of the steps described in Section   In order to solve the optimization
problem   the simplest approach is via projected gradient descent  At iteration    using   step size        we
form the new matrix At  based on the previous iterate At
according to 

 cid 

 cid 
At         cid   At 

 

 

At      

Here    cid   denotes the gradient of the objective function

de ned in   and    denotes the Euclidean projection
onto the nuclear norm ball       cid   cid       This nuclear
norm projection can be obtained by  rst computing the singular value decomposition of    and then projecting the
vector of singular values onto the  cid ball  This latter projection step can be carried out ef ciently by the algorithm
of Duchi et al    There are other ef cient optimiza 

tion algorithms  Duchi et al    Xiao   Zhang   
for solving the problem   All these algorithms can be
executed in   stochastic fashion  so that each gradient step
processes   minibatch of examples 
The computational complexity of each iteration depends on
the width   of the matrix    Setting     nP allows
us to solve the exact kernelized problem  but to improve
the computation ef ciency  we can use Nystr om approximation  Drineas   Mahoney    or random feature approximation  Rahimi   Recht    both are randomized
methods to obtain   talland thin matrix     RnP   such
that     QQ cid  Typically  the parameter   is chosen to be
much smaller than nP   In order to compute the matrix   
the Nystr om approximation method takes     nP   time 
The random feature approximation takes   mnP    time 
but can be improved to   mnP log    time using the fast
Hadamard transform  Le et al    The time complexity
of project gradient descent also scales with   rather than
with nP  

  Theoretical results

In this section  we upper bound the generalization error of
Algorithm   proving that it converges to the best possible
generalization error of CNN  We focus on the binary classi cation case where the output dimension is       
The learning of CCNN requires   kernel function    We
consider kernel functions whose associated RKHS is large
enough to contain any function of the following form     cid 
  cid      cid  where   is an arbitrary polynomial function and
    Rd  is an arbitrary vector  As   concrete example  we
consider the inverse polynomial kernel 

       cid   

 

     cid      cid cid   

 cid   cid     cid   cid cid     

 

This kernel was studied by ShalevShwartz et al    for
learning halfspaces  and by Zhang et al      for learning fullyconnected neural networks  We also consider the
Gaussian RBF kernel 

       cid      cid     cid cid 

   cid   cid     cid   cid cid     

 
As shown by Appendix    the inverse polynomial kernel
and the Gaussian kernel satisfy the above notion of richness  We focus on these two kernels for the theoretical
analysis 

Let  cid fccnn be the CCNN that minimizes the empirical
tions  the generalization error of  cid fccnn is comparable to that

risk   using one of the two kernels above  Our main
theoretical result is that for suitably chosen activation func 

of the best CNN model  In particular  we consider the following types of activation functions  

 We can treat the multiclass case by performing   standard

oneversus all reduction to the binary case 

Convexi ed Convolutional Neural Networks

    arbitrary polynomial functions       used by Chen  

Manning   Livni et al   

 

    erf function  erf        

by Sopena et al    Isa et al   

    sinusoid activation function       sin          used

 cid   
      smoothed hinge loss  sh       cid   

dz  which represents   close approximation to the sigmoid function  Zhang et al     

   erf      
 dz  which represents   close approximation to the
ReLU function  Zhang et al     

      

   

the above activation functions         cid 

To understand how these activation functions pair with our
choice of kernels  we consider polynomial expansions of
   ajtj  and
note that the smoothness of these functions are characterized by the rate of their coef cients  aj 
   converging to
zero  If   is   polynomial in category     then the richness
of the RKHS guarantees that it contains the class of  lters
activated by function   If   is   nonpolynomial function
in categories         then as Appendix   shows  the
RKHS contains the  lter only if the coef cients  aj 
  
converge quickly enough to zero  the criterion depends on
the concrete choice of the kernel  Concretely  the inverse
polynomial kernel is shown to capture all of the four categories of activations 
thus              and     are all
are referred as valid activation functions for the inverse
polynomial kernel  The Gaussian kernel induces   smaller
RKHS  so only     and     are valid activation functions
for the Gaussian kernel  In contrast  the sigmoid function
and the ReLU function are not valid for either kernel  because their polynomial expansions fail to converge quickly
enough  or more intuitively speaking  because they are not
smooth enough to be contained in the RKHS 
In the
We are ready to state the main theoretical result 
theorem statement  we use        RP   to denote the
random kernel matrix obtained from an input vector    
Rd  drawn randomly from the population  More precisely 
the       th entry of      is given by   zp    zq   
Theorem   Assume that the loss function       is LLipchitz continuous for every         and that   is the
inverse polynomial kernel or the Gaussian kernel  For any
valid activation function   there is   constant      such
that by choosing hyperparameter             in Algorithm   the expected generalization error is at most
EX               

EX      cid fccnn          inf

  LC       cid log nP   EX  cid     cid 

   

 

  Fcnn
 

 

  cid 

  

 cid 
   cid    cid 
  cid 

  

 cid 

 

tains the class of CNNs  This function class is de ned as 
   phj zp              

Fccnn  

and

 cid   cid cid hj cid             

 

  

where  cid cid   is the norm of the RKHS associated with the
kernel  This new function class relaxes the class of CNNs
in two ways    the  lters are relaxed to belong to the
RKHS  and   the  cid norm bounds on the weight vectors
are replaced by   single constraint on  cid   cid  and  cid hj cid   
must be an empirical risk minimizer of Fccnn  This property
holds even though equation   de nes   nonparametric

We prove the following property for the predictor  cid fccnn  it
function class Fccnn  while Algorithm   optimizes  cid fccnn in

generalization loss of  cid fccnn converges to the least possible

  parametric function class 
Second  we characterize the Rademacher complexity of
this new function class Fccnn  proving an upper bound for
it based on the matrix concentration theory  Combining
this bound with the classical Rademacher complexity theory  Bartlett   Mendelson    we conclude that the
generalization error of Fccnn  The latter loss is bounded by
the generalization loss of CNNs  because Fcnn   Fccnn 
which establishes the theorem  See the full version of this
paper  Zhang et al      for   rigorous proof of Theo 
 cid 
rem  

Remark on activation functions 
It is worth noting that
the quantity      depends on the activation function  
and more precisely  depends on the convergence rate of
the polynomial expansion of   Appendix   shows that
if   is   polynomial function of degree  cid  then       
    cid 
  If   is the sinusoid function  the erf function or
the smoothed hinge loss  then the quantity      will be
exponential in    From an algorithmic perspective  we
don   need to know the activation function for executing
Algorithm   From   theoretical perspective  however  the
choice of   is relevant from the point of Theorem   to com 

pare  cid fccnn with the best CNN  whose representation power

is characterized by the choice of   Therefore  if   CNN
with   lowdegree polynomial   performs well on   given
task  then CCNN also enjoys correspondingly strong generalization  Empirically  this is actually borne out  in Section   we show that the quadratic activation function performs almost as well as the ReLU function for digit classi 
 cation 

where       is   universal constant 

Proof sketch The proof of Theorem   consists of two
parts  First  we consider   larger function class that con 

Remark on parameter sharing 
In order to demonstrate
the importance of parameter sharing  consider   CNN without parameter sharing  so that we have  lter weights wj  
for each  lter index   and patch index    With this change 

Convexi ed Convolutional Neural Networks

the new CNN output   is

  cid 

  cid 

  

  

       

       cid 

  pzp   

 cid  
             cid 

where          and wj     Rd  Note that the hidden layer of this new network has   times more parameters than that of the convolutional neural network
with parameter sharing 
These networks without parameter sharing can be learned by the recursive kernel
 cid  
method proposed by Zhang et al      Their paper
shows that under the norm constraints  cid wj cid      cid 
 cid Kmax   
  and
  the excess risk of the recursive kernel method is at most   LC   cid 
where Kmax   maxz cid   cid          is the maximal value
of the kernel function  Plugging in the norm constraints
    Thus  the expected risk of the estimated  cid   is
of the function class Fcnn  we have   cid 
   
   
EX      cid             inf
bounded by 

EX               

       and   cid 

   cid 

 

  

 

  Fcnn
  LC      
 

 

 

 

  Kmax

 

 

 

 cid   cid     cid 

Comparing this bound to Theorem   we see that  apart
from the logarithmic terms  they differ in the multiplicative factors of
matrix      is    dimensional  we have
 cid     cid    max
     

  Kmax versus cid   cid     cid  Since the
 cid 

   zp    zq        Kmax 
     
 

This demonstrates that

  Kmax is always greater than
In general  the  rst term can be up to
factor of
  times greater  which implies that the sample
complexity of the recursive kernel method is up to   times
greater than that of the CCNN  This difference is intuitive
given that the recursive kernel method learns   model with
  times more parameters  Although comparing the upper
bounds doesn   rigorously show that one method is better
than the other  it gives intuition for understanding the importance of parameter sharing 

 

  Learning multilayer CCNNs
In this section  we describe   heuristic method for learning
CNNs with more layers  The idea is to estimate the parameters of the convolutional layers incrementally from bottom to top  Before presenting the multilayer algorithm  we
present two extensions  average pooling and multichannel
inputs 

Average pooling  Average pooling is   technique to reduce the output dimension of the convolutional layer from
dimensions       to dimensions    cid      with    cid       
For the CCNN model  if we apply average pooling after

   kernel function    number of layers

Algorithm   Learning multilayer CCNNs
Input Data  xi  yi  
   regularization parameters            Rm  number of  lters
           rm 
De ne           For each layer                 
  Train   twolayer network by Algorithm   taking

   as training examples and Rs  rs as

 Hs xi  yi  
parameters  Let Hs be the output of the convolutional layer

and  cid fs be the predictor 
Output  Predictor  cid fm and the top layer output Hm 

the convolutional layer  then the kth output of the CCNN
model becomes tr GZ   Ak  where     RP  cid   is the
pooling matrix  Thus  performing   pooling operation requires only replacing every matrix   xi  in problem  
by the pooled matrix GZ xi  Note that the linearity of
the CCNN allows us to effectively pool before convolution  even though for the CNN  pooling must be done after
applying the nonlinear  lters  The resulting ERM problem
is still convex  and the number of parameters have been reduced by      cid fold 

Processing multichannel inputs 
If our input has  
channels  corresponding to RGB colors  for example  then
the input becomes   matrix     RC    The cth row of
matrix    denoted by        Rd  is   vector representing
the cth channel  We de ne the multichannel patch vector
as   concatenation of patch vectors for each channel 
zp       zp            zp        RCd 

  

form  cid  

Then we construct the feature matrix      using the concatenated patch vectors  zp    
   From here  everything
else of Algorithm   remains the same  We note that this
approach learns   convex relaxation of  lters taking the
  cid wc  zp     cid  parametrized by the vectors
 wc  
Multilayer CCNN  Given these extensions  we are
ready to present the algorithm for learning multilayer CCNNs  summarized in Algorithm   For each layer    we call
Algorithm   using the output of previous convolutional layers as input note that this consists of   channels  one from
each previous  lter  thus we must use the multichannel
extension  Algorithm   outputs   new convolutional layer
along with   prediction function  which is kept only at the
last layer  We optionally use averaging pooling after each
successive layer  to reduce the output dimension of the convolutional layers 

  Experiments
In this section  we compare the CCNN approach with
other methods on the MNIST dataset and more challenging variations  VariationsMNIST  including adding white
noise  rand  random rotation  rot  random image back 

Convexi ed Convolutional Neural Networks

SVMrbf  Vincent et al   
NN   Vincent et al   
CNN   ReLU 
CCNN 
TIRBM  Sohn   Lee   
SDAE   Vincent et al   
ScatNet   Bruna   Mallat   
PCANet   Chan et al   
CNN   ReLU 
CNN   Quad 
CCNN 

 

 

 

 

rand

rot

img rot
basic
img
       
 
 
       
 
 
   
         
 
 
 
 
 
 
 

 
 
   
 
   
   
 
 
 
 
 
 
 
   
 

 
 
 
 
 
 

Table   Classi cation error on the basic MNIST and its four variations  The best performance within each block is bolded   ReLU  and
 Quad  denote using the ReLU and quadratic activation functions  respectively 

ground  img  or combining the last two  img rot  For
all datasets  we use   images for training    images for validation and   images for testing  This
       partitioning is standard for MNIST variations  VariationsMNIST 
For the CCNN method and the baseline CNN method  we
train twolayer and threelayer models respectively  The
models with   convolutional layers are denoted by CCNNk and CNNk  Each convolutional layer is constructed on
      patches with unit stride  followed by       average
pooling  The  rst and the second convolutional layers contains   and    lters  respectively  The loss function is
chosen as the  class logistic loss  We use Gaussian kernel for the CCNN  The feature matrix      is constructed
via random feature approximation  Rahimi   Recht   
with dimension       for the  rst convolutional layer
and       for the second  Before training each CCNN
layer  we preprocess the input vectors zp xi  using local
contrast normalization and ZCA whitening  Coates et al 
  The convex optimization problem is solved by projected SGD with minibatches of size   Code and reproducible experiments are available on the CodaLab platform 
As   baseline approach  the CNN models are activated by
the ReLU function       max     or the quadratic
function          We train them using minibatch
SGD  The input images are preprocessed by global contrast normalization and ZCA whitening  Srivastava et al 
  We compare our method against several alternative baselines  The CCNN  model is compared against
an SVM with the Gaussian RBF kernel  SVMrbf   and  
fully connected neural network with one hidden layer  NN 
  The CCNN  model is compared against methods that
report the stateof theart results on these datasets  including the translationinvariant RBM model  TIRBM   Sohn
  Lee    the stacked denoising autoencoder with

 http worksheets codalab org 

worksheets       fba     aacde 

three hidden layers  SDAE   Vincent et al    the
ScatNet  model  Bruna   Mallat    and the PCANet 
  model  Chan et al   
Table   shows the classi cation errors on the test set  The
models are grouped with respect to the number of layers
that they contain  For models with one convolutional layer 
the errors of CNN  are signi cantly lower than that of
NN  highlighting the bene ts of local  lters and parameter sharing  The CCNN  model outperforms CNN  on
all datasets  For models with two or more hidden layers 
the CCNN  model outperforms CNN  on all datasets 
and is competitive against the stateof theart 
In particular  it achieves the best accuracy on the rand  img and
img rot dataset  and is comparable to the stateof theart
on the remaining two datasets  Further adding   third convolutional layer doesn   notibly improve the performance
on these datasets 
In Section   we showed that if the activation function   is
  polynomial function  then the CCNN  which does not depend on   requires lower sample complexity to match the
performance of the best possible CNN using   More precisely  if   is   degree cid  polynomial  then      in the upper bound will be controlled by     cid  This motivates us
to study the performance of lowdegree polynomial activations  Table   shows that the CNN  model with   quadratic
activation function achieves error rates comparable to that
with   ReLU activation  CNN   Quad  outperforms CNN 
   ReLU  on the basic and rand datasets  and is only
slightly worse on the rot and img dataset  Since the performance of CCNN matches that of the best possible CNN 
the good performance of the quadratic activation in part explains why the CCNN is also good 

Acknowledgements  MJW and YZ were partially supported by the Of ce of Naval Research Grant DOD ONRN  and the NSF Grant NSFDMS  PL and
YZ were partially supported by the Microsoft Faculty Fellowship 

Convexi ed Convolutional Neural Networks

References
Aslan   Ozlem  Cheng  Hao  Zhang  Xinhua  and Schuurmans  Dale  Convex twolayer modeling  In Advances in
Neural Information Processing Systems  pp   
 

Aslan 

 Ozlem  Zhang  Xinhua  and Schuurmans  Dale 
In AdConvex deep learning via normalized kernels 
vances in Neural Information Processing Systems  pp 
   

Bartlett  Peter   and Mendelson  Shahar  Rademacher and
Gaussian complexities  Risk bounds and structural results  The Journal of Machine Learning Research   
   

Bengio  Yoshua  Roux  Nicolas    Vincent  Pascal  Delalleau  Olivier  and Marcotte  Patrice  Convex neural netIn Advances in Neural Information Processing
works 
Systems  pp     

Blum  Avrim   and Rivest  Ronald    Training    node
neural network is NPcomplete  Neural Networks   
   

Bottou    eon  Online learning and stochastic approximations  Online learning in neural networks   
 

Bruna  Joan and Mallat  St ephane  Invariant scattering convolution networks  Pattern Analysis and Machine Intelligence  IEEE Transactions on     

Chan  TsungHan  Jia  Kui  Gao  Shenghua  Lu  Jiwen 
Zeng  Zinan  and Ma  Yi  Pcanet    simple deep learning baseline for image classi cation  IEEE Transactions
on Image Processing     

Chen  Danqi and Manning  Christopher      fast and accurate dependency parser using neural networks  In Proceedings of the   Conference on Empirical Methods
in Natural Language Processing  EMNLP  volume  
pp     

Coates  Adam  Lee  Honglak  and Ng  Andrew    An
analysis of singlelayer networks in unsupervised feature
learning  Ann Arbor     

Duchi  John  ShalevShwartz  Shai  Singer  Yoram  and
Chandra  Tushar  Ef cient projections onto the  cid ball
for learning in high dimensions  In Proceedings of the
 th International Conference on Machine Learning  pp 
  ACM   

Duchi  John  Hazan  Elad  and Singer  Yoram  Adaptive
subgradient methods for online learning and stochastic
optimization  The Journal of Machine Learning Research     

Fahlman  Scott    An empirical study of learning speed in
backpropagation networks  Journal of Heuristics   

Hinton  Geoffrey  Deng  Li  Yu  Dong  Dahl  George   
Mohamed  Abdelrahman  Jaitly  Navdeep  Senior  Andrew  Vanhoucke  Vincent  Nguyen  Patrick  Sainath 
Tara    et al  Deep neural networks for acoustic modeling in speech recognition  The shared views of four
research groups  Signal Processing Magazine  IEEE   
   

Isa  IS  Saad     Omar     Osman  MK  Ahmad  KA  and
Sakim  HA Mat  Suitable mlp network activation functions for breast cancer and thyroid disease detection 
In   Second International Conference on Computational Intelligence  Modelling and Simulation  pp   
   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In Advances in Neural Information Processing
Systems  pp     

Lawrence  Steve  Giles    Lee  Tsoi  Ah Chung  and Back 
Andrew    Face recognition    convolutional neuralnetwork approach  Neural Networks  IEEE Transactions
on     

Le  Quoc  Sarl os  Tam as  and Smola  Alex  Fastfoodapproximating kernel expansions in loglinear time 
In
Proceedings of the International Conference on Machine
Learning   

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  Proceedings of the IEEE   
   

Daniely  Amit  Frostig  Roy  and Singer  Yoram  Toward
deeper understanding of neural networks  The power
of initialization and   dual view on expressivity  arXiv
preprint arXiv   

Livni  Roi  ShalevShwartz  Shai  and Shamir  Ohad  On
the computational ef ciency of training neural networks 
In Advances in Neural Information Processing Systems 
pp     

Drineas  Petros and Mahoney  Michael    On the Nystr om
method for approximating   Gram matrix for improved
kernelbased learning  The Journal of Machine Learning
Research     

Mairal  Julien  Koniusz  Piotr  Harchaoui  Zaid  and
Schmid  Cordelia  Convolutional kernel networks 
In
Advances in Neural Information Processing Systems  pp 
   

Convexi ed Convolutional Neural Networks

Xiao  Lin and Zhang  Tong    proximal stochastic gradient method with progressive variance reduction  SIAM
Journal on Optimization     

Zhang  Yuchen  Lee  Jason    and Jordan  Michael     cid 
regularized neural networks are improperly learnable in
In Proceedings on the  rd Internapolynomial time 
tional Conference on Machine Learning     

Zhang  Yuchen  Liang  Percy  and Wainwright  Martin   
CoRR 
Convexi ed convolutional neural networks 
abs      URL http arxiv org 
abs 

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David 
Rusu  Andrei    Veness  Joel  Bellemare  Marc   
Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas   
Ostrovski  Georg  et al  Humanlevel control through
deep reinforcement learning  Nature   
   

Rahimi  Ali and Recht  Benjamin  Random features for
largescale kernel machines  In Advances in Neural Information Processing Systems  pp     

ShalevShwartz  Shai  Shamir  Ohad  and Sridharan 
Karthik  Learning kernelbased halfspaces with the  
  loss  SIAM Journal on Computing   
 

Silver  David  Huang  Aja  Maddison  Chris    Guez 
Arthur  Sifre  Laurent  Van Den Driessche  George 
Schrittwieser  Julian  Antonoglou  Ioannis  Panneershelvam  Veda  Lanctot  Marc  et al  Mastering the game of
Go with deep neural networks and tree search  Nature 
   

Sohn  Kihyuk and Lee  Honglak  Learning invariant representations with local transformations  In Proceedings of
the  th International Conference on Machine Learning
 ICML  pp     

Sopena  Josep    Romero  Enrique  and Alquezar  Rene 
Neural networks with periodic and monotonic activation
functions    comparative study in classi cation problems  In ICANN   pp     

Srivastava  Nitish  Hinton  Geoffrey  Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan  Dropout 
  simple way to prevent neural networks from over tting  The Journal of Machine Learning Research   
   

VariationsMNIST  Variations on the MNIST digits  http 

 www iro umontreal ca lisa twiki 
bin view cgi Public MnistVariations 
 

Vincent  Pascal  Larochelle  Hugo  Lajoie  Isabelle  Bengio  Yoshua  and Manzagol  PierreAntoine  Stacked
denoising autoencoders  Learning useful representations
in   deep network with   local denoising criterion  The
Journal of Machine Learning Research   
 

Wang  Tao  Wu  David    Coates  Andrew  and Ng  Andrew    Endto end text recognition with convolutional
neural networks  In Pattern Recognition  ICPR   
 st International Conference on  pp    IEEE 
 

