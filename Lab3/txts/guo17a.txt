OnCalibrationofModernNeuralNetworksChuanGuo GeoffPleiss YuSun KilianQ Weinberger AbstractCon dencecalibration theproblemofpredictingprobabilityestimatesrepresentativeofthetruecorrectnesslikelihood isimportantforclassi cationmodelsinmanyapplications Wediscoverthatmodernneuralnetworks unlikethosefromadecadeago arepoorlycalibrated Throughextensiveexperiments weobservethatdepth width weightdecay andBatchNormalizationareimportantfactorsin uencingcalibration Weevaluatetheperformanceofvariouspostprocessingcalibrationmethodsonstate ofthe artarchitectureswithimageanddocumentclassi cationdatasets Ouranalysisandexperimentsnotonlyofferinsightsintoneuralnet worklearning butalsoprovideasimpleandstraightforwardrecipeforpracticalsettings onmostdatasets temperaturescaling asingleparametervariantofPlattScaling issurprisinglyeffectiveatcalibratingpredictions IntroductionRecentadvancesindeeplearninghavedramaticallyimprovedneuralnetworkaccuracy Simonyan Zisserman Srivastavaetal Heetal Huangetal Asaresult neuralnetworksarenowentrustedwithmakingcomplexdecisionsinapplications suchasobjectdetection Girshick speechrecognition Hannunetal andmedicaldiagnosis Caruanaetal Inthesesettings neuralnetworksareanessentialcomponentoflargerdecisionmakingpipelines Inrealworlddecisionmakingsystems classi cationnetworksmustnotonlybeaccurate butalsoshouldindicatewhentheyarelikelytobeincorrect Asanexample consideraself drivingcarthatusesaneuralnetworktodetectpedestriansandotherobstructions Bojarskietal Equalcontribution alphabeticalorder CornellUniversity Correspondenceto ChuanGuo cg cornell edu GeoffPleiss geoff cs cornell edu YuSun ys cornell edu Proceedingsofthe thInternationalConferenceonMachineLearning Sydney Australia PMLR Copyright bytheauthor   ofSamplesAvg con denceAccuracyLeNet CIFAR Avg con denceAccuracyResNet CIFAR AccuracyError OutputsGap Error OutputsGapCon denceFigure Con dencehistograms top andreliabilitydiagrams bottom fora layerLeNet left anda layerResNet right onCIFAR Refertothetextbelowfordetailedillustration Ifthedetectionnetworkisnotabletocon dentlypredictthepresenceorabsenceofimmediateobstructions thecarshouldrelymoreontheoutputofothersensorsforbraking Alternatively inautomatedhealthcare controlshouldbepassedontohumandoctorswhenthecon denceofadiseasediagnosisnetworkislow Jiangetal Specifically anetworkshouldprovideacalibratedcon dencemeasureinadditiontoitsprediction Inotherwords theprobabilityassociatedwiththepredictedclasslabelshouldre ectitsgroundtruthcorrectnesslikelihood Calibratedcon denceestimatesarealsoimportantformodelinterpretability Humanshaveanaturalcognitiveintuitionforprobabilities Cosmides Tooby Goodcon denceestimatesprovideavaluableextrabitofinformationtoestablishtrustworthinesswiththeuser especiallyforneuralnetworks whoseclassi cationdecisionsareoftendif culttointerpret Further goodprobabilityestimatescanbeusedtoincorporateneuralnetworksintootherprobabilisticmodels Forexample onecanimproveperformancebycombiningnetworkoutputswithalanguagemodelinspeechrecognition Hannunetal Xiongetal orwithcamerainformationforobjectdetection Kendall Cipolla In NiculescuMizil Caruana showedthatneuralnetworkstypicallyproducewellcalibratedproba bilitiesonbinaryclassi cationtasks Whileneuralnetworkstodayareundoubtedlymoreaccuratethantheywereadecadeago wediscoverwithgreatsurprisethatmodernneuralnetworksarenolongerwell calibrated ThisisvisualizedinFigure whichcomparesa layerLeNet left LeCunetal witha layerResNet right Heetal ontheCIFAR dataset Thetoprowshowsthedistributionofpredictioncon dence     probabilitiesassociatedwiththepredictedlabel ashistograms Theaveragecon denceofLeNetcloselymatchesitsaccuracy whiletheaveragecon denceoftheResNetissubstantiallyhigherthanitsaccuracy Thisisfurtherillustratedinthebottomrowreliabilitydiagrams DeGroot Fienberg NiculescuMizil Caruana whichshowaccuracyasafunctionofcon dence WeseethatLeNetiswellcalibrated ascon dencecloselyapproximatestheexpectedaccuracy     thebarsalignroughlyalongthediagonal Ontheotherhand theResNet saccuracyisbetter butdoesnotmatchitscon dence Ourgoalisnotonlytounderstandwhyneuralnetworkshavebecomemiscalibrated butalsotoidentifywhatmethodscanalleviatethisproblem Inthispaper wedemonstrateonseveralcomputervisionandNLPtasksthatneu ralnetworksproducecon dencesthatcannotrepresenttrueprobabilities Additionally weofferinsightandintuitionintonetworktrainingandarchitecturaltrendsthatmaycausemiscalibration Finally wecomparevariouspostprocessingcalibrationmethodsonstate ofthe artneuralnetworks andintroduceseveralextensionsofourown Surprisingly we ndthatasingleparametervariantofPlattscaling Plattetal whichwerefertoastemperaturescaling isoftenthemosteffectivemethodatobtainingcalibratedprobabilities Becausethismethodisstraightforwardtoimplementwithexistingdeeplearningframeworks itcanbeeasilyadoptedinpracticalsettings De nitionsTheproblemweaddressinthispaperissupervisedmulticlassclassi cationwithneuralnetworks TheinputX XandlabelY     arerandomvariablesthatfollowagroundtruthjointdistribution           Lethbeaneuralnetworkwithh       where Yisaclasspredictionand Pisitsassociatedcon dence     probabilityofcorrectness Wewouldlikethecon denceestimate Ptobecalibrated whichintuitivelymeansthat Prepresentsatrueprobability Forexample given predictions eachwithcon denceof weexpectthat shouldbecorrectlyclassi ed Moreformally wede neperfectcalibrationasP cid         cid     wheretheprobabilityisoverthejointdistribution Inallpracticalsettings achievingperfectcalibrationisimpossible Additionally theprobabilityin cannotbecomputedusing nitelymanysamplessince Pisacontinuousrandomvariable Thismotivatestheneedforempiricalapproximationsthatcapturetheessenceof ReliabilityDiagrams     Figure bottom areavisualrepresentationofmodelcalibration DeGroot Fienberg NiculescuMizil Caruana Thesediagramsplotexpectedsampleaccuracyasafunctionofcon dence Ifthemodelisperfectlycalibrated     if holds thenthediagramshouldplottheidentityfunction Anydeviationfromaperfectdiagonalrepresentsmiscalibration Toestimatetheexpectedaccuracyfrom nitesamples wegrouppredictionsintoMintervalbins eachofsize   andcalculatetheaccuracyofeachbin LetBmbethesetofindicesofsampleswhosepredictioncon dencefallsintotheintervalIm     mM TheaccuracyofBmisacc Bm Bm Xi Bm yi yi where yiandyiarethepredictedandtrueclasslabelsforsamplei Basicprobabilitytellsusthatacc Bm isanunbiasedandconsistentestimatorofP       Im Wede netheaveragecon dencewithinbinBmasconf Bm Bm Xi Bm pi where piisthecon denceforsamplei acc Bm andconf Bm approximatethelefthandandright handsidesof respectivelyforbinBm Therefore aperfectlycalibratedmodelwillhaveacc Bm conf Bm forallm   Notethatreliabilitydiagramsdonotdisplaytheproportionofsamplesinagivenbin andthuscannotbeusedtoestimatehowmanysamplesarecalibrated ExpectedCalibrationError ECE Whilereliabilitydiagramsareusefulvisualtools itismoreconvenienttohaveascalarsummarystatisticofcalibration Sincestatisticscomparingtwodistributionscannotbecomprehensive previousworkshaveproposedvariants eachwithauniqueemphasis Onenotionofmiscalibrationisthedifferenceinexpectationbetweencon denceandaccuracy       Ph cid cid cid   cid         cid   cid cid cid   ExpectedCalibrationError Naeinietal orECE approximates bypartitioningpredictionsintoMequallyspacedbins similartothereliabilitydiagrams and Depth Error ECEVaryingDepthResNetCIFAR ErrorECE FiltersperlayerVaryingWidthResNet CIFAR ErrorECEWithoutWithBatchNormalizationUsingNormalizationConvNetCIFAR ErrorECE WeightdecayVaryingWeightDecayResNet CIFAR ErrorECEFigure Theeffectofnetworkdepth farleft width middleleft BatchNormalization middleright andweightdecay farright onmiscalibration asmeasuredbyECE lowerisbetter takingaweightedaverageofthebins accuracy con dencedifference Moreprecisely ECE MXm Bm   cid cid cid cid acc Bm conf Bm cid cid cid cid wherenisthenumberofsamples Thedifferencebetweenaccandconfforagivenbinrepresentsthecalibrationgap redbarsinreliabilitydiagrams     Figure WeuseECEastheprimaryempiricalmetrictomeasurecalibration SeeSectionS formoreanalysisofthismetric MaximumCalibrationError MCE Inhighriskap plicationswherereliablecon dencemeasuresareabsolutelynecessary wemaywishtominimizetheworstcasedeviationbetweencon denceandaccuracy maxp cid cid cid   cid         cid   cid cid cid TheMaximumCalibrationError Naeinietal orMCE estimatesanupperboundofthisdeviation SimilarlytoECE thisapproximationinvolvesbinning MCE maxm   acc Bm conf Bm Inreliabilitydiagrams MCEmeasuresthelargestcalibrationgap redbars acrossallbins whereasECEmeasuresaweightedaverageofallgaps Forperfectlycalibratedclassi ers MCEandECEbothequal Negativeloglikelihoodisastandardmeasureofaprobabilisticmodel squality Friedmanetal Itisalsoreferredtoasthecrossentropylossinthecontextofdeeplearning Bengioetal Givenaprobabilisticmodel     andnsamples NLLisde nedas   nXi log yi xi Itisastandardresult Friedmanetal that inexpectation NLLisminimizedifandonlyif     recoversthegroundtruthconditionaldistribution     ObservingMiscalibrationThearchitectureandtrainingproceduresofneuralnetworkshaverapidlyevolvedinrecentyears InthissectionweidentifysomerecentchangesthatareresponsibleforthemiscalibrationphenomenonobservedinFigure Thoughwecannotclaimcausality we ndthatmodelcapacityandlackofregularizationarecloselyrelatedtomodel mis calibration Modelcapacity Themodelcapacityofneuralnetworkshasincreasedatadramaticpaceoverthepastfewyears Itisnowcommontoseenetworkswithhundreds ifnotthousandsoflayers Heetal Huangetal andhundredsofconvolutional ltersperlayer Zagoruyko Komodakis Recentworkshowsthatverydeeporwidemodelsareabletogeneralizebetterthansmallerones whileexhibitingthecapacitytoeasily tthetrainingset Zhangetal Althoughincreasingdepthandwidthmayreduceclassi cationerror weobservethattheseincreasesnegativelyaffectmodelcalibration Figure displayserrorandECEasafunctionofdepthandwidthonaResNettrainedonCIFAR Thefarleft gurevariesdepthforanetworkwith convolutional ltersperlayer whilethemiddleleft gure xesthedepthat layersandvariesthenumberofconvolutional ltersperlayer Thougheventhesmallestmodelsinthegraphexhibitsomedegreeofmiscalibra tion theECEmetricgrowssubstantiallywithmodelcapacity Duringtraining afterthemodelisabletocorrectlyclassify almost alltrainingsamples NLLcanbefurtherminimizedbyincreasingthecon denceofpredictions IncreasedmodelcapacitywilllowertrainingNLL andthusthemodelwillbemore over con dentonaverage BatchNormalization Ioffe Szegedy improvestheoptimizationofneuralnetworksbyminimizingdistributionshiftsinactivationswithintheneuralnetwork shid  EpochError     NLL  scaled NLL Overfitting on CIFAR   Test errorTest NLLFigure TesterrorandNLLofa layerResNetwithstochasticdepthonCIFAR duringtraining NLLisscaledbyaconstantto tinthe gure Learningratedropsby xatepochs and Theshadedareamarksbetweenepochsatwhichthebestvalidationlossandbestvalidationerrorareproduced denlayers Recentresearchsuggeststhatthesenormalizationtechniqueshaveenabledthedevelopmentofverydeeparchitectures suchasResNets Heetal andDenseNets Huangetal IthasbeenshownthatBatchNormalizationimprovestrainingtime reducestheneedforadditionalregularization andcaninsomecasesimprovetheaccuracyofnetworks Whileitisdif culttopinpointexactlyhowBatchNormalizationaffectsthe nalpredictionsofamodel wedoobservethatmodelstrainedwithBatchNormalizationtendtobemoremiscalibrated InthemiddlerightplotofFigure weseethata layerConvNetobtainsworsecalibrationwhenBatchNormalizationisapplied eventhoughclassi cationaccuracyimprovesslightly We ndthatthisresultholdsregardlessofthehyperparametersusedontheBatchNormalizationmodel     loworhighlearningrate etc Weightdecay whichusedtobethepredominantregularizationmechanismforneuralnetworks isdecreasinglyutilizedwhentrainingmodernneuralnetworks Learningtheorysuggeststhatregularizationisnecessarytopreventover tting especiallyasmodelcapacityincreases Vapnik However duetotheapparentregularizationeffectsofBatchNormalization recentresearchseemstosuggestthatmodelswithlessL regularizationtendtogeneralizebetter Ioffe Szegedy Asaresult itisnowcommontotrainmodelswithlittleweightdecay ifanyatall ThetopperformingImageNetmodelsof alluseanorderofmagnitudelessweightdecaythanmodelsofpreviousyears Heetal Simonyan Zisserman We ndthattrainingwithlessweightdecayhasanegativeimpactoncalibration ThefarrightplotinFigure displaystrainingerrorandECEfora layerResNetwithvaryingamountsofweightdecay TheonlyotherformsofregularizationaredataaugmentationandBatchNormalization Weobservethatcalibrationandaccuracyarenotoptimizedbythesameparametersetting Whilethemodelexhibitsbothoverregularizationandunder regularizationwithrespecttoclassi cationerror itdoesnotappearthatcalibrationisnegativelyimpactedbyhavingtoomuchweightdecay Modelcalibrationcontinuestoimprovewhenmoreregularizationisadded wellafterthepointofachievingoptimalaccuracy Theslightuptickattheendofthegraphmaybeanartifactofusingaweightdecayfactorthatimpedesoptimization NLLcanbeusedtoindirectlymeasuremodelcalibration Inpractice weobserveadisconnectbetweenNLLandaccuracy whichmayexplainthemiscalibrationinFigure Thisdisconnectoccursbecauseneuralnetworkscanover ttoNLLwithoutover ttingtothe loss Weobservethistrendinthetrainingcurvesofsomemiscalibratedmodels Figure showstesterrorandNLL rescaledtomatcherror onCIFAR astrainingprogresses BotherrorandNLLimmediatelydropatepoch whenthelearningrateisdropped however NLLover tsduringtheremainderoftraining Surprisingly over ttingtoNLLisbene cialtoclassi cationaccuracy OnCIFAR testerrordropsfrom to intheregionwhereNLLover ts Thisphenomenonrendersaconcreteexplanationofmiscalibration thenetworklearnsbetterclassi cationaccuracyattheexpenseofwellmodeledprobabilities Wecanconnectthis ndingtorecentworkexaminingthegeneralizationoflargeneuralnetworks Zhangetal observethatdeepneuralnetworksseeminglyviolatethecommonunderstandingoflearningtheorythatlargemodelswithlittleregularizationwillnotgeneralizewell TheobserveddisconnectbetweenNLLand losssuggeststhatthesehighcapacitymodelsarenotnecessarilyimmunefromover tting butrather over ttingmanifestsinprobabilisticerrorratherthanclassi cationerror CalibrationMethodsInthissection we rstreviewexistingcalibrationmethods andintroducenewvariantsofourown Allmethodsarepostprocessingstepsthatproduce calibrated probabilities Eachmethodrequiresaholdoutvalidationset whichinpracticecanbethesamesetusedforhyperparametertuning Weassumethatthetraining validation andtestsetsaredrawnfromthesamedistribution CalibratingBinaryModelsWe rstintroducecalibrationinthebinarysetting       Forsimplicity throughoutthissubsection weassumethemodeloutputsonlythecon denceforthepositiveclass Givenasamplexi wehaveaccessto pi thenetwork spredictedprobabilityofyi aswellaszi   whichisthenetwork snonprobabilisticoutput orlogit Thepredictedprobability piisderivedfromziusingasigmoidfunction     pi zi Ourgoalistoproduceacalibratedprobability qibasedonyi pi andzi Histogrambinning Zadrozny Elkan isasimplenon parametriccalibrationmethod Inanutshell alluncalibratedpredictions piaredividedintomutuallyexclusivebinsB BM Eachbinisassignedacalibratedscore       if piisassignedtobinBm then qi   Attesttime ifprediction ptefallsintobinBm thenthecalibratedprediction qteis   Moreprecisely forasuitablychosenM usuallysmall we rstde nebinboundaries     aM wherethebinBmisde nedbytheinterval am am Typicallythebinboundariesareeitherchosentobeequallengthintervalsortoequalizethenumberofsamplesineachbin Thepredictions iarechosentominimizethebinwisesquaredloss min MMXm nXi am pi am   yi where istheindicatorfunction Given xedbinsboundaries thesolutionto resultsin mthatcorrespondtotheaveragenumberofpositiveclasssamplesinbinBm Isotonicregression Zadrozny Elkan arguablythemostcommonnonparametriccalibrationmethod learnsapiecewiseconstantfunctionftotransformuncalibratedoutputs     qi   pi Speci cally isotonicregressionproducesftominimizethesquarelossPni   pi yi Becausefisconstrainedtobepiecewiseconstant wecanwritetheoptimizationproblemas minM Ma aM MXm nXi am pi am   yi subjectto     aM   whereMisthenumberofintervals   aM aretheintervalboundaries and Marethefunctionvalues Underthisparameterization isotonicregressionisastrictgeneralizationofhistogrambinninginwhichthebinboundariesandbinpredictionsarejointlyoptimized BayesianBinningintoQuantiles BBQ Naeinietal isaextensionofhistogrambinningusingBayesian ThisisincontrastwiththesettinginSection inwhichthemodelproducesbothaclasspredictionandcon dence modelaveraging Essentially BBQmarginalizesoutallpossiblebinningschemestoproduce qi Moreformally abinningschemesisapair     whereMisthenumberofbins andIisacorrespondingpartitioningof intodisjointintervals     aM Theparametersofabinningschemeare   Underthisframework histogrambinningandisotonicregressionbothproduceasinglebinningscheme whereasBBQconsidersaspaceSofallpossiblebinningschemesforthevalidationdatasetD BBQperformsBayesianaveragingoftheprobabilitiesproducedbyeachscheme   qte pte   Xs SP qte     pte   Xs SP qte pte               whereP qte pte       isthecalibratedprobabilityusingbinningschemes Usingauniformprior theweightP       canbederivedusingBayes rule                 Ps SP       Theparameters McanbeviewedasparametersofMindependentbinomialdistributions Hence byplacingaBetaprioron   wecanobtainaclosedformexpressionforthemarginallikelihoodP       ThisallowsustocomputeP qte pte   foranytestinput Plattscaling Plattetal isaparametricapproachtocalibration unliketheotherapproaches Thenonprobabilisticpredictionsofaclassi erareusedasfeaturesforalogisticregressionmodel whichistrainedonthevalidationsettoreturnprobabilities Morespeci cally inthecontextofneuralnetworks NiculescuMizil Caruana Plattscalinglearnsscalarparametersa   Randoutputs qi azi   asthecalibratedprobability ParametersaandbcanbeoptimizedusingtheNLLlossoverthevalidationset Itisimportanttonotethattheneuralnetwork sparametersare xedduringthisstage ExtensiontoMulticlassModelsForclassi cationproblemsinvolvingK classes wereturntotheoriginalproblemformulation Thenetworkoutputsaclassprediction yiandcon dencescore piforeachinputxi Inthiscase thenetworklogitsziarevectors where yi argmaxkz     and piistypicallyderivedusingthesoftmaxfunction SM SM zi   exp       PKj exp       pi maxk SM zi   Thegoalistoproduceacalibratedcon dence qiand possiblynew classprediction   ibasedonyi yi pi andzi Becausethevalidationdatasetis nite Sisaswell DatasetModelUncalibratedHist BinningIsotonicBBQTemp ScalingVectorScalingMatrixScalingBirdsResNet CarsResNet CIFAR ResNet CIFAR ResNet SD CIFAR WideResNet CIFAR DenseNet CIFAR LeNet CIFAR ResNet CIFAR ResNet SD CIFAR WideResNet CIFAR DenseNet CIFAR LeNet ImageNetDenseNet ImageNetResNet SVHNResNet SD NewsDAN ReutersDAN SSTBinaryTreeLSTM SSTFineGrainedTreeLSTM Table ECE withM bins onstandardvisionandNLPdatasetsbeforecalibrationandwithvariouscalibrationmethods Thenumberfollowingamodel snamedenotesthenetworkdepth Extensionofbinningmethods OnecommonwayofextendingbinarycalibrationmethodstothemulticlasssettingisbytreatingtheproblemasKone versusallproblems Zadrozny Elkan Fork   weformabinarycalibrationproblemwherethelabelis yi   andthepredictedprobabilityis SM zi   ThisgivesusKcalibrationmodels eachforaparticularclass Attesttime weobtainanunnormalizedprobabilityvector           where     iisthecalibratedprobabilityforclassk Thenewclassprediction   iistheargmaxofthevector andthenewcon dence   iisthemaxofthevectornormalizedbyPKk       Thisextensioncanbeappliedtohistogrambinning isotonicregression andBBQ Matrixandvectorscalingaretwomulticlassexten sionsofPlattscaling Letzibethelogitsvectorproducedbeforethesoftmaxlayerforinputxi MatrixscalingappliesalineartransformationWzi btothelogits qi maxk SM Wzi         argmaxk Wzi     TheparametersWandbareoptimizedwithrespecttoNLLonthevalidationset AsthenumberofparametersformatrixscalinggrowsquadraticallywiththenumberofclassesK wede nevectorscalingasavariantwhereWisrestrictedtobeadiagonalmatrix Temperaturescaling thesimplestextensionofPlattscaling usesasinglescalarparameterT forallclasses Giventhelogitvectorzi thenewcon dencepredictionis qi maxk SM zi     Tiscalledthetemperature andit softens thesoftmax     raisestheoutputentropy withT AsT theprobability qiapproaches   whichrepresentsmaximumuncertainty WithT werecovertheoriginalprobability pi AsT theprobabilitycollapsestoapointmass     qi TisoptimizedwithrespecttoNLLonthevalidationset BecausetheparameterTdoesnotchangethemaximumofthesoftmaxfunction theclassprediction   iremainsunchanged Inotherwords temperaturescalingdoesnotaffectthemodel saccuracy Temperaturescalingiscommonlyusedinsettingssuchasknowledgedistillation Hintonetal andstatisticalmechanics Jaynes Tothebestofourknowledge wearenotawareofanyprioruseinthecontextofcalibratingprobabilisticmodels Themodelisequivalenttomaximizingtheentropyoftheoutputprobabilitydistributionsubjecttocertainconstraintsonthelogits seeSectionS OtherRelatedWorksCalibrationandcon dencescoreshavebeenstudiedinvariouscontextsinrecentyears Kuleshov Ermon studytheproblemofcalibrationintheonlinesetting wheretheinputscancomefromapotentiallyadversarialsource Kuleshov Liang investigatehowtoproducecalibratedprobabilitieswhentheoutputspaceisastructuredobject Lakshminarayananetal useensemblesofnetworkstoobtainuncertaintyestimates Pereyraetal penalizeovercon dentpredictionsasaformofregularization Hendrycks Gimpel usecon dence Tohighlighttheconnectionwithpriorworkswede netemperaturescalingintermsof Tinsteadofamultiplicativescalar scorestodetermineifsamplesareoutof distribution Bayesianneuralnetworks Denker Lecun MacKay returnaprobabilitydistributionoveroutputsasanalternativewaytorepresentmodeluncertainty Gal Ghahramani drawaconnectionbetweenDropout Srivastavaetal andmodeluncertainty claimingthatsamplingmodelswithdroppednodesisawaytoestimatetheprobabilitydistributionoverallpossiblemodelsforagivensample Kendall Gal combinethisapproachwithamodelthatoutputsapredictivemeanandvarianceforeachdatapoint Thisnotionofuncertaintyisnotrestrictedtoclassi cationproblems Incontrast ourframework whichdoesnotrequiremodelsampling returnsacon denceforagivenoutputratherthanreturningadistributionofpossibleoutputs ResultsWeapplythecalibrationmethodsinSection toimageclassi cationanddocumentclassi cationneuralnetworks Forimageclassi cationweuse datasets CaltechUCSDBirds Welinderetal birdspecies imagesfortrain validation testsets StanfordCars Krauseetal classesofcarsbymake model andyear imagesfortrain validation test ImageNet Dengetal Naturalsceneimagesfrom classes million imagesfortrain validation test CIFAR CIFAR Krizhevsky Hinton Colorimages from classes imagesfortrain validation test StreetViewHouseNumbers SVHN Netzeretal coloredimagesofcroppedouthousenumbersfromGoogleStreetView imagesfortrain validation test Wetrainstateof theartconvolutionalnetworks ResNets Heetal ResNetswithstochasticdepth SD Huangetal WideResNets Zagoruyko Komodakis andDenseNets Huangetal Weusethedatapreprocessing trainingprocedures andhyperparametersasdescribedineachpaper ForBirdsandCars we netunenetworkspretrainedonImageNet Fordocumentclassi cationweexperimentwith datasets News Newsarticles partitionedinto categoriesbycontent documentsfortrain validation test Reuters Newsarticles partitionedinto categoriesbytopic documentsfortrain validation test StanfordSentimentTreebank SST Socheretal Moviereviews representedassentenceparsetreesthatareannotatedbysentiment Eachsampleincludesacoarsebinarylabelanda negrained classlabel Asdescribedin Taietal thetraining validation testsetscontain documentsforbinary and for negrained On NewsandReuters wetrainDeepAveragingNetworks DANs Iyyeretal with feedforwardlayersandBatchNormalization Thesenetworksobtaincompetitiveaccuracyusingtheoptimizationhyperparameterssuggestedbytheoriginalpaper OnSST wetrainTreeLSTMs LongShortTermMemory Taietal usingthedefaultsettingsintheauthors code CalibrationResults Table displaysmodelcalibration asmeasuredbyECE withM bins beforeandafterapplyingthevariousmethods seeSectionS forMCE NLL anderrortables Itisworthnotingthatmostdatasetsandmodelsexperiencesomedegreeofmiscalibration withECEtypicallybetween to Thisisnotarchitecturespeci   weobservemiscalibrationonconvolutionalnetworks withandwithoutskipconnections recurrentnetworks anddeepaveragingnetworks ThetwonotableexceptionsareSVHNandReuters bothofwhichexperienceECEvaluesbelow Bothofthesedatasetshaveverylowerror and respectively andthereforetheratioofECEtoerroriscomparabletootherdatasets Ourmostimportantdiscoveryisthesurprisingeffectivenessoftemperaturescalingdespiteitsremarkablesimplic ity Temperaturescalingoutperformsallothermethodsonthevisiontasks andperformscomparablytoothermethodsontheNLPdatasets WhatisperhapsevenmoresurprisingisthattemperaturescalingoutperformsthevectorandmatrixPlattscalingvariants whicharestrictlymoregeneralmethods Infact vectorscalingrecoversessentiallythesamesolutionastemperaturescaling thelearnedvectorhasnearlyconstantentries andthereforeisnodifferentthanascalartransformation Inotherwords networkmiscalibrationisintrinsicallylowdimensional TheonlydatasetthattemperaturescalingdoesnotcalibrateistheReutersdataset Inthisinstance onlyoneoftheabovemethodsisabletoimprovecalibration Becausethisdatasetiswellcalibratedtobeginwith ECE thereisnotmuchroomforimprovementwithanymethod andpostprocessingmaynotevenbenecessarytobeginwith Itisalsopossiblethatourmeasurementsareaffectedbydatasetsplitorbytheparticularbinningscheme Matrixscalingperformspoorlyondatasetswithhundredsofclasses     Birds Cars andCIFAR andfailstoconvergeonthe classImageNetdataset Thisisexpected sincethenumberofparametersscalesquadrat  AccuracyECE Uncal CIFAR ResNet SD OutputsGap ECE Temp ScaleCIFAR ResNet SD OutputsGap ECE Hist Bin CIFAR ResNet SD OutputsGap ECE Iso Reg CIFAR ResNet SD OutputsGapCon denceFigure ReliabilitydiagramsforCIFAR before farleft andaftercalibration middleleft middleright farright icallywiththenumberofclasses Anycalibrationmodelwithtensofthousands ormore parameterswillover ttoasmallvalidationset evenwhenapplyingregularization Binningmethodsimprovecalibrationonmostdatasets butdonotoutperformtemperaturescaling Additionally binningmethodstendtochangeclasspredictionswhichhurtsaccuracy seeSectionS Histogrambinning thesimplestbinningmethod typicallyoutperformsisotonicregressionandBBQ despitethefactthatbothmethodsarestrictlymoregeneral Thisfurthersupportsour ndingthatcalibrationisbestcorrectedbysimplemodels Reliabilitydiagrams Figure containsreliabilitydiagramsfor layerResNetsonCIFAR beforeandaftercalibration Fromthefarleftdiagram weseethattheuncalibratedResNettendstobeovercon dentinitspredictions Wethencanobservetheeffectsoftemperaturescaling middleleft histogrambinning middleright andisotonicregression farright oncalibration Allthreedisplayedmethodsproducemuchbettercon denceestimates Ofthethreemethods temperaturescalingmostcloselyrecoversthedesireddiagonalfunction Eachofthebinsarewellcalibrated whichisremarkablegiventhatalltheprobabilitiesweremodi edbyonlyasingleparameter WeincludereliabilitydiagramsforotherdatasetsinSectionS Computationtime Allmethodsscalelinearlywiththenumberofvalidationsetsamples Temperaturescalingisbyfarthefastestmethod asitamountstoaonedimensionalconvexoptimizationproblem Usingaconjugategradientsolver theoptimaltemperaturecanbefoundin iterations orafractionofasecondonmostmodernhardware Infact evenanaivelinesearchfortheoptimaltemperatureisfasterthananyoftheothermethods Thecomputationalcomplexityofvectorandmatrixscalingarelinearandquadraticrespectivelyinthenumberofclasses re ectingthenumberofparametersineachmethod ForCIFAR   ndinganearoptimalvectorscal ingsolutionwithconjugategradientdescentrequiresatleast ordersofmagnitudemoretime Histogrambinningandisotonicregressiontakeanorderofmagnitudelongerthantemperaturescaling andBBQtakesroughly ordersofmagnitudemoretime Easeofimplementation BBQisarguablythemostdif culttoimplement asitrequiresimplementingamodelaveragingscheme Whileallothermethodsarerelativelyeasytoimplement temperaturescalingmayarguablybethemoststraightforwardtoincorporateintoaneuralnetworkpipeline InTorch Collobertetal forexample weimplementtemperaturescalingbyinsertingann MulConstantbetweenthelogitsandthesoftmax whoseparameteris   WesetT duringtraining andsubsequently nditsoptimalvalueonthevalidationset ConclusionModernneuralnetworksexhibitastrangephenomenon probabilisticerrorandmiscalibrationworsenevenasclassi cationerrorisreduced Wehavedemonstratedthatrecentadvancesinneuralnetworkarchitectureandtraining modelcapacity normalization andregularization havestrongeffectsonnetworkcalibration Itremainsfutureworktounderstandwhythesetrendsaffectcalibrationwhileimprovingaccuracy Nevertheless simpletechniquescaneffectivelyremedythemiscalibrationphenomenoninneuralnetworks Temperaturescalingisthesimplest fastest andmoststraightforwardofthemethods andsurprisinglyisoftenthemosteffective AcknowledgmentsTheauthorsaresupportedinpartbytheIII III andIIS grantsfromtheNationalScienceFoundation aswellastheBillandMelindaGatesFoundationandtheOf ceofNavalResearch ReferencesBengio Yoshua Goodfellow IanJ andCourville Aaron Deeplearning Nature Bojarski Mariusz DelTesta Davide Dworakowski Daniel Firner Bernhard Flepp Beat Goyal Prasoon Jackel LawrenceD Monfort Mathew Muller Urs Zhang Jiakai etal Endtoendlearningforselfdrivingcars arXivpreprintarXiv Caruana Rich Lou Yin Gehrke Johannes Koch Paul Sturm Marc andElhadad Noemie Intelligiblemodelsforhealthcare Predictingpneumoniariskandhospital dayreadmission InKDD Collobert Ronan Kavukcuoglu Koray andFarabet Cl ement Torch Amatlablikeenvironmentforma chinelearning InBigLearnWorkshop NIPS Cosmides LedaandTooby John Arehumansgoodintuitivestatisticiansafterall rethinkingsomeconclusionsfromtheliteratureonjudgmentunderuncertainty cognition DeGroot MorrisHandFienberg StephenE Thecomparisonandevaluationofforecasters Thestatistician pp Deng Jia Dong Wei Socher Richard Li LiJia Li Kai andFeiFei Li Imagenet Alargescalehierarchicalimagedatabase InCVPR pp Denker JohnSandLecun Yann Transformingneuralnetoutputlevelstoprobabilitydistributions InNIPS pp Friedman Jerome Hastie Trevor andTibshirani Robert Theelementsofstatisticallearning volume SpringerseriesinstatisticsSpringer Berlin Gal YarinandGhahramani Zoubin Dropoutasabayesianapproximation Representingmodeluncertaintyindeeplearning InICML Girshick Ross Fastrcnn InICCV pp Hannun Awni Case Carl Casper Jared Catanzaro Bryan Diamos Greg Elsen Erich Prenger Ryan Satheesh Sanjeev Sengupta Shubho Coates Adam etal Deepspeech Scalingupendto endspeechrecognition arXivpreprintarXiv He Kaiming Zhang Xiangyu Ren Shaoqing andSun Jian Deepresiduallearningforimagerecognition InCVPR pp Hendrycks DanandGimpel Kevin Abaselinefordetectingmisclassi edandoutof distributionexamplesinneuralnetworks InICLR Hinton Geoffrey Vinyals Oriol andDean Jeff Distillingtheknowledgeinaneuralnetwork Huang Gao Sun Yu Liu Zhuang Sedra Daniel andWeinberger Kilian Deepnetworkswithstochasticdepth InECCV Huang Gao Liu Zhuang Weinberger KilianQ andvanderMaaten Laurens Denselyconnectedconvolutionalnetworks InCVPR Ioffe SergeyandSzegedy Christian Batchnormalization Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift Iyyer Mohit Manjunatha Varun BoydGraber Jordan andDaum eIII Hal Deepunorderedcompositionrivalssyntacticmethodsfortextclassi cation InACL Jaynes EdwinT Informationtheoryandstatisticalmechanics Physicalreview Jiang Xiaoqian Osl Melanie Kim Jihoon andOhnoMachado Lucila Calibratingpredictivemodelestimatestosupportpersonalizedmedicine JournaloftheAmericanMedicalInformaticsAssociation Kendall AlexandCipolla Roberto Modellinguncertaintyindeeplearningforcamerarelocalization Kendall AlexandGal Yarin Whatuncertaintiesdoweneedinbayesiandeeplearningforcomputervision arXivpreprintarXiv Krause Jonathan Stark Michael Deng Jia andFeiFei Li dobjectrepresentationsfor negrainedcatego rization InIEEEWorkshopon DRepresentationandRecognition dRR Sydney Australia Krizhevsky AlexandHinton Geoffrey Learningmultiplelayersoffeaturesfromtinyimages Kuleshov VolodymyrandErmon Stefano Reliablecon denceestimationviaonlinelearning arXivpreprintarXiv Kuleshov VolodymyrandLiang Percy Calibratedstructuredprediction InNIPS pp Lakshminarayanan Balaji Pritzel Alexander andBlundell Charles Simpleandscalablepredictiveuncertaintyestimationusingdeepensembles arXivpreprintarXiv LeCun Yann Bottou   eon Bengio Yoshua andHaffner Patrick Gradientbasedlearningappliedtodocumentrecognition ProceedingsoftheIEEE SupplementaryMaterials OnCalibrationofModernNeuralNetworksMacKay DavidJC Apracticalbayesianframeworkforbackpropagationnetworks Neuralcomputation Naeini MahdiPakdaman Cooper GregoryF andHauskrecht Milos Obtainingwellcalibratedprobabilitiesusingbayesianbinning InAAAI pp Netzer Yuval Wang Tao Coates Adam Bissacco Alessandro Wu Bo andNg AndrewY Readingdigitsinnaturalimageswithunsupervisedfeaturelearning InDeepLearningandUnsupervisedFeatureLearningWorkshop NIPS NiculescuMizil AlexandruandCaruana Rich Predictinggoodprobabilitieswithsupervisedlearning InICML pp Pereyra Gabriel Tucker George Chorowski Jan Kaiser ukasz andHinton Geoffrey Regularizingneuralnetworksbypenalizingcon dentoutputdistributions arXivpreprintarXiv Platt Johnetal Probabilisticoutputsforsupportvectormachinesandcomparisonstoregularizedlikelihoodmethods Advancesinlargemarginclassi ers Simonyan KarenandZisserman Andrew Verydeepconvolutionalnetworksforlarge scaleimagerecognition InICLR Socher Richard Perelygin Alex Wu Jean Chuang Jason Manning ChristopherD Ng Andrew andPotts Christopher Recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank InEMNLP pp Srivastava Nitish Hinton Geoffrey Krizhevsky Alex Sutskever Ilya andSalakhutdinov Ruslan Dropout Asimplewaytopreventneuralnetworksfromover tting JournalofMachineLearningResearch Srivastava RupeshKumar Greff Klaus andSchmidhuber   urgen Highwaynetworks arXivpreprintarXiv Tai KaiSheng Socher Richard andManning ChristopherD Improvedsemanticrepresentationsfromtreestructuredlongshort termmemorynetworks Vapnik VladimirN StatisticalLearningTheory WileyInterscience Welinder   Branson   Mita   Wah   Schroff   Belongie   andPerona   CaltechUCSDBirds TechnicalReportCNSTR CaliforniaInstituteofTechnology Xiong Wayne Droppo Jasha Huang Xuedong Seide Frank Seltzer Mike Stolcke Andreas Yu Dong andZweig Geoffrey Achievinghumanparityinconversationalspeechrecognition arXivpreprintarXiv Zadrozny BiancaandElkan Charles Obtainingcalibratedprobabilityestimatesfromdecisiontreesandnaivebayesianclassi ers InICML pp Zadrozny BiancaandElkan Charles Transformingclassi erscoresintoaccuratemulticlassprobabilityestimates InKDD pp Zagoruyko SergeyandKomodakis Nikos Wideresidualnetworks InBMVC Zhang Chiyuan Bengio Samy Hardt Moritz Recht Benjamin andVinyals Oriol Understandingdeeplearningrequiresrethinkinggeneralization InICLR 