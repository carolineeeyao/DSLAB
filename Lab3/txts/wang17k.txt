Variational Policy for Guiding Point Processes

Yichen Wang   Grady Williams   Evangelos Theodorou   Le Song  

Abstract

Temporal point processes have been widely applied to model event sequence data generated by
online users  In this paper  we consider the problem of how to design the optimal control policy
for point processes  such that the stochastic system driven by the point process is steered to  
target state  In particular  we exploit the key insight to view the stochastic optimal control problem from the perspective of optimal measure and
variational inference  We further propose   convex optimization framework and an ef cient algorithm to update the policy adaptively to the
current system state  Experiments on synthetic
and realworld data show that our algorithm can
steer the user activities much more accurately and
ef ciently than other stochastic control methods 

  Introduction
Nowadays  user generated event data are becoming increasingly available  Each user is typically logged in the database
with the precise timestamp of the event  together with additional context such as tag  text  image  and video  Furthermore  these data are generated in an asynchronous fashion
since any user can generate an event at any time and there
may not be any coordination or synchronization between
two events  Among different representations of user behaviors  temporal point processes have been widely applied to
model the complex dynamics of online user behaviors  Zhou
et al    Du et al    Lian et al      He et al 
    Dai et al    Wang et al         
In spite of the broad applicability of point processes  there
is little work in the area of controlling these processes to in 
 uence user behaviors  In this paper  we study the problem
of designing the best intervention policy to in uence the intensity function of point processes  such that user behaviors
can be in uenced towards   target state 

 College of Computing  Georgia Tech  Atlanta  GA  USA
 School of Aerospace Engineering  Georgia Tech  Correspondence
to  Yichen Wang  yichen wang gatech edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

Find optimal measure  
min   

Our work

 in closed form

min   

Variational Inference

Optimal measure 
perspective

min     

Direct optimization 

Previous works

  Not scalable
  Need approximation
  Proper control cost 

Simple solution

 

Optimal Policy

Figure   Illustration of the measuretheoretic view and bene   of
our framework compared with existing approaches 

  framework for doing this is critically important  For example  government agents may want to effectively suppress
the spread of terrorist propaganda  which is important for
understanding the vulnerabilities of social networks and
increasing their resilience to rumor and false information 
online merchants may want to promote users  frequency
of visiting the website to increase sales  administrators of
    sites such as StackOver ow design various badges to
motivate users to answer questions and provide feedbacks
to increase the online engagement  Anderson et al   
to gain more attention    broadcaster on Twitter may want to
design   smart tweeting strategy such that his posts always
remain on top of his followers  feeds  Karimi et al   
Interestingly  the social science setting also introduces
new challenges  Previous stochastic optimal control methods  Boel   Varaiya    Pham    Oksendal   Sulem 
  Hanson    in robotics are not applicable for four
reasons      they mostly focus on the cases where the policy
is in the drift part of the system  which is quite different from
our case where the policy is on the intensity function   ii 
they require linear approximations of the nonlinear system
and quadratic approximations of the objective function   iii 
to obtain   feedback control policy  these methods require
the solution of the HamiltonJacobi Bellman  HJB  Partial
Differential Equation  which have severe limitations in scalability and feasibility to the nonlinear systems  especially in
social applications where the system   dimension is huge 
 iv  the systems they study are driven by Wiener processes
and Poisson processes  However  social sciences require

Variational Policy for Guiding Point Processes

us to consider more advanced processes  such as Hawkes
processes  which are models for long term memory process
and mutual exciting phenomena in social interactions 
To address these limitations  we propose an ef cient framework by exploiting the novel view of measuretheoretic
formulation and variational inference  Figure   illustrates
our method  We make the following contributions 
Uni ed framework  Our work offers   generic way to
control nonlinear stochastic differential equations driven
by point processes with stochastic intensities  Unlike prior
works  Oksendal   Sulem    no approximations of the
system or the objective function are needed 
Natural control cost  Our framework provides   meaningful control cost function to optimize  it arises naturally from
the structure of the stochastic dynamics  This property is
in stark contrast with the stochastic dynamic programming
methods in control theory  where the control cost is imposed
beforehand  despite the form of the dynamics 
Superior performance  We propose   scalable model predictive control algorithm  The control policy is computed
with forward sampling  hence it is scalable with parallel
sampling and runs in real time  Moreover  it enjoys superior
empirical performance on diverse social applications 
  Background and Preliminaries
Point processes    temporal point process  Aalen et al 
  is   random process whose realization consists of  
list of discrete events localized in time   ti  It is widely applied to model usergenerated event data and user behavior
patterns  Farajtabar et al      Pan et al    Tan
et al    Wang et al           
The point process can also be represented as   counting process        which records the number of events before time
   An important way to characterize it is via the conditional
intensity function         stochastic model for the time of
the next event given historical events          ti ti      It
is the probability of observing   new event on         dt      
   dt     event in         dt          dN        
where one typically assumes that only one event happens in
  small window of size dt       dN          
The function form of the intensity is often designed to capture the phenomena of interests  Some useful forms include 
    Poisson process  the intensity is independent of history 
 ii  Hawkes process  Hawkes    It models the mutual excitation between events  and the intensity of   user  
depends on events from   collection of   users 

            XM
where       exp    is   triggering kernel that models

 ijXtj Hj    

     tj 

 

  

the decay of past events  in uence         is the base intensity  Nj    is the point process representing the historical
events Hj    from user    and  ij     models the strength
of in uence from user   to user    Here  the occurrence
of each historical event increases the intensity by   certain
amount determined by     and the weight  ij  making
      history dependent and   stochastic process by itself 
The key rationale of using point processes for user behaviors is that these models treat time as   continuous random
variable  which has been shown to be more expressive to
capture the uncertainty in real world than discrete time and
epoch based models  Xiong et al    Wang et al   
Stochastic Differential Equations  SDEs    SDE is  
differential equation in which one or more of the terms is  
stochastic process  The SDE models the evolution of state
xi        for user   with   drift  diffusion and jump term 
 
dxi         xi dt

   xi dwi   
diffusion noise

 

 drift

 Xj

  xj dNj   
jump  point process

 

where dxi      xi     dt  xi    describes the increment
of xi    The functions           are nonlinear  The drift
term captures the evolution of the system  the diffusion term
models the noise with the Wiener process  wi           
which follows   Gaussian distribution  the point process
Nj    models events generated by user   and its intensity
      is stochastic  The in uence function   xj  captures
social in uence       how user   in uences user   
  Intensity Stochastic Control Problem
In this section  we  rst de ne the control policy and the controlled stochastic processes  then formulate the stochastic
intensity control problem 
De nition    Controlled Stochastic Processes  Set      
as the original  uncontrolled  intensity for Ni    ui       
as the control policy  and    ui       as the controlled intensity of controlled point process  Ni ui       The uncontrolled SDE in   is modi ed as the controlled SDE 

dxi      xi dt   xi dwi  XM

  

For each user    the form of control policy is 

  xj    Nj uj      

   ui              ui             

 

The control policy ui    helps each user   decide the scale of
changes to his original intensity       at time    and controls
the frequency of generating events  The larger ui    the
more likely an event will happen  Moreover  the control
policy is in the multiplicative form  The rationale behind
this choice is that it makes the policy easy to execute and
meaningful in practice  For example    network moderator
may request   user to reduce his tweeting intensity  ve times

Variational Policy for Guiding Point Processes

if he spreads rumors  or double the original intensity if he
posts educational topics  Alternative policy formulations
that are based on addition are less intuitive and not easy to
execute in practice  For example  if the moderator asks the
user to decrease his posting intensity by one  this instruction
is dif cult to be interpreted in   meaningful way  Finally 
since intensity functions are positive  we set ui       
Our goal is to  nd the best control policy such that this
controlled SDE achieves   target state  Next  we formulate
the stochastic intensity control problem 
De nition    Intensity Control Problem  Given the controlled SDE in   the goal is to  nd      for          
such that the following objective function is minimized 

     argminu  ExhS              

 
where                    is the controlled SDE trajectory on         denotes the policy on       The expectation Ex is taken over all trajectories of    whose stochasticity comes from the Wiener process      and controlled
point process           on       The function      is the
control cost  and      is the state cost de ned as follows 

                        

 

         dt

 

It is   function of the trajectory   and measures its cost
on                 is the instantaneous state cost at time
   and            is the terminal state cost  The scalar  
controls the tradeoff between state cost and control cost 

The state cost is   userde ned function and its form depends
on different applications  We will provide detailed examples
in section   later  The control cost captures the budget and
effort  such as time and money  to control the system 
  Solution Overview
Directly computing the optimal policy in   is dif cult
using previous control methods  Pham    Oksendal  
Sulem    Hanson    The challenges are as follows 
Challenges  The  rst two challenges lie in different problem scopes  First  the control policy in these works is in the
drift of SDE  and not directly applicable to the intensity control problem  Second  these works typically consider simple
Poisson processes with deterministic intensity  However  in
our problem the intensity can also be stochastic  which adds
another layer of stochasticity  Besides the problem scopes 
these works have two fundamental technical challenges 
   Choice of control cost  These works need to de ne the
form of control cost beforehand  which is nontrivial  For
example  ui        means there is no control  However  it
is not clear which of the two heuristic forms works better 

  ku          
   
  Xi

 ui      log ui   dt  

Unfortunately  prior works need tedious and heuristic tuning
of the function forms of control cost     
II  Scalability and approximations  Prior works rely on
the Bellman optimality condition and use stochastic programming to derive the corresponding HamiltonJacobi 
Bellman  HJB  partial differential equation  PDE  Solving
this PDE for multidimensional nonlinear SDEs is dif cult
due to scalability limitations       curse of dimensionality  Hanson    This is especially challenging in social
network applications where the SDE has thousands or millions of dimensions  each user represents one dimension 
Ef cient solution for the PDE only exists in the special case
of linear SDE and quadratic control cost and state cost  This
case is restrictive when the underlying model is   nonlinear
SDE  and the state cost is arbitrary function 
Our approach  To address the above challenges  we propose   generic framework with the following key steps 
   Optimal measuretheoretic formulation  We establish
  novel view of the intensity control problem by linking
it to the optimal probability measure  The key insight is
to compute the optimal measure    which is induced by
optimal policy    With this view  the control cost comes
naturally as   KLdivergence term  Section  

     argminQhEQ         DKL      

II  Variational inference for the optimal policy 
It is
much easier to  nd the optimal measure    compared with
directly solving   Based on its form  we then parameterize
     and compute    by minimizing the distance between
   and      This approach leads to   scalable and simple
algorithm  and does not need any approximations to the
nonlinear SDE or cost functions  Section  

     argminu  DKL       

Finally  we transform the openloop policy to the feedback
policy and develop   scalable algorithm 
  Variational Policy
In this section  we will present technical details of our
framework  Variational Policy  We  rst provide   measuretheoretic view of the control problem  and show that  nding
optimal measure is equivalent to  nding the optimal control 
Then we compute the optimal measure and  nd the optimal
control policy from the view of variational inference 
  Optimal measuretheoretic formulation of

intensity optimal control problem

Each trajectory  sample path  of   SDE is stochastic  Hence
we can de ne   probability measure on all possible trajectories  and   SDE uniquely induces   probability measure 
At   conceptual level  the SDE and the measure induced

Variational Policy for Guiding Point Processes

 
 

     

 
 

 

time

     
 

time

    uncontrolled trajectories

    controlled trajectories

Figure   Explanation of the measures induced by SDEs      the
three green uncontrolled trajectories are in the region of   Since
  is induced by the uncontrolled SDE  naturally it has high probability on the region   compared with    Similarly  the three
yellow trajectories are in   and   has high probability in this
region since   is induced by the controlled SDE 
by the SDE are equivalent mathematical representations 
obtaining   trajectory from this SDE by simulation  forward
propagating the SDE  is equivalent to generating   sample
from the probability measure induced by the SDE 
Next  we link this probability measure view to the intensity control problem  The problem in   aims at  nding
an optimal policy  which uniquely determines the optimal
controlled SDE  Since the SDE induces   measure    is
equivalent to the problem of  nding the optimal measure 
Mathematically  we set   as the probability measure induced
by the uncontrolled SDE in   and set   as the measure
induced by the controlled SDE in   Hence Ex   EQ      
taking the expectation over stochastic trajectories   in the
original objective function is essentially taking expectation
over the measure    Moreover  the difference between  
and   is just the effect of the control policy  Therefore    
uniquely induces    Figure   demonstrates   and   
Based on this idea  instead of directly computing    we
aim at  nding the optimal measure    such that EQ     
is minimized  We set the constraint such that   is as close to
  as possible  and propose the following objective function 

 

min

  hEQ         DKL               dQ    
whereR dQ     ensures   is   probability measure  and
dQ is the probability density  DKL        EQ log  dQ
dP  
is the KL divergence between these two measures 
Natural control cost  This KL divergence term provides an
elegant way of measuring the distance between controlled
and uncontrolled SDEs  Minimizing this term sets   to
be close to    hence it provides an implicit measure of the
control cost  Mathematically  we express it as follows 

dQ
dP  

DKL        EQ log 
  EQ    

  Xi   log ui     

 

 

ui           ui   dt 

Appendix   contains derivations  With this formulation 
we set the control cost        log  dQ
dP   This function

reaches its minimum when ui        since the function
       reaches the minimum when
         log       
      Interestingly       is none of the heuristics in  
Hence our control cost comes naturally from the dynamics 
Another bene   of our formulation is that the probability
measure that minimizes   is easy to derive  Appendix  
contains derivations  The optimal measure is

 

      
      

exp   
EP exp   

dQ 
dP  
The term dQ 
is called the RadonNikodym derivadP
tive  Dupuis   Ellis    Theodorou    This expression is intuitive  if   trajectory   has low state cost 
then dQ 
dP is large  This means that this trajectory is likely
to be sampled from    In summary  our  rst contribution
is the link between the problem of  nding optimal control
to that of  nding optimal measure  Computing the optimal
measure is much easier than directly solving  
However  the main challenge in our measuretheoretic formulation is that there is no explicit transformation between
the optimal measure    and the optimal control    To
solve this problem  next we design   convex objective function by matching probability measures 
  Finding optimal policy with variational inference
We formulate our objective function based on the optimal
measure  More speci cally  we  nd   control   which
pushes the induced measure      as close to the optimal
measure as possible  Mathematically  we have 

     argminu  DKL       

 
From the view of variational inference  Wainwright   Jordan    Williams et al    our objective function
describes the amount of information loss when      is
used to approximate    This objective is in sharp contrast
to traditional methods that solve the problem by computing
the solution of the HJB PDE  which have severe limitations
in scalability and feasibility to nonlinear SDEs  Oksendal  
Sulem    Hanson   
Next  we simplify the objective in   and compute the
optimal control policy  From the de nition of KL divergence
and chain rule of derivatives    is expressed as 

 

DKL          EQ   log  dQ 

dP

dP

dQ     

The derivative dQ dP is given in   and we only need
to compute dP dQ    This derivative is the relative density of probability distribution               The change
of probability measure happens because the intensity is
changed from     to        Hence dP dQ    is essentially the likelihood ratio between the uncontrolled and controlled point process  We summarize its form in Theorem  

Variational Policy for Guiding Point Processes

Theorem   For the intensity control problem  we have 

dP dQ      exp      where      is expressed as 
     
   ui           ds     
XM
log ui   dNi   

 

Appendix   contains details of the proof  Next we substitute
dQ dP and dP dQ    to   After removing terms
independent of    the objective function is simpli ed as 

     argminu  EQ     

Next  we will solve this optimization problem to compute
   As in traditional stochastic optimal control works  Oksendal   Sulem    Hanson      control policy is
obtained by solving the HJB PDE at discrete timestamps on
      Hence it suf ces to parameterize our policy      as
  piecewise constant function on      
We denote the kth piece of   as uk  which is de ned on
               with               tk       and
    tK  Now we express the objective function as follows 
         ds 
   dNi     

EQ       XiXk EQ   tk 

  EQ   tk 

log uk

  denotes the ith dimension of uk  We just need to
where uk
focus on the parts that involves uk
  and move it outside of
the expectation  Further we can show the  nal expression is
convex in uk
    Finally  setting the gradient to zero yields the
following optimal control policy  denoted as uk  

 uk

tk

tk

 

 

uk    

EP  exp   
EP  exp   

        tk 
        tk 

tk

tk

dNi   
     ds 

Appendix   contains complete derivations  Note we have
transformed EQ  to EP using   It is important because
EQ  is not directly computable  Inspired by the idea of
importance sampling  since we only know the SDE of the
uncontrolled dynamics in   and can only compute the
expectation under    the change of expectation is necessary 
To compute EP  we use the Monte Carlo method to sample
  trajectories from   on       and take the sample average 
To obtain the mth sample xm  we use the classic routine 
sample point process              Hawkes process  using
thinning algorithm  Ogata    sample Wiener process
wm    from Gaussion distribution  and apply the Euler
method  Hanson    to obtain xm  Since each sample is
independent  it can be scaled up easily with parallelization 
Next  we compute wm   exp   xm  by evaluating
the state cost  and computeR tk 
      as the number

dN  

tk

Algorithm   KL   Model Predictive Control
  Input  sample size    optimization window length      total

time window     timestamps  tk  on      

  Output  optimal control    at each tk on      
  for       to       do
for       to   do
 
 

Sample dN     dw    and generate xm on  tk  tk       
according to   and the current state 

  xm      

    xm dt    xm  wm   exp   
from   for each    and execute uk  receive

    

end for
Compute uk  
state feedback and update state 

 
 
 

  end for

of events that occurred during  tk  tk  at the ith dimeni     is historydependent  given the
sion  Moreover  since   
      is  xed with   paraevents history in the mth sample    
     ds can also be computed
  
numerically or in closed form  The closed form expression
exists for the Hawkes process  In summary  the sample
average approximation of   is 

metric form  HenceR tk 

tk

tk

tk

 

   wmR tk 
uk     PI
   wmR tk 
PI

dN  
     
  
     ds
Next  we discuss the properties of our policy 
Stochastic intensity  The intensity function       is history
independent and stochastic       Hawkes process  Since
      is inside the expectation EP in   our policy naturally considers its stochasticity by taking the expectation 
General SDE   arbitrary cost  Since we only need the
SDE system to sample trajectories  our framework is applicable to general nonlinear SDEs and arbitrary cost functions 
  From openloop policy to feedback policy
The current control policy in   does not depend on the
system   feedback  However    more effective policy should
consider the current state of SDE  and integrate such feedback into the policy  In this section  we will transform the
openloop policy into   feedback policy 
To design this feedback policy  we use the model predictive
control  MPC  scheme  Camacho   Alba    where the
Model of the process is used to Predict the future evolution
of the process to optimize the Control  In MPC  online
optimization and execution are interleaved as follows 
    Optimization  At time    we compute the control policy    on              using   for   short time horizon
       in the future  Therefore  we only need to sample
trajectories on              for computation instead of      
 ii  Execution  We apply the  rst optimal move      at this

time    and observe the new system state 

 iii  Feedback   reoptimization  At time       with the new
observed state  we recompute the control and repeat the

Variational Policy for Guiding Point Processes

above process  Algorithm   summarizes the procedure 
The advantage of MPC is that it yields   feedback control
that implicitly depends on the current state      Moreover 
separating the optimization horizon    from   is also advantageous since it makes little sense to consider choosing  
deterministic set of actions far out into the future 
  Applications
In this section  we apply our framework to two realworld
applications in social sciences 
Guiding opinion diffusion  The continuoustime opinion
model considers the opinion and timing of each posting
event  De et al    He et al    It assigns each user  
  Hawkes intensity       and an opinion process xi       
where xi        corresponds to neutral opinion  Users are
connected according to   network adjacency matrix    
 ij  The opinion change of user is captured by three terms 

dxi     bi xi dt dwi   Xj

 ijxjdNj     

where bi is the baseline opinion       personal characteristics 
The noise process dwi    captures the normal  uctuations
in the dynamics due to unobserved factors such as activity
outside the social platform and unexpected events  The jump
term captures the fact that the change of user     opinion is
  weighted summation of his neighbors  in uence  and  ij
ensures only the opinion of   user   neighbor is considered 
How to control users  posting intensity  such that the opinion dynamics is steered towards   target  We can modify
each user   opinion posting process Nj    as  Nj uj     with
policy uj    Common choices of state costs are as follows 
  Least square opinion shaping  The goal is to make the
expected opinion to achieve the target         nobody
believes the rumor during the period  Mathematically  we
set     kx      ak  and     kx       ak 
  Opinion in uence maximization  The goal is to maximize each user   positive opinion         political party
maximizes the support during the election period  Mathematically  we set      Pi xi    and      Pi xi    

Guiding broadcasting behavior  When   user posts in
social network  he competes with others that his followers
follow  and he will gain greater attention if his posts remain
top among followers  feeds  His position de ned as the rank
of his post among his followers   Karimi et al    models
the change of   broadcaster   position due to the posting
behavior of other competitors and himself as follows 

dxj      dNo     xj       dNi   

 
where   is the broadcaster and          denote one follower
of    The stochastic process xj        denotes the rank of
broadcaster     posts among all the posts that his follower  

receives  Rank xj     means     posts is the top  among
all posts   receives  Ni    is   Poisson process capturing the
broadcaster   posting behavior  No    is the Hawkes process
for the behavior of all other broadcasters that   follows 
How to change the posting intensity of the broadcaster  such
that his posts always remain on top  We use the policy to
change Ni    to  Ni ui     and help user   decide when to
post messages  The state cost minimizes his rank among all
followers  news feed  Speci cally  we set the state and ter 

minal cost as    Pj      xj    and    Pj      xj    

  Experiments
We focus on two applications in the previous section  least
square opinion guiding and smart broadcasting  We compare
with suitable stochastic optimization approaches that are
popular in reinforcement learning and heuristics 
  Cross Entropy  CE   Stulp   Sigaud    It samples
controls from   Gaussian distribution  sorts the samples
in ascending order with respect to the cost and recomputes the distribution parameters based on the  rst  
elite samples  Then it returns to the  rst step with   new
distribution until the cost converges 

  Finite Difference  FD   Peters   Schaal    It generates   samples of perturbed policies     and computes
perturbed cost        Then it uses them to approximate
the true gradient of the cost with respect to the policy 

  Greedy  It controls the system when local state cost is
high  We divide the window into   state cost observation timestamps  At each timestamp  Greedy computes
state cost and controls the system based on prespeci ed
control rules if current cost is more than   times of the
optimal cost of our algorithm  It will stop if it has reached
the current budget bound  We vary   from   to     from
  to   and report the best performance 
  Base Intensity  BI   Farajtabar et al    It sets the
policy for the base parameterization of the intensity only
at initial time and does not consider the system feedback 
We provide both MPC and openloop  OL  versions for
our KL algorithm  Finite Difference and Cross Entropy 
For MPC  we set the optimization window          and
sample size         It is ef cient to generate these
samples and takes less than one second using parallelization 
  Experiments on Opinion Guiding
We generate   synthetic network with   users  We simulate the opinion SDE on window     by applying Euler
forward method    li   Mayers    to compute the difference form of the SDE in   The time window is divided
into   timestamps  We set the initial opinion xi     
and the target opinion ai     for each user  For model parameters  we set       and adjacency matrix   generated
uniformly on     with sparsity of   We simulate
the Hawkes process using thethinning algorithm  Ogata 

Variational Policy for Guiding Point Processes

 

  

 

 
 
 
 
 
 

    

   

 
Time

   

    Opinion evolution

         

 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 

 

 
 

 
 
 

 

 
 
 
 
 
  
   
  

KLMPC
CEMPC
KLOL
CEOL
FDMPC
FDOL
Greedy
BaseIntensity

 
Time

 

    kx      ak vs   

 

 
 
 
 
 

 

 
 
 
 
 
 
 
  

 
 
 
 
 
 
 
  

 

 
 
 
 
 

 

user  
user  
user  
user  
user  
target state

 

 
Time

    State costR  

  kx      akdt

user  
user  
user  
user  
user  
target state

 

 
Time

         

         

Figure   Controlled opinion dynamics of   users  The initial
opinions are uniformly sampled from     and sorted  target
opinion   is polarized with   and       shows the opinion value
per user over time   bd  are network snapshots of the opinion
polarity of   subusers  Yellow blue means positive negative 

  We set the base intensity in   to be       the
in uence matrix is the same as the adjacency matrix    We
set the cost tradeoff parameter to be      
Figure   shows the controlled opinion at different times 
Our method works ef ciently with fast convergence speed 
Figure     shows the instantaneous cost kx    ak at each
time    The opinion system is gradually steered towards
the target  and the cost decreases over time  Our KLMPC
achieves the lowest instantaneous cost at each time and has
the fastest convergence to the optimal cost  Hence the total
state cost is also the lowest 
Figure     shows that KLMPC has   cost improvement
than CEMPC  with less variance and faster convergence 
This is because KLMPC is more  exible and has less restrictions on the control policy  CEMPC is   popular method
for the traditional control problem in robotics  where the
SDE does not contain the jump term and control is in the
drift  However  CEMPC assumes the control is sampled
from   Gaussian distribution  which might not be the ideal
assumption in the intensity control problem  FD performs
worse than CE due to the error in the gradient estimation
process  Finally  for the same method  the MPC always
performs better than openloop version  which shows the
importance of incorporating state feedback to the policy 
Figure       compare the controlled intensity with the uncontrolled intensity at the beginning  Since the goal is to
in uence everyone to be positive      shows that if the user
tweets positive opinion  the control will increase its intensity
to in uence others positively  On the contrary      shows
that if the user   opinion is negative  his intensity will be

    KLMPC trajectory

    CEMPC trajectory

Figure   Experiments on least square guiding      Instantaneous
cost vs  time  Line is the mean and pale region is the variance     
state cost        sample opinion trajectories of  ve users 

controlled to be small      and     show scenarios near the
terminal time  Since the system is around the target state 
the policy is small and the original and controlled intensity
are similar for positive and negative users 
  Experiments on Smart Broadcasting
We evaluate on   realworld Twitter dataset  Farajtabar
et al    which contains   users with  
tweets retweets  We  rst learn the parameters of the point
processes that capture each user   posting behavior by maximizing the likelihood function of data  Karimi et al   
For each broadcaster  we track down all followers and record
all the tweets they posted and reconstruct followers  timelines by collecting all the tweets by people they follow 
We use two evaluation schemes  First  similar to the synthetic case  with learned parameters  we simulate posting
events on     and conduct control over the simulated
dynamics with the cost tradeoff parameter as       The
time window is divided into ten timestamps  We repeat this
simulation procedure ten times 
The second and more interesting scheme is to carry the
policy in   real platform  Since it is very challenging to
do so  we mimic it using heldout data  We partition the
data into ten intervals and use one interval for training and
others for testing  Each method essentially predicts which
interval has smaller cost  by measuring the optimal position
computed from that method to real position  Speci cally  for
each broadcaster  the procedure is as follows      Estimate
model parameters using data in interval    ii  Compute the
optimal policy and obtain the broadcaster   optimal position
    in each other interval    Then sort intervals according to
 xi         iii  Sort intervals according to the actual value

 MethodsState costMethodsKLMPCCE MPCKLOLCE OLFDMPCFD OLGreedyBIVariational Policy for Guiding Point Processes

 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

  

controlled intensity
original intensity

 

 

 

 

 

 

 

 
 
 
 
 
 

 

 

 

controlled intensity
original intensity

controlled intensity
original intensity

 

 

 

 

 

 

 

 

 

 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 
 

 

 

 

controlled intensity
original intensity

 

 

 

 

 

 

 

 

 
 
 
 
 
 

 

 

 

Broadcaster
Competitors

  

Time

 

 

  

  

Time

 

 

 

 

Time

 

 

 

 

Time

 

 

 

 

 
 
Time  hour 

 

 

    POS user on    

    NEG user on    

    POS user on    

    NEG user on    

    broadcaster vs  others

Figure   Intensity comparison   ad  Opinion guiding experiment  visualization for users with positive  POS  and negative  NEG 
opinions during different periods      Smart broadcasting  visualization for one randomly picked broadcaster and his competitors 

itself  but they are not generalizable for two reasons      the
processes are simple  such as Poisson process  Br maud 
  and   powerlaw decaying function  Bayraktar   Ludkovski     ii  the systems only contain point process 
However  in social sciences  the system can be driven by
many other stochastic processes  Based on Hawkes process   Farajtabar et al    designed its baseline intensity
to achieve   steady state behavior  However  this policy does
not incorporate system feedback  Recently   Zarezade et al 
  proposed to control   user   posting intensity  which is
driven by   homogeneous Poisson process  and their system
is   linear SDE  This method solves   HJB PDE and uses
polynomial functions to approximate cost functions  On
the contrary  our framework is applicable to general point
processes with stochastic intensities and nonlinear SDEs 
In the area of stochastic optimal control    relevant line
of research focuses on event triggered control  Ades et al 
  Lemmon    Heemels et al    Meng et al 
  But the problem is different  their system is linear
and only contains   diffusion process  with the control af ne
in drift and updated at event time  The event times are driven
by    xed point process  However  we study jump diffusion
SDEs and directly control the intensity that drives the time
of event  Hence our work is unique among previous works 
  Conclusions
We have presented   generic framework to control the
stochastic intensity function of   general point process  such
that   nonlinear SDE driven by the point process is steered
towards   target state  We exploit the measuretheoretic
view of the stochastic intensity control problem  derive an
analytical form of the optimal measure  and compute the
optimal policy using   KL divergence objective  We provide
  scalable algorithm with superior performance in diverse
social problems  There are many interesting venues for
future work  For example  we can apply our method to
other interesting problems  such as in uence and activity
maximization  Kempe et al    Farajtabar et al   
Acknowledgements  This project was supported in part by
NSF IIS NIH BIGDATA    GM  NSF
CAREER IIS  NSF IIS  EAGER  ONR
   NVIDIA  Intel and Amazon AWS 

    State cost  average rank 

    prediction accuracy

Figure   Real world experiment with two evaluation schemes 

of xi   iv  Compute prediction accuracy by dividing the
number of pairs with consistent ordering in step   and step
  by total number of pairs  We report the accuracy over ten
runs by choosing each different interval for training once 
Figure     compares the average rank of the broadcaster
of different methods  We compute the average rank by
dividing the state cost by window length  and average over
all broadcasters  KLMPC achieves the lowest average rank
and is   lower than the CEMPC  Speci cally  it achieves
the rank around   at each time  which is nearly the ideal
scenario where the broadcaster always remains on top 
Figure     further shows that our method performs the best 
it achieves more than   improvement over CEMPC 
hence our method has   more of the total realizations
correctly  Accurate prediction means that if applying our
control policy to the users  we will achieve the objective
much better than alternative methods 
Figure     compares the controlled intensity of one broadcaster with the uncontrolled intensity of his competitors  It
shows that KLMPC increases his intensity when that of
other competitors is large  and decreases his intensity when
competitor   intensity is small  For example  around timestamp   and   competitors have large intensities  hence to
remain on top  this broadcaster needs to double his intensity
to create more posts  Moreover  on     others are not
active and this broadcaster keeps   low intensity  His behavior is adaptive since our control cost ensures the broadcaster
not to deviate too much from his original intensity 
  Further Related Work
We  rst review relevant works in the machine learning community  Some works focus on controlling the point process

 MethodsAverage RankMethodsKLMPCCE MPCKLOLCE OLFDMPCFD OLGreedyBI MethodsPrediction accuracyMethodsKLMPCCE MPCKLOLCE OLFDMPCFD OLGreedyBIVariational Policy for Guiding Point Processes

References
Aalen  Odd  Borgan  Ornulf  and Gjessing  Hakon  Survival and event history analysis    process point of view 
Springer   

Ades  Michel  Caines  Peter    and Malham  Roland   
Stochastic optimal control under poissondistributed observations  Automatic Control  IEEE Transactions on   
   

Anderson  Ashton  Huttenlocher  Daniel  Kleinberg  Jon 
and Leskovec  Jure  Steering user behavior with badges 
In WWW  pp     

Bayraktar  Erhan and Ludkovski  Michael  Liquidation in
limit order books with controlled intensity  Mathematical
Finance     

Boel    and Varaiya     Optimal control of jump processes 
SIAM Journal on Control and Optimization   
   

Br maud  Pierre  Point processes and queues   
Camacho  Eduardo   and Alba  Carlos Bordons  Model
predictive control  Springer Science   Business Media 
 

Dai  Hanjun  Wang  Yichen  Trivedi  Rakshit  and Song 
Le  Deep coevolutionary network  Embedding user
and item features for recommendation  arXiv preprint
arXiv   

Daley       and VereJones     An introduction to the theory of point processes  volume II  general theory and
structure  volume   Springer   

De  Abir  Valera  Isabel  Ganguly  Niloy  Bhattacharya 
Sourangshu  and Rodriguez  Manuel Gomez  Learning
opinion dynamics in social networks  arXiv preprint
arXiv   

Du  Nan  Wang  Yichen  He  Niao  and Song  Le  Time
sensitive recommendation from recurrent user activities 
In NIPS  pp     

Dupuis  Paul and Ellis  Richard      weak convergence
approach to the theory of large deviations  volume  
John Wiley   Sons   

Farajtabar  Mehrdad  Du  Nan  GomezRodriguez  Manuel 
Valera  Isabel  Zha  Hongyuan  and Song  Le  Shaping
social activity by incentivizing users  In NIPS  pp   
   

Farajtabar  Mehrdad  Wang  Yichen  GomezRodriguez 
Manuel  Li  Shuang  Zha  Hongyuan  and Song  Le  Coevolve    joint point process model for information diffusion and network coevolution  In NIPS  pp   
 

Hanson  Floyd    Applied stochastic processes and control
for Jumpdiffusions  modeling  analysis  and computation  volume   Siam   

Hawkes  Alan    Spectra of some selfexciting and mutually
exciting point processes  Biometrika     

He  Niao  Harchaoui  Zaid  Wang  Yichen  and Song  Le 
Fast and simple optimization for poisson likelihood models  arXiv preprint arXiv   

He  Xinran  Rekatsinas  Theodoros  Foulds  James  Getoor 
Lise  and Liu  Yan  Hawkestopic    joint model for
network inference and topic modeling from textbased
cascades  In ICML  pp     

Heemels  WPMH  Johansson  Karl Henrik  and Tabuada 
Paulo  An introduction to eventtriggered and selftriggered control  In   IEEE  st IEEE Conference
on Decision and Control  CDC  pp    IEEE 
 

Karimi     Tavakoli     Farajtabar     Song     and
GomezRodriguez     Smart broadcasting  Do you want
to be seen  In KDD  pp     

Kempe  David  Kleinberg  Jon  and Tardos   va  Maximizing the spread of in uence through   social network  In
KDD  pp    ACM   

Lemmon  Michael  Eventtriggered feedback in control  estimation  and optimization  In Networked Control Systems 
pp    Springer   

Lian  Wenzhao  Rao  Vinayak  Eriksson  Brian  and Carin 
Lawrence  Modeling correlated arrival events with latent
semimarkov processes  In ICML  pp     

Lian  Wenzhao  Henao  Ricardo  Rao  Vinayak  Lucas 
Joseph    and Carin  Lawrence    multitask point process
predictive model  In ICML  pp     

Meng  Xiangyu  Wang  Bingchang  Chen  Tongwen  and
Darouach  Mohamed  Sensing and actuation strategies for
event triggered stochastic optimal control  In  nd IEEE
Conference on Decision and Control  pp   
IEEE   

Ogata  Yosihiko  On lewis  simulation method for point
processes  IEEE Transactions on Information Theory   
   

Oksendal  Bernt Karsten and Sulem  Agnes  Applied
jump diffusions  volume  

stochastic control of
Springer   

Pan  Jiangwei  Rao  Vinayak  Agarwal  Pankaj  and Gelfand 
Alan  Markovmodulated marked poisson processes for
checkin data  In ICML  pp     

Variational Policy for Guiding Point Processes

Robotics and Automation  ICRA    IEEE International Conference on  pp    IEEE   

Xiong  Liang  Chen  Xi  Huang  TzuKuo  Schneider 
Jeff    and Carbonell  Jaime    Temporal collaborative
 ltering with bayesian probabilistic tensor factorization 
In SDM  pp     

Zarezade  Ali  Upadhyay  Utkarsh  Rabiee  Hamid    and
GomezRodriguez  Manuel  Redqueen  An online algorithm for smart broadcasting in social networks  In
WSDM  pp    ACM   

Zhou  Ke  Zha  Hongyuan  and Song  Le  Learning social infectivity in sparse lowrank networks using multidimensional hawkes processes  In AISTAT  volume  
pp     

Peters  Jan and Schaal  Stefan  Policy gradient methods
In Intelligent Robots and Systems   

for robotics 
IEEE RSJ International Conference on  IEEE   

Pham  Huy    Optimal stopping of controlled jump diffusion processes    viscosity solution approach  In Journal
of Mathematical Systems  Estimation and Control  Citeseer   

Stulp  Freek and Sigaud  Olivier  Path integral policy improvement with covariance matrix adaptation  In ICML 
 

  li  Endre and Mayers  David    An introduction to numer 

ical analysis  Cambridge university press   

Tan  Xi  Naqvi  Syed AZ  Qi  Alan Yuan  Heller  Katherine    and Rao  Vinayak  Contentbased modeling of
reciprocal relationships using hawkes and gaussian processes  In UAI  pp     

Theodorou  Evangelos    Nonlinear stochastic control and
information theoretic dualities  Connections  interdependencies and thermodynamic interpretations  Entropy   
   

Wainwright        and Jordan        Graphical models 
exponential families  and variational inference  Technical Report   UC Berkeley  Department of Statistics 
September  

Wang  Yichen  Chen  Robert  Ghosh  Joydeep  Denny 
Joshua    Kho  Abel  Chen  You  Malin  Bradley    and
Sun  Jimeng  Rubik  Knowledge guided tensor factorization and completion for health data analytics  In KDD 
pp     

Wang  Yichen  Du  Nan  Trivedi  Rakshit  and Song  Le 
Coevolutionary latent feature processes for continuoustime useritem interactions 
In NIPS  pp   
   

Wang  Yichen  Theodorou  Evangelos  Verma  Apurv  and
Song  Le    stochastic differential equation framework
for guiding online user activities in closed loop  arXiv
preprint arXiv     

Wang  Yichen  Xie  Bo  Du  Nan  and Song  Le  Isotonic

hawkes processes  In ICML  pp       

Wang  Yichen  Ye  Xiaojing  Zhou  Haomin  Zha 
Hongyuan  and Song  Le  Linking micro event history to
macro prediction in point process models  In AISTAT  pp 
   

Williams  Grady  Drews  Paul  Goldfain  Brian  Rehg 
James    and Theodorou  Evangelos    Aggressive
driving with model predictive path integral control  In

