Dictionary Learning Based on Sparse Distribution Tomography

Pedram Pad     Farnood Salehi     Elisa Celis   Patrick Thiran   Michael Unser  

Abstract

We propose   new statistical dictionary learning
algorithm for sparse signals that is based on an
 stable innovation model  The parameters of
the underlying model that is  the atoms of the
dictionary  the sparsity index   and the dispersion of the transformdomain coef cients are
recovered using   new type of probability distribution tomography  Speci cally  we drive our
estimator with   series of random projections of
the data  which results in an ef cient algorithm 
Moreover  since the projections are achieved using linear combinations  we can invoke the generalized central limit theorem to justify the use
of our method for sparse signals that are not necessarily  stable  We evaluate our algorithm by
performing two types of experiments  image inpainting and image denoising  In both cases  we
 nd that our approach is competitive with stateof theart dictionary learning techniques  Beyond
the algorithm itself  two aspects of this study are
interesting in their own right  The  rst is our statistical formulation of the problem  which uni es
the topics of dictionary learning and independent
component analysis  The second is   generalization of   classical theorem about isometries of
 cid pnorms that constitutes the foundation of our
approach 

  Introduction
The problem of  nding the mixing matrix   from   set of
observation vectors   in the model

    Ax

 

is only solvable if one can bene   from strong hypotheses
on the signal vector    For instance  one may assume that

 Equal contribution  Biomedical Imaging Group  EPFL  Lausanne  Switzerland  Computer Communications and Applications
Laboratory   EPFL  Lausanne  Switzerland  Correspondence to 
Pedram Pad  pedram pad ep ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

the entries of   are statistically independent  which results
in   class of methods refered to as independent component
analysis  ICA   Hyv rinen et al      more recent
trend is to assume that the vector   is sparse  so that the
recovery can be recast as   deterministic dictionary learning
problem  the prototypical example being sparse component
analysis  SCA   Gribonval   Lesage    Aharon et al 
  Spielman et al    Extensive research has been
conducted on these problems in the past three decades 
Prior work  In the literature  ICA precedes SCA and can
be traced back to  Herault   Jutten    In fact  ICA constitutes the nonGaussian generalization of the much older
principal component analysis  PCA  which is widely used
in classical signal processing  ICA is usually formalized
as an optimization problem involving   cost function that
measures the independence of the estimated xi       the
entries of the vector      common measure of independence  which is inspired by information theory  is the mutual
information of the entries of    However  due to its computational complexity  other measures such as the kurtosis 
which measures the nonGaussianity of the components  are
often used  Hyv rinen   Oja    Naik   Kumar   
 except in special cases such as the analysis of stable AR 
processes by  Pad   Unser    The main drawback of
ICA is that the system   needs to be determined        
should be square otherwise the complexity is so high that
the methods can only be implemented for small problems
 Lathauwer et al    Lathauwer   Castaing   
SCA  on the other hand  is usually achieved by putting
constraints on the sparsity of the representation or by optimizing   sparsitypromoting cost function  Thanks to the
emergence of very ef cient algorithms  SCA has found
wide use in different applications  see  Mairal et al   
Marvasti et al    The underlying framework for
SCA is deterministic this is the primary difference with
ICA  which aims to decouple signals that are realizations of
stochastic processes 
 stable distributions  In this paper  we aim to achieve the
best of both worlds  the use of   statistical formulation in
the spirit of ICA with   restriction to   parametric class
of stochastic models that is well adapted to the notion of
sparsity  Speci cally  we assume that the entries of the
vector   are random variables that are        symmetric 

Dictionary Learning Based on Sparse Distribution Tomography

We also show that the proposed algorithm provides an ef 
cient estimator of the spectral measure of   stable random
vector  An enabling component of our method is   new
theorem that generalizes   classical result about isometries
of  cid pnorms 
Organization  In the next section  we brie   review    
random variables and present our mathematical model  In
Section   we establish our main result which then yields an
algorithm for  nding the matrix   as well as the sparsity
index   In Section   we present the simulation results and
compare their performance with existing algorithms  In Section   we summarize the paper and give some suggestions
for future work 

  Preliminaries and problem formulation
We begin by recalling some basic properties of symmetric 
 stable random variables  We then proceed with the formulation of the estimation problem that we solve in Section  
The notation that we use throughout the paper is as follows 
we use italic symbols for random variables  capital boldface
symbols for matrices and lowercase boldface symbols for
vectors  Thus    is   deterministic matrix    is   random
matrix and   is   random variable  Likewise    and   denote
  random and   deterministic vector respectively 

  Symmetric stable random variables
For any         and         random variable   with

characteristic function cid pX     exp 

 

is called   symmetric stable       random variable with
dispersion   and stability parameter    Nikias   Shao 
  This class of random variables is   generalization of
the Gaussian model  For         is   Gaussian random
variable with zero mean and variance   As their name
suggests   stable variables share the property of stability
under linear combination  Nikias   Shao         if
           Xn are          copies of   and            an     are
  real numbers  then the random variable
               anXn

 

has the same distribution as

 cid           an cid   

    

 

     an cid   

In other words  the random variable   is an     random
variable with dispersion    cid   cid 

  where  cid   cid     cid     

  is the  pseudo norm of the vector    
            an  This property makes     random variables
convenient for the study of linear systems 

Figure   Illustration of the effect of   on the sparsity of the signal 
Two realizations of            signals 
stable  The family of  stable distributions is   generalization of the Gaussian probability density function  PDF 
Since  stability is preserved through linear transformation  this class of models has   central position in the study
of stochastic processes  Samoradnitsky   Taqqu   
Nikias   Shao    Shao   Nikias    The family is
parametrized by         which controls the rate of decay
of the distribution  The extreme case of       corresponds
to the Gaussian distribution the only nonsparse member
of the family  By contrast  the other members of the    
family for       are heavytailed with unbounded variance 
This property implies that an        sequence of such random variables generates   sparse signal  Amini et al   
Gribonval et al    By decreasing   the distribution
becomes more heavytailed and thus the signal becomes
more sparse  the effect of   is illustrated in Figure  
This class of random variables has also been widely used
in practice  Typical applications include  modeling of ultrasound RF signals   Achim et al    signal detection
theory  Kuruoglu et al    communications  Middleton 
  image processing  Achim   Kuruoglu    audio
processing  Georgiou et al    sea surface  Gallagher 
  network traf    Resnick    and  nance  Nolan 
  Ling   
Main contributions  Our main contribution in this paper
is   new dictionary learning algorithm based on the signal
modeling mentioned above  The proposed method has the
following advantages 

  all parameters can be estimated from the data  it is

hyperparameterfree 

  it learns the dictionary without the need to recover the

signal    and

  it is fast and remarkably robust 

Once the matrix   is estimated  it is then possible to ef 
ciently recover   by using standard procedures  Bickson  
Guestrin   

     SampleIndex SampleValue Dictionary Learning Based on Sparse Distribution Tomography

The other property of     random variables with       is
their heavytailed PDF  When       we have
   pX           

 

lim   

where pX is the PDF of   and      is   positive constant that depends on   and    Nikias   Shao    This
implies that the variance of     random variables is unbounded for       Also  note that   smaller   results in
heavier tails 
In nitevariance random variables are considered to be appropriate candidates for sparse signals  Amini et al   
Gribonval et al    Because an        sequence of heavytailed random variables has most of its energy concentrated
on   small fraction of samples  they are good candidates to
model signals that exhibit sparse behavior 
Yet  the truly fundamental aspect of  stable random variables is their role in the generalized central limit theorem 
As we know  the limit distribution of normalized sums of
        nitevariance random variables are Gaussian  Likewise  any properly normalized sum of heavytailed       
random variables converges to an  stable random variable
whee the   depends on the weight of their tail  Meerschaert
  Schef er    This implies that   linear combination
of   large number of samples of   sparse signal is well
represented by  stable random variables 

  Problem formulation

Our underlying signal model is

    Ax

 
where   is an unknown       random vector with           
entries and         is an       observation vector and  
is an       dictionary matrix  We are given   realizations
of    namely             yK  and our goal is to estimate   

  Dictionary learning for     signals
In the problem of dictionary learning  the maximum information that we can asymptotically try to retrieve from
           yK is the exact distribution of    However  even if
we knew    identifying   is still not tractable in general 
for instance  in the case of Gaussian vectors    is only
identi able up to rightmultiplication by   unitary matrix 
Moreover  obtaining an acceptable estimate of the distribution of   requires  in general    vast amount of data and
processing power  since it is   mdimensional function with
  possibly large  In this section  we leverage the property
of stability under linear combination of     random variables explained in Section   to propose   new algorithm
to estimate   for the dictionary learning problem stated in
Section  

  New cost function for sparse     signals

 

 cid 

Recall that  the random vector    see Equations   and
  is an mdimensional  stable vector with characteristic
function

 cid py    exp cid cid   cid cid 

 
for     Rm  Thus  knowing  cid   cid   cid  for all         
where      is the       dimensional unit sphere      
 
is equivalent to knowing the distribution of    Note that
  cid       cid Ax  see Equations   and   is an     random variable with dispersion

            Rm    cid   cid       

       cid   cid   cid 
 

 
Thus  knowing the dispersion of the marginal distributions
of   for all          is equivalent to knowing the distribution of    In other words  in the case of     random
vectors  knowing their marginal dispersions is equivalent to
knowing the Radon transform of their PDFs or  equivalently 
their joint characteristic function  Helgason    Due to
the relationship between the Radon transform and the  eld
of tomography  we call our algorithm sparse distribution
tomography  SparsDT 
Another interesting fact is that  in the nonGaussian case
      knowing the marginal dispersions of            
identi es the matrix   uniquely  up to negation and permutation of the columns  Formally  we have the following
theorem  which is proved in Appendix   
Theorem   Let   be an       matrix where columns are
pairwise linearly independent  If         and   is an
      matrix for which we have

 cid   cid   cid 

     cid   cid   cid 

 
for all     Rm  then   is equal to   up to negation and
permutation of its columns 

 

Remark   This theorem can be seen as   generalization of
the result in  Rolewicz    that states that the isometries
of  cid pnorms are generalized permutation matrices  permutation matrices with some of their rows negated  To the
best of our knowledge  this result is novel and could be of
independent interest 

This theorem suggests that in order to  nd   all we need is
to  nd     for     Rm  Intuitively  we can say that as  
has    nite number of parameters  entries    is identi able
based on the knowledge of     for an appropriate  nite
set of vectors                uL  for some     mn  We can
then solve the set of nonlinear equations

 cid   cid   cid 

       

 cid   cid uL cid 

     uL 

 

 

Dictionary Learning Based on Sparse Distribution Tomography

merged into the learned dictionary  Recall that there are
wellknown methods for estimating   from data  among
which we use

 cid log    cid yk    log    cid     

 cid   

 

 

 cid 

  cid 

  

     

 

  

from  Achim et al    where

 

 

log    cid yk 

log      

 
 

This gives us an estimate for   for any given     Rm 
Hence  the estimated value of   is the average over all
   cid  for  cid                     

  cid 

  

  cid 

 cid 

   

 
 

   cid 

 

 

Now  using this estimate  Equation   becomes
log         log            

We also replace   with   in Equation   which results in
  parameterfree cost function  This is in contrast with most
existing cost functions that have parameters one must set 

  cid 

  

for   to obtain   
Now  the problem is to  nd     for   given     Rm  Recall
that     is the dispersion of the     random variable uT   
As            yK are realizations of    uT            uT yK are
realizations of uT    There is   rich literature on the estimation of the parameters of  stable random variables through
their realizations  see        Nikias   Shao    We use
the estimation from  Achim et al    in the following
equation

log      

 
 

log  uT yk         

 

where   is the digamma function   is the negative of the
EulerMascheroni constant and is approximately  
and     denotes the estimation of     Note that    
tends to     when the number of observations     tends
to in nity  This means that we can obtain the exact value of
    asymptotically 
However  nonexact values for    cid  for  cid               
 which is the case when   is  nite  can lead to the nonexistence of   solution for the system of equations  
To overcome this problem  instead of solving this system
of equations exactly  we minimize the following objective
function

  cid 
     cid cid 
  cid cid   cid   cid cid 
  cid 
    log   cid cid cid 
 cid cid log cid   cid   cid cid 

 cid 

      

 
 

 

 
  

 cid 

where log             log  uL  are   numbers calculated
  log     log    is
from   The cost function            
  continuous positive function  from    to    whose global
minimum is   and is reached over the line        When
                minimizes      Thus  if     is
close enough to     due to the continuity of    we expect
that the minimizer of   will be close to    Therefore  our
approach to dictionary learning is to solve

 cid     argmin

 

    

  cid 

 cid 

  argmin

 

 
  

 cid cid log cid cid   cid   cid cid 

 

 

 cid    log    cid cid cid   

 

  Learning algorithm

  cid 

To solve the minimization problem in Equation   we
propose   variation on   gradientdescent algorithm with
an adaptive step size that has   changing cost function  To
do so  we  rst derive the gradient of   at    Using matrix
calculus  see Appendix    we  nd that
       

 

 
 cid   cid   cid cid 
  
where sgn  is the sign function       sgn        if      
and sgn        otherwise  and

sgn cid log cid   cid   cid cid 
    log    cid cid     cid   cid   cid cid 
 sgn cid   cid 
    cid cid cid   cid 
    cid cid cid   cid 
sgn cid   cid 

    cid cid 
    cid cid 

 cid   cid   cid 

 

     

  cid 

  cid 

 

 

 

 cid 

 cid 

 

The only parameter that needs to be set now is the stability
parameter   Note that the dispersion parameter   in Equation   does not need to be set as it will be automatically

 In our simulations we also implemented other natural candidates for         and all of them gave approximately the same
performance  Due to the limited space  we do not present results
for other cost functions 

where bi is the ith column of   
The cost function in Equation   is nonconvex in    In
order to avoid getting trapped in local minima  we iteratively
change the cost function inside the gradient descent algorithm  The idea is that instead of keeping            uL  xed
throughout the optimization process  we regenerate them
randomly with   uniform distribution on Rm after some

Dictionary Learning Based on Sparse Distribution Tomography

iterations of steepest descent  We repeat this process until
convergence  Note that   holds for any            uL and
thus changing this set does not change the end result of  

Remark   Using this idea always results in convergence
to the global minimum in our computer simulations   
plausible explanation of this phenomenon is that each set of
           uL yields   nonconvex cost function with different
local minima  Yet they all have the same global minimum 
Therefore  switching between them during the optimization
process prevents getting trapped in any of the local minima 
which ultimately results in  nding the global minimum of
the cost function 

The pseudocode of our dictionary learning method is given
in Algorithm   There    is the step size of the gradient
descent that increases or decreases by factors of   or  
upon taking   good or poor step  The adaptive step size is
especially helpful for       where the cost function is not
smooth  The algorithm does not depend on the exact choice
of convergence criteria 

In our problem  we have    cid  

Remark   Algorithm   can also be seen as an ef cient
method for estimating the spectral measure of stable random
vectors  In fact  the problem of estimating   from   set of
realizations of   can also be seen as parameter estimation
for   stable random vector   with   symmetric distribution
around the origin  Such random vectors are parametrized
by   measure   on      that is called the spectral measure 
    cid ai cid   ai  where
the  ais are unit point masses at
and ai is the ith
ai cid ai cid 
column of    Some methods have been proposed to solve
this problem        Nolan et al    However  they tend
to be computationally intractable for dimensions greater
than  

Remark   According to the generalized central limit theorem  under some mild conditions  the distribution of the sum
of symmetric heavytailed random variables tends to      
distribution as the number of summands tends to in nity 
This means that we can represent   cid       cid Ax with an
    random variable for large enough   irrespective of the
distribution of the xis provided that the latter are heavy
tailed  Therefore  we expect Algorithm   to  nd applications for other classes of sparse signals  provided that   is
suf ciently large 

  Empirical results
In this section  we analyze the performance of the proposed
algorithm SparsDT and compare it with existing methods 
Recall that the actual dictionary is   and the learned dictio 

nary is  cid    We run two types of the experiments  We  rst

test the algorithm on synthetic     data and then we test it
on real images 

Algorithm   SparseDT
  initialize       
  initialize        and      
  initialize  generate            bn       Im    and

   cid         bn

 cid 

initialize  generate            uL       Im   
estimate   from  
        
repeat
Bold    
Eold    
               
        
if     Eold then

  repeat
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  until   converges

         
    Bold
    Eold
         

end if

else

return  

  Benchmarks

until   converges  for this choice of            uL 

We compare our algorithm with three commonly used algorithms that are available in the Python package SPAMS 
These constrained optimization problems  are as follows 
   cid cid  Maximizing the data  delity while controling

the sparsity of representation with parameter  

 cid   cid cid    argmin

 

 
  

 cid yk   Bxk cid 

 

      cid xi cid     

   cid cid  Maximizing the sparsity of representation while

controling the data  delity with parameter  

 cid   cid cid    argmin

 

 
  

 cid xk cid 

      cid yk   Bxk cid     

   cid     cid  Combining sparsity and data  delity in the

cost function using Lagrange multipliers 

  cid 

  

  cid 

  

 cid   cid cid    argmin

 

 
  

  cid 

  

 cid yk   Bxk cid 
   cid xk cid     cid xk cid 
 

 

 http spamsdevel gforge inria fr 
 Other cost functions are also available in the package SPAMS 

but those retained here yield the best results in our experiments 

Dictionary Learning Based on Sparse Distribution Tomography

Comparison metrics
Algorithm   found Avg  time    
SparsDT
 cid cid 
 cid cid 
 cid     cid 

 
 
 
 

 
 
 
 

Figure   Impact of the number of samples   on the average correlation for   
One of the challenges in utilizing these benchmarks is determining the regularization parameters             In our
experiments  the regularization parameters are optimized
 by grid search  in order to maximize the performance of
each of the benchmarks above  This is in contrast to our
algorithm  which has no regularization parameter to tune 

  Experimental results for synthetic data

We  rst test the algorithms on synthetic data  In order to
quantify the performance of the algorithms  we use several
metrics  One is the average correlation of the dictionaries  Speci cally  we calculate the correlation between all

columns of  cid   and    and then match each column of  cid  

with one of the columns of      oneto one map  such that
the average correlation between the corresponding columns
is maximized  Additionally  we say that the dictionary is
found if the average correlation of the columns is larger than
 
Effect of    We demonstrate the effect of the number
of samples   on the performance of our proposed algorithm SparsDT  Intuitively  the precision of the estimation
increases with the number of samples    and  as   goes
to in nity  the estimation error goes to zero  which ultimately gives the exact    We demonstrate this effect with
the following experiment  We take             and
      and   Then  for each    we run the experiment
for   random matrices    and  for each case  we run Algorithm   with both exact and estimated    from   The
results are depicted in Figure   where the vertical axis is
the average correlation of the estimated dictionary with the
exact one  and the horizontal axis is the number of samples    Interestingly  the performance of the algorithm is
almost the same when using the exact or estimated value
of   which suggests that the estimator of   is robust and
accurate  Moreover  we see that the average correlation is
an increasing function of    as expected  Also note that the
convergence is faster for       which corresponds to the
setting with more sparsity 

Table   Performance of Algorithms on     Signals       
   matrix       
Comparison against benchmarks  We compare SparsDT
against the  cid cid   cid cid  and  cid     cid  methods described
above  We compare the algorithms with regard to their
success rate       the percentage of the dictionaries found
by the algorithm  and the time that they take to  nd the
dictionary  in the cases of success only  We again take
            and generate   random matrices    In
Table   the results for       and       are given 
Finally  in Figure   we compare the algorithms success
rate for different   we take             and    
  These results indicate that SparsDT outperforms the
other methods in the rate of success  Also  its average
learning time is typically much less than the others  except
for  cid cid  which does not  nd the correct dictionary at best
in   of the time  The range of   that was observed in
our experiments is         which is also the range
where our algorithm works well and which is interesting
for many applications including image processing  We do
not recommend using the method for       because
the convergence properties degrade as we get closer to  
   larger value of   is then needed to reach high success
rates 

  Experimental results for real images

Since images often have sparse representations  we apply
our algorithm to problems in image processing applications 
Our experiments are missing pixel recovery  inpainting 
and denoising  based on dictionary learning  We use  

Figure   Impact of         on the success rate of the algorithms 

  AvgCorrolation exact estimated exact estimated founddictionary SparsDTDictionary Learning Based on Sparse Distribution Tomography

Algorithm PSNR  dB 
 
SparsDT
 
 cid cid 
 
 cid cid 
 
 cid     cid 

Table   Performance of different methods for denoising images
contaminated by Gaussian noise 
database of face images provided by AT    and crop them
to have size       so we can chop each image to  
patches of size       which correspond to yi in our model 
In this situation  the data is not exactly      so we must
adapt our choice of   in Step   of Algorithm   Speci cally 
in Equation   we eliminate projection vectors   that
result in   greater than    as   is required to be less than  
In addition  we only select   that results in an   close to our
estimated   in   The number of iterations are chosen
such that all algorithms converge 
Missing pixel recovery  In this experiment  we reconstruct
an image from   of its pixels  We take out the image
shown in Figure   remove   of its pixels uniformly
at random  and learn the dictionary using the patches of
the other images in the collection  We assume   atoms
in the dictionary  Then  using the learned dictionary  we
reconstruct the image using orthogonal matching pursuit
 for   detailed analysis see  Sahoo   Makur    The
results for different dictionary learning methods are depicted
in Figure   SparsDT outperforms existing methods both
visually and in term of PSNR 
Image denoising  In this experiment  we use the dictionaries learned in the previous experiment to denoise the image
in Figure   More precisely  we add Gaussian noise with
standard deviation   to the original image and use orthogonal matching pursuit to denoise it  The performance of
each method in PSNR can be seen in Table   As we see 
SparsDT outperforms the other methods by at least   dB 

  Summary and future work
In this paper  we consider   stochastic generation model of
sparse signals that involves an     innovation  Then  by
designing an estimator of the spectral measure of sode ned
stable random vectors  we propose   new dictionary learning algorithm  The proposed algorithm  SparsDT  turns
out to be quite robust  it works well on sparse realworld
signals  even when these do not rigorously follow the    
model  This surprising fact can be explained by invoking
the generalized central limit theorem  We validate SparsDT
on several imageprocessing tasks and found it to outperform popular dictionary learning methods often signi cantly 

 www cl cam ac uk research dtg attarchive facedatabase html

Moreover  SparsDT has no parameter to tune  contrary to
other algorithms 
Extending this work to nonsymmetric  stable random variables is   possible direction of future research  Given the
excellent numerical behavior of the algorithm  it is of interest to get   good handle on the accuracy of the estimation in
terms of the number of samples and the dimensionality of
signals 

   Proof of Theorem  
Denote the jth column of   and   by aj and bj  respectively  Also  denote the set of indices   for which bj  cid   
by                  Note that due to the assumption of the
pairwise linear independence of columns of    aj  cid    for
all                  Since

 cid cid   cid   cid cid 

   

  cid 

  

 cid cid   cid aj

 cid cid 

 

 cid 

   

 cid cid   cid bj

 cid cid 

 cid cid   cid   cid cid 

   

 cid cid   cid   cid cid 

for all     Rm  the partial derivatives of any order of the
two side of the equation are also equal  In particular  we
have

 

 

   

 
for all                 and        where ui is the ith entry of
  
First we prove the theorem for           In   we set
      and obtain

 

  
 ui

 cid cid   cid   cid cid 

  
 ui

  cid 
 cid cid  sgn cid   cid aj
 cid cid   cid aj
 cid  aij
 cid 
 cid cid  sgn cid   cid bj
 cid cid   cid bj
 cid  bij 

  

 

   

 

Exploiting this equation  we prove the following lemma 

Lemma   Under the assumptions of Theorem   for any
  cid                 there exists       and tj cid   cid    such that
tj cid aj cid    bj 
Proof  Take   cid  such that ai cid   cid 
             de ne

 cid    Also  for all    

   cid     Rm   cid ar    cid 

Ka

 
which is an       dimensional subspace of Rm  Since
for any    cid    cid  aj cid  and aj are linearly independent  the
subspace Ka
  is       dimensional  This implies
that their       dimensional Lebesgue measure is zero 

  cid    Ka

and the same holds for the union cid 
  cid     cid 

  cid     cid 

Ka
    Ka

 cid  Since
 cid   

  cid   cid cid Ka
 cid Ka

  cid    Ka
  cid    Ka

Ka

 

 

 

  cid   cid 

  cid   cid 

Dictionary Learning Based on Sparse Distribution Tomography

Figure   Comparing the proposed method with the existing ones for recovering missing pixels 

  cid     cid 

  cid     cid 

  cid     cid 

  is in nity 
  cid   cid  Ka

we conclude that the       dimensional Lebesgue measure of Ka
  cid   cid  Ka
Note that any     Ka
  is only orthogonal to
aj cid  and not to any other column of    This yields that if
we set       cid  in the lefthand side of   for any    
Ka
    the only discontinuous term at   is the
  cid th one  because the function    sgn    has   single
point of discontinuity at       As   result  the sum itself
is discontinuous over Ka
    Hence  the same
should hold for the righthand side of the equation 
Similar to   de ne

  cid     cid 

  cid   cid  Ka

  cid   cid  Ka

Kb

   cid     Rm   cid br    cid   
  cid     cid 
  cid     cid 

     cid 
     cid 
 cid Ka

   Therefore  we have
Kb

    Kb
Ka

  cid    Kb

Ka

  cid   cid 

   

 

 

  cid   cid 

   

 

 

 

 cid 

which can also be written as
Ka

Ka

is   subset of cid 

The set of discontinuity points of the righthand side of  

  cid    Kb

Now  if none of the columns of   is linearly dependent to
aj cid  all Ka
  will be       dimensional spaces  and
their       dimesnional Lebesgue measure is zero  This
implies that the       dimensional Lebesgue measure of
the righthand side of   is also zero  which contradicts
the result after   Therefore  there exists         such
that bj is linearly dependent to aj cid  which completes the
 cid 
proof of the lemma 
The  rst consequence of Lemma   is that none of the
columns of   is the zero vector and thus                 

 cid  aij    
  cid     cid 

Also  since all pairs of columns of   are linearly independent  the correspondence between   column of   and
  column of   that are linearly dependent is oneto one 
Thus  we can simplify   to be

     tj cid cid   cid aj

 cid cid  sgn cid   cid aj

 

  cid 

  

which holds for all    This implies that the left handside of
the above equation is   continuous function  However  as we
saw in the proof of the lemma  every     Ka
  cid   cid  Ka
is   discontinuity point of the lefthand unless        cid 
      
which completes the proof for the case of          
For the case of           we set       in   and obtain

 

 cid cid   cid aj

 cid cid 

  cid 

  

 cid cid   cid bj

 cid cid 

  cid 

  

  
ij  

  
ij 

 

Replacing   by   the same reasoning as for          
works to prove the theorem for          
   Derivation of the gradient of     
To calculate the gradient of      we  rst calculate the
gradient of  cid   cid   cid 
  using the de nition of the gradient      
    cid   

 cid cid cid 
 cid cid   cid       cid   cid cid 
  cid 
    cid cid cid   cid 
    cid cid 
    sgn cid   cid 

 cid cid   cid   cid 

 
 

  cid 

   

 

 

  

Here   cid      cid    tr   cid    is the standard inner product on
the space of matrices  Writing the last equation in the matrix
dx  log     
form  we obtain   Now  using the fact  
  and the chain rule for differentiation yields
sgn log     
 

Originalimage missingpixelsSparsDTPSNR dB PSNR dB PSNR dB PSNR dBDictionary Learning Based on Sparse Distribution Tomography

Acknowledgements
The research was partially supported by the Hasler Foundation under Grant   by the European Research Council
under Grant      ERC Project GlobalBioIm 
and by the SNF Project Grant    

References
Achim     and Kuruoglu     Image denoising using bivariate
 stable distributions in the complex wavelet domain 
IEEE Signal Processing Letters     

Hyv rinen     and Oja     Independent component analysis 
algorithms and applications  Neural networks   
   

Hyv rinen     Karhunen     and Oja    

Independent
component analysis  volume   John Wiley   Sons 
 

Kuruoglu        Fitzgerald        and Rayner        Near
optimal detection of signals in impulsive noise modeled
IEEE
with   symmetric spl alpha stable distribution 
Communications Letters     

Achim     Basarab     Tzagkarakis     Tsakalides    
and Kouam     Reconstruction of ultrasound RF echoes
modeled as stable random variables  IEEE Transactions
on Computational Imaging    June  

Lathauwer     De and Castaing     Blind identi cation of
underdetermined mixtures by simultaneous matrix diagonalization  IEEE Transactions on Signal Processing   
   

Aharon     Elad     and Bruckstein     Ksvd  An algorithm for designing overcomplete dictionaries for sparse
representation  IEEE Transactions on signal processing 
   

Lathauwer     De  Castaing     and Cardoso     Fourthorder
cumulantbased blind identi cation of underdetermined
mixtures  IEEE Transactions on Signal Processing   
   

Amini     Unser     and Marvasti     Compressibility
of deterministic and random in nite sequences  IEEE
Transactions on Signal Processing   
November  

Bickson     and Guestrin     Inference with multivariate
heavytails in linear models  In Advances in Neural Information Processing Systems  pp     

Gallagher       method for  tting stable autoregressive
models using the autocovariation function  Statistics  
probability letters     

Georgiou     Tsakalides     and Kyriakakis     Alphastable modeling of noise and robust timedelay estimation
in the presence of impulsive noise  IEEE transactions on
multimedia     

Gribonval     and Lesage       survey of sparse component
analysis for blind source separation  principles  perspectives  and new challenges  In ESANN  proceedings 
 th European Symposium on Arti cial Neural Networks 
pp    dside publi   

Gribonval     Cevher     and Davies        Compressible distributions for highdimensional statistics  IEEE
Transactions on Information Theory   
August  

Helgason     Integral Geometry and Radon Transforms 

Springer   

Herault     and Jutten     Space or time adaptive signal
processing by neural network models  In Neural networks
for computing  volume   pp    AIP Publishing 
 

Ling     Selfweighted least absolute deviation estimation
for in nite variance autoregressive models  Journal of the
Royal Statistical Society  Series    Statistical Methodology     

Mairal     Bach     Ponce     and Sapiro     Online learning for matrix factorization and sparse coding  Journal of
Machine Learning Research   Jan   

Marvasti     Amini     Haddadi     Soltanolkotabi    
Khalaj       Aldroubi     Sanei     and Chambers      
uni ed approach to sparse signal processing  EURASIP
journal on advances in signal processing   
 

Meerschaert     and Schef er     Limit distributions for
sums of independent random vectors  Heavy tails in theory and practice  volume   John Wiley   Sons   

Middleton     Nongaussian noise models in signal processing for telecommunications  new methods an results for
class   and class   noise models  IEEE Transactions on
Information Theory     

Naik     and Kumar     An overview of independent component analysis and its applications  Informatica   
 

Nikias        and Shao     Signal Processing with AlphaStable Distributions and Applications  Wiley  New York 
 

Nolan  JP  Modeling  nancial data with stable distributions  Handbook of Heavy Tailed Distributions in Finance 
Handbooks in Finance  Book     

Dictionary Learning Based on Sparse Distribution Tomography

Nolan  JP  Panorska  AK  and McCulloch  JH  Estimation
of stable spectral measures  Mathematical and Computer
Modelling     

Pad     and Unser     Optimality of operatorlike wavelets
for representing sparse AR  processes  IEEE Transactions on Signal Processing    September  

Resnick     Heavy tail modeling and teletraf   data  special
invited paper  The Annals of Statistics   
 

Rolewicz     Metric Linear Spaces  Mathematics and
its applications     Reidel Publishing Company  East
European series     Reidel   

Sahoo     and Makur     Signal recovery from random
measurements via extended orthogonal matching pursuit 
IEEE Trans  Signal Processing     

Samoradnitsky     and Taqqu     Stable nonGaussian random processes  stochastic models with in nite variance 
volume   CRC press   

Shao     and Nikias        Signal processing with fractional lower order moments  stable processes and their
applications  Proceedings of the IEEE   
Jul  

Spielman     Wang     and Wright     Exact recovery of

sparselyused dictionaries  In COLT  pp     

