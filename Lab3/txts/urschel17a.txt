Learning Determinantal Point Processes with Moments and Cycles

John Urschel   VictorEmmanuel Brunel   Ankur Moitra   Philippe Rigollet  

Abstract

Determinantal Point Processes  DPPs  are   family of probabilistic models that have   repulsive
behavior  and lend themselves naturally to many
tasks in machine learning where returning   diverse set of objects is important  While there are
fast algorithms for sampling  marginalization and
conditioning  much less is known about learning the parameters of   DPP  Our contribution
is twofold 
    we establish the optimal sample complexity achievable in this problem and
show that it is governed by   natural parameter 
which we call the cycle sparsity   ii  we propose
  provably fast combinatorial algorithm that implements the method of moments ef ciently and
achieves optimal sample complexity  Finally  we
give experimental results that con rm our theoretical  ndings 

  Introduction
Determinantal Point Processes  DPPs  are   family of probabilistic models that arose from the study of quantum mechanics  Macchi    and random matrix theory  Dyson 
  Following the seminal work of Kulesza and Taskar
 Kulesza   Taskar    discrete DPPs have found numerous applications in machine learning  including in document and timeline summarization  Lin   Bilmes   
Yao et al    image search  Kulesza   Taskar   
Affandi et al    and segmentation  Lee et al   
audio signal processing  Xu   Ou    bioinformatics  Batmanghelich et al    and neuroscience  Snoek
et al    What makes such models appealing is that
they exhibit repulsive behavior and lend themselves naturally to tasks where returning   diverse set of objects is
important 
One way to de ne   DPP is through an       symmetric positive semide nite matrix    called   kernel  whose

 Department of Mathematics  MIT  USA  Correspondence to 

John Urschel  urschel mit edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

eigenvalues are bounded in the range     Then the DPP
associated with    which we denote by DPP    is the
distribution on                         that satis es  for
any         

            det KJ  

where KJ is the principal submatrix of   indexed by the
set    The graph induced by   is the graph            
on the vertex set      that connects             if and only if
Ki    cid   
There are fast algorithms for sampling  or approximately
sampling  from DPP     Deshpande   Rademacher 
  Rebeschini   Karbasi    Li et al       
Marginalizing the distribution on   subset          and conditioning on the event that       both result in new DPPs
and closed form expressions for their kernels are known
 Borodin   Rains   
There has been much less work on the problem of learning
the parameters of   DPP    variety of heuristics have been
proposed  including ExpectationMaximization  Gillenwater et al    MCMC  Affandi et al    and  xed
point algorithms  Mariet   Sra    All of these attempt
to solve   nonconvex optimization problem  and no guarantees on their statistical performance are known  Recently 
Brunel et al   Brunel et al    studied the rate of estimation achieved by the maximum likelihood estimator  but
the question of ef cient computation remains open 
Apart from positive results on sampling  marginalization
and conditioning  most provable results about DPPs are actually negative  It is conjectured that the maximum likelihood estimator is NPhard to compute  Kulesza   
Actually  approximating the mode of size   of   DPP to
within   ck factor is known to be NPhard for some      
    ivril   MagdonIsmail    Summa et al    The
best known algorithms currently obtain   ek       approximation factor  Nikolov    Nikolov   Singh   
In this work  we bypass the dif culties associated with
maximum likelihood estimation by using the method of moments to achieve optimal sample complexity  We introduce
  parameter  cid  which we call the cycle sparsity of the graph
induced by the kernel    which governs the number of moments that need to be considered and  thus  the sample complexity  Moreover  we use   re ned version of Horton   al 

Moments  Cycles and Learning DPPs

gorithm  Horton    Amaldi et al    to implement
the method of moments in polynomial time 
The cycle sparsity of   graph is the smallest integer  cid  so
that the cycles of length at most  cid  yield   basis for the cycle space of the graph  Even though there are in general
exponentially many cycles in   graph to consider  Horton  
algorithm constructs   minimum weight cycle basis and 
in doing so  also reveals the parameter  cid  together with  
collection of at most  cid  induced cycles spanning the cycle
space 
We use such cycles in order to construct our method of moments estimator  For any  xed  cid      our overall algorithm
has sample complexity

 cid cid   

 cid cid 

 

     

 cid 

 

log  
 

for some constant       and runs in time polynomial in
  and    and learns the parameters up to an additive  
with high probability  The    cid  term corresponds to
the number of samples needed to recover the signs of the
entries in    We complement this result with   minimax
lower bound  Theorem   to show that this sample complexity is in fact near optimal  In particular  we show that
there is an in nite family of graphs with cycle sparsity  cid 
 namely length  cid  cycles  on which any algorithm requires
at least    cid cid  samples to recover the signs of the entries
of   for some constant   cid      Finally  we show experimental results that con rm many quantitative aspects of our
theoretical predictions  Together  our upper bounds  lower
bounds  and experiments present   nuanced understanding
of which DPPs can be learned provably and ef ciently 

  Estimation of the Kernel
  Model and de nitions
Let            Yn be   independent copies of     DPP   
for some unknown kernel   such that    cid     cid  IN  
It is well known that   is identi ed by DPP    only
up to  ips of the signs of its rows and columns  If   cid 
is another symmetric matrix with    cid    cid   cid  IN   then
DPP   cid DPP    if and only if   cid    DKD for some
    DN   where DN denotes the class of all       diagonal matrices with only   and   on their diagonals
 Kulesza    Theorem   We call such   transform
  DN  similarity of   
In view of this equivalence class  we de ne the following
pseudodistance between kernels   and   cid 

      cid    inf
  DN

 DKD     cid   

where for any matrix          maxi        Ki    denotes the entrywise supnorm 

 cid 

For any          we write      det KS  where KS
denotes the           submatrix of   obtained by keeping
rows and colums with indices in    Note that for        cid 
       we have the following relations 

Ki              

                     

Ki iKj           Therefore  the princiand  Ki     
pal minors of size one and two of   determine   up to the
sign of its offdiagonal entries  In fact  for any    there
exists an  cid  depending only on the graph GK induced by   
such that   can be recovered up to   DN  similarity with
only the knowledge of its principal minors of size at most
 cid  We will show that this  cid  is exactly the cycle sparsity 

  DPPs and graphs

In this section  we review some of the interplay between
graphs and DPPs that plays   key role in the de nition of
our estimator 
We begin by recalling some standard graph theoretic notions  Let                        cycle   of   is
any connected subgraph in which each vertex has even degree  Each cycle   is associated with an incidence vector     GF    such that xe     if   is an edge in
  and xe     otherwise  The cycle space   of   is
the subspace of GF    spanned by the incidence vectors of the cycles in    The dimension    of the cycle
space is called cyclomatic number  and it is well known
that                  where     denotes the number of connected components of   
Recall that   simple cycle is   graph where every vertex
has either degree two or zero and the set of vertices with
degree two form   connected set    cycle basis is   basis
of     GF    such that every element is   simple cycle 
It is well known that every cycle space has   cycle basis of
induced cycles 
De nition   The cycle sparsity of   graph   is the minimal  cid  for which   admits   cycle basis of induced cycles of
length at most  cid  with the convention that  cid      whenever
the cycle space is empty    corresponding cycle basis is
called   shortest maximal cycle basis 

  shortest maximal cycle basis of the cycle space was also
studied for other reasons by  Chickering et al    We
defer   discussion of computing such   basis to Section  
For any subset          denote by GK              
the subgraph of GK induced by      matching of GK   
is   subset          such that any two distinct edges in
  are not adjacent in      The set of vertices incident
to some edge in   is denoted by        We denote by
     the collection of all matchings of GK    Then 
if GK    is an induced cycle  we can write the principal

Moments  Cycles and Learning DPPs

minor      det KS  as follows 

    

 cid 
     cid 
           cid 

      

      

 cid 

  cid       

Ki  

   
   

Ki   

 

         

Others have considered the relationship between the principal minors of   and recovery of DPP    There has been
work regarding the symmetric principal minor assignment
problem  namely the problem of computing   matrix given
an oracle that gives any principal minor in constant time
 Rising et al   
In our setting  we can approximate the principal minors of
  by empirical averages  However the accuracy of our
estimator deteriorates with the size of the principal minor 
and we must therefore estimate the smallest possible principal minors in order to achieve optimal sample complexity 
Here  we prove   new result  namely  that the smallest  cid 
such that all the principal minors of   are uniquely determined by those of size at most  cid  is exactly the cycle sparsity
of the graph induced by   
Proposition   Let     RN   be   symmetric matrix 
GK be the graph induced by    and  cid      be some integer 
The kernel   is completely determined up to DN  similarity
by its principal minors of size at most  cid  if and only if the
cycle sparsity of GK is at most  cid 

Proof  Note  rst that all the principal minors of   completely determine   up to   DN  similarity  Rising et al 
  Theorem   Moreover  recall that principal minors of degree at most   determine the diagonal entries of
  as well as the magnitude of its offdiagonal entries  In
particular  given these principal minors  one only needs to
recover the signs of the offdiagonal entries of    Let the
sign of cycle   in   be the product of the signs of the
entries of   corresponding to the edges of   
Suppose GK has cycle sparsity  cid  and let                be  
cycle basis of GK where each Ci        is an induced cycle of length at most  cid  By   the sign of any Ci        is
completely determined by the principal minor     where
  is the set of vertices of Ci and is such that        cid 
Moreover  for       let xi   GF    denote the incidence vector of Ci  By de nition  the incidence vector   of
    xi for some subset      
The sign of   is then given by the product of the signs of
Ci        and thus by corresponding principal minors  In
particular  the signs of all cycles are determined by the principal minors    with        cid  In turn  by Theorem  
in  Rising et al    the signs of all cycles completely
determine    up to   DN  similarity 
Next  suppose the cycle sparsity of GK is at least  cid     

any cycle   is given by cid 

and let   cid  be the subspace of GF    spanned by the induced cycles of length at most  cid  in GK  Let               be
  basis of   cid  made of the incidence column vectors of induced cycles of length at most  cid  in GK and form the matrix
    GF     by concatenating the xi    Since   cid  does
not span the cycle space of GK       GK      Hence 
the rank of   is less than    so the null space of   cid  is
non trivial  Let    be the incidence column vector of an induced cycle    that is not in   cid  and let     GL   with
  cid          cid    and    cid       These three conditions are
compatible because        cid  We are now in   position to
de ne an alternate kernel   cid  as follows  Let   cid 
      Ki  
and    cid 
        Ki    for all             We de ne the signs
of the offdiagonal entries of   cid  as follows  For all edges
              cid     sgn   cid 
     sgn Ke  if he     and
       sgn Ke  otherwise  We now check that  
sgn   cid 
and   cid  have the same principal minors of size at most  cid 
but differ on   principal minor of size larger than  cid  To that
end  let   be the incidence vector of   cycle   in   cid  so that
    Aw for some     GL  Thus the sign of   in  

is given by cid 

    xe 

Ke      cid    cid 
     cid   cid    cid 

    xe 

  cid 

 

 cid 

  cid 

 

  cid 

   

    xe 

    xe 

because   cid       Therefore  the sign of any       cid  is
the same in   and   cid  Now  let          with        cid  and
let     GKS   GK cid 
be the graph corresponding to KS
 or  equivalently  to   cid 
   For any induced cycle   in   
  is also an induced cycle in GK and its length is at most
 cid  Hence        cid  and the sign of   is the same in   and
  cid  By  Rising et al    Theorem   det KS   
det   cid 
   Next observe that the sign of    in   is given by

 

 cid 

Ke      cid    cid 

       cid 

  cid 

  cid 
  

     xe 

     xe 

    xe 

Note also that since    is an induced cycle of GK   GK cid 
the above quantity is nonzero  Let    be the set of vertices
in     By   and the above display  we have det        cid 
det   cid 
    Together with  Rising et al    Theorem
  it yields    cid  DK cid   for all     DN  

  De nition of the Estimator

Our procedure is based on the previous result and can be
summarized as follows  We  rst estimate the diagonal entries       the principal minors of size one  of   by the
method of moments  By the same method  we estimate the
principal minors of size two of    and we deduce estimates
of the magnitude of the offdiagonal entries  To use these
estimates to deduce an estimate    of GK  we make the
following assumption on the kernel   

Moments  Cycles and Learning DPPs

Assumption   Fix         For all               
either Ki       or  Ki       
Finally  we  nd   shortest maximal cycle basis of     and
we set the signs of our nonzero offdiagonal entry estimates by using estimators of the principal minors induced
by the elements of the basis  again obtained by the method
of moments 
For          set     

   Yp  and de ne

  cid 

 
 

  

 Ki        

and

 Bi      Ki    Kj          

    

where  Ki   and  Bi   are our estimators of Ki   and    
respectively 
De ne               where  for    cid                 if and
only if  Bi      
    The graph    is our estimator of GK 
Let               
  be   shortest maximal cycle basis of the
cycle space of     Let  Si        be the subset of vertices of
 Ci  for               We de ne
 Hi      Si

 cid 

 Ki   

 Bi  

      Si 

   cid 
     cid 
Hi          Si   cid 

      

for               In light of   for large enough    this
quantity should be close to

  cid       

Ki    

         Si 

We note that this de nition is only symbolic in nature  and
computing  Hi in this fashion is extremely inef cient  Instead  to compute it in practice  we will use the determinant
of an auxiliary matrix  computed via   matrix factorization 

Namely  let us de ne the matrix  cid     RN   such that
 cid Ki      Ki   for            and  cid Ki        
     cid 
 cid 
 cid 
det  cid    Si
         Si   cid 
  det cid    Si

so that we may equivalently write
 Hi      Si

           Si   cid 

  cid       
   
     

      We have

           Si 

      

   
     

   

 Ki  

 Bi  

 

           Si 

    GF      with bi    

Finally  let            Set the matrix     GF         
with ith row representing  Ci in GF                  
   sgn   Hi     
                     
              and let     GF    be   solution to the linear
system Ax     if   solution exists         otherwise 
We de ne  Ki       if             and  Ki      Kj    
               

for all             

   

  Geometry

The main result of this subsection is the following lemma
which relates the quality of estimation of   in terms of  
to the quality of estimation of the principal minors    
Lemma   Let   satisfy Assumption   and let  cid  be the
cycle sparsity of GK  Let       If                for all
         with         and if                  for all
         with            cid  then

              

Proof  We can bound    Bi        
 Bi      Ki      Kj                  

     namely 

     

       

and
 Bi      Ki      Kj                  

     

       

 cid cid cid 

giving    Bi        
         Thus  we can correctly determine whether Ki       or  Ki        yielding      GK 
In particular  the cycle basis                   
of    is   cycle
basis of GK  Let               Denote by      Si 
We have
 cid 

 cid cid cid   Hi   Hi
     Si         Si cid 
 cid   Si 

         Si  max
  

 cid 
 cid cid 

     tx   Si     

         Si     

       Si

     Si

 cid 

 cid 

     Si     

        Si   Si 

   Si 

 

   Si 
     Si        
       Si     
     Si        Si 
     Si     Hi 

 

  

 cid  

where 

for positive integers        we denote by
Therefore  we can deter 

 cid 
            cid  
mine the sign of the product  cid         Si  Ki   for ev 
 cid cid cid   Ki       Ki   cid cid cid 
 cid cid cid   Ki       Ki   cid cid cid       Ki     Ki        For  
 cid cid cid   Bi        
 cid cid cid      yielding
 cid cid     
       Ki   cid cid     

in the cycle basis and recover the signs
ery element
of the nonzero offdiagonal entries of Ki    Hence 
           max      
For       
 cid    with
        one can easily show that

       Ki     

        

     

 

 

   

 

which completes the proof 

Moments  Cycles and Learning DPPs

We are now in   position to establish   suf cient sample
size to estimate   within distance  
Theorem   Let   satisfy Assumption   and let  cid  be the
cycle sparsity of GK  Let       For any       there
exists       such that

 cid   
     cid cid   

 

 cid cid cid 

     

log    

yields              with probability at least         

  cid 

Proof  Using the previous lemma  and applying   union
bound 

  cid              
 cid     cid 
 cid 
  cid                cid 
 cid 

            

   

 

   cid 

                 cid     cid 
 
 

where we used Hoeffding   inequality 

  Information theoretic lower bound
We prove an informationtheoretic lower bound that holds
already if GK is an  cid cycle  Let     cid   cid  and        cid 
denote respectively the KullbackLeibler divergence and
the Hellinger distance between DPP    and DPP   cid 
Lemma   For         let     be the  cid cid  matrix with
elements given by

where     denotes the cardinality of    The inclusionexclusion principle also yields that          det      
      for all         where      stands for the  cid     cid  diagonal matrix with ones on its entries        for        zeros
elsewhere 
Denote by      cid    the Kullback Leibler divergence
between DPP     and DPP   

     cid     

     log

 cid      

 cid 

    

  cid 

 cid 
   cid 
   cid   cid 

  cid 

  cid 

    
    

            

  det            
  det             

 

by   Using the fact that           and the Gershgorin
circle theorem  we conclude that the absolute value of all
eigenvalues of            are between   and   Thus we
obtain from   the bound      cid       cid 
Using the same arguments as above  the Hellinger distance
         between DPP     and DPP    satis es

          

 cid 

 cid 

 cid 
   cid 

  cid 

  cid 

           

 cid       cid     

 cid 
     cid     cid 

which completes the proof 

 

 cid 

 

 cid   cid 

Ki    

 
 
 
 

if      
if          
if             cid   cid   
otherwise

 

The sample complexity lower bound now follows from
standard arguments 
Theorem   Let               and      cid       There
exists   constant       such that if

Then  for any       it holds
    cid   cid     cid 
and

       cid     cid   

     

 cid   

log   cid 
 cid   

log  

 

Proof  It is straightforward to see that

det    

      det   

     

 cid 

 cid 
 

then the following holds  for any estimator    based on  
samples  there exists   kernel   that satis es Assumption  
and such that the cycle sparsity of GK is  cid  and for which
             with probability at least  

if      cid 
else

 

If   is sampled from DPP     we denote by       
         for      cid 
It follows from the inclusionexclusion principle that for all      cid 
             

   det    

      det   

 cid 

     

  cid  

   cid   det       det       cid   
 

Proof  Recall the notation of Lemma   First consider the
      block diagonal matrix    resp    cid  where its  rst
block is      resp     and its second block is IN cid  By  
standard argument  the Hellinger distance Hn      cid  between the product measures DPP     and DPP   cid  
satis es
      

 cid           cid 

 cid    cid     cid 

       cid 

 cid  

 

 

 

     cid 

Moments  Cycles and Learning DPPs

which yields the  rst term in the desired lower bound 
Next  by padding with zeros  we can assume that       cid 
is an integer  Let     be   block diagonal matrix where
each block is      using the notation of Lemma   For
                 de ne the       block diagonal matrix
      as the matrix obtained from     by replacing its jth
block with     again using the notation of Lemma  
Since DPP       is the product measure of   lower dimensional DPPs that are each independent of each other 
using Lemma   we have        cid        cid  Hence 
by Fano   lemma  see       Corollary   in  Tsybakov 
  the sample complexity to learn the kernel of   DPP
within   distance       is

 cid  log   cid 

 cid 

 

 cid 

which yields the second term 
The third term follows from considering       IN
and letting Kj be obtained from    by adding   to
the jth entry along the diagonal 
It is easy to see that
  Kj cid        Hence    second application of Fano  
lemma yields that the sample complexity to learn the kernel
of   DPP within   distance   is   log  

   

The third term in the lower bound is the standard parametric
term and is unavoidable in order to estimate the magnitude
of the coef cients of    The other terms are more interesting  They reveal that the cycle sparsity of GK  namely   cid 
plays   key role in the task of recovering the sign pattern of
   Moreover the theorem shows that the sample complexity of our method of moments estimator is near optimal 

  Algorithms
  Horton   algorithm
We  rst give an algorithm to compute the estimator   
de ned in Section     wellknown algorithm of Horton  Horton    computes   cycle basis of minimum
total length in time         Subsequently  the running
time was improved to        log     time  Amaldi et al 
  Also  it is known that   cycle basis of minimum total
length is   shortest maximal cycle basis  Chickering et al 
  Together  these results imply the following 
Lemma   Let                      There is an
algorithm to compute   shortest maximal cycle basis in
       log     time 

In addition  we recall the following standard result regarding the complexity of Gaussian elimination  Golub  
Van Loan   

Algorithm   Compute Estimator   

Input  samples      Yn  parameter      
Compute    for all        
Set  Ki         for           
Form  cid     RN   and              
Compute  Bi   for               
Compute   shortest maximal cycle basis             
Compute    Si
for              
Compute     Si
Construct     GF             GF      
Solve Ax     using Gaussian elimination 
Set  Ki      Kj                  

using det  cid    Si
for              

      for all             

 

Lemma   Let     GF         GF   Then Gaussian elimination will  nd   vector     GF    such that
Ax     or conclude that none exists in      time 

We give our procedure for computing the estimator    in
Algorithm   In the following theorem  we bound the running time of Algorithm   and establish an upper bound on
the sample complexity needed to solve the recovery problem as well as the sample complexity needed to compute
an estimate    that is close to   
Theorem   Let     RN   be   symmetric matrix satisfying    cid     cid     and satisfying Assumption   Let GK be
the graph induced by   and  cid  be the cycle sparsity of GK 
Let      Yn be samples from DPP    and         If

   

log    cid 

 cid 

 

then with probability at least       Algorithm   computes
an estimator    which recovers the signs of   up to   DN  
similarity and satis es

 cid    log    cid 

 cid 

 

 

         

 
 

in        nN   time 

Proof    follows directly from   in the proof of Theorem   That same proof also shows that with probability
at least       the support of GK and the signs of   are
recovered up to   DN  similarity  What remains is to upper bound the worst case run time of Algorithm   We
will perform this analysis line by line  Initializing    requires       operations  Computing    for all subsets
        requires   nN   operations  Setting  Ki   requires
      operations  Computing  Bi   for              

requires       operations  Forming  cid   requires      

operations  Forming GK requires       operations  By

Moments  Cycles and Learning DPPs

Lemma   computing   shortest maximal cycle basis requires   mN   operations  Constructing the subsets Si 
              requires   mN   operations  Computing  Si
 CSi using det cid   Si  for              requires     cid 
for              requires   nm  operations  Computing
operations  where   factorization of each  cid   Si  is used to

compute each determinant in   cid  operations  Constructing   and   requires     cid  operations  By Lemma  
 nding   solution   using Gaussian elimination requires
     operations  Setting  Ki   for all edges           
requires      operations  Put this all together  Algorithm
  runs in        nN   time 

Algorithm   Compute Signs of Edges in Chordal Graph

Input  GK           chordal     for        
Compute   PEO       vN 
Compute the spanning forest   cid           cid 
Set all edges in   cid  to have positive sign 
Compute           for all                cid        
Order edges       cid             such that       if
max ei   max ej 
Visit edges in sorted order and for                   set
sgn         sgn            sgn       sgn      

  Chordal Graphs

Here we show that it is possible to obtain faster algorithms
by exploiting the structure of GK  Speci cally  in the case
where GK chordal  we give an      time algorithm to
determine the signs of the offdiagonal entries of the estimator     resulting in an improved overall runtime of
      nN   Recall that   graph             is said
to be chordal if every induced cycle in   is of length three 
Moreover    graph             has   perfect elimination ordering  PEO  if there exists an ordering of the vertex set       vN  such that  for all    the graph induced
by  vi     vj                   is   clique  It is well
known that   graph is chordal if and only if it has   PEO   
PEO of   chordal graph with   edges can be computed in
     operations using lexicographic breadth rst search
 Rose et al   
Lemma   Let             be   chordal graph and
      vn  be   PEO  Given    let      min      
  vi  vj       Then the graph   cid           cid  where
  cid     vi  vi     
Proof  We  rst show that there are no cycles in   cid  Suppose to the contrary  that there is an induced cycle   of
length   on the vertices  vj    vjk  Let   be the vertex
of smallest index  Then   is connected to two other vertices
in the cycle of larger index  This is   contradiction to the
construction 
What remains is to show that    cid            It suf ces
to prove the case         Suppose to the contrary  that
there exists   vertex vi         with no neighbors of larger
index  Let   be the shortest path in   from vi to vN   By
connectivity  such   path exists  Let vk be the vertex of
smallest index in the path  However  it has two neighbors
in the path of larger index  which must be adjacent to each
other  Therefore  there is   shorter path 

  is   spanning forest of   

  

Now  given the chordal graph GK induced by   and the
estimates of principal minors of size at most three  we provide an algorithm to determine the signs of the edges of

GK  or  equivalently  the offdiagonal entries of   
Theorem   If GK is chordal  Algorithm   correctly determines the signs of the edges of GK in      time 

Proof  We will simultaneously perform   count of the operations and   proof of the correctness of the algorithm 
Computing   PEO requires      operations  Computing
the spanning forest requires      operations  The edges
of the spanning tree can be given arbitrary sign  because it
is   cyclefree graph  This assigns   sign to two edges of
each  cycle  Computing each           requires   constant
number of operations because  cid      requiring   total of
          operations  Ordering the edges requires     
operations  Setting the signs of each remaining edge requires      operations 

Therefore  when GK is chordal  the overall complexity
required by our algorithm to compute    is reduced to
      nN  

  Experiments
Here we present experiments to supplement the theoretical
results of the paper  We test our algorithm on two types
of random matrices  First  we consider the matrix    
RN   corresponding to the cycle on   vertices 

   

 
 

   

 
 

  

where   is symmetric and has nonzero entries only on the
edges of the cycle  either   or   each with probability
  By the Gershgorin circle theorem     cid     cid     Next 
we consider the matrix     RN   corresponding to the
clique on   vertices 

   

 
 

   

 
 
 
 

  

Moments  Cycles and Learning DPPs

where   is symmetric and has all entries either   or  
 
each with probability   It is well known that  
   cid 
 
  with high probability  implying    cid     cid    
   cid   
For both cases and for   range of values of matrix dimension   and samples    we run our algorithm on   randomly generated instances  We record the proportion of
trials where we recover the graph induced by    and the
proportion of the trials where we recover both the graph
and correctly determine the signs of the entries 
In Figure   the shade of each box represents the proportion of trials where recovery was successful for   given pair
        completely white box corresponds to zero success
rate  black to   perfect success rate 
The plots corresponding to the cycle and the clique are
telling  We note that for the clique  recovering the sparsity pattern and recovering the signs of the offdiagonal entries come handin hand  However  for the cycle  there is
  noticeable gap between the number of samples required
to recover the sparsity pattern and the number of samples
required to recover the signs of the offdiagonal entries 
This empirically con rms the central role that cycle sparsity plays in parameter estimation  and further corroborates
our theoretical results 

  Conclusion and open questions
In this paper  we gave the  rst provable guarantees for
learning the parameters of   DPP  Our upper and lower
bounds reveal the key role played by the parameter  cid  which
is the cycle sparsity of graph induced by the kernel of the
DPP  Our estimator does not need to know  cid  beforehand 
but can adapt to the instance  Moreover  our procedure
outputs an estimate of  cid  which could potentially be used
for further inference questions such as testing and con 
dence intervals  An interesting open question is whether
on   graph by graph basis  the parameter  cid  exactly determines the optimal sample complexity  Moreover when the
number of samples is too small  can we exactly characterize
which signs can be learned correctly and which cannot  up
to   similarity transformation by    Such results would
lend new theoretical insights into the output of algorithms
for learning DPPs  and which individual parameters in the
estimate we can be con dent about and which we cannot 

    

Acknowledgements 
is supported in part by
NSF CAREER Award CCF  NSF Large CCF 
    David and Lucile Packard Fellowship and an
Alfred    Sloan Fellowship       is supported in part
by NSF CAREER DMS  NSF DMS 
DARPA   NF  ONR   
and   grant from the MIT NEC Corporation 

    graph recovery  cycle

    graph and sign recovery  cycle

    graph recovery  clique

    graph and sign recovery  clique

Figure   Plots of the proportion of successive graph recovery  and graph and sign recovery  for random matrices with
cycle and clique graph structure  respectively  The darker
the box  the higher the proportion of trials that were recovered successfully 

Moments  Cycles and Learning DPPs

References
Affandi  Raja Ha    Fox  Emily    Adams  Ryan    and
Taskar  Benjamin  Learning the parameters of determinantal point process kernels  In Proceedings of the  th
International Conference on Machine Learning  ICML
  Beijing  China    June   pp   
 

Amaldi  Edoardo  Iuliano  Claudio  and Rizzi  Romeo  Ef 
 cient deterministic algorithms for  nding   minimum
cycle basis in undirected graphs  In International Conference on Integer Programming and Combinatorial Optimization  pp    Springer   

Batmanghelich  Nematollah Kayhan  Quon  Gerald 
Kulesza  Alex  Kellis  Manolis  Golland  Polina  and
Bornn  Luke  Diversifying sparsity using variational determinantal point processes  ArXiv     

Borodin  Alexei and Rains  Eric    Eynard mehta theorem  schur process  and their pfaf an analogs  Journal
of statistical physics     

Brunel  VictorEmmanuel  Moitra  Ankur  Rigollet 
Philippe 
Maximum likelihood estimation of determinantal point processes 
arXiv   

and Urschel 

John 

Chickering  David    Geiger  Dan  and Heckerman 
David  On  nding   cycle basis with   shortest maxiInformation Processing Letters     
mal cycle 
   

   ivril  Ali and MagdonIsmail  Malik  On selecting   maximum volume submatrix of   matrix and related problems  Theoretical Computer Science   
   

Deshpande  Amit and Rademacher  Luis  Ef cient volume
sampling for row column subset selection  In Foundations of Computer Science  FOCS     st Annual
IEEE Symposium on  pp    IEEE   

Dyson  Freeman    Statistical theory of the energy levels
of complex systems  III     Mathematical Phys   
    ISSN  

Gillenwater  Jennifer    Kulesza  Alex  Fox  Emily  and
Taskar  Ben  Expectationmaximization for learning determinantal point processes  In NIPS   

Golub  Gene   and Van Loan  Charles    Matrix computa 

tions  volume   JHU Press   

Horton  Joseph Douglas    polynomialtime algorithm to
 nd the shortest cycle basis of   graph  SIAM Journal on
Computing     

Kulesza     Learning with determinantal point processes 

PhD thesis  University of Pennsylvania   

Kulesza  Alex and Taskar  Ben  kDPPs  Fixedsize determinantal point processes  In Proceedings of the  th
International Conference on Machine Learning  ICML
  Bellevue  Washington  USA  June     July  
  pp     

Kulesza  Alex and Taskar  Ben  Determinantal Point
Now Publishers
ISBN  

Processes for Machine Learning 
Inc  Hanover  MA  USA   
 

Lee  Donghoon  Cha  Geonho  Yang  MingHsuan  and Oh 
Songhwai  Individualness and determinantal point proIn Computer Vision  
cesses for pedestrian detection 
ECCV      th European Conference  Amsterdam 
The Netherlands  October     Proceedings 
Part VI  pp     

Li  Chengtao  Jegelka  Stefanie  and Sra  Suvrit  Fast sampling for strongly rayleigh measures with application to
determinantal point processes       

Li  Chengtao  Jegelka  Stefanie  and Sra  Suvrit  Fast dpp
sampling for nystrom with application to kernel methInternational Conference on Machine Learning
ods 
 ICML     

Lin  Hui and Bilmes  Jeff    Learning mixtures of submodular shells with application to document summarization 
In Proceedings of the TwentyEighth Conference on Uncertainty in Arti cial Intelligence  Catalina Island  CA 
USA  August     pp     

Macchi  Odile  The coincidence approach to stochastic
point processes  Advances in Appl  Probability   
    ISSN  

Mariet  Zelda and Sra  Suvrit  Fixedpoint algorithms for
learning determinantal point processes  In Proceedings
of the  nd International Conference on Machine Learning  ICML  pp     

Nikolov  Aleksandar  Randomized rounding for the largest
In Proceedings of the FortySeventh
simplex problem 
Annual ACM on Symposium on Theory of Computing 
pp    ACM   

Nikolov  Aleksandar and Singh  Mohit  Maximizing determinants under partition constraints  In STOC  pp   
   

Rebeschini  Patrick and Karbasi  Amin  Fast mixing for
discrete point processes  In COLT  pp     

Moments  Cycles and Learning DPPs

Rising  Justin  Kulesza  Alex  and Taskar  Ben  An ef cient
algorithm for the symmetric principal minor assignment
problem  Linear Algebra and its Applications   
     

Rose  Donald    Tarjan    Endre  and Lueker  George    Algorithmic aspects of vertex elimination on graphs  SIAM
Journal on computing     

Snoek 

Jasper  Zemel  Richard   

and Adams 
Ryan Prescott 
  determinantal point process latent variable model for inhibition in neural spiking data 
In Advances in Neural Information Processing Systems
   th Annual Conference on Neural Information
Processing Systems   Proceedings of   meeting
held December     Lake Tahoe  Nevada  United
States  pp     

Summa  Marco Di  Eisenbrand  Friedrich  Faenza  Yuri 
and Moldenhauer  Carsten  On largest volume simplices
In Proceedings of the Twentyand subdeterminants 
Sixth Annual ACMSIAM Symposium on Discrete Algorithms  pp    Society for Industrial and Applied
Mathematics   

Tsybakov  Alexandre    Introduction to nonparametric estimation  Springer Series in Statistics  Springer  New
York   

Xu  Haotian and Ou  Haotian  Scalable discovery of audio  ngerprint motifs in broadcast streams with determinantal point process based motif clustering  IEEE ACM
Trans  Audio  Speech   Language Processing   
   

Yao  Jinge  Fan  Feifan  Zhao  Wayne Xin  Wan  Xiaojun 
Chang  Edward    and Xiao  Jianguo  Tweet timeline
generation with determinantal point processes  In Proceedings of the Thirtieth AAAI Conference on Arti cial
Intelligence  February     Phoenix  Arizona 
USA  pp     

