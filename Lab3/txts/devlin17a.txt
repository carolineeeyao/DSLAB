RobustFill  Neural Program Learning under Noisy    

Jacob Devlin     Jonathan Uesato     Surya Bhupatiraju     Rishabh Singh   Abdelrahman Mohamed  

Pushmeet Kohli  

Abstract

The problem of automatically generating   computer program from some speci cation has been
studied since the early days of AI  Recently  two
competing approaches for automatic program
learning have received signi cant attention   
neural program synthesis  where   neural network is conditioned on input output       examples and learns to generate   program  and  
neural program induction  where   neural network generates new outputs directly using   latent program representation  Here  for the  rst
time  we directly compare both approaches on  
largescale  realworld learning task and we additionally contrast to rulebased program synthesis  which uses handcrafted semantics to guide
the program generation  Our neural models use
  modi ed attention RNN to allow encoding of
variablesized sets of     pairs  which achieve
  accuracy on   realworld test set  compared
to the   accuracy of the previous best neural
synthesis approach  The synthesis model also
outperforms   comparable induction model on
this task  but we more importantly demonstrate
that the strength of each approach is highly dependent on the evaluation metric and enduser
application  Finally  we show that we can train
our neural models to remain very robust to the
type of noise expected in realworld data      
typos  while   highlyengineered rulebased system fails entirely 

  Introduction
The problem of program learning       generating   program consistent with some speci cation  is one of the oldest problems in machine learning and arti cial intelligence

 Equal contribution  Microsoft Research  Redmond  Washington  USA  MIT  Cambridge  Massachusetts  USA  Correspondence to  Jacob Devlin  jdevlin microsoft com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Input String

Output String

john Smith
DOUG    Macklin
Frank Lee  
Laura Jane Jones
Steve    Green  

Smith  Jhn
Macklin  Doug
LEe  Frank
Jones  Laura
 

Program

GetToken Alpha         
ToCase Proper  GetToken Alpha   

     

Figure   An anonymized example from FlashFillTest with noise
 typos  The goal of the task is to  ll in the blank          
 Green  Steve  Synthesis approaches achieve this by generating
  program like the one shown  Induction approaches generate the
new output string directly  conditioned on the the other examples 

Waldinger   Lee   Manna   Waldinger   The
classical approach has been that of rulebased program
synthesis  Manna   Waldinger    where   formal
grammar is used to derive   program from   wellde ned
speci cation  Providing   formal speci cation is often
more dif cult than writing the program itself  so modern
program synthesis methods generally rely on input output
examples      examples  to act as an approximate speci 
cation  Modern rulebased synthesis methods are typically
centered around handcrafted function semantics and pruning rules to search for programs consistent with the     examples  Gulwani et al    Alur et al   
These handengineered systems are often dif cult to extend
and fragile to noise  so statistical program learning methods have recently gained popularity  with   particular focus on neural network models  This work has fallen into
two overarching categories    neural program synthesis 
where the program is generated by   neural network conditioned on the     examples  Balog et al    Parisotto
et al    Gaunt et al    Riedel et al    and  
neural program induction  where network learns to generate the output directly using   latent program representation
 Graves et al      Kurach et al    Kaiser  
Sutskever    Joulin   Mikolov    Reed   de Freitas    Neelakantan et al    Although many of
these papers have achieved impressive results on   variety of tasks  none have thoroughly compared induction and
synthesis approaches on   realworld test set  In this work 
we not only demonstrate strong empirical results compared

RobustFill  Neural Program Learning under Noisy    

to past work  we also directly contrast the strengths and
weaknesses of both neural program learning approaches
for the  rst time 
The primary task evaluated for this work is   Programming By Example  PBE  system for string transformations
similar to FlashFill  Gulwani et al    Gulwani   
FlashFill allows Microsoft Excel endusers to perform regular expressionbased string transformations using examples without having to write complex macros  For example 
  user may want to extract zip codes from   text  eld containing addresses  or transform   timestamp to   different
format  An example is shown in Figure     user manually provides   small number of example output strings
to convey the desired intent and the goal of FlashFill is to
generalize the examples to automatically generate the corresponding outputs for the remaining input strings  Since
the end goal is to emit the correct output strings  and not  
program  the task itself is agnostic to whether   synthesis
or induction approach is taken 
For modeling  we develop novel variants of the attentional RNN architecture  Bahdanau et al    to encode   variablelength unordered set of inputoutput examples  For program representation  we have developed
  domainspeci   language  DSL  similar to that of Gulwani et al    that de nes an expressive class of regular expressionbased string transformations  The neural
network is then used to generate   program in the DSL  for
synthesis  or an output string  for induction  Both systems
are trained endto end using   large set of inputoutput examples and programs uniformly sampled from the DSL 
We compare our neural induction model  neural synthesis
model  and the rulebased architecture of Gulwani et al 
  on   realworld FlashFill test set  We also inject
varying amounts of noise       simulated typos  into the
FlashFill test examples to model the robustness of different
learning approaches  While the manual approaches work
reasonably well for wellformed     examples  we show
that its performance degrades dramatically in presence of
even small amounts of noise  We show that our neural
architectures are signi cantly more robust in presence of
noise and moreover obtain an accuracy comparable to manual approaches even for nonnoisy examples  thereby motivating the name RobustFill 
This paper makes the following key contributions 

  We present   novel variant of the attentional RNN architecture  which allows for encoding of   variablesize set of inputoutput examples 
  We evaluate the architecture on   realworld FlashFill instances and signi cantly outperform the previous best statistical system   vs    accuracy 
  We compare the model to   handcrafted synthesis

algorithm and show that while both systems achieve
similar performance on clean test data  our model is
signi cantly more robust to realistic noise  with noise 
  accuracy vs    accuracy 
  We compare our neural synthesis architecture with
  neural induction architecture  and demonstrate that
each approach has its own strengths under different
evaluation metrics and decoding constraints 

  Related Work
There has been an abundance of recent work on neural program induction and synthesis 
Neural Program Induction 
Neural Turing Machine
 NTM   Graves et al    uses   neural controller to read
and write to an external memory tape using soft attention
and is able to learn simple algorithmic tasks such as array copying and sorting  StackRNNs  Joulin   Mikolov 
  augment   neural controller with an external stackstructured memory and is able to learn algorithmic patterns of small description length  Neural GPU  Kaiser  
Sutskever    presents   Turingcomplete model similar to NTM  but with   parallel and shallow design similar
to that of GPUs  and is able to learn complex algorithms
such as long binary multiplication  Neural ProgrammerInterpreters  Reed   de Freitas    teach   controller
to learn algorithms from program traces as opposed to examples  Neural RandomAccess Machines  Kurach et al 
  uses   continuous representation of   highlevel
modules consisting of simple arithmetic functions and
reading writing to   variablesize randomaccess memory
to learn algorithmic tasks requiring pointer manipulation
and dereferencing to memory  The domain of string transformations is different than the domains handled by these
approaches and moreover  unlike RobustFill 
these approaches need to be retrained per problem instance 
Neural Program Synthesis 
The most closely related
work to ours uses   RecursiveReverse Recursive neural
network    NN  to learn string transformation programs
from examples  Parisotto et al    and is directly compared in Section   DeepCoder  Balog et al    trains
  neural network to predict   distribution over possible
functions useful for   given task from inputoutput examples  which is used to augment an external search algorithm  Unlike DeepCoder  RobustFill performs an endto 
end synthesis of programs from examples  Terpret  Gaunt
et al    and Neural Forth  Riedel et al    allow
programmers to write sketches of partial programs to express prior procedural knowledge  which are then completed by training neural networks on examples 
DSLbased synthesis  Nonstatistical DSLbased synthesis approaches  Gulwani et al    exploit indepen 

RobustFill  Neural Program Learning under Noisy    

dence properties of DSL operators to develop   divideand 
conquer based search algorithm with several handcrafted
pruning and ranking heuristics  Polozov   Gulwani   
In this work  we present   neural architecture to automatically learn an ef cient synthesis algorithm  There is also
some work on using learnt clues to guide the search in DSL
expansions  Menon et al    but this requires handcoded textual features of examples 

  Problem Overview
We now formally de ne the problem setting and the
domainspeci   language of string transformations 

  Problem Formulation

     

      Oy

inputoutput

Given   set of
string examples
          In  On  and   set of unpaired input strings
   
   the goal of of this task is to generate the cor 
         
responding output strings  Oy
   For each example set  we assume there exists at least one program  
that will correctly transform all of these examples      
                   
      Throughout this work 
we refer to  Ij  Oj  as observed examples and     
    Oy
   
as assessment examples  We use InStr and OutStr to
generically refer to     examples that may be observed or
assessed  We refer to this complete set of information as an
instance 

      Oy

     January
     jan
     February      feb
     mar
     March
Oy
   
    apr
    April
   
Oy
    May
    may
    ToCase Lower  SubStr 

Intuitively  imagine that    nonprogrammer  user has  
large list of InStr which they wish to process in some
way  The goal is to only require the user to manually create
  small number of corresponding OutStr  and the system
will generate the remaining OutStr automatically 
In the program synthesis approach  we train   neural model
which takes           In  On  as input and generates  
as output  tokenby token  It is trained fully supervised on
  large corpus of pairs of synthetic     examples and their
respective programs  It is not conditioned on the assessment input strings    
    but it could be in future work  At
test time  the model is provided with new set of observed
    examples and attempts to generate the corresponding
  which it  maybe  has never seen in training  Crucially 
the system can actually execute the generated   on each
observed input string Ij and check if it produces Oj  If
not  it knows that   cannot be the correct program  and it

 This execution is deterministic  not neural 

      Oy

can search for   different     Of course  even if   is consistent on all observed examples  there is no guarantee that
it will generalize to new examples       assessment examples  We can think of consistency as   necessary  but not
suf cient  condition  The actual success metric is whether
this program generalizes to the corresponding assessment
examples             
    There also may be multiple
valid programs 
In the program induction approach  we train   neural model
which takes           In  On  and     as input and generates Oy as output  characterby character  Our current
model decodes each assessment example independently 
Crucially  the induction model makes no explicit use of
program   at training or test time 
Instead  we say that
it induces   latent representation of the program 
If we
had   large corpus of realworld     examples  we could
in fact train an induction model without any explicit program representation  Since such   corpus is not available 
it is trained on the same synthesized     examples as the
synthesis model  Note that since the program representation is latent  there is no way to measure consistency 
We can comparably evaluate both approaches by measuring generalization accuracy  which is the percent of test instances for which the system has successfully produced the
correct OutStr for all assessment examples  For synthesis this means       
    For induction this
means all Oy generated by the system are exactly correct 
We typically use four observed examples and six assessment examples per test instance  All six must be exactly
correct for the model to get credit 

      

      Oy

    Oy

  The Domain Speci   Language

The Domain Speci   Language  DSL  used here to represent   models   rich set of string transformations based
on substring extractions  string conversions  and constant
strings  The DSL is similar to the DSL described in
Parisotto et al    but is extended to include nested expressions  arbitrary constant strings  and   powerful regexbased substring extraction function  The syntax of the DSL
is shown in Figure   and the formal semantics are presented
in the supplementary material 
  program     string   string in the DSL takes as
input   string and returns another string as output  The
toplevel operator in the DSL is the Concat operator
that concatenates    nite list of string expressions ei   
string expression   can either be   substring expression
     nesting expression    or   constant string expression    substring expression can either be de ned using two constant positions indices    and     where negative indices denote positions from the right  or using the
GetSpan    ii              construct that returns the
substring between the ith
  occurrence of regex    and the

RobustFill  Neural Program Learning under Noisy    

Program     Concat           

Expression  
Substring  

                         ConstStr   
  SubStr      
 

GetSpan                  
Nesting     GetToken         ToCase   

Regex  
Type  

Replace      Trim 
 
GetUpto      GetFrom   
 
 
GetFirst         GetAll   
           tn             
  Number   Word   Alphanum
 
 
  Proper   AllCaps   Lower
Position                
             
                         
         cid 
  Start   End

AllCaps   PropCase   Lower
Digit   Char

Index  
Character  
Delimiter  
Boundary  

Case  

Figure   Syntax of the string transformation DSL 

  occurrence of regex    where    and    denotes either
ith
the start or end of the corresponding regex matches  The
nesting expressions allow for further nested string transformations on top of the substring expressions allowing
to extract kth occurrence of certain regex  perform casing transformations  and replacing   delimiter with another
delimiter  The notation             is sometimes used as
  shorthand for Concat         The nesting and substring expressions take   string as input  implicitly as  
lambda parameter  We sometimes refer expressions such
as ToCase Lower    as ToCase Lower   
There are approximately   million unique string expressions    which can be concatenated to create arbitrarily
long programs  Any search method that does not encode
inverse function semantics  either by hand or with   statistical model  cannot prune partial expressions  Thus  even
ef cient techniques like dynamic programming  DP  with
blackbox expression evaluation would still have to search
over many millions of candidates 

  Training Data and Test Sets

Since there are only   few hundred realworld FlashFill instances  the data used to train the neural networks was generated automatically  To do this  we use   strategy of random sampling and generation  First  we randomly sample
programs from our DSL  up to   maximum length   expressions  Given   sampled program  we compute   simple set of heuristic requirements on the InStr such that
the program can be executed without throwing an exception  For example  if an expression in the program retrieves
the  th number  the InStr must have at least   numbers 
Then  each InStr is generated as   random sequence of

ASCII characters  constrained to satisfy the requirements 
The corresponding OutStr is generated by executing the
program on the InStr 
For evaluating the trained models  we use FlashFillTest   
set of   realworld examples collected from Microsoft
Excel spreadsheets  and provided to us by the authors of
Gulwani et al    and Parisotto et al    Each
FlashFillTest instance has ten     examples  of which the
 rst four are used as observed examples and the remaining
six are used as assessment examples  Some examples of
FlashFillTest instances are provided in the supplementary
material  Intuitively  it is possible to generalize to   realword test set using randomly synthesized training because
the model is learning function semantics  rather than   particular data distribution 

  Program Synthesis Model Architecture
We model program synthesis as   sequenceto sequence
generation task  along the lines of past work in machine
translation  Bahdanau et al    image captioning  Xu
et al    and program induction  Zaremba   Sutskever 
 
In the most general description  we encode the
observed     using   series of recurrent neural networks
 RNN  and generate   using another RNN one token at  
time  The key challenge here is that in typical sequenceto sequence modeling  the input to the model is   single
sequence  In this case  the input is   variablelength  unordered set of sequence pairs  where each pair       an
    example  has an internal conditional dependency  We
describe and evaluate several multiattentional variants of
the attentional RNN architecture  Bahdanau et al    to
model this scenario 

  SingleExample Representation

We  rst consider   model which only takes   single observed example        as input  and produces   program  
as output  Note that this model is not conditioned on the assessment input      In all models described here    is generated using   sequential RNN  rather than   hierarchical
RNN  Parisotto et al    Tai et al    As demonstrated in Vinyals et al    sequential RNNs can be
surprisingly strong at representing hierarchical structures 
We explore four increasingly complex model architectures 
shown visually in Figure  

  Basic Seqto Seq  Each sequence is encoded with  
nonattentional LSTM  and the  nal hidden state is

 In cases where less than   observed examples are used  only

the   assessment examples are used to measure generalization 

 Even though the DSL does allow limited hierarchy  preliminary experiments indicated that using   hierarchical representation did not add enough value to justify the computational cost 

RobustFill  Neural Program Learning under Noisy    

used as the initial hidden state of the next LSTM 

attending to   and   attending to   

  AttentionA    and   are attentional LSTMs  with  
  AttentionB  Same as AttentionA  but   uses   double attention architecture  attending to both   and  
simultaneously 
  AttentionC  Same as AttentionB  but   and   are

bidirectional LSTMs 

In all cases  the InStr and OutStr are processed at
the character level  so the input to   and   are character
embeddings  The vocabulary consists of all   printable
ASCII tokens 
The inputs and targets for the   layer is the sourcecode 
order linearization of the program  The vocabulary consists of   total program tokens  which includes all function names and parameter values  as well as special tokens
for concatenation and endof sequence  Note that numerical parameters are also represented with embedding tokens 
The model is trained to maximize the loglikelihood of the
reference program    

  Double Attention

Double attention is   straightforward extension to the standard attentional architecture  similar to the multimodal attention described in Huang et al      typical attentional layer takes the following form 

ci   Attention hi  xi    
hi   LST    hi  xi  ci 

Where   is the set of vectors being attended to  hi  is
the previous recurrent state  and xi is the current input 
The Attention  function takes the form of the  general 
model from Luong et al    Double attention takes the
form 

cA
    Attention hi  xi  SA 
    SB 
    Attention hi  xi  sA
cB
hi   LST    hi  xi  sA
    cB
   

Note that sA
is concatenated to hi  when computing ati
tention on SB  so there is   directed dependence between
the two attentions  Here  SA is   and SB is    Exact details
of the attentional formulas are given in the supplementary
material 

Figure   The network architectures used for program synthesis 
  dotted line from   to   means that   attends to   

stances  and the examples are unordered  which suggests
  poolingbased approach  Previous work  Parisotto et al 
  has pooled on the  nal encoder hidden states  but this
approach cannot be used for attentional models 
Instead  we take an approach which we refer to as late pooling  Here  each     example has its own layers for      
and    with shared weights across examples  but the hidden states of      Pn are pooled at each timestep before
being fed into   single output softmax layer  The architecture is shown at the bottom of Figure   We did not  nd it
bene cial to add another fullyconnected layer or recurrent
layer after pooling 
Formally  the layers labeled  FC  and  MaxPool  perform
the operation mi   MaxPoolj   tanh   hji  where  
is the current timestep    is the number of observed examples  hji   Rd is the output of Pj at the timestep   
and     Rd   is   set of learned weights  The layer denoted as  Output Softmax  performs the operation yi  
Softmax   mi  where     Rd   is the output weight
matrix  and   is the number of tokens in the program vocabulary  The model is trained to maximize the logsoftmax of
the reference program sequence  as is standard 

  MultiExample Pooling

  Hyperparameters and Training

The previous section only describes an architecture for encoding   single     example  However  in general we assume the input to consist of multiple     examples  The
number of     examples can be variable between test in 

   variant where   and   are reversed performs signi cantly

worse 

In all experiments  the size of the recurrent and fully connected layers is   and the size of the embeddings is
  Models were trained with plain SGD with gradient
clipping  All models were trained for   million minibatch
updates  where each minibatch contained   training instances         programs with four     examples each 
Each minibatch was resampled  so the model saw   mil 

RobustFill  Neural Program Learning under Noisy    

lion random programs and   million random     examples during training  Training took approximately   hours
on   Pascal Titan   GPUs  using an inhouse toolkit   
small amount of hyperparameter tuning was done on   synthetic validation set that was generated like the training 

  Program Synthesis Results
Once training is complete  the synthesis models can be decoded with   beam search decoder  Sutskever et al   
Unlike   typical sequence generation task  where the model
is decoded with   beam   and then only the  best output is
taken  here all kbest candidates are executed oneby one to
determine consistency  If multiple program candidates are
consistent with all observed examples  the program with
the highest model score is taken as the output  This program is referred to as    
In addition to standard beam search  we also propose  
variant referred to as  DPBeam  which adds   search
constraint similar to the dynamic programming algorithm
mentioned in Section   Here  each time an expression is completed during the search  the partial program
is executed in   blackbox manner  If any resulting partial
OutStr is not   string pre   of the observed OutStr  the
partial program is removed from the beam  This technique
is effective because our DSL is largely concatenative 

Figure   Generalization Results  Comparison of generalization
accuracy for of several program synthesis architectures 

Generalization accuracy is computed by applying     to all
six assessment examples  The percentage score reported
in the  gures represents the proportion of test instances for
which   consistent program was found and it resulted in
the exact correct output for all six assessment examples 
Consistency is evaluated in Section  
Results are shown in Figure   The most evident result
is that all attentional variants outperform the basic seqto 
seq model by   very large margin   roughly   absolute improvement  The difference between the three variants is smaller  but there is   clear improvement in accuracy as the models progress in complexity  Both Attention 

 We tried several alternative heuristics  such as taking the

shortest program  but these did not perform better 

  and AttentionC each add roughly   absolute accuracy  and this improvement appears even for   large beam 
The DPBeam variant also improves accuracy by roughly
  Overall  the best absolute accuracy achieved is  
by AttentionC DP    Beam  Although we have not
optimized our decoder for speed  the amortized endto end
cost of decoding is roughly   seconds per test instance for
AttentionC DP    Beam  and four observed examples
  accuracy  on   Pascal Titan   GPU 

  Comparison to Past Work

Prior to this work  the strongest statistical model for solving FlashFillTest was Parisotto et al    The generalization accuracy is shown below 

System

Parisotto et al   
Basic Seqto Seq
AttentionC
AttentionC DP

Beam

 
 
   
   
   
   

We believe this improvement in accuracy is due to two key
reasons  First  we used   seqto seq model with double attention instead of the treebased   NN model  which is dif 
 cult to train because of batching issues of treebased structures for larger programs  Second  late pooling allows us
to effectively incorporate powerful attention mechanisms
into our model  Because the architecture in Parisotto et al 
  performed pooling at the     encoding level  it could
not exploit the attention mechanisms which we show are
critical to achieving high accuracy 
Comparison to the FlashFill implementation currently deployed in Microsoft Excel is given in Section  

  Consistency vs  Generalization Results

Figure   Consistency vs  Generalization  Results were obtained using AttentionC 
The conceptual difference between consistency and generalization is detailed in Section   Results for different
beam sizes and different number of observed IO examples
are presented in Figure   As expected  the generalization
accuracy increases with the number of observed examples
for both beam sizes  although this is signi cantly more pro 

RobustFill  Neural Program Learning under Noisy    

nounced for   Beam  Interestingly  the consistency is
relatively constant when the number of observed examples
increases  There was no   priori expectation about whether
consistency would increase or decrease  since more examples are consistent with fewer total programs  but also give
the network   stronger input signal  Finally  we can see
that the Beam  decoding only generates consistent output roughly   of the time  which implies that the latent
function semantics learned by the model are still far from
perfect 

  Program Induction Results
An alternative approach to solving the FlashFill problem
is program induction  where the output string is generated
directly by the neural network without the need for   DSL 
More concretely  we can train   neural network which takes
as input   set of   observed examples         In  On 
as well an unpaired InStr       and generates the corresponding OutStr  Oy  As an example  from Figure  
      john Smith        Smith  Jhn        DOUG
   Macklin   
         Steve    Green  Oy  
 Green  Steve  Both approaches have the same end
goal   determine the Oy corresponding to       but have
several important conceptual differences 
The  rst major difference is that the induction model does
not use the program   anywhere  The synthesis model generates     which is executed by the DSL to produced Oy 
The induction model generates Oy directly by sequentially
predicting each character  In fact  in cases where it is possible to obtain   very large amount of realworld     example
sets  induction is   very appealing approach since it does
not require an explicit DSL  The core idea is the model
learns some latent program representation which can generalize beyond   speci   DSL  It also eliminates the need
to handdesign the DSL  unless the DSL is needed to synthesize training data 
The second major difference is that program induction has
no concept of consistency  As described previously  in program synthesis    kbest list of program candidates is executed oneby one  and the  rst program consistent with
all observed examples is taken as the output  As shown
in Section   if   consistent program can be found  it is
likely to generalize to new inputs  Program induction  on
the other hand  is essentially   standard sequence generation task akin to neural machine translation or image captioning   we directly decode Oy with   beam search and
take the highestscoring candidate as our output 

 In the results shown here  the induction model is trained on
data synthesized with the DSL  but the model training is agnostic
to this fact 

  Comparison of Induction and Synthesis Models

Despite these differences  it is possible to model both approaches using nearlyidentical network architectures  The
induction model evaluated here is identical to synthesis
AttentionA with late pooling  except for the following two
modi cations 

  Instead of generating     the system generates the new

OutStr Oy characterby character 

  There is an additional LSTM to encode      The de 

coder layer Oy uses double attention on Oj and     

The induction network diagram is given in the supplementary material  Each       Oy  pair is decoded independently 
but conditioned on all observed examples  The attention 
pooling  hidden sizes  training details  and decoder are otherwise identical to synthesis  The induction model was
trained on the same synthetic data as the synthesis models 

Figure   Synthesis vs  Induction AllExample Accuracy  The
synthesis model uses AttentionA with standard beam search 

Results are shown in Figure   The induction model is compared to synthesis AttentionA using the same measure of
generalization accuracy as previous sections   all six assessment examples must be exactly correct  Induction performs similarly to synthesis    beam  but both are signi cantly outperformed by synthesis    beam  The
generalization accuracy achieved by the induction model is
  compared to   for the synthesis model  The induction model uses   beam of   and does not improve with  
larger search because there is no way to evaluate candidates
after decoding 

  AverageExample Accuracy

All previous sections have used   strict de nition of  generalization accuracy  requiring all six assessment examples
to be exactly correct  We refer to this as allexample accuracy  However  another useful metric is to measure the total
percent of correct assessment examples  averaged over all
instances  With this metric  generalizing on  outof  assessment examples accumulates more credit than   We
refer to this as averageexample accuracy 

 The example still must be exactly correct   character edit rate

is not measured here 

RobustFill  Neural Program Learning under Noisy    

examples  All noise was applied with uniform random
probability into the InStr or OutStr using character insertions  deletions  or substitutions  Noise is not applied
to the assessment examples  as this would make evaluation
impossible 
We compare the models in this paper to the actual FlashFill implementation found in Microsoft Excel  as described
in Gulwani et al    An overview of this model is
described in Section   The results were obtained using  
macro in Microsoft Excel  

Figure   Neural Synthesis vs  FlashFill  All models use      
examples  and synthesis models use Beam 

The noise results are shown in Figure   The neural models behave very similarly  each degrading approximately
  absolute accuracy for each noise character introduced 
The behavior of Excel FlashFill is quite different  Without
noise  it achieves   accuracy  matching the best result
reported earlier in this paper  However  with just one or
two characters of noise  Excel FlashFill is effectively  broken  This result is expected  since the ef ciency of their
algorithm is critically centered around exact string matching  Gulwani et al    We believe that this robustness
to noise is one of the strongest attributes of DNNbased approaches to program synthesis 

  Conclusions
We have presented   novel variant of an attentional RNN architecture for program synthesis which achieves   accuracy on   realworld Programming By Example task  This
matches the performance of   handengineered system and
outperforms the previousbest neural synthesis model by
  Moreover  we have demonstrated that our model remains robust to moderate levels of noise in the     examples  while the handengineered system fails for even small
amounts of noise  Additionally  we carefully contrasted our
neural program synthesis system with   neural program induction system  and showed that even though the synthesis
system performs better on this task  both approaches have
their own strengths under certain evaluation conditions  In
particular  synthesis systems have an advantage when evaluating if all outputs are correct  while induction systems
have strength when evaluating which system has the most
correct outputs 

 FlashFill was manually developed on this exact set 

Figure   Synthesis vs  Induction AverageExample Accuracy

Averageexample results are presented in Figure   The
outcome matches our intuitions  Synthesis models tend to
be  all or nothing  since it must  nd   single program that
is jointly consistent with all observed examples  For both
synthesis conditions  less than   of the test instances are
partially correct  Induction models  on the other hand  have
  much higher chance of getting some of the assessment
examples correct  since they are decoded independently 
Here    of the test instances are partially correct  Examining the right side of the  gure  the induction model
shows relative strength under the averageexample accuracy metric  However  in terms of absolute performance 
the synthesis model still bests the induction model by  
It is dif cult to suggest which metric should be given more
credence  since the utility depends on the downstream application  For example  if   user wanted to automatically
 ll in an entire column in   spreadsheet  they may prioritize allexample accuracy   if the system proposes   solution  they can be con dent it will be correct for all rows 
However  if the application instead offered autocomplete
suggestions on   percell basis  then   model with higher
averageexample accuracy might be preferred 

  Handling Noisy     Examples
For the FlashFill task  realworld     examples are typically manually composed by the user  so noise       typos 
is expected and should be wellhandled  An example is
given in Figure  
Because neural network methods   are inherently probabilistic  and   operate in   continuous space representation  it is reasonable to believe that they can learn to be
robust to this type of noise  In order to explicitly account
for noise  we only made two small modi cations  First 
noise was synthetically injected into the training data using
random character transformations  Second  the best program     was selected by using character edit rate  CER 
 Marzal   Vidal    to the observed examples  rather
than exact match 
Since the FlashFillTest set does not contain any noisy examples  noise was synthetically injected into the observed

 This did not degrade the results on the noisefree test set 
 Standard beam is also used instead of DPBeam 

RobustFill  Neural Program Learning under Noisy    

References
Alur  Rajeev  Bodik  Rastislav  Juniwal  Garvit  Martin 
Milo MK  Raghothaman  Mukund  Seshia  Sanjit   
Singh  Rishabh  SolarLezama  Armando  Torlak  Emina  and Udupa  Abhishek  Syntaxguided synthesis 
IEEE   

Bahdanau  Dzmitry  Cho  Kyunghyun  and Bengio 
Yoshua  Neural machine translation by jointly learning
to align and translate  arXiv preprint arXiv 
 

Balog  Matej  Gaunt  Alexander    Brockschmidt  Marc 
DeeparXiv preprint

Nowozin  Sebastian  and Tarlow  Daniel 
coder  Learning to write programs 
arXiv   

Gaunt  Alexander    Brockschmidt  Marc  Singh 
Rishabh  Kushman  Nate  Kohli  Pushmeet  Taylor 
Jonathan  and Tarlow  Daniel  Terpret    probabilistic
programming language for program induction  CoRR 
abs   

Graves  Alex  Wayne  Greg  and Danihelka  Ivo  Neural
turing machines  arXiv preprint arXiv   

Graves  Alex  Wayne  Greg     Reynolds     Harley    
Danihelka     GrabskaBarwiska  SG  Colmenarejo    
Grefenstette     Ramalho     Agapiou  and AP  Badia 
Hybrid computing using   neural network with dynamic
external memory  Nature     

Gulwani  Sumit  Automating string processing in spreadsheets using inputoutput examples  In ACM SIGPLAN
Notices  ACM   

Gulwani  Sumit  Harris  William    and Singh  Rishabh 
Spreadsheet data manipulation using examples  Communications of the ACM   

Huang  PoYao  Liu  Frederick  Shiang  SzRung  Oh 
Jean  and Dyer  Chris  Attentionbased multimodal neural machine translation  In Proceedings of the First Conference on Machine Translation  Berlin  Germany   

Joulin  Armand and Mikolov  Tomas  Inferring algorithmic
patterns with stackaugmented recurrent nets  In NIPS 
pp     

Kaiser  Lukasz and Sutskever  Ilya  Neural gpus learn al 

gorithms  CoRR  abs   

Kurach  Karol  Andrychowicz  Marcin  and Sutskever 

Ilya  Neural randomaccess machines  ICLR   

Luong  MinhThang  Pham  Hieu  and Manning  Christopher    Effective approaches to attentionbased neural
machine translation  arXiv preprint arXiv 
 

Manna  Zohar and Waldinger  Richard  Knowledge and
reasoning in program synthesis  Arti cial intelligence   
   

Manna  Zohar and Waldinger  Richard    deductive approach to program synthesis  ACM Transactions on Programming Languages and Systems  TOPLAS   
   

Marzal  Andres and Vidal  Enrique  Computation of norIEEE transacmalized edit distance and applications 
tions on pattern analysis and machine intelligence   

Menon  Aditya Krishna  Tamuz  Omer  Gulwani  Sumit 
Lampson  Butler    and Kalai  Adam    machine learning framework for programming by example  In ICML 
pp     

Neelakantan  Arvind  Le  Quov    and Sutskever  Ilya 
Neural programmer  Inducing latent programs with gradient descent  ICLR   

Parisotto  Emilio  Mohamed  Abdelrahman  Singh 
Rishabh  Li  Lihong  Zhou  Dengyong  and Kohli  Pushmeet  Neurosymbolic program synthesis  ICLR   

Polozov  Oleksandr and Gulwani  Sumit  Flashmeta   
In OOP 

framework for inductive program synthesis 
SLA  pp     

Reed  Scott and de Freitas  Nando  Neural programmer 

interpreters  ICLR   

Riedel  Sebastian  Bosnjak  Matko  and Rockt aschel  Tim 
Programming with   differentiable forth interpreter 
CoRR  abs   

Sutskever  Ilya  Vinyals  Oriol  and Le  Quoc    Sequence
In NIPS 

to sequence learning with neural networks 
 

Tai  Kai Sheng  Socher  Richard  and Manning  ChristoImproved semantic representations from treearXiv

pher   
structured long shortterm memory networks 
preprint arXiv   

Vinyals  Oriol  Kaiser   ukasz  Koo  Terry  Petrov  Slav 
Sutskever  Ilya  and Hinton  Geoffrey  Grammar as  
foreign language  In NIPS   

Waldinger  Richard    and Lee  Richard       Prow    step

toward automatic program writing  In IJCAI   

Xu  Kelvin  Ba  Jimmy  Kiros  Ryan  Cho  Kyunghyun 
Courville  Aaron    Salakhutdinov  Ruslan  Zemel 
Richard    and Bengio  Yoshua  Show  attend and tell 
Neural image caption generation with visual attention 
In ICML   

Zaremba  Wojciech and Sutskever  Ilya  Learning to exe 

cute  arXiv preprint arXiv   

