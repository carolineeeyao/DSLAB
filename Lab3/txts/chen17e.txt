Learning to Learn without Gradient Descent
by Gradient Descent

Yutian Chen   Matthew    Hoffman   Sergio   omez Colmenarejo   Misha Denil   Timothy    Lillicrap  

Matt Botvinick   Nando de Freitas  

Abstract

We learn recurrent neural network optimizers
trained on simple synthetic functions by gradient descent  We show that these learned optimizers exhibit   remarkable degree of transfer in that
they can be used to ef ciently optimize   broad
range of derivativefree blackbox functions  including Gaussian process bandits  simple control
objectives  global optimization benchmarks and
hyperparameter tuning tasks  Up to the training horizon  the learned optimizers learn to tradeoff exploration and exploitation  and compare
favourably with heavily engineered Bayesian optimization packages for hyperparameter tuning 

  Introduction
Findings in developmental psychology have revealed that
infants are endowed with   small number of separable
systems of core knowledge for reasoning about objects 
actions  number  space  and possibly social interactions
 Spelke and Kinzler    These systems enable infants
to learn many skills and acquire knowledge rapidly  The
most coherent explanation of this phenomenon is that the
learning  or optimization  process of evolution has led to
the emergence of components that enable fast and varied
forms of learning  In psychology  learning to learn has  
long history  Ward    Harlow    Kehoe   
Inspired by this  many researchers have attempted to build
agents capable of learning to learn  Schmidhuber   
Naik and Mammone    Thrun and Pratt    Hochreiter et al    Santoro et al    Duan et al   
Wang et al    Ravi and Larochelle    Li and Malik    The scope of research under the umbrella of
learning to learn is very broad  The learner can implement
and be trained by many different algorithms  including gra 

 DeepMind  London  United Kingdom  Correspondence to 

Yutian Chen  yutianc google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

dient descent  evolutionary strategies  simulated annealing 
and reinforcement learning 
For instance  one can learn to learn by gradient descent by
gradient descent  or learn local Hebbian updates by gradient descent  Andrychowicz et al    Bengio et al 
  In the former  one uses supervised learning at the
metalevel to learn an algorithm for supervised learning 
while in the latter  one uses supervised learning at the metalevel to learn an algorithm for unsupervised learning 
Learning to learn can be used to learn both models and
algorithms 
In Andrychowicz et al    the output of
metalearning is   trained recurrent neural network  RNN 
which is subsequently used as an optimization algorithm to
   other models to data  In contrast  in Zoph and Le  
the output of metalearning can also be an RNN model  but
this new RNN is subsequently used as   model that is    to
data using   classical optimizer  In both cases the output
of metalearning is an RNN  but this RNN is interpreted
and applied as   model or as an algorithm  In this sense 
learning to learn with neural networks blurs the classical
distinction between models and algorithms 
In this work  the goal of metalearning is to produce an
algorithm for global blackbox optimization  Speci cally 
we address the problem of  nding   global minimizer of an
unknown  blackbox  loss function    That is  we wish to
compute   cid    arg minx           where   is some search
space of interest  The blackbox function   is not available
to the learner in simple closed form at test time  but can be
evaluated at   query point   in the domain  This evaluation
produces either deterministic or stochastic outputs      
such that                     In other words  we can only
observe the function   through unbiased noisy pointwise
observations   
Bayesian optimization is one of the most popular blackbox
optimization methods  Brochu et al    Snoek et al 
  Shahriari et al   
It is   sequential modelbased decision making approach with two components 
The  rst component is   probabilistic model  consisting of
  prior distribution that captures our beliefs about the behavior of the unknown objective function and an observation model that describes the data generation mechanism 

Learning to Learn without Gradient Descent by Gradient Descent

The model can be   BetaBernoulli bandit    random forest    Bayesian neural network  or   Gaussian process  GP 
 Shahriari et al    Bayesian optimization is however
often associated with GPs  to the point of sometimes being
referred to as GP bandits  Srinivas et al   
The second component is an acquisition function  which
is optimized at each step so as to tradeoff exploration
and exploitation  Here again we encounter   huge variety
of strategies  including Thompson sampling  information
gain  probability of improvement  expected improvement 
upper con dence bounds  Shahriari et al    The requirement for optimizing the acquisition function at each
step can be   signi cant cost  as shown in the empirical section of this paper  It also raises some theoretical concerns
 Wang et al   
In this paper  we present   learning to learn approach for
global optimization of blackbox functions and contrast it
with Bayesian optimization 
In the metalearning phase 
we use   large number of differentiable functions generated with   GP to train RNN optimizers by gradient descent  We consider two types of RNN  longshort term
memory networks  LSTMs  by Hochreiter and Schmidhuber   and differentiable neural computers  DNCs  by
Graves et al   
During metalearning  we choose the horizon  number of
steps  of the optimization process  We are therefore considering the  nite horizon setting that is popular in AB tests
 Kohavi et al    Scott    and is often studied under the umbrella of best arm identi cation in the bandits
literature  Bubeck et al    Gabillon et al   
The RNN optimizer learns to use its memory to store information about previous queries and function evaluations 
and learns to access its memory to make decisions about
which parts of the domain to explore or exploit next  That
is  by unrolling the RNN  we generate new candidates for
the search process  The experiments will show that this
process is much faster than applying standard Bayesian optimization  and in particular it does not involve either matrix inversion or optimization of acquisition functions 
In the experiments we also investigate distillation of acquisition functions to guide the process of training the RNN
optimizers  and the use of parallel optimization schemes
for expensive training of deep networks 
The experiments show that the learned optimizers can
transfer to optimize   large and diverse set of blackbox functions arising in global optimization  control  and
hyperparameter tuning  Moreover  withing the training
horizon  the RNN optimizers are competitive with stateof theart heavily engineered packages such as Spearmint 
SMAC and TPE  Snoek et al    Hutter et al     
Bergstra et al   

  Learning Blackbox Optimization
  blackbox optimization algorithm can be summarized by
the following loop 

  Given the current state of knowledge ht propose  

query point xt

  Observe the response yt
  Update any internal statistics to produce ht 

This easily maps onto the classical frameworks presented in
the previous section where the update step computes statistics and the query step uses these statistics for exploration 
In this work we take this framework as   starting point and
de ne   combined update and query rule using   recurrent
neural network parameterized by   such that

ht  xt   RNN ht  xt  yt 

yt         xt   

 
 

Intuitively this rule can be seen to update its hidden state
using data from the previous time step and then propose
  new query point 
In what follows we will apply this
RNN  with shared parameters  to many steps of   blackbox optimization process  An example of this computation
is shown in Figure   Additionally  note that in order to
generate the  rst query    we arbitrarily set the initial  observations  to dummy values        and        this is  
point we will return to in Section  

  Loss Function

Given this rule we now need   way to learn the parameters
  with stochastic gradient descent for any given distribution of differentiable functions       Perhaps the simplest
loss function one could use is the loss of the  nal iteration    nal    Ef         xT   for some timehorizon
    This loss was considered by Andrychowicz et al   
in the context of learning  rstorder optimizers  but ultimately rejected in favor of the summed loss

Lsum    Ef     

   xt 

 

 

  

  key reason to prefer Lsum is that the amount of information conveyed by   nal is temporally very sparse  By
instead utilizing   sum of losses to train the optimizer we
are able to provide information from every step along this
trajectory  Although at test time the optimizer typically
only has access to the observation yt  at training time the
true loss can be used  Note that optimizing the summed
loss is equivalent to  nding   strategy which minimizes the
expected cumulative regret  Finally  while in many optimization tasks the loss associated with the best observation
mint    xt  is often desired  the cumulative regret can be
seen as   proxy for this quantity 

 cid    cid 

 cid 

Learning to Learn without Gradient Descent by Gradient Descent

Figure   Computational graph of the learned blackbox optimizer unrolled over multiple steps  The learning process will consist of
differentiating the given loss with respect to the RNN parameters

By using the above objective function we will be encouraged to trade off exploration and exploitation and hence
globally optimize the function    This is due to the fact that
in expectation  any method that is better able to explore and
 nd small values of       will be rewarded for these discoveries  However the actual process of optimizing the above
loss can be dif cult due to the fact that nothing explicitly
encourages the optimizer itself to explore 
We can encourage exploration in the space of optimizers by
encoding an exploratory force directly into the meta learning loss function  Many examples exist in the bandit and
Bayesian optimization communities  for example

 cid 

 cid    cid 

  

LEI     Ef     

EI xt       

 

where EI  is the expected posterior improvement of
querying xt given observations up to time    This can
encourage exploration by giving an explicit bonus to the
optimizer rather than just implicitly doing so by means of
function evaluations  Alternatively  it is possible to use the
observed improvement  OI 

LOI    Ef     

min

   xt    min

   

    xi   

 cid 

 cid    cid 

  

of the loss with respect to the RNN parameters   and perform stochastic gradient descent  SGD  In order to evaluate these derivatives we assume that derivatives of   can
be computed with respect to its inputs  This assumption is
made only in order to backpropagate errors from the loss to
the parameters  but crucially is not needed at test time  If
the derivatives of   are also not available at training time
then it would be necessary to approximate these derivatives
via an algorithm such as REINFORCE  Williams   

  Training Function Distribution

To this point we have made no assumptions about the distribution of training functions       In this work we are
interested in learning generalpurpose blackbox optimizers  and we desire our distribution to be quite broad 
As   result we propose the use of GPs as   suitable training distribution  Under the GP prior  the joint distribution of function values at any  nite set of query points follows   multivariate Gaussian distribution  Rasmussen and
Williams    and we generate   realization of the training function incrementally at the query points using the
chain rule with   total time complexity of       for every function sample 
The use of functions sampled from   GP prior also provides functions whose gradients can be easily evaluated at
training time as noted above  Further  the posterior expected improvement used within LEI can be easily computed  Mo ckus    and differentiated as well  Search
strategies based on GP losses  such as LEI  can be thought
of as   distilled strategies  The major downside of search
strategies which are based on GP inference is their cubic
complexity 

 cid cid 

 

We also studied   loss based on GPUCB  Srinivas et al 
  but in preliminary experiments this did not perform
as well as the EI loss and is thus not included in the later
experiments 
The illustration of Figure   shows the optimizer unrolled
over many steps  ultimately culminating in the loss function  To train the optimizer we will simply take derivatives

RNNRNNRNNfffht xt yt xt yt xtytht htht Lossyt Learning to Learn without Gradient Descent by Gradient Descent

algorithm 
  welltrained optimizer must learn to condition on ot  in
order to either generate initial queries or generate queries
based on past observations  Another key point to consider
is that the batch nature of the optimizer can result in the
ordering of queries being permuted       although xt is proposed before xt  it is entirely plausible that xt  is evaluated  rst  In order to account for this at training time and
not allow the optimizer to rely on   speci   ordering  we
simulate   runtime      Uniform          associated
with the tth query  Observations are then made based on
the order in which they complete 
It is worth noting that the sequential setting is   special case
of this parallel policy where       and every observation
is made with ot      Note also that we have kept the
number of workers  xed for simplicity of explanation only 
The architecture allows for the number of workers to vary 
It is instructive to contrast this strategy with what is done
in parallel Bayesian optimization  Desautels et al   
Snoek et al    There care must be taken to ensure
that   diverse set of queries are utilized in the absence of
additional data the standard sequential strategy would propose   queries at the same point  Exactly computing the
optimal Nstep query is typically intractable  and as   result handengineered heuristics are employed  Often this
involves synthetically reducing the uncertainty associated
with outstanding queries in order to simulate later observations  In contrast  our RNN optimizer can store in its hidden state any relevant information about outstanding observations  Decisions about what to store are learned during
training and as   result should be more directly related to
later losses 

  Experiments
We present several experiments that show the breadth
of generalization that is achieved by our learned algorithms  We train our algorithms to optimize very simple
functions samples from   GP with    xed length scale 
and show that the learned algorithms are able to generalize
from these simple objective functions to   wide variety of
other test functions that were not seen during training 
We experimented with two different RNN architectures 
LSTMs and DNCs  However  we found the DNCs to perform slightly  but not signi cantly  better  For clarity  we
only show plots for DNCs in most of the  gures 
We train each RNN optimizer with trajectories of   steps 
and update the RNN parameters using BPTT with Adam 
We use   curriculum to increase the length of trajectories
gradually from       to   We repeat this process for
each of the loss functions discussed in Section   Hyper 

Figure   Graph depicting   single iteration of the parallel algorithm with   workers  Here we explicitly illustrate the fact that
the query xt is being assigned to the ith worker for evaluation 
and that the the next observation pair  xt   yt  is the output of the
jth worker  which has completed its function evaluation 

While training with   GP prior grants us the convenience
to assess the ef cacy of our training algorithm by comparing headto head with GPbased methods  it is worth noting that our model can be trained with any distribution that
permits ef cient sampling and function differentiation  The
 exibility could become useful when considering problems
with speci   prior knowledge and or side information 

  Parallel Function Evaluation

The use of parallel function evaluation is   common technique in Bayesian optimization  often used for costly  but
easy to simulate functions  For example  as illustrated in
the experiments  when searching for hyperparameters of
deep networks  it is convenient to train several deep networks in parallel 
Suppose we have   workers  and that the process of
proposing candidates for function evaluation is much faster
than evaluating the functions  We augment our RNN optimizer   input with   binary variable ot as follows 

ht  xt   RNN ht  ot   xt   yt 

 
For the  rst       steps  we set ot      arbitrarily
set the inputs to dummy values  xt      and  yt     
and generate   parallel queries       As soon as   worker
 nishes evaluating   query  the query and its evaluation are
fed back to the network by setting ot      resulting in
  new query xt  Figure   displays   single iteration of this

RNNht xt yt ot xt fxt yt ht fifjfN Learning to Learn without Gradient Descent by Gradient Descent

Figure   Average minimum observed function value  with   con dence intervals  as   function of search steps on functions sampled
from the training GP distribution  Left four  gures  Comparing DNC with different reward functions against Spearmint with  xed and
estimated GP hyperparameters  TPE and SMAC  Right bottom  Comparing different DNCs and LSTMs  As the dimension of the search
space increases  the DNC   performance improves relative to the baselines 

Figure   How different methods tradeoff exploration and exploitation in   onedimensional example  Blue  Unknown function being
optimized  Green crosses  Function values at query points  Red trajectory  Query points over   steps 

parameters for the RNN optimization algorithm  such as
learning rate  number of hidden units  and memory size
for the DNC models  are found through grid search during training  When ready to be used as an optimizer  the
RNN requires neither tuning of hyperparameters nor handengineering  It is fully automatic 
In the following experiments  DNC sum refers to the DNC
network trained using the summed loss Lsum  DNC OI to
the network trained using the loss LOI  and DNC EI to the
network trained with the loss LEI 
We compare our learning to learn approach with popular stateof theart Bayesian optimization packages  including Spearmint with automatic inference of the GP hyperparameters and input warping to deal with nonstationarity
 Snoek et al    Hyperopt  TPE   Bergstra et al 
  and SMAC  Hutter et al      For test functions

with integer inputs  we treat them as piecewise constant
functions and round the network output to the closest values  We evaluate the performance at   given search step
          according to the minimum observed function value up to step    mini      xi 

  Performance on Functions Sampled from the

Training Distribution

We  rst evaluate performance on functions sampled from
the training distribution  Notice  however  that these functions are never observed during training  Figure   shows
the best observed function values as   function of search
step    averaged over     sampled functions for RNN
models and   sampled functions for other models  we
can afford to do more for RNNs because they are very fast
optimizers  For Spearmint  we consider both the default

 Min function valueGP samples  dim Min function valueGP samples  dim Min function valueGP samples  dim Min function valueGP samples  dim SpearmintSpearmint Fixed HyperparametersTPESMACDNC sumDNC OIDNC EI Min function valueCompare DNC vs LSTM  dim DNC sumLSTM sumDNC OILSTM OIDNC EILSTM EI DNC sum DNC OI DNC EI Spearmint Fixed Hypers Learning to Learn without Gradient Descent by Gradient Descent

Figure   Left  Average minimum observed function value  with   con dence intervals  as   function of search steps on   benchmark
functions  Branin  Goldstein price   dHartmann and  dHartmann  Again we see that as the dimension of the search space increases 
the learned DNC optimizers are more effective than the Spearmint  TPE and SMAC packages within the training horizon  Right 
Average minimum observed function value in terms of the optimizer   runtime  seconds  illustrating the superiority in speed of the
DNC optimizers over existing blackbox optimization methods 

setting with   prior distribution that estimates the GP hyperparameters by Monte Carlo and   setting with the same
hyperparameters as those used in training  For the second
setting  Spearmint knows the ground truth and thus provides   very competitive baseline  As expected Spearmint
with    xed prior proves to be one of the best models
under most settings  When the input dimension is   or
higher  however  neural network models start to outperform Spearmint  We suspect it is because in higher dimensional spaces  the RNN optimizer learns to be more
exploitative given the  xed number of iterations  Among
all RNNs  those trained with expected observed improvement perform better than those trained with direct function
observations 
Figure   shows the query trajectories xt                 
for different blackbox optimizers in   onedimensional example  All of the optimizers explore initially  and later settle in one mode and search more locally  The DNCs trained
with EI behave most similarly to Spearmint  DNC with direct function observations  DNC sum  tends to explore less
than the other optimizers and often misses the global optimum  while the DNCs trained with the observed improvement  OI  keep exploring even in later stages 

  Transfer to Global Optimization Benchmarks

We compare the algorithms on four standard benchmark functions for blackbox optimization with dimensions

ranging from   to   To obtain   more robust evaluation of
the performance of each model  we generate multiple instances for each benchmark function by applying   random
translation   scaling    ipping  and dimension permutation in the input domain 
The lef hand side of Figure   shows the minimum observed
function values achieved by the learned DNC optimizers 
and contrasts these against the ones attained by Spearmint 
TPE and SMAC  All methods appear to have similar performance with Spearming doing slightly better in low dimensions  As the dimension increases  we see that the DNC optimizers converge at at   much faster rate within the horizon
of       steps 
We also observe that DNC OI and DNC EI both outperform
DNC with direct obsevations of the loss  DNC sum  It is
encouraging that the curves for DNC OI and DNC EI are
so close  While DNC EI is distilling   popular acquisition
function from the EI literature  the DNC OI variant is much
easier to train as it never requires the GP computations necessary to construct the EI acquisition function 
The right hand side of Figure   shows that the neural network optimizers run about   times faster than Spearmint
and   times faster than TPE and SMAC with the DNC
architecture  There is an additional   times speedup when
using the LSTM architecture  as shown in Table   The
negligible runtime of our optimizers suggests new areas
of application for global optimization methods that require

 Min function valueBranin dim Min function valueGoldsteinPrice dim Min function valueHartmann  dim SpearmintTPESMACDNC sumDNC OIDNC EI Min function valueHartmann  dim Min function valueGoldsteinPrice dim Min function valueHartmann  dim Learning to Learn without Gradient Descent by Gradient Descent

both high sample ef ciency and realtime performance 

Table   Runtime  seconds  for   iterations excluding the
blackbox function evaluation time 

Branin
Goldstein
Hartmann  
Hartmann  

Spearmint TPE SMAC DNC LSTM
 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

  Transfer to   Simple Control Problem

We also consider an application to   simple reinforcement learning task described by  Hoffman et al   
In this problem we simulate   physical system consisting
of   number of repellers which affect the fall of particles
through    Dspace  The goal is to direct the path of the
particles through high reward regions of the state space and
maximize the accumulated discounted reward  The fourdimensional statespace in this problem consists of   particle   position and velocity  The path of the particles can
be controlled by the placement of repellers which push the
particles directly away with   force inversely proportional
to their distance from the particle  At each time step the
particle   position and velocity are updated using simple
deterministic physical forward simulation  The control policy for this problem consists of   learned parameters for
each repeller     location and the strength of the repeller 
In our experiments we consider   problem with   repellers 
       parameters  An example trajectory along with the reward structure  contours  and repeller positions  circles  is
displayed in Figure   We apply the same perturbation as in
the previous subsection to study the average performance 
The loss  minimal negative reward  of all models are also
plotted in Figure   Neural network models outperform all
the other competitors in this problem 

  Transfer to ML Hyperparameter Tuning

Lastly  we consider hyperparameter tuning for machine
learning problems  We include the three standard benchmarks in the HPOLib package  Eggensperger et al   
SVM  online LDA  and logistic regression with     and  
hyperparameters respectively  We also consider the problem of training    hyperparameter residual network for
classi cation on the CIFAR  dataset 
For the  rst three problems  the objective functions have already been precomputed on   grid of hyperparameter values  and therefore evaluation with different random seeds
  for Spearmint    for TPE and SMAC  is cheap 
For the last experiment  however  it takes at least   GPU
hours to evaluate one hyperparameter setting  For this reason  we test the parallel proposal idea introduced in Section

Figure   Top  An example trajectory of   falling particle in red 
where solid circles show the position and strength of the two repellers and contour lines show the reward function  The aim to to
position and choose the strength of the repellers so that the particle spends more time in regions of high reward  Bottom  The
results of each method on optimizing the controller by direct policy search  Here  the learned DNC OI optimizer appears to have
an edge over the other techniques 

  with   parallel proposal mechanisms  This approach is
about  ve times more ef cient 
For the  rst three tasks  our model is run once because
the setup is deterministic  For the residual network task 
there is some random variation so we consider three runs
per method 
The results are shown in Figure   The plots report the
negative accuracy against number of function evaluations
up to   horizon of       The neural network models
especially when trained with observed improvement show
competitive performance against the engineered solutions 
In the ResNet experiment  we also compare our sequential
DNC optimizers with the parallel versions with   workers  In this experiment we  nd that the learned and engineered parallel optimizers perform as well if not slightly

repeller repeller sampletrajectorycontours ofimmediate reward Min function valueRepellersSpearmintTPESMACDNC sumDNC OIDNC EILearning to Learn without Gradient Descent by Gradient Descent

Figure   Average test loss  with   con dence intervals  for the SVM  online LDA  and logistic regression hyperparameter tuning
benchmarks  The bottomright plot shows the performance of all methods on the problem of tuning   residual network  demonstrating
that the learned DNC optimizers are close in performance to the engineered optimizers  and that the faster parallel versions work
comparably well 

better than the sequential ones  These minor differences
arise from random variation 

  Conclusions and Future Work
The experiments have shown that up to the training horizon the learned RNN optimizers are able to match the performance of heavily engineered Bayesian optimization solutions  including Spearmint  SMAC and TPE  The trained
RNNs rely on neither heuristics nor hyperparameters when
being deployed as blackbox optimizers 
The optimizers trained on synthetic functions were able to
transfer successfully to   very wide class of blackbox functions  associated with GP bandits  control  global optimization benchmarks  and hyperparameter tuning 
The experiments have also shown that the RNNs are massively faster than other Bayesian optimization methods 
Hence  for applications involving   known horizon and
where speed is crucial  we recommend the use of the RNN

optimizers  The parallel version of the algorithm also
performed well when tuning the hyperparameters of an
expensiveto train residual network 
However 
the current RNN optimizers also have some
shortcomings  Training for very long horizons is dif cult 
This issue was also documented recently in  Duan et al 
  We believe curriculum learning should be investigated as   way of overcoming this dif culty  In addition   
new model has to be trained for every input dimension with
the current network architecture  While training optimizers
for every dimension is not prohibitive in low dimensions 
future works should extend the RNN structure to allow  
variable input dimension    promising solution is to serialize the input vectors along the search steps 

References
   Andrychowicz     Denil     Gomez        Hoffman     Pfau 
   Schaul     Shillingford  and    de Freitas  Learning to learn
by gradient descent by gradient descent  In Advances in Neural
Information Processing Systems   

 Min function valueSVM  dim Min function valueOnline LDA  dim Min function valueLogistic Regression  dim Min function valueResNet on Cifar  dim SpearmintSpearmint ParallelTPESMACDNC sumDNC OIDNC OI ParallelDNC EIDNC EI ParallelLearning to Learn without Gradient Descent by Gradient Descent

   Bengio     Bengio     Cloutier  and    Gecsei  On the optimization of   synaptic learning rule  In Conference on Optimality
in Biological and Arti cial Networks   

      Bergstra     Bardenet     Bengio  and      egl  Algorithms
for hyperparameter optimization  In Advances in Neural Information Processing Systems  pages    

   Brochu        Cora  and    de Freitas    tutorial on Bayesian
optimization of expensive cost functions  with application to
active user modeling and hierarchical reinforcement learning 
Technical Report UBC TR  and arXiv   
Dept  of Computer Science  University of British Columbia 
 

   Bubeck     Munos  and    Stoltz  Pure exploration in multiarmed bandits problems  In International Conference on Algorithmic Learning Theory   

   Desautels     Krause  and    Burdick  Parallelizing explorationexploitation tradeoffs with Gaussian process bandit optimization  Journal of Machine Learning Research   

   Duan     Schulman     Chen     Bartlett     Sutskever  and
   Abbeel  Rl  Fast reinforcement learning via slow reinforcement learning  Technical report  UC Berkeley and OpenAI 
 

   Eggensperger     Feurer     Hutter     Bergstra     Snoek 
   Hoos  and    LeytonBrown  Towards an empirical foundation for assessing bayesian optimization of hyperparameters 
In NIPS workshop on Bayesian Optimization in Theory and
Practice   

   Gabillon     Ghavamzadeh  and    Lazaric  Best arm identi cation    uni ed approach to  xed budget and  xed con 
dence  In Advances in Neural Information Processing Systems 
pages    

   Graves     Wayne     Reynolds     Harley     Danihelka 
   GrabskaBarwi  Aska        Colmenarejo     Grefenstette 
   Ramalho     Agapiou           Badia        Hermann 
   Zwols     Ostrovski     Cain     King     Summer eld 
   Blunsom     Kavukcuoglu  and    Hassabis  Hybrid computing using   neural network with dynamic external memory 
Nature   

      Harlow  The formation of learning sets  Psychological re 

view     

   Hochreiter and    Schmidhuber  Long shortterm memory  Neu 

ral computation     

   Hochreiter        Younger  and       Conwell  Learning to
learn using gradient descent  In International Conference on
Arti cial Neural Networks  pages   Springer   

      Hoffman     Kueck     de Freitas  and    Doucet  New inference strategies for solving Markov decision processes using
In Uncertainty in Arti cial Intellireversible jump MCMC 
gence  pages    

   Hutter        Hoos  and    LeytonBrown  Sequential modelIn

based optimization for general algorithm con guration 
LION  pages      

   Hutter        Hoos  and    LeytonBrown  Sequential modelbased optimization for general algorithm con guration  In International Conference on Learning and Intelligent Optimization  pages   Springer     

      Kehoe    layered network model of associative learning 
learning to learn and con guration  Psychological review   
   

   Kohavi     Longbotham     Sommer eld  and       Henne 
Controlled experiments on the web  survey and practical guide 

Data mining and knowledge discovery     
   Li and    Malik  Learning to optimize  In International Con 

ference on Learning Representations   

   Mo ckus  The Bayesian approach to global optimization 

In
Systems Modeling and Optimization  volume   pages  
  Springer   

      Naik and    Mammone  Metaneural networks that learn
by learning  In International Joint Conference on Neural Networks  volume   pages    

      Rasmussen and          Williams  Gaussian Processes for

Machine Learning  The MIT Press   

   Ravi and    Larochelle  Optimization as   model for fewshot
learning  In International Conference on Learning Representations   

   Santoro     Bartunov     Botvinick     Wierstra  and    Lillicrap  Metalearning with memoryaugmented neural networks 
In International Conference on Machine Learning   

   Schmidhuber 

Evolutionary Principles in SelfReferential
Learning  On Learning how to Learn  The MetaMeta Meta 
Hook  PhD thesis  Institut    Informatik  Tech  Univ  Munich 
 

      Scott    modern Bayesian look at the multiarmed bandit  Applied Stochastic Models in Business and Industry   
   

   Shahriari     Swersky     Wang        Adams  and    de Freitas  Taking the human out of the loop    review of Bayesian
optimization  Proceedings of the IEEE     
   Snoek     Larochelle  and       Adams  Practical Bayesian optimization of machine learning algorithms  In Advances in Neural Information Processing Systems  pages    
   Snoek     Swersky        Zemel  and       Adams  Input warping for Bayesian optimization of nonstationary functions  In
International Conference on Machine Learning   

      Spelke and       Kinzler  Core knowledge  Developmental

science     

   Srinivas     Krause        Kakade  and    Seeger  Gaussian
process optimization in the bandit setting  No regret and exIn International Conference on Machine
perimental design 
Learning  pages    

   Thrun and    Pratt  Learning to learn  Springer Science  

Business Media   

      Wang     KurthNelson     Tirumala     Soyer        Leibo 
   Munos     Blundell     Kumaran  and    Botvinick  Learning to reinforcement learn  arXiv Report    

   Wang     Shakibi     Jin  and    de Freitas  Bayesian multiscale optimistic optimization  In AI and Statistics  pages  
   

      Ward  Reminiscence and rote learning  Psychological

Monographs     

      Williams  Simple statistical gradientfollowing algorithms
for connectionist reinforcement learning  Machine learning   
   

   Zoph and       Le  Neural architecture search with reinforcement learning  In International Conference on Learning Representations   

