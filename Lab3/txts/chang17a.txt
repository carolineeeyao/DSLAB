Multiple Clustering Views from Multiple Uncertain Experts

Yale Chang   Junxiang Chen   Michael    Cho   Peter    Castaldi   Edwin    Silverman   Jennifer    Dy  

Abstract

Expert
input can improve clustering performance  In today   collaborative environment  the
availability of crowdsourced multiple expert input is becoming common  Given multiple experts  inputs  most existing approaches can only
discover one clustering structure  However  data
is multifaceted by nature and can be clustered
in different ways  also known as views  In an
exploratory analysis problem where ground truth
is not known  different experts may have diverse
views on how to cluster data  In this paper  we
address the problem on how to automatically discover multiple ways to cluster data given potentially diverse inputs from multiple uncertain experts  We propose   novel Bayesian probabilistic model that automatically learns the multiple
expert views and the clustering structure associated with each view  The bene ts of learning the experts  views include   enabling the
discovery of multiple diverse clustering structures  and   improving the quality of clustering solution in each view by assigning higher
weights to experts with higher con dence 
In
our approach  the expert views  multiple clustering structures and expert con dences are jointly
learned via variational inference  Experimental
results on synthetic datasets  benchmark datasets
and   realworld disease subtyping problem show
that our proposed approach outperforms competing baselines  including meta clustering  semisupervised clustering  semicrowdsourced clustering and consensus clustering 

  Introduction
As   cornerstone of unsupervised learning  clustering has
been widely used in knowledge discovery problems  Jain

 Northeastern University  Boston  MA  Brigham and
Women   Hospital  Harvard Medical School  Boston  MA  Correspondence to  Yale Chang  ychang coe neu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

et al    Given data matrix and   notion of similarity between samples  clustering aims to categorize data
into different clusters so that samples in the same cluster
are similar and samples in different clusters are dissimilar 
Thus  depending on users  notions of similarity  the same
dataset can be clustered in different ways  also known as
views  Niu et al    For example  face images can be
clustered based on pose or identity  marbles can be clustered based on shape or color  However  how to properly
de ne similarity between samples in   knowledge discovery problem is nontrivial  To help solve this challenge 
semisupervised clustering utilizes expert supervision to
guide the clustering towards the right solution  Wagstaff  
Cardie    Basu et al    Supervision is usually in
the form of pairwise constraints between samples  including mustlink  ML  and cannotlink  CL  constraints 
Instead of supervision from one expert  it is becoming more
common for supervision to be available from multiple experts as data can be shared and processed by increasingly
larger audiences       crowdsourcing  Howe    mechanisms such as Amazon Mechanical Turk and large collaborative consortiums  Zhang et al   
In an exploratory data analysis setting  where ground truths are not
known  different experts might provide supervision  pairwise contraints  with varying views in mind  For example  one expert might be thinking of similarity clustering
based on pose and another expert might be providing inputs based on identity on the face image problem  Moreover  because experts are not oracles  their inputs are prone
to errors as well  In this paper  we address   new clustering paradigm  how to discover multiple clustering structures in the data given potentially diverse constraints
from multiple uncertain experts 
Our objective of  nding multiple clustering structures
given inputs from multiple experts is motivated by discovering subtypes  clusters  of   complex lung disease called
Chronic Obstructive Pulmonary Disease  COPD  COPD is
currently the third leading cause of death in the US  Murphy et al    Although traditionally being called one
disease  doctors believe there exist multiple disease subtypes and providing personalized clinical care to patients
according to their disease subtypes can lead to more effective treatments  We have collected constraints provided by
multiple experts in the consortium  The experts had var 

Multiple Clustering Views from Multiple Uncertain Experts

ied backgrounds    clinicians tend to provide constraints
by comparing patients  clinical measurements  and   radiologists tend to provide constraints based on examining
patients  computed tomography  CT  images  On the one
hand  experts have disagreements on whether to put   pair
of patients in the same group because they are focusing on
different aspects of patients  On the other hand  experts of
similar backgrounds  clinicians  radiologists  tend to provide shared views of clustering data albeit with noisy constraints  Because the various experts might have different
views of clustering data  it does not make sense to learn
  single consensus clustering solution  rather  we need to
discover the multiple consensus clustering solutions in the
different views 
One naive way to generate multiple clustering solutions
from multiple uncertain experts is to separately apply semisupervised clustering  Bilenko et al    Davis et al 
  Basu et al    using constraints from each expert 
The potential drawbacks of this strategy are   the clustering performance often drastically degrades in the presence
of noisy constraints due to uncertainty of one expert  and
  the resulting clustering solutions can be highly redundant due to the existence of similar expert views 
There are   few existing approaches that can combine constraints from multiple experts but they are mostly designed
to generate one clustering solution  Semicrowdsourced
clustering  SemiCrowd  combines constraints provided by
multiple experts through  ltering out uncertain pairs in the
average sample similarity matrix  Yi et al    However  this approach can only generate one clustering solution  Similar to SemiCrowd  most consensus clustering
methods are designed to generate one clustering solution
 Strehl   Ghosh    Fern   Brodley    Topchy
et al    Ghosh   Acharya    Another disadvantage is that consensus clustering methods do not work when
only   small number of pairwise constraints are available 
There are   few multiple alternative clustering approaches
that can output multiple clustering solutions  Caruana et al 
  Cui et al    Jain et al    Niu et al   
  However  all these methods are unsupervised  none
of these methods are able to utilize expert inputs 
There are   few existing crowdsourcing approaches that
learns an underlying grouping of experts  Tian   Zhu 
  Kajino et al    Moreno et al    However 
they are all designed for classi cation problems  where experts provide labels on samples queried  In our clustering
task  experts can not provide labels because how to de ne
different clusters is still unknown and yet to be discovered 
Instead  they can provide pairwise constraints by comparing sample pairs using their domain knowledge  Therefore 
these classi cationbased approaches above cannot be used
in our clustering task 

Contributions  To address this new clustering paradigm 
we build   Bayesian probabilistic model for learning multiple alternative consensus clustering views from experts 
constraints  we call Multiple Clustering Views from the
Crowd  MCVC  Multiple experts are automatically assigned to different latent views and constraints provided
by each expert is assumed to be noisy perturbations of the
clustering associated with that expert   view  Thus  multiple clustering structures can be discovered  Furthermore 
by explicitly modeling the uncertainty of each expert  experts with higher accuracies are assigned higher weights 
leading to improved quality of the learned clustering structure in each view  The clustering structure for each expert view is modeled by   discriminative clustering model
 Gomes et al    which has the advantages of   naturally introducing uncertainties in cluster assignments   
avoiding making assumptions on the generative process of
clusters  and   being able to cluster samples that do not
appear in the training set  We demonstrate that our MCVC
outperforms competing alternatives on synthetic  benchmark data  and   realworld disease subtyping problem 

  Proposed Approach
We collect data matrix     Rn    where   is the number of samples and   is the number of features  and pairwise constraints provided by   experts       where
           NULL     represents the constraints provided by the mth expert  For sample pair  xi  xj      
ij  
  means the mth expert provides mustlink  ML  constraint      
    means cannotlink  CL  constraint 
    
ij   NULL means no constraint is provided  Our objective is to utilize constraints collected from these   experts to guide the clustering algorithm to discover multiple
clustering structures in the data 

ij

  Multiple Alternative Clustering Views

We assume there exist multiple alternative expert views and
the constraints provided by experts in each view are perturbations of the clustering solution associated with that
view   Let cm represent the latent view to which the mth expert is assigned to  Furthermore     cm  are the latent
clusters for each cm view  Since we do not know the underlying number of possible expert views  we automatically
learn the number of expert views by assuming   Dirichlet

 Note that  view  in multiview clustering  Bickel   Scheffer    means coming from different sources or feature sets 
whereas   view  in our case follows the terminology in multiple alternative clusterings which means different interpretation or
point of view of the data  Multiview clustering only  nds ONE
clustering solution from multiple feature sets which are given  In
contrast  our goal is to  nd MULTIPLE clustering solutions views
which are latent 

Multiple Clustering Views from Multiple Uncertain Experts

parameters to make most of their probability densities be
far away from   and close to  

       Beta     
       Beta     

         
   
         
   

 
 

     Beta           

To automatically learn the number of expert views  we assume   Dirichlet process prior on cm and utilize the stickbreaking construction as follows  Blei et al   

  cid 
        cm   Cat 
 cid 

  

 

  cid 
       cid cmg
Therefore    cm  can be written as

  cm   

 cid  

 cid 

 cmg
   

  

  

  

where cmg     if cm     and cmg     otherwise 
We assume the prior distributions of both weight       and
offset      are factorized Gaussian distributions 

  cid 

  

  cid 
  cid 

  

         

        

     

Wij      
Wij  

     

bi     
bi   

  

  Joint Distribution

 

 

 

process  Ferguson    prior on cm 

  Model Uncertainties of Experts  Constraints

Since clustering is widely used for knowledge discovery 
experts might not be certain on the constraints they provided  To incorporate the assumption that different experts
may have different levels of expertises when providing constraints  we assume the uncerntainty of the mth expert can
be characterized by accuracy parameters         where
   represents the mth expert   sensitivity and    represents speci city  Sensitivity is de ned as the probability of
providing ML constraints for sample pairs from the same
cluster in the ground truth  Speci city is de ned as the
probability of providing CL constraints for sample pairs
from different clusters in the ground truth  We model the
conditional likelihood of      given cm  the latent view 
and    cm   the latent cluster in each view  by   Bernoulli
distribution as follows 

      
      

ij       cm 
ij       cm 

 

 

 

     cm 
 cid     cm 

 

          

          

 

 

  Discriminative Clustering

We consider the following when choosing the clustering
model    Since we need to model uncertainties of experts 
instead of generating hard clustering results  the assignments to clusters should be associated with probabilities 
  We should avoid making strong assumptions on the generative process of clusters  which can be easily violated
in practice    We should be able to cluster samples outside the training set  The discriminative clustering model
 Gomes et al    satis es all these requirements  Instead of assuming the generative process of data    discriminative clustering directly models the conditional distribution of cluster label given data  We model the conditional distribution of the latent cluster label     cm  given
weight    cm  and offset   cm  with   multiple logistic regression model 

 cid  

 

ew cm  
   ew cm   

 

xi   cm 

 

     cm 

 

       cm    cm      

 
is the kth column of    cm    Rd   and

where   cm 
  cm 
 

 

is the kth row of   cm    RK 

  Prior Distributions

We describe the prior distributions of parameters used in
our model  To incorporate the assumptions that experts 
accuracies should be far away from random guess     
       we put Beta priors on        and set their

xi   cm 

 

The respective graphical model is shown in Figure  

  cid 

  

 

The overall joint distribution of observations and latent
variables for our model is 
                                    

             cm             
 cid 

  cm 

                                     

  

  Variational Inference
Our learning objective is to maximize the marginal likelihood of observed constraints  which is intractable  Thus 
we apply variational inference  Given variational distribution        where   is the collection of latent variables
and   is their parameters 
the log of the marginal likelihood log         can be decomposed as

log           Lq      KL

  Lq   

                

 cid 

 

 cid 

Multiple Clustering Views from Multiple Uncertain Experts

  

 

  

    

     

     

cm

 

  

    
 

Figure   Overall Graphical Model

where the inequality holds due to the nonnegativity of the
the KullbackLiebler  KL  divergence  Lq    is called the
evidence lower bound  ELBO  of log        

 cid 
log              log       

 cid 

 

Lq      Eq   
In variational inference  the learning objective becomes
maximizing Lq           variational parameters  
Let                                         
where   is the number of components of the truncated
Dirichlet Process  Blei et al    We apply mean eld
and assume        can be factorized as follows 

                                     

  cid 
 cid     

  

 

  cid 

  cid 

                 cm 

  cid 

  cid 

       

 

 

       
ij  

  

  

  

  

 cid   

      

 

  cid 

  

where the marginal distribution of each random variable is

         
   
         
   

       Beta     
       Beta     
  cm    Cat   
         
       Beta     
   
    Cat   
    
Wij      
ij          
Wij  
         
bi      
 

       
       
      

bi

 

 

 
 
 
 
 

 

 

         

         

         
       

We use   to denote all the variational parameters  which
consist of      
                 
     
         
and      
bi            
Besides the simplex constraints on the parameters of the
categorical distribution  both the parameters of the Beta
distribution and the standard deviation of Gaussian distribution should have positive constraints 

bi      

        

Wij      

Wij

 

We derive the closedform formula of Lq    as   function
of   and put the detailed steps in the supplementary materials due to space constraints  Since ELBO Lq    can be
written as   function of variational parameters   we can directly maximize Lq    using gradientbased optimization
approaches  The gradient  Lq   
can be automatically
computed using reversemode differentiation  Maclaurin
et al    We choose to use   limitedmemory projected
quasiNewton algorithm  PQN  to optimize our objective
because it has both superlinear convergence rate and linear
memory requirement  Schmidt et al    Because our
objective Lq    is not concave  we provide multiple initializations   to the optimization algorithm and choose
the one resulting in the maximal objective value  We set
the number of random initializations to be   in all experiments and the results are stable across different runs 

  Experimental Results
In this section  we aim to demonstrate our MCVC can automatically   assign multiple experts to different views  and
  improve the quality of clustering solution in each view
by assigning higher weights to uncertain experts of higher
accuracies  We also analyze how   the settings of the number of clusters  and   the constraints provided by irrelevant
experts affect the performance of MCVC 

  Competing Alternatives

For aim   we construct two views of experts based on two
different ways to cluster the data  These two expert views
are treated as the ground truth of assigning multiple experts
to different views  Then we compare our MCVC against an
adapted version of meta clustering  Caruana et al   
Meta Spectral Clustering  MetaClust  The original
meta clustering approach cannot handle multiple experts 
constraints 
Instead of using the data matrix to generate
multiple clustering solutions as input  we directly use the
constraint sets provided by multiple experts as input  Meta
clustering  rst computes the similarity between experts by
computing the rand index  Rand    between the constraint sets they provide  Given the resulting similarity matrix between experts  instead of hierarchical clustering as in
the original paper  we apply spectral clustering  Ng et al 
  to assign multiple experts to different views  We determine the number of expert views by maximizing the gap
between the consecutive eigenvalues of the graph Laplacian  Von Luxburg   
For aim   we  rst construct one view of experts based
on one way to cluster the data  The underlying clustering
structure is treated as the ground truth of clustering samples  Then we compare our MCVC against the following
alternatives in generating clusters of high quality 

Multiple Clustering Views from Multiple Uncertain Experts

SemiCrowd  SemiCrowd  Yi et al    combines multiple expert constraints by  ltering out uncertain pairs in the
average similarity matrix  applying matrix completion  and
then learning   distance metric for clustering 
Semisupervised Clustering  Given   set of pairwise constraints  semisupervised clustering either learns   better
distance metric for clustering or guides the clustering algorithm to satisfy those constraints  We use Informationtheoretic Metric Learning  ITML   Davis et al   
and Metric Pairwise Constrained KMeans  MPCKMeans 
 Bilenko et al    as the representatives of those two
strategies due to their superior performances compared to
alternatives  Since semisupervised clustering can only
take one set of constraints as input  we combine constraints
from multiple experts through majority voting    sample
pair is given ML constraint if the majority of experts provide ML constraints for them and CL constraint otherwise 
Consensus Clustering  Most consensus clustering algorithms only work with cluster labels instead of pairwise
constraints  However  Clusterbased Similarity Partitioning Algorithm  CSPA   Strehl   Ghosh      consensus clustering approach that only need average similarity
matrix between samples as input  can be used in our setup 
We provide the parameter setting details for all methods in
the supplementary materials due to space constraint 

  Synthetic and Benchmark Experiments
Synthetic Dataset  To help understand the algorithms  we
generate   synthetic data that has multiple alternative clustering views  We generate   synthetic dataset containing
  samples and six features  The scatterplots between
pairwise features in this dataset are shown in Figure  
First  there exist three clusters in the subspace spanned by
the  rst two features  which we denote as    In all these
three  gures  the red  blue and green colors represent the
true cluster indicator of    Second  there exist an alternative clustering structure in the subspace spanned by the
third and fourth features  which we denote as    Note that
   and    are very distinct  Third  the subspace spanned by
the  fth and sixth feature does not contain wellseparated
clusters and we consider these as noisy features 

Figure   Scatterplots of pairwise features in the synthetic dataset 

WebKB Dataset  The WebKB dataset  web    con 

tains webpages collected from four universities  After removing stop words and extracting the top   words with
most frequent occurrences  we obtain   data matrix with
  samples and   features  The data can be clustered
according to either owner types  course  faculty  project 
student  which we treat as    or universities  Cornell 
Austin  Washington  Wisconsin  which we use as   
Face Dataset  The Face dataset  Lichman    consists of   face images of people taken with varying poses
 straight  left  right  up  Each image has   raw pixels 
We apply principal component analysis  PCA  and keep  
principal components  explaining   variance  Thus  we
obtain   data matrix with   samples and   features  The
data can be clustered based on pose     or identity    
Simulating Constraints from Multiple Experts 
For
synthetic and benchmark datasets  we do not have access
to realworld constraints provided by experts  Therefore 
we simulate these constraints  Given   groundtruth clustering solution     the number of ML constraints nM    the
number of CL constraints nCL  and accuracy parameters
of   experts           we can generate the constraints
provided by the mth expert as follows    randomly sample nM   ML constraints and nCL CL constraints from    
  randomly  ip nM          ML pairs to CL pairs and
 ip nCL        CL pairs to ML pairs 
  TASK   LEARNING THE VARIOUS LATENT

VIEWS FROM MULTIPLE EXPERTS

In this subsection  we test the performance of our MCVC
on automatically learning the latent views from multiple
experts  We simulate noisy constraints provided by multiple expert from two latent views     and    as follows 
  the  rst view consists of experts   who provide constraints based on clustering solution    and have accuracy
parameters                  
 
the second view consists of experts   who
provide constraints based on clustering solution   
and have accuracy parameters        
         
We compare the performance of our MCVC and meta spectral clustering  MetaClust  in recovering the groundtruth
expert views as the number of constraints  ncon  is varied from   to   large number that makes the performances of both approaches become stable  We repeat the
constraints generation process ten times to avoid the randomness of   single run  As   result  we obtain ten constraint sets for    xed number of constraints  Given one
constraint set  we run MCVC and MetaClust to generate
two possibly different ways to group experts  which are denoted as LMCVC and LMetaClust respectively  We measure
performance based on the normalized mutual information
 NMI   Strehl   Ghosh    between LMCVC  LMetaClust

 Feature Feature Feature Feature Feature Feature Multiple Clustering Views from Multiple Uncertain Experts

and LTrue  the groundtruth expert views  NMI measures
the similarity between two partitions  In our case  higher
NMI values indicate better performance  For    xed number of constraints  one constraint set  and one approach  we
obtain ten NMI values  We plot the mean and standard deviation for every set of ten NMI values of each approach as
we vary the number of constraints as shown in Figure  

tions  We measure performance based on NMI between the
resulting clustering solutions and the groundtruth solution
   For    xed number of constraints  one constraint set 
and one approach  we obtain   NMI values  We plot the
mean and standard deviation for every set of   NMI values
for each approach as we vary the number of constraints as
shown in Figure   To compare the performance of different approaches  we apply the KruskalWallis test  Kruskal
  Wallis    which can be used to test whether two
groups of samples are drawn from the same distribution 
on their corresponding groups of NMI values 

    Synthetic

    WebKB

    Face

Figure   Compare our MCVC against meta clustering  MetaClust  in assigning multiple experts to different views on     synthetic      WebKB and     Face datasets 

We have the following observations    On the synthetic
dataset  our MCVC can consistently recover the groundtruth expert views    On WebKB and Face  the performance of MCVC improves as the number of constraints
increases and can recover the groundtruth expert views
when the number of constraints becomes large enough   
On WebKB and Face  when the number of constraints is too
small  as is shown in the left part of each  gure  MCVC will
be dominated by the priors and therefore cannot perfectly
recover the expert views    On all three datasets  meta
clustering fails to recover the groundtruth expert views 

  TASK   DISCOVER CLUSTERING SOLUTION IN

AN EXPERT VIEW

In this subsection  we test the performance of our MCVC
against competing methods in learning the clustering structure given constraints provided by multiple experts from
one view  We simulate noisy constraints provided by multiple expert based on one groundtruth view     We
consider two different settings for their accuracy parameters    experts have unequal accuracies        
            experts have equal and high
accuracies                  
Our objective is to show that through learning different accuracies of multiple uncertain experts  our MCVC can generate better clustering results compared to competing alternatives  SemiCrowd  ITML  MPCKMeans and CSPA 
We vary the total number of ML CL constraints provided
by each expert from   to    as described in the previous subsection  For each  xed number of constraints  we
randomly generate   constraint sets  For each constraint
set  we run all approaches and obtain their clustering solu 

    Synthetic  unequal

    Synthetic  equal

    WebKB  unequal

    WebKB  equal

    Face  unequal

    Face  equal

Figure   Compare our MCVC against SemiCrowd 
ITML 
MPCKMeans  CSPA in generating better clustering solutions on
      synthetic        WebKB and       Face datasets  in the left
 gures         the accuracy parameters are set to be unequal 
                  in the right  gures
        the accuracy parameters are set to be equal and have high
values                   

From the left  gures  where the accuracy parameters are
set to be unequal  we have the following observations    
Our MCVC consistently outperforms all competing alternatives 
II  The performances of semisupervised clustering approaches  including ITML and MPCKMeans  do
not consistently improve as the number of constraints increases  III  The performance of SemiCrowd increases as
the number of constraints increases  This means it can ef 

 NumberofConstraints NormalizedMutualInformationMCVCMetaClust NumberofConstraints NormalizedMutualInformationMCVCMetaClust NumberofConstraints NormalizedMutualInformationMCVCMetaClust NumberofConstraints NormalizedMutualInformationMCVCSemiCrowdITMLMPCKMeansCSPA NumberofConstraints NormalizedMutualInformationMCVCSemiCrowdITMLMPCKMeansCSPA NumberofConstraints NormalizedMutualInformationMCVCSemiCrowdITMLMPCKMeansCSPA NumberofConstraints NormalizedMutualInformationMCVCSemiCrowdITMLMPCKMeansCSPA NumberofConstraints NormalizedMutualInformationMCVCSemiCrowdITMLMPCKMeansCSPA NumberofConstraints NormalizedMutualInformationMCVCSemiCrowdITMLMPCKMeansCSPAMultiple Clustering Views from Multiple Uncertain Experts

fectively reduce the noise in the constraints  However  it
does not work well when the number of constraints is too
small  IV  consensus clustering  CSPA  fails because the
number of constraints is too small to be used to construct
accurate sample similarity matrix 
From the right  gures  where the accuracy parameters are
set to be equal and have high values  we have the following observations     Our MCVC consistently outperforms
SemiCrowd  ITML and CSPA but does not consistently
outperform MPCKMeans 
By comparing the right against the left  gures  we have
the following observations  VI  MPCKMeans works well
only when all the experts have high accuracies  it fails when
not all experts have high accuaracies  as shown on the left
 gures  VII  Both ITML and SemiCrowd do not bene  
much from higher percentage of correct constraints  VIII 
Our MCVC perfoms well in both settings 
Explanation  Our MCVC has good performance in the unequal accuracies case because it can learn the accuracy parameters of different experts and assign higher weights to
more accurate experts and lower weights to uncertain experts respectively  On the synthetic dataset  the posterior
distributions of accuracy parameters     computed
from MCVC are shown in Figure  

MCVC  we vary   from   to   and run MCVC on the synthetic dataset using two expert views constructed from the
procedures described in subsection   The number of
constraints is  xed to be   and the constraints generation process is repeated   times  First  for any value of   
MCVC can correctly assign experts to two views and also
generate two clustering solutions  Second  to evaluate how
the clustering performances are affected by    we compute the NMI between the two output clustering solutions
and       respectively 
The errorbar plot is shown in Figure       As we can
see  our MCVC is very robust to the setting of   as long
as        where    is the maximal number of clusters among all clustering views and        for our synthetic data  In problems where    is unknown  we can set
  to be some large value to increase the possibility that
       However  in practice  we also observe that more
constraints are needed to effectively train the model when
  becomes large because there will be   larger number of
parameters 

   

   

Figure   On the synthetic dataset  the posterior distributions of
sensitivities       and those of speci cities       when
experts have unequal accuracy parameters        
         

    

In our MCVC 

As we can see  the modes of their distributions are very
close to their true values 
the constraints from the mth expert are assigned weight     
      when combining the constraints from  
log
experts  we put the derivation details in the supplementary
materials  Therefore  higher accuracies         naturally lead to higher weights  Thus  our MCVC becomes
robust to noisy constraints in the unequal accuracies case 

  SENSITIVITY ANALYSIS  TO THE NUMBER OF

CLUSTERS AND TO IRRELEVANT EXPERTS

Number of Clusters  To investigate how the setting of
   the number of clusters  affects the performance of our

   

   

   

Figure       shows the NMI between output clustering solutions
and       as   increases in the experiment studying the setting
of the number of clusters      and     show the posterior distributions of sensitivities   and speci cities   in the experiment studying the existence of irrelevant experts  green means
experts   blue means experts   red means experts  

Irrelevant Experts  We de ne irrelevant experts as those
who provide constraints based on   notion of similarity that is not supported by any feature in the data  To
demonstrate how the existence of irrelevant experts affect the performance of our MCVC  we simulate three expert views using the synthetic data    the  rst two views
are the same as those described in subsection   and
  the third view consists of  ve irrelevant experts who
provide constraints based on   random clustering solution    and have accuracy parameters        
          We generate   constraints
from each expert  First  the  rst two expert views can still
be perfectly recovered by our MCVC  Second  the irrelevant experts  experts   are either assigned to the  rst
two expert views or new expert views 
We plot the posterior distributions of accuracy parameters
for all   experts in Figure         As we can see  the

 ProbabilityDensityFunctionq         ProbabilityDensityFunctionq         NumberofClusters   NormalizedMutualInformationNMIwithY NMIwithY ProbabilityDensityFunction ProbabilityDensityFunctionMultiple Clustering Views from Multiple Uncertain Experts

densities of irrelevant experts  accuracy parameters  red
curves  concentrate around   which indicates the constraints from those experts are treated as random guesses
and assigned near zero weights by MCVC  In practice 
we can identify irrelevant experts by checking whether the
modes of their accuracy parameters  posteriors are close to
  Therefore  our MCVC is robust to irrelevant experts 

  COPD Subtyping Experiment

Chronic Obstructive Pulmonary Disease  COPD  is   complex lung disease characterized by increasing breathlessness  Although being called one disease  doctors believe
there exist different disease subtypes  The identi cation
of different disease subtypes  clusters  can lead to tailored
medical care for each patient 
Dataset  We collected   features from   COPD patients 
including clinical measurements  demographics 
lung function and measures from CT chest imaging 
Experts  Constraints  We also collected constraints provided by   experts  including clinicians and radiologists 
We need to utilize experts  constraints to guide the clustering algorithm  However  the key challenge is that different
experts disagree on whether to put   pair of patients in the
same cluster  We suspect there exist different expert views
and experts in each view provide constraints based on  
shared way to cluster the data  The discovery of these expert views and their corresponding clustering solutions can
provide more options for further investigation 
Evaluation  Since ground truth is not known  we can no
longer use NMI to compare the performances of competing
approaches  However  there are some key genetic variables
that are known to be related to COPD  including copdScore
 Busch et al    HHIP  Pillai et al    MMP 
 Cho et al      clustering solution is considered as
useful relevant if patients in different clusters show signi 
cant differences on these genetic variables 
We randomly split the dataset into half training set and half
testing set  All approaches are learned using the training
set and the constraints from multiple experts  The learned
models can be used to cluster test samples and generate
clustering solutions  To evaluate each solution  we compute
its associated pvalues on these genetic variables by applying KruskalWallis test on copdScore  continuous  and  
test on HHIP and MMP   discrete  We use       to
identify signi cant differences in these genetic variables 
Methods and Results  We  rst apply our MCVC and
obtain   expert views  After removing irrelevant experts
and views containing only one expert  there are   expert
views left  Our physician collaborators identify   interesting expert views by analyzing the cluster characteristics of their associated clustering solutions  which are de 

noted as MCVCA and MCVCB respectively    Solution MCVCA contains emphysemadominant and airwaydominant clusters  where emphysema cluster means the destruction of lung tissue and airway cluster means the increase of airway wall thickness    Solution MCVCB contains clusters of different levels of disease severity 
For competing approaches  we  rst run meta clustering and
all   experts were lumped into one view  Then we apply
SemiCrowd  ITML and MPCKMeans to combine the data
matrix and constraints from all experts 

Table   pvalues on three key COPDrelated genetic variables 

Solutions
MCVCA
MCVCB
SemiCrowd
ITML
MPCKMeans

copdScore HHIP
   
   
   
   
   
   
   
   
   
   

MMP 
   
   
   
   
   

The pvalues of solutions provided by our MCVC and competing approaches are shown in Table   As we can see 
solutions provided by MCVC and MPCKMeans contain
different clusters that show signi cant differences on all
three COPDrelated genetic variables 
In contrast  both
ITML and SemiCrowd are not signi cantly correlated with
all three genetic variables  There   some overlap between
solution MPCKMeans and solution MCVCB  with NMI
value   However  solution MCVCA can only be discovered by our approach  This way of clustering COPD
patients is consistent with some COPD investigators  latest
discovery of COPD subtypes  Castaldi et al   

  Conclusions
In this paper  we build   probabilistic model to discover
multiple ways to cluster the data given potentially diverse
inputs from multiple uncertain experts  This is achieved by
automatically assigning multiple experts to different views
and learning the clustering structure associated with each
expert view  The quality of clustering solution in each expert view are improved by assigning higher weights to experts of higher accuracies  Experimental results on synthetic data  benchmark datasets and   realworld disease
subtyping problem demonstrate that our MCVC outperforms its competing alternatives  including meta clustering 
semisupervised clustering  semicrowdsourced clustering
and consensus clustering 

  Acknowledgements
We would like to acknowledge support for this project
from the NIH grant NIH NHLBI RO HL 
RO HL  and NSF IIS 

Multiple Clustering Views from Multiple Uncertain Experts

References
Webkb dataset    URL http www cs cmu 
edu afs cs project theo www data 

Basu  Sugato  Davidson  Ian  and Wagstaff  Kiri  Constrained clustering  Advances in algorithms  theory  and
applications  CRC Press   

Bickel  Steffen and Scheffer  Tobias  Multiview clustering  In IEEE International Conference on Data Mining 
volume   pp     

Bilenko  Mikhail  Basu  Sugato  and Mooney  Raymond   
Integrating constraints and metric learning in semisupervised clustering  In Proceedings of the Twenty rst
International Conference on Machine Learning  pp   
 

Blei  David    Jordan  Michael    et al  Variational inference for dirichlet process mixtures  Bayesian Analysis 
   

Busch  Robert  Hobbs  Brian    Zhou  Jin  Castaldi 
Peter    McGeachie  Michael    Hardin  Megan   
Hawrylkiewicz  Iwona  Sliwinski  Pawel  Yim  JaeJoon 
Kim  Woo Jin  et al  Genetic association and risk scores
in   copd metaanalysis of   subjects  American Journal of Respiratory Cell and Molecular Biology 
 

Caruana  Rich  Elhawary  Mohamed  Nguyen  Nam  and
Smith  Casey  Meta clustering  In IEEE International
Conference on Data Mining  pp     

Castaldi  Peter    Dy  Jennifer  Ross  James  Chang  Yale 
Washko  George    CurranEverett  Douglas  Williams 
Andre  Lynch  David    Make  Barry    Crapo  James   
et al  Cluster analysis in the copdgene study identi es
subtypes of smokers with distinct patterns of airway disease and emphysema  Thorax   

Cho  Michael    McDonald  MerryLynn    Zhou  Xiaobo 
Mattheisen  Manuel  Castaldi  Peter    Hersh  Craig   
DeMeo  Dawn    Sylvia  Jody    Ziniti  John  Laird 
Nan    et al  Risk loci for chronic obstructive pulmonary disease    genomewide association study and
metaanalysis  The Lancet Respiratory Medicine   
   

Cui  Ying  Fern  Xiaoli    and Dy  Jennifer    Nonredundant multiview clustering via orthogonalization 
In IEEE International Conference on Data Mining  pp 
   

Ferguson  Thomas      bayesian analysis of some nonparametric problems  The Annals of Statistics  pp   
   

Fern  Xiaoli Zhang and Brodley  Carla    Solving cluster
In
ensemble problems by bipartite graph partitioning 
Proceedings of the Twenty rst International Conference
on Machine learning  pp     

Ghosh  Joydeep and Acharya  Ayan  Cluster ensembles  Wiley Interdisciplinary Reviews  Data Mining and
Knowledge Discovery     

Gomes  Ryan    Krause  Andreas  and Perona  Pietro  Discriminative clustering by regularized information maximization  In Advances in Neural Information Processing
Systems  pp     

Howe  Jeff  Crowdsourcing  How the power of the crowd
is driving the future of business  Random House   

Jain  Anil    Murty    Narasimha  and Flynn  Patrick   
Data clustering    review  ACM Computing Surveys   
   

Jain  Prateek  Meka  Raghu  and Dhillon  Inderjit    Simultaneous unsupervised learning of disparate clusterings  Statistical Analysis and Data Mining   
   

Kajino  Hiroshi  Tsuboi  Yuta  and Kashima  Hisashi  Clustering crowds  In AAAI Conference on Arti cial Intelligence   

Kruskal  William   and Wallis    Allen  Use of ranks in
onecriterion variance analysis  Journal of the American
Statistical Association     

Lichman     UCI machine learning repository    URL

http archive ics uci edu ml 

Maclaurin     Duvenaud     Johnson     and Adams 
RP  Autograd  Reversemode differentiation of native
python  http github  com HIPS autograd   

Moreno  Pablo    Art esRodr guez  Antonio  Teh 
Yee Whye  and PerezCruz  Fernando  Bayesian nonparametric crowdsourcing  Journal of Machine Learning
Research     

Murphy  Sherry    Xu  Jiaquan  and Kochanek  Kenneth   
Deaths   nal data for   National vital statistics reports  from the Centers for Disease Control and Prevention  National Center for Health Statistics  National
Vital Statistics System     

Davis  Jason    Kulis  Brian  Jain  Prateek  Sra  Suvrit  and
Dhillon  Inderjit    Informationtheoretic metric learning  In Proceedings of the TwentyFourth International
Conference on Machine Learning  pp     

Ng  Andrew    Jordan  Michael    Weiss  Yair  et al  On
spectral clustering  Analysis and an algorithm  In Advances in Neural Information Processing Systems  volume   pp     

Multiple Clustering Views from Multiple Uncertain Experts

Zhang  Junjun  Baran  Joachim  Cros  Anthony  Guberman  Jonathan    Haider  Syed  Hsu  Jack  Liang  Yong 
Rivkin  Elena  Wang  Jianxin  Whitty  Brett  et al 
International cancer genome consortium data portala onestop shop for cancer genomics data  Database   
 

Niu  Donglin  Dy  Jennifer    and Jordan  Michael    Multiple nonredundant spectral clustering views  In Proceedings of the Twentyseventh International Conference on
Machine Learning  pp     

Niu  Donglin  Dy  Jennifer    and Ghahramani  Zoubin 
  nonparametric bayesian model for multiple clustering
In Proceedings of the
with overlapping feature views 
Fifteenth International Conference on Arti cial Intelligence and Statistics  pp     

Pillai  Sreekumar    Ge  Dongliang  Zhu  Guohua  Kong 
Xiangyang  Shianna  Kevin    Need  Anna    Feng 
Sheng  Hersh  Craig    Bakke  Per  Gulsvik  Amund 
et al    genomewide association study in chronic obstructive pulmonary disease  copd  identi cation of two
major susceptibility loci  PLoS Genetics     

Rand  William    Objective criteria for the evaluation of
clustering methods  Journal of the American Statistical
Association     

Schmidt  Mark  Berg  Ewout  Friedlander  Michael  and
Murphy  Kevin  Optimizing costly functions with simple
constraints    limitedmemory projected quasinewton
algorithm  In Proceedings of the Twelfth International
Conference on Arti cial Intelligence and Statistics  pp 
   

Strehl  Alexander and Ghosh 

Cluster
ensembles   knowledge reuse framework for combining multiple partitions  Journal of Machine Learning
Research     

Joydeep 

Tian  Yuandong and Zhu  Jun  Learning from crowds in
the presence of schools of thought  In Proceedings of the
Eighteenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining  pp   
 

Topchy  Alexander  Jain  Anil    and Punch  William 
Clustering ensembles  Models of consensus and weak
partitions  IEEE Transactions on Pattern Analysis and
Machine Intelligence     

Von Luxburg  Ulrike    tutorial on spectral clustering 

Statistics and Computing     

Wagstaff  Kiri and Cardie  Claire  Clustering with instancelevel constraints  AAAI Conference on Arti cial Intelligence     

Yi  Jinfeng  Jin  Rong  Jain  Shaili  Yang  Tianbao  and
Jain  Anil    Semicrowdsourced clustering  Generalizing crowd labeling by robust distance metric learning 
In Advances in Neural Information Processing Systems 
pp     

