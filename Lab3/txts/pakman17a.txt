Stochastic Bouncy Particle Sampler

Ari Pakman     Dar Gilboa     David Carlson   Liam Paninski  

Abstract

We introduce   stochastic version of the nonreversible  rejectionfree Bouncy Particle Sampler  BPS    Markov process whose sample trajectories are piecewise linear  to ef ciently sample Bayesian posteriors in big datasets  We prove
that in the BPS no bias is introduced by noisy
evaluations of the loglikelihood gradient  On the
other hand  we argue that ef ciency considerations favor   small  controllable bias  in exchange
for faster mixing  We introduce   simple method
that controls this tradeoff  We illustrate these
ideas in several examples which outperform previous approaches 

  Introduction
The advent of the Big Data era presents special challenges to practitioners of Bayesian modeling because typical samplingbased inference methods have   computational cost per sample linear in the size of the dataset  This
computational burden has been addressed in recent years
through two major approaches  see  Bardenet et al   
for   recent overview      split the data into batches and
combine posterior samples obtained in parallel from each
batch  or  ii  use variants of the Markov Chain Monte Carlo
 MCMC  algorithm that only query   subset of the data at
every iteration  Our interest in the paper is in the latter approach  where many methods are based on modifying both
steps of the MetropolisHastings  MH  algorithm  in the
proposal step  only   minibatch of the data is used  and the
acceptreject step is either ignored or approximated  Korattikara et al    Bardenet et al    This strategy has
been explored using proposals from Langevin  Welling  
Teh    Riemannian Langevin  Patterson   Teh   
Hamiltonian  Chen et al    and Riemannian Hamiltonian  Ma et al    dynamics  Other relevant works

 Equal contribution

 Statistics Department and Grossman
Center for the Statistics of Mind  Columbia University  New York 
NY   USA  Duke University  Durham  NC   USA 
Correspondence to  Ari Pakman  aripakman gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

include  Ahn et al    Ding et al   
Despite the success of the above approach 
the partial
acceptreject step is   source of bias  the precise size of
which is dif cult to control  and which tends to be ampli 
 ed by the noisy evaluation of the gradient  This has motivated the search for unbiased stochastic samplers  such
as the Fire   MCMC algorithm  Maclaurin   Adams 
  the debiased pseudolikelihood approach of  Quiroz
et al    and the quasistationary distribution approach
of  Pollock et al   
The present work is motivated by the idea that the bias
could be reduced by starting from   rejectionfree MCMC
algorithm  avoiding thus the MetropolisHastings algorithm altogether  Two similar algorithms of this type
have been recently proposed 
the Bouncy Particle Sampler  BPS   Peters   de With    BouchardC ot   et al 
  and ZigZag Monte Carlo  Bierkens   Roberts 
  Bierkens et al    These algorithms sample
from the target distribution through nonreversible  piecewise linear Markov processes  Nonreversibility       the
failure to satisfy detailed balance  has been shown in many
cases to yield faster mixing rates  Neal    Vucelja 
  BouchardC ot   et al   
Our contributions in this paper are twofold  Firstly  we
show that the BPS algorithm is particularly well suited to
sample from posterior distributions of big datasets  because
the target distribution is invariant under zeromean noisy
perturbations of the loglikelihood gradient  such as those
introduced by using minibatches of the full dataset in each
iteration 
Stochastic variants of BPS or ZigZag that preserve exactly the target distribution have been proposed  such as
Local BPS  BouchardC ot   et al    or ZigZag with
subsampling  ZZSS   Bierkens et al    but they lead
to extremely slow mixing because are based on overly
conservative bounds  which moreover must be derived on
  caseby case basis  and in many cases may not hold
at all  This leads us to our second contribution 
the
Stochastic Bouncy Particle Sampler  SBPS    stochastic
version of the BPS algorithm which trades   small amount
of bias for signi cantly reduced variance  yielding superior performance  and requiring no parameter tuning or
derivation of problemspeci   bounds  compared to exist 

Stochastic Bouncy Particle Sampler

Algorithm   Bouncy Particle Sampler

Initialize particle position      RD and velocity
    SD 
while desired do

Sample Poisson process  rst arrivals tr  tb
with rates                       vt 
Let     min tb  tr 
Move wt        vt  
if tb   tr then

Re ect                 wt    wt 
    wt 
Refresh  sample     Unif SD 

else

end if
Let      wt

end while
RETURN piecewise linear trajectory of  

ing subsamplingbased Monte Carlo methods  SBPS inherits the piecewise linear sample paths of BPS  and therefore
enjoys faster convergence of empirical means  particularly
of rapidly varying test functions  compared to more standard approaches 
We organize this paper as follows  In Section   we review
the Bouncy Particle Sampler  in Section   we study the invariance of the target distribution under noise perturbations
to the BPS updates  in Section   we introduce SBPS  and in
Section     preconditioned variant  In Section   we discuss
related works and in Section   we illustrate the advantages
of SBPS in several examples 

  The Bouncy Particle Sampler
Consider   distribution                      RD  where
the normalization factor may be intractable  The Bouncy
Particle Sampler  BPS  proposed in  Peters   de With 
  Monmarch      and formalized and developed
in  BouchardC ot   et al    introduces   random velocity vector   distributed uniformly in the unit sphere SD 
and de nes   continuous Markov process in        To describe this process we begin in discrete time and then take
the continuoustime limit  Denoting time by    consider
  discretetime Markov process that acts on the variables
       as

 cid             prob           

        vr    prob       

           

where

      max       
                
vr                      
 

      

 

 
 
 

Note that   in   is the directional derivative of       in
the direction    and vr is   re ection of   with respect
to the plane perpendicular to the gradient     satisfying
vr                and  vr       
In other words 
the particle   moves along   straight line in the direction
of   and this direction is re ected as   with probability
      This probability is nonzero only if the particle is
moving in   direction of lower target probability      or
equivalently higher potential      
Applying the transition   repeatedly and taking       
the random re ection point becomes an event in an inhomogeneous Poisson process with intensity     The resulting sampling procedure generates   piecewise linear
Markov process  Davis    Dufour et al    and is
summarized in Algorithm   Note that the algorithm also
includes occasional resamplings of    to ensure ergodicity  BouchardC ot   et al    Remarkably  in the limit
       the algorithm leaves the joint factorized distribution          invariant  as we review in Supp  Material   
The ZigZag process  Bierkens   Roberts    Bierkens
et al    is similar to BPS  but velocity components can
take only   values  and the piecewise linear trajectories
change direction only in   single coordinate at each random
breakpoint  For   review of these methods  see  Fearnhead
et al    Bierkens et al   

  Noise Resilience and Big Data
  Noise Resilience

Let us assume that only   noisy version of the gradient is
available to compute the probability of bouncing and the
re ected velocity in   In the Big Data scenario described
below  this is the result of using   random subset of the data
at each gradient evaluation  and can be represented as
                    nw  
nw     nw     
where nw   RD and   nw    has zero mean 

 

Theorem   The invariance of         under the BPS algorithm is unaffected by the zeromean noise   if nw  and
nw  are independent for     cid    

See Supp  Material    for   proof sketch  De ning     
             the intensity of the inhomogeneous Poisson
process       which determines the time of the velocity
bounce  now becomes stochastic  and the resulting point
process is called   doubly stochastic  or Cox  process  Cox 
  Grandell    The effect of the gradient noise
is to increase the average point process intensity  since
  from Jensen   inequality  This
 

 cid     cid 

      

 cid 

     

 cid 

 

Stochastic Bouncy Particle Sampler

Figure   Noisy vs  noiseless gradients in BPS  Above  Contour plot of the    density considered and sample BPS trajectories during
  bounces  with exact and noisy gradients  The noise was sampled in each component from         distribution  Below  Left 
smoothed histogram of travel times until bouncing  with shorter times for noisy gradients  Middle  QQplot of one of the coordinates 
showing that the invariant distribution is not changed by the noise  Right  ACFs of one of the coordinates  with slower mixing per
iteration in the noisy case  However  note that these ACF plots do not account for computational cost per iteration 

leads to more frequent bounces and typically   slower mixing of the Markov process  as illustrated in Figure  
Many Cox processes are based on Poisson intensities
obeying stochastic differential equations  or assume that
the joint distribution at several     has   nontrivial wdependent structure  Our case is different because we assume that nw  and nw  are independent even when    and
   are in nitesimally close 

  Sampling from Big Data posteriors

In   prototypical Bayesian setting  we have   prior      
       data points xi  and the negative logposterior gradient
is

          

log        

log   xi   

 

 

When   is big we consider replacing the above gradient
by the noisy approximation

            

log          
 

log   xri   

 

 

where    cid    and the   indices  ri  are sampled randomly without replacement  To sample from the posterior
using the noisy gradient   we want to simulate the  rst arrival time in   doubly stochastic Poisson process with random intensity         where

                      vt   

 

Note that    is   stochastic process  and noise independence
for different     implies that different     require independent minibatches  Out of several methods to sample from

 cid 

 cid 

 cid 

 cid 

  cid 

  

  cid 

  

 noisy  Poisson processes  the thinning method  Lewis  
Shedler    is compatible with the noise independence
assumption  This is   form of rejection sampling which
proposes    rst arrival time    sampled from an inhomogeneous Poisson process with intensity     such that
              The particle moves   distance tv  and accepts the proposal to bounce the velocity with probability
          Note that this acceptreject step is different
from the MH algorithm  Robert   Casella    since
the particle always moves the distance tv  and   rejection
only affects the velocity bouncing  This can greatly improve the ef ciency of the sampler  As in the noiseless
case  one should in general also resample   occasionally  to
ensure ergodicity  BouchardC ot   et al    although in
the examples we considered this was not empirically necessary  since the minibatch noise serves to randomize the
velocity suf ciently  preventing  nonergodic  trajectories
that do not explore the full space 
In some special cases one can derive   bound     that always holds  BouchardC ot   et al    Bierkens et al 
  But this is atypical  due to the dependence of      
in   on the changing velocity   and the minibatch noise 
Even when such bounds do exist  they tend to be conservatively high  leading to an inef cient sampler with many
rejected proposals  wasting many minibatches of data  before accepting 
Instead  we propose below an adaptive approximate bound
which achieves   biasvariance tradeoff between the frequency of the bounce proposals and   controllable probability of bound violation 

Density Contours Exact Gradient Noisy Gradient Time until bouncing FrequencyExact GradientNoisy Gradient Exact Gradient Noisy GradientQQplot of   coordinate Lag ACFExact GradientNoisy GradientStochastic Bouncy Particle Sampler

  Proposal from Local Regression
Our approach to an adaptive and tractable proposal intensity     relies on   predictive model of    based on previous observations  the key idea is to exploit the correlations
between nearby    values  The upper value of the resulting predictive con dence band can then be used as    
and this band is adaptively updated as more proposals are
generated 
While there are many possibilities for such   predictive
model  we found that   simple local linear model was very
effective and computationally trivial  Consider then the linear regression of   observed values  Gi      ti  since the
previous bounce 

 Gi    ti        ti

 ti         

ti

   

 

where                 and the noise variance can be estimated from the minibatch in   as

       
  

       

   Vari        log   xri     

 

Here Vari denotes the sample variance of the minibatch 
and we included the  nite population correction factor
     
    because the indices  ri  are sampled without replacement  The Gaussian noise assumption in       in   is
valid when the minibatch is suf ciently large that we can
appeal to   central limit theorem   For heavytailed noise
we could consider more robust estimators  but we do not
pursue this direction here 
Adding   Gaussian prior       to   and de ning xi  
  ti  the log posterior of          is

  log   ti   Gi    

ti

        cid 

   Gi xi 

  
ti

  

   

    const 

Let   and   be the mean and covariance of this distribution  Using these estimates  we obtain the predictive distribution       for       for     tm 

                   

            

where         xT     
tm

 

 

with          Note that as usual the noise variance is
different in   and   since in   we are  tting observed
pairs  Gi  ti  while in   we are predicting the value of
      and we include the uncertainty from the   estimates 
Also  for simplicity we extrapolate the observation noise to
be the same as in the last minibatch    
We can now construct   tractable approximate thinning proposal intensity by choosing   con dence band multiple   

tm 

Figure   Thinning proposal intensity for bounce times from  
linear regression predictive con dence interval applied to   twodimensional logistic regression posterior with          
  Left  Starting from       the piecewise linear intensity    
is used to propose bounce times  green points  As these proposals
are rejected additional observations  Gi are made until   proposal
is accepted  red point  The decrease in the slope of     indicates
the decreasing uncertainty in the estimated regression parameters
as observations increase  note that the linear approximation for the
true      is quite accurate here  Note also the reduced observation
frequency at lower values of      indicating more ef cient sampling than is achievable with the constant and much higher bounds
used in  BouchardC ot   et al    Bierkens et al    which
were in the range         for this data  Right  The corresponding SBPS particle trajectory  with arrows indicating the
initial velocity and the velocity after the bounce  The contours
show the Laplace approximation of the log posterior 

and de ning     as   linear interpolation between selected
points along the nonlinear curve

               

 

The proposal intensity is now           and sampling from an inhomogeneous Poisson process with piecewise linear rate     can be done analytically using the inverse CDF method  When   bounce time is proposed at
time    the particle moves   distance tv    noisy observation       is made as in   and the bounce time is accepted
with probability min           
If the bounce is
accepted  the velocity is re ected as in    using    instead of    and the set of observed values is reinitialized
with         ct  which are the values one would obtain
from sampling the same minibatch after the bounce  since
vr                          On the other hand  if the proposal is rejected  the observed         ct  are added to the
set of observed values  The hyperparameters     of the
regression model can be learned by performing  after each
bounce    gradient ascent step on the marginal likelihood 
    Gi    this gradient can be computed analytically
and does not signi cantly impact the computational cost 
The linear model for    is good when the target distribution can be locally approximated by   Gaussian  since      
in   is   projection of the derivative of the negative log

Stochastic Bouncy Particle Sampler

posterior  When the posterior is highly nonGaussian    decaying weight can be used for moredistant observations 
leading to   local regression  the scale of this decay can
be    again via stochastic gradient ascent on the predictive
likelihood  We have also explored   Gaussian Process regression model  but it did not improve over the linear model
in the cases we considered  In Supp  Material   we discuss
  potential problem with our approach in the case of multimodal distributions  and propose   solution for such cases 
Finally  note that the directional derivative of        needed
in   can in many cases be computed at   cheaper cost
 by   factor of     dim    than the full gradient  The
latter is only needed when   bounce is accepted  This is in
contrast to other gradient based samplers which require the
full gradient at every step 
We dub this approach to BPS with noisy gradients Stochastic BPS  SBPS  See Supp  Material   for pseudocode  Figure   illustrates the evolution of these dynamic proposal intensities in   simple example  In Section   we consider  
variant to SBPS  called pSBPS  that learns   diagonal preconditioning factor for the gradient  and leads to   more ef 
 cient exploration of the space when the posterior is highly
anisotropic and roughly axisaligned 

  Bias in the Samples

The constant   in   controls the tradeoff between bias
from possible               cases and lower computational cost  higher   leads to   more conservative
 higher  proposal intensity and therefore   lessbiased but
more datainef cient sampler  We present   bound on the
Wasserstein distance between the exact and bias distributions in Supp  Material    and explore this biasvariance
tradeoff further in Supp  Material      quick bias diagnostic is the rate at which the bound is violated       cases with
              if this rate is signi cantly higher than
expected under the local linear regression model  then  
different approach should be considered 

  Preconditioned SBPS
Consider now the linear transformation     Az with an
arbitrary square matrix      distribution      of interest
can be expressed in terms of   as

pz   dz         dw     Az   dz  

  exp Uz   dz  

 
 

The SBPS algorithm can be applied to the density pz   
using the gradients of       For this note that  zUz     
  wU     The Poisson intensity to compute bounces is
    with                 and the velocity re ection

Figure   Effect of diagonal preconditioning on SBPS performance  Sampling is from the logistic regression posterior as described in Section     with                        
The preconditioner parameters are             Left 
Contour plots of posterior log likelihood under the Laplace approximation  Center  right  ACF and trajectories in the direction
of greatest covariance 

is computed as

vr        

                   

        

 

 

The piecewise linear trajectory zt     vt becomes wt  
     Avt  The matrix   is called   preconditioner in the
optimization literature  but can also be used in   sampling
context to reduce anisotropy of posterior distributions  it
is often the case that   good preconditioner is not known
in advance but is instead learned adaptively  Duchi et al 
 
We use   diagonal preconditioner for simplicity  Denoting
the ith component at the jth evaluation of the gradient by
gj
    we de ne

 

 

 
 

aj
     gj
 aj    
 

 cid  
           aj 
 cid 

 
aj
   

  

 

 cid 

for some              cid    The preconditioner at iteration
  is de ned as Aj   Diag
  This is the same

 aj 
aj
   

preconditioner used in  Li et al    up to the  aj factor 
the latter is needed here in order to prevent scaling of    
As noted in  Li et al      time dependent preconditioner requires adding   term proportional to  Aj
   to the
gradient  yet this term is negligibly small and can be ignored when       since in this parameter regime the preconditioner changes slowly as   function of   and thus of   
We call
It performs favorably compared to SBPS when the posterior
is anisotropic and axisaligned  since we use   diagonal approximation of the Hessian in the preconditioner 
See  Bierkens et al    for   related approach  As Figure   shows  pSBPS converges to the posterior mode faster
than SBPS  and mixes faster in the direction of greatest covariance 

this preconditioned variant pSBPS 

 pSBPS code at https github com dargilboa SBPSpublic 

Stochastic Bouncy Particle Sampler

Figure   Logistic regression posterior sampling  with                          best seen in color  Top Left  Negative
Log Likelihood  NLL  per data point of samples from SBPS compared with SGLD  step sizes     mSGNHT  step size  
lipSBPS and SSZZ  see text for de nitions and references  all initialized at the same random positions  Also shown are the normalized
NLL of the MAP estimator   LL       and the mean   std  dev  of the Laplace approximation NLL  distributed as
        
  LL        The continuous samplers  SBPS  SSZZ  lipSBPS  were run to match the data cost of the discrete  SGLD  mSGNHT 
and for their ACFs we discretized the continuous paths uniformly to obtain the same number of samples  Note that SBPS is the fastest
to converge  Center Right  Trajectories and ACFs in the directions of largest and smallest eigenvalues of the Laplace approximation
inverse Hessian  The ACFs were calculated after burnin  while the trajectory plots only show initial convergence  Inset  CPU runtime
for   epochs  showing     advantage of       SBPS over       SSZZ and lipSBPS

 

  Related Works
Biased Samplers  Many stochastic gradient samplers      
 Welling   Teh    can be formulated exactly using
  Wiener process  Ma et al    but they are biased because     the Gaussian assumption in the noise may not hold
for small minibatches  and  ii  the MH correction to the
time discretization is avoided or approximated  Recently 
irreversible samplers have been studied in this context  Ma
et al    Choosing the step size in these samplers can
be quite challenging  as discussed below 
toolarge step
sizes increase the bias  while toosmall step sizes slow the
mixing  and in generic highdimensional examples there
is no way to automatically tune the step size  though see
 Giles et al    for recent progress 
In contrast  the
bias in SBPS  controlled by the constant    does not come
from time discretization  but from easyto track violations
of the thinning bound when              
Exact nonBPS like Samplers  Fire   MCMC  Maclaurin   Adams    augments the target distribution with
one binary variable per data point  and yields unbiased
samples while only querying   subset of data points at each
iteration  But it needs distributiondependent lower bounds
on the likelihood and requires an initial full sweep of the

data  Also mixing can be extremely slow  Quiroz et al 
  Bardenet et al    and all the dataset must be
available for access all the time 
Two recent novel proposals are  Quiroz et al    based
on debiased pseudolikelihood combined with variance reduction techniques  and  Pollock et al    based on
quasistationary distributions  These methods are relatively
more complex  and we have not yet systematically compared them against SBPS 
Exact BPSlike Samplers  Two subsampling variants
of BPS which preserve the exact distribution are Local BPS  BouchardC ot   et al    that needs   preprocessing step of computational cost     log     and
ZZSS  Bierkens et al   
In these approaches  the
requirement to preserve the distribution exactly leads to extremely conservative thinning bounds  which in turn yield
  very slow exploration of the space  as we will see below 
Also  the bounds need to be rederived for each new model
 if possible at all  unlike SBPS which can be used for any
differentiable posterior distribution 

Stochastic Bouncy Particle Sampler

  Experiments
  Logistic Regression

Although simpler MCMC methods perform well
in
Bayesian logistic regression  BLR  models  Chopin  
Ridgway    we begin with this wellunderstood case
for comparing SBPS against   few of the existing stochastic MCMC methods discussed in the previous section  To
generate the data  we sampled the components of the true
    Rd from Unif    and   data points  xi  from
  ddimensional zeromean Gaussian  with one component
of the diagonal covariance set to   and all the rest to   Labels  yi  are drawn from yi   Bern     xi  where
          ex  In the regime    cid    the Laplace approximation holds fairly well  providing another good comparison method  Figure   shows results for          
             
We run comparisons against the biased stochastic samplers
Stochastic Gradient Langevin Dynamics  SGLD   Welling
  Teh    and multivariate Stochastic Gradient NoseHoover Thermostat  mSGNHT   Li et al    with  xed
step sizes  As noted above  choosing optimal step sizes for
these samplers is challenging  To allow SGLD and mSGNHT to perform best  we performed   scan to  nd the
largest  fastestmixing  step size that did not lead to overly
large bias compared to the Laplace approximation   Note 
importantly  that this scan is expensive and is not possible
in highdimensional examples where the Laplace approximation does not hold   precisely the cases where MCMC
methods are most valuable  See Supp  Material   for details of this scan  which led to an optimal step size of  
for SGLD  Larger step sizes led to visible biases in the samples  not shown  we also show the results with step size
  for comparison to note that the results do depend sensitively on this parameter 
We also compare against ZZSS  Instead of Local BPS  we
ran comparisons against an unbiased method we call lipSBPS  short for Lipshitz BPS  where the velocity bounces
occur as  rst arrival events in   Poisson process with noisy
intensity               built from   noisy gradient   of
minimal size       and simulated with thinning using
an exact upper bound derived in Supp  Material    One
can verify that the resulting stochastic process is identical to that of Local BPS  Our bound is higher than that
used in  BouchardC ot   et al    by up to   factor of  
which results in up to twice as many bounce proposals  On
the other hand  our bound can be computed in       time 
does not require nonnegative covariates  and can be used
also for       Again  we note that this lipSBPS method 
like Local BPS and ZZSS  are not generally applicable because the derived bounds only apply in special cases 
The results of Figure   show that SBPS outperforms the op 

Figure   Estimated mean of         sin           under
continuous and discrete samples  with different ratios      where
      is the average linear trajectory length  The posterior
distribution and settings are as in Figure   Assuming the Laplace
approximation holds  the expectation of   is   Left  For        
there is little difference between continuous or discrete samples 
Center  For         the continuous mean converges faster
than the discrete  Right  Expectation of the absolute value of the
test function averaged over   runs of   epochs  as   function
of      The advantage of the continuous expectation when this
ratio is      cid    is evident 

timally tuned SGLD and mSGNHT  and converges orders
of magnitude faster than lipSBPS and ZZSS  While the latter two methods are unbiased  our results suggest that the
small bias introduced by SBPS is worth the massive reduction in variance 
In Supp  Material   we explore the effects of the hyperparameters        and   refresh rate     The conclusion is
that in this logistic example no manual hyperparameter tuning was required  in stark contrast to the careful step size
tuning required for SGLD  the biascontrolling constant  
can be set in the range          consistent with the tails of
the Gaussian in the linear regression model  and the minibatch size   should be small  but large enough for the CLT
to justify the noise term in         worked well  but
the results were not sensitively dependent on    For small
values of   the minibatch variability provided suf cient
velocity randomness that no additional velocity refreshes
were necessary  so we did not have to tune    either 
The comparison to pSBPS shows an improvement in the
rate of convergence to the posterior mode  The MAP estimator    was calculated using SAG  Roux et al   
and the Hessian was computed exactly 

  Continuous Trajectory Sampling

  unique feature of BPSlike samplers is that their output is
  continuous trajectory  Given    and   set of   velocities
and bounce times  vi  ti  the estimated expectation of  
test function       is

ti cid 
  cid 

  

 

 cid      cid BP      
 

   wi   vit dt

 

where wi    wi   viti and   is the total particle
travel time  For simple test functions this integral is analytic  while more generally it can be computed numerically

Stochastic Bouncy Particle Sampler

Figure   Neural network posterior sampling for   single hidden layer network trained on MNIST                    For
SBPS       The posterior is compared to an expensive MetropolisHastings run  SBPS shows comparable mixing to an appropriately
chosen SGLD without the need for   scan over step sizes  As can be seen    poor choice of SGLD step size can lead to slow mixing or
bias in the narrow directions of the target

with standard ef cient onedimensional quadrature methods  When       varies across   characteristic length  
shorter than the average trajectory length   of the linear segments  we intuitively expect the error in the estimate  
to be smaller than in estimators based on discrete samples 
Note that this advantage tends to diminish for higher SBPS
noise  since the linear segments become shorter 
Figure   explores empirically this idea in   simple setting by comparing the value of the expectation of        
sin           under the posterior distribution of the logistic example considered above  Here         are the  rst
coordinates of the vectors            is the MAP value  and
  the characteristic length of    As expected  the error in
the expectation is lower in the continuous case for        

  Neural Network Posterior Sampling

  cid 

  

  

  

log pi     
 

ization term         cid 

We considered   simple model of one hidden layer followed by   softmax  For Bayesian approaches to neural
networks see  Neal    Gal    The likelihood was
the standard cross entropy with an additional    regularj where pi is the
probability of classifying the ith example correctly    was
approximated via subsampling  and       This architecture was trained on the MNIST dataset    subset of the
training set was preprocessed by downsampling the images
to       removing pixels that are   for all training examples and decreasing the number of digits to   The resulting
training set size was       The resulting dimensionality of the posterior was       Minibatch size was
      for all methods  All weights were initialized at
  and all methods were run for   epochs  SBPS is compared with SGLD at different step sizes  and performance

is comparable to SGLD with an appropriate step size without requiring an expensive scan over step sizes  Since the
additional regularization term can lead to unbounded gradients of the log posterior        one can no longer use the
bounds derived for the Local BPS and ZZSS algorithms
and thus they cannot be applied to this problem without
further work  This is not the case for SBPS  The posterior
is not Gaussian due to the likelihood terms and thus the
Laplace approximation is not effective unless the posterior
is dominated by the prior 
In order to assess the quality of the sampling  we compare
the trajectories to   standard costly MetropolisHastings
MCMC using   Gaussian with variance   as the proposal distribution  This algorithm was run for      
epochs and the proposal acceptance rate was   Figure  
shows samples in the directions of the largest  median and
smallest variance of the empirical covariance matrix of the
MetropolisHastings samples 

  Conclusions
This paper introduced   nonreversible sampler that can
be applied to big datasets by means of subsampling the
data in each iteration  At the price of   small  controllable
bias  it provides the bene ts of     high mixing speed associated with nonreversibility  and  ii  continuous sample
trajectories  with  iii  minimal hyperparameter tuning required  leading to state of the art performance and making
it   convenient alternative to biased  dif cultto tune MHbased stochastic samplers 

Stochastic Bouncy Particle Sampler

References
Ahn  Sungjin  Korattikara  Anoop  and Welling  Max 
Bayesian posterior sampling via stochastic gradient
 sher scoring  ICML   

Bardenet    emi  Doucet  Arnaud  and Holmes  Chris  Towards scaling up Markov chain Monte Carlo  an adapIn ICML  pp   
tive subsampling approach 
 

Bardenet    emi  Doucet  Arnaud  and Holmes  Chris 
On Markov chain Monte Carlo methods for tall data 
arXiv   

Bierkens  Joris and Roberts  Gareth    piecewise deterministic scaling limit of Lifted MetropolisHastings in
the CurieWeiss model  arXiv   

Bierkens  Joris  Fearnhead  Paul  and Roberts  Gareth 
The ZigZag Process and SuperEf cient Sampling
arXiv preprint
for Bayesian Analysis of Big Data 
arXiv   

Bierkens  Joris  BouchardC ot    Alexandre  Doucet  Arnaud  Duncan  Andrew    Fearnhead  Paul  Roberts 
Gareth  and Vollmer  Sebastian   
Piecewise Deterministic Markov Processes for Scalable Monte Carlo on
Restricted Domains  arXiv preprint arXiv 
 

BouchardC ot    Alexandre  Vollmer  Sebastian    and
Doucet  Arnaud  The Bouncy Particle Sampler    NonReversible RejectionFree Markov Chain Monte Carlo
Method  arXiv   

Chen  Tianqi  Fox  Emily    and Guestrin  Carlos  Stochas 

tic gradient HMC  In ICML  pp     

Chopin  Nicolas and Ridgway  James  Leave Pima Indians
alone  binary regression as   benchmark for Bayesian
computation  arXiv preprint arXiv   

Cox  David    Some statistical methods connected with
series of events     Royal Stat  Soc  Series    Methodological  pp     

Davis  Mark HA  Piecewisedeterministic Markov processes    general class of nondiffusion stochastic models     Royal Stat  Soc  Series    Methodological  pp 
   

Ding  Nan  Fang  Youhan  Babbush  Ryan  Chen 
Changyou  Skeel  Robert    and Neven  Hartmut 
Bayesian sampling using stochastic gradient
thermostats  In NIPS  pp     

Duchi  John  Hazan  Elad  and Singer  Yoram  Adaptive
subgradient methods for online learning and stochastic
optimization  Journal of Machine Learning Research 
 Jul   

Dufour  Franc ois  Zhang  Huilong  et al  Numerical Methods for Simulation and Optimization of Piecewise Deterministic Markov Processes  John Wiley   Sons   

Fearnhead  Paul  Bierkens  Joris  Pollock  Murray  and
Roberts  Gareth    Piecewise Deterministic Markov
arXiv
Processes for ContinuousTime Monte Carlo 
preprint arXiv   

Gal  Yarin  Uncertainty in Deep Learning  Cambridge PhD

Thesis   

Giles  Mike  Nagapetyan  Tigran  Szpruch  Lukasz 
Vollmer  Sebastian  and Zygalakis  Konstantinos  Multilevel Monte Carlo for Scalable Bayesian Computations 
arXiv preprint arXiv   

Grandell  Jan 

Springer   

Doubly stochastic Poisson processes 

Korattikara  Anoop  Chen  Yutian  and Welling  Max  Austerity in MCMC land  Cutting the MetropolisHastings
budget  arXiv   

Lewis  Peter   and Shedler  Gerald    Simulation of nonhomogeneous Poisson processes by thinning  Naval Research Logistics Quarterly     

Li  Chunyuan  Chen  Changyou  Fan  Kai  and Carin 
Lawrence  HighOrder Stochastic Gradient Thermostats
for Bayesian Learning of Deep Models  arXiv preprint
arXiv   

Li  Chunyuan  Chen  Changyou  Carlson  David  and
Carin  Lawrence  Preconditioned stochastic gradient
langevin dynamics for deep neural networks  AAAI 
 

Ma  YiAn  Chen  Tianqi  and Fox  Emily    complete
In NIPS  pp 

recipe for stochastic gradient MCMC 
   

Ma  YiAn  Chen  Tianqi  Wu  Lei  and Fox  Emily   
  Unifying Framework for Devising Ef cient and
arXiv preprint
Irreversible MCMC Samplers 
arXiv   

Maclaurin  Dougal and Adams  Ryan   

Fire  
Monte Carlo  Exact MCMC with subsets of data 
arXiv   

Monmarch    Pierre  Piecewise deterministic simulated an 

nealing  arXiv preprint arXiv   

Stochastic Bouncy Particle Sampler

Neal  Radford   

Improving asymptotic variance of
MCMC estimators  Nonreversible chains are better 
arXiv preprint math   

Neal  Radford    Bayesian learning for neural networks 
volume   Springer Science   Business Media   

Patterson  Sam and Teh  Yee Whye  Stochastic gradient
Riemannian Langevin dynamics on the probability simplex  In NIPS  pp     

Peters  EAJF and de With     Rejectionfree Monte Carlo
sampling for general potentials  Phys  Rev      
   

Pollock  Murray  Fearnhead  Paul  Johansen  Adam    and
Roberts  Gareth    The Scalable Langevin Exact Algorithm  Bayesian Inference for Big Data  arXiv preprint
arXiv   

Quiroz  Matias  Villani  Mattias  and Kohn  Robert  Speeding up MCMC by ef cient data subsampling  Riksbank
Research Paper Series     

Quiroz  Matias  Villani  Mattias  and Kohn  Robert  Exact

Subsampling MCMC  arXiv   

Robert  Christian and Casella  George  Monte Carlo statistical methods  Springer Science   Business Media 
 

Roux  Nicolas    Schmidt  Mark  and Bach  Francis     
stochastic gradient method with an exponential convergence rate for  nite training sets  In Advances in Neural
Information Processing Systems  pp     

Vucelja  Marija  Lifting   Nonreversible MCMC Algo 

rithm  arXiv   

Welling  Max and Teh  Yee    Bayesian learning via
In ICML  pp 

stochastic gradient Langevin dynamics 
   

