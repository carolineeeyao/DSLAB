Scaling Up Sparse Support Vector Machines

by Simultaneous Feature and Sample Reduction

Weizhong Zhang       Bin Hong       Wei Liu   Jieping Ye   Deng Cai   Xiaofei He   Jie Wang  

Abstract

Sparse support vector machine  SVM  is   popular classi cation technique that can simultaneously learn   small set of the most interpretable
features and identify the support vectors  It has
achieved great successes in many realworld applications  However  for largescale problems involving   huge number of samples and extremely
highdimensional features  solving sparse SVMs remains challenging  By noting that sparse
SVMs induce sparsities in both feature and sample spaces  we propose   novel approach  which
is based on accurate estimations of the primal and
dual optima of sparse SVMs  to simultaneously
identify the features and samples that are guaranteed to be irrelevant to the outputs  Thus  we can
remove the identi ed inactive samples and features from the training phase  leading to substantial savings in both the memory usage and computational cost without sacri cing accuracy  To
the best of our knowledge  the proposed method
is the  rst static feature and sample reduction
method for sparse SVM  Experiments on both
synthetic and real datasets       the kddb dataset
with about   million samples and   million
features  demonstrate that our approach signi 
cantly outperforms stateof theart methods and
the speedup gained by our approach can be orders of magnitude 

  Introduction
Sparse support vector machine  SVM   Bi et al   
Wang et al    is   powerful technique that can simultaneously perform classi cation by margin maximiza 

 Equal contribution  State Key Lab of CAD CG  Zhejiang
University  China  Tencent AI Lab  Shenzhen  China  University
of Michigan  USA  Correspondence to 
Jie Wang  jiewangustc gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tion and variable selection by  cid norm penalty  The last
few years have witnessed many successful applications of sparse SVMs  such as text mining  Joachims   
Yoshikawa et al    bioinformatics  Narasimhan   Agarwal    and image processing  Mohr   Obermayer    Kotsia   Pitas    Many algorithms  Hastie
et al    Fan et al    Catanzaro et al    Hsieh
et al    ShalevShwartz et al    have been proposed to ef ciently solve sparse SVM problems  However  the applications of sparse SVMs to largescale learning
problems  which involve   huge number of samples and extremely highdimensional features  remain challenging 
An emerging technique  called screening  El Ghaoui et al 
  has been shown to be promising in accelerating
largescale sparse learning  The essential idea of screening
is to quickly identify the zero coef cients in the sparse solutions without solving any optimization problems such that
the corresponding features or samples that are called inactive features or samples can be removed from the training phase  Then  we only need to perform optimization on
the reduced datasets instead of the full datasets  leading to
substantial savings in the computational cost and memory
usage  Here  we need to emphasize that screening differs
greatly from feature selection methods  although they look
similar at the  rst glance  To be precise  screening is devoted to accelerating the training of many sparse models
including Lasso  Sparse SVM  etc  while feature selection
is the goal of these models  In the past few years  many
screening methods are proposed for   large set of sparse
learning techniques  such as Lasso  Tibshirani et al   
Xiang   Ramadge    Wang et al    group Lasso  Ndiaye et al     cid regularized logistic regression
 Wang et al    and SVM  Ogawa et al    Empirical studies indicate that screening methods can lead to
orders of magnitude of speedup in computation time 
However  most existing screening methods study either feature screening or sample screening individually  Shibagaki
et al    and their applications have very different scenarios  Speci cally  to achieve better performance  say 
in terms of speedup  we favor feature screening methods when the number of features   is much larger than the
number of samples    while sample screening methods are

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

preferable when    cid     Note that there is another class
of sparse learning techniques  like sparse SVMs  which induce sparsities in both feature and sample spaces  All these
screening methods are helpless in accelerating the training
of these models with large   and    We also cannot address
this problem by simply combining the existing feature and
sample screening methods  The reason is that they could
mistakenly discard relevant data as they are speci cally designed for different sparse models  Recently  Shibagaki et
al   Shibagaki et al    consider this problem and propose   method to simultaneously identify the inactive features and samples in   dynamic manner  Bonnefoy et al 
  that is  during the optimization process  they trigger
their testing rule when there is   suf cient decrease in the duality gap  Thus  the method in  Shibagaki et al    can
discard more inactive features and samples as the optimization proceeds and one has smallscale problems to solve in
the late stage of the optimization  Nevertheless  the overall
speedup can be limited as the problems  size can be large
in the early stage of the optimization  To be speci    the
method in  Shibagaki et al    depends heavily on the
duality gap during the optimization process  The duality
gap in the early stage can always be large  which makes the
dual and primal estimations inaccurate and  nally results in
ineffective screening rules  Hence  it is essentially solving
  large problem in the early stage 
In this paper  to address the limitations in the dynamic
screening method  we propose   novel screening method
that can Simultaneously identify Inactive Features and
Samples  SIFS  for sparse SVMs in   static manner  that
is  we only need to perform SIFS once before  instead of
during  optimization  Thus  we only need to run the optimization algorithm on smallscale problems  The major
technical challenge in developing SIFS is that we need to
accurately estimate the primal and dual optima  The more
accurate the estimations are  the more effective SIFS is in
detecting inactive features and samples  Thus  our major
technical contribution is   novel framework  which is based
on the strong convexity of the primal and dual problems of
sparse SVMs  see problems     and     in Section   for
deriving accurate estimations of the primal and dual optima
 see Section   Another appealing feature of SIFS is the
socalled synergy effect  Shibagaki et al    Speci 
cally  the proposed SIFS consists of two parts       Inactive
Feature Screening  IFS  and Inactive Samples Screening
 ISS  We show that discarding inactive features  samples 
identi ed by IFS  ISS  leads to   more accurate estimation
of the primal  dual  optimum  which in turn dramatically
enhances the capability of ISS  IFS  in detecting inactive
samples  features  Thus  SIFS applies IFS and ISS in an
alternating manner until no more inactive features and samples can be identi ed  leading to much better performance
in scaling up largescale problems than the application of

ISS or IFS individually  Moreover  SIFS  see Section   is
safe in the sense that the detected features and samples are
guaranteed to be absent from the sparse representations  To
the best of our knowledge  SIFS is the  rst static screening
rule for sparse SVM that is able to simultaneously detect
inactive features and samples  Experiments  see Section  
on both synthetic and real datasets demonstrate that SIFS signi cantly outperforms the stateof theart  Shibagaki
et al    in improving the ef ciency of sparse SVMs and the speedup can be orders of magnitude  Detailed
proofs of theoretical results in the main text are in the supplementary supplements 
Notations  Let  cid     cid   cid     cid  and  cid     cid  be the  cid   cid  and
 cid  norms  respectively  We denote the inner product of
vectors   and   by  cid      cid  and the ith component of   by
      Let              for   positive integer    Given
  subset           jk  of     let      be the cardinality of     For   vector    let                   jk     
For   matrix    let         xj    xjk   and        
 xj          xjk         where xi and xj are the ith row and
jth column of    respectively  For   scalar    we denote
max     by    

  Basics and Motivations
In this section  we brie   review some basics of sparse
SVMs and then motivate SIFS via the KKT conditions 
Speci cally  we focus on the  cid regularized SVM with   smoothed hinged loss that has strong theoretical guarantees
 ShalevShwartz   Zhang    which takes the form of

  cid 

 
 
     

  

min
  Rp

           

 cid     cid xi    cid   

 cid   cid 

 
 

   

where   is the parameter vector
to be estimated 
   is the training set  xi   Rp  yi      
 xi  yi  
 xi   yixi    and   are positive parameters  and the loss
function  cid          is

 

  
   
     
   

 cid     

if      
if          
if      

where         We present the Lagrangian dual problem
of problem     and the KKT conditions in the following
theorem  which plays   fundamentally important role in developing our screening rule 
Theorem   Let                 xn  and    be
the softthresholding operator  Hastie et al        
         sign              Then  for problem
    the followings hold 

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

      The dual problem of     is

 iii    Suppose that     is known  Then 

 cid cid cid cid   

 cid   

 cid cid cid cid cid 

   

 

 cid cid 

 
  

 
 
   
 

 
 cid   cid 

min
  

        

   
where     Rn is   vector with all components equal to  
 ii    Denote the optima of     and     by      and
    respectively  Then 

      

     

 

 KKT 

 cid 

      

if      cid xi      cid     
if      cid xi      cid     
otherwise  

       cid xi      cid 

 KKT 

According to KKT  and KKT  we de ne   index sets 

 
 

  

 cid   
 

 
 

 

 cid 

 cid 

 
 

         

             

   
 
                    cid       xi cid     
                    cid       xi cid       
                    cid       xi cid     

 cid   
 cid 
             

 

            

  

 
 

Lemma   indicates that  if we can identify index sets    and
   and the cardinalities of      and  Dc are much smaller than
the feature dimension   and the dataset size    we only need
to solve   problem  scaledD  that may be much smaller
than problem     to exactly recover the optima     
and     without sacri cing any accuracy 
However  we cannot directly apply the rules in     to identify subsets of       and    as they require the knowledge of      and     that are usually unavailable 
Inspired by the idea in  El Ghaoui et al    we can
 rst estimate regions   and   that contain      and
    respectively  Then  by denoting

 cid 
 cid 
 cid 

      

 cid cid cid cid cid 

 cid cid cid cid cid   

 cid 
 cid 
   
          max
 
 cid 
        cid     xi cid     
          max
        cid     xi cid     

          min

 

 

 

 

    

    

    

 

 

 

which imply that

                      

 cid                  

                

 ii 

since it is easy to know that                and        
the rules in     can be relaxed as follows 
                       

 cid                   

   

   

 ii 

                 

   

Thus  we call the jth feature inactive if        The samples in   are the socalled support vectors and we call the
samples in   and   inactive samples 
Suppose that we are given subsets of       and    then
by     we can see that many coef cients of      and
    are known  Thus  we may have much less unknowns to solve and the problem size can be dramatically
reduced  We formalize this idea in Lemma  
Lemma   Given index sets                 and        
the followings hold
                                        
 ii    Let                                Dc  and                    
where                 Dc           and  Lc           Then 
     Dc solves the following scaled dual problem 

min

   Dc 

   

   

 
 

   

 cid   

 cid cid cid cid   
 cid   
 cid   cid cid 

 

 

 
   
 

 cid cid cid cid cid 

 

 cid cid 

 
  
 scaledD 

In view of    and    we sketch the development of SIFS
as follows 

Step   Derive estimations   and   such that         

and         respectively 

Step   Develop SIFS by deriving the relaxed screening rules
   and         by solving the optimization problems
in Eq    Eq    and Eq   

  Estimate the Primal and Dual Optima
In this section  we  rst show that the primal and dual optima admit closed form solutions for speci   values of  
and    see Section   Then  in Sections   and   we
present accurate estimations of the primal and dual optima 
respectively 

  Effective Intervals of the Parameters   and  

We  rst show that  if the value of   is suf ciently large  no
matter what   is  the primal solution is  

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

Theorem   Let  max    cid   
     max  we have

 

   cid  Then  for       and

  Dual Optimum Estimation

        

       

For any   the next result shows that  if   is large enough 
the primal and dual optima admit closed form solutions 
Theorem   If we denote

 max   

 
     

max
    

then for all      max max        we have

      

  

 
 

   

         

 

 cid   

 

   cid cid 

 
 

 cid cid xi   
 cid 

By Theorems   and   we only need to consider the cases
with        max  and        max 

  Primal Optimum Estimation

In Section   we mention that the proposed SIFS consists of
IFS and ISS  and an alternating application of IFS and ISS
can improve the estimation of the primal and dual optima 
which can in turn make ISS and IFS more effective in identifying inactive samples and features  respectively  Lemma
  shows that discarding inactive features by IFS leads to  
more accurate estimation of the primal optimum 
Lemma   Suppose that the reference solution     
with        max  and        max  is known 
Consider problem     with parameters       and   Let
   be the index set of the inactive features identi ed by the
previous IFS steps                    We de ne

   

    

     
 
     

           
 cid     cid 

 

       

 
Then  the following holds 

 cid         cid 

                       cid       cid      

 

 

 

As    is the index set of identi ed inactive features  we
have              Hence  we only need to  nd an
accurate estimation of             Lemma   shows that
           lies in   ball of radius   centered at    Note
that  before we perform IFS  the set    is empty and thus the
second term on the right hand side  RHS  of Eq    is   If
we apply IFS multiple times  alternating with ISS  the set
   will be monotonically increasing  Thus  Eq    implies
that the radius will be monotonically decreasing  leading to
  more accurate primal optimum estimation 

Similar to Lemma   the next result shows that ISS can
improve the estimation of the dual optimum 
Lemma   Suppose that the reference solution    
with        max  and        max  is known 
Consider problem     with parameters       and  
Let    and    be the index sets of inactive samples identi ed by the previous ISS steps                 
           and               We de ne

     
   
 
     

   

    

     Dc 

 

     

 cid cid cid cid       

 

 cid cid cid cid 

 

 

 cid cid cid cid           
 cid cid cid cid       

 

 

   

 

 

         

 

      

     

 

      

 

 

 cid cid cid cid 

 

 cid cid cid cid 

Then  the following holds 

     Dc        cid          cid      

 

Similar to Lemma   Lemma   also bounds      Dc
by   ball  In view of Eq      similar discussion of Lemma
 that is  the index sets    and    monotonically increase
and thus the last two terms on the RHS of Eq    monotonically increase when we perform ISS multiple times  alternating with IFS implies that the ISS steps can reduce the
radius and thus improve the dual optimum estimation 
Remark   To estimate      and     by Lemmas   and   we have   free reference solution pair
     and     with      max  From
Theorems   and   we know that in this setting      
and     admit closed form solutions 

  The Proposed SIFS Screening Rule
We  rst present the IFS and ISS rules in Sections   and
  respectively  Then  in Section   we develop the SIFS
screening rule by an alternating application of IFS and ISS 

  Inactive Feature Screening  IFS 
Suppose that      and     are known  we derive IFS to identify inactive features for problem     at
    by solving the optimization problem in Eq     see
Section   in the supplementary material 

 cid   

 

 cid 
 cid xi   Dc    cid     cid xi       cid 

si      max
 

           
 

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

where   is given by Eq    and    and             are the
index sets of inactive features and samples that have been identi ed in previous screening processes  respectively  The
next result shows the closed form solution of problem  
Lemma   Consider problem   Let   and   be given by
Eq    and Eq    Then  for all           we have

si     

 
 

 cid xi   Dc    cid     cid xi       cid     cid xi   Dc cid   

We are now ready to present the IFS rule 
Theorem   Consider problem     We suppose that
     and     are known  Then 

  The feature screening rule IFS takes the form of

si                              IFS 

  We update the index set    by

               si              

 

Recall that  Lemma   previous sample screening results
give us   more tighter dual estimation         smaller feasible region   for problem   which results in   smaller
si   
It  nally leads us to   more powerful feature
screening rule IFS  This is the so called synergy effect 

  Inactive Sample Screening  ISS 

Similar to IFS  we derive ISS to identify inactive samples
by solving the optimization problems in Eq    and Eq   
 see Section   in the supplementary material for details 

 

ui      max

        cid xi          cid       Dc 
        cid xi          cid       Dc 

li      min

 
where   is given by Eq    and    and             are the
index sets of inactive features and samples that have been
identi ed in previous screening processes  We show that
problems   and   admit closed form solutions 
Lemma   Consider problems   and   Let   and  
be given by Eq    and Eq    Then 

ui           cid xi           cid     cid xi       cid         Dc 
li           cid xi           cid     cid xi       cid         Dc 

We are now ready to present the ISS rule 
Theorem   Consider problem     We suppose that
     and     are known  Then 

  The sample screening rule ISS takes the form of

ui                  
li                  

      Dc  ISS 

 
 
 
 

 

 

 
 

 

  We update the the index sets    and    by

               ui             Dc 
               li             Dc 

 
 

The synergy effect also exists here  Recall that  Lemma  
previous feature screening results lead   smaller feasible
region   for the problems   and   which results in
smaller ui    and bigger li    It  nally leads us to
  more accurate sample screening rule ISS 

  The Proposed SIFS Rule by An Alternating

Application of IFS and ISS

In real applications  the optimal parameter values of   and
  are usually unknown  To determine appropriate parameter values  common approaches  like cross validation and
stability selection  need to solve the model over   grid of
parameter values                               with
 max                  and  max              
         This can be very timeconsuming  Inspired by
Strong Rule  Tibshirani et al    and SAFE  El Ghaoui
et al    we develop   sequential version of SIFS in
Algorithm   Speci cally  given the primal and dual opti 

Algorithm   SIFS
  Input   max                  and  max     

                      

  for       to   do
 

Compute the  rst reference solution         
and         using the closeform formula  
for       to   do

Initialization                  
repeat

Run sample screening using rule ISS based on
          
Update    and    by Eq    and Eq    respectively 
Run feature screening using rule IFS based on
         
Update    by Eq   

until No new inactive features or samples are identi ed
Compute            and           by solving the scaled problem 

end for

 
  end for
  Output            and                       

    

ma            and           at           we
apply SIFS to identify the inactive features and samples for
problem     at           Then  we perform optimization
on the reduced dataset and solve the primal and dual optima

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

at           We repeat this process until we solve problem
    at all pairs of parameter values 
Note that we insert    into every sequence           
       see line   in Algorithm   to obtain   closedform
solution as the  rst reference solution  In this way  we can
avoid solving problem at                  directly  without screening  which is time consuming  At last  we would
like to point out that the values                         
     in SIFS can be speci ed by users arbitrarily 
SIFS applies ISS and IFS in an alternating manner to reinforce their capability in identifying inactive samples and
features  In Algorithm   we apply ISS  rst  Of course  we
can also apply IFS  rst  The theorem below demonstrates
that the orders have no impact on the performance of SIFS 
Theorem   Given the optimal solutions           
and           at           as the reference solution
pair at           for SIFS  we assume SIFS with ISS  rst
stops after applying IFS and ISS for   times and denote
the identi ed inactive features and samples as     
  and
 LA
    Similarly  when we apply IFS  rst  the results are denoted as     
        
      
  With different orders of applying ISS and IFS  the difference of the times of ISS and IFS we need to apply in SIFS
can never be larger than   that is             
Remark   From Remark   we can see that our SIFS can
also be applied to solve   single problem  due to the existence of the free reference solution pair 

    Then  the followings hold 
  and  LA

  and  LB
     RB

     RA

     RB
     RA

     LB
   

np

and  pi
  

  Experiments
We evaluate SIFS on both synthetic and real datasets in
terms of three measurements  The  rst one is the scaling
ratio               
  where            and   are the numbers
of inactive samples and features identi ed by SIFS  sample
size  and feature dimension of the datasets  The second
measure is rejection ratios of each triggering of ISS and
  where  ni and  pi are the numbers
IFS in SIFS   ni
  
of inactive samples and features identi ed in ith triggering
of ISS and IFS in SIFS     and    are the numbers of inactive samples and features in the solution  The third measure
is speedup       the ratio of the running time of the solver
without screening to that with screening 
Recall that  we can integrate SIFS with any solvers for
problem    
In this experiment  we use Accelerated
Proximal Stochastic Dual Coordinate Ascent  AcceleratedProx SDCA   ShalevShwartz   Zhang    as it is one
of the stateof thearts  As we mentioned in the introduction section that screening differs greatly from features selection methods  it is not appropriate to make comparisons with feature selection methods  To this end  we only

choose the stateof art screening method for Sparse SVMs
in  Shibagaki et al    as   baseline in the experiments 
For each dataset  we solve problem     at   grid of turning
parameter values  Speci cally  we  rst compute  max by
Theorem   and then select   values of   that are equally
spaced on the logarithmic scale of  max from   to  
Then  for each value of   we  rst compute  max  by
Theorem   and then select   values of   that are equally spaced on the logarithmic scale of  max  from  
to   Thus  for each dataset  we solve problem     at
  pairs of parameter values in total  We write the code
in    along with Eigen library for some numerical computations  We perform all the computations on   single core
of Intel    Core TM       GHz   GB MEM 

  Simulation Studies

We evaluate SIFS on   synthetic datasets named
syn 
syn  and syn  with sample and feature size
                    
We present each data point as            with     
    and           We use Gaussian distributions
                             and           
to generate the data points  where       and    
      is the identity matrix  To be precise     for
positive and negative points are sampled from    and   
respectively  For each entry in    it has chance      
to be sampled from    and chance       to be  

    The scaling ratios of ISS  IFS  and SIFS on syn 

    The scaling ratios of ISS  IFS  and SIFS on syn 

    The scaling ratios of ISS  IFS  and SIFS on syn 

Figure   Scaling ratios of ISS  IFS and SIFS  from left to right 

Fig    shows the scaling ratios by ISS  IFS  and SIFS on
the synthetic datasets at   parameter values  We can
see that IFS is more effective in scaling problem size than
ISS  with scaling ratios roughly   against      
Moreover  SIFS  which is an alternating application of IFS
and ISS  signi cantly outperforms ISS and IFS  with scal 

log max log max log max log max log max log max log max log max log max log max log max log max Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

     max 

     max 

     max 

     max 

     max 

     max 

     max 

     max 

Figure   Rejection ratios of SIFS on syn    rst row  Feature Screening  second row  Sample Screening 
Table   Running time  in seconds  for solving problem     at   pairs of parameter values on three synthetic datasets 

ISS Solver
Solver
 
 
 

Speedup

 
 
 

IFS Solver
Solver
 
 
 

Speedup

 
 
 

IFS
 
 
 

SIFS
 
 
 

Data
syn 
syn 
syn 

Solver
 
 
 

ISS
 
 
 

SIFS Solver
Solver

Speedup

 
 
 

 
 
 

ing ratios roughly   This high scaling ratios imply
that SIFS can lead to   signi cant speedup 
Due to the space limitation  we only report the rejection
ratios of SIFS on syn  Other results can be found in the
supplementary material  Fig    shows that SIFS can identify most of the inactive features and samples  However  few
features and samples are identi ed in the second and later
triggerings of ISS and IFS  The reason may be that the task
here is so simple that one triggering is enough 
Table   reports the running time of solver without and with
IFS  ISS and SIFS for solving problem     at   pairs of
parameter values  We can see that SIFS leads to signi cant
speedups  that is  up to   times  Taking syn  for example  without SIFS  the solver takes more than two hours to
solve problem     at   pairs of parameter values  However  combined with SIFS  the solver only needs less than
three minutes for solving the same set of problems  From
the theoretical analysis in  ShalevShwartz   Zhang   
for AcceleratedProx SDCA  we can see that its computational complexity rises proportionately to the sample size
  and the feature dimension    From this theoretical result 
we can see that the results in Figure   are roughly consistent with the speedups we achieved shown in Table  

  Experiments on Real Datasets

In this experiment  we evaluate the performance of SIFS
on   largescale real datasets  realsim  rcv train  rcv 

test  url  and kddb  which are all collected from the project
page of LibSVM  Chang   Lin    See Table   for  
brief summary  We note that  the kddb dataset has about  
million samples with   million features 

Table   Statistics of the real datasets 

Dataset
realsim
rcv train
rcv test
url
kddb

Feature size   
 
 
 
 
 

Sample size  
 
 
   
 
 

Recall that  SIFS detects the inactive features and samples in   static manner       we perform SIFS only once
before the optimization and thus the size of the problem
we need to perform optimization on is  xed  However  the
method in  Shibagaki et al    detects inactive features
and samples in   dynamic manner  Bonnefoy et al   
     they perform their method along with the optimization
and thus the size of the problem would keep decreasing
during the iterative process  Thus  comparing SIFS with
the method in  Shibagaki et al    in terms of rejection ratios is inapplicable  We compare the performance of
SIFS with the method in  Shibagaki et al    in terms of
speedup  Speci cally  we compare the speedup gained by
SIFS and the method in  Shibagaki et al    for solving
problem     at   pairs of parameter values  The code
of the method in  Shibagaki et al    is obtained from
 https github com husk   fs 

 max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

     max 

     max 

     max 

     max 

     max 

     max 

     max 

     max 

Figure   Rejection ratios of SIFS on the realsim dataset  rst row  Feature Screening  second row  Sample Screening 

Table   Running time  in seconds  for solving problem     at   pairs of parameter values on  ve real datasets 

Data
Set

realsim
rcv train
rcv test

url
kddb

Solver

   
   
   

 
 

Solver

Speedup

Method in  Shibagaki et al   Solver
Screen
 
 
 
   
   

   
   
   
   
   

 
 
 
 
 

Screen
 
 

   
   
   

SIFS Solver

Solver
 
 

   
   
   

Speedup
 
 
 

 
 

Fig    shows the rejection ratios of SIFS on the realsim
dataset  other results are in the supplementary material  In
Fig    we can see that some inactive features and samples
are identi ed in the  nd and  rd triggering of ISS and IFS 
which veri es the necessity of the alternating application
of ISS and IFS  SIFS is ef cient since it always stops in  
times of triggering  In addition  most of     the inactive features can be identi ed in the  st triggering of IFS
while identifying inactive samples needs to apply ISS two
or more times  It may result from two reasons    We run
ISS  rst  which reinforces the capability of IFS due to the
synergy effect  see Sections   and   see Section    in
the supplementary material for further veri cation    Feature screening here may be easier than sample screening 
Table   reports the running time of solver without and with
the method in  Shibagaki et al    and SIFS for solving problem     at   pairs of parameter values on real
datasets  The speedup gained by SIFS is up to   times on
realsim  rcv train and rcv test  Moreover  SIFS significantly outperforms the method in  Shibagaki et al   
in terms of speedup by about   to   times faster on the
aforementioned three datasets  For datasets url and kddb 
we do not report the results of the solver as the sizes of the
datasets are huge and the computational cost is prohibitive 
Instead  we can see that the solver with SIFS is about  

times faster than the solver with the method in  Shibagaki
et al    on both datasets url and kddb  Take the dataset
kddb as an example  The solver with SIFS takes about  
hours to solve problem     for all   pairs of parameter values  while the solver with the method in  Shibagaki
et al    needs   days to  nish the same task 

  Conclusion
In this paper  we develop   novel data reduction method
SIFS to simultaneously identify inactive features and samples for sparse SVM  Our major contribution is   novel
framework for an accurate estimation of the primal and dual optima based on strong convexity  To the best of our
knowledge  the proposed SIFS is the  rst static screening
method that is able to simultaneously identify inactive features and samples for sparse SVMs  An appealing feature
of SIFS is that all detected features and samples are guaranteed to be irrelevant to the outputs  Thus  the model learned
on the reduced data is identical to the one learned on the
full data  Experiments on both synthetic and real datasets
demonstrate that SIFS can dramatically reduce the problem
size and the resulting speedup can be orders of magnitude 
We plan to generalize SIFS to more complicated models 
     SVM with   structured sparsityinducing penalty 

 max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  max Rejection Ratio Trigger  Trigger  Trigger  Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

Acknowledgements
This work was supported by the National Basic Research Program of China   Program  under Grant
 CB  National Natural Science Foundation of
China under Grant   and National Youth Topnotch Talent Support Program 

References
Bi  Jinbo  Bennett  Kristin  Embrechts  Mark  Breneman 
Curt  and Song  Minghu  Dimensionality reduction via
sparse support vector machines  The Journal of Machine
Learning Research     

Bonnefoy  Antoine  Emiya  Valentin  Ralaivola  Liva  and
Gribonval    emi    dynamic screening principle for
In Signal Processing Conference  EUSIPthe lasso 
CO    Proceedings of the  nd European  pp   
IEEE   

Catanzaro  Bryan  Sundaram  Narayanan  and Keutzer 
Kurt  Fast support vector machine training and classiIn Proceedings of the
 cation on graphics processors 
 th international conference on Machine learning  pp 
  ACM   

Chang  ChihChung and Lin  ChihJen  Libsvm    library
for support vector machines  ACM Transactions on Intelligent Systems and Technology  TIST     

El Ghaoui  Laurent  Viallon  Vivian  and Rabbani  Tarek 
Safe feature elimination in sparse supervised learning 
Paci   Journal of Optimization     

Kotsia  Irene and Pitas  Ioannis  Facial expression recognition in image sequences using geometric deformation
features and support vector machines  Image Processing 
IEEE Transactions on     

Mohr  Johannes and Obermayer  Klaus    topographic support vector machine  Classi cation using local label con 
 gurations  In Advances in Neural Information Processing Systems  pp     

Narasimhan  Harikrishna and Agarwal  Shivani  Svm pauc
tight    new support vector method for optimizing partial
auc based on   tight convex upper bound  In Proceedings of the  th ACM SIGKDD international conference
on Knowledge discovery and data mining  pp   
ACM   

Ndiaye  Eugene  Fercoq  Olivier  Gramfort  Alexandre  and
Salmon  Joseph  Gap safe screening rules for sparsegroup lasso 
In Lee        Sugiyama     Luxburg 
      Guyon     and Garnett      eds  Advances in
Neural Information Processing Systems   pp   
  Curran Associates  Inc   

Ogawa  Kohei  Suzuki  Yoshiki  and Takeuchi  Ichiro  Safe
screening of nonsupport vectors in pathwise svm computation  In Proceedings of the  th International Conference on Machine Learning  pp     

ShalevShwartz  Shai and Zhang  Tong  Accelerated proximal stochastic dual coordinate ascent for regularized loss
minimization  Mathematical Programming   
   

Fan  RongEn  Chang  KaiWei  Hsieh  ChoJui  Wang 
XiangRui  and Lin  ChihJen  Liblinear    library
for large linear classi cation  The Journal of Machine
Learning Research     

ShalevShwartz  Shai  Singer  Yoram  Srebro  Nathan  and
Cotter  Andrew  Pegasos  Primal estimated subgradient
solver for svm  Mathematical programming   
   

Hastie  Trevor  Rosset  Saharon  Tibshirani  Robert  and
Zhu  Ji  The entire regularization path for the support
vector machine  The Journal of Machine Learning Research     

Hastie  Trevor  Tibshirani  Robert  and Wainwright  Martin  Statistical learning with sparsity  the lasso and generalizations  CRC Press   

Hsieh  ChoJui  Chang  KaiWei  Lin  ChihJen  Keerthi 
  Sathiya  and Sundararajan  Sellamanickam    dual
coordinate descent method for largescale linear svm  In
Proceedings of the  th international conference on Machine learning  pp    ACM   

Joachims  Thorsten  Text categorization with support vector machines  Learning with many relevant features 
Springer   

Shibagaki  Atsushi  Karasuyama  Masayuki  Hatano  Kohei  and Takeuchi  Ichiro  Simultaneous safe screening
of features and samples in doubly sparse modeling  In
Proceedings of The  rd International Conference on
Machine Learning   

Tibshirani  Robert  Bien  Jacob  Friedman  Jerome  Hastie 
Trevor  Simon  Noah  Taylor  Jonathan  and Tibshirani 
Ryan    Strong rules for discarding predictors in lassotype problems  Journal of the Royal Statistical Society  Series    Statistical Methodology   
 

Wang  Jie  Zhou  Jiayu  Wonka  Peter  and Ye  Jieping  Lasso screening rules via dual polytope projection  In Advances in Neural Information Processing Systems  pp 
   

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

Wang  Jie  Zhou  Jiayu  Liu  Jun  Wonka  Peter  and Ye 
Jieping    safe screening rule for sparse logistic regression  In Advances in Neural Information Processing Systems  pp     

Wang  Li  Zhu  Ji  and Zou  Hui  The doubly regularized
support vector machine  Statistica Sinica  pp   
 

Xiang  Zhen James and Ramadge  Peter    Fast lasso
In Acoustics 
screening tests based on correlations 
Speech and Signal Processing  ICASSP    IEEE International Conference on  pp    IEEE   

Yoshikawa  Yuya  Iwata  Tomoharu  and Sawada  Hiroshi 
Latent support measure machines for bagof words data
classi cation  In Advances in Neural Information Processing Systems  pp     

