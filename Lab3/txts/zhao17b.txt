Theoretical Properties for Neural Networks with Weight Matrices of Low

Displacement Rank

Liang Zhao   Siyu Liao   Yanzhi Wang   Zhe Li   Jian Tang   Bo Yuan  

Abstract

Recently low displacement rank  LDR  matrices  or socalled structured matrices  have been
proposed to compress largescale neural networks  Empirical results have shown that neural networks with weight matrices of LDR matrices  referred as LDR neural networks  can
achieve signi cant reduction in space and computational complexity while retaining high accuracy  We formally study LDR matrices in deep
learning  First  we prove the universal approximation property of LDR neural networks with
  mild condition on the displacement operators 
We then show that the error bounds of LDR neural networks are as ef cient as general neural networks with both singlelayer and multiplelayer
structure  Finally  we propose backpropagation
based training algorithm for general LDR neural
networks 

  Introduction
Neural networks  especially largescale deep neural networks  have made remarkable success in various applications such as computer vision  natural language processing  etc   Krizhevsky et al   Sutskever et al   
However  largescale neural networks are both memoryintensive and computationintensive  thereby posing severe challenges when deploying those largescale neural network models on memoryconstrained and energyconstrained embedded devices  To overcome these limitations  many studies and approaches  such as connection
pruning  Han et al   Gong et al    low rank approximation  Denton et al   Jaderberg et al   
sparsity regularization  Wen et al   Liu et al   

Co rst authors  Liang Zhao and Siyu Liao   The City University of New York  New York  New York  USA  Syracuse University  Syracuse  New York  USA  Correspondence to  Bo Yuan
 byuan ccny cuny edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Examples of commonly used LDR  structured  matrices       circulant  Cauchy  Toeplitz  Hankel  and Vandermonde
matrices 

etc  have been proposed to reduce the model size of largescale  deep  neural networks 
LDR Construction and LDR Neural Networks  Among
those efforts  low displacement rank  LDR  construction is
  type of structureimposing technique for network model
reduction and computational complexity reduction  By regularizing the weight matrices of neural networks using the
format of LDR matrices  when weight matrices are square 
or the composition of multiple LDR matrices  when weight
matrices are nonsquare    strong structure is naturally imposed to the construction of neural networks  Since an
LDR matrix typically requires      independent parameters and exhibits fast matrix operation algorithms  Pan 
  an immense space for network model and computational complexity reduction can be enabled  Pioneering
work in this direction  Cheng et al   Sindhwani et al 
  applied special types of LDR matrices  structured
matrices  such as circulant matrices and Toeplitz matrices 
for weight representation  Other types of LDR matrices exist such as Cauchy matrices  Vandermonde matrices  etc 
as shown in Figure  
Bene ts of LDR Neural Networks  Compared with other
types of network compression approaches  the LDR construction shows several unique advantages  First  unlike
heuristic weightpruning methods  Han et al   Gong
et al    that produce irregular pruned networks  the
LDR construction approach always guarantees the strong
structure of the trained network  thereby avoiding the stor 

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

age space and computation time overhead incurred by the
complicated indexing process  Second  as    train from
scratch  technique  LDR construction does not need extra retraining  and hence eliminating the additional complexity to the training process  Third  the reduction in
space complexity and computational complexity by using the structured weight matrices are signi cant  Different from other network compression approaches that can
only provide   heuristic compression factor  the LDR construction can enable the model reduction and computational complexity reduction in BigO complexity  The storage requirement is reduced from      to      and the
computational complexity can be reduced from      to
    log    or     log     because of the existence of fast
matrixvector multiplication algorithm  Pan   Bini
et al    for LDR matrices  For example  when applying structured matrices to the fullyconnected layers of
AlexNet using ImageNet dataset  Deng et al    the
storage requirement can be reduced by more than   
while incurring negligible degradation in overall accuracy
 Cheng et al   
Motivation of This Work  Because of its inherent
structureimposing characteristic  convenient retraining 
free training process and unique capability of simultaneous
BigO complexity reduction in storage and computation 
LDR construction is   promising approach to achieve high
compression ratio and high speedup for   broad category of
network models  However  since imposing the structure to
weight matrices results in substantial reduction of weight
storage from      to      cautious researchers need to
know whether the neural networks with LDR construction 
referred to as LDR neural networks  will consistently yield
the similar accuracy as compared with the uncompressed
networks  Although  Cheng et al   Sindhwani et al 
  have already shown that using LDR construction still
results the same accuracy or minor degradation on various datasets  such as ImageNet  Deng et al    CIFAR
 Krizhevsky   Hinton    etc  the theoretical analysis  which can provide the mathematically solid proofs that
the LDR neural networks can converge to the same  effectiveness  as the uncompressed neural networks  is still very
necessary in order to promote the wide application of LDR
neural networks for emerging and largerscale applications 
Technical Preview and Contributions  To address the
above necessity  in this paper we study and provide   solid
theoretical foundation of LDR neural networks on the ability to approximate an arbitrary continuous function  the error bound for function approximation  applications on shallow and deep neural networks  etc  More speci cally  the
main contributions of this paper include 

  We prove the universal approximation property for
LDR neural networks  which states that the LDR neu 

ral networks could approximate an arbitrary continuous function with arbitrary accuracy given enough
parameters neurons  In other words  the LDR neural
network will have the same  effectiveness  of classical neural networks without compression  This property serves as the theoretical foundation of the potential broad applications of LDR neural networks 

  We show that  for LDR matrices de ned by      parameters  the corresponding LDR neural networks are
still capable of achieving integrated squared error of
order      which is identical to the error bound of
unstructured weight matricesbased neural networks 
thereby indicating that there is essentially no loss for
restricting to the weight matrices to LDR matrices 

  We develop   universal training process for LDR neural networks with computational complexity reduction compared with backward propagation process for
classical neural networks  The proposed algorithm is
the generalization of the training process in  Cheng
et al   Sindhwani et al    that restricts the
structure of weight matrices to circulant matrices or
Toeplitz matrices 

Outline  The paper is outlined as follows  In Section   we
review the related work on this topic  Section   presents
necessary de nitions and properties of matrix displacement
and LDR neural networks  The problem statement is also
presented in this section  In Section   we prove the universal approximation property for   broad family of LDR
neural networks  Section   addresses the approximation
potential  error bounds  with   limited amount of neurons
on shallow LDR neural networks and deep LDR neural networks  respectively  The proposed detailed procedure for
training general LDR neural networks are derived in Section   Section   concludes the article 

  Related Work
Universal Approximation   Error Bound Analysis  For
feedforward neural networks with one hidden layer   Cybenko    and  Hornik et al    proved separately
the universal approximation property  which guarantees
that for any given continuous function or decision function and any error bound       there always exists  
singlehidden layer neural network that approximates the
function within   integrated error  However  this property
does not specify the number of neurons needed to construct
such   neural network  In practice  there must be   limit on
the maximum amount of neurons due to the computational
limit  Moreover  the magnitude of the coef cients can be
neither too large nor too small  To address these issues for
general neural networks   Hornik et al    proved that it
is suf cient to approximate functions with weights and bi 

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

ases whose absolute values are bounded by   constant  depending on the activation function   Hornik    further
extended this result to an arbitrarily small bound   Barron 
  showed that feedforward networks with one layer of
sigmoidal nonlinearities achieve an integrated squared error with order of      where   is the number of neurons 
More recently  several interesting results were published
on the approximation capabilities of deep neural networks 
 Delalleau   Bengio    have shown that there exist certain functions that can be approximated by threelayer neural networks with   polynomial amount of neurons  while twolayer neural networks require exponentially larger amount to achieve the same error   Montufar
et al    and  Telgarsky    have shown the exponential increase of linear regions as neural networks grow
deeper   Liang   Srikant    proved that with log 
layers  the neural network can achieve the error bound   for
any continuous function with   polylog  parameters in
each layer 
LDR Matrices in Neural Networks   Cheng et al   
have analyzed the effectiveness of replacing conventional
weight matrices in fullyconnected layers with circulant
matrices  which can reduce the time complexity from
     to     log    and the space complexity from
     to      respectively   Sindhwani et al    have
demonstrated signi cant bene ts of using Toeplitzlike matrices to tackle the issue of large space and computation requirement for neural networks training and inference  Experiments show that the use of matrices with low displacement rank offers superior tradeoffs between accuracy and
time space complexity 

  Preliminaries on LDR Matrices and Neural

Networks

  Matrix Displacement
An       matrix   is called   structured matrix when it
has   low displacement rank    Pan    More precisely 
with the proper choice of operator matrices   and    if the
Sylvester displacement

          AM   MB

and the Stein displacement

              AMB

 

 

of matrix   have   rank   bounded by   value that is independent of the size of    then matrix   is referred to
as   matrix with   low displacement rank  Pan    In
this paper we will call these matrices as LDR matrices 
Even   fullrank matrix may have small displacement rank
with appropriate choice of displacement operators       

Operator Matrix

 
  
  
  

 
  
  
  
  

Structured
Rank of
Matrix          
Circulant
Toeplitz
Henkel

   
   
   
   
   

diag   
diag    diag   

Vandermonde

Cauchy

Table   Pairs of Displacement Operators and Associated Structured Matrices     and    represent the  unitcirculant matrix
and the  unitcirculant matrix respectively  and vector   and  
denote vectors de ning Vandermonde and Cauchy matrices  cf 
 Sindhwani et al   

Figure   illustrates   series of commonly used structured
matrices  including   circulant matrix    Cauchy matrix   
Toeplitz matrix    Hankel matrix  and   Vandermonde matrix  and Table   summarizes their displacement ranks and
corresponding displacement operators 
The general procedure of handling LDR matrices generally takes three steps  Compression  Computation with Displacements  Decompression  Here compression means to
obtain   lowrank displacement of the matrices  and decompression means to converting the results from displacement
computations to the answer to the original computational
problem  In particular  if one of the displacement operator
has the property that its power equals the identity matrix 
then one can use the following method to decompress directly 
Lemma   If   is an apotent matrix       Aq   aI for
some positive integer        then

 cid    cid 

Ak       Bk cid 

   

     aBq 

 

  

Proof  See Corollary   in  Pan   

One of the most important characteristics of structured matrices is their low number of independent variables  The
number of independent parameters is      for an nby 
  structured matrix instead of the order of    which indicates that the storage complexity can be potentially reduced to      Besides  the computational complexity for
many matrix operations  such as matrixvector multiplication  matrix inversion  etc  can be signi cantly reduced
when operating on the structured ones  The de nition and
analysis of structured matrices have been generalized to the
case of nby   matrices where    cid          the blockcirculant matrices  Pan et al    Our application of
LDR matrices to neural networks would be the general nby   weight matrices  For certain lemmas and theorems
such as Lemma   only the form on       square matrices is needed for the derivation procedure in this paper 

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

kn cid 

So we omit the generalized form of such statements unless
necessary 

  LDR Neural Networks

In this paper we study the viability of applying LDR matrices in neural networks  Without loss of generality  we focus
on   feedforward neural network with one fullyconnected
 hidden  layer  which is similar network setup as  Cybenko 
  Here the input layer  with   neurons  and the hidden
layer  with kn neurons  are assumed to be fully connected
with   weight matrix     Rn kn of displacement rank
at most   corresponding to displacement operators       
where    cid     The domain for the input vector   is the ndimensional hypercube             and the output layer
only contains one neuron  The neural network can be expressed as 

    GW     

   wj

         

 

  

Here   is the activation function  wj   Rn denotes the jth column of the weight matrix    and            for    
    kn  When the weight matrix          wkn 
has   lowrank displacement  we call it an LDR neural network  Matrix displacement techniques ensure that LDR
neural network has much lower space requirement and
higher computational speed comparing to classical neural
networks of the similar size 

  Problem Statement

In this paper  we aim at providing theoretical support on the
accuracy of function approximation using LDR neural networks  which represents the  effectiveness  of LDR neural networks compared with the original neural networks 
Given   continuous function       de ned on       we
study the following tasks 

  For any        nd an LDR weight matrix   so that

the function de ned by equation   satis es
         GW       

max
   

 

  Fix   positive integer     nd an upper bound   so that
for any continuous function       there exists   bias
vector   and an LDR matrix with at most   rows satisfying equation  

  Find   multilayer LDR neural network that achieves

error bound   but with fewer parameters 

 Please note that this assumption does not sacri ce any generality because the nby   case can be transformed to nby kn
format with the nearest   using zero padding  Cheng et al   

The  rst task is handled in Section   which is the universal
approximation property of LDR neural networks  It states
that the LDR neural networks could approximate an arbitrary continuous function arbitrarily well and is the underpinning of the widespread applications  The error bounds
for shallow and deep neural networks are derived in Section   In addition  we derived explicit backpropagation
expressions for LDR neural networks in Section  

  The Universal Approximation Property of

LDR Neural Networks

We call   family of matrices   to have representation
property if for any vector     Rn  there exists   matrix
    SA   such that   is   column of    Note that all
 ve types of LDR matrices shown in Fig    have this representation property because of their explicit pattern  In this
section we will prove that this property also holds for many
other LDR families  Based on this result  we are able to
prove the universal approximation property of neural networks utilizing only LDR matrices 
Theorem   Let      be two       nonsingular diagonalizable matrices  De ne Sr
    as the set of matrices  
such that         has rank at most    Then the representation property holds for Sr
   Aq   aI for some positive integer       and   scalar
   cid    ii       aBq  is nonsingular  iii  the eigenvalues of
  have distinguishable absolute values 

    if   and   satisfy

Proof  It suf ces to prove for the case       as increasing
  only provides more candidate matrices to choose from 
By the property of Stein displacement  any matrix      
can be expressed in terms of       and its displacement as
follows 

   

Ak       Bk     aBq 

 

  cid 

  

Next we express         as   product of two vectors
    hT since it has rank   Also write          where
    diag        is   diagonal matrix generated by
the eigenvalues of    Now de ne ej to be the jth unit
column vector for            Write

QMej   

Ak       Bk     aBq ej

  

  cid 
  cid 
 cid    cid 
sh     cid Qg 

  

  

  

     kghT Bk     aBq ej

 

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

  

  cid 
  cid 
  cid 

  

  

 

 

such that the matrix cid   

Here we use sh   to denote the resulting scalar from matrix product hT Bk     aBq ej for            De ne
         aBq 
In order to prove the theorem  we
need to show that there exists   vector   and an index  
   sh     is nonsingular  In order
to distinguish scalar multiplication from matrix multiplication  we use notation       to denote the multiplication of
  scalar value and   matrices whenever necessary  Rewrite
the expression as

sh    

hT  cid BkTej   diag  

  cid 

      

diag hT   Bk         

 cid 

hT  cid    cid 

nej 

 ej  
 cid 

    hT  cid    cid 

BkT  

  

 ej

 diag

   BTk  

The diagonal matrix  cid   
note the column vector cid   

   sh     is nonsingular if and
only if all of its diagonal entries are nonzero  Let bij dei ej  Unless for every   there is an index ij such that bij       we can always choose an appropriate vector   so that the resulting
diagonal matrix is nonsingular  Next we will show that
the former case is not possible using proof by contradiction  Assume that there is   column bij       for every
             we must have 
 cid    cid 
   bi bi binn 
  cid 

     cid 

BkT  
in

BkT  
  

 cid 

en

  

  

 

BkT   diag  

 

      
in

 

  

  

Since   is diagonalizable  we write          where
    diag        Also we have          aBq   
             Then

 cid 

  

  cid 
 cid    cid 
  cid 

  

    

    

  

    diag

   

BTkdiag  
  

      
in

 

 cid 

 

 

      
in

           diag  

 cid 
 cid    cid 

diag

  

           in      cid 
 in      cid 

  cid 

       

  

  

          

           

 cid cid 

 

ii  By equation    nd

     cid    cid 

sh     cid 

QTv 

BkT  

nej

  

  

This implies that        in   are solutions to the equation

                 xq     

Thus it is incorrect to assume that matrix cid   

 
By assumption of matrix           have different absolute values  and so are           since all    have
the same absolute value because Aq   aI  This fact suggests that there are   distinguished solutions of equation
  which contradicts the fundamental theorem of algebra 
   sh     is
singular for all     Rn  With this property proven  given
any vector     Rn  one can take the following procedure
to  nd   matrix       and   index   such that the jth
column of   equals   
   Find   vector   and   index   such that matrix

 cid   

   sh     is nonsingular 

iii  Construct       with   and   by equation   Then
its jth column will equal to   
With the above construction  we have shown that for any
vector     Rn one can  nd   matrix       and   index  
such that the jth column of   equals    thus the theorem
is proved 

Our main goal of this section is to show that neural networks with many types of LDR matrices  LDR neural networks  can approximate continuous functions arbitrarily
well  In particular  we are going to show that Toeplitz matrices and circulant matrices  as speci   cases of LDR matrices  have the same property  In order to do so  we need to
introduce the following de nition of   discriminatory function and state one of its key property as Lemma  
De nition     function             is called as
discriminatory if the zero measure is the only measure  
that satis es the following property 

 wTx                Rn        

 

   

Lemma    cf   Cybenko    Any bounded  measurable sigmoidal function is discriminatory 

Now we are ready to present the universal approximation
theorem of LDR neural networks with nby kn weight
matrix   
Theorem    Universal Approximation Theorem for LDR
Neural Networks  Let   be any continuous discriminatory
function and Sr
    be   family of LDR matrices having
representation property  Then for any continuous function

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

      de ned on     and any       there exists   function
     in the form of equation   so that its weight matrix
consists of   submatrices from Sr

    and
                 

max
     

 

Proof  Denote the ith      submatrix of   as Wi  Then
  can be written as

   cid     Wk

 cid 

 

Let SI   denote the set of all continuous functions de ned
on      Let UI   be the linear subspace of SI   that can be
expressed in form of equation   where   consists of  
submatrices with displacement rank at most    We want to
show that UI   is dense in the set of all continuous functions
SI   
Suppose not  by HahnBanach Theorem  there exists  
bounded linear functional    cid    such that                
Moreover  By Riesz Representation Theorem    can be
written as

      

                  

for some measure  
Next we show that for any     Rn and        the function
 yTx     belongs to the set UI    and thus we must have

   

 yTx            

 
For any vector     Rn  Theorem   guarantees that there
exists an     LDR matrix        bn  and an index
  such that bj      Now de ne   vector         such
that        and                Also let the value
of all bias be   Then the LDR neural network function
becomes

 cid 

   

 cid 

  cid 

      

   bT

       

  

   bT

           yTx    

 

From the fact that            we derive that

         

 cid 

 

  cid 

   

  

 cid 

In

   bT

         

 yTx        

Theorem  which is obtained based on the assumption that
the set UI   of LDR neural network functions are not dense
in SI    As this assumption is not true  we have the universal approximation property of LDR neural networks 

Reference work  Cheng et al     Sindhwani et al 
  have utilized   circulant matrix or   Toeplitz matrix
for weight representation in deep neural networks  Please
note that for the general case of nby   weight matrices 
either the more general Blockcirculant matrices should
be utilized or padding extra columns or rows of zeroes
are needed  Cheng et al    Circulant matrices and
Topelitz matrices are both special form of LDR matrices 
and thus we could apply the above universal approximation property of LDR neural networks and provide theoretical support for the use of circulant and Toeplitz matrices in
 Cheng et al     Sindhwani et al    Moreover  it
is possible to consolidate the choice of parameters so that
  blockToeplitz matrix also shows Toeplitz structure globally  Therefore we arrive at the following corollary 
Corollary   Any continuous function can be arbitrarily
approximated by neural networks constructed with Toeplitz
matrices or circulant matrices  with padding or using
Blockcirculant matrices 

  Error Bounds on LDR Neural Networks
With the universal approximation property proved  naturally we seek ways to provide error bound estimates for
LDR neural networks  We are able to prove that for
LDR matrices de ned by      parameters    represents
the number of rows and has the same order as the number of columns  the corresponding structured neural network is capable of achieving integrated squared error of
order      where   is the number of parameters  This
result is asymptotically equivalent to Barron   aforementioned result on general neural networks  indicating that
there is essentially no loss for restricting to LDR matrices 
The functions we would like to approximate are those who
are de ned on   ndimensional ball Br        Rn        
        dx       where   is an arbitrary measure normalized so that  Br      Let   call this
set    Br   Barron    considered the following set of
bounded multiples of   sigmoidal function composed with
linear functions 
      yTx                   Rn          

   such that cid 

Br

Since     is   discriminatory function by Lemma   We
can conclude that   is the zero measure  As   result  the
function de ned as an integral with measure   must be zero
for any input function            The last statement contradicts the property that    cid    from the HahnBanach

He proved the following theorem 
Theorem    Barron    For every function in
   Br  every sigmoidal function   every probability measure  and every       there exists   linear combination of

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

sigmoidal functions fk    of the form

have the following equality

  cid 

fk     

   yT

         

 

      

 cid  kn cid 

  

such that  cid 

Br

         fk   dx        
 

 

 

Here yj   Rn and        for every              Moreover  the coef cients of the linear combination may be re 

stricted to satisfy cid  

    cj     rC 

Now we will show how to obtain   similar result for LDR
matrices  Fix operator        and de ne

Skn

   

   yT

                      yj   Rn 

  

                    
and              yin 
is an LDR matrix            
 

 cid 

 cid cid 

 
  be the set of function that can be exMoreover  let Gk
pressed as   sum of no more than   terms from    De ne
             dx  Theothe metric        
rem   essentially states that the minimal distance between
  function          and Gm
  is asymptotically     
  is in fact contained in
The following lemma proves that Gk
   
Skn
Lemma   For any       Gk

    Skn
   

Br

Proof  Any function fk      Gk
form

  can be written in the

fk     

   yT

         

 

For each            de ne         LDR matrix Wj such
that one of its column is yj  Let tij be the ith column
of Wj  Let ij correspond to the column index such that
tij   yj for all    Now consider the following function

  cid 

  

  cid 

  cid 

      

 ij tT

ijx      

 

  cid 

  

  cid 
  cid 
  cid 

  

 

 

 ij tT

ijx      

  

 ij   tT

ijx      

   yT

            fk   

 cid 

  

Notice that the matrix          Wk  consists  
LDR submatrices  Thus fk    belongs to the set Skn
   

  with Skn

By Lemma   we can replace Gk
  in Theorem
  and obtain the following error bound estimates on LDR
neural networks 
Theorem   For every disk Br   Rn  every function in
   Br  every sigmoidal function   every normalized measure   and every       there exists neural network de ned
by   weight matrix consists of   LDR submatrices such that

         fkn   dx        
 

 

 

Br

restricted to satisfy cid  

    ck     rC 

Moreover  the coef cients of the linear combination may be

Theorem   is the  rst theoretical result that gives   general error bound on LDR neural networks  Empirically 
 Cheng et al    reported that circulant neural networks are capable of achieving the same level of accuracy as AlexNet with more than    space saving on
fullyconnected layers 
 Sindhwani et al    applied
Toeplitztype LDR matrices to several benchmark image
classi cation datasets  retaining the performance of stateof theart models with very high compression ratio 
The next theorem naturally extended the result from  Liang
  Srikant    to LDR neural networks  indicating that
LDR neural networks can also bene     parameter reduction if one uses more than one layers  More precisely  we
have the following statement 
Theorem   Let   be   continuous function on     and
is        times differentiable in     for      cid log  
   
 cid  If               holds for all         and    

 cid      cid  then for any       matrices   and   satisfying

the conditions of Theorem   there exists   LDR neural
network GA      with   log  
    binary
step units    log   

    layers    log   
    recti er linear units such that
         GA         

max
  

  

  

where  ij   equals     and  ij     if    cid  ij  Notice that we

Proof  The theorem with better bounds and without assumption of being LDR neural network is proved in  Liang

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

  Srikant    as Theorem   For each binary step unit
or recti er linear unit in the construction of the general neural network  attach        dummy units  and expand the
weights associated to this unit from   vector to an LDR
matrix based on Theorem   By doing so we need to ex 
    and the
pand the number units by   factor of order log  
asymptotic bounds are relaxed accordingly 

  
 Gi

can be derived as 

  cid 
  cid 

  

  

  
 Gi

 

 

 Ak

      

 Ak

      

  
   Gik

 

  

 Wik

    HT
ik 

 

  Training LDR Neural Networks
In this section  we reformulate the gradient computation of
LDR neural networks  The computation for propagating
through   fullyconnected layer can be written as

     WT      

 
where   is the activation function      Rn kn is the
weight matrix      Rn is input vector and     Rkn is bias
vector  According to Equation   if Wi is an LDR matrix
with operators  Ai  Bi  satisfying conditions of Theorem
  then it is essentially determined by two matrices Gi  
Rn    Hi   Rn   as

 cid    cid 

 cid 

Wi  

Ak

  GiHT

  Bk
 

     aBq

   

 

  

To    the backpropagation algorithm  our goal is to com 
   for any objective funcpute derivatives   
 Gi
tion                  Wk 
In general  given that     WTx     we can have 

and   

    
 Hi

  
  

    

  
  

    

  
  

   

  
  

 

  
 

 

  
  

 

 

where   is   column vector full of ones  Let  Gik   Ak
 Hik   HT
derivatives of

  Gi 
    and Wik    Gik  Hik  The

can be computed as following 

       aBq

  Bk

  

 Wik

  

 Wik

 

  
 Wi

 

 

According to Equation   if we let     Wik       GT
ik
and      Hik  then   
   Gik

can be derived as 

and   
   Hik

 cid    

   GT
ik

 cid  

 cid   Hik

  
   Gik

 cid  

  

 Wik

   

  

 Wik

    HT
ik 

  
   Hik

   GT
ik

  

 Wik

 

 

 

Similarly  let      Gik       Ak

     and     Gi  then

Substituting with      Hik      HT
aBq

    we have   

derived as 

 Hi

  and     Bk

      

  cid 
  cid 

  

  

  
 Hi

 

 

       aBq
Bk

   

  
   Hik

  

       aBq
Bk

   

  

 Wik

    Gik 

 

which is equal to   
 Wi

can be calculated from previous layer and   

can be computed
and   
In this way  derivatives   
 Gi
 Hi
given   
  The essence of back 
 Wik
propagation algorithm is to propagate gradients backward
from the layer with objective function to the input layer 
   will be
  
 Wi
propagated to the next layer if necessary 
For practical use one may want to choose matrices Ai and
Bi with fast multiplication method such as diagonal matrices  permutation matrices  banded matrices  etc  Then
the space complexity  the number of parameters for storage  of Wi can be        nr  rather than      of
traditional dense matrix  The    is for Ai and Bi and
 nr is for Gi and Hi  The time complexity of WT
    will
be          nr  compared with      of dense matrix  Particularly  when Wi is   structured matrix like the
Toeplitz matrix  the space complexity will be      This
is because the Toeplitz matrix is de ned by    parameters 
Moreover  its matrixvector multiplication can be accelerated by using Fast Fourier Transform  for Toeplitz and circulant matrices  resulting in time complexity     log   
In this way the backpropagation computation for the layer
can be done with nearlinear time 

  Conclusion
In this paper  we have proven that the universal approximation property of LDR neural networks 
In addition 
we also theoretically show that the error bounds of LDR
neural networks are at least as ef cient as general unstructured neural network  Besides  we also develop the backpropagation based training algorithm for universal LDR
neural networks  Our study provides the theoretical foundation of the empirical success of LDR neural networks 

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple

layers of features from tiny images   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In Advances in neural information processing
systems  pp     

Liang  Shiyu and Srikant     Why deep neural networks 

arXiv preprint arXiv   

Liu  Baoyuan  Wang  Min  Foroosh  Hassan  Tappen  Marshall  and Pensky  Marianna  Sparse convolutional neural networks  In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pp   
 

Montufar  Guido    Pascanu  Razvan  Cho  Kyunghyun 
and Bengio  Yoshua  On the number of linear regions
of deep neural networks  In Advances in neural information processing systems  pp     

Pan  Victor  Structured matrices and polynomials  uni ed
superfast algorithms  Springer Science   Business Media   

Pan  Victor    Svadlenka  John  and Zhao  Liang  Estimating the norms of random circulant and toeplitz matrices
and their inverses  Linear algebra and its applications 
   

Sindhwani  Vikas  Sainath  Tara  and Kumar  Sanjiv  StrucIn
tured transforms for smallfootprint deep learning 
Advances in Neural Information Processing Systems  pp 
   

Sutskever  Ilya  Vinyals  Oriol  and Le  Quoc   

Sequence to sequence learning with neural networks 
In
Advances in neural information processing systems  pp 
   

Telgarsky  Matus  Bene ts of depth in neural networks 

arXiv preprint arXiv   

Wen  Wei  Wu  Chunpeng  Wang  Yandan  Chen  Yiran 
and Li  Hai  Learning structured sparsity in deep neural
networks  In Advances in Neural Information Processing
Systems  pp     

References
Barron  Andrew    Universal approximation bounds for
superpositions of   sigmoidal function  IEEE Transactions on Information theory     

Bini  Dario  Pan  Victor  and Eberly  Wayne  Polynomial
and matrix computations volume   Fundamental algorithms  SIAM Review     

Cheng  Yu  Yu  Felix    Feris  Rogerio    Kumar  Sanjiv 
Choudhary  Alok  and Chang  ShiFu  An exploration of
parameter redundancy in deep networks with circulant
In Proceedings of the IEEE International
projections 
Conference on Computer Vision  pp     

Cybenko  George  Approximation by superpositions of  
sigmoidal function  Mathematics of Control  Signals 
and Systems  MCSS     

Delalleau  Olivier and Bengio  Yoshua  Shallow vs  deep
sumproduct networks  In Advances in Neural Information Processing Systems  pp     

Deng  Jia  Dong  Wei  Socher  Richard  Li  LiJia  Li  Kai 
and FeiFei  Li 
Imagenet    largescale hierarchical
image database  In Computer Vision and Pattern Recognition    CVPR   IEEE Conference on  pp   
  IEEE   

Denton  Emily    Zaremba  Wojciech  Bruna  Joan  LeCun  Yann  and Fergus  Rob  Exploiting linear structure
within convolutional networks for ef cient evaluation  In
Advances in Neural Information Processing Systems  pp 
   

Gong  Yunchao  Liu  Liu  Yang  Ming  and Bourdev 
Lubomir  Compressing deep convolutional networks using vector quantization  arXiv preprint arXiv 
 

Han  Song  Mao  Huizi  and Dally  William    Deep compression  Compressing deep neural networks with pruning  trained quantization and huffman coding  arXiv
preprint arXiv   

Hornik  Kurt  Approximation capabilities of multilayer
feedforward networks  Neural networks   
 

Hornik  Kurt  Stinchcombe  Maxwell  and White  Halbert 
Multilayer feedforward networks are universal approximators  Neural networks     

Jaderberg  Max  Vedaldi  Andrea  and Zisserman  Andrew 
Speeding up convolutional neural networks with low
rank expansions  arXiv preprint arXiv   

