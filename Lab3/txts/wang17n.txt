  Uni ed Variance ReductionBased Framework for Nonconvex LowRank

Matrix Recovery

Lingxiao Wang     Xiao Zhang     Quanquan Gu  

Abstract

We propose   generic framework based on  
new stochastic variancereduced gradient descent
algorithm for accelerating nonconvex lowrank
matrix recovery  Starting from an appropriate
initial estimator  our proposed algorithm performs projected gradient descent based on  
novel semistochastic gradient speci cally designed for lowrank matrix recovery  Based upon
the mild restricted strong convexity and smoothness conditions  we derive   projected notion of
the restricted Lipschitz continuous gradient property  and prove that our algorithm enjoys linear
convergence rate to the unknown lowrank matrix with an improved computational complexity  Moreover  our algorithm can be employed to
both noiseless and noisy observations  where the
 near  optimal sample complexity and statistical
rate can be attained respectively  We further illustrate the superiority of our generic framework
through several speci   examples  both theoretically and experimentally 

  Introduction
Lowrank matrix recovery problem has been extensively
studied during the past decades  due to its wide range
of applications  such as collaborative  ltering  Srebro
et al    Rennie   Srebro    and multilabel learning  Cabral et al    Xu et al    The objective of
lowrank matrix recovery is to estimate the unknown lowrank matrix      Rd    from partial observations  such
as   set of linear measurements in matrix sensing or   subset
of its entries in matrix completion  Signi cant efforts have
been made to estimate lowrank matrices  among which
one of the most prevalent approaches is nuclear norm re 

 Equal contribution  Department of Computer Science  University of Virginia  Charlottesville  Virginia  USA  Correspondence to  Quanquan Gu  qg   virginia edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

laxation based optimization  Srebro et al    Cand es  
Tao    Rohde et al    Recht et al    Negahban   Wainwright      Gui   Gu    While
such convex relaxation based methods enjoy   rigorous theoretical guarantee to recover the unknown lowrank matrix 
due to the nuclear norm regularization minimization  these
algorithms involve   singular value decomposition at each
iteration  whose time complexity is      to recover      
matrix  Hence  they are computationally very expensive 
In order to address the aforementioned computational issue  recent studies  Keshavan et al      Jain et al 
    Jain   Netrapalli    Hardt    Hardt  
Wootters    Hardt et al    Zhao et al    Chen
  Wainwright    Sun   Luo    Zheng   Lafferty 
    Tu et al    Bhojanapalli et al    Park
et al      Wang et al    have been carried out to
perform factorization on the matrix space  which naturally
ensures the lowrankness of the produced estimator  Although this matrix factorization technique converts the previous optimization problem into   nonconvex one  which is
more dif cult to analyze  it signi cantly improves the computational ef ciency 
However  for largescale matrix recovery  such nonconvex
optimization approaches are still computationally expensive  because they are based on gradient descent or alternating minimization  which involve the timeconsuming
calculation of full gradient at each iteration  De Sa et al 
  developed   stochastic gradient descent approach
for Gaussian ensembles  but the sample complexity      
number of measurements or observations required for exact
recovery  of their algorithm is not optimal  Recently  Jin
et al    and Zhang et al      proposed stochastic
gradient descent algorithms for noiseless matrix completion and matrix sensing  respectively  Although these algorithms achieve linear rate of convergence and improved
computational complexity over aforementioned deterministic optimization based approaches  they are limited to speci   lowrank matrix recovery problems  and unable to be
extended to more general problems and settings 
In this paper  inspired by the idea of variance reduction
for stochastic gradient  Schmidt et al    Kone cn    
Richt arik    Johnson   Zhang    Defazio et al 

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

      Mairal    Xiao   Zhang    Kone cn  
et al    Reddi et al    AllenZhu   Hazan   
Chen   Gu    Zhang   Gu    we propose   uni 
 ed stochastic gradient descent framework with variance
reduction for lowrank matrix recovery  which integrates
both optimizationtheoretic and statistical analyses  To the
best of our knowledge  this is the  rst uni ed accelerated
stochastic gradient descent framework for lowrank matrix
recovery with strong convergence guarantees  With   desired initial estimator given by   general initialization algorithm  we show that our algorithm achieves linear convergence rate and better computational complexity against the
stateof theart algorithms  The contributions of our work
are further highlighted as follows 
  We develop   generic stochastic variancereduced gradient descent algorithm for lowrank matrix recovery  which
can be applied to various low rankmatrix estimation problems  including matrix sensing  noisy matrix completion
and onebit matrix completion  In particular  for noisy matrix sensing  it is guaranteed to linearly converge to the unknown lowrank matrix up to the minimax statistical precision  Negahban   Wainwright    Wang et al   
while for noiseless matrix sensing  our algorithm achieves
the optimal sample complexity  Recht et al    Tu et al 
  Wang et al    and attains   linear rate of convergence  Besides  for noisy matrix completion  it achieves
the bestknown sample complexity required by nonconvex
matrix factorization  Zheng   Lafferty   
  At the core of our algorithm  we construct   novel
semistochastic gradient term  which is substantially different from the one if following the original stochastic
variancereduced gradient using chain rule  Johnson  
Zhang    This uniquely constructed semistochastic
gradient has not appeared in the literature  and is essential
for deriving the minimax optimal statistical rate 
  Our uni ed framework is built upon the mild restricted
strong convexity and smoothness conditions  Negahban
et al    Negahban   Wainwright    regarding the
objective function  Based on the above mentioned conditions  we derive an innovative projected notion of the restricted Lipschitz continuous gradient property  which we
believe is of independent interest for other nonconvex problems to prove sharp statistical rates  We further establish
the linear convergence rate of our generic algorithm  Besides  for each speci   examples  we verify that the conditions required in the generic setting are satis ed with
high probability  which demonstrates the applicability of
our framework 
  Our algorithm has   lower computational complexity compared with existing approaches  Jain et al     
Zhao et al    Chen   Wainwright    Zheng   Lafferty      Tu et al    Bhojanapalli et al   

Park et al      Wang et al    More speci cally 
to achieve   precision  the gradient complexity  of our al 

gorithm is           log  Here   denotes the

total number of observations    denotes the dimensionality
of the unknown lowrank matrix      denotes the batch
size  and   denotes the condition number of     see Section   for   detailed de nition  In particular  if the condition number satis es          our algorithm is computationally more ef cient than the stateof theart generic
algorithm in Wang et al   
Notation  We use     and Id to denote                and
    identity matrix respectively  We write       Id  if
    Rd    is orthonormal  For any matrix     Rd   
we use Ai  and     to denote the ith row and jth column of    respectively 
In addition  we use Aij to denote the       th element of    Denote the row space and
column space of   by row    and col    respectively 
Let     max       and     be the  th largest singular value of    For vector     Rd  we use kxkq  
  xi     to denote its    vector norm for          
  
Denote the spectral and Frobenius norm of   by kAk  and
kAkF respectively  We use kAk    maxi    Aij  to
denote the elementwise in nity norm of    and we use
kAk  to represent the largest  norm of its rows  Given
two sequences  an  and  bn  we write an     bn  if
there exists   constant            such that an     bn 
Note that other notations are de ned throughout the paper 

  Methodology
In this section  we present our generic stochastic gradient
descent algorithm with variance reduction as well as several illustrative examples 

  Stochastic VarianceReduced Gradient for

LowRank Matrix Recovery

First  we brie   introduce the general problem setup for
lowrank matrix recovery  Suppose      Rd    is an unknown rankr matrix  Let the singular value decomposition
 SVD  of    be           where      Rd   
     Rd   are orthonormal matrices  and     Rr  
is   diagonal matrix  Let                be
the sorted nonzero singular values of    and denote the
condition number of    by                Besides  let
        and         Recall that we
aim to recover    through   collection of   observations
or measurements  Let LN   Rd        be the sample
loss function  which evaluates the  tness of any matrix  
associated with the total   observations  Then the lowrank
 Gradient complexity is de ned as the number of gradients

calculated in total 

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

matrix recovery problem can be formulated as follows 

minX Rd    LN        

NPN
subject to        rank        

      

 

 

    UV 

NPN

where       measures the  tness of   associated with the
ith observation  Here    Rd    is   feasible set  such
that         In order to more ef ciently estimate the unknown lowrank matrix  following Jain et al      Tu
et al    Zheng   Lafferty   Park et al     
Wang et al    we decompose   as UV  and consider
the following nonconvex optimization problem via matrix
factorization 
minU        LN  UV     
where      Rd        Rd   are the rotationinvariant
sets induced by    Recall    can be factorized as     
     then we need to make sure that         and
        Besides  it can be seen from   that the optimal
solution is not unique in terms of rotation  In order to deal
with such identi ability issue  following Tu et al   
Zheng   Lafferty   Park et al      we consider
the following regularized optimization problem 
minU        FN          LN  UV           
where the regularization term is de ned as          
    We further decompose the objective
kU      Vk 
function FN        into   components to apply stochastic
variancereduced gradient descent 

FN           

   Fi      

 

where we assume     nb  and   denotes batch size      
the number of observations associated with each Fi  More
speci cally  we have

nPn

 

Fi         Li UV           
bPb
Li UV     
    ij  UV 

Therefore  based on   and   we are able to apply the
stochastic variancereduced gradient  which is displayed as
Algorithm   As will be seen in later theoretical analysis 
the variance of the proposed stochastic gradient indeed decreases as the iteration number increases  which leads to  
faster convergence rate  Let PCi be the projection operator
onto the feasible set Ci in Algorithm   where        
Note that our proposed Algorithm   is different from the
standard stochastic variancereduced gradient algorithm
 Johnson   Zhang    in several aspects  First  instead
of conducting gradient descent directly on    our algorithm
performs alternating stochastic gradient descent on the factorized matrices   and    which leads to   better computational complexity but   more challenging analysis  Second 

we construct   novel semistochastic gradient term for  

 resp     as rUFit         rL it eX     rLN  eX   
which is different from rUFit         rUFit eU eV   
rUFN  eU eV  if following the original stochastic variance

reduced gradient descent  Johnson   Zhang    This
uniquely devised semistochastic gradient is essential for
deriving the minimax optimal statistical rate  Last but not
least  we introduce   projection step to ensure that the estimator produced at each iteration belongs to   feasible set 
which is necessary for various lowrank matrix recovery
problems  We also note that Reddi et al    AllenZhu
  Hazan   recently developed SVRG algorithms for
general nonconvex  nitesum optimization problem  However  their algorithms only guarantee   sublinear rate of
convergence to   stationary point  and cannot exploit the
special structure of lowrank matrix factorization  In stark
contrast  our algorithm is able to leverage the structure of
the problem and guaranteed to linearly converge to the unknown lowrank matrix instead of   stationary point 
Algorithm   LowRank Stochastic VarianceReduced Gradient Descent  LRSVRG 
Input  loss function LN  step size   number of iterations
      initial solution  eU eV 
eU   eUs  eV   eVs  eX   eUeV 
     eU       eV
for                          do
Randomly pick it                 
Ut    PC Ut    rUFit Ut  Vt 
 rLit eX Vt   rLN  eX Vt 
Vt    PC Vt    rVFit Ut  Vt 
 rLit eX Ut   rLN  eX Ut 
 eUs eVs     Ut  Vt  random                    
Output   eUS eVS 
Algorithm   Initialization
Input  loss function LN  step size   iteration number    

for                    do

end for

end for

initialize        
for                      do

 

     

end for
  

Xt   Pr Xt     rLN  Xt 
eU     
Output   eU eV 

  eV     

    SVDr XT  
 

 

 

 

As will be seen in later analysis  Algorithm   requires  
good initial solution to guarantee the linear convergence
rate  To obtain such an initial solution  we employ the initialization algorithm in Algorithm   which is originally
proposed in Wang et al    For any rankr matrix
    Rd    we use SVDr    to denote its singular

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

value decomposition  If SVDr               we use
Pr           to denote the best rankr approximation
of    or in other words  Pr denotes the projection operator
such that Pr      argminrank     kX   YkF  
  Applications to Speci   Models
In this subsection  we introduce two examples  which include matrix sensing and matrix completion  to illustrate
the applicability of our proposed algorithm  Algorithm  
The application of our algorithm to onebit matrix completion can be found in Appendix    To apply the proposed
method  we only need to specify the form of FN       
for each speci   model  as de ned in  

  MATRIX SENSING
In matrix sensing  Recht et al    Negahban   Wainwright    we intend to recover the unknown matrix
     Rd    with rankr from   set of noisy linear measurements such that              where the linear
measurement operator     Rd      RN is de ned
as         hA  Xi hA  Xi         hAN   Xi  for any
    Rd    Here   denotes the number of observations 
and   represents   subGaussian noise vector with        elements and parameter   In addition  for each sensing matrix Ai   Rd    it has        standard Gaussian entries 
Therefore  we formulate FN        for matrix sensing as
follows FN            Pn
   FSi       where for
each component function  we have FSi         kySi  
              Note that         deASi UV   
notes the regularizer  which is de ned in Section   In
addition   Si  
   denote the mutually disjoint subsets such
  Si        and ASi is de ned as   linear measurethat   
ment operator ASi   Rd      Rb  satisfying ASi     
 hAi  Xi hAi  Xi         hAib  Xi  with corresponding
observations ySi    yi  yi          yib 
  MATRIX COMPLETION
For matrix completion with noisy observations  Rohde
et al    Koltchinskii et al    Negahban   Wainwright    our primary goal is to recover the unknown
lowrank matrix      Rd    from   set of randomly observed noisy elements  For example  one commonlyused
model is the uniform observation model  which is de ned
as follows 

Yjk     jk   Zjk  with probability   

otherwise 

 

where     Rd    is   noise matrix such that each
element Zjk follows        Gaussian distribution with
variance       and we call     Rd    the obIn particular  we observe each eleservation matrix 
ments independently with probability         We

    Si

each component function is de ned as   Si

denote               by the index set of the observed entries  then         for matrix completion is
formulated as             Pn
       where
        
      Si
 Uj        Yjk              Note that
   denote the mutually disjoint subsets such that
 Si  
  Si    
In addition  we have  Si      for
  
                such that     nb 
  Main Theory
In this section  we present our main theoretical results
for Algorithms   and   We  rst introduce several de 
nitions for simplicity  Recall that the singular value decomposition of    is           then following Tu et al    Zheng   Lafferty   we de ne
                as the corresponding lifted positive semide nite matrix of      Rd    in higher dimension

             

                

where                 and    is
de ned as                        Besides  we
de ne the solution set in terms of the true parameter    as
follows 

   nZ                    for some     Qro 

where Qr denotes the set of       orthonormal matrices 
According to this de nition  for any        we can obtain
     ZU       where ZU and ZV denote the top        and
bottom        matrices of             respectively 
De nition   De ne the distance between   and    in
terms of the optimal rotation as         such that

          min

eZ   kZ  eZkF   min

  Qr kZ     RkF  

Note that if              we have kX     kF  
cp         where   is   constant  Yi et al   
De nition   De ne the neighbourhood of    with radius   as

      nZ                      Ro 

Next  we lay out several conditions  which are essential for
proving our main theory  We impose restricted strong convexity  RSC  and smoothness  RSS  conditions  Negahban
et al    Loh   Wainwright    on the sample loss
function LN 
Condition    Restricted Strong Convexity  Assume LN
is restricted strongly convex with parameter   such that for
all matrices        Rd    with rank at most   
 
 kY   Xk 
LN                hrLN         Xi  
   

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

Condition    Restricted Strong Smoothness  Assume
LN is restricted strongly smooth with parameter    such
that for all matrices        Rd    with rank at most   
 
  kY   Xk 
LN                hrLN         Xi  
   

Based on Conditions   and   we prove that the sample loss function LN satis es   projected notion of the
restricted Lipschitz continuous gradient property as displayed in the following lemma 
Lemma   Suppose the sample loss function LN satis es Conditions   and   For any rankr matrices
       Rd    let the singular value decomposition of
  be     

    then we have

LN                hrLN         Yi

 

 

 

 

 

 LkeU rLN       rL        
  
 Lk rLN       rL      eVk 
where eU   Rd    is an orthonormal matrix with        
which satis es col      col eU  and eV   Rd    is an
orthonormal matrix with         that satis es col     
col eV  and   is the RSS parameter 

Lemma   is essential to analyze the nonconvex optimization for lowrank matrix recovery and derive   linear convergence rate  Since the RSC and RSS conditions can
only be veri ed over the subspace of lowrank matrices 
the standard Lipschitz continuous gradient property could
not be derived  That is why we need such   restricted version of Lipschitz continuous gradient property  To the best
of our knowledge  this new notion of Lipschitz continuous
gradient has never been proposed in the literature before 
We believe it can be of broader interests for other nonconvex optimization problems to prove tight bounds 
Moreover  we assume that the gradient of the sample loss
function rLN at    is upper bounded 
Condition   Recall the unknown rankr matrix     
Rd    Given    xed sample size   and tolerance parameter         we let       be the smallest scalar such
that with probability at least       we have
krLN              

where       depends on sample size   and  
Finally  we assume that each component loss function Li in
  satis es the restricted strong smoothness condition 
Condition    Restricted Strong Smoothness for each
Component  Given    xed batch size    assume Li is restricted strongly smooth with parameter    such that for

all matrices        Rd    with rank at most   
Li              hrLi        Xi  

  
  kY   Xk 
   

In latter analysis for generic setting  we assume that Conditions   hold  while for each speci   model  we will
verify these conditions respectively in the appendix 

  Results for the Generic Setting
The following theorem shows that  in general  Algorithm  
converges linearly to the unknown lowrank matrix    up
to   statistical precision 
Theorem    LRSVRG  Suppose that Conditions  
    and   are satis ed  There exist constants

         and    such that for any eZ     eU eV   
         with      min         if
the sample size   is large enough such that        
        where     min    and the con 
     
  
traction parameter   is de ned as follows 

   

 

     

   

      

 

then with the step size        and the number of iter 

 

        
      

outputed from Algorithm   satis es

ations   properly chosen  the estimator eZS    eUS eVS 
    eZS        Sd eZ      

with probability at least      
Remark   Theorem   implies that to achieve linear
rate of convergence  it is necessary to set the step size  
to be small enough and the inner loop iterations   to be
large enough such that     Here we present   speci  
example to demonstrate such   is attainable  As stated in
Theorem   if we set the step size        where

           then the contraction parameter   is

calculated as follows 

   

 

  

 

 
 

 

Therefore  under the condition that        we obtain
          which leads to the linear convergence rate
of Algorithm   Besides  our algorithm also achieves the
linear convergence in terms of reconstruction error  since
the reconstruction error keXs     
  can be upper bounded
by        eZs     where   is   constant 

Remark   The right hand side of   consists of two
parts  where the  rst one represents the optimization error and the second one denotes the statistical error  Note
that in the noiseless case  since           the statistical error becomes zero  As stated in Remark   with

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

appropriate   and    we are able to achieve the linear rate
of convergence  Therefore  in order to make sure the op 

timization error satis es  Sd eZ         it suf ces to
perform        log  outer loop iterations  Recall

that from Remark   we have        Since for
each outer loop iteration  it is required to calculate   mixed
stochastic variancereduced gradients and one full gradient 
the overall gradient complexity for our algorithm to achieve
  precision is

          log   
 

However  the gradient complexity of the stateof theart
gradient descent based algorithm  Wang et al    to

achieve   precision is      log  Therefore  provided that        our method is computationally more ef 
 cient than the stateof theart gradient descent approach 
The detailed comparison of the overall computational complexity among different methods for each speci   model
can be found in next subsection 

  according to Lemma   in Tu et al    it suf ces

To satisfy the initial conditioneZ             in Theorem
to guarantee that eX  is close enough to the unknown rankr matrix    such that keX      kF        where    
min      The following theorem shows the output
of Algorithm   can satisfy this condition 
Theorem    Wang et al    Suppose the sample
loss function LN satis es Conditions     and   Let
eX    eU eV  where  eU eV  is the produced initial solution in Algorithm   If          then with step
size         we have with probability at least       that

keX      kF    TkX kF  

         
      
where              is the contraction parameter 
Theorem   suggests that  in order to guarantee keX   
  kF        we need to perform at
least    
log     kX kF   log  number of iterations to ensure
the optimization error is small enough  and it is also necessary to make sure the sample size   is large enough such
that                        which corresponds

to   suf ciently small statistical error 

 

  Implications for Speci   Models
In this subsection  we demonstrate the implications of our
generic theory to speci   models  For each speci   model 
we only need to verify Conditions   We denote    
max       in the following discussions 
  MATRIX SENSING
We provide the theoretical guarantee of our algorithm for
matrix sensing 

 

 

 

  satis es

Corollary   Consider matrix sensing with standard
normal linear operator   and noise vector   whose entries
follow        subGaussian distribution with parameter  
There exist constants  ci 
   such that if the number of
observations satis es       rd and we choose the parameters        where                 then
for any initial solution satis eseZ             with probability at least      exp       the output of Algorithm
    eZS        Sd eZ          rd
the output of our algorithm achieves   prd    statistical error after    log   rd  number of outer loop it 

where the contraction parameter    
Remark   According to   in the noisy setting 

erations  This statistical error matches the minimax lower
bound for matrix sensing  Negahban   Wainwright   
In the noiseless case  to ensure the restricted strong convexity and smoothness conditions of our objective function  we require sample size       rd  which attains
the optimal sample complexity for matrix sensing  Recht
et al    Tu et al    Wang et al    Most importantly  from Remark   we know that for the output

of our algorithm to achieve   precision for matrix sensing

computational complexity for the stateof theart gradient
descent algorithms in both noiseless  Tu et al    and
noisy  Wang et al    cases to obtain   precision is

eZS of our algorithm  the overall computational complexity
is           bd  log  Nevertheless  the overall
        log  Therefore  our algorithm is more ef 
 cient provided that        which is consistent with the
result obtained by  Zhang et al     
In their work 
they proposed an accelerated stochastic gradient descent
method for matrix sensing based on the restricted isometry property  However  since the restricted isometry property is more restrictive than the restricted strong convex and
smoothness conditions  their results cannot be applied to
more general lowrank matrix recovery problems 

  MATRIX COMPLETION
We provide the theoretical guarantee of our algorithm for
matrix completion  In particular  we consider   partial observation model  which means only the elements over  
subset              are observed  In addition  we assume   uniform sampling model for     which is de ned
as                 uniform        uniform   
To avoid overly sparse matrices  Gross    Negahban
  Wainwright    we impose the following incoherence condition  Cand es   Recht    More specifically  suppose the singular value decomposition of   
is           we assume the following condikV     

tions hold kU            

and

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

rd log  

 

 

       where   denotes the rank of    and   denotes

the incoherence parameter for   
In order to make sure our produced estimator satis es incoherence constraint  we need   projection step  which is
displayed in Algorithm   Therefore  we construct two fea 

sible sets Ci        Rdi     kAk        where
           di  and         Thus for any       
and        we have     UV           
Rd    kAk      where          pd   

We have the following convergence result of our algorithm
for matrix completion 
Corollary   Consider noisy matrix completion under
uniform sampling model  Suppose    satis es incoherence condition  There exist constants  ci 
   such that if
we choose parameters        where         
       and the number of observations satis es
          log    then for any initial solution satis es
eZ             then with probability at least         
the output of Algorithm   satis es
    eZS        Sd eZ         
where     max      
   
  Corollary
Remark
   log       log    number of outer
algorithm achieves   rpd log      statistical error 
bound   prd log       for matrix completion proved in

 
  the contraction parameter
after
 
that
loops  our

Negahban   Wainwright   Koltchinskii et al   
And its sample complexity is       log    which matches
the bestknown sample complexity of matrix completion
using nonconvex matrix factorization  Zheng   Lafferty 
  Recall that from Remark   the overall computational complexity of our algorithm to reach   accuracy

which is near optimal compared with the minimax lower

However  for the stateof theart gradient descent based
algorithms  the computational complexity for both noiseless  Zheng   Lafferty    and noisy  Wang et al 

for matrix completion is              log 
  cases to obtain   accuracy is          log 
Thus the computational complexity of our algorithm is
lower than the stateof theart gradient descent methods
if we have        In addition  for the online stochastic
gradient descent algorithm  Jin et al    the overall
computational complexity is       log  Since
their results has   fourth power dependency on both   and
  our method can yield   signi cant improvement over
the online method when      is large 

implies

numerical simulations and real data experiments 

  Numerical Simulations
We  rst investigate the effectiveness of our proposed algorithm compared with the stateof theart gradient descent
algorithm  Wang et al    Zheng   Lafferty   
Then  we evaluate the sample complexity required by both
methods to achieve exact recovery in the noiseless case 
Finally  we illustrate the statistical error of our method in
the noisy case  Note that both algorithms use the same initialization method  Algorithm   with optimal parameters
selected by cross validation  Furthermore  all results are
averaged over   trials  Note that due to the space limit 
we only lay out simulation results for matrix completion 
results for other models can be found in Appendix   
For matrix completion  we consider the unknown lowrank
matrix    in the following settings                 
         ii                       iii      
               First  we generate the unknown lowrank matrix    as           where      Rd   and
     Rd   are randomly generated  Next  we use uniform observation model to obtain data matrix    Finally 
we consider two settings    noisy case  the noise follows
       normal distribution with variance       and  
noiseless case 
For the results of convergence rate  we show the mean
        in log scale versus number of effective data passes  Figures     and     illustrate
the linear rate of convergence of our algorithm  LRSVRG 
in the setting     The results imply that after the same number of effective data passes  our algorithm is more ef cient
than the stateof theart gradient descent algorithm in estimation error  For the results of sample complexity  we
illustrate the empirical probability of exact recovery under

squared error kbX      

given by different algorithms  it is considered to achieve ex 

rescaled sample size   rd log    For the estimator bX
act recovery  if the relative error kbX   kF  kX kF is less

than   Figure     shows the empirical recovery probability of different methods in the setting     It implies  
phase transition around      rd log    Although our theoretical results requires       log    sample complexity 
the simulation results suggest that our method achieves the
optimal sample complexity       rd log    Note that
we leave out results in other settings to avoid redundancy
since we get similar patterns for these results  The results
of statistical error are displayed in Figure     which is
consistent with our main result in Corollary  

  Experiments
In this section  we present the experimental performance
of our proposed algorithm for different models based on

  Real Data Experiments
We apply our proposed stochastic variancereduced gradient algorithm for matrix completion to collaborative  ltering in recommendation system  and compare it with sev 

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

Table   Experimental results of collaborative  ltering in terms of averaged RMSE and CPU time for different algorithms 

Dataset

Performance

SVP

SOFTIMPUTE ALTMIN

TNC

RIMP

NUCLEAR

SCAD

GD

LRSVRG

JESTER 

JESTER 

JESTER 

RMSE
Time    
RMSE
Time    
RMSE
Time    

 

 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 

 

 

 

 

 

GD
LRSVRG

 

 

 

 

 

 

 

 

 

 

GD
LRSVRG

 

 

 

 

 

 

 

 

 

 

 

GD
LRSVRG

 

 

 

 
 
 
 

 

 
 
 
 

      
      
      

 

 

 

 

 

 

 

 

 

 

 

  rd log  

    Noiseless Case

    Sample Complexity

    Noisy Case

    Statistical Error

Figure   Numerical results for matrix completion      and     Rate of convergence for matrix completion in the noiseless and noisy
        versus number of effective data passes  which demonstrates the
effectiveness of our method      Empirical probability of exact recovery versus   rd log        Statistical error for matrix completion 

        versus rescaled sample size   rd log   

case  respectively  logarithm of mean squared error kbX      
mean squared error in Frobenius norm kbX       

eral stateof theart matrix completion algorithms  including singular value projection  SVP   Jain et al    trace
norm constraint  TNC   Jaggi et al    alternating minimization  AltMin   Jain et al      spectral regularization  SoftImpute   Mazumder et al    rankone matrix pursuit  Wang et al    nuclear norm penalty  Negahban   Wainwright    nonconvex SCAD penalty
 Gui   Gu    and gradient descent  Zheng   Lafferty    In particular  we use three large recommendation datasets called Jester  Jester  and Jester   Goldberg et al    which contain anonymous ratings on
  jokes from different users  The jester datasets consist
of       rows and   columns respectively  with           ratings correspondingly 
Besides  the rating scales take value from     Our
goal is to recover the whole rating matrix based on partial observations  Therefore  we randomly choose half of
the ratings as our observed data  and predict the other half
based on different matrix completion algorithms  We perform   different observed unobserved entry splittings  and
record the averaged root mean square error  RMSE  as well
as CPU time for different algorithms  We summarize the
comparisons in Table   which suggests that our proposed
LRSVRG algorithm outperforms all the other baseline algorithms in terms of RMSE and CPU time  which aligns
well with our theory 

  Conclusions and Future Work
We proposed   uni ed stochastic variancereduced gradient descent framework for lowrank matrix recovery that
integrates both optimizationtheoretic and statistical analyses  Based on the mild restricted strong convexity and
smoothness conditions  we derived   projected notion of
the restricted Lipschitz continuous gradient property  and
established the linear convergence rate of our proposed algorithm  With an appropriate initialization procedure  we
proved that our algorithm enjoys improved computational
complexity compared with existing approaches  There are
still many interesting problems along this line of research 
For example  we will study accelerating the lowrank plus
sparse matrix tensor recovery  Gu et al      Yi
et al    Zhang et al      through variance reduction technique in the future 

Acknowledgment
We would like to thank the anonymous reviewers for their
helpful comments  This research was sponsored in part
by the National Science Foundation IIS  and IIS 
  The views and conclusions contained in this paper are those of the authors and should not be interpreted
as representing any funding agencies 

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

References
Agarwal  Alekh  Negahban  Sahand  and Wainwright  Martin   
Fast global convergence rates of gradient methods for highdimensional statistical recovery  In Advances in Neural Information Processing Systems  pp     

AllenZhu  Zeyuan and Hazan  Elad  Variance reduction for faster
nonconvex optimization  arXiv preprint arXiv 
 

Bhaskar  Sonia   and Javanmard  Adel   bit matrix completion under exact lowrank constraint  In Information Sciences
and Systems  CISS     th Annual Conference on  pp   
IEEE   

Bhojanapalli  Srinadh  Kyrillidis  Anastasios  and Sanghavi  Sujay  Dropping convexity for faster semide nite optimization 
arXiv preprint   

Cabral  Ricardo Silveira  De la Torre  Fernando  Costeira 
Jo ao Paulo  and Bernardino  Alexandre  Matrix completion
for multilabel image classi cation  In NIPS  volume   pp 
   

Cai  Tony and Zhou  WenXin    maxnorm constrained minimization approach to  bit matrix completion  Journal of Machine Learning Research     

Cand es  Emmanuel   and Recht  Benjamin  Exact matrix completion via convex optimization  Foundations of Computational
mathematics     

Cand es  Emmanuel   and Tao  Terence  The power of convex
relaxation  Nearoptimal matrix completion  Information Theory  IEEE Transactions on     

Chen  Jinghui and Gu  Quanquan  Accelerated stochastic block
coordinate gradient descent for sparsity constrained nonconvex
optimization  In Conference on Uncertainty in Arti cial Intelligence   

Chen  Yudong and Wainwright  Martin    Fast lowrank estimation by projected gradient descent  General statistical and algorithmic guarantees  arXiv preprint arXiv   

Davenport  Mark    Plan  Yaniv  van den Berg  Ewout  and Wootters  Mary   bit matrix completion  Information and Inference 
   

De Sa  Christopher  Olukotun  Kunle  and      Christopher 
Global convergence of stochastic gradient descent for some
nonconvex matrix problems  arXiv preprint arXiv 
 

Gross  David  Recovering lowrank matrices from few coef 
cients in any basis  IEEE Transactions on Information Theory 
   

Gu  Quanquan  Gui  Huan  and Han  Jiawei  Robust tensor decomposition with gross corruption  In Advances in Neural Information Processing Systems  pp     

Gu  Quanquan  Wang  Zhaoran Wang  and Liu  Han  Lowrank
In

and sparse structure pursuit via alternating minimization 
Arti cial Intelligence and Statistics  pp     

Gui  Huan and Gu  Quanquan  Towards faster rates and oracle property for lowrank matrix estimation  arXiv preprint
arXiv   

Hardt  Moritz  Understanding alternating minimization for matrix

completion  In FOCS  pp    IEEE   

Hardt  Moritz and Wootters  Mary  Fast matrix completion with 

out the condition number  In COLT  pp     

Hardt  Moritz  Meka  Raghu  Raghavendra  Prasad  and Weitz 
In

Benjamin  Computational limits for matrix completion 
COLT  pp     

Jaggi  Martin  Sulovsk  Marek  et al    simple algorithm for
In Proceedings of the
nuclear norm regularized problems 
 th International Conference on Machine Learning  ICML 
  pp     

Jain  Prateek and Netrapalli  Praneeth  Fast exact matrix comple 

tion with  nite samples  arXiv preprint   

Jain  Prateek  Meka  Raghu  and Dhillon  Inderjit    Guaranteed
rank minimization via singular value projection  In Advances
in Neural Information Processing Systems  pp     

Jain  Prateek  Netrapalli  Praneeth  and Sanghavi  Sujay  LowIn

rank matrix completion using alternating minimization 
STOC  pp       

Jain  Prateek  Netrapalli  Praneeth  and Sanghavi  Sujay  Lowrank matrix completion using alternating minimization  In Proceedings of the forty fth annual ACM symposium on Theory
of computing  pp    ACM     

Jin  Chi  Kakade  Sham    and Netrapalli  Praneeth  Provable
ef cient online matrix completion via nonconvex stochastic
gradient descent  arXiv preprint arXiv   

Johnson  Rie and Zhang  Tong  Accelerating stochastic gradient
In Advances in

descent using predictive variance reduction 
Neural Information Processing Systems  pp     

Defazio  Aaron  Bach  Francis  and LacosteJulien  Simon  Saga 
  fast incremental gradient method with support for nonstrongly convex composite objectives  In Advances in Neural
Information Processing Systems  pp       

Keshavan  Raghunandan    Oh  Sewoong  and Montanari  Andrea  Matrix completion from   few entries 
In   IEEE
International Symposium on Information Theory  pp   
IEEE   

Defazio  Aaron    Caetano  Tib erio    and Domke  Justin  Finito 
  faster  permutable incremental gradient method for big data
problems  In Proceedings of the International Conference on
Machine Learning     

Goldberg  Ken  Roeder  Theresa  Gupta  Dhruv  and Perkins 
Chris  Eigentaste    constant time collaborative  ltering algorithm  Information Retrieval     

Keshavan  Raghunandan    Montanari  Andrea  and Oh  Sewoong  Matrix completion from noisy entries  Journal of Machine Learning Research   Jul   

Koltchinskii  Vladimir  Lounici  Karim  Tsybakov  Alexandre   
et al  Nuclearnorm penalization and optimal rates for noisy
lowrank matrix completion  The Annals of Statistics   
   

  Universal Variance ReductionBased Catalyst for Nonconvex LowRank Matrix Recovery

Kone cn    Jakub and Richt arik  Peter  Semistochastic gradient

descent methods  arXiv   

Kone cn    Jakub  Liu  Jie  Richt arik  Peter  and Tak      Martin 
ms gd  Minibatch semistochastic gradient descent in the
proximal setting  arXiv   

Loh  PoLing and Wainwright  Martin   

Regularized mestimators with nonconvexity  Statistical and algorithmic theory for local optima  In Advances in Neural Information Processing Systems  pp     

Mairal  Julien 

Incremental majorizationminimization optimization with application to largescale machine learning 
arXiv   

Mazumder  Rahul  Hastie  Trevor  and Tibshirani  Robert  Spectral regularization algorithms for learning large incomplete matrices  Journal of machine learning research   Aug 
   

Negahban  Sahand and Wainwright  Martin    Estimation of
 near  lowrank matrices with noise and highdimensional scaling  The Annals of Statistics  pp     

Negahban  Sahand and Wainwright  Martin    Restricted strong
convexity and weighted matrix completion  Optimal bounds
with noise  Journal of Machine Learning Research   May 
   

Negahban  Sahand  Yu  Bin  Wainwright  Martin    and Ravikumar  Pradeep      uni ed framework for highdimensional
analysis of mestimators with decomposable regularizers  In
Advances in Neural Information Processing Systems  pp 
   

Ni  Renkun and Gu  Quanquan  Optimal statistical and computational rates for one bit matrix completion 
In Proceedings
of the  th International Conference on Arti cial Intelligence
and Statistics  pp     

Park  Dohyung  Kyrillidis  Anastasios  Bhojanapalli  Srinadh 
Caramanis  Constantine  and Sanghavi  Sujay  Provable burermonteiro factorization for   class of normconstrained matrix
problems  stat       

Park  Dohyung  Kyrillidis  Anastasios  Caramanis  ConstanFinding lowrank solutions to
arXiv preprint

tine  and Sanghavi  Sujay 
matrix problems  ef ciently and provably 
arXiv     

Recht  Benjamin  Fazel  Maryam  and Parrilo  Pablo    Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization  SIAM review     

Reddi  Sashank    Hefny  Ahmed  Sra  Suvrit    ocz os  Barnab as 
and Smola  Alex  Stochastic variance reduction for nonconvex
optimization  arXiv preprint arXiv   

Rennie  Jasson DM and Srebro  Nathan  Fast maximum margin
matrix factorization for collaborative prediction  In Proceedings of the  nd international conference on Machine learning 
pp    ACM   

Rohde  Angelika  Tsybakov  Alexandre    et al  Estimation of
highdimensional lowrank matrices  The Annals of Statistics 
   

Schmidt  Mark  Roux  Nicolas Le  and Bach  Francis  Minimizing  nite sums with the stochastic average gradient 
arXiv   

Srebro  Nathan  Rennie 

Jason  and Jaakkola  Tommi   
Maximummargin matrix factorization  In Advances in neural
information processing systems  pp     

Sun  Ruoyu and Luo  ZhiQuan  Guaranteed matrix completion
via nonconvex factorization  In Foundations of Computer Science  FOCS    IEEE  th Annual Symposium on  pp   
  IEEE   

Tu  Stephen  Boczar  Ross  Soltanolkotabi  Mahdi  and Recht 
Benjamin  Lowrank solutions of linear matrix equations via
procrustes  ow  arXiv preprint arXiv   

Vershynin     Introduction to the nonasymptotic analysis of ran 

dom matrices  arXiv preprint arXiv   

Wang  Lingxiao  Zhang  Xiao  and Gu  Quanquan    uni ed computational and statistical framework for nonconvex lowrank
matrix estimation  arXiv preprint arXiv   

Wang  Zheng  Lai  MingJun  Lu  Zhaosong  Fan  Wei  Davulcu 
Hasan  and Ye  Jieping  Rankone matrix pursuit for matrix
completion  In ICML  pp     

Xiao  Lin and Zhang  Tong    proximal stochastic gradient
method with progressive variance reduction  SIAM Journal on
Optimization     

Xu  Miao  Jin  Rong  and Zhou  ZhiHua  Speedup matrix completion with side information  Application to multilabel learning  In Advances in Neural Information Processing Systems 
pp     

Yi  Xinyang  Park  Dohyung  Chen  Yudong  and Caramanis 
Constantine  Fast algorithms for robust pca via gradient descent  In Advances in Neural Information Processing Systems 
pp     

Zhang  Aston and Gu  Quanquan  Accelerated stochastic block
coordinate descent with optimal sampling  In Proceedings of
the  nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pp    ACM   

Zhang  Xiao  Wang  Lingxiao  and Gu  Quanquan    nonconvex free lunch for lowrank plus sparse matrix recovery  arXiv
preprint arXiv     

Zhang  Xiao  Wang  Lingxiao  and Gu  Quanquan  Stochaslow rank maarXiv preprint

tic variancereduced gradient descent
trix recovery from linear measurements 
arXiv     

for

Zhao  Tuo  Wang  Zhaoran  and Liu  Han    nonconvex optimization framework for low rank matrix estimation  In Advances in
Neural Information Processing Systems  pp     

Zheng  Qinqing and Lafferty  John    convergent gradient descent algorithm for rank minimization and semide nite programming from random linear measurements  In Advances in
Neural Information Processing Systems  pp     

Zheng  Qinqing and Lafferty  John  Convergence analysis for
rectangular matrix completion using burermonteiro factorization and gradient descent  arXiv preprint arXiv 
 

