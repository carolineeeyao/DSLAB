RuleEnhanced Penalized Regression by Column Generation

using Rectangular Maximum Agreement

Jonathan Eckstein   Noam Goldberg   Ai Kagawa  

Abstract

We describe   procedure enhancing   penalized
regression by adding dynamically generated
rules describing multidimensional  box  sets 
Our
ruleadding procedure is based on the
classical column generation method for highdimensional linear programming  The pricing
problem for our column generation procedure
reduces to the NPhard rectangular maximum
agreement  RMA  problem of  nding   box
that best discriminates between two weighted
datasets  We solve this problem exactly using   parallel branchand bound procedure  The
resulting ruleenhanced regression method is
computationintensive  but has promising prediction performance 

  Motivation and Overview
This paper considers the general learning problem in which
we have   observation vectors            Xm   Rn  with
matching response values            ym      Each response
yi is   possibly noisy evaluation of an unknown function
    Rn     at Xi  that is  yi      Xi    ei  where ei  
  represents the noise or measurement error  The goal is
to estimate   by some      Rn     such that     Xi  is
  good    for yi  that is        Xi    yi  tends to be small 
The estimate    may then be used to predict the response
value   corresponding to   newly encountered observation
    Rn through the prediction               classical
linear regression model is one simple example of the many
possible techniques one might employ for constructing    
The classical regression approach to this problem is to posit

 Management Science and Information Systems  Rutgers
University  Piscataway  NJ  USA  Department of Management  BarIlan University  Ramat Gan  Israel  Doctoral Program in Operations Research  Rutgers University  Piscataway  NJ  USA  Correspondence to  Jonathan Eckstein  jeckstei business rutgers edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  particular functional form for         for example  an af ne
function of    and then use an optimization procedure to
estimate the parameters in this functional form 
Here  we are interested in cases in which   concise candidate functional form for    is not readily apparent  and
we wish to estimate    by searching over   very highdimensional space of parameters  For example  Breiman
  proposed the method of random forests  which
constructs    by training regression trees on multiple random subsamples of the data  and then averaging the resulting predictors  Another proposal is the RuleFit algorithm  Friedman   Popescu    which enhances   
regularized regression by generating boxbased rules to use
as additional explanatory variables  Given        Rn with
       the rule function          Rn       is given by

             cid     aj   xj   bj cid   

 
that is              if            componentwise  and
             otherwise  RuleFit generates rules through  
twophase procedure   rst  it determines   regression tree
ensemble  and then decomposes these trees into rules and
determines the regression model coef cients  including for
the rules 
The approach of Dembczy nski et al      generates
rules more directly  without having to rely on an initial ensemble of decision trees  within gradient boosting  Friedman    for nonregularized regression  In this scheme 
  greedy procedure generates the rules within   gradient
descent method runs that for   predetermined number of iterations  Aho et al    extended the RuleFit method to
solve more general multitarget regression problems  For
the special case of singletarget regression  however  their
experiments suggest that random forests and RuleFit outperform several other methods  including their own extended implementation and the algorithm of Dembczy nski
et al      Compared with random forests and other
popular learning approaches such as kernelbased methods
and neural networks  rulebased approaches have the advantage of generally being considered more accessible and
easier to interpret by domain experts  Rulebased methods
also have   considerable history in classi cation settings  as
in for example Weiss   Indurkhya   Cohen   Singer

RuleEnhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement

  cid 

  

 cid 

   

  and Dembczy nski et al     
Here  we propose an iterative optimizationbased regression procedure called REPR  RuleEnhanced Penalized
Regression 
Its output models resemble those of RuleFit  but our methodology draws more heavily on exact optimization techniques from the  eld of mathematical programming  While it is quite computationally intensive  its
prediction performance appears promising  As in RuleFit  we start with   linear regression model  in this case 
with   penalized coef cients to promote sparsity  and
enhance it by synthesizing rules of the form   We incrementally adjoin such rules to our  penalized  linear regression model as if they were new observation variables 
Unlike RuleFit  we control the generation of new rules using the classical mathematical programming technique of
column generation  Our employment of column generation
roughly resembles its use in the LPBoost ensemble classi 
 cation method of Demiriz et al   
Column generation involves cyclical alternation between
optimization of   restricted master problem  in our case
  linear or convex quadratic program  and   pricing problem that  nds the most promising new variables to adjoin
to the formulation 
In our case  the pricing problem is
equivalent to an NPhard combinatorial problem we call
Rectangular Maximum Agreement  RMA  which generalizes the Maximum Mononial Agreement  MMA  problem
as formulated and solved by Eckstein   Goldberg  
We solve the RMA problem by   similar branchand bound
method procedure  implemented using parallel computing
techniques 
To make our notation below more concise  we let   denote the matrix whose rows are   cid 
   and also let
                ym    Rm  We may then express   problem instance by the pair        We also let xij denote the
      th element of this matrix  that is  the value of variable
  in observation   

              cid 

    Penalized Regression Model with Rules
Let   be   set of pairs          Rn   Rn with        constituting   catalog of all the possible rules of the form  
that we wish to be available to our regression model  The
set   will typically be extremely large  restricting each
aj and bj to values that appear as xij for some    which
 cid  
is suf cient to describe all possible distinct behaviors of
 cid      cid  
rules of the form   on the dataset   
there are still
    cid   cid            possible choices for        where
  xij  is the number of distinct values for xij 
The predictors    that our method constructs are of the form

            

 jxj  

 krk   

 

for some                             Finding an    of
this form is   matter of linear regression  but with the regression coef cients in   space with the potentially very
high dimension of           As is now customary in regression models in which the number of explanatory variables potentially outnumbers the number of observations 
we employ   LASSOclass model in which all explanatory
variables except the constant term have    penalties  Letting                     Rn and          let     
denote the predictor function in   We then propose to
 cid 
estimate       by solving

   Xi    yi        cid cid       cid cid 

 

 cid    cid 

min
 

  

  and           

 
where         and          are scalar parameters  For
      and           this model is essentially the classic
LASSO as originally proposed by Tibshirani  
To put   into   more convenient form for our purposes 
we split the regression coef cient vectors into positive and
negative parts  so           and           with
      Rn
    Introducing one more
vector of variables     Rm  the model shown as   in Figure   is equivalent to   The model is constructed so that
        Xi    yi  for                  If       the
model is   linear program  and if       it is   convex  linearly constrained quadratic program  In either case  there
are    constraints  other than nonnegativity  but the number of variables is                 
Because of this potentially unwieldy number of variables 
we propose to solve   by using the classical technique
of column generation  which dates back to Ford   Fulkerson   and Gilmore   Gomory   see for example
Section   of Griva et al    for   recent textbook treatment  In brief  column generation cycles between solving
two optimization problems  the restricted master problem
and the pricing problem  In our case  the restricted master
problem is the same as   but with   replaced by some
 presumably far smaller    cid       We initially choose
  cid      Solving the restricted master problem yields optimal Lagrange multipliers     Rm
   for the
constraints other than simple nonnegativity  For each rule
       these Lagrange multipliers yield respective reduced
costs rc 
  that are in the
master problem  but not the restricted master  One then
solves the pricing problem  whose job is to identify the
smallest of these reduced costs  The reduced cost rc    of  
variable   indicates the rate of change of the objective function as one increases   away from   If the smallest reduced

    for the variables  

  and     Rm

    rc 

     

RuleEnhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement

min

     

  cid 

  

  
     

      cid 
      cid 
     

 cid 

   

  cid 
 cid 
     
 
         cid 
         

   

  

       

rk Xi 

rk Xi 
   
       

       

     
 
   
     
           yi 
     
            yi 

               

               

 

Figure   Formulation of the REPR master problem  with scalar parameters               and       The decision variables are
    Rm               Rn  and            The input data are           Xm cid    Rm   and     Rm 

     

cost is nonnegative  then clearly all the reduced costs are
nonnegative  which means that the current restricted master
problem yields an optimal solution to the master problem
      for all         cid  and the process
by setting  
terminates  If the smallest reduced cost is negative  we adjoin elements to   cid  including at least one corresponding
to   variable  
  with   negative reduced cost  and
we repeat the process  resolving the expanded restricted
master problem 
In our case  the reduced costs take the form

  or  

            cid 
  cid 

  

         

  

rc 

rc 

rk xi  

rk xi  

  

rk xi    

  cid 
rk xi       cid 
 cid cid cid cid cid    cid 

  

  

and hence we have for each       that

min cid rc 

   cid       

    rc 

rk xi        

 cid cid cid cid cid     

Therefore  the pricing problem may be solved by maximizing the second term on the righthand side of   that is 
 nding

 cid cid cid cid cid    cid 

  

 cid cid cid cid cid   

     max
   

rk xi        

 

and the stopping condition for the column generation procedure is         This problem turns out to be equivalent
to the RMA problem  whose formulation and solution we
now describe 

  The RMA Problem
  Formulation and Input Data

Suppose we have   observations and   explanatory variables  expressed using   matrix     Rm   as above 
Each observation                  is assigned   nonegative weight wi      For any set                 

let         cid 

    wi  We also assume we are given
  partition of the observations into two subsets     positive  subset                  and    negative  subset
                
Given two vectors        Rn  let         denote the  box 
     Zn              Given the input data    the coverage CvrX        of         consists of the indices of the
observations from   falling within         that is 

CvrX                                  Xi       

       Rn 

The rectangular maximum agreement  RMA  problem is

max  cid cid   cid    CvrX       cid      cid    CvrX       cid cid cid 

    
 
with decision variables        Rn  Essentially implicit
in this formulation is the constraint that        since if
   cid    then CvrX            and the objective value is
  The previously mentioned MMA problem is the special case of RMA in which all the observations are binary              Since the MMA problem is NPhard  Eckstein   Goldberg    so is RMA 
If we take   to be the set of all possible boxes on Rn  the
pricing problem   may be reduced to RMA by setting

                     wi           
                                  

 

 

and thus                                 
  Preprocessing and Restriction to  
Any RMA problem instance may be converted to an equivalent instance in which all the observation data are integer 
Essentially  for each coordinate                  one may simply record the distinct values of xij and replace each xij
with its ordinal position among these values  Algorithm  
with its parameter   set to   performs exactly this procedure  outputing   equivalent data matrix     Nm   and
  xij 

  vector  cid    Nn whose jth element is  cid      cid  

RuleEnhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement

Algorithm   Preprocessing discretization algorithm
  Input      Rm         
  Output      Nm     cid    Nn
  ProcessData
  for       to   do
 
 

 cid      
Sort              xmj and set             km  such that
 xk      
for       to       do

xk     xk         xkmj

if xki     xkij        xkmj   xk    then

 cid      cid      

 
 
 
 
 
 
 
 
  end for
  return      cid 

end if
 xki      cid  
end for
 cid      cid      

as de ned in the previous section  Algorithm    output
values  xij for attribute   vary between   and  cid      
The number of distinct values  cid   of each explanatory variable   directly in uences the dif culty of RMA instances 
To obtain easier instances  Algorithm   can combine its
 integerization  process with some binning of nearby values  Essentially  if the parameter   is positive  the algorithm
bins together consecutive values xij that are within relative
tolerance   resulting in   smaller number of distinct values
 cid   for each explanatory variable   
Some datasets contain both categorical and numerical data 
In addition to Algorithm   we also convert each kway
categorical attribute into       binary attributes 
Within the context of our REPR regression method  we set
the RMA weight vector and data partition as in   integerize the data   using Algorithm   with some  small 
parameter value   solve the RMA problem  and then translate the resulting boxes back to the original  preintegerized
coordinate system  We perform this translation by expanding box boundaries to lie halfway between the boundaries
of the clusters of points grouped by Algorithm   except
when the lower boundary of the box has the lowest possible value or the upper boundary has the largest possible
value 
In these cases  we expand the box boundaries to
  or   respectively  More precisely  for each observation variable   and                cid       let xmin
    be the
smallest value of xij assigned to the integer value   by Alj   be the largest  If          Nn         
gorithm   and xmax
describe an integerized box arising from the solution of the
preprocessed RMA problem  we choose the corresponding
box boundaries        Rn in the original coordinate system

to be given by  for                 

 cid 
 cid 

 

   xmax
 
   xmax
  bj

 

aj  

bj  

  aj    xmin
  aj

if  aj    
  otherwise

if  bj    cid      

  xmin

  bj  

  otherwise 

Overall  our procedure is equivalent to solving the pricing
problem   over some set of boxes          For
      the resulting set of boxes      is such that the corresponding set of rules  rk            comprises every boxbased rule distinguishable on the dataset    For
small positive values of   the set of boxes      excludes those corresponding to rules that  cut  between very
closely spaced observations 

  Branchand Bound Subproblems

In this and the following two subsections  we describe the
key elements of our branchand bound procedure for solving the RMA problem  assuming that the data   have already been preprocessed as above  For brevity  we omit
some details which will instead be covered in   forthcoming publication  For general background on branchand 
bound algorithms  Morrison et al    provide   recent
survey with numerous citations 
Branchand bound methods search   tree of subproblems 
each describing some subset of the search space 
In our
RMA method  each subproblem   is characterized by
four vectors                           Nn  and represents
search space subset consisting of vector pairs        for
which                   and                   Any valid
subproblem conforms to                            
              and               The root problem  
of the branchand bound tree is        cid         cid     
where where  cid    Nn is as output from Algorithm   and
  and   respectively denote the vectors                 Nn
and                 Nn 

  Inseparability and the Bounding Function

In branchand bound methods  the bounding function provides an upper bound  when maximizing  on the best possible objective value in the region of the search space corresponding to   subproblem  Our bounding function is based
on an extension of the notion of inseparability developed
by Eckstein   Goldberg   Consider any subproblem
                 and two observations   and   cid  If xij   xi cid  
or aj   xij  xi cid     bj for each                  then
xi  xi cid    Nn are inseparable with respect to        Nn 
in the sense that any box         with       and      
must either cover both of xi  xi cid  or neither of them 
Inseparability with respect to      is an equivalence relation 

RuleEnhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement

 

        

        

 cid 
 cid 

 cid       CvrX                    CvrX           cid 
 cid       CvrX                    CvrX           cid 

   

 

 

               max

 

Figure   The RMA bounding function 

and we denote the equivalence classes it induces among the
observation indices             by         That is  observation indices   and   cid  are in the same equivalence class of
        if xi and xi cid  are inseparable with respect to      
Our bounding function              for each subproblem
                 is shown in   in Figure   The reasoning
behind this bound is that each possible box in the set speci 
 ed by              must either cover or not cover the entirety
of each             The  rst argument to the  max  operation re ects the situation that every equivalence class  
with   positive net weight is covered  and no classes with
negative net weight are covered  this is the best possible
situation if the box ends up covering   higher weight of
positive observations than of negative  The second  max 
argument re ects the opposite situation  the best possible
case in which the box covers   greater weight of negative
observations than of positive ones 

  Branching

The branching scheme of   branchand bound algorithm
divides subproblems into smaller ones in order to improve their bounds  In our case  branching   subproblem
                 involves choosing an explanatory variable
                 and   cutpoint      aj          bj       Nn 
There are three possible cases  the  rst of which is when
bj   aj and      bj          aj     In this case  our scheme
creates three children based on the disjunction that either
bj          the box lies below    aj       bj  the box
straddles    or aj          the box lies above    The next

case is that    cid aj          min aj  bj     cid  in which case
case occurs when    cid max aj  bj          bj cid  in which

the box cannot lie below   and we split   into two children based on the disjunction that either aj      the box
straddles    or aj          the box is above    The third

case we split   into two children based on the disjunction
that either bj      the box is does not extend above    or
bj          the box extends above    If no   falling under
one of these three cases exists for any dimension    then the
subproblem represents   single possible box  that is       
and        Such   subproblem is   terminal node of the
branchand bound tree  and in this case we simply compute
the RMA objective value for           and           as
the subproblem bound 

When more than one possible variablecutpoint pair       
exists  as is typically the case  our algorithm must select one  We use two related procedures for branching selection  strong branching and cutpoint caching  In
strong branching  we simply experiment with all applicable
variablecutpoint pairs        and select one that the maximizes the minimum bound of the resulting two or three
children  This is   standard technique in branchand bound
algorithms  and involves evaluating the bounds of all the
potential children of the current search node  To make this
process as ef cient as possible  we have developed specialized data structures for manipulating equivalence classes 
and we analyze the branching possibilities in   particular
order  In cutpoint caching  some subproblems use strong
branching  while others select from   list of cutpoints that
were chosen by strong branching for previously processed
search nodes  The details of these procedures will be covered in   forthcoming companion publication 

  Full Algorithm and Implementation
The pseudocode in Algorithm   describes our full REPR
column generation procedure for solving   using the
RMA preprocessing and branchand bound methods described above to solve the pricing problem  Several points
bear mentioning   rst  the nonnegative scalar parameter  
allows us to incorporate   tolerance into the column generation stopping criterion  so that we terminate when all
reduced costs exceed   instead of when all reduced costs
are nonnegative  This kind of tolerance is customary in
column generation methods  The tolerance   on the other
hand  controls the space of columns searched over  Furthermore  our implementation of the RMA branchand bound
algorithm can identify any desired number       of the best
possible RMA solutions  as opposed to just one value of  
attaining the maximum in   This   is also   parameter to
our procedure  so at each iteration of Algorithm   we may
adjoin up to   new rules to   cid  Adding multiple columns
per iteration is   common technique in column generation
methods  Finally  the algorithm has   parameter   specifying   limit on the number of column generation iterations 
meaning that at the output model will contain at most St
rules 
We implemented the algorithm in    using the GuRoBi

RuleEnhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement

Algorithm   REPR  Ruleenhanced penalized regression
  Input  data     Rm        Rm  penalty parameters
         column generation tolerance       integer
      aggregation tolerance       iteration limit  
  Output             Rn    cid                 cid 
  REPR
    cid     
  for                 do
 

Solve the restricted master problem to obtain optimal primal variables           and dual
variables    
Use the RMA branchand bound algorithm  with
preprocessing as in Algorithm   to identify   tbest
solution            kt to

 

 cid cid cid cid cid    cid 

  

max

      

 cid cid cid cid cid   

rk xi        

 

with            kt having respective objective values
              zt
if            break
for each                  with zl         do
  cid      cid     kl 

 
 
 
 
  end for
  return               cid           

end for

commercial optimizer  Gurobi Optimization    to
solve the restricted master problems  We implemented
the RMA algorithm using using the PEBBL    class library  Eckstein et al    an opensource    framework for parallel branch and bound  PEBBL employs MPIbased parallelism  Gropp et al    Since solving the
RMA pricing problem is by far the most timeconsuming
part of Algorithm   we used true parallel computing only
in that portion of the algorithm  The remainder of the algorithm  including solving the restricted master problems 
was executed in serial and redundantly on all processors 

  Preliminary Testing of REPR
For preliminary testing of REPR  we selected   datasets
from the UCI repository  Lichman    choosing small
datasets with continuous response variables  The  rst four
columns of Table   summarize the number of observations
   the number of attributes    and the maximum number
of distinguishable boxbased rules       for these data
sets 
In our initial testing  we focused on the       case in which
 tting errors are penalized quadratically  and set       that
is  we added one model rule per REPR iteration  We set the
iteration limit   to   and effectively set the termination

Dataset
SERVO
CONCRETE
MACHINE
YACHT
MPG
COOL
HEAT
AIRFOIL

REPR
         
Time
   
   
 
   
 
 
 
   
 
 
 
 
 
   
 
     
 
 
 
   
 
 
 
   
 
 
 
 
   
 

RMA
Nodes
   
   
   
   
   
   
   
   

Table   Summary of experimental datasets  The last two columns
respectively show REPR   average run time for an   sample of
the dataset  on    core workstation  in hh mm ss format  and the
average resulting number of RMA search nodes per RMA invocation 

tolerance   so that REPR terminated when

     max cid     cid           cid cid     

where      denotes the sample mean of the response variable and     its sample standard deviation  We found this
rule of thumb to work well in pracice  but it likely merits
further study  We also chose       and       We used
      for SERVO  YACHT  and MPG  and       for
the remaining datasets 
With the  xed parameters given above  we tested REPR
and some competing regression procedures on ten different randomlychosen partitions of each dataset  each partition consists of   training data and   testing data 
The competing procedures are RuleFit  random forests 
LASSO  and classical linear regression  The penalty parameter in LASSO is the same as the value of   chosen for
REPR  To implement RuleFit and random forests  we used
their publicly available   packages  Table   shows the averages of the resulting mean square errors and Table   shows
their standard deviations  REPR has the smallest average
MSE for   of the   datasets and has the second smallest average MSE on the remaining   datasets  coming very close
to random forests on MPG  For the standard deviation of
the MSE  which we take as   general measure of prediction stability  REPR has the lowest values for   of the  
datasets  The box plots in Figures   and   visualize these
results in more detail for HEAT and MACHINE  respectively  Figure   displays the average MSEs in   barchart
format  with the MSE of REPR normalized to  
Figures   give more detailed information for speci  
datasets  Figure   and   respectively show how REPR  
prediction MSEs for HEAT and CONCRETE evolve with
each iteration  with each data point averaged over the  
different REPR runs  the horizontal lines indicate the average MSE level for the competing procedures  MSE generally declines as REPR adds rules  although some diminish 

RuleEnhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement

Method Dataset 
REPR
RuleFit
Random forests
Lasso
Linear regression

SERVO CONCRETE MACHINE YACHT MPG
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

COOL
 
 
 
 
 

HEAT
 
 
 
 
 

AIRFOIL
 
 
 
 
 

Table   Average MSE over the experimental datasets  The smallest value in each column is bolded 

Method Dataset 
REPR
RuleFit
Random forests
Lasso
Linear regression

SERVO CONCRETE MACHINE YACHT MPG
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

COOL
 
 
 
 
 

HEAT
 
 
 
 
 

AIRFOIL
 
 
 
 
 

Table   Standard deviation of MSE over the experimental datasets  The smallest value in each column is bolded 

ing returns are evident for CONCRETE  Interestingly  neither of these  gures shows appreciable evidence of over tting by REPR  even when large numbers of rules are incorporated into the model  Figures   and   display testingset
predictions for speci    arbitrarily chosen  partitions of the
MACHINE and CONCRETE datasets  respectively  with
the observations sorted by response value  REPR seems
to outperform the other methods in predicting extreme response values  although it is somewhat worse than the other
methods at predicting nonextreme values for MACHINE 
The last two columns of Table   show  for    core Xeon
   workstation  REPR   average total run time per
data partition and the average number of search node per
invocation of RMA  The longer runs could likely be accelerated by the application of more parallel processors 

Figure   MSE box plots of for the HEAT data set 

Figure   MSE box plots of for the MACHINE data set 

  Conclusions and Future Research
The results presented here suggest that REPR has signi cant potential as   regression tool  at least for small
datasets  Clearly  it should be tested on more datasets and
larger datasets 
Here  we have tested REPR using  xed values of most of
its parameters  and we expect we should be able to improve
its performance by using intelligent heuristics or crossvalidation procedures to select key parameters such as  
and    Improved preprocessing may also prove helpful  judicious normalization of the input data        should assist
in  nding good parameter choices  and we are also working
on more sophisticated discretization technique for preprocessing the RMA solver input  as well as branch selection
heuristics that are more ef cient for large  cid   

REPRRuleFitRandomForestsLASSOLinearRegression MSEREPRRuleFitRandomForestsLASSOLinearRegression MSERuleEnhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement

Figure   Comparison the average MSEs  with the MSE of REPR
normalized to   The random forest value for YACHT is truncated 

Figure   MSE as   function of iterations for the CONCRETE
dataset 

Figure   MSE as   function of iterations for the HEAT dataset 

It would be interesting to see how well REPR performs if
the pricing problems are solved less exactly  For example 
one could use various techniques for truncating the branchand bound search  such as setting   limit on the number of
subproblems explored or loosening the conditions for pruning unpromising subtrees  Or one could use  perhaps selectively  some entirely heuristic procedure to identify rules to
add to the restricted master problem 
For problems with large numbers of observations    it
is conceivable that solving the restricted master problems
could become   serial bottleneck in our current implementation strategy  If this phenomenon is observed in practice 
it could be worth investigating parallel solution strategies
for the restricted master 

Figure   Sorted predictions for the MACHINE data set 

Figure   Sorted predictions for the CONCRETE data set 

SERVOCONCRETEMACHINEYACHTMPGCOOLHEATAIRFOILDataset Relative MSEREPRRuleFitRandom Forests Iterations MSEREPRRuleFitRandom ForestsLinear Regression Iterations MSEREPRRuleFitRandom ForestsLinear Regression Observation Response valueResponseREPRRuleFitRandom Forests Observation Response valueResponseREPRRuleFitRandom ForestsRuleEnhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement

References
Aho  Timo   Zenko  Bernard    zeroski  Sa so  and Elomaa 
Tapio  Multitarget regression with rule ensembles    
Mach  Learn  Res   Aug   

Breiman  Leo  Random forests  Mach  Learn   

 

Cohen  William    and Singer  Yoram    simple  fast  and
effective rule learner  In Proc  of the  th Nat  Conf  on
Arti cial Intelligence  pp     

Dembczy nski  Krzysztof  Kot owski  Wojciech 

and
  owi nski  Roman  Solving regression by learning an
ensemble of decision rules  In International Conference
on Arti cial Intelligence and Soft Computing    volume   of Lecture Notes in Arti cial Intelligence  pp 
  SpringerVerlag     

Dembczy nski  Krzysztof  Kot owski  Wojciech 

and
  owi nski  Roman  Maximum likelihood rule ensembles  In Proceedings of the  th International Conference on Machine Learning  ICML   pp   
New York  NY  USA      ACM 

Demiriz  Ayhan  Bennett  Kristin    and ShaweTaylor 
John  Linear programming boosting via column generation  Mach  Learn     

Eckstein 

Jonathan and Goldberg  Noam 

An improved branchand bound method for maximum monomial agreement  INFORMS    Comput   
 

Eckstein  Jonathan  Hart  William    and Phillips  Cynthia    PEBBL  an objectoriented framework for scalable parallel branch and bound  Math  Program  Comput     

Ford  Jr  Lester    and Fulkerson  David      suggested computation for maximal multicommodity network  ows  Manage  Sci     

Friedman  Jerome    Greedy function approximation   
gradient boosting machine  Ann  of Stat  pp   
 

Friedman  Jerome    and Popescu  Bogdan    Predictive
learning via rule ensembles  Ann  Appl  Stat   
   

Gilmore  Paul    and Gomory  Ralph      linear programming approach to the cuttingstock problem  Oper  Res 
   

Griva  Igor  Nash  Stephen    and Sofer  Ariela  Linear and Nonlinear Optimization  SIAM  second edition 
 

Gropp  William  Lusk  Ewing  and Skjellum  Anthony 
Using MPI  Portable Parallel Programming with the
MessagePassing Interface  MIT Press   

Gurobi Optimization  Inc  Gurobi optimizer reference
manual    URL http www gurobi com 
documentation refman index html 

Lichman  Moshe  UCI machine learning repository   

URL http archive ics uci edu ml 

Morrison  David    Jacobson  Sheldon    Sauppe  Jason    and Sewell  Edward    Branchand bound algorithms    survey of recent advances in searching  branching  and pruning  Discrete Optim       

Tibshirani  Robert  Regression shrinkage and selection via

the lasso        Statist  Soc        

Weiss  Sholom    and Indurkhya  Nitin  Optimized rule

induction  IEEE Expert     

