Dual Supervised Learning

Yingce Xia   Tao Qin   Wei Chen   Jiang Bian   Nenghai Yu   TieYan Liu  

Abstract

Many supervised learning tasks are emerged in
dual forms       Englishto French translation
vs  Frenchto English translation  speech recognition vs  text to speech  and image classi cation
vs  image generation  Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models  This
connection is  however  not effectively utilized
today  since people usually train the models of
two dual tasks separately and independently  In
this work  we propose training the models of two
dual tasks simultaneously  and explicitly exploiting the probabilistic correlation between them to
regularize the training process  For ease of reference  we call the proposed approach dual supervised learning  We demonstrate that dual supervised learning can improve the practical performances of both tasks  for various applications
including machine translation  image processing 
and sentiment analysis 

  Introduction
Deep learning brings stateof theart results to many arti 
cial intelligence tasks  such as neural machine translation
 Wu et al    image classi cation  He et al       
image generation  van den Oord et al        speech
recognition  Graves et al    Amodei et al    and
speech generation synthesis  Oord et al   
Interestingly  we  nd that many of the aforementioned AI
tasks are emerged in dual forms       the input and output
of one task are exactly the output and input of the other
task respectively  Examples include translation from language   to language   vs  translation from language   to
   image classi cation vs 
image generation  and speech

 School of Information Science and Technology  University of Science and Technology of China  Hefei  Anhui  China
 Microsoft Research  Beijing  China  Correspondence to  Tao
Qin  taoqin microsoft com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

recognition vs  speech synthesis  Even more interestingly
 and somehow surprisingly  this natural duality is largely
ignored in the current practice of machine learning  That is 
despite the fact that two tasks are dual to each other  people usually train them independently and separately  Then
  question arises  Can we exploit the duality between two
tasks  so as to achieve better performance for both of them 
In this work  we give   positive answer to the question 
To exploit the duality  we formulate   new learning scheme 
which involves two tasks    primal task and its dual task 
The primal task takes   sample from space   as input and
maps to space    and the dual task takes   sample from space   as input and maps to space     Using the language
of probability  the primal task learns   conditional distribution          xy  parameterized by  xy  and the dual task
learns   conditional distribution          yx  parameterized
by  yx  where       and       
In the new scheme 
the two dual tasks are jointly learned and their structural
relationship is exploited to improve the learning effectiveness  We name this new scheme as dual supervised learning  brie    DSL 
There could be many different ways of exploiting the duality in DSL  In this paper  we use it as   regularization term to govern the training process  Since the joint probability          can be computed in two equivalent
ways                                         for any
               ideally the conditional distributions of the
primal and dual tasks should satisfy the following equality 

              xy                  yx 

 
However  if the two models  conditional distributions  are
learned separately by minimizing their own loss functions
 as in the current practice of machine learning  there is no
guarantee that the above equation will hold  The basic idea
of DSL is to jointly learn the two models  xy and  yx by
minimizing their loss functions subject to the constraint of
Eqn  By doing so  the intrinsic probabilistic connection
between  yx and  xy are explicitly strengthened  which is
supposed to push the learning process towards the right direction  To solve the constrained optimization problem of
DSL  we convert the constraint Eqn  to   penalty term
by using the method of Lagrange multipliers  Boyd   Vandenberghe    Note that the penalty term could also be
seen as   datadependent regularization term 

Dual Supervised Learning

To demonstrate the effectiveness of DSL  we apply it to
three arti cial intelligence applications  
  Neural Machine Translation  NMT  We  rst apply DSL to NMT  which formulates machine translation as  
sequenceto sequence learning problem  with the sentences
in the source language as inputs and those in the target language as outputs  The input space and output space of NMT are symmetric  and there is almost no information loss
while mapping from   to   or from   to    Thus  symmetric tasks in NMT  ts well into the scope of DSL  Experimental studies illustrate signi cant accuracy improvements
by applying DSL to NMT    points measured by
BLEU scores for English French translation   
points for English Germen translation and  
points on English Chinese 
  Image Processing We then apply DSL to image processing  in which the primal task is image classi cation
and the dual task is image generation conditioned on category labels  Both tasks are hot research topics in the
deep learning community  We choose ResNet  He et al 
    as our baseline for image classi cation  and PixelCNN Salimans et al    as our baseline for image
generation  Experimental results show that on CIFAR 
DSL could reduce the error rate of ResNet  from  
to   and obtain   better image generation model with
both clearer images and smaller bits per dimension  Note
that these primal and dual tasks do not yield   pair of completely symmetric input and output spaces since there is
information loss while mapping from an image to its class
label  Therefore  our experimental studies reveal that DSL
can also work well for dual tasks with information loss 
  Sentiment Analysis Finally  we apply DSL to sentiment
analysis  in which the primal task is sentiment classi cation       to predict the sentiment of   given sentence  and
the dual one is sentence generation with given sentiment
polarity  Experiments on the IMDB dataset show that DSL
can improve the error rate of   widelyused sentiment classi cation model by   point  and can generate sentences
with clearer richer styles of sentiment expression 
All of above experiments on real arti cial intelligence applications have demonstrated that DSL can improve practical performance of both tasks  simultaneously 

  Framework
In this section  we formulate the problem of dual supervised learning  DSL  describe an algorithm for DSL  and
discuss its connections with existing learning schemes and

 In our experiments  we chose the most cited models with either opensource codes or enough implementation details  to ensure that we can reproduce the results reported in previous papers 
All of our experiments are done on   single Telsa     GPU 

its application scope 

  Problem Formulation

To exploit the duality  we formulate   new learning scheme 
which involves two tasks    primal task that takes   sample
from space   as input and maps to space    and   dual task
takes   sample from space   as input and maps to space    
Assume we have   training pairs  xi  yi  
          sampled from the space       according to some unknown
distribution     Our goal is to reveal the bidirectional relationship between the two inputs   and    To be speci   
we perform the following two tasks    the primal learning task aims at  nding   function        cid    such that the
prediction of   for   is similar to its real counterpart     
the dual learning task aims at  nding   function        cid   
such that the prediction of   for   is similar to its real counterpart    The dissimilarity is penalized by   loss function 
Given any        let  cid          and  cid         denote the
loss functions for   and   respectively  both of which are
mappings from       to   
  common practice to design        is the maximum likelihood estimation based on the parameterized conditional
distributions        xy  and        yx 

       xy   cid  arg max
      yx   cid  arg max

  cid        cid     xy 
  cid        cid     yx 

where  xy and  yx are the parameters to be learned 
By standard supervised learning  the primal model   is
learned by minimizing the empirical risk in space   

  cid    xi   xy  yi 

and dual model   is learned by minimizing the empirical
risk in space    

min xy    cid  
min yx   cid  

  cid   yi   yx  xi 

Given the duality of the primal and dual tasks  if the learned
primal and dual models are perfect  we should have
              xy                  yx                 

We call this property probabilistic duality  which serves as
  necessary condition for the optimality of the learned two
dual models 
By the standard supervised learning scheme  probabilistic duality is not considered during the training  and the
primal and the dual models are trained independently and
separately  Thus  there is no guarantee that the learned dual models can satisfy probabilistic duality  To tackle this
problem  we propose explicitly reinforcing the empirical
probabilistic duality of the dual modes by solving the fol 

lowing multiobjective optimization problem instead 

Algorithm   Dual Supervise Learning Algorithm

Dual Supervised Learning

Input  Marginal distributions     xi  and     yi  for any
        Lagrange parameters  xy and  yx  optimizers
Opt  and Opt 
repeat

  

Get   minibatch of   pairs  xj  yj  
Calculate the gradients as follows 

Gf    xy    cid  
Gg    yx   cid  

 cid cid    xj   xy  yj 
   xy cid duality xj  yj   xy   yx cid 
 cid cid   yj   yx  xj 
   yx cid duality xj  yj   xy   yx cid 

  

  

 

Update the parameters of   and   
 xy   Opt xy  Gf    yx   Opt yx  Gg 

until models converged

the primal and the dual models get optimized by minimizing the difference between   cid  with    In contrast  by making use of the intrinsic probabilistic connection between the
primal and dual models  DSL takes an innovative attempt
to extend the bene   of duality to supervised learning 
While  cid duality can be regarded as   regularization term  it
is data dependent  which makes DSL different from Lasso  Tibshirani    or SVM  Hearst et al    where
the regularization term is dataindependent  More accurately speaking  in DSL  every training sample contributes
to the regularization term  and each model contributes to
the regularization of the other model 
DSL is different from the following three learning schemes 
  Cotraining focuses on singletask learning and assumes that different subsets of features can provide enough
and complementary information about data  while DSL targets at learning two tasks with structural duality simultaneously and does not yield any prerequisite or assumptions
  Multitask learning requires that differon features 
ent tasks share the same input space and coherent feature
representation while DSL does not    Transfer Learning
uses auxiliary tasks to boost the main task  while there is
no difference between the roles of two tasks in DSL  and
DSL enables them to boost the performance of each other
simultaneously 
We would like to point that there are several requirements
to apply DSL to   certain scenario    Duality should exist for the two tasks    Both the primal and dual models should be trainable           and         in Eqn   
should be available  If these conditions are not satis ed 
DSL might not work very well  Fortunately  as we have
discussed in the paper  many machine learning tasks related to image  speech  and text satisfy these conditions 

   cid  
   cid  

  cid    xi   xy  yi 

objective   min
 xy
objective   min
  cid   yi   yx  xi 
 yx
                   xy                  yx      

 

where       and       are the marginal distributions  We
call this new learning scheme dual supervised learning  abbreviated as DSL 
We provide   simple theoretical analysis which shows that
DSL has theoretical guarantees in terms of generalization
bound  Since the analysis is straightforward  we put it in
Appendix   due to space limitations   

  Algorithm Description

In practical arti cial intelligence applications  the groundtruth marginal distributions are usually not available  As
an alternative  we use the empirical marginal distributions
       and        to ful ll the constraint in Eqn 
To solve the DSL problem  following the common practice
in constraint optimization  we introduce Lagrange multipliers and add the equality constraint of probabilistic duality
into the objective functions  First  we convert the probabilistic duality constraint into the following regularization
term  with the empirical marginal distributions included 

 cid duality  log          log          xy 
  log          log          yx 

 

Then  we learn the models of the two tasks by minimizing
the weighted combination between the original loss functions and the above regularization term  The algorithm is
shown in Algorithm  
In the algorithm  the choice of optimizers Opt  and Opt 
is quite  exible  One can choose different optimizers such
as Adadelta  Zeiler    Adam  Kingma   Ba    or
SGD for different tasks  depending on common practice in
the speci   task and personal preferences 

  Discussions

The duality between tasks has been used to enable learning
from unlabeled data in  He et al      As an early attempt to exploit the duality  this work actually uses the exterior connection between dual tasks  which helps to form
  closed feedback loop and enables unsupervised learning 
For example  in the application of machine translation  the
primal task model  rst translates an unlabeled English sentence   to   French sentence   cid  then  the dual task model
translates   cid  back to an English sentence   cid   nally  both
 All the appendices are left in the supplementary document 

Dual Supervised Learning

  Application to Machine Translation
We  rst apply our dual supervised learning algorithm to
machine translation and study whether it can improve the
translation qualities by utilizing the probabilistic duality
of dual translation tasks  In the following of the section 
we perform experiments on three pairs of dual tasks  
English French  En Fr  English Germany  En De 
and English Chinese  En Zh 

  Settings

distribution of   sentence    de ned as cid Tx

Datasets We employ the same datasets as used in  Jean
et al    to conduct experiments on En Fr and
En De  As   part of WMT  the training data consists
of    sentences pairs for En Fr and    for En De 
respectively  WMT    We combine newstest  and
newstest  together as the validation sets and use newstest  as the test sets  For the dual tasks of En Zh 
we use    sentence pairs obtained from   commercial
company as training data  We leverage NIST  as the
validation set and NIST  as well as NIST  as the
test sets  Note that  during the training of all three pairs of
dual tasks  we drop all sentences with more than   words 
Marginal Distributions        and        We use the LSTMbased language modeling approach  Sundermeyer et al 
  Mikolov et al    to characterize the marginal
      xi     
where xi is the ith word in    Tx denotes the number of
words in    and the index     indicates            
More details about such language modeling approach can
be referred to Appendix   
Model We apply the GRU as the recurrent module to implement the sequenceto sequence model  which is the same
as  Bahdanau et al    Jean et al    The word embedding dimension is   and the number of hidden node
is   Regarding the vocabulary size of the source and
target language  we set it as         and    for En Fr 
En De  and En Zh  respectively  The outof vocabulary
words are replaced by   special token UNK  Following
the common practice  we denote the baseline algorithm
proposed in  Bahdanau et al    Jean et al    as
RNNSearch  We implement the whole NMT learning system based on an open source code 

 Since both tasks in each pair are symmetric  they play the
same role in the dual supervised learning framework  Consequently  any one of the dual tasks can be viewed as the primal
task while the other as the dual task 
 The three NIST datasets correspond to Zh En translation
task  in which each Chinese sentence has four English references 
To build the test set for En Zh  we use the Chinese sentence
with one randomly picked English sentence to form up   En Zh
validation test pair 

 https github com nyudl dl mttutorial

Evaluation Metrics The translation qualities are measured
by tokenized casesensitive BLEU  Papineni et al   
scores  which is implemented by  multi bleu    The
larger the BLEU score is  the better the translation quality is  During the evaluation process  we use beam search
with beam width   to generate sentences  Note that  following the common practice  the Zh En is evaluated by
caseinsensitive BLEU score 
Training Procedure We initialize the two models in DSL
      the  xy and  yx  by using two warmstart models 
which is generated by following the same process as  Jean
et al    Then  we use SGD with the minibatch size
of   as the optimization method for dual training  During
the training process  we  rst set the initial learning rate  
to   and then halve it if the BLEU score on the validation
set cannot grow for   certain number of mini batches  In
order to stabilize parameters  we will freeze the embedding
matrix once halving learning rates can no long improve the
BLEU score on the validation set  The gradient clip is set
as     and   during the training for En Fr  En De 
and En Zh  respectively  Pascanu et al    The value
of both  xy and  yx in Algorithm   are set as   according to empirical performance on the validation set  Note
that  during the optimization process  the LSTMbased language models will not be updated 

  Results

Table   shows the BLEU scores on the dual tasks by the DSL method with that by the baseline RNNSearch method 
Note that  in this table  we use  MT  and  MT  to denote results carried out on NIST  and NIST  respectively  From this table  we can  nd that  on all these
three pairs of symmetric tasks  DSL can improve the performance of both dual tasks  simultaneously 

Table   BLEU scores of the translation tasks    represents the
improvement of DSL over RNNSearch 

Tasks
En Fr
Fr En
En De
De En

En Zh  MT 
Zh En  MT 
En Zh  MT 
Zh En  MT 

RNNSearch DSL
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

To better understand the effects of applying the probabilistic duality constraint as the regularization  we compute the
 cid duality on the test set by DSL compared with RNNSearch 
In particular  after applying DSL to En Fr  the  cid duality decreases from   to   which also indicates that

Dual Supervised Learning

the two models become more coherent in terms of probabilistic duality 
 Jean et al    proposed an effective postprocess technique  which can achieve better translation performance by
replacing the  UNK  with the corresponding wordlevel
translations  After applying this technique into DSL  we
report its results on En Fr in Table   compared with several baselines with the same model structures as ours that
also integrate the  UNK  postprocessing technique  From
this table  it is clear to see that DSL can achieve better performance than all baseline methods 

Table   Summary of some existing En Fr translations

Brief description
standard NMT

Model
NMT 
MRT  Direct optimizing BLEU
Refer to Algorithm  

BLEU
 
 
 
   Jean et al       Shen et al   

DSL

In the previous experiments  we use   warmstart approach
in DSL using the models trained by RNNSearch  Actually  we can use stronger models for initialization to achieve
even better accuracy  We conduct   light experiment to verify this  We use the models trained by  He et al      as
the initializations in DSL on En Fr translation  We  nd
that BLEU score can be improved from   to   for
En Fr translation  and from   to   for Fr En
translation 
Effects of  
There are two hyperparameters  xy and  yx in our DSL algorithm  We conduct some experiments to investigate their
effects  Since the input and output space are symmetric  we
set  xy    yx     and plot the validation accuracy of different    in Figure     From this  gure  we can see that
both En Fr and Fr En reach the best performance when
      and thus the results of DSL reported in Table
  are obtained with       Moreover  we  nd that 
within   relatively large interval of   DSL outperforms standard supervised learning       the point with       We

    Valid BLEU        
Figure   Visualization of En Fr tasks with DSL

    Valid   Test BLEU curves

Table   An example of En Fr

 Source  En    board member at   German bluechip
company concurred that when it comes to economic espionage 
 the French are the worst 
 Source  Fr  Un membre du conseil   administration   une
soci    allemande renomm   estimait que lorsqu il   agit
  espionnage  conomique     les Fran ais sont les pires    
 RNNSearch  Fr En    member of the board of directors
of   renowned German society felt that when it was economic
espionage   the French are the worst   
 RNNSearch  En Fr  Un membre du conseil   une
compagnie allemande UNK   reconnu que quand il   agissait
  espionnage  conomique   le fran ais est le pire 
 DSL  Fr En    board member of   renowned German
company felt that when it comes to economic espionage 
 the French are the worst   
 DSL  En Fr  Un membre du conseil   une compagnie
allemande UNK   reconnu que   lorsqu il   agit   espionnage
 conomique   les Fran ais sont les pires 

also plot the BLEU scores for       on the validation
and test sets in Figure     with respect to training iterations  We can see that  in the  rst couple of rounds  the
test BLEU curves  uctuate with large variance  The reason
is that two separately initialized models of dual tasks yield
are not consistent with each other       Eqn    does not
hold  which causes the declination of the performance of
both models as they play as the regularizer for each other 
As the training goes on  two models become more consistent and  nally boost the performance of each other 
Case studies
Table   shows   couple of translation examples produced
by RNNSearch compared with DSL  From this table  we
 nd that DSL demonstrates three major advantages over
RNNSearch  First  by leveraging the structural duality of
sentences  DSL can result in the improvement of mutual translation        when it comes to  and  lorsqu qu il
  agit de  which better    the semantics expressed in the
sentences  Second  DSL can consider more contextual information in translation  For example  in Fr En  une soci    is translated to company  however  in the baseline  it
is translated to society  Although the word level translation
is not bad  it should de nitely be translated as  company 
given the contextual semantics  Furthermore  DSL can better handle the plural form  For example  DSL can correctly
translate  the French are the worst  which are of plural
form  while the baseline deals with it by singular form 

  Application to Images Processing
In the domain of image processing  image classi cation
 image label  and image generation  label image  are in

         BLEU  En FrFr En TrainingIterations BLEU  ValidEn FrTestEn FrValidFr EnTestFr EnDual Supervised Learning

the dual form 
In this section  we apply our dual supervised learning framework to these two tasks and conduct experimental studies based on   public dataset  CIFAR 
   Krizhevsky   Hinton    with   classes of images 
In our experiments  we employ   popular method  ResNet 
for image classi cation and   most recent method  PixelCNN  for image generation  Let   denote the image space and   denote the category space related to CIFAR 

  Settings

is usually de ned as cid  

Marginal Distributions
In our experiments  we simply
use the uniform distribution to set the marginal distribution
       of  class labels  which means the marginal distribution of each class equals   The image distribution       
     xi      where all pixels of
the image is serialized and xi is the value of the ith pixel
of an mpixel image  Note that the model can predict xi
only based on the previous pixels xj with index        We
use the PixelCNN  which is so far the best algorithm  to
model the image distribution 
Models For the task of image classi cation  we choose  
layer ResNet  denoted as ResNet  and  layer ResNet
 denoted as ResNet  as two baselines  respectively  in
order to examine the power of DSL on both relatively simple and complex models  For the task of image generation 
we use PixelCNN  again  Compared to the PixelCNN 
used for modeling distribution  the difference lies in the
training process  When used for image generation given  
certain class  PixelCNN  takes the class label as an addiP xi        

tional input       it tries to characterize cid  

  

where   is the  hot label vector 
Evaluation Metrics We use the classi cation error rates to
measure the performance of image classi cation  We use
bits per dimension  brie    bpd   Salimans et al    to
assess the performance of image generation  In particular 
for an image   with label    the bpd is de ned as 

   log    xi        cid cid Nx log cid 

 cid cid Nx

 

where Nx is the number of pixels in image    By using
the dataset CIFAR  Nx is   for any image    and we
will report the average bpd on the test set 
Training Procedure We  rst initialize both the primal and
the dual models with the ResNet model and PixelCNN 
model pretrained independently and separately  We obtain
   layer ResNet with error rate of   and    layer
ResNet with error rate of   as the pretrained models
for image classi cation  The error rates of these two pretrained models are comparable to results reported in  He
et al      We generate   pretrained conditional image

 https github com tensor ow models tree master resnet
 https github com openai pixelcnn

Table   Error rates   of image classi cation tasks  Baseline is
from  He et al        denotes the improvement of DSL
over baseline 

ResNet  ResNet 

baseline

DSL
 

 
 
 

 
 
 

generation model with the test bpd of   which is the
same as reported in  Salimans et al    For DSL training  we set the initial learning rate of image classi cation
model as   and that of image generation model as  
The learning rates follow the same decay rules as those
in  He et al      and  Salimans et al    The whole
training process takes about two weeks before convergence 
Note that experimental results below are based on the training with  xy     and  yx    

  Results on Image Classi cation

Table   compares the error rates of two image classi cation models       DSL vs  Baseline  on the test set  From
this table  we  nd that  with using either ResNet  or
ResNet  DSL achieves better accuracy than the baseline method 
Interestingly  we observe from Table   that  DSL leads to
higher relative performance improvement on the ResNet 
  over the ResNet  We hypothesize one possible reason is that  due to the limited training data  an appropriate regularization can bene   more to the  layer ResNet
with higher model complexity  and the dualityoriented
regularization  cid duality indeed plays this role and consequently gives rise to higher relative improvement 

  Results on Image Generation

Our further experimental results show that  based on
ResNet  DSL can decrease the test bpd from  
 baseline  to    DSL  which is   new stateof theart
result on CIFAR  Indeed  it is quite dif cult to improve
bpd by   which though seems like   minor change 
We also  nd that  there is no signi cant improvement on
test bpd based on ResNet  An intuitive explanation
is that  since ResNet  is stronger than ResNet  in
modeling the conditional probability         it can better
help the task of image generation through the constraint regularization of the probabilistic duality 
As pointed out in  Theis et al    bpd is not the only
evaluation rule of image generation  Therefore  we further
conduct   qualitative analysis by comparing images generated by dual supervised learning with those by the baseline model for each of image categories  some examples of

which are shown in Figure  

  Experimental Setup

Dual Supervised Learning

Dataset Our experiments are performed based on the
IMDB movie review dataset  IMDB    which consists
of    training and    test sentences  Each sentence in
this dataset is associated with either   positive or   negative sentiment label  We randomly sample   subset of  
sentences from the training data as the validation set for
hyperparameter tuning and use the remaining training data
for model training 
Marginal Distributions We simply use the uniform distribution to set the marginal distribution        of polarity
labels  which means the marginal distribution of positive
or negative class equals   On the other side  we take advantage of the LSTMbased language modeling to model
the marginal distribution        of   sentence    The test
perplexities  Bengio et al    of the obtained language
model is  
Model Implementation We leverage the widely used LSTM  Dai   Le    modeling approach for sentiment classi cation  model  We set the embedding dimension as  
and the hidden layer size as   For sentence generation 
we use another LSTM model with    
  Esy
as input  where xt  denotes the      th word  Ew and
Es represent the embedding matrices for word and sentiment label respectively  and      represent the connections between embedding matrix and LSTM cells    sentence is generated word by word sequentially  and the
probability that word xt is generated is proportional to
  Esy   Whht  where ht  is the
exp    
hidden state outputted by LSTM  Note the      and the    
are the parameters to learn in training  In the following  we
call the model for sentiment based sentence generation as
contextual language model  brie    CLM 
Evaluation Metrics We measure the performance of sentiment classi cation by the error rate  and that of sentence
generation       CLM  by test perplexity 
Training Procedure To obtain baseline models  we use
Adadelta as the optimization method to train both the sentiment classi cation and sentence generation model  Then 
we use them to initialization the two models for DSL  At
the beginning of DSL training  we use plain SGD with an
initial learning rate of   and then decrease it to   for
both models once there is no further improvement on the
validation set  For each        pair  we set  xy    lx 
and  yx    lx  where lx is the length of    The
whole training process of DSL takes less than two days 

wEwxt       

wEwxt       

 Both supervised and semisupervised sentiment classi cation
are studied in  Dai   Le    We focus on supervised learning
here  Therefore  we do not compare with the models trained with
semisupervised  labeled   unlabeled  data 

Figure   Generated images

Each row in Figure   corresponds to one category in
CIFAR  the  ve images in the left side are generated by
the baseline model  and the  ve ones in the right side are
generated by the model trained by DSL  From this  gure 
we  nd that DSL generally generates images with clearer and more distinguishable characteristics regarding the
corresponding category  Speci cally  those right  ve images in Row     and   can illustrate more distinguishable
characteristics of birds  cats and dogs respectively  which
is mainly due to bene ts of introducing the probabilistic duality into DSL  But  there are still some cases that neither
the baseline model nor DSL can perform well  like deers it
Row   and frogs in Row   One reason is that the bpd of
images in the category of deer and frogs are   and  
which are signi cant larger than the average   This
shows that the images of these two categories are harder to
generate 

  Application to Sentiment Analysis
Finally  we apply the dual supervised learning framework
to the domain of sentiment analysis  In this domain  the
primal task  sentiment classi cation  Maas et al    Dai
  Le    is to predict the sentiment polarity label of  
given sentence  and the dual task  though not quite apparent
but really existed  is sentence generation based on   sentiment polarity  In this section  let   denote the sentences
and   denote the sentiment related to our task 

Dual Supervised Learning

  Results

Table   compares the performance of DSL with the baseline method in terms of both the error rates of sentiment classi cation and the perplexity of sentence generation 
Note that the test error of the baseline classi cation model 
which is   as shown in the table  is comparable to the
recent results as reported in  Dai   Le    We have
two observations from the table  First  DSL can reduce the
classi cation error by   without modifying the LSTMbased model structure  Second  DSL slightly improves the
perplexity for sentence generation  but the improvement is
not very signi cant  We hypothesize the reason is that the
sentiment label can merely supply at most   bit information
such that the perplexity difference between the language
model       the marginal distribution        and CLM      
the conditional distribution         are not large  which
limits the improvement brought by DSL 

Table   Results on IMDB

Test Error  

Perplexity

Baseline

DSL

 
 

 
 

Qualitative analysis on sentence generation
In addition to quantitative studies as shown above  we further conduct qualitative analysis on the performance of sentence generation  Table   demonstrates some examples of
generated sentences based on sentiment labels  From this
table  we can  nd that both the baseline model and DSL
succeed in generating sentences expressing the certain sentiment  The baseline model prefers to produce the sentence
with those words yielding highfrequency in the training
data  such as the  the plot is simple predictable  the acting
is great bad  etc  This is because the sentence generation
model itself is essentially   language model based generator  which aims at catching the highfrequency words in
the training data  Meanwhile  since the training of CLM
in DSL can leverage the signals provided by the classi er 
DSL makes it more possible to select those words  phrases 
or textual patterns that can present more speci   and more
intense sentiment  such as  nothing but good    don  
waste your time  etc  As   result  the CLM in DSL can
generate sentences with richer expressions for sentiments 

  Discussions

In previous experiments  we start DSL training with welltrained primal and dual models  We conduct some further
experiments to verify whether warm start is   must for DSL    We train DSL from   warmstart sentence generator
and   coldstart  randomly initialized  sentence classi er 
In this case  DSL achieves   classi cation error of  
which is better than the baseline classi er in Table    

Table   Sentence generation with given sentiments

Base
 Pos 

DSL
 Pos 

Base
 Neg 

DSL
Neg

  ve seen this movie   few times  it   still one of my
favorites  the plot is simple  the acting is great 
It     very good movie  and   think it   one of the
best movies   ve seen in   long time 
  have nothing but good things to say about this
movie    saw this movie when it  rst came out 
and   had to watch it again and again    really
enjoyed this movie    thought it was   very good
movie  The acting was great  the story was great 
  would recommend this movie to anyone 
  give it      
after seeing this  lm    thought it was going to be
one of the worst movies   ve ever seen  the acting
was bad  the script was bad  the only thing   can
say about this movie is that it   so bad 
this is   dif cult movie to watch  and would  not
recommend it to anyone  The plot is predictable 
the acting is bad  and the script is awful 
Don   waste your time on this one 

We train DSL from   warmstart classi er and   coldstart
sentence generator  The perplexity of the generator after
DSL training reach   which is better than the baseline
generator    We train DSL from both coldstart models 
The  nal classi cation error is   and the perplexity of
the generator is   which are both better than the baselines  These results show that the success of DSL does not
necessarily require warmstart models  although they can
speed up the training of DSL 

  Conclusions and Future Work
Observing the existence of structure duality among many
AI tasks  we have proposed   new learning framework  dual supervised learning  which can greatly improve the performance for both the primal and the dual tasks  simultaneously  We have introduced   probabilistic duality term
to serve as   datadependent regularizer to better guide the
training  Empirical studies have validated the effectiveness
of dual supervised learning 
There are multiple directions to explore in the future  First 
we will test dual supervised learning on more dual tasks 
such as speech recognition and speech synthesis  Second 
we will enrich theoretical study to better understand dual
supervised learning  Third  it is interesting to combine dual
supervised learning with unsupervised dual learning  He
et al      to leverage unlabeled data so as to further
improve the two dual tasks  Fourth  we will combine dual
supervised learning with dual inference  Xia et al    so
as to leverage structural duality to enhance both the training
and inference procedures 

Dual Supervised Learning

Acknowledgements
We would like to thank Yue Wang  Fei Tian  Di He and
Yanmei Duan for the helpful discussions  This work is partially supported by the National Natural Science Foundation of China  Grant No   

References
Amodei  Dario  Anubhai  Rishita  Battenberg  Eric  Case 
Carl  Casper  Jared  Catanzaro  Bryan  Chen  Jingdong 
Chrzanowski  Mike  Coates  Adam  Diamos  Greg  et al 
Deep speech   Endto end speech recognition in english
and mandarin  In  rd International Conference on Machine Learning   

Bahdanau  Dzmitry  Cho  Kyunghyun  and Bengio 
Yoshua  Neural machine translation by jointly learning
In International Conference on
to align and translate 
Learning Representations   

Bengio  Yoshua  Ducharme    jean  Vincent  Pascal  and
Jauvin  Christian    neural probabilistic language model  Journal of machine learning research   Feb 
   

Boyd  Stephen and Vandenberghe  Lieven  Convex opti 

mization  Cambridge university press   

Dai  Andrew   and Le  Quoc    Semisupervised sequence
learning  In Advances in Neural Information Processing
Systems  pp     

Graves  Alex  Mohamed  Abdelrahman  and Hinton  Geoffrey  Speech recognition with deep recurrent neural
networks  In Acoustics  speech and signal processing  icassp    ieee international conference on  pp   
  IEEE   

He  Di  Xia  Yingce  Qin  Tao  Wang  Liwei  Yu  Nenghai  Liu  TieYan  and Ma  WeiYing  Dual learning for
machine translation  In Advances In Neural Information
Processing Systems  pp       

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition 
In IEEE Conference on Computer Vision and Pattern
Recognition     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Identity mappings in deep residual networks  In
European Conference on Computer Vision  pp   
Springer     

Hearst  Marti    Dumais  Susan    Osuna  Edgar  Platt  John  and Scholkopf  Bernhard  Support vector machines  IEEE Intelligent Systems and their Applications 
   

IMDB 

Imdb dataset 

maas data sentiment   

http ai stanford edu    

Jean    bastien  Cho  Kyunghyun  Memisevic  Roland  and
Bengio  Yoshua  On using very large target vocabulary
for neural machine translation  In ACL   

Kingma  Diederik and Ba  Jimmy  Adam    method for stochastic optimization  arXiv preprint arXiv 
 

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple

layers of features from tiny images   

Maas  Andrew    Daly  Raymond    Pham  Peter    Huang 
Dan  Ng  Andrew    and Potts  Christopher  Learning
word vectors for sentiment analysis  In Proceedings of
the  th Annual Meeting of the Association for Computational Linguistics  Human Language TechnologiesVolume   pp    Association for Computational
Linguistics   

Mikolov  Tomas  Kara    Martin  Burget  Lukas  Cernock    Jan  and Khudanpur  Sanjeev  Recurrent neural network based language model  In Interspeech  volume   pp     

multi bleu  multibleu pl  https github com mosessmt 

mosesdecoder blob master scripts generic multibleu perl   

Oord  Aaron van den  Dieleman  Sander  Zen  Heiga  Simonyan  Karen  Vinyals  Oriol  Graves  Alex  Kalchbrenner  Nal  Senior  Andrew  and Kavukcuoglu  Koray  Wavenet    generative model for raw audio  arXiv
preprint arXiv   

Papineni  Kishore  Roukos  Salim  Ward  Todd  and Zhu 
WeiJing  Bleu    method for automatic evaluation of
machine translation  In Proceedings of the  th annual
meeting on association for computational linguistics  pp    Association for Computational Linguistics 
 

Pascanu  Razvan  Mikolov  Tomas  and Bengio  Yoshua 
On the dif culty of training recurrent neural networks 
ICML      

Salimans  Tim  Karpathy  Andrej  Chen  Xi     Kingma 
Diederik  and Bulatov  Yaroslav  Pixelcnn    pixelcnn implementation with discretized logistic mixture likelihood and other modi cations  In International Conference on Learning Representations   

Shen  Shiqi  Cheng  Yong  He  Zhongjun  He  Wei  Wu 
Hua  Sun  Maosong  and Liu  Yang  Minimum risk training for neural machine translation  ACL   

Dual Supervised Learning

Sundermeyer  Martin  Schl ter  Ralf  and Ney  Hermann 
Lstm neural networks for language modeling  In Interspeech  pp     

Theis  Lucas  Oord    ron van den  and Bethge  Matthias 
  note on the evaluation of generative models  arXiv
preprint arXiv   

Tibshirani  Robert  Regression shrinkage and selection via
the lasso  Journal of the Royal Statistical Society  Series
   Methodological  pp     

van den Oord  Aaron  Kalchbrenner  Nal  Espeholt  Lasse 
Vinyals  Oriol  Graves  Alex  et al  Conditional image generation with pixelcnn decoders  In Advances in
Neural Information Processing Systems  pp   
   

van

den Oord  Aaron  Kalchbrenner  Nal 

and
recurrent neural netIn  rd International Conference on Machine

Pixel

Kavukcuoglu  Koray 
works 
Learning     

Wmt

dataset

for machine

translation 

WMT 

http www statmt org wmt translationtask html 
 

Wu  Yonghui  Schuster  Mike  Chen  Zhifeng  Le  Quoc   
Norouzi  Mohammad  Macherey  Wolfgang  Krikun 
Maxim  Cao  Yuan  Gao  Qin  Macherey  Klaus  et al 
Google   neural machine translation system  Bridging
the gap between human and machine translation  arXiv
preprint arXiv   

Xia  Yingce  Bian  Jiang  Qin  Tao  Yu  Nenghai  and Liu 
TieYan  Dual inference for machine learning  In The
 th International Joint Conference on Arti cial Intelligence   

Zeiler  Matthew    Adadelta  an adaptive learning rate

method  arXiv preprint arXiv   

