Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

Ian Osband     Benjamin Van Roy  

Abstract

reinforcement

Computational results demonstrate that posterior
sampling for
learning  PSRL 
dramatically outperforms existing algorithms
driven by optimism  such as UCRL  We provide insight into the extent of this performance
boost and the phenomenon that drives it  We
leverage this insight to establish an    HpSAT  
Bayesian regret bound for PSRL in  nitehorizon
episodic Markov decision processes  This improves upon the best previous Bayesian regret
bound of    HSpAT   for any reinforcement
learning algorithm  Our theoretical results are
supported by extensive empirical evaluation 

  Introduction
We consider the reinforcement learning problem in which
an agent interacts with   Markov decision process with the
aim of maximizing expected cumulative reward  Burnetas
  Katehakis    Sutton   Barto    Key to performance is how the agent balances between exploration
to acquire information of longterm bene   and exploitation to maximize expected nearterm rewards 
In principle  dynamic programming can be applied to compute the
Bayesoptimal solution to this problem  Bellman   Kalaba    However  this is computationally intractable
for anything beyond the simplest of toy problems and direct approximations can fail spectacularly poorly  Munos 
  As such  researchers have proposed and analyzed  
number of heuristic reinforcement learning algorithms 
The literature on ef cient reinforcement learning offers statistical ef ciency guarantees for computationally tractable
algorithms  These provably ef cient algorithms  Kearns
  Singh    Brafman   Tennenholtz    predominantly address the explorationexploitation tradeoff via
optimism in the face of uncertainty  OFU  at any state  the
agent assigns to each action an optimistically biased esti 

 Stanford University  California  USA  Deepmind  London 
UK  Correspondence to  Ian Osband  ian osband gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

mate of future value and selects the action with the greatest
estimate  If   selected action is not nearoptimal  the estimate must be overly optimistic  in which case the agent
learns from the experience  Ef ciency relative to less sophisticated exploration arises as the agent avoids actions
that can neither yield high value nor informative data 
An alternative approach  based on Thompson sampling
 Thompson    involves sampling   statistically plausibly set of action values and selecting the maximizing
action  These values can be generated  for example  by
sampling from the posterior distribution over MDPs and
computing the stateaction value function of the sampled
MDP  This approach  originally proposed in Strens  
is called posterior sampling for reinforcement learning
 PSRL  Computational results from Osband et al   
demonstrate that PSRL dramatically outperforms existing
algorithms based on OFU  The primary aim of this paper is
to provide insight into the extent of this performance boost
and the phenomenon that drives it 
We show that  in Bayesian expectation and up to constant
factors  PSRL matches the statistical ef ciency of any standard algorithm for OFURL  We highlight two key shortcomings of existing state of the art algorithms for OFU
 Jaksch et al    and demonstrate that PSRL does not
suffer from these inef ciencies  We leverage this insight
to produce an    HpSAT   bound for the Bayesian regret
of PSRL in  nitehorizon episodic Markov decision processes where   is the horizon    is the number of states 
  is the number of actions and   is the time elapsed  This
improves upon the best previous bound of    HSpAT   for
any RL algorithm  We discuss why we believe PSRL satis es   tighter    pHSAT   though we have not proved
that  We complement our theory with computational experiments that highlight the issues we raise  empirical results
match our theoretical predictions 
More importantly  we highlights   tension in OFU RL between statistical ef ciency and computational tractability 
We argue that any OFU algorithm that matches PSRL in
statistical ef ciency would likely be computationally intractable  We provide proof of this claim in   restricted
setting  Our key insight  and the potential bene ts of exploration guided by posterior sampling  are not restricted
to the simple tabular MDPs we analyze 

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

  Problem formulation
We consider the problem of learning to optimize   random  nitehorizon MDP               over repeated episodes of interaction  where            is
the state space             is the action space    is
the horizon  and   is the initial state distribution  In each
time period           within an episode  the agent observes state sh      selects action ah      receives  
reward rh     sh  ah  and transitions to   new state
sh       sh  ah  We note that this formulation  where
the unknown MDP    is treated as itself   random variable  is often called Bayesian reinforcement learning 
  policy   is   mapping from state      and period    
      to action        For each MDP   and policy  
we de ne the stateaction value function for each period   

 

QM

          EM 

HXj  

rM  sj aj sh     ah     

        QM

      for all     and        

where rM                RM       The subscript   indicates that actions over periods        are selected
according to the policy   Let    
         
We say   policy    is optimal for the MDP   if
     argmax    
Let Ht denote the history of observations made prior
this time evolution within
to time   
episodes  with some abuse of notation  we let skh   st for
                so that skh is the state in period   of
episode    We de ne Hkh analogously  An RL algorithm
is   deterministic sequence          of functions 
each mapping Hk  to   probability distribution    Hk 
over policies  from which the agent samples   policy   
for the kth episode  We de ne the regret incurred by an RL
algorithm   up to time   to be

To highlight

Regret        

dT  HeXk 

   

 

where    denotes regret over the kth episode  de ned with
respect to true MDP    by

    XS

       

        

     

 

  Relating performance guarantees
For the most part  the literature on ef cient RL is sharply
divided between the frequentist and Bayesian perspective
 Vlassis et al    By volume  most papers focus on
minimax regret bounds that hold with high probability for
any     some class of MDPs  Jaksch et al   
Bounds on the BayesRegret are generally weaker analytical statements than minimax bounds on regret    regret
bound for any     implies an identical bound on the
BayesReget for any   with support on      partial converse is available for    drawn with nonzero probability
under   but does not hold in general  Osband et al   
Another common notion of performance guarantee is given
by socalled  samplecomplexity  or PAC analyses that
bound the number of  suboptimal decisions taken by an
algorithm  Kakade    Dann   Brunskill   
In
general  optimal bounds on regret    pT   imply optimal
bounds on sample complexity     whereas optimal
bounds on the sample complexity give only an       
bound on regret  Osband   
Our formulation focuses on the simple setting on  nite
horizon MDPs  but there are several other problems of interest in the literature  Common formulations include the
discounted setting  and problems with in nite horizon under some connectedness assumption  Bartlett   Tewari 
  This paper may contain insights that carry over to
these settings  but we leave that analysis to future work 
Our analysis focuses upon Bayesian expected regret in  
nite horizon MDPs  We  nd this criterion amenable to  relatively  simple analysis and use it obtain actionable insight
to the design of practical algorithms  We absolutely do not
 close the book  on the exploration exploitation problem  
there remain many important open questions  Nonetheless 
our work may help to develop understanding within some
of the outstanding issues of statistical and computational
ef ciency in RL  In particular  we shed some light on how
and why posterior sampling performs so much better than
existing algorithms for OFURL  Crucially  we believe that
many of these insights extend beyond the stylized problem
of  nite tabular MDPs and can help to guide the design of
practical algorithms for generalization and exploration via
randomized value functions  Osband   

with         We note that the regret in   is random 
since it depends on the unknown MDP    the learning
algorithm   and through the history Ht on the sampled
transitions and rewards  We de ne
BayesRegret          Regret           
as the Bayesian expected regret for    distributed according to the prior   We will assess and compare algorithm
performance in terms of the regret and BayesRegret 

 

  Posterior sampling as stochastic optimism
There is   wellknown connection between posterior sampling and optimistic algorithms  Russo   Van Roy   
In this section we highlight the similarity of these approaches  We argue that posterior sampling can be thought
of as   stochastically optimistic algorithm 

 Discount          gives an effective horizon     

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

Before each episode    typical OFU algorithm constructs
  con dence set to represent the range of MDPs that are
statistically plausible given prior knowledge and observations  Then    policy is selected by maximizing value simultaneously over policies and MDPs in this set  The agent
then follows this policy over the episode  It is interesting to
contrast this approach against PSRL where instead of maximizing over   con dence set  PSRL samples   single statistically plausible MDP and selects   policy to maximize
value for that MDP 

Algorithm   OFU RL
Input  con dence set constructor  
  for episode       do
 
 
 
 
 
 
  end for

Construct con dence set Mk   Hk 
Compute     argmax   Mk    
for timestep        do

take action akh      skh   
update Hkh   Hkh   skh akh rkh skh 

end for

 

Algorithm   PSRL
Input  prior distribution  
  for episode       do
 
 
 
 
 
 
  end for

end for

Sample MDP Mk   Hk 
Compute     argmax   Mk
for timestep        do
take action akh      skh   
update Hkh   Hkh   skh akh rkh skh 

 

  The blueprint for OFU regret bounds
The general strategy for the analysis of optimistic algorithms follows   simple recipe  Strehl   Littman   
Szita   Szepesv ri    Munos   

  Design con dence sets  via concentration inequality 
such that       for all   with probability      

  Decompose the regret in each episode

   

 

 opt

 

   

 conc

 

 

   Mk

         

          

      

where Mk is the imagined optimistic MDP 

    Mk
  
 

        
  
 
  By step    opt
      for all   with probability      
  Use concentration results with   pigeonhole argument
over all possible trajectories       to bound 
with probability at least    
Regret      
 conc

   Mk            

dT  HeXk 

 

  Anything OFU can do  PSRL can expect to do too
In this section  we highlight the connection between posterior sampling and any optimistic algorithm in the spirit of
Section   Central to our analysis will be the following
notion of stochastic optimism  Osband et al   
De nition    Stochastic optimism 
Let   and   be realvalued random variables with  nite
expectation  We will say that   is stochastically optimistic
for   if for any convex and increasing          

 

              
We will write    so   for this relation 
This notion of optimism is dual to second order stochastic
dominance  Hadar   Russell       so   if and only
if     ssd    We say that PSRL is   stochastically optimistic algorithm since the random imagined value function
  Mk
    is stochastically optimistic for the true optimal value
function     
  conditioned upon any possible history Hk 
 Russo   Van Roy    This observation leads us to  
general relationship between PSRL and the BayesRegret of
any optimistic algorithm 
Theorem    PSRL matches OFURL in BayesRegret 
Let  opt be any optimistic algorithm for reinforcement
learning in the style of Algorithm   If  opt satis es regret bounds such that  for any    any       and any    
the regret is bounded with probability at least    

Regret    opt               

 

 

Then  if   is the distribution of the true MDP    and the
proof of   follows Section   then for all      
BayesRegret    PSRL                     
Sketch proof  This result is established in Osband et al 
  for the special case of  opt    UCRL  We include
this small sketch as   refresher and   guide for high level
intuition  First  note that conditioned upon any data Hk 
the true MDP    and the sampled Mk are identically
distributed  This means that   opt Hk    for all   
Therefore  to establish   bound upon the Bayesian regret
of PSRL  we just need to boundPdT  He
We can use that   Hk     Mk Hk  again in step  
from Section   to say that both   Mk lie within Mk
for all   with probability at least     via   union bound 
This means we can bound the concentration error in PSRL 

 Hk 

  conc

  

 

BayesRegret    PSRL 

dT  HeXk 

  conc

 

   Mk Mk  

The  nal step follows from decomposing  conc
by
adding and subtracting the imagined optimistic value  Vk
generated by  opt  Through an application of the triangle

 

inequality   conc
mirror step  
  conc

 

      Mk

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 
     Vk     Vk        we can
to bound the regret from concentration 
    Mk                  

  

PdT  He

This result  and proof strategy  was established in multiarmed bandits by Russo   Van Roy   We complete
the proof of Theorem   with the choice         and that
the regret is uniformly bounded by    

Theorem   suggest that  according to Bayesian expected regret  PSRL performs within   factor of   of any optimistic
algorithm whose analysis follows Section   This includes the algorithms UCRL   Jaksch et al    UCFH
 Dann   Brunskill    MORMAX  Szita   Szepesv ri 
  and many more 
Importantly  and unlike existing OFU approaches  the algorithm performance is separated from the analysis of the
con dence sets Mk  This means that PSRL even attains
the big   scaling of asyet undiscovered approaches to
OFU  all at   computational cost no greater than solving
  single known MDP   even if the matched OFU algorithm
 opt is computationally intractable 

  Some shortcomings of existing OFURL
In this section  we discuss how and why existing OFU algorithms forgo the level of statistical ef ciency enjoyed by
PSRL  At   high level  this lack of statistical ef ciency
emerges from suboptimal construction of the con dence
sets Mk  We present several insights that may prove crucial to the design of improved algorithms for OFU  More
worryingly  we raise the question that perhaps the optimal
statistical con dence sets Mk would likely be computationally intractable  We argue that PSRL offers   computationally tractable approximation to this unknown  ideal 
optimistic algorithm 
Before we launch into   more mathematical argument it
is useful to take intuition from   simple estimation problem  without any decision making  Consider an MDP with
                     as described in Figure   Every
episode the agent transitions from       uniformly to   
    and receives   deterministic reward from  
depending upon this state  The simplicity of these examples means even   naive montecarlo estimate of the value
should concentrate      pn  after   episodes of interaction  Nonetheless  the con dence sets suggested by
state of the art OFURL algorithm UCRL  Jaksch et al 
  become incredibly miscalibrated as   grows 
To see how this problem occurs  consider any algorithm for
for modelbased OFURL that builds up con dence sets for
each state and action independently  such as UCRL  Even
if the estimates are tight in each state and action  the resulting optimistic MDP  simultaneously optimistic across each
state and action  may be far too optimistic  Geometrically

Figure   MDPs to illustrate the scaling with   

Figure   MDPs to illustrate the scaling with   

Figure   Union bounds give loose rectangular con dence sets 

these independent bounds form   rectangular con dence
set  The corners of this rectangle will be pS misspeci ed
to the underlying distribution  an ellipse  when combined
across   independent estimates  Figure  
Several algorithms for OFURL do exist which address
this loose dependence upon    Strehl et al    Szita
  Szepesv ri    However  these algorithms depend
upon   partitioning of data for future value  which leads
to   poor dependence upon the horizon   or equivalently
the effective horizon  
  in discounted problems  We can
use   similar toy example from Figure   to understand
why combining independently optimistic estimates through
time will contribute to   loose bound in   

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

The natural question to ask is   Why don   we simply apply
these observations to design an optimistic algorithm which
is simultaneously ef cient in   and    The  rst impediment is that designing such an algorithm requires some
new intricate concentration inequalities and analysis  Doing this rigorously may be challenging  but we believe it
will be possible through   more careful application of existing tools to the insights we raise above  The bigger challenge is that  even if one were able to formally specify such
an algorithm  the resulting algorithm may in general not be
computationally tractable 
  similar observation to this problem of optimistic optimization has been shown in the setting of linear bandits
 Dani et al    Russo   Van Roy    In these works
they show that the problem of ef cient optimization over
ellipsoidal con dence sets can be NPhard  This means that
computationally tractable implementations of OFU have to
rely upon inef cient rectangular con dence sets that give
up   factor of pD where   is the dimension of the underlying problem  By contrast  Thompson sampling approaches
remain computationally tractable  since they require solving only   single problem instance  and so do not suffer
from the loose con dence set construction  It remains an
open question whether such an algorithm can be designed
for  nite MDPs  However  these previous results in the
simpler bandit setting       show that these problems with
OFURL cannot be overcome in general 

  Computational illustration
In this section we present   simple series of computational
results to demonstrate this looseness in both   and   
We sample       episodes of data from the MDP and
then examine the optimistic sampled Qvalues for UCRL 
and PSRL  We implement   version of UCRL  optimized
for  nite horizon MDPs and implement PSRL with   uniform Dirichlet prior over the initial dynamics      
         and       prior over rewards updating as if
rewards had     noise  For both algorithms  if we say
that   or   are known then we mean that we use the true  
or   inside UCRL  or PSRL  In each experiment  the estimates guided by OFU become extremely miscalibrated 
while PSRL remains stable 
The results of Figure   are particularly revealing  They
demonstrates the potential pitfalls of OFURL even when
the underlying transition dynamics entirely known  Several OFU algorithms have been proposed to remedy the
loose UCRLstyle    concentration from transitions  Filippi et al    Araya et al    Dann   Brunskill 
  but none of these address the inef ciency from
hyperrectangular con dence sets  As expected  these loose
con dence sets lead to extremely poor performance in
terms of the regret  We push full results to Appendix  
along with comparison to several other OFU approaches 

Figure     known    unknown  vary   in the MDP Figure  

Figure     known    unknown  vary   in the MDP Figure  

Figure       unknown  vary   in the MDP Figure  

  Better optimism by sampling
Until now  all analyses of PSRL have come via comparison
to some existing algorithm for OFURL  Previous work 
in the spirit of Theorem   leveraged the existing analysis for UCRL  to establish an    HSpAT   bound upon
the Bayesian regret  Osband et al    In this section 
we present   new result that bounds the expected regret
of PSRL    HpSAT   We also include   conjecture that
improved analysis could result in   Bayesian regret bound
   pHSAT   for PSRL  and that this result would be unimprovable  Osband   Van Roy   
  From   to pS
In this section we present   new analysis that improves the
bound on the Bayesian regret from   to pS  The proof of
this result is somewhat technical  but the essential argument
comes from the simple observation of the loose rectangular
con dence sets from Section   The key to this analysis
is   technical lemma on GaussianDirichlet concentration
 Osband   Van Roy   

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

Theorem   Let    be the true MDP distributed according to prior   with any independent Dirichlet prior over
transitions  Then the regret for PSRL is bounded

BayesRegret    PSRL       HpSAT 

 

      Mk

Our proof of Theorem   mirrors the standard OFUTo condense our noRL analysis from Section  
tation we write xkh skh akh  and    
      Let
rewards  rk         Hk 
the posterior mean of
transitions  Pk          Hk  with respective deviations from sampling noise wR   rk   rk    and
     Pk     Pk        
wP
We note that  conditional upon the data Hk  the true
reward and transitions are independent of the rewards
and transitions sampled by PSRL  so that       Hk   
 rk          Hk     Pk   
However 
  wR   Hk  and   wP
     Hk  are generally nonzero 
since the agent chooses its policy to optimize its reward under Mk  We can rewrite the regret from concentration via
the Bellman operator  section   of Osband et al   

for any   

kh 

     xk           Hk  

   
  

EhV  
       Hk  
  Eh rk   xk Pk xk      
    rk   xk Pk xk   Pk xk  
              xk    Hk 
  Eh    
    PH
  Pk xkh   Pk xkh  
 PH
  EhPH
  wR xkh PH

  rk xk   xk 

kh    Hk 
   xkh    Hk   

  wP

   

   

 

We can bound the contribution from unknown rewards
   xkh  with   standard argument from earlier work
wR
 Buldygin   Kozachenko    Jaksch et al   
Lemma    SubGaussian tail bounds 
Let   xn be independent samples from subGaussian
random variables  Then  for any    

 

nXi 

    
  

   
xi    log 
contribution from the transition estimatePH

The key piece of our new analysis will be to show that the
  wP  xkh 
concentrates at   rate independent of    At the root of
our argument is the notion of stochastic optimism  Osband 
  which introduces   partial ordering over random
variables  We make particular use of Lemma   that relates the concentration of   Dirichlet posterior with that of  
matched Gaussian distribution  Osband   Van Roy   

 

wP

and

         

Lemma    GaussianDirichlet dominance 
For all  xed               with       
for
if           
    Dirichlet  then    so    
We can use Lemma   to establish   similar concentration
bound on the error from sampling wP
Lemma    Transition concentration  For any independent
prior over rewards with      additive subGaussian
noise and an independent Dirichlet prior over transitions
at stateaction pair xkh  then

     

 log 

max nk xkh   

   xkh   Hs
with probability at least    
Sketch proof  Our proof relies heavily upon some technical results from the note from Osband   Van Roy  
We cannot apply Lemma   directly to wP   since the future value    
kh  is itself be   random variable whose value
depends on the sampled transition Pk xkh  However  although    
kh  can vary with Pk  the structure of the MDP
means that resultant wP  xkh  is still no more optimistic
than the most optimistic possible  xed          
We begin this proof only for the simply family of MDPs
with       which we call    We write     Pk xkh 
for the  rst component of the unknown transition at xkh
and similarly       Pk xkh  We can then bound the transition concentration 

 

 wP

   xkh     Pk xkh   Pk xkh      
kh     

kh 
kh 

kh     

kh 

 
 
 

          
       sup
        

Rk Pk    

 
Lemma   now implies that for any      with       
the random variables    Dirichlet  and            
     are ordered 

   xkh 

   so              so          so wP

 
We conclude the proof for        through an application
of Lemma   To extend this argument to multiple states
      we consider the marginal distribution of Pk over any
subset of states  which is Beta distributed similar to  
We push the details to Appendix   

To complete the proof of Theorem   we combine Lemma  
with Lemma   We rescale     SAT so that these con 
 dence sets hold at each               via union bound
with probability at least    
   

EhPH
  wR xkh   wP
          log SAT  
  PH

max nk xkh   

   xkh    Hk  

 

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

We can now use   together with   pigeonhole principle
over the number of visits to each state and action 

BayesRegret    PSRL 

          log SAT  

  PdT  He
   PH
   HpSAT log SAT  

nk xkh     SA    

This completes the proof of Theorem  
Prior work has designed similar OFU approaches that improve the learning scaling with    MORMAX  Szita  
Szepesv ri    and delayed Qlearning  Strehl et al 
  in particular  come with sample complexity bounds
that are linear in    and match lower bounds  But even in
terms of sample complexity  these algorithms are not necessarily an improvement over UCRL  or its variants  Dann  
Brunskill    For clarity  we compare these algorithms

DelayQ

in terms of       min      
       SA
       SA
   
   

MORMAX

UCRL 

  BayesRegret         
          
       SA
   
   

Theorem  

PSRL

Table   Learning times compared in terms of    

 

Theorem   implies   PSRL           SA
  MORMAX and
delayed Qlearning reduces the Sdependence of UCRL 
but this comes at the expense of worse dependence on   
and the resulting algorithms are not practical 
  From   to pH
Recent analyses  Lattimore   Hutter    Dann   Brunskill    suggest that simultaneously reducing the dependence of   to pH may be possible  They note that
 local value variance  satis es   Bellman equation  Intuitively this captures that if we transition to   bad state    
  then we cannot transition anywhere much worse during this episode  This relation means thatPH
   xkh 
should behave more as if they were independent and grow
  pH  unlike our analysis which crudely upper bounds
them each in turn      We present   sketch towards an
analysis of Conjecture   in Appendix   
Conjecture   For any prior over rewards with     
additive subGaussian noise and any independent Dirichlet
prior over transitions  we conjecture that

  wP

  Regret    PSRL         pHSAT 

and that this matches the lower bounds for any algorithm
up to logarithmic factors 

The results of  Bartlett   Tewari    adapted to  nite
horizon MDPs would suggest   lower bound  HpSAT  
on the minimax regret for any algorithm  However  the associated proof is incorrect  Osband   Van Roy    The

strongest lower bound with   correct proof is  pHSAT  
 Jaksch et al    It remains an open question whether
such   lower bound applies to Bayesian regret over the class
of priors we analyze in Theorem  
One particularly interesting aspect of Conjecture   is that
we can construct another algorithm that satis es the proof
of Theorem   but would not satisfy the argument for Conjecture   of Appendix    We call this algorithm Gaussian
PSRL  since it operates in   manner similar to PSRL but actually uses the Gaussian sampling we use for the analysis
of PSRL in its algorithm 

Algorithm   Gaussian PSRL
Input  Posterior MAP estimates rk   Pk  visit counts nk
Output  Random Qk       soQ        for all        
  Initialize Qk          for all      
  for timestep        do
 
Vk      max Qk     
Sample wk         
 
  Qk        rk       Pk          wk             
  end for

max nk     

   

Algorithm   presents the method for sampling random Qvalues according to Gaussian PSRL  the algorithm then
follows these samples greedily for the duration of the
episode  similar to PSRL  Interestingly  we  nd that our
experimental evaluation is consistent with    HSpAT  
   HpSAT   and    pHSAT   for UCRL  Gaussian
PSRL and PSRL respectively 

  An empirical investigation
We now discuss   computational study designed to illustrate how learning times scale with   and    and to empirically investigate Conjecture   The class of MDPs we
consider involves   long chain of states with          
and with two actions  left and right  Each episode the agent
begins in state   The optimal policy is to head right at every timestep  all other policies have zero expected reward 
Inef cient exploration strategies will take      episodes
to learn the optimal policy  Osband et al   

 

Figure   MDPs that highlight the need for ef cient exploration 

We evaluate several learning algorithms from ten random
seeds and       for up to ten million episodes each 
Our goal is to investigate their empirical performance and
scaling  We believe this is the  rst ever large scale empirical investigation into the scaling properties of algorithms
for ef cient exploration 

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

We highlight results for three algorithms with    pT  
Bayesian regret bounds  UCRL  Gaussian PSRL and
PSRL  We implement UCRL  with con dence sets optimized for  nite horizon MDPs  For the Bayesian algorithms we use   uniform Dirichlet prior for transitions and
    prior for rewards  We view these priors as simple
ways to encode very little prior knowledge  Full details and
  link to source code are available in Appendix   
Figure   display the regret curves for these algorithms for
      As suggested by our analysis  PSRL
outperforms Gaussian PSRL which outperforms UCRL 
These differences seems to scale with the length of the
chain   and that even for relatively small MDPs  PSRL
is many orders of magnitude more ef cient than UCRL 

Remarkably  these high level predictions match our empirical results almost exactly  as we show in Figure   These results provide some support to Conjecture   and even  since
the spirit of these environments is similar example used
in existing proofs  the ongoing questions of fundamental
lower bounds  Osband   Van Roy    Further  we note
that every single seed of PSRL and Gaussian PSRL learned
the optimal policy for every single    We believe that this
suggests it may be possible to extend our Bayesian analysis to provide minimax regret bounds of the style in UCRL 
for suitable choice of diffuse uninformative prior 

Figure   Empirical scaling matches our conjectured analysis 
  Conclusion
PSRL is orders of magnitude more statistically ef cient
than UCRL and the same computational cost as solving  
known MDP  We believe that analysts will be able to formally specify an OFU approach to RL whose statistical ef 
 ciency matches PSRL  However  we argue that the resulting con dence sets which address both the coupling over  
and   may result in   computationally intractable optimization problem  Posterior sampling offers   computationally
tractable approach to statistically ef cient exploration 
We should stress that the  nite tabular setting we analyze is
not   reasonable model for most problems of interest  Due
to the curse of dimensionality  RL in practical settings will
require generalization between states and actions  The goal
of this paper is not just to improve   mathematical bound
in   toy example  although we do also do that  Instead 
we hope this simple setting can highlight some shortcomings of existing approaches to  ef cient RL  and provide
insight into why algorithms based on sampling may offer
important advantages  We believe that these insights may
prove valuable as we move towards algorithms that solve
the problem we really care about  synthesizing ef cient exploration with powerful generalization 

Figure   PSRL outperforms other methods by large margins 

We investigate the empirical scaling of these algorithms
with respect to    The results of Theorem   and Conjecture   only bound the Bayesian regret according to the prior
  The family of environments we consider in this example are decidedly not from this uniform distribution  in fact
they are chosen to be as dif cult as possible  Nevertheless 
the results of Theorem   and Conjecture   provide remarkably good description for the behavior we observe 

       
De ne learning time       minnK    
for the algorithm   on the MDP from Figure   with size   
For any        the regret bound    pB     would imply
log learning time             log         log    
In the cases of Figure   with           then the bounds
   HSpAT  
   HpSAT   and    pHSAT   would
suggest   slope    of   and   respectively 

KPK

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

Acknowledgements
This work was generously supported by DeepMind    research grant from Boeing    Marketing Research Award
from Adobe  and   Stanford Graduate Fellowship  courtesy of PACCAR  The authors would like to thank Daniel
Russo for many hours of discussion and insight leading to
this research  Shipra Agrawal and Tor Lattimore for pointing out several  aws in some early proof steps  anonymous
reviewers for their helpful comments and many more colleagues at DeepMind including Remi Munos  Mohammad
Azar and more for inspirational conversations 

References
Araya  Mauricio  Buffet  Olivier  and Thomas  Vincent 
Nearoptimal brl using optimistic local transitions  arXiv
preprint arXiv   

Asmuth  John  Li  Lihong  Littman  Michael    Nouri  Ali 
and Wingate  David    Bayesian sampling approach to
exploration in reinforcement learning  In Proceedings of
the TwentyFifth Conference on Uncertainty in Arti cial
Intelligence  pp    AUAI Press   

Bartlett  Peter    and Tewari  Ambuj  REGAL    regularization based algorithm for reinforcement learning in
weakly communicating MDPs 
In Proceedings of the
 th Conference on Uncertainty in Arti cial Intelligence
 UAI  pp    June  

Bellman  Richard and Kalaba  Robert  On adaptive control
processes  IRE Transactions on Automatic Control   
   

Brafman  Ronen    and Tennenholtz  Moshe  Rmax    
general polynomial time algorithm for nearoptimal reinforcement learning  Journal of Machine Learning Research     

Filippi  Sarah  Capp  Olivier  and Garivier  Aur lien  Optimism in reinforcement learning and kullbackleibler divergence  In Communication  Control  and Computing
 Allerton     th Annual Allerton Conference on 
pp    IEEE   

Fonteneau  Rapha    Korda  Nathan  and Munos    mi 
An optimistic posterior sampling strategy for Bayesian
reinforcement learning 
In NIPS   Workshop on
Bayesian Optimization  BayesOpt   

Gopalan  Aditya and Mannor  Shie  Thompson sampling
for learning parameterized Markov decision processes 
arXiv preprint arXiv   

Hadar  Josef and Russell  William    Rules for ordering
uncertain prospects  The American Economic Review 
pp     

Jaksch  Thomas  Ortner  Ronald  and Auer  Peter  Nearoptimal regret bounds for reinforcement learning  Journal of Machine Learning Research   
 

Kakade  Sham  On the Sample Complexity of Reinforcement Learning  PhD thesis  University College London 
 

Kearns  Michael    and Singh  Satinder    Nearoptimal
reinforcement learning in polynomial time  Machine
Learning     

Kolter    Zico and Ng  Andrew    NearBayesian exploration in polynomial time 
In Proceedings of the  th
Annual International Conference on Machine Learning 
pp    ACM   

Lattimore  Tor and Hutter  Marcus  PAC bounds for discounted MDPs  In Algorithmic learning theory  pp   
  Springer   

Buldygin  Valerii   and Kozachenko  Yu    Subgaussian
random variables  Ukrainian Mathematical Journal   
   

Munos    mi  From bandits to montecarlo tree search 
The optimistic principle applied to optimization and
planning   

Burnetas  Apostolos   and Katehakis  Michael    Optimal
adaptive policies for Markov decision processes  Mathematics of Operations Research     

Dani  Varsha  Hayes  Thomas    and Kakade  Sham   
Stochastic linear optimization under bandit feedback  In
COLT  pp     

Osband  Ian  Deep Exploration via Randomized Value

Functions  PhD thesis  Stanford   

Osband  Ian and Van Roy  Benjamin  Modelbased reinforcement learning and the eluder dimension 
In Advances in Neural Information Processing Systems  pp 
     

Dann  Christoph and Brunskill  Emma  Sample complexity of episodic  xedhorizon reinforcement learning  In
Advances in Neural Information Processing Systems  pp 
TBA   

Osband  Ian and Van Roy  Benjamin  Nearoptimal reinforcement learning in factored MDPs 
In Advances in
Neural Information Processing Systems  pp   
   

Why is Posterior Sampling Better than Optimism for Reinforcement Learning 

Osband  Ian and Van Roy  Benjamin  On lower bounds
arXiv preprint

for regret in reinforcement learning 
arXiv   

Osband  Ian and Van Roy  Benjamin  Gaussiandirichlet
arXiv

posterior dominance in sequential
preprint arXiv   

learning 

Osband  Ian  Russo  Daniel  and Van Roy  Benjamin 
 More  ef cient reinforcement learning via posterior
sampling  In NIPS  pp    Curran Associates 
Inc   

Osband  Ian  Van Roy  Benjamin  and Wen  Zheng  Generalization and exploration via randomized value functions  arXiv preprint arXiv   

Russo  Daniel and Van Roy  Benjamin  Learning to optimize via posterior sampling  Mathematics of Operations
Research     

Strehl  Alexander   and Littman  Michael      theoretical
analysis of modelbased interval estimation  In Proceedings of the  nd international conference on Machine
learning  pp    ACM   

Strehl  Alexander    Li  Lihong  Wiewiora  Eric  Langford  John  and Littman  Michael    PAC modelfree
reinforcement learning  In ICML  pp     

Strens  Malcolm         Bayesian framework for reinforce 

ment learning  In ICML  pp     

Sutton  Richard and Barto  Andrew  Reinforcement Learn 

ing  An Introduction  MIT Press  March  

Szita  Istv   and Szepesv ri  Csaba  Modelbased reinforcement learning with nearly tight exploration complexity bounds  In Proceedings of the  th International
Conference on Machine Learning  ICML  pp   
   

Thompson       On the likelihood that one unknown probability exceeds another in view of the evidence of two
samples  Biometrika     

Vlassis  Nikos  Ghavamzadeh  Mohammad  Mannor  Shie 
and Poupart  Pascal  Bayesian reinforcement learning  In
Reinforcement Learning  pp    Springer   

