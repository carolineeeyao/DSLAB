Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo

Matthew    Hoffman  

Abstract

  

using
an

Deep latent Gaussian models are powerful
and popular probabilistic models of highThese models are almost
dimensional data 
always
variational
expectationtrue
maximization 
to
approximation
maximummarginal likelihood estimation 
In
this paper  we propose   different approach 
rather
than use   variational approximation
 which produces biased gradient signals  we
use Markov chain Monte Carlo  MCMC  which
allows us to trade bias for computation  We
 nd that our MCMCbased approach has several
advantages 
likelihoods  produces sharper images  and does not
suffer from the variational overpruning effect 
MCMC   additional computational overhead
proves to be signi cant  but not prohibitive 

it yields higher heldout

  Introduction
Deep latent Gaussian models  DLGMs 
Kingma  
Welling    Rezende et al    are powerful generative models that can be ef ciently    to very large datasets
of complicated  highdimensional data  These models assume that the observed data were generated by sampling
some latent variables  feeding them into   deep neural network  and adding some noise to that network   output 
The central computational challenge in  tting these models
is approximate posterior inference  to make the model better    an observation    we need to estimate the posterior
distribution          over the latent variables   that generated    Exact posterior inference is intractable  so these
models are almost always    with variational inference
 Jordan et al    which approximates the intractable
posterior with   distribution   from some tractable family 
The choice of family for   matters more expressive families will better approximate the posterior  leading to better

 Google  San Francisco  California  USA  Correspondence to 

Matthew    Hoffman  mhoffman google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

learning signals  There has been    urry of papers recently
proposing more  exible variational families  all of which
show that the quality of the variational approximation matters  Salimans et al    Rezende   Mohamed   
Burda et al    Tran et al      nderby et al   
Kingma   Salimans   
The other workhorse of approximate posterior inference is
Markov chain Monte Carlo  MCMC  Neal    which
sets up and simulates   random process whose stationary
distribution is the posterior of interest  Compared to variational methods  MCMC methods have   reputation for being slow  but accurate  Perhaps surprisingly  there is little
work exploring the use of MCMC to    DLGMs  Salimans
et al    Wolf et al    Han et al   
In this paper  we explore the advantages of the MCMC approach  and  nd that  as promised  it allows us to meaningfully trade off bias and computation  By initializing our
chains with   sample from   variational approximation  we
are able to both speed up convergence and guarantee that
our posterior approximation is strictly better than that of
the underlying variational approximation  Some minor additional re nements dramatically speed up the algorithm 
In section   we show that the MCMC approach eliminates overpruning and blurriness  two known issues with
variational autoencoders  and achieves good heldout
loglikelihood on the dynamically binarized permutationinvariant MNIST dataset 

  Background
  Variational Autoencoders and DLGMs

  deep latent Gaussian model  Kingma   Welling   
Rezende et al    DLGM    assumes the following generative process for each observation   
  Sample   vector     RK variables independently from

the standard normal distribution 

  Compute   vectorvalued nonlinear function       
is typically   deep neural network with some parameters  

  Sample   from some distribution         that takes

the output of   as   parameter 

Learning DLGMs with MCMC

if   is   vector of

For example 
realvalued numbers    relatively simple DLGM would assume       
  max               and xd      gd           is the
output of   onehidden layer ReLU network  and the model
assumes that   is further corrupted by Gaussian noise with
known variance   In general    could be more complex
      it might be   deep convolutional network  and different elements of   could be injected at different layers of
computation 
This procedure de nes   joint probability distribution

                              

 

We can easily sample from and compute this joint distribution for any given parameters   latent vector    and observation   
We want to    the model to data by maximizing the
marginal likelihood of   dataset     under the model 
This is an optimization problem  the goal is to choose

 cid    arg max

 

  arg max

 

 
 

 
 

 cid 
 cid 

 

log   xn 

 cid 

 

     xn dz 

log

 

 

Unfortunately  the integral over   is usually intractable 
The typical solution is to apply variational expectationmaximization  VEM  Bishop    which approximates
the intractable marginal log      with the evidence lower
bound  ELBO      
         cid  Eq log            KL qz     pz 

 
  log        KL qz     pz      log     
         is any family of distributions that we choose 
it is indexed by some parameters   and an observation   
Eq denotes expectations with respect to    KL        denotes the KullbackLeibler divergence  KLD  between two
distributions   and    The inequality follows from the nonnegativity of the KLD  and since KL pz     pz        the
bound is tight when                    
The standard choice in DLGMs is to choose           cid 
          where      is the vectorvalued output of an
additional neural network and         is   tractable distribution with parameters      common form for   is   mulk   zk        is
called    mean eld  approximation  Jordan et al   
In this framework  sometimes   is called an  encoder  network and   is called    decoder  network by analogy to
classical neural autoencoders  This analogy suggested the
name  variational autoencoder   VAE  Kingma   Welling 
  in this paper we will use  DLGM  to refer to the

tivariate Gaussian  when             cid 

generative model and  VAE  to refer to the paired encoder decoder architecture for inference generation 
In VAEs the  rst expectation Eq log          is usually intractable  since it is the integral of   complicated nonlinear
function  But we can estimate it by Monte Carlo sampling
from    Usually   single sample suf ces  Depending on
   the KLD term may or may not be tractable to compute
exactly  but again it can be approximated by Monte Carlo
as long as   can be computed 
This architecture has   lot to offer  if the variational distribution   is inexpensive to compute  then we can quickly
and easily estimate and optimize the ELBO using stochastic optimization 
But this speed and ease comes at the price of accuracy 
To the extent that          diverges from          the
ELBO may be   poor approximation of the true marginal
likelihood  and the VEM estimate of   may be systematically different than the true maximumlikelihood estimate
 MLE  In particular  there are two ways to adjust    to
increase the ELBO  increase the true marginal likelihood
log      or make the typical posterior          close in
KLD to         

  Variational pruning

In VAEs this issue tends to manifest itself most dramatically as  pruning   phenomenon where the optimizer
severs the connections between most of the latent variables
  and the data  MacKay    Pruning occurs because of
the KL qz     pz    term in the ELBO  Imagine that we
sever the connections between    and    so that    and  
are independent   This can be easily done by zeroing out
some weights in the neural network      Then
KL qz     pz      KL qz     pz    KL qz       pz     

 
The  rst KLD term can be set to zero by setting       
               To the extent that it is easier for the
optimizer to improve the ELBO by such   strategy  it will
 nd    possibly bad  local optimum that uses only   few
latent dimensions  Hoffman   Blei   Theis   Hoffman   found that for classic mixture models and latent
factor models such as latent Dirichlet allocation  Blei et al 
  this sort of pruning was mostly   local optimum
problem that could be remedied by improved variational
approximations or optimization schemes  Bowman et al 
  and   nderby et al    found improved local
optima in VAEs by initially ignoring and then gradually introducing the KL qz     pz  term  and Burda et al   
found that initializing   mean eld VAE with   model that
had been trained with   more  exible   distribution yielded
less pruning and better local optima  But unlike the results
of Hoffman   Blei   for LDA  Burda et al   

Learning DLGMs with MCMC

still found signi cant pruning even with the more  exible
posterior approximation  and that pruning increased when
moving from the  exible approximation to the mean eld
approximation 
The results of Burda et al    suggest that this pruning phenomenon is not solely due to local optima  Adding
another latent variable to the model should make it possible to improve the average true marginal loglikelihood
log      but it will also make the typical posterior      
   more dif cult to approximate  increasing the average
KL qz     pz    term  At some point the marginal cost in
KLD will be greater than the marginal bene   in log     
even at the global optimum 
What makes the VAE ELBO so susceptible to pruning compared to shallow mixture and factor models  One explanation is that the expressive power of shallow models is
tightly bound to latent dimensionality      fewer mixture
components makes for   lesspowerful mixture model  So
if   shallow model is    to   large  highdimensional dataset
drawn from   dif cultto model population  it may be possible to justify using many latent dimensions  since there is
no other way to explain the observed variation in the data 
By contrast  the expressive power of DLGMs is   function
of both the latent dimensionality and the complexity of the
nonlinear function      Even if   is onedimensional 
  DLGM can in principle approximate arbitrary smooth
densities in RD if   approximates   space lling curve in
RD and   adds some lowvariance noise to    An example of   onelatent dimension DLGM approximating   twodimensional uniform distribution is given in  gure   This
example is contrived  but it could be easily implemented
by   neural network  though it might be hard to     and it
demonstrates the decoupling of latent dimensionality from
expressive power 

  Markov chain Monte Carlo and Hamiltonian

Monte Carlo

Markov chain Monte Carlo  MCMC  Neal    is the
classic alternative to variational methods for approximate
posterior inference  MCMC methods proceed by designing
  Markov chain whose stationary distribution is the distribution of interest  then simulating that chain to draw samples from that distribution  For our purposes here  MCMC
methods have many weaknesses compared to variational
methods 
they may require many  burn in  iterations to
forget their initial state  successive samples from the chain
may be highly correlated  it is sometimes challenging to
diagnose when   chain has burned in  Gelman   Shirley 
  and getting an estimate of marginal probabilities using MCMC is relatively expensive and complicated  Neal 
 

   cid   

Here                          cid   

Figure   Approximating      distribution with      DLGM 
          
              
               and xd  
Uniform gd       
       denotes the standard
normal CDF  and   denotes the sigmoid function  Here      
and       As   and   get large  the model converges to  
   uniform distribution 

     
     gd       

Algorithm   Hamiltonian Monte Carlo

Input  previous latent variable    data   
step size vector   number of steps   
Sample           
Initialize   cid         cid      
for       to   do

Update   cid      cid     
Update   cid      cid          cid 
Update   cid      cid     

       log     cid    
       log     cid    

end for
Sample     Uniform   
   cid     
if     exp   

        cid   

       then

Return   cid 

Return  

else

end if

MCMC   great advantage is that it allows us to trade computation for accuracy without limit  We can estimate any
expectation with respect to the posterior arbitrarily precisely if we run the chain long enough  In contrast  even
very powerful variational methods may be limited by the
form of the variational approximation 
  particularly powerful MCMC algorithm is Hamiltonian
Monte Carlo  HMC  sometimes called hybrid Monte Carlo 
Duane et al    Neal      Suppose the goal
is to sample from                    HMC augments
this model to              cid                     where the
 momentum  variable   has the same dimensionality as   
HMC transforms the problem of posterior sampling into
the problem of simulating the timeevolution of    ctitious
physical system  The algorithm interprets the latent vari 

         Samples from           Samples from     ables   as   position vector  log         as   negative potential energy function  and the Gaussian loglikelihood
   
       
  log   as   negative kinetic energy function  The sum of the potential and kinetic energy functions de nes   Hamiltonian  which in turn de nes   set of
dynamics that are timereversible  conserve volume  and
conserve energy  Those dynamics can be simulated with
the  leapfrog  integrator  which is also timereversible and
volumepreserving  and for small enough integration time
steps is approximately energyconserving over long trajectories  Leimkuhler   Reich    Since the leapfrog integrator is reversible and volumepreserving  we can use
it to de ne   deterministic Metropolis proposal  The acceptance probability of this proposal is min      cid   cid   
          
which depends only on the difference in energy between
the timeevolved state   cid    cid  and the original state       if
the simulation is accurate  that difference will be small and
the probability of acceptance will be high 
Algorithm   outlines the procedure for   single iteration of
HMC  Each iteration  we resample the momentum    apply   iterations of the symplectic  leapfrog  integrator  and
then apply   Metropolis correction to accept or reject the
proposal   cid  Each call requires   calls to the gradient function    log     cid    each leapfrog update begins with the
gradient from the previous update  and if the algorithm is
applied repeatedly then we can cache the initial gradient
   log        
The algorithm can be used with separate step sizes for each
dimension of   and    Neal    which is analogous to
using   diagonal preconditioner in gradient descent  The
optimal step size vector will depend on the problem  but
we can get intuitions from the idealized case where      
     zk          In this case  the optimal setting is
         this allows each dimension to be explored equally
quickly  We will use this intuition below 

    cid 

  log cid 
 cid 

  Practical MCMC for DLGMs
We are interested in optimizing the average marginal loglikelihood

 cid 
    log cid 
 cid 
 cid 

 
 

 
 

  log   xn     
 

       xn dz 

 

Its gradient with respect to   is

       xn dz

 cid 
        xn  log      xn dz 

 

   
 

 
since this is the gradient of the ELBO when           
         and the ELBO is tight  Dempster et al   
The standard VAE training recipe typically approximates
this expectation by replacing          with   more tractable

Learning DLGMs with MCMC

 cid 

distribution          and computing   Monte Carlo estimate
of  
 

Eq  log      xn 

 

 cid 
      log   zs  xs 

 VAE      

 
where   is   randomly  or cyclically  chosen set of integers
and zs is drawn from       xs  That is  the expectation
with respect to   is approximated with   single sample  and
the expectation with respect to the training set is approximated with   minibatch  These stochastic approximations
introduce noise  but no bias 
However  replacing the true posterior          with      
   does introduce bias that cannot be averaged away  We
would like to reduce or eliminate this bias 
  naive approach would be to simply run HMC from some
generic initialization to estimate   log      However  if
we do not run the chain for long enough  this approach may
actually produce   more biased estimate  since it may not
have had time to forget its initialization  Running it for
 long enough  to compete with   variational approximation
may be quite expensive  Instead  we propose using HMC
to improve on an initial variational approximation 
The core idea is this 
initialize an HMC sampler with  
sample from   variational approximation  run it for   small
number of iterations  and use the last sample to get    hopefully nearly unbiased  estimate of   log      More formally  we de ne the re ned distribution   cid  as

              cid 
  cid 

        HMC                    

 

where HMC                 is the distribution of the last
sample from an Mstep HMC chain with stepsize vector  
and   leapfrog steps per iteration  In this approach  we substitute   cid  for   when estimating the gradient of log     
Regardless of how many HMC re nement steps we run    cid 
is guaranteed to have lower KL divergence to the posterior
         than   does  Cover   Thomas    section    
In fact  in some cases the KLD from   cid        to         
may go down exponentially fast in    Ottobre   Pavliotis 
  This is an appealing property  The bias in the gradient of the ELBO comes precisely from the KL qz     pz   
term  and as that term approaches   so too must its gradient
 since   is the minimum possible KLD 
This observation suggests that we should still care about
making the initial variational distribution          as close
as possible to the posterior          since that should reduce the number of HMC steps needed to bring   cid  tolerably
close to the posterior  To that end  we use   standard mean 
 eld inference network approach as proposed by Kingma  
Welling   and Rezende et al    This inference
network   parameters   are trained as usual to maximize

 cid 

  

Learning DLGMs with MCMC

Eq log       
  using this objective 

         But  unlike in VAEs  we will not optimize

  Re nements

In practice  two re nements to the approach outlined above
make it possible to achieve good results with far less computation 
Learning   shared shearing matrix to rotate         
DLGMs are unidenti able up to   rotation  For example 
suppose we have two simple DLGMs de ned by

                           max               

                     

  cid   cid         cid         cid   cid      max        cid           
 

  cid       cid              cid 

where   is an arbitrary rotation matrix  These models de 
 ne the same marginal distribution                cid    We
can see this by substituting    cid      cid  the rotation does
not change the marginal distribution of    and it makes
         cid   cid  and therefore              cid       cid 
Such   rotation does  however  rotate the posteriors  that is 
             cid   cid        This is bad  because mean eld
variational inference is not rotationally invariant it works
best when the posterior exhibits no correlations  In normal
VAE training  the generative network is optimized to be as
friendly as possible to mean eld variational inference  so
it learns to rotate its weight matrices to break this rotational
symmetry  In our setup  this does not naturally happen  our
whole goal is in   sense to avoid letting information about
the variational approximation leak through to the generative network 
To address this issue  we introduce an extra lowertriangular matrix   in the generative network that can partially correct for such rotations  so that our model is
          cid         cid 

          

  cid   cid  Az 

 

  is optimized with   to maximize the variational ELBO
Eq log       
           is constrained to have all of its diagonal
elements equal to   so that it cannot prune out any latent
dimensions  If                      diag    then
    cid              cid       Adiag     cid  so adding  
allows     cid       to have correlation structure  We found
that adding this correlation structure signi cantly improves
the quality of the initial mean eld approximation  In conjunction with the pervariable step size technique outlined
below  we found that using this shearing matrix speeds up
the algorithm by    

Setting pervariable step sizes and the number of
leapfrog steps  HMC has two critical tuning parameters 

the number of leapfrog steps    and the step size vector  
As in gradient descent 
large step sizes allow for fast
progress  but step sizes that are too large lead to unstable results  and therefore Metropolis rejections  It makes sense
to use smaller step sizes in the dimensions with larger gradients and more oscillatory dynamics 
If   is too small  then the algorithm will make small updates each iteration  leading to randomwalk behavior and
slow mixing  Neal   
If   is too large  eventually
the trajectory will turn back and retrace its steps  wasting
computation  Hoffman   Gelman   proposed the noU turn sampler  NUTS    variant that automatically determines an appropriate number of steps  but NUTS is  
complicated recursive algorithm that is quite dif cult to implement in computational graph languages such as TensorFlow  Abadi et al   
We adopt   simple heuristic for setting the step sizes and
leapfrog steps based on intuitions from the case where the
posterior is Gaussian with diagonal covariance 
In this
case  the continuoustime Hamiltonian dynamics in each
dimension   are sinusoidal with period     where    is
the scale of dimension   
If we use   step size vector  
such that          then after   steps of leapfrog integration the dynamics in each dimension will have advanced
 cid  so that each
by   
HMC iteration simulates approximately       periods  This number is somewhat arbitrary  it is chosen to be
slightly less than   periods  We found that using larger
integration times sometimes caused undesirable resonances
and aliasing  Neal   
We tune   throughout training to give   reasonable worstcase acceptance rate  In each minibatch  we compute the
average acceptance rate over the   HMC iterations for
each example in the minibatch  If the smallest average acceptance rate is less than   we decrease   by  
otherwise we increase   by   This worstcase procedure ensures that   is small enough to allow the chains
for all training examples to mix  simply tuning the average
acceptance rate would not guarantee this  Although this resembles an adaptive MCMC algorithm  Andrieu   Thoms 
  note that adaptation is done between MCMC runs 
so the usual caveats and requirements of vanishing adaptation do not apply 
We summarize our HMCbased approach to DLGM training in algorithm   Figure   sketches   speci   training
architecture 

  periods  We therefore set      cid   

 

  Evaluating heldout likelihood

  bene   of variational methods over MCMC is that they
immediately provide easyto estimate lower bounds on the
marginal likelihood    relatively reliable  but expensive 

Learning DLGMs with MCMC

Algorithm   Hamiltonian Monte Carlo for DLGMs

Input  dataset    SGD step size schedule     minibatch size    initial HMC step size   number of HMC
steps    number of iterations    
Randomly initialize the inference network parameters  
the generative network parameters   and the shearing
matrix   
for       to   do

Compute      cid   cid 
Select   minibatch   from the dataset   
for xs     do

Sample zs       xs   xs 
Compute   
Compute   
Compute         xs 
for       to   do

      log
       log

  zs xs 

   zs xs xs 
   zs xs xs 

  zs xs 

sample zs   from HMC zs    xs        

end for
Compute   

      log   zs     xs 

end for
Apply gradient update to   using      
 
    
 
Apply gradient update to   using      
 
    
 
   set
Apply gradient update to   using       
    
Ak       Ak       for all                  and       
 

 cid 
 cid 
 cid 

end for

way to use MCMC to estimate marginal likelihoods is annealed importance sampling  AIS  Neal    and we
follow Wu et al    in using AIS to evaluate our trained
models on heldout data  We use   samples per test example    annealing steps  and the same HMC step size and
number of leapfrog steps as during training 

  Comparison to Related Work
The idea of using MCMC to improve variational approximations has come up   number of times       De Freitas
et al    Mimno et al   
Our approach is most closely related to the Hamiltonian
variational inference  HVI  approach proposed by Salimans et al    HVI also initializes an HMC algorithm
with   sample from an inference network  But HVI learns
to explicitly lowerbound the entropy of the marginal distribution of the  nal sample from the MCMC chain  making it possible to directly optimize and estimate   bound
on the ELBO obtained by using this marginal distribution
as   variational distribution  By contrast  we make no attempt to estimate that ELBO  and instead optimize our variational parameters   to maximize the much looser ELBO
de ned by the initial variational distribution  This is   relative strength of HVI 

Our approach also has several relative strengths over HVI 
First  it is simpler HVI requires training an auxiliary inverse inference network to reverse the HMC Markov chain
that approximates the posterior  Second  to the extent that
this inverse network cannot exactly reverse the Markov
chain  the HVI lower bound on the ELBO will not be tight 
reintroducing some bias in the gradient estimates  Finally 
using HVI with Markov chains that involve multiple accept reject steps is dif cult and or expensive  since the inverse inference network must then learn to infer the binary accept reject random variables  This last issue is highlighted by our results in section   where  like Wolf et al 
  we  nd that using multiple HMC steps is important 
More recently  Han et al 
  proposed using
Metropolisadjusted Langevin       HMC with one
leapfrog step  to    DLGMs in   stochastic EM framework 
Their approach is effective for small datasets  but it is inherently   batch method that involves maintaining   Markov
chain for each example throughout training  This limits its
ability to scale to very large datasets 
There has been    urry of work in the last few years developing more and more accurate variational approximations
for VAEs  Salimans et al    Rezende   Mohamed 
  Burda et al    Tran et al      nderby et al 
  Kingma   Salimans    This work has demonstrated the bene ts of using accurate posterior approximations  which originally motivated us to explore using highly
accurate MCMC approximations 

  Experiments
Below  we compare the results of our proposed method to  
baseline mean eld VAE and to other published results on
the binarized MNIST dataset  LeCun et al    In all
experiments  we used   simple fully connected architecture
with one stochastic layer of   latent variables and two deterministic hidden layers with   hidden units each and
softplus nonlinearities  The architecture is sketched in  gure   For MNIST  we used   images for training
and held out   for testing  The training images were
rebinarized each epoch to prevent over tting  as done by 
     Burda et al    All models were optimized using Adam  Kingma   Ba    with default parameters
and   minibatch size of   We trained all models for  
epochs with   learning rate of   then for another  
epochs with   learning rate of  

  Heldout likelihoods

We estimate heldout likelihood using annealed importance
sampling  AIS  Neal    Performance as   function of
number of steps of HMC is summarized in  gure  
Using more HMC steps during training clearly leads to

Learning DLGMs with MCMC

Figure   Heldout likelihood as   function of number of HMC
steps   used during training  More computation leads to better
results  but even   few HMC steps provide   major advantage over
standard VAE training       

Table   Reported heldout loglikelihoods on dynamically binarized permutationinvariant MNIST  with number of stochastic
layers  HMCDLGM  and HMCDLGM  denote our approach with   and   HMC steps  respectively  Other approaches
are due to   Salimans et al      Burda et al     
Tran et al        nderby et al   

MODEL
VAE BASELINE
HVI  
HMCDLGM 
IWAE  
IWAE  
HMCDLGM 
VGP  
LVAE  

  LAYERS   log     
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

   GPU    times longer than with       It may be
instructive to compare this number with the cost of training an IWAE  Burda et al    which scales linearly in
the number of samples used during training Burda et al 
  used   samples in their experiments 

  Pruning and blurriness

In this section  we evaluate some qualitative features of the
MNIST model trained with our HMCbased procedure 
Figure   shows some samples from the VAE and from the
DLGM trained with       The VAE samples are blurry 
while the HMCtrained model generates sharp samples 
Some of the HMCtrained model   samples look strange 

 This is less than might be expected  given that each HMC
iteration applies several leapfrog steps  typically   The cost
of   leapfrog step is less than the cost of computing gradients in  
VAE  since we need not backprop through the inference network 

Figure   Architecture used in the experiments  Dotted lines denote weight sharing   fc     denotes   dense matrix multiply
with output dimension     shear     denotes   unitdiagonal
lowertriangular matrix multiply with output dimension    Parameters to cyan nodes are optimized with respect to the inference
logloss  parameters to purple nodes are optimized with respect to
the generation logloss 

better models  Our best loglikelihood of   compares favorably with other reported values in the literature for permutationinvariant       nonconvolutional  binarized MNIST models  see table   These number could
doubtless be improved we did very little experimentation
with architectural choices  For example  all models in table   that outperform ours  and some that don    use  
stochastic layers  whereas we use only one 
The heldout likelihood with two HMC steps is very close
to that obtained by Salimans et al    who used only
one HMC update due to the computational dif culty of using many Metropolis accept reject steps in their framework 
This suggests that the improved performance of our approach is indeed due to the use of many HMC steps  rather
than due to architecture  hyperparameters  etc 
The perminibatch cost of our approach goes up linearly
with    the number of HMC steps  Training an MNIST
model with       took about   hours on an NVIDIA

  fc  softplusfc  softplusfc          softplusfc  softplusxfc  Normal    HMCshear    zMfc  softplusfc  softplusfc    shear    inferencelogloss     generationlogloss  Performance versus ComputationNumber of HMC steps MHeldout loglikelihood Learning DLGMs with MCMC

dimensions to estimate

          cid 

  cid       cid                      cid dz cid 

 
where   cid  is   vector of dimension        and   denotes
concatenation  If we  rst rotate   by     then     will in
  sense denote the   mostimportant directions in zspace 
and information that is encoded by the lessimportant directions zC   will be blurred out in           

Figure   Left  Samples from   VAE  Right  Samples from  
DLGM trained with HMC  More examples are in the supplement 

but many look good  which is consistent with the observation of Theis et al    that maximumlikelihood training places   relatively small penalty on generating many
bad samples       generating   garbage costs only  
nats of loglikelihood 

Figure   In uence of latent directions on output space 

Following Krishnan   Hoffman  
to determine
which directions in zspace are most and least important  we sample   random vectors    from their
       marginal distribution and compute the singular
value decomposition of the average Jacobian matrix   of
the expected observation vector with respect to   
    SV  cid 

 cid 

   cid   

 

     

  

 cid cid cid zi

 

  

Figure   shows the singular value spectra given by the diagonal of   for the VAE and the HMCtrained DLGM  The
VAE   spectrum falls off sharply at about   showing that
the remaining   dimensions are not used to explain the
data  The spectrum for the DLGM trained with HMC falls
off much more gradually 
To get   qualitative idea of what is encoded in which dimensions  we can marginalize out all but the  rst   latent

Figure   Effects of keeping only the  rst   most important dimensions of    marginalizing out the remaining dimensions  More
examples are in the supplement 
Figure   displays            for various values of   for
 ve   vectors  for the model trained with       Identity and broad structure is indeed encoded by the lowerorder principal components  but the images remain somewhat blurry until many higherorder components are added 
This suggests that variational pruning is at least partially responsible for some of the blurriness associated with VAEs 

  Conclusion
We have proposed   practical approach to using HMC to
train DLGMs  This approach yields lessbiased gradients
of the true marginal likelihood of the data than mean eld
VAEs  and thereby learns better generative models 
It is
more expensive than standard VAE training  but not prohibitively so 

References
Abadi  Mart    Agarwal  Ashish  Barham  Paul  Brevdo 
Eugene  Chen  Zhifeng  Citro  Craig  Corrado  Gregory    Davis  Andy  Dean  Jeffrey  Devin  Matthieu 

 The   vectors are randomly sampled  but curated due to space

constraints  Uncurated examples are in the supplement 

LogSingular Value Spectrum of JSingular value indexSingular value VAEHMC                               Learning DLGMs with MCMC

Ghemawat  Sanjay  Goodfellow 
Ian    Harp  Andrew  Irving  Geoffrey  Isard  Michael  Jia  Yangqing 
  ozefowicz  Rafal  Kaiser  Lukasz  Kudlur  Manjunath 
Levenberg  Josh  Man    Dan  Monga  Rajat  Moore 
Sherry  Murray  Derek Gordon  Olah  Chris  Schuster 
Mike  Shlens  Jonathon  Steiner  Benoit  Sutskever  Ilya 
Talwar  Kunal  Tucker  Paul    Vanhoucke  Vincent 
Vasudevan  Vijay  Vi egas  Fernanda    Vinyals  Oriol 
Warden  Pete  Wattenberg  Martin  Wicke  Martin  Yu 
Yuan  and Zheng  Xiaoqiang  TensorFlow  Largescale
machine learning on heterogeneous distributed systems 
arXiv preprint arXiv   

Andrieu  Christophe and Thoms  Johannes    tutorial on
adaptive MCMC  Statistics and Computing   
   

Bishop     Pattern Recognition and Machine Learning 

Springer New York   

Blei     Ng     and Jordan     Latent Dirichlet allocation  Journal of Machine Learning Research   
  January  

Bowman  Samuel    Vilnis  Luke  Vinyals  Oriol  Dai  Andrew    Jozefowicz  Rafal  and Bengio  Samy  Generating sentences from   continuous space  CoNLL  pp   
 

Burda  Yuri  Grosse  Roger  and Salakhutdinov  Ruslan 
In International

Importance weighted autoencoders 
Conference on Learning Representations   

Cover  Thomas   and Thomas  Joy    Elements of infor 

mation theory  John Wiley   Sons   

De Freitas  Nando    jenS rensen  Pedro 

Jordan 
Michael    and Russell  Stuart  Variational MCMC 
In Proceedings of the Seventeenth conference on Uncertainty in arti cial intelligence  pp    Morgan
Kaufmann Publishers Inc   

Dempster     Laird     and Rubin     Maximum likelihood from incomplete data via the EM algorithm  Journal of the Royal Statistical Society  Series     
 

Duane  Simon  Kennedy  Anthony    Pendleton  Brian   
and Roweth  Duncan  Hybrid monte carlo  Physics letters       

Gelman  Andrew and Shirley  Kenneth 

Inference from
simulations and monitoring convergence  In Handbook
of Markov chain Monte Carlo  pp    Chapman
and Hall CRC   

Hoffman  Matthew    and Blei  David    Structured
In International Constochastic variational inference 
ference on Arti cial Intelligence and Statistics  pp   
   

Hoffman  Matthew    and Gelman  Andrew  The nou turn
sampler  adaptively setting path lengths in Hamiltonian
Monte Carlo  Journal of Machine Learning Research 
   

Jordan     Ghahramani     Jaakkola     and Saul     Introduction to variational methods for graphical models 
Machine Learning     

Kingma  Diederik and Ba  Jimmy  Adam    method for
stochastic optimization  In International conference on
learning representations   

Kingma  Diederik   and Salimans  Tim  Improving variational inference with inverse autoregressive  ow  In Advances in Neural Information Processing Systems   

Kingma       and Welling  Max  Autoencoding variational Bayes  In International Conference on Learning
Representations   

Krishnan  Rahul   and Hoffman  Matthew   

Inference
  introspection in deep generative models of sparse
In NIPS Workshop on Advances in Approximate
data 
Bayesian Inference   

LeCun  Yann  Cortes  Corinna  and Burges  Christopher JC  The mnist database of handwritten digits   

Leimkuhler  Benedict and Reich  Sebastian  Simulating
hamiltonian dynamics  volume   Cambridge University Press   

MacKay  David       Local minima  symmetrybreaking 
and model pruning in variational free energy minimization  Technical report   

Mimno     Hoffman     and Blei     Sparse stochastic
inference for latent Dirichlet allocation  In International
Conference on Machine Learning   

Neal     Probabilistic inference using Markov chain Monte
Carlo methods  Technical Report CRGTR  Department of Computer Science  University of Toronto 
 

Neal  Radford    Annealed importance sampling  Statis 

tics and Computing     

Han  Tian  Lu  Yang  Zhu  SongChun  and Wu  Ying Nian 
Alternating backpropagation for generator network  In
AAAI Conference on Arti cial Intelligence   

Neal  Radford    MCMC using Hamiltonian dynamics  In
Handbook of Markov Chain Monte Carlo  pp   
Chapman and Hall CRC   

Learning DLGMs with MCMC

Ottobre    and Pavliotis  GA  Asymptotic analysis for
the generalized Langevin equation  Nonlinearity   
   

Rezende  Danilo and Mohamed  Shakir  Variational inferIn International Confer 

ence with normalizing  ows 
ence on Machine Learning  pp     

Rezende  Danilo Jimenez  Mohamed  Shakir  and Wierstra 
Daan  Stochastic backpropagation and approximate inference in deep generative models  In International Conference on Machine Learning   

Salimans  Tim  Kingma  Diederik    Welling  Max  et al 
Markov chain Monte Carlo and variational inference 
Bridging the gap  In International Conference on Machine Learning  volume   pp     

  nderby  Casper Kaae  Raiko  Tapani  Maal    Lars 
  nderby    ren Kaae  and Winther  Ole  Ladder variational autoencoders  In Advances in Neural Information
Processing Systems  pp     

Theis     van den Oord     and Bethge       note on the
evaluation of generative models  In International Conference on Learning Representations  Apr   URL
http arxiv org abs 

Theis  Lucas and Hoffman  Matthew      trustregion
method for stochastic variational inference with applications to streaming data  In International Conference
on Machine Learning  pp     

Tran     Ranganath     and Blei     The variational Gaussian process  In International Conference on Learning
Representations   

Wolf  Christopher  Karl  Maximilian  and van der Smagt 
Patrick  Variational inference with Hamiltonian Monte
Carlo  arXiv preprint arXiv   

Wu  Yuhuai  Burda  Yuri  Salakhutdinov  Ruslan  and
On the quantitative analysis of
arXiv preprint

Grosse  Roger 
decoderbased generative models 
arXiv   

