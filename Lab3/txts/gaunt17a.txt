Differentiable Programs with Neural Libraries

Alexander    Gaunt   Marc Brockschmidt   Nate Kushman   Daniel Tarlow  

Abstract

We develop   framework for combining differentiable programming languages with neural networks  Using this framework we create endto 
end trainable systems that learn to write interpretable algorithms with perceptual components 
We explore the bene ts of inductive biases for
strong generalization and modularity that come
from the programlike structure of our models  In
particular  modularity allows us to learn   library
of  neural  functions which grows and improves
as more tasks are solved  Empirically  we show
that this leads to lifelong learning systems that
transfer knowledge to new tasks more effectively
than baselines 

  Introduction
Recently  there has been much work on learning algorithms
using neural networks  Following the idea of the Neural Turing Machine  Graves et al    this work has focused on
extending neural networks with interpretable components
that are differentiable versions of traditional computer components  such as external memories  stacks  and discrete
functional units  However  trained models are not easily
interpreted as the learned algorithms are embedded in the
weights of   monolithic neural network  In this work we  ip
the roles of the neural network and differentiable computer
architecture  We consider interpretable controller architectures which express algorithms using differentiable programming languages  Gaunt et al    Riedel et al   
Bunel et al    In our framework  these controllers can
execute discrete functional units  such as those considered
by past work  but also have access to   library of trainable 
uninterpretable neural network functional units  The system is endto end differentiable such that the source code
representation of the algorithm is jointly induced with the
parameters of the neural function library  In this paper we

 Microsoft Research  Cambridge  UK  Google Brain  Montr eal 
Canada  work done while at Microsoft  Correspondence to 
Alexander    Gaunt  algaunt microsoft com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

explore potential advantages of this class of hybrid model
over purely neural systems  with   particular emphasis on
lifelong learning systems that learn from weak supervision 
We concentrate on perceptual programming by example
 PPBE  tasks that have both algorithmic and perceptual elements to exercise the traditional strengths of programlike
and neural components  Examples of this class of task include navigation tasks guided by images or natural language
 see Fig    or handwritten symbol manipulation  see Sec   
Using an illustrative set of PPBE tasks we aim to emphasize
two speci   bene ts of our hybrid models 
First  the source code representation in the controller allows
modularity  the neural components are small functions that
specialize to different tasks within the larger program structure  It is easy to separate and share these functional units to
transfer knowledge between tasks  In contrast  the absence
of wellde ned functions in purely neural solutions makes
effective knowledge transfer more dif cult  leading to problems such as catastrophic forgetting in multitask and lifelong learning  McCloskey   Cohen    Ratcliff   
In our experiments  we consider   lifelong learning setting
in which we train the system on   sequence of PPBE tasks
that share perceptual subtasks 
Second  the source code representation enforces an inductive bias that favors learning solutions that exhibit strong
generalization  For example  once   suitable control  ow
structures         for loop  for   list manipulation problem was learned on short examples  it trivially generalizes
to lists of arbitrary length  In contrast  although some neural architectures demonstrate   surprising ability to generalize  the reasons for this generalization are not fully understood  Zhang et al    and generalization performance
invariably degrades as inputs become increasingly distinct
from the training data 
This paper is structured as follows  We  rst present   language  called NEURAL TERPRET  NTPT  for specifying
hybrid source code neural network models  Sec    and
then introduce   sequence of PPBE tasks  Sec    Our
NTPT models and purely neural baselines are described
in Sec    and   respectively  The experimental results are
presented in Sec   

Differentiable Programs with Neural Libraries

Figure   Components of an illustrative NTPT program for learning loopy programs that measure path length  path len 
through   maze of street sign images  The learned program  parameterized by instr and goto  must control the position
       of an agent on   grid of       street sign images each of size       The agent has   single register of memory
 reg  and learns to interpret street signs using the LOOK neural function  Our system produces   solution consisting of  
correctly inferred program and   trained neural network  see Supplementary Material  Learnable components are shown in
blue and the NTPT extensions to the TERPRET language are highlighted  The red path on the img grid shows the desired
behavior and is not provided at training time 

  Building hybrid models
The TERPRET language  Gaunt et al    provides   system for constructing differentiable program interpreters that
can induce source code operating on basic data types      
integers  from inputoutput examples  We extend this language with the concept of learnable neural functions  These
can either be embedded inside the differentiable interpreter
as mappings from integer to integer or  as we emphasize in
this work  can act as learnable interfaces between perceptual data represented as  oating point Tensors and the differentiable interpreter   integer data type  Below we brie  
review the TERPRET language and describe the NEURAL
TERPRET extensions 

  TERPRET

TERPRET programs specify   differentiable interpreter by
de ning the relationship between Inputs and Outputs
via   set of inferrable Params  that de ne an executable
program  and Vars  that store intermediate results  TERPRET requires all of these variables to range over bounded
integers  The model is made differentiable by   compilation
step that lifts the relationships between integers speci ed
by the TERPRET code to relationships between marginal

distributions over integers in  nite ranges  Fig    illustrates
an example application of the language 
TERPRET can be translated into   TensorFlow  Abadi et al 
  computation graph which can then be trained using
standard methods  For this  two key features of the language
need to be translated 

The

is

  Function

     cid 

application 
statement
into
  set to foo      
  where    represents the
  
marginal distribution for the variable   and   is an
indicator tensor      foo       This approach
extends to all functions mapping any number of
integer arguments to an integer output 

jk Iijk  

translated

    

  Conditional

statements The

statements
        set to    elif      
     
  set to    are translated to        
Statements switching between more than two cases
follow   similar pattern  with details given in  Gaunt
et al   

         

if

  Discrete operations Runtime max int  max int defINC   return         max int Runtime max int  max int defDEC   return       max int Runtime        defMOVE      dir ifdir    return              elifdir    return            else  returnx Runtime        defMOVE      dir ifdir    return            elifdir    return              else  returny  Helper functions Runtime defeq zero dir return  ifdir    else  Learned operations Learn Tensor     hid sizes defLOOK img pass  constantsmax int      instr                                 variablesimg grid  InputTensor           init    Input   init    Input   final    Output   final    Output   path len  Output max int is halted at end  Output instr  Param   instr goto  Param   instr   instr     Var         Var     dir  Var   reg  Var max int   instr ptr  Var   instr   is halted  Var    initializationX set to init     set to init   dir set to reg set to instr ptr set to fort inrange    is halted   set to eq zero dir   ifis halted            halteddir      set to dir          set to            set to     reg      set to reg   instr ptr      set to instr ptr   elifis halted          not haltedwithinstr ptr    asi ifinstr                           INCreg      set to INC reg   elifinstr                         DECreg      set to DEC reg   else reg      set to reg   ifinstr                           MOVEX      set to MOVE        dir          set to MOVE        dir   else        set to            set to     ifinstr                           LOOKwith      as   with      as   dir      set to LOOK img grid     else dir      set to dir   instr ptr      set to goto   final   set to      final   set to      path len set to reg    is halted at end set to ishalted    Instruction SetDeclaration   initializationExecution modelInputoutput data setimg grid init     init     final     final     path len   instr   goto     if not halted dir  LOOKhalt if dir gotoL   if not halted MOVE dir gotoL   if not halted reg  INC reg gotoL SolutionLOOK Differentiable Programs with Neural Libraries

Figure   Overview of tasks in the     ADD        APPLY    and     MATH scenarios      denotes the APPLY operator
which replaces the   tiles with the selected operators and executes the sum  We show two MATH examples of different
length 

  NEURAL TERPRET

To handle perceptual data  we relax the restriction that all
variables need to be  nite integers  We introduce   new
 oating point Tensor type whose dimensions are  xed
at declaration  and which is suitable for storing perceptual data  Additionally  we introduce learnable functions
that can process integer or tensor variables    learnable
function is declared using  Learn            dD  dout 
hid sizes cid           cid    where the  rst component
speci es the dimensions  resp  ranges             dD of the
input tensors  resp  integers  and the second speci es the dimension of the output  NTPT compiles such functions into
  fullyconnected feedforward neural network whose layout is controlled by the hid sizes component  specifying
the number neurons in each layer  The inputs of the function are simply concatenated  Tensor output is generated by
learning   mapping from the last hidden layer  and  nite
integer output is generated by   softmax layer producing  
distribution over integers up to the declared bound  Learnable parameters for the generated network are shared across
every use of the function in the NTPT program  and as they
naturally    into the computation graph for the remaining
TERPRET program  the whole system is trained endto end 
We illustrate an example NTPT program for learning navigation tasks in   maze of street signs  Stallkamp et al   
in Fig   

    Lifetime of PPBE Tasks
Motivated by the hypothesis that the modularity of the
source code representation bene ts knowledge transfer  we
devise   sequence of PPBE tasks to be solved by sharing
knowledge between tasks  Our tasks are based on algorithmic manipulation of handwritten digits and mathematical
operators 
In early tasks the model learns to navigate simple      
grids of images  and to become familiar with the concepts
of digits and operators from   variety of weak supervision 
Despite their simplicity  these challenges already pose problems for purely neural lifelong learning systems 

The  nal task in the learning lifetime is more complex and
designed to test generalization properties  the system must
learn to compute the results of variablelength mathematical
expressions expressed using handwritten symbols  The algorithmic component of this task is similar to arithmetic tasks
presented to contemporary Neural GPU models  Kaiser  
Sutskever    Price et al    The complete set of
tasks is illustrated in Fig    and described in detail below 

ADD    scenario  The  rst scenario in Fig      uses of
        grid of MNIST digits  We set   tasks based on this
grid  compute the sum of the digits in the   top row    left
column    bottom row    right column  All tasks require
classi cation of MNIST digits  but need different programs
to compute the result  As training examples  we supply only
  grid and the resulting sum  Thus  we never directly label
an MNIST digit with its class 

APPLY    scenario  The second scenario in Fig     
presents         grid of of handwritten arithmetic operators  Providing three auxiliary random integers         
we again set   tasks based on this grid  namely to evaluate the expression     op     op     where  op  op 
are the operators represented in the   top row    left column    bottom row    right column  In comparison to
the  rst scenario  the dataset of operators is relatively small
and consistent  making the perceptual task of classifying
operators considerably easier 

MATH scenario  The  nal task in Fig      requires combination of the knowledge gained from the weakly labeled
data in the  rst two scenarios to execute   handwritten arithmetic expression 

 Note that for simplicity  our toy system ignores operator precedence and executes operations from left to right        the sequence
in the text is executed as     op     op    

  handwritten examples of each operator were collected
from   single author to produce   training set of   symbols and
  test set of   symbols from which to construct random      
grids 

   AAA           Differentiable Programs with Neural Libraries

   

   

  initialization  
     READ
  program  
     MOVE EAST
     MOVE SOUTH
     SUM        
     NOOP
return   

  initialization  
     InputInt  
     InputInt  
     InputInt  
     READ
  program  
     MOVE EAST
     MOVE SOUTH
     APPLY             
     APPLY             
return   

Figure   Example solutions for the tasks on the right
columns of the     ADD    and     APPLY    scenarios 
The read head is initialized READing the top left cell and
any auxiliary InputInts are loaded into memory  Instructions and arguments shown in black must be learned 

  Models
We study two kinds of NTPT model  First  for navigating
the introductory       grid scenarios  we create   model
which learns to write simple straightline code  Second  for
the MATH scenario we ask the system to use   more complex language which supports loopy control  ow  note that
the baselines will also be specialized between the      
scenarios and the MATH scenario  Knowledge transfer is
achieved by de ning   library of   neural network functions
shared across all tasks and scenarios  Training on each task
should produce   taskspeci   source code solution  from
scratch  and improve the overall usefulness of the shared
networks  All models are included in Supplementary Material  and below we outline further details of the models 

  Shared components

We refer to the   networks in the shared library as net  
and net   Both networks have similar architectures  they
take         monochrome image as input and pass this
sequentially through two fully connected layers each with
  neurons and ReLU activations  The last hidden vector
is passed through   fully connected layer and   softmax to
produce     dimensional output  net   or   dimensional
output  net   to feed to the differentiable interpreter  the
output sizes are chosen to match the number of classes of
MNIST digits and arithmetic operators respectively 
One restriction that we impose is that when   new task is
presented  no more than one new untrained network can
be introduced into the library       in our experiments the
very  rst task has access to only net   and all other tasks
have access to both nets  This restriction is imposed because if   differentiable program tries to make   call to one
of   untrained networks based on an unknown parameter
net choice   Param    then the system effectively
sees the   nets together with the net choice parameter
as one large untrained network  which cannot usefully be
split apart into the   components after training 

        model
For the     scenarios we build   model capable of writing
short straight line algorithms with up to   instructions  The
model consists of   read head containing net   and net  
which are connected to   set of registers each capable of
holding integers in the range              where      
The head is initialized reading the top left cell of the      
grid  At each step in the program  one instruction can be
executed  and lines of code are constructed by choosing an
instruction and addresses of arguments for that instruction 
We follow  Feser et al    and allow each line to store
its result in   separate immutable register  For the ADD   
scenario the instruction set is 

  NOOP    trivial nooperation instruction 
  MOVE NORTH  MOVE EAST  MOVE SOUTH 

MOVE WEST 
translate the head  if possible  and
return the result of applying the neural network chosen
by net choice to the image in the new cell 

  ADD  accepts two register addresses and returns

the sum of their contents 

The parameter net choice is to be learned and decides
which of net   and net   to apply  In the APPLY   
scenario we extend the ADD instruction to APPLY      
op  which interprets the integer stored at op as an arithmetic operator and computes    op    In addition  for the
APPLY    scenario we initialize three registers with the
auxiliary integers supplied with each       operator grid
 see Fig      In total  this model exposes   program space
of up to     syntactically distinct programs 

  MATH model

The  nal task investigates the synthesis of more complex 
loopy control  ow    natural solution to execute the expression on the tape is to build   loop with   body that alternates
between moving the head and applying the operators  see
Fig      This loopy solution has the advantage that it generalizes to handle arbitrary length arithmetic expressions 
Fig      shows the basic architecture of the interpreter used
in this scenario  We provide   set of three blocks each containing the instruction MOVE or APPLY  an address    register and   net choice    MOVE instruction increments
the position of the head and loads the new symbol into  
block   register using either net   or net   as determined
by the block   net choice  After executing the instruction  the interpreter executes   GOTO IF statement which
checks whether the head is over the end of the tape and if not

 All operations are performed modulo        and division

by zero returns   

Differentiable Programs with Neural Libraries

   

   

Figure   Overview of the MATH model      The general
form of   block in the model  Blue elements are learnable 
      loopbased solution to the task in the MATH scenario 

then it passes control to the block speci ed by goto addr 
otherwise control passes to   halt block which returns  
chosen register value and exits the program  This model
describes   space of     syntactically distinct programs 

  Baselines
To evaluate the merits of including the source code structure
in NTPT models  we build baselines that replace the differentiable program interpreter with neural networks  thereby
creating purely neural solutions to the lifelong PPBE tasks 
We specialize these neural baselines for the     task  with
emphasis on lifelong learning  and for the MATH task  with
emphasis on generalization 
        baselines
We de ne   column as the following neural architecture  see
Fig     

  Each of the images in the       grid is passed through
an embedding network with   layers of   neurons  cf 
net   to produce    dimensional embedding 
The weights of the embedding network are shared
across all   images 

  These   embeddings are concatenated into    
dimensional vector and for the APPLY    the auxiliary integers are represented as onehot vectors and
concatenated with this  dimensional vector 

  This is then passed through   network consisting of
  hidden layers of   neurons to produce    
dimensional output 

We construct   different neural baselines derived from this
column architecture  see Fig   

  Indep  Each task is handled by an independent col 

umn with no mechanism for transfer 

  Progressive Neural Network  PNN  We follow
 Rusu et al    and build lateral connections linking each task speci   column to columns from tasks
appearing earlier in the learning lifetime  Weights in
all columns except the active task   column are frozen
during   training update  Note that the number of layers in each column must be identical to allow lateral
connections  meaning we cannot tune the architecture
separately for each task 

  Multitask neural network  MTNN  We split the column into   shared perceptual part and   task speci  
part  The perceptual part consists of net   and net  
embedding networks  note that we use   similar symmetry breaking technique mentioned in Sec    to encourage specialization of these networks to either digit
or operator recognition respectively 

The taskspeci   part consists of   neural network that
maps the perceptual embeddings to     dimensional
output  Note that unlike PNNs  the precise architecture
of the task speci   part of the MTNN can be tuned for
each individual task  We consider two MTNN architectures 

    MTNN  All taskspeci   parts are   layer net 

works comparable to the PNN case 

    MTNN  We manually tune the number of layers
for each task and  nd best performance when the
task speci   part contains   hidden layer for the
ADD    tasks and   layers for the APPLY   
tasks 

  MATH baselines

For the MATH task  we build purely neural baselines which
  have previously been shown to offer competitive generalization performance for some tasks with sequential inputs
of varying length   are able to learn to execute arithmetic
operations and   are easily integrated with the library of
perceptual networks learned in the     tasks  We consider
two models ful lling these criteria  an LSTM and   Neural
GPU 
For the LSTM  at each image in the mathematical expression the network takes in the embeddings of the current
symbol from net   and net   updates an LSTM hidden
state and then proceeds to the next symbol  We make   classi cation of the  nal answer using the last hidden state of
the LSTM  Our best performance is achieved with     layer
LSTM with   elements in each hidden state and dropout
between layers 

ifis MOVE pos else Ri  APPLY Ri  READ Labeli GOTO IF halt returninstrinet choiceiarg iarg iarg igoto addrinstrjLabelj return addrL MOVER  READ net GOTO IFL      APPLY         GOTO IFL   MOVER  READ net GOTO IFL halt returnR Differentiable Programs with Neural Libraries

Figure   Cartoon illustration of all models used in the       experiments  See text for details 

For the Neural GPU  we use the implementation from the
original authors   Kaiser   Sutskever   

  Experiments
In this section we report results illustrating the key bene ts
of NTPT for the lifelong PPBE tasks in terms of knowledge
transfer  Sec    and generalization  Sec   

  Lifelong Learning

Demonstration of lifelong learning requires   series of tasks
for which there is insuf cient data to learn independent solutions to all tasks and instead  success requires transferring
knowledge from one task to the next  Empirically  we  nd
that training any of the purely neural baselines or the NTPT
model on individual tasks from the ADD    scenario with
only    distinct       examples produces low accuracies of
around      measured on   heldout test set of    examples  Since none of our models can satisfactorily solve
an ADD    task independently in this small data regime 
we can say that any success on these tasks during   lifetime
of learning can be attributed to successful knowledge transfer  In addition  we check that in   data rich regime      
   examples  all of the baseline models and NTPT can
independently solve each task with   accuracy  This
indicates that the models all have suf cient capacity to represent satisfactory solutions  and the challenge is to  nd
these solutions during training 
We train on batches of data drawn from   timeevolving
probability distribution over all   tasks in the   scenarios
 see the top of Fig      During training  we observe the
following key properties of the knowledge transfer achieved
by NTPT 

 available

at https github com tensorflow 

models tree master neural gpu

Reverse transfer  Fig      focuses on the performance of
NTPT on the  rst task  ADD   top  The red bars indicate
times where the the system was presented with an example
from this task  Note that even when we have stopped presenting examples  the performance on this task continues to
increase as we train on later tasks   an example of reverse
transfer  We verify that this is due to continuous improvement of net   in later tasks by observing that the accuracy
on the ADD   top task closely tracks measurements of
the accuracy of net   directly on the digit classi cation
task 
Avoidance of catastrophic forgetting  Fig      shows the
performance of the NTPT on the remaining ADD    tasks 
Both Fig      and     include results for the MTNN  baseline  the best baseline for these tasks  Note that whenever
the dominant training task swaps from an ADD    task to
an APPLY    task the baseline   performance on ADD   
tasks drops  This is because the shared perceptual network
becomes corrupted by the change in task   an example of
catastrophic forgetting  To try to limit the extent of catastrophic forgetting and make the shared components more
robust  we have   separate learning rate for the perceptual
networks in both the MTNN baseline and NTPT which is
  fold smaller than the learning rate for the taskspeci  
parts  With this balance of learning rates we  nd empirically
that NTPT does not display catastrophic forgetting  while
the MTNN does 
Final performance  Fig      focuses on the ADD   left
and APPLY   left tasks to illustrate the relative performance of all the baselines described in Sec    Note that
although PNNs are effective at avoiding catastrophic forgetting  there is no clear overall winner between the MTNN
and PNN baselines  NTPT learns faster and to   higher accuracy than all baselines for all the tasks considered here 
For clarity we only plot results for the  left tasks  the other
tasks show similar behavior and the accuracies for all tasks
at the end of the lifetime of learning are presented in Fig   

concatconcatconcatR    REAR    MOVR    SUMR    NOOR    NOOreturn RR    InpR    InpR    InpR    MOVR    MOVR    APPR    REAR    MOVR    MOVR    SUMR    NOOreturn RLibrary    PNN    NTPTTASK  TASK  TASK  concat    indep concat    MTNN Differentiable Programs with Neural Libraries

   

   

   

Figure   Lifelong learning with NTPT      top  the sequential learning schedule for all   tasks  bottom  performance of
NTPT  solid  and the MTNN  baseline  dashed  on the  rst ADD    task      performance on the remaining ADD   
tasks      Performance of all the baselines on the  left tasks 

task

indep PNN MTNN  MTNN  NTPT

  top
left

   
   
bottom    
right
   

 
 
 
 
 

  top
left

   
   
bottom    
right
   

 
 
 
 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

Figure   Final accuracies on all       tasks for all models
at the end of lifelong learning

Figure   Generalization behavior on MATH expressions 
Solid dots indicate expression lengths used in training  We
show results on       simpler nonperceptual MATH task
 numbers in parentheses indicate parameter count in each
model  and     the MATH task including perception 

  Generalization

In the  nal experiment we take net   from the end of
the NTPT       training and start training on the MATH
scenario  For the NTPT model we train on arithmetic expressions containing only   digits  The known dif culty
in training differentiable interpreters with free loop structure  Gaunt et al    is revealed by the fact that only
  random restarts converge on   correct program in  
global optimum of the loss landscape  We detect convergence by   rapid increase in the accuracy on   validation
set  typically occurring after around    training examples 
Once the correct program is found  continuing to train the

model mainly leads to further improvement in the accuracy
of net   which saturates at   on the digit classi 
cation task  The learned source code provably generalizes
perfectly to expressions containing any number of digits 
and the only limitation on the performance on long expressions comes from the repeated application of the imperfect
net  
To pick   strong baseline for the MATH problem  we  rst
perform   preliminary experiment with two simpli cations 
  rather than expecting strong generalization from just  
digit training examples  we train candidate baselines with
supervision on examples of up to   digits and   operators  and   we remove the perceptual component of the
task  presenting the digits and operators as onehot vectors
rather than images  Fig      shows the generalization performance of the LSTM and Neural GPU  lter  baselines in this simpler setting after training to convergence 
Based on these results  we restrict attention to the LSTM
baseline and return to the full task including the perceptual
component  In the full MATH task  we initialize the embedding networks of each model using net   from the end
of the NTPT       training  Fig      shows generalization
of the NTPT and LSTM models on expressions of up to  
digits   symbols  after training to convergence  We  nd
that even though the LSTM shows surprisingly effective
generalization when supplied supervision for up to   digits 
NTPT trained on only  digit expressions still offers better
results 

  Related work
Lifelong Machine Learning  We operate in the
paradigm of Lifelong Machine Learning  LML   Thrun 
    Thrun     Sullivan    Silver et al   

 Note that  Price et al    also  nd poor generalization
performance for   Neural GPU applied to the similar task of evaluating arithmetic expressions involving binary numbers 

 training example    accuracy probability training example    ADD    top rowADD    left columnADD    bottom rowADD    right columnAPPLY    tasks training example    APPLY   leftaccuracyindep PNNMTNN MTNN NTPT ADD   leftaccuracy accuracy  digits in expressionLSTM    digitLSTM    digitNTPT    digit accuracy  Neural GPU    LSTM          TerpreT            Differentiable Programs with Neural Libraries

Chen et al    where   learner is presented   sequence
of different tasks and the aim is to retain and reuse knowledge from earlier tasks to more ef ciently and effectively
learn new tasks  This is distinct from related paradigms of
multitask learning  where   set of tasks is presented rather
than in sequence  Caruana    Kumar   Daume III 
  Luong et al    Rusu et al    transfer learning  transfer of knowledge from   source to target domain
without notion of knowledge retention  Pan   Yang   
and curriculum learning  training   single model for   single
task of varying dif culty  Bengio et al   
The challenge for LML with neural networks is the problem
of catastrophic forgetting  if the distribution of examples
changes during training  then neural networks are prone to
forget knowledge gathered from early examples  Solutions
to this problem involve instantiating   knowledge repository  KR  either directly storing data from earlier tasks or
storing  sub networks trained on the earlier tasks with their
weights frozen  This knowledge base allows either   rehearsal on historical examples  Robins      rehearsal
on virtual examples generated by the frozen networks  Silver   Mercer    Silver   Poirier    or   creation
of new networks containing frozen sub networks from the
historical tasks  Rusu et al    Shultz   Rivest   
To frame our approach in these terms  our KR contains
partiallytrained neural network classi ers which we call
from learned source code  Crucially  we never freeze the
weights of the networks in the KR  all parts of the KR can
be updated during the training of all tasks   this allows us to
improve performance on earlier tasks by continuing training
on later tasks  socalled reverse transfer  Reverse transfer
has been demonstrated previously in systems which assume
that each task can be solved by   model parameterized by an
 uninterpretable  taskspeci   linear combination of shared
basis weights  Ruvolo   Eaton    The representation
of taskspeci   knowledge as source code  learning from
weak supervision  and shared knowledge as   deep neural
networks distinguishes this work from the linear model used
in  Ruvolo   Eaton   
Neural Networks Learning Algorithms  Recently  extensions of neural networks with primitives such as memory
and discrete computation units have been studied to learn
algorithms from inputoutput data  Graves et al    Weston et al    Joulin   Mikolov    Grefenstette et al 
  Kurach et al    Kaiser   Sutskever    Reed
  de Freitas    Bunel et al    Andrychowicz  
Kurach    Zaremba et al    Graves et al   
Riedel et al    Gaunt et al    Feser et al   
  dominant trend in these works is to use   neural network
controller to managing differentiable computer architecture 
We  ip this relationship  and in our approach    differentiable interpreter acts as the controller that can make calls
to neural network components 

The methods above  with the exception of  Reed   de Freitas    and  Graves et al    operate on inputs of
 arrays of  integers  However   Reed   de Freitas   
requires extremely strong supervision  where the learner
is shown all intermediate steps to solving   problem 
our learner only observes inputoutput examples   Reed  
de Freitas    also show the performance of their system in   multitask setting  In some cases  additional tasks
harm performance of their model and they freeze parts of
their model when adding to their library of functions  Only
 Bunel et al     Riedel et al    and  Gaunt et al 
  aim to consume and produce source code that can
be provided by   human       as sketch of   solution  or
returned to   human  to potentially provide feedback 
  Discussion
We have presented NEURAL TERPRET    framework for
building endto end trainable models that structure their solution as   source code description of an algorithm which
may make calls into   library of neural functions  Experimental results show that these models can successfully be
trained in   lifelong learning context  and they are resistant
to catastrophic forgetting  in fact  they show that even after instances of earlier tasks are no longer presented to the
model  performance still continues to improve 
Our experiments concentrated on two key bene ts of the
hybrid representation of task solutions as source code and
neural networks  First  the source code structure imposes
modularity which can be seen as focusing the supervision 
If   component is not needed for   given task  then the differentiable interpreter can choose not to use it  which shuts off
any gradients from  owing to that component  We speculate
that this could be   reason for the models being resistant to
catastrophic forgetting  as the model either chooses to use
  classi er  or ignores it  which leaves the component unchanged  The second bene   is that learning programs imposes   bias that favors learning models that exhibit strong
generalization  Additionally  the source code representation
has the advantage of being interpretable by humans  allowing veri cation and incorporation of domain knowledge describing the shape of the problem through the source code
structure 
The primary limitation of this design is that it is known
that differentiable interpreters are dif cult to train on problems signi cantly more complex than those presented here
 Kurach et al    Neelakantan et al    Gaunt et al 
  However  if progress can be made on more robust
training of differentiable interpreters  perhaps extending
ideas in  Neelakantan et al    and  Feser et al   
then we believe there to be great promise in using hybrid
models to build large lifelong learning systems 

Differentiable Programs with Neural Libraries

References
Abadi  Mart    Agarwal  Ashish  Barham  Paul  Brevdo 
Eugene  Chen  Zhifeng  Citro  Craig  Corrado  Greg   
Davis  Andy  Dean  Jeffrey  Devin  Matthieu  Ghemawat 
Sanjay  Goodfellow  Ian  Harp  Andrew  Irving  Geoffrey  Isard  Michael  Jia  Yangqing  Jozefowicz  Rafal 
Kaiser  Lukasz  Kudlur  Manjunath  Levenberg  Josh 
Man    Dan  Monga  Rajat  Moore  Sherry  Murray  Derek 
Olah  Chris  Schuster  Mike  Shlens  Jonathon  Steiner 
Benoit  Sutskever  Ilya  Talwar  Kunal  Tucker  Paul  Vanhoucke  Vincent  Vasudevan  Vijay  Vi egas  Fernanda 
Vinyals  Oriol  Warden  Pete  Wattenberg  Martin  Wicke 
Martin  Yu  Yuan  and Zheng  Xiaoqiang  TensorFlow 
Largescale machine learning on heterogeneous systems 
  URL http tensorflow org  Software
available from tensor ow org 

Andrychowicz  Marcin and Kurach  Karol  Learning ef 
 cient algorithms with hierarchical attentive memory 
arXiv preprint arXiv   

Bengio  Yoshua  Louradour    er ome  Collobert  Ronan  and
Weston  Jason  Curriculum learning  In Proceedings of
the  th Annual International Conference on Machine
Learning  ICML  pp     

Bunel  Rudy  Desmaison  Alban  Kohli  Pushmeet  Torr 
Philip       and Kumar     Pawan  Adaptive neural compilation  CoRR  abs    URL
http arxiv org abs 

Caruana  Rich  Multitask learning  Machine Learning   

   

Chen  Zhiyuan  Ma  Nianzu  and Liu  Bing  Lifelong learnIn Proceedings of the
ing for sentiment classi cation 
 rd Annual Meeting of the Association for Computational Linguistics  ACL  pp     

Feser  John    Brockschmidt  Marc  Gaunt  Alexander   
and Tarlow  Daniel  Neural functional programming 
  Submitted to ICLR  

Gaunt  Alexander    Brockschmidt  Marc  Singh  Rishabh 
Kushman  Nate  Kohli  Pushmeet  Taylor  Jonathan  and
Tarlow  Daniel  Terpret    probabilistic programming
language for program induction  CoRR  abs 
 
URL http arxiv org abs 
 

Graves  Alex  Wayne  Greg  and Danihelka  Ivo  Neural
turing machines  CoRR  abs    URL
http arxiv org abs 

Graves  Alex  Wayne  Greg  Reynolds  Malcolm  Harley 
Tim  Danihelka  Ivo  GrabskaBarwi nska  Agnieszka 

Colmenarejo  Sergio   omez  Grefenstette  Edward  Ramalho  Tiago  Agapiou  John  et al  Hybrid computing
using   neural network with dynamic external memory 
Nature   

Grefenstette  Edward  Hermann  Karl Moritz  Suleyman 
Mustafa  and Blunsom  Phil  Learning to transduce with
In Proceedings of the  th Conunbounded memory 
ference on Advances in Neural Information Processing
Systems  NIPS  pp     

Joulin  Armand and Mikolov  Tomas 

Inferring algorithmic patterns with stackaugmented recurrent nets 
In
Advances in Neural Information Processing Systems  
 NIPS Conference  Denver  Colorado  USA  November
    pp     

Kaiser   ukasz and Sutskever  Ilya  Neural GPUs learn algorithms  In Proceedings of the  th International Conference on Learning Representations  ICLR    URL
http arxiv org abs 

Kumar  Abhishek and Daume III  Hal  Learning task grouping and overlap in multitask learning  arXiv preprint
arXiv   

Kurach  Karol  Andrychowicz  Marcin  and Sutskever  Ilya 
Neural randomaccess machines  In Proceedings of the
 th International Conference on Learning Representations     URL http arxiv org abs 
 

Luong  MinhThang  Le  Quoc    Sutskever  Ilya  Vinyals 
Oriol  and Kaiser  Lukasz  Multitask sequence to sequence learning  In International Conference on Learning Representations  ICLR   

McCloskey  Michael and Cohen  Neal    Catastrophic interference in connectionist networks  The sequential learning problem  Psychology of learning and motivation   
   

Neelakantan  Arvind  Le  Quoc    and Sutskever  Ilya  Neural programmer  Inducing latent programs with gradient
descent  In Proceedings of the  th International Conference on Learning Representations    

Pan  Sinno Jialin and Yang  Qiang    survey on transfer
learning  IEEE Transactions on knowledge and data engineering     

Price  Eric  Zaremba  Wojciech  and Sutskever  Ilya  Extensions and limitations of the neural GPU    Submitted
to ICLR  

Ratcliff  Roger  Connectionist models of recognition memory  constraints imposed by learning and forgetting functions  Psychological review     

Differentiable Programs with Neural Libraries

Weston  Jason  Chopra  Sumit  and Bordes  Antoine  Memory networks  In Proceedings of the  rd International
Conference on Learning Representations    
URL http arxiv org abs 

Zaremba  Wojciech  Mikolov  Tomas  Joulin  Armand  and
Fergus  Rob  Learning simple algorithms from examples 
In Proceedings of the  nd International Conference on
Machine Learning  ICML   pp     

Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht  Benjamin  and Vinyals  Oriol  Understanding deep learning
requires rethinking generalization  In International Conference on Learning Representations   

Reed  Scott    and de Freitas  Nando  Neural programmer 

interpreters   

Riedel  Sebastian  Bosnjak  Matko  and Rockt aschel  Tim 
Programming with   differentiable forth interpreter 
CoRR  abs    URL http arxiv 
org abs 

Robins  Anthony  Catastrophic forgetting  rehearsal and
pseudorehearsal  Connection Science   
 

Rusu  Andrei    Rabinowitz  Neil    Desjardins  Guillaume 
Soyer  Hubert  Kirkpatrick  James  Kavukcuoglu  Koray 
Pascanu  Razvan  and Hadsell  Raia  Progressive neural
networks  arXiv preprint arXiv   

Ruvolo  Paul and Eaton  Eric  Ella  An ef cient lifelong

learning algorithm  ICML      

Shultz  Thomas   and Rivest  Francois  Knowledgebased
cascadecorrelation  Using knowledge to speed learning 
Connection Science     

Silver  Daniel   and Mercer  Robert    The task rehearsal
method of lifelong learning  Overcoming impoverished
data  In Conference of the Canadian Society for Computational Studies of Intelligence  pp    Springer 
 

Silver  Daniel   and Poirier  Ryan  Machine lifelong learn 

ing with csmtl networks  In AAAI   

Silver  Daniel    Yang  Qiang  and Li  Lianghao  Lifelong
machine learning systems  Beyond learning algorithms 
In AAAI Spring Symposium  Lifelong Machine Learning 
pp     

Stallkamp  Johannes  Schlipsing  Marc  Salmen  Jan  and
Igel  Christian 
The German Traf   Sign Recognition Benchmark    multiclass classi cation competition 
In IEEE International Joint Conference on Neural Networks  pp     

Thrun  Sebastian    lifelong learning perspective for mobile
robot control  In Proceedings of IEEE RSJ International
Conference on Intelligent Robots and Systems  IROS  pp 
   

Thrun  Sebastian  Is learning the nth thing any easier than
learning the  rst  In Advances in Neural Information
Processing Systems    NIPS  pp     

Thrun  Sebastian and   Sullivan  Joseph  Discovering structure in multiple learning tasks  The TC algorithm 
In
Machine Learning  Proceedings of the Thirteenth International Conference  ICML  pp     

