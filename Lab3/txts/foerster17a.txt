Input Switched Af ne Networks  An RNN Architecture Designed for

Interpretability

Jakob    Foerster     Justin Gilmer     Jascha SohlDickstein   Jan Chorowski   David Sussillo  

Abstract

There exist many problem domains where the
interpretability of neural network models is essential for deployment  Here we introduce   recurrent architecture composed of inputswitched
af ne transformations   in other words an RNN
without any explicit nonlinearities  but with inputdependent recurrent weights  This simple form
allows the RNN to be analyzed via straightforward linear methods  we can exactly characterize
the linear contribution of each input to the model
predictions  we can use   changeof basis to disentangle input  output  and computational hidden unit subspaces  we can fully reverseengineer
the architecture   solution to   simple task  Despite this ease of interpretation  the input switched
af ne network achieves reasonable performance
on   text modeling tasks  and allows greater computational ef ciency than networks with standard
nonlinearities 

  Introduction
  The importance of interpretable machine learning

As neural networks move into applications where the outcomes of human lives depend on their decisions  it is increasingly crucial that we are able to interpret the decisions
they make  Indeed  the European Union is considering legislation with   clause that asserts that individuals have  rights
to explanation      
individuals should be able to understand how algorithms make decisions about them  Council
of European Union    Example problem domains re 

 Equal contribution  This work was performed as an intern
at Google Brain  Work done as   member of the Google Brain
Residency program    co brainresidency   Google Brain 
Mountain View  CA  USA  Work performed when author was
  visiting faculty at Google Brain  Correspondence to  Jakob
   Foerster  jakob foerster cs ox ac uk  David Sussillo  sussillo google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

quiring interpretable ML include selfdriving cars  Bojarski
et al    air traf   control  Katz et al    power
grid control  Siano et al    hiring and promotion decisions while preventing bias  Scarborough   Somers   
automated sentencing decisions in US courts  Tashea   
Berk et al    and medical diagnosis  Gulshan et al 
  For many of these applications  practitioners will not
adopt ML models without fully understanding what drives
their predictions  including understanding when and how
these models fail  Ching et al    Deo   

  Post hoc analysis

One approach to interpreting neural networks is to train
the network as normal  and then apply analysis techniques
after training  Often this approach yields systems that perform extremely well  but where interpretability is challenging  For example  Sussillo   Barak   used linearization and nonlinear dynamical systems theory to understand
RNNs solving   set of simple but varied tasks  Karpathy
et al    analyzed an LSTM  Hochreiter   Schmidhuber    trained on   characterbased language modeling
task  They were able to break down LSTM language model
errors into classes  such as       rare word  errors  Concurrently with our submission  Murdoch   Szlam  
decomposed LSTM outputs using telescoping sums of statistics computed from memory cells at different RNN steps 
The decomposition is exact  but not unique and the authors
justify it by demonstrating good performance of decision
rules formed using the computed cell statistics 
The community is also interested in post hoc interpretation
of feedforward networks  Examples include the use of
linear probes in Alain   Bengio   and   variety of
techniques  most driven by backpropagation  to assign
credit for activations to speci   inputs or input patterns in
feedforward networks  Zeiler et al    Le et al   
Mordvintsev et al   

  Building interpretability into the architecture

  second approach is to build   neural network where interpretability is an explicit design constraint  In this approach 
  typical outcome is   system that can be better understood 
but at the cost of reduced performance  Model classes whose

Interpretable RNNs with Input Switched Af ne Networks

decisions are naturally interpretable include logistic regression  Freedman    decision trees  Quinlan    and
support vector machines with simple       linear  kernels
 Andrew   
In this work we follow this second approach and build interpretability into our network model  while maintaining
good  though not always stateof theart  performance for
the tasks we study  We focus on the commonly studied
task of character based language modeling  We develop
and analyze   model trained on   onestep ahead prediction task of the Text  dataset  which is   million characters of Wikipedia text  Mahoney    on the Billion
Word Benchmark  Chelba et al    and  nally on   toy
multiple parentheses counting task which we fully reverse
engineer 

  Switched af ne systems

The model we introduce is an Input Switched Af ne Network  ISAN  where the input determines the switching
behavior by selecting   transition matrix and bias as   function of that input  and there is no nonlinearity  Linear timevarying systems are standard material in undergraduate electrical engineering text books  and are closely related to our
technique 
Although the ISAN is deterministic  probabilistic versions of
switching linear models with discrete latent variables have
  history in the context of probabilistic graphical models   
recent example is the switched linear dynamical system in
 Linderman et al    Focusing on language modeling 
 Belanger   Kakade    de ned   probabilistic linear
dynamical system  LDS  as   generative language model
for creating contextdependent token embeddings and then
used steadystate Kalman  ltering for inference over token
sequences  They used singular value decomposition and
discovered that the right and left singular vectors were semantically and syntactically related    critical difference
between the ISAN and the LDS is that the ISAN weight
matrices are input token dependent  while the biases of both
models are input dependent 
Multiplicative neural networks  MRNNs  were proposed
precisely for character based language modeling in
 Sutskever et al    Martens   Sutskever    The
MRNN architecture is similar to our own  in that the dynamics matrix switches as   function of the input character 
However  the MRNN relied on   tanh nonlinearity  while
the ISAN is explicitly linear  It is this property of our model
which makes it both amenable to analysis  and computationally ef cient 
The Observable Operator Model  OOM   Jaeger    is
similar to the ISAN in that the OOM updates   latent state
using   separate transition matrix for each input symbol

and performs probabilistic sequence modeling  Unlike the
ISAN  the OOM requires that   linear projection of the hidden state corresponds to   normalized sequence probability 
This imposes strong constraints on both the model parameters and the model dynamics  and restricts the choice of
training algorithms  In contrast  the ISAN applies an af ne
readout to the hidden state to obtain logits  which are then
pushed through the softmax function to obtain probabilities 
Therefore no constraints need to be imposed on the ISAN  
parameters and training is easy using backprop  Lastly  the
ISAN is formulated as an af ne  rather than linear model 
While this doesn   change the class of processes that can be
modeled  it stabilizes training and greatly enhances interpretability  facilitating the analysis in Section  

  Paper structure

In what follows  we de ne the ISAN architecture  demonstrate its performance on the onestep ahead prediction task 
and then analyze the model in   multitude of ways  most of
which would be currently dif cult or impossible to accomplish with modern nonlinear recurrent architectures 

  Methods
  Model de nition

In what follows Wx and bx respectively denote   transition
matrix and   bias vector for   speci   input    the symbol
xt is the input at time    and ht is the hidden state at time   
Our ISAN model is de ned as

ht   Wxt ht    bxt 

 

The network also learns an initial hidden state    We emphasize the intentional absence of any nonlinear activation
function 

  Character level language modeling with ISAN

We trained RNNs on the Text  Wikipedia dataset and the billion word benchmark  BWB  for onestep ahead character
prediction  The Text  dataset consists only of the   characters       and    space  The BWB dataset consist of
Unicode text and was modelled as   sequence of bytes  
discrete tokens  that formed the UTF encoded data  Given
  character sequence of      xt  the RNNs are trained to
minimize the crossentropy between the true next character 
and the output prediction  We map from the hidden state 
ht  into   logit space via an af ne map  The probabilities
are computed as

   xt    softmax  lt 

lt   Wro ht   bro 

 
 

where Wro and bro are the readout weights and biases 
and lt is the logit vector  For the Text  dataset  we split

Interpretable RNNs with Input Switched Af ne Networks

Table   The ISAN has similar performance to other RNN architectures on the Text  dataset  Performance of RNN architectures
on Text  onestep ahead prediction  measured as crossentropy
loss on   heldout test set  in bits per character  The loss is shown
as   function of the maximum number of parameters   model is
allowed  The values reported for all other architectures are taken
from  Collins et al   

Parameter count

RNN
IRNN
GRU
LSTM
ISAN

   

 
 
 
 
 

   

   

 
 
 
 
 

 
 
 
 
 

the data into     and   for train  validation  and
test respectively  in line with  Mikolov et al    The
network was trained with the same hyperparameter tuning
infrastructure as in  Collins et al    For the BWB
dataset  we used data splits and evaluation setup identical to
   zefowicz et al    Due to long experiment running
times  we manually tuned the hyperparameters 

  Results and analysis
  ISAN performance on Text  prediction

The results on Text  are shown in Table   For the largest
parameter count  the ISAN matches almost exactly the performance of all other nonlinear models with the same number of maximum parameters  RNN  IRNN  GRU  LSTM 
However  we note that for small numbers of parameters the
ISAN performs considerably worse than other architectures 
All analyses use ISAN trained with     maximum parameters   bpc cross entropy  Samples of generated
text from this model are relatively coherent  We show two
examples  after priming with  annual reve  at inverse temperature of   and   respectively 

   annual revenue and producer of the telecommunications and former communist action and saving its new
state house of replicas and many practical persons 
   annual revenue seven  ve three million one nine nine
eight the rest of the country in the united states and
south africa new 

As   preliminary  comparative analysis  we performed PCA
on the state sequence over   large set of sequences for the
vanilla RNN  GRU of varying sizes  and ISAN  This is
shown in Figure   The eigenvalue spectra  in log of variance
explained  was signi cantly  atter for the ISAN than the
other architectures 
We compared the ISAN performance to   fully linear RNN

Figure   The ISAN makes fuller and more uniform use of its latent
space than vanilla RNNs or GRUs  Figure shows explained variance ratio of the  rst   most signi cant PCA dimensions of the
hidden states across several architectures for the Text  dataset  The
legend provides the number of latent units for each architecture 

without input switched dynamics  This achieves   crossentropy of   bits   char  independent of network size  This
perplexity is only slightly better than that of   Naive Bayes
model on the task  at   bits   char  The output probability
of the fully linear network is   product of contributions from
each previous character  as in Naive Bayes  Those factorial
contributions are learned however  giving the nonswitched
af ne network   slight advantage  We also trained   fully
linear network with   nonlinear readout  This achieves  
bits   char  independent of network size  Both of these
comparisons illustrate the importance of the input switched
dynamics for achieving good results 
Lastly we also test to what extent the ISAN can deal with
large dictionaries by running it on   bytepair encoding of
the text  task  where the input dictionary consists of the  
different possible character combinations  We  nd that in
this setup the LSTM consistently outperforms the ISAN for
the same number of parameters  At    parameters the
LSTM achieves   cross entropy of   bits   charpair  while
ISAN achieves   One explanation for this  nding is that
the matrices in ISAN are   times smaller than the matrices
of the LSTMs  For very large numbers of parameters the
performance of any architecture saturates in the number of
parameters  at which point the ISAN can  catchup  with
more parameter ef cient architectures like LSTMs 

  ISAN performance on Billion Word Benchmark

prediction

We trained ISAN and LSTM models on the BWB dataset 
All networks were trained using asynchronous gradient descent using the Adagrad learning rule  Our best LSTM
model reached   bits per character  which matches published results  Hwang   Sung    The LSTM model
had one layer of   LSTM units whose outputs were
projected onto   dimensions     parameters  Our
best ISAN models reached   bits per character and used

Interpretable RNNs with Input Switched Af ne Networks

   from previous
Figure   Using the linearity of the hidden state dynamics  predictions at step   can be broken out into contributions    
steps  Accordingly  each row of the top panel corresponds to the propagated contribution   
   of the input character at time    to the
prediction at time    summed to create the logit at time    The penultimate row contains the output bias vector replicated at every time
step  The last row contains the logits of the predicted next character  which is the sum of all rows above  The bottom panel contains the
corresponding softmax probabilities at each time   for all characters  time is separated by gray lines  Labeled is the character with the
maximum predicted probability  The time step boxed in red is examined in more detail in Figure  

  hidden units    reduced set of most common   input
tokens and   output tokens     parameters  Increasing
ISAN   hidden layer size to   units     parameters 
yielded   perplexity improvement to   bits char  Investigation of generated samples shows that the ISAN learned
the distinction between lowerand uppercased letters and is
able to generate text which is coherent over short segments 
To demonstrate sample variability we show continuations
of the prompt  The  Pp ol  generated using the ISAN 

  The Pol ish pilgrims are as angry over the holiday trip
  The Pol ice Department subsequently slipped toward
  The Pol ice Federation has sought Helix also investors
  The Pol itico is in   tight crowd ever to moderated the
  The pol itical scientist in the Red Shirt Romance cannot
  The pol icy for all Balanchine had formed when it set  
  The pol   conducted when   suspected among Hispanic
  The pol itical frenzy sparked primary care programs

  Decomposition of current predictions based on

previous time steps

Analysis in this paper is carried out on the bestperforming
Text  ISAN model  which has       parameters  corresponding to   hidden units  and   dynamics matrices
Wx and biases bx 
With ISAN we can analyze which factors were important
in the past for determining the current character prediction 
Taking advantage of the linearity of the hidden state dynamics for any sequence of inputs  we decompose the current
latent state ht into contributions originating from different

time points   in the history of the input 

 cid    cid 

  cid 

  

  cid   

 cid 

ht  

Wxs cid 

bxs  

 

where the empty product when           is   by convention 
and bx       is the learned initial hidden state 
Using this decomposition and the fact that matrix multiplication is   linear transformation we can also write the
unnormalized logitvector  lt  as   sum of terms linear in the
biases 

  cid 
 cid    cid 

  

  
 

  cid   

lt   bro  

  

    Wro

 cid 

Wxs cid 

bxs 

 

 

  is the contribution from time step   to the logits at
where   
time step    and   
    bxt  For notational convenience we
will sometimes replace the subscript   with the corresponding input character xs at step   when referring to   
   For
example    
    refers to the contribution from the character
    in   string  Similarly  when discussing the summed contributions from   word or substring we will sometimes write
  from
word to mean the summed contributions of all the   
  
 the  refers

that source word  For example cid 

to the total contribution from the word  the  to the logit 
While in standard RNNs the nonlinearity causes interdependence of the bias terms across time steps  in the ISAN
the bias terms contribute to the state as independent linear

  word   

      

Interpretable RNNs with Input Switched Af ne Networks

Figure   Detailed view of the prediction stack for the  nal     in
 annual revenue  In    all   
  are shown  in    only the contributions to the     logit and     logits are shown  in orange and red
respectively  from each earlier character in the string  This corresponds to   zoom in view of the columns highlighted in orange and
red in    In    we show how the sum of the contributions from the
string  annual    
 annual  pushes the prediction at  annual reve 
from     to     Without this contribution the model decodes based
 reve  leading to   MAP prediction of  reverse  With
only on   
 annual  it instead predicts  revenue  The
the contribution from   
contribution of   
 annual  to the     and     logits is linear and exact 

terms that are propagated and transformed through time  We
emphasize that   
  includes the multiplicative contributions
from the Wxs cid  for       cid       It is however independent
of prior inputs  xs cid  for   cid       This is the main difference
between the analysis we can carry out with the ISAN compared to   nonlinear RNN  In   general recurrent network
the contribution of   speci   character sequence will depend
on the hidden state at the start of the sequence  Due to the
linearity of the dynamics  this dependency does not exist in
the ISAN 
In Figure   we show an example of how this decomposition allows us to understand why   particular prediction is
made at   given point in time  and how previous characters
in uence the decoding  For example  the sequence  annual revenue  is processed by the ISAN  Starting with an
allzero hidden state  we use equation   to accumulate
  sequence of   
   cid    We then used these
values to understand the prediction of the network at some
time    by simple addition across the   index 
We provide   detailed view of how past characters contribute
to the logits predicting the next character in Figure   There
are two competing options for the next letter in the word
stem  reve  either  revenue  or  reverse  We show that
without the contributions from  annual  the most likely
decoding of the character after the second     is      to form
 reverse  while the contributions from  annual  tip the
balance in favor of     decoding to  revenue 
Using ISAN  we can investigate information timescales in
the network  For example  we investigated how quickly the
  decay as   function of       on average 
contributions of   
Figure    shows that this contribution decays on two different exponential timescales  We hypothesize that the  rst
time scale corresponds to the decay within   word  while the

   cid    

   cid    

 cid    

 

 cid cid cid cid 

Figure   The time decay of the contributions from each character to prediction     Average norm of   
  across training text 

 cid  plotted as   function of        and averaged across

  cid cid cid cid cid  

all source characters  The norm appears to decay exponentially at
two rates    faster rate for the  rst ten or so characters  and then
  slower rate for more long term contributions     The median
cross entropy as   function of the position in the word under three
different circumstances  the red line uses all of the   
   baseline 
the green line sets all   
  to zero  while the blue line
only sets   
  to zero  The results from panel   demonstrate the
disproportionately large importance of   in decoding  especially
at the onset of   word     The crossentropy as   function of history
when arti cially limiting the number of characters available for
prediction  This corresponds to only considering the most recent
  of the   where   is the length of the history 

  apart from   

next corresponds to the decay of information across words
and sentences  We also show the relevance of the   
  contributions to the decoding of characters at different positions
in the word  Figure     For example  we observe that   
 
makes important contributions to the prediction of the next
character at time    We show that using only the   
  the
model can achieve   cross entropy of less than   bit   char
when the position of the character is more than   letters from
the beginning of the word  Finally  we link the normdecay
of   
  to the importance of past characters for the decoding
quality   Figure     By arti cially limiting the number of
past   available for prediction we show that the prediction
quality improves rapidly when extending the history from  
to   characters and then saturates  This rapid improvement
aligns with the range of faster decay in Figure    

  From characters to words

The ISAN provides   natural means of moving from character level representation to word level  Using the linearity
of the hidden state dynamics we can aggregate all of the
  belonging to   given word and visualize them as   sin 
  
gle contribution to the prediction of the letters in the next
word  This allows us to understand how each preceding
word impacts the decoding for the letters of later words  In
Figure   we show that the words  was  and  higher  make
large contributions to the prediction of the characters in
 than  as measured by the norm of the   
 higher 

 was  and   

Interpretable RNNs with Input Switched Af ne Networks

Figure   By transforming the ISAN dynamics into   new basis  we
can better understand the action of the inputdependent biases    
We observe   strong correlation between the norms of the input dependent biases  bx  and the logprobability of the unigram   in the
training data  We can begin to understand this correlation structure
using   basis transform into the  readout basis  Breaking out the
norm into its components in Pro cid  and Pro  in    and    respectively 
shows that the correlation is due to the component orthogonal to
Wro  This implies   connection between information or  surprise 
and distance in the  computational  subspace of state space 

word  for each word  Shown is the norm of   

     

Figure   The ISAN architecture can be used to precisely characterize the relationship between words and characters  The top
panel shows how exploiting the linearity of the network   operation
sn in   word to   single contribution 
we can combine the   
word    measure of
  
the magnitude of the effect of the previous word on the selection
of the current character  red corresponds to   norm of   blue
to   The bottom panel shows the probabilities assigned by the
network to the next sequence character  Lighter lines show predictions conditioned on   decreasing number of preceding words 
For example  when predicting the characters of  than  there is  
large contribution from both   
 higher  as shown in the
top pane  The effect on the log probabilities can be seen in the
bottom panel as the model becomes less con dent when excluding
  
 was 
and   
 higher  This word based representation clearly shows that the
system leverages contextual information across multiple words 

 was  and signi cantly less con dent when excluding both   

 was  and   

  Change of basis

We are free to perform   change of basis on the hidden state 
and then to run the af ne ISAN dynamics in that new basis 
Note that this change of basis is not possible for other RNN
architectures  since the action of the nonlinearity depends
on the choice of basis 
In particular we can construct    readout basis  that explicitly
divides the latent space into   subspace Pro cid  spanned by
the rows of the readout matrix Wro  and its orthogonal
complement Pro    This representation explicitly divides
the hidden state dynamics into    dimensional  readout 
subspace that is accessed by the readout matrix to make
predictions  and    computational  subspace comprising the
remaining       dimensions that are orthogonal to the
readout matrix 
We apply this change of basis to analyze an intriguing observation about the hidden offsets bx  As shown in Figure   the norm of the bx is strongly correlated to the
logprobability of the unigram   in the training data  Reexpressing network parameters using the  readout basis 
shows that this correlation is not related to reading out the
nextstep prediction  This is because the norm of the projection of bx into Pro  remains strongly correlated with
character frequency  while the projection into Pro cid  shows
little correlation  This indicates that the information content
or  surprise  of   letter is encoded through the norm of the

Figure   By transforming ISAN dynamics into   new basis  we
can better interpret structure in the inputdependent biases  In    we
show the cosine distance between the input dependent bias vectors 
split between vowels and consonants     is  rst  In    we show
the correlation only considering the components in the subspace
Pro cid  spanned by the rows of the readout matrix Wro     shows the
correlation of the components in the orthogonal complement Pro   
In all plots white corresponds to    aligned  and black to  

component of bx in the computational space  rather than in
the readout space 
Similarly  in Figure   we illustrate that the structure in the
correlations between the biases bx  across all    is due to
their components in Pro cid    while the correlation in Pro  is
relatively uniform  We can clearly see two blocks of high
correlations between the vowels and consonants respectively 
while    is uncorrelated to either 

  Comparison with ngram model with backoff

We compared the computation performed by ngram language models and those performed by the ISAN  An ngram
model with backoff weights expresses the conditional probability    xt   xt  as   sum of smoothed count ratios of
ngrams of different lengths  with the contribution of shorter
ngrams downweighted by backoff weights  On the other
hand  the computations performed by the ISAN start with
the contribution of bro to the logits  which as shown in Fig 

Interpretable RNNs with Input Switched Af ne Networks

ure     corresponds to the unigram logprobabilities  The
logits are then additively updated with contributions from
longer ngrams  represented by   
   This additive contribution to the logits corresponds to   multiplicative modi cation of the emission probabilities from histories of different
length  For long time lags  the additive correction to logprobabilities becomes small  Figure   which corresponds
to multiplication by   uniform distribution  Despite these
differences in how ngram history is incorporated  we nevertheless observe an agreement between empirical models
estimated on the training set and model predictions for unigrams and bigrams  Figure   shows that the bias term bro
gives the unigram probabilities of letters  while the addition
of the offset terms bx accurately predict the bigram distribution of    xt xt  Shown in panel   is an example 
     cid  and in panel      summary plot for all   letters 
We further explore the ngram comparison by arti cially
limiting the length of the character history that is available
to the ISAN for making predictions  as shown in Figure    

  Analyses of   parentheses counting task
To show the possibility of complete interpretability of the
ISAN we train   model on   parenthesis counting task 
Bringing together ideas from section   we reexpress the
transition dynamics in   new basis that fully reveals performed computations 
We analyze the task of counting the nesting levels of multiple parentheses types    simpli ed version of   task de ned
in  Collins et al    Brie       unit ISAN is required
to keep track of the nesting level of   different types of
parentheses independently  The inputs are the onehot encoding of the different opening and closing parentheses      
        as well as   noise character     The output
is the onehot encoding of the nesting level between  
one set of counts for each parenthesis type  so the complete
output vector is     dimensional  hot vector  Furthermore  the target output is the nesting level at the previous
time step  This arti cial delay requires the model to develop
  memory  One change from  Collins et al    is that
we exchange the crossentropy error with an    error  This
leads to slightly cleaner  gures  but does not qualitatively
change the results 
To elucidate the mechanism of ISAN   operation we  rst reexpress the af ne transitions ht    Wht     by their linear equivalents   cid 
   where   cid              
and   cid 
     ht    Next  we used linear regression to  nd  
change of basis for which all augmented character matrices
and the hidden states are sparse  To do this we construct
the  readout   Pro cid    and  computational    Pro    subspace
decomposition as discussed in Section   We choose  
basis for Pro  which makes the projections of the hidden

       cid   cid 

Figure   The predictions of ISAN for one and two characters well
approximate the predictions of unigram and bigram models  In   
we compare softmax bro  to the empirical unigram distribution
      In    we compare softmax Wrob    bro  with the empirical distribution    xt  In    we show the correlation of
softmax Wrobx   bro  with    xt xt  for all   characters
 yaxis  and compare this to the correlation between the empirical unigram probabilities       to    xt xt   xaxis  The plot
shows that the readout of the bias vector is   better predictor of the
conditional distribution than the unigram probability 

states into this computational subspace  hot vectors  With
this subspace decomposition  the hidden states and character
matrices have the form

    cid 

   

  cid 

   

  br
  Wrc
 
  bc
  Wcc
 
       

 
hc
 
 

 
 hr
   

 

 

Wcr

 Wrr
 Wrr

Wcr

   

and the update equation can be written as

  cid 
       cid 

xh cid 

  hr
  hr

    Wrc
    Wcc

  hc
  hc

    br
 
    bc
 

 

    Wcc

    Wrc

    Wcr

  and hc

  denote the readout and computational porHere hr
tions of ht  and Wrr
  denote the readout
to readout  readout to computation  computation to readout 
and computation to computation blocks of the character
matrix for character    respectively 
In Figure    we show the hidden states in the rotated basis as
  sequence of column vectors  The   dimensional hidden
states are all  hot  We can treat them as   concatenation of  
  and   computation hc
readout hr
  part  The  dimensional
readout hr
  corresponds to network   output at time step
  and encodes the counts from time step       as    hot
vector  one count per parenthesis type  The computational
  is           dimensional  and encodes the
space hc
current counts as another  hot vector  Note that in this
basis the ISAN effectively uses only   dimensions and the
remaining   dimensions have no noticeable effect on the
computation  In Figure    we show   cid 
  in the rotated basis 
We see from the leftmost   columns that Wrr
  are
  has no in uence on ht 
both nearly   This means that hr
Furthermore  the computation to readout block  Wrc
  is
 

  and Wcr

Interpretable RNNs with Input Switched Af ne Networks

    hc

identity on the  rst   dimensions  effectively implementing
   The current counts are
the lagging output hr
implemented as delay lines and identity submatrices in
  which respectively has the effect of incrementing the
Wcc
 
count of   by one  saturating at   and leaving the count of
  parentheses  xed  The matrices          behave
analogously  It is clear that this solution is general  in that
retraining for increased numbers of parentheses types or an
increased counting maximum  would have the analogous
solution 

  Discussion
In this paper we motivated an inputswitched af ne recurrent
network for the purpose of interpretability  We showed that  
switched af ne architecture achieves the same performance
as LSTMs on the Text  dataset for the same number of
maximum parameters  and reasonable performance on the
BWB  We performed   series of analyses  demonstrating
the ability to understand how inputs at one point in the input
sequence affect the outputs later in the output sequence  We
showed further in the multiple parentheses counting task that
the ISAN dynamics can be completely reverse engineered 
In summary  this work provides evidence that the ISAN is
able to express complex dynamical systems  yet its operation
can in principle be fully understood    prospect that remains
out of reach for many popular recurrent architectures 

  Computational bene ts

 cid cid   

 cid 

 

Switched af ne networks hold the potential to be massively
more computationally and memory ef cient for text processing than other recurrent architectures  First  inputdependent
af ne transitions reduce the number of parameters used at
every step  For   possible inputs and   parameters  the

computational cost per update step is   cid   

 cid    factor of

 

    factor of    

  speedup over nonswitched architectures  Similarly  the
number of hidden units is  
  memory improvement for storage of the latent state 
Furthermore  the ISAN is unique in its ability to precompute af ne transformations corresponding to input
strings  This is possible because the composition of af ne
transformations is also an af ne transformation  This property is used in Section   to evaluate the linear contributions
of words  rather than characters  This means that the hidden
state update corresponding to an entire input sequence can
be computed with identical cost to the update for   single
character  plus the dictionary lookup cost for the composed transformation  ISAN can therefore achieve very
large speedups on input processing  at the cost of increased
memory use  by accumulating large lookup tables of the
Wx and bx corresponding to common input sequences  Of
course  practical implementations will have to incorporate

Figure     visualization of the dynamics of an ISAN for the two
parentheses counting task with   time lag  count either   or  
nesting levels with   onestep readout delay  In    the weight
matrix for   is shown in the original basis  In    it is shown transformed to highlight the delayline dynamics  The activations of the
hidden units are shown    in the original basis  and    rotated to the
same basis as in    to highlight the delayline dynamics in   more
intelligible way  The white line delineates the transition matrix
elements and hidden state dimensions that directly contribute to
the output  All matrices for parentheses types appear similarly 
with closing parentheses         changing the direction of the
delay line 

complexities of memory management  batching  etc 

  Future work

There are some obvious future directions to this work  Currently  we de ne switching behavior using an input set with
 nite and manageable cardinality  Studying wordlevel language models with enormous vocabularies may require
some additional logic to scale  Another idea is to build
  language model that switches on bigrams or trigrams 
rather than characters or words  targeting an intermediate
number of af ne transformations  Adapting this model
to continuousvalued inputs is another important direction 
One approach is to use   tensor factorization similar to that
employed by the MRNN  Sutskever et al    or de ning
weights via additional networks  as in HyperNetworks  Ha
et al    Finally  we expect that automated methods
for changing bases to enable sparse representations in the
hidden state and dynamics matrices will be   particularly
fruitful direction to pursue 

    Transition matrix for   in original basis aaaaaa   Hidden dimensionHidden states in the original basis    Transition matrix for   in rotated basis aaaaaa   Hidden dimensionHidden states in rotated basisInterpretable RNNs with Input Switched Af ne Networks

Acknowledgements
We would like to thank Jasmine Collins for her help and
advice  and Quoc Le  David Ha and Mohammad Norouzi
for helpful discussions  We would also like to thank Herbert
Jaeger for insightful discussions regarding the ObservableOperator Model 

References
Alain  Guillaume and Bengio  Yoshua  Understanding intermediate layers using linear classi er probes  arXiv
preprint arXiv   

Andrew  Alex    An introduction to support vector machines and other kernelbased learning methods  Kybernetes   

Belanger  David and Kakade  Sham    linear dynamical
system model for text  In Proceedings of the  nd International Conference on Machine Learning  ICML 
pp     

Berk  Richard  Heidari  Hoda  Jabbari  Shahin  Kearns 
Michael  and Roth  Aaron  Fairness in criminal justice
risk assessments  The state of the art  arXiv preprint
arXiv   

Bojarski  Mariusz  Del Testa  Davide  Dworakowski  Daniel 
Firner  Bernhard  Flepp  Beat  Goyal  Prasoon  Jackel 
Lawrence    Monfort  Mathew  Muller  Urs  Zhang  Jiakai  et al  End to end learning for selfdriving cars  arXiv
preprint arXiv   

Chelba     Mikolov     Schuster     Ge     Brants    
Koehn     and Robinson     One Billion Word Benchmark for Measuring Progress in Statistical Language
Modeling  ArXiv eprints  December  

Ching  Travers  Himmelstein  Daniel    BeaulieuJones 
Brett    Kalinin  Alexandr    Do  Brian    Way  Gregory    Ferrero  Enrico  Agapow  PaulMichael  Xie  Wei 
Rosen  Gail    et al  Opportunities and obstacles for deep
learning in biology and medicine  bioRxiv  pp   
 

Collins  Jasmine  SohlDickstein  Jascha  and Sussillo 
David  Capacity and trainability in recurrent neural networks  ICLR   submission   

Council of European Union  General Data Protection
Regulation  Article    Regulation  EU   
  URL http www privacyregulation 
eu en htm 

Deo  Rahul    Machine learning in medicine  Circulation 

   

Freedman  David    Statistical models  theory and practice 

cambridge university press   

Gulshan  Varun  Peng  Lily  Coram  Marc  Stumpe  Martin    Wu  Derek  Narayanaswamy  Arunachalam  Venugopalan  Subhashini  Widner  Kasumi  Madams  Tom 
Cuadros  Jorge  et al  Development and validation of  
deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs  JAMA   
   

Ha  David  Dai  Andrew  and Le  Quoc    Hypernetworks 

arXiv preprint arXiv   

Hochreiter  Sepp and Schmidhuber    rgen  Long shortterm

memory  Neural computation     

Hwang  Kyuyeon and Sung  Wonyong  Characterlevel
language modeling with hierarchical recurrent neural
networks  CoRR  abs    URL http 
 arxiv org abs 

Jaeger  Herbert  Observable operator models for discrete
stochastic time series  Neural Computation   
   

  zefowicz  Rafal  Vinyals  Oriol  Schuster  Mike  Shazeer 
Noam  and Wu  Yonghui  Exploring the limits of language modeling  CoRR  abs    URL
http arxiv org abs 

Karpathy  Andrej  Johnson  Justin  and Li  FeiFei  Visualizing and understanding recurrent networks  arXiv preprint
arXiv   

Katz  Guy  Barrett  Clark  Dill  David  Julian  Kyle  and
Kochenderfer  Mykel  Reluplex  An ef cient smt solver
arXiv preprint
for verifying deep neural networks 
arXiv   

Le  Quoc    Ranzato  Marc    Monga  Rajat  Devin 
Matthieu  Chen  Kai  Corrado  Greg    Dean     and
Ng  Andrew    Building highlevel features using large
scale unsupervised learning  In International Conference
on Machine Learning   

Linderman  Scott    Miller  Andrew    Adams  Ryan   
Blei  David    Paninski  Liam  and Johnson  Matthew   
Recurrent switching linear dynamical systems  arXiv
preprint arXiv   

Mahoney  Matt  Large text compression benchmark  About
the test data    URL http mattmahoney 
net dc textdata   Online  accessed  November 
 

Martens  James and Sutskever  Ilya  Learning recurrent neural networks with hessianfree optimization  In Proceedings of the  th International Conference on Machine
Learning  ICML  pp     

Interpretable RNNs with Input Switched Af ne Networks

Mikolov  Tom  Sutskever  Ilya  Deoras  Anoop  Le  HaiSon  and Kombrink  Stefan  Subword language modeling
with neural networks  preprint   

Mordvintsev  Alexander  Olah  Christopher  and Tyka  Mike 
Inceptionism  Going deeper into neural networks  Google
Research Blog  Retrieved June     

Murdoch     James and Szlam  Arthur  Automatic rule
extraction from long short term memory networks  In
ICLR   

Quinlan     Ross  Simplifying decision trees  International

journal of manmachine studies     

Scarborough  David and Somers  Mark John  Neural networks in organizational research  Applying pattern recognition to the analysis of organizational behavior  American Psychological Association   

Siano  Pierluigi  Cecati  Carlo  Yu  Hao  and Kolbusz 
Janusz  Real time operation of smart grids via fcn networks and optimal power  ow  IEEE Transactions on
Industrial Informatics     

Sussillo  David and Barak  Omri  Opening the black box 
lowdimensional dynamics in highdimensional recurrent
neural networks  Neural computation   
 

Sutskever  Ilya  Martens  James  and Hinton  Geoffrey   
Generating text with recurrent neural networks  In Proceedings of the  th International Conference on Machine Learning  ICML  pp     

Sutskever  Ilya  Vinyals  Oriol  and Le  Quoc    Sequence
to sequence learning with neural networks  In Advances
in neural information processing systems  pp   
 

Tashea  Jason  Courts are using ai to sentence criminals 

that must stop now  WIRED magazine   

Zeiler  Matthew    Krishnan  Dilip  Taylor  Graham    and
Fergus  Rob  Deconvolutional networks  In Computer
Vision and Pattern Recognition  CVPR    IEEE Conference on  pp    IEEE   

