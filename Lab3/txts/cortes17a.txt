AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

Corinna Cortes   Xavier Gonzalvo   Vitaly Kuznetsov   Mehryar Mohri     Scott Yang  

Abstract

We present new algorithms for adaptively learning arti cial neural networks  Our algorithms
 ADANET  adaptively learn both the structure
of the network and its weights 
They are
based on   solid theoretical analysis  including
datadependent generalization guarantees that we
prove and discuss in detail  We report the results of largescale experiments with one of our
algorithms on several binary classi cation tasks
extracted from the CIFAR  dataset and on the
Criteo dataset  The results demonstrate that our
algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved by neural networks found by standard approaches 

  Introduction
Multilayer arti cial neural networks form   powerful
learning model which has helped achieve   remarkable performance in several applications in recent years  Representing the input through increasingly more abstract layers of feature representation has shown to be very effective in natural language processing  image captioning 
speech recognition and several other areas  Krizhevsky
et al    Sutskever et al    However  despite the
compelling arguments for adopting multilayer neural networks as   general template for tackling learning problems 
training these models and designing the right network for  
given task has raised several theoretical questions and faced
numerous practical challenges 
  critical step in learning   large multilayer neural network for   speci   task is the choice of its architecture 
which includes the number of layers and the number of
units within each layer  Standard training methods for neural networks return   model admitting precisely the number

 Google Research  New York  NY  USA   Courant Institute
of Mathematical Sciences  New York  NY  USA  Correspondence
to  Vitaly Kuznetsov  vitalyk google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

of layers and units speci ed since there needs to be at least
one path through the network for the hypothesis to be nontrivial  Single weights can be pruned  Han et al   
via   technique originally termed Optimal Brain Damage
 LeCun et al    but the global architecture remains
unchanged  Thus  this imposes   stringent lower bound on
the complexity of the model  which may not match that
of the learning task considered  complex networks trained
with insuf cient data may be prone to over tting and  in
reverse  simpler architectures may not suf ce to achieve an
adequate performance 
This places   considerable burden on the user who is left
with the requirement to specify an architecture with the
right complexity  which is often   dif cult task even with  
signi cant level of experience and domain knowledge  As  
result  the choice of the network is typically left to   hyperparameter search using   validation set  This search space
can quickly become exorbitantly large  Szegedy et al 
  He et al    and largescale hyperparameter tuning to  nd an effective network architecture often wasteful of data  time  and resources       grid search  random
search  Bergstra et al   
This paper seeks precisely to address some of these issues 
We present   theoretical analysis of the problem of learning simultaneously both the network architecture and its
parameters  To the best of our knowledge  our results are
the  rst generalization bounds for the problem of structural
learning of neural networks  These general guarantees can
guide the design of   variety of different algorithms for
learning in this setting  We describe in detail two such algorithms  ADANET algorithms  that directly bene   from
our theory 
Rather than enforcing   prespeci ed architecture and
thus    xed network complexity  our ADANET algorithms
adaptively learn the appropriate network architecture for
  learning task  Starting from   simple linear model  our
algorithms incrementally augment the network with more
units and additional layers  as needed  The choice of the
additional subnetworks depends on their complexity and is
directly guided by our learning guarantees  Remarkably 
the optimization problems for both of our algorithms turn
out to be strongly convex and thus guaranteed to admit  
unique global solution 

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

 cid 

sents   function denoted by hk    before composition with
an activation function  Let   denote the input space and
for any        let       Rn  denote the corresponding
feature vector  Then  the family of functions de ned by the
 rst layer functions              is the following 
   cid                Rn cid   cid      

 
where       de nes an lpnorm and       is   hyperparameter on the weights connecting layer   and layer   The
family of functions hk         nk  in   higher layer      
is then de ned as follows 

    

 cid 

 

Hk  

us        hs     

  

us   Rns cid us cid           hk     Hs

 cid 

 

 

 cid 

   cid    cid 

  cid 

nk cid 

where  for each unit function hk    us in   denotes the
vector of weights for connections from that unit to   lower
layer        The    ss are nonnegative hyperparameters
and      hs abusively denotes   coordinatewise composition       hs        hs               hs ns   The  ss
are assumed to be  Lipschitz activation functions  In particular  they can be chosen to be the Recti ed Linear Unit
function  ReLU function     cid  max     or the sigmoid
function    cid   
       The choice of the parameter      
determines the sparsity of the network and the complexity
of the hypothesis sets Hk 
For the networks we consider  the output unit can be connected to all intermediate units  which therefore de nes  
function   as follows 

   

wk jhk    

  

  

  

wk   hk 

 

where hk    hk          hk nk  cid  Hnk
and wk   Rnk is
the vector of connection weights to units of layer    Observe that  for us     for           and wk     for       
our architectures coincides with standard multilayer feedforward ones 
We will denote by   the family of functions   de ned by
  with the absolute value of the weights summing to one 

 cid 
 cid    cid 
Let  cid Hk denote the union of Hk and its re ection   cid Hk  
 cid Hk       cid  
    cid Hk  Then    coincides with the convex

Hk    Hk  and let   denote the union of the families

wk   hk   hk   Hnk
   

 cid wk cid     

  cid 

hull of        conv   
For any         we will also consider the family   
  derived from Hk by setting          for           which

   

  

  

 

  cid 

 

Figure   An example of   general network architecture  the output layer  in green  is connected to all of the hidden units as well
as some input units  Some hidden units  in red and yellow  are
connected not only to the units in the layer directly below  but
also to units at other levels 

The paper is organized as follows  In Appendix    we give
  detailed discussion of previous work related to this topic 
Section   describes the general network architecture and
therefore the hypothesis set that we consider  Section  
provides   formal description of our learning scenario  In
Section   we prove strong generalization guarantees for
learning in this setting  which help guide the design of the
algorithm described in Section   as well as   variant described in Appendix    We report the results of our experiments with ADANET in Section  

  Network architecture
In this section  we describe the general network architecture we consider for feedforward neural networks  thereby
also de ning our hypothesis set  To simplify the presentation  we restrict our attention to the case of binary classi 
cation  However  all our results can be straightforwardly
extended to multiclass classi cation  including the network architecture  by augmenting the number of output
units  and  our generalization guarantees  by using existing
multiclass counterparts of the binary classi cation ensemble margin bounds we use 
  common model for feedforward neural networks is the
multilayer architecture where units in each layer are only
connected to those in the layer below  We will consider
more general architectures where   unit can be connected
to units in any of the layers below  as illustrated by Figure   In particular  the output unit in our network architectures can be connected to any other unit  These more general architectures include as special cases standard multilayer networks  by zeroing out appropriate connections  as
well as somewhat more exotic ones  He et al    Huang
et al    In fact  our de nition covers any architecture
that can be represented as   directed acyclic graph  DAG 
More formally  the arti cial neural networks we consider
are de ned as follows  Let   denote the number of intermediate layers in the network and nk the maximum number of
units in layer         Each unit      nk  in layer   repre 

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

similarly de ne  cid   

      

       

   and        

corresponds to units connected only to the layer below  We
  
  
and de ne    as the convex hull      conv    Note
that the architecture corresponding to the family of functions    is still more general than standard feedforward
neural network architectures since the output unit can be
connected to units in different layers 

  

  Learning problem
We consider the standard supervised learning scenario and
assume that training and test points are drawn        according to some distribution   over         and denote
by                     xm  ym    training sample of size
  drawn according to Dm 
For   function   taking values in    we denote by        
        yf     its generalization error and  for any

      by  cid RS     its empirical margin error on the sample
    cid RS        

 cid  

    yif  xi 

 

The learning problem consists of using the training sample   to determine   function   de ned by   with small
generalization error       For an accurate predictor    we
expect many of the weights to be zero and the corresponding architecture to be quite sparse  with fewer than nk units
at layer   and relatively few nonzero connections  In that
sense  learning an accurate function   implies also learning
the underlying architecture 
In the next section  we present datadependent learning
bounds for this problem that will help guide the design of
our algorithms 

learning bounds are expressed in terms of

  Generalization bounds
Our
the
Rademacher complexities of the hypothesis sets Hk  The
empirical Rademacher complexity of   hypothesis set   for

  sample   is denoted by  cid RS    and de ned as follows 

 cid RS     

 cid 

  cid 

  

 
 

 
 

sup
   

 ih xi 

 

 cid 

where                   with  is independent uniformly
distributed random variables taking values in    
Its Rademacher complexity is de ned by Rm     

ES Dm cid RS    These are datadependent complexity

measures that lead to  ner learning guarantees  Koltchinskii   Panchenko    Bartlett   Mendelson   
As pointed out earlier  the family of functions   is the convex hull of    Thus  generalization bounds for ensemble
methods can be used to analyze learning with    In particular  we can leverage the recent marginbased learning guarantees of Cortes et al    which are  ner than those

that can be derived via   standard Rademacher complexity analysis  Koltchinskii   Panchenko    and which
admit an explicit dependency on the mixture weights wk
de ning the ensemble function    That leads to the following learning guarantee 
Theorem    Learning bound  Fix       Then  for any
      with probability at least       over the draw of  
sample   of size   from Dm  the following inequality holds

for all    cid  
         cid RS      

where             

 cid   

 cid  log  

 cid 

 

 

 

 cid  

   wk   hk     
  cid 
 cid cid wk
 cid cid   

 
 

  

 cid cid Rm cid Hk   
log    cid  log  

            

 cid 

 
 

log  
 

  log    

    log 

    

The proof of this result  as well as that of all other
main theorems are given in Appendix    The bound of
the theorem can be generalized to hold uniformly for all
        at the price of an additional term of the form

 cid log log   using standard techniques  Koltchin 

skii   Panchenko   
Observe that the bound of the theorem depends only logarithmically on the depth of the network    But  perhaps
more remarkably  the complexity term of the bound is  
 cid wk cid weighted average of the complexities of the layer
hypothesis sets Hk  where the weights are precisely those
de ning the network  or the function    This suggests that
  function   with   small empirical margin error and   deep
architecture bene ts nevertheless from   strong generalization guarantee  if it allocates more weights to lower layer
units and less to higher ones  Of course  when the weights
are sparse  that will imply an architecture with relatively
fewer units or connections at higher layers than at lower
ones  The bound of the theorem further gives   quantitative guide for apportioning the weights  depending on the
Rademacher complexities of the layer hypothesis sets 
This datadependent learning guarantee will serve as  
foundation for the design of our structural learning algorithms in Section   and Appendix    However  to fully
exploit it  the Rademacher complexity measures need to
be made explicit  One advantage of these datadependent
measures is that they can be estimated from data  which
can lead to more informative bounds  Alternatively  we can
derive useful upper bounds for these measures which can
be more conveniently used in our algorithms  The next results in this section provide precisely such upper bounds 
thereby leading to   more explicit generalization bound 
We will denote by   the conjugate of    that is  
and de ne      maxi     cid xi cid 

     

     

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

Our  rst result gives an upper bound on the Rademacher
complexity of Hk in terms of the Rademacher complexity
of other layer families 
Lemma   For any       the empirical Rademacher
complexity of Hk for   sample   of size   can be upperbounded as follows in terms of those of Hss with       

 cid RS Hk     

   cid RS Hs 

 
 

      

  cid 

  

For the family   
   which is directly relevant to many of
our experiments  the following more explicit upper bound
can be derived  using Lemma  

         and Nk  cid  

Lemma   Let     cid  

   ns 
Then  for any       the empirical Rademacher complexity
of   
  for   sample   of size   can be upper bounded as
follows 

 cid 

 cid RS   

       kN

 
 
 

log   

  

 

Note that Nk  which is the product of the number of units
in layers below    can be large  This suggests that values of
  closer to one  that is larger values of    could be more
helpful to control complexity in such cases  More generally  similar explicit upper bounds can be given for the
Rademacher complexities of subfamilies of Hk with units
connected only to layers                         with    xed 
       Combining Lemma   with Theorem   helps derive
the following explicit learning guarantee for feedforward
neural networks with an output unit connected to all the
other units 
Corollary    Explicit learning bound  Fix       Let
   ns  Then  for any
      with probability at least       over the draw of  
sample   of size   from Dm  the following inequality holds

         and Nk  cid  
 cid 

    cid  
for all    cid  
   wk   hk     
         cid RS      
 cid 

  log   

 

  cid 

 cid cid 

  kN

 cid 

 
 

 
 
 

  

 

 
 

log  
 

            

log    cid  log  

  log    

    log   
   

    

 cid cid wk
 cid cid   

where             

 cid   

 cid  log  

 cid 

 

 

 cid  

  and where      ES Dm   

The learning bound of Corollary   is    ner guarantee than
previous ones by Bartlett   Neyshabur et al   
or Sun et al    This is because it explicitly differentiates between the weights of different layers while previous
bounds treat all weights indiscriminately  This is crucial

to the design of algorithmic design since the network complexity no longer needs to grow exponentially as   function
of depth  Our bounds are also more general and apply to
more other network architectures  such as those introduced
in  He et al    Huang et al   

  Algorithm
This section describes our algorithm  ADANET  for adaptive learning of neural networks  ADANET adaptively
grows the structure of   neural network  balancing model
complexity with empirical risk minimization  We also describe in detail in Appendix   another variant of ADANET
which admits some favorable properties 
Let    cid      be   nonincreasing convex function
upperbounding the zeroone loss     cid      such that  
is differentiable over   and  cid     cid    for all    This surrogate loss   may be  for instance  the exponential function
      ex as in AdaBoost  Freund   Schapire    or
the logistic function        log    ex  as in logistic
regression 

  Objective function
Let             hN  be   subset of    In the most general
case    is in nite  However  as discussed later  in practice 
the search is limited to    nite set  For any          we
will denote by rj the Rademacher complexity of the family
Hkj that contains hj  rj   Rm Hkj  

ADANET seeks to  nd   function      cid  

   wjhj  
    or neural network  that directly minimizes the datadependent generalization bound of Corollary   This leads
to the following objective function 

  cid 

 cid 

  cid 

 cid 

  cid 

       

 
 

    yi

 

wjhj

 

   wj 

 

  

  

  

 cid 

where     RN and       rj     with       and
      hyperparameters  The objective function   is  
convex function of    It is the sum of   convex surrogate
of the empirical error and   regularization term  which is  
weightedl  penalty containing two subterms    standard
norm  regularization which admits   as   hyperparameter  and   term that discriminates the functions hj based on
their complexity 
The optimization problem consisting of minimizing the objective function   in   is de ned over   very large space
of base functions hj  ADANET consists of applying coordinate descent to   In that sense  our algorithm is similar
to the DeepBoost algorithm of Cortes et al    However  unlike DeepBoost  which combines decision trees 
ADANET learns   deep neural network  which requires new
methods for constructing and searching the space of func 

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

   

   

Figure   Illustration of the algorithm   incremental construction
of   neural network  The input layer is indicated in blue  the output layer in green  Units in the yellow block are added at the  rst
iteration while units in purple are added at the second iteration 
Two candidate extensions of the architecture are considered at the
the third iteration  shown in red        twolayer extension     
  threelayer extension  Here    line between two blocks of units
indicates that these blocks are fullyconnected 
tions hj  Both of these aspects differ signi cantly from the
decision tree framework  In particular  the search is particularly challenging  In fact  the main difference between
the algorithm presented in this section and the variant described in Appendix   is the way new candidates hj are
examined at each iteration 

  Description

lt 

We start with an informal description of ADANET  Let
      be    xed parameter determining the number of
units per layer of   candidate subnetwork  The algorithm
proceeds in   iterations  Let lt  denote the depth of the
neural network constructed before the start of the tth iteration  At iteration    the algorithm selects one of the following two options 
  augmenting the current neural network with   subnetwork with the same depth as that of the current network
       
  with   units per layer  Each unit in layer   of
this subnetwork may have connections to existing units in
layer       of ADANET in addition to connections to units
in layer       of the subnetwork 
  augmenting the current neural network with   deeper
subnetwork   cid       
  with depth lt      The set of
connections allowed is de ned in the same way as for   
The option selected is the one leading to the best reduction
of the current value of the objective function  which depends both on the empirical error and the complexity of the
subnetwork added  which is penalized differently in these
two options 
Figure   illustrates this construction and the two options

lt 

just described  An important aspect of our algorithm is that
the units of   subnetwork learned at   previous iteration
 say    in Figure   can serve as input to   deeper subnetwork added later  for example    or    in the Figure 
Thus  the deeper subnetworks added later can take advantage of the embeddings that were learned at the previous
iterations  The algorithm terminates after   rounds or if
the ADANET architecture can no longer be extended to improve the objective  
More formally  ADANET is   boostingstyle algorithm that
applies  block  coordinate descent to   At each iteration
of block coordinate descent  descent coordinates    base
learners in the boosting literature  are selected from the
space of functions    These coordinates correspond to
the direction of the largest decrease in   Once these coordinates are determined  an optimal step size in each of
these directions is chosen  which is accomplished by solving an appropriate convex optimization problem 
Note that  in general  the search for the optimal descent
coordinate in an in nitedimensional space or even in  
nite but large sets such as that of all decision trees of some
large depth may be intractable  and it is common to resort
to   heuristic search  weak learning algorithm  that returns
 optimal coordinates  For instance  in the case of boosting
with trees one often grows trees according to some particular heuristic  Freund   Schapire   
We denote the ADANET model after       rounds by
ft  which is parameterized by wt  Let hk    denote the vector of outputs of units in the kth layer of the
ADANET model  lt  be the depth of the ADANET architecture  nk    be the number of units in kth layer after
      rounds  At round    we select descent coordinates
and
lt  that are generated by   weak learning algorithm WEAKLEARNER  Some choices for this algorithm in
our setting are described below  Once we obtain   and   cid 
we select one of these vectors of units  as well as   vector of
weights     RB  so that the result yields the best improvement in   This is equivalent to minimizing the following
objective function over     RB and           cid 

by considering two candidate subnetworks      cid   
  cid     cid   

lt 

 cid Hlt 

 cid  otherwise 

where       ru     and ru is Rm
  and Rm
minw Ft         minw Ft      cid  then

     argmin
  RB

Ft       ht    

 cid 

 

  cid 

  

Ft        

 
 

    yift xi    yiw     xi 
     cid   cid 

 cid 
 cid  if    

 

 cid Hlt 

In other words 

if

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

ADANET      xi  yi  
      
for       to   do

  

 cid 

     cid    WEAKLEARNER cid    ft 
    MINIMIZE cid Ft      cid 
  cid    MINIMIZE cid Ft      cid cid 

if Ft      cid    Ft   cid    cid  then
ht    
else ht     cid 
if    wt            wt  then

ft   ft         ht

else return ft 

 
 
 
 
 
 
 
 
 
 
 
  return fT

Figure   Pseudocode of the ADANET algorithm  On line   two
candidate subnetworks are generated       randomly or by solving
  On lines   and     is solved for each of these candidates 
On lines   the best subnetwork is selected and on lines  
termination condition is checked 

and otherwise

     argmin
  RB

Ft      cid  ht     cid 

If    wt            wt  then we set ft   ft   
     ht and otherwise we terminate the algorithm 
There are many different choices for the WEAKLEARNER
algorithm  For instance  one may generate   large number
of random networks and select the one that optimizes  
Another option is to directly minimize   or its regularized
version 

 cid Ft        

 cid 

 

  cid 

  

 
 

 cid 

 yift xi yiw     xi 
         

 
over both   and    Here         is   regularization term
that  for instance  can be used to enforce that  cid us cid         
in   Note that  in general    is   nonconvex objective 
However  we do not rely on  nding   global solution to
the corresponding optimization problem  In fact  standard
guarantees for regularized boosting only require that each
  that is added to the model decreases the objective by  
constant amount       it satis es  optimality condition  for
  boosting algorithm to converge    atsch et al    Luo
  Tseng   
Furthermore  the algorithm that we present in Appendix  
uses   weaklearning algorithm that solves   convex subproblem at each step and that additionally has   closedform solution  This comes at the cost of   more restricted
search space for  nding   descent coordinate at each step
of the algorithm 

We conclude this section by observing that in our description of ADANET we have  xed   for all iterations and
only two candidate subnetworks are considered at each
step  Our approach easily extends to an arbitrary number
of candidate subnetworks  for instance of different depth   
as well as varying number of units per layer    Furthermore  selecting an optimal subnetwork among the candidates is easily parallelizable allowing for ef cient and effective search for optimal descent directions  We also note
that the choice of subnetworks need not be restricted to
standard feedforward architectures and more exotic choices
can be employed including the ones in  He et al   
Huang et al   
In our experiments we will restrict
attention to simple feedforward subnetworks 

  Experiments
In this section we present the results of our experiments
with ADANET  Some additional experimental results are
given in Appendix   and further implementation details
presented in Appendix   

  CIFAR 

In our  rst set of experiments  we used the CIFAR 
dataset  Krizhevsky    This dataset consists of  
images evenly categorized in   different classes  To
reduce the problem to binary classi cation  we considered  ve pairs of classes  deertruck  deerhorse 
automobiletruck  catdog  doghorse  Raw
images have been preprocessed to obtain color histograms
and histogram of gradient features  The result is   real
valued features with ranges in    
We compared ADANET to standard feedforward neural
networks  NN  and logistic regression  LR  models  Note
that convolutional neural networks are often   more natural choice for image classi cation problems such as
CIFAR  However  the goal of our experiments was not
to obtain stateof theart results for this particular task  but
  proofof concept showing that our structural learning approach can be very competitive with traditional approaches
for  nding ef cient architectures and training corresponding networks 
Our ADANET algorithm requires the knowledge of complexities rj  which  in some cases  can be estimated from
data 
In our experiments  we used the upper bound of
Lemma   Our algorithm admits   number of hyperparameters  regularization hyperparameters     number of units
  in each layer of new subnetworks that are used to extend
the model at each iteration  and   bound    on weights  
in each unit  As discussed in Section   there are different
approaches to  nding candidate subnetworks in each iteration  In our experiments  we searched for candidate subnet 

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

Table   Experimental results for ADANET  NN  LR and NNGP for different pairs of labels in CIFAR  Boldfaced results are
statistically signi cant at     con dence level 

Label pair

ADANET

LR

NN

NNGP

deertruck
deerhorse
automobiletruck
catdog
doghorse

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

works by minimizing   with       This also requires
  learning rate hyperparameter   These hyperparamers
have been optimized over the following ranges     
                     
            We have used   single   
for all       optimized over          
For simplicity  we chose      
Neural network models also admit   learning rate  
and   regularization coef cient   as hyperparameters  as
well as the number of hidden layers   and the number of units   in each hidden layer 
The range of
  was the same as for ADANET and we varied   in
        in           and    
            Logistic regression
only admits as hyperparameters   and   which were optimized over the same ranges  Note that the total number of
hyperparameter settings for ADANET and standard neural
networks is exactly the same  Furthermore  the same holds
for the number of hyperparameters that determine the resulting architecture of the model    and   for ADANET
and   and   for neural network models  Observe that  while
  particular setting of   and   determines    xed architecture    and   parameterize   structural learning procedure
that may result in   different architecture depending on the
data 
In addition to the grid search procedure  we have conducted   hyperparameter optimization for neural networks using Gaussian process bandits  NNGP  which
is   sophisticated Bayesian nonparametric method for
responsesurface modeling in conjunction with   bandit
algorithm  Snoek et al   
Instead of operating on
  prespeci ed grid 
this allows one to search for hyperparameters in   given range  We used the following
ranges                          and
        This algorithm was run for   trials 
which is more than the number of hyperparameter settings
considered by ADANET and NN  Observe that this search
procedure can also be applied to our algorithm but we chose
not to use it in this set of experiments to further demonstrate
competitiveness of our structural learning approach 
In all our experiments  we use ReLu as the activation func 

tions  NN  NNGP and LR are trained using stochastic
gradient method with batch size of   and maximum of
  iterations  The same con guration is used for solving   We use       for ADANET in all our experiments although in most cases algorithm terminates after  
rounds 
In each of the experiments  we used standard  fold crossvalidation for performance evaluation and model selection 
In particular  the dataset was randomly partitioned into  
folds  and each algorithm was run   times  with   different
assignment of folds to the training set  validation set and
test set for each run  Speci cally  for each                
fold   was used for testing  fold        mod   was used
for validation  and the remaining folds were used for training  For each setting of the parameters  we computed
the average validation error across the   folds  and selected the parameter setting with maximum average accuracy across validation folds  We report the average accuracy  and standard deviations  of the selected hyperparameter setting across test folds in Table  
Our results show that ADANET outperforms other methods on each of the datasets  The average architectures
for all label pairs are provided in Table   Note that NN
and NNGP always select   onelayer architecture  The
architectures selected by ADANET also typically admit  
single layer  with fewer nodes than those selected by NN
and NNGP  However  for the more challenging problem
catdog  ADANET opts for   more complex model with
two layers  which results in   better performance  This further illustrates how our approach helps learn network architectures in an adaptive fashion  based on the complexity
of the task 
As discussed in Section   different heuristics can be used
to generate candidate subnetworks on each iteration of
ADANET  In   second set of experiments  we varied the
objective function   as well as the domain over which
it is optimized  This allowed us to study the sensitivity of ADANET to the choice of   heuristic used to generate candidate subnetworks 
In particular  we considered the following variants of ADANET  ADANET   uses
             cid   cid  as   regularization term in   As

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

Table   Average number of units in each layer 

Table   Experimental results for Criteo dataset 

Label pair

ADANET

NN NNGP

 st layer  nd layer

 
deertruck
 
deerhorse
automobiletruck  
 
catdog
 
doghorse

 
 
 
 
 

   
 
 
   
 
 
   

Table   Experimental results for different variants of ADANET 
for the deertruck label pair in CIFAR 

Algorithm

Accuracy   std  dev 

ADANET SD
ADANET  
ADANET  
ADANET  

     
     
     
     

the ADANET architecture grows  each new subnetwork is
connected to all the previous subnetworks  which signi 
cantly increases the number of connections in the network
and the overall complexity of the model  ADANET   and
ADANET   are restricting connections to existing subnetworks in different ways  ADANET   connects each new
subnetwork only to the subnetwork that was added on the
previous iteration  ADANET   uses dropout on the connections to previously added subnetworks  Finally  while
ADANET is based on the upper bounds on the Rademacher
complexities of Lemma   ADANET SD uses instead standard deviations of the outputs of the last hidden layer on
the training data as surrogates for Rademacher complexities  The advantage of using this datadependent measure of complexity is that it eliminates the hyperparameter   thereby reducing the hyperparameter search space 
We report the average accuracies across test folds for the
deertruck pair in Table  

  Criteo Click Rate Prediction

We also compared ADANET to NN on the Criteo Click
Rate Prediction dataset  https www kaggle com   
criteodisplay adchallenge  This dataset consists of   days of data where each instance is an impression
and   binary label  clicked or not clicked  Each impression admits   count features and   categorical features 
Count features have been transformed by taking the natural logarithm  The values of categorical features appearing less than   times are replaced by   The rest of the
values are then converted to integers  which are then used
as keys to look up embeddings  that are trained together
with each model  If the number of possible values for  
feature   is      then the embedding dimension is set to

Algorithm Accuracy

ADANET
NN

 
 

       for           Otherwise  the embedding dimension is       Missing feature values are set to   We
split the labeled set provided in the link above into training  validation and test sets  Our training set covered the
 rst   days of data   instances  and the validation and test sets consisted of   day   instances 
Gaussian processes bandits were used to  nd the best hyperparameter settings on validation set both for ADANET
and NN  For ADANET we optimized over the following
hyperparameter ranges                   
                For NN the ranges
were as follows                     
                We trained NNs
for   iterations using minibatch stochastic gradient
method with batch size of   The same con guration was
used at each iteration of ADANET to solve   The maximum number of hyperparameter trials was   for both
methods  The results are presented in Table   In this experiment  NN chooses an architecture with four hidden layers
and   units in each hidden layer  Remarkably  ADANET
achieves   better accuracy with an architecture consisting
of single layer with just   nodes  While the difference
in performance appears to be small  it is in fact statistically
signi cant in this challenging task 

  Conclusion
We presented   new framework and algorithms for adaptively learning arti cial neural networks  Our algorithm 
ADANET  bene ts from strong theoretical guarantees  It
simultaneously learns   neural network architecture and its
parameters  by balancing   tradeoff between model complexity and empirical risk minimization  We reported favorable experimental results demonstrating that our algorithm is able to learn network architectures that perform
better than those found via   grid search  Our techniques
are general and can be applied to other neural network architectures such as CNNs and RNNs 

Acknowledgments
The work of    Mohri and that of    Yang were partly
funded by NSF awards IIS  and CCF 

 The test set available from this link does not include ground

truth labels and therefore could be used in our experiments 

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

References
Alvarez  Jose   and Salzmann  Mathieu  Learning the

number of neurons in deep networks  In NIPS   

Arora  Sanjeev  Bhaskara  Aditya  Ge  Rong  and Ma 
Tengyu  Provable bounds for learning some deep representations  In ICML  pp     

Arora  Sanjeev  Liang  Yingyu  and Ma  Tengyu  Why are
deep nets reversible    simple theory  with implications
for training  arXiv   

Baker  Bowen  Gupta  Otkrist  Naik  Nikhil  and Raskar 
Ramesh  Designing neural network architectures using
reinforcement learning  CoRR   

Bartlett  Peter    The sample complexity of pattern classi 
 cation with neural networks  the size of the weights is
more important than the size of the network  Information
Theory  IEEE Transactions on     

Bartlett  Peter    and Mendelson  Shahar  Rademacher and
Gaussian complexities  Risk bounds and structural results  JMLR     

Bergstra  James    Bardenet    emi  Bengio  Yoshua  and
  egl  Bal azs  Algorithms for hyperparameter optimization  In NIPS  pp     

Chen  Tianqi  Goodfellow  Ian    and Shlens  Jonathon 
Net net  Accelerating learning via knowledge transfer 
CoRR   

Choromanska  Anna  Henaff  Mikael  Mathieu  Michael 
Arous    erard Ben  and LeCun  Yann  The loss surfaces
of multilayer networks  arXiv   

Cohen  Nadav  Sharir  Or  and Shashua  Amnon  On the expressive power of deep learning    tensor analysis  arXiv 
 

Cortes  Corinna  Mohri  Mehryar  and Syed  Umar  Deep

boosting  In ICML  pp         

Daniely  Amit  Frostig  Roy  and Singer  Yoram  Toward
deeper understanding of neural networks  The power of
initialization and   dual view on expressivity  In NIPS 
 

Eldan  Ronen and Shamir  Ohad  The power of depth for
feedforward neural networks  arXiv   

Freund  Yoav and Schapire  Robert      decisiontheoretic
generalization of online learning and an application to
boosting  Journal of Computer System Sciences   
   

Ha  David  Dai  Andrew    and Le  Quoc    Hypernet 

works  CoRR   

Han  HongGui and Qiao  JunFei    structure optimisation
algorithm for feedforward neural network construction 
Neurocomputing     

Han  Song  Pool  Jeff  Tran  John  and Dally  William   
Learning both weights and connections for ef cient neural networks  In NIPS   

Hardt  Moritz  Recht  Benjamin  and Singer  Yoram  Train
faster  generalize better  Stability of stochastic gradient
descent  arXiv   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition 
CoRR  abs   

Huang  Gao  Liu  Zhuang  and Weinberger  Kilian   
CoRR 

Densely connected convolutional networks 
 

Islam  Md  Monirul  Yao  Xin  and Murase  Kazuyuki 
  constructive algorithm for training cooperative neural
network ensembles  IEEE Trans  Neural Networks   
   

Islam  Md  Monirul  Sattar  Md  Abdus  Amin  Md  Faijul 
Yao  Xin  and Murase  Kazuyuki    new adaptive merging and growing algorithm for designing arti cial neural
networks  IEEE Trans  Systems  Man  and Cybernetics 
Part       

Janzamin  Majid  Sedghi  Hanie  and Anandkumar  Anima 
Generalization bounds for neural networks through tensor factorization  arXiv   

Kawaguchi  Kenji  Deep learning without poor local min 

ima  In NIPS   

Kingma  Diederik    and Ba  Jimmy  Adam    method for

stochastic optimization  CoRR  abs   

Koltchinskii  Vladmir and Panchenko  Dmitry  Empirical margin distributions and bounding the generalization
error of combined classi ers  Annals of Statistics   
 

Kotani  Manabu  Kajiki  Akihiro  and Akazawa  Kenzo 
  structural learning algorithm for multilayered neural
In International Conference on Neural Netnetworks 
works  volume   pp    IEEE   

Krizhevsky  Alex  Learning multiple layers of features
from tiny images  Master   thesis  University of Toronto 
 

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In NIPS  pp     

AdaNet  Adaptive Structural Learning of Arti cial Neural Networks

Snoek  Jasper  Larochelle  Hugo  and Adams  Ryan   
Practical Bayesian Optimization of Machine Learning
Algorithms  In Pereira     Burges           Bottou    
and Weinberger         eds  NIPS  pp    Curran Associates  Inc   

Sun  Shizhao  Chen  Wei  Wang  Liwei  Liu  Xiaoguang 
and Liu  TieYan  On the depth of deep neural networks 
  theoretical view  In AAAI   

Sutskever  Ilya  Vinyals  Oriol  and Le  Quoc    Sequence
In NIPS 

to sequence learning with neural networks 
 

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott    Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 
Going deeper with convolutions  In CVPR   

Telgarsky  Matus  Bene ts of depth in neural networks  In

COLT   

Zhang  Saizheng  Wu  Yuhuai  Che  Tong  Lin  Zhouhan 
Memisevic  Roland  Salakhutdinov  Ruslan  and Bengio 
Yoshua  Architectural complexity measures of recurrent
neural networks  CoRR   

Zhang  Yuchen  Lee  Jason    and Jordan  Michael     cid   
regularized neural networks are improperly learnable in
polynomial time  arXiv   

Zoph  Barret and Le  Quoc    Neural architecture search

with reinforcement learning  CoRR   

Kuznetsov  Vitaly  Mohri  Mehryar  and Syed  Umar 

Multiclass deep boosting  In NIPS   

Kwok  TinYau and Yeung  DitYan  Constructive algorithms for structure learning in feedforward neural netIEEE Transactions on
works for regression problems 
Neural Networks     

LeCun  Yann  Denker  John    and Solla  Sara    Optimal

brain damage  In NIPS   

Lehtokangas  Mikko  Modelling with constructive back 

propagation  Neural Networks     

Leung  Frank HF  Lam  HakKeung  Ling  SaiHo  and
Tam  Peter KS  Tuning of the structure and parameters of   neural network using an improved genetic algorithm  IEEE Transactions on Neural Networks   
   

Lian  Xiangru  Huang  Yijun  Li  Yuncheng  and Liu  Ji 
Asynchronous parallel stochastic gradient for nonconvex
optimization  In NIPS  pp     

Livni  Roi  ShalevShwartz  Shai  and Shamir  Ohad  On
the computational ef ciency of training neural networks 
In NIPS  pp     

Luo  ZhiQuan and Tseng  Paul  On the convergence of coordinate descent method for convex differentiable minimization  Journal of Optimization Theory and Applications         

Ma  Liying and Khorasani  Khashayar    new strategy
for adaptively constructing multilayer feedforward neural networks  Neurocomputing     

Narasimha  Pramod    Delashmit  Walter    Manry 
Michael    Li  Jiang  and Maldonado  Francisco  An
integrated growingpruning method for feedforward network training  Neurocomputing   
 

Neyshabur  Behnam  Tomioka  Ryota  and Srebro  Nathan 
In

Normbased capacity control in neural networks 
COLT   

  atsch  Gunnar  Mika  Sebastian  and Warmuth  Manfred    On the convergence of leveraging  In NIPS  pp 
   

Sagun  Levent  Guney    Ugur  Arous  Gerard Ben  and
LeCun  Yann  Explorations on high dimensional landscapes  arXiv   

Saxena  Shreyas and Verbeek  Jakob  Convolutional neural

fabrics  CoRR  abs   

