Bottleneck Conditional Density Estimation

Rui Shu   Hung    Bui   Mohammad Ghavamzadeh  

Abstract

We introduce   new framework for training deep
generative models for highdimensional conditional density estimation  The Bottleneck Conditional Density Estimator  BCDE  is   variant of the conditional variational autoencoder
 CVAE  that employs layer    of stochastic variables as the bottleneck between the input  
and target    where both are highdimensional 
Crucially  we propose   new hybrid training
method that blends the conditional generative
model with   joint generative model  Hybrid
blending is the key to effective training of the
BCDE  which avoids over tting and provides  
novel mechanism for leveraging unlabeled data 
We show that our hybrid training procedure enables models to achieve competitive results in
the MNIST quadrant prediction task in the fullysupervised setting  and sets new benchmarks in
the semisupervised regime for MNIST  SVHN 
and CelebA 

  Introduction
Conditional density estimation  CDE  refers to the problem of estimating   conditional density        for the input
  and target    In contrast to classi cation where the target   is simply   discrete class label    is typically continuous or highdimensional in CDE  Furthermore  we want
to estimate the full conditional density  as opposed to its
conditional mean in regression  an important task the conditional distribution has multiple modes  CDE problems
in which both   and   are highdimensional have   wide
range of important applications  including video prediction 
crossmodality prediction      
imageto caption  model
estimation in modelbased reinforcement learning  and so
on 

 Stanford University  Adobe Research  DeepMind  The work
was done when all the authors were with Adobe Research  Correspondence to  Rui Shu  ruishu stanford edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Classical nonparametric conditional density estimators
typically rely on local Euclidean distance in the original input and target spaces  Holmes et al    This approach
quickly becomes ineffective in highdimensions from both
computational and statistical points of view  Recent advances in deep generative models have led to new parametric models for highdimensional CDE tasks  namely the
conditional variational autoencoders  CVAE   Sohn et al 
  CVAEs have been applied to   variety of problems 
such as MNIST quadrant prediction  segmentation  Sohn
et al    attributebased image generation  Yan et al 
  and machine translation  Zhang et al   
But CVAEs suffer from two statistical de ciencies  First 
they do not learn the distribution of the input    We argue that in the case of highdimensional input   where
there might exist   lowdimensional representation  such
as   lowdimensional manifold  of the data  recovering this
structure is important  even if the task at hand is to learn the
conditional density        Otherwise  the model is susceptible to over tting  Second  for many CDE tasks  the acquisition of labeled points is costly  motivating the need for
semisupervised CDE    purely conditional model would
not be able to utilize any available unlabeled data  We
note that while variational methods  Kingma   Welling 
  Rezende et al    have been applied to semisupervised classi cation  where   is   class label   Kingma
et al    Maal   et al    semisupervised CDE
 where   is highdimensional  remains an open problem 
We focus on   set of deep conditional generative models  which we call bottleneck conditional density estimators  BCDEs  In BCDEs  the input   in uences the target
  via layers of bottleneck stochastic variables      zi  in
the generative path  The BCDE naturally has   joint generative sibling model which we denote the bottleneck joint
density estimator  BJDE  where the bottleneck   generates   and   independently  Motivated by Lasserre et al 
  we propose   hybrid training framework that regularizes the conditionallytrained BCDE parameters toward
the jointlytrained BJDE parameters  This is the key feature that enables semisupervised learning for conditional
density estimation in the BCDEs 

 We de ne    labeled point  to be   paired        sample  and

an  unlabeled point  to be unpaired   or   

Bottleneck Conditional Density Estimation

Our BCDE hybrid training framework is   novel approach
for leveraging unlabeled data for conditional density estimation  Using our BCDE hybrid training framework 
we establish new benchmarks for the quadrant prediction
task  Sohn et al    in the semisupervised regime for
MNIST  SVHN  and CelebA  Our experiments show that
  hybrid training is competitive for fullysupervised CDE 
  in semisupervised CDE  hybrid training helps to avoid
over tting  performs signi cantly better than conditional
training with unlabeled data pretraining  and achieves
stateof theart results  and   hybrid training encourages
the model to learn better and more robust representations 

  Background
  Variational Autoencoders
Variational Autoencoder  VAE  is   deep generative model
for density estimation  It consists of   latent variable   with
unit Gaussian prior        Ik  which in turn generates
an observable vector    The observation is usually con 
    where
ditionally Gaussian            diag 

  and   are neural networks whose parameters are represented by   VAE can be seen as   nonlinear generalization of the probabilistic PCA  Tipping   Bishop 
  and thus  can recover nonlinear manifolds in the
data  However  VAE    exibility makes posterior inference
of the latent variables intractable  This inference issue is
addressed via   recognition model        which serves
as an amortized variational approximation of the intractable
posterior        Learning in VAE   is done by jointly optimizing the parameters of both the generative and recognition models so as to maximize an objective that resembles
an autoencoder regularized reconstruction loss  Kingma  
Welling        

sup
 

Eq      ln          DKL              

 

We note that the objective Eq    can be rewritten in the
following form that exposes its connection to the variational lower bound of the loglikelihood

sup

    ln        inf

 

  sup
 

DKL                
Eq     ln
        

       

 

We make two remarks regarding the minimization of the

  conditionally independent Gaussian  this approximation
is at best as good as the mean eld approximation that min 

term DKL                 in Eq    First  when    is
imizes DKL            over all independent Gaussian

 For discrete    one can use   deep network to parameterize  

Bernoulli or   discretized logistic distribution 

     Second  this term serves as   form of amortized posterior regularization that encourages the posterior       
to be close to an amortized variational family  Dayan et al 
  Ganchev et al    Hinton et al    In practice  both   and   are jointly optimized in Eq    and
the reparameterization trick  Kingma   Welling    is
used to transform the expectation over            into
       Ik            diag 
    which leads to
an easily obtained stochastic gradient 

  Conditional VAEs  CVAEs 
In Sohn et al    the authors introduce the conditional
version of variational autoencoders  The conditional generative model is similar to VAE  except that the latent variable   and the observed vector   are both conditioned on
the input    The conditional generative path is

                       diag 
                             diag 
              Ber             

and when we use   Bernoulli decoder is

    
       

Here    denotes the parameters of the neural networks used
in the generative path  The CVAE is trained by maximizing
  lower bound of the conditional likelihood

 

 

 

               

ln          Eq       ln
but with   recognition network           which is typically Gaussian           diag 

       and takes

both   and   as input 

         

   

 

  Blending Generative and Discriminative
It is wellknown that   generative model may yield suboptimal performance when compared to the same model
trained discriminatively  Ng   Jordan      phenomenon attributable to the generative model being misspeci ed  Lasserre et al    However  generative models can easily handle unlabeled data in semisupervised setting  This is the main motivation behind blending generative and discriminative models  Lasserre et al    proposed   principled method for hybrid blending by duplicating the parameter of the generative model into   discriminatively trained   and   generatively trained       
  Xl  Yl  Xu              Xu   Xl   Yl Xl   
 
The discriminatively trained parameter   is regularized toward the generatively trained parameter   via   prior     
that prefers small          As   result  in addition to

Bottleneck Conditional Density Estimation

BJDE

 

Regularization

BCDE

 

 

 

 

 

Unpaired Data
 xi  yi 

Paired Data
 xi  yi 

Figure   The hybrid training procedure that regularizes BCDE towards BJDE  This regularization enables the BCDE to indirectly
leverage unpaired   and   for conditional density estimation 

learning from the labeled data  Xl  Yl  the discriminative parameter   can be informed by the unlabeled data
Xu via   enabling   form of semisupervised discriminatively trained generative model  However  this approach is
limited to simple generative models       naive Bayes and
HMMs  where exact inference of          is tractable 
  Neural Bottleneck Conditional Density

Estimation

While Sohn et al    has successfully applied the
CVAE to CDE  CVAE suffers from two limitations  First 
the CVAE does not learn the distribution of its input   
and thus  is far more susceptible to over tting  Second 
it cannot incorporate unlabeled data  To resolve these limitations  we propose   new approach to highdimensional
CDE that blends the discriminative model that learns the
conditional distribution        with   generative model
that learns the joint distribution        

  Overview
Figure   provides   highlevel overview of our approach
that consists of   new architecture and   new training procedure  Our new architecture imposes   bottleneck constraint 
resulting   class of conditional density estimators  we call it
bottleneck conditional density estimators  BCDEs  Unlike
CVAE  the BCDE generative path prevents   from directly
in uencing    Following the conditional training paradigm
in Sohn et al    conditional discriminative training of
the BCDE means maximizing the lower bound of   conditional likelihood similar to      

ln                    

  Eq       ln

            

         

   

When trained over   dataset of paired        samples  the
overall conditional training objective is

             Xx      

          

 

However  this approach suffers from the same limitations as
CVAE and imposes   bottleneck that limits the  exibility of
the generative model  Instead  we propose   hybrid training
framework that takes advantage of the bottleneck architecture to avoid over tting and supports semisupervision 
One component in our hybrid training procedure tackles the
problem of estimating the joint density         To do this 
we use the joint counterpart of the BCDE  the bottleneck
joint density estimator  BJDE  Unlike conditional models 
the BJDE allows us to incorporate unpaired   and   data
during training  Thus  the BJDE can be trained in   semisupervised fashion  We will also show that the BJDE is
wellsuited to factored inference  see Section         
factorization procedure that makes the parameter space of
the recognition model more compact 
The BJDE also serves as   way to regularize the BCDE 
where the regularization constraint can be viewed as softtying between the parameters of these two models  generative and recognition networks  Via this regularization 
BCDE bene ts from unpaired   and   for conditional density estimation 

  Bottleneck Joint Density Estimation
In the BJDE  we wish to learn the joint distribution of  
and    The bottleneck is introduced in the generative path
via the bottleneck variable    which points to   and    see
Figs      to     Thus  the variational lower bound of the

Bottleneck Conditional Density Estimation

 

 

 

 

 

 

 

 

  

 

    Joint     

    Joint     

    Joint        

    Conditional        

Figure   The joint and conditional components of the BCDE  Dotted lines represent recognition models  The conditional model parameters are regularized toward the joint model    The natural pairing of the conditional and joint parameters is described in Table  

Standard BJDE 
BCDE 
Factored BJDE 
BCDE 

          
         

 
 

       

 

        
        

       
      
       
      

      
      
      
      

      

 

      

 

Table   Soft parameter tying between the BJDE and BCDE  For each network within the BCDE  there is   corresponding network within
the BJDE  We show the correspondence among the networks with and without the application of factored inference  We regularize all
the BCDE networks to their corresponding BJDE network parameters 

joint likelihood is
ln            xy         
  Eq        ln

                

          

   

 

We use     to indicate the parameters of the BJDE networks and reserve     for the BCDE parameters  For
samples in which   or   is unobserved  we will need to
compute the variational lower bound for the marginal likelihoods  Here  the bottleneck plays   critical role 
If  
were to directly in uence   in   nontrivial manner  any
attempt to incorporate unlabeled   would require the recognition model to infer the unobserved   from the observed
    conditional density estimation problem which might
be as hard as our original task 
In the bottleneck architecture  the conditional independence of   and   given  
implies that only the lowdimensional bottleneck needs to
be marginalized  Thus  the usual variational lower bounds
for the marginal likelihoods yield

ln                   Eq      ln
ln                   Eq      ln

          

           
           

          

Since   takes on the task of reconstructing both   and    the
BJDE is sensitive to the distributions of   and   and learns
  joint manifold over the two data sources  Thus  the BJDE
provides the following bene ts    learning the distribution

of   makes the inference of   given   robust to perturbations in the inputs      becomes   jointembedding of  
and      the model can leverage unlabeled data  Following
the convention in Eq    the joint training objectives is
      Xu  Yu  Xl  Yl   

 
Jx    Xu    Jy    Yu    Jxy    Xl  Yl 
where  Xl  Yl  is   dataset of paired        samples  and
Xu and Yu are datasets of unpaired samples 

  Blending Joint and Conditional Deep Models
Because of potential model misspeci cations  the BJDE is
not expected to yield good performance if applied to the
conditional task  Thus  we aim to blend the BJDE and
BCDE models in the spirit of Lasserre et al    However  we note that   is not directly applicable since the
BCDE and BJDE are two different models  and not two
different views  discriminative and generative  of the same
model  Therefore  it is not immediately clear how to tie the
BCDE and BJDE parameters together  Further  these models involve conditional probabilities parameterized by deep
networks and have no closed form for inference 
Any natural prior for the BCDE parameter   and the BJDE
parameter   should encourage pBCDE        to be close to
pBJDE        In the presence of the latent variable    it is
then natural to encourage          to be close to         
and          to be close to          However  enforcing the former condition is intractable as we do not have  
closed form for pBJDE        Fortunately  an approxima 

Bottleneck Conditional Density Estimation

tion of pBJDE        is provided by the recognition model
         Thus  we propose to softly tie together the parameters of networks de ning          and          This
strategy effectively leads to   joint prior over the model network parameters  as well as the recognition network parameters             
As   result  we arrive at the following hybrid blending of
deep stochastic models and its variational lower bound
ln   Xl  Yl  Xu  Yu            ln           

Jx    Xu    Jy    Yu   
Jx    Xl         Xl  Yl 

 

We interpret ln          as    regularization term that
softly ties the joint parameters     and conditional parameters     in an appropriate way  For the BCDE and
BJDE  there is   natural oneto one mapping from the conditional parameters to   subset of the joint parameters 
For the joint model described in Fig      and conditional
model in Fig      the parameter pairings are provided in
Table   Formally  we de ne         and use the index
     to denote the parameter of the neural network on the
Bayesian network link       in the BCDE  For example
                              Similarly  let        
In the BJDE  the same notation yields              The
hybrid blending regularization term can be written as

ln             

       ik 

    const 

 

 

 Xi  

where   denotes the set of common indices of the joint and
conditional parameters  When the index is      it effectively means that          is softly tied to              

           xk 

               xk 
   

Setting       unties the BCDE from the BJDE  and effectively yields to   conditionally trained BCDE  while letting
      forces the corresponding parameters of the BCDE
and BJDE to be identical 
Interestingly  Eq    does not contain the term Jxy  Since
explicit training of Jxy may lead to learning   better joint
embedding in the space of    we note the following generalization of Eq    that trades off the contribution between
Jxy and  Jx     
ln   Xl  Yl  Xu  Yu         
           Xl  Yl  Xu  Yu 
  ln           

Jx    Xu    Jy    Yu   
    Jxy    Xl  Yl   
       hJx    Xl         Xl  Yl      

the equation computes the lower bound of
Intuitively 
  Xl  Yl  either using the joint parameters     or factorizes   Xl  Yl  into   Xl   Yl   Xl  before computing the
lower bound of   Yl   Xl  with the conditional parameters    proof that the lower bound holds for any          
is provided in Appendix    For simplicity  we set      
and do not tune   in our experiments 

  Factored Inference
The inference network           is usually parameterized
as   single neural network that takes both   and   as input 
Using the precisionweighted merging scheme proposed by
  nderby et al    we also consider an alternative parameterization of           that takes   weightedaverage
of the Gaussian distribution        and   Gaussian likelihood term         see Appendix    Doing so offers  
more compact recognition model and more sharing parameters between the BCDE and BJDE       see the bottom
two rows in Table   but at the cost of lower  exibility for
the variational family          
  Experiments
We evaluated the performance of our hybrid training procedure on the permutationinvariant quadrant prediction task
 Sohn et al    Sohn et al    for MNIST  SVHN 
and CelebA  The quadrant prediction task is   conditional
density estimation problem where an image data set is partially occluded  The model is given the observed region and
is evaluated by its perplexity on the occluded region  The
quadrant prediction task consists of four subtasks depending on the degree of partial observability   quadrant prediction  the bottom left quadrant is observed   quadrant
prediction  the left half is observed   quadrant prediction 
the bottom right quadrant is not observed  Topdown prediction  the top half is observed 
In the fullysupervised case  the original MNIST training set      
is converted into our CDE training set
 Xl  Yl     xi  yi 
by splitting each image into
its observed   and unobserved   regions according to the
quadrant prediction task  Note that the training set does not
contain the original class label information  In the nllabel
semisupervised case  we randomly subsampled nl pairs
to create our labeled training set  xi  yi nl
   The remaining nu paired samples are decoupled and put into our unlabeled training sets Xu    xi nu
   Test
performance is the conditional density estimation performance on the entire test set  which is also split into input
  and target   according to the quadrant prediction task 
Analogous procedure is used for SVHN and CelebA 
For comparison against Sohn et al    we evaluate the performance of our models on the MNIST  

     Yu    yi nu

  

  

Models
CVAE  Sohn et al   
BCDE  conditional 
BCDE  na ve pretrain 
BCDE  hybrid 
BCDE  hybrid   factored 

Bottleneck Conditional Density Estimation

nl    

nl    

nl    

nl    

 

     
     
     
     

 

     
     
     
     

     
     
     
     

     
     
     
     

Table   MNIST quadrant prediction task   quadrant  We report the test set loss  IW  and standard error 

Models
CVAE  Sohn et al   
BCDE  conditional 
BCDE  na ve pretrain 
BCDE  hybrid 
BCDE  hybrid   factored 

nl    

nl    

nl    

nl    

 

     
     
     
     

     
     
     
     

     
     
     
     

 

     
     
     
     

Table   MNIST quadrant prediction task   quadrant 

Models
CVAE  Sohn et al   
BCDE  conditional 
BCDE  na ve pretrain 
BCDE  hybrid 
BCDE  hybrid   factored 

nl    

nl    

nl    

nl    

 

     
     
     
     

     
     
     
     

     
     
     
     

 

     
     
     
     

Table   MNIST quadrant prediction task   quadrant 

 

 

 

 

 

 

Models
BCDE  conditional 
BCDE  na ve pretrain 
BCDE  hybrid 
BCDE  hybrid   factored 

nl    
     
     
     
     

nl    
     
     
     
     

Table   SVHN prediction task  TopDown 

quadrant   quadrant  and  quadrant prediction tasks  The
MNIST digits are staticallybinarized by sampling from
the Bernoulli distribution according to their pixel values
 Salakhutdinov   Murray    We use   sigmoid layer
to learn the parameter of the Bernoulli observation model 
We provide the performance on the topdown prediction
task for SVHN and CelebA  We used   discretized logistic
observation model Kingma et al    to model the pixel
values for SVHN and   Gaussian observation model with
 xed variance for CelebA  For numerical stability  we rely
on the implementation of the discretized logistic distribution described in Salimans et al   
In all cases  we extracted   validation set of   samples
for hyperparameter tuning  While our training objective
uses   single  IW  importanceweighted sample  Burda
et al    we measure performance using IW  to
get   tighter bound on the test loglikelihood  Sohn et al 
  We run replicates of all experiments and report the
mean performance with standard errors  For   more expressive variational family  Ranganath et al    we use
two stochastic layers in the BCDE and perform inference

Models
BCDE  conditional 
BCDE  na ve pretrain 
BCDE  hybrid 
BCDE  hybrid   factored 

nl    
     
     
     
     

nl    
     
     
     
     

Table   CelebA prediction task  TopDown 

via topdown inference    nderby et al    We use
multilayered perceptrons  MLPs  for MNIST and SVHN 
and convolutional neural networks  CNNs  for CelebA  All
neural networks are batchnormalized  Ioffe   Szegedy 
  and updated with Adam  Kingma   Ba    The
number of training epochs is determined based on the validation set  The dimensionality of each stochastic layer is
    and   for MNIST  CelebA  and SVHN respectively  All models were implemented in Python  using Tensor ow  Abadi   

  Conditional LogLikelihood Performance
Tables   to   show the performance comparisons between
the CVAE and the BCDE  For baselines  we use the CVAE 
the BCDE trained with the conditional objective  and the
BCDE initialized via pretraining Jx  and Jy  using the
available   and   data separately  and then trained conditionally  Against these baselines  we measure the performance of the BCDE  with and without factored inference 

 github com ruishu bcde

Bottleneck Conditional Density Estimation

trained with the hybrid objective    We tuned the regularization hyperparameter                  on
the MNIST  quadrant semisupervised tasks and settled
on using       for all tasks 
Fullysupervised regime  By comparing in the fullysupervised regime for MNIST  Tables   to   nl    
we show that the hybrid BCDE achieves competitive performance against the pretrained BCDE and outperforms
previously reported results for CVAE  Sohn et al   
Semisupervised regime  As the labeled training size nl
reduces  the bene   of having the hybrid training procedure
becomes more apparent  The BCDEs trained with the hybrid objective function tend to signi cantly improve upon
its conditionallytrained counterparts 
On MNIST  hybrid training of the factored BCDE achieves
the best performance  Both hybrid models achieve over
   nat difference than the pretrained baseline in some
cases   signi cant difference for binarized MNIST  Wu
et al    Conditional BCDE performs very poorly in
the semisupervised tasks due to over tting 
On CelebA  hybrid training of the factored BCDE also
achieves the best performance  Both hybrid models signi cantly outperform the conditional baselines and yield
better visual predictions than conditional BCDE  see Appendix    The hybrid models also outperform pretrained
BCDE with only half the amount of labeled data 
On SVHN 
the hybrid BCDE with standard inference
model signi cantly outperforms the conditional baselines 
However  the use of factored inference results in much
poorer performance  Since the decoder is   discretized logistic distribution with learnable scale  it is possible that the
factored inference model is not expressive enough to model
the posterior distribution 
Model entropy  In Figure   we sample from        for
the conditional BCDE and the hybrid BCDE  We show
that the conditionallytrained BCDE achieves poorer performance because it learns   lowerentropy model  In contrast  hybrid training learns   lower perplexity model  resulting in   highentropy conditional image generator that
spreads the conditional probability mass over the target output space  Theis et al   

  Conditional Training Over ts
To demonstrate the hybrid training   regularization behavior  we show the test set performance during training
 Fig    on the  quadrant MNIST task  nl     Even
with pretrained initialization of parameters  models that
were trained conditionally quickly over    resulting in poor
test set performance  In contrast  hybrid training regularizes the conditional model toward the joint model  which is

    Conditional

    Hybrid

Figure   Comparison of conditional image generation for the
conditional versus hybrid BCDE on the semisupervised  
quadrant task  Row   shows the original images  Rows   show
three attempts by each model to sample   according to    the
bottomleft quadrant  indicated in gray  Hybrid training yields  
higherentropy model that has lower perplexity 

much more resilient against over tting 

Figure   Comparison of the BCDE variants on the  quadrant
MNIST prediction task with nl     labeled points  In contrast to conditional training  hybrid training is less susceptible to
over tting 

  Robustness of Representation
Since hybrid training encourages the BCDE to consider the
distribution of    we can demonstrate that models trained
in   hybrid manner are robust against structured perturbations of the data set  To show this  we experimented with
two variants of the MNIST quadrant task called the shiftsensitive and shiftinvariant topbottom prediction tasks  In
these experiments  we set      

  SHIFTSENSITIVE ESTIMATION
In the shiftsensitive task  the objective is to learn to predict
the bottom half of the MNIST digit     when given the top
half     However  we introduce structural perturbation to
the top and bottom halves of the image in our training  validation  and test sets by randomly shifting each pair       
horizontally by the same number of pixels  shift varies between               We then train the BCDE using either the conditional or hybrid objective in the fully 

Bottleneck Conditional Density Estimation

supervised regime  Note that compared to the original topdown prediction task  the perplexity of the conditional task
remains the same after the perturbation is applied 

Models
Conditional
Hybrid
Hybrid   Factored

No Shift

Shift

     
     
     

     
     
     

 

 
 
 

Table   Shiftsensitive topbottom MNIST prediction  Performance with and without structural corruption reported  along with
the performance difference  Hybrid training is robust against
structural perturbation of       

Table   shows that hybrid training consistently achieves
better performance than conditional training  Furthermore 
the hybridly trained models were less affected by the introduction of the perturbation  demonstrating   higher degree of robustness  Because of its more compact recognition model  hybrid   factored is less vulnerable to over tting  resulting in   smaller performance gap between performance on the shifted and original data 

  SHIFTINVARIANT ESTIMATION
The shiftinvariant task is similar to the shiftsensitive topbottom task  but with one key difference  we only introduce
structural noise to the top half of the image in our training 
validation  and test sets  The goal is thus to learn that the
prediction of    which is always centered  is invariant to
the shifted position of   

Models
Conditional
Hybrid
Hybrid   Factored

No Shift

Shift

     
     
     

     
     
     

 

 
 
 

Table   Shiftinvariant topbottom MNIST prediction  Performance with and without structural corruption reported  along with
the performance difference  Hybrid training is robust against
structural corruption of   

the latent

Figure   Visualization of
space of hybrid and
conditionallytrained BCDEs  PCA plots of the latent space subregion for all     whose class label     are shown  Fill color
indicates the degree of shift  blue     orange    
Table   shows similar behavior to Table   Hybrid training

continues to achieve better performance than conditional
models and suffer   much smaller performance gap when
structural corruption in   is introduced 
In Fig    we show the PCA projections of the latent space
subregion populated by digits   and colorcoded all points
based on the degree of shift  We observe that hybrid training versus conditional training of the BCDE result in very
different learned representations in the stochastic layer  Because of regularization toward the joint model  the hybrid
BCDE   latent representation retrains information about  
and learns to untangle shift from other features  And as expected  conditional training does not encourage the BCDE
to be aware of the distribution of    resulting in   latent
representation that is ignorant of the shift feature of   

  Conclusion
We presented   new framework for highdimensional conditional density estimation  The building blocks of our
framework are   pair of sibling models 
the Bottleneck
Conditional Density Estimator  BCDE  and the Bottleneck
Joint Density Estimator  BJDE  These models use layers
of stochastic neural networks as bottleneck between the input and output data  While the BCDE learns the conditional
distribution        the BJDE learns the joint distribution
        The bottleneck constraint implies that only the bottleneck needs to be marginalized when either the input   or
the output   are missing during training  thus  enabling the
BJDE to be trained in   semisupervised fashion 
The key component of our framework is our hybrid objective function that regularizes the BCDE towards the BJDE 
Our new objective is   novel extension of Lasserre et al 
  that enables the principle of hybrid blending to be
applied to deep variational models  Our framework provides   new mechanism for the BCDE    conditional model 
to become more robust and to learn from unlabeled data in
semisupervised conditional density estimation 
Our experiments showed that hybrid training is competitive in the fullysupervised regime against pretraining 
and achieves superior performance in the semisupervised
quadrant prediction task in comparison to conditional
models  achieving new stateof theart performances on
MNIST  SVHN  and CelebA  Even with pretrained weight
initializations  the conditional model is still susceptible to
over tting  In contrast  hybrid training is signi cantly more
robust against over tting  Furthermore  hybrid training
transfers the nice embedding properties of the BJDE to the
BCDE  allowing the BCDE to learn better and more robust
representation of the input    The success of our hybrid
training framework makes it   prime candidate for other
highdimensional conditional density estimation problems 
especially in semisupervised settings 

Bottleneck Conditional Density Estimation

Ranganath     Tran     and Blei        Hierarchical Variational Models  ArXiv eprints    November
 

Rezende     Mohamed     and Wierstra     Stochastic Backpropagation and Approximate Inference in Deep
Generative Models  ArXiv eprints    January
 

Salakhutdinov     and Murray     On the quantitative analysis of deep belief networks  International Conference
on Machine Learning   

Salimans  Tim  Karpathy  Andrej  Chen  Xi  and Kingma 
Diederik    Pixelcnn  Improving the pixelcnn with
discretized logistic mixture likelihood and other modi 
 cations  CoRR  abs    URL http 
 arxiv org abs 

Sohn     Shang     and    Lee  Improved multimodal
deep learning with variation of information  Neural Information Processing Systems   

Sohn     Yan     and Lee     Learning structured output
representation using deep conditional generative models 
Neural Information Processing Systems   

  nderby        Raiko     Maal       Kaae   nderby 
   and Winther     Ladder Variational Autoencoders 
arXiv   

Theis     van den Oord     and Bethge       note on
the evaluation of generative models  arXiv 
 

Tipping     and Bishop     Probabilistic Principal Com 

ponent Analysis        Statist  Soc      

Wu     Burda     Salakhutdinov     and Grosse     On
the Quantitative Analysis of DecoderBased Generative
Models  arXiv   

Yan     Yang     Sohn     and Lee     Attribute Image 
Conditional Image Generation from Visual Attributes 
arXiv   

Zhang     Xiong     Su     Duan     and Zhang     Variational Neural Machine Translation  arXiv 
 

References
Abadi  Mart    et  al  TensorFlow  Largescale machine
learning on heterogeneous systems    URL http 
 tensorflow org  Software available from tensor ow org 

Burda     Grosse     and Salakhutdinov     Importance
Weighted Autoencoders  arXiv preprints 
 

Dayan     Hinton     Neal     and Zemel     The

Helmholtz Machine  Neural computation   

Ganchev     Graca     Gillenwater     and Taskar     Posterior regularization for structured latent variable models  JMLR   

Hinton     Dayan     Frey     and Radford     The
 wakesleep  algorithm for unsupervised neural networks  Science   

Holmes        Gray       

and Isbell       
Fast Nonparametric Conditional Density Estimation 
arXiv   

Ioffe     and Szegedy     Batch Normalization  Accelerating Deep Network Training by Reducing Internal Covariate Shift  arXiv   

Kingma     and Ba     Adam    Method for Stochastic

Optimization  arXiv   

Kingma       and Welling     AutoEncoding Variational

Bayes  arXiv   

Kingma        Rezende        Mohamed     and Welling 
   SemiSupervised Learning with Deep Generative
Models  arXiv   

Kingma  Diederik    Salimans  Tim  and Welling  Max 
Improving variational inference with inverse autoregressive  ow  CoRR  abs    URL http 
 arxiv org abs 

Lasserre     Bishop     and Minka     Principled hybrids
of generative and discriminative models 
In The IEEE
Conference on Computer Vision and Pattern Recognition   

Maal       Kaae   nderby     Kaae   nderby    
and Winther     Auxiliary Deep Generative Models 
arXiv   

Ng     and Jordan     On discriminative vs  generative
classi ers    comparison of logistic regression and naive
bayes  Neural Information Processing Systems   

