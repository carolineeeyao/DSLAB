MultiClass Optimal Margin Distribution Machine

Teng Zhang   ZhiHua Zhou  

Abstract

Recent studies disclose that maximizing the minimum margin like support vector machines does
not necessarily lead to better generalization performances  and instead 
it is crucial to optimize the margin distribution  Although it has
been shown that for binary classi cation  characterizing the margin distribution by the  rstand secondorder statistics can achieve superior
performance 
It still remains open for multiclass classi cation  and due to the complexity
of margin for multiclass classi cation  optimizing its distribution by mean and variance can
also be dif cult 
In this paper  we propose
mcODM  multiclass Optimal margin Distribution Machine  which can solve this problem ef 
 ciently  We also give   theoretical analysis for
our method  which veri es the signi cance of
margin distribution for multiclass classi cation 
Empirical study further shows that mcODM always outperforms all four versions of multiclass
SVMs on all experimental data sets 

  Introduction
Support vector machines  SVMs  and Boosting have
been two mainstream learning approaches during the past
decade  The former  Cortes   Vapnik    roots in the
statistical learning theory  Vapnik    with the central
idea of searching   large margin separator       maximizing the smallest distance from the instances to the classi 
 cation boundary in   RKHS  reproducing kernel Hilbert
space 
It is noteworthy that there is also   long history
of applying margin theory to explain the latter  Freund  
Schapire    Schapire et al    due to its tending to
be empirically resistant to over tting  Reyzin   Schapire 
  Wang et al    Zhou   

 National Key Laboratory for Novel Software Technology 
Nanjing University  Nanjing   China  Correspondence to 
ZhiHua Zhou  zhouzh lamda nju edu cn 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Recently  the margin theory for Boosting has  nally been
defended  Gao   Zhou    and has disclosed that the
margin distribution rather than   single margin is more crucial to the generalization performance 
It suggests that
there may still exist large space to further enhance for
SVMs  Inspired by this recognition   Zhang   Zhou   
  proposed   binary classi cation method to optimize
margin distribution by characterizing it through the  rstand secondorder statistics  which achieves quite satisfactory experimental results  Later   Zhou   Zhou    extends the idea to an approach which is able to exploit unlabeled data and handle unequal misclassi cation cost   
brief summary of this line of early research can be found
in  Zhou   
Although it has been shown that for binary classi cation 
optimizing the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously can get superior performance  it still remains open for
multiclass classi cation  Moreover  the margin for multiclass classi cation is much more complicated than that for
binary class classi cation  which makes the resultant optimization be   dif cult nondifferentiable nonconvex programming  In this paper  we propose mcODM  multiclass
Optimal margin Distribution Machine  to solve this problem ef ciently  For optimization  we relax mcODM into  
series of convex quadratic programming  QP  and extend
the Block Coordinate Descent  BCD  algorithm  Tseng 
  to solve the dual of each QP  The subproblem of
each iteration of BCD is also   QP  By exploiting its special
structure  we derive   sorting algorithm to solve it which is
much faster than general QP solvers  We further provide
  generalization error bound based on Rademacher complexity  and further present the analysis of the relationship
between generalization error and margin distribution for
multiclass classi cation  Extensive experiments on twenty
two data sets show the superiority of our method to all four
versions of multiclass SVMs 
The rest of this paper is organized as follows  Section  
introduces some preliminaries  Section   formulates the
problem  Section   presents the proposed algorithm  Section   discusses some theoretical analyses  Section   reports on our experimental studies and empirical observations  Finally Section   concludes with future work 

MultiClass Optimal Margin Distribution Machine

  Preliminaries
We denote by     Rd the instance space and        
the label set  where                  kg  Let   be an unknown  underlying  distribution over    cid       training
set                             xm  ym         cid     
is drawn identically and independently         according to
distribution    Let           be   feature mapping associated to some positive de nite kernel  cid  For multiclass
classi cation setting  the hypothesis is de ned based on  
weight vectors            wk      where each weight vector
wl        de nes   scoring function      
 
      and the
label of instance   is the one resulting in the largest score 
 
      This decision function
            argmaxl    
naturally leads to the following de nition of the margin for
  labeled instance       
       cid  max
 
   

 cid            

 
     

 

Thus   misclassi es        if and only if it produces   negative margin for this instance 
Given   hypothesis set   of functions mapping   to  
and the labeled training set    our goal is to learn   function       such that the generalization error       
      cid          is small 

  Formulation
To design optimal margin distribution machine for multiclass classi cation  we need to understand how to optimize
the margin distribution   Gao   Zhou    proved that  to
characterize the margin distribution  it is important to consider not only the margin mean but also the margin variance  Inspired by this idea   Zhang   Zhou     
proposed the optimal margin distribution machine for binary classi cation  which characterizes the margin distribution according to the  rstand secondorder statistics 
that is  maximizing the margin mean and minimizing the
margin variance simultaneously  Speci cally  let  cid cid  denote
the margin mean  and the optimal margin distribution machine can be formulated as 

min

  cid cid cid    

  
      cid   xi  yi   cid   cid cid   cid   cid   

     cid   cid cid cid   

 cid 
 

  

 cid   xi  yi   cid   cid cid           

viation of the margin of  xi  yi  to the margin mean is
  cid   xi  yi   cid      and the optimal margin distribution machine can be reformulated as

  

min
  cid    

     

 cid 
     cid 
   cid   cid 
 
      cid   xi  yi   cid     cid   cid   cid   cid   

 cid 
 

  

 cid   xi  yi   cid       cid           

 

where  cid        is   parameter to trade off two different kinds of deviation  larger or less than margin mean 
 cid        is   parameter of the zero loss band  which can
control the number of support vectors       the sparsity of
the solution  and  cid   cid  in the denominator is to scale the
second term to be   surrogate loss for   loss 
For multiclass classi cation  let the regularization term
 wl    and combine with the de nition
     
  
of margin  and we arrive at the formulation of mcODM 
 wl    
 xi   cid  max
  yi
 xi   cid  max
  yi

  
  
   xi   cid     cid   cid   cid   cid   
 
   xi   cid       cid           
 

 cid 
     cid 
   cid   cid 
 

  
 
yi
 
yi

      

wl cid    

 cid 
 

 
  

min

 

 
 

 

 

 

where  cid   cid  and  cid  are the parameters for tradingoff described previously 

  Optimization
Due to the max operator in the second constraint  mcODM
is   nondifferentiable nonconvex programming  which is
quite dif cult to solve directly 
In this section  we  rst relax mcODM into   series of convex quadratic programming  QP  which can be much easier to handle  Speci cally  at each iteration  we recast the
 rst constraint as    cid    linear inequality constraints 
   xi   cid     cid   cid   cid   cid        yi 
 

 xi   cid   

 

 
yi

 cid 

     
   

and replace the second constraint with

 
yi

 

 xi   cid  Mi  cid       cid       

 

where     is the regularization term to penalize the
model complexity   cid  and  cid  are tradingoff parameters   cid  
and    are the deviation of the margin  cid   xi  yi  to the mari    is exactly the
gin mean  It   evident that
margin variance 
By scaling   which doesn   affect the  nal classi cation
results  the margin mean can be  xed as   then the de 

 
  cid 

     

 
where Mi   maxl yi  cid  
   xi  and  cid wl is the solution to
the previous iteration  Then we can repeatedly solve the
following convex QP problem until convergence 

  
  
 wl    
 cid 
 
 
 xi   cid   
   xi   cid     cid   cid   cid   cid         yi 
 
 xi   cid  Mi  cid       cid           

 cid 
     cid 
   cid   cid 
 

  
 
yi
 
yi

  

min

wl cid    

 
 

      

 

MultiClass Optimal Margin Distribution Machine
 cid        yi for the
Introduce the lagrange multipliers  cid   
 rst    cid    constraints and  cid    cid    for the last constraint
 
respectively  the Lagrangian function of Eq    leads to

             cid  

  

  

    cid   

 cid 
 

 cid 
     cid 
   cid   cid 
 

 
yi

 xi   cid   

   xi   cid       cid     cid   
 

 xi   cid  Mi  cid     cid   cid   cid     

 

  

  

 
 

  yi

 cid   
   

  
  wl   cid         cid   
 wl    
 
 cid    
  
 cid yi  

  

 cid    

 
yi

  

 

 
 

  yi

  

    cid   cid 

 cid 

  yi

wl  

 cid    

By setting the partial derivations of variables fwl   cid     ig to
zero  we have

    xi 

 cid     cid   cid yi   cid   

 

 cid   cid yi   cid  

 cid   
 

 cid   
  

    

    cid   cid 

 cid cid 

 cid   

  Solving the subproblem

 

Speci cally  we sequentially select   group of       varii    cid   associated with instance xi to miniables  cid 
mize  while keeping other variables as constants  and repeat
this procedure until convergence 
Algorithm   below details the kenrel mcODM 

Algorithm   Kenrel mcODM
  Input  Data set   
  Initialize  cid 

   cid 

 

 

 cid 

           cid  

           cid 

            cid  

   and

 

   cid           cid    as zero vector 

for                 do
Mi   maxl yi
end for
Solve Eq    by block coordinate descent method 

  while  cid  and  cid  not converge do
 
 
 
 
  end while
  Output   cid   cid 

 cid   cid yj    cid   cid xj  xi 

 
  cid  
 

The subproblem in step   of Algorithm   is also   convex QP with       variables  which can be accomplished
by some standard QP solvers  However  by exploiting its
special structure       only   small quantity of cross terms
are involved  we can derive an algorithm to solve this subproblem just by sorting  which can be much faster than general QP solvers 
Note that all variables except  cid 
we have the following subproblem 

     cid   are  xed  so

             cid  

min
  cid yi
 cid  

   cid  

  yi

 
 

 cid  

    

Bl cid  

   

     cid    cid yi
 cid yi
   cid  

 
 

 

  yi

 

 cid 
       cid  

    

 
 

   

  Byi cid yi

  
 cid  
 cid         yi 

     

  
 cid  
 
 cid    cid   

 
 
 cid 
     cid xi  xj cid  
 cid 
 
     cid xi  xj cid yi
 
            cid cid 
and

where      cid xi  xi  Bl  
 cid yj    cid       cid   cid  for      yi  Byi  
 cid yj  yi cid              cid cid 
   cid  Mi        cid   cid  Byi 
The KKT conditions of Eq    indicate that there are scalars
 cid   cid   and  cid  such that

 cid cid 

 cid 

  
 cid  
 cid         yi 

     

  
 cid  
 

 

 

 

  

where  cid yi   equals   when yi     and   otherwise  We
further simplify the expression of wl as

 

  

 

 cid  
 

  yi

wl  

 cid   cid yi   cid   xi 

 cid   cid cid   
  

 cid 
  for      yi and  cid yi
by de ning  cid  
  and
 cid   
 
substituting Eq    and Eq    into the Lagrangian function 
  
then we have the following dual problem
    cid   cid 
 wl    
  
    cid   cid 
       cid   cid 
  
 cid 

  

 

min
  cid yi
 cid  

  yi

   

 cid yi

 cid cid 

   cid  

 cid  
 

 
 

 cid 

  

  

  

  

 

 

    

 cid  

  

   Mi        cid 

  
         
 cid  
 cid           yi 

  
 cid  
 
 cid    cid       

The objective function in Eq    involves         variables in total  so it is not easy to optimize with respect
to all the variables simultaneously  Note that all the constraints can be partitioned into   disjoint sets  and the ith
     cid    so the variables can be
set only involves  cid 
divided into   decoupled groups and an ef cient block coordinate descent algorithm  Tseng    can be applied 

             cid  

MultiClass Optimal Margin Distribution Machine

 cid    cid   
 cid   cid  
  cid  
 cid cid        cid   cid   
 cid    cid yi
  cid yi
 

       cid    cid         yi 
    Bl  cid   cid     cid            yi 

      cid        cid   cid     
 cid    cid     Byi
 cid   cid     

According to Eq    Eq    and Eq    are equivalent to

  cid  

    Bl  cid   cid     
Bl  cid   cid   cid   

           yi 
           yi 

if  cid  
if  cid  

In the same way  Eq    and Eq    are equivalent to

Thus KKT conditions turn to Eq      Eq    and Eq     
Eq    Note that

if  cid      
if  cid      

 cid   cid yi

      cid          
       cid   
 cid   cid yi
 
 

 cid  min

 cid  
 

 

 

 cid   cid  Bl
 

       yi 
 

satis es KKT conditions Eq    and Eq      Eq    and

 cid    cid  max

 

 cid   
  cid yi
 
 

 

 

satis es KKT conditions Eq    and Eq      Eq    By
substituting Eq    into Eq    we obtain

  cid yi

    Byi

 cid   cid    max

 

 
 

   cid yi
 

 

 

 
 cid     

 

 cid      according to Eq    and Eq    we
 cid     

Let   consider the following two cases in turn 
Case     cid yi
     cid cid Byi
    Thus     cid cid Byi
 
have  cid       and  cid yi
which implies that  cid   cid  Byi   DF
   
Case     cid yi
        according to Eq    and Eq    we
      cid cid AF cid EByi
have  cid       cid yi
  Thus 
    cid cid AF cid EByi
      which implies that  cid    Byi   DF
   
The remaining task is to  nd  cid  such that Eq    holds  With
Eq    and Eq    it can be shown that

    and  cid yi

DE cid   

DE cid   

 cid  

 
 

 

 cid   

 
AByi
 
   Bl
   
     gj
    jflj cid  
 
DE cid     
   Bl
     gj
DE cid      jflj cid  

  cid  

  cid  

AE

AEByi     

  Case  

 

  Case  

 

 cid   

 

In both cases 

   Bl    jflj cid  

the optimal  cid  takes the form of     
     gj  where   and   are some

  cid  

 
 
 
 
 
 

 
 

 
 

 

constants   Fan et al    showed that it can be found
by sorting fBlg for      yi in decreasing order and then
sequentially adding them into an empty set  cid  until

 cid 

 cid 

 

   

 cid  max
  cid 

Bl 

 

 
      cid  

  cid  Bl

Note that the Hessian matrix of the objective function of
Eq    is positive de nite  which guarantees the existence
and uniqueness of the optimal solution  so only one of
 cid  acEq    and Eq    can hold  We can  rst compute  cid 
cording to Eq    for Case   and then check whether the
 cid 
constraint of  cid  is satis ed  If not  we further compute  cid 
for Case   Algorithm   summarizes the pseudocode for
solving the subproblem 

 cid     cid     Bi 
         

 Bnf      in decreasing order 

Algorithm   Solving the subproblem
  Input  Parameters        fB          Bkg           
  Initialize         then swap     and  Byi  and sort
         cid    AByi   
  while    cid    and  cid    cid              Bi do
 
 
  end while
  if  cid   cid  Byi   DF   then
  min   cid   cid  Bl        yi 
 
   cid   cid  Byi   
 
 
  else
 
 

 cid  
 
 cid yi
 cid      
 
       cid     AE            DE  cid    
while    cid    and  cid   cid      AE DE  cid        Bi
do
 cid     cid     Bi 
         
  min   cid   cid  Bl        yi 
     cid   cid  AF  cid  EByi DE  cid    

 
 
 
 
 
 
  end if
  Output   cid 

end while
 cid  
 
 cid yi
 cid        cid yi
 

 

 cid       

             cid  

     cid   

  Speedup for linear kernel

In section   the proposed method can ef ciently deal
with kernel mcODM  However  the computation of Mi in
step   of Algorithm   and the computation of parameters
 cid Bl in Algorithm   both involve the kernel matrix  whose
inherent computational cost takes      time  so it might
be computational prohibitive for large scale problems 
When linear kernel is used  these problems can be alleviated  According to Eq      is spanned by the training
instance so it lies in    nite dimensional space under this

MultiClass Optimal Margin Distribution Machine

 

circumstance  By storing            wk explicitly  the com 
 
putational cost of Mi   maxl yi  
  xi can be much less 
 cid   cid yj    cid     
 
Moreover  note that  cid Bl  
  xj cid  
     
 cid cid yi  
  xi cid 
 
 
 
 
 cid cid       
  xj cid cid  
  xi cid cid  
 cid   cid yi   cid    so  cid Bl can also be computed ef ciently 
 
 

 cid cid yj   

 
    

  cid  
 

 
 cid cid   cid  

  Analysis
In this section  we study the statistical property of mcODM 
To present the generalization bound of mcODM  we need
to introduce the following loss function  cid 

 cid         cid   

    cid       cid 
   cid   cid 
fw
       cid  max
 
   

 cid   cid          

   cid cid cid 
       cid     cid   cid   yg 
 

       

where  cid  is the indicator function that returns   when
the argument holds  and   otherwise 
As can be
seen   cid   cid       is   lower bound of  cid         and
 cid cid   cid          cid cid        
Theorem   Let                  cid     
 
 wl    cid   cid   be the hypothesis space
 
 
of mcODM  where           is   feature mapping
induced by some positive de nite kernel  cid  Assume that
   cid  fx    cid        cid       then for any  cid      with probability at least    cid   cid  the following generalization bound
holds for any       

 
  

 

 

      cid   
 

 cid cid   xi  yi   

  

   cid 
   cid   cid 

 cid  
 

   

ln  
 cid 
  

 

Proof  Let    cid  be the family of hypotheses mapping    cid 
      de ned by    cid               cid   cid             Hg 
with McDiarmid inequality  McDiarmid    yields the
following inequality with probability at least    cid   cid 

  

  

  

  cid cid   cid        cid   

 

 cid cid   cid xi  yi 

 

   RS cid       cid     

        cid 

ln  
 cid 
  

 

Note that  cid cid   cid     cid cid             cid       cid   cid 
  cid   cid     cid   cid    cid cid   cid       and  cid    is
 cid cid   
 
Lipschitz function  by using Talagrand   lemma  Mohri
et al    we have

  
 
According to Theorem   of  Lei et al    we have
RS     cid   cid     cid 

 cid     and proves the stated result 

 RS     cid 
   cid   cid 

      cid   
 

 cid cid   xi  yi   

ln  
 cid 
  

   

  

 

Theorem   shows that we can get   tighter generalization
bound for smaller   cid  and smaller  cid  Since  cid   cid     cid  so
the former can be viewed as an upper bound of the margin 
Besides     cid   cid  is the lower bound of the zero loss band of
mcODM  This veri es that better margin distribution      
larger margin mean and smaller margin variance  can yield
better generalization performance  which is also consistent
with the work of  Gao   Zhou   

  Empirical Study
In this section  we empirically evaluate the effectiveness
of our method on   broad range of data sets  We  rst
introduce the experimental settings and compared methods in Section   and then in Section   we compare
our method with four versions of multiclass SVMs      
mcSVM  Weston   Watkins    Crammer   Singer 
    oneversus all SVM  ovaSVM  oneversus 
one SVM  ovoSVM   Ulrich    and errorcorrecting
output code SVM  ecocSVM   Dietterich   Bakiri   
In addition  we also study the in uence of the number of
classes on generalization performance and margin distribution in Section   Finally  the computational cost is
presented in Section  

  Experimental Setup

We evaluate the effectiveness of our proposed methods on
twenty two data sets  Table   summarizes the statistics of
these data sets  The data set size ranges from   to more
than   and the dimensionality ranges from   to more
than   Moreover  the number of class ranges from  
to   so these data sets cover   broad range of properties  All features are normalized into the interval    
For each data set  eighty percent of the instances are randomly selected as training data  and the rest are used as
testing data  For each data set  experiments are repeated
for   times with random data partitions  and the average
accuracies as well as the standard deviations are recorded 
mcODM is compared with four versions of multiclass
SVMs       ovaSVM  ovoSVM  ecocSVM and mcSVM 
These four methods can be roughly classi ed into two
groups  The  rst group includes the  rst three methods by converting the multiclass classi cation problem
into   set of binary classi cation problems  Specially 
ovaSVM consists of learning   scoring functions hl      
  cid             each seeking to discriminate one class
      from all the others  as can be seen it need train  
SVM models  Alternatively  ovoSVM determines the scoring functions for all the combinations of class pairs  so it
need train      cid    SVM models  Finally  ecocSVM
is   generalization of the former two methods  This technique assigns to each class         code word with length
   which serves as   signature for this class  After training

MultiClass Optimal Margin Distribution Machine

Table  Characteristics of experimental data sets 

ID Dataset

 Instance

 Feature

 Class

ID Dataset

 Instance

 Feature

 Class

 
 
 
 
 
 
 
 
 
 
 

iris
wine
glass
svmguide 
svmguide 
vehicle
vowel
segment
dna
satimage
usps

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

  binary SVM models   cid          hc cid  the class predicted
for each testing instance is the one whose signatures is the
closest to               hc    in Hamming distance  The
weakness of these methods is that they may produce unclassi able regions and their computational costs are usually quite large in practice  which can be observed in the
following experiments  On the other hand  mcSVM belongs to the second group 
It directly determines all the
scroing functions at once  so the time cost is usually less
than the former methods  In addition  the unclassi able regions are also resolved 
For all the methods  the regularization parameter  cid  for
mcODM or   for binary SVM and mcSVM is selected
by  fold cross validation from              
For
mcODM  the regularization parameters  cid  and  cid  are both selected from         For ecocSVM  the exhaustive codes strategy is employed       for each class  we construct   code of length    cid   cid    as the signature  All the
selections for parameters are performed on training sets 

  Results

Table   summarizes the detailed results on twenty two
data sets  As can be seen  the overall performance of
our method is superior or highly competitive to the other
compared methods  Speci cally  mcODM performs significantly better than mcSVM ovaSVM ovoSVM ecocSVM
on   over   data sets respectively  and achieves
the best accuracy on   data sets  In addition  as can be
seen  in comparing with other four methods which don  
consider margin distribution  the win tie loss counts show
that mcODM is always better or comparable  almost never
worse than it 

  In uence of the Number of Classes

In this section we study the in uence of the number of
classes on generalization performance and margin distribution  respectively 

sector
pendigits
news 
letter
protein
shuttle
connect 

 
 
 
 
 
 
 
  mnist
 
 
 

aloi
rcv 
covtype

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

  GENERALIZATION PERFORMANCE

Figure   plots the generalization performance of all the  ve
methods on data set segment  and similar observation can
be found for other data sets  As can be seen  when the number of classes is less than four  all methods perform quite
well  However  as the  fth class is added  the generalization performance of other four methods decreases drastically  This might be attributable to the introduction of some
noisy data  which SVMstyle algorithms are very sensitive
to since they optimize the minimum margin  On the other
hand  our method considers the whole margin distribution 
so it can be robust to noise and relatively more stable 

Figure Generalization performance of all the  ve methods on
data set segment with the increase of the number of classes 

  MARGIN DISTRIBUTION

Figure   plots the frequency histogram of margin distribution produced by mcSVM  ovaSVM and mcODM on data
set segment as the number of classes increases from two
to seven  As can be seen  when the number of classes is
less than four  all methods can achieve good margin distribution  whereas with the increase of the number of classes 
the other two methods begin to produce negative margins 
At the same time  the distribution of our method becomes

 generalization performance Class mcSVM ovaSVM ovoSVM ecocSVM mcODMMultiClass Optimal Margin Distribution Machine

Table Accuracy  mean cid std  comparison on twenty two data sets  Linear kernel is used  The best accuracy on each data set is bolded 
 cid  indicates the performance of mcODM is signi cantly better worse than the compared methods  paired ttests at   signi cance
level  The average rank and top  times are listed in the third and second row from the bottom  The win tie loss counts for mcODM are
summarized in the last row  ovoSVM and ecocSVM did not return results on some data sets in   hours 

Dataset

iris
wine
glass
svmguide 
svmguide 
vehicle
vowel
segment
dna
satimage
usps
sector
pendigits
news 
letter
protein
shuttle
connect 
mnist
aloi
rcv 
covtype

Avg  Rank

mcSVM
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid 
 cid cid 
 cid cid 
 cid cid 
 cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid 
 cid 
 

mcODM       

 

ovaSVM
 cid cid 
 cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid 
 cid cid 
 

 

ovoSVM
 cid cid 
 cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
   
 cid cid 
 cid 
 

 

ecocSVM
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
 cid cid 
   
 cid cid 
   
   
 cid cid 
 cid cid 
 cid cid 
 cid cid 
   
   
 cid cid 
 

 

mcODM
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 cid 
 

 sharper  which prevents the instance with small margin 
so our method can still perform relatively well as the number of classes increases  which is also consistent with the
observation from Figure  

  Time Cost

We compare the single iteration time cost of our method
with mcSVM  ovaSVM  ovoSVM on all the data sets except aloi  on which ovoSVM could not return results in  
hours  All the experiments are performed with MATLAB
   on   machine with  cid  GHz CPUs and  GB
main memory  The average CPU time  in seconds  on each
data set is shown in Figure   The binary SVM used in
ovaSVM  ovoSVM and mcSVM are both implemented by
the LIBLINEAR  Fan et al    package  It can be seen
that for small data sets  the ef ciency of all the methods are
similar  however  for data sets with more than ten classes 
     sector and rcv  mcSVM and mcODM  which learn
all the scroing functions at once  are much faster than
ovaSVM and ovoSVM  owing to the inef ciency of binarydecomposition as discussed in Section   Note that LIBLINEAR are very fast implementations of binary SVM and
mcSVM  and this shows that our method is also computationally ef cient 

  Conclusions
Recent studies disclose that for binary class classi cation 
maximizing the minimum margin does not necessarily lead
to better generalization performances  and instead  it is
crucial to optimize the margin distribution  However  it
remains open to the in uence of margin distribution for
multiclass classi cation  We try to answer this question
in this paper  After maximizing the margin mean and minimizing the margin variance simultaneously  the resultant
optimization is   dif cult nondifferentiable nonconvex
programming  We propose mcODM to solve this problem
ef ciently  Extensive experiments on twenty two data sets
validate the superiority of our method to four versions of
multiclass SVMs  In the future it will be interesting to extend mcODM to more general learning settings       multilabel learning and structured learning 

Acknowledgements
This research was supported by the NSFC  
and the Collaborative Innovation Center of Novel Software
Technology and Industrialization  Authors want to thank
reviewers for helpful comments  and thank Dr  Wei Gao
for reading   draft 

MultiClass Optimal Margin Distribution Machine

Figure Frequency histogram of the margin distribution produced by mcSVM  ovaSVM and mcODM on data set segment as the
number of classes increase from two to seven 

Figure  Single iteration time cost of mcSVM  ovaSVM  ovoSVM and mcODM on all the data sets except aloi 

 Frequencymargin mcSVM ovaSVM mcODM  class Frequencymargin mcSVM ovaSVM mcODM  class Frequencymargin mcSVM ovaSVM mcODM  class Frequencymargin mcSVM ovaSVM mcODM  class Frequencymargin mcSVM ovaSVM mcODM  class Frequencymargin mcSVM ovaSVM mcODM  classiriswineglasssvmguide svmguide vehiclevowelsegmentdnasatimageuspssectorpendigitsnews letterproteinshuttleconnect mnistrcv covtype   CPU time  sec  mcSVM ovaSVM ovoSVM mcODMMultiClass Optimal Margin Distribution Machine

Ulrich        Kreel  Pairwise classi cation and support
vector machines  In Schlkopf     Burges           and
Smola         eds  Advances in Kernel Methods  Support Vector Machines  pp    Cambridge  MA 
December   MIT Press 

Vapnik     The Nature of Statistical Learning Theory 

SpringerVerlag  New York   

Wang        Sugiyama     Yang     Zhou       and
Feng       re ned margin analysis for boosting algorithms via equilibrium margin  Journal of Machine
Learning Research     

Weston     and Watkins     Multiclass support vector machines  In Proceedings of the European Symposium on
Arti cial Neural Networks  Brussels  Belgium   

Zhang     and Zhou       Large margin distribution machine  In Proceedings of the  th ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pp 
  New York  NY   

Zhang     and Zhou       Optimal margin distribution

machine  CoRR  abs   

Zhou       and Zhou       Large margin distribution
IEEE
learning with cost interval and unlabeled data 
Transactions on Knowledge and Data Engineering   
   

Zhou       Ensemble Methods  Foundations and Algo 

rithms  CRC Press  Boca Raton  FL   

Zhou       Large margin distribution learning 

In Proceedings of the  th IAPR International Workshop on Arti cial Neural Networks in Pattern Recognition  pp   
  Montreal  Canada   

References
Cortes     and Vapnik     Supportvector networks  Ma 

chine Learning     

Crammer     and Singer     On the algorithmic implementation of multiclass kernelbased vector machines  Journal of Machine Learning Research     

Crammer     and Singer     On the learnability and design of output codes for multiclass problems  Machine
Learning     

Dietterich        and Bakiri     Solving multiclass learning
problems via errorcorrecting output codes  Journal of
Arti cial Intelligence Research     

Fan        Chang        Hsieh        Wang        and
Lin        Liblinear    library for large linear classi 
cation  Journal of Machine Learning Research   
   

Freund     and Schapire          decisiontheoretic generalization of online learning and an application to boosting  In Proceedings of the  nd European Conference on
Computational Learning Theory  pp    Barcelona 
Spain   

Gao     and Zhou       On the doubt about margin explanation of boosting  Arti cial Intelligence   
 

Lei     rn Dogan  Binder     and Kloft     Multiclass svms  From tighter datadependent generalization
bounds to novel algorithms 
In Cortes     Lawrence 
      Lee        Sugiyama     and Garnett      eds 
Advances in Neural Information Processing Systems  
pp    Curran Associates  Inc   

McDiarmid     On the method of bounded differences 

Surveys in Combinatorics     

Mohri     Rostamizadeh     and Talwalkar     Foundations of Machine Learning  MIT Press  Cambridge  MA 
 

Reyzin     and Schapire        How boosting the margin can also boost classi er complexity  In Proceedings
of  rd International Conference on Machine Learning 
pp    Pittsburgh  PA   

Schapire        Freund     Bartlett        and Lee       
Boosting the margin    new explanation for the effectives
of voting methods  Annuals of Statistics   
   

Tseng     Convergence of   block coordinate descent
method for nondifferentiable minimization  Journal of
Optimization Theory and Applications   
 

