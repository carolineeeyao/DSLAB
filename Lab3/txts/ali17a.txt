  Semismooth Newton Method for Fast  Generic Convex Programming

Alnur Ali     Eric Wong        Zico Kolter  

Abstract

We introduce NewtonADMM    method for fast
conic optimization  The basic idea is to view
the residuals of consecutive iterates generated by
the alternating direction method of multipliers
 ADMM  as   set of  xed point equations  and
then use   nonsmooth Newton method to  nd  
solution  we apply the basic idea to the Splitting Cone Solver  SCS    stateof theart method
for solving generic conic optimization problems 
We demonstrate theoretically  by extending the
theory of semismooth operators  that NewtonADMM converges rapidly       quadratically  to
  solution  empirically  NewtonADMM is signi cantly faster than SCS on   number of problems  The method also has essentially no tuning parameters  generates certi cates of primal
or dual infeasibility  when appropriate  and can
be specialized to solve speci   convex problems 

  Introduction and related work
Conic optimization problems  or cone programs  are convex optimization problems of the form

minimize

cT  

subject to     Ax     

  Rn

 
where     Rn      Rm        Rm    are problem data 
speci ed by the user  and   is   proper cone  Nesterov  
Nemirovskii    BenTal   Nemirovski    Boyd  
Vandenberghe    we give   formal treatment of proper
cones in Section   but   simple example of   proper cone 
for now  is the nonnegative orthant       the set of all points
in Rm with nonnegative components  These problems are
quite general  encapsulating   number of standard problem
classes       taking   as the nonnegative orthant yields  
linear program  taking   as the positive semide nite cone 
 Equal contribution  Machine Learning Department  Carnegie
Mellon University  Computer Science Department  Carnegie
Mellon University 
Correspondence to  Alnur Ali  alnurali cmu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

     the space of       positive semide nite matrices Sm
   
yields   semide nite program  and taking   as the secondorder  or Lorentz  cone          Rm         cid   cid      
yields   secondorder cone program    quadratic program
is   special case 
Due  in part  to their generality  cone programs have been
the focus of much recent work  and additionally form the
basis of many convex optimization modeling frameworks 
     sdpsol  Wu   Boyd    YALMIP  Lofberg   
and the CVX family of frameworks  Grant    Diamond
  Boyd    Udell et al    These frameworks generally make it easy to quickly solve small and mediumsized convex optimization problems to high accuracy  they
work by allowing the user to specify   generic convex optimization problem in   way that resembles its mathematical
representation  then convert the problem into   form similar to   and  nally solve the problem  Primaldual interior point methods       SeDuMi  Sturm    SDPT 
 Toh et al    and CVXOPT  Andersen et al   
are common for solving these cone programs  These methods are useful  as they generally converge to high accuracy
in just tens of iterations  but they solve   Newton system
on each iteration  and so have dif culty scaling to highdimensional       largen  problems 
In recent work    Donoghue et al    use the alternating direction method of multipliers  ADMM   Boyd
et al    to solve generic cone programs  operator splitting methods       ADMM  PeacemanRachford splitting
 Peaceman   Rachford    DouglasRachford splitting  Douglas   Rachford    and dual decomposition 
generally converge to modest accuracy in just   few iterations  so the approach  called the splitting conic solver  or
SCS  is scalable  and also has   number of other bene ts 
     provding certi cates of primal or dual infeasibility 
In this paper  we introduce   new method  called  NewtonADMM  for solving largescale  generic cone programs
rapidly to high accuracy  The basic idea is to view the usual
ADMM recurrence relation as    xed point iteration  and
then use   truncated  nonsmooth Newton method to  nd  
 xed point  to justify the approach  we extend the theory
of semismooth operators  coming out of the applied mathematics literature over the last two decades  Mif in   
Qi   Sun    Mart nez   Qi    Facchinei et al 

  Semismooth Newton Method for Fast  Generic Convex Programming

  although it has received little attention from the machine learning community  Ferris   Munson    We
apply the approach to the  xed point iteration associated
with SCS  to obtain   general purpose conic optimizer  We
show  under regularity conditions  that NewtonADMM is
quadratically convergent  empirically  NewtonADMM is
signi cantly faster than SCS  on   number of problems 
Also  NewtonADMM has essentially no tuning parameters  and generates certi cates of infeasibility  helpful in
diagnosing problem misspeci cation 
The rest of the paper is organized as follows  In Section
  we give the background on cone programs  SCS  and
semismooth operators  required to derive our method for
solving generic cone programs  NewtonADMM    In Section   we present NewtonADMM  and establish some of
its basic properties  In Section   we give various convergence guarantees 
In Section   we empirically evaluate
NewtonADMM  and describe an extension as   specialized solver  We conclude with   discussion in Section  

  Background
We  rst give some background on cones  Using this background  we go on to describe SCS  the cone program solver
of   Donoghue et al    in more detail  Finally  we
give an overview of semismoothness  Mif in      generalization of smoothness  central to our Newton method 

  Cone programming
We say that   set   is   cone if  for all        and      
we get that         The dual cone    associated with the
cone    is de ned as the set      yT              
Additionally    cone   is   convex cone if  for all          
and         we get that                cone  
is   proper cone if it is     convex   ii  closed   iii  solid 
     its interior is nonempty  and  iv  pointed       if both
         then we get that      

The nonnegative orthant  secondorder cone  and positive
semide nite cone are all proper cones  Boyd   Vandenberghe    Section   these cones  along with the
exponential cone  de ned below  can be used to represent
most convex optimization problems encountered in practice  The exponential cone  see       Serrano   Kexp 
is   threedimensional proper cone  de ned as the closure
of the epigraph of the perspective of exp    with       
Kexp                                  exp     

                          

Cone programs resembling   were  rst described by Nesterov   Nemirovskii   page   although special
cases were  of course  considered earlier  Standard refer 

ences include BenTal   Nemirovski   and Boyd  
Vandenberghe   Section  

  SCS

Roughly speaking  SCS is an application of ADMM to
  particular feasibility problem arising from the KarushKuhn Tucker  KKT  optimality conditions associated with
  cone program  To see this  consider   reformulation of
the cone program   with slack variable     Rm 
cT   subject to Ax                

 

minimize
  Rn   

The KKT conditions can be seen  after introducing dual
variables     Rn         for the implicit constraint    
Rn and the explicit constraints  respectively  to be

AT          

 stationarity 

Ax                 primal feasibility 
              
 cT     bT        complementary slackness 

 dual feasibility 

where    is the dual cone of    thus  we can obtain   solution to   by solving the KKT system

   

 cid   

 cid 

   

   

   

   

AT
  
 
 
 cT  bT
 
    Rn                       

 
 

 

 

 

   

Selfdual homogeneous embedding  When the cone
program   is primal dual infeasible  there is no solution
to the KKT system   so  consider embedding the system
  in   larger system  with new variables     and solving

   

   

   

   

AT
  
 
 cT  bT
    Rn                                     

 
 
 

 

 
 

 
 

which is always solvable  The embedding   due to Ye
et al    has   number of other nice properties  Observe that when    cid       cid      are solutions to the embedding   we recover the KKT system   it turns out
that the solutions    cid   cid  characterize the primal or dual
 in feasibility of the cone program  
In particular  if
   cid       cid      then the cone program   is feasible 
with   primaldual solution    cid   cid    cid    cid    cid  on the
other hand  if    cid       cid      then   is primal or dual infeasible  or both  depending on the exact values of    cid   cid 
   Donoghue et al    Section   The embedding  
can also be seen as  rstorder homogeneous  in the sense
that    cid    cid     cid    cid    cid   cid  being   solution to   implies
that     cid    cid     cid    cid    cid   cid  for       is also   solution 
Finally  viewing the embedding   as   feasibility problem 
the dual of the feasibility problem turns out to be the original feasibility problem       the embedding is selfdual 

  Semismooth Newton Method for Fast  Generic Convex Programming

ADMMbased algorithm  As mentioned 
ding   can be viewed as the feasibility problem

the embed 

 nd     

subject to Qu                     

where we write     Rn                  

   

   

AT
  
 
 cT  bT

 
 
 

   

   

   

 
 

   

   

 
 

   

   

 
Introducing new variables          Rk  where         
and rewriting so that we may apply ADMM  we get 

minimize
            
subject to

IC             IQu cid   cid         

 cid 

 cid   

 

 

 cid    

  

 cid 

 

where IC    and IQu cid   cid  are the indicator functions of the
product space        and the af ne space of solutions to
Qu      respectively  after simplifying  see   Donoghue
et al    Section   the ADMM recurrences are just

                  
    PC       
               

 
 
 
where PC denotes the projection onto    For the update  
  is   skewsymmetric matrix  hence       is nonsingular 
so the update can be done ef ciently via the Schur complement  matrix inversion lemma  and LDLT factorization 

Projections onto dual cones  For the update   the projection onto   boils down to separate projections onto the
 free  cone Rn  the dual cone of    and the nonnegative orthant    These projections  for many    are wellknown 

  Free cone  Here  PRn          for     Rn 
  Nonnegative orthant  Kno  The projection onto Kno is
simply given by applying the positive part operator 

PKno      max     

 

  Secondorder cone  Ksoc  Write             
Rm       Rm          Then the projection is

 

PKsoc     

  
        cid   cid 
 

 cid   cid       
 cid   cid      
   cid   cid  otherwise 
 

 cid 

  Positive semide nite cone  Kpsd  The projection is

PKpsd      

where      cid 

tion of   

   iqiqT
 

max     qiqT
   

 

 

is the eigenvalue decomposi 

  Exponential cone  Kexp  If     Kexp  then PKexp      
   If        
exp  then PKexp         If          
     the  rst two components of   are negative  then
PKexp       max      max      Otherwise 
the projection is given by

argmin
       
subject to

 cid       cid 

 

    exp           

 

which can be computed using   Newton method
 Parikh   Boyd    Section  

The nonnegative orthant  secondorder cone  and positive
semide nite cone are all selfdual  so projecting onto these
cones is equivalent to projecting onto their dual cones  to
project onto the dual of the exponential cone  we use the
Moreau decomposition to get

exp           PKexp    

PK 

 

  Semismooth operators

Here  we give an overview of semismoothness  good references include Ulbrich   and Izmailov   Solodov
  We consider maps     Rk   Rk that are locally Lipschitz       for all      Rk  and             
where         is   ball centered at    with radius      
there exists some Lz      such that  cid              cid   
Lz cid        cid  By   result known as Rademacher   theorem  Evans   Gariepy    Section   Theorem  
we get that   is differentiable almost everywhere  we let
  denote the points at which   is differentiable  so that
Rk     is   set of measure zero 

The generalized Jacobian  Clarke   suggested the
generalized Jacobian as   way to de ne the derivative of
  locally Lipschitz map     Rk   Rk  at all points  The
generalized Jacobian is related to the subgradient  as well
as the directional derivative  as we discuss later on  the
generalized Jacobian  though  turns out to be quite useful
for de ning effective nonsmooth Newton methods  The
generalized Jacobian       at   point     Rk of   map
    Rk   Rk  is de ned as  co denotes convex hull 

 cid 
     zi     zi        zi     

        co

 cid 

lim

 
where   zi    Rk   is the usual Jacobian of   at zi  Two
useful properties of the generalized Jacobian  Clarke   

 

  Semismooth Newton Method for Fast  Generic Convex Programming

Proposition             at any    is always nonempty 
and  ii  if each component Fi is convex  then the ith row of
any element of       is just   subgradient of Fi at   

 Strong  semismoothness and consequences  We say
that   map     Rk   Rk is semismooth if it is locally
Lipschitz  and if  for all        Rk  the limit

lim

         

  

 

exists  see       Mif in   De nition   and Qi  
Sun   Section   The above de nition is somewhat
opaque  so various works have provided an alternative characterization of semismoothness    is semismooth if and
only if it is     locally Lipschitz   ii  directionally differentiable  in every direction  and  iii  we get

 cid                      cid 

   

lim

         

 cid cid 

      cid                      cid      cid cid         see 
     Qi   Sun   Theorem   Hinterm uller  
Theorem   Qi   Sun   page   and Mart nez  
Qi   Proposition   Examples of semismooth functions include log        all convex functions  and all
smooth functions  Mif in     Smieta nski    on the

other hand   cid    is not semismooth    linear combina 

tion of semismooth functions is semismooth  Izmailov  
Solodov    Proposition   Finally  we say that  
map is strongly semismooth if  under the same conditions
as above  we can replace   with

lim sup

         

 cid                      cid 

   

 cid cid 

 

         see
      cid                      cid      cid cid 
Facchinei et al    Proposition   and Facchinei  
Kanzow   De nition  

  NewtonADMM and its basic properties
Next  we describe NewtonADMM  our nonsmooth Newton method for generic convex programming  again  the basic idea is to view the ADMM recurrences       used by
SCS  as    xed point iteration  and then use   nonsmooth
Newton method to  nd    xed point  Accordingly  we let

                    

    PC       

   

       

      

which are just the residuals of the consecutive ADMM iterates given by       and                      multiplying by diag              to change coordinates gives

                    

    PC       

   

      

       

 

Now  we would like to apply   Newton method to    
but projections onto proper cones are not differentiable 
in general  However  for many cones of interest  they are
 strongly  semismooth  the following lemma summarizes 
Lemma   Projections onto the nonnegative orthant 
secondorder cone  and positive semide nite cone are all
strongly semismooth  see       Kong et al    Section
  Kanzow   Fukushima   Lemma   and Sun  
Sun   Corollary   respectively 

Additionally  we give the following new result  for the exponential cone  which may be of independent interest 
Lemma   The projection onto the exponential cone is
semismooth 

We defer all proofs to the supplement 
Putting the pieces together  the following lemma establishes that     de ned in   is  strongly  semismooth 
Lemma   When   
is
from the cone program  
the nonnegative orthant  secondorder cone  or positive
semide nite cone 
is
strongly semismooth  when   is the exponential cone  then
the map   is semismooth 

then the map     de ned in  

The preceding results lay the groundwork for us to use  
semismooth Newton method  Qi   Sun    applied to
    where we replace the usual Jacobian with any element
of the generalized Jacobian   however  as many have
observed  Khan   Barton    it is not always straightforward to compute an element of the generalized Jacobian 
Fortunately  for us  we can just compute   subgradient of
each row of     as the following lemma establishes 
Lemma   The ith row of each element of the generalized
Jacobian       at   of the map   is just   subgradient of
Fi                    at   
Using the lemma  an element           of the generalized Jacobian of the map         is then just

             

   

Ju
  

 

   

where

    

 

 

 

 

 
 
 

 
 
 

Ju  

 
 
 cid 

 
  JPK 
 

   JPK 
 

 
 
 cid 
 
is        dimensional matrix forming the second row of
    cid  equals   if              and   otherwise  and JPK   
Rm   is the Jacobian of the projection onto the dual cone
   Here and below  we use subscripts to select components           selects the  component of    from   and
we write   to mean      where                     

 

 

 

 
 
 

  Semismooth Newton Method for Fast  Generic Convex Programming

  Final algorithm
Later  we discuss computing JPK    the Jacobian of the projection onto the dual cone    for various cones    these
pieces let us compute an element    given in      
of the generalized Jacobian of the map     de ned in  
which we use instead of the usual Jacobian  in   semismooth Newton method  below  we describe   way to scale
the method to larger problems       values of   

Truncated  semismooth Newton method  The conjugate gradient method is  seemingly  an appropriate choice
here  as it only approximately solves the Newton system

        

 
with variable          unfortunately  in our case    is nonsymmetric  so we appeal instead to the generalized minimum residual method  GMRES   Saad   Schultz   
We run GMRES until

 cid        cid     cid   cid 

 

where   is the approximate solution from   particular iteration of GMRES  and   is   userde ned tolerance      
we run GMRES until the approximation error is acceptable 
After GMRES computes an approximate Newton step  we
use backtracking line search to compute   step size 
Now recall  from Section   that  cid      is always   trivial solution to the Newton system   due to homogeneity  so  we initialize the              components of   to  
which avoids converging to the trivial solution  Finally  we
mention that when    in the cone program   is the direct
product of several proper cones  then Ju  in   simply
consists of multiple such matrices  just stacked vertically 
We describe the entire method in Algorithm   The method
has essentially no tuning parameters  since  for all the experiments  we just    the maximum number of Newton
iterations       the backtracking line search parameters             and the GMRES tolerances
             for each Newton iteration    The cost of
each Newton iteration is the number of backtracking line
search iterations times the sum of two costs  the cost of
projecting onto   dual cone and the cost of GMRES      
  max       assuming GMRES returns early  Similarly  the cost of each ADMM iteration of SCS is the cost
of projecting onto   dual cone plus   max      

  Jacobians of projections onto dual cones

Here  we derive the Jacobians of projections onto the dual
cones of the nonnegative orthant  secondorder cone  positive semide nite cone  and the exponential cone  here  we
write JPK  to mean JPK      where      uy   vs   Rm 

Algorithm   NewtonADMM for convex optimization

Input  problem data     Rn      Rm        Rm 
cones    maximum number of Newton iterations    
backtracking line search parameters            
    GMRES approximation tolerances     
Output    solution to  
initialize                   and    
  
     
initialize               
for                 do

  avoids trivial solution

      

   

  

compute               
  see     Sec   
compute the Newton step          by approximately
solving                     using GMRES with
approximation tolerance    
initialize         
while  cid                 cid 
do

             cid        cid 

  initialize step size     

  see  

 

            

  for backtracking line search

end while
update                     

end for
return the uxdivided by the     components of      

Nonnegative orthant  Since the nonnegative orthant is
selfdual  we can simply  nd   subgradient of each component in   to get that JPK  is diagonal with  say   JPK   ii
set to   if  uy vs       and   otherwise  for                 
    
Secondorder cone  Write           
Rm          The secondorder cone is selfdual  as
well  so we can  nd subgradients of   to get that

 

 cid   cid       
 cid   cid      
  
   otherwise 

JPK   

 

where   is   lowrank matrix  details in the supplement 

Positive semide nite cone  The projection map onto the
 selfdual  positive semide nite cone is matrixvalued  so
computing the Jacobian is more involved  We leverage the
fact that most implementations of GMRES need only the
product JPK   vec    provided by the below lemma using
matrix differentials  Magnus   Neudecker    here 
vec is the vectorization of   real  symmetric matrix   
Lemma   Let       QT be the eigenvalue decomposition of    and let    be   real  symmetric matrix  Then

 vec   vec       vec cid dQ  max   QT
     max   QT     max   dQ   cid   

JPKpsd

where  here  the max is interpreted diagonally 
dQi    iiI       ZQi     max   ii     ii QT

 

 ZQi 

  Semismooth Newton Method for Fast  Generic Convex Programming

    denotes the pseudoinverse of    and    is the indicator function of the nonnegative orthant 

   For Theorem   we assume  for all         and

         and for some        that

Exponential cone  Recall  from   that the projection
onto the exponential cone is not analytic  so computing the
Jacobian is much more involved  as well  The following
lemma provides   Newton method for computing the Jacobian  using the KKT conditions for   and differentials 
         
Lemma   Let       
JPKexp     where

Then JPK 

exp

   

JPKexp      

   
diag           

    Kexp
      
         

exp

otherwise  JPKexp     is   particular     matrix given in the
supplement  due to space constraints 

  Convergence guarantees
Here  we give some convergence results for NewtonADMM  the method presented in Algorithm  
First  we show that  under standard regularity assumptions 
the iterates      
   generated by Algorithm   are globally convergent       given some initial point  the iterates
converge to   solution of           where   is   Newton iteration counter  We break the statement  and proof  of the
result up into two cases  Theorem   establishes the result  when the sequence of step sizes      
   converges to
some number bounded away from zero and one  Theorem
  establishes the result when the step sizes converge zero 
Below  we state our regularity conditions  which are similar
to those given in Han et al    Mart nez   Qi  
Facchinei et al    we elaborate in the supplement 

   For Theorem   we assume lim supi          
   For Theorem   we assume lim supi          
   For Theorem   we assume     that the GMRES approximation tolerances     are uniformly bounded by
      
  as in                  ii  that    
and  iii  that         cid        cid 

   For Theorem   we assume  for every convergent sei  satisfying assump 

           

quence      
tion     above  and    

       that

lim
    

 cid                cid 

     cid        cid 

 

   

  lim

                           

where  for notational convenience  we write

                       

  cid cid     cid          cid 

   For Theorem   we assume  for all                
          that  cid     cid       for some constant     
  and  ii  that every element of       is invertible 

 

The two global convergence results are given below  the
proofs are based on arguments in Mart nez   Qi  
Theorem     but we use fewer userde ned parameters 
and   different line search method 
Theorem
with
lim supi            for some           Assume
condition     stated above  Then limi             
Theorem
with
lim supi           Assume conditions            
and     stated above  Suppose the sequence      
converges to some          Then          

convergence 

convergence 

 Global

 Global

 

  

Next  we show  in Theorem   that when   is strongly
semismooth         is the nonnegative orthant  secondorder cone  or positive semide nite cone 
the iterates
     
   generated by Algorithm   are locally quadratically convergent  the proof is similar to that of Facchinei
et al    Theorem     for semismooth maps 
Theorem    Local quadratic convergence  Assume condition     stated above 
Then the sequence of iteri      generated by Algorithm   converges
ates      
quadratically  with           for large enough   
When   is the exponential cone         is semismooth  the
iterates generated by Algorithm   are locally superlinearly
convergent  Facchinei et al    Theorem    

  Numerical examples
Next  we present an empirical evaluation of NewtonADMM  on several problems  in these  we directly compare to SCS  which NewtonADMM builds on  as it is the
most relevant benchmark for us    Donoghue et al   
observe that  with an optimized implementation  SCS outperforms SeDuMi  as well as SDPT  We evaluate  for
both methods  the time taken to reach the solution as well
as the optimal objective value  we obtained these by running an interior point method  Andersen et al    to
high accuracy  Table   describes the problem sizes  for both
the cone form of   as well as the familiar form that the
problem is usually written in  Later  we also describe extending NewtonADMM to accelerate any ADMMbased
algorithm  applied to any convex problem  here  we compare to stateof theart baselines for speci   problems 

  Semismooth Newton Method for Fast  Generic Convex Programming

Table   Problem sizes  for the cone form        of   and the
familiar form        that the problem is usually written in 

PROBLEM

 

 

 

 

CONES

LINEAR PROG 
PORTFOLIO OPT 
LOGISTIC REG 
ROBUST PCA

 
 
 
 

 
 
 
 

 
 
 
 

Kno

 
Ksoc  Kno
 
  Kexp  Kno
Kpsd  Kno
 

  Random linear programs  LPs 

We compare NewtonADMM and SCS on   linear program

minimize

  Rp

cT  

subject to Gx           

where     Rp      RN        RN are problem
data  and the inequality is interpreted elementwise  To ensure primal feasibility  we generated   solution   cid  by sampling its entries from   normal distribution  then projecting onto the nonnegative orthant  we generated    with
            so   is wide  by sampling entries from
  normal distribution  then taking     Gx cid  To ensure dual
feasibility  we generated dual solutions  cid   cid  associated
with the equality and inequality constraints  by sampling
their entries from   normal and Uniform    distribution 
respectively  to ensure complementary slackness  we set
     GT  cid     cid  Finally  to put the linear program into
the cone form of   and hence   we just take

   

   

     

 

   

     

 

        Kno 

The  rst column of Figure   presents the time taken  by
both NewtonADMM and SCS  to reach the optimal objective value  as well as to reach the solution  we see that
NewtonADMM outperforms SCS in both metrics 

  Minimum variance portfolio optimization

We consider   minimum variance portfolio optimization
problem  see       Khare et al    Ali et al   

minimize

 Rp

    

subject to         

 

where  here  the problem data     Sp
  is the covariance
matrix associated with the prices of         assets  we
generated   by sampling   positive de nite matrix  The
goal of the problem is to allocate wealth across   assets
such that the overall risk is minimized  shorting is allowed 
Putting the above problem into the cone form of   yields 
for    the direct product of the secondorder cone and the
nonnegative orthant  details in the supplement  The second column of Figure   shows the results  we again see
that NewtonADMM outperforms SCS 

 Rp

minimize

   cid penalized logistic regression
We consider  cid penalized logistic regression      

 cid  
   log    exp yiXi     cid cid 

 
where  here      RN here is   response vector      RN  
is   data matrix  with Xi  denoting the ith row of    and
      is   tuning parameter  We generated       sparse
underlying coef cients  cid  by sampling entries from   normal distribution  then setting     of the entries to zero 
we generated    with         by sampling its entries
from   normal distribution  then set       cid      where
  is  additive  Gaussian noise  For simplicity  we set the
tuning parameter       Putting the above problem into
the cone form of   yields  for    the direct product of the
exponential cone and the nonnegative orthant  details in the
supplement  the problem size in cone form ends up being
large  see Table   In the third column of Figure   we see
that NewtonADMM outperforms SCS 

  Robust principal components analysis  PCA 

Finally  we consider robust PCA 

 cid   cid  subject to  cid   cid                  
minimize
    RN  
where  cid     cid  and  cid     cid  are the nuclear and elementwise  cid 
norms  respectively  and     RN           Cand es et al 
  Equation   We generated   lowrank matrix   cid 
with rank    
       sparse matrix   cid  by sampling entries
from Uniform    then setting     of the entries to
zero  and  nally set       cid      cid  We set       The
goal is to decompose the obsevations   into lowrank  
and sparse   components  Putting the above problem into
the cone form of   yields  for    the direct product of the
positive semide nite cone and nonnegative orthant  details
in the supplement  We see that NewtonADMM and SCS
are comparable  in the fourth column of Figure  

  Extension as   specialized solver

Finally  we observe that the basic idea of treating the residuals of consecutive ADMM iterates as    xed point iteration  and then  nding    xed point using   Newton method 
is completely general       the same idea can be used to accelerate  virtually  any ADMMbased algorithm  for   convex problem  To illustrate  consider the lasso problem 

 cid       cid 

     cid cid 

minimize

 Rp

 
where     RN       RN          the ADMM recurrences  Parikh   Boyd    Section   are

                             
          
             

 
 
 

  Semismooth Newton Method for Fast  Generic Convex Programming

Figure   Comparison of NewtonADMM and SCS    Donoghue et al    on several convex problems  Columns  from left to right 
linear programming  portfolio optimization   cid penalized logistic regression  robust PCA  Top row  wallclock time vs  logdistance to
the optimal objective value  obtained by running an interior point method  Bottom row  wallclock time vs  logdistance  in   Euclidean
norm sense  to the solution  Each plot is one representative run out of    the variance was negligible  Best viewed in color 
where             Rp are the tuning parameter and
auxiliary variables  introduced by ADMM  respectively 
and    is the softthresholding operator  The map
               from   with components set to the
residuals of the ADMM iterates given in       is then

                               

          

   

     

       

where                  and we also changed coordinates  similar to before  An element           of the
generalized Jacobian of   is then

               

  
  

 
 

   

   

  
 
 

where     Rp   is diagonal with Dii set to   if         
  and   otherwise  for                 
In the left panel of Figure   we compare   specialized
NewtonADMM applied directly to the lasso problem  
with the ADMM algorithm for         proximal
gradient method  Beck   Teboulle    and   heavilyoptimized implementation of coordinate descent  Friedman
et al    we set                        
Here  the specialized NewtonADMM is quite competitive with these strong baselines  the specialized NewtonADMM outperforms NewtonADMM applied to the cone
program   so we omit the latter from the comparison 
Stella et al    recently described   related approach 
In the right panel of Figure   we present   similar comparison  for sparse inverse covariance estimation  with the
QUIC method of Hsieh et al    NewtonADMM
clearly performs best                       
details in the supplement 

Figure   Left  wallclock time vs  logdistance to the optimal
objective value  on the lasso problem  for the specialized NewtonADMM method  standard ADMM    proximal gradient method 
and   heavilyoptimized coordinate descent implementation  as
  reference benchmark  Right  for   sparse inverse covariance
estimation problem  with specialized NewtonADMM  standard
ADMM  and QUIC  Hsieh et al    Best viewed in color 

  Discussion
We introduced NewtonADMM    new method for generic
convex programming  The basic idea is use   nonsmooth
Newton method to  nd    xed point of the residuals of the
consecutive ADMM iterates generated by SCS    stateof 
theart solver for cone programs  we showed that the basic idea is fairly general  and can be applied to accelerate
 virtually  any ADMMbased algorithm  We presented theoretical and empirical support that NewtonADMM converges rapidly       quadratically  to   solution  outperforming SCS across several problems 

Acknowledgements  AA was supported by the DoE
Computational Science Graduate Fellowship DEFG 
 ER  EW was supported by DARPA  under award
number FA  We thank PoWei Wang and
the referees for   careful proofreading 

 Seconds SuboptimalitySCSNewtonADMM Seconds SuboptimalitySCSNewtonADMM Seconds SuboptimalitySCSNewtonADMM Seconds SuboptimalityNewtonADMMSCS Seconds Estimation errorSCSNewtonADMM Seconds Estimation errorSCSNewtonADMM Seconds Estimation errorSCSNewtonADMM Seconds Estimation errorNewtonADMMSCS Seconds SuboptimalityProximal gradient methodCoordinate descentADMMNewtonADMM Seconds SuboptimalityQUICADMMNewtonADMM  Semismooth Newton Method for Fast  Generic Convex Programming

References
Ali  Alnur  Khare  Kshitij  Oh  SangYun  and Rajaratnam 
Bala  Generalized pseudolikelihood methods for inverse
covariance estimation  Technical report    Available
at http arxiv org pdf pdf 

Andersen  Martin  Dahl  Joachim  Liu  Zhang  and Vandenberghe  Lieven 
Interior point methods for largescale
cone programming  Optimization for machine learning 
pp     

Beck  Amir and Teboulle  Marc    fast iterative shrinkagethresholding algorithm for
inverse problems 
SIAM Journal on Imaging Sciences     

linear

BenTal  Aharon and Nemirovski  Arkadi  Lectures on
Modern Convex Optimization  Analysis  Algorithms 
and Engineering Applications  SIAM   

Boyd  Stephen and Vandenberghe  Lieven  Convex Opti 

mization  Cambridge University Press   

Boyd  Stephen  Parikh  Neal  Chu  Eric  Peleato  Borja  and
Eckstein  Jonathan  Distributed optimization and statistical learning via the alternating direction method of multipliers  Foundations and Trends in Machine Learning 
   

Cand es  Emmanuel  Li  Xiaodong  Ma  Yi  and Wright 
Journal

John  Robust principal component analysis 
of the ACM     

Clarke  Frank  Optimization and Nonsmooth Analysis 

SIAM   

Diamond  Steven and Boyd  Stephen  CVXPY    Pythonembedded modeling language for convex optimization 
Journal of Machine Learning Research   
 

Douglas  Jim and Rachford  Henry  On the numerical solution of heat conduction problems in two and three space
variables  Transactions of the American Mathematical
Society     

Evans  Lawrence and Gariepy  Ronald  Measure Theory

and Fine Properties of Functions  CRC Press   

Facchinei  Francisco and Kanzow  Christian    nonsmooth inexact Newton method for the solution of largescale nonlinear complementarity problems  Mathematical Programming     

Facchinei  Francisco  Fischer  Andreas  and Kanzow 
Inexact Newton methods for semismooth
inequality

Christian 
equations with applications to variational
problems   

Ferris  Michael and Munson  Todd  Semismooth support
vector machines  Mathematical Programming   
   

Friedman  Jerome  Hastie  Trevor      ing  Holger  and
Tibshirani  Robert  Pathwise coordinate optimization 
The Annals of Applied Statistics     

Grant  Michael  Disciplined Convex Programming  PhD

thesis  Stanford University   

Han  ShihPing  Pang  JongShi  and Rangaraj  Narayan 
Globally convergent Newton methods for nonsmooth
equations  Mathematics of Operations Research   
   

Hinterm uller  Michael 

Semismooth Newton methods
and applications  Technical report    Available at
http www math unihamburg de home 
hinze Psfiles Hintermueller OWNotes 
pdf 

Hsieh  ChoJui  Sustik    aty as  Dhillon  Inderjit  and
Ravikumar  Pradeep  QUIC  Quadratic approximation
for sparse inverse covariance estimation  Journal of Machine Learning Research     

Izmailov  Alexey and Solodov  Mikhail  NewtonType
Methods for Optimization and Variational Problems 
Springer   

Kanzow  Christian and Fukushima  Masao  Semismooth
methods for linear and nonlinear secondorder cone programs  Technical report   

Khan  Kamil   and Barton  Paul  Generalized derivatives
IEEE Transactions on Automatic

for hybrid systems 
Control   

Khare  Kshitij  Oh  SangYun  and Rajaratnam  Bala   
convex pseudolikelihood framework for high dimensional partial correlation estimation with convergence
guarantees  Journal of the Royal Statistical Society  Series       

Kong  Lingchen  Tunc el  Levent  and Xiu  Naihua  Clarke
generalized Jacobian of the projection onto symmetric
cones  SetValued and Variational Analysis   
   

Lofberg  Johan  YALMIP    toolbox for modeling and
optimization in MATLAB  In   IEEE International
Symposium on Computer Aided Control Systems Design 
pp    IEEE   

Magnus  Jan and Neudecker  Heinz  Matrix Differential
Calculus with Applications in Statistics and Econometrics  John Wiley   Sons   

  Semismooth Newton Method for Fast  Generic Convex Programming

Mart nez  Jos   and Qi  Liqun  Inexact Newton methods for
solving nonsmooth equations  Journal of Computational
and Applied Mathematics     

Sun  Defeng and Sun  Jie  Semismooth matrixvalued functions  Mathematics of Operations Research   
   

Toh  KimChuan  Todd  Michael  and   ut unc    Reha  On
the implementation and usage of SDPT      MATLAB software package for semide nite quadratic linear
In Handbook on Semide 
programming  version  
nite  Conic  and Polynomial Optimization  pp   
Springer   

Udell  Madeleine  Mohan  Karanveer  Zeng  David  Hong 
Jenny  Diamond  Steven  and Boyd  Stephen  Convex
In Proceedings of the  st First
optimization in Julia 
Workshop for High Performance Technical Computing
in Dynamic Languages  pp    IEEE   

Ulbrich  Michael  Semismooth Newton Methods for Variational Inequalities and Constrained Optimization Problems in Function Spaces  SIAM   

Wu  ShaoPo and Boyd  Stephen  sdpsol    parser solver
for semide nite programs with matrix structure  Advances in Linear Matrix Inequality Methods in Control 
pp     

nL iteration homogeneous and selfdual

 
Ye  Yinyu  Todd  Michael  and Mizuno  Shinji  An
linear
  
programming algorithm  Mathematics of Operations Research     

Mif in  Robert  Semismooth and semiconvex functions in
constrained optimization  SIAM Journal on Control and
Optimization     

Nesterov  Yurii and Nemirovskii  Arkadii 

Interior Point
Polynomial Algorithms in Convex Programming  SIAM 
 

  Donoghue  Brendan  Chu  Eric  Parikh  Neal  and Boyd 
Stephen  Conic optimization via operator splitting and
Journal of Optihomogeneous selfdual embedding 
mization Theory and Applications  pp     

Parikh  Neal and Boyd  Stephen  Proximal algorithms 
Foundations and Trends in Optimization   
 

Peaceman  Donald and Rachford  Henry  The numerical
solution of parabolic and elliptic differential equations 
Journal of the Society for Industrial and Applied Mathematics     

Qi  Liqun and Sun  Defeng    survey of some nonsmooth

equations and smoothing Newton methods   

Qi  Liqun and Sun  Jie    nonsmooth version of Newton  
method  Mathematical Programming   
 

Saad  Youcef and Schultz  Martin  GMRES    generalized minimal residual algorithm for solving nonsymmetric linear systems  SIAM Journal on Scienti   and Statistical Computing     

Serrano  Santiago  Algorithms for Unsymmetric Cone Optimization and an Implementation for Problems with the
Exponential Cone 
PhD thesis  Stanford University 
 

 Smieta nski  Marek    generalized Jacobian based Newton method for semismooth block triangular system of
equations  Journal of Computational and Applied Mathematics     

Stella  Lorenzo  Themelis  Andreas  and Patrinos  Panagiotis  Forwardbackward quasiNewton methods for
nonsmooth optimization problems  Technical report 
  Available at https arxiv org pdf 
 pdf 

Sturm  Jos  Implementation of interior point methods for
mixed semide nite and second order cone optimization
problems  Optimization Methods and Software   
   

