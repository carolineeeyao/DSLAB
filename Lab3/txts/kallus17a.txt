Recursive Partitioning for Personalization using Observational Data

Nathan Kallus  

Abstract

We study the problem of learning to choose from
  discrete treatment options       news item or
medical drug  the one with best causal effect for
  particular instance       user or patient  where
the training data consists of passive observations
of covariates  treatment  and the outcome of the
treatment  The standard approach to this problem is regress and compare  split the training data
by treatment       regression model in each split 
and  for   new instance  predict all   outcomes
and pick the best  By reformulating the problem
as   single learning task rather than   separate
ones  we propose   new approach based on recursively partitioning the data into regimes where
different treatments are optimal  We extend this
approach to an optimal partitioning approach that
 nds   globally optimal partition  achieving  
compact  interpretable  and impactful personalization model  We develop new tools for validating and evaluating personalization models on observational data and use these to demonstrate the
power of our novel approaches in   personalized
medicine and   job training application 

  Introduction
Personalization is the problem of determining the best
treatment option for   given instance    treatment can  for
example  be   movie recommendation  Zhou et al   
  display ad  Goldfarb   Tucker    or   pharmacological therapy  Lesko    and an instance is usually an
individual person  In this paper  we study the problem of
learning how to personalize from observational data  which
is an important problem in emergent contexts such as personalized medicine 
In this and related contexts  experimentation can be prohibitively smallscale  costly  dangerous  and unethical in comparison to passive data collection 

 School of Operations Research and Information Engineering
and Cornell Tech  Cornell University  Correspondence to  Nathan
Kallus  kallus cornell edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

which can be potentially massive as in electronic medical records  EMRs  but  at the same time  lack experimental manipulation so that isolated causal effects of speci  
treatments are hidden by confounding factors  We show
that standard approaches that pose the problem as multiple
supervised learning tasks fall short in this setting and propose new learning algorithms as well as evaluation methods
used for validation  selection  and tuning 
Speci cally  we consider the problem of learning how to
assign the best of   treatments to an instance  given an
observation of associated baseline covariates     Rd  An
instance is characterized by the random variables     Rd
and                        which denote the covariates and
the   potential outcomes of applying each of the treatments  Imbens   Rubin    Chs    We use the
convention that smaller outcome is better    personalization model is   map     Rd                     
that  given an observation of covariates    prescribes  
treatment      
Its  outof sample  personalization risk
is its average causal effect in the population       
            the expectation is taken with respect to the
joint distribution of                      The Bayes optimal risk is         where              
arg mint                      is the Bayes optimal personalization model 
The learning task is to train   personalization model    
on   data points  Sn                      Xn  Tn  Yn   
where the observed outcome Yi   Yi Ti  corresponds only
to the treatment Ti administered  This data is observational  we may not control the historic administration of
treatment  as we would in   controlled experiment  and
the values Yi    for    cid  Ti are missing data  We assume
the data is independent and identically distributed  iid  and
let                            represent   generic draw  Although the data is iid  the ttreated sample      Ti      differs systematically from the sample   cid treated      Ti     cid 
for    cid    cid       not just by chance as in   randomized controlled trial  RCT  Our second assumption about the data
is unconfoundedness 
Assumption   For each               is independent of
  given   and       is possible for almost every        
              and                         

The assumption is standard in causal effect estimation

Recursive Partitioning for Personalization using Observational Data

 see      Kallus        for ensuring identi ability
 Rosenbaum   Rubin    It says that we measured the
right covariates to separate the effect of the treatment itself
from the effect of assignment 
In the context of personalized medicine  this assumption would be justi ed if the
EMR contained all the patient information used by   doctor
to prescribe treatment up to the vagaries and idiosyncrasies
of individual doctors or hospitals  Under Asn    the conditional causal effect is equal to regressing   on       
                                            
                        

  Standard Approach  Regress and Compare
Since under Asn    the optimal model   chooses   treatment by minimizing among   regression functions  one
obvious approach to personalization is to estimate these
regression functions   tting each to the subset of the data
that received each treatment  and then use these to predict outcomes and pick the smallest prediction  For example  in medicine  there is   vast literature on predicting patientspeci   responses to treatment  Feldstein et al 
  Stoehlmacher et al    and picking the best by
comparing  Qian   Murphy    Bertsimas et al   
The same approach is also generally taken in the contextual multiarm bandit problem  Li et al    which is
similar to our problem with the differences that we consider an of ine learning problem and that bandit arm pulls
are controlled interventions    bandit problem is essentially
  dynamic RCT  so the treated subpopulations are always
statistically equivalent  The standard solution is to      regression functions  and  for   new instance  predict   outcomes and pick the smallest prediction subject to cleverly
ensuring suf cient exploration by       adding con dence
bounds that vanish with    The regression  assumed linear 
is done using ridge regression as in Li et al     LinUCB  ordinary least squares  OLS  as in Goldenshluger  
Zeevi   or LASSO as in Bastani   Bayati  
The regress and compare       approach to personalization from observational data can be summarized as 
  For each        
    Consider the ttreated subsample St nt    Xi  Yi   

        Ti      of size nt  cid  

   Ti     

    Fit   regression model    nt    of the response   to
regressors   using training data St nt       by OLS 
 Note that separate OLS on each subsample St nt is equivalent to OLS on the whole sample if we include interaction terms
with dummy variables    Ti      At the same time  OLS on the
whole sample without interaction terms provides no personalization            
    is constant  Similarly  separate nonparametric
regressions on each subsample is equivalent to using the whole
sample and endowing the   variable with   discrete topology 

 

  

  Personalize by choosing the best predicted outcome 

     
 

      arg mint       nt   

Under Asn    if our regression estimator is consistent then
so is     personalization consistent as shown below  All
proofs are given in the supplementary materials 
Theorem   If Asn    holds and    nt    are pointwise
consistent regressions          nt                      
almost surely                then      
           
eventually     

 

Examples of pointwise consistent regression estimators are
knearest neighbors  kNN  and kernel regression  Walk 
 
If   linear model is wellspeci ed  then OLS is
also pointwise consistent 
In practice  however      is
not effective for personalization because it attempts to learn
much more than it needs to  it splits the training data into
   and in training it addresses estimation or prediction risk
rather than personalization risk 

  Other Related Problems and Approaches

In learning heterogeneous causal effects  one is concerned
with the case of observational data with two treatments     
   Control  and        Treatment  and the estimation
of the relative conditional average treatment effect  CATE 
                            Under Asn    CATE is
the difference between two regression functions       
                                       And so one
way to estimate it is by regressing outcome in each treatment population and taking differences  When the conditioning variables in CATE are   proper subset of the covariates needed to satisfy Asn    Abrevaya et al   
propose estimates based on propensityscore weighting and
kernel regression  Athey   Imbens   develop the
Causal Tree  CT  which uses recursive partitioning  as an
alternative to differencing two CART regressions 
For personalization  learning heterogeneous effects can be
used to choose between two treatments by comparing an
estimate of their relative CATE to zero  As   learning problem  however  this addresses estimation risk rather than personalization risk and deals only with two treatments 
In
Sec    we propose onevs one and onevs all strategies
for personalization using CATE estimates and show it is
consistent  We compare to this strategy using CT in our
empirical investigation 
In learning from logged bandit feedback  Beygelzimer
  Langford    Swaminathan   Joachims       
Kallus      one is concerned with learning   good policy for   contextual multiarm bandit problem based on
logged data from another  known policy  rather than online interactions  This problem differs in that it assumes
the data is experimental and the policy that generated the
data is known and available  In Sec    we discuss adapt 

Recursive Partitioning for Personalization using Observational Data

ing these methods using imputed estimated propensities  to
which we compare in our empirical investigation 

  Recursive Partitioning for Personalization
In this section we present three new algorithms that tackle
personalization directly as   single learning task 

  Recasting the Problem

We begin by reformulating personalization risk  Following Hirano   Imbens   Def    we de ne the generalized propensity score  GPS  as            where
                           The GPS of subject    Qi  is
an unknown quantity given by taking the unknown       
and plugging in the known variables Ti  Xi  Using the GPS
we can relate the personalization risk of   personalization
model   to its accuracy as   classi cation model for labels
    weighted by outcome and GPS 
Theorem   Under Asn   

                              

 

For       and randomized data          constant 
Zhao et al    is   special case of Thm    Thm   
suggests using   weighted form of empirical classi cation
risk minimization  When   is fully known as in the logged
bandit setting  this approach is closely related to the approach taken by Beygelzimer   Langford   Swaminathan   Joachims       In the observational setting 
we explore estimating and imputing   to use this approach
in Sec    However  because estimating the GPS generally either relies heavily on model speci cation or  in nonparametric settings  can be biased and variable  this will
lead to severe instability and limited practical use  Moreover  it does not address the personalization problem as  
single learning task  rather as two  learning   propensity
model task and then   weighted classi cation task  Instead 
in the following sections we present   singletask approach
that does not rely on estimating propensities separately 

  An Impurity Measure for Personalization

Classi cation and regression trees  CART  are predictive models based on recursive partitioning  the covariate
space is recursively partitioned by axisaligned hyperplanes
   cid      for  cid        and        in order to minimize  
withinpartition impurity measure  Breiman et al   
Impurities for classi cation include entropy and Gini and
for regression include sum of squared errors  Athey  
Imbens   develop impurities for estimating heterogeneous effects  Motivated by Thm    we develop an impurity for personalization leading to   recursive partitioning
algorithm called personalization tree  PT 

Figure   Personalization Tree for Warfarin Dosing  Sec   
from each node  the  rst arrow clockwise starting from noon corresponds to  No  and the other to  Yes 

Note that for   subset     Rd such that                
we have that                             whenever    
    To develop the personalization impurity  we use this to
establish the following as   corollary of Thm   
Corollary   Consider    xed partition of the covariate
space  Rd       XL where   cid     cid cid      whenever
 cid   cid   cid cid  Suppose that the partition is suf ciently  ne so that

              cid   cid                

 

Then   Rn       cid  
 cid  

 Rn        

  

 cid  

  

  Xi    
 

sistent estimator for      where

 Rn   cid      is an unbiased and con 

 cid 

 cid  

  Xi    Ti   Xi  
  cid 

  Xi cid    Ti cid     Xi   

Note that the conditional independence in the condition  
which requires that leaf membership be   balancing score
as de ned by Rosenbaum   Rubin   holds trivially
when we condition on   itself  Therefore  the  ner the
partition           XL  the more accurate this assumption 
 while possibly not truly satis able by any  nite partition 
Given   partition  we can use this risk estimate to optimize
  Letting    cid     Xi  Ti  Yi    Xi     cid  we can rewrite

min   Rd      cid  

 cid 

 Rn   cid     cid  

 cid  Ipers     cid 

where Ipers is the personalization impurity given by
Ipers Xi  Ti  Yi           Xik   Tik   Yik  
  cid  
 cid  

  mint   

  

  Tij    Yij  
  Tij    

 

  

Therefore  to achieve good personalization risk we may
wish to seek partitions that have minimal sum of withinpartition personalization impurities as de ned by Ipers 

  Personalization Tree

The PT algorithm attempts to  nd    ne partition of the
data to minimize the sum of withinpartition personalization impurities  It does so by partitioning the dataset along
axisaligned cuts  at each stage choosing to cut   partition
into the two partitions with least sum of impurities and proceeding recursively  In an attempt to  nd    ne partition

Weight kg VKORC     Low Diabe   Med VKORC     High CYP   High Med Valve replacement Weight kg VKORC     Takes carbamazepine VKORC       Med High Low Med High Med Start Recursive Partitioning for Personalization using Observational Data

that satis es condition   PT continues to recurse until
  speci ed stopping criterion  One criterion is that there
be at least nminleaf     subjects of each treatment in the
partition  Another criterion may be   maximum recursion
depth  max  but this criterion is not necessary  We also allow for   limited number  features of features to be sampled
as candidate cut dimensions  We summarize the PT recursive subroutine as Alg    The PT algorithm is given by
passing the whole dataset Sn and initial depth       to
Alg    The PT algorithm is notable for producing an interpretable decision tree for the personalization rule  Fig   

  Personalization Forest

The  ner the partition produced by PT  the closer we are
to condition   and the less bias the estimate of risk has 
The coarser the partition  the less variance the estimate has 
Therefore  there is an inherent tradeoff to the  neness parameters in PT  To address this we can bag  bootstrap aggregate  many very  ne PTs  which will have the effect of
reducing variance without incurring too much bias as in the
case of random forests  Breiman    The corresponding personalization forest  PF  algorithm is summarized in
  to achieve suf 
Alg    Generally  we set  features to
cient independence between trees for variance reduction 

 

  Optimal Personalization Tree

PT is   greedy algorithm for minimizing personalization
impurity  In this section we propose the optimal personalization tree  OPT  algorithm  which solves the global problem of  nding partitions that minimize the sum of withinpartition personalization impurities 

 cid 

min

Ipers Xi  Ti  Yi    Xi     cid   
  XL Rd 
where   is the restriction that           XL be disjoint regions de ned by the leaves of   binary decision tree  In
the case of classi cation and regression  Bennett   Blue
  Bertsimas   Dunn   attempt to  nd globally optimal prediction trees  while the problem is NPhard 
see Hya     Rivest    For our personalization problem  motivated by Bertsimas   Dunn   we propose  
mixedinteger programming  MIP  approach to the optimal
personalization tree problem  
We consider    xed binary tree structure on nodes              
Let Ap        be the unique path from the root to node
        its ancestors  For     Ap  let Rpq     if the
right branch is taken to reach   from    otherwise  
Let                                    be the set of
 Note that Ipers     is only de ned when there is at least  
subject for each treatment in the partition     An alternative appropriate for scarce data and large   allows for any number of
subjects but chooses the best treatment only from among those
with at least nminleaf subjects in the partition 

  cid 

input  Data part       cid Xi    Ti    Yi             Xik   Tik   Yik  cid  cur 

rent depth   tuning parameters nminleaf   max   features 

for  cid        do sort the data along   cid  Xi cid   cid            Xi cid   cid 
  Tij     

  Tij     Yij  cid  

Algorithm   PT subroutine
 cid  
 cid  

  

  

  Tij        nminleaf then

Set    cid       cid cid        cid     
Draw  cid           cid features at random from     without replacement 
for  cid     cid           cid features do

  

   cid  

  

  

            SL

      SL
  Tij      SR

      kL    
  Tij     Yij   kR     

            kL
Set kL
Set kR
for            do
Update kL  kR      Ti cid      kL
   Yi cid     
   kL
    kR mint    SR
kmin   mint    min kL
    kR
   

SL
   Yi cid      SR
Set     kL mint    SL
if        cid  and kmin   nminleaf then set    cid        cid cid     cid    cid      

    kR

  

   kR
   

   cid  

 
  Set            arg mint   
if      max and mint   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  end if

end for

end if

output          

end for
if    cid      then
Set  SL    Xi cid cid     
 SR    Xi cid cid     
   SL   Alg     SL           SR   Alg     SR       
 cid     
            if   cid cid     cid  then    SL     else    SR    

   Xi cid cid     cid    Xi cid cid     cid 

  Ti cid cid     
  Ti cid cid     

  Yi cid cid     
  Yi cid cid     

              cid 
      cid            

  

  

 cid  

     ip subject to

leaf nodes and let LC         be the nonleaf nodes 
Let Cp           be the  nite set of potential cuts at
each nonleaf node     LC  where  cid      Cp denotes that the cut   cid      is to be considered at node
 Usually we take   to be the data midpoints along
  
dimension  cid  Let       Yi   minj    Yj    max  
   Ti       
let          cid 
maxi      and       max maxt   
    nminleaf  For   vector   with index set             
 cid      Xi cid       cid  For     LC 
let kp    cid log   Cp cid  and Zp      kp Cp  be such that
 Zp ij     if  cid     cid  is odd and otherwise   Our MIP
minimize  cid  
 cid 
formulation of the OPT problem   follows 
wip      cid 
 cid 
  Ti   wip   nminleaf
 cid 
 cid 
 cid 
Problem   is   MIP with        cid 

    Rpq     Cq 
              
        
   
                  
                  
      
   
 ip   wipY             pt                 
   
 ip   wipY         pt                        
  LC log   Cp  binary variables  The variables    encode choice of cut at
each node   and constraint     ensures only one is cho 

                            RL
        Cp           kp   Zp             LC
wip    Rpq

   
   
   
    Rpq     Cq                      Ap    
   

 ip     maxwip   ip     
 ip          max    wip 

           

      pt    

   Rpq

  Ap

  Ti  

  Ti  

 

Recursive Partitioning for Personalization using Observational Data

Algorithm   PF

input  Data Sn                      Xn  Tn  Yn  tuning paramefor          do
Draw     

     cid Xi    Ti    Yi             Xin   Tin   Yin  cid  at random

ters    nminleaf   max   features 

 
 

from Sn with replacement 
    Alg       

Set      

 
  end for

      nminleaf   max   features 

output          mode   

                    

     

sen  see   ld     Vielma    The variable wip encodes
membership of datapoint   to leaf   and constraints    
    enforce that wip is the product of indicators of whether
Xi goes in the left or right branch of the ancestor nodes 
Since these constraints are integral  Ahuja et al    we
need not enfoce wip be binary  Constraint     ensures at
least nminleaf samples per leaf  The variable    encodes the
mean outcome of the prescribed treatment in leaf   and the
variable  ip encodes its product with wip  as ensured by
constraints       The variable  pt encodes the choice of
treatment   in leaf   and constraint     ensures only one is
chosen  Constraints       ensure the consistency between
the choice of treatment  pt and the mean outcome     We
summarize the OPT algorithm for   complete binary tree in
Alg    We use Gurobi to solve MIP   and use PT as  
heuristic warm start  randomly splitting leaf nodes at depth
less than  

  Adapting Existing Methods to Personalization

Using Observational Data

As discussed earlier  methods that estimate CATE  notably CT  Athey   Imbens    can be used to
choose between two treatments by comparing      
                                         
to zero 
However  such methods are directed at estimation rather
than personalization and only address two treatments  To
address the latter  we propose onevs all  vA  and onevs 
one     strategies for personalization 
For  vA  for each         we learn an estimate  tvA
      of
 tvA                                              cid    
by applying   base algorithm       CT  to the modi ed
     Xi         Ti        Yi            then
dataset StvA
we assign the treatment that does the best compared to the
rest     vA
 cid    we learn an estimate
For    
    of  tvs                            
 tvs
                     on the modi ed data
nt ns
subset
   Xi         Ti        Yi    Ti          then we
Stvs
nt ns
either assign the treatment that does the best compared to
the worst        
   
or the one that gets the most votes in oneto one comparisons        
 

      arg mint    mins   

        arg mint   

      arg maxt   

  cid tvs

for each  

 tvA   

 tvs
nt ns

 cid 

       

 cid 

 

 

  cid  

nt ns

Algorithm   OPT  complete binary tree 

input  Data Sn                      Xn  Tn  Yn  tuning parame 

ters nminleaf     features   cuts 
Rpq    cid   cid log   cid cid 

  Set       LC     Ap  cid cid     cid         cid 
for  cid        do sort the data along   cid  Xi cid   cid            Xi cid   cid 
for                 do
Draw Fp       with  Fp     features  Set        cid    
Set Cp    cid 

     cid    Fp        

Xi cid   cid Xi cid   cid 

 cid              

 
 
 
 

 cuts

 
  end for
  Find     that solve problem  

 

output  Personalization model       that proceeds as follows 
Set       for       do set  cid      inf     Cp           
              cid      return inf            pt    

We can prove that each of our  vA and     proposals are
consistent given pointwise consistent estimates of CATE 
Theorem   Let Asn    hold  Then 
         tvA        
  If  tvA
              eventually     
   vA
       tvs        

        
 cid    

then

then

  

nt ns
          

            eventually     

  If  tvs
      
 

 

Note that  vA and     with CT do not inherit trees  interpretability because the partitions of the  vX models may
not overlap 
POEM and NPOEM  Swaminathan   Joachims       
solve the problem of learning from logged bandit feedback 
assuming access to the logging policy that generated the
data  To adapt these to personalizing from observational
data  we propose to impute the logging policy using estimated GPS       pretend the data were generated by the
policy that assigns   when context is   with probability
         where    is   probabilistic classi cation model  tted to the data  Xi  Ti            We call these IPOEM
and INPOEM 

  Submatching for Validating

Personalization using Observational Data

In this section we discuss how one can evaluate and validate
personalization policies  such as the ones from the last section  Usually    new policy would be evaluated using   randomized controlled trial  but these can be infeasibly costly 
We consider how to evaluate   personalization policy using
observational data  Such   dataset can be   subset removed
from the training data either for the purpose of testing or for
tuning and selection by  cross validation  The dif culty in
using observational data is that if   policy prescribes any
treatment    Xi   cid  Ti  then it is not immediately clear how
to score this 
For of ine evaluation of contextual bandits with experimental data  Li et al    show that rejection sampling
is suf cient    similar solution to evaluation with obser 

Recursive Partitioning for Personalization using Observational Data

vational data is   combined rejection and importance sampling approach suggested by Thm    If we had the GPS
Qi  we could omit any datapoint where    Xi   cid  Ti while
giving score Yi Qi to each datapoint where the prescription matched the data     Xi    Ti  Per Thm    and the law
of large numbers  this will provide   consistent estimate
for outof sample personalization risk  However  not only
does this throw away many datapoints  but to implement
this in practice we would have to estimate the GPS from
data  Estimating the GPS generally either relies heavily on
model speci cation or  in nonparametric settings  can be
biased and variable  This may be acceptable for training
purposes  as in imputing GPS in IPOEM and INPOEM  as
it is already   black box  However  for evaluation    more
reliable estimate of risk is desirable for evidence of success 
We propose the use of submatching for evaluation  Matching is   common tool for causal inference  Rosenbaum 
  Abadie   Imbens    where every subject is
matched to   subject that received the opposite treatment 
creating   complete matched dataset  In submatching  we
instead seek only   subset of the data that is wellmatched 
In this subset  each subject is matched based on   metric
 cid       cid cid  to       subjects that received each of the treatments the subject did not  Their outcome is imputed as
the counterfactual outcome of those treatments for the subject  All matched subjects are not used for training in order
to avoid insample bias  Usually  Mahalanobis distance is
used         cid         cid  where   is the sample covariance   Note that due to personalization on    matching
on propensity scores alone would be insuf cient 

  Greedy Submatching

The simplest way to extract   matched subset of size ntest
from Sn is to do so greedily  draw random            intest
from     without replacement  for each      ntest  and    
    if     Tij then set  Yij     Yij and if    cid  Tij then  nd
    arg mini   Ti    cid Xi   Xij cid   with replacement  let
 Yij     Yi and  ag subject    and  nally remove all drawn
and  agged subjects from training data  The imputed value
for the unknown Yij     is  Yij   and our estimate for per 
 Yij    Xij  
sonalization risk of       is          
ntest
When matching is exact       Xi   Xij for all matches 
this estimate is unbiased 

 cid ntest

  

Theorem   Under Asn    and exact matching 
            

  Optimal Submatching

The greedy method for constructing   matched dataset is
simple but it can be wasteful  limiting the amount of the
data available for training  We may be able to do better
for testing and evaluation when       when the problem

Tijt     and minimal cid npair

reduces to average treatment effect estimation  Consider
the problem of  nding the subset of the data with the closest matches  That is   nding               inpair  inpair  with
    cid Xij     Xij  cid  and using the
pairs for imputations  This problem can be reduced to bipartite matching  which can be solved ef ciently  Hopcroft
  Karp    Consider the complete bipartite graph with
left nodes being subjects with Ti     and right nodes being
subjects with Ti     along with   npair dummy nodes  Put
weight  cid Xi Xj cid  on edges between datapoints and weight
  on edges to dummy nodes  The subset of the data with
the closest matches is given by the nodes incident to edges
not incident to dummy nodes in the leastweight bipartite
match  We extract these to construct   wellmatched  economical test set with ntest    npair  Although this test set
may be biased relative to the whole population       it may
emphasize areas of treatment overlap  the corresponding
risk estimate is unbiased conditioned on the test set       it
corresponds to risk on an alternative population  which is
often suf cient for comparison and selection 

  Coef cient of Personalization

In prediction  the coef cient of determination    is   unitless quantity bounded by   that measures both how well
data   predict outcomes   and how well   predictive
model leverages    One way to interpret outof sample
   is as the percent of the way that   and the model go
from   noX data solution       sample average  to perfect
foresight       realized value  Using this interpretation  we
construct two analogous quantities for personalization  the
 st and  nd coef cients of personalization 

                      mint         
mint             mint           
                      mint         
 
          mint         

These are also analogous to the coef cient of prescription for conditional stochastic optimization  Bertsimas  
Kallus    The  rst measures the improvement toward
perfect  prescient  personalization relative to no personalization at all and the second does relative to current practice
or standard of care  whatever determined   in the data 
They are unitless  bounded by   Using   matched dataset 
we can estimate these as 

  

  

   mint   

 ntest
 Yij   ntest
 ntest
   mint   

 Yij    Xij
mint     ntest
  
 Yij    Xij
  Yij  ntest
 ntest

             ntest
             ntest
  More ef cient estimates may be possible using analogues of
Robins   Rotnitzky   Hahn   on the submatched data 
  This assumes that potential outcomes are conditionally independent given    Indeed  the conditional copula of potential
outcomes has no physical meaning and is unidenti able 

   mint   
 Yij  

   mint   
 Yij  

 Yij  

 

 Yij  

 

Recursive Partitioning for Personalization using Observational Data

Figure   Personalization Risk for Personalized Warfarin Dosing

  Empirical Investigation
We conclude with an empirical investigation of personalization using observational data and our new algorithms 

  Personalized Warfarin Dosing

According to the International Warfarin Pharmacogenetics
Consortium   warfarin is the most widely used oral anticoagulant agent worldwide  and  nding the appropriate dose
is both dif cult and important  because it can vary by   factor of   among patients  and  incorrect doses contribute
to   high rate of adverse effects   Consortium    Currently  the common practice is to start   new patient at
  mg week and slowly adjust the dose  Jaffer   Bragg 
  We present an application of our methods to personalizing dosage based on data on   warfarin patients
collected by Consortium  
The baseline data collected on each patient include demographic characteristics  sex  ethnicity  age  weight  height 
and smoker  reason for treatment       atrial  brillation  current medications  comorbidities       diabetes 
genotype of two polymorphisms in CYP    and genotype of seven single nucleotide polymorphisms  SNPs  in
VKORC  The correct stable therapeutic dose of warfarin 
determined by adjustment over   few weeks  is recorded
for each patient and segmented into three dose groups  low
    mg week        medium         mg week 
      and high     mg week        The dataset
was also studied in an online  bandit  setting in  Bastani  
Bayati    where an     approach is analyzed 
In our experiment  we let       be   if the dose   is incorrect and otherwise   To generate observational data  where
dosage is not revealed by experimentation  we consider  
chosen based on body mass index  BMI 
                  

  xBMI BMI BMI    xBMI BMI  BMI  

    xBMI BMI BMI

where  BMI and  BMI are the sample mean and standard
deviation of BMI  As an example  we run the PT algorithm with  max     on the whole data  generating the
tree shown in Fig     We use  max     due to length constraints  It is known that VKORC  and CYP    genotypes are strongly associated to warfarin dosage requirements  Li et al    PT is able to learn this relationship
and it provides an ef cient and interpretable dosing guideline where the effect of these genotypes is clear 
To assess the ef ciency of various personalization algorithms  for each                   we consider
  replications in which we randomly select   training
subjects and ntest     test subjects  disjoint  without
replacement 
In each replication  we run   personalization algorithms and evaluate their risk on the test set
 where full counterfactuals are available  We test standard     using four predictive models  OLS  logistic
      cid 
regression  CART  scikitlearn defaults  and kNN
  cid  We compare these to our three direct
personalization methods  PT  nminleaf      max  
   features      PF        nminleaf      max  
   features  
   and OPT  nminleaf      features  
    cuts                      MIP solve time
limited to   hour  We also compare to our  vA strategy using Athey   Imbens    CTA  adaptive  and
CTH  honest with   split  and to IPOEM and INPOEM  parameters tuned on   holdout validation as in
Swaminathan   Joachims        with GPS imputed by
crossvalidated  cid regularized multinomial regression using   package glmnet   Due to limited space  we focus
on  vA  which outperformed    
We plot the average risk over replicates in Fig     note
log scale  It is evident that     approaches make inef 
 cient use of the available data by splitting it and learning more than is necessary  While eventually reaching low
risk         using OLS and logistic regression take

 

 PT OLS CTA vA PF Logit CTH vA OPT CART IPOEM kNN INPOEM   RiskRecursive Partitioning for Personalization using Observational Data

much longer        to get there than our direct methods  which achieve low risk very quickly        and
nearoptimal risk     soon after        Nonparametric      CART  kNN  IPOEM  and INPOEM
converge slowly   vA with CTA and CTH offers competitive performance for moderate    but fails to achieve
nearoptimal risk even at       CTA offers   small
edge over CTH  which can be attributed to CTH   splitting
of the training data   indeed  CTH   primary advantage are
correctly sized con dence intervals  which are not used 
Among our direct methods  PF appears to work the best
overall  for both small and large    while PT achieve similar performance for       For smaller    OPT outperforms PT  and PF for       attributed to OPT   ability
to  nd the best simple tree to    the scarce data  For larger
   the MIP becomes so large that Gurobi is hardly able to
improve the PT warm start  which has very limited depth 
Therefore  we see performance deteriorate  We conclude
OPT is best either for small datasets or for  nding models
that are reasonably ef cient while being exceedingly simple and interpretable  depth   compared to depth  
for PT at       albeit at computational cost  Our best
outof sample risk is   which translates to        
                or    of the way from no personalization  or  standard of care  to perfect personalization 

  Personalized Job Training

We consider an application to personalized recommendations for   job training program  We use data from the
National Supported Work Demonstration  LaLonde   
 combining the experimental sample of   subjects with
the   PSID controls to create an observational dataset 
The data includes   individuals    of which received
  job training program in    Ti     The data
includes information about age  education level  ethnicity 
marital status  earnings in years   and earnings in
  This data is the standard benchmark in evaluation of
causal methodologies for estimating an average treatment
effect  Dehejia   Wahba    We consider an alternative setting where we give   personalized recommendation
as to whether to enroll in the job training program  assuming enrollment costs   Therefore  we let Yi equal
  earnings less   if Ti    
From the   we extract an optimal matched test set of  
pairs  ntest     perfectly matched in all covariates except for   mean absolute deviation of   and   in  
and   earnings  respectively  within pairs  On the remaining       subjects  we train the same personalization models as above with the following changes  we omit
logistic regression  outcomes not binary  use nminleaf    
for PT and OPT and     for PF  use       for OPT and let
the MIP solve for   hours  use logistic regressions to im 

Figure   Personalization Bene   for Personalized Job Training

pute GPS for IPOEM and INPOEM  and include the causal
forest  CF  extension  Wager   Athey    of CT as implemented by the   package gradient forest 
We plot the estimated average personalized net income  after enrollment costs  in Fig    We see   clear bene   to our
methods  direct targeting of personalization and that  with
only two treatments  CF and CTA provide highly competitive performance  Average net income of   due to PF
translates to                         or   
of the way from no personalization  or  standard of care  to
perfect personalization 

  Conclusions
We developed   new approach to the unique problem of
personalization from observational data  The approach was
based on   new formulation of the problem and   new
impurity measure for personalization  This lead to three
recursivepartitioning based algorithms 
the personalization tree that greedily partitions the data to minimize the
sum of withinpartition personalization impurities  the personalization forest that bagged many personalization trees 
and the optimal personalization tree that used   MIP to
globally optimize the partitioning problem  We developed
new submatching techniques to evaluate and validate these
algorithms as well as ones adapted from existing methods 
And we used these techniques to evaluate all algorithms
in two example applications  personalized warfarin dosing and personalized job training 
In both examples  we
demonstrated the bene ts of our algorithms in terms of ef 
 cacy and interpretability  We phrased the success of our
personalization techniques in terms of the new coef cients
of personalization  which quantify the bene   we achieve
from personalization as   percentage of the bene   that impossibly perfect personalization can achieve relative to either no personalization or the standard of care 

 PTPFOPTOLSCARTkNNCTACT HCFIPOEMINPOEM AverageNetIncomeRecursive Partitioning for Personalization using Observational Data

References
Abadie  Alberto and Imbens  Guido    Large sample properties of matching estimators for average treatment effects  Econometrica     

Abrevaya  Jason  Hsu  YuChin  and Lieli  Robert    Estimating conditional average treatment effects    Bus Econ
Stat     

Ahuja  RK  Magnanti  TL  and Orlin  JB  Network  ows 

theory  algorithms  and applications   

Athey  Susan and Imbens  Guido  Recursive partitioning
for heterogeneous causal effects  PNAS   
   

Bastani  Hamsa and Bayati  Mohsen  Online decision 

making with highdimensional covariates   

Bennett  Kristin   and Blue     Optimal decision trees 

 

Bertsimas  Dimitris and Dunn  Jack  Optimal trees  Mach

Learn   

Bertsimas  Dimitris and Kallus  Nathan  From predictive

to prescriptive analytics   

Bertsimas  Dimitris  Kallus  Nathan  Weinstein  Alexander    and Zhuo  Ying Daisy  Personalized diabetes
management using electronic medical records  Diabetes
Care     

Beygelzimer  Alina and Langford  John  The offset tree for
learning with partial labels  In SIGKDD  pp   
ACM   

Breiman  Leo  Random forests  Mach Learn   

 

Breiman  Leo  Friedman  Jerome  Stone  Charles  and Olshen  Richard  Classi cation and Regression Trees 
 

Consortium  International Warfarin Pharmacogenetics  Estimation of the warfarin dose with clinical and pharmacogenetic data  New Engl   Med     

Dehejia  Rajeev   and Wahba  Sadek  Propensity scorematching methods for nonexperimental causal studies 
Rev Econ Stat     

Goldfarb  Avi and Tucker  Catherine  Online display advertising  Targeting and obtrusiveness  Market Sci   
   

Hahn  Jinyong  On the role of the propensity score in ef 
 cient semiparametric estimation of average treatment
effects  Econometrica  pp     

Hirano  Keisuke and Imbens  Guido    The propensity
score with continuous treatments  In Gelman  Andrew
and Meng  XiaoLi  eds  Applied Bayesian modeling
and causal inference from incompletedata perspectives 
pp     

Hopcroft  John   and Karp  Richard    An    algorithm for maximum matchings in bipartite graphs  SIAM
  Comput     

Hya    Laurent and Rivest  Ronald    Constructing optimal binary decision trees is npcomplete  Inform Process
Lett     

Imbens  Guido   and Rubin  Donald    Causal inference

in statistics  social  and biomedical sciences   

Jaffer  Amir and Bragg  Lee  Practical tips for warfarin
dosing and monitoring  Clev Clin   Med   
 

Kallus  Nathan  Generalized optimal matching methods for

causal inference   

Kallus  Nathan  Balanced policy evaluation and learning 

   

Kallus  Nathan    framework for optimal matching for

causal inference  In AISTATS  pp       

LaLonde  Robert    Evaluating the econometric evaluations
of training programs with experimental data  Am Econ
Rev  pp     

Lesko  LJ  Personalized medicine  elusive dream or imminent reality  Clin Pharmacol Ther   
 

Li  Lihong  Chu  Wei  Langford  John  and Schapire 
Robert      contextualbandit approach to personalized
In WWW  pp   
news article recommendation 
 

Feldstein  Michael    Savlov  Edwin    and Hilf  Russell 
  statistical model for predicting response of breast cancer patients to cytotoxic chemotherapy  Cancer Res   
   

Li  Lihong  Chu  Wei  Langford  John  and Wang  Xuanhui 
Unbiased of ine evaluation of contextualbandit based
news article recommendation algorithms  In WSDM  pp 
   

Goldenshluger  Alexander and Zeevi  Assaf    linear response bandit problem  Stoch Syst     

Li  Tao  Lange  Leslie    Li  Xiangli  Susswein  Lisa 
Bryant  Betsy  Malone  Robb  Lange  Ethan    Huang 

Recursive Partitioning for Personalization using Observational Data

TengYi  Stafford  Darrel    and Evans  James    Polymorphisms in the vkorc  gene are strongly associated
with warfarin dosage requirements in patients receiving
anticoagulation    Med Genet     

Qian  Min and Murphy  Susan    Performance guarantees
for individualized treatment rules  Ann Stat   
   

Robins  James   and Rotnitzky  Andrea  Semiparametric
ef ciency in multivariate regression models with missing
data  Journal of the American Statistical Association   
   

Rosenbaum  Paul    Optimal matching for observational
studies  Journal of the American Statistical Association 
   

Rosenbaum  Paul   and Rubin  Donald    The central
role of the propensity score in observational studies for
causal effects  Biometrika     

Stoehlmacher     Park  DJ  Zhang     Yang     Groshen    
Zahedy     and Lenz  HJ    multivariate analysis of genomic polymorphisms  prediction of clinical outcome to
 fu oxaliplatin combination chemotherapy in refractory
colorectal cancer  Brit   Cancer     

Swaminathan  Adith and Joachims  Thorsten  Counterfactual risk minimization  Learning from logged bandit
feedback  In ICML  pp       

Swaminathan  Adith and Joachims  Thorsten  The selfIn

normalized estimator for counterfactual learning 
NIPS  pp       

Wager  Stefan and Athey  Susan  Estimation and inference
of heterogeneous treatment effects using random forests 
Journal of the American Statistical Association   justaccepted   

Walk  Harro  Strong laws of large numbers and nonparaIn Recent Developments in Applied

metric estimation 
Probability and Statistics  pp     

  ld    Sercan and Vielma  Juan Pablo 

Incremental and
encoding formulations for mixed integer programming 
Oper Res Lett     

Zhao  Yingqi  Zeng  Donglin  Rush    John  and Kosorok 
Michael    Estimating individualized treatment rules using outcome weighted learning    Am Stat Assoc   
   

Zhou  Yunhong  Wilkinson  Dennis  Schreiber  Robert  and
Pan  Rong  Largescale parallel collaborative  ltering
for the net ix prize  In AAIM  pp     

