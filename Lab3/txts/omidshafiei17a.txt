Deep Decentralized Multitask MultiAgent Reinforcement Learning

under Partial Observability

Shayegan Omidsha ei   Jason Pazis   Christopher Amato   Jonathan    How   John Vian  

Abstract

Many realworld tasks involve multiple agents
with partial observability and limited communication  Learning is challenging in these settings
due to local viewpoints of agents  which perceive
the world as nonstationary due to concurrentlyexploring teammates  Approaches that learn specialized policies for individual tasks face problems when applied to the real world  not only
do agents have to learn and store distinct policies for each task  but in practice identities of
tasks are often nonobservable  making these approaches inapplicable  This paper formalizes and
addresses the problem of multitask multiagent
reinforcement learning under partial observability  We introduce   decentralized singletask
learning approach that is robust to concurrent interactions of teammates  and present an approach
for distilling singletask policies into   uni ed
policy that performs well across multiple related
tasks  without explicit provision of task identity 

  Introduction
In multitask reinforcement learning  MTRL  agents are
presented several related target tasks  Taylor   Stone 
  Caruana    with shared characteristics  Rather
than specialize on   single task  the objective is to generalize performance across all tasks  For example    team of
autonomous underwater vehicles  AUVs  learning to detect
and repair faults in deepsea equipment must be able to do
so in many settings  varying water currents  lighting  etc 
not just under the circumstances observed during training 
Many realworld problems involve multiple agents with

 Laboratory for Information and Decision Systems  LIDS 
MIT  Cambridge  MA  USA  College of Computer and Information Science  CCIS  Northeastern University  Boston  MA  USA
 Boeing Research   Technology  Seattle  WA  USA  Correspondence to  Shayegan Omidsha ei  shayegan mit edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

partial observability and limited communication       the
AUV example   Oliehoek   Amato    but generating accurate models for these domains is dif cult due to
complex interactions between agents and the environment 
Learning is dif cult in these settings due to partial observability and local viewpoints of agents  which perceive the
environment as nonstationary due to teammates  actions 
Ef cient learners must extract knowledge from past tasks
to accelerate learning and improve generalization to new
tasks  Learning specialized policies for individual tasks
can be problematic  as not only do agents have to store  
distinct policy for each task  but in practice face scenarios
where the identity of the task is often nonobservable 
Existing MTRL methods focus on singleagent and or fully
observable settings  Taylor   Stone    By contrast 
this work considers cooperative  independent learners operating in partiallyobservable  stochastic environments  receiving feedback in the form of local noisy observations
and joint rewards  This setting is general and realistic
for many multiagent domains  We introduce the multitask multiagent reinforcement learning  MTMARL  under partial observability problem  where the goal is to maximize executiontime performance on   set of related tasks 
without explicit knowledge of the task identity  Each MTMARL task is formalized as   Decentralized Partially Observable Markov Decision Process  DecPOMDP   Bernstein et al      general formulation for cooperative
decisionmaking under uncertainty  MTMARL poses signi cant challenges  as each agent must learn to coordinate
with teammates to achieve good performance  ensure policy generalization across all tasks  and conduct  implicit 
executiontime inference of the underlying task ID to make
sound decisions using local noisy observations  As typical in existing MTRL approaches  this work focuses on
average asymptotic performance across all tasks  Caruana 
  Taylor   Stone    and sampleef cient learning 
We propose   twophase MTMARL approach that
in combi 
 rst uses
nation with Deep Recurrent QNetworks
 DRQNs 
for actionvalue approxi 
 Hausknecht   Stone   
mation  We introduce Concurrent Experience Replay
Trajectories  CERTs    decentralized extension of ex 

cautiouslyoptimistic

learners

Deep Decentralized Multitask MultiAgent RL under Partial Observability

perience replay  Lin    Mnih et al    targeting
sampleef cient and stable MARL  This  rst contribution
enables coordination in singletask MARL under partial
observability  The second phase of our approach distills
each agent   specialized actionvalue networks into  
generalized recurrent multitask network  Using CERTs
and optimistic learners  wellperforming distilled policies
 Rusu et al    are learned for multiagent domains 
Both the singletask and multitask phases of the algorithm
are demonstrated to achieve good performance on   set
of multiagent target capture DecPOMDP domains  The
approach makes no assumptions about communication
capabilities and is fully decentralized during learning and
execution  To our knowledge  this is the  rst formalization
of decentralized MTMARL under partial observability 

  Background
  Reinforcement Learning

turn as Rt    cid  

Singleagent RL under full observability is typically formalized using Markov Decision Processes  MDPs   Sutton   Barto    de ned as tuple  cid            cid  At
the agent with state       executes actimestep   
tion       using policy       receives reward rt  
          and transitions to state   cid      with probability      cid                    cid  Denoting discounted ret cid      cid trt  with horizon   and discount factor         the actionvalue  or Qvalue  is
de ned as             Rt st      at      Optimal
policy   maximizes the Qvalue function    
        
max         
In RL  the agent interacts with the environment to learn   without explicit provision of the MDP
model  Modelbased methods  rst learn   and    then use
  planner to  nd   
  Modelfree methods typically directly learn Qvalues or policies  so can be more space and
computation ef cient 
Qlearning  Watkins   Dayan    iteratively estimates
the optimal Qvalue function using backups           
                 maxa cid      cid    cid            where    
    is the learning rate and the term in brackets is the
temporaldifference  TD  error  Convergence to   
is
guaranteed in the tabular  no approximation  case provided suf cient state action space exploration  however 
tabulated learning is unsuitable for problems with large
state action spaces  Practical TD methods instead use function approximators  Gordon    such as linear combinations of basis functions or neural networks  leveraging
inductive bias to execute similar actions in similar states 
Deep Qlearning is   stateof theart approach using   Deep
QNetwork  DQN  for Qvalue approximation  Mnih et al 
  At each iteration    experience tuple  cid            cid cid  is
sampled from replay memory   and DQN parameters  
are updated to minimize loss Lj                cid      

  maxa cid      cid    cid                    Replay memory  
is    rstin  rstout queue containing the set of latest experience tuples from  greedy policy execution  Target
network parameters    are updated less frequently and  in
combination with experience replay  are critical for stable
deep Qlearning 
Agents in partiallyobservable domains receive observations of the latent state 
Such domains are formalized as Partially Observable Markov Decision Processes
 POMDPs  de ned as  cid                cid   Kaelbling
et al    After each transition  the agent observes
      with probability        cid              cid     Due to
noisy observations  POMDP policies map observation histories to actions  As Recurrent Neural Networks  RNNs 
inherently maintain an internal state ht to compress input
history until timestep    they have been demonstrated to
be effective for learning POMDP policies  Wierstra et al 
  Recent work has introduced Deep Recurrent QNetworks  DRQNs   Hausknecht   Stone    combining Long ShortTerm Memory  LSTM  cells  Hochreiter  
Schmidhuber    with DQNs for RL in POMDPs  Our
work extends this singletask  singleagent approach to the
multitask  multiagent setting 

  Multiagent RL

Multiagent RL  MARL  involves   set of agents in  
shared environment  which must learn to maximize their
individual returns  Bus oniu et al    Our work focuses
on cooperative settings  where agents share   joint return 
Claus   Boutilier   dichotomize MARL agents into
two classes  Joint Action Learners  JALs  and Independent
Learners  ILs  JALs observe actions taken by all agents 
whereas ILs only observe local actions  As observability of
joint actions is   strong assumption in partially observable
domains  ILs are typically more practical  despite having
to solve   more challenging problem  Claus   Boutilier 
  Our approach utilizes ILs that conduct both learning and execution in   decentralized manner 
Unique challenges arise in MARL due to agent interactions during learning  Bus oniu et al    Matignon
et al    Multiagent domains are nonstationary from
agents  local perspectives  due to teammates  interactions
with the environment 
ILs  in particular  are susceptible
to shadowed equilibria  where local observability and nonstationarity cause locally optimal actions to become   globally suboptimal joint action  Fulda   Ventura    Effective MARL requires each agent to tightly coordinate
with fellow agents  while also being robust against destabilization of its own policy due to environmental nonstationarity  Another desired characteristic is robustness to
alterexploration  or drastic changes in policies due to exploratory actions of teammates  Matignon et al   

Deep Decentralized Multitask MultiAgent RL under Partial Observability

  Transfer and MultiTask Learning

Transfer Learning  TL  aims to generalize knowledge from
  set of source tasks to   target task  Pan   Yang    In
singleagent  fullyobservable RL  each task is formalized
as   distinct MDP       MDPs and tasks are synonymous 
 Taylor   Stone    While TL assumes sequential
transfer  where source tasks have been previously learned
and even may not be related to the target task  MultiTask
Reinforcement Learning  MTRL  aims to learn   policy
that performs well on related target tasks from an underlying task distribution  Caruana    Pan   Yang   
MTRL tasks can be learned simultaneously or sequentially
 Taylor   Stone    MTRL directs the agent   attention towards pertinent training signals learned on individual tasks  enabling   uni ed policy to generalize well across
all tasks  MTRL is most bene cial when target tasks share
common features  Wilson et al    and most challenging when the task ID is not explicitly speci ed to agents
during execution   the setting addressed in this paper 

  Related Work
  Multiagent RL

Bus oniu et al    present   taxonomy of MARL approaches  Partiallyobservable MARL has received limited attention  Works include modelfree gradientascent
based methods  Peshkin et al    Dutech et al   
simulatorsupported methods to improve policies using  
series of linear programs  Wu et al    and modelbased approaches where agents learn in an interleaved fashion to reduce destabilization caused by concurrent learning  Banerjee et al    Recent scalable methods use
Expectation Maximization to learn  nite state controller
 FSC  policies  Wu et al    Liu et al     
Our approach is most related to IL algorithms that learn Qvalues  as their representational power is more conducive
to transfer between tasks  in contrast to policy tables or
FSCs  The majority of existing IL approaches assume
full observability  Matignon et al    survey these approaches  the most straightforward being Decentralized Qlearning  Tan    where each agent performs independent Qlearning  This simple approach has some empirical success  Matignon et al    Distributed Qlearning
 Lauer   Riedmiller    is an optimal algorithm for deterministic domains  it updates Qvalues only when they
are guaranteed to increase  and the policy only for actions that are no longer greedy with respect to Qvalues 
Bowling   Veloso   conduct Policy Hill Climbing using the Winor Learn Fast heuristic to decrease  increase 
each agent   learning rate when it performs well  poorly 
Frequency Maximum QValue heuristics  Kapetanakis  
Kudenko    bias action selection towards those con 

sistently achieving max rewards  Hysteretic Qlearning
 Matignon et al    addresses miscoordination using
cautious optimism to stabilize policies while teammates explore  Its track record of empirical success against complex
methods  Xu et al    Matignon et al    Barbalios
  Tzionas    leads us to use it as   foundation for
our MTMARL approach  Foerster et al    present
architectures to learn communication protocols for DecPOMDP RL  noting best performance using   centralized
approach with interagent backpropagation and parameter
sharing  They also evaluate   model combining Decentralized Qlearning with DRQNs  which they call Reinforced
InterAgent Learning  Given the decentralized nature of
this latter model  called DecDRQN herein for clarity  we
evaluate our method against it  Concurrent to our work  Foerster et al    investigated an alternate means of stabilizing experience replay for the centralized learning case 

  Transfer and Multitask RL

Taylor   Stone   and Torrey   Shavlik   provide excellent surveys of transfer and multitask RL  which
almost exclusively target singleagent  fullyobservable
settings  Tanaka   Yamamura   use  rst and secondorder statistics to compute   prioritized sweeping metric
for MTRL  enabling an agent to maximize lifetime reward
over task sequences  Fern andez   Veloso   introduce
an MDP policy similarity metric  and learn   policy library
that generalizes well to tasks within   shared domain  Wilson et al    consider TL for MDPs  learning   Dirichlet Process Mixture Model over source MDPs  used as an
informative prior for   target MDP  They extend the work
to multiagent MDPs by learning characteristic agent roles
 Wilson et al    Brunskill   Li   introduce an
MDP clustering approach that reduces negative transfer in
MTRL  and prove reduction of sample complexity of exploration using transfer  Taylor et al    introduce parallel transfer to accelerate multiagent learning using interagent transfer  Recent work extends the notion of neural network distillation  Hinton et al    to DQNs for
singleagent  fullyobservable MTRL   rst learning   set of
specialized teacher DQNs  then distilling teachers to   single multitask network  Rusu et al    The ef cacy of
the distillation technique for singleagent MDPs with large
state spaces leads our work to use it as   foundation for the
proposed MTMARL under partial observability approach 

  Multitask Multiagent RL
This section introduces MTMARL under partial observability  We formalize singletask MARL using the Decentralized Partially Observable Markov Decision Pro 

cess  DecPOMDP  de ned as  cid                  cid 
where   is   set of   agents    is the state space     

Deep Decentralized Multitask MultiAgent RL under Partial Observability

   

    where  cid ot

                
   cid Ot

           
    Singleagent policy    

 iA    is the joint action space  and           is the
joint observation space  Bernstein et al    Each
agent   executes action             where joint action    
 cid                cid  causes environment state       to transition with probability      cid                    cid  At each
timestep  each agent receives observation            with
joint observation probability        cid              cid    
where      cid                cid  Let local observation history
     
at timestep   be  cid ot
 cid       con 
 cid Ot
ducts action selection  and the joint policy is denoted    
 cid             cid  For simplicity  we consider only pure
joint policies  as  nitehorizon DecPOMDPs have at least
one pure joint optimal policy  Oliehoek et al    The
team receives   joint reward rt     st  at      at each
timestep    the objective being to maximize the value  or
    trt  While DecPOMDP
planning approaches assume agents do not observe intermediate rewards  we make the typical RL assumption that
they do  This assumption is consistent with prior work in
MARL  Banerjee et al    Peshkin et al   
ILs provide   scalable way to learn in DecPOMDPs  as
each agent   policy maps local observations to actions 
However  the domain appears nonstationary from the perspective of each DecPOMDP agent    property we formalize by extending the de nition by Laurent et al   
De nition   Let                  Local decision process for agent   is stationary if  for all timesteps          

expected return        cid  

     cid   cid      
     

   
 

 cid   

     cid   cid      

   
 

 cid 

 

       
    

 cid 
and cid 

   
 
 

   
 

 

 cid 
 cid 

         cid cid      
     

   
 

 cid   
       
    

         cid cid      

   
 

 cid 

   
 

 
Letting               nonstationarity from the lo 
   
cal perspective of agent   follows as in general  
 
 
   cid ot   cid     cid ou     
  which causes violation
of   and   Thus  MARL extensions of singleagent
algorithms that assume stationary environments  such as
DecDRQN  are inevitably illfated  This motivates our decision to  rst design   singletask  decentralized MARL approach targeting nonstationarity in DecPOMDP learning 
The MTMARL problem in partially observable settings
is now introduced by extending the singleagent  fullyobservable de nition of Fern andez   Veloso  
De nition     partiallyobservable MTMARL Domain  
is   tuple  cid           cid  where   is the set of agents    is

 Superscripts indicate local parameters for agent       

the environment state space    is the joint action space   
is the joint observation space  and   is the discount factor 
De nition     partiallyobservable MTMARL Task Tj
is   tuple  cid   Tj Rj Oj cid  where   is   shared underlying domain  Tj  Rj  Oj are  respectively  the taskspeci  
transition  reward  and observation functions 
In MTMARL  each episode                  consists of  
randomly sampled Task Tj from domain    The team observes the task ID     during learning  but not during execu 
 cid He
tion  The objective is to  nd   joint policy that maximizes
average empirical executiontime return in all   episodes 
    tRe st  at  where He is the time
      
 
horizon of episode   

 cid  

  

  Approach
This section introduces   twophase approach for partiallyobservable MTMARL  the approach  rst conducts singletask specialization  and subsequently uni es taskspeci  
DRQNs into   joint policy that performs well in all tasks 

  Phase    DecPOMDP SingleTask MARL

As DecPOMDP RL is notoriously complex  and solving for the optimal policy is NEXPcomplete even with  
known model  Bernstein et al    we  rst introduce an
approach for stable singletask MARL  This enables agents
to learn coordination  while also learning Qvalues needed
for computation of   uni ed MTMARL policy 

  DECENTRALIZED HYSTERETIC DEEP

RECURRENT QNETWORKS  DECHDRQNS 

Due to partial observability and local nonstationarity 
modelbased DecPOMDP MARL is extremely challenging  Banerjee et al    Our approach is modelfree and
decentralized  learning Qvalues for each agent 
In contrast to policy tables or FSCs  Qvalues are amenable to the
multitask distillation process as they inherently measure
quality of all actions  rather than just the optimal action 
Overlyoptimistic MARL approaches       Distributed Qlearning  Lauer   Riedmiller    completely ignore
low returns  which are assumed to be caused by teammates  exploratory actions  This causes severe overestimation of Qvalues in stochastic domains  Hysteretic Qlearning  Matignon et al    instead  uses the insight
that low returns may also be caused by domain stochasticity  which should not be ignored  This approach uses two
learning rates  nominal learning rate    is used when the
TDerror is nonnegative    smaller learning rate    is used
otherwise  where               The result is hysteresis  lag  of Qvalue degradation for actions associated with
positive past experiences that occurred due to successful

Deep Decentralized Multitask MultiAgent RL under Partial Observability

cooperation  Agents are  therefore  robust against negative
learning due to teammate exploration and concurrent actions  Notably  unlike Distributed Qlearning  Hysteretic
Qlearning permits eventual degradation of Qvalues that
were overestimated due to outcomes unrelated to their associated action 
Hysteretic Qlearning has enjoyed   strong empirical
track record in fullyobservable MARL  Xu et al   
Matignon et al    Barbalios   Tzionas    exhibiting similar performance as more complex approaches 
Encouraged by these results  we introduce Decentralized
Hysteretic Deep Recurrent QNetworks  DecHDRQNs 
for partiallyobservable domains  This approach exploits
the robustness of hysteresis to nonstationarity and alterexploration  in addition to the representational power and
memorybased decision making of DRQNs  As later
demonstrated  DecHDRQN is wellsuited to DecPOMDP
MARL  as opposed to nonhysteretic DecDRQN 

  CONCURRENT EXPERIENCE REPLAY

TRAJECTORIES  CERTS 

Experience replay  sampling   memory bank of experience
tuples  cid            cid cid  for TD learning  was  rst introduced by
Lin   and recently shown to be crucial for stable deep
Qlearning  Mnih et al    With experience replay 
sampling cost is reduced as multiple TD updates can be
conducted using each sample  enabling rapid Qvalue propagation to preceding states without additional environmental interactions  Experience replay also breaks temporal
correlations of samples used for Qvalue updates crucial
for reducing generalization error  as the stochastic optimization algorithms used for training DQNs typically assume        data  Bengio   
Despite the bene ts in singleagent settings  existing
MARL approaches have found it necessary to disable experience replay  Foerster et al    This is due to the
nonconcurrent  and nonstationary  nature of local experiences when sampled independently for each agent  despite
the agents learning concurrently    contributing factor
is that interagent desynchronization of experiences compounds the prevalence of earliermentioned shadowed equilibria challenges  destabilizing coordination  As   motivating example  consider     agent game where     
            Let there be two optimal joint actions   cid      cid  and  cid      cid        only these joint actions
have positive  equal reward  Given independent experience samples for each agent  the  rst agent may learn action    as optimal  whereas the second agent learns    resulting in arbitrarily poor joint action  cid      cid  This motivates   need for concurrent  synchronized  sampling of
experiences across the team in MARL settings  Concurrent
experiences induce correlations in local policy updates  so

    CERT structure 

    CERT minibatches 

Figure   Concurrent training samples for MARL  Each cube signi es an experience tuple  cid     
  cid  Axes         correspond to episode  timestep  and agent indices  respectively 

  rt      

      
 

 

 

      
 

  rt      

that given existence of multiple equilibria  agents tend to
converge to the same one  Thus  we introduce Concurrent Experience Replay Trajectories  CERTs  visualized in
Fig      During execution of each learning episode       
each agent   collects experience tuple  cid     
  cid  at
timestep    where ot  at  and rt are current observation  action  and reward  and ot  is the subsequent observation 
Fig     visualizes each experience tuple as   cube  Experiences in each episode are stored in   sequence  along time
axis   of Fig      as DecHDRQN assumes an underlying RNN architecture that necessitates sequential samples
for each training iteration 
Importantly  as all agents are
aware of timestep   and episode    they store their experiences concurrently  along agent index axis   of Fig     
Upon episode termination    new sequence is initiated   
new row along episode axis   of Fig      No restrictions
are imposed on terminal conditions       varying trajectory
lengths are permitted along axis   of Fig      CERTs are
   rstin  rstout circular queue along the episode axis   
such that old episodes are eventually discarded 

  TRAINING DECHDRQNS USING CERTS

Each agent   maintains DRQN         
      
           
 
where     
is the latest local observation      
   is the RNN
 
hidden state       is the action  and     are the local DRQN
parameters  DRQNs are trained on experience sequences
 traces  with tracelength   Figure    visualizes the minibatch sampling procedure for training  with       In each
training iteration  agents  rst sample   concurrent minibatch of episodes  All agents  sampled traces have the same
starting timesteps       are coincident along agent axis  
in Fig      Guaranteed concurrent sampling merely requires   onetime  of ine  consensus of agents  random
number generator seeds prior to initiating learning  This
ensures our approach is fully decentralized and assumes
no explicit communication  even during learning  Fig    
shows   minibatch of   episodes     sampled in red  To train
DRQNs  Hausknecht   Stone   suggest randomly

                  Deep Decentralized Multitask MultiAgent RL under Partial Observability

sampling   timestep within each episode  and training using   backward steps  However  this imposes   bias where
experiences in each episode    nal   timesteps are used
in fewer recurrent updates 
Instead  we propose that for
each sampled episode    agents sample   concurrent start
timestep    for the trace from interval               He 
where He is the timestep of the episode    nal experience 
For example  the three sampled  red  traces in Fig     start
at timesteps     and   respectively  This ensures
all experiences have equal probability of being used in updates  which we found especially critical for fast training
on tasks with only terminal rewards 
Sampled traces sometimes contain elements outside the
episode interval  indicated as   in Fig      We discard
  experiences and zeropad the suf   of associated traces
 to ensure all traces have equal length   enabling seamless
use of  xedlength minibatch optimizers in standard deep
learning libraries  Suf    rather than pre    padding ensures RNN internal states of non  samples are unaffected 
In training iteration    agent   uses the sampling procedure
to collect   minibatch of traces from CERT memory     
     cid cid ob
 
  cid cid     
 cid ob
where    is the start timestep for each trace    is trace index 
and   is number of traces  minibatch size  Each trace   is
used to calculate   corresponding sequence of target values 

  cid         
  rb
  ob
  
  
   ab
   rb

   ob

  ab
  

 cid cid yb

  

 cid         cid yb

  cid cid     
     cid     

    rb

      maxa cid    ob

where yb
work parameters    
 
learning  Mnih et al    Loss over all traces is 

    Target netare updated less frequently  for stable

   hb

 

 ob

   ab

   rb

   ob

         

   

 

Lj   

       
      ob

      

   hb

   ab

    yb

    Loss contributions
where   
of suf    padding elements are masked out  Parameters
are updated via gradient descent on Eq    with the caveat
of hysteretic learning rates               where learning
rate   is used if   

      and   is used otherwise 

  Phase II  DecPOMDP MTMARL

Following task specialization  the second phase involves
distillation of each agent   set of DRQNs into   uni ed
DRQN that performs well in all tasks without explicit provision of task ID  Using DRQNs  our approach extends
the singleagent  fullyobservable MTRL method proposed
by Rusu et al    to DecPOMDP MTMARL  Specifically  once DecHDRQN specialization is conducted for

 For notational simplicity  agent superscripts     are excluded

from local experiences  cid                cid   cid  in Eqs    to  

 

      

   cid  where     

each task  multitask learning can be treated as   regression
problem over Qvalues  During multitask learning  our approach iteratively conducts data collection and regression 
For data collection  agents use each specialized DRQN
 from Phase    to execute actions in corresponding tasks 
resulting in   set of regression CERTs  MR   one per
task  each containing sequences of regression experiences
 cid     
        is the specialized DRQN   Qvalue vector for agent   at timestep
   Supervised learning of Qvalues is then conducted 
Each agent samples experiences from its local regression
CERTs to train   single distilled DRQN with parameters
   
    Given   minibatch of regression experience traces
  cid cid     
BR    cid cid ob
the following tempered KullbackLeibler  KL  divergence
loss is minimized for each agent 
LKL BR     

        

 cid         cid ob

   Qb

  Qb
  

   cid ot

 

  

       

   

 ob

   Qb

   MR    

softmaxa 

Qb
 
 

  ln

softmaxa  Qb
   
softmaxa Qb
    

 

 

     cid 

  

      

    cid ot

      Qb

    is the vector of actionwhere Qb
values predicted by distilled DRQN given the same input
as the specialized DRQN    is the softmax temperature 
and softmaxa refers to the ath element of the softmax output  The motivation behind loss function   is that low
temperatures           lead to sharpening of specialt  ensuring that the distilled
ized DRQN actionvalues  Qb
DRQN ultimately chooses similar actions as the specialized policy it was trained on  We refer readers to Rusu
et al    for additional analysis of the distillation loss 
Note that concurrent sampling is not necessary during the
distillation phase  as it is entirely supervised  CERTs are
merely used for storage of the regression experiences 

  Evaluation
  Task Specialization using DecHDRQN

We  rst evaluate singletask performance of the introduced
DecHDRQN approach on   series of increasingly challenging domains  Domains are designed to support   large
number of task variations  serving as   useful MTMARL
benchmarking tool  All experiments use DRQNs with
  multilayer perceptron  MLP  layers  an LSTM layer
 Hochreiter   Schmidhuber    with   memory cells 
and another   MLP layers  MLPs have   hidden units each
and recti ed linear unit nonlinearities are used throughout 
with the exception of the  nal  linear  layer  Experiments
use       and Adam optimizer  Kingma   Ba   
with base learning rate   DecHDRQNs use hysteretic
learning rate       to   All results are reported for
batches of   randomlyinitialized episodes 

Deep Decentralized Multitask MultiAgent RL under Partial Observability

    Learning via DecDRQN 

    Learning via DecHDRQN  our approach 

Figure   Task specialization for MAMT domain with       agents  Pf         Without hysteresis DecDRQN policies destabilize
in the       task and fails to learn in the       and       tasks      DecHDRQN  our approach  performs well in all tasks 

 south   east   west  and  wait  but transitions are noisy
  probability of moving to an unintended adjacent cell 
In the MAST domain  each task is speci ed by   unique
grid size    in MAMT  each task also has   unique agenttarget assignment  The challenge is that agents must learn
particular roles  to ensure coordination  and also discern
aliased states  to ensure quick capture of targets  using local noisy observations  Tasks end after   timesteps  or
upon simultaneous target capture  Cardinality of local policy space for agent   at timestep   is             
     
 Oliehoek et al    where                    for
MAST  and            for MAMT  Across all tasks 
nonzero reward signals are extremely sparse  appearing in
the terminal experience tuple only if targets are simultaneously captured  Readers are referred to the supplementary
material for domain visualizations 
The failure point of DecDRQN is  rst compared to DecHDRQN in the MAST domain with       and Pf      full
observability  for increasing task size  starting from      
Despite the domain simplicity  DecDRQN fails to match
DecHDRQN at the     mark  receiving value  
in contrast to DecHDRQN      full results reported in supplementary material  Experiments are then
scaled up to     agent    target MAMT domain with
Pf     Empirical returns throughout training are shown
in Fig    In the MAMT tasks    wellcoordinated policy
induces agents to capture targets simultaneously  yielding
joint   reward  If any agent strays from this strategy during learning       while exploring  teammates receive no
reward even while executing optimal local policies  leading them to deviate from learned strategies  Due to lack
of robustness against alterexploration nonstationarity  the
DecDRQN becomes unstable in the       task  and fails
to learn altogether in the       and       tasks  Fig     
Hysteresis affords DecHDRQN policies the stability necessary to consistently achieve agent coordination  Fig     
  centralizedlearning variation of DecDRQN with interagent parameter sharing  similar to RIALPS in Foerster
et al    was also tested  but was not found to improve performance  see supplementary material  These re 

Figure   The advantage of hysteresis is even more pronounced
for MAMT with       agents  Pf     for       task  and
Pf     for       task  DecHDRQN indicated by    

Figure   DecHDRQN sensitivity to learning rate        
MAMT domain        agents  Pf     Anticipated return
        upper bounds actual return due to hysteretic optimism 

Performance is evaluated on both multiagent singletarget
 MAST  and multiagent multitarget  MAMT  capture domains  variations of the existing meetingin agrid DecPOMDP benchmark  Amato et al    Agents    
             in an       toroidal grid receive   terminal reward only when they simultaneously capture moving
targets   target in MAST  and   targets in MAMT  Each
agent always observes its own location  but only sometimes observes targets  locations  Target dynamics are unknown to agents and vary across tasks  Similar to the Pong
POMDP domain of Hausknecht   Stone   our domains include observation  ickering  in each timestep  observations of targets are sometimes obscured  with probability Pf   In MAMT  each agent is assigned   unique target
to capture  yet is unaware of the assignment  which also
varies across tasks  Agent target locations are randomly
initialized in each episode  Actions are  move north 

     KTrainingEpoch EmpiricalReturn     KTrainingEpoch EmpiricalReturn       KTrainingEpoch EmpiricalReturn           EmpiricalReturnDeep Decentralized Multitask MultiAgent RL under Partial Observability

Figure   MTMARL performance of the proposed DecHDRQN specialization distillation approach  labeled as Distilled  and simultaneous learning approach  labeled as Multi  Multitask policies for both approaches were trained on all MAMT tasks from     through
      Performance shown only for       and       domains for clarity  Distilled approach shows specialization training  Phase   of
approach  until    epochs  after which distillation is conducted  Phase II of approach  Letting the simultaneous learning approach run
for up to    episodes did not lead to signi cant performance improvement  By contrast  the performance of our approach during the
distillation phase  which includes task identi cation  is almost as good as its performance during the specialization phase 

sults further validate that  despite its simplicity  hysteretic
learning signi cantly improves the stability of MARL in
cooperative settings  Experiments are also conducted for
the       MAMT domain  Fig    This domain poses
signi cant challenges due to reward sparsity  Even in the
      task  only   of the joint state space has   nonzero reward signal  DecDRQN fails to  nd   coordinated
joint policy  receiving nearzero return after training  DecHDRQN successfully coordinates the   agents  Note the
high variance in empirical return for the       task is due
to  ickering probability being increased to Pf    
Sensitivity of DecHDRQN empirical performance to hysteretic learning rate   is shown in Fig    where lower
  corresponds to higher optimism        causes monotonic increase of approximated Qvalues during learning 
whereas       corresponds to DecDRQN  Due to the
optimistic assumption  anticipated returns at the initial
timestep          overestimate true empirical return 
Despite this          consistently enables learning
of   wellcoordinated policy  with         achieving
best performance  Readers are referred to the supplementary material for additional sensitivity analysis of convergence trends with varying   and CERT tracelength  

  Multitasking using Distilled DecHDRQN

We now evaluate distillation of specialized DecHDRQN
policies  as learned in Section   for MTMARL     rst
approach is to forgo specialization and directly learn  
DecHDRQN using   pool of experiences from all tasks 
This approach  called MultiDQN by Rusu et al   
is susceptible to convergence issues even in singleagent 
fullyobservable settings  In Fig    we compare these approaches  where we label ours as  Distilled  and MultiHDRQN as  Multi  Both approaches were trained to perform multitasking on  agent MAMT tasks ranging from
  to   with Pf     Our distillation approach uses
no taskspeci   MLP layers  unlike Rusu et al    due
to our stronger assumptions on task relatedness and lack of

executiontime observability of task identity 
In Fig    our MTMARL approach  rst performs DecHDRQN specialization training on each task for   
epochs  and then performs distillation for    epochs   
grid search was conducted for temperature hyperparameter
in Eq           was found suitable  Note that performance is plotted only for the       and       tasks  simply
for plot clarity  see supplementary material for MTMARL
evaluation results on all tasks  MultiHDRQN exhibits
poor performance across all tasks due to the complexity involved in concurrently learning over multiple DecPOMDPs  with partial observability  transition noise  nonstationarity  varying domain sizes  varying target dynamics  and random initializations  We experimented with
larger and smaller network sizes for MultiHDRQN  with
no major difference in performance  we also include training results for    MultiHDRQN iterations in the supplementary  By contrast  our proposed MTMARL approach achieves nearnominal executiontime performance
on all tasks using   single distilled policy for each agent  
despite not explicitly being provided the task identity 

  Contribution
This paper introduced the  rst formulation and approach
for multitask multiagent reinforcement learning under
partial observability  Our approach combines hysteretic
learners  DRQNs  CERTs  and distillation  demonstrably
achieving multiagent coordination using   single joint policy in   set of DecPOMDP tasks with sparse rewards  despite not being provided task identities during execution 
The parametric nature of the capture tasks used for evaluation       variations in grid size  target assignments and
dynamics  sensor failure probabilities  makes them good
candidates for ongoing benchmarks of multiagent multitask learning  Future work will investigate incorporation of
skills  macroactions  into the framework  extension to domains with heterogeneous agents  and evaluation on more
complex domains with much larger numbers of tasks 

               KTrainingEpoch EmpiricalReturn Distilled Distilled Multi Multi Deep Decentralized Multitask MultiAgent RL under Partial Observability

Acknowledgements
The authors thank the anonymous reviewers for their insightful feedback and suggestions  This work was supported by Boeing Research   Technology  ONR MURI
Grant    and BRC Grant   

References
Amato  Christopher  Dibangoye  Jilles Steeve  and Zilberstein  Shlomo  Incremental policy generation for  nitehorizon DECPOMDPs  In ICAPS   

Banerjee  Bikramjit  Lyle  Jeremy  Kraemer  Landon  and
Yellamraju  Rajesh  Sample bounded distributed reinforcement learning for decentralized POMDPs  In AAAI 
 

Barbalios  Nikos and Tzionas  Panagiotis    robust approach for multiagent natural resource allocation based
on stochastic optimization algorithms  Applied Soft
Computing     

Bengio  Yoshua  Practical recommendations for gradientbased training of deep architectures  In Neural networks 
Tricks of the trade  pp    Springer   

Bernstein  Daniel    Givan  Robert  Immerman  Neil  and
Zilberstein  Shlomo  The complexity of decentralized
control of markov decision processes  Mathematics of
operations research     

Bowling  Michael and Veloso  Manuela  Multiagent learning using   variable learning rate  Arti cial Intelligence 
   

Brunskill  Emma and Li  Lihong 

of multitask reinforcement learning 
arXiv   

Sample complexity
arXiv preprint

Bus oniu  Lucian  Babu ska  Robert  and De Schutter  Bart 
Multiagent reinforcement learning  An overview  In Innovations in multiagent systems and applications  pp 
  Springer   

Caruana  Rich  Multitask learning  In Learning to learn 

pp    Springer   

Claus  Caroline and Boutilier  Craig  The dynamics of reinforcement learning in cooperative multiagent systems 
AAAI IAAI     

Fern andez  Fernando and Veloso  Manuela  Probabilistic policy reuse in   reinforcement learning agent 
In
Proc  of the  fth international joint conf  on Autonomous
agents and multiagent sys  pp    ACM   

Foerster  Jakob  Nardelli  Nantas  Farquhar  Gregory  Torr 
Philip  Kohli  Pushmeet  Whiteson  Shimon  et al  Stabilising experience replay for deep multiagent reinforcement learning  arXiv preprint arXiv 
 

Foerster  Jakob    Assael  Yannis    de Freitas  Nando 
and Whiteson  Shimon  Learning to communicate to
solve riddles with deep distributed recurrent Qnetworks 
arXiv preprint arXiv   

Fulda  Nancy and Ventura  Dan  Predicting and preventing
coordination problems in cooperative Qlearning systems  In IJCAI  volume   pp     

Gordon  Geoffrey    Stable function approximation in dynamic programming  In Proc  of the twelfth international
conf  on machine learning  pp     

Hausknecht  Matthew and Stone  Peter  Deep recurrent Qlearning for partially observable MDPs  arXiv preprint
arXiv   

Hinton  Geoffrey  Vinyals  Oriol  and Dean  Jeff  Distilling the knowledge in   neural network  arXiv preprint
arXiv   

Hochreiter  Sepp and Schmidhuber    urgen  Long short 

term memory  Neural comp     

Kaelbling  Leslie Pack  Littman  Michael    and Cassandra  Anthony    Planning and acting in partially observable stochastic domains  Arti cial intelligence   
   

Kapetanakis  Spiros and Kudenko  Daniel  Reinforcement
learning of coordination in cooperative multiagent systems  AAAI IAAI     

Kingma  Diederik and Ba 

Jimmy 
method for stochastic optimization 
arXiv   

Adam 

 
arXiv preprint

Lauer  Martin and Riedmiller  Martin  An algorithm for
distributed reinforcement learning in cooperative multiagent systems  In Proc  of the Seventeenth International
Conf  on Machine Learning  Citeseer   

Dutech  Alain  Buffet  Olivier  and Charpillet  Franc ois 
Multiagent systems by incremental gradient reinforcement learning  In Proc  of the International Joint Conf 
on Arti cial Intelligence  pp     

Laurent  Guillaume    Matignon  La etitia  FortPiat  Le 
et al  The world of independent learners is not markovian  International Journal of Knowledgebased and Intelligent Engineering Systems     

Deep Decentralized Multitask MultiAgent RL under Partial Observability

Lin  LongJi  Selfimproving reactive agents based on reinforcement learning  planning and teaching  Machine
learning     

Tan  Ming  Multiagent reinforcement learning  Independent vs  cooperative agents  In Proc  of the tenth international conf  on machine learning  pp     

Liu  Miao  Amato  Christopher  Liao  Xuejun  Carin 
Lawrence  and How  Jonathan    Stickbreaking policy
learning in DecPOMDPs  In Proc  of the International
Joint Conf  on Arti cial Intelligence   

Liu  Miao  Amato  Christopher  Anesta  Emily  Grif th 
   Daniel  and How  Jonathan    Learning for decentralized control of multiagent systems in large partially
observable stochastic environments  In AAAI   

Matignon  La etitia  Laurent  Guillaume    and Le FortPiat 
Nadine  Hysteretic Qlearning  an algorithm for decentralized reinforcement learning in cooperative multiagent teams  In IROS   

Matignon  Laetitia  Laurent  Guillaume    and Le FortPiat 
Nadine 
Independent reinforcement learners in cooperative markov games    survey regarding coordination
problems  The Knowledge Engineering Review   
   

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David 
Rusu  Andrei    Veness  Joel  Bellemare  Marc   
Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas   
Ostrovski  Georg  et al  Humanlevel control through
deep reinforcement learning  Nature   
   

Oliehoek  Frans    and Amato  Christopher    Concise
Introduction to Decentralized POMDPs  Springer   

Oliehoek  Frans    Spaan  Matthijs TJ  Vlassis  Nikos   
et al  Optimal and approximate qvalue functions for decentralized POMDPs  Journal of Arti cial Intelligence
Research  JAIR     

Pan  Sinno Jialin and Yang  Qiang    survey on transfer
learning  IEEE Transactions on knowledge and data engineering     

Peshkin  Leonid  Kim  KeeEung  Meuleau  Nicolas  and
Kaelbling  Leslie Pack  Learning to cooperate via policy
search  In Proc  of the Sixteenth conf  on Uncertainty in
arti cial intelligence  pp    Morgan Kaufmann
Publishers Inc   

Rusu  Andrei    Colmenarejo  Sergio Gomez  Gulcehre 
Caglar  Desjardins  Guillaume  Kirkpatrick  James  Pascanu  Razvan  Mnih  Volodymyr  Kavukcuoglu  Koray 
and Hadsell  Raia  Policy distillation  arXiv preprint
arXiv   

Sutton  Richard   and Barto  Andrew    Reinforcement
learning  An introduction  volume   MIT press Cambridge   

Tanaka  Fumihide and Yamamura  Masayuki  Multitask
reinforcement learning on the distribution of mdps  In
Computational Intelligence in Robotics and Automation 
  Proceedings    IEEE International Symposium
on  volume   pp    IEEE   

Taylor  Adam  Dusparic  Ivana  Galv anL opez  Edgar 
Clarke  Siobh an  and Cahill  Vinny  Transfer learning in multiagent systems through parallel transfer  In
Workshop on Theoretically Grounded Transfer Learning at the  th International Conf  on Machine Learning
 Poster  volume   pp    Omnipress   

Taylor  Matthew   and Stone  Peter  Transfer learning for
reinforcement learning domains    survey  Journal of
Machine Learning Research   Jul   

Torrey  Lisa and Shavlik  Jude  Transfer learning  Handbook of Research on Machine Learning Applications and
Trends  Algs  Methods  and Techniques     

Watkins  Christopher JCH and Dayan  Peter  Qlearning 

Machine learning     

Wierstra  Daan  Foerster  Alexander  Peters  Jan  and
Schmidhuber  Juergen  Solving deep memory POMDPs
with recurrent policy gradients  In International Conf  on
Arti cial Neural Networks  pp    Springer   

Wilson  Aaron  Fern  Alan  Ray  Soumya  and Tadepalli 
Prasad  Multitask reinforcement learning    hierarchical bayesian approach  In Proc  of the  th international
conf  on Machine learning  pp    ACM   

Wilson  Aaron  Fern  Alan  Ray  Soumya  and Tadepalli 
Prasad  Learning and transferring roles in multiagent
reinforcement  In Proc  AAAI  Workshop on Transfer
Learning for Complex Tasks   

Wu  Feng  Zilberstein  Shlomo  and Chen  Xiaoping  Rollout sampling policy iteration for decentralized POMDPs 
arXiv preprint arXiv   

Wu  Feng  Zilberstein  Shlomo  and Jennings  Nicholas   
Montecarlo expectation maximization for decentralized
POMDPs  In Proc  of the International Joint Conf  on
Arti cial Intelligence  pp     

Xu  Yinliang  Zhang  Wei  Liu  Wenxin  and Ferrese 
Frank  Multiagentbased reinforcement learning for opIEEE Transactions on
timal reactive power dispatch 
Systems  Man  and Cybernetics  Part    Applications
and Reviews     

