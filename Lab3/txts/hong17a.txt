ProxPDA  The Proximal PrimalDual Algorithm for Fast Distributed

Nonconvex Optimization and Learning Over Networks

Mingyi Hong   Davood Hajinezhad   MingMin Zhao  

Abstract

In this paper we consider nonconvex optimization and learning over   network of distributed
nodes  We develop   Proximal PrimalDual Algorithm  ProxPDA  which enables the network
nodes to distributedly and collectively compute
the set of  rstorder stationary solutions in  
global sublinear manner  with   rate of     
where   is the iteration counter  To the best
of our knowledge  this is the  rst algorithm that
enables distributed nonconvex optimization with
global sublinear rate guarantees  Our numerical
experiments also demonstrate the effectiveness
of the proposed algorithm 

  Introduction
We consider the following optimization problem

  cid 

  

min
  RM

      

fi   

 

where each fi                    is   nonconvex cost
function  and we assume that it is smooth and has Lipschitz
continuous gradient 
Such    nite sum problem plays   central role in machine
learning and signal information processing  Cevher et al 
  Hong et al    In particular  in the class of empirical risk minimization  ERM  problem    represents the
feature vectors to be learned  and each fi can represent   
  minibatch of  possibly nonconvex  loss functions modeling data  delity  Antoniadis et al      nonconvex activation functions of neural networks  AllenZhu   Hazan 

of

Industrial

Information Science

of
Iowa State University  Ames 

 Department
and Manufacturing SysIA  USA
tems Engineering 
 College
and Electronic Engineering  Zhejiang University  China 
Correspondence
to  Mingyi Hong  mingyi iastate edu  Davood Hajinezhad  dhaji iastate edu  MingMin Zhao  zmmblack zju edu cn 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

    nonconvex utility functions used in applications
such as resource allocation  Bjornson   Jorswieck   
Recently    number of works in machine learning community have been focused on designing fast algorithms for
solving problem   in centralized setting       SAG  Defazio et al    SAGA  Schmidt et al    and SVRG
 Johnson   Zhang    for convex problems  and  Reddi
et al    AllenZhu   Hazan    Hajinezhad et al 
    Rahimpour et al    for nonconvex problems 
In this work  we are interested in designing algorithms
that solve problem   in   distributed manner  In particular  we focus on the scenario where each fi  or equivalently  each subset of data points in the ERM problem  is
available locally at   given computing node          and
the nodes are connected via   network  Clearly  such distributed optimization and learning scenario is important for
machine learning  because in contemporary applications
such as document topic modeling and or social network
data analysis  oftentimes data corporas are stored in geographically distributed locations without any central controller managing the entire network of nodes  see  Forero
et al    Yan et al    Rahmani   Atia    Aybat
  Hamedani   
Related Works  Distributed convex optimization and
learning has been thoroughly investigated in the literature 
In  Nedic   Ozdaglar      the authors propose   distributed subgradient algorithm  DSG  which allows the
agents to jointly optimize problem   Subsequently  many
variants of DSG have been proposed  either with special assumptions on the underlying graph  or having additional
structures of the problem  see        Lobel   Ozdaglar 
  Lobel et al    Nedic   Olshevsky    The
rate of convergence for DSG is   log   
   under certain diminishing stepsize rules  Recently    number of algorithms such as the exact  rstorder algorithm  EXTRA 
 Shi et al    and DLM  Ling et al    have been
proposed  which use constant stepsize and achieve faster
     rate for convex problems  Recent works that apply distributed optimization algorithms to machine learning applications include  Scardapane et al    Aybat  
Hamedani    Scardapane   Lorenzo   
On the other hand 

there has been little work for dis 

 

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

   

 
 
 

 
 
 

   

 
 
   
   
   

    

tributed optimization and learning when the objective function involves nonconvex problems    dual subgradient
method has been proposed in  Zhu   Martinez   
which relaxes the exact consensus constraint  In  Bianchi
  Jakubowicz      stochastic projection algorithm using diminishing stepsizes has been proposed  An ADMM
based algorithm has been presented in  Hong et al   
Hajinezhad   Hong    for   special type of problem called global consensus  where all distributed nodes
are directly connected to   central controller  Utilizing
certain convexi cation decomposition technique the authors of  Lorenzo   Scutari    designed an algorithm
named NEXT  which converges to the set of stationary solutions when using diminishing stepsizes  To the best of
our knowledge  no multi agent distributed algorithm is able
to guarantee global sublinear convergence rate for problem
 
Our Contributions  In this work  we propose   proximal
primaldual algorithm  ProxPDA  for problem   over an
undirected connected network  We show that ProxPDA
converges to the set of stationary solutions of problem  
 satisfying the  rstorder optimality condition  in   globally sublinear manner  We also show that ProxPDA can
be extended in several directions to improve its practical
performance  To the best of our knowledge  this is the  rst
algorithm that is capable of achieving global sublinear convergence rate for distributed nonconvex optimization 
Further  our work reveals an interesting connection between the primaldual based algorithm ProxPDA and the
primalonly fast distributed algorithms such as EXTRA
 Shi et al    Such new insight of the connection between primaldual and primalonly algorithms could be of
independent interest for the optimization community  Finally  we generalize the theory for ProxPDA based algorithms to   challenging distributed matrix factorization
problem 

  System Model
De ne   graph           where   and   are the
node and edge sets  Let         and          Each
node       represents an agent in the network  and each
edge eij              indicates that node   and   are
neighbors  see Fig Left  Assume that each node   can
only communicate with its immediate neighbors  de ned
as Ni                    with  Ni    di  The distributed
version of problem   is given as below

       

fi xi       xi   xj               

 
min
xi RM
Clearly the above problem is equivalent to   as long as  
is connected  For notational simplicity  de ne      xi   
RNM  and           

  

  cid 

Figure    Left  An undirected Connected Network 
 Right  Incidence Matrix 
To proceed  let us introduce   few useful quantities related
to graph   
  The incidence matrix      RE   is   matrix with entires
             and              if                with       
and all the rest of the entries being zero  For example  for
the network in Fig   Left  the incidence matrix is given in
Fig   Right  De ne the extended incidence matrix as

 

         IM   REM   
where   denotes the Kronecker product 
  The Degree matrix      RN   is given by     
diag      dN   Let          IM   RQ   
  The signed and the signless Laplacian matrices  denoted
as    and    respectively  are given below
       cid     RQ                cid     RQ   

 

Using the above notations  one can verify that problem  
can be written in the following compact form

min
  RQ

     

     Ax    

 

  The ProxPDA Algorithm
The proposed algorithm builds upon the classical augmented Lagrangian  AL  method  Bertsekas    Powell 
  Let us de ne the AL function for   as

 
 

 cid Ax cid 

                  cid  Ax cid   

 
where     RQ is the dual variable        is   penalty
parameter  Let     RQ   be some arbitrary matrix to be
determined shortly  Then the proposed algorithm is given
in the table below  Algorithm  
In ProxPDA  the primal iteration     minimizes the augmented Lagrangian plus   proximal term  
BT   
We emphasize that the proximal term is critical in both the
algorithm implementation and the analysis  It is used to ensure the following key properties 
  The primal problem is strongly convex 
  The primal problem is decomposable over different
network nodes  hence distributedly implementable 
To see the  rst point  suppose BT   is chosen such that
AT     BT    cid  IQ  and that       has Lipschitz gradient 
Then by   result in  Zlobec   Theorem   we know

 cid     xr cid 

                Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

Algorithm   The ProxPDA Algorithm
  At iteration   initialize       and      RQ 
  At each iteration       update variables by 
 
 

         cid    Ax cid   

 cid Ax cid 

xr    arg min
  RQ
 
 

 

            Axr 

 cid     xr cid 

BT   

   

   

that there exists       large enough such that the objective function of     is strongly convex  To see the second
point  Let         where the absolute value is taken for
each component of    It can be veri ed that BT       
and step     becomes

  cid 

  

xr    arg min

 

fi xi     cid    Ax cid 

 

xT  

 

 
 

  arg min

 

   

  cid 

  

     xr         xr 

 
 
fi xi     cid    Ax cid     xT Dx    xT   xr 

Clearly this problem is separable over the nodes  therefore
it can be solved completely distributedly 
  The Convergence Analysis
In this section we provide convergence analysis for Algorithm   The key in the analysis is the construction of  
novel potential function  which decreases at every iteration
of the algorithm 
In particular  the constructed potential
function is   conic combination of the AL function and the
size of the violation of the consensus constraint  therefore it
measures the progress of both the primal and dual updates 
We  rst state our main assumptions below 
    The function       is differentiable and has Lipschitz
continuous gradient      

 cid               cid      cid       cid 

         RQ 

Further assume that AT     BT    cid  IQ 
    There exists   constant       such that

       

            

 cid Ax cid              RQ 

 
 

Without loss of generality we will assume that      
Below we provide   few nonconvex smooth functions that
satisfy our assumptions  all of which are commonly used
as activation functions for neural networks 
  The sigmoid function sigmoid       
  The arctan and tanh function 
  The logit function logit      ex

      

ex 

  The Analysis Steps

Below we provide the analysis of ProxPDA  First we provide   bound on the size of the constraint violation using
  quantity related to the primal iterates  Let  min denotes
the smallest nonzero eigenvalue of AT    and we de ne
wr    xr  xr   xr   xr  for notational simplicity 
We have the following result 
Lemma   Suppose Assumptions     and     are satis 
 ed  Then the following is true for ProxPDA

 cid         cid 

 
 
     
 min

 cid cid xr   xr cid cid 

 

 
 min

 cid BT Bwr cid 

 

Then we bound the descent of the AL function 
Lemma   Suppose Assumptions     and     are satis 
 ed  Then the following is true for Algorithm  
  xr          xr         cid BT   cid 
 cid       
 

 cid xr    xr cid 

 cid wr cid 

 cid 

 min

BT  

 

     
 min

 

  key observation from Lemma   is that no matter how
large   is  the rhs of   cannot be made negative  This
observation suggests that in contrast to  Hong et al   
Hajinezhad et al      the augmented Lagrangian alone
cannot serve as the potential function for ProxPDA  In
search for an appropriate potential function  we need   new
object that is decreasing in the order of    cid wr cid 
BT    The
following lemma shows that the descent of the sum of the
constraint violation and the proximal term has the desired
property 

 cid 

 

BT  

 cid cid Axr cid     cid xr   xr cid 

BT  

Lemma   Suppose Assumption     is satis ed  Then the
following is true

 
 
    cid xr    xr cid   
   
 

 cid cid Axr cid     cid xr    xr cid 
 cid cid wr cid 

 cid 
BT      cid   xr    xr cid cid   
 cid 

 
 

is

BT  

 cid cid Axr cid     cid xr    xr cid 

interesting to observe that

It
the new object 
increases
in
  cid xr    xr cid  and decreases in  cid wr cid 
BT    while the
AL behaves in an opposite manner  cf  Lemma   More
importantly  in our new object  the constant in front of
 cid xr    xr cid  is independent of   Although neither of
these two objects decreases by itself  quite surprisingly   
proper conic combination of these two objects decreases at
every iteration of ProxPDA  To precisely state the claim 
let us de ne the potential function for Algorithm   as

Pc xr  xr          xr     

 cid cid Axr cid     cid xr    xr cid 

 cid   

BT  

 

  
 

 

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

where       is some constant to be determined later  We
have the following result 

Lemma   Suppose the assumptions made in Lemmas    
  are satis ed  Then we have the following

Pc xr  xr        Pc xr  xr     
 cid xr    xr cid 
 

 cid       
 cid    

 

 

 

 cid 
 cid 

     
  cL
 min
   cid BT   cid  

 min

 cid wr cid 

BT    

Theorem   Suppose Assumption   and the conditions  
and   are satis ed  Then we have
  Eventual Consensus 

               

lim

   Axr    

lim

  Convergence to Stationary Points  Every limit point of
the iterates  xr      generated by Algorithm   converges
to   KKT point of problem   Further    xr         
  Sublinear Convergence Rate  For any given      
let us de ne   to be the  rst time that the optimality gap
reaches below       

 

From the above analysis  it is easy to see that as long as  
and   are suf ciently large  the potential function decreases
at each iteration of ProxPDA  Below we derive the precise
bounds for   and   First    suf cient condition for   is
given below  note  that       is de ned in Assumption
   

 cid   

 cid 

    max

 cid BT   cid  

 min

 

 

 

 

Second  for any given    we need   to satisfy   

   

   
 min

  cL     which implies the following

        

 cid 

   

 
 

   

        

   
 min

 

We conclude that if both   and   are satis ed  then
the potential function Pc xr  xr      decreases at
every iteration 
Our next step shows that by using the particular choices of
  and   in   and   the constructed potential function
is lower bounded 

Lemma   Suppose           are satis ed  and      
are chosen according to   and   Then the following statement holds true

             Pc xr  xr                   

Now we are ready to present the main result of this section 
To this end  de ne   xr      as the optimality gap of
problem   given by
  xr         cid xL xr     cid     cid Axr cid 
 
It is easy to see that   xr          implies that any
limit point       if it exists  is   KKT point of   that
satis es the following conditions

             AT   Ax     

 
In the following we show that the gap    not only decreases to zero  but does so in   sublinear manner 

    arg min

 

  xr         

Then for some       we have      
optimality gap   xr      converges sublinearly 

     That is  the

  Variants of ProxPDA
In this section  we discuss two important extensions of the
ProxPDA  one allows the xproblem     to be solved
inexactly  while the second allows the use of increasing
penalty parameter   In many practical applications  exactly minimizing the augmented Lagrangian may not be
easy  Therefore  we propose the proximal gradient primaldual algorithm  ProxGPDA  whose main steps are given
below

xr    arg min
  RQ
 cid Ax cid   

 

 
 

 cid    xr      xr cid     cid    Ax cid 

 cid     xr cid 

BT   

 
 

            Axr 

 

 

The analysis of this algorithm follows similar steps as that
for ProxPDA  For detailed discussion see  Hong   
Our second variant do not require to explicitly compute the
bound for   given in   Indeed  the bounds on   derived in the previous sections are the worst case bounds 
and algorithms that use stepsizes that strictly satisfy such
bounds may be slow at the beginning  In practice  one may
prefer to start with   small penalty parameter and gradually increase it  We denote this algorithm by ProxPDA IP 
whose main steps are given below

         cid    Ax cid 

xr    arg min
  RQ
 cid Ax cid   

   

 

   

 cid     xr cid 

BT   

 

 
              Axr 

 

 
Note that one can also replace       in   by  cid    xr    
xr cid  to obtain   similar variant for ProxGPDA denoted by
ProxGPDA IP  Throughout this section we will still as 

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

 cid 

  

sume that Assumption   holds true  Further  we will assume that    satis es the following conditions

 

      

 

                

              for some  nite      

max

 

Also without loss of generality we will assume that

BT    cid   

and  cid BT   cid      

 

 

Note that this is always possible  by adding an identity
matrix to BT   if necessary 
The analysis for ProxPDA IP is long and technical  therefore we refer the readers to  Hong    The key step is
to construct   new potential function  given below

      xr  xr            xr     

 

     

 

 cid Axr cid   

     

 

 cid xr   xr cid 

BT   

The insight here is that
in order to achieve the desired descent  in the potential function the coef cients for
BT   and  cid Axr cid  should be proportional to
 cid xr    xr cid 

  cid   cid  We have the following theorem regarding to the

convergence of ProxPDA IP 
Theorem   Suppose Assumption   and   are satis ed 
Suppose that   is selected such that   holds true  Then
the following hold for ProxPDA IP
  Eventual Consensus 

               

lim

   Axr    

lim

  Convergence to KKT Points  Every limit point of the
iterates  xr      generated by ProxPDA IP converges to
  KKT point of problem   Further    xr         

  Connections and Discussions
In this section we present an interesting observation which
establishes links between the socalled EXTRA algorithm
 Shi et al     developed for distributed  but convex optimization  and the ProxGPDA 
Speci cally  the optimality condition of the xupdate step
  is given by
    xr    AT       Axr     BT   xr    xr     
Utilizing the fact that AT        BT        and     
         we have

    xr    AT       Dxr       xr    

Subtracting the same equation evaluated at the previous
iteration  we obtain

    xr        xr       xr      xr    xr 
     xr   xr     

where we have used the fact that AT           
 AT Axr      xr  Rearranging terms  we have

xr    xr    
 

  cid    xr        xr cid 
  cid    xr        xr cid      xr

    xr 

 
 

         xr    
 

 
  xr    
 
   
 

        xr 

 

where in the last equality we have de ned the weight ma 
            which is   row stochastic
trix      
matrix 
Iteration   has the same form as the EXTRA algorithm
given in  Shi et al    therefore we can conclude that
EXTRA is   special case of ProxGPDA  Moreover  by appealing to our analysis in Section   it readily follows that
EXTRA works for the nonconvex distributed optimization
problem as well 
We remark that each node   can distributedly implement
iteration   by performing the following

    xr
xr 

 cid 

 

       

     
 di
     
xr
 

 
di

 cid fi xr
   cid 

       

 cid 

       fi xr 

 

xr 
    xr 

 

 
di

   

 

from its neighbors cid 

Clearly  at iteration       besides the local gradient information  node   only needs the aggregated information
   Therefore the algorithm

        xr
is distributedly implementable 
  Distributed Matrix Factorization
In this section we study   variant of the ProxPDA ProxPDA IP for the following distributed matrix factorization
problem  Ling et al   
 cid XY     cid 

     cid   cid 

         

 

min
   

 cid Xyi   zi cid     cid   cid 

    hi yi 

 
 

 
 

  cid 

  

 

      cid yi cid         

matrix         cid  

where     RM        RK     for each    yi   RK
consists of one column of         RM   is some known
   hi yi  is some convex but possibly
nonsmooth penalization term        is some given constant  for notation simplicity we have de ned         

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

It is easy to extend the above formulation to the case where
  and   both have     columns  and each yi and zi consists of   columns of   and   respectively 
We assume that       is lower bounded over dom     One
application of problem   is the distributed sparse dictionary learning problem where   is the dictionary to be
learned  each zi is   training data sample  and each yi is the
sparse coef cient corresponding to the particular training
sample zi  The constraint  cid yi cid      simply says that the
size of the coef cient must be bounded 
Consider   distributed scenario where   agents form  
graph       each having   column of     We reformulate problem   as

min

 Xi yi 

 cid Xiyi   zi cid    hi yi     cid Xi cid 
      cid yi cid          Xi   Xj               

  

 

 

 cid   

  cid 

 cid 

Let us stack all
the variables Xi  and de ne    
         XN     RNM    De ne the block signed
incidence matrix as          IM   REM NM   where
  is the standard graph incidence matrix  De ne the block
signless incidence matrix     REM NM similarly  If the
graph is connected  then the condition AX     implies
networkwide consensus  We formulate the distributed matrix factorization problem as

min

 Xi yi                   

 cid   

  cid 

 cid Xiyi   zi cid     cid Xi cid 

 
      cid yi cid          AX    

  

 

    hi yi 

 cid 

 

 

 cid 

Clearly the above problem does not satisfy Assumption
   because the objective function is not smooth  and neither  Xf         nor              is Lipschitz continuous 
The latter fact poses signi cant dif culty in algorithm development and analysis  De ne the blocksigned signless
Laplacians as

The AL function for the above problem is given by

     AT         BT   
  cid 

 cid   

 cid Xiyi   zi cid     cid Xi cid 

           

    hi yi 

 

  
 
 

 cid AX  AX cid 

   cid  AX cid   
 
where           REM   is the matrix of the dual
variable  with      RM   being the dual variable for the
consensus constraint on link         Xi   Xj            
Let us generalize Algorithm   for distributed matrix factorization given in Algorithm   In Algorithm   we have
introduced   sequence   
      which measures the size

  yr

Algorithm   ProxPDA for Distr  Matrix Factorization
  At iteration   initialize       and       
  At each iteration       update variables by 
     cid    
  
yr 
    arg min
 cid yi cid 
   cid 
       arg min
 

    zi cid 
 
 
    
              cid    AX cid 

    
  yi   zi cid    hi yi 
 cid    

 cid yi   yr

  
 
 

 

   

   

   

 cid AX  AX cid   

 

 
 

 
 

 cid                      cid 

            AX   

   

 

First note that

   cid yi   yr

is naturally distributed to each node 

of the local factorization error  We note that including the
   cid  is the key to achieve converproximal term   
gence for Algorithm  
Let us comment on the distributed implementation of
the   subproblem
the algorithm 
   
is 
information is needed to perform the uponly local
date 
the   subproblem     can also be
decomposed into   subproblems  one for each node 
 cid   
To be more precise 
let us examine the terms in
    one by one 
the term             
 cid Xiyr 
    zi cid    hi yr 
it is decomposable  Second  the term  cid    AX cid  can be
  cid 
expressed as

 cid  hence

     cid Xi cid 

   Xi cid     cid 

 cid    AX cid   

 cid  

 cid 

   Xi cid 

Second 

 cid  

 cid  

First 

that

  

 

 

  

       

      

where the sets       and      are de ned as

                                   
                                  

Similarly  we have

 cid BX    BX cid   

Xi  diX  

   

 cid 

  cid 

  

 cid 

   
 

 cid 

       

 cid AX  AX cid     cid BX  BX cid 

 
 
   cid DX    cid     

  cid 

di cid Xi cid 
   

  

where          IM   RNM NM with    being the
degree matrix  It is easy to see that the   subproblem    
is separable over the distributed agents  Finally  one can
verify that the   update step     can be implemented by
each edge       as follows
   

     cid     

                 

        

      

 cid   

 

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

 

 

 

 

 

 cid 

      

 cid 

 cid          

 cid                        cid cid   
 cid cid yi cid     cid  to denote the feasible set of     and used

To show convergence rate of the algorithm  we need the
following de nition
                   cid AX   cid     cid     
where we have de ned
                    
    
    
      
  proxh   
In the above expression  we have used  
 
    to denote the indicator function of such set  Similarly
as in Section   we can show that                  
  implies that every limit point of                is  
KKT point of problem  
Next we present the main convergence analysis for Algorithm   The proof is long and technical  therefore we refer
the readers to  Hong   
Theorem   Consider using Algorithm   to solve the distributed matrix factorization problem   Suppose that
      is lower bounded over dom      and that the penalty
parameter   together with two positive constants   and   
satis es the following conditions
  cd
 
   
 min 

         

    
 

     

 

   

   

   

 
 

 
 min
   
   
 
   cid BT   cid 

 min 

 min

 
 
  
 

   

Then in the limit  consensus will be achieved      
             

   cid     

       

  cid    

lim

 

Further  the sequences       and     are both
bounded  and every limit point generated by Algorithm  
is   KKT point of problem  
Additionally  Algorithm   converges sublinearly  Speci 
cally  for any given       de ne   to be the  rst time that
the optimality gap reaches below       

    

    arg min

                   
Then for some constant       we have      
We can see that it is always possible to  nd the tuple
           that satis es     can be solely determined by the last inequality  for  xed    the constant  
needs to be chosen large enough such that      
     
and       
      are satis ed  After   and   are  xed 
one can always choose   large enough to satisfy the  rst
three conditions  In practice  we typically prefer to choose
  as small as possible to improve the convergence speed 
Therefore empirically one can start with  for some small
           cid BT   cid 
    max      and then

   

 min

gradually increase   to  nd an appropriate   that satis es
the  rst three conditions 
We remark that Algorithm   can be extended to the case
with increasing penalty  Due to the space limitation we
omit the details here 
  Numerical Results
In this section  we demonstrate the performance of the proposed algorithms  All experiments are performed in Matlab
    on   desktop with an Intel Core TM     CPU
  GHz  and  GB RAM running Windows  
  Distributed Binary Classi cation
In this subsection  we study the problem of binary classi 
cation using nonconvex regularizers in the minibach setup
     each node stores    batch size  data points  and each
component function is given by

fi xi   

 
   

log    exp yijxT

  vij   

  

  

where vij   RM and yij     are the feature vector
and the label for the jth date point in ith agent  Antoniadis
et al    We use the parameter settings of      
      and       We randomly generated     data
points and distribute them into       nodes          
  We use the optimality gap  optgap  and constraint
violation  convio  displayed below  to measure the quality
of the solution generated by different algorithms

 cid 

  cid 

   

   
       

   

 cid    cid 

 cid cid cid cid    cid 

  

 cid cid cid cid 

optgap  

 fi zi 

   cid Ax cid  convio    cid Ax cid 

We compare the the ProxGPDA  and ProxGPDA IP
with the distributed subgradient  DSG  method  Nedic  
Ozdaglar         which is only known to work for convex cases  and the Pushsum algorithm  Tatarenko   Touri 
  The performance of all three algorithms in terms of
the consensus error and the optimality gap  averaged over
  problem instances  are presented in Fig    The penalty
parameter for ProxGPDA is chosen such that satisfy  
and    for ProxGPDA IP is set as   log    the stepsizes of the DSG algorithm and the Pushsum algorithm are
chosen as   log    and     respectively  Note that
these parameters are tuned for each algorithm to achieve
the best results  All Algorithms will stop after   iterations  It can be observed that the ProxGPDA with constant
stepsize outperforms other algorithms  The Pushsum algorithm does not seem to converge within   iterations 
To see more results  we compare different algorithms with
different number of agents in the network     The problem and the algorithms setup are set as the previous case 
We measure the optimality gap as well as the constraint
violation and the results are respectively reported in Table   and Table   In the tables Alg  Alg  Alg  Alg 

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

Figure   Results for the matrix factorization problem 

Figure   Results for the matrix factorization problem 

Table   Optimality Gap for different Algorithms

Alg 
   
   
   
   
   

Alg 
   
   
   
   
   

Alg  Alg 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

Table   Constraint Violation for different Algorithms

Alg 
   
   
   
   
   

Alg 
   
   
   
   
   

Alg  Alg 
 
 
 
 
 
 
 
 
 
 

denote ProxGPDA  ProxGPDA IP  DGS  and Pushsum
algorithms respectively  As seen  the performance of the
proposed algorithms  Alg  Alg  are signi cantly better
than DGS and PushSum 

  Distributed Matrix Factorization

In this section we consider the distributed matrix factorization problem   The training data is constructed

by randomly extracting   overlapping patches from the
      image of barbara png  each with size      
pixels  Each of the extracted patch is vectorized  resulting   training data set   of size       We consider
  network of       agents  and the columns of   are
evenly distributed among the agents  each having      
columns  We compare ProxPDA IP with the EXTRAAO algorithm proposed in       Wai   Scaglione   
Note that the EXTRAAO is also designed for   similar
distributed matrix factorization problem and it works well
in practice  However  it does not have formal convergence
proof  We initialize both algorithms with   being the   
discrete cosine transform  DCT  matrix  We set      
      and         and the results are averaged
over   problem instances  The stepsizes of the EXTRAAO is set as  AO     and  AO     In Fig   
we compare the performance of the proposed ProxPDA 
IP and the EXTRAAO  It can be observed that our proposed algorithm converges faster than the EXTRAAO  We
have observed that the EXTRAAO does have reasonably
good practical performance  however it lacks formal convergence proof 

Acknowledgment
The authors supported by National Natural Science Foundation  Grant No  CCF 

 Iterationnumber OptimalityGapProxPDA IPEXTRAAO IterationNumber ConsensusErrorProxPDA IPEXTRAAOProximal Primal Dual Algorithm for Distributed Nonconvex Optimization

References
AllenZhu     and Hazan     Variance Reduction for Faster NonConvex Optimization  In Proceedings of the  rd International
Conference on Machine Learning  ICML   

Antoniadis     Gijbels     and Nikolova     Penalized likelihood
regression for generalized linear models with nonquadratic
penalties  Annals of the Institute of Statistical Mathematics 
   

Aybat  NS  and Hamedani  EY    primaldual method for conic
constrained distributed optimization problems  Advances in
Neural Information Processing Systems   

Bertsekas        Constrained Optimization and Lagrange Multi 

plier Method  Academic Press   

Bianchi     and Jakubowicz     Convergence of   multiagent projected stochastic gradient algorithm for nonconvex optimization  IEEE Transactions on Automatic Control   
 

Bjornson     and Jorswieck     Optimal resource allocation in coordinated multicell systems  Foundations and Trends in Communications and Information Theory     

Cevher     Becker     and Schmidt     Convex optimization
for big data  Scalable  randomized  and parallel algorithms for
big data analytics  IEEE Signal Processing Magazine   
   

Defazio     Bach     and LacosteJulien     Saga    fast incremental gradient method with support for nonstrongly convex
composite objectives  In The Proceeding of NIPS   

Forero        Cano     and Giannakis        Consensusbased distributed support vector machines  Journal of Machine
Learning Research   May   

     Wai       Chang and Scaglione       consensusbased
decentralized algorithm for nonconvex optimization with application to dictionary learning  In the Proceedings of the IEEE
ICASSP   

Hajinezhad     and Hong     Nonconvex alternating direction
method of multipliers for distributed sparse principal component analysis  In IEEE Global Conference on Signal and Information Processing  GlobalSIP  IEEE   

Hajinezhad     Chang  TH  Wang     Shi       and Hong    
Nonnegative matrix factorization using admm  Algorithm and
In IEEE International Conference on
convergence analysis 
Acoustics  Speech and Signal Processing  ICASSP     

Hajinezhad     Hong     Zhao     and Wang     Nestt   
nonconvex primaldual splitting method for distributed and
In Advances in Neural Information
stochastic optimization 
Processing Systems   pp       

Hong     Decomposing linearly constrained nonconvex problems
by   proximal primal dual approach  Algorithms  convergence 
and applications  arXiv preprint arXiv   

Hong     Luo       and Razaviyayn     Convergence analysis
of alternating direction method of multipliers for   family of
nonconvex problems   
technical report  University of
Minnesota 

Hong     Razaviyayn     Luo       and Pang         uni 
 ed algorithmic framework for blockstructured optimization
involving big data  IEEE Signal Processing Magazine   
   

Johnson     and Zhang     Accelerating stochastic gradient descent using predictive variance reduction  In the Proceedings
of the Neural Information Processing  NIPS   

Ling     Xu     Yin     and Wen     Decentralized lowrank
matrix completion  In   IEEE International Conference on
Acoustics  Speech and Signal Processing  ICASSP  pp   
  March  

Ling     Shi     Wu     and Ribeiro     DLM  Decentralized
IEEE
linearized alternating direction method of multipliers 
Transactions on Signal Processing    Aug
 

Lobel     and Ozdaglar     Distributed subgradient methods for
convex optimization over random networks  Automatic Control  IEEE Transactions on     

Lobel     Ozdaglar     and Feijer     Distributed multiagent optimization with statedependent communication  Mathematical
Programming     

Lorenzo     Di and Scutari     Next  Innetwork nonconvex optimization  IEEE Transactions on Signal and Information Processing over Networks     

Nedic     and Olshevsky     Distributed optimization over timevarying directed graphs  IEEE Transactions on Automatic Control     

Nedic     and Ozdaglar     Cooperative distributed multiagent
optimization  In Convex Optimization in Signal Processing and
Communications  Cambridge University Press     

Nedic     and Ozdaglar     Distributed subgradient methods for
IEEE Transactions on Automatic

multiagent optimization 
Control       

Powell           An ef cient method for nonlinear constraints
in minimization problems  In Optimization  Academic Press 
 

Rahimpour  Alireza  Taalimi  Ali  Luo  Jiajia  and Qi  Hairong 
In
Distributed object recognition in smart camera networks 
IEEE International Conference on Image Processing  Phoenix 
Arizona  USA  IEEE   

Rahmani     and Atia       decentralized approach to robust subspace recovery  In    rd Annual Allerton Conference on
Communication  Control  and Computing  Allerton  pp   
  IEEE   

Reddi     Sra     Poczos     and Smola    

Fast incremental method for nonconvex optimization  arXiv preprint
arXiv   

Scardapane     and Lorenzo     Di    framework for parallel
and distributed training of neural networks  arXiv preprint
arXiv   

Scardapane     Fierimonte     Lorenzo     Di  Panella     and
Uncini     Distributed semisupervised support vector machines  Neural Networks     

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

Schmidt     Roux     Le  and Bach     Minimizing  nite sums
with the stochastic average gradient    Technical report 
INRIA 

Shi     Ling     Wu     and Yin     Extra  An exact  rstorder
SIAM

algorithm for decentralized consensus optimization 
Journal on Optimization     

Tatarenko     and Touri     Nonconvex distributed optimization 

  arXiv Preprint  arXiv 

Yan     Sundaram     Vishwanathan           and Qi     Distributed autonomous online learning  Regrets and intrinsic
privacypreserving properties  IEEE Transactions on Knowledge and Data Engineering     

Zhu     and Martinez     An approximate dual subgradient algorithm for multiagent nonconvex optimization  In  th IEEE
Conference on Decision and Control  CDC  pp   
 

Zlobec     On the liu oudas convexi cation of smooth programs 

Journal of Global Optimization     

