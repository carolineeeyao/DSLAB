Uniform Convergence Rates for Kernel Density Estimation

Heinrich Jiang  

Abstract

Kernel density estimation  KDE  is   popular
nonparametric density estimation method  We
  derive  nitesample highprobability density
estimation bounds for multivariate KDE under
mild density assumptions which hold uniformly
in     Rd and bandwidth matrices  We apply these results to   mode    density level
set  and   class probability estimation and attain optimal rates up to logarithmic factors  We
then   provide an extension of our results under the manifold hypothesis  Finally  we   give
uniform convergence results for local intrinsic dimension estimation 

  Introduction
KDE  Rosenblatt    Parzen    is   foundational aspect of nonparametric statistics  It is   powerful method to
estimate the probability density function of   random variable  Moreover  it is simple to compute and has played
  signi cant role in   very wide range of practical applications  Its convergence properties have been studied for  
long time with most of the work dedicated to its asymptotic
behavior or meansquared risk  Tsybakov    However  there is still   surprising amount not yet fully understood about its convergence behavior 
In this paper  we
focus on the uniform  nitesample facet of KDE convergence theory  We handle the multivariate KDE setting in
Rd which allows         bandwidth matrix    This generalizes the scalar bandwidth                     Such
  generalization is signi cant to multivariate statistics     
Silverman   Simonoff  
Our work begins by using VCbased Bernsteintype uniform convergence bounds to attain  nitesample rates for
   xed unknown density   over Rd  Theorem   These
bounds hold with highprobability under general assumptions on   and the kernel      we only require   to be

 Google 

Correspondence to  Heinrich Jiang  hein 

rich jiang gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

bounded as well as decay assumptions on the kernel functions  Moreover  these bounds hold uniformly over Rd and
bandwidth matrices   
We then show the versatility of our results by applying it to
the related areas of KDE rates under  cid  mode estimation 
density levelset estimation  and class probability estimation  We then extend our analysis to the manifold setting 
Finally  we provide uniform  nitesample results for local
intrinsic dimension estimation  Each of these contributions
are signi cant on their own 

  Contributions and Related Works
 cid  bounds for KDE  It must  rst be noted that bounding

 cid fh      where  cid fh is the KDE of   for scalar      
meansquared error Ef  cid fh       Gine   Guillon  

is   more dif cult problem than for example bounding the

 fh 

and Einmahl   Mason   give asymptotic convergence
results on KDE for    fh   Ef
In their work about
density clustering  Rinaldo   Wasserman   extends
the results of the former to obtain highprobability  nitesample bounds  This is to our knowledge the strongest and
most general uniform  nitesample result about KDE thus
far 

We show   general bound of form  cid fh             cid      
 cid log   nhd where    is   function of the kernel and the

smoothness of   at   which holds with probability     
uniformly over     Rd and    Theorem   An almost
direct consequence is that if we take   to be    older continuous then under the optimal choice for         
we have    fh      cid       with probability     
 Theorem   This matches the known lower bound  Tsybakov   
When comparing our  nitesample results to that of Rinaldo   Wasserman   there are   few notable differences  Our results hold uniformly across bandwidths and
the probability that the bounds hold are independent of the
bandwidth  in fact  holds with probability         Our
results also extends to general bandwidth matrix   
This can be signi cant to analyze KDEbased procedures
with adaptive bandwidths       when the bandwidths
change depending on the region  Then the need for bounds
which hold simultaneously over bandwidth choices be 

Uniform Convergence Rates for Kernel Density Estimation

comes clear  Such an example includes adaptive or variable KDE  Terrell   Scott    Botev et al    which
extends KDE to bandwidths that vary over the data space 
Thus our result for uniform  nitesample KDE bounds can
be seen as   re nement to existing results 
Mode estimation Estimating the modes of   distribution
has   long history      Parzen   Chernoff  
Eddy   Silverman   Cheng   Abraham
et al    Li et al    Dasgupta   Kpotufe  
Genovese et al    Jiang   Kpotufe   The
modes can be viewed as the central tendancies of   distribution and this line of work has played   signi cant role in
areas such as clustering  image segmentation  and anomaly
detection 
Much of

the early work focused on the estimator

argmaxx Rd cid fh    While many useful insights have come
tor argmaxx   cid fh    and showed that it behaves asymptotically as argmaxx Rd cid fh    where   is the data  In this

from studying this  it is dif cult to algorithmically compute  Abraham et al    turned to the simple estima 

paper  we show that this estimator is actually   rateoptimal
estimator of the mode under  nite samples with appropriate bandwidth choice  This would not have been possible
without the appropriate bounds on KDE  This approach is
similar to that of Dasgupta   Kpotufe   who apply
their kNN density estimation bounds to show that the kNN analogue of the estimator is rateoptimal 
Another approach to mode estimation that must be noted
is meanshift  Fukunaga   Hostetler    Cheng   
Comaniciu   Meer    AriasCastro et al    which
is   popular clustering algorithm amongst practitioners
based on performing   gradientascent of the KDE  Its theoretical analysis however is still far from complete  the dif 
culty comes from analyzing KDE   ability to estimate gradients  Here we are focused on density estimation rather
than density derivative estimation so our results do not appear immediately applicable to meanshift 
Density levelset estimation The problem of densitylevel
set estimation has been extensively studied      Carmichael
et al    Hartigan   Cuevas   Fraiman  
Tsybakov   Cadre   Rigollet   Vert  
Singh et al    Rinaldo   Wasserman   SteinIt involves estimating     
wart   Jiang  
          for some       and density   based on samples drawn from    This turns out to be one of the earliest
and still currently most popular means of modeling clusters
in the context of densitybased clustering  The levelsets
also in uenced much of the work on hierarchical clustering  Chaudhuri   Dasgupta   
Naturally  we must use some density estimator to get   handle on   It turns out that in order to obtain the most gen 

eral uniform recovery bounds        nitesample Hausdorff
rates  Singh et al    one also needs similar uniform
density estimation bounds  The strongest known results
thus far use density estimators that are often impractical
      histogram density estimator  in order to obtain these
theoretical rates over   practical one such as KDE  Much
of the work  especially ones using more practical density
estimators have focused on bounding metrics such as symmetric set difference  which are computed as an expectation over    This is considerably weaker than the Hausdorff metric  which imposes   uniform guarantee over each
estimated point and each point in the levelset 
We show that   simple KDEbased estimator is consistent
under the Hausdorff metric  moreoever when the bandwidth is appropriately chosen  it attains the minimax optimal rate established by Tsybakov  
Class probability estimation Class probability estimation
involves estimating the probability distribution over   set
of classes for   given input 
In other words  it is an approach to classi cation which involves  rst estimating the
marginal density           where   is the observation and
  is its category  and then choosing the category with highest probability  This densitybased approach to classi cation has been studied in many places under nonparametric assumptions       Rigollet   Chaudhuri et al 
  However  there are still aspects about its convergence properties that haven   been fully understood  In the
current work  we give uniform rates on the approximation
of          Much of the related work assume the binary
classi cation case and derive   hard classi er based on the
marginal density and compare the risk between that and
the Bayesoptimal classi er  Our work differs in that we
give uniform bounds on the recovery of the marginal density  which is   considerably stronger notion of consistency 
This is important in situations where   worstcase bound on
classi er performance is required 
Density Estimation on Manifolds Density estimation on
manifolds has received much less attention than the fulldimensional counterpart  However  understanding density
estimation in situations where the intrinsic dimension can
be much lower than the ambient dimension is becoming
ever more important  modern systems are able to capture
data at an increasing resolution while the number of degrees of freedom stays relatively constant  One of the limiting aspects of densitybased approaches is their performance in high dimensions  It takes an exponential in dimension number of samples to estimate the density   this
is the socalled curse of dimensionality  Here we give results whose rates of convergence depend on the dimension
of the manifold dM compared to   much higher ambient dimension    thus the convergence properties become much
more attractive under the manifold hypothesis 

Uniform Convergence Rates for Kernel Density Estimation

Local Intrinsic Dimension Estimation Many learning algorithms require the intrinsic dimension as an input in order
to take advantage of the lower dimensional structure that
arises  There has been much work on estimating the intrinsic dimension of the data given  nite samples         egl 
  However  the more interesting problem of estimating the local intrinsic dimension has received much less attention  The bulk of the work in this area       Costa et al 
  Houle    Amsaleg et al    provide interesting estimators  but are unable to establish strong  nitesample guarantees under nonparametric assumptions 
In
this paper  we consider   simple notion of local intrinsic
dimension based on the doubling dimension and utilize  
simple estimator  We then give   uniform  nitesample
convergence result for the estimator under nonparametric
assumptions  To the best of our knowledge  this is perhaps
the strongest  nitesample result obtained this far for this
problem 

  Background and Setup
De nition   Let   be   probability density over Rd with
corresponding distribution    Let           Xn  be  
 cid  
       samples drawn from it and let Fn denote the empirical
    Xi     
distribution                Fn       

 

We only require that   is bounded 
Assumption          
De nition   De ne kernel function     Rd      where
   denotes the nonnegative real numbers such that

 cid 

    du    

Rd

 Spherically

We make the following mild regularity assumptions on   
Assumption  
nonand
increasing 
function
            such that             for     Rd 
Assumption  
            such that for       

 Exponential Decays  There

nonincreasing

Symmetric

There

exists

exists

            exp   

Remark   These assumptions allow the popular kernels
such as Gaussian  Exponential  Silverman  uniform  triangular  tricube  Cosine  and Epanechnikov 

Assumption   implies the next result which will be useful
later on  The proof is elementary and is omitted 
Lemma   For all       we have

 cid 

Rd

      mdu    

De nition    Bandwidth matrix    is   valid bandwidth
matrix if it is   positive de nite and symmetric     matrix 
   is   unit bandwidth matrix if it is   valid bandwidth
matrix and        
Let                     be the eigenvalues of
  
Remark   In the scalar bandwidth case         
Remark   It will be useful later on that if         
where    is   unit bandwidth  then for     Rd 

 cid                         cid             

De nition    Kernel Density Estimation  Given   kernel
  and       and    the KDE for          is given
by

 cid fH     

       

 
 

  cid 

  

 

 

 

    hd

 cid 

  cid 
 cid 

  

      Xi 

 

  

     Xi 

 

 cid 

 cid 

 

  Uniform Convergence Bounds
The following is   paraphrase of Bousquet et al   
which was given in Chaudhuri   Dasgupta  
Lemma   Let   be   class of functions from   to    
with VC dimension       and     probability distribution on    Let   denote expectation with respect to   
Suppose   points are drawn independently at random from
   let En denote expectation with respect to this sample 
Then with probability at least         the following holds
for all       
  min  

 cid Eg 
where     cid       log      

      
  Eg   Eng   min 

 cid Eng    

 cid Eng   

 cid Eg 

      

Chaudhuri   Dasgupta   takes   to be the indicator
functions over balls  Dasgupta   Kpotufe   uses this
to provide similar bounds for the kNN density estimator
as in this paper  Here  we extend this idea to ellipsoids
by taking        the indicator functions over ellipsoids 
which has VC dimension           as determined by
Akama   Irie  
Lemma   De ne ellipsoid BH            cid    Rd  
   
       cid       and      BH              
Rd           is   unit bandwidth  With probability at
least         the following holds uniformly for every
      and      

           Fn            
           Fn            

 
 

     
  
     
  

Uniform Convergence Rates for Kernel Density Estimation

where         cid log     
      by taking         cid log     log    In this

Remark   We could have alternatively used    xed con 
 dence   so that our results would hold with probability at
least       This would only require   modi cation of   

paper  we have simply taken        

  KDE Bound
De ne the following which characterizes how much the
density can respectively decrease and increase from   in
       
De nition  

 ux              inf

     cid 
  cid       
     cid         

 cid 
 cid 

 ux      sup

The  rst are general upper and lower bounds for  cid fH 
Theorem    Uniform Upper and Lower Bounds for  cid fH 

  cid       

Let vd be the volume of the unit ball in Rd  Then the following holds uniformly in     Rd        unit bandwidths   
and      log       with probability at least        

 

log  
    hd  

log  
    hd  

Let         cid fH                   
Rd         ux     cid     du     and
if cid 
 cid fH                   
Rd         ux     cid     du     where    
if  cid 
   cid vd      cid cid   
         td dt    cid            
 cid 

Remark   The conditions on  ux     
    and
 
 ux     
    can be interpreted as   bound on
their expectations over the probability measure        
Rd     du     These conditions can be satis ed by
taking   suf ciently small 
Remark   The parameter   allows us the amount of slack
in the estimation errors  This is useful in   few aspects 
Oftentimes  we don   require tight bounds  especially when
reasoning about low density regions thus having   large  
allows us to satisfy the conditions more easily  In the case
that we want tight bounds  the additive error controlled by
the pointwise smoothness of the density can be encoded in
  so to not require global smoothness assumptions 
Remark   Besides the     factor  the value of   at the
end of the theorem statement is   quantity which can be
known without any   priori knowledge of    We can bound
    in terms of known quantities given smoothness assumptions near argmaxxf     This is used in later results
where knowing   value of   is important 

In order to prove Theorem   we  rst de ne the following
two functions which serve to approximate   as   stepwise
linear combination of uniform kernels 
De nition   Let      

                            

 cid 
 cid 

  

      

      

                                

  

Then it is clear that the following holds for all      

                  

The next Lemma is useful in computing the expectations of
functions over the kernel measure 
Lemma   Suppose   is an integrable function over   
and let vd denote the volume of   unit ball in Rd  Then

        du   vd  

       tdg   du 

 cid 

Rd

 cid 

 cid   

 cid   

 

 cid   

Proof of Lemma   Let Sd         denote the
surface area of the unit ball in Rd 

        du   Sd

       td        dt

Rd

 

Sd
 

 

            tddt   vd

 

 

            tddt 

 cid   

where the second last equality follows from integration by
parts and the last follows from the fact that vd   Sd   

The following follows immediately from Lemma  
Corollary  

    ux     du   vd  
    ux     du   vd  

       td    ux ht dt 
       td    ux ht dt 

 cid   
 cid   

 

 

 cid 
 cid 

Rd

Rd

 

 cid 

 cid 

  cid 

Proof of Theorem   Assume that the event that Lemma  
holds  which occurs with probability at least         We

 rst show the lower bound for  cid fH    De ne
 cid         
It is clear that  cid fH       cid        for all     Rd  Let us use
 cid         

 cid 
               Fn  BH      jh   

the following shorthand             We have

     Xi 

    hd

  

  

  

 

 

 
hd

  

We next get   handle on each Fn  BH      jh  We have

   BH      jh    vd    jh     Fj 

where Fj   max         ux jh cid      Thus 
vd    jh     cid Fj    
 cid vd          jh       

by Lemma   we have
Fn  BH    jh 
  vd    jh     Fj     
  vd    jh     Fj     
Therefore 

 

  

 

  vd

 cid       
 cid 
 cid 
 cid vd      

  vd

  

  

    

hd 

   

 

  
hd  

                       

                   ux

 cid 

 cid 

jh cid     

                

 cid 

  

 

We handle each term separately  For the  rst term  we have

 cid 
 cid   

  

lim
 

vd

  vd

 

               

    tddt    

 cid 

jh cid     

where the last equality follows from Lemma   Next  we
have

lim
 

vd

  vd

Finally  we have

  

                   ux

 cid 
 cid 
 cid   
       td    ux th cid     dt    
 cid 
 cid   

                

lim
 

  

 

       td dt    

 

 

Thus  taking       we get

 cid fH                    

 cid vd      

hd 

 cid   

 

 

       td dt

   

 

  
hd  

sup
  Rd

Uniform Convergence Rates for Kernel Density Estimation

 

 

 

  

Km

    hd

  cid 

 cid         

This gives us the lower bound  Next we derive an upper
bound  Let us rede ne

 cid      Xi
 cid 
It is clear that  cid fH       cid        for all     Rd and
 cid       
 cid 

               Fn  BH              
We next get   handle on each Fn  BH      jh  We have

where Fj   min               jh cid     

  BH    jh    vd    jh     Fj

 
hd

  

 

  

Using this  we now have

Thus by Lemma   we have
Fn  BH      jh   

  vd jh dFj      jh   cid vd   Fj    
 cid       
 cid 
 cid 
 cid vd      

                          

                      ux

 cid 

  vd

 cid 

  vd

  

  

  

 

                   

 cid 

 cid     

        

 

hd 

   
 

  
hd  

  

We proceed the same way as the other direction  Thus taking       we get

 cid vd      

  

hd 

 cid   

 

 

       td dt

 cid                     

   
 

  
hd  

The result follows 

  Supnorm Bounds for KDE
Theorem    cid  bound for    older continuous func 
              cid   
tions  If   is   oldercontinuous      
         for      cid    Rd and           then there
exists positive constant   cid      cid            such that
the following holds with probability at least        uniformly in      log       and unit bandwidths    Let
        

 cid 

 cid 

 

  

     

 

log  
    hd

 cid fH                cid   

 cid 

Uniform Convergence Rates for Kernel Density Estimation

Remark   Taking          in the above        optimizes the rates to       ignoring log factors 
Remark   We can attain similar results  although not
uniform in bandwidth  by   straightforward application
of Theorem   of Sriperumbudur   Steinwart   or
Proposition   of Rinaldo   Wasserman  

  Mode Estimation Results
The goal of this section is to utilize the KDE to estimate
the mode of   unimodal distribution from its samples  We
borrow the estimator from Abraham et al   

     argmaxx   cid fH   

where         
We adopt the mode estimation framework assumptions
from Dasgupta   Kpotufe   which are summarized
below 
De nition      is   mode of   if               for all
              for some      
Assumption  

    has   single mode   

    is twice differentiable in   neighborhood around   
    has   negativede nite Hessian at   

These assumptions lead to the following 
Lemma    Dasgupta   Kpotufe    Let   satisfy Assumption   Then there exists                  such that the
following holds 

                                             

for all     Ax where    is   connected component of     
          and    contains        
We obtain the following result for the estimation error of    
Theorem   Suppose that Assumptions         hold 
Choose   such that  log            and log   nhd   
  as       Then  for   suf ciently large depending on
                   the following holds with probability
least        

          max

    
  

 log              

log  
    hd

Remark   Taking          optimizes the above
expression so that          cid        ignoring log factors  which matches the lower bound rate for mode estimation as established in Tsybakov  
Remark   This result can be extended to multimodal
distributions as done by Dasgupta   Kpotufe   by using the connected components of nearest neighbor graphs
at appropriate empirical density levels to isolate the modes
away from each other 

 cid 

 cid 

 cid 

 

  Density Level Set Estimation Results
In this section  we estimate the density level set Lf    
               where       is given  We make the following standard regularity assumptions       Singh et al 
  To simplify the analysis  let us take          It is
clear that the results that follow can be extended to arbitrary
  
Assumption    regularity  Let           There exists           and                such that the following
holds for     Lf  Lf  

           Lf                            Lf  

where           inf   cid         cid  and   Lf        
Lf   where                            
Then we consider following estimator 

 cid Lf  

         cid fH            

 cid 

 cid 

 

log  
    hd

 cid 

where    is obtained by taking   and replacing the    

factor by   maxx Xn   cid fH    where Xn  is    xed sam 

ple of size    Then     can be viewed as   constant       
  and can be known without any   priori knowledge of  
while ensuring that      max     
We use the following Hausdorff metric 
De nition    Hausdorff Distance 

dH          max sup
   

        sup
   

       

Theorem   Suppose that Assumptions         hold and
that   is    older continuous for some           Choose
  such that  log            and log   nhd     
as       Then  for   suf ciently large depending on
                          the following holds with probability least        

dH  cid Lf   Lf       cid cid 

 cid 

 cid  log  

    hd

 cid cid 

 

 log         

where   cid cid      cid cid                     
Remark   Choosing          gives us   densitylevel set estimation rate of        This matches
the lower bound  ignoring log factors  determined by Tsybakov  
Remark   This result can be extended so that we can recover each component separately       identify which points
correspond to which connected components of Lf  
Similar to the mode estimation result  this can be done using nearest neighbor graphs at the appropriate level to isolate the connected components of Lf   away from each
other  This has been done extensively in the related area of
cluster tree estimation       Chaudhuri   Dasgupta   

Uniform Convergence Rates for Kernel Density Estimation

Remark   The global    older continuous assumption
is not required and is only here for simplicity  Smoothness
in   neighborhood around   maximizer of   is suf cient so
that for    large enough          

works      Audibert et al    Chaudhuri   Dasgupta
  Note that misclassi cation rate for   hard classi er
is   slightly different but very related to what is done here 
which is directly estimating the marginal density 

  Class Probability Estimation Results
We consider the setting where we have observations from
compact subset     Rd and labels            Given
  label    an instance     Rd has density fy    where
fy is       
Instancelabel
pairs         are thus drawn according to   mixture distribution where   is chosen from        with correspondj         and then  

ing probabilities             cid  

the uniform measure on Rd 

is chosen according to fY  
Then given         we can de ne the marginal distribution
as follows 

               gL   
gy                       

 cid 

 yfy   
   jfj   

 

The goal of class probability estimation is to learn   based
on samples           xn  yn  We de ne our estimator
to

naturally as follows  Let  cid fh   be the KDE of fy       

bandwidth matrix         

 gh       gh       gh     

 gh       

and     

   cid fh     
 cid 
     cid fh     

  cid 

  

 
 

     yi 

We make the following regularity assumption on fy 
Assumption  
       and     Rd we have

   older densities  For each    

 fy      fy   cid            cid 

where          

We state the result below 
Theorem   Suppose that Assumptions         hold 
Then for   suf ciently large depending on miny     there
exists positive constants   cid cid      cid cid               and
        miny        such that the following holds with
probability at least        uniformly in      log       

 cid 

 cid 

 cid 

 

log  
    hd

 gh               cid cid   

    

sup
  Rd

Remark   This corresponds to an optimized rate of

 cid        This matches the lower bounds up to

log factors for misclassi cation as established in related

  Extension to Manifolds
We make the following regularity assumptions which are
standard among works in manifold learning       Baraniuk
  Wakin    Genovese et al    Balakrishnan et al 
 
Assumption     is supported on   where 

    is   dM  dimensional smooth compact Riemannian
manifold without boundary embedded in compact subset     RD 

  The volume of   is bounded above by   constant 
    has condition number   which controls the cur 

vature and prevents selfintersection 

Let   be the density of   with respect to the uniform measure on   

In this section  we assume that our density estimator is       
to dM instead of the ambient dimension   

 cid fH     

 

    hdM

 cid 

  cid 

  

 

 cid 

 

     Xi 

  

 

Remark   It is then the case that we must know the intrinsic dimension dM   There are numerous known techniques for doing so     
 Kegl    Levina   Bickel 
  Hein   Audibert    Farahmand et al   

Next  we need the following guarantee on the volume of
the intersection of   Euclidean ball and    this is required
to get   handle on the true mass of the ball under   in
later arguments  The upper and lower bounds follow from
Chazal   and Lemma   of Niyogi et al    The
proof can be found in  Jiang   
Lemma    Ball Volume  If         min   dM    
and       then

            voldM               

vdM rdM

       dM   

where voldM is the volume       
  

the uniform measure on

We then give analogues to Theorem   and Theorem  
Theorem  
and Lower Bounds
CM  dM               such that

for  cid fH  There exists CM  

Case Uniform Upper

the following holds

 Manifold

Uniform Convergence Rates for Kernel Density Estimation

uniformly in              unit bandwidths    and
     log     dM with probability at least         Let
        

log  
    hdM

 cid 

    

 cid 
 cid fH                  CM
Rd         ux     cid     du     and
 cid 
 cid fH                  CM
Rd         ux     cid     du    

 cid 

   

if cid 

if cid 

log  
    hdM

 cid 

 

 cid 

 

Remark   The extra    and   term in the lower and
upper bounds respectively come from the approximation of
the volume of the fulldimensional balls        the uniform
measure on   in Lemma  

Proof Sketch of Theorem   The proof mirrors that of the
full dimensional case so we only highlight the differences 
For the lower bound  instead of

   BH    jh    vd    jh     Fj 

we have
   BH      jh    vdM  jh dM Fj       jh 
  vdM  jh dM Fj   hdM  vdM          dM    
The  rst term can be treated in the same way as before 
while the second term contributes in an extra term with an
   factor after taking the total summation 
For the upper bound  instead of

  BH      jh    vd    jh     Fj 

we have
  BH      jh    vdM    jh dM   Fj     dM  jh   
Similary  this contributes an extra term with an   factor
after taking the total summation 

then there exists positive constant   cid 

Theorem    Manifold Case  cid  bound for    older continuous functions  If   is   oldercontinuous               
     cid             for      cid    Rd with    
   
     
                dM            such that the folC cid 
lowing holds with probability at least        uniformly in
 cid 
  satisfying  log     dM        

 cid 

 cid fH                cid 
   

 cid 

    

 

log  
    hdM

sup
   

  Local Intrinsic Dimension Estimation
In this section  we only assume   distribution   on Rd
whose support is de ned as          Rd              
         and   is assumed to be compact  We use the
following notion of intrinsic dimension  which is based on
the doubling dimension and adapted from previous works
such as Houle  
De nition   For         de ne the following local intrinsic dimension wherever the quantity exists

 cid           

         

 cid 

 

ID      lim
  

log 

We can then de ne our estimator of local intrinsic dimension at       as follows 

 cid IDn        log 

 cid Fn         

Fn        

 cid 

 

The following is   uniform convergence result

for

 cid IDn     

Theorem   De ne the following

 cid           

         

 cid 

 

IDh      log 

  inf   cid   cid       cid    
 cid IDn        IDh     

Suppose
that       and   satisfy   
 
with probability at least        uniformly in        

 
Then the following holds

 

  

inf   cid   cid     cid     
 cid      gives us    nitesample

Remark   The        goes to   as       Moreover  if IDh    converges to ID    uniformly in        
then simultaneously taking       and       such that

   cid 

inf   cid   cid     cid     

uniform convergence rate for local intrinsic dimension estimation 
Remark  
mension    and   density 
 

  inf   cid   cid       cid     can be interpreted as log  
intrinsic di 
 cid  log  
the condition     
nhd     
nhd   

and the        of the bound is on the order of

If we assume   global

 cid 

 cid cid  log  

      cid fh   fh     

In fact  this result is similar to the uniform convergence
results for the KDE for estimating the smoothed density 
when  ignoring some log
factors  nhd     where fh is the density convolved with
the uniform kernel with bandwidth    It is interesting that
an analogous result comes up when estimating the intrinsic
dimension with our notion of smoothed ID 

nhd

Uniform Convergence Rates for Kernel Density Estimation

References
Abraham     Biau     and Cadre     On the asymptotic
properties of   simple estimate of the mode  ESAIM 
Probability and Statistics   

Akama  Yohji and Irie  Kei  Vc dimension of ellipsoids 

arxiv   

Amsaleg  Laurent  Chelly  Oussama  Furon  Teddy  Girard 
St ephane  Houle  Michael    Kawarabayashi  Kenichi 
and Nett  Michael  Estimating local intrinsic dimensionality  In Proceedings of the  th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining  pp    ACM   

AriasCastro  Ery  Mason  David  and Pelletier  Bruno  On
the estimation of the gradient lines of   density and the
consistency of the meanshift algorithm  Journal of Machine Learning Research   

Audibert  JeanYves  Tsybakov  Alexandre    et al  Fast
learning rates for plugin classi ers  The Annals of statistics     

Balakrishnan     Narayanan     Rinaldo     Singh    
and Wasserman     Cluster trees on manifolds  In Advances in Neural Information Processing Systems  pp 
   

Baraniuk  Richard   and Wakin  Michael    Random projections of smooth manifolds  Foundations of computational mathematics     

Botev  Zdravko    Grotowski  Joseph    Kroese  Dirk   
et al  Kernel density estimation via diffusion  The Annals
of Statistics     

Bousquet     Boucheron     and Lugosi     Introduction
to statistical learning theory  Lecture Notes in Arti cial
Intelligence   

Cadre  Benoit  Kernel estimation of density level sets 

Journal of Multivariate Analysis   

Carmichael     George     and Julius     Finding natural

clusters  Systematic Zoology   

Chaudhuri     and Dasgupta     Rates for convergence for
the cluster tree  Advances in Neural Information Processing Systems   

Chaudhuri  Kamalika and Dasgupta  Sanjoy  Rates of convergence for nearest neighbor classi cation  In Advances
in Neural Information Processing Systems  pp   
   

Chaudhuri  Probal  Ghosh  Anil    and Oja  Hannu  Classi cation based on hybridization of parametric and nonIEEE transactions on pattern
parametric classi ers 
analysis and machine intelligence   
 

Chazal     An upper bound for the volume of geodesic balls

in submanifolds of euclidean spaces   

Cheng     Mean shift  mode seeking  and clustering  IEEE
Transactions on Pattern Analysis and Machine Intelligence   

Chernoff  Herman  Estimation of the mode  Annals of the

Institute of Statistical Mathematics   

Comaniciu    and Meer     Mean shift    robust approach
IEEE Transactions on

toward feature space analysis 
Pattern Analysis and Machine Intelligence   

Costa  Jose    Girotra  Abhishek  and Hero  AO  Estimating local intrinsic dimension with knearest neighbor
graphs  In Statistical Signal Processing    IEEE SP
 th Workshop on  pp    IEEE   

Cuevas     and Fraiman       plugin approach to support

estimation  Annals of Statistics   

Dasgupta     and Kpotufe     Optimal rates for knn density
and mode estimation  Advances in Neural Information
Processing Systems   

Eddy  William    Optimum kernel estimators of the mode 

Annals of Statistics   

Einmahl  Uwe and Mason  David    Uniform in bandwidth consistency of kerneltype function estimators 
Annals of Statistics   

Farahmand     Szepesvari     and Audibert     Manifold 

adaptive dimension estimation  ICML   

Fukunaga and Hostetler  The estimation of the gradient of
  density function  with applications in pattern recognition  IEEE Transactions on Information Theory   

Genovese 

Christopher 

PeronePaci co  Marco 
Verdinelli  Isabella  and Wasserman  Larry  Minimax manifold estimation  Journal of machine learning
research   May   

Genovese  Christopher    Verdinelli  Marco PeronePaci coand Isabella  and Wasserman  Larry  Nonparametric inference for density modes  Series   Statistical Methodology   

Gine and Guillon  Rates of strong uniform consistency
for multivariate kernel density estimators  Ann  Inst    
Poincare Probab  Statist   

Uniform Convergence Rates for Kernel Density Estimation

Simonoff  Jeffrey   

Springer   

Smoothing Methods in Statistics 

Singh  Aarti  Scott  Clayton  and Nowak  Robert  Adaptive hausdorff estimation of density level sets  Annals of
Statistics   

Sriperumbudur  Bharath    and Steinwart  Ingo  Consistency and rates for clustering with dbscan  AISTATS 
 

Steinwart     Adaptive density level set clustering   th

Annual Conference on Learning Theory   

Terrell  George   and Scott  David    Variable kernel density estimation  The Annals of Statistics  pp   
 

Tsybakov     Recursive estimation of the mode of   multivariate distribution  Problemy Peredachi Informatsii 
 

Tsybakov    

Introduction to nonparametric estimation 

Springer   

Tsybakov        On nonparametric estimation of density

level sets  Annals of Statistics   

Hartigan     Clustering algorithms  Wiley   

Hein  Matthias and Audibert  JeanYves  Intrinsic dimensionality estimation of submanifolds in rd  ICML   

Houle  Michael    Dimensionality  discriminability  denIn Data Mining Worksity and distance distributions 
shops  ICDMW    IEEE  th International Conference on  pp    IEEE   

Jiang  Heinrich  Density level set estimation on manifolds with dbscan  International Conference on Machine
Learning  ICML   

Jiang  Heinrich and Kpotufe  Samory  Modalset estima 

tion with an application to clustering  AISTATS   

Kegl  Balazs  Intrinsic dimension estimation using packing

numbers  NIPS   

  egl  Bal azs  Intrinsic dimension estimation using packing
numbers  In Advances in neural information processing
systems  pp     

Levina  Elizaveta and Bickel  Peter    Maximum likelihood

estimation of intrinsic dimension  NIPS   

Li     Ray     and Lindsay       nonparametric statistical
approach to clustering via mode identi cation  Journal
of Machine Learning Research   

Niyogi     Smale     and Weinberger     Finding the homology of submanifolds with high con dence from random samples  Discrete and Computational Geometry 
 

Parzen  Emanual  On estimation of   probability density
function and mode  Annals of mathematical statistics 
 

Rigollet     and Vert     Fast rates for plugin estimators of

density level sets  Bernoulli     

Rigollet  Philippe  Generalization error bounds in semisupervised classi cation under the cluster assumption 
Journal of Machine Learning Research   Jul 
   

Rinaldo     and Wasserman     Generalized density clus 

tering  Annals of Statistics   

Rosenblatt     Remarks on some nonparametric estimates

of   density function  Ann  Math  Statist   

Silverman     Density Estimation for Statistics and Data

Analysis  CRC Press   

Silverman        Using kernel density estimates to investigate multimodality  Journal of the Royal Statistical Society  Series    Methodological   

