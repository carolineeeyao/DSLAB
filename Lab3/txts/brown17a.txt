Reduced Space and Faster Convergence in
ImperfectInformation Games via Pruning

Noam Brown   Tuomas Sandholm  

Abstract

Iterative algorithms such as Counterfactual Regret
Minimization  CFR  are the most popular way
to solve large zerosum imperfectinformation
games  In this paper we introduce BestResponse
Pruning  BRP  an improvement to iterative algorithms such as CFR that allows poorlyperforming
actions to be temporarily pruned  We prove that
when using CFR in zerosum games  adding BRP
will asymptotically prune any action that is not
part of   best response to some Nash equilibrium 
This leads to provably faster convergence and
lower space requirements  Experiments show that
BRP results in   factor of   reduction in space 
and the reduction factor increases with game size 

  Introduction
Imperfectinformation extensiveform games model strategic multistep scenarios between agents with hidden information  such as auctions  security interactions  both physical
and virtual  negotiations  and military situations  Typically
in imperfectinformation games  one wishes to  nd   Nash
equilibrium  which is   pro le of strategies in which no
player can improve her outcome by unilaterally changing
her strategy    linear program can  nd an exact Nash equilibrium in twoplayer zerosum games containing fewer
than about   nodes  Gilpin   Sandholm    For
larger games  iterative algorithms are used to converge to  
Nash equilibrium  There are   number of such iterative algorithms  Heinrich et al    Nesterov    Hoda et al 
  Pays    Kroer et al    the most popular of
which is Counterfactual Regret Minimization  CFR   Zinkevich et al    CFR minimizes regret independently at
each decision point in the game  CFR    variant of CFR 

 Computer Science Department  Carnegie Mellon University  Pittsburgh  PA  USA  Correspondence to  Noam
Brown  noamb cs cmu edu  Tuomas Sandholm  sandholm cs cmu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

was used to essentially solve Limit Texas Hold em  the
largest imperfectinformation game ever to be essentially
solved  Bowling et al   
Both computation time and storage space are dif cult challenges when solving large imperfectinformation games 
For example  solving Limit Texas Hold em required nearly
  million core hours and   complex  domainspeci   streaming compression algorithm to store the   TiB of uncompressed data in only   TiB  This data had to be repeatedly
decompressed from disk into memory and then compressed
back to disk in order to run CFR   Tammelin et al   
In certain situations  pruning can be applied to speed up the
traversal of the game tree in iterative algorithms  Lanctot
et al    Brown   Sandholm      Brown et al   
However  these past pruning techniques do not reduce the
space needed to solve   game and lack theoretical guarantees
for improved performance 
In this paper we introduce BestResponse Pruning  BRP 
  new form of pruning for iterative algorithms such as CFR
in large imperfectinformation games  BRP leverages the
fact that in iterative algorithms we are typically interested in
performance against the opponent   average strategy over all
iterations  and that the opponent   average strategy cannot
change faster than   rate of  
    where   is the number of iterations conducted so far  Thus  if partway through   run one
of our actions has done very poorly relative to other available
actions against the opponent   average strategy  then after
just   few more iterations the opponent   average strategy
cannot change suf ciently for the poorlyperforming action
to now be doing well against the opponent   updated average
strategy  In fact  we can bound how much an action   performance can improve over any number of iterations against
the opponent   average strategy  So long as the upper bound
on that performance is still not competitive with the other
actions  then we can safely ignore the poorlyperforming
action 
BRP provably reduces the computation time needed to solve
imperfectinformation games  Additionally    primary advantage of BRP is that in addition to faster convergence 

 Earlier versions of this paper referred to BestResponse Prun 

ing as Total RegretBased Pruning  Total RBP 

Reduced Space and Faster Convergence in ImperfectInformation Games via Pruning

it also reduces the space needed over time  Speci cally 
once pruning begins on   branch  BRP discards the memory
allocated on that branch and does not reallocate the memory
until pruning ends and the branch cannot immediately be
pruned again  In Section   we prove that after enough
iterations of CFR are completed  space for certain pruned
branches will never need to be allocated again  Speci 
cally  we prove that when using BRP it is asymptotically
only necessary to store data for parts of the game that are
reached with positive probability in   best response to  
Nash equilibrium  This is extremely advantageous when
solving large imperfectinformation games  which are often
constrained by space and in which the set of best response
actions may be orders of magnitude smaller than the size of
the game  Schmid et al   
While BRP still requires enough memory to store the entire
game in the early iterations  recent work has shown that
these early iterations can be skipped in CFR  and possibly
other iterative algorithms  by  rst solving   lowmemory
abstraction of the game and then using its solution to warm
start CFR in the full game  Brown   Sandholm   
BRP   reduction in space is also helpful to the Simultaneous Abstraction and Equilibrium Finding  SAEF  algorithm  Brown   Sandholm      which starts CFR with
  small abstraction of the game and progressively expands
the abstraction while also solving the game  SAEF   space
requirements increase the longer the algorithm runs  and
may eventually exceed the constraints of   system  BRP
can counter this increase in space by eliminating the need
to store suboptimal paths of the game tree 
BRP shares some similarities to the earlier pruning algorithm RegretBased Pruning  which has shown empirical
evidence of improving the performance of CFR  In contrast 
this paper proves that CFR converges faster when using
BRP  because suboptimal paths in the game tree will only

need to be traversed   cid  ln    cid  times over   iterations  We

also prove that BRP uses asymptotically less space  while
RegretBased Pruning does not reduce the space needed
to solve   game  Moreover  BestResponse Pruning easily generalizes to iterative algorithms beyond CFR such as
Fictitious Play  Heinrich et al   
The magnitude of the gains in speed and space that BRP
provides varies depending on the game  It is possible to
construct games where BRP provides no bene    However 
if there are many suboptimal actions in the game as is
frequently the case in large games BRP can speed up
CFR by multiple orders of magnitude and require orders of
magnitude less space  Our experiments show an order of
magnitude space reduction already in mediumsized games 
and   reduction factor increase with game size 

  Background
In   twoplayer zerosum imperfectinformation extensiveform game there are two players          Let   be
the set of all possible histories  nodes  in the game tree 
represented as   sequence of actions  The actions available
in   history is      and the player who acts at that history
is                where   denotes chance  Chance plays
an action          with    xed probability  The history   cid 
reached after action   in   is   child of    represented by
          cid  while   is the parent of   cid  More generally    cid  is
an ancestor of    and   is   descendant of   cid  represented
by   cid   cid     if there exists   sequence of actions from   cid  to   
      are terminal histories  For each player        there
is   payoff function ui        cid  where          De ne
     maxz   ui      minz   ui    and     maxi    
Imperfect information is represented by information sets for
each player       by   partition Ii of                   
For any information set     Ii  all histories      cid      are
indistinguishable to player    so            cid       is
the information set   where              is the player  
such that     Ii       is the set of actions such that for
all                   
 Ai    maxI Ii       and
      maxi  Ai  De ne       to be the maximum payoff
reachable from   history in    and      to be the minimum 
That is          maxz         cid   uP       and       
minz         cid   uP       De ne                   
to be the range of payoffs reachable from   history in   
Similarly                  and        are the maximum 
minimum  and range of payoffs  respectively  reachable
from   history in   after taking action    De ne         to
be the set of information sets reachable by player       after
taking action    Formally    cid            if for some history
      and   cid      cid         cid    cid  and              cid 
  strategy       is   probability vector over      for player
  in information set    The probability of   particular action
  is denoted by          Since all histories in an information
set belonging to player   are indistinguishable  the strategies
in each of them must be identical  That is  for all       
              and                     De ne    to be  
probability vector for player   over all available strategies
   in the game    strategy pro le   is   tuple of strategies 
one for each player  ui        is the expected payoff
for player   if all players play according to the strategy
pro le  cid       cid  If   series of strategies are played over  
iterations  then   
         cid   cid        cid   cid     is the joint probability of
reaching   if all players play according to    
      is the
contribution of player   to this probability  that is  the probability of reaching   if all players other than    and chance 
always chose actions leading to          is the contribution of all players other than    and chance        cid  is the

      
 

 cid 

   

 

 

Reduced Space and Faster Convergence in ImperfectInformation Games via Pruning

     

 cid 

 cid 

as   

      

       

probability of reaching   cid  given that   has been reached 
and   if    cid cid    cid  In   perfectrecall game        cid        Ii 
             cid  In this paper we focus on perfectrecall
games  Therefore  for           de ne               for
       Moreover    cid   cid    if for some   cid      cid  and some
         cid   cid     Similarly    cid       cid    if   cid       cid     The
      for an information set   is de ned
average strategy   
  
      
 
     
     
  best response to   
such that
ui 
         Nash equilibi         max cid 
rium   is   strategy pro le where every player plays  
best response      ui 
     
Nash equilibrium strategy for player   as   strategy    that
is part of any Nash equilibrium  In twoplayer zerosum
games  if    and    are both Nash equilibrium strategies 
then  cid       cid  is   Nash equilibrium  An  equilibrium
as   strategy pro le   such that     ui 
         
max cid 

 
is   strategy  

    ui cid 
     

    ui cid 

      max cid 

     

    ui cid 

    

    

   

 

  Counterfactual Regret Minimization

Counterfactual Regret Minimization  CFR  is   popular algorithm for extensiveform games in which the strategy
vector for each information set is determined according to  
regretminimization algorithm  Zinkevich et al    We
use regret matching  RM   Hart   MasColell    as the
regretminimization algorithm  but the material presented in
this paper also applies to other regret minimizing algorithms
such as Hedge  Brown et al   
The analysis of CFR makes frequent use of counterfactual
value  Informally  this is the expected utility of an information set given that player   tries to reach it  For player   at
information set   given   strategy pro le   this is de ned
as

 cid 

 cid 
 cid 
 cid 

   

   

 cid      ui   cid cid 
 cid 
 cid          ui   cid cid 
 cid 

   

   

 

 

      

     

The counterfactual value of an action   is

         

     

  counterfactual best response  Moravcik et al   
 CBR  is   strategy similar to   best response  except that
it maximizes counterfactual value even at information sets
that it does not reach due to its earlier actions  Specifically    counterfactual best response to    is   strategy CBR    such that if CBR             then
  cid CBR     cid         maxa cid    cid CBR     cid      cid 
The counterfactual best response value CBV       is similar to counterfactual value  except that player          
plays according to   CBR to     Formally  CBV        
  cid CBRi     cid   

action   in   on iteration   is RT         cid 

Let    be the strategy pro le used on iteration    The instantaneous regret on iteration   for action   in information
set   is rt            
    and the regret for
    rt      
         max RT          and RT      
 cid 
       Regret for player   in the entire game is

Additionally  RT
maxa RT
RT
    max cid 

         cid   

           ui  

 cid ui cid 

            

   

   

In regret matching    player picks   distribution over actions
in an information set in proportion to the positive regret on
those actions  Formally  on each iteration       player  
selects actions          according to probabilities

      cid     

  cid  RT
otherwise

 
If   player plays according to RM on every iteration then on

 

RT

     cid   

           

     
  cid      RT

 cid 
       

if  cid 
iteration     RT          cid     
     cid 

   

  Ii

RT     So  as       RT

If   player plays according to CFR in every iteration then
     
In
RT
twoplayer zerosum games  if both players  average re 
   cid  form    
gret RT
equilibrium  Waugh et al    Thus  CFR constitutes
an anytime algorithm for  nding an  Nash equilibrium in
zerosum games 

      their average strategies  cid  

      

 

 

  Prior Approaches to Pruning

This section reviews forms of pruning that allow parts of the
game tree to be skipped in CFR  In vanilla CFR  the entire
game tree is traversed separately for each player historyby history  On each traversal  the regret for each action
of   history   information set is updated based on the expected value for that action on that iteration  weighed by the
probability of opponents taking actions to reach the history
 that is  weighed by   
      However  if   history   is
reached on iteration   in which   
          then from  
and   the strategy at   contributes nothing on iteration
  to the regret of       or to the information sets above
it  Moreover  any history that would be reached beyond  
would also contribute nothing to its information set   regret
     cid      for every history   cid  where    cid    cid 
because   
and      cid          Thus  when traversing the game tree
for player    there is no need to traverse beyond any history
  when   
          The bene   of this form of pruning 
which we refer to as partial pruning  varies depending on
the game  but empirical results show   factor of   improvement in some games  Lanctot et al   
While partial pruning allows one to prune paths that an opponent reaches with zero probability  RegretBased Pruning
allows one to also prune paths that the traverser reaches

Reduced Space and Faster Convergence in ImperfectInformation Games via Pruning

with zero probability  Brown   Sandholm      However  this pruning is necessarily temporary  Consider an
action          such that              and assume that it
is known action   will not be played with positive probability until some farfuture iteration   cid   in RM  this would be
the case if Rt        cid    Since action   is played with zero
probability on iteration    so from   the strategy played and
reward received following action    that is  in         will
not contribute to the regret for any information set preceding
action   on iteration    In fact  what happens in         has
no bearing on the rest of the game tree until iteration   cid  is
reached  So one could  in theory   procrastinate  in deciding
what happened beyond action   on iteration           cid 
until iteration   cid 
However  upon reaching iteration   cid  rather than individually making up the   cid      iterations over         one can
instead do   single iteration  playing against the average of
the opponents  strategies in the   cid      iterations that were
missed  and declare that strategy was played on all the   cid     
iterations  This accomplishes the work of the   cid   iterations
in   single traversal  Moreover  since player   never plays
action   with positive probability between iterations   and
  cid  that means every other player can apply partial pruning
on that part of the game tree for iterations   cid       and skip
it completely  This  in turn  means that player   has free
rein to play whatever they want in         without affecting
the regrets of the other players  In light of that  and of the
fact that player   gets to decide what is played in        
after knowing what the other players have played  player
  might as well play   strategy that ensures zero regret for
all information sets   cid            in the iterations   to   cid   
CBR to the average of the opponent strategies on the   cid     
iterations would qualify as such   zeroregret strategy 
RegretBased Pruning only allows   player to skip traversing         for as long as              Thus 
in
RM  if Rt           we can prune the game tree
beyond action   from iteration    until iteration    so
               

          cid   

long as  cid   
 cid   

       

      

      

   

  BestResponse Pruning
This section describes the behavior of BRP  In particular
we focus on the case where BRP is applied to the most
popular family of iterative algorithms  CFR  BRP begins
pruning an action in an information set whenever playing
perfectly beyond that action against the opponent   average
strategy  that is  playing   CBR  still does worse than what
has been achieved in the iterations played so far  that is 
    Pruning continues for the minimum number
of iterations it could take for the opponent   average strategy
to change suf ciently such that the pruning starting condition  that is  playing   CBR beyond the action against the

 cid  

      

      

opponent   average strategy does worse than what has been
achieved in the iterations so far  no longer holds  When
pruning ends  BRP calculates   CBR in the pruned branch
against the opponent   average strategy over all iterations
played so far  and sets regret in the pruned branch as if that
CBR strategy were played on every iteration played in the
game so far even those that were played before pruning
began 
While using   CBR works correctly when applying BRP to
CFR  it is also sound to choose   strategy that is almost  
CBR  formalized later in this section  as long as that strat 

 cid RT
      cid   cid   cid         In

egy ensures cid 

practice  this means that the strategy is close to   CBR  and
approaches   CBR as       We now present the theory
to show that such   nearCBR can be used  However  in
practice CFR converges much faster than the theoretical
bound  so the potential function is typically far lower than
the theoretical bound  Thus  while choosing   nearCBR
rather than an exact CBR may allow for slightly longer pruning according to the theory  it may actually result in worse
performance  All of the theoretical results presented in this
paper  including the improved convergence bound as well
as the lower space requirements  still hold if only   CBR is
used  and our experiments use   CBR  Nevertheless  clever
algorithms for deciding on   nearCBR may lead to even
better performance in practice 
We de ne   strategy         as      near counterfactual
best response     near CBR  to    if for all   belonging to
player  

If xT

      

where xT

 cid 
 cid   cid        

 cid   cid        cid        cid        cid   cid 

  can be any value in the range     xT

    xT
 
   
 
   
      then      near CBR is
always   CBR  The set of strategies that are    near
CBRs to    is represented as         We also
de ne the    near counterfactual best response value
    cid       and
as               min cid 
           min cid 
When applying BRP to CFR  an action is pruned only if it
would still have negative regret had      near CBR against
the opponent   average strategy been played on every iteration  Speci cally  on iteration   of CFR with RM  if

            cid cid 
    cid   

            cid cid 

  cid               cid      cid 

  

   

   

 

Reduced Space and Faster Convergence in ImperfectInformation Games via Pruning

then         can be pruned for

 cid  

   cid   

      

                     

               

 
iterations  After those    cid  iterations are over  we calculate
         cid near CBR in         to the opponent   average
strategy and set regret as if that        cid near CBR had been
played on every iteration  Speci cally  for each            cid 
we set     

                  cid 

       so that

       cid 

  

        cid       cid cid cid         cid 

  

       cid 

RT     cid 

      cid       cid cid 

  

   

   

   cid    cid            cid 
       cid 

 
and for every information set   cid            we
   cid   
set    
         cid 
RT     cid 

  
   cid  so that

   cid    cid  and    

   cid    cid   

       cid 

  

       cid 

   cid    cid             cid 

  

       cid 

 

 cid        cid cid cid        cid 

  

   cid cid 

Theorem   proves that if   holds for some action  then the
action can be pruned for    cid  iterations  where    cid  is de ned
in   The same theorem holds if one replaces the    near
counterfactual best response values with exact counterfactual best response values  The proof for Theorem   draws
from recent work on warm starting CFR using only an average strategy pro le  Brown   Sandholm    Essentially 
we warm start regrets in the pruned branch using only the
average strategy of the opponent and knowledge of    
Theorem   Assume   iterations of CFR with RM have
been played in   twoplayer zerosum game and assume
    where            Let

  cid               cid     cid  

     cid              cid 

      

  

 cid 

      

       cid 

           

        cid cid         cid 

      cid     cid       cid 

   cid     cid 
If both players
play according to CFR with RM for the next    cid  itera 
 cid          except that
tions in all information sets   cid cid 
then
       is set to zero and     is renormalized 
    Moreover  if one then sets    
       for
each            cid  and    
   cid    cid 
for each   cid            then after    cid cid  additional iterations
of CFR with RM  the bound on exploitability of        cid    cid cid 
is no worse than having played        cid       cid cid  iterations of
CFR with RM without BRP 

                  cid 
   cid    cid             cid 

      
  

       cid 

       cid 

  

 cid  

In practice  rather than check whether   is met for an
action on every iteration  one could only check actions that

 In practice  only the sums  cid  
 cid  

       or RT        are stored 

      

      

    and either

have very negative regret  and do   check only once every
several iterations  This would still be safe and would save
some computational cost of the checks  but would lead to
less pruning 
Similar to RegretBased Pruning  the duration of pruning in
BRP can be increased by giving up knowledge beforehand
of exactly how many iterations can be skipped  From  
and   we see that rT            
Thus  if   
      is very low  then   would continue to hold
for more iterations than   guarantees  Speci cally  we can
prune         from iteration    until iteration    as long as

     cid                cid 

 cid              cid   

  

  cid 

                  cid 

  

    

  

   

   

 

  BestResponse Pruning Requires Less Space

  key advantage of BRP is that setting the new regrets according to   and   requires no knowledge of what the
regrets were before pruning began  Thus  once pruning
begins  all the regrets in         can be discarded and the
space that was allocated to storing the regret can be freed 
That space need only be reallocated once   ceases to hold
and we cannot immediately begin pruning again  that is 
  does not hold  Theorem   proves that for any information set   and action          that is not part of   best
response to   Nash equilibrium  there is an iteration TI  
such that for all     TI    action   in information set  
 and its descendants  can be pruned  Thus  once this TI  
is reached  it will never be necessary to allocate space for
regret in         again 
Theorem  
zerosum game 
if
for every opponent Nash equilibrium strategy  
      
CBV  
           then there exists  
TI   and          such that after     TI   iterations of
CFR  CBV                cid  

                CBV  

In   twoplayer

       

      

   

 

While such   constant TI   exists for any suboptimal action 
BRP cannot determine whether or when TI   is reached 
Thus  it is still necessary to check whether   is satis ed
whenever   no longer holds  and to recalculate how much
longer         can safely be pruned  This requires the algorithm to periodically calculate   best response  or nearbest
response  in         However  this  near best response
calculation does not require knowledge of regret in        

 If CFR converges to   particular Nash equilibrium  then this
condition could be broadened to any information set   and action
         that is not   best response to that particular Nash equilibrium  While empirically CFR does appear to always converge
to   particular Nash equilibrium  there is no known proof that it
always does so 

Reduced Space and Faster Convergence in ImperfectInformation Games via Pruning

  

 

  

 

 

so it is still never necessary to store regret after iteration
TI   is reached 
While it is possible to discard regrets in         without
penalty once pruning begins  regret is only half the space
requirement of CFR  Every information set   also stores  

sum of the strategies played cid  

 cid  
         cid  which is

           

normalized once CFR ends in order to calculate        Fortunately  if action   in information set   is pruned for long
enough  then the stored cumulative strategy in         can
also be discarded at the cost of   small increase in the distance of the  nal average strategy from   Nash equilibrium 
Speci cally  if     
  where   is some constant  then setting               and renormalizing       
and setting       cid    cid      for   cid            can result in
at most     
higher exploitability for the whole strategy
     Since CFR only guarantees that    is      
   
 
 
Nash equilibrium anyway      
            cid       cid 
 cid  
 cid  
is only   constant factor
of the bound  If an action is pruned from    cid  to     then
 cid  
            cid   
for long enough  then eventually cid  
 cid  
            cid  could be set to
for any    so cid  
    Thus  if an action is pruned

  
zero  as well as all descendants of        while suffering at
 
most   constant factor increase in exploitability  As more
iterations are played  this penalty will continue to decrease
and eventually be negligible  The constant   can be set by
the user    higher   allows the average strategy to be discarded sooner  while   lower   reduces the potential penalty
in exploitability 
We de ne IS as the set of information sets that are not guaranteed to be asymptotically pruned by Theorem   Specifically      IS if    cid      cid    cid  for some   cid  and   cid        cid 
such that for every opponent Nash equilibrium strategy
      cid  CBV  
 
      cid       cid  Theorem   implies the following 

      cid     cid    cid    CBV  

  

 

 

 

  

Corollary   In   twoplayer zerosum game with some
threshold on the average strategy   
for       after
 
   nite number of iterations CFR with BRP requires only

  cid IS   cid  space 

 

  rather than   

Using   threshold of  
does not change the
theoretical properties of the corollary  and may lead to faster
convergence in some situations  but it may also result in  
slower reduction in the space used by the algorithm  though
the asymptotic space used is identical  In particular  if
BRP can be extended to  rstorder methods that converge to
an  Nash equilibrium in     
    iterations rather than     
   
iterations  such as the Excessive Gap Technique  Hoda et al 
  Kroer et al    then   threshold of  
  may be more
appropriate when those algorithms are used    threshold of
  may also be preferable when using an algorithm which
 

empirically converges to an  Nash equilibrium in faster
than     

    iterations  such as CFR  on some games 

  BestResponse Pruning Converges Faster

We now prove that BRP in CFR speeds up convergence to an
 Nash equilibrium  Section   proved that CFR with BRP
converges in the same number of iterations as CFR alone  In
this section  we prove that BRP allows each iteration to be
traversed more quickly  Speci cally  if an action         
is not   CBR to   Nash equilibrium  then         need only
be traversed   ln     times over   iterations  Intuitively 
as both players converge to   Nash equilibrium  actions that
are not   counterfactual best response will eventually do
worse than actions that are  so those suboptimal actions
will accumulate increasing amounts of negative regret  This
negative regret allows the action to be safely pruned for
increasingly longer periods of time 
Speci cally  let       be the set of histories where        
if       and action   is part of some CBR to some Nash
equilibrium  Formally    contains   and every history      
such that       and CBV  
        
for some Nash equilibrium  
Theorem   In   twoplayer zerosum game  if both players
choose strategies according to CFR with BRP  then conduct 

ing   iterations requires only   cid           ln    cid  nodes

              CBV  

to be traversed 

The de nition of   uses properties of the Nash equilibria
of the game  and an action          not in   is only
guaranteed to be pruned by BRP after some TI   is reached 
which also depends on the Nash equilibria of the game 
Since CFR converges to only an  Nash equilibrium  CFR
cannot determine with certainty which nodes are in   or
when TI   is reached  Nevertheless  both   and TI   are
 xed properties of the game 

  Experiments
We compare the convergence speed of BRP to RegretBased
Pruning  to only partial pruning  and to no pruning at all 
We also show that BRP uses less space as as more iterations
are conducted  unlike prior pruning algorithms  The experiments are conducted on Leduc Hold em  Southey et al 
  and Leduc   Brown   Sandholm      Leduc
Hold em is   common benchmark in imperfectinformation
game solving because it is small enough to be solved but
still strategically complex  In Leduc Hold em  there is  
deck consisting of six cards  two each of Jack  Queen  and
King  There are two rounds  In the  rst round  each player
places an ante of   chip in the pot and receives   single private card    round of betting then takes place with   twobet
maximum  with Player   going  rst    public shared card is
then dealt face up and another round of betting takes place 

Reduced Space and Faster Convergence in ImperfectInformation Games via Pruning

Again  Player   goes  rst  and there is   twobet maximum 
If one of the players has   pair with the public card  that
players wins  Otherwise  the player with the higher card
wins  The bet size in the  rst round is   chips  and   chips in
the second round  Leduc  is like Leduc Hold em but larger 
there are   bet sizes to choose from  In the  rst round  
player may bet         or   chips  while in the second
round   player may bet         or   chips 
Nodes touched is   hardware and implementationindependent proxy for time which we use to measure performance of the various algorithms  Overhead costs are
counted in nodes touched  CFR  is   variant of CFR in
which    oor on regret is set at zero and each iteration
is weighted linearly in the average strategy  that is  iteration   is weighted by    rather than each iteration being
weighted equally  Since RegretBased Pruning can only
prune negativeregret actions  RegretBased Pruning modi 
 es the de nition of CFR  so that regret can be negative  but
immediately jumps up to zero as soon as regret increases 
BRP does not require this modi cation  Still  BRP also
modi es the behavior of CFR  because without pruning 
CFR  would put positive probability on an action as soon
as its regret increases  while BRP waits until pruning is over 
This is not  by itself    problem  However  CFR   linear
weighting of the average strategy is only guaranteed to converge to   Nash equilibrium if pruning does not occur  While
both RegretBased Pruning and BRP do well empirically
with CFR  the convergence is noisy  This noise can be
reduced by using the lowestexploitability average strategy
pro le found so far  which we do in the experiments  BRP
does not do as well empirically with the linearaveraging
component of CFR  Thus  for BRP we only measure performance using RM  with CFR  which is the same as CFR 
but without linear averaging  CFR  with and without linear
averaging has the same theoretical performance as CFR 
but CFR  does better empirically  particularly with linear
averaging 
Figure   and Figure   show the reduction in space needed
to store the average strategy and regrets for BRP for various values of the constant threshold    where an action  
probability is set to zero if it is reached with probability
less than   
in the average strategy  as we explained in
Section   In both games    threshold between   and
  performed well in both space and number of iterations 
with the lower thresholds converging somewhat faster and
the higher thresholds reducing space faster  We also tested
thresholds below   but the speed of convergence was essentially the same as when using   In Leduc  all variants
resulted in   quick dropoff in space to about half the initial

 

 Exploitability is no harder to compute than one iteration of
CFR or CFR  Snapshots are not plotted at every iteration but
only after every   nodes touched except for the  rst
few snapshots 

amount  In Leduc    threshold of   resulted in about  
factor of   reduction for both CFR with RM and CFR with
RM  This space reduction factor appears to continue to
increase 

Figure   Convergence and space required for CFR using RM and
RM  with bestresponse pruning in Leduc Hold em  The yaxis
on the top graph is linear scale 

Figure   Convergence and space required for CFR using RM and
RM  with bestresponse pruning in Leduc  The yaxis on the
top graph is linear scale 

Figure   and Figure   compare the convergence rates of
BRP  RegretBased Pruning  and only partial pruning for
CFR with RM  CFR with RM  and CFR  In Leduc  BRP
and RegretBased Pruning perform comparably when added
to CFR  RegretBased Pruning with CFR  does signi cantly
better  while BRP with CFR using RM  sees no improve 

Reduced Space and Faster Convergence in ImperfectInformation Games via Pruning

ment over BRP with CFR  In Leduc  which is   far larger
game  BRP outperforms RegretBased Pruning by   factor
of   when added to CFR  BRP with CFR using RM  also
performs comparably to RegretBased Pruning with CFR 
while retaining theoretical guarantees and not suffering from
noisy convergence 

Figure   Convergence for partial pruning  regretbased pruning 
and bestresponse pruning in Leduc   CFR   No Prune  is CFR
without any pruning 

Figure   Convergence for partial pruning  regretbased pruning 
and bestresponse pruning in Leduc   CFR   No Prune  is CFR
without any pruning 

  Conclusions
We introduced BRP    new form of pruning that provably reduces both the space needed to solve an imperfectinformation game and the time needed to reach an  Nash
equilibrium  This addresses both of the major bottlenecks
in solving large imperfectinformation games  Experimentally  BRP reduced the space needed to solve   game by  
factor of   with the reduction factor increasing with game
size  While the early iterations may still be slow and require the same amount of space as CFR without BRP  these
early iterations can be skipped by warm starting CFR with
an abstraction of the game  Brown   Sandholm   
This paper focused on the theory of BRP when applied
to CFR  the most popular algorithm for solving imperfectinformation games  However  BRP can also be applied to
Fictitious Play  Heinrich et al    and likely extends to
other iterative algorithms as well  Hoda et al   

  Acknowledgments
This material is based on work supported by the National
Science Foundation under grant IIS  and the ARO
under award   NF 

References
Bowling  Michael  Burch  Neil  Johanson  Michael  and
Tammelin  Oskari  Headsup limit hold em poker is
solved  Science    January  

Brown  Noam and Sandholm  Tuomas  Regretbased prunIn Advances in Neural
ing in extensiveform games 
Information Processing Systems  pp       

Brown  Noam and Sandholm  Tuomas  Simultaneous abstraction and equilibrium  nding in games  In Proceedings of the International Joint Conference on Arti cial
Intelligence  IJCAI     

Brown  Noam and Sandholm  Tuomas  Strategybased
warm starting for regret minimization in games  In AAAI
Conference on Arti cial Intelligence  AAAI   

Brown  Noam  Kroer  Christian  and Sandholm  Tuomas 
Dynamic thresholding and pruning for regret minimizaIn AAAI Conference on Arti cial Intelligence
tion 
 AAAI   

Gilpin  Andrew and Sandholm  Tuomas  Lossless abstraction of imperfect information games  Journal of the ACM 
    Early version  Finding equilibria in large
sequential games of imperfect information  appeared in
the Proceedings of the ACM Conference on Electronic
Commerce  EC  pages    

Hart  Sergiu and MasColell  Andreu    simple adaptive
procedure leading to correlated equilibrium  Econometrica     

Heinrich  Johannes  Lanctot  Marc  and Silver  David  Fictitious selfplay in extensiveform games  In International
Conference on Machine Learning  ICML  pp   
 

Hoda  Samid  Gilpin  Andrew  Pe    Javier  and Sandholm 
Tuomas  Smoothing techniques for computing Nash equilibria of sequential games  Mathematics of Operations
Research      Conference version appeared in WINE 

Kroer  Christian  Waugh  Kevin        Karzan  Fatma 
and Sandholm  Tuomas  Faster  rstorder methods for
extensiveform game solving  In Proceedings of the ACM
Conference on Economics and Computation  EC   

Reduced Space and Faster Convergence in ImperfectInformation Games via Pruning

Kroer  Christian  Waugh  Kevin        Karzan  Fatma  and
Sandholm  Tuomas  Theoretical and practical advances
on smoothing for extensiveform games  In Proceedings
of the ACM Conference on Economics and Computation
 EC   

Lanctot  Marc  Waugh  Kevin  Zinkevich  Martin  and Bowling  Michael  Monte Carlo sampling for regret minimization in extensive games  In Proceedings of the Annual
Conference on Neural Information Processing Systems
 NIPS  pp     

Moravcik  Matej  Schmid  Martin  Ha  Karel  Hladik  Milan 
and Gaukrodger  Stephen  Re ning subgames in large
imperfect information games  In AAAI Conference on
Arti cial Intelligence  AAAI   

Nesterov  Yurii  Excessive gap technique in nonsmooth
convex minimization  SIAM Journal of Optimization   
   

Pays  Fran ois  An interior point approach to large games
In AAAI Computer Poker

of incomplete information 
Workshop   

Schmid  Martin  Moravcik  Matej  and Hladik  Milan 
Bounding the support size in extensive form games with
imperfect information  In AAAI Conference on Arti cial
Intelligence  AAAI  pp     

Southey  Finnegan  Bowling  Michael  Larson  Bryce  Piccione  Carmelo  Burch  Neil  Billings  Darse  and Rayner 
Chris  Bayes  bluff  Opponent modelling in poker  In
Proceedings of the  st Annual Conference on Uncertainty in Arti cial Intelligence  UAI  pp    July
 

Tammelin  Oskari  Burch  Neil  Johanson  Michael  and
Bowling  Michael  Solving headsup limit texas hold em 
In Proceedings of the International Joint Conference on
Arti cial Intelligence  IJCAI  pp     

Waugh  Kevin  Schnizlein  David  Bowling  Michael  and
Szafron  Duane  Abstraction pathologies in extensive
In International Conference on Autonomous
games 
Agents and MultiAgent Systems  AAMAS   

Zinkevich  Martin  Johanson  Michael  Bowling  Michael   
and Piccione  Carmelo  Regret minimization in games
with incomplete information  In Proceedings of the Annual Conference on Neural Information Processing Systems  NIPS  pp     

