Algorithms for  cid   LowRank Approximation

Flavio Chierichetti   Sreenivas Gollapudi   Ravi Kumar   Silvio Lattanzi   Rina Panigrahy  

David    Woodruff  

Abstract

We consider the problem of approximating  
given matrix by   lowrank matrix so as to minimize the entrywise  cid papproximation error  for
any       the case       is the classical SVD
problem  We obtain the  rst provably good approximation algorithms for this version of lowrank approximation that work for every value of
      including       Our algorithms are simple  easy to implement  work well in practice  and
illustrate interesting tradeoffs between the approximation quality  the running time  and the rank of
the approximating matrix 

  Introduction
The problem of lowrank approximation of   matrix is usually studied as approximating   given matrix by   matrix of
low rank so that the Frobenius norm of the error in the approximation is minimized  The Frobenius norm of   matrix
is obtained by taking the sum of the squares of the entries
in the matrix  Under this objective  the optimal solution is
obtained using the singular value decomposition  SVD  of
the given matrix  Lowrank approximation is useful in large
data analysis  especially in predicting missing entries of  
matrix by projecting the row and column entities       users
and movies  into   lowdimensional space  In this work we
consider the lowrank approximation problem  but under the
general entrywise  cid   norm  for any      
There are several reasons for considering the  cid   version of

 Equal contribution

 Sapienza University  Rome 

Italy 
Work done in part while visiting Google 
Supported in
part by   Google Focused Research Award  by the ERC
Starting Grant DMAP  
and by the SIR Grant
 Google  Mountain View  CA  Google 
RBSI   
Zurich  Switzerland  IBM Almaden  San Jose  CA  Correspondence to  Flavio Chierichetti  avio di uniroma it 
Sreenivas Gollapudi  sgollapu yahoo com  Ravi Kumar
 ravi   gmail com  Silvio Lattanzi  silviol google com 
Rina Panigrahy  rinapy gmail com  David    Woodruff  dpwoodru us ibm com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

lowrank approximation instead of the usually studied  cid 
      Frobenius  version  For example  it is widely acknowledged that the  cid  version is more robust to noise and outliers
than the  cid  version  Cand es et al    Huber    Xu
  Yuille    Several data mining and computer visionrelated applications exploit this insight and resort to  nding
  lowrank approximation to minimize the  cid  error  Lu et al 
  Meng   Torre    Wang   Yeung    Xiong
et al    Furthermore  the  cid  error is typically used as  
proxy for capturing sparsity in many applications including
robust versions of PCA  sparse recovery  and matrix completion  see  for example  Cand es et al    Xu et al 
  For these reasons the problem has already received
attention  Gillis   Vavasis    and was suggested as
an open question in   survey on sketching techniques for
linear algebra  Woodruff    Likewise  the  cid  version
 dubbed also as the Chebyshev norm  has been studied for
the past many years  Goreinov   Tyrtyshnikov     
though to the best of our knowledge  no result with theoretical guarantees was known for  cid  before our work  Our
algorithm is quite general  and works for every      
Working with  cid   error  however  poses many technical challenges  First of all  unlike  cid  the general  cid   space is not
amenable to spectral techniques  Secondly  the  cid   space
is not as nicely behaved as the  cid  space  for example  it
lacks the notion of orthogonality  Thirdly  the  cid   version
quickly runs into computational complexity barriers  for
example  even the rank  approximation in  cid  has been
shown to be NPhard by Gillis and Vavasis  Gillis   Vavasis    However  there has been no dearth in terms of
heuristics for the  cid   lowrank approximation problem  in
particular for       and       this includes alternating
convex  and  in fact  linear  minimization  Ke   Kanade 
  methods based on expectationmaximization  Wang
et al    minimization with augmented Lagrange multipliers  Zheng et al    hyperplanes projections and
linear programming  Brooks et al    and generalizations of the Wiberg algorithm  Eriksson   van den Hengel 
  These heuristics  unfortunately  do not come with
any performance guarantees  While theoretical approximation guarantees have been given for the rank  version for
the GF  and the Boolean cases  Dan et al    to the
best of our knowledge there have been no provably good

Algorithms for  cid   LowRank Approximation

 approximation  algorithms for general matrices  or for rank
more than one  or for general  cid   

  Our contributions

In this paper we obtain the  rst provably good algorithms
for the  cid   rankk approximation problem for every      
Let       be the dimensions of the input matrix  From an
algorithmic viewpoint  there are three quantities of interest 
the running time of the algorithm  the approximation factor guaranteed by the algorithm  and the actual number of
vectors in the lowrank approximation that is output by the
algorithm  even though we only desire   
Given this setting  we show three main algorithmic results
intended for the case when   is not too large  First  we
show that one can obtain         approximation to the
rankk problem in time mk poly       note that this running time is not polynomial once   is larger than   constant  To address this  next we show that one can get
an     approximation to the best kfactorization in time
  poly nm  however  the algorithm returns     log   
columns  which is more than the desired    this is referred
to as   bicriteria approximation  Next  we combine these
two algorithms  We  rst show that the output of the second
algorithm can further be re ned to output exactly   vectors  with an approximation factor of poly    and   running
time of   poly        log      The running time now
is polynomial as long as       log    log log    Finally 
we show that for any constant       we can obtain an
approximation factor of    log        and   running time
of poly       for every value of   
Our  rst algorithm is existential in nature  it shows that there
are   columns in the given matrix that can be used  along
with an appropriate convex program  to obtain         
approximation  Realizing this as an algorithm would therefore na vely incur   factor mk in the running time  Our
second algorithm works by sampling columns and iteratively  covering  the columns of the given matrix  for an
appropriate notion of covering  In each round of sampling
our algorithm uniformly samples from   remaining set of
columns  we note here that it is critical that our algorithm
is adaptive as otherwise uniform sampling would not work 
While this is computationally ef cient and maintains an
    approximation to the best rankk approximation  it
can end up with more than   columns  in fact     log   
Our third algorithm  xes this issue by combining the  rst
algorithm with the notion of   nearisoperimetric transformation for the  cid pspace  which lets us transform   given
matrix into another matrix spanning the same subspace but
with small  cid   distortion  Our fourth algorithm uses  cid   leverage scores to overcome the sampling step  this improves
the running time while mildly worsening the approximation
factor 

  useful feature of our algorithms is that they are uniform
with respect to all values of    We test the performance of
our algorithms  for       and       on real and synthetic
data and show that they produce lowrank approximations
that are substantially better than what the SVD            
would obtain 

  Related work

Very recently  Song  Woodruff  and Zhong  Song et al 
  obtained   lowrank approximation that holds for
every         using sketchingbased techniques 
In
particular 
their main result is an    log    poly   
approximation in nnz        poly    time  for every
   where nnz    is the number of nonzero entries in   
In our work  we also obtain such   result for        
but via very different samplingbased methods  In addition  we obtain an algorithm with   poly   approximation
factor that is independent of   and    though this latter
algorithm requires       log    log log    in order to be
polynomial time  Another result in  Song et al    shows
how to achieve      poly log   approximation  in nO   
time for         For   larger than   constant  this is
larger than polynomial time  whereas our algorithm with
poly   approximation is polynomial time for   as large as
 log    log log    Importantly  our results hold for every
      rather than only when        
In addition we note that there exist papers solving problems that  at  rst blush  might seem similar to ours  For
instance   Deshpande et al    study   convex relation 
and   rounding algorithm to solve the subspace approximation problem  an  cid   generalization of the least squares    
which is related to but different from our problem  Also 
 Feldman et al    offer   bicriteria solution for another
related problem of approximating   set of points by   collection of  ats  they use convex relaxations to solve their
problem and are limited to bicriteria solutions  unlike ours 
Finally  in some special settings robust PCA can be used
to solve  cid  lowrank approximation  Cand es et al   
However  robust PCA and  cid  lowrank approximation have
some apparent similarities but they have key differences 
Firstly   cid  lowrank approximation allows to recover an approximating matrix of any chosen rank  whereas robust PCA
returns some matrix of some unknown  possibly full  rank 
While variants of robust PCA have been proposed to force
the output rank to be   given value  Netrapalli et al   
Yi et al    these variants make additional noise model
and incoherence assumptions on the input matrix  whereas
our results hold for every input matrix  Secondly  in terms
of approximation quality  it is unclear if nearoptimal solutions of robust PCA provide nearoptimal solutions for  cid 
lowrank approximation 
Finally  we mention example matrices for which the SVD

Algorithms for  cid   LowRank Approximation

gives   poor approximation factor for  cid papproximation
error  First  suppose       and       Consider the
following       block diagonal matrix composed of two
blocks        matrix with value   and an            
matrix with all     The SVD returns as   solution the  rst
column  and therefore incurs polynomial in   error for    
      Now suppose       and       Consider the
following       block diagonal matrix composed of two
blocks      matrix with value    and an      
matrix with all     The SVD returns as   solution the matrix
spanned by the bottom block  and so also incurs an error
polynomial in   for          

 cid cid 
     Mi     cid  

  Background
For   matrix    let Mi   denote the entry in its ith row
and jth column and let Mi denote its ith column  Let    
denote its transpose and let       
denote its entrywise   norm  Given   set                 it  of
column indices  let MS   Mi it be the matrix composed
of the columns of   with the indices in   
Given   matrix   with   columns  we will use span    
    iMi           to denote the vectors spanned by
its columns  If   is   matrix and   is   vector  we let
dp        denote the minimum  cid   distance between   and  
vector in span   

 cid  

dp         

inf

  span  

         

Let     Rn   denote the input matrix and let      
denote the target rank  We assume  without loss of generality
 wlog  that        Our  rst goal is  given   and    to  nd
  subset     Rn   of   columns of   and     Rk   to
minimize the  cid   error        given by

            

Our second goal is  given   and    to  nd     Rn       
Rk   to minimize the  cid   error        given by

            

Note that in the second goal  we do not require   be   subset
of columns 
We refer to the  rst problem as the kcolumns subset selection problem in the  cid   norm  denoted kCSSp  and to the
second problem as the rankk approximation problem in the
 cid   norm  denoted kLRAp  In the paper we often call     
the kfactorization of    Note that   solution to kCSSp
can be used as   solution to kLRAp  but not necessarily
vice versa 

 kLRA  is the classical SVD problem of  nding    

Rn        Rk   so as to minimize           

In this paper we focus on solving the two problems for
general    Let    cid    cid  be   kfactorization of   that is optimal in the  cid   norm  where    cid    Rn   and    cid    Rk   
and let optk                cid    cid    An algorithm is said
to be an  approximation  for an       if it outputs
    Rn        Rk   such that

                  optk     

It is often convenient to view the input matrix as    
   cid    cid          cid      where   is some error matrix of
minimum  cid pnorm  Let          optk     
We will use the following observation 
Lemma   Let     Rn   and     Rn  Suppose that
there exists     Rk  such that                   Then 
there exists   polynomial time algorithm that  given   and
    nds     Rk  such that                 

Proof  This  cid   regression problem is   convex program and
wellknown to be solvable in polynomial time 

  An  mk poly nm time algorithm for

kLRAp

In this section we will present an algorithm that runs in time
mk poly nm  and produces         approximation to kCSSp  hence to kLRAp  of   matrix     Rn          
for any       The algorithm simply tries all possible
subsets of   columns of   for producing one of the factors 
   and then uses Lemma   to  nd the second factor    

  The existence of one factor in  
For simplicity  we assume that          for each column   
To satisfy this  we can add an arbitrary small random error to
each entry of the matrix  For instance  for any       and to
each entry of the matrix  we can add an independent uniform
value in     This would guarantee that          for
each        
Recall that       cid      is the perturbed matrix  and we
only have access to    not   cid  Consider Algorithm   and
its output    Note that we cannot actually run this algorithm
since we do not know   cid  It is   hypothetical algorithm used
for the purpose of our proof       the algorithm serves as  
proof that there exists   subset of   columns of   providing
  good low rank approximation  In Theorem   we prove
that the columns in   indexed by the subset   can be used
as one factor of   kfactorization of   
Before proving the main theorem of the section  we show  
useful property of the matrix    cid       the matrix having the
       as the ith column  Then we will use this
vector   cid 
property to prove Theorem  

Algorithm   Enumerating and selecting   columns of   
Require    rank   matrix   cid  and perturbation matrix  
Ensure    column indices of   cid 
       
  For each column index    let    cid 
  Write    cid                        Rn         Rk   
  Let   be the subset of   columns of      Rk   that
has maximum determinant in absolute value  note that
the subset   indexes         submatrix 

      cid 

  Output   

 cid 

Lemma   For each column    cid 

  of    cid  one can write    cid 

   

    Mi       cid 

    where  Mi        for all      

Proof  Fix an                  Consider the equation
 VSMi    Vi for Mi   Rk  We can assume the columns
in  VS are linearly independent  since wlog     cid  has rank
   Hence  there is   unique solution Mi      VS   Vi  By
Cramer   rule  the jth coordinate Mi    of Mi satis es
Mi      det      
   
  is the matrix obtained by redet   VS  
placing the jth column of  VS with  Vi  By our choice of   
        det   VS  which implies  Mi        Mul 
  det      
tiplying both sides of equation  VSMi    Vi by     we have
   cid 

  where     

SMi      cid 
   

Now we prove the main theorem of this section 
Theorem   Let     AS  For       let            Mm
be the vectors whose existence is guaranteed by Lemma  
and let     Rk   be the matrix having the vector
        Mi              Mi   ik     as its ith column  Then   Ai                         and hence
                      

Proof  We consider the generic column         

              

  cid 
  cid 

  

  

      

      

 cid 

Aij

   ij

 cid  Mi   
 ij  
 cid 

  

  cid 
 cid  Mi   
 cid 

 ij  

  cid 
ij

Mi       cid 
ij

  Mi   

Mi   

 cid 

  cid 

  

 cid cid 

 cid 
 ij
 cid 
 ij  
 ij
 cid 
 ij  

          cid 

        
 cid 
  cid 

  

    cid 

   

       Mi   

 ij
 ij  

    cid 

    Ei 

Observe that Ei
           ik
   
    
 ik  

is the weighted sum of   vectors 
  having unit  cid pnorm  Observe fur 

Algorithms for  cid   LowRank Approximation

that 

since the sum of

 cid  
ther
their weights satis es
    
    Mi             we have that the  cid pnorm of
Ei is not larger than  Ei            The proof is complete
using the triangle inequality 
 Ai                  cid 

    Ai        cid 

             

          Ei  
            

  An mk poly nm time algorithm

In this section we give an algorithm that returns  
      approximation to the kLRAp problem in time
mk poly nm 

 optk     

 cid  do

Algorithm           approximation to kLRAp 
Require  An integer   and   matrix  
Ensure      Rn        Rk                          

  for all    cid   
  Return      that minimizes dI  for    cid   

 
Let     AI
Use Lemma   to compute   matrix   that minimizes
the distance dI              

  end for

 cid 

 
 

 

The following statement follows directly from the existence
of   columns in   that make up   factor   having small  cid  
error  Theorem  
Theorem   Algorithm   obtains         approximation
to kLRAp in time mk poly nm 

    poly nm time bicriteria algorithm for

kCSSp

We next show an algorithm that runs in time poly nm 
but returns     log    columns of   that can be used in
place of    with an error      times the error of the best
kfactorization 
In other words  it obtains more than  
columns but achieves   polynomial running time  we will
later build upon this algorithm in Section   to obtain   faster
algorithm for the kLRAp problem  We also show   lower
bound  there exists   matrix   for which the best possible
approximation for the kCSSp  for       is   
De nition    Approximate coverage  Let   be   subset of   column indices  We say that column Ai is cpapproximately covered by   if for       we have
minx Rk   ASx  Ai  
  and for      
minx Rk   ASx   Ai            If       we
say Ai is covered by   

      

     

 

 

We  rst show that if we select   set   columns of size   

uniformly at random in cid   

 cid  with constant probability we

cover   constant fraction of columns of   
Lemma   Suppose   is   set of    uniformly random
chosen columns of    With probability at least    
covers at least     fraction of columns of   

  

 

 

 

 

    

Proof  Let   be   column index of   selected uniformly at
random and not in    Let             and let   be the cost
of the best  cid   rankk approximation to AT   Note that   is  
uniformly random subset of        columns of   
Case        Since   is   uniformly random subset
  Let   
of        columns of    ET      
denote the event          
  By   Markov bound 
Pr       
By Theorem   there exists   subset   of   columns of
AT for which minX  ALX   AT  
               Since
  is itself uniformly random in the set     it holds that
Ei minx  ALx   Ai  
          
  Let    denote the event
          
 minx  ALx   Ai  
  By   Markov bound 
Pr       
Let    denote the event         Since   is uniformly
random in the set     Pr        
Clearly Pr            Conditioned on       
we have

      

   

   

 ARx   Ai  

 

min

 

 

 

 ALx   Ai  

   min
            
            

      

 

 

 

let      cid 

  Zi  We have         cid 

which implies that   is covered by    Note that the  rst
inequality uses that   is   subset of   given    and so
the regression cost using AL cannot be smaller than that of
using AR
Let Zi be an indicator variable if   is covered by   and
 
   
    hence              
    By   Markov bound 
Pr           
Case       Then       since AT is   submatrix of
   By Theorem   there exists   subset   of   columns of
AT for which minX  ALX   AT           De ning
   as before and conditioning on it  we have

    Zi     cid 

       
 

 

min

 

 ARx   Ai    min
  min
        

 ALx   Ai 
 ALX   AT 

 

 

       is covered by    Again de ning Zi to be the event that

Algorithms for  cid   LowRank Approximation

  is covered by    we have   Zi     
    which implies Pr           

       

 

 
     

  and so           

We are now ready to introduce Algorithm   We can wlog 
assume that the algorithm knows   number   for which
             Indeed  such   value can be obtained
by  rst computing   using the SVD  Note that although
one does not know   one does know   since this is the
Euclidean norm of all but the top   singular values of   
which one can compute from the SVD of    Then  note
that for                     while for      
                Hence  given   there
are only   log    values of    one of which will satisfy
             One can take the best solution found
by Algorithm   for each of the   log    guesses to   

Algorithm   Selecting     log    columns of   
Require  An integer    and   matrix       cid     
Ensure      log    columns of  
SELECTCOLUMNS       

if number of columns of        then

return all the columns of  

else

repeat

Let   be uniform at random    columns of  

until at least  fraction columns of   are cpapproximately covered
Let AR be the columns of   not approximately covered
by  
return AR  SELECTCOLUMNS     AR 

end if

Theorem   With probability at least   Algorithm  
runs in time poly nm  and returns     log    columns
that can be used as   factor of the whole matrix inducing  cid  
error       
Proof  First note that if             and if   is
covered by   set   of columns  then   is cpapproximately
covered by   for   constant cp  here cp      for       and
       By Lemma   the expected number of repetitions
of selecting    columns until  fraction of columns
of   are covered is    When we recurse on SELECTCOLUMNS on the resulting matrix AR  each such matrix
admits   rankk factorization of cost at most     Moreover  the number of recursive calls to SELECTCOLUMNS
can be upper bounded by log     In expectation there will
be   log    total repetitions of selecting    columns  and
so by   Markov bound  with probability   the algorithm
will choose     log    columns in total and run in time
poly nm 
Let   be the union of all columns of   chosen by the algorithm  Then for each column   of    for      

Algorithms for  cid   LowRank Approximation

we have minx  ASx   Ai  
  and so
minX  ASX      
   For      
               
we instead have minx  ASx   Ai           and
so minX  ASX              

            

 

 

De nition    Almost isoperimetry    matrix     Rn  
is almost cid pisoperimetric if for all    we have

    
  

   Bx          

    lower bound for kCSSp
In this section we prove an existential result showing that
there exists   matrix for which the best approximation to the
kCSSp is   
Lemma   There exists   matrix   such that the best approximation for the kCSSp problem  for       is
  

We now show that given   full rank     Rn    it is possible
to construct in polynomial time   matrix     Rn   such
that   and   span the same space and   is almost cid pisoperimetric 
Lemma   Given   full  column  rank     Rn    there
is an algorithm that transforms   into   matrix   such
that span     span   and   is almost cid pisoperimetric 
Furthermore the running time of the algorithm is poly nm 

Proof  Consider           Ik  where Ik  is the     
           identity matrix  And consider the matrix    
         Ik       where   is the                 all
ones matrix  Note that   has rank at most    since the sum
of its columns is  
Case            If we choose any   columns of    then
the  cid   cost of using them to approximate   is        On
the other hand              which means that  cid   cost of

  is smaller or equal than cid      cid   

Case        If we choose any   columns of    then the
 cid  cost of using them to approximate   is       On the
other hand              which means that  cid  cost of  
is smaller or equal than  

 
Note also that in  Song et al    the authors show that
for       the best possible approximation is  
   up to
poly log    factors 

       log     poly mn time algorithm for

kLRAp

In the previous section we have shown how to get   rankO   log        approximation in time poly nm  to the
kCSSp and kLRAp problems  In this section we  rst show
how to get   rankk  poly   approximation ef ciently starting from   rankO   log    approximation  This algorithm
runs in polynomial time as long as      
  We
then show how to obtain      log       approximation
ratio in polynomial time for every   
Let   be the columns of   selected by Algorithm  

 cid  log  

 cid 

log log  

  An isoperimetric transformation

The  rst step of our proof is to show that we can modify the
selected columns of   to span the same space but to have
small distortion  For this  we need the following notion of
isoperimetry 

Proof  In  Dasgupta et al    speci cally  Equation
  in the proof of Theorem   the authors show that in
polynomial time it is possible to  nd   matrix   such that
span     span   and for all   

       Bx      

    

for any      
If       their result implies

     

 

         Bx      

        

      

  makes it almost cid pisoperimetric 

 

and so rescaling   by
On the other hand  if       then

              Bx      

             

and rescaling   by   makes it almost cid pisoperimetric 

Note that the algorithm used in  Dasgupta et al    relies on the construction of the   owner John ellipsoid for  
speci   set of points  Interestingly  we can also show that
there is   more simple and direct algorithm to compute such
  matrix    this may be of independent interest  We provide
the details of our algorithm in the full version  Chierichetti
et al   

  Reducing the rank to  

The main idea for reducing the rank is to  rst apply the
almost cid pisoperimetric transformation to the factor   to
obtain   new factor     For such       the  cid pnorm of     
is at most the  cid pnorm of     Using this fact we show that
  has   lowrank approximation and   rankk approximation of   translates into   good rankk approximation of
      But   good rankk approximation of   can be obtained by exploring all possible ksubsets of rows of     as
in Algorithm   More formally  in Algorithm   we give the
pseudocode to reduce the rank of our lowrank approximation from     log    to    Let          optk     

Algorithms for  cid   LowRank Approximation

Algorithm   An algorithm that transforms an     log   
rank matrix decomposition into   krank matrix decomposition without in ating the error too much 
Require      Rn     log        RO   log    
Ensure      Rn        Rk  
  Apply Lemma   to   to obtain matrix    
  Apply Lemma   to obtain matrix                     
   
  Apply Algorithm   with input         Rn     log   
  Set        
  Set           
  Output   and  

          is minimized

and   to obtain   and  

Theorem   Let     Rn        Rn     log       
RO   log     be such that                    Then 
Algorithm   runs in time     log     mn    and outputs     Rn        Rk   such that             
     log   

  Improving the running time

Interestingly it is possible to improve the running time
to  mn    for every   and every constant       at
the cost of   poly   log   approximation instead of the
poly   approximation we had previously  See the full version  Chierichetti et al    for   proof 
Theorem   Let     Rn            min       and
      be an arbitrary constant  Let     Rn     log   
and     RO   log     be such that                   
There is an algorithm that runs in time  mn    and outputs     Rn        Rk   such that             
   log       

  Experiments
In this section  we show the effectiveness of Algorithm  
compared to the SVD  We run our comparison both on
synthetic as well as real data sets  For the real data sets 
we use matrices from the FIDAP set  and   word frequency
dataset from UC Irvine   The FIDAP matrix is       with
  real asymmetric nonzero entries  The KOS blog entries
matrix  representing word frequencies in blogs  is    
  with   nonzero entries  For the synthetic data
sets  we use two matrices  For the  rst  we use      
  random matrix with   nonzero entries this random
matrix was generated as follows  independently  we set each
entry to   with probability   and to   uniformly random

 ihttp math nist gov MatrixMarket data 

SPARSKIT fidap fidap html

 https archive ics uci edu ml datasets 

Bag of Words

value in     with probability   Both matrices are full
rank  For the second matrix  we use   random        
matrix 
In all our experiments  we run   simpli ed version of Al 

gorithm   where instead of running for all possible cid   
 cid 

 

subsets of   columns  which would be computationally prohibitive  we repeatedly sample   columns    few thousand
times  uniformly at random  We then run the  cid pprojection
on each sampled set and  nally select the solution with
the smallest  cid perror   While this may not guarantee provable approximations  we use this   reasonable heuristic that
seems to work well in practice  without much computational
overhead  We focus on       and      
Figure   illustrates the relative performance of Algorithm  
compared to the SVD for different values of   on the real
data sets  In the  gure the green line is the ratio of the total
error  The  cid error for Algorithm   is always less than the
corresponding error for the SVD and in fact consistently
outperforms the SVD by roughly   for small values of
  on the FIDAP matrix  On the larger KOS matrix  the
relative improvement in performance with respect to  cid 
error is more uniform  around  
We observe similar trends for the synthetic data sets as
well  Figures   and   illustrate the trends  Algorithm  
performs consistently better than the SVD in the case of
 cid error for both the matrices  In the case of  cid error  it
outperforms SVD by around   for higher values of   on
the random matrix  Furthermore  it consistently outperforms
SVD  between   and   for all values of   on the
random   matrix 
To see why our  cid  error is always   for   random   matrix
   note that by setting our rankk approximation to be the
zero matrix  we achieve an  cid  error of   This is optimal
for large values of   and   and small   as can be seen
by recalling the notion of the signrank of   matrix    
        which is the minimum rank of   matrix   for
which the sign of Bi   equals Ai   for all entries       If the
signrank of   is larger than    then for any rankk matrix
   we have  cid       cid      since necessarily there is an
entry Ai   for which  Ai     Bi        It is known that the
signrank of   random       matrix    and thus also of  
random       matrix    is  
   with high probability
 Forster   

 

  Conclusions
We studied the problem of lowrank approximation in the
entrywise  cid   error norm and obtained the  rst provably
good approximation algorithms for the problem that work
for every       Our algorithms are extremely simple 
which makes them practically appealing  We showed the
effectiveness of our algorithms compared with the SVD on

Algorithms for  cid   LowRank Approximation

    FIDAP matrix  cid 

    KOS matrix  cid 

Figure   Comparing the performance of Algorithm   with SVD on the real data sets 

    Random matrix  cid 

    Random matrix  cid 

Figure   Comparing the performance of Algorithm   with SVD on the random matrix 

      matrix  cid 

      matrix  cid 

Figure   Comparing the performance of Algorithm   with SVD on the   matrix 

real and synthetic data sets  We obtain   kO  approximation factor for every   for the column subset selection
problem  and we showed an example matrix for this problem for which      approximation factor is necessary  It
is unclear if better approximation factors are possible by
designing algorithms that do not choose   subset of input
columns to span the output low rank approximation  Resolving this would be an interesting and important research
direction 

 Ratio of Alg   to SVD ErrorsAbsolute ErrorKAlg  SVDRatio Ratio of Alg   to SVD ErrorsAbsolute ErrorKAlg  SVDRatio Ratio of Alg  to SVD ErrorsAbsolute ErrorKAlg  SVDRatio Ratio of Alg  to SVD ErrosAbsolute ErrorKAlg  SVDRatio Ratio of Alg   to SVD ErrorsAbsolute ErrorKAlg  SVDRatio Ratio of Alg   to SVD ErrorsAbsolute ErrorKAlg  SVDRatioAlgorithms for  cid   LowRank Approximation

References
Brooks     Paul  Dul    Jose     and Boone  Edward     
pure  cid norm principal component analysis  Computational Statistics   Data Analysis     

Cand es  Emmanuel    Li  Xiaodong  Ma  Yi  and Wright 
John  Robust principal component analysis  JACM   
   

Chierichetti  Flavio  Gollapudi  Sreenivas  Kumar  Ravi 
Lattanzi  Silvio  Panigrahy  Rina  and Woodruff  David   
Algorithms for  cid   lowrank approximation  Technical
Report   arXiv   

Dan  Chen  Hansen  Kristoffer    Jiang  He  Wang  Liwei 
and Zhou  Yuchen  On low rank approximation of binary
matrices  Technical Report     arXiv   

Dasgupta  Anirban  Drineas  Petros  Harb  Boulos  Kumar 
Ravi  and Mahoney  Michael    Sampling algorithms and
coresets for lp regression  SICOMP   
 

Deshpande  Amit  Tulsiani  Madhur 

and Vishnoi 
Nisheeth    Algorithms and hardness for subspace approximation  In SODA  pp     

Eriksson  Anders and van den Hengel  Anton  Ef cient
computation of robust lowrank matrix approximations
using the    norm  PAMI     

Feldman  Dan  Fiat  Amos  Sharir  Micha  and Segev  Danny 
Bicriteria lineartime approximations for generalized kmean median center  In SoCG  pp     

Forster    urgen    linear lower bound on the unbounded
error probabilistic communication complexity     Comput 
Syst  Sci     

Gillis  Nicolas and Vavasis  Stephen    On the complexity
of robust PCA and  cid norm lowrank matrix approximation  Technical Report   arXiv   

Goreinov  Sergei    and Tyrtyshnikov  Eugene    The
maximalvolume concept in approximation by lowrank
matrices  Contemporary Mathematics     

Goreinov  Sergei    and Tyrtyshnikov  Eugene    Quasioptimality of skeleton approximation of   matrix in the
Chebyshev norm  Doklady Mathematics   
 

Huber  Peter    Robust Statistics  John Wiley   Sons  New

York   

Ke  Qifa and Kanade  Takeo  Robust    norm factorization
in the presence of outliers and missing data by alternative
convex programming  In CVPR  pp     

Lu  Cewu  Shi  Jiaping  and Jia  Jiaya  Scalable adaptive

robust dictionary learning  TIP     

Meng  Deyu and Torre  Fernando        Robust matrix
factorization with unknown noise  In ICCV  pp   
   

Netrapalli  Praneeth  Niranjan        Sanghavi  Sujay 
Anandkumar  Animashree  and Jain  Prateek  Nonconvex
robust PCA  In NIPS  pp     

Song  Zhao  Woodruff  David    and Zhong  Pelin  Low
In

rank approximation with entrywise  cid norm error 
STOC   

Wang  Naiyan and Yeung  DitYan  Bayesian robust matrix
factorization for image and video processing  In ICCV 
pp     

Wang  Naiyan  Yao  Tiansheng  Wang  Jingdong  and Yeung  DitYan    probabilistic approach to robust matrix
factorization  In ECCV  pp     

Woodruff  David    Sketching as   tool for numerical linear
algebra  Foundations and Trends in Theoretical Computer
Science     

Xiong  Liang  Chen  Xi  and Schneider  Jeff  Direct robust
matrix factorization for anomaly detection  In ICDM  pp 
   

Xu  Huan  Caramanis  Constantine  and Sanghavi  Sujay 
Robust PCA via outlier pursuit  TOIT   
 

Xu  Lei and Yuille  Alan    Robust principal component analysis by selforganizing rules based on statistical physics approach  IEEE Transactions on Neural
Networks     

Yi  Xinyang  Park  Dohyung  Chen  Yudong  and Caramanis 
Constantine  Fast algorithms for robust pca via gradient
descent  In NIPS  pp     

Zheng  Yinqiang  Liu  Guangcan  Sugimoto  Shigeki  Yan 
Shuicheng  and Okutomi  Masatoshi  Practical lowrank
matrix approximation under robust   norm  In CVPR 
pp     

