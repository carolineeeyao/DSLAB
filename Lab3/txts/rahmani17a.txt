Coherence Pursuit  Fast  Simple  and Robust Subspace Recovery

Mostafa Rahmani   George Atia  

Abstract

dimensional subspace by solving

  remarkably simple  yet powerful  algorithm
termed Coherence Pursuit for robust Principal
Component Analysis  PCA  is presented  In the
proposed approach  an outlier is set apart from an
inlier by comparing their coherence with the rest
of the data points  As inliers lie in   low dimensional subspace  they are likely to have strong
mutual coherence provided there are enough inliers  By contrast  outliers do not typically admit
low dimensional structures  wherefore an outlier is unlikely to bear strong resemblance with
  large number of data points  As Coherence
Pursuit only involves one simple matrix multiplication  it is signi cantly faster than the stateof theart robust PCA algorithms  We provide  
mathematical analysis of the proposed algorithm
under   random model for the distribution of the
inliers and outliers  It is shown that the proposed
method can recover the correct subspace even if
the data is predominantly outliers  To the best
of our knowledge  this is the  rst provable robust PCA algorithm that is simultaneously noniterative  can tolerate   large number of outliers
and is robust to linearly dependent outliers 

  Introduction
Standard tools such as Principal Component Analysis
 PCA  have been instrumental in reducing dimensionality by  nding linear projections of highdimensional data
along the directions where the data is most spread out to
minimize information loss  These techniques are widely
applicable in   broad range of data analysis problems  including computer vision  image processing  machine learning and bioinformatics  Basri   Jacobs    Costeira  
Kanade    Hosseini et al   
Given   data matrix     Rm    PCA  nds an   
 University of Central Florida  Orlando  Florida  USA  Corre 

spondence to  Mostafa Rahmani  mostafa knights ucf edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

 cid         UT   cid  

min
  

subject to

 UT        

 

where      Rm   is an orthonormal basis for the rdimensional subspace    denotes the identity matrix and
 cid cid   the Frobenius norm  Despite its notable impact on
exploratory data analysis and multivariate analyses  PCA
is notoriously sensitive to outliers that prevail much of the
real world data since the solution to   can arbitrarily deviate from the true subspace in presence of   small number
of outlying data points that do not conform with the low dimensional model  Hauberg et al    Lerman   Zhang 
  Maronna    McCoy et al    Xu et al     
Zhang     
As   result  much research work was devoted to investigate
PCA algorithms that are robust to outliers  The corrupted
data can be expressed as

           

 

where   is   low rank matrix whose columns span  
lowdimensional subspace  and the matrix   models the
data corruption  and is referred to as the outlier matrix 
There are two main models for the outlier matrix that
were considered in the literature  and these two models
are mostly incomparable in theory  practice and analysis
techniques  The  rst corruption model is the elementwise
model in which   is   sparse matrix with arbitrary support  whose entries can have arbitrarily large magnitudes
 Chandrasekaran et al    Cand es et al    Netrapalli et al    Yi et al    In view of the arbitrary
support of    any of the columns of   may be affected by
the nonzero elements of    We do not consider this model
in this paper  The second model  which is the focus of our
paper  is   columnwise model wherein only   fraction of
the columns of   are nonzero  wherefore   portion of the
columns of    the socalled inliers  remain unaffected by
   Lerman   Maunu    Xu et al      Chen et al 
  Rahmani   Atia    Next  we formally describe
the data model adopted in this paper  which only focuses
on the columnwise outlier model 
Data Model   The given data matrix   satis es the following 

Submission and Formatting Instructions for ICML  

  The matrix   can be expressed as

                     

 
where     Rm        Rm    and   is an arbitrary
permutation matrix 
  The columns of   lie in an rdimensional subspace   
namely  the column space of    The columns of   do not
lie entirely in         the columns of   are the inliers and
the columns of   are the outliers 

The columnwise model for robust PCA has direct bearing
on   host of applications in signal processing and machine
learning  which spurred enormous progress in dealing with
subspace recovery in the presence of outliers  This work is
motivated by some of the limitations of existing techniques 
which we further detail in Section   on related work  The
vast majority of existing approaches to robust PCA have
high computational complexity  which makes them unsuitable in high dimensional settings  For instance  many of the
existing iterative techniques incur   long run time as they
require   large number of iterations  each with   Singular
Value Decomposition  SVD  operation  Also  most iterative solvers have no provable guarantees for exact subspace
recovery  Moreover  many of the existing robust PCA algorithms cannot tolerate   large number of outliers as they
rely on sparse outlier models  while others cannot handle
linearly dependent outliers  In this paper  we present   new
provable noniterative robust PCA algorithm  dubbed Coherence Pursuit  CoP  which involves one simple matrix
multiplication  and thereby achieves remarkable speedups
over the stateof theart algorithms  CoP can tolerate  
large number of outliers   even if the ratio of inliers to outapproaches zero   and is robust to linearly depenliers   
  
dent outliers and to additive noise 

  Notation and de nitions

Boldface uppercase letters are used to denote matrices
and boldface lowercase letters are used to denote vectors 
Given   matrix     cid   cid  denotes its spectral norm and  cid   cid 
its nuclear norm  For   vector     cid   cid   denotes its  cid pnorm
and      its ith element  Given two matrices    and   
with an equal number of rows  the matrix

           

is the matrix formed by concatenating their columns  For  
matrix    ai denotes its ith column  and     is equal to  
with the ith column removed  The function orth  returns
an orthonormal basis for the range of its matrix argument 
De nition   The mutual coherence of two nonzero vectors      Rm  and      Rm  is de ned as

    
 vT
 cid   cid cid   cid   

   

  Related Work
Some of the earliest approaches to robust PCA relied on
robust estimation of the data covariance matrix  such as Sestimators  the minimum covariance determinant  the minimum volume ellipsoid  and the StahelDonoho estimator
 Huber    This is   class of iterative approaches that
compute   full SVD or eigenvalue decomposition in each
iteration and generally have no explicit performance guarantees  The performance of these approaches greatly degrades for   
  
To enhance robustness to outliers  another approach is to
replace the Frobenius norm in   with other norms  Lerman et al    For example   Ke   Kanade    uses
an  cid norm relaxation commonly used for sparse vector estimation  yielding robustness to outliers  Candes   Tao 
  Cand es et al      However  the approach
presented in  Ke   Kanade    has no provable guarantees and requires   to be column sparse         very small
portion of the columns of   can be nonzero  The work in
 Ding et al    replaces the  cid norm in  Ke   Kanade 
  with the  cid norm  While the algorithm in  Ding
et al    can handle   large number of outliers  the complexity of each iteration is   nm  and its iterative solver
has no performance guarantees  Recently  the idea of using
  robust norm was revisited in  Lerman et al    Zhang
  Lerman    Therein  the nonconvex constraint set is
relaxed to   larger convex set and exact subspace recovery
is guaranteed under certain conditions  The algorithm presented in  Lerman et al    obtains the column space
of   and  Zhang   Lerman     nds its complement 
However  the iterative solver of  Lerman et al    computes   full SVD of an       weighted covariance matrix in each iteration  Thus  the overall complexity of the
solver of  Lerman et al    is roughly        nm 
per iteration  where the second term is the complexity of
computing the weighted covariance matrix  Similarly  the
solver of  Zhang   Lerman    has   nm      complexity per iteration  In  Tsakiris   Vidal    the complement of the column space of   is recovered via   series
of linear optimization problems  each obtaining one direction in the complement space  This method is sensitive to
linearly dependent outliers and requires the columns of  
not to exhibit   clustering structure  which in fact prevails
much of the real world data  Also  the approach presented
in  Tsakiris   Vidal    requires solving     linear optimization problems consecutively resulting in high computational complexity and long run time for high dimensional
data 
Robust PCA using convex rank minimization was  rst analyzed in  Chandrasekaran et al    Cand es et al   
for the elementwise corruption model 
In  Xu et al 
    the algorithm analyzed in  Chandrasekaran et al 

Submission and Formatting Instructions for ICML  

  Cand es et al    was extended to the columnwise corruption model where it was shown that the optimal
point of

min
      
subject to

 cid   cid     cid     cid 

           

 

should be roughly less than  

yields the exact subspace and correctly identi es the outliers provided that   is suf ciently columnsparse  The
solver of   requires too many iterations  each computing
the SVD of an       dimensional matrix  Also  the algorithm can only tolerate   small number of outliers   the
ratio   
  
  different approach to outlier detection was proposed
in  Soltanolkotabi   Candes    Elhamifar   Vidal 
  the idea being that outliers do not typically follow
low dimensional structures  thereupon few outliers cannot
form   linearly dependent set  While this approach can recover the correct subspace even if   remarkable portion of
the data is outliers  it cannot detect linearly dependent outliers and has      complexity per iteration  Elhamifar  
Vidal    In the outlier detection algorithm presented
in  Heckel     olcskei      data point is identi ed as
an outlier if the maximum value of its mutual coherences
with the other data points falls below   prede ned threshold  However  this approach is unable to detect outliers that
lie in close neighborhoods  For instance  any repeated outliers will be falsely detected as inliers since their mutual
coherence is  

  Motivation and summary of contributions

This work is motivated by the limitations of prior work on
robust PCA as summarized below 
Complex iterations  Most of the stateof theart robust
PCA algorithms require   large number of iterations each
with high computational complexity  For instance  many of
these algorithms require the computation of the SVD of an
     or      or     matrix in each iteration  Hardt  
Moitra    Xu et al      Lerman et al    which
leads to long run time 
Guarantees  While the optimal points of the optimization
problems underlying many of the existing robust subspace
recovery techniques yield the exact subspace  there are no
such guarantees for their corresponding iterative solvers 
Examples include the optimal points of the optimization
problems presented in  Xu et al      Ding et al   
Robustness issues  Most existing algorithms are tailored
to one speci   class of outlier models  For example  algorithms based on sparse outlier models utilize sparsity promoting norms  thus can only handle   small number of outliers  On the other hand  algorithms such as  Heckel  
  olcskei    Soltanolkotabi   Candes    can han 

dle   large number of outliers  albeit they fail to locate
outliers with high similarity or linearly dependent outliers 
Spherical PCA  SPCA  is   noniterative robust PCA algorithm that is also scalable  Maronna et al   
In
this algorithm  all the columns of   are  rst projected
onto the unit sphere Sm  then the subspace is identi ed
as the span of the principal directions of the normalized
data  However  in the presence of outliers  the recovered
subspace is never equal to the true subspace and it signi 
cantly deviates from the underlying subspace when outliers
abound 
To the best of our knowledge  CoP is the  rst algorithm
that addresses these concerns all at once  In the proposed
method  we distinguish outliers from inliers by comparing
their degree of coherence with the rest of the data  The advantages of the proposed algorithm are summarized below 

  Coherence Pursuit  CoP  is   considerably simple
noniterative algorithm which roughly involves one
matrix multiplication to compute the Gram matrix  It
also has provable performance guarantees       Section  

  CoP can tolerate   large number of outliers 

It is
shown that exact subspace recovery is guaranteed with
goes to zero provided that
high probability even if   
  
  is suf ciently large 
  
  

 

  CoP is robust to linearly dependent outliers since it
measures the coherence of   data point with respect to
all the other data points 

Algorithm   CoP  Proposed Robust PCA Algorithm
Initialization  Set       or      
  Data Normalization  De ne matrix     Rm   as
xi   di cid di cid 
  Mutual Coherence Measurement
  De ne     XT   and set its diagonal elements to
zero 
  De ne vector     Rn  as         cid gi cid       
            
  Subspace Identi cation  Construct matrix   from the
columns of   corresponding to the largest elements of  
such that they span an rdimensional subspace 
Output  The columns of   are   basis for the column
space of   

  Proposed Method
In this section  we present the Coherence Pursuit algorithm
and provide some insight into its characteristics  The main
theoretical results are provided in Section   The table of

Submission and Formatting Instructions for ICML  

Figure   The values of vector   for different values of   and   
  

 

Algorithm   presents the proposed method along with the
de nitions of the used symbols 

Figure   The elements of vector   with different values of parameter  

Coherence  The inliers lie in   low dimensional subspace
   In addition  for most practical purposes the inliers are
highly coherent with each other in the sense of having large
values of the mutual coherence in De nition   By contrast 
the outlying columns do not typically follow low dimensional structures and do not bear strong resemblance with
the rest of the data  As such  in CoP all mutual coherences
between the columns of   are  rst computed  Then  the
column space of   is obtained as the span of those columns
that have strong mutual coherence with the rest of the data 
For instance  assume that the distributions of the inliers and
the outliers follow the following assumption 
Assumption   The subspace   is   random rdimensional
subspace in Rm  The columns of   are drawn uniformly
at random from the intersection of Sm  and    The
columns of   are drawn uniformly at random from Sm 
To simplify the exposition and notation  it is assumed that
  in   is equal to the identity matrix without any loss of
generality                

In addition  suppose that    and    are two inliers and   
and    are two outliers  It can be shown that

  vT

         

while

  uT

              uT

           

Accordingly  if    cid     the inliers exhibit much stronger
mutual coherences 

  Large number of outliers

Unlike most robust PCA algorithms which require    to
be much smaller than    the proposed method tolerates  
large number of outliers  For instance  consider   setting

in which                    and the distributions
of inliers and outliers follow Assumption   Fig    shows
the vector         Algorithm   for different values of   and
   In all the  gures  the maximum element is scaled to  
One can observe that even if          the proposed
technique can recover the exact subspace since there is  
clear gap between the values of   corresponding to outliers
and inliers  more so for      

  Robustness to noise

In the presence of additive noise  we model the data as

                 

 

where   represents the noise component 
The high coherence between the inliers  columns of    and
the low coherence of the outliers  columns of    with each
other and with the inliers result in the large gap between
the elements of   observed in Fig    even when       
  This gap gives the algorithm tolerance to high levels
of noise  For example  assume                  
  and the distribution of the inliers and outliers follow
Assumption   De ne the parameter   as

   

  cid   cid 
  cid   cid 

 

where   and   are arbitrary columns of   and    respectively  Fig    shows the entries of   for different values
of   As shown  the elements corresponding to inliers are
clearly separated from the ones corresponding to outliers
even at very low signal to noise ratio             and
      The mathematical analysis of CoP with noisy data
con rms that the proposed method remains robust to high
levels of noise even with data that is predominantly outliers
 Rahmani   Atia   

Submission and Formatting Instructions for ICML  

Figure   The values of vector   when the  th to  th columns
are repeated outliers 

  Linearly dependent outliers

Many of the existing robust PCA algorithms cannot detect linearly dependent outliers  Hardt   Moitra   
Soltanolkotabi   Candes    For instance 
in the
outlier detection algorithm presented in  Soltanolkotabi  
Candes      data column is identi ed as an outlier if
it does not admit   sparse representation in the rest of the
data  As such  if some outlying columns are repeated  the
algorithm in  Soltanolkotabi   Candes    fails to detect them 
At   fundamental level 
the proposed approach affords
  more comprehensive de nition of an outlying column 
namely    data column is identi ed as an outlier if it has
weak total coherency with the rest of the data  This global
view of   data point        the rest of the data allows the algorithm to identify outliers that are linearly dependent with
few other data points 
For illustration  assume the given data follows Data model
                      where the  th to  th
columns are repeated outliers  Fig    shows the elements
of vector   with       and       All the elements corresponding to inliers are clearly greater than the elements
corresponding to outliers and the algorithm correctly identi es the subspace  Fig    suggests that CoP with       is
better at handling repeated outliers since the entries of  
with large absolute magnitudes are more gracefully ampli 
 ed by the  cid norm 

  Subspace identi cation

In the third step of Algorithm   we sample the columns
of   with the largest coherence values which span an rdimensional space  In this section  we present some tips
for ef cient implementation of this step  One way is to
start sampling the columns with the highest coherence values and stop when the rank of the sampled columns is equal
to    However  if the columns of   admit   clustering
structure and their distribution is highly nonuniform  this
method will sample many redundant columns  which can
in turn increase the run time  In this section  we propose
two lowcomplexity techniques to accelerate the subspace
identi cation step 

  In many applications  we may have an upper bound on
     For instance  suppose we know that up to   percent
of the data could be outliers  In this case  we simply remove
  percent of the columns corresponding to the smallest
values of   and obtain the subspace using the remaining
data points 
  The second technique is an adaptive sampling method
presented in the table of Algorithm   First  the data is
projected onto   random krdimensional subspace to reduce the computational complexity for some integer      
According to the analysis presented in  Rahmani   Atia 
  Li   Haupt    even       is suf cient to preserve the rank of   and the structure of the outliers        
the rank of    is equal to   and the columns of    do
not lie in the column space of     where   is the projection matrix  The parameter   that thresholds the  cid norms
of the columns of the projected data is chosen based on
the noise level  if the data is noise free        In Algorithm   the data is projected onto the span of the sampled
columns  step   Thus    newly sampled column brings
innovation with respect to the previously sampled columns 
Therefore  redundant columns are not sampled 

Algorithm   Adaptive Column Sampling for the Subspace
Identi cation Step  step   of CoP
Initialization  Set   equal to an integer greater than    
threshold   greater than or equal to   and   an empty matrix 
  Data Projection  De ne      Rkr   as         
where     Rkr   projects the columns of   onto   random krdimensional subspace 
  Column Sampling
For   from   to   do
  Set equal to zero the elements of   corresponding to
columns of    with  cid norms less than or equal to  
  De ne     arg max
and set         
  Update           FFT   
End For
Output Construct   using the columns of   that correspond to the columns that formed   

     update     orth

   xj 

 

 cid 

 cid 

 

Remark   Suppose we run Algorithm     times  each
time the sampled columns are removed from the data and
newly sampled columns are added to    If the given data
is noisy  the  rst   singular values of   are the dominant
ones and the rest correspond to the noise component  If we
increase    the span of the dominant singular vectors will
be closer to the column space of    However  if   is chosen
unreasonably high  the sampler may also sample outliers 

Submission and Formatting Instructions for ICML  

  increases 
mutual coherence also increases  Similarly  if   
the mutual coherence between the outliers increases  Thus 
the main requirement is that   
  should be suf ciently larger
   
than   
II  In real applications     cid    and         hence the
suf cient conditions are easily satis ed  This fact is observed in Fig    which shows that Coherence Pursuit can
recover the correct subspace even if         
III  In high dimensional settings     cid     Therefore    
could be much greater than   
    Accordingly  the conditions in Lemma   are stronger than the conditions of
Lemma   suggesting that with       Coherence Pursuit
can tolerate   larger number of outliers than with      
This is con rmed by comparing the plots in the last row of
Fig   

 

The following theorems show that the same set of factors
are important to guarantee that the proposed algorithm recovers the exact subspace with high probability 
Theorem   If Assumption   is true and

 cid cid   

 

  
 

 
 
       

 

   

 cid 

 cid 

 
   

    

 cid 

    log   
 

     

 

 

   

    

 

  
 

    log   
 
     

 

 
 

 

then Algorithm   with       recovers the exact subleast       where    
space with probability at
max  log      and      
    
Theorem   If Assumption   is true and

 cid 

 

   

 

 

  

           
 

         

 
 

 

 

then Algorithm   with       recovers the correct subspace
with probability at least       where

 cid   

 

  Computational complexity

The main computational complexity is in the second step
of Algorithm   which is of order   mn  If we utilize
Algorithm   as the third step of Algorithm   the overall
complexity is of order   mn         However  since the
algorithm roughly involves only one matrix multiplication 
it is very fast and very simple for hardware implementation
      Section   on run time  Moreover  if we utilize the
randomized designs presented in  Rahmani   Atia   
Li   Haupt    the overall complexity can be reduced
to     

  Theoretical Investigations
In this section  we  rst establish suf cient conditions to ensure that the expected value of the elements of the vector
  corresponding to the inliers are much greater than the
elements corresponding to the outliers  in which case the
algorithm is highly likely to yield exact subspace recovery 
Subsequently  we provide two theorems establishing performance guarantees for the proposed approach for      
and      
The following lemmas establish suf cient conditions for
the expected value of the elements of   corresponding to
inliers to be at least twice as large as those corresponding
to outliers  Due to space limitations  the proofs of all lemmas and theorems are deferred to an extended version of
this paper  Rahmani   Atia   
Lemma   Suppose Assumption   holds  the ith column is
an inlier and the         th column is an outlier  If

 cid cid   

 cid 

 cid 

 cid   

 

 

   
 

 

 

 
    
 
 

 

 

 cid   

  

   cid gi cid         cid gn   cid 

recalling that gi is the ith column of matrix   
Lemma   Suppose Assumption   holds  the ith column is
an inlier and the         th column is an outlier  If

  
 

       
 

   

  
 

 

 
 

 

then

   cid gi cid 

         cid gn   cid 
   

Remark   The suf cient conditions provided in Lemma  
and Lemma   reveal three important points 

  and
   The important performance factors are the ratios   
  increases  the density of the
    The intuition is that as   
  
inliers in the subspace increases  and consequently their

  
 

then

 cid 
 cid 

 cid 
 cid 

    max

 
 

log

 rn 

 

 

 

  
 

log

 rn 

 

 cid 
 cid 

 

    max

 
 

 

 

log

 mn 

  
 
    
    and      

 

log

 mn 

 

 

 cid 

 cid 

 cid 

 

and

      

    max    log   
Remark   The dominant factors of the LHS and the RHS
of   are   
  respectively 
    
  log  mn 
 
As in Lemma   we see the factor   
    but under the square
root  Thus  the requirement of Theorem   is less stringent
than that of Lemma   This is because Theorem   guarantees that the elements corresponding to inliers are greater
than those corresponding to outliers with high probability 
but does not guarantee   large gap between the values as
in Lemma  

 

Submission and Formatting Instructions for ICML  

Algorithm   Subspace Clustering Error Correction
Method
Input  The matrices    Di  
   represent the clustered data
 the output of   subspace clustering algorithm  and   is the
number of clusters 
Error Correction
For   from   to   do
  Apply the robust PCA algorithm to the matrices    Di  
De ne the orthonormal matrices    Ui  
bases for the inliers of    Di  
  Update the data clustering with respect to the obtained
bases    Ui  
   are updated      
  data point   is assigned to the ith cluster if    
arg max
End For
Output  The matrices    Di  
data and the matrices    Ui  
for the identi ed subspaces 

   represent the clustered
   are the orthonormal bases

  
   as the learned

    the matrices    Di  

   respectively 

 cid xT  Uk cid 

 

  Numerical Simulations
In this section  the performance of the proposed method is
investigated with both synthetic and real data  We compare the performance of CoP with the stateof theart robust PCA algorithms including FMS  Lerman   Maunu 
  GMS  Zhang   Lerman      PCA  Ding
et al    OP  Xu et al      and SPCA  Maronna
et al   

  Phase transition plot

Our analysis has shown that CoP yields exact subspace recovery with high probability if     is suf ciently greater
than      In this experiment  we investigate the phase
transition of CoP in the     and     plane  Suppose
            and the distributions of inliers outliers
follow Assumption   De ne   and    as the exact and
recovered orthonormal bases for the span of inliers  respectively    trial is considered successful if

 cid cid         UT   cid    cid   cid  

 cid       

In this simulation  we construct the matrix   using  
columns of   corresponding to the largest   elements of
the vector    Fig    shows the phase transition plot  White
indicates correct subspace recovery and black designates
incorrect recovery  As shown  if     increases  we need
higher values of      However  one can observe that with
        the algorithm can yield exact recovery even if
       

Figure   The phase transition plot of CoP versus     and
    

  Running time

In this section  we compare the run time of CoP to the existing approaches  Table   presents the run time in seconds
for different data sizes  In all experiments          and
         One can observe that CoP is remarkably faster
given its simplicity  single step algorithm 

Table   Running time of the algorithms

      CoP
 
 
 
 
 
 
 
 

FMS

 
 
 
 

OP
 
 
 
 

  PCA

 
 
 
 

  Subspace recovery in presence of outliers

In this experiment  we assess the robustness of CoP to outliers in comparison to existing approaches  It is assumed
that                    and the distribution of
inliers outliers follows Assumption   De ne   and    as
before  and the recovery error as

 cid cid         UT   cid    cid   cid  

 cid 

 

LogRecovery Error   log 

In this simulation  we use   columns to form the matrix
   Fig    shows the recovery error versus      for different values of    In addition to its remarkable simplicity 
CoP gives the highest accuracy and yields exact subspace
recovery even if the data is overwhelmingly outliers 

  Clustering error correction

In this section  we consider   very challenging robust subspace recovery problem with real data  We use the robust
PCA algorithm as   subroutine for error correction in   subspace clustering problem  In this experiment  the outliers

              Submission and Formatting Instructions for ICML  

Figure   The subspace recovery error versus     

   lie in the linear subspaces  Si  

can be close to the inliers and can also be linearly dependent 
The subspace clustering problem is   general form of PCA
in which the data points lie in   union of an unknown number of unknown linear subspaces  Vidal    Rahmani  
Atia      subspace clustering algorithm identi es the
subspaces and clusters the data points with respect to the
subspaces  The performance of the subspace clustering algorithms   especially the ones with scalable computational
complexity   degrades in presence of noise or when the
subspaces are closer to each other  Without loss of generality  suppose           DL  is the given data where the
columns of  Di  
  
respectively  and   is the number of subspaces  De ne
   Di  
   as the output of some clustering algorithm  the
clustered data  De ne the clustering error as the ratio of
misclassi ed points to the total number of data points  With
the clustering error  some of the columns of  Di believed to
lie in Si may actually belong to some other subspace  Such
columns can be viewed as outliers in the matrix  Di  Accordingly  the robust PCA algorithm can be utilized to correct the clustering error  We present Algorithm   as an error
correction algorithm which can be applied to the output of
any subspace clustering algorithm to reduce the clustering
error  In each iteration  Algorithm   applies the robust PCA
algorithm to the clustered data to obtain   set of bases for
the subspaces  Subsequently  the obtained clustering is updated based on the obtained bases 
In this experiment  we imagine   subspace clustering algorithm with   percent clustering error and apply Algorithm
  to the output of the algorithm to correct the errors  We use
the Hopkins  dataset which contains video sequences of
  or   motions  Tron   Vidal    The data is generated
by extracting and tracking   set of feature points through

Figure   The clustering error after each iteration of Algorithm  

In motion segmentation  each motion correthe frames 
sponds to one subspace  Thus  the problem here is to cluster data lying in two or three subspaces  Vidal    We
use the traf   data sequences  which include   scenarios
with two motions and   scenarios with three motions 
When CoP is applied    percent of the columns of   are
used to form the matrix    Fig    shows the average clustering error  over all traf   data matrices  after each iteration of Algorithm   for different robust PCA algorithms 
CoP clearly outperforms the other approaches  As   matter
of fact  most of the robust PCA algorithms fail to obtain
the correct subspaces and end up increasing the clustering
error 
Acknowledgment
This work was supported by NSF CAREER Award CCF 
  and NSF Grant CCF 

References
Basri  Ronen and Jacobs  David    Lambertian re ectance
and linear subspaces  Pattern Analysis and Machine Intelligence  IEEE Transactions on     

Candes  Emmanuel   and Tao  Terence  Decoding by linear
programming  Information Theory  IEEE Transactions
on     

Cand es  Emmanuel    Romberg  Justin  and Tao  Terence 
Robust uncertainty principles  Exact signal reconstruction from highly incomplete frequency information  Information Theory  IEEE Transactions on   
   

Cand es  Emmanuel    Li  Xiaodong  Ma  Yi  and Wright 
John  Robust principal component analysis  Journal of
the ACM  JACM     

Chandrasekaran  Venkat  Sanghavi  Sujay  Parrilo 

        logRecovery ErrorCoPFMSR PCAGMSSPCAOP Iteration Number Clustering ErrorCoPFMSR PCAGMSSPCASubmission and Formatting Instructions for ICML  

Pablo    and Willsky  Alan    Ranksparsity incoherence for matrix decomposition  SIAM Journal on
Optimization     

Chen  Yudong  Xu  Huan  Caramanis  Constantine  and
Sanghavi  Sujay  Robust matrix completion with corrupted columns  arXiv preprint arXiv   

Costeira  Jo ao Paulo and Kanade  Takeo    multibody factorization method for independently moving objects  International Journal of Computer Vision   
 

Ding  Chris  Zhou  Ding  He  Xiaofeng  and Zha 
Hongyuan    PCA  rotational invariant    norm principal component analysis for robust subspace factorization  In Proceedings of the  rd international conference
on Machine learning  pp    ACM   

Elhamifar  Ehsan and Vidal  Rene  Sparse subspace clustering  Algorithm  theory  and applications  Pattern Analysis and Machine Intelligence  IEEE Transactions on   
   

Hardt  Moritz and Moitra  Ankur  Algorithms and hardarXiv preprint

ness for robust subspace recovery 
arXiv   

Hauberg  Soren  Feragen  Aasa  En ciaud  Raf  and
Black  Michael  Scalable robust principal component
analysis using grassmann averages  IEEE Transactions
on Pattern Analysis and Machine Intelligence   
  Nov  

Heckel  Reinhard and   olcskei  Helmut  Robust subarXiv preprint

space clustering via thresholding 
arXiv   

Hosseini  MohammadParsa  NazemZadeh  Mohammad    Mahmoudi  Fariborz  Ying  Hao  and SoltanianZadeh  Hamid  Support vector machine with nonlinearkernel optimization for lateralization of epileptogenic
In    th Annual
hippocampus in mr images 
International Conference of the IEEE Engineering in
Medicine and Biology Society  pp    IEEE 
 

Huber  Peter    Robust statistics  Springer   

Ke  Qifa and Kanade  Takeo  Robust     norm factorization
in the presence of outliers and missing data by alternative
convex programming  In Computer Vision and Pattern
Recognition    CVPR   IEEE Computer Society
Conference on  volume   pp    IEEE   

Lerman  Gilad and Maunu  Tyler 
and nonconvex subspace recovery 
arXiv   

Fast 

robust
arXiv preprint

Lerman  Gilad and Zhang  Teng       recovery of the
most signi cant subspace among multiple subspaces
with outliers  Constructive Approximation   
   

Lerman  Gilad  Zhang  Teng  et al  Robust recovery of
multiple subspaces by geometric lp minimization  The
Annals of Statistics     

Lerman  Gilad  McCoy  Michael  Tropp  Joel    and
Zhang  Teng  Robust computation of linear models  or
how to  nd   needle in   haystack  Technical report 
DTIC Document   

Li  Xingguo and Haupt  Jarvis 

Identifying outliers in
large matrices via randomized adaptive compressive
sampling  Signal Processing  IEEE Transactions on   
   

Maronna  RARD  Martin  Douglas  and Yohai  Victor  Robust statistics  John Wiley   Sons  Chichester  ISBN 
 

Maronna  Ricardo  Principal components and orthogonal
regression based on robust scales  Technometrics   
   

McCoy  Michael  Tropp  Joel    et al  Two proposals for
robust pca using semide nite programming  Electronic
Journal of Statistics     

Netrapalli  Praneeth  Niranjan  UN  Sanghavi  Sujay 
Anandkumar  Animashree  and Jain  Prateek  Nonconvex robust pca  In Advances in Neural Information
Processing Systems  pp     

Rahmani  Mostafa and Atia  George  Innovation pursuit 
  new approach to subspace clustering  arXiv preprint
arXiv   

Rahmani  Mostafa and Atia  George  Coherence pursuit 
Fast  simple  and robust principal component analysis 
arXiv preprint arXiv   

Rahmani  Mostafa and Atia  George  Randomized robust
subspace recovery and outlier detection for high dimensional data matrices  IEEE Transactions on Signal Processing    March  

Soltanolkotabi  Mahdi and Candes  Emmanuel      geometric analysis of subspace clustering with outliers  The
Annals of Statistics  pp     

Tron  Roberto and Vidal  Ren      benchmark for the comparison of    motion segmentation algorithms  In Computer Vision and Pattern Recognition    CVPR 
IEEE Conference on  pp    IEEE   

Submission and Formatting Instructions for ICML  

Tsakiris  Manolis   and Vidal  Ren    Dual principal component pursuit  In Proceedings of the IEEE International
Conference on Computer Vision Workshops  pp   
 

Vidal  Rene  Subspace clustering  IEEE Signal Processing

Magazine     

Xu  Huan  Caramanis  Constantine  and Mannor  Shie 
contaminated
arXiv preprint

Principal
data  The high dimensional case 
arXiv     

component

analysis with

Xu  Huan  Caramanis  Constantine  and Sanghavi  Sujay 
In Advances in Neural
Robust pca via outlier pursuit 
Information Processing Systems  pp       

Yi  Xinyang  Park  Dohyung  Chen  Yudong  and Caramanis  Constantine  Fast algorithms for robust pca via gradient descent  arXiv preprint arXiv   

Zhang  Teng  Robust subspace recovery by geodesically
convex optimization  arXiv preprint arXiv 
 

Zhang  Teng and Lerman  Gilad    novel mestimator for
robust pca  The Journal of Machine Learning Research 
   

