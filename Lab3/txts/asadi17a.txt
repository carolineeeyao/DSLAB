An Alternative Softmax Operator for Reinforcement Learning

Kavosh Asadi   Michael    Littman  

Abstract

  softmax operator applied to   set of values
acts somewhat like the maximization function
and somewhat like an average 
In sequential
decision making 
softmax is often used in
settings where it is necessary to maximize utility
but also to hedge against problems that arise
from putting all of one   weight behind   single
maximum utility decision 
The Boltzmann
softmax operator is the most commonly used
softmax operator in this setting  but we show
that this operator is prone to misbehavior 
In
this work  we study   differentiable softmax
operator
is  
nonexpansion ensuring   convergent behavior in
learning and planning  We introduce   variant
of SARSA algorithm that  by utilizing the new
operator  computes   Boltzmann policy with
  statedependent temperature parameter  We
show that the algorithm is convergent and that it
performs favorably in practice 

that  among other properties 

 

 Thrun 

  Introduction
There is   fundamental
tension in decision making
between choosing the action that has highest expected
utility and avoiding  starving  the other actions  The issue
the exploration exploitation
arises in the context of
dilemma
decision
problems  Sutton    and when interpreting observed
decisions  Baker et al   
In reinforcement learning  an approach to addressing the
tension is the use of softmax operators for valuefunction
optimization  and softmax policies for action selection 
Examples include valuebased methods such as SARSA
 Rummery   Niranjan    or expected SARSA  Sutton
  Barto    Van Seijen et al    and policysearch
methods such as REINFORCE  Williams   

nonstationary

 Brown University  USA  Correspondence to  Kavosh Asadi

 kavosh brown edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

An ideal softmax operator is   parameterized set of
operators that 

  has parameter settings that allow it to approximate
maximization arbitrarily accurately to perform
rewardseeking behavior 

  is   nonexpansion for all parameter settings ensuring

convergence to   unique  xed point 

  is differentiable to make it possible to improve via

gradientbased optimization  and

  avoids the starvation of nonmaximizing actions 

Let                xn be   vector of values  We de ne the
following operators 

max      max

     xi  

mean     

 
 

xi  

  cid   

eps        mean            max     

boltz       cid  
 cid  

   xi   xi
     xi

 

 Property  

is known to be

is nondifferentiable

The  rst operator  max   
 
nonexpansion  Littman   Szepesv ari    However 
it
and ignores
nonmaximizing selections  Property  
The next operator  mean    computes the average of its
inputs  It is differentiable and  like any operator that takes  
 xed convex combination of its inputs  is   nonexpansion 
However  it does not allow for maximization  Property  
The third operator eps    commonly referred to as
epsilon greedy  Sutton   Barto   
interpolates
between max and mean  The operator is   nonexpansion 
because it is   convex combination of two nonexpansion
operators  But it is nondifferentiable  Property  
The Boltzmann operator boltz    is differentiable 
It
also approximates max as       and mean as    
  However  it is not   nonexpansion  Property   and
therefore  prone to misbehavior as will be shown in the next
section 

An Alternative Softmax Operator for Reinforcement Learning

two actions  and
Figure     simple MDP with two states 
        The use of   Boltzmann softmax policy is not sound
in this simple domain 

In the following section  we provide   simple example
illustrating why the nonexpansion property is important 
especially in the context of planning and onpolicy
learning  We then present   new softmax operator
that
is  
nonexpansion  We prove several critical properties of this
new operator  introduce   new softmax policy  and present
empirical results 

to the Boltzmann operator yet

is similar

  Boltzmann Misbehaves
We  rst show that boltz  can lead to problematic behavior 
To this end  we ran SARSA with Boltzmann softmax policy
 Algorithm   on the MDP shown in Figure   The edges
are labeled with   transition probability  unsigned  and  
reward number  signed  Also  state    is   terminal state 
so we only consider two action values  namely         
and          Recall that the Boltzmann softmax policy
assigns the following probability to each action 

       

 

          

 cid             

Algorithm   SARSA with Boltzmann softmax policy
Input  initial                           and  
for each episode do
Initialize  
    Boltzmann with parameter  
repeat

Take action    observe      cid 
 cid 
 

  Boltzmann with parameter  
                       cid            cid    cid            cid 

 cid 

 cid 

     

       
until   is terminal

end for

In Figure   we plot state action value estimates at the end
of each episode of   single run  smoothed by averaging
over ten consecutive points  We set       and      
The value estimates are unstable 

Figure   Values estimated by SARSA with Boltzmann softmax 
The algorithm never achieves stable values 

SARSA is known to converge in the tabular setting using
 greedy exploration  Littman   Szepesv ari    under
decreasing exploration  Singh et al    and to   region
in the functionapproximation setting  Gordon   
There are also variants of the SARSA update rule that
converge more generally  Perkins   Precup    Baird
  Moore    Van Seijen et al    However  this
example is the  rst  to our knowledge  to show that SARSA
fails to converge in the tabular setting with Boltzmann
policy  The next section provides background for our
analysis of the example 

  Background
  Markov decision process  Puterman    or MDP 
is speci ed by the tuple  cid           cid  where   is the
set of states and   is the set of actions  The functions
              and                     denote the
reward and transition dynamics of the MDP  Finally     
    the discount rate  determines the relative importance
of immediate reward as opposed to the rewards received in
the future 
  typical approach to  nding   good policy is to estimate
how good it
is to be in   particular state the state
value function  The value of   particular state   given
  policy   and initial action   is written         We
de ne the optimal value of   state action pair   cid        
max          It is possible to de ne   cid       recursively
and as   function of the optimal value of the other
state action pairs 

  cid                  cid   cid  

           

 cid 

  max

 cid 
  cid    cid  

 cid 
   

   

Bellman equations  such as the above  are at the core
of many reinforcementlearning algorithms such as Value
Iteration  Bellman    The algorithm computes the

        episode numberAn Alternative Softmax Operator for Reinforcement Learning

Figure   Fixed points of GVI under boltz  for varying   Two
distinct  xed points  red and blue  coexist for   range of  

Figure   max is   nonexpansion under the in nity norm 

value of the best policy in an iterative fashion 

                      cid   cid            

 cid 

  max

  cid 

 cid 
    

 cid 
   

 

Regardless of its initial value     will converge to   
Littman   Szepesv ari   generalized this algorithm by

resulting in the generalized value iteration  GVI  algorithm
with the following update rule 

replacing the max operator by any arbitrary operator cid 
                  cid   cid  

 cid   cid 

          

 cid 
    

   

 cid 
   

 cid 

Algorithm   GVI algorithm
Input  initial                        and       
repeat
diff    
for each       do
for each       do
Qcopy           
          cid   cid             cid 
             cid cid       cid   
diff   max cid diff Qcopy           cid 

end for

end for

until diff    

 cid 
  

the in nity norm 

Crucially  convergence of GVI to   unique  xed point

          cid  

follows if operator cid  is   nonexpansion with respect to
 cid cid cid cid  
      cid cid cid 
Therefore  each of these operators can play the role of cid  in

for any        cid  and    As mentioned earlier  the max
operator is known to be   nonexpansion  as illustrated in
Figure   mean and eps  operators are also nonexpansions 

      cid cid cid    max

GVI  resulting in convergence to the corresponding unique

 cid cid cid               

 

 cid 

  

 cid 

 cid 
  

 cid 
      

 cid 
   

 

 

   cid   cid  
 cid 

 xed point  However  the Boltzmann softmax operator 
boltz  is not   nonexpansion  Littman    Note that
we can relate GVI to SARSA by observing that SARSA  
update is   stochastic implementation of GVI   update 
Under   Boltzmann softmax policy   the target of the
 expected  SARSA update is the following 
 

 cid 

 cid 
   

 cid           

 cid cid      cid   
           cid   cid            

 cid 

 cid 

 cid cid 

boltz cid       cid cid 
This matches the GVI update   when cid    boltz 

  Boltzmann Has Multiple Fixed Points
Although it has been known for   long time that the
Boltzmann operator is not   nonexpansion  Littman 
  we are not aware of   published example of an
MDP for which two distinct  xed points exist  The MDP
presented in Figure   is the  rst example where  as shown
in Figure   GVI under boltz  has two distinct  xed points 
We also show 
in Figure     vector  eld visualizing
GVI updates under boltz  The updates can move
the current estimates farther from the  xed points  The
behavior of SARSA  Figure   results from the algorithm
stochastically bouncing back and forth between the two
 xed points  When the learning algorithm performs  
sequence of noisy updates  it moves from    xed point to
the other  As we will show later  planning will also progress
extremely slowly near the  xed points  The lack of the
nonexpansion property leads to multiple  xed points and
ultimately   misbehavior in learning and planning 

  Mellowmax and its Properties
We advocate for an alternative softmax operator de ned as
follows 

mm     

log   

  cid  

     xi 
 

 

which can be viewed as   particular instantiation of the
quasiarithmetic mean  Beliakov et al    It can also

 maxaQ     maxaQ     maxa             An Alternative Softmax Operator for Reinforcement Learning

     yi

    cid  
   cid cid  log
 cid  
 cid cid   cid cid   cid cid    max
   cid cid  log     

 cid cid 

     yi

 

 cid cid xi   yi cid cid   

allowing us to conclude that mellowmax is   nonexpansion
under the in nity norm 

  Maximization

Mellowmax includes parameter settings that allow for
maximization  Property   as well as for minimization  In
particular  as   goes to in nity  mm  acts like max 
Let     max    and let      xi        
             Note that       is the number of maximum
values  winners  in    Then 

  mm      lim
lim
 

  lim
 

  lim
 

log   

log   

log   

     xi 
 

  cid  
      cid  

     xi   
 
    mW  
 

  lim
 

log        log      log    
    log      log    

 

 

      lim
      max     

is 

That
the operator acts more and more like pure
maximization as the value of   is increased  Conversely 
as   goes to   the operator approaches the minimum 
  Derivatives

We can take the derivative of mellowmax with respect to
each one of the arguments xi and for any nonzero  

 mm   

 xi

 

  xi
 cid  
     xi      

Note that the operator is nondecreasing in each component
of   
Moreover  we can take the derivative of mellowmax with
respect to   We de ne        log   
     xi  and
         Then 

  cid  

     

 

and so 

 mm   

 

and

     

   xie xi
     xi

   cid  
 cid  
                   

     

 

 

 

 

     

    

ensuring differentiablity of the operator  Property  

Figure     vector  eld showing GVI updates under boltz 
Fixed points are marked in black  For some points  such as the
large blue point  updates can move the current estimates farther
from the  xed points  Also  for points that lie in between the two
 xedpoints  progress is extremely slow 

be derived from information theoretical principles as   way
of regularizing policies with   cost function de ned by KL
divergence  Todorov    Rubin et al    Fox et al 
  Note that the operator has previously been utilized
in other areas  such as power engineering  Safak   
We show that mm  which we refer to as mellowmax  has
the desired properties and that it compares quite favorably
to boltz  in practice 

  Mellowmax is   NonExpansion

We prove that mm  is   nonexpansion  Property   and
therefore  GVI and SARSA under mm  are guaranteed to
converge to   unique  xed point 
Let                xn and                yn be two vectors
of values  Let      xi   yi for                  be the
difference of the ith components of the two vectors  Also 
let    be the index with the maximum componentwise
difference       argmaxi     For simplicity  we assume
that    is unique and       Also  without loss of
generality  we assume that xi    yi      It follows that 

 
 

  cid   

  yi cid cid 

 

 

 
 

  xi    log 
     xi
     yi

 cid cid mm      mm   cid cid 
  cid   
   cid cid  log 
  cid  
 cid cid 
   cid cid  log
  cid  
     cid yi   cid 
   cid cid  log cid  
 cid cid 
 cid  
     cid yi   cid 
   cid cid  log cid  
 cid cid 
 cid  

     yi

     yi

An Alternative Softmax Operator for Reinforcement Learning

  Averaging

multipliers  Here  the Lagrangian is 

Because of the division by   in the de nition of mm 
the parameter   cannot be set to zero  However  we can
examine the behavior of mm  as   approaches zero and
show that the operator computes an average in the limit 
Since both the numerator and denominator go to zero as  
goes to zero  we will use     opital   rule and the derivative
given in the previous section to derive the value in the limit 

      log cid     cid 

        cid    
 cid cid    
         cid 
 cid cid    
                 mm cid         cid cid   

mm   

lim
 

 

    opital

 

     xi 
 

   xie xi
     xi

lim
 

lim
 

 

 

log   

  cid  
  cid  
  cid  
  cid   

xi   mean     

 

 
 

That is  as   gets closer to zero  mm    approaches the
mean of the values in   

  Maximum Entropy Mellowmax Policy
As described  mm  computes   value for   list of
numbers somewhere between its minimum and maximum 
However  it is often useful to actually provide   probability
distribution over the actions such that     nonzero
probability mass is assigned to each action  and   the
resulting expected value equals the computed value  Such
  probability distribution can then be used for action
selection in algorithms such as SARSA 
In this section  we address the problem of identifying
such   probability distribution as   maximum entropy
problem over all distributions that satisfy the properties
above 
information
entropy  Cover   Thomas    Peters et al   
We formally de ne the maximum entropy mellowmax
policy of   state   as 

that maximizes

pick the one

 mm      argmin

subject to cid   cid                      mm         

      log cid     cid 

   cid    
 cid                

         

 

Note that this optimization problem is convex and can be
solved reliably using any numerical convex optimization
library 
One way of  nding the solution  which leads to an
interesting policy form  is to use the method of Lagrange

Taking the partial derivative of the Lagrangian with respect
to each       and setting them to zero  we obtain 

  

  log cid     cid                        
     
These     equations 
together with the two linear
constraints in   form         equations to constrain the
        variables              and the two Lagrangian
multipliers   and  
Solving this system of equations  the probability of taking
an action under the maximum entropy mellowmax policy
has the form 

 mm       

        

          

 cid               

where   is   value for which 

  cid         mm       cid cid             mm         cid       

 cid    
The argument for the existence of   unique root is simple 
As       the term corresponding to the best action
dominates  and so  the function is positive  Conversely 
as       the term corresponding to the action with
lowest utility dominates  and so the function is negative 
Finally  by taking the derivative  it is clear that the function
is monotonically increasing  allowing us to conclude that
there exists only   single root  Therefore  we can  nd  
easily using any root nding algorithm  In particular  we
use Brent   method  Brent    available in the Numpy
library of Python 
This policy has the same form as Boltzmann softmax  but
with   parameter   whose value depends indirectly on
  This mathematical form arose not from the structure
of mm  but from maximizing the entropy  One way to
view the use of the mellowmax operator  then  is as   form
of Boltzmann policy with   temperature parameter chosen
adaptively in each state to ensure that the nonexpansion
property holds 
Finally  note that the SARSA update under the maximum
entropy mellowmax policy could be thought of as  

An Alternative Softmax Operator for Reinforcement Learning

Figure   GVI updates under mm  The  xed point is
unique  and all updates move quickly toward the  xed point 

stochastic implementation of the GVI update under the
mm  operator 
 

 cid 

 cid 
   
 cid 

 mm cid           
 cid   cid            

 cid cid      cid   

              

 cid 

   cid   cid  
 cid 

 cid 
 mm  

 cid 
  

 cid 
      

 cid 
   

 cid cid 

mm cid       cid cid 

due to the  rst constraint of the convex optimization
problem   Because mellowmax is   nonexpansion 
SARSA with the maximum entropy mellowmax policy is
guaranteed to converge to   unique  xed point  Note also
that  similar to other variants of SARSA  the algorithm
simply bootstraps using the value of the next state while
implementing the new policy 

  Experiments on MDPs
We observed that in practice computing mellowmax can
yield over ow if the exponentiated values are large  In this
case  we can safely shift the values by   constant before
exponentiating them due to the following equality 

log   

  cid  

     xi 
 

     

log   

  cid  
     xi   
 

 

  value of     maxi xi usually avoids over ow 
We repeat the experiment from Figure   for mellowmax
with       to get   vector  eld  The result  presented
in Figure   show   rapid and steady convergence towards
the unique  xed point  As   result  GVI under mm  can
terminate signi cantly faster than GVI under boltz  as
illustrated in Figure  
three additional experiments 
The  rst
We present
investigates the behavior of GVI with the
experiment
softmax operators on randomly generated MDPs  The
second experiment evaluates the softmax policies when
used in SARSA with   tabular representation  The last

Figure   Number of iterations before termination of GVI on the
example MDP  GVI under mm  outperforms the alternatives 

experiment is   policy gradient experiment where   deep
neural network  with   softmax output layer  is used to
directly represent the policy 

  Random MDPs

 cid 
 cid 

The example in Figure   was created carefully by hand  It
is interesting to know whether such examples are likely to
be encountered naturally  To this end  we constructed  
MDPs as follows  We sampled     from         and
    from         uniformly at random  We initialized
the transition probabilities by sampling uniformly from
    We then added to each entry  with probability  
Gaussian noise with mean   and variance   We next
added  with probability   Gaussian noise with mean  
and variance   Finally  we normalized the raw values to
ensure that we get   transition matrix  We did   similar
process for rewards  with the difference that we divided
each entry by the maximum entry and multiplied by  
to ensure that Rmax      
We measured the failure rate of GVI under boltz  and
mm  by stopping GVI when it did not terminate in  
iterations  We also computed the average number of
iterations needed before termination    summary of results
is presented in the table below  Mellowmax outperforms
Boltzmann based on the three measures provided below 

no

MDPs 
terminate
  of  
 

MDPs     
 xed points
  of  
 

average
iterations
 
 

boltz 
mm 

  Multipassenger Taxi Domain

We evaluated SARSA on the multipassenger taxi domain
introduced by Dearden et al     See Figure  
One challenging aspect of this domain is that it admits
many locally optimal policies 
Exploration needs
to be set carefully to avoid either overexploring
or underexploring the state space 
Note also that
Boltzmann softmax performs
remarkably well on
outperforming sophisticated Bayesian
this domain 

An Alternative Softmax Operator for Reinforcement Learning

Figure   Multipassenger taxi domain  The discount rate   is
  Reward is   for delivering one passenger    for two
passengers  and   for three passengers  Reward is zero for all
the other transitions  Here        and   denote passengers  start
state  and destination respectively 

reinforcementlearning algorithms  Dearden et al   
As shown in Figure   SARSA with the epsilongreedy
policy performs poorly 
In fact  in our experiment  the
algorithm rarely was able to deliver all the passengers 
However  SARSA with Boltzmann softmax and SARSA
with the maximum entropy mellowmax policy achieved
signi cantly higher average reward  Maximum entropy
mellowmax policy is no worse than Boltzmann softmax 
here  suggesting that the greater stability does not come at
the expense of less effective exploration 

  Lunar Lander Domain

In this section  we evaluate the use of the maximum entropy
mellowmax policy in the context of   policygradient
algorithm 
Speci cally  we represent   policy by  
neural network  discussed below  that maps from states
to probabilities over actions    common choice for the
activation function of the last
layer is the Boltzmann
softmax policy  In contrast  we can use maximum entropy
mellowmax policy  presented in Section   by treating the
inputs of the activation function as    values 
We used the lunar lander domain  from OpenAI Gym
 Brockman et al    as our benchmark    screenshot
of the domain is presented in Figure   This domain has
  continuous state space with   dimensions  namely xy
coordinates  xy velocities  angle and angular velocities 
and legtouchdown sensors  There are   discrete actions to
control   engines  The reward is   for   safe landing in
the designated area  and   for   crash  There is   small
shaping reward for approaching the landing area  Using the
engines results in   negative reward  An episode  nishes
when the spacecraft crashes or lands  Solving the domain
is de ned as maintaining mean episode return higher than
  in   consecutive episodes 
The policy in our experiment is represented by   neural
network with   hidden layer comprised of   units with
RELU activation functions  followed by   second layer
with   units and softmax activation functions  We used
REINFORCE to train the network    batch episode size

Figure   Comparison on the multipassenger
taxi domain 
Results are shown for different values of     and   For each
setting  the learning rate is optimized  Results are averaged over
  independent runs  each consisting of   time steps 

Figure     screenshot of the lunar lander domain 

of   was used  as we had stability issues with smaller
episode batch sizes  We used the Adam algorithm  Kingma
  Ba    with       and the other parameters as
suggested by the paper  We used Keras  Chollet   
and Theano  Team et al    to implement the neural
network architecture  For each softmax policy  we present
in Figure   the learning curves for different values of
their free parameter  We further plot average return over
all   episodes  Mellowmax outperforms Boltzmann at
its peak 

  Related Work
Softmax operators play an important role in sequential
decisionmaking algorithms 
In modelfree reinforcement learning  they can help strike

FFSFDAn Alternative Softmax Operator for Reinforcement Learning

 Ng   Russell   

Algorithms for inverse reinforcement learning  IRL  the
problem of inferring reward functions from observed
behavior
frequently use  
Boltzmann operator to avoid assigning zero probability
to nonoptimal actions and hence assessing an observed
sequence as impossible  Such methods include Bayesian
IRL  Ramachandran   Amir    natural gradient
IRL  Neu   Szepesv ari    and maximum likelihood
IRL  Babes et al    Given the recursive nature of
value de ned in these problems  mellowmax could be  
more stable and ef cient choice 
In linearly solvable MDPs  Todorov    an operator
similar to mellowmax emerges when using an alternative
characterization for cost of action selection in MDPs 
Inspired by this work Fox et al    introduced an
offpolicy Glearning algorithm that uses the operator to
perform valuefunction updates 
Instead of performing
offpolicy updates  we introduced   convergent variant
of SARSA with Boltzmann policy and   statedependent
temperature parameter  This is in contrast to Fox et al 
  where an epsilon greedy behavior policy is used 

Figure   Comparison of Boltzmann  top  and maximum
entropy mellowmax  middle  in Lunar Lander  Mean return over
all episodes  bottom  Results are  run averages 

  balance between exploration  mean  and exploitation
 max 
Decision rules based on epsilongreedy and
Boltzmann softmax  while very simple  often perform
surprisingly well in practice  even outperforming more
advanced exploration techniques  Kuleshov   Precup 
  that require signi cant approximation for complex
domains  When learning  on policy 
exploration
steps can  Rummery   Niranjan    and perhaps
should  John    become part of the valueestimation
process itself  Onpolicy algorithms like SARSA can be
made to converge to optimal behavior in the limit when the
exploration rate and the update operator is gradually moved
toward max  Singh et al    Our use of softmax in
learning updates re ects this point of view and shows that
the valuesensitive behavior of Boltzmann exploration can
be maintained even as updates are made stable 
Analyses of the behavior of human subjects in choice
experiments very frequently use softmax 
Sometimes
referred to in the literature as logit choice  Stahl  
Wilson   
it forms an important part of the most
accurate predictor of human decisions in normalform
games  Wright   LeytonBrown    quantal levelk
reasoning  QLk  Softmaxbased  xed points play   crucial
role in this work  As such  mellowmax could potentially
make   good replacement 

  Conclusion and Future Work
We proposed the mellowmax operator as an alternative
to the Boltzmann softmax operator  We showed that
mellowmax has several desirable properties and that it
works favorably in practice 
Arguably  mellowmax
could be used in place of Boltzmann throughout
reinforcementlearning research 
to analyze the  xed point
  future direction is
reinforcementlearning  and gameplaying
of planning 
algorithms when using the mellowmax operators 
In
particular  an interesting analysis could be one that bounds
the suboptimality of the  xed points found by GVI 
An important future work is to expand the scope of our
theoretical understanding to the more general function
approximation setting 
in which the state space or the
action space is large and abstraction techniques are used 
Note that the importance of nonexpansion in the function
approximation case is wellestablished   Gordon   
Finally  due to the convexity of mellowmax  Boyd  
Vandenberghe   
in  
gradientbased algorithm in the context of sequential
decision making 
IRL is   natural candidate given the
popularity of softmax in this setting 

is compelling to use it

it

  Acknowledgments
The authors gratefully acknowledge the assistance of
George    Konidaris  as well as anonymous ICML
reviewers for their outstanding feedback 

An Alternative Softmax Operator for Reinforcement Learning

References
Babes  Monica  Marivate  Vukosi    Littman  Michael   
and Subramanian  Kaushik  Apprenticeship learning
In International Conference
about multiple intentions 
on Machine Learning  pp     

Baird  Leemon and Moore  Andrew    Gradient descent
In Advances in
for general reinforcement learning 
Neural Information Processing Systems  pp   
 

Baker  Chris    Tenenbaum 

Joshua    and Saxe 
Rebecca    Goal inference as inverse planning 
In
Proceedings of the  th Annual Meeting of the Cognitive
Science Society   

Beliakov  Gleb  Sola  Humberto Bustince  and   anchez 
  Practical Guide to Averaging

Tomasa Calvo 
Functions  Springer   

Bellman  Richard    Markovian decision process  Journal

of Mathematics and Mechanics     

Boyd       and Vandenberghe     Convex optimization 

Cambridge University Press   

Brent  Richard    Algorithms for minimization without

derivatives  Courier Corporation   

Brockman  Greg  Cheung  Vicki  Pettersson  Ludwig 
Schneider  Jonas  Schulman  John  Tang  Jie  and
Zaremba  Wojciech  Openai gym   

Chollet  Franc ois  Keras  https github com 

fchollet keras   

Cover       and Thomas       Elements of Information

Theory  John Wiley and Sons   

Dearden  Richard  Friedman  Nir  and Russell  Stuart 
Bayesian Qlearning  In Fifteenth National Conference
on Arti cial Intelligence  AAAI  pp     

Fox  Roy  Pakman  Ari  and Tishby  Naftali  Taming
the noise in reinforcement learning via soft updates 
In Proceedings of
the ThirtySecond Conference on
Uncertainty in Arti cial Intelligence  pp   
AUAI Press   

Gordon  Geoffrey   

Stable function approximation
the
in dynamic programming 
twelfth international conference on machine learning 
pp     

In Proceedings of

Gordon  Geoffrey   

learning with
function approximation converges to   region   
Unpublished 

Reinforcement

John  George    When the best move isn   optimal 
In Proceedings of the
Qlearning with exploration 
Twelfth National Conference on Arti cial Intelligence 
pp    Seattle  WA   

Kingma  Diederik and Ba 

Jimmy 
method for stochastic optimization 
arXiv   

Adam 

 
arXiv preprint

Kuleshov  Volodymyr and Precup  Doina  Algorithms
arXiv preprint

for multiarmed bandit problems 
arXiv   

Littman  Michael    and Szepesv ari  Csaba 

 
generalized reinforcementlearning model  Convergence
and applications  In Saitta  Lorenza  ed  Proceedings
of the Thirteenth International Conference on Machine
Learning  pp     

Littman  Michael Lederman  Algorithms for Sequential
Decision Making  PhD thesis  Department of Computer
Science  Brown University  February  
Also
Technical Report CS 

Neu  Gergely and Szepesv ari  Csaba  Apprenticeship
learning and

learning using inverse reinforcement
gradient methods  In UAI   

Ng  Andrew    and Russell  Stuart  Algorithms for inverse
reinforcement learning  In International Conference on
Machine Learning  pp     

Perkins  Theodore   and Precup  Doina    convergent form
of approximate policy iteration  In Advances in Neural
Information Processing Systems  pp     

Peters  Jan    ulling  Katharina  and Altun  Yasemin 
Relative entropy policy search  In AAAI  Atlanta   

Martin

  

Puterman 

Decision
Processes Discrete Stochastic Dynamic Programming 
John Wiley   Sons  Inc  New York  NY   

Markov

Ramachandran  Deepak and Amir  Eyal  Bayesian inverse

reinforcement learning  In IJCAI   

Rubin  Jonathan  Shamir  Ohad  and Tishby  Naftali 
In Decision
Trading value and information in mdps 
Making with Imperfect Decision Makers  pp   
Springer   

Rummery        and Niranjan     Online Qlearning
using connectionist
Technical Report
CUED FINFENG TR   Cambridge University
Engineering Department   

systems 

Safak  Aysel 

Statistical analysis of the power sum
IEEE
of multiple correlated lognormal components 
Transactions on Vehicular Technology   
 

An Alternative Softmax Operator for Reinforcement Learning

Singh  Satinder  Jaakkola  Tommi  Littman  Michael   
and Szepesv ari  Csaba 
Convergence results for
singlestep onpolicy reinforcementlearning algorithms 
Machine Learning     

Stahl  Dale    and Wilson  Paul   

Experimental
evidence on players  models of other players  Journal of
Economic Behavior and Organization   
 

Sutton  Richard   

Integrated architectures for learning 
planning  and reacting based on approximating dynamic
the Seventh
programming 
International Conference on Machine Learning  pp 
  Austin  TX    Morgan Kaufmann 

In Proceedings of

Sutton  Richard    and Barto  Andrew    Reinforcement

Learning  An Introduction  The MIT Press   

Team  The Theano Development  AlRfou  Rami  Alain 
Guillaume  Almahairi  Amjad  Angermueller  Christof 
Bahdanau  Dzmitry  Ballas  Nicolas  Bastien  Fr ed eric 
Bayer  Justin  Belikov  Anatoly  et al  Theano   
python framework for fast computation of mathematical
expressions  arXiv preprint arXiv   

Thrun  Sebastian    The role of exploration in learning
In White  David    and Sofge  Donald   
control 
 eds  Handbook of Intelligent Control  Neural  Fuzzy 
and Adaptive Approaches  pp    Van Nostrand
Reinhold  New York  NY   

Todorov  Emanuel  Linearlysolvable markov decision

problems  In NIPS  pp     

Van Seijen  Harm  Van Hasselt  Hado  Whiteson  Shimon 
  theoretical and empirical
and Wiering  Marco 
In   IEEE Symposium
analysis of Expected Sarsa 
on Adaptive Dynamic Programming and Reinforcement
Learning  pp    IEEE   

Williams  Ronald    Simple statistical gradientfollowing
learning 

algorithms for connectionist reinforcement
Machine Learning     

Wright  James    and LeytonBrown  Kevin  Beyond
equilibrium  Predicting human behavior in normalform
games  In AAAI   

