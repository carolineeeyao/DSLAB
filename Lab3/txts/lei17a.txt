Deriving Neural Architectures from Sequence and Graph Kernels

Tao Lei    Wengong Jin    Regina Barzilay   Tommi Jaakkola  

Abstract

The design of neural architectures for structured
objects is typically guided by experimental insights rather than   formal process  In this work 
we appeal to kernels over combinatorial structures  such as sequences and graphs  to derive
appropriate neural operations  We introduce  
class of deep recurrent neural operations and formally characterize their associated kernel spaces 
Our recurrent modules compare the input to virtual reference objects  cf   lters in CNN  via the
kernels  Similar to traditional neural operations 
these reference objects are parameterized and directly optimized in endto end training  We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression 
achieving stateof theart results across these applications 

  Introduction
Many recent studies focus on designing novel neural architectures for structured data such as sequences or annotated graphs  For instance  LSTM  Hochreiter   Schmidhuber    GRU  Chung et al    and other complex
recurrent units  Zoph   Le    can be easily adapted
to embed structured objects such as sentences  Tai et al 
  or molecules  Li et al    Dai et al    into
vector spaces suitable for later processing by standard predictive methods  The embedding algorithms are typically
integrated into an endto end trainable architecture so as to
tailor the learnable embeddings directly to the task at hand 
The embedding process itself is characterized by   sequence operations summarized in   structure known as
the computational graph  Each node in the computational

 Equal contribution

Intelligence Laboratory 

ti cial
Tao Lei  taolei csail mit edu  Wengong
gong csail mit edu 

 MIT Computer Science   Arto 
Jin  wen 

Correspondence

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

graph identi es the unit mapping applied while the arcs
specify the relative arrangement order of operations  The
process of designing such computational graphs or associated operations for classes of objects is often guided by
insights and expertise rather than   formal process 
Recent work has substantially narrowed the gap between
desirable computational operations associated with objects
and how their representations are acquired  For example 
value iteration calculations can be folded into convolutional
architectures so as to optimize the representations to facilitate planning  Tamar et al    Similarly  inference
calculations in graphical models about latent states of variables such as atom characteristics can be directly associated
with embedding operations  Dai et al   
We appeal to kernels over combinatorial structures to de 
 ne the appropriate computational operations  Kernels give
rise to wellde ned function spaces and possess rules of
composition that guide how they can be built from simpler
ones  The comparison of objects inherent in kernels is often broken down to elementary relations such as counting
of common substructures as in

where   is the set of possible substructures  For example  in   string kernel  Lodhi et al      may refer
to all possible subsequences while   graph kernel  Vishwanathan et al    would deal with possible paths
in the graph  Several studies have highlighted the relation between feedforward neural architectures and kernels  Hazan   Jaakkola    Zhang et al    but we
are unaware of any prior work pertaining to kernels associated with neural architectures for structured objects 
In this paper  we introduce   class of deep recurrent neural
embedding operations and formally characterize their associated kernel spaces  The resulting kernels are parameterized in the sense that the neural operations relate objects of
interest to virtual reference objects through kernels  These
reference objects are parameterized and readily optimized
for endto end performance 
To summarize  the proposed neural architectures  or Kernel
Neural Networks     enjoy the following advantages 

 Code available at https github com taolei icml  knn

      Xs  

           

 

Deriving Neural Architectures from Sequence and Graph Kernels

tations 

  The architecture design is grounded in kernel compu 
  Our neural models remain endto end trainable to the
  Resulting architectures demonstrate stateof theart

performance against strong baselines 

task at hand 

 

 

 

 

 

 

 

 

In the following sections  we will introduce these neural
components derived from string and graph kernels  as well
as their deep versions  Due to space limitations  we defer
proofs to supplementary material 

Figure   An unrolled view of the derived recurrent module for
   Horizontal lines denote decayed propagation from        
to      while vertical lines represent   linear mapping Wxt that
is propagated to the internal states     

  From String Kernels to Sequence NNs
Notations We de ne   sequence  or   string  of tokens
        sentence  as        xi  
   where xi   Rd represents its ith element and         denotes the length  Whenever it is clear from the context  we will omit the subscript
and directly use    and    to denote   sequence  For   pair
of vectors  or matrices        we denote hu  vi  Pk ukvk
as their inner product  For   kernel function Ki  with
subscript    we use     to denote its underlying mapping 
     Ki                                 
String Kernel String kernel measures the similarity between two sequences by counting shared subsequences
 see Lodhi et al    For example  let   and   be two
strings    bigram string kernel         counts the number of bigrams  xi  xj  and  yk  yl  such that  xi  xj   
 yk  yl 
                  
 
where                     are contextdependent weights
and        is an indicator that returns   only when       
The weight factors can be realized in various ways  For
instance  in temporal predictions such as language modeling  substrings       patterns  which appear later may have
higher impact for prediction  Thus   realization         
      and                 penalizing substrings far
from the end  can be used to determine weights given  
constant decay factor        
In our case  each token in the sequence is   vector  such as
onehot encoding of   word or   feature vector  We shall
replace the exact match        by the inner product hu  vi 
To this end  the kernel function   can be rewritten as 
                 
              hxi  yki   xj  yli

               xi  yk     xj  yl 

       

 An extended version with supplementary material is available

at https arxiv org abs 

 We de ne ngram as   subsequence of original string  not

necessarily consecutive 

                   
 Xi  

      xi   xj  Xk  

              hxi   xj  yk   yli

      yk   yl 

 

where xi   xj   Rd    and similarly yk   yl  is the outerproduct  In other words  the underlying mapping of kernel
   de ned above is                    xi  
xj  Note we could alternatively use   partial additive scoring hxi  yki  hxj  yli  and the kernel function can be generalized to ngrams when       Again  we commit to one
realization in this section 
String Kernel NNs We introduce   class of recurrent modules whose internal feature states embed the computation of
string kernels  The modules project kernel mapping    
into multidimensional vector space       internal states of
recurrent nets  Owing to the combinatorial structure of
    such projection can be realized and factorized via
ef cient computation  For the example kernel discussed
above  the corresponding neural component is realized as 

                      xt 
cj          cj        cj             xt 

        cn   

         

 

where cj    are the preactivation cell states at word xt  and
     is the  postactivation  hidden vector  cj  is initialized with   zero vector            are weight matrices
to be learned from training examples 
The network operates like other RNNs by processing each
input token and updating the internal states  The elementwise multiplication   can be replaced by addition    corresponding to the partial additive scoring above  As   special case  the additive variant becomes   wordlevel convolutional neural net  Kim    when      

           xt              xt  when      

Deriving Neural Architectures from Sequence and Graph Kernels

  Single Layer as Kernel Computation
Now we state how the proposed class embeds string kernel
computation  For            let cj      be the ith entry
of state vector cj        
represents the ith row of matrix      De ne wi        
    as    reference sequence  constructed by taking the ith row from
each matrix          
Theorem   Let     be the pre   of   consisting of  rst
  tokens  and Kj be the string kernel of jgram shown in
Eq  Then cj      evaluates kernel function 

        

    

 

 

 

cj        Kj       wi                 wi    

for any                     
In other words  the network embeds sequence similarity
computation by assessing the similarity between the input
sequence     and the reference sequence wi    This interpretation is similar to that of CNNs  where each  lter
is    reference pattern  to search in the input  String kernel NN further takes nonconsecutive ngram patterns into
consideration  seen from the summation over all ngrams
in Eq 
Applying Nonlinear Activation In practice    nonlinear
activation function such as polynomial or sigmoidlike activation is added to the internal states to produce the  
nal output state     
It turns out that many activations
are also functions in the reproducing kernel Hilbert space
 RKHS  of certain kernel functions  see ShalevShwartz
et al    Zhang et al    When this is true  the
underlying kernel of      is the composition of string kernel and the kernel containing the activation  We give the
formal statements below 
Lemma   Let   and   be multidimensional vectors with
 nite norm  Consider the function               with
nonlinear activation   For functions such as polynomials and sigmoid function  there exists kernel functions
   and the underlying mapping   such that      
is in the reproducing kernel Hilbert space of        

                          

 

 hx yi

for some mapping      constructed from    In particular 
for
        can be the inversepolynomial kernel
the above activations 
Proposition   For one layer string kernel NN with nonlinear activation   discussed in Lemma          as  
function of input   belongs to the RKHS introduced by the
composition of    and string kernel Kn  Here  
kernel composition           is de ned with the underlying mapping           and hence            
         

Proposition   is the corollary of Lemma   and Theorem   since           cn         Kn      wi     
          wi ji and     is the mapping for the
composed kernel  The same proof applies when      is  
linear combination of all ci    since kernel functions are
closed under addition 

  Deep Networks as Deep Kernel Construction
We now address the case when multiple layers of the same
module are stacked to construct deeper networks  That is 
the output states        of the lth layer are fed to the    
th layer as the input sequence  We show that layer stacking
corresponds to recursive kernel construction             
th kernel is de ned on top of lth kernel  which has been
proven for feedforward networks  Zhang et al   
We  rst generalize the sequence kernel de nition to enable
recursive construction  Notice that the de nition in Eq 
uses the linear kernel  inner product  hxi  yki as    subroutine  to measure the similarity between substructures      
tokens  within the sequences  We can therefore replace it
with other similarity measures introduced by other  base
kernels 
In particular  let    be the string kernel
 associated with   single layer  The generalized sequence
kernel           can be recursively de ned as 
Xi  
 Xi  

       
where     denotes the preactivation mapping of the lth kernel     
          denotes the underlying
 postactivation  mapping for nonlinear activation  
and     
    is the lth postactivation kernel  Based on
this de nition    deeper model can also be interpreted as  
kernel computation 
Theorem   Consider   deep string kernel NN with  
layers and activation function   Let the  nal output
state               
       or any linear combination of
     
        

               For           
        as   function of input   belongs to the RKHS
of kernel     

                 
       Xk  

   
             

                  

   
             

              

 ii           belongs to the RKHS of kernel     

   

   

 

  From Graph Kernels to Graph NNs
In the previous section  we encode sequence kernel computation into neural modules and demonstrate possible extensions using different base kernels  The same ideas apply

Deriving Neural Architectures from Sequence and Graph Kernels

to other types of kernels and data  Speci cally  we derive
neural components for graphs in this section 
Notations   graph is de ned as            with each
vertex       associated with feature vector fv  The neighbor of node   is denoted as       Following previous
notations  for any kernel function    with underlying mapping   we use    to denote the postactivation kernel induced from the composed underlying
mapping      
  Random Walk Kernel NNs
We start from random walk graph kernels    artner et al 
  which count common walks in two graphs  Formally  let Pn    be the set of walks          xn  where
      xi  xi       Given two graphs   and    an nth
order random walk graph kernel is de ned as 
               Xx Pn    Xy Pn   
Kn
where fxi   Rd is the feature vector of node xi in the walk 
Now we show how to realize the above graph kernel with
  neural module  Given   graph    the proposed neural
module is 

hfxi  fyii

nYi 

 

         fv

cj        Xu      
hG    Xv

cn   

cj          fv

 

         

where again      is the cell state vector of node    and hG
is the representation of graph   aggregated from node vectors  hG could then be used for classi cation or regression 
Now we show the proposed model embeds the random
walk kernel  To show this  construct Ln   as    reference walk  consisting of the row vectors    
          
   
from the parameter matrices  Here Ln      LV   LE 
where LV            vn  LE    vi  vi  and vi  
feature vector is     
Theorem   For any       the state value cn       the
kth coordinate of cn    satis es 

    We have the following theorem 

Xv

cn        Kn

      Ln   

thusPv cn    lies in the RKHS of kernel Kn

lary  hG lies in the RKHS of kernel Kn

  

   single node could appear multiple times in   walk 

    As   corol 

 

  Uni ed View of Graph Kernels
The derivation of the above neural module could be extended to other classes of graph kernels  such as subtree
kernels  cf   Ramon     artner    Vishwanathan et al 
  Generally speaking  most of these kernel functions
factorize graphs into local substructures      
Kloc      

         Xv Xv 

 

where Kloc       measures the similarity between local
substructures centered at node   and   
For example  the random walk kernel Kn
lently de ned with Kn

  can be equiva 

loc        

hfv  fv  

hfv  fv      Pu       Pu      Kn 

loc       

if      
if      

Other kernels like subtree kernels could be recursively de 
 ned similarly  Therefore  we adopt this uni ed view of
graph kernels for the rest of this paper 
In addition  this de nition of random walk kernel could be
further generalized and enhanced by aggregating neighbor
features nonlinearly 

loc         hfv  fv       Xu       Xu      
Kn
where   could be either multiplication or addition   
denotes   nonlinear activation and Kn 
loc  denotes the
postactivation kernel when   is involved  The generalized kernel could be realized by modifying Eq  into 

Kn 
loc      

 

 cj   

cj          fv     Xu      
where   could be either   or   operation 
  Deep Graph Kernels and NNs
Following Section   we could stack multiple graph kernel
NNs to form   deep network  That is 

    
                  
    

                         Xu      

     
    

     

                  

                    
function is recursively de ned in
The local kernel
two dimensions 
depth  term      and width  term
cj  Let the preactivation kernel in the lth layer be
    
loc               
loc        and the postactivation kernel be     
loc       We recursively de ne

loc               

Deriving Neural Architectures from Sequence and Graph Kernels

if      
if      

loc

Finally 

loc        

      
loc         
 
    
loc        
loc             Pu       Pu            
    
for           
              Pv          
rem   we have
Theorem   Consider   deep graph kernel NN with   layers and activation function   Let the  nal output state
hG  Pv        For                      

is
       Similar to Theo 

        as   function of input   and graph   belongs
to the RKHS of kernel       

the graph kernel

loc  

        

 ii           belongs to the RKHS of kernel       
 iii  hG    belongs to the RKHS of kernel       

loc 

  Connection to WeisfeilerLehman Kernel
We derived the above deep kernel NN for the purpose of
generality  This model could be simpli ed by setting    
  without losing representational power  as nonlinearity
is already involved in depth dimension  In this case  we
rewrite the network by reparametrization 

    

         

      

 

  Xu      
      

         

 

   

In this section  we further show that this model could be enhanced by sharing weight matrices   and   across layers 
This parameter tying mechanism allows our model to embed WeisfeilerLehman kernel  Shervashidze et al   
For clarity  we brie   review basic concepts of WeisfeilerLehman kernel below 
WeisfeilerLehman Graph Relabeling WeisfeilerLehman kernel borrows concepts from the WeisfeilerLehman isomorphism test for labeled graphs  The key idea
of the algorithm is to augment the node labels by the sorted
set of node labels of neighbor nodes  and compress these
augmented labels into new  short labels  Figure   Such
relabeling process is repeated   times  In the ith iteration 
it generates   new labeling li    for all nodes   in graph   
with initial labeling   
Generalized Graph Relabeling The key observation here
is that graph relabeling operation could be viewed as neighbor feature aggregation  As   result  the relabeling process
naturally generalizes to the case where nodes are associated
with continuous feature vectors  In particular  let   be the
relabeling function  For   node       
          fv      Xu      

 Vfu 

 

Figure   Node relabeling in WeisfeilerLehman isomorphism test

Note that our de nition of      is exactly the same as hv in
Equation   with   being additive composition 
WeisfeilerLehman Kernel Let   be any graph kernel  called base kernel  Given   relabeling function   
WeisfeilerLehman kernel with base kernel   and depth  
is de ned as

LXi 

    
           

  ri    ri   

 

 

 

 

    

    

where          and ri    ri    are the ith relabeled
graph of   and    respectively 
WeisfeilerLehman Kernel NN Now with the above kernel de nition  and random walk kernel as the base kernel 
we propose the following recurrent module 
    
                
 
    
    
                 
     Xu      

          Xu      
           
    Xv

                    

  

 Vh   

    
     

where   

       
   

    fv and         are shared across layers 

The  nal output of this network is hG  PL

The above recurrent module is still an instance of deep kernel  even though some parameters are shared    minor difference here is that there is an additional random walk kernel NN that connects ith layer and the output layer  But
this is just   linear combination of   deep random walk
kernels  of different depth  Therefore  as an corollary of
Theorem   we have 
Proposition   For   WeisfeilerLehman Kernel NN with  
iterations and random walk kernel Kn
  as base kernel  the
 nal output state hG   Pl     
  belongs to the RKHS of
kernel     

    

Deriving Neural Architectures from Sequence and Graph Kernels

  Adaptive Decay with Neural Gates
The sequence and graph kernel  and their neural components  discussed so far use   constant decay value   regardless of the current input  However  this is often not the
case since the importance of the input can vary across the
context or the applications  One extension is to make use of
neural gates that adaptively control the decay factor  Here
we give two illustrative examples 
Gated String Kernel NN By replacing constant decay  
with   sigmoid gate  we modify our singlelayer sequence
module as 

        xt  ht      

                       xt 
cj           cj        cj             xt 

        cn   

         

As compared with the original string kernel  now the decay
factor        is no longer       but rather an adaptive
value based on current context 
Gated Random Walk Kernel NN Similarly  we could introduce gates so that different walks have different weights 

          fu  fv      
         fv

cj      Xu      
hG    Xv

       cj          fv

cn   

         

The underlying kernel of the above gated network becomes

           Xx Pn    Xy Pn   
Kn

nYi 

 xi yi hfxi  fyii

where each path is weighted by different decay weights 
determined by network itself 

  Related Work
Sequence Networks Considerable effort has gone into designing effective networks for sequence processing  This
includes recurrent modules with the ability to carry persistent memories such as LSTM  Hochreiter   Schmidhuber    and GRU  Chung et al    as well as
nonconsecutive convolutional modules  RCNNs  Lei et al 
  and others  More recently  Zoph   Le   exempli ed   reinforcement learningbased search algorithm
to further optimize the design of such recurrent architectures  Our proposed neural networks offer similar state

evolution and feature aggregation functionalities but derive the motivation for the operations involved from wellestablished kernel computations over sequences 
Recursive neural networks are alternative architectures to
model hierarchical structures such as syntax trees and logic
forms  For instance  Socher et al    employs recursive networks for sentence classi cation  where each node
in the dependency tree of the sentence is transformed into
  vector representation  Tai et al    further proposed
treeLSTM  which incorporates LSTMstyle architectures
as the transformation unit  Dyer et al      recently introduced   recursive neural model for transitionbased language modeling and parsing  While not speci 
cally discussed in the paper  our ideas do extend to similar
neural components for hierarchical objects       trees 
Graph Networks Most of the current graph neural architectures perform either convolutional or recurrent operations on graphs  Duvenaud et al    developed Neural
Fingerprint for chemical compounds  where each convolution operation is   sum of neighbor node features  followed by   linear transformation  Our model differs from
theirs in that our generalized kernels and networks can aggregate neighboring features in   nonlinear way  Other approaches       Bruna et al    and Henaff et al   
rely on graph Laplacian or Fourier transform 
For recurrent architectures  Li et al    proposed gated
graph neural networks  where neighbor features are aggregated by GRU function  Dai et al    considers  
different architecture where   graph is viewed as   latent
variable graphical model  Their recurrent model is derived
from Belief Propagationlike algorithms  Our approach is
most closely related to Dai et al    in terms of neighbor feature aggregation and resulting recurrent architecture  Nonetheless  the focus of this paper is on providing
  framework for how such recurrent networks could be derived from deep graph kernels 
Kernels and Neural Nets Our work follows recent work
demonstrating the connection between neural networks and
kernels  Cho   Saul    Hazan   Jaakkola   
For example  Zhang et al    showed that standard
feedforward neural nets belong to   larger space of recursively constructed kernels  given certain activation functions  Similar results have been made for convolutional
neural nets  Anselmi et al    and general computational graphs  Daniely et al    We extend prior work
to kernels and neural architectures over structured inputs 
in particular  sequences and graphs  Another difference is
how we train the model  While some prior work appeals
to convex optimization through improper learning  Zhang
et al    Heinemann et al     since kernel space is
larger  we use the proposed networks as building blocks in
typical nonconvex but  exible neural network training 

Deriving Neural Architectures from Sequence and Graph Kernels

  Experiments
The leftover question is whether the proposed class of operations  despite its formal characteristics  leads to more
effective architecture exploration and hence improved performance  In this section  we apply the proposed sequence
and graph modules to various tasks and empirically evaluate their performance against other neural network models 
These tasks include language modeling  sentiment classi 
cation and molecule regression 

  Language Modeling on PTB
Dataset and Setup We use the Penn Tree Bank  PTB  corpus as the benchmark  The dataset contains about   million
tokens in total  We use the standard train development test
split of this dataset with vocabulary of size  
Model Con guration Following standard practice  we use
SGD with an initial learning rate of   and decrease the
learning rate by   constant factor after   certain epoch  We
backpropagate the gradient with an unroll size of   and
use dropout  Hinton et al    as the regularization  Unless otherwise speci ed  we train  layer networks with
      and normalized adaptive decay  Following  Zilly
et al    we add highway connections  Srivastava et al 
  within each layer 
                                               
         ft                ft          
where        xt     is the gated decay factor and ft is
the transformation gate of highway connections 
Results Table   compares our model with various stateof 
theart models  Our small model with   million parameters
achieves   test perplexity of   already outperforming
many results achieved using much larger network  By increasing the network size to   million  we obtain   test
perplexity of   with standard dropout  Adding variational dropout  Gal   Ghahramani    within the recurrent cells further improves the perplexity to   Finally  the model achieves   perplexity when the recurrence depth is increased to   being stateof theart and on
par with the results reported in  Zilly et al    Zoph  
Le    Note that Zilly et al    uses   neural layers and Zoph   Le   adopts   complex recurrent cell
found by reinforcement learning based search  Our network is architecturally much simpler 
Figure   analyzes several variants of our model  Wordlevel CNNs are degraded cases       that ignore noncontiguous ngram patterns  Clearly  this variant performs
worse compared to other recurrent variants with    

 See the supplementary sections for   discussion of network

variants  https arxiv org abs 

Table   Comparison with stateof theart results on PTB 
  denotes the number of parameters  Following recent
work  Press   Wolf    we share the input and output word embedding matrix  We report the test perplexity
 PPL  of each model  Lower number is better 

Model
LSTM  large   Zaremba et al   
Character CNN  Kim et al   
Variational LSTM  Gal   Ghahramani 
Variational LSTM  Gal   Ghahramani 
Pointer SentinelLSTM  Merity et al 
Variational RHN  Zilly et al   
Neural Net Search  Zoph   Le   
Kernel NN      
Kernel NN   learned as parameter 
Kernel NN  gated  
Kernel NN  gated  

  variational dropout
  variational dropout    RNN layers

PPL
 
    
    
    
    
    
    
    
    
    
    
    
    
    

CNNs

constants  

constants  trained 

adaptive    

adaptive    and   

Figure   Comparison between kernel NN variants on PTB     
   for all models  Hyperparameter search is performed for each
variant 

Moreover  the test perplexity improves from   to  
when we train the constant decay vector as part of the
model parameters  Finally  the last two variants utilize neural gates  depending on input xt only or on both input xt
and previous state      improving the performance  
points 

  Sentiment Classi cation
Dataset and Setup We evaluate our model on the sentence
classi cation task  We use the Stanford Sentiment Treebank benchmark  Socher et al    The dataset consists of   parsed English sentences annotated at both
the root       sentence  level and the phrase level using  
class  negrained labels  We use the standard split for training  development and testing  Following previous work  we
also evaluate our model on the binary classi cation variant
of this benchmark  ignoring all neutral sentences 

               Deriving Neural Architectures from Sequence and Graph Kernels

Table   Classi cation accuracy on Stanford Sentiment
Treebank  Block    recursive networks  Block II  convolutional or recurrent networks  Block III  other baseline
methods  Higher number is better 

Model
RNN  Socher et al   
RNTN  Socher et al   
DRNN  Irsoy   Cardie  
RLSTM  Tai et al   
DCNN  Kalchbrenner et al   
CNNMC  Kim  
BiLSTM  Tai et al   
LSTMN  Cheng et al   
PVEC  Le   Mikolov  
DAN  Iyyer et al   
DMN  Kumar et al   
Kernel NN       
Kernel NN  gated  

Fine Binary
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Following the recent work of DAN  Iyyer et al    and
RLSTM  Tai et al    we use the publicly available
 dimensional GloVe word vectors  Pennington et al 
  Unlike prior work which  ne tunes the word vec 
      and  xed
tors  we normalize the vectors       kwk 
them for simplicity 
Model Con guration Our best model is    layer network
with       and hidden dimension       We average
the hidden states      across    and concatenate the averaged
vectors from the   layers as the input of the  nal softmax
layer  The model is optimized with Adam  Kingma   Ba 
  and dropout probability of  
Results Table   presents the performance of our model
and other networks  We report the best results achieved
across   independent runs  Our best model obtains  
and   test accuracies on  negrained and binary tasks
respectively  Our model with only   constant decay factor
also obtains quite high accuracy  outperforming other baseline methods shown in the table 

  Molecular Graph Regression
Dataset and Setup We further evaluate our graph NN
models on the Harvard Clean Energy Project benchmark 
which has been used in Dai et al    Duvenaud et al 
  as their evaluation dataset  This dataset contains  
million candidate molecules  with each molecule labeled
with its power conversion ef ciency  PCE  value 
We follow exactly the same traintest split as Dai et al 
  and the same resampling procedure on the training
data  but not the test data  to make the algorithm put more

Table   Experiments on Harvard Clean Energy Project 
We report Root Mean Square Error RMSE  on test set  The
 rst block lists the results reported in Dai et al    for
reference  For fair comparison  we reimplemented their
best model so that all models are trained under the same
setup  Results under our setup is reported in second block 

Model  Dai et al   
Mean Predicator
Weisfeilerlehman Kernel  degree 
Weisfeilerlehman Kernel  degree 
Embedded Mean Field
Embedded Loopy BP
Under Our Setup
Neural Fingerprint
Embedded Loopy BP
Weisfeiler Kernel NN
Weisfeiler Kernel NN  gated  

RMSE
 
 
 
 
  
    
 
  
  
 

    
    
    
    

emphasis on molecules with higher PCE values  since the
data is distributed unevenly 
We use the same feature set as in Duvenaud et al    for
atoms and bonds  Initial atom features include the atoms
element  its degree  the number of attached hydrogens  its
implicit valence  and an aromaticity indicator  The bond
feature is   concatenation of bond type indicator  whether
the bond is conjugated  and whether the bond is in   ring 
Model Con guration Our model is   WeisfeilerLehman
NN  with   recurrent iterations and       All models
 including baseline  are optimized with Adam  Kingma  
Ba    with learning rate decay factor  
Results
In Table   we report the performance of our
model against other baseline methods  Neural Fingerprint  Duvenaud et al    is    layer convolutional neural network  Convolution is applied to each atom  which
sums over its neighbors  hidden state  followed by   linear transformation and nonlinear activation  Embedded
Loopy BP  Dai et al    is   recurrent architecture  with
  recurrent iterations  It maintains message vectors for each
atom and bond  and propagates those vectors in   message
passing fashion  Table   shows our model achieves stateof theart against various baselines 

  Conclusion
We proposed   class of deep recurrent neural architectures
and formally characterized its underlying computation using kernels  By linking kernel and neural operations  we
have    template  for deriving new families of neural architectures for sequences and graphs  We hope the theoretical
view of kernel neural networks can be helpful for future
model exploration 

Deriving Neural Architectures from Sequence and Graph Kernels

Acknowledgement
We thank Prof  Le Song for sharing Harvard Clean Energy Project dataset  We also thank Yu Zhang  Vikas Garg 
David Alvarez  Tianxiao Shen  Karthik Narasimhan and
the reviewers for their helpful comments  This work was
supported by the DARPA MakeIt program under contract
ARO   NF 

References
Anselmi  Fabio  Rosasco  Lorenzo  Tan  Cheston  and Poggio  Tomaso  Deep convolutional networks are hierarchical kernel machines  preprint arXiv   

Bruna  Joan  Zaremba  Wojciech  Szlam  Arthur  and LeCun  Yann  Spectral networks and locally connected networks on graphs  arXiv preprint arXiv   

Cheng  Jianpeng  Dong  Li  and Lapata  Mirella  Long
shortterm memory networks for machine reading  Proceedings of the Conference on Empirical Methods in
Natural Language Processing  pp     

Cho  Youngmin and Saul  Lawrence    Kernel methods for
deep learning  In Bengio     Schuurmans     Lafferty 
      Williams           and Culotta      eds  Advances
in Neural Information Processing Systems   pp   
   

Chung  Junyoung  Gulcehre  Caglar  Cho  KyungHyun 
and Bengio  Yoshua  Empirical evaluation of gated recurrent neural networks on sequence modeling  arXiv
preprint arXiv   

Dai  Hanjun  Dai  Bo  and Song  Le  Discriminative embeddings of latent variable models for structured data 
arXiv preprint arXiv   

Daniely  Amit  Frostig  Roy  and Singer  Yoram  Toward
deeper understanding of neural networks  The power of
initialization and   dual view on expressivity  CoRR 
abs   

Duvenaud  David    Maclaurin  Dougal 

Iparraguirre 
Jorge  Bombarell  Rafael  Hirzel  Timothy  AspuruGuzik  Al an  and Adams  Ryan    Convolutional networks on graphs for learning molecular  ngerprints  In
Advances in neural information processing systems  pp 
   

Dyer  Chris  Ballesteros  Miguel  Ling  Wang  Matthews 
Austin  and Smith  Noah    Transitionbased dependency parsing with stack long shortterm memory 
In
Proceedings of the  rd Annual Meeting of the Association for Computational Linguistics  Volume   Long Papers  Beijing  China  July  

Dyer  Chris  Kuncoro  Adhiguna  Ballesteros  Miguel  and
Smith  Noah    Recurrent neural network grammars  In
Proceedings of the   Conference of the North American Chapter of the Association for Computational Linguistics  San Diego  California  June  

Gal  Yarin and Ghahramani  Zoubin    theoretically
grounded application of dropout in recurrent neural networks 
In Advances in Neural Information Processing
Systems    NIPS   

  artner  Thomas  Flach  Peter  and Wrobel  Stefan  On
graph kernels  Hardness results and ef cient alternatives 
In Learning Theory and Kernel Machines  pp 
  Springer   

Hazan  Tamir and Jaakkola  Tommi  Steps toward deep
kernel methods from in nite neural networks  arXiv
preprint arXiv   

Heinemann  Uri  Livni  Roi  Eban  Elad  Elidan  Gal  and
Globerson  Amir  Improper deep kernels  In Proceedings of the  th International Conference on Arti cial
Intelligence and Statistics  pp     

Henaff  Mikael  Bruna  Joan  and LeCun  Yann  Deep
convolutional networks on graphstructured data  arXiv
preprint arXiv   

Hinton  Geoffrey    Srivastava  Nitish  Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan    Improving
neural networks by preventing coadaptation of feature
detectors  arXiv preprint arXiv   

Hochreiter  Sepp and Schmidhuber    urgen  Long shortterm memory  Neural computation   
 

Irsoy  Ozan and Cardie  Claire  Deep recursive neural networks for compositionality in language  In Advances in
Neural Information Processing Systems   

Iyyer  Mohit  BoydGraber  Jordan  Claudino  Leonardo 
Socher  Richard  and Daum   III  Hal    neural network for factoid question answering over paragraphs  In
Proceedings of the   Conference on Empirical Methods in Natural Language Processing  EMNLP  pp   
  Doha  Qatar  October  

Iyyer  Mohit  Manjunatha  Varun  BoydGraber  Jordan 
and Daum   III  Hal  Deep unordered composition rivals
syntactic methods for text classi cation  In Proceedings
of the  rd Annual Meeting of the Association for Computational Linguistics  Volume   Long Papers   

Kalchbrenner  Nal  Grefenstette  Edward  and Blunsom 
Phil    convolutional neural network for modelling sentences  In Proceedings of the  th Annual Meeting of the
Association for Computational Linguistics   

Deriving Neural Architectures from Sequence and Graph Kernels

Kim  Yoon  Convolutional neural networks for sentence
classi cation  In Proceedings of the Empiricial Methods
in Natural Language Processing  EMNLP    
Kim  Yoon  Jernite  Yacine  Sontag  David  and Rush 
Alexander    Characteraware neural language models  TwentyNinth AAAI Conference on Arti cial Intelligence   

Kingma  Diederik   and Ba  Jimmy Lei  Adam    method
for stochastic optimization  In International Conference
on Learning Representation   

Kumar  Ankit  Irsoy  Ozan  Ondruska  Peter  Iyyer  Mohit  James Bradbury  Ishaan Gulrajani  Zhong  Victor 
Paulus  Romain  and Socher  Richard  Ask me anything  Dynamic memory networks for natural language
processing   

Le  Quoc and Mikolov  Tomas  Distributed representations
of sentences and documents  In Proceedings of the  st
International Conference on Machine Learning  ICML 
  pp     

Lei  Tao  Joshi  Hrishikesh  Barzilay  Regina  Jaakkola 
Tommi  Tymoshenko  Katerina  Moschitti  Alessandro  and Marquez  Lluis 
Semisupervised question retrieval with gated convolutions  arXiv preprint
arXiv   

Li  Yujia  Tarlow  Daniel  Brockschmidt  Marc  and Zemel 
Richard  Gated graph sequence neural networks  arXiv
preprint arXiv   

Lodhi  Huma  Saunders  Craig  ShaweTaylor  John  Cristianini  Nello  and Watkins  Chris  Text classi cation
using string kernels  Journal of Machine Learning Research   Feb   

Merity  Stephen  Xiong  Caiming  Bradbury  James  and
Socher  Richard  Pointer sentinel mixture models  arXiv
preprint arXiv   

Pennington 

Jeffrey  Socher  Richard  and Manning 
Christopher    Glove  Global vectors for word representation  volume    

Press      and Wolf  Lior  Using the output embedarXiv preprint

ding to improve language models 
arXiv   

Ramon  Jan and   artner  Thomas  Expressivity versus ef 
ciency of graph kernels  In First international workshop
on mining graphs  trees and sequences  pp    Citeseer   

ShalevShwartz  Shai  Shamir  Ohad  and Sridharan 
Karthik  Learning kernelbased halfspaces with the  
  loss  SIAM Journal on Computing   
 

Shervashidze  Nino  Schweitzer  Pascal  Leeuwen  Erik
Jan van  Mehlhorn  Kurt  and Borgwardt  Karsten   
Weisfeilerlehman graph kernels  Journal of Machine
Learning Research   Sep   

Socher  Richard  Pennington  Jeffrey  Huang  Eric   
Ng  Andrew    and Manning  Christopher    Semisupervised recursive autoencoders for predicting sentiment distributions  In Proceedings of the Conference on
Empirical Methods in Natural Language Processing  pp 
   

Socher  Richard  Perelygin  Alex  Wu  Jean  Chuang  Jason  Manning  Christopher    Ng  Andrew    and Potts 
Christopher  Recursive deep models for semantic compositionality over   sentiment treebank  In Proceedings
of the   Conference on Empirical Methods in Natural
Language Processing  pp    October  

Srivastava  Rupesh    Greff  Klaus  and Schmidhuber 
  urgen  Training very deep networks 
In Advances in
neural information processing systems  pp   
 

Tai  Kai Sheng  Socher  Richard  and Manning  ChristoImproved semantic representations from treepher   
structured long shortterm memory networks 
In Proceedings of the  th Annual Meeting of the Association
for Computational Linguistics   

Tamar  Aviv  Levine  Sergey  Abbeel  Pieter  Wu  Yi  and
Thomas  Garrett  Value iteration networks  In Advances
in Neural Information Processing Systems  pp   
   

Vishwanathan    Vichy    Schraudolph  Nicol    Kondor 
Risi  and Borgwardt  Karsten    Graph kernels  Journal of Machine Learning Research   Apr 
 

Zaremba  Wojciech  Sutskever  Ilya  and Vinyals  Oriol 
Recurrent neural network regularization  arXiv preprint
arXiv   

Zhang  Yuchen  Lee  Jason    and Jordan  Michael     
regularized neural networks are improperly learnable in
polynomial time 
In Proceedings of the  nd International Conference on Machine Learning   

Zilly  Julian Georg  Srivastava  Rupesh Kumar  Koutn   
Jan  and Schmidhuber    urgen  Recurrent Highway Networks  arXiv preprint arXiv   

Zoph  Barret and Le  Quoc   

search with reinforcement learning 
arXiv   

Neural architecture
arXiv preprint

