Scalable MultiClass Gaussian Process Classi cation

using Expectation Propagation

Carlos VillacampaCalvo     Daniel Hern andezLobato    

Abstract

This paper describes an expectation propagation
 EP  method for multiclass classi cation with
Gaussian processes that scales well to very large
datasets 
In such   method the estimate of the
logmarginal likelihood involves   sum across
the data instances  This enables ef cient training using stochastic gradients and minibatches 
When this type of training is used  the computational cost does not depend on the number of
data instances    Furthermore  extra assumptions in the approximate inference process make
the memory cost independent of    The consequence is that the proposed EP method can be
used on datasets with millions of instances  We
compare empirically this method with alternative
approaches that approximate the required computations using variational inference  The results
show that it performs similar or even better than
these techniques  which sometimes give signi 
cantly worse predictive distributions in terms of
the test loglikelihood  Besides this  the training
process of the proposed approach also seems to
converge in   smaller number of iterations 

  Introduction
Gaussian processes  GPs  are nonparametric models that
can be used to address multiclass classi cation problems
 Rasmussen   Williams    These models become
more expressive as the number of data instances   grows 
They are also very useful to introduce prior knowledge in
the learning problem  as many properties of the model are
speci ed by   covariance function  Moreover  GPs provide an estimate of the uncertainty in the predictions made
which may be critical in some applications  Neverthe 

 Equal contribution

 Universidad Aut onoma de Madrid 
Madrid  Spain  Correspondence to  Carlos VillacampaCalvo
 carlos villacampa uam es 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

less  in spite of these advantages  GPs scale poorly to large
datasets because their training cost is       where   is
the number of instances  An additional challenge is that
exact inference in these models is generally intractable and
one has to resort to approximate methods in practice 
Traditionally  GP classi cation has received more attention
in the binary case than in the multiclass setting  Kuss  
Rasmussen    Nickisch   Rasmussen    The reason is that approximate inference is more challenging in
the multiclass case where there is one latent function per
class  To this one has to add more complicated likelihood
factors  which often have the form of softmax functions or
intractable Gaussian integrals  In spite of these dif culties 
there have been several works addressing multiclass GP
classi cation  Williams   Barber    Kim   Ghahramani    Girolami   Rogers    Chai    Riihim aki et al    Nevertheless  most of the proposed
methods do not scale well with the size of the training set 
In the literature there have been some efforts to scale up
GPs  These techniques often introduce   set of    cid   
inducing points whose location is learnt alongside with the
other model hyperparameters  The use of inducing points
in the model can be understood as an approximate GP prior
with   lowrank covariance structure  Qui noneroCandela
  Rasmussen    When inducing points are considered  the training cost can be reduced to         This
allows to address datasets with several thousands of instances  but not millions  The reason is the dif culty of estimating the model hyperparameters  which is often done
by maximizing an estimate of the logmarginal likelihood 
Because such an estimate does not involve   sum across the
data instances  one cannot rely on ef cient methods for optimization based on stochastic gradients and minibatches 
  notable exception is the work of  Hensman et al     
which uses variational inference to approximate the calculations  Such   method allows for stochastic optimization and can address datasets with millions of instances  In
this work we propose an alternative based on expectation
propagation  EP   Minka    and recent advances on
binary GP classi cation  Hern andezLobato   Hern andezLobato    The proposed approach also allows for ef 
 cient training using minibatches  This leads to   training

Scalable MultiClass Gaussian Process Classi cation using Expectation Propagation

cost that is   CM   where   is the number of classes 
An experimental comparison with the variational approach
and related methods from the literature shows that the proposed approach has bene ts both in terms of the training
speed and the accuracy of the predictive distribution 

  Scalable Multiclass Classi cation
Here we describe multiclass Gaussian process classi cation and the proposed method  Such   method uses the expectation propagation algorithm whose original description
is modi ed to be more ef cient both in terms of memory
and computational costs  For this  we consider stochastic
gradients to update the hyperparameters and an approximate likelihood that avoids onedimensional quadratures 

  Multiclass Gaussian Process Classi cation

We consider   dataset of   instances in the form of   matrix of attributes                 xN    with labels    
            yN     where yi                and       is the
total number of different classes  The task of interest is to
predict the class label of   new data instance   cid 
  typical approach in multiclass Gaussian process
 GP  classi cation is to assume the following labeling rule for yi given xi 
    xi 
for                  where each      is   nonlinear
latent function  Kim   Ghahramani   
De ne
                          xN      RN and fi  
    xi              xi     RC  The likelihood of    
     yi fi  is then  

                     RN             cid  

yi   arg maxk

product of   factors of the form 

 cid   yi xi        xi cid   

 

  yi fi   

 cid 

  cid yi

    where   

         
  

where   is the Heaviside step function  This likelihood
takes value one if   can explain the observed data and zero
otherwise  Potential classi cation errors can be easily introduced in   by considering that each     has been contaminated with Gaussian noise with variance  
   That is 
    xi         xi      
In multiclass GP classi cation   GP prior is assumed
for each function       Rasmussen   Williams   
Namely        GP       where        is some
covariance function with hyperparameters     Often these
priors are assumed to be independent  That is         
          where each        is   multivariate Gaussian distribution  The task of interest is to make inference about   and for that Bayes  rule is used          
                 where      is   normalization constant
 the marginal likelihood  which can be maximized to  nd
good hyperparameters     for                  However  because the likelihood in   is nonGaussian  evaluating     

 cid  

 

 

              xk

              xk

and        is intractable  Thus  these computations must
be approximated  Often  one computes   Gaussian approximation to         Kim   Ghahramani    This results
in   nonparametric classi er with training cost      
where   is the number of data instances 
To reduce the computational cost of the method described
  typical approach is to consider   sparse representation
for each GP  With this goal  one can introduce   datasets
of    cid    inducting points  
     
     for
with associated values  
       xk
                 Snelson   Ghahramani    Naishk the prior for
Guzman   Holden    Given each  
   
   
  in
which the conditional Gaussian distribution        
 
 
has been approximated by the factorizing distribution
  This approximation is known as the
full independent training conditional  FITC   Qui noneroCandela   Rasmussen    and it leads to   Gaussian prior pFITC      
  with   lowrank covariance matrix 
This allows for approximate inference with cost        
The inducing points   
   can be regarded as hyperparameters and can be learnt by maximizing the estimate
of the marginal likelihood     

    is approximated as         cid         
 cid   cid  
 cid  

 df
  pFITC      
 

   xi  

   xi  

        

        

   

   

    

    

 df

 

 

 

 

 

 

 

  Method Speci cation and Expectation Propagation

The formulation of the previous section is limited because
the estimate of the logmarginal likelihood log      cannot
be expressed as   sum across the data instances  This makes
infeasible the use of ef cient methods based on stochastic
optimization for  nding the model hyperparameters 
  recent work focusing on the binary case has shown that it
is possible to obtain an estimate of log      that involves  
  associated
sum across the data instances if the values  
to the inducing points are not marginalized  Hern andez 
 cid              df  where       
Lobato   Hern andezLobato    We follow that
work and consider the posterior approximation         
 cid  
             
          
 
      
and   is   Gaussian approximation to        This distribution   is obtained in three steps  First  we use on the exact
posterior the FITC approximation 

  we have de ned        cid  
 cid                df      
 cid         pFITC      df      
 cid  

        

   

           

    

    

              

 

 

 

 

 

 

 

    

 

         xi 

 

 

 

  

where we have de ned pFITC        cid  
             cid  
        cid   cid 
   cid  

 cid  
 cid   yi  xi        xi cid 

          

  and

  

 dfi  

 

 

with       xi  

 

    vk

    where

  cid yi
         xi  
           xi mk
XkXk   
xiXk    Kk

xiXk    Kk
   kk

 

 

mk

     kk
sk
      

XkXk  kk

xixi

xiXk  

 
In the previous expressions       is the        of  
Gaussian with mean   and variance   Furthermore  kk
xiXk
  
is   vector with the covariances between     xi  and  
Xk Xk is         matrix with the cross covariances beKk
xixi is the prior variance of     xi 
tween  
  practical dif culty is that the integral in   is intractable 
Although it can be evaluated using onedimensional
quadrature techniques  Hern andezLobato et al    in
this paper we follow   different approach  For that  we note
that   is simply the probability that   yi xi        xi  for
   cid  yi  given    Let   yi
        xi  The
second step consists in approximating   as follows 

      yi xi  and    

   and   nally    

  cid yi

  cid 
   cid 
where Sj    cid 

  yi             yi               yi        
        yi     yi Syi        yi     yi Syi 

    yi         cid 

     

  cid yi

  cid yi

  

 cid 

 
     yi    yi          is the       
   
of   standard Gaussian and   
syi
    sk
with myi
  de ned in   We have omitted in   the dependence on   to improve the readability 
The quality of this approximation is supported by the good
experimental results obtained in Section   When   is replaced in   we get an approximate posterior distribution
in which we can evaluate all the likelihood factors 

    mk

  and sk

     myi

    mk

    syi

   

          cid  

  

 cid 

  

  cid yi

 cid 

   yi

        

     yi

     yi

         

         yi

         yi

       myi
     

    and we have de ned   

       cid  

         yi
       mi        
XkXk   In      yi

     Vi    
where  Vyi
     
    
   
   
    and
xiXk    Kk
 kk
 si   are free parameters adjusted by EP  Because the precision matrices in   are onerank  see the supplementary material for details  we only have to store in memory       parameters for each   
    The posterior approximation   is obtained by replacing in   each exact factor
     by the corresponding approximate factor       That is 
  
           Zq  where Zq is   normalization constant that approximates the marginal likelihood
     Because all the factors involved in the computation
of   are Gaussian  and we assume independence among the
latent functions of different classes in     is   product of
  multivariate Gaussians  on per class  on   dimensions 
In EP each   
is updated until convergence as follows 
  is removed from   by computing              
 
   
First    
Because the Gaussian family is closed under the product
and division operations        is also Gaussian with parameters given by the equations in  Roweis    Then 
the KullbackLeibler divergence between   
        and
           is minimized with respect to   
        KL   
         This
where Zi   is the normalization constant of   
is done by matching the moments of   
         These
moments can be obtained from the derivatives of Zi   with
respect to the parameters of        Seeger    After updating    the new approximate factor is        Zi kq       
We update all the approximate factors at the same time  and
reconstruct   afterwards by computing the product of all the
  
  and the prior  as in  Hern andezLobato et al   
The EP approximation to the marginal likelihood is the normalization constant of    Zq  The log of its value is 

      

      

      

log Zq          prior   cid  

  

  cid yk

log  si    

 
where log  si     log Zi                         and
 prior are the natural parameters of          and the prior  respectively  and    is the lognormalizer of   multivariate
Gaussian distribution with natural parameters  
It is possible to show that if EP converges  the gradient of
log Zq       the parameters of each      is zero  Thus  the
gradient of log Zq          hyperparameter   
  of the kth
covariance function  including the inducing points  is 

 cid 

  log Zq

  
 

         

prior 

 prior
  
 

 

log Zi  

  
 

 

 

where   and  prior are the expected suf cient statistics under   and the prior  respectively  Importantly  only the direct dependency of log Zi   on   
  has to be taken into account  See  Seeger    The dependency through      
     the natural parameters of       can be ignored 

  cid 

 cid 

  

  cid yi

Scalable MultiClass Gaussian Process Classi cation using Expectation Propagation

  
           

  cid yk
    

 

 

   

           

where we have de ned   
The        of   is intractable due to the nonGaussian form
of the likelihood factors  The third and last step uses expectation propagation  EP   Minka    to get   Gaussian
approximation   to   This approximation is obtained by
  with an approximate Gaussian factor   
replacing each   
   

  
          si   exp

yi    Vyi
  kf

yi     

yi    myi

   

 cid   
 cid   

    

 

    
    Vi kf

exp

 cid 

 cid 

 

    

 

    mi  

 

 

Scalable MultiClass Gaussian Process Classi cation using Expectation Propagation

After obtaining   and  nding the model hyperparameters
by maximizing log Zq  one can get an approximate predictive distribution for the label   cid  of   new instance   cid 

    cid   cid      cid      cid   cid          df df cid   

 cid      cid   cid     df cid  has the same form as the likelihood factor

where we have de ned   cid          cid                cid    and

 

in   The resulting integral in   is again intractable 
However  it can be approximated using   onedimensional
quadrature  See the supplementary material 
Because some simpli cations occur when computing the
derivatives of log Zq       the inducing points  the total
training time of EP is         while the total memory
cost is           Snelson   

  Scalable Expectation Propagation

Traditionally  for  nding the model hyperparameters with
EP one reruns EP until convergence  using the previous
solution as the starting point  after each gradient ascent
update of the hyperparameters  The reason for this is that
  is only true if EP has converged       the approximate
factors do not change any more  This approach is particularly inef cient initially  when there are strong changes to
the model hyperparameters  and EP may require several
iterations to converge  Recently    more ef cient method
has been proposed in  Hern andezLobato   Hern andezLobato   
In that work the authors suggest to update both the approximate factors and the model hyperparameters at the same time  Because we do not wait for
EP to converge  one should ideally add to   extra terms
to get the gradient  These terms account for the mismatch
between the moments of   
        and    However  according to  Hern andezLobato   Hern andezLobato   
these extra terms can be ignored and one can simply use
  for an inner update of the hyperparameters 

      

Figure   Estimate of      on the Vehicle dataset as   function of
the training time for the proposed EP method when considering
three different schemes to update the model hyperparameters 

Figure   shows  for the Vehicle dataset from UCI repository
 Lichman    the estimate of the marginal likelihood
log Zq with respect to the training time  for   updates of
the hyperparameters  and        We compare three

methods      rerunning EP until convergence each time and
using   to update the hyperparameters  EPouter   ii 
updating at the same time the approximate factors   
  and
the hyperparameters with    EPinner approx  and  iii 
the same approach as the previous one  but using the exact
gradient for the update instead of    EPinner exact  All
approaches successfully maximize log Zq  However  the
inner updates are more ef cient as they do not wait until
EP converges  Moreover  using the approximate gradient
is faster  it is cheaper to compute  and it gives almost the
same results as the exact gradient 

  STOCHASTIC EXPECTATION PROPAGATION

  

  cid yk

 cid 

stored is their product            cid  

The memory cost of EP can be signi cantly reduced by  
technique called stochastic EP  SEP   Li et al    In
SEP all the approximate factors   
  are tied  This means
that instead of storing their individual parameters  what is
  
      consequence of this is that we no longer have direct access to
their individual parameters  This only affects the computation of the cavity distribution       which now is obtained
in an approximate way               
    where   is the total number of factors and    
  approximates each individual
factor  Thus  SEP reduces the memory costs of EP by   factor of    All the other steps are carried out as in the original
EP algorithm  including the computation of log Zq and its
gradients  Figure   shows the differences between EP and
SEP on   toy example  When SEP is used in the proposed
method  the memory cost is reduced to   CM  

Figure    top  EP approximation of   distribution over the variable   with complicated likelihood factors  but tractable prior 
 bottom  SEP approximation of the same distribution 

  TRAINING USING MINIBATCHES
Both the estimate of the logmarginal likelihood in  
and its gradient in   contain   sum across the data instances  This allows to write an EP algorithm that processes minibatches of data  as in  Hern andezLobato  
Hern andezLobato    For this  the data are split in
minibatches Mj of size    cid     where   is the number
of instances  Given   minibatch Mj  we process all the
approximate factors corresponding to that minibatch      
    
     cid yi    xi  yi    Mj  Then  we update the model
hyperparameters using   stochastic approximation of  
  log Zq

 cid 

 cid 

log Zi  

  Mj

  cid yi

  
 

   

         

prior 

 prior
  
 

   

  
 

where       Mj  We reconstruct   after each update
of the approximate factors and each update of the hyper 

 TrainingTimeinSecondslogZqEPInner Approx GradientEPInner ExactGradientEPOuterUpdateEPSEPScalable MultiClass Gaussian Process Classi cation using Expectation Propagation

 

  associated to the inducing points  

parameters  When using minibatches of data  we update
more frequently   and the hyperparameters  The consequence is that the training cost is   CM   assuming  
constant number of updates until convergence  This training scheme can handle datasets with millions of instances 
  Related Work
The likelihood used in   was  rst considered for multiclass Gaussian process classi cation in  Kim   Ghahramani    That work considers full nonparametric GP
priors  which lead to   training cost that is   CN   The
consequence is that it can only address small classi cation problems  It is  however  straight forward to replace
the nonparametric GP priors with the FITC approximate
priors pFITC      
   Qui noneroCandela   Rasmussen 
  These priors are obtained by marginalizing the lak  as
tent variables  
indicated in Section   This allows to address datasets
with   few thousand instances  This is precisely the approach followed in  NaishGuzman   Holden    to
address binary GP classi cation problems  We refer to
such an approach as the generalized FITC approximation  GFITC  Nevertheless  such an approach cannot use
stochastic optimization  The reason is that the estimate
of the logmarginal likelihood  needed for hyperparameter
estimation  does not contain   sum across the instances 
Thus  GFITC cannot scale well to very large datasets  Nevertheless  unlike the proposed approach  it can run expectation propagation over the exact likelihood factors in   In
GFITC we follow the traditional approach and run EP until
convergence before updating the hyperparameters 
Multiclass GP classi cation for potentially huge datasets
has also been considered in  Hensman et al      using
variational inference  VI  However  such an approach cannot use the likelihood in   since its logarithm is not well
de ned  note that it takes value zero for some values of fi 
As an alternative  Hensman et al      have considered
the robust likelihood of  Hern andezLobato et al   

 cid   yi  xi        xi cid     

  yi fi         cid 

  cid yi

   

 

where   is the probability of   labeling error  in that case 
yi is chosen at random from the potential class labels  In
 Hensman et al      it is suggested to set      
We now describe the VI approach in detail  Using  
and the de nitions of Section   we know that          
equality we get the bound log            
       log        
Consider now   Gaussian approximation   to        Then 

 cid                df  If we take the log and use Jensen   inlog       cid                        df

   

     log           KL             

 

where we have used Jensen   inequality and KL is the Kull 

 cid  

back Leibler divergence  If we use the  rst bound we get
log         

       log        KL           

      

  Eq    log           KL           

  

 

Eq fi log   yi fi    KL             

where          cid              df and the corresponding
 cid  
marginal over fi       xi              xi   is   fi   
          xi   mk
    Note that   fi  is Gaussian because       involves   Gaussian convolution  As in the proposed approach        is assumed to be   Gaussian factorizC  However  its mean
ing over the latent functions  
and covariance parameters        mk  
   and  Sk  
   are
found by maximizing   The parameters of   fi  are 

           

     sk

 

 mk  

 mk

     kk
 sk
      
   kk

xiXk    Kk
xixi    kk
xiXk    Kk

XkXk  
xiXk    Kk
XkXk  

XkXk  
 Sk Kk

 kk

xiXk

XkXk  

 kk

xiXk  

 

 

Hensman et al      consider Markov chain Monte
Carlo  MCMC  to sample the hyperparameters  Here we
simply maximize   to  nd the hyperparameters and the
inducing points  The reason for this is that in very large
datasets MCMC is not expected to give much better results 
We refer to the described approach as VI  The objective in
  contains   sum across the data instances  Thus  VI
also allows for stochastic optimization and it results in the
same cost as the proposed approach  However  the expectations in   must be approximated using onedimensional
quadratures  This is   drawback with respect to the proposed method which is free of any quadrature  Finally 
there are some methods related to the VI approach just described  Dezfouli   Bonilla   assume that   can be
  mixture of Gaussians  and Chai   uses   softmax
likelihood  but does not consider stochastic optimization 
Both works need to introduce extra approximations 
In the literature there are other research works addressing multiclass Gaussian process classi cation  Some examples include  Williams   Barber    Girolami  
Rogers    Hern andezLobato et al    Henao  
Winther    Riihim aki et al    These works
employ expectation propagation  variational inference or
the Laplace approximation to approximate the computations  Nevertheless  the corresponding estimate of the logmarginal likelihood cannot be expressed as   sum across
the data instances  This avoids using ef cient techniques
for optimization based on stochastic gradients  Thus  one
cannot address very large datasets with these methods 
  Experiments
We evaluate the performance of the method proposed in
Section   We consider two versions of it     rst

Scalable MultiClass Gaussian Process Classi cation using Expectation Propagation

one  using expectation propagation  EP    second  using the memory ef cient stochastic EP  SEP  EP and SEP
are compared with the methods described in Section  
Namely  GFITC and VI  All methods are codi ed in the
  language  the source code is in the supplementary material  and they consider the same initial values for the model
hyperparameters  including the inducing points  that are
chosen at random from the training instances  The hyperparameters are optimized by maximizing the estimate of
the marginal likelihood    Gaussian covariance function
with automatic relevance determination  an amplitude parameter and an additive noise parameter is employed 

  Performance on Datasets from the UCI Repository

We evaluate the performance of each method on   datasets
from the UCI repository  Lichman    The characteristics of the datasets are displayed in Table   We use batch
training in each method       we go through all the data to
compute the gradients  Batch training does not scale to
large datasets  However  it is preferred on small datasets
like the ones considered here  We use   of the data for
training and   for testing  expect for Satellite which is
fairly big  where we use   for training and   for testIn Waveform  which is synthetic  we generate  
ing 
instances and split them in   for training and   for
testing  Finally  in Vowel we consider only the points that
belong to the    rst classes  All methods are trained for
  iterations using gradient ascent  GFITC and VI use lBFGS  EP and SEP use an adaptive learning rate described
in the supplementary material  We consider three values
for    the number of inducing points  Namely    
and   of the number of training instances  We report
averages over   repetitions of the experiments 

Table   Characteristics of the datasets from the UCI Repository 

 Instances

 Attributes

 Classes

Dataset
Glass
Newthyroid
Satellite
Svmguide 
Vehicle
Vowel
Waveform
Wine

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

Table   shows  for each value of    the average negative
test loglikelihood of each method with the corresponding error bars  test errors are shown in the supplementary material  The average training time of each method
is also displayed  The best method  the lower the better  for each dataset is highlighted in bold face  We observe that the proposed approach  EP  obtains very similar results to those of GFITC  and sometimes it obtains
the best results  The memory ef cient version of EP  SEP 
seems to provide similar results without reducing the performance  Regarding the computational cost  SEP is the

fastest method  between   and   times faster than GFITC 
VI is slower as   consequence of the quadratures required
by this method  VI also gives much worse results in some
datasets       Glass  Svmguide  and Waveform  This is related to the optimization of Eq fi log   yi fi  in   instead of log Eq fi   yi fi  which is closer to the data loglikelihood 
log Zi   is
probably more similar to log Eq fi   yi fi  This explains
the much better results obtained by EP and SEP 

In the EP objective in    cid 

  cid yi

Table   Average negative test log likelihood for each method and
average training time in seconds on UCI repository datasets 

VI

EP

SEP

GFITC

Problem
                       
Glass
Newthyroid                        
                       
Satellite
Svmguide                         
                       
Vehicle
                       
Vowel
Waveform                        
                       
Wine
                       
Avg  Time
                       
Glass
Newthyroid                        
                       
Satellite
Svmguide                         
                       
Vehicle
                       
Vowel
Waveform                        
                       
Wine
                       
Avg  Time
                       
Glass
Newthyroid                        
                       
Satellite
Svmguide                         
                       
Vehicle
                       
Vowel
Waveform                        
                       
Wine
Avg  Time
                       

 
 
 
 

 
 
 
 
 

 
 
 
 
 

  Analysis of Inducing Point Learning

We generate   synthetic two dimensional problem with
three classes by sampling the latent functions from the
GP prior and applying the rule yi   arg maxk     xi 
The distribution of xi is uniform in the box      
    We consider   training instances and  
growing number of inducing points             to    
  The initial location of the inducing points is chosen at
random and it is the same for all the methods  We are interested in the location of the inducing points after training 
Thus  we set the other hyperparameters to their true values
 speci ed before generating the data  and we keep them
 xed  All methods but VI are trained using batch methods
during   iterations  VI is trained using stochastic gradients for   epochs  the batch version often gets stuck
in local optima  We use ADAM with the default settings
 Kingma   Ba    and   as the minibatch size 

Scalable MultiClass Gaussian Process Classi cation using Expectation Propagation

     

     

     

     

     

     

     

     

     

 
 
 
 
 

 
 

 
 
 

 
 

 cient than the EP updates  Similar results are obtained
in terms of the test error  See the supplementary material 
However  in that case VI does not over   the training data 

Figure   Decision boundaries and location of the inducing points after training for each method  GFITC  EP and SEP seem to place
the inducing points one on top of each other  By contrast  VI prefers to place them near the decision boundaries  Best seen in color 
Figure   shows the location learnt by each method for the
inducing points  Blue  red and green points represent the
training data  black lines are decision boundaries and black
border points are the inducing points  As we increase the
number of inducing points the methods become more accurate  However  GFITC  EP and SEP identify decision
boundaries that are better with   smaller number of inducing points  VI fails in this task  This is probably because VI
updates the inducingpoints with   bad estimate of   during the initial iterations  VI uses gradient steps to update
   which is less ef cient than the EP updates  free of any
learning rate  GFITC  EP and SEP overlap the inducing
points  which can be seen as   pruning mechanism  if two
inducing points are equal  it is like having only one  This
has already been observed in regression problems  Bauer
et al    By contrast  VI places the inducing points
near the decision boundaries  This agrees with previous
results on binary classi cation  Hensman et al     
  Performance as   Function of the Training Time

Figure   Negative test loglikelihood for GFITC  SEP and VI on
Satellite as   function of the training time  Best seen in color 

Figure   shows the negative test loglikelihood of each
method as   function of the training time on the Satellite
dataset  EP results are not shown since it performs equal
to SEP  Training is done as in Section   We consider
  growing number of inducing points           and
report averages over   repetitions of the experiments  In
all methods we use batch training  We observe that SEP
is the method with the best performance at the lowest cost 
Again  it is faster than GFITC because it optimizes   and
the hyperparameters at the same time  while GFITC waits
until EP has converged to update the hyperparameters 
VI is not very ef cient for small values of    due to the
quadratures  It also takes more time to get   good estimate
of    which is updated by gradient descent and is less ef 

  Performance When Using Stochastic Gradients

In very large datasets batch training is infeasible  and one
must use minibatches to update   and to approximate the
required gradients  We evaluate the performance of each
method on the MNIST dataset  LeCun et al    with
      inducing points and minibatches with   instances  This dataset has     instances for training and
    for testing  The learning rate of each method is
set using ADAM with the default parameters  Kingma  
Ba    GFITC does not allow for stochastic optimization  Thus  it is ignored in the comparison  The test error
and the negative test loglikelihood of each method is displayed in Figure    top  as   function of the training time 
In this larger dataset all methods perform similarly  However  EP and SEP take less time to converge than VI  SEP
obtains   test error that is   and average negative test
loglikelihood that is   The results of VI are  
and   respectively  These results are similar to the

lll                    llllll                              llllllllllll                                        llllllllllllllllllllllll                                                      lllllllllllllllllllllllllllllllllllllllllll                                                        lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          ll                    llllll                    llllllllllll                                        llllllllllllllllllllllll                                                        lllllllllllllllllllllllllllllllllllllllllllllll                                                          llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                            lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                            ll                    llllll                                        llllllllllll                                        llllllllllllllllllllllll                                                        llllllllllllllllllllllllllllllllllllllllllllllll                                                          llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                            lllllllll                    llllllllllll                              llllllllllllllllllllllll                              lllllllllllllllllllllllllllllllllllllllllllllll                                                  llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                  lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                          llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                                                           Satellite DatasetAvg  Time in seconds in log Avg  Negative Test Log LikelihoodGFITC      GFITC      GFITC      SEP      SEP      SEP      VI      VI      VI      Scalable MultiClass Gaussian Process Classi cation using Expectation Propagation

Figure   Average test error and average negative test loglikelihood for each method on the MNIST  top  and the Airline  bottom 
dataset  In the Airline dataset   linear model based on logistic regression is included in the comparison  Best seen in color 
ones reported in  Hensman et al      using      
  last experiment considers all  ights within the USA between   and    http statcomputing 
org dataexpo  The task is to classify the  ights
according to their delay using three classes  On time  more
than   minutes of delay  or more than   minutes before
time  We consider   attributes  age of the aircraft  distance covered  airtime  departure time  arrival time  day
of the week  day of the month and month  After removing all instances with missing data       instances remain  from which     are used for testing and the rest
for training  We use the same settings as on the MNIST
dataset and evaluate each method  The results obtained
are shown in Figure    bottom  We also report the performance of   logistic regression classi er  Again  all methods perform similarly in terms of test error  However  EP
and SEP converge faster and quickly outperform the linear
model  Importantly  the negative test loglikelihood of VI
starts increasing at some point  which is again probably due
to the optimization of Eq fi log   yi fi  in   The supplementary material has further evidence supporting this 
  Conclusions
We have proposed the  rst method for multiclass classi 
cation with Gaussian processes  based on expectation propagation  EP  that scales well to very large datasets  Such
  method uses the FITC approximation to reduce the number of latent variables in the model from       to      
where    cid     and   is the number of data instances  For
this    inducing points are introduced for each latent function in the model 
Importantly  the proposed method allows for stochastic optimization as the estimate of the log 

marginallikelihood involves   sum across the data  Moreover  we have also considered   stochastic version of EP
 SEP  to reduce the memory usage  When minibatches
and stochastic gradients are used for training  the computational cost of the proposed approach is   CM   with  
the number of classes  The memory cost is   CM  
We have compared the proposed method with other approaches from the literature based on variational inference
 VI   Hensman et al      and with the model considered by Kim   Ghahramani   which has been combined with FITC approximate priors  GFITC   Qui noneroCandela   Rasmussen    The proposed approach
outperforms GFITC in large datasets as this method does
not allow for stochastic optimization  and in small datasets
it produces similar results  The proposed method  SEP 
is slightly faster than VI which also allows for stochastic
optimization 
In particular  VI requires onedimensional
quadratures which in small datasets are expensive  We have
also observed that SEP converges faster than VI  This is
probably because the EP updates  free of any learning rate 
are more ef cient for  nding   good posterior approximation than the gradient ascent updates employed by VI 
An important conclusion of this work is that VI sometimes
gives bad predictive distributions in terms of the test loglikelihood  The EP and SEP methods do not seem to have
this problem  Thus  if one cares about accurate predictive
distributions  VI should be avoided in favor of the proposed
methods  In our experiments we have also observed that
the proposed approaches tend to place the inducing points
one on top of each other  which can be seen as an inducing
point pruning technique  Bauer et al    By contrast 
VI tends to place them near the decision boundaries 

 Training Time in Seconds in   Log  ScaleTest ErrorMethodsEPSEPVIMNIST Training Time in Seconds in   Log  ScaleNeg  Test Log LikelihoodMethodsEPSEPVIMNIST       Training Time in Seconds in   Log  ScaleTest ErrorMethodsEPLinearSEPVIAirline Delays       Training Time in Seconds in   Log  ScaleNeg  Test Log LikelihoodMethodsEPLinearSEPVIAirline DelaysScalable MultiClass Gaussian Process Classi cation using Expectation Propagation

Acknowledgements
The authors gratefully acknowledge the use of the facilities of Centro de Computaci on Cient ca  CCC  at
Universidad Aut onoma de Madrid 
The authors also
acknowledge  nancial support from Spanish Plan Nacional        Grants TIN    TIN 
   TIN REDT and TEC REDT
 MINECO FEDER EU  and from Comunidad de Madrid 
Grant   ICE 

References
Bauer     van der Wilk     and Rasmussen        Understanding probabilistic sparse Gaussian process approximations  In Advances in Neural Information Processing
Systems   pp     

Chai           Variational multinomial logit Gaussian process  Journal of Machine Learning Research   
   

Dezfouli     and Bonilla        Scalable inference for
Gaussian process models with blackbox likelihoods  In
Advances in Neural Information Processing Systems  
pp     

Girolami     and Rogers     Variational Bayesian multinomial probit regression with Gaussian process priors 
Neural Computation     

Henao     and Winther     Predictive active set selection
methods for Gaussian processes  Neurocomputing   
   

Hensman     Matthews     and Ghahramani     Scalable
variational Gaussian process classi cation  In Proceedings of the Eighteenth International Conference on Arti 
 cial Intelligence and Statistics  pp       

Hensman     Matthews        Filippone     and Ghahramani     MCMC for variationally sparse Gaussian processes  In Advances in Neural Information Processing
Systems   pp       

Hern andezLobato     and Hern andezLobato        Scalable Gaussian process classi cation via expectation
In Proceedings of the  th International
propagation 
Conference on Arti cial Intelligence and Statistics  pp 
   

Hern andezLobato      andez Lobato       and Dupont 
   Robust multiclass Gaussian process classi cation  In
Advances in Neural Information Processing Systems  
pp     

Kim       and Ghahramani     Bayesian Gaussian process
classi cation with the EMEP algorithm  IEEE Transactions on Pattern Analysis and Machine Intelligence   
   

Kingma        and Ba     ADAM    method for stochastic
optimization  In Inrernational Conference on Learning
Representations  pp     

Kuss     and Rasmussen        Assessing approximate inference for binary Gaussian process classi cation  Journal of Machine Learning Research     

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  Proceedings of the IEEE   
 

Li     Hern andezLobato        and Turner        Stochastic expectation propagation  In Advances in Neural Information Processing Systems   pp     

Lichman     UCI machine learning repository    URL

http archive ics uci edu ml 

Minka    

Expectation propagation for approximate
Bayesian inference  In Proceedings of the  th Annual
Conference on Uncertainty in Arti cial Intelligence  pp 
   

NaishGuzman     and Holden     The generalized FITC
approximation  In Advances in Neural Information Processing Systems   pp     

Nickisch     and Rasmussen        Approximations for binary Gaussian process classi cation  Journal of Machine
Learning Research     

Qui noneroCandela     and Rasmussen          unifying
view of sparse approximate Gaussian process regression 
Journal of Machine Learning Research   
 

Rasmussen        and Williams           Gaussian Processes for Machine Learning  Adaptive Computation
and Machine Learning  The MIT Press   

Riihim aki     Jyl anki     and Vehtari     Nested expectation propagation for Gaussian process classi cation with
Journal of Machine
  multinomial probit likelihood 
Learning Research     

Roweis     Gaussian identities  Technical report  New York

University   

Seeger     Expectation propagation for exponential families  Technical report  Department of EECS  University
of California  Berkeley   

Scalable MultiClass Gaussian Process Classi cation using Expectation Propagation

Snelson     Flexible and ef cient Gaussian process models

for machine learning  PhD thesis   

Snelson     and Ghahramani     Sparse Gaussian processes
using pseudoinputs  In Advances in Neural Information
Processing Systems   pp     

Williams           and Barber     Bayesian classi cation
IEEE Transactions on Patwith Gaussian processes 
tern Analysis and Machine Intelligence   
 

