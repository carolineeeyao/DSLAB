Contextual Decision Processes with low Bellman rank are PACLearnable

Nan Jiang   Akshay Krishnamurthy   Alekh Agarwal   John Langford   Robert    Schapire  

Abstract

This paper studies systematic exploration for reinforcement learning  RL  with rich observations
and function approximation  We introduce contextual decision processes  CDPs 
that unify
most prior RL settings  Our  rst contribution is
  complexity measure  the Bellman rank  that we
show enables tractable learning of nearoptimal
behavior in CDPs and is naturally small for many
wellstudied RL models  Our second contribution is   new RL algorithm that does systematic exploration to learn nearoptimal behavior in
CDPs with low Bellman rank  The algorithm requires   number of samples that is polynomial
in all relevant parameters but independent of the
number of unique contexts  Our approach uses
Bellman error minimization with optimistic exploration and provides new insights into ef cient
exploration for RL with function approximation 

  Introduction
In this paper  we study reinforcement learning  RL  problems where the agent receives rich sensory observations
from the environment  forms complex contexts from sensorimotor streams  uses function approximation to generalize to unseen contexts  and must perform systematic exploration to learn ef ciently  Such problems are at the core
of empirical RL research       Mnih et al    Bellemare et al    yet no existing theory provides rigorous
and satisfactory guarantees in   general setting  This situation motivates an important question  how can we solve
RL problems where exploration is critical and the agent receives rich observations  in   sampleef cient manner 

To answer the question  we propose   new formulation 
Contextual Decision Processes  CDPs  to capture   large
class of sequential decisionmaking problems  CDPs gen 

 University of Michigan  Ann Arbor  University of Massachusetts  Amherst  Microsoft Research  New York  Correspondence to  Nan Jiang  nanjiang umich edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

eralize MDPs where the state forms the context  Ex    and
POMDPs where the history forms the context  Ex    and
can be much more concise than alternative formulations
based on suf cient statistics       Hutter    We de ne
CDPs in Section   and the learning goal is to  nd   nearoptimal policy for   CDP with the help of   valuefunction
approximator in   sampleef cient manner 
  structural assumption  When the context space is very
large or in nite  as is common in practice  lower bounds
that are exponential in the problem horizon preclude ef 
cient learning in CDPs  even when simple function approximators are used  However  RL problems arising in applications are often far more benign than the pathological lower
bound instances  and we identify   structural assumption
capturing this intuition  As our  rst major contribution  we
de ne   notion of Bellman factorization  De nition   in
Section   and focus on problems with low Bellman rank 
At   high level  Bellman rank is an algebraic dimension
capturing the interplay between the CDP and the valuefunction approximator that we show is small for many
previouslystudied settings  For example  every MDP with
  tabular valuefunction has Bellman rank bounded by the
rank of its transition matrix  which is at most the number
of states but can be considerably smaller  For   POMDP
with reactive valuefunctions  the Bellman rank is at most
the number of hidden states and has no dependence on the
observation space  We provide other instances of low Bellman rank including Linear Quadratic Regulators and Predictive State Representations  Overall  CDPs with   small
Bellman rank yield   uni ed framework for   large class of
sequential decision making problems 
  new algorithm  Our second contribution is   new algorithm for episodic RL called OLIVE  Optimism Led Iterative Valuefunction Elimination  detailed in Section  
OLIVE iteratively re nes   space of candidate Qvalue
functions    At each iteration  it chooses   value function   using an optimistic criterion and collects trajectories
from the corresponding greedy policy      If    attains  
highvalue  the algorithm terminates and outputs    Other 

 Throughout the paper  by sampleef cient we mean   number
of trajectories that is polynomial in the problem horizon  number
of actions  Bellman rank  to be introduced  and polylogarithmic
in the number of candidate valuefunctions 

Contextual Decision Processes with low Bellman rank are PACLearnable

Model
Bellman rank
PAC Learning

Tabular MDP Lowrank MDP Reactive POMDP Reactive PSR

LQR

  states
known

rank
new

  hidden states

extended

PSR rank

new

  state variables

known 

Table   Summary of settings having low Bellman rank  with formal statements in Section    Proposition   to   from left to right in the
table  The  nd row gives the parameters that bound the Bellman rank  In the  rd row   known  means that sampleef cient algorithms
already exist       tabular MDPs   extended  means our results substantially extend previous work        Krishnamurthy et al   
for reactive POMDPs  and  new  means our result gives the  rst sampleef cient algorithm       MDPs with lowrank transitions 

wise  it eliminates all       which violate certain Bellman
equations under trajectories generated by    and performs
the next iteration with this re ned class of functions 
  PAC guarantee  We prove that OLIVE performs
sampleef cient
learning in CDPs with   small Bellman rank  Section  
Concretely  when the   cid 
function for the CDP is contained in    OLIVE requires            log    trajectories to  nd an  
suboptimal policy  where   is the Bellman rank    is the
length of   trajectory    is the number of actions    is the
cardinality of    and   is the failure probability 
Importantly  the sample complexity bound has   logarithmic dependence on    enabling powerful function approximation  and no direct dependence on the size of the context
space  which can be very large or in nite  As many existing models  including the ones highlighted in Table   have
low Bellman rank  the result immediately implies sampleef cient learning in all of these settings 
The main PACguarantee can be extended in several ways 
discussed in Appendix    Speci cally  OLIVE is robust to
the failure of our assumptions  can adapt to unknown Bellman rank  and can handle in nite function classes with
bounded statistical complexity  These extensions demonstrate that the Bellman rank robustly captures the dif culty
of exploration in sequentialdecision making problems 
To summarize  this work advances our understanding of RL
with complex observations where longterm planning and
exploration are critical  While OLIVE represents an exponential advance in statistical ef ciency  its computational
complexity  which is polynomial in    is intractable for
the powerful function classes of interest  This computational issue must be addressed before we can empirically
evaluate the effectiveness of the proposed algorithm  We
discuss this and other future directions in Section  

Related work  There is rich theoretical literature on RL
in tabular settings  including MDPs  Kearns   Singh   
Brafman   Tennenholtz    Strehl et al    and
POMDPs  Azizzadenesheli et al    with small state

   logarithmic dependence on   norm parameter   is omitted

here  as   is polynomial in most cases 

 Our algorithm requires discrete action spaces and does not

immediately apply to LQRs  see more discussion in Section  

and observation spaces  with an emphasis on sophisticated exploration to  nd nearoptimal policies in   sampleef cient manner  While there have been extensions to large
state spaces  Kakade et al    Jong   Stone   
Pazis   Parr    these approaches fail to be   good
   for practical scenarios where the environment is typically perceived through complex observations such as image  text  or audio signals  Alternatively  Monte Carlo Tree
Search  MCTS  methods can handle large state spaces  but
only at the cost of exponential dependence on the planning
horizon  Kearns et al    Kocsis   Szepesv ari   
Closest to our work are the results of Wen   Van Roy
  and Krishnamurthy et al    which also obtain
sample complexity independent of the number of unique
contexts  but only under deterministic dynamics and other
special structures  In contrast  we study   much broader
class of problems with relatively mild conditions 
On the empirical side  recent successes on both the Atari
platform  Mnih et al    Wang et al    and Go  Silver et al    have sparked    urry of research interest 
These approaches leverage advances in deep learning for
powerful function approximation  but typically use simple
strategies  such as  greedy  for exploration  Better exploration strategies  such as pseudocounts in Bellemare et al 
  and combining MCTS with function approximation
      Silver et al    typically require strong domain
knowledge and large amounts of data to be successful 
Hallak et al    have proposed Contextual MDPs 
where each context parametrizes an MDP  In contrast  our
use of contexts is in analogy with contextual bandits  Langford   Zhang    and is similar to state features in RL 

  Contextual Decision Processes  CDPs 
In this section  we introduce   new model  called   Contextual Decision Process  as   uni ed framework for reinforcement learning with rich observations 

  Model and Examples

CDPs make minimal assumptions to capture   general class
of RL problems and are de ned as follows 

De nition    Contextual Decision Process  CDP   

Contextual Decision Processes with low Bellman rank are PACLearnable

 nitehorizon  CDP is de ned as   tuple              
where   is the context space    is the action space  and
  is the horizon of the problem             is the system descriptor  where           is   distribution over
initial contexts  that is         and              
                      elicits the next reward and
context from the interactions so far                  xh  ah 

 rh  xh                       xh  ah 

 cid cid  

      EP

 cid cid cid         

 cid 

In   CDP  the agent interacts with the environment in
episodes 
In an episode  the agent observes   context
   takes action    receives reward    and observes   
repeating   times    policy           speci es the agent   decisionmaking strategy 
ah  
 xh           and induces   distribution over trajectories                   xH   aH   rH   xH  via the system descriptor     The value of   policy      is de ned as

    

 

   rH

like MDPs and

 
where         abbreviates for                  aH  
 xH   Throughout  the expectation is always taken over
contexts and rewards drawn according to the system descriptor     so we suppress the subscript     The goal of the
agent is to  nd   policy   that attains the largest value 
CDPs capture classical RL models 
POMDPs  with appropriately chosen contexts 
Example    MDPs with states as contexts  Consider  
 nitehorizon MDP                 where   is the
state space    is the action space    is the horizon     
    is the initial state distribution                 
is the state transition function                 
is the reward function  and an episode takes the form of
                  sH   aH   rH   We can convert the MDP
to   CDP               by letting             and
xh    sh     which allows the set of policies     
   to contain the optimal policy  Puterman    The
system descriptor is            where       
and   rh  xh                     xh  ah   
   
  rh sh  ah   sh sh  ah 
As above  the system descriptor for   model is usually obvious and we omit its speci cation in the following examples 
Turning to POMDPs  it might seem that   CDP limits the
agent   decisionmaking strategies to memoryless  or reactive  policies  as we only consider policies in        
This is not true  We clarify this issue by showing that we
can use the history as context  and the induced CDP suffers
no loss in the ability to represent optimal policies 
Example    POMDPs with histories as contexts  Consider    nitehorizon POMDP with   hidden state space   
an observation space    and an emission process Ds specifying   distribution over    We can convert the POMDP to

  CDP               by letting              and
xh                     oh  is the observed history at level   
Our next example considers   POMDP where the context
can be substantially more concise than the full history  As
will be formalized in Section   all we need is that the
context can express   good value function  which is signi cantly weaker than requiring it be   suf cient statistic
 unlike      Hutter   Therefore  it is important to separate the context in   CDP from any precise notion of state
in the process  and instead keep it as   modeling choice 
Example    POMDPs with sliding windows of observations as contexts  Sometimes partial observability can be
resolved by using   small history  for example  in Atari
games  it is common to keep track of the last   images
 Mnih et al    In this case  we can represent the problem as   CDP by letting xh    oh  oh  oh  oh 
We hope the above examples demonstrate the generality
and  exibility of the CDP framework  Finally  we introduce   regularity assumption on the rewards 
Assumption    Boundedness of rewards  We assume
that regardless of how actions are chosen  for any    

             rh     and cid  

   rh     almost surely 

  Valuebased RL and Function Approximation

  CDP makes no assumptions on the cardinality of the context space  which makes it critical to generalize across contexts  since an agent might not observe the same context
twice  Hence  we consider valuebased RL with function
approximation  That is  the agent is given   set of functions                 and uses it to approximate
an actionvalue function  or Qfunction  To avoid imposing boundaryconditions  we set    xH               For
ease of presentation  we assume that   is  nite with      
      throughout the paper  In Appendix    we allow
in nite function classes with bounded complexity 
As in typical valuebased RL  the goal is to identify      
which respects   particular set of Bellman equations and
achieves   high value with its greedy policy         
argmaxa            We next set up the appropriate extensions of Bellman equations to CDPs and the optimal value
   cid   through   series of de nitions  Unlike MDPs  these involve both the CDP and function approximator   
De nition    Average Bellman error  Given   policy    
      and   function                 the average
Bellman error of   under   at level   is de ned as

              cid    xh ah    rh      xh  ah 
 cid cid           ah        

 
 The bound of   is          More generally  we may simply
replace   with    in all the sample complexity results when the
bound is    See more discussion in Kakade   Section  

 cid 

Contextual Decision Processes with low Bellman rank are PACLearnable

The average Bellman error measures the selfconsistency
of   between its predictions at levels   and       when all
the previous actions are taken according to some policy  
We now de ne   set of Bellman equations 
De nition    Bellman equations and validity of    Given
an          triple    Bellman equation posits            
  We say       is valid if the Bellman equation on
       cid     holds for every   cid              

Note that the validity assumption only considers rollins
according to the greedy policies      which is the natural
policy class given    In MDPs  each Bellman equation can
be viewed as the linear combination of the standard Bellman optimality equations for   cid  where the coef cients
are the probabilities with which the rollin policy   visits
each state  This leads to the following consequence 
Fact      cid  is always valid  Given an MDP and   space of
functions                    if   cid       then in the
corresponding CDP with               cid  is valid 

While   cid  satis es the Bellman equations and yields the
optimal policy  cid       cid  there can be other functions
which also satisfy the equations while yielding suboptimal
policies  For instance  if              correctly predicts the
longterm reward of      then   is always valid  Since validity alone does not imply that we get   good policy  it
is natural to search for   valid value function which also
induces   highvalue policy  We formalize this goal next 
De nition    Optimal value  De ne

   cid    argmax

        is valid

       and    cid           cid   

Fact   In the setting of Fact   we have    cid      cid      
and    cid        cid  which is the optimal longterm value 

De nition   implicitly assumes that there is at least one
valid        This is weaker than the realizability assumption made in the valuebased RL literature  that   contains
  cid  of an MDP       Krishnamurthy et al     see
Facts   and   While some works only require   cid  to be
approximately captured       Antos et al    our algorithm can also be adapted to work with an approximate
notion of validity as discussed in Appendix   

  Bellman Factorization and Bellman Rank
CDPs are general models for sequential decision making 
but are there ef cient RL algorithms for them 
Unfortunately  without further assumptions 
learning in
CDPs is generally hard  since they subsume MDPs and

 We refer the readers who are not familiar with the de nition

of   cid  to standard texts  such as  Sutton   Barto   

POMDPs with arbitrarily large state observation spaces 
Formally  the sample complexity of learning CDPs in the
worstcase is        when         even when the complexity of the function class  measured by log     is small 
The result is due to Krishnamurthy et al    and is included in Appendix    for completeness 
Of course the lower bound instances are quite pathological and devoid of any structure that is often present in real
problems  To capture these realistic scenarios  we propose
  new complexity measure and restrict our attention to settings where this measure is low  As we will see  this measure is naturally small for many existing models  and  when
it is small  ef cient reinforcement learning is possible 
The complexity measure we propose is   structural characterization of the set of Bellman equations induced by the
CDP and the class    recall De nitions   and   that we
need to check to  nd valid functions  Checking validity by
enumeration is statistically intractable for large    since it
requires     samples to perform all rollins  However 
observe that the Bellman equations are structured in tabular
MDPs  the average Bellman error under any rollin policy
is   stochastic combination of the singlestate errors  and
checking the singlestate errors  which is tractable  is suf 
 cient to guarantee validity  This observation hints toward
  more general phenomenon  whenever the collection of
Bellman errors across all rollin policies can be concisely
represented  we may be able to check the validity of all
functions in   tractable way 
This intuition motivates   new complexity measure that we
call the Bellman rank  De ne the Bellman error matrices 
one for each    to be       matrices where the       cid th
entry is the Bellman error         cid    
Informally  the
Bellman rank for   CDP and   given valuefunction class
  is   uniform upper bound on the rank of these   Bellman error matrices 
De nition    Bellman factorization and Bellman rank  We
say that   CDP               and                admit Bellman factorization with Bellman rank   and norm
parameter   if there exists          RM            RM
for each         such that for any      cid              

        cid        cid     cid        cid 

 

and  cid     cid cid     cid      cid         

The exact factorization in Eq    can be relaxed to an approximate version as is discussed in Appendix    Unlike
rankbased notions in PSRs  Littman et al    and multiplicity automata  Sch utzenberger    Bellman rank
depends both on the process and the class   
In the remainder of this section we showcase the generality of Definition   by describing   number of common RL settings
that have   small Bellman rank  Throughout  we see how

Contextual Decision Processes with low Bellman rank are PACLearnable

the Bellman rank captures the processspeci   structures
that allow for ef cient exploration  Proofs of all claims in
this section are deferred to Appendix   
We start with the tabular MDP setting  and show that the
Bellman rank is at most the number of states 
Proposition    Bellman rank bounded by number of states
in MDPs  Consider the setting of Example   with the corresponding CDP  With any class    this model admits  
 
Bellman factorization with         and      

  

The MDP example is particularly simple as each coordinate
of the Mdimensional space corresponds to   state  which
is observable  Our next few examples show that this is not
necessary  and that the Bellman factorization can be based
on latent properties of the process  We next consider large
MDPs whose transition dynamics have   lowrank structure    closely related setting has been considered by Barreto et al      where the lowrank structure is exploited to speed up MDP planning  but no sampleef cient
RL algorithms were previously known for this setting 
Proposition    Bellman rank in lowrank MDPs  informally  Consider the setting of Example   with   transition matrix   having rank at most    The induced CDP
along with any                 admits   Bellman
factorization with Bellman rank   

The next example considers POMDPs with large observations spaces and reactive value functions  where the Bellman rank is at most the number of hidden states 
Proposition    Bellman rank bounded by hidden states in
reactive POMDPs  Consider the setting of Example   with
        and   sliding window of size   Given any    
           this model admits   Bellman factorization
 
with         and      

  

Propositions   and   can be proved under   uni ed model
that generalizes POMDPs by allowing the transition and
reward functions to depend on the observation  Figure  
This model captures the experimental settings considered
in stateof theart empirical RL work  where agents act in  
gridworld     is small  and receives complex and rich observations such as raw pixel images     is large  see     
Johnson et al    The model also subsumes and generalizes the setting of Krishnamurthy et al    which
requires deterministic transitions in the underlying MDP 
Next  we consider Predictive State Representations  PSRs 
which are models of partially observable systems with parameters grounded in observable quantities  Littman et al 
  Similar to the case of POMDPs  we can bound the
Bellman rank in terms of the rank of the PSR  when the
candidate value functions are reactive 

 Every POMDP has an equivalent PSR whose rank is bounded

by the number of hidden states  Singh et al   

  cid 

  cid 

     

     

 

 

 

 

Figure     uni ed model that subsumes MDPs and reactive
POMDPs and has   low Bellman rank  Gray nodes represent unobservable quantities while diamonds are actions controlled by
the agent  The dashed arrow indicates that the action is   function
only of the current observation  so value functions are reactive 

Proposition    Bellman rank in PSRs  informally  Consider   partially observable system with observation space
  and the induced CDP               with xh    oh    
If the linear dimension of the system       rank of its PSR
model  is at most    then given any                
the Bellman rank is bounded by LK 

The last example considers   class of linear control problems called Linear Quadratic Regulators  LQRs  We show
that the Bellman rank in LQRs is bounded by the dimension
of the state space  Unlike previous examples  here we crucially use structure of the quadratic value functions  which
is the form   cid  takes  Exploration in this class of problems has been previously considered by Osband   Van Roy
  Note that the algorithm to be introduced in the next
section does not directly apply to LQRs due to the continuous action space  and adaptations that exploit the structure
of the action space may be needed 
Proposition    Bellman rank in LQRs  informally  An
LQR can be viewed as an MDP with continuous state space
Rd and action space RK  where the dynamics are described by some linear equations  Given the function class
  which consists of nonstationary quadratic functions of
the state  the Bellman rank is bounded by       

  Algorithm and Main Results
In this section we present our algorithm for learning CDPs
that have   Bellman factorization with small Bellman rank 
along with the main sample complexity guarantee  To aid
presentation and help convey the main ideas  we make three
simplifying assumptions  We assume that   the agent
knows the Bellman rank   and the corresponding norm
bound    the function class   is  nite with cardinality   
and   the validity and Bellman factorization conditions
 De nitions   and   hold exactly  We relax these assumptions in Section   and Appendix   
We are interested in designing an algorithm for PAC
Learning CDPs  We say that an algorithm PAC learns  

Contextual Decision Processes with low Bellman rank are PACLearnable

CDP if given    two parameters           and access to the CDP  the algorithm outputs   policy   with
         cid       with probability at least       The
sample complexity is the number of episodes needed to
achieve such   guarantee  and is typically expressed in
terms of     and other relevant parameters  The goal
is to design an algorithm with sample complexity that
is Poly            log     log  where   is the
Bellman rank    is the number of actions  and   is the
time horizon  Importantly  the bound allows no dependence
on the number of unique contexts    

  Algorithm

Pseudocode for our algorithm  OLIVE  Optimism Led Iterative Valuefunction Elimination  is displayed in Algorithm   Theorem   describes how to set the parameters
nest  neval     and   For brevity  we introduce   shorthand
for empirical Bellman errors given   tuple             cid 

               cid                        cid        cid 

 

At   high level  the algorithm aims to eliminate functions
      that fail to satisfy the validity condition in De nition   This is done by Lines   and   inside the loop of
the algorithm  Line   uses importance weighting to get
an unbiased estimate of          ht  the average Bellman
error for function   on rollin policy    at time ht  Thus 
Line   eliminates functions that have high average Bellman error under    and hence are not valid 
The other major component of the algorithm involves
choosing the rollin policy    and level ht on which to
do the learning step  At iteration    we choose the rollin
policy    optimistically  by choosing ft that predicts the
highest value at the starting context distribution and setting
      ft  To pick ht  we compute ft   average Bellman
error on its own rollin distribution  Line   and set ht to
be any level for which this average Bellman error is high
 See Line   As we will show  these choices ensure that
substantial learning happens on each iteration  guaranteeing that the algorithm uses polynomially many episodes 
The last component is the termination criterion  The algorithm terminates if ft has small average Bellman error on
its own rollin distribution at all levels  This criteria guarantees that    is near optimal 
Computationally  the algorithm requires enumeration of the
valuefunction class  which we expect to be extremely large
or in nite in practice    computationally ef cient implementation is essential for   practical algorithm  which remains an open question  We focus on the sample ef ciency
of the algorithm in this paper 
Intuition for OLIVE  To convey intuition  it is helpful to
ignore any sampling effects by replacing all empirical esti 

mates with population values and set   to   The  rst important fact is that the algorithm never eliminates   valid
function  since the learning step in Line   only eliminates   function   if we can  nd   distribution on which
it has   large average Bellman error 
If   is valid  then
              for all      so   is never eliminated 
The second fact is that if   function   is valid  then its predicted value is exactly the value achieved by the greedy
policy      that is Vf                           This is
based on the following lemma 
Lemma    Valuefunction error decomposition  De ne
Vf                  Then                 

  cid 

Vf         

            

 

  

Therefore  since ft is chosen optimistically as the maximizer of the value prediction among the surviving functions  and since we never eliminate valid functions 
if
OLIVE terminates  it must output   policy with value    cid    
In the analysis  we incorporate sampling effects to derive
robust versions of these facts so the algorithm always outputs   policy that is at most  suboptimal 
The more challenging component is bounding the number
of iterations of the algorithm  which is critical for obtaining   polynomial sample complexity bound  This argument crucially relies on the Bellman factorization  De 
nition   which enables us to embed the distributions over
contexts for any rollin policy into   dimensions and measure progress in this lowdimensional space 
For now     some   and focus on the iterations where
ht      If we ignore sampling effects we can set       By
using the Bellman factorization to write       ft     as an
inner product  we can think of the learning step in Line  
as introducing   homogeneous linear constraint on the set
of        vectors   cid   ft        cid      Now  if we execute the learning step at   again in   later iteration   cid  we
have  cid   ft cid     ft cid cid   cid    from Line   Importantly  this
means that    ft cid  must be linearly independent from previous    ft  since  cid   ft     ft cid cid      Since every time
ht      the number of linearly independent constraints increases by   the number of iterations where ht     is at
most    the dimension of the space  Thus the Bellman
rank  times    upperbounds the number of iterations 
The above heuristic reasoning  despite relying on the brittle
notion of linear independence  can be made robust  With
sampling effects  rather than homogeneous linear equalities  the learning step for level   introduces linear inequality constraints to the        vectors  But if   cid  is   surviving function that forces us to train at    it means that
 cid     cid       cid cid  is very large  while  cid         cid cid  is very
small for all previous     vectors used in the learning

Contextual Decision Processes with low Bellman rank are PACLearnable

   nest
  

nest

   

          

            

 cid nest

Algorithm   OLIVE                Optimism Led Iterative Valuefunction Elimination
  Collect nest trajectories with actions taken in an arbitrary manner  save initial contexts      
  Estimate the predicted value for each         Vf    
         
  for               do
 
 
 
 
 
 
 
 

Choose policy ft   argmaxf Ft 
 Vf         ft 
Collect neval trajectories by following              
Estimate             ft            

   ft             then

  
Terminate and output    

           
        

    for all   and               neval 
        

if cid  

end if
Pick ht       such that    ft      ht       
Collect   trajectories where     

 cid  
    for all    cid  ht and     
Estimate          cid          ht     
 cid cid cid     
 cid cid cid cid          ht 
 cid 
ht
        
ht

Learn Ft  

    Ft   

           

        
ht
  

 cid neval

           

        

  

 cid 

     
ht

 

  

 

neval

 

 

is drawn uniformly at random 
      
ht

ht 

      
ht

      

 see Eq   

 
  end for

step  Intuitively this means that the new      cid  vector is
quite different from all of the previous ones  Our proof uses
  volumetric argument to show that this suf ces to guarantee substantial learning takes place  In more detail  we
track the volume of an enclosing ellipsoid of the surviving
       functions and show that each time we learn at level
  this volume shrinks multiplicatively  which results in an
iteration complexity that is linear in     
The optimistic choice for ft is critical for driving the
agent   exploration  With this choice  if ft is valid  then the
algorithm terminates correctly  and if ft is not valid  then
substantial progress is made  Thus the agent does not get
stuck exploring with many valid but suboptimal functions 
which could result in exponential sample complexity 

  Sample Complexity

We now turn to the main result  which guarantees that
OLIVE PAClearns Contextual Decision Processes with
polynomial sample complexity 
Theorem   For any           any CDP and function
class   that admit   Bellman factorization with parameters
  and   run OLIVE with the following parameters 

   

neval  

 
 
  
    

 

 

 cid 

log

 

      log  

nest  

 

 
  log   
   
 

 

   HM log  

 cid 

 cid 

 

   

 cid 

 

   

       

 

log

 

Then  with probability at least   OLIVE returns   policy

  that satis es          cid        recall De nition   for    cid    
and the number of episodes required is at most 

 cid         

 

  

 cid 

log    

 

 

Thus  if   CDP and function class   admit   Bellman factorization with small Bellman rank and   contains valid
functions  OLIVE is guaranteed to  nd   near optimal valid
function using only polynomially many episodes  To our
knowledge  this is the most general polynomial sample
complexity bound for RL with rich observations and function approximation  as many popular models are shown to
admit small Bellman rank  see Section   Table   The result also certi es that the notion of Bellman factorization 
which is quite general  is suf cient for ef cient exploration
and learning in sequential decision making problems 
It is worth brie   comparing this result with prior work 

  The most closely related result is the recent work
of Krishnamurthy et al    who also consider
episodic RL with in nite observation spaces and function approximation  The model studied there is   CDP
with Bellman rank    so our result applies as is to that
setting  Importantly  we eliminate the need for deterministic transitions in that work  while improving the
dependence on   and   although with worse scaling
in    We emphasize that our result applies to   much
more general class of models 

  Several works provide sample complexity bounds
for  tted value policy iteration methods       Munos
 We use     notation to suppress polylogarithmic depen 

dence on everything except   and  

Contextual Decision Processes with low Bellman rank are PACLearnable

  Antos et al    Munos   Szepesv ari
  While these results are relevant  they do not
address the exploration issue  which is our main focus  and circumvent it by impliciting assuming an exploratory policy for data collection 

  Ng   Jordan   proposed   policy search method
for POMDPs called PEGASUS  with   sample complexity that scales polynomially with the statistical
complexity of the policy class and the horizon  Despite the powerful result  the algorithm requires careful control over the random numbers that determine
the state transitions  While the assumption can hold
for certain simulated environments  the scope of applications is relatively limited 

  Since CDPs include smallstate MDPs  Kearns  
Singh    Brafman   Tennenholtz    Strehl
et al    the algorithm can be applied as is to
these problems  Unfortunately  our sample complexity is polynomially worse than the state of the
art     Mpoly    
log  bounds for PAClearning
MDPs  Dann   Brunskill    On the other hand 
the algorithm also applies to MDPs with in nite state
spaces with Bellman factorizations  which cannot be
handled by tabular approaches 

 

  Finally  Contextual Decision Processes also encompass contextual bandits  where the sample complexity
is    log      Agarwal et al    As contextual bandits have           OLIVE achieves
the optimal sample complexity in this case 

Turning brie   to lower bounds  since the CDP setting with
Bellman factorization is new  general lower bounds for the
broad class do not exist  However  we can use MDP lower
bounds for guidance on the question of optimality  since the
smallstate MDPs in Example   are   special case  While
no existing MDP lower bounds apply as is  because formulations vary  in Appendix    we adapt ideas from Auer
et al    to obtain      KH  sample complexity
lower bound for learning the MDPs in Example  
In comparison  the sample complexity in Theorem   is
worse in       and log     factors  but of course the
smallstate MDP is   signi cantly simpler special case  We
leave as future work the question of optimal sample complexity for learning CDPs with low Bellman rank 

  Extensions
The basic result presented here is quite robust and admits
many extensions  some of which we brie   describe here 
the details are deferred to Appendix   

  Handling in nite function classes with dependence
on VCdimension like quantities  This result uses  

contextvalue function class             and  
policy class           instead of   contextaction
value class as in OLIVE  with sample complexity depending on the pseudodimension of   and the Natarajan dimension of   These are standard measures for
regression and multiclass classi cation  and several
natural classes have known bounds 

  Competing with approximately valid valuefunctions
with inexact Bellman factorization  For this result  we
extend the de nition of validity and    cid    Defs    and  
to allow small but nonzero Bellman errors  and also
only require that the Bellman error matrices have  
low rank approximation with small  cid  error 

  Adapting to unknown Bellman rank  Here we run
OLIVE with choices of   growing at   doubling
schedule and show that the PACguarantee is preserved without loss in sample complexity 

  Discussion
In this paper  we presented   new model for RL with rich
observations  called Contextual Decision Processes  and
  structural property  the Bellman factorization  of these
models that enables sampleef cient learning  The uni ed
approach allows us to address several settings of practical
interest that have largely eluded RL theory to date  Our
work also elicits several further questions 

  Can we obtain   computationally ef cient algorithm
for some form of this setting  Prior related work
 for instance in contextual bandits  Dudik et al   
Agarwal et al    used supervised learning oracles for computationally ef cient approaches  Is there
  suitable oracle for this setting 

  The sample complexity depends polynomially on the
cardinality of the action space  Can we extend the results to handle large or continuous action spaces      
by incorporating concepts such as Eluder dimension
 Russo   Van Roy   

  Can we address sampleef cient RL given only   policy class rather than   value function class  Empirical
approaches often rely on policy gradients  which are
subject to local optima  Are there parallel results to
this work  without access to value functions 

Resolutions to these questions are important for further
connecting RL theory with practice 

Acknowledgements
Part of this work was completed while NJ and AK were at
Microsoft Research  NJ was partially supported by Rackham Predoctoral Fellowship in University of Michigan 

Contextual Decision Processes with low Bellman rank are PACLearnable

References
Agarwal  Alekh  Hsu  Daniel  Kale  Satyen  Langford  John  Li 
Lihong  and Schapire  Robert    Taming the monster    fast
and simple algorithm for contextual bandits  In International
Conference on Machine Learning   

Haussler  David  Sphere packing numbers for subsets of the
Boolean ncube with bounded VapnikChervonenkis dimension  Journal of Combinatorial Theory  Series     

Haussler  David and Long  Philip      generalization of Sauer  

lemma  Journal of Combinatorial Theory  Series     

Anderson  Brian      and Moore  John    Optimal Control  Lin 

ear Quadratic Methods  Courier Corporation   

Hutter  Marcus  Universal Arti cial Intelligence  Sequential De 

cisions based on Algorithmic Probability  Springer   

Antos  Andr as  Szepesv ari  Csaba  and Munos    emi  Learning nearoptimal policies with bellmanresidual minimization
based  tted policy iteration and   single sample path  Machine
Learning   

Johnson  Matthew  Hofmann  Katja  Hutton  Tim  and Bignell 
David  The Malmo Platform for arti cial intelligence experimentation  In International Joint Conference on Arti cial Intelligence   

Auer  Peter  CesaBianchi  Nicolo  Freund  Yoav  and Schapire 
Robert    The nonstochastic multiarmed bandit problem  SIAM
Journal on Computing   

Jong  Nicholas    and Stone  Peter  Modelbased exploration in
In Abstraction  Reformulation  and

continuous state spaces 
Approximation   

Azizzadenesheli  Kamyar  Lazaric  Alessandro  and Anandkumar  Animashree  Reinforcement learning of POMDPs using
spectral methods  Conference on Learning Theory   

Barreto  Andr   da Motta Salles  Pineau  Joelle  and Precup 
Doina  Policy iteration based on stochastic factorization  Journal of Arti cial Intelligence Research   

Barreto  Andre    Precup  Doina  and Pineau  Joelle  Reinforcement learning using kernelbased stochastic factorization  In
Advances in Neural Information Processing Systems   

Bellemare  Marc    Srinivasan  Sriram  Ostrovski  Georg 
Schaul  Tom  Saxton  David  and Munos  Remi  Unifying
countbased exploration and intrinsic motivation  In Advances
in Neural Information Processing Systems   

BenDavid  Shai  CesaBianchi  Nicolo  and Long  Philip   
Characterizations of learnability for classes of           
valued functions  In Conference on Learning Theory   

Bland  Robert    Goldfarb  Donald  and Todd  Michael    The

ellipsoid method    survey  Operations research   

Boots  Byron  Siddiqi  Sajid    and Gordon  Geoffrey    Closing the learningplanning loop with predictive state representations  International Journal of Robotics Research   

Brafman  Ronen    and Tennenholtz  Moshe  Rmax     general polynomial time algorithm for nearoptimal reinforcement
learning  Journal of Machine Learning Research   

Dann  Christoph and Brunskill  Emma  Sample complexity of
episodic  xedhorizon reinforcement learning  In Advances in
Neural Information Processing Systems   

Devroye  Luc  Gy or    aszl    and Lugosi    abor    Probabilistic

Theory of Pattern Recognition  SpringerVerlag   

Dudik  Miroslav  Hsu  Daniel  Kale  Satyen  Karampatziakis 
Nikos  Langford  John  Reyzin  Lev  and Zhang  Tong  Ef 
cient optimal learning for contextual bandits  In Uncertainty in
Arti cial Intelligence   

Hallak  Assaf  Di Castro  Dotan  and Mannor  Shie  Contextual

Markov decision processes  arXiv   

Haussler  David  Decision theoretic generalizations of the PAC
model for neural net and other learning applications  Information and computation   

Kakade  Sham  On the sample complexity of reinforcement learn 

ing  PhD thesis  University College London   

Kakade  Sham  Kearns  Michael  and Langford  John  Exploration in metric state spaces  In International Conference on
Machine Learning   

Kearns  Michael and Singh  Satinder  Nearoptimal reinforcement

learning in polynomial time  Machine Learning   

Kearns  Michael  Mansour  Yishay  and Ng  Andrew      sparse
sampling algorithm for nearoptimal planning in large Markov
decision processes  Machine Learning   

Kocsis  Levente and Szepesv ari  Csaba  Bandit based MonteCarlo planning  In European Conference on Machine Learning   

Krishnamurthy  Akshay  Agarwal  Alekh  and Langford  John 
In Ad 

PAC reinforcement learning with rich observations 
vances in Neural Information Processing Systems   

Langford  John and Zhang  Tong  The epochgreedy algorithm
for multiarmed bandits with side information  In Advances in
Neural Information Processing Systems   

Li  Lihong    unifying framework for computational reinforcement learning theory  PhD thesis  Rutgers  The State University of New Jersey   

Littman  Michael    Sutton  Richard    and Singh  Satinder  Predictive representations of state  In Advances in Neural Information Processing Systems   

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David  Rusu  Andrei    Veness  Joel  Bellemare  Marc    Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas    Ostrovski  Georg  Petersen  Stig  Beattie  Charles  Sadik  Amir  Antonoglou  Ioannis  King  Helen  Kumaran  Dharshan  Wierstra  Daan  Legg 
Shane  and Hassabis  Demis  Humanlevel control through
deep reinforcement learning  Nature   

Munos    emi  Error bounds for approximate policy iteration  In

International Conference on Machine Learning   

Munos    emi and Szepesv ari  Csaba  Finitetime bounds for  tted
value iteration  Journal of Machine Learning Research   

Natarajan  Balas    On learning sets and functions  Machine

Learning   

Contextual Decision Processes with low Bellman rank are PACLearnable

Ng  Andrew   and Jordan  Michael  Pegasus    policy search
method for large MDPs and POMDPs  In Uncertainty in Arti 
 cial Intelligence   

Osband  Ian and Van Roy  Benjamin  Modelbased reinforcement
learning and the eluder dimension  In Advances in Neural Information Processing Systems   

Panchenko  Dmitriy  Some extensions of an inequality of Vapnik
and Chervonenkis  Electronic Communications in Probability 
 

Pazis  Jason and Parr  Ronald  Ef cient PACoptimal exploration
in concurrent  continuous state MDPs with delayed updates  In
Conference on Arti cial Intelligence   

Pollard  David  Convergence of Stochastic Processes  Springer

Science   Business Media   

Puterman  Martin  Markov Decision Processes  Discrete Stochas 

tic Dynamic Programming  WileyInterscience   

Russo  Dan and Van Roy  Benjamin  Eluder dimension and the
sample complexity of optimistic exploration  In Advances in
Neural Information Processing Systems   

Sch utzenberger       On the de nition of   family of automata 

Information and Control   

Silver  David  Huang  Aja  Maddison  Chris    Guez  Arther 
Sifre  Laurent  van den Driessche  George  Schrittwieser 
Julian  Antonoglou  Ioannis  Penneershelvam  Veda  Lanctot  Marc  Dieleman  Sander  Grewe  Dominik  Nham  John 
Kalchbrenner  Nal  Sutskever  Ilya  Lillicrap  Timothy  Leach 
Madeleine  Kavukcuoglu  Koray  Graepel  Thore  and Hassabis  Demis  Mastering the game of Go with deep neural networks and tree search  Nature   

Singh  Satinder and Yee  Richard    An upper bound on the loss
from approximate optimalvalue functions  Machine Learning 
 

Singh  Satinder  James  Michael    and Rudary  Matthew   
Predictive state representations    new theory for modeling
In Uncertainty in Arti cial Intelligence 
dynamical systems 
 

Strehl  Alexander    Li  Lihong  Wiewiora  Eric  Langford  John 
and Littman  Michael    PAC modelfree reinforcement learning  In International Conference on Machine Learning   

Sutton  Richard   and Barto  Andrew    Reinforcement Learning 

An Introduction  MIT Press   

Todd  Michael    On minimum volume ellipsoids containing part
of   given ellipsoid  Mathematics of Operations Research 
 

Todd  Michael   and   ld        Alper  On Khachiyan   algorithm
for the computation of minimumvolume enclosing ellipsoids 
Discrete Applied Mathematics   

Wang  Ziyu  de Freitas  Nando  and Lanctot  Marc  Dueling network architectures for deep reinforcement learning  In International Conference on Machine Learning   

Wen  Zheng and Van Roy  Benjamin  Ef cient exploration and
value function generalization in deterministic systems  In Advances in Neural Information Processing Systems   

