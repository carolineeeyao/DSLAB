Kernelized Support Tensor Machines

Lifang He   ChunTa Lu   Guixiang Ma   Shen Wang   Linlin Shen   Philip    Yu     Ann    Ragin  

Abstract

In the context of supervised tensor learning  preserving the structural information and exploiting the discriminative nonlinear relationships of
tensor data are crucial for improving the performance of learning tasks  Based on tensor factorization theory and kernel methods  we propose   novel Kernelized Support Tensor Machine  KSTM  which integrates kernelized tensor factorization with maximummargin criterion  Speci cally  the kernelized factorization
technique is introduced to approximate the tensor data in kernel space such that the complex
nonlinear relationships within tensor data can be
explored  Further  dual structural preserving kernels are devised to learn the nonlinear boundary between tensor data  As   result of joint
optimization  the kernels obtained in KSTM exhibit better generalization power to discriminative analysis  The experimental results on realworld neuroimaging datasets show the superiority of KSTM over the stateof theart techniques 

  Introduction
In many realworld applications  data samples intrinsically
come in the form of twodimensional  matrices  or multidimensional arrays  tensors  In medical neuroimaging  for
instance    functional magnetic resonance imaging  fMRI 
sample is naturally   thirdorder tensor consisting of   
voxels  There has been extensive work in supervised tensor
learning  STL  recently  For example   Tao et al   
proposed   STL framework that extends the standard linear support vector machine  SVM  learning framework to
tensor patterns by constructing multilinear models  Under

 Department of Computer Science  University of Illinois at
Chicago  Chicago  IL  USA  Institute for Computer Vision  Shenzhen University  Shenzhen  China  Institute for Data Science 
Tsinghua University  Beijing  China  Department of Radiology 
Northwestern University  Chicago  IL  USA  Correspondence to 
Linlin Shen  llshen szu edu cn 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

this learning framework  several tensorbased linear models  Zhou et al    Hao et al    have been developed  These methods assume  explicitly or implicitly  that
data are linearly separable in the input space  However 
in practice this assumption is often violated and the linear
decision boundaries do not adequately separate the classes 
In order to apply kernel methods for tensor data  several
works  Signoretto et al      Zhao et al     
have been presented to convert the input tensors into vectors  or matrices  which are then used to construct kernels 
This kind of conversion  though  will destroy the structure
information of the tensor data  Moreover  the dimensionality of the resulting vector typically becomes very high 
which leads to the curse of dimensionality and small sample size problems  Lu et al    Yan et al   
Recently 
 Hao et al    He et al    Ma et al 
  employed CANDECOMP PARAFAC  CP  factorization  Kolda   Bader    on the input tensor to foster
the use of kernel methods for STL problems  However  as
indicated in  Rubinov et al    Luo et al    the
underlying structure of real data is often nonlinear  Although the CP factorization provides   good approximation
to the original tensor data  it only concerned with multilinear formulas  Thus  it is dif cult to model complex nonlinear relationships within the tensor data  Most recently 
 He et al    extended CP factorization to the nonlinear
case through the exploitation of the representer theorem 
and then used kernelized CP  KCP  factorization to facilitate kernel learning  To the best of our knowledge  there
is no existing work that tackles factorization and prediction
as   joint optimization problem over the kernel methods 
This paper focuses on developing kernelized tensor factorization with kernel maximummargin constraint  referred
as Kernelized Support Tensor Machine  KSTM  KSTM
includes two principal ingredients  First  inspired by  Signoretto et al    we introduce   general formulation
of kernelized tensor factorization in the tensor product reproducing kernel Hilbert space  namely kernelized Tucker
model  which provides   new perspective on understanding the KCP process  Second  we apply kernel trick to
embed the compact representations extracted by KCP into
the dual structurepreserving kernels  He et al    in
conjunction with   maximummargin method to solve the

Kernelized Support Tensor Machines

STL problems  By integrating KCP and classi cation as
  joint optimization problem  KSTM can bene   from label information during the factorization process such that
the extracted representations from KCP are more discriminative  Alternately  the kernels obtained in KSTM have
greater discriminating power and have potential to enhance
the classi cation performance 
To demonstrate the effectiveness of the proposed KSTM 
we conduct experiments on reallife fMRI neuroimaging
data for classi cation problems  The experimental results
show that KSTM has signi cant improvements over other
related stateof theart classi cation methods 
including
vector based  matrix unfolding based  other tensor based
kernel methods  and    convolutional neural networks 
The remainder of the paper is organized as follows  In Section   we give the notation and preliminaries  In Section  
we review our concerned problem  In Section   we present
our model and the learning algorithm  In Section   we conduct experimental analysis to justify the proposed method 
Finally  we conclude the paper in Section  

  Notation and Preliminaries
In this section we introduce some preliminary knowledge
on tensor algebra  Kolda   Bader    and tensor product reproducing kernel Hilbert space  Signoretto et al 
  together with notation  Table   lists basic symbols
that will be used throughout the paper 

  Tensor Algebra

  tensor is   multidimensional array that generalizes
matrix representation  whose dimension is called mode
or way  An Mth order tensor is represented as    
RI   IM   and its element is denoted as xi   iM  
The mmode matricization of tensor   is denoted by
       RIm    where       
  cid mIk  The inner product
of two tensors        RI IM is de ned by  cid      cid   
   xi   iM yi   iM   The outer product of
  vectors        RIm for             is an Mth

    cid IM
 cid   
order tensor and de ned elementwise by  cid         
     cid 

for all values of the indices 

       

    
  

    iM

iM

The two most commonly used factorizations are the Tucker
For   generic tensor    
model and CP model 
RI IM   its Tucker factorization is de ned as

  RM cid 

      cid 
 cid              cid 

rM  

  

gr rM   
  

           

rM

 

where             
    are factor matrices of
size Im   Rm      RR RN is called the core tensor 

 

        

Table   List of basic symbols 

Symbol De nition and description

 
 
 
 
       
vec 
 cid cid 
 
 
 cid 
 
 

each lowercase letter represents   scale
each boldface lowercase letter represents   vector
each boldface uppercase letter represents   matrix
each calligraphic letter represents   tensor  set or space
  set of integers in the range of   to   inclusively 
denotes column stacking operator
denotes inner product
denotes tensor product  outer product 
denotes Hadamard  elementwise  product
denotes KhatriRao product
denotes delta function
represents   kernel function

and cid cid  is used for shorthand  When all the factor matrices

have the same number of components  and the core tensor
is superdiagonal  Tucker model simpli es to CP model  In
general  CP model is considered to be   multilinear lowrank approximation while Tucker model is regarded as  
multilinear subspace approximation  Zhao et al     

  Tensor Product Reproducing Kernel Hilbert Space
For any             let  Hm cid cid        be   reproducing kernel Hilbert space  RKHS  of functions on   set
Xm with   reproducing kernel       Xm   Xm    
and the inner product operator  cid cid    The space    
  HM is called   tensor product RKHS of functions
on domain     where       XM   In particular  assume   is the generic tuple                       let
de ne the tensor product space formed by the linear combinations of the functions       for             as

                      cid    cid 
It holds that cid 
 cid 
  cid 

            

      
  

jM

 

  

 cid      

jm

      

 

 cid   

 

  

     

                  Hm 

 cid 

  

  cid 

 

  

     
jm

     

 

 

  

where           jM   is   multiindex     is the combiis the function            
nation coef cient  and     
   cid             

 

  Problem Formulation and Related Work
Assume we are given   collection of training samples    
 Xi  yi  
   where Xi   RI   IM is the ith input
sample and yi       is its corresponding class label 

Kernelized Support Tensor Machines

  cid 

  

As we have seen  Xi is represented in tensor form  To   
  classi er    commonly used approach is to stack Xi into
  vector  Let xi  cid  vec Xi    RI   IM   The soft margin
SVM is de ned as

min
   

 
 

wTw    

    yi wTxi     

 

where          max          and   is   or   When
      Problem   is called   SVM  and when      
  SVM      RI   IM is   vector of regression coef 
 cients        is an offset term  and   is   prespeci ed
regularization parameter 
When reshaped into vector vec Xi  however  the correlation among the tensor is ignored  It would be more reasonable to exploit the correlation information in developing  
classi er  because the correlation is useful and bene cial in
improving the classi cation performance  Intuitively  one
can consider the following formulation 

  cid   cid     cid      cid         

 

arg min

   

where     RI   IM is the tensor of regression coef cients     is   loss function and       is   penalty
function de ned on    For example 

minW  

 
 

 cid     cid     

 cid    yi

 cid cid   Xi cid      cid cid 

  cid 

  

 

 

is

this

essentially

formulation

equivHowever 
to Problem   when     vec    bealent
cause  cid     cid    vec   Tvec      wTw and
 cid   Xi cid    vec   Tvec Xi    wTxi  This implies that
the formulation   cannot directly address our concern 
To capture the multiway correlation  an alternative approach is to consider the dependency of the regression tensor    In particular  one can impose   lowrank constraint
on   to leverage the structure information within Xi  For
example   Tao et al    and  Cao et al    assumed
that                    where for            
       RIm   Kotsia et al    and  Lu et al   

assumed that      cid           cid  Several re 

searchers  Hao et al    Liu et al    however  have
pointed out that this type of models might end up with  
suboptimal or even worse solution since it is only an approximation of Problems   and  
On the other hand  some tensor based kernel methods  Signoretto et al    Zhao et al      He et al    Guo
et al    have been proposed to solve Problem   in the
dual space  which takes the form 

      

max

 

 TK    qT 

 

where   is the vector of dual variables    is the domain
of     is   vector with qi    yi  and     RN   is
  kernel matrix de ned by   kernel function  Xi Xj   
 cid Xi   Xj cid  Notice that kernel function becomes the
only domain speci   module of Problem   In this line 
it is essential to optimize kernel design and learn directly
from data  In general terms  it can be viewed as the problem
of  nding   mapping on the input data 

 

                 

arg min

 
where    is   certain criterion between   and      and
     is   speci   constraint imposed on the priors of    
It is worthwhile to note that   can be implicit or explicit
depending on the learning criterion 
In the conventional
tensor based kernel methods  Signoretto et al    Zhao
et al      He et al    Problem   is learned separately without considering any label information  While
it is usually bene cial to utilize label information during
kernel learning 
It is desirable to consider both label information and multiway correlations within tensor data  Pursuing this idea  it
is natural to study the Problem   and the Problem   together  Besides  considering   can be explicitly expressed
      Xi  we can make   transformation between   and      thus expressing            through
   Based on this idea  we present the following formulation 

  cid   cid     cid      cid                

 
Notice that as    is inherently associated with   and    
thus all three terms in Problem   will be dependent on
each other 

by    cid 

arg min

   

  Methodology
Tensor provides   natural representation for multiway
data  but there is no guarantee that it will be effective for
learning  Learning will only be successful if the regularities
that underlie data can be captured by the learning model 
Hence  how to de ne      is critical to solve Problem  
In the following  we  rst introduce   kernelized tensor factorization to de ne      Then we propose the kernelized
support tensor machine  KSTM  to solve the whole Problem   followed by the learning algorithm 

  Kernelized Tensor Factorization

It is wellknown that tensor factorization provides   means
to capture the multiway correlation of tensor data  However  it only gives   good approximation   rather than the
discriminative capacity  Besides  the standard factorization
is only concerned with multilinear formulas without considering the nonlinear relationships within the tensor  To

Kernelized Support Tensor Machines

capture the multiway and nonlinear correlations within the
tensor data  On the other hand  by sharing the kernel matrices      for different data samples Xi  it makes KSTM
possible to characterize tensor data taking into account both
common and discriminative features 
The traditional solution for SVM classi er is generally obtained in the dual domain  Vapnik    However  since
the weight tensor   and the latent factor matrices     
are inherently coupled in   it is complicated to obtain
the dual form of Problem   Inspired by the idea of primal optimizations of nonlinear SVMs  Chapelle   
the wellknown kernel trick is introduced here to implicitly
capture the nonlinear structures  Therefore  we replace  

with   functional form    cid     as follows 
   cid Xi   cid    
   cid     Problem   is transformed as follows 

   cid      

  cid 

where   is   kernel function  After replacing   by

 

  

 

min
      

    

 

 

  

  cid 
 cid Xi  cid     
  cid 
     cid Xi   cid Xj 
 cid    cid 
  cid 
   cid Xi   cid Xj      cid cid 
 cid    yi

    

 

   

 

              

 

 cid cid 

 

 

 

  

  

where        is the weight between the loss function
and the margin  and   is the relative weight between the
generative and the discriminative components 

Writing the kernel matrix  cid    such that cid ki      cid Xi   cid Xj 
and cid ki is the ith column of cid    we can rewrite Problem  
 cid cid 

  cid 
 cid Xi  cid     
  cid 
 cid cid kT
        cid cid 
 cid    yi
     cid     

              

min
      

as follows 

 

    

  

 

 

 

 

 

 

  

This can be regarded as the dual form of Problem  

overcome these issues  here we present   general formulation of kernelized tensor factorization  on making use of
the notion of tensor product RKHS  In particular  given an
Mth order tensor     RI IM   we treat   as an element of the tensor product RKHS    and assume that it
has   lowrank structure in space    such that the following
 tting criterion holds 

  cid 

 cid 
 cid xi  cid 
 cid 
 cid    cid                     cid cid 

 cid     

       
jm

 cid  

  

  

 

 

 

 

arg min
          
jm

  min
       

 

 

and      
jm

respectively 

where     RJ JM consists of the elements         
     jM          RIm Im and        RIm Jm consist
of the elements     
Eq    can be viewed as   type of kernelized Tucker factorization model  where kernel matrices      de ned on each
mode allow to capture the nonlinear relationships within
each mode and the major discriminative features between
the modes  Speci cally  when   is superdiagonal and the
size of each mode of   is the same                JM   it
reduces to the kernelized CP  KCP  factorization model in
 He et al    which can be formulated as

 cid    cid                  cid cid 

     
         
Note that   is absorbed into the matrices  Moreover  it
should be noted that although Eq    is   special case of
Eq    they have different application scenarios that are
similar to CP and Tucker models  see       Cichocki et al 
  Wang et al    Shao et al    Cao et al   

min

  Kernelized Support Tensor Machine

To solve Problem   we pursue   discriminative and nonlinear factorization machine by coupling kernelized tensor
factorization with   maximummargin classi er  For simplicity  we focus on the KCP model  We formulate the primal model of KSTM as follows 

min
        

    

 

 

 cid 

 cid   cid cid   cid 

   cid     cid 
     

   

 

  

  cid 
 cid Xi  cid     
 cid cid 
  cid 
 cid    yi
  cid     cid     cid   cid      cid 

 cid cid     cid Xi cid      cid cid 
 cid cid 

    

  

 cid 

 

 

 cid 

where   and   are parameters to control the approxi 

mate error and prediction loss respectively  and  cid Xi  cid 
 cid   
 cid  which have the same size as Xi  Recall

that the principle of KCP factorization  our KSTM is able to

         

 

 

              

 

 cid cid 
 cid 

 

 

  Learning Algorithm

Now we discuss the solution of Problem   As an example  we consider the case of    loss               
max         The objective function is nonconvex 
and solving for the global minimum is dif cult in general 
Therefore we derive an ef cient iterative algorithm to reach
the local optimum  by alternatively minimizing Problem

Kernelized Support Tensor Machines

  for each variable while  xing the other  For the sake

of simplicity  we let cid yi  cid cid kT

        in the following 

 

 

 

 

  

  

 

 cid 

 cid 

    

 cid 

  cid 

      
    

 cid Xi       

Update        Since there is no supervised information
involving      we can utilize the original CP factorization optimization technique to  nd   solution  by solving
the following linear system of equation 

  cid 
 cid     
 cid  
of the vector   we say that   sample  cid Xi is   support tensor
if yi cid yi     that is  if the loss on this sample is nonzero 

of the data Xi 
Update     Similar to  Chapelle    for   given value

   cid  
 
  and Xi    is the mmode matricization

        

where     

After reordering the samples such that the  rst Nv samples
are support tensors  the  rst order gradient with respect to
  is as follows 

     cid       cid KI cid            

  cid           

    

 

 

 

 

where   is the class label vector    is   vector of all ones
of length    and    is an       diagonal matrix with the
 rst Ns diagonal entries  number of support tensors  being
  and   others      

    

 

 

 cid INs  
 cid 

 

 

 

Setting Eq    equal to zero  we can  nd an optimal solution  
Update     
  As the kernel function is inherently coupled to     
  selecting an appropriate kernel function is  
crucial point for optimization design and performance enhancement  Following  He et al    we consider mapping the tensor data into tensor Hilbert space and then performing the CP factorization in the Hilbert space  The mapping function is de ned as 

 

  cid 

  cid 

      cid 

     

  cid 

  

  

  

  

 cid 

 

   

    

 

 

 

 cid 

In this respect  it corresponds to mapping the latent representations of tensor data into highdimensional tensor
Hilbert space that retains the multiway structure  Then the
kernel is just the standard inner product of tensors on that
space  Formally  we can derive   dual structural preserving
kernel function as follows 

 cid 

     

jq

 cid    cid 
  cid 
  cid 

  

  cid 
  cid 

  

   

 

  

  

  

  cid 

  cid 
 cid 

 

     

ip

 

 cid 

 

    
ip       

jq

  

  

 cid cid Xi   cid Xj

 cid 

 

By virtue of its derivation  we see that such   kernel function can take the multiway structure within tensor  exibility into consideration  Different kernel functions specify different hypothesis spaces or even different knowledge
embeddings of the data and thus can be viewed as capturing different notions of correlations  Here we consider both
linear and nonlinear cases as examples  By using the innear
product  the linear kernel can be directly derived as 

     
ip     

jq

      

 

 cid cid 

 

 

 cid cid Xi   cid Xj

 cid 

 

 

  

  

  

  cid 
  cid 
  cid 
     cid    cid 
 cid      
 cid 

  

  cid 

  cid 

  cid 

 

exp

For the case of Gaussian RBF kernel  it can be formulated
in the similar form

 cid 

 cid     

ip       
jq  cid 

 cid cid Xi   cid Xj

 cid 

 

 

  

  
   THij 

  

 

where   is used to choose an appropriate kernel width 
and Hij   RR   is   matrix whose element hpq
ij  

exp cid  

    cid     

ip       

jq  cid 

In general cases  the  rst order gradient with respect to
    

can be written as

 

     

 

 

 
     

 

 

    
     

 

 

   
     

 

 

      Xi       

 

 

 

 
     
    
     
   
     

 

 

with 

       TK       

 

  

  

 

     

      

 cid cid Xi   cid Xj
  cid 
Ns cid 
 cid yj   yj  
Ns cid 
 cid yj   yj 

 cid 
 cid cid Xi   cid Xj
 cid cid Xi   cid Xj
 cid 

     

 

 

  

 

 

     

 

  

 cid 

    

     

    

 
where   is   delta function indicating that sample is  
support tensor 

The partial gradient of     
the linear kernel is given by

 cid cid Xi   cid Xj

 cid 

     

 

 

      

 

  cid  

  with respect to  cid Xi   cid Xj  for
 cid    cid 

 cid      

 cid cid 

      

 

 

 

Kernelized Support Tensor Machines

 

parameters   and  

         

for       to   do

Fixing      

Algorithm   Learning KSTMs
Input  Training data    rank of factorization    regularization
Output  Model parameters      
  Initialize      
         
  repeat
 
 
 
 
 
 
 
 
 
end for
 
Fixing      
 
  until convergence

Compute kernel matrix  cid   by Eq    or Eq   

end for
Fixing      
for       to   do

        and    update   by Eq   

      and   update   by Eq   

Fixing         and    update     

  update      by Eq   

for       to   do

by Eq   

 

 

 

 

end for

 

For Gaussian RBF kernel  it is given by

 cid cid Xi   cid Xj

 cid 

     

 

 

 cid 

   

    

  HT

ij       

 

Ns cid 
 cid yi   yi 

      

 cid 

diag Hij 

 

 

 

Update     The  rst order gradient with respect to   is 

  

The overall optimization process is given in Algorithm  
Convergence Analysis  Although we have to solve Problem   in an iterative process due to nonconvexity  each
of subproblem is convex with respect to one variable  The
objective monotonically decreases in each iteration and it
has   lower bound  proof can be derived immediately based
on the result in  Tao et al    Therefore  it guarantees
that we can  nd the optimal solution of each iteration and
 nally  Algorithm   can converge to   local minimum of
the objective function in  
Computational Analysis  For brevity  we denote     
  Im  and
  Im  Assuming Im      In Algorithm   solv 
      
ing      in lines   requires        RS          
          Line   solves   with       Lines  
taking      NsM                       
solve     
              Line   solves   with     Ns 
Classi cation Rules  By solving Problem   on the
training data  we can obtain the shared kernel matrices             Given   test sample     according to the relation between CP and KCP  we have

 cid  
   Im        cid  
             RS  In line   computing  cid   requires

  Im        cid  

   cid           cid   cid                  cid 

Therefore  we  rst compute CP factorization on the test

 

Figure       Visualization of an fMRI image from six angles     
An illustration of thirdorder tensor of an fMRI image 

sample     and then use  cid    cid           cid  as its KCP

     for        
factorization  where            
    Upon solution  we can classify the test sample   by
using the test kernel matrix 

  Experiments and Results
In order to empirically evaluate the effectiveness of the proposed KSTM  we conduct extensive experiments on reallife neuroimaging  fMRI  data for disease prediction and
compare with several stateof theart methods  In the following  we introduce the datasets and baselines used and
describe how we performed the experiments  Then we
present the experimental results as well as the analysis 

  Data Collection and Preprocessing

In the experiments  we consider three restingstate fMRI
datasets as follows 
Alzheimer   Disease  ADNI  This dataset is collected from
the Alzheimer   Disease Neuroimaging Initiative  It contains the restingstate fMRI images of   subjects  including patients with Mild Cognitive Impairment  MCI  or
Alzheimer   Disease  AD  and normal controls  We applied SPM    and REST  to preprocess the data  After
this  we averaged each individual image over time domain 
resulting in   samples of size           We treat
AD MCI as the negative class  and the normal controls as
positive class  Finally  we scaled each individual to    
as the normalization is very important for group analyses
among different subjects    detailed description of preprocessing is available in  He et al   
Human Immunode ciency Virus Infection  HIV  This
dataset is collected from Chicago Early HIV Infection
Study in Northwestern University  Wang et al   

 http adni loni usc edu 
 http www   ion ucl ac uk spm software spm 
 http restingfmri sourceforge net

 mode mode     Kernelized Support Tensor Machines

SVM   SVM PCA

Table   Summary of compared methods    is tradeoff parameter    is kernel width parameter    is the rank of tensor factorization 
KSTM
Methods
Type of Input Data
Tensor
Correlation Exploited
Kernel Explored
Nonlinear Factorization
Parameters

Unsupervised Unsupervised Unsupervised
Unexplored

Multiway Multiway
Supervised
Supervised
Explored
        

Unsupervised Unsupervised
Unexplored

Vectors
Oneway
Supervised

  rd
Vectors
Oneway

Explored
      

Matrices
Oneway

Matrices
Oneway

   CNN
Tensor

 
Many 

DuSK
Tensor

Unexplored

      

MMK
Tensor

Multiway

Multiway

Multiway

Unexplored

    

STuM
Tensor

 

sKL

FK

    

      

 
    

 
    

HIV

ADNI

ADHD

Table   Classi cation accuracy comparison  mean   standard
deviation 
Methods
SVM
SVM PCA
  rd
sKL
FK
STuM
DuSK
MMKbest
MMKcov
   CNN
KSTM

     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     

which contains   fMRI brain images of patients with early
HIV infection  negative  and normal controls  positive 
We used the same preprocessing steps as in ADNI dataset 
resulting in   samples of size          
Attention De cit Hyperactivity Disorder  ADHD  This
dataset is collected from ADHD  global competition
dataset  which originally contains   subjects  either
ADHD patients  negative  or normal controls  positive 
Since the dataset is unbalanced  we randomly sampled  
ADHD patients and   normal controls for this study  Finally  we averaged each individual over time domain  resulting in   samples of size          

  Baselines and Metrics

To establish   comparative study  we use the following nine
stateof theart methods as baselines  each representing  
different strategy 

  SVM  It is SVM with RBF kernel  which is the most
widely used vector method for classi cation  In the
following methods  we use it as the classi er  if not
stated explicitly 

  SVMPCA  It is   vectorbased subspace learning
algorithm  which  rst uses PCA to reduce the input dimension and then feeds into SVM model 
This method is commonly used to deal with highdimensional classi cation  in particular fMRI classi 
 cation  Song et al    Xie et al   

    rd  It is   vector based tensor kernel method  which
 http neurobureau projects nitrc org ADHD 

exploits the input tensor along each mode to capture
structural information and has been used to analyze
fMRI data together with RBF kernel  Park   

  sKL  It is   matrix unfolding based tensor kernel
method that de ned based on the symmetric KullbackLeibler divergence  and has been used to reconstruct
   movement  Zhao et al     

  FK  It is also   matrix unfolding based tensor kernel
method  but de ned based on multilinear SVD  The
constituent kernels are from the class of RBF kernels
 Signoretto et al   

  STuM  It is   support tucker machine approach  where
the weight tensor parameter is decomposed using the
Tucker factorization  Kotsia   Patras   

  DuSK  It is   tensor kernel method based upon CP
factorization  which has been used to analyze fMRI
data together with RBF kernel  He et al   

  MMK  It is the most recent tensor kernel method
based upon KCP factorization  He et al    which
incorporates the KCP into the DuSK  Since the shared
kernel matrices involved in KCP are hard to estimate
without   prior knowledge  we consider two schemes 
 rst  we randomly generate them and select the best
result of   repeated times  denoted as MMKbest 
Second  we perform CP factorization for each data
sample and use the covariance matrix of each mode
as input  denoted as MMKcov 

   DCNN  It is      convolutional neural network extended from    version  Gupta et al    which
uses the convolution kernel  The convolution kernel is
the cubic  lters learned from data  which has   small
receptive  eld  but extends through the full depth of
the input volume 

Table   summarizes the compared methods  We consider
eleven methods in total and evaluate their classi cation performance  For the evaluation where SVM is needed  we
apply LibSVM  Chang   Lin      widely used implementation of SVM  with RBF kernel as the classi er 
We perform  fold crossvalidation and use classi cation
accuracy as the evaluation measure  This process was repeated   times for all methods and the average classi 
cation accuracy of each method is reported as the result 
The optimal parameters for all methods are determined by
grid search  The optimal tradeoff parameter is selected

Kernelized Support Tensor Machines

    ADNI

    HIV

    ADHD

Figure   Test accuracy vs    on     ADNI      HIV  and     ADHD  where the red triangles indicate the peak positions 

from             the kernel width parameter is selected from               the optimal
rank   is selected from         and the regularized
factorization parameter is selected from        
      Note that in the proposed method         The
optimal parameters for  DCNN       receptive  eld    
zeropadding      the input volume dimensions  Width  
Height   Depth  or             and stride length     are
tuned based on  Gupta et al   

  Classi cation Performance

Experimental results in Table   shows classi cation performance of compared methods  where the best result is highlighted in bold type  We can see that the proposed KSTM
outperforms all the other methods on three datasets  This
is mainly because KSTM can learn the nonlinear relationships embedded within the tensor together with considering prior knowledge across different data samples  while
the other methods fail to fully explore the nonlinear relationships or prior knowledge in the tensor object 
Speci cally  the kernel methods which unfold the input
data into vectors or matrices tend to have   lower performance compared to those who preserve the tensor structure  This indicates that unfolding tensor into vectors or
matrices would lose the multiway structural information
within tensor  leading to the degraded performance  Besides  KSTM always performs better than DuSK  which
empirically shows the effectiveness of feature extraction
in tensor data rather than approximation  Compared to
MMK method  we can see that   further improvement can
be achieved by bene ting from the use of label information
during the factorization procedure  These results demonstrate the effectiveness and considerable advantages of the
proposed KSTM method for fMRI classi cation 

  Parameter Sensitivity

Although the optimal values of the parameters in our proposed KSTM are found by grid search  it is still important
to see the sensitivity of KSTM to the rank of factorization

   To this end  we vary             while the
other parameters are still selected by grid search  Figure  
shows the variation of test accuracy over different   on
three datasets  We can observe that the rank parameter  
has   signi cant effect on the test accuracy and the optimal
value of   depends on the data  while in general the optimal value of   lies in the range           which may
provide   good guidance for selection of the   in advance 
In summary  the classi cation performance of KSTM relies on parameter    which is dif cult to specify the optimal value in advance  However  in most cases the optimal value of   lies in   small range of values and it is not
timeconsuming to  nd it using the grid search strategy in
practical applications 

  Conclusion
In this paper  we have introduced   Kernelized Support
Tensor Machine  KSTM  with an application to neuroimaging classi cation  Different from conventional kernel methods  KSTM is based on the integration of kernelized tensor factorization with kernel maximummargin
classi er  Typically this is done by de ning   joint optimization problem  so that the kernels obtained in KSTM
have   greater discriminating power  The kernelized tensor
factorization is introduced to capture the complex nonlinear relationships within tensor data  by means of the notion of tensor product RKHS  which supplies   new perspective on tensor factorization methods  Empirical studies on three different neurological disorder prediction tasks
demonstrated the superiority of KSTM over existing stateof theart tensor classi cation methods 

Acknowledgements
This work is supported in part by NSFC through grants
    and   NSF through grants
IIS  and CNS  NIH through grant   
MH  and the Science Foundation of Shenzhen
through grant JCYJ 

 RANK   Accuracy RANK   Accuracy Rank   AccuracyKernelized Support Tensor Machines

References
Cao  Bokai  He  Lifang  Kong  Xiangnan  Yu  Philip   
Hao  Zhifeng  and Ragin  Ann    Tensorbased multiview feature selection with applications to brain diseases  In ICDM  pp    IEEE   

Cao  Bokai  He  Lifang  Wei  Xiaokai  Xing  Mengqi  Yu 
Philip    Klumpp  Heide  and Leow  Alex   
tbne 
Tensorbased brain network embedding  In SDM   

Chang  ChihChung and Lin  ChihJen  Libsvm    library
for support vector machines  ACM Transactions on Intelligent Systems and Technology     

Chapelle  Olivier  Training   support vector machine in the

primal  Neural computation     

Cichocki  Andrzej  Mandic  Danilo  De Lathauwer  Lieven 
Zhou  Guoxu  Zhao  Qibin  Caiafa  Cesar  and Phan 
Huy Anh  Tensor decompositions for signal processing applications  From twoway to multiway component
analysis  IEEE Signal Processing Magazine   
   

Liu  Xiaolan  Guo  Tengjiao  He  Lifang  and Yang  Xiaowei    lowrank approximationbased transductive
support tensor machine for semisupervised classi caIEEE Transactions on Image Processing   
tion 
   

Lu  ChunTa  He  Lifang  Shao  Weixiang  Cao  Bokai 
and Yu  Philip    Multilinear factorization machines for
multitask multiview learning  In WSDM  pp   
ACM   

Lu  Haiping  Plataniotis  Konstantinos    and Venetsanopoulos  Anastasios    Mpca  Multilinear principal
IEEE Transaccomponent analysis of tensor objects 
tions on Neural Networks     

Luo  Dijun  Nie  Feiping  Huang  Heng  and Ding  Chris   
Cauchy graph embedding  In ICML  pp     

Ma  Guixiang  He  Lifang  Lu  ChunTa  Yu  Philip   
Shen  Linlin  and Ragin  Ann    Spatiotemporal tensor analysis for wholebrain fmri classi cation  In SDM 
pp    SIAM   

Guo  Tengjiao  Han  Le  He  Lifang  and Yang  Xiaowei 
  gabased feature selection and parameter optimization
for linear support higherorder tensor machine  Neurocomputing     

Park  Sung Won  Multifactor analysis for fmri brain image
classi cation by subject and motor task  Electrical and
computer engineering technical report  Carnegie Mellon
University   

Gupta  Ashish  Ayhan  Murat  and Maida  Anthony  NatIn

ural image bases to represent neuroimaging data 
ICML  pp     

Hao  Zhifeng  He  Lifang  Chen  Bingqian  and Yang  Xiaowei    linear support higherorder tensor machine for
classi cation  IEEE Transactions on Image Processing 
   

He  Lifang  Kong  Xiangnan  Yu  Philip    Yang  Xiaowei  Ragin  Ann    and Hao  Zhifeng  Dusk    dual
structurepreserving kernel for supervised tensor learning with applications to neuroimages  In SDM  pp   
   

He  Lifang  Lu  ChunTa  Ding  Hao  Wang  Shen  Shen 
Linlin  Yu  Philip    and Ragin  Ann    Multiway
multilevel kernel modeling for neuroimaging classi cation  In CVPR   

Kolda  Tamara   and Bader  Brett    Tensor decompositions and applications  SIAM review   
 

Kotsia  Irene and Patras  Ioannis  Support tucker machines 

In CVPR  pp    IEEE   

Kotsia  Irene  Guo  Weiwei  and Patras  Ioannis  Higher
rank support tensor machines for visual recognition  Pattern Recognition     

Rubinov  Mikail  Knock  Stuart    Stam  Cornelis   
Micheloyannis  Si    Harris  Anthony WF  Williams 
Leanne    and Breakspear  Michael  Smallworld properties of nonlinear brain activity in schizophrenia  Human brain mapping     

Shao  Weixiang  He  Lifang  and Yu  Philip    Clustering
on multisource incomplete data via tensor modeling and
factorization  In PAKDD  pp    Springer   

Signoretto  Marco  De Lathauwer  Lieven  and Suykens 
Johan AK    kernelbased framework to tensorial data
analysis  Neural networks     

Signoretto  Marco  Olivetti  Emanuele  De Lathauwer 
Lieven  and Suykens  Johan AK  Classi cation of mulIEEE
tichannel signals with cumulantbased kernels 
Transactions on Signal Processing   
 

Signoretto  Marco  De Lathauwer  Lieven  and Suykens 
Johan AK 
Learning tensors in reproducing kernel
hilbert spaces with multilinear spectral penalties  arXiv
preprint arXiv   

Song  Sutao  Zhan  Zhichao  Long  Zhiying  Zhang  Jiacai  and Yao  Li  Comparative study of svm methods
combined with voxel selection for object category classi cation on fmri data  PloS one       

Kernelized Support Tensor Machines

Tao  Dacheng  Li  Xuelong  Wu  Xindong  Hu  Weiming 
and Maybank  Stephen    Supervised tensor learning 
Knowledge and Information Systems     

Vapnik  Vladimir  The nature of statistical learning theory 

Springer science   business media   

Wang  Senzhang  He  Lifang  Stenneth  Leon  Yu  Philip   
and Li  Zhoujun  Citywide traf   congestion estimation
with social media  In GIS  pp    ACM   

Wang  Xue  Foryt  Paul  Ochs  Renee  Chung  JaeHoon 
Wu  Ying  Parrish  Todd  and Ragin  Ann    Abnormalities in restingstate functional connectivity in early
human immunode ciency virus infection  Brain connectivity     

Yan  Shuicheng  Xu  Dong  Yang  Qiang  Zhang  Lei  Tang 
Xiaoou  and Zhang  HongJiang  Multilinear discrimiIEEE Transactions
nant analysis for face recognition 
on Image Processing     

Zhao  Qibin  Zhou  Guoxu  Adal    ulay  Zhang  Liqing 
and Cichocki  Andrzej  Kernelbased tensor partial
least squares for reconstruction of limb movements  In
ICASSP  pp    IEEE     

Zhao  Qibin  Zhou  Guoxu  Adali  Tulay  Zhang  Liqing 
and Cichocki  Andrzej  Kernelization of tensorbased
models for multiway data analysis  Processing of multidimensional structured data  IEEE Signal Processing
Magazine       

Xie  Songyun  Guo  Rong  Li  Ningfei  Wang  Ge  and
Zhao  Haitao  Brain fmri processing and classi cation
In IJCNN  pp 
based on combination of pca and svm 
  IEEE   

Zhou  Hua  Li  Lexin  and Zhu  Hongtu  Tensor regression
with applications in neuroimaging data analysis  Journal
of the American Statistical Association   
   

