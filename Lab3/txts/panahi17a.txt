Clustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence

and Cluster Recovery

Ashkan Panahi   Devdatt Dubhashi   Fredrik    Johansson   Chiranjib Bhattacharyya  

Abstract

Standard clustering methods such as   means 
Gaussian mixture models  and hierarchical clustering  are beset by local minima  which are
sometimes drastically suboptimal  Moreover the
number of clusters   must be known in advance 
The recently introduced sum of norms  SON 
or Clusterpath convex relaxation of kmeans and
hierarchical clustering shrinks cluster centroids
toward one another and ensure   unique global
minimizer  We give   scalable stochastic incremental algorithm based on proximal iterations to
solve the SON problem with convergence guarantees  We also show that the algorithm recovers clusters under quite general conditions which
have   similar form to the unifying proximity
condition introduced in the approximation algorithms community  that covers paradigm cases
such as Gaussian mixtures and planted partition
models  We give experimental results to con rm
that our algorithm scales much better than previous methods while producing clusters of comparable quality 

  Introduction
Clustering is perhaps the most fundamental problem in unsupervised learning  Many clustering algorithms have been
proposed in the literature  Jain et al    including Kmeans  spectral clustering  Gaussian mixture models and
hierarchical clustering  to solve problems with respect to
  wide range of cluster shapes  However  much research
has pointed out that these methods all suffer from instabilities  For example  the formulation of Kmeans is NPhard
and the typical way to solve it is the Lloyds method  which

 ECE  North Carolina State University  Raleigh  NC  CSE 
Chalmers University of Technology    oteborg  Sweden  IMES 
MIT  Cambridge  MA  CSA  IISc  Bangalore  India  Correspondence to  Ashkan Panahi  panahi gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

requires randomly initializing the clusters  However  one
needs to know the number of clusters in advance and different initializations may lead to signi cantly different  nal
cluster results 
Lindsten et al    and Hocking et al    proposed
the following convex optimization procedure for clustering 
called SON  Sum of norms  clustering  by the former and
Clusterpath by the latter 

 cid xi   ui cid 

     

 cid ui   uj cid 

 

  

   

  cid 

min

 ui Rm 

 
 

 cid 

The main idea of the formulation is that if input data points
xi and xj belong to the same cluster  then their corresponding centroids ui and uj should be forced to be the same 
Intuitively  this is due to the fact that the second term is  
regularization term that enforces zeroes in the vector consisting of entries  cid ui   uj cid  and can be seen as   generalization of the fused Lasso penalty  From another point of
view  the regularization term can be seen as an  cid  norm 
     the sum of  cid  norms  Such   group norm is known to
encourage block sparse solutions  Bach et al    Thus
for many pairs        we expect to enforce ui   uj 
Lindsten et al    used an off the shelf convex solver 
CVX to generate solution paths  Hocking et al    introduced three distinct algorithms for the three most commonly encountered norms  For the  cid  norm  the objective function separates  and they solve the convex clustering problem by the exact path following method designed
for the fused lasso  For the  cid  and  cid  norms  they employ
subgradient descent in conjunction with active sets  Recently  Chi   Lange   Chen et al    introduce
two similar generic frameworks for minimizing the convex
clustering objective function with an arbitrary norm  One
approach solves the problem by the alternating direction
method of multipliers  ADMM  while the other solves it
by the alternating minimization algorithm  AMA  However both algorithms have issues with scalablity 
Moreover  none of these papers provide any theoretical
guarantees about the cluster recovery property of the algorithm  The  rst theoretical result on cluster recovery was
shown by Zhu et al    if the samples are drawn from

Clustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence and Cluster Recovery

two cubes  each being one cluster  then SON can provably
recover the clusters provided that the distance between the
two cubes is larger than   threshold which depends  linearly  on the size of the cube and the ratio of numbers of
samples in each cluster  Unfortunately  the conditions for
recovery represent an extremely narrow special case  only
two clusters which both have to be cubes  Moreover in
their paper  there is no algorithm or analysis of the speed of
convergence  No other theoretical guarantees for SON are
known previously 
Here we develop   new algorithm in the spirit of recent
advances in stochastic methods for large scale optimization
 Bottou et al    to solve the optimization problem  
We give   convergence analysis and provide quite general
cluster recovery guarantees 
There has been    urry of advances  Johnson   Zhang 
  Defazio et al    Schmidt et al    in developing algorithms for solving optimization problems for the
case when the objective consists of the sum of two convex functions  one is the average of   large number of
smooth component functions  and the other is   general
convex function that admits   proximal mapping  and the
whole objective function is strongly convex  The optimization   is of this form but here we exploit the structure
of   further by observing that the second function can also
be split into component functions  This results in an incremental algorithm with proximal iterations consisting of
very simple and natural steps  Our algorithm can be seen
as   special case of the methods of Bertsekas   We
compute the proximal operator in closed form to yield very
simple and cheap iterations  Using the fact that the proximal operator is nonexpansive  we re ne and strengthen
Bertsekas  convergence results  The stochastic incremental
nature of our algorithm makes it highly suited to large scale
problems  Bottou et al    in contrast to the methods in
Chi   Lange   Chen et al   
We show that the SON formualation   provides strong
cluster recovery properties that go far beyond the special
case considered in Zhu et al    Our cluster recovery
conditions are similar in spirit to the unifying general conditions recently formulated in    Kumar     Awasthi
  of the form that the means of the clusters are well 
separated       the distance between the means of any two
clusters is at least     standard deviations  the notion of
standard deviations is based on the spectral norm of the matrix whose rows represent the difference between   point
and the mean of the cluster to which it belongs  Besides
containing the result of Zhu et al    as   special case 
the condition essentially recovers the well known cluster
recovery conditions for paradigm examples such as mixtures of Gaussians and planted partition models  The algorithms in    Kumar     Awasthi   are based on

an SVDbased initialization followed by applying Lloyd  
  means algorithm  so   must be known in advance  Our
method does not need to know   and is independent of any
initialization 
  summary of our contributions are 

  We develop   new incremental proximal algorithm for

the SON optimization problem  

  We give   convergence analysis for our algorithm
that re nes and strengthens the analysis in Bertsekas
 

  We show that the SON formulation   provides strong
cluster recovery guarantees that is far more general
than previously known recovery results  essentially
similar to the recently discovered unifying center separation conditions 

  We give experimental results giving evidence that our
algorithm produces clusters of comparable quality to
previous methods but scales much better to large scale
problems 

  Related Work
The SON formulation  rst appeared in  Lindsten et al 
  and in closely related forms in Hocking et al   Hocking et al    Lindsten et al  Lindsten et al    used
an off the shelf convex solver  CVX to generate solution
paths  Hocking et al 
 Hocking et al    introduced
three distinct algorithms for the three most commonly encountered norms  For the  cid  norm  the objective function
separates  and they solve the convex clustering problem
by the exact path following method designed for the fused
lasso  For the  cid  and  cid  norms  they employ subgradient
descent in conjunction with active sets  Neither provides
any theoretical results on cluster recovery  Chi et al  Chi
  Lange    Chen et al    introduce two similar
generic frameworks for minimizing the convex clustering
objective function with an arbitrary norm  One approach
solves the problem by the alternating direction method of
multipliers  ADMM  while the other solves it by the alternating minimization algorithm  AMA  The  rst  and only 
theoretical results on cluster recovery are in  Zhu et al 
  but this is   very simple special case of exactly two
cube shaped clusters that are well separated  This work
also does not develop   specialized algorithm for the SON
formulation 

  Cluster Recovery
To express our results  we  rst review few de nitions 
De nition   Take    nite set                    xn   
Rm and its partitioning                    VK  where each

Clustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence and Cluster Recovery

Vk is   subset of    We say that   map   on   perfectly
recovers   when  xi     xj  is equivalent to xi  xj
belonging to the same cluster  or in other words  there exist
distinct vectors               vK such that  xi       holds
whenever xi     
De nition   For any set     Rm  its diameter is de ned
as

       sup cid       cid             

Moreover  for any  nite set     Rm we de ne its separation as

        min cid       cid                 cid    

and its Euclidean centroid as

       

 cid 

 
   
    

Finally  for any family of mutually disjoint  nite sets    
 Ti   Rm  we de ne            Ti 
De nition   Take    nite set                    xn   
Rm and its partitioning                    VK  We call  
partitioning                    WL  of     coarsening
of   if each partition Wl is obtained by taking the union of
  number of partitions Vk  Further    is called the trivial
coarsening of   if   has exactly one element          
    Otherwise  it is called   nontrivial coarsening 

Based on the above de nitions  our result can be explained
as follows 
Theorem   Consider    nite set      xi   Rm  
             of vectors and its partitioning    
               VK  Take the SON optimization in   Denote its optimal solution by  ui  and de ne the map    
xi    xi     ui 

  If

                 
 
  
then the map   perfectly recovers   

max
    

     

 

 

  If

max
    

     

           max
    

 cid             cid 

          

 

then the map   perfectly recovers   nontrivial coarsening of   

Proof  We introduce associated centroid optimization 

  cid 

  

min

   Rm 

 
 

 cid        cid 

       

 cid 

 cid 

    cid        cid 

 

 

min
 cid 

where

    

 cid 

xi

    
  

We prove the following results  which clearly imply Theorem  
  Suppose that for every        
 cid xi   xj cid 
  

max
      

   

Then  ui      for        is   global solution of the
SON clustering 

  If all     are distinct and

 cid        cid  then all centroids    are distinct 

  

 

 
 

    where    

  If max

 cid     cid 
    

    where    
two centroids    are distinct 

 

xi    then at least

  cid 

  

To prove the above  notice that the solution of the centroid
optimization satis es

           

    

 cid 

 

where  cid   cid               and whenever     cid    
the relation          
holds  Now  for the solution
 cid     cid 
ui      for        de ne

 cid    

xi xj
   

  cid 
ij  

   cid   
     

 

where               It is easy to see that  cid   cid 
ij      cid 
ji and whenever ui  cid  uj  we have that   cid 
  cid 
ui uj
 cid 
  Further for each   
 cid ui uj cid 

xi   xj

 cid 

 cid 

ij cid     
ij  

 

  cid 
       

 

 

      

    

  

            xi        xi        xi   ui

This shows that the local optimality conditions for the SON
optimization holds and proves item   
For item    denote the solution of the centroid optimization
by    and notice that the solution of SON consists of
distinct elements         and is continuous at      
Hence      remain distinct in an interval        
Take   as the supremum of all possible     Hence  the
solution in         contains distinct element and at
      contains two equal elements  otherwise  one can
extend     to some       which is against   being
supremum  Now  notice that for         the objective

Clustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence and Cluster Recovery

function is smooth at the optimal point  Hence     is
differentiable and satis es

 cid dv 

 cid 

  

 

   

       
 

 

where     and     denote block vectors and block matrices respectively  Moreover    and   are the Hessian and
the gradient of the objective function at the optimal point  It
 
is possible  by explicitly expanding   and    to show that
 cid cid     
   see the supplementary material for more
detailed derivations 
Hence 

 cid 

  

 cid cid cid cid cid cid 

This yields for       to

 cid        cid   

 cid cid cid cid dv 

  

   cid cid   

 cid cid cid cid 
 cid cid cid cid cid cid          
 cid 
 cid cid cid cid dv 
 cid 

 

 

  
 

         

 

 

Kn

 cid dv 

  

  dv 
  

 cid cid cid cid 

  dv 
  

  

   cid        cid   

 

 

      or        

Since at       we have that         for some    cid    we
get that        
   this proves
item   
For item    Take   value of   where                   vK 
It is simple to see that in this case         The optimality
condition leads to

          

    

 cid 

 cid 

Hence   cid       cid            This proves item   

Remark   The study in Zhu et al    establishes some
results for the special case of two clusters in rectangular
boxes  In this special case  we observe that our result improves theirs 

Proof  Consider the notation in Zhu et al    with two
clusters       and notice that         denotes regularization parameter in Zhu et al    Moreover 
  Vi     cid si cid  as  cid si cid  is the diameter of the rectangle surrounding Vi  We observe that

 cid  ni 

 cid 

  
 

      ni

    

  

 

    Vi 

   

    Vi 
ni

for         which shows that the condition       
  in
Zhu et al    is tighter than     max          in
ours  On the other hand 

 cid   Vi        cid 

    ni

 

 

 cid   Vi                

    

    ni
 cid            cid 
 
      

 cid 

 

 

 

 

  ni

in Zhu et al    is the

 
Hence  the condition           
same as our condition      cid   Vi     cid 
Remark   The second result in Theorem   re ects   hierarchical structure in the SON clusters  Under weaker condition than the  rst part  SON may merge some clusters and
provide larger clusters than the true ones  In   recursive
way  SON clustering can be applied to each of these large
clusters to re ne them  which improves the guarantees in
Theorem   We postpone careful study of this method to
future work 

  Comparison with Center Separation Conditions

Recently  there have been   number of theoretical results of
the form that if we have data points generated by   mixture of   probability distributions  then one can cluster
the data points into the   clusters  one corresponding to
each component  provided the means of the different components are well separated  There are different notions of
wellseparated  but mainly  the  best known  results can be
qualitatively stated as   If the means of every pair of densities are at least poly    standard deviations apart  then we
can learn the mixture in polynomial time  These results
generally make heavy use of the generative model and particular properties of the distributions  Indeed  many of them
specialize to Gaussians or independent Bernoulli trials 
Kumar and Kannan     Kumar    and Awasthi and
Sheffet    Awasthi    uni ed these into   general deterministic condition which can be roughly stated as follows   If the means of every pair of clusters are at least
    times standard deviations apart  then we can learn
the mixture in polynomial time  Here the spectral norm of
the matrix       scaled by  
  plays the role of standard
deviation  where   is the data matrix and   is the matrix of
cluster centers  More formally  for any two distinct clusters
   
 cid            cid     

 cid   

 cid       cid   

 cid 

 

  
Our condition is similar in spirit 
 

 cid            cid   

 
  

 cid   

  

 

    

 cid 

 

Clustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence and Cluster Recovery

If      wn for all clusters   then this becomes

 cid            cid   

    

 

 

 
 

In the sequel  we specialize the above discussion in   number of examples and provide an explicit comparison of our
result with the center separation condition  In some cases 
our condition is slightly tighter than the center separation
guarantees  but we remind that the latter is obtained by
applying Kmeans and   SVDbased initialization  which
can be intractable in large problems  while our techniques
scales with the problem size more suitably 

  MIXTURES OF GAUSSIANS

Suppose we have   mixture of   Gaussians in   dimensions with mixture weights      wK  let     mini wi
and let        denote their means respectively  If we
have      poly      points sampled from this mixture
distribution  then with high probability  the center separation condition is satis ed if 

 cid        cid    cK 
 

polylog     

Here   is the maximum variance in any direction of any of
the Gaussians  Our cluster recovery condition   is satis 
 ed if 

 cid        cid    cK 
 

polylog   

  PLANTED PARTITION MODEL

In the planted partition model of McSherry    set of   points
is implicitly partitioned into   groups  There is an  unknown        matrix of probabilities     We are given  
graph   on these   points  where an edge between two
vertices from groups   and   is present with probability
Pr    We can consider these   points      xn   Rn
where coordinate   in xi is   if            and   otherwise 
The center    of cluster   has in coordinate   the value
Pr    where     is the cluster vertex   belongs to  Kumar and Kannan show that the center separation condition
holds with probability at least       if 

 cid        cid        

 
 

  log

 
 

 

where   is   large constant    is such that every group has
size at least       and     maxr   Pr    Our center separation condition   is satis ed if 

 cid        cid     

 

 

  

 

  REGULAR AND DIRECTED CLUSTERS

Besides the stochastic models  we take   closer look at the
result in    Kumar   and identify deterministic cases
where the SON has better performance than the proved
bounds for Kmeans  These cases essentially guarantee that
the term  cid     cid  in   remains large and the bound therein
becomes highly restrictive 
De nition   We
say
               VK 
of  
   expanded if

partition  
               xn 

that
 

 
is

 

          cid          cid            

We further say that this partition is           regular if
for all       we have                   wn and it is
     expanded 
De nition   We say that   set                    xn  is
 directed if there exists   unit vector     Rm such that

 cid 

        

 vT          
 cid         cid 

 

     

For             regular partition  the bound in   implies
          cK  

mwn   This is because

 

 

  

 cid    cid 

 cid 

Tr

 

    
 

 cid 

    
 

 cid    cid 

  
 

 

 cid       cid     max

  cid 

  

 

  

 cid   cid 

 

 

       
 

 

where      xi        for        Notice that our conditions can be implied by           nD
  Hence SON
can improve Kmeans if     wKc  which means that
the number of clusters   is large and the smallest fraction
of cluster size   is  
If the           regular partition is further  directed
we may improve the previous bounds as

wn

 

 cid    cid 

  

 cid 

   cid 

   

 max

    
 

 vT            

 
Hence   implies           cK  
 
SON improves Kmeans if wK    
of clusters is higher than    xed value 

wn

  

  This means that
          the number

  Stochastic Splitting Algorithm
Our implementation is identical to the socalled proximalbased incremental technique in Bertsekas   which is

Clustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence and Cluster Recovery

 cid 

performed in   way that it requires little amount of calculations  precisely      and independent of other parameters  in each iteration  The proximalbased incremental
method is   variant of the stochastic gradient technique  in
problems where many terms in the objective function are
not differentiable  and the local gradient steps are replaced
by local proximal operators  To perform the proximalbased incremental method  we  rst write the SON objective
function as

               un   

 ij ui  uj 

where

   

 
  

 ij ui  uj   

 cid xi ui cid 
   

   cid ui uj cid 
Then  we introduce and explicitly calculate the proximal
operator  

ij of  ij with step size   as

 cid xj uj cid 

 
  

arg min
  cid 
    cid 
 

 ij   cid 

 
      

ij  ui  uj   
 cid   cid 

    ui cid 

     cid 
     
    ui    xi  uj    xj 

 cid   cid 

    uj cid 

 

 

 cid 

where we also introduce the pairwise softthresholding operator          
         
 cid     cid 
       

 cid   cid       cid     

 cid   cid 
 cid     

           
 cid     cid 

 cid       cid     

 
and the  nal equality is obtained by the local optimality
conditions and straightforward calculations  Our algorithm
simply consists in iteratively applying randomly selected
proximal operators  This is depicted in Algorithm  

 

 

Algorithm   Stochastic Splitting Algorithm

  

Input  The data vectors  xk  
   
Initialization  Set               un arbitrarily  we use
                  un    
for               do

   and step sizes

Select   pair        with       uniformly randomly 
Update  ui  uj       

 ui  uj 

ij

end for

  Convergence Analysis

Convergence of proximalbased incremental method is
discussed in Bertsekas   We further elaborate
on the convergence by further exploitation of the nonexpansiveness property of proximal operators  This allows
us to complement the result in Bertsekas   in the following two directions  First  we establish convergence in

the probability sense  uniform convergence  while the result in Bertsekas   is pointwise  Second  we prove
guaranteed speed of convergence with probability one  We
present these results by the following theorem  In this theorem  we consider  xed data dimension   and bounded data
vectors        cid xk cid      for some absolute constant   
Theorem  

 cid 

 cid 

 cid 

  Assume that     is nonincreasing

       and
      Then  the sequence Uk converges to   
 

 

 
in the following strong probability sense 

     

lim
   Pr

sup
   

 cid Ul      cid 

     

     

 cid 

  Take       

   for               and  

          For

suf ciently small values of       the relation

 cid    

 cid 

 cid Ul      cid 

  
holds for every      with probability  

     

Proof  We skip many steps in our proof for lack of space 
These steps can be found in the supplement  Denote by Uk
  matrix where the ith column is the value of ui at the kth
iteration  De ne

         Uk    Uk              

 

Starting from           the initialization of the algorithm  we de ne the characteristic sequence    Uk 
   by
the following iteration 

 Uk          Uk 

Our proof is based on the following two results  which we
prove in the supplementary material 

  We have that

 cid 

 cid Uk    Uk cid 

   

Pr

sup

 

 cid 

 

     

 

 cid 

   

 cid 

  
 

 
 

 

ii De ne    as the unique optimal solution of the SON
optimization and suppose that     is   nonincreasing
sequence  There exists   universal constant   such that
 cid   Uk      cid 

  is upper bounded by

  cid 

    

   
  

 
   

  

   cid         cid 
Fe

   
  

  cid 

  

  

  cid 

  

 

Clustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence and Cluster Recovery

To prove Theorem   de ne          Uk
quence obtained by starting from  Uk

   as the se 
    Uk and applying

   

 Uk

             Uk
   

 cid 

Take arbitrary  nonzero  positive numbers     Take  
such that      
    Take some values      which we
 
specialize later  Now  we de ne two outcomes    and   

  

 

             cid Uk      cid 
           cid   Uk

     
    Ul   cid     
 
From item     it is simple to see that Pr    
  and Pr    
 
are less than   Furthermore we can show by  ii  that
under         and suitable      
         cid Ul       cid 

       cid 
  

   cid   Uk

   cid 

     cid Ul     Uk
   

     

 

 
 

 
 

This is detailed in the supplement  We conclude that

Pr  sup

     

 cid Ul      cid 

        Pr    

    Pr    

     

Figure   Running times of the exact SON clustering algorithm
 implemented in CVX  and stochastic splitting for samples from
  mixture of two Gaussians with increasing sample size 

where we introduce     bn  and      an    for simplicity  This leads to
 cid Ukr       cid 
    LeLk 
lr cid 

    cid Ukr      cid 

   Lk 

  

 

     kr  eL kr   Lk 

  

      

  

which proves part   of Theorem 
For part   de ne kr              where        
   
              and the outcomes 
   cid 
 cid Ul kr    Ukr

       

 

Qr   sup
  

Pr Qr      Hence by
By item     we have that
BorelCantelli lemma  Qc
         simultaner  Qc
  
ously hold for some    with probability   For simplicity
and without loss of generality  we assume that        as it
does not affect the asymptotic rate  Then for any       we
have that

  
  Qc

 cid Ul kr    Ukr

   cid 

      

 cid 

sup
  

In particular 

 cid Ukr     Ukr

lr

 cid 
      

where lr   kr    kr  From item  ii  we also conclude
that

 cid   Ukr

lr

     cid 

     

lr cid 

    

  

 

   kr  

lr cid 

  

 

     kr   
lr cid 

  

  

 cid Ukr      cid 
Fe

 

   kr  

 cid Ukr      cid 

where   is   suitable constant with different values at different occurrences  Postponing few more steps to the supplementary material  we obtain that
      log  
    
Take kr       kr  We obtain that
     cid Ukr      cid 
      

     cid Ukr   Ul cid 
 
      

 cid Ul      cid 

   
  

      

 
 

 

 

 

By taking               we obtain part  

  Experiments
We evaluate the proposed stochastic splitting algorithm in
the task of clustering points generated by Gaussians mixture models  We compare the results to the exact algorithm
proposed by Lindsten et al    in terms of    the quality of the produced clustering and    the time spent solving
the optimization problem  The results of both algorithms
are dense embeddings of the points that are then thresholded to form clusters  The clusters are the largest subsets
of nodes such that the maximum pairwise distance within
the subset is less than   The stochastic splitting algorithm
is implemented as in Algorithm   We observed in practice

       Exact Stochastic SplittingClustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence and Cluster Recovery

         

         

         

         

Figure   Adjusted Rand index for different choices of   Each plot represents quality of the clustering produced by solving the SON
objective exactly or with stochastic splitting  The different plots represent clustering of   samples from   mixture of two Gaussians in
   with  xed separation    

 
  and variance  

 

that   heuristic for adaptively setting the stepsize improved
robustness and rate of convergence  Speci cally  the step
size was reduced by   constant factor whenever the average
change in the objective over successive rounds in   small
window was positive  If the same average was negative 
but small in absolute value  the step size was increased by
  small constant factor 
The data is generated from Gaussian mixture models with
two components in    where the means are separated by
  and the variance   is varied  The number of samd  
ples is also varied  to illustrate the computational gains of
the stochastic splitting method  As pointed out by Lindsten
et al    the choice of the regularization parameter  
is perhaps the most challenging hurdle in applying SON
clustering  Choosing   too high might result in   single
large cluster  and choosing it too low may cause each point
to be represented by its own cluster  While this problem
is of great importance in applications  we focus on the relative performance of the Lindsten et al    algorithm
 CVX  and stochastic splitting  SS  We report the adjusted
Rand index  Rand    as measure of cluster quality  and
would like to emphasize that this does not rely on identifying the number of clusters beforehand 

Results The results of the experiments are presented in
Figures   and   We see in Figure   that the quality of the
clustering produced by the stochastic splitting algorithm is
comparable to that of the exact algorithm  This pattern is
consistant across choices of   where   high   implies low
sample separation between the clusters  We also note that
the range of   for which the stochastic splitting algorithm
achieves as good result as the exact algorithm is less wide
than for the exact  We believe this is due to the stochastic
nature of the algorithm which makes the resulting embedding clusters less separated than in the exact version  Deviations from the optimal embedding could be magni ed
by the thresholding step  effectively making the stochastic
algorithm more sensitive to the choice of threshold  and in
effect the quality more sensitive to the choice of   In these

experiments  the same threshold was used for both algorithms  but tailored choices could be considered given an
appropriate selection criterion 
Furthermore  we see in Figure   that the running time of
the stochastic splitting algorithm is lower than that of the
exact algorithm  and grows signi cantly slower  While the
stochastic splitting algorithm could in principle be implemented in time constant in the number of samples  and instead determined by the number of iterations  the adaptive
stepsize used to improve performance requires evaluation
of the objective value which scales with the number of samples  This could be improved by subsampling the terms in
the objective function  but this was not done here 

  Conclusions
We developed   stochastic incremental algorithm based on
proximal iterations for the SON convex relaxtion of clustering that is highly suited to large scale problems and gave
an analysis of its convergence propertis  We also gave quite
general theoretical guaranteees for exact recovery of clusters similar to the unifying proximity condition in approximation algorithms that covers paradigm models for clustering data 
It has not escaped our attention that our algorithm can easily be adapted to incorporate similarity weights as used in
Chi   Lange   Chen et al    Hocking et al 
  and that it is amenable to acceleration using variance reduction and other techniques  The cluster recovery
conditions can also be extended to cover almost perfect recovery      correctly clustering all except   small fraction
of points    more complete experimental evaluation of our
algorithm and comparison to others will be included in  
longer version of the paper 

Acknowledgements
This work is supported in part by the Swedish Foundation
for Strategic Research  SSF 

 AdjustedRand  Exact Stochastic Splitting AdjustedRand  Exact Stochastic Splitting AdjustedRand  Exact Stochastic Splitting AdjustedRand  Exact Stochastic SplittingClustering by Sum of Norms  Stochastic Incremental Algorithm  Convergence and Cluster Recovery

References
   Kumar     Kannan  Clustering with spectral norm and the   

means algorithm  In FOCS   

Bach     Jenatton     Mairal     and Obozinski     Optimization with sparsityinducing penalties  Foundation and Trends
in Machine Learning     

Bertsekas     Incremental proximal methods for large scale con 

vex optimization  Math  Program     

Bottou    eon  Curtis  Frank    and Nocedal  Jorge  Optimization methods for largescale machine learning  Technical report  arXiv  June   URL http leon 
bottou org papers troptml 

Chen  Gary    Chi  Eric    Ranola  John      and Lange 
Kenneth  Convex clustering  An attractive alternative to
hierarchical clustering  PLoS Computational Biology   
     doi   journal pcbi  URL
http journals plos org ploscompbiol 
article id journal pcbi 

Chi  Eric    and Lange  Kenneth  Splitting methods for conJournal of Computational and Graphical
vex clustering 
Statistics     
doi   
  URL http dx doi org 
 

Defazio        Bach     and LacosteJulien       fast incremental
gradient method with support for nonstrongly convex composite objectives  In NIPS  pp     

Hocking     Vert  JP  Bach     and Joulin     Clusterpath 
an algorithm for clustering using convex fusion penalties  In
ICML   

Jain        Murty        and Flynn        Data clustering   
review  ACM Comput  Surv    September  
ISSN   doi    URL http 
 doi acm org 

Johnson     and Zhang     Accelerating stochastic gradient descent using predictive variance reduction  In NIPS  pp   
   

Lindsten     Ohlsson     and Ljung     Clustering using sumof 
norms regularization  With application to particle  lter output
computation   

  Awasthi    Sheffet  Improved spectral norm bounds for cluster 

ing  In APPROXRANDOM   

Rand  William    Objective criteria for the evaluation of clustering methods  Journal of the American Statistical association 
   

Schmidt     Roux     Le  and Bach     Minimizing  nite sums
with the stochastic average gradient  Mathematical Programming   

Zhu     Xu     Leng     and Yan     Convex optimization pro 

cedure for clustering  Theoretical revisit  In NIPS   

