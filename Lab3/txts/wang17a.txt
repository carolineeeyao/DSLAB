Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

YuXiang Wang   Alekh Agarwal   Miroslav Dud    

Abstract

We study the offpolicy evaluation problem 
estimating the value of   target policy using data
collected by another policy under the contextual
bandit model  We consider the general  agnostic 
setting without access to   consistent model of rewards and establish   minimax lower bound on the
mean squared error  MSE  The bound is matched
up to constants by the inverse propensity scoring
 IPS  and doubly robust  DR  estimators  This
highlights the dif culty of the agnostic contextual
setting  in contrast with multiarmed bandits and
contextual bandits with access to   consistent reward model  where IPS is suboptimal  We then
propose the SWITCH estimator  which can use an
existing reward model  not necessarily consistent 
to achieve   better biasvariance tradeoff than IPS
and DR  We prove an upper bound on its MSE
and demonstrate its bene ts empirically on   diverse collection of data sets  often outperforming
prior work by orders of magnitude 

  Introduction
Contextual bandits refer to   learning setting where the
learner repeatedly observes   context  takes an action and
observes   reward for the chosen action in the observed
context  but no feedback on any other action  An example is
movie recommendation  where the context describes   user 
actions are candidate movies and the reward measures if the
user enjoys the recommended movie  The learner produces
  policy  meaning   mapping from contexts to actions   
common question in such settings is  given   target policy 
what is its expected reward  By letting the policy choose
actions       recommend movies to users  we can compute
its reward  Such online evaluation is typically costly since it
exposes users to an untested experimental policy  and does

 Carnegie Mellon University  Pittsburgh  PA  Microsoft Research  New York  NY  Correspondence to  YuXiang Wang  yuxiangw cs cmu edu  Alekh Agarwal  alekha microsoft com 
Miroslav Dud    mdudik microsoft com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

not scale to evaluating many different target policies 
Offpolicy evaluation is an alternative paradigm for the same
question  Given logs from the existing system  which might
be choosing actions according to   very different logging
policy than the one we seek to evaluate  can we estimate
the expected reward of the target policy  There are three
classes of approaches to address this question  the direct
method  DM  also known as regression adjustment  inverse
propensity scoring  IPS   Horvitz   Thompson    and
doubly robust  DR  estimators  Robins   Rotnitzky   
Bang   Robins    Dud   et al     
Our  rst goal in this paper is to study the optimality of
these three classes of approaches  or lack thereof  and more
fundamentally  to quantify the statistical hardness of offpolicy evaluation  This problem was previously studied
for multiarmed bandits  Li et al    and is related to
  large body of work on asymptotically optimal estimators
of average treatment effects  ATE   Hahn    Hirano
et al    Imbens et al    Rothe    which can be
viewed as   special case of offpolicy evaluation  In both settings    major underlying assumption is that rewards can be
consistently estimated from the features       covariates  describing contexts and actions  either via   parametric model
or nonparametrically  Under such consistency assumptions 
it has been shown that DM and or DR are optimal  Imbens
et al    Li et al    Rothe    whereas standard
IPS is not  Hahn    Li et al    but it becomes
 asymptotically  optimal when the true propensity scores
are replaced by suitable estimates  Hirano et al   
Unfortunately  consistency of   reward model can be dif 
cult to achieve in practice  Parametric models tend to suffer
from   large bias  see       the empirical evaluation of Dud  
et al    and nonparametric models are limited to small
dimensions  otherwise nonasymptotic terms become too
large  see       the analysis of nonparametric regression by
Bertin et al    Therefore  here we ask  What can be
said about hardness of policy evaluation in the absence of
rewardmodel consistency 

In this pursuit  we provide the  rst rateoptimal lower bound
on the meansquared error  MSE  for offpolicy evaluation

 The precise assumptions vary for each estimator  and are

somewhat weaker for DR than for DM 

Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

in contextual bandits without consistency assumptions  Our
lower bound matches the upper bounds of IPS and DR up
to constants  when given   nondegenerate context distributions  This result is in contrast with the suboptimality of IPS
under previously studied consistency assumptions  which
implies that the two settings are qualitatively different 
Whereas IPS and DR are both minimax optimal  our experiments  similar to prior work  show that IPS is readily
outperformed by DR  even when using   simple parametric
regression model that is not asymptotically consistent  We
attribute this to   lower variance of the DR estimator  We
also empirically observe that while DR is generally highly
competitive  it is sometimes substantially outperformed by
DM  We therefore ask whether it is possible to achieve an
even better biasvariance tradeoff than DR  We answer af 
 rmatively and propose   new class of estimators  called
the SWITCH estimators  that adaptively interpolate between
DM and DR  or IPS  We show that SWITCH has MSE no
worse than DR  or IPS  in the worst case  but is robust to
large importance weights and can achieve   substantially
smaller variance than DR or IPS 
We empirically evaluate the SWITCH estimators against  
number of strong baselines from prior work  using   previously used experimental setup to simulate contextual bandit
problems on realworld multiclass classi cation data  The
results af rm the superior biasvariance tradeoff of SWITCH
estimators  with substantial improvements across   number
of problems 
In summary  the  rst part of our paper initiates the study
of optimal estimators in    nitesample setting and without
making strong modeling assumptions  while the second
part shows how to practically exploit domain knowledge by
building better estimators 

  Setup
In contextual bandit problems  the learning agent observes
  context    takes an action   and observes   scalar reward
  for the action chosen in the context  Here the context
  is   feature vector from some domain     Rd  drawn
according to   distribution   Actions   are drawn from  
 nite set    Rewards   have   distribution conditioned on  
and   denoted by             The decision rule of the agent
is called   policy  which maps contexts to distributions over
actions  allowing for randomization in the action choice 
We write         and         to denote the logging and
target policies respectively  Given   policy   we extend it
to   joint distribution over           where       action
            and                 With this notation  given
         samples  xi  ai  ri      we wish to compute the
value of  

            Ex Ea   Er         

 

In order to correct for the mismatch in the action distributions under   and   it is typical to use importance weights 
de ned as                    For consistent estimation  it is standard to assume that         cid    corresponding to absolute continuity of   with respect to   meaning
that whenever           then also           We make
this assumption throughout the paper  In the remainder of
the setup we present three common estimators of   
The  rst is the inverse propensity scoring  IPS  estimator
 Horvitz   Thompson    de ned as

   
IPS  

 xi  ai ri 

 

  cid 

  

IPS is unbiased and makes no assumptions about how rewards might depend on contexts and actions  When such
information is available  it is natural to posit   parametric or
nonparametric model of             and    it on the logged
data to obtain   reward estimator          Policy evaluation
can now simply be performed by scoring   according to   
as

     xi   xi    

 

  cid 

 cid 

  

   

   
DM  

 
 

where the DM stands for direct method  Dud   et al   
also known as regression adjustment or imputation  Rothe 
  IPS can have   large variance when the target and
logging policies differ substantially  and parametric variants
of DM can be inconsistent  leading to   large bias  Therefore 
both in theory and practice  it is bene cial to combine the
approaches into   doubly robust estimator  Cassel et al 
  Robins   Rotnitzky    Dud   et al    such
as the following variant 

  cid 

 cid 
 xi  ai cid ri      xi  ai cid 

   
DR  

 
 

  

 

     xi   xi    

 cid 

   

 cid 

 

 

Note that IPS is   special case of DR with        In the
sequel  we mostly focus on IPS and DR  and then suggest
how to improve them by further interpolating with DM 

  Limits of Offpolicy Evaluation
In this section  we study the offpolicy evaluation problem
in   minimax setup  After setting up the framework  we
present our lower bound and the matching upper bounds for
IPS and DR under appropriate conditions 
While minimax optimality is standard in statistical estimations  it is not the only notion of optimality  An alternative
framework is that of asymptotic optimality  which establishes CramerRao style bounds on the asymptotic variance
of estimators  We use the minimax framework  because it

Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

is the most amenable to  nitesample lower bounds  and is
complementary to previous asymptotic results  as we discuss
after presenting our main results 

required for consistency of IPS  see       Dud   et al   
Our assumption holds for instance when the context space
is  nite  because then both   and Rmax are bounded 

  Minimax Framework

  Minimax Lower Bound for Offpolicy Evaluation

Offpolicy evaluation is   statistical estimation problem 
where the goal is to estimate    given          samples generated according to   policy   We study this problem in  
standard minimax framework and seek to answer the following question  What is the smallest MSE that any estimator
can achieve in the worst case over   large class of contextual
bandit problems  As is usual in the minimax setting  we
want the class of problems to be rich enough so that the estimation problem is not trivial  and to be small enough so that
the lower bounds are not driven by complete pathologies  In
our problem  we        and   and only take worst case
over   class of reward distributions  This allows the upper
and lower bounds to depend on     and   highlighting
how these groundtruth parameters in uence the problem
dif culty  The family of reward distributions            that
we study is   natural generalization of the class studied by
Li et al    for multiarmed bandits  We assume we are
given maps Rmax             and              
and de ne the class of reward distributions    Rmax  as 
   Rmax   
              ED         Rmax      
and VarD                for all     

 cid 

 cid 

 

Note that   and Rmax are allowed to change over contexts
and actions  Formally  an estimator is any function     
                 that takes   data points collected by
  and outputs an estimate of    The minimax risk of offpolicy evaluation over the class    Rmax  denoted by
Rn        Rmax  is de ned as

  cid       cid   

 

inf
  

sup

          Rmax 

Recall that the expectation is taken over the   samples
drawn from   along with any randomness in the estimator  The main goal of this section is to obtain   lower
bound on the minimax risk  To state our bound  recall
that                            is an importance
weight at        We make the following technical assumption on our problem instances  described by tuples of the
form         Rmax 
Assumption   There exists       such that   
and   

 cid Rmax cid  are  nite 

 cid cid 

This assumption is only   slight strengthening of the assumption that    and   Rmax  be  nite  which is
 Technically  the inequalities in the de nition of    Rmax 

need to hold almost surely with       and           

With the minimax setup in place  we now give our main
lower bound on the minimax risk for offpolicy evaluation
and discuss its consequences  Our bound depends on  
parameter         and   derived indicator random variable
                    which separates out the pairs
       that appear  frequently  under   As we will see 
the  frequent  pairs         where       correspond to the
intrinsically realizable part of the problem  where consistent
reward models can be constructed  The  infrequent  pairs
 where       constitute the part that is nonrealizable in
the worstcase  When     Rd and   is continuous with
respect to the Lebesgue measure  then            for
all         so the problem is nonrealizable everywhere
in the worstcase  Our result uses the following problemdependent constant  de ned with the convention      

 cid   

    

  Rmax 
  Rmax 

 cid 

 

 

       max

Theorem   Assume that   problem instance satis es Assumption   with some       Then for any         and

any     max cid    
 cid cid      
 cid   

  

 

risk Rn        Rmax  satis es the lower bound
        log 

max cid  the minimax
 cid 

      

      

 cid cid 

max

  

The bound holds for every         and we can take the
maximum over   In particular  we get the following simple
corollary under continuous context distributions 
Corollary   Under conditions of Theorem   assume further that   has   density relative to Lebesgue measure  Then
Rn        Rmax      
lower bound  In general  choosing       cid   log   cid 

If   is   mixture of   density and point masses  then      
will exclude the point masses from the second term of the

 cid cid      

 cid   

  

 cid 

max

 

excludes the contexts likely to appear multiple times  and
ensures that the second term in Theorem   remains nontrivial  when            with positive probability 
Before sketching the proof of Theorem   we discuss its
preconditions and implications 

 Formally         corresponds to               the measure under   of the set        For example  when   is   continuous distribution then            everywhere 

Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

Preconditions of the theorem  The theorem assumes the
existence of    problemdependent  constant    which
depends on the constant   and various moments of the
importanceweighted rewards  When Rmax and   are
bounded    common situation     measures how heavytailed the importance weights are  Note that        for
all         whenever Assumption   holds  and so the
condition on   in Theorem   is eventually satis ed as long
as the random variable  Rmax has   bounded second moment  This is quite reasonable since in typical applications
the   priori bound on expected rewards is on the same order or larger than the   priori bound on the reward noise 
For the remainder of the discussion  we assume that   is
appropriately large so the preconditions of the theorem hold 
Comparison with upper bounds  The setting of Corollary   is typical of many contextual bandit applications  In
this setting both IPS and DR achieve the minimax risk up to
  multiplicative constant  Let                       Recall
that DR is using an estimator          of         and IPS
can be viewed as   special case of DR with        By
Lemma     of Dud   et al    the MSE of DR is

    

DR     
 
 
 

 cid      Varx DEa     
 cid 

  Ex DVara         

 
Note that          Rmax  so if the estimator    also satis es
         Rmax  we obtain that the risk of DR  with IPS as

 

            

  special case  is at most   cid   
risk is precisely  cid   

This means that IPS and DR are unimprovable  in the worst
case  beyond constant factors  Another implication is that
the lower bound of Corollary   is sharp  and the minimax

            

max cid 
max cid  While

IPS and DR exhibit the same minimax rates  Eq    also
immediately shows that DR will be better than IPS whenever
   is even moderately good  better than       
Comparison with asymptotic optimality results  As discussed in Section   previous work on optimal offpolicy
evaluation  speci cally the average treatment estimation  assumes that it is possible to consistently estimate          
            Under such an assumption it is possible to
 asymptotically  match the risk of DR with the perfect
reward estimator        cid  and this is the best possible asymptotic risk  Hahn    This optimal risk is
 
 
 rst two terms of Eq    with no dependence on Rmax 
Several estimators achieve this risk  including the multiplicative constant  under various consistency assumptions
 Hahn    Hirano et al    Imbens et al    Rothe 
  Note that this is strictly below our lower bound for
continuous   That is  consistency assumptions yield   better asymptotic risk than possible in the agnostic setting  The

 cid      Varx DE        cid  corresponding to the

gap in constants between our upper and lower bounds is due
to the  nitesample setting  where lowerorder terms cannot
be ignored  but have to be explicitly bounded  Indeed  apart
from the result of Li et al    discussed below  ours is
the  rst  nitesample lower bound for offpolicy evaluation 
Comparison with multiarmed bandits  For multiarmed
bandits  equivalent to contextual bandits with   single context  Li et al    show that the minimax risk equals
      and is achieved       by DM  whereas IPS
is suboptimal  They also obtain   similar result for contextual bandits  assuming that each context appears with  
largeenough probability to estimate its associated rewards
by empirical averages  amounting to realizability  While
we obtain   larger lower bound  this is not   contradiction 
because we allow arbitrarily small probabilities of individual contexts and even continuous distributions  where the
probability of any single context is zero 
On   closer inspection  the  rst term of our bound in Theorem   coincides with the lower bound of Li et al   
 up to constants  The second term  optimized over   is
nonzero only if there are contexts with small probabilities
relative to the number of samples  In multiarmed bandits 
we recover the bound of Li et al    When the context
distribution is continuous  or the probability of seeing repeated contexts in   data set of size   is small  we get the
minimax optimality of IPS 
One of our key contributions is to highlight this agnostic contextual regime where IPS is optimal  In the noncontextual
regime  where each context appears frequently  the rewards
for each contextaction pair can be consistently estimated
by empirical averages  Similarly  the asymptotic results
discussed earlier focus on   setting where rewards can be
consistently estimated thanks to parametric assumptions or
smoothness  for nonparametric estimation  with the goal
of asymptotic ef ciency  Our work complements that line
of research  In many practical situations  we wish to evaluate policies on highdimensional context spaces  where the
consistent estimation of rewards is not   feasible option  In
other words  the agnostic contextual regime dominates 
The distinction between the contextual and noncontextual
regime is also present in our proof  which combines   noncontextual lower bound due to the reward noise  similar to
the analysis of Li et al    and an additional bound
arising for nondegenerate context distributions  This latter
result is   key technical novelty of our paper 

Proof sketch  We only sketch some of the main ideas
here and defer the full proof to Appendix    For simplicity 
we discuss the case where   is   continuous distribution 
We consider two separate problem instances corresponding
to the two terms in Theorem   The  rst part is relatively
straightforward and reduces the problem to Gaussian mean

Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

 cid 

estimation  We focus on the second part which depends on
Rmax  Our construction de nes   prior over the reward distributions              Given any          problem instance
is given by

                      

Rmax                  
 

               

for        to be appropriately chosen  Once   is drawn  we
consider   problem instance de ned by   where the rewards
are deterministic and the only randomness is in the contexts 
In order to lower bound the MSE across all problems  it
suf ces to lower bound   MSE    That is  we can
compute the MSE of an estimator for each individual   and
take expectation of the MSEs under the prior prescribed
by   If the expectation is large  we know that there is  
problem instance where the estimator incurs   large MSE 
  key insight in our proof is that this expectation can be
lower bounded by MSEE        corresponding to the
MSE of   single problem instance with the actual rewards 
rather than        drawn according to   and with the mean
reward function         This is powerful  since this
new problem instance has stochastic rewards  just like Gaussian mean estimation  and is amenable to standard techniques  The lower bound by MSEE        is only valid
when the context distribution   is rich enough       continuous  In that case  our reasoning shows that with enough
randomness in the context distribution    problem with even
  deterministic reward function is extremely challenging 

  Incorporating Reward Models
As discussed in the previous section  it is generally possible
to beat our minimax bound when consistent reward models
exist  We also argued that even in the absence of   consistent
model  when DR and IPS both achieve optimal risk rates 
the performance of DR on  nite samples will be better than
IPS as long as the reward model is even moderately good
 see Eq    However  under   large reward noise   DR may
still suffer from high variance when the importance weights
are large  even when given   perfect reward model  In this
section  we derive   class of estimators that leverage reward
models to directly address this source of high variance  in  
manner very different from the standard DR approach 

  The SWITCH Estimators

Our starting point is the observation that insistence on maintaining unbiasedness puts the DR estimator at one extreme
end of the biasvariance tradeoff  Prior works have considered ideas such as truncating the rewards or importance
weights when the importance weights are large  see      
Bottou et al    which can dramatically reduce the variance at the cost of   little bias  We take the intuition   step

further and propose to estimate the rewards for actions by
two distinct strategies  based on whether they have   large
or   small importance weight in   given context  When
importance weights are small  we continue to use our favorite unbiased estimators  but switch to directly applying
the  potentially biased  reward model on actions with large
importance weights  Here   small  and  large  are de ned
via   threshold parameter   Varying this parameter between   and   leads to   family of estimators which we call
the SWITCH estimators as they switch between an agnostic
approach  such as DR or IPS  and the direct method 
We now formalize this intuition  and begin by decomposing
   according to importance weights 
                              

            

 cid cid 

   

  Ex 

ED                           

 

 cid 

Conceptually  we split our problem into two  The  rst problem always has small importance weights  so we can use
unbiased estimators such as IPS or DR  The second problem 
where importance weights are large  is addressed by DM 
Writing this out leads to the following estimator 

  cid 
 cid 
  cid 

 
 

  

  

   

 vSWITCH  

 

 
 

 ri          

   xi        xi xi          

 

Note that the above estimator speci cally uses IPS on the
 rst part of the problem  When DR is used instead of IPS 
we refer to the resulting estimator as SWITCHDR  The
reward model used within the DR part of the SWITCHDR
estimator can be the same or different from the reward model
used to impute rewards in the second part  We next present
  bound on the MSE of the SWITCH estimator using IPS   
similar bound holds for SWITCHDR 
Theorem   Let                               be the bias
of    and assume              Rmax       almost surely 
Then for  vSWITCH  with       the MSE is at most
 
 

max       cid cid 
 cid   

 cid       cid      

 cid    

 cid cid   
 cid       cid 

max

    

 

The proposed estimator interpolates between DM and IPS 
For       SWITCH coincides with DM  while      
yields IPS  Consequently  SWITCH estimator is minimax
optimal when   is appropriately chosen  However  unlike
IPS and DR  the SWITCH and SWITCHDR estimators are
by design more robust to large  or heavytailed  importance
weights  Several estimators related to SWITCH have been
previously studied 

Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

  Bottou et al    consider   special case of SWITCH
with        meaning that all the actions with large
importance weights are eliminated from IPS  We refer
to this method as Trimmed IPS 

  Thomas   Brunskill   study an estimator similar
to SWITCH in the more general setting of reinforcement
learning  Their MAGIC estimator can be seen as using
several candidate thresholds   and then evaluating the
policy by   weighted sum of the estimators corresponding to each   Similar to our approach of automatically
determining   they determine the weighting of estimators via optimization  as we discuss below 

  Automatic Parameter Tuning

So far we have discussed the properties of the SWITCH estimators assuming that the parameter   is chosen well  Our
goal is to obtain the best of IPS and DM  but   poor choice
of   might easily give us the worst of the two estimators 
Therefore    method for selecting   plays an essential role 
  natural criterion would be to pick   that minimizes the
MSE of the resulting estimator  Since we do not know the
precise MSE  as    is unknown  an alternative is to minimize its datadependent estimate  Recalling that the MSE
can be written as the sum of variance and squared bias  we
estimate and bound the terms individually 
Recall that we are working with   data set  xi  ai  ri  and
      ai   xi ai   xi  Using this data  it is straightforward to estimate the variance of the SWITCH estimator 
Let Yi    denote the estimated value that   obtains on the
data point xi according to the SWITCH estimator with the
threshold   that is
Yi      ri       

   xi      xi xi      

 cid 

and           
 
xi are        the variance can be estimated as

   Yi    Since  vSWITCH          and the

  cid 
 Yi              cid Var   

 

Var            
  

  

where the approximation above is clearly consistent since
the random variables Yi are appropriately bounded as long
as the rewards are bounded  because the importance weights
are capped at the threshold  
Next we turn to the bias term  For understanding bias 
we look at the MSE bound in Theorem   and observe
that the last term in that theorem is precisely the squared
bias  Rather than using   direct bias estimate  which would
require knowledge of the error in     we will upper bound
this term  We assume that the function Rmax       is known 
This is not limiting since in most practical applications an  
priori bound on the rewards is known  Then we can upper

   

 cid  

bound the squared bias as

  

 cid       cid      
 cid   
 cid Bias 

 cid Rmax       cid 
 cid cid 
 cid Rmax       cid cid  xi

  cid 

  

   

 

 

Replacing the expectation with an average  we obtain

 

  

With these estimates  we pick the threshold cid  by optimizing

the sum of estimated variance and the upper bound on bias 

 cid    argmin

 

 cid Var     cid Bias 

   

 

Our upper bound on the bias is rather conservative  as it
upper bounds the error of DM at the largest possible value
for every data point  This has the effect of favoring the
use of the unbiased part in SWITCH whenever possible 
unless the variance would overwhelm even an arbitrarily
biased DM  This conservative choice  however  immediately
implies the minimax optimality of the SWITCH estimator

using cid  because the incurred bias is no more than our upper

bound  and it is incurred only when the minimax optimal
IPS estimator would be suffering an even larger variance 
Our automatic tuning is related to the MAGIC estimator
of Thomas   Brunskill   The key differences are
that we pick only one threshold   while they combine
the estimates with many different    using   weighting
function  They pick this weighting function by optimizing  
biasvariance tradeoff  but with signi cantly different bias
and variance estimators  In our experiments  the automatic
tuning using Eq    generally works better than MAGIC 

  Experiments
We next empirically evaluate the proposed SWITCH estimators on the   UCI data sets previously used for offpolicy
evaluation  Dud   et al    We convert the multiclass
classi cation problem to contextual bandits by treating the
labels as actions for   policy   and recording the reward of
  if the correct label is chosen  and   otherwise 
In addition to this deterministic reward model  we also consider   noisy reward model for each data set  which reveals
the correct reward with probability   and outputs   random coin toss otherwise  Theoretically  this should lead
to bigger   and larger variance in all estimators  In both
reward models  Rmax     is   valid bound 
The target policy   is the deterministic decision of   logistic
regression classi er learned on the multiclass data  while
the logging policy   samples according to the probability
estimates of   logistic model learned on   covariateshifted
version of the data  The covariate shift is obtained as in
prior work  Dud   et al    Gretton et al   

Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

    Deterministic reward

    Noisy reward

Figure   The number of UCI data sets where each method achieves at least   given Rel  MSE  On the left  the UCI labels are used as is 
on the right  label noise is added  Curves towards topleft achieve smaller MSE in more cases  Methods in dashed lines are  cheating 
by choosing the threshold   to optimize test MSE  SWITCHDR outperforms baselines and our tuning of   is not too far from the best
possible  Each data set uses an   which is the size of the data set  drawn via bootstrap sampling and results are averaged over   trials 

    yeast   deterministic reward

    yeast   noisy reward

    optdigits   deterministic reward

    optdigits   noisy reward

Figure   MSE of different methods as   function of input data size  Top  optdigits data set  Bottom  yeast data set 

Relative error        IPS Number of data sets Relative error        IPS Number of data sets IPSDMDRSWITCHDRoracle SWITCHDRoracle Trim TrunIPSSWITCHDR magicIPSDMDRSWITCHDRoracle SWITCHDRoracle Trim TrunIPSSWITCHDR magicn MSE yeast                     MSE yeast                     MSE optdigits                     MSE optdigits                   Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

In each data set with   examples  we treat the uniform distribution over the data set itself as   surrogate of the population
distribution so that we know the ground truth of the rewards  Then  in the simulator  we randomly draw        data
sets of size                     until reaching    with   different repetitions of each size 
We estimate MSE of each estimator by taking the empirical average of the squared error over the   replicates 
note that we can calculate the squared error exactly  because
we know    For some of the methods       IPS and DR 
the MSE can have   very large variance due to the potentially large importance weights  This leads to very large
error bars if we estimate their MSE even with   replicates  To circumvent this issue  we report   clipped version
of the MSE that truncates the squared error to   namely
MSE                This allows us to get valid con 
dence intervals for our empirical estimates of this quantity 
Note that this does not change the MSE estimate of our
approach at all  but is signi cantly more favorable towards
IPS and DR  In this section  whenever we refer to  MSE 
we are referring to this truncated version 
We compare SWITCH and SWITCHDR against the following baselines    IPS    DM trained via logistic regression 
  DR    Truncated and Reweighted IPS  TrunIPS  and
  Trimmed IPS  TrimIPS 
In DM  we train    and then evaluate the policy on the same
contextual bandit data set  Following Dud   et al   
DR is constructed by randomly splitting the contextual bandit data into two folds  estimating    on one fold  and then
evaluating   on the other fold and vice versa  obtaining
two estimates  The  nal estimate is the average of the two 
TrunIPS is   variant of IPS  where importance weights are
capped at   threshold   and then renormalized to sum to
one  see       Bembom   van der Laan    TrimIPS is  
special case of SWITCH due to Bottou et al    described
earlier  where       
For SWITCH and SWITCHDR as well as TrunIPS and TrimIPS we select the parameter   by our automatic tuning from
Section   To evaluate our tuning approach  we also include the results for the   tuned optimally in hindsight 
which we refer to as the oracle setting  and also show results obtained by the multithreshold MAGIC approach  In
all these approaches we optimize among   possible thresholds  from an exponential grid between the smallest and the
largest importance weight observed in the data  considering
all actions in each observed context 
In order to stay comparable across data sets and data sizes 
our performance measure is the relative MSE with respect to the IPS  Thus  for each estimator     we calculate
Rel  MSE      MSE   
The results are summarized in Figure   plotting the number

MSE vIPS 

of data sets where each method achieves at least   given
relative MSE  Thus  methods that achieve smaller MSE
across more data sets are towards the topleft corner of
the plot  and   larger area under the curve indicates better
performance  Some of the differences in MSE are several
orders of magnitude large since the relative MSE is shown
on the logaritmic scale  As we see  SWITCHDR dominates
all baselines and our empirical tuning of   is not too far from
the best possible  The automatic tuning by MAGIC tends
to revert to DM  because its bias estimate is too optimistic
and so DM is preferred whenever IPS or DR have some
signi cant variance  The gains of SWITCHDR are even
greater in the noisyreward setting  where we add label
noise to UCI data 
In Figure   we illustrate the convergence of MSE as   increases  We select two data sets and show how SWITCHDR
performs against baselines in two typical cases      when the
direct method works well initially but is outperformed by
IPS and DR as   gets large  and  ii  when the direct method
works poorly  In the  rst case  SWITCHDR outperforms
both DM and IPS  while DR improves over IPS only moderately  In the second case  SWITCHDR performs about
as well as IPS and DR despite   poor performance of DM 
In all cases  SWITCHDR is robust to additional noise in
the reward  while IPS and DR suffer from higher variance 
Results for the remaining data sets are in Appendix   

  Conclusion
In this paper we have carried out minimax analysis of offpolicy evaluation in contextual bandits and showed that
IPS and DR are minimax optimal in the worstcase  when
no consistent reward model is available  This result complements existing asymptotic theory with assumptions on
reward models  and highlights the differences between agnostic and consistent settings  Practically  the result further
motivates the importance of using side information  possibly
by modeling rewards directly  especially when importance
weights are too large  Given this observation  we propose
  new class of estimators called SWITCH that can be used
to combine any importance weighting estimators  including
IPS and DR  with DM  The estimators adaptively switch
between DM when the importance weights are large and
either IPS or DR when the importance weights are small 
We show that the new estimators have favorable theoretical
properties and also work well on realworld data  Many interesting directions remain open for future work  including
highprobability upper bounds on the  nitesample MSE of
SWITCH estimators  as well as sharper  nitesample lower
bounds under realistic assumptions on the reward model 

 For clarity  we have excluded SWITCH  which signi cantly
outperforms IPS  but is dominated by SWITCHDR  Similarly  we
only report the better of oracleTrimIPS and oracleTrunIPS 

Optimal and Adaptive Offpolicy Evaluation in Contextual Bandits

Hoeffding  Wassily  Probability inequalities for sums of
bounded random variables  Journal of the American
Statistical Association     

Horvitz  Daniel   and Thompson  Donovan      generalization of sampling without replacement from    nite
universe  Journal of the American Statistical Association 
   

Imbens  Guido  Newey  Whitney  and Ridder  Geert  Meansquared error calculations for average treatment effects 
Technical report   

Lafferty  John  Liu  Han  and Wasserman  Larry  Minimax
theory    URL http www stat cmu edu 
 larry sml Minimax pdf 

Li  Lihong  Munos    mi  and Szepesv ri  Csaba  Toward
minimax offpolicy value estimation  In AISTATS   

Robins  James   and Rotnitzky  Andrea  Semiparametric
ef ciency in multivariate regression models with missing
data  Journal of the American Statistical Association   
   

Rothe  Christoph  The value of knowing the propensity
score for estimating average treatment effects  IZA Discussion Paper Series   

Sierpi nski  Wac aw  Sur les fonctions   ensemble additives
et continues  Fundamenta Mathematicae   
 

Thomas  Philip   and Brunskill  Emma  Dataef cient offpolicy policy evaluation for reinforcement learning  In
ICML   

Acknowledgments
The work was partially completed during YW   internship at
Microsoft Research NYC from May   to Aug   The
authors would like to thank Lihong Li and John Langford
for helpful discussions  Edward Kennedy for bringing our
attention to related problems and recent developments in
causal inference  and an anonymous reviewer for pointing
out relevant econometric references and providing valuable
feedback that helped connect our work with research on
average treatment effects 

References
Bang  Heejung and Robins  James    Doubly robust estimation in missing data and causal inference models 
Biometrics     

Bembom  Oliver and van der Laan  Mark    Dataadaptive
selection of the truncation level for inverseprobability 
oftreatment weighted estimators   

Bertin  Karine et al  Asymptotically exact minimax estimation in supnorm for anisotropic   lder classes  Bernoulli 
   

Bottou    on  Peters  Jonas  Candela  Joaquin Quinonero 
Charles  Denis Xavier  Chickering  Max  Portugaly  Elon 
Ray  Dipankar  Simard  Patrice    and Snelson  Ed  Counterfactual reasoning and learning systems  the example of
computational advertising  Journal of Machine Learning
Research     

Cassel  Claes      rndal  Carl    and Wretman  Jan   
Some results on generalized difference estimation and
generalized regression estimation for  nite populations 
Biometrika     

Dud    Miroslav  Langford  John  and Li  Lihong  Doubly

robust policy evaluation and learning  In ICML   

Dud    Miroslav  Erhan  Dumitru  Langford  John  and Li 
Lihong  Doubly robust policy evaluation and optimization  Statistical Science     

Gretton  Arthur  Smola  Alex  Huang  Jiayuan  Schmittfull 
Marcel  Borgwardt  Karsten  and Sch lkopf  Bernhard 
Covariate shift by kernel mean matching  Dataset Shift
in Machine Learning     

Hahn  Jinyong  On the role of the propensity score in ef 
 cient semiparametric estimation of average treatment
effects  Econometrica  pp     

Hirano  Keisuke  Imbens  Guido    and Ridder  Geert  Ef 
 cient estimation of average treatment effects using the
estimated propensity score  Econometrica   
   

