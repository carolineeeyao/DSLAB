ZeroIn ated Exponential Family Embeddings

LiPing Liu     David    Blei  

Abstract

Word embeddings are   widelyused tool to analyze language  and exponential family embeddings  Rudolph et al    generalize the technique to other types of data  One challenge to
 tting embedding methods is sparse data  such
as   document term matrix that contains many
zeros  To address this issue  practitioners typically downweight or subsample the zeros  thus
focusing learning on the nonzero entries  In this
paper  we develop zeroin ated embeddings   
new embedding method that is designed to learn
from sparse observations  In   zeroin ated embedding  ZIE    zero in the data can come from
an interaction to other data       an embedding 
or from   separate process by which many observations are equal to zero         probability
mass at zero  Fitting   ZIE naturally downweights the zeros and dampens their in uence
on the model  Across many types of data 
language  movie ratings  shopping histories  and
bird watching logs we found that zeroin ated
embeddings provide improved predictive performance over standard approaches and  nd better
vector representation of items 

  Introduction
Word embeddings use distributed representations to capture usage patterns in language data  Harris    Rumelhart et al    Bengio et al    Mikolov et al 
      Pennington et al    The main idea is to   
the conditional distribution of words by using vector representations  called embeddings  The learned parameters 
the embedding vectors are useful as features about the
meanings of words  Word embeddings have become  
widely used method for unsupervised analysis of text 

 Columbia University       th St  New York  NY  
 Tufts University    College Ave  Medford  MA   Correspondence to  LiPing Liu  ll columbia edu  David   
Blei  david blei columbia edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

In   recent paper  Rudolph et al    developed exponential family embeddings  Their work casts embeddings
in   probabilistic framework and generalizes them to model
various types of highdimensional data 
Exponential family embeddings give   recipe for creating
new types of embeddings  There are three ingredients  First
is   notion of   context         neighborhood of surrounding words around   word in   document  Second is   conditional distribution of data given its context         categorical distribution for   word  Third is an  embedding structure  that captures parameter sharing       that the embedding vector for PHILOSOPHY is the same wherever it appears in the data  Rudolph et al    show that exponential family embeddings embody many existing methods for
word embeddings  Further  they easily extend to other scenarios  such as movie ratings  shopping basket purchases 
and neuroscience data 
Many applications of embeddings both the classical application to language and the others types of data involve
sparse observations  data that contain many zero entries 
As examples  shoppers do not purchase most of the items in
the store  authors do not use most of the words in   vocabulary  and moviewatchers do not view most of the movies
in an online collection  Sparse observations are challenging for many machine learning methods because the zeros
dominate the data  Most methods will focus on capturing
and predicting them  and embeddings are no exception 
Folk wisdom says that zeros in sparse data contain less information than the nonzeros  Consequently  practitioners
use various methods to downweight them  Hu et al   
or downsample them  as is often the case in word embeddings  Mikolov et al      Rudolph et al    Empirically  methods that downweight and downsample the zeros
far outperform their counterparts 
What this folk wisdom suggests is that zeros often occur
for one of two reasons either they are part of the underlying process that we are trying to model  such as capturing
the meanings of words  or they part of   different process 
such as that only   particular part of speech belongs in  
particular location in   sentence  As other examples     lm
enthusiast might not watch   movie either because she does
not think she will like it or because she has never heard of
it    shopper might not buy   brand of cookies either be 

ZeroIn ated Exponential Family Embeddings

cause he doesn   like them or because he didn   see them 
Motivated by this intuition  we develop zeroin ated embeddings    probabilistic embedding model that captures
the special status of the zeros in the data matrix 
The main idea is as follows  Exponential family embeddings model the conditional distribution of each data point
given its context  where the parameter to that distribution
relates to the embedding vectors  In   zeroin ated embedding  the conditional distribution places extra probability
mass on zero  capturing conditions other than the embedding under which an item might not appear  If an observed
zero is explained by this extra probability  then the corresponding item is not exposed to its relation with other
items 
The probability of seeing   zero that is not from the embedding model can be    xed quantity or can depend on
other properties  such as the popularity of an item  demographics about the shopper  or the parts of speech of the
context words  While zeroin ated embeddings sometimes
fall into the class of an exponential family embedding  as in
the Bernoulli case  they sometimes re ect   more complex
distribution 
The practical effect is that zeroin ated embeddings intelligently downweight the zeros of the data the embeddings
no longer need to explain all of the zeros and empirically
improve the learned representations of the words or other
type of data  We will demonstrate zeroin ated embeddings on language  movie ratings  and shopping baskets  as
described above  We also study zeroin ated embeddings
on bird watching logs  Munson et al    where features like time and location can in uence which birds are
possible to see 
Below  we develop zeroin ated embeddings and show
how we can  exibly de ne the  exposure model  alongside the exponential family embedding model  We derive
two algorithms to    them and study them on   variety of
data sets  Zeroin ated embeddings improve performance
in language  shopping histories  recommendation system
data  and bird watching logs 
Related work  The main thread of work on downweighting zeros comes from recommendation systems  where absent useritem interaction can be misconstrued as   user disliking an item  Hu et al    and Rendle et al   
proposed to manually downweight zeros and showed excellent empirical performance  Liang et al    builds
on this work and introduces an exposure model to explain
zero entries  The exposure model captures whether   user
does not see an item or intentionally chooses not to consume it 
In   sense  zeroin ated embeddings build on
Liang et al    using   type of  exposure model  in
the context of embeddings and capturing itemitem interac 

tions  They also share similarities with the spike and slab
model  Mitchell   Beauchamp    and zeroin ated regression models  Lambert   

  ZeroIn ated Embeddings
We  rst review exponential family embeddings  We then
develop zeroin ated embeddings 

  Exponential Family Embedding

An Exponential Family Embedding  EFE  generalizes
word embedding to other types of data 
It uses vector
representations to capture conditional probabilities of data
given   context  Speci cally  an EFE aims to learn vector representation of items  that is  to represent each item
                 with an embedding vector      RK and  
context vector      RK  Vectors    and    for all   are
denoted as   and   respectively 
An EFE model learns from observationcontext pairs  In
each pair                  the observation xi    xij      
si  contains values observed from one or multiple items in
si  and the context yi    yij       ci  contains the values
of related items in   context set ci 
An EFE is de ned by three elements  the context  the conditional distribution  and the embedding structure  We
have de ned the context  The conditional distribution of
xi given its context yi is from the exponential family 

xi   ExpFam cid yi  si     xi cid 

 

where the context yi provides the natural parameter
through  yi  si  and    xi  is the suf cient statistics 
The embeddings come into play in the natural parameter 

 yi  si     

yij  

 

 

 cid 

 cid 

si

 cid 

  ci

 cid 

where the columns of  si are embedding vectors of items
in si and     is   link function  The EFE conditional probability   xi yi   si   ci  speci es the distribution of the
values of items in si in their context  When there is no
ambiguity  we write the probability as   xi yi  By  tting
the conditional probability  the embedding model captures
the interaction between items in si and items in ci 
The embedding structure of EFE decides how the vectors
of items are shared by different observationcontext pairs 
It is essentially the de nition of ci and si  which indicate
how to attach item indices js to pair indices is 
Finally  an EFE puts Gaussian prior over   and   For all
  

     Gaussian   
     Gaussian   

   
   

 
 

ZeroIn ated Exponential Family Embeddings

  and  

where   is   Kidentity matrix and  
  are hyperparameters controlling the variance of the context and embedding vectors 
An EFE infers   and   by maximizing the conditional log
likelihood of observations given their contexts  The learned
vectors   and   are able to capture the correlation between
items and their contexts 
We give two examples  In movie ratings  we can extract
observationcontext pairs from   person   ratings  the observation xi is the rating of   single movie in si si     
and the context is the ratings yi of all other movies rated by
the same person  In language  the observation at   text position is   onehot vector xi indicating which word is there
and the context yi is the vector representation of words in
the context  In this case si and ci are both the entire vocabulary               The original word embedding model
 Bengio et al    Mnih   Hinton    assumes the
conditional distribution to be the multinomial distribution
with   trial  The word vec model optimized by negative
sampling  NEG   Mikolov et al      uses   product of
Bernoulli distributions as the conditional distribution 

  Exposure modeling with zeroin ation

An EFE explains every observation xi by an item   interaction with its context  However  as we described in Section   we may not want the embeddings to explain every
observation  especially when they are dominated by zeros 
  ZeroIn ated Embedding  ZIE  places extra probability
mass at zero in the embedding distribution  This mass can
be thought of as the probability that the corresponding item
is not  exposed  to its interaction with other items  In  
ZIE  the embeddings vectors need not capture the  zero 
data that are explained by the extra mass 
In more detail  for each observed value xij we explicitly
de ne an exposure indicator bij to indicate whether the corresponding item   is exposed to the interaction with context
items  bij     or not  bij     Each bij is   random variable from Bernoulli distribution with probability uij 

bij   Bernoulli uij 

 

The exposure indicators and exposure probabilities of the
items in the observation xi are collectively denoted as bi  
 bij       si  and ui    uij       si  respectively 
In many applications  we have the information about the
exposure probability uij  Suppose we have   set of covariates vi   Rd for each   related to the exposure probability 
We    uij with   logistic regression 

uij   logistic   cid 

 
  vi     
   
where wj are the coef cients and   
  is the intercept   If
there is no such covariates  then only the intercept term is

used  which means that the exposure probabilities of items
are shared by observationcontext pairs 
Next  we incorporate the exposure indicator into the embedding model  When xi    xij  is an observation with
only one item    the indicator bij decides whether xij is
zero or from the embedding distribution 

 cid   
ExpFam cid yi  si     xij cid 

xij  

if bij    
if bij    

   

The distribution   has probability mass   at  
When xi has multiple entries  the indicator vector bi decides the exposure of each item separately  The items not
exposed have zero values 

   cid 

xij    

     si  bij    
 
             si  bij     and
is

Let the items exposed be   
their values are   
from   smaller embedding model restricted to   
   

     xij       si  bij     Then   

 

    ExpFam cid yi    

  

         

 
   yi  bi  when si has either

We have   xi yi  bi        
single or multiple items 
Finally  if the exposure probabilities are    by covariates
then each weight vector wj is also given   Gaussian prior 

wj   Gaussian   

wI                  

 

The identity matrix   has size   here  and  
parameter 

  is the hyper 

  Inference

likelihood cid 

In this subsection  we derive the method of inferring  
and   from ZIE  The approach is to maximize the logi log   xi yi  ui  with all hidden variables bi

marginalized 
In this subsection  we derive the solution with uis instead
of  wj    
     for notational simplicity  Once we can calculate the gradient of each ui  the gradient calculation of each
   yi  bi 
    is straightforward  The probability     
 wj    
is indeed from the basic embedding model  and we denote
   yi  to explicitly show how the basic embedding
it as      
model is positioned in the solution 
The inference is easy when the observation xi is about one
item  We can either use EM to maximize its exact variational lower bound or directly marginalize out the hidden
variable  We brie   give both solutions here  as they have
different indications of zero in ation 
We  rst give the EM solution  In   step  we calculate the
posterior distribution   bij xij  yi  uij  of each exposure

ZeroIn ated Exponential Family Embeddings

indicator bij  This distribution is   Bernoulli distribution 
and its parameter is denoted as  ij  Applying the Bayesian
rule  we have

 ij  

uij    xij  yi 

 uij  uij    xij  yi 
 

if xij    
if xij  cid   

 

 

 cid 

The lower bound of the loglikelihood of pair   is

  cid  log   xij yi  bij cid    KL ij  uij   
 cid   ij log    xij yi    KL ij  uij 

log    xij yi    log uij

if xij    
if xij  cid   

 

 

The expectation is taken with respect to   bij xij  yi  uij 
and KL ij  uij  is the KLdivergence from the prior to
posterior of bij  In   step  the lower bound is maximized
with respect to   and   Note that  there is no need to take
derivative with respect to  ij when taking gradient steps
even though it is   function of   and   because  ij already
maximizes the lower bound  Hoffman et al   
Eq    shows that zeroin ation downweights zero entries by  ij when learning   and   This method of downweighting zeros is derived in   systematic way instead of  
hack 
We can also marginalize bij directly 

  xij yi  uij   

 cid  uij    xij    yi        uij 

uij    xij yi 

if xij    
if xij  cid   

 

 

This equation makes the zeroin ation clearer 
Now let   work on the inference problem when xi has values of multiple items  In this case  we need to consider an
   yi  for each con guration of bi 
embedding model      
so there are potentially exponential number of embedding
models to consider  We have to exploit the structure of the
embedding model to give   tractable MLE problem  In this
paper  we consider two special cases  that the embedding
models are independent for items in si  and that the observation xi is from   multinomial distribution 
In the  rst case  the embedding model is the product of
the embedding models with the same context  Word vec
with NEG training is   special case with single models as
Bernoulli embedding  Mikolov et al     
  xi yi  bi   si   ci   

  xij yi  bij       ci 

 cid 

 

  si

The exposure indicators are independent of each other  so
the entire model can be decomposed over items in si and
solved as single models  We omit the detail here 

Now we consider the case when the embedding distribution
is multinomial with   trial  which is the model assumption
of word embedding prior to the proposal of NEG training
 Bengio et al    Mikolov et al      The link function     is the identity function  so the vector products
in   directly give logits of the multinomial distribution 
Denote the natural parameter as       cid 
yij   
The probability vector of the multinomial distribution is
     softmax   
The logarithm of the joint probability of the model for one
pair is

 cid 

  ci

si

   yi    log   bi 
log   xi  bi yi    log      
 
   yi  is from the embedding
Note that the probability      
model decided by bi  To learn the model  we need to maximize the loglikelihood of the data with the hidden variable
bi marginalized 
To avoid considering exponentially large number of models  we use the following relation between the full embedding model and the one with items exposed only in   
   

     

log      

log      

   yi    log    xi yi    log   cid 

 
Here the last term renormalizes the probability of    xi yi 
to get the probability of picking one from these items that
are exposed 
Expand    xi yi  and cancel the normalizer  then we have
 

   yi     ij    log   cid 
Here    is the index such that xij     
Now we consider the problem of marginalizing bi via variational inference  Let   bi  be the variational distribution  Combine Eq    and   then the variational lower
bound is 

 cid  log      
   yi cid    KL   bi    bi 
  exp   cid    KL   bi    bi 
 cid  log   cid 

   ij    Eq

Lq   Eq bi 

  exp   

 

KL   bi    bi  is the KLdivergence of   bi  from the
posterior   bi 
This lower bound often needs to be maximized in the online
manner due the large quantity of data  but the expectation
of the logarithm is challenging to estimate even with moderate size of si  In this work  we  nd   datarelated lower
bound of the expectation term 
Let   be any subset of si such that        and        and
bi  be the subvector of bi indexed by   If

 cid 

exp ij   
  si

exp ij 

   
 si   

 

ZeroIn ated Exponential Family Embeddings

  exp       si 

then   cid 
    cid 
we have another lower bound

max

 

Lq   max

  bi   

   exp    for any bi  and then

 cid  log   cid 

   exp   cid 
 ij    Eq bi   
 si    KL   bi    bi 

 

 

  log

MovieLens  

dataset
eBirdPA

Market
WikiS

sparsity

Table   Information about datasets 
 
 
 
 

  item   nonzero
 
 
 
 

  
  
  
  

 
 
 
 

range

  covar

 
 
 
 

Here   bi  is the marginal of some   bi 
We maximize the objective on the        of Eq    with respect to   bi  and model parameters  In each iteration of
calculation  we randomly sample   subset   maximize the
lower bound with respect to   bi  and calculate the gradient of model parameters  The maximization with respect
to   bi  is tractable for small    For larger    we restrict
the form of   bi  In our experiment  we set       and let
  bi  assign zero probability to any bi that has more than
one zero entry  Then we only need to consider   con gurations of bi  the case that all items in   are exposed and the
cases that only one item           cid     is not exposed  so
the maximization with respect to   bi  can be calculated
ef ciently 
The lower bound in Eq    essentially uses        negative  items in the random set   to contrast the item    in
  smaller multinomial distribution  Since the set   is randomly selected  every    cid     has the chance to be used as  
negative sample  The maximization procedure encourages
the model to get larger value of  ij  and empirically the
condition   often holds 

  Computation with Subgradient

The data for embedding is often in large amount  so optimization with stochastic gradients is critically important 
In problems where si has only one item    pair with nonzero observation often provides more informative gradients
than   pair with zero observation  In our optimization  we
keep all pairs with nonzero observations and subsample
zero observations to estimate an unbiased stochastic gradient  The resultant optimization is much faster than that
with full gradient 

  Empirical Study
In this section  we empirically evaluate the ZeroIn ated
Embeddings  We compare four models  two baselines and
two variants of our model  in the following subsections 
  EFE is the basic exponential family embedding model 
  EFEdz assigns weight   to zero entries in the training data  same as  Rudolph et al      ZIE  is the
zeroin ated embedding model and  ts the exposure probabilities with the intercept term only  and   ZIEcov  ts
exposure probabilities with covariates 
All models are evluated with four datasets  eBirdPA 

MovieLens    Market  and WikiS  which will be introduced in detail in the following subsections  Their general information is tabulated in Table   The last column
lists the number of exposure covariates  which is used by
ZIEcov to    the exposure probability 
All four models are optimized by AdaGrad  Duchi et al 
  implemented in TensorFlow  and the AdaGrad parameter   for step length is set to   One tenth of the
training set is separated out as the validation set  whose
loglikelihood is used to check whether the optimization
procedure converges  The variance parameters of     and
  are set to   for all experiments 
We report two types of predictive loglikelihood on the test
set  the loglikelihood of all observations  denoted as  all 
and that of nonzero entries only  denoted as  pos  For
nonzero entries  the predictive loglikelihood is calculated
as log   xi yi  xi       log   xi yi    log      xi  
 yi  The predictive loglikelihood is also estimated
through subsampling in the same way as in training  We
use   vectors as embeddings of items 

  Bird embedding from bird observations

In this experiment  we embed bird species into the vector
space by studying their cooccurance pattern in bird observations Munson et al    The data subset eBirdPA
consists of bird observations from   rectangular area that
mostly overlaps Pennsylvania and the period from day  
to day   of years from   to   Each datum in the
subset is   checklist of counts of   bird species reported
from one observation event  The values of these counts
range from zero to hundreds  Some extraordinarily large
counts are treated as outliers and set to the mean of positive counts of that species  Associated with each checklist
there are   observation covariates  such as effort time  effort distance  and observation time of the day  The dataset
is randomly split into two thirds as the training set and one
third as the test set 
The embedding model use Poisson distribution to    the
count of each species   given the counts of all other species
for each checklist  so si       and ci               
The link function is log softplus  which means the Poisson parameter   is the softplus function of the linear product  The embedding dimension   iterates over the set

 https www tensor ow org 

ZeroIn ated Exponential Family Embeddings

Table   ZIE models improve predictive loglikelihood on bird
data 

 

 

 

 

EFE

EFEdz

ZIE 

ZIEcov

   
all  
pos        
   
all  
pos        
   
all  
pos        

 
 
 

Table   The measures        of embedded vectors calculated
at   levels of downsampling  smaller is better  Embedded vectors
learned by ZIE models are more resistant to downsampling 
ZIEcov

EFEdz

ZIE 

EFE

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

     ratio
 
 
 
 

     
Performance comparison  Table   shows the predictive
loglikelihoods of the four models with different values of
   The two models with zeroin ation get much better predictive loglikelihood on the entire test set  On positive observations  ZIE models get similar results with the model
downweighting zero entries  ZIEcov performs slightly
better than ZIE  on all observations and has similar performance with ZIE  on positive observations  We have
also tried other negative weights     for EFEdz and
found   similar trend  smaller weight gives slightly better predictive loglikelihood on positive observations but
worse overall predictive loglikelihood 
Sensitiveness to data sparsity  We downsample positive observations of one common species  American
Robin  Figure   left  and test how the embedded vectors changes 
Speci cally  we randomly set positive
counts of American Robin to zero and keep only    
        of positive counts  For each   
we compared the embedded vectors learned from downsampled data with those learned from the original data 
The vectors are compared by their respectively induced
Poisson parameter   Let    be the index of American
Robin  then for each species    cid     the distribution of
the count of   given one American Robin has parameter
     softplus cid 
From the original and downsampled data  we get two embeded vectors and then have two Poisson parameters  orig
and  down
  The  rst measure of difference is the symmetric KL divergence  sKL  of the predictive probabilities of
presence calculated from the two   values with Poisson distribution 

       

 

 

     sKL cid   xj      orig

 

    xj      down

 

 cid 

 

     down

The second measure is the absolute difference of   values 
  These two measures are averaged
     orig
over all species 
Table   shows these measures calculated from different
models with       We can see that the embedding
model with exposure explanation is less sensitive to missing observations 
Exposure explanation  We also explore what the exposure

Figure   Three bird species in study  Left  American Robin  middle  Bald Eagle  right  Eastern ScreechOwl 

probability captures about zero observations  We check the
learned coef cients   of the species Bald Eagle  Figure  
middle  Its coef cient corresponding to the effort hours
of the observation is large  at the percentage of   of all
birds  It agrees with bird watching  since eagles are usually rare and need long time to be spotted  Another example is Eastern ScreechOwl  Figure   right  which is only
active at night 
Its two coef cients corresponding to the
observation time at hours   and hours   of the day
are the smallest among all species  With the learned coef 
 cients  the exposure probabilites are able distinguish the
generative process of zeros according to their observation
conditions and thus downweight zeros more correctly 

  Movie embedding from movie ratings

In this experiment  we study movie ratings and embed
movies as vectors  The MovieLens   dataset  Harper
  Konstan    consists of movie ratings from different
users  It is preprocessed in the same way as Rudolph et al 
 
translating ratings above   to the range   and
treating absent ratings and ratings no greater than   as zeros  We remove all movies with less than   ratings  Six
covariates  the age  the gender  and four profession categories  are extracted from each user and used as exposure
covariates 
For the ratings from the same person  the embedding model
   the rating of one movie given the ratings of all other
movies  The embedding distribution is the binomial distribution with   trials  The parameter   ranges over
      We run   fold cross validation on this dataset 
Performance comparison  In this result  ZIE  and ZIEcov performs much better than the two baselines in predictive performance on all entries  With zeroentries downweighted  EFEdz is able to predict nonzeros better than

ZeroIn ated Exponential Family Embeddings

 

EFE

EFEdz

Table   ZIE models improves predictive loglikelihood values on
movie rating data 
  all  
pos    
  all  
pos    
  all  
pos    

     
 
     
 
     
 

 
 
 

ZIEcov

ZIE 

Table   ZIE models improves the predictive loglikelihood on
Market data

EFE

EFEdz

ZIE 

 

 

 

 

all        
pos        
all        
pos        
all        
pos        

ZIEcov

ing status  to describe each user 
The embedding model  ts the count of one item given
counts of other items in the same shopping trip  All
embedding models use Poisson distribution as the conditional distribution and log softplus  as the link function  The covariates of each itemcontext pair are the covariates of the customer making the shopping trip  This
dataset is randomly split into   training set and   test set by
  The number   of embedding dimensions ranges over
     
Performance comparison  On this dataset  we compare
the performances by the predictive loglikelihood on the
test set  Table   shows the predictive loglikelihood of different models  ZIEcov gives the largest values of predictive loglikelihood of both type  The exposure covariates of
this dataset is informative  so the improvement of ZIEcov
is signi cant 

  Word embedding

In this subsection  we test word embedding with Wikipedia
documents  We take the  rst million documents of the
Wikipedia corpus prepared by Reese et al    The
words in these documents have been tagged by FreeLing
with partof speech  POS  tags  We keep the top  
frequent words as our vocabulary and remove all words not
in the vocabulary  The covariates of each word is from
the POS tag of the preceding word no matter the word is
removed or not  The original FreeLing POS tags has  
types  and we combine them into   larger types  such as
noun  verb  and adverb  The covariates are indicators of
these   types  The subset is further split into   training set
and   test set by  
On this dataset  we test embedding models with Bernoulli
distribution  ZIE  and our relaxed multinomial distribution
  ZIEm  ZIEcov and ZIEm cov use the tag type as the
exposure covariates  For Bernoulli distributions  zero entries are downweighted by weight    the target word
versus   negative words  For multinomial distribution  we
randomly sample   words as the set   for each wordcontext
pair  We use   context window size of   so the context of
  word position is the two words before and after the position  Note that word vec with CBOW and NEG training

Figure   Exposure probability versus rating frequency  Movies
with high rating frequency and movies with low ratings tend to
get high exposure probability 

ZIE  and ZIEcov  but it is much less capable to predict
which entries are zero  ZIEcov slightly improves over
ZIE  in predictive loglikelihood of both types 
Exposure probability versus rating frequency  We investigate the exposure probability learned without covariates  We plot the exposure probability of movie versus its
frequency of ratings in Fig  Each dot represents   movie 
its position indicating its exposure probability and rating
frequency  and its color being the average of its positive
ratings  The exposure probability is from the vector   
This  gure shows that the exposure probability generally
correlates with the popularity of the movie  However  some
movies with low rating frequency are given high exposure
probabilities  These movies have low ratings  which can
and should be explained by the embedding component  For
example  the movie  Money Train  whose average rating is at the percentage of   among all movies  has the
largest ratio of exposure probability over rating frequency 
The movies with high rating frequency get high exposure
probabilities no matter their average rating is high or not 

  Product embedding from market baskets

In this experiment  we embed products in grocery stores
into the vector space  The Market dataset  Bronnenberg
et al    consists of purchase records of anonymous
households in grocery stores  Each record is for   single shopping trip and contains the costumer ID  respective
counts of items purchased  and other information  From
the dataset  we take   related covariates  such as income
range  working hours  age group  marital status  and smok 

 Rating frequency Exposure probabilityExposure probability versus rating frequency of movies ZeroIn ated Exponential Family Embeddings

EFE

Table   Predictive loglikelihood per document 
 
       
       
       

ZIEcov

ZIE 

Table   Performance of different models on word similarity tasks 

MEN
WS 

SIMLEX 

EFE
 
 
 

ZIE 
 
 
 

ZIEcov
 
 
 

ZIEm 
 
 
 

ZIEm cov GloVe
 
 
 

 
 
 

Table   Nearest words  nd by three embedding models 

EFE

ZIE 

ZIEcov

battle

combat  
defeat  
invasion  

 re  

battles  
assault  
battles  
assault  
attack  

philosophy

sociology  
theology  
tradition  
religion  
society  
theology  
principles  
religion  
theology  

novel

book  
novels  
poem  
story  
book  
novels  
novels  
story  
fantasy  

class

division  

   

 eld  

division  
family  
classes  
classes  
rank  
grade  

english

welsh  
french  
spanish  
french  
swedish  

irish  
french  
spanish  
swedish  

bible

hebrew  
study  
biblical  
poetry  
hebrew  
dictionary  
biblical  
hebrew  
texts  

 Mikolov et al      is generally the same as EFE when
it takes some settings related to implementation details 
Performance comparison 
Tab    shows the average predictive loglikelihood per document on the test set by Bernoulli embedding models 
The embedding model gets the best predictive performance
when it uses POS tags to    the exposure probability 
We also compare all models on three benchmark datasets
of word similarities  MEN  Baroni et al    WS 
 Finkelstein et al    and SIMLEX   Hill et al 
  and show the results in Table   with each entry being Spearman   correlation of embedding similarities and
humanrated similarities  The code is from the repository  
constructed by Jastrzebski et al    The last column as
  reference shows the performance of GloVe  Pennington
et al    word vectors with dimension   trained on
  billion documents  The results on the three tasks shows
that ZIE models perform slightly better than the baseline
model 
Word relation learned with exposure explanation  To
better understand how the exposure model affects the embedded vectors of words  we check cosine similarities of
word vectors   learned by different models  Table   shows
examples of similar words discovered by Bernoulli embedding models 
We have two interesting observations  First  word embedding with zeroin ation often gives lower similarity scores
than the one without  This means the embedded word vectors of ZIE models are more spread out in the space  Second  the distance between   noun word and its plural form
often have shorter distance with the vectors learned by ZIE 

 https github com kudkudak wordembeddings benchmarks

cov  The difference of   word and its plural form are more
in grammar than semantics  The POS tags can partially
explain the grammatical difference  For example  numbers
 tagged as number  often go before   plural instead of   singular word  In such cases  the singular word as   negative
example will be downweighted  and thus the embedding
component is more likely to treat the singular word and its
plural as similar words 

  Summary
In this work  we have proposed zeroin ated exponential
family embedding  With the exposure indicator explaining
unrelated zero observations  the real embedding component
is able to focus more on observations from item interactions  so the embedded vectors can better represent items in
terms of item relations  We have investigated ZIE for two
types of embedding models 
the observation being from
one and multiple items  We have also developed the inference algorithms for different embedding models with zero
in ation  Experiment results indicate that ZIE improves
the predictive loglikelihood of the data  Qualitative analysis shows that the embedded vectors from the ZIE model
have better quality than the vectors learned with the basic
embedding model 

Acknowledgements
This work is supported by NSF IIS  ONR
   DARPA PPAML FA 
DARPA SIMPLEX      and the Alfred
   Sloan Foundation  Thank all reviewers of the paper
for their feedback  Thank Francisco       Ruiz  Maja   
Rudolph  Kriste Krstovski  and Aonan Zhang for helpful
comments and discussion 

ZeroIn ated Exponential Family Embeddings

References
Baroni     Dinu     and Kruszewski     Don   count  predict    systematic comparison of contextcounting vs 
contextpredicting semantic vectors  In Proceedings of
the  nd Annual Meeting of the Association for Computational Linguistics  pp    Association for Computational Linguistics   

Bengio     Ducharme     Vincent     and Janvin      
neural probabilistic language model  Journal of Machine
Learning Research    March  

Bronnenberg        Kruger        and Mela       
Database paper the IRI marketing data set  Marketing
Science     

Duchi     Hazan     and Singer     Adaptive subgradient
methods for online learning and stochastic optimization 
Journal of Machine Learning Research   
July  

Finkelstein     Gabrilovich     Matias     Rivlin    
Solan     Wolfman     and Ruppin     Placing search
in context  the concept revisited  ACM Transactions on
Information Systems     

Harper        and Konstan        The MovieLens datasets 
history and context  ACM Transactions on Interactive
Intelligent Systems  TiiS     

Harris     Distributional structure  Word   

 

Hill     Reichart     and Korhonen     SimLex  evaluating semantic models with  genuine  similarity estimation  arXiv preprint arXiv   cs CL   

Hoffman        Blei        Wang     and Paisley    
Journal of Machine

Stochastic variational inference 
Learning Research     

Hu     Koren     and Volinsky     Collaborative  ltering
for implicit feedback datasets  In The  th IEEE International Conference on Data Mining  pp     

Jastrzebski     Le sniak     and Czarnecki        How
to evaluate word embeddings  On importance of data
ef ciency and simple supervised tasks  arXiv preprint
arXiv   cs CL   

Lambert     Zeroin ated poisson regression  with an application to defects in manufacturing  Technometrics   
   

Liang     Charlin     McInerney     and Blei        ModIn Proceedeling user exposure in recommendation 
ings of the  th International Conference on World Wide
Web  pp     

Mikolov     Chen     Corrado     and Dean     Ef cient
estimation of word representations in vector space  arXiv
preprint arXiv cs CL     

Mikolov     Sutskever     Chen     Corrado     and
Dean     Distributed representations of words and
phrases and their compositionality  In Burges          
Bottou     Welling     Ghahramani     and Weinberger         eds  Advances in Neural Information
Processing Systems   pp    Curran Associates  Inc     

Mitchell        and Beauchamp        Bayesian variable selection in linear regression  Journal of the American Statistical Association     

Mnih     and Hinton        Three new graphical models
for statistical language modelling  In Proceedings of the
 th International Conference on Machine Learning  pp 
   

Munson        Webb     Sheldon     Fink    
Hochachka        Iliff     Riedewald     Sorokina 
   Sullivan     Wood     and Kelling     The eBird
reference dataset  version    

Pennington     Socher     and Manning        GloVe 
In Empirical
global vectors for word representation 
Methods in Natural Language Processing  EMNLP  pp 
   

Reese     Boleda     Cuadros     Padr       and Rigau 
   Wikicorpus    wordsense disambiguated multilingual wikipedia corpus  In Proceedings of the  th Language Resources and Evaluation Conference  pp   
   

Rendle     Freudenthaler     Gantner     and SchmidtThieme     BPR  Bayesian personalized ranking from
In Proceedings of the TwentyFifth
implicit feedback 
Conference on Uncertainty in Arti cial Intelligence  pp 
   

Rudolph     Ruiz     Mandt     and Blei        Exponential family embeddings 
In Lee        Sugiyama 
   Luxburg        Guyon     and Garnett      eds 
Advances in Neural Information Processing Systems  
pp    Curran Associates  Inc   

Rumelhart        Hinton        and Williams        Learning representations by backpropagating errors  In Anderson        and Rosenfeld      eds  Neurocomputing 
Foundations of Research  pp    MIT Press  Cambridge  MA  USA   

