ADistributionalPerspectiveonReinforcementLearningMarcG Bellemare WillDabney   emiMunos AbstractInthispaperweargueforthefundamentalimportanceofthevaluedistribution thedistributionoftherandomreturnreceivedbyareinforcementlearningagent Thisisincontrasttothecommonapproachtoreinforcementlearningwhichmodelstheexpectationofthisreturn orvalue Althoughthereisanestablishedbodyofliteraturestudyingthevaluedistribution thusfarithasalwaysbeenusedforaspeci cpurposesuchasimplementingriskawarebehaviour Webeginwiththeoreticalresultsinboththepolicyevaluationandcontrolsettings exposingasigni cantdistributionalinstabilityinthelatter WethenusethedistributionalperspectivetodesignanewalgorithmwhichappliesBellman sequationtothelearningofapproximatevaluedistri butions WeevaluateouralgorithmusingthesuiteofgamesfromtheArcadeLearningEnvironment Weobtainbothstateof theartre sultsandanecdotalevidencedemonstratingtheimportanceofthevaluedistributioninapproximatereinforcementlearning Finally wecombinetheoreticalandempiricalevidencetohigh lightthewaysinwhichthevaluedistributionimpactslearningintheapproximatesetting IntroductionOneofthemajortenetsofreinforcementlearningstatesthat whennototherwiseconstrainedinitsbehaviour anagentshouldaimtomaximizeitsexpectedutilityQ orvalue Sutton Barto Bellman sequationsuccintlydescribesthisvalueintermsoftheexpectedrewardandexpectedoutcomeoftherandomtransition               ER     EQ     Inthispaper weaimtogobeyondthenotionofvalueandargueinfavourofadistributionalperspectiveonreinforce Equalcontribution DeepMind London UK Correspondenceto MarcG Bellemare bellemare google com Proceedingsofthe thInternationalConferenceonMachineLearning Sydney Australia PMLR Copyright bytheauthor   mentlearning Speci cally themainobjectofourstudyistherandomreturnZwhoseexpectationisthevalueQ Thisrandomreturnisalsodescribedbyarecursiveequation butoneofadistributionalnature                     ThedistributionalBellmanequationstatesthatthedistributionofZischaracterizedbytheinteractionofthreerandomvariables therewardR thenextstateaction     anditsrandomreturnZ     Byanalogywiththewellknowncase wecallthisquantitythevaluedistribution AlthoughthedistributionalperspectiveisalmostasoldasBellman sequationitself Jaquette Sobel White inreinforcementlearningithasthusfarbeensubordinatedtospeci cpurposes tomodelparametricuncertainty Deardenetal todesignrisksensitiveal gorithms Morimuraetal     orfortheoreticalanalysis Azaretal Lattimore Hutter Bycontrast webelievethevaluedistributionhasacentralroletoplayinreinforcementlearning ContractionofthepolicyevaluationBellmanoperator BasingourselvesonresultsbyR osler weshowthat fora xedpolicy theBellmanoperatorovervaluedistributionsisacontractioninamaximalformoftheWasserstein alsocalledKantorovichorMallows metric Ourparticularchoiceofmetricmatters thesameoperatorisnotacontractionintotalvariation KullbackLeiblerdivergence orKolmogorovdistance Instabilityinthecontrolsetting WewilldemonstrateaninstabilityinthedistributionalversionofBellman soptimalityequation incontrasttothepolicyevaluationcase Speci cally althoughtheoptimalityoperatorisacontractioninexpectedvalue matchingtheusualoptimalityresult itisnotacontractioninanymetricoverdistributions Theseresultsprovideevidenceinfavouroflearningalgorithmsthatmodeltheeffectsofnonstationarypolicies Betterapproximations Fromanalgorithmicstandpoint therearemanybene tstolearninganapproximatedistributionratherthanitsapproximateexpectation ThedistributionalBellmanoperatorpreservesmultimodalityinvaluedistributions whichwebelieveleadstomorestablelearning Approximatingthefulldistributionalsomitigatestheeffectsoflearningfromanonstationarypolicy Asawhole ADistributionalPerspectiveonReinforcementLearningwearguethatthisapproachmakesapproximatereinforcementlearningsigni cantlybetterbehaved Wewillillustratethepracticalbene tsofthedistributionalperspectiveinthecontextoftheArcadeLearningEnvironment Bellemareetal BymodellingthevaluedistributionwithinaDQNagent Mnihetal weobtainconsiderablyincreasedperformanceacrossthegamutofbenchmarkAtari games andinfactachievestateof theartperformanceonanumberofgames OurresultsechothoseofVenessetal whoobtainedextremelyfastlearningbypredictingMonteCarloreturns Fromasupervisedlearningperspective learningthefullvaluedistributionmightseemobvious whyrestrictourselvestothemean Themaindistinction ofcourse isthatinoursettingtherearenogiventargets Instead weuseBellman sequationtomakethelearningprocesstractable wemust asSutton Barto putit learnaguessfromaguess Itisourbeliefthatthisguessworkultimatelycarriesmorebene tsthancosts SettingWeconsideranagentinteractingwithanenvironmentinthestandardfashion ateachstep theagentselectsanactionbasedonitscurrentstate towhichtheenvironmentrespondswitharewardandthenextstate Wemodelthisinteractionasatime homogeneousMarkovDecisionProcess         Asusual XandAarerespectivelythestateandactionspaces PisthetransitionkernelP     isthediscountfactor andRistherewardfunction whichinthisworkweexplicitlytreatasarandomvariable Astationarypolicy mapseachstatex XtoaprobabilitydistributionovertheactionspaceA Bellman sEquationsThereturnZ isthesumofdiscountedrewardsalongtheagent strajectoryofinteractionswiththeenvironment ThevaluefunctionQ ofapolicy describestheexpectedreturnfromtakingactiona Afromstatex   thenactingaccordingto       EZ       Xt tR xt at xt   xt at at xt         FundamentaltoreinforcementlearningistheuseofBellman sequation Bellman todescribethevaluefunction       ER     EP       Inreinforcementlearningwearetypicallyinterestedinactingsoastomaximizethereturn Themostcommonap   ZR     ZP             Figure AdistributionalBellmanoperatorwithadeterministicrewardfunction   Nextstatedistributionunderpolicy   Discountingshrinksthedistributiontowards   Therewardshiftsit and   Projectionstep Section proachfordoingsoinvolvestheoptimalityequationQ     ER     EPmaxa AQ     Thisequationhasaunique xedpointQ theoptimalvaluefunction correspondingtothesetofoptimalpolicies isoptimalifEa       maxaQ     WeviewvaluefunctionsasvectorsinRX   andtheexpectedrewardfunctionasonesuchvector Inthiscontext theBellmanoperatorT andoptimalityoperatorTareT       ER     EP       TQ     ER     EPmaxa AQ     TheseoperatorsareusefulastheydescribetheexpectedbehaviourofpopularlearningalgorithmssuchasSARSAandQLearning Inparticulartheyarebothcontractionmappings andtheirrepeatedapplicationtosomeinitialQ convergesexponentiallytoQ orQ respectively Bertsekas Tsitsiklis TheDistributionalBellmanOperatorsInthispaperwetakeawaytheexpectationsinsideBellman sequationsandconsiderinsteadthefulldistributionoftherandomvariableZ Fromhereon wewillviewZ asamappingfromstateactionpairstodistributionsoverreturns andcallitthevaluedistribution Our rstaimistogainanunderstandingofthetheoreticalbehaviourofthedistributionalanaloguesoftheBellmanoperators inparticularinthelesswellunderstoodcontrolsetting Thereaderstrictlyinterestedinthealgorithmiccontributionmaychoosetoskipthissection DistributionalEquationsItwillsometimesbeconvenienttomakeuseoftheprobabilityspace   Pr ThereaderunfamiliarwithmeaADistributionalPerspectiveonReinforcementLearningsuretheorymaythinkof asthespaceofallpossibleoutcomesofanexperiment Billingsley WewillwritekukptodenotetheLpnormofavectoru RXfor   thesameappliestovectorsinRX   TheLpnormofarandomvectorU RX orRX   isthenkUkp cid   cid kU kpp cid cid   andforp wehavekUk supkU   wewillomitthedependencyon wheneverunambiguous Wewilldenotethec     ofarandomvariableUbyFU   Pr     anditsinversec     byF     inf   FU     AdistributionalequationUD VindicatesthattherandomvariableUisdistributedaccordingtothesamelawasV Withoutlossofgenerality thereadercanunderstandthetwosidesofadistributionalequationasrelatingthedistributionsoftwoindependentrandomvariables DistributionalequationshavebeenusedinreinforcementlearningbyEngeletal Morimuraetal   amongothers andinoperationsresearchbyWhite TheWassersteinMetricThemaintoolforouranalysisistheWassersteinmetricdpbetweencumulativedistributionfunctions seee   Bickel Freedman whereitiscalledtheMallowsmetric ForF Gtwoc   fsoverthereals itisde nedasdp     infU VkU Vkp wherethein mumistakenoverallpairsofrandomvariables     withrespectivecumulativedistributionsFandG Thein mumisattainedbytheinversec     transformofarandomvariableUuniformlydistributedon dp     kF       kp Forp thisismoreexplicitlywrittenasdp     cid   cid cid         cid cid pdu cid   GiventworandomvariablesU Vwithc   fsFU FV wewillwritedp     dp FU FV Wewill nditconvenienttocon atetherandomvariablesunderconsiderationwiththeirversionsundertheinf writingdp     infU VkU Vkp wheneverunambiguous webelievethegreaterlegibilityjusti esthetechnicalinaccuracy Finally weextendthismetrictovectorsofrandomvariables suchasvaluedistributions usingthecorrespondingLpnorm ConsiderascalaraandarandomvariableAindependentofU   Themetricdphasthefollowingproperties dp aU aV   dp       dp         dp       dp AU AV kAkpdp       Wewillneedthefollowingadditionalproperty whichmakesnoindependenceassumptionsonitsvariables Itsproof andthatoflaterresults isgivenintheappendix Lemma Partitionlemma LetA   beasetofrandomvariablesdescribingapartitionof     Ai andforany thereisexactlyoneAiwithAi LetU Vbetworandomvariables Thendp cid     cid Xidp AiU AiV LetZdenotethespaceofvaluedistributionswithboundedmoments FortwovaluedistributionsZ   ZwewillmakeuseofamaximalformoftheWassersteinmetric dp     supx adp             Wewilluse dptoestablishtheconvergenceofthedistributionalBellmanoperators Lemma dpisametricovervaluedistributions PolicyEvaluationInthepolicyevaluationsetting Sutton Barto weareinterestedinthevaluefunctionV associatedwithagivenpolicy TheanaloguehereisthevaluedistributionZ InthissectionwecharacterizeZ andstudythebehaviourofthepolicyevaluationoperatorT WeemphasizethatZ describestheintrinsicrandomnessoftheagent sinteractionswithitsenvironment ratherthansomemeasureofuncertaintyabouttheenvironmentitself WeviewtherewardfunctionasarandomvectorR   andde nethetransitionoperatorP   ZP                           whereweusecapitalletterstoemphasizetherandomnatureofthenextstate actionpair     Wede nethedistributionalBellmanoperatorT   ZasT                       WhileT bearsasurfaceresemblancetotheusualBellmanoperator itisfundamentallydifferent Inparticular threesourcesofrandomnessde nethecompounddistributionT   ADistributionalPerspectiveonReinforcementLearninga TherandomnessintherewardR   TherandomnessinthetransitionP andc ThenextstatevaluedistributionZ     Inparticular wemaketheusualassumptionthatthesethreequantitiesareindependent Inthissectionwewillshowthat isacontractionmappingwhoseunique xedpointistherandomreturnZ CONTRACTIONIN dpConsidertheprocessZk   Zk startingwithsomeZ   Wemayexpectthelimitingexpectationof Zk toconvergeexponentiallyquickly asusual toQ Aswenowshow theprocessconvergesinastrongersense   isacontractionin dp whichimpliesthatallmomentsalsoconvergeexponentiallyquickly Lemma     Zisa contractionin dp UsingLemma weconcludeusingBanach   xedpointtheoremthatT hasaunique xedpoint Byinspection this xedpointmustbeZ asde nedin Asweassumeallmomentsarebounded thisissuf cienttoconcludethatthesequence Zk convergestoZ in dpfor   Toconclude weremarkthatnotalldistributionalmetricsareequal forexample Chung Sobel haveshownthatT isnotacontractionintotalvariationdistance SimilarresultscanbederivedfortheKullback LeiblerdivergenceandtheKolmogorovdistance CONTRACTIONINCENTEREDMOMENTSObservethatd     andmoregenerally dp relatestoacouplingC     inthesensethatd             cid   cid cid EC cid Asaresult wecannotdirectlyused toboundthevariancedifference                   However   isinfactacontractioninvariance Sobel seealsoappendix Ingeneral   isnotacontractioninthepthcenteredmoment   butthecenteredmomentsoftheiterates Zk stillconvergeexponentiallyquicklytothoseofZ theproofextendstheresultofR osler ControlThusfarwehaveconsidereda xedpolicy andstudiedthebehaviourofitsassociatedoperatorT Wenowsetouttounderstandthedistributionaloperatorsofthecontrolsetting whereweseekapolicy thatmaximizesvalue andthecorrespondingnotionofanoptimalvaluedistribution Aswiththeoptimalvaluefunction thisnotionisintimatelytiedtothatofanoptimalpolicy However whilealloptimalpoliciesattainthesamevalueQ inourcaseadif cultyarises ingeneraltherearemanyoptimalvaluedistributions InthissectionweshowthatthedistributionalanalogueoftheBellmanoptimalityoperatorconverges inaweaksense tothesetofoptimalvaluedistributions However thisoperatorisnotacontractioninanymetricbetweendistributions andisingeneralmuchmoretemperamentalthanthepolicyevaluationoperators Webelievetheconvergenceissuesweoutlinehereareasymptomoftheinherentinstabilityofgreedyupdates ashighlightedbye   Tsitsiklis andmostrecentlyHarutyunyanetal Let bethesetofoptimalpolicies Webeginbycharacterizingwhatwemeanbyanoptimalvaluedistribution De nition Optimalvaluedistribution Anoptimalvaluedistributionisthev   ofanoptimalpolicy ThesetofoptimalvaluedistributionsisZ   Themappingfrompoliciestovaluedistributions   iscontinuous Asaresult   inheritsmanyofthepropertiesof itisconvexand in nitestateactionspaces compact WeemphasizethatnotallvaluedistributionswithexpectationQ areoptimal theymustmatchthefulldistributionofthereturnundersomeoptimalpolicy De nition Agreedypolicy forZ ZmaximizestheexpectationofZ ThesetofgreedypoliciesforZisGZ Xa     EZ     maxa AEZ     RecallthattheexpectedBellmanoptimalityoperatorTisTQ     ER     EPmaxa AQ     Themaximizationatx correspondstosomegreedypolicy Althoughthispolicyisimplicitin wecannotignoreitinthedistributionalsetting WewillcalladistributionalBellmanoptimalityoperatoranyoperatorTwhichimplementsagreedyselectionrule     TZ   Zforsome GZ Asinthepolicyevaluationsetting weareinterestedinthebehaviouroftheiteratesZk TZk     Our rstresultistoassertthatEZkbehavesasexpected Lemma LetZ     ThenkETZ ETZ   kEZ EZ   andinparticularEZk   exponentiallyquickly ByinspectingLemma wemightexpectthatZkconvergesquicklyin dptosome xedpointinZ Unfortunately convergenceisneitherquicknorassuredtoreacha xedpoint Infact thebestwecanhopeforispointwiseconvergence noteventothesetZ buttothelargersetofnonstationaryoptimalvaluedistributions ADistributionalPerspectiveonReinforcementLearningDe nition AnonstationaryoptimalvaluedistributionZ isthevaluedistributioncorrespondingtoasequenceofoptimalpolicies Thesetofn       isZ Theorem Convergenceinthecontrolsetting LetXbemeasurableandsupposethatAis nite Thenlimk infZ   dp Zk               IfXis nite thenZkconvergestoZ uniformly Furthermore ifthereisatotalordering on suchthatforanyZ   TZ     with GZ GZ ThenThasaunique xedpointZ   ComparingTheorem toLemma revealsasigni cantdifferencebetweenthedistributionalframeworkandtheusualsettingofexpectedreturn WhilethemeanofZkconvergesexponentiallyquicklytoQ itsdistributionneednotbeaswellbehaved Toemphasizethisdifference wenowprovideanumberofnegativeresultsconcerningT Proposition TheoperatorTisnotacontraction Considerthefollowingexample Figure left Therearetwostates   andx auniquetransitionfromx tox fromx actiona yieldsnoreward whiletheoptimalactiona yields or withequalprobability Bothactionsareterminal Thereisauniqueoptimalpolicyandthereforeaunique xedpointZ NowconsiderZasgiveninFigure right anditsdistancetoZ                     wherewemadeuseofthefactthatZ   everywhereexceptat     WhenweapplyTtoZ however thegreedyactiona isselectedandTZ         Butd TZ TZ   TZ       forasuf cientlysmall Thisshowsthattheundiscountedupdateisnotanonexpansion   TZ TZ       With thesameproofshowsitisnotacontraction Usingamoretechnicallyinvolvedargument wecanextendthisresulttoanymetricwhichseparatesZandTZ Proposition Notalloptimalityoperatorshavea xedpointZ TZ Toseethis considerthesameexample nowwith andagreedyoperatorTwhichbreakstiesbypickinga ifZ   anda otherwise ThenthesequenceTZ         alternatesbetweenZ     andZ                                         TZ Figure UndiscountedtwostateMDPforwhichtheoptimalityoperatorTisnotacontraction withexample Theentriesthatcontributeto       and   TZ   arehighlighted Proposition ThatThasa xedpointZ TZ isinsuf cienttoguaranteetheconvergenceof Zk toZ Theorem paintsaratherbleakpictureofthecontrolsetting Itremainstobeseenwhetherthedynamicaleccentricieshighlightedhereactuallyariseinpractice Oneopenquestioniswhethertheoreticallymorestablebehaviourcanbederivedusingstochasticpolicies forexamplefromconservativepolicyiteration Kakade Langford ApproximateDistributionalLearningInthissectionweproposeanalgorithmbasedonthedistributionalBellmanoptimalityoperator Inparticular thiswillrequirechoosinganapproximatingdistribution AlthoughtheGaussiancasehaspreviouslybeenconsidered Morimuraetal   Tamaretal tothebestofourknowledgewearethe rsttousearichclassofparametricdistributions ParametricDistributionWewillmodelthevaluedistributionusingadiscretedistributionparametrizedbyN NandVMIN VMAX   andwhosesupportisthesetofatoms zi VMIN           VMAX VMINN Inasense theseatomsarethe canonicalreturns ofourdistribution Theatomprobabilitiesaregivenbyaparametricmodel     RNZ     ziw   pi             Pje       Thediscretedistributionhastheadvantagesofbeinghighlyexpressiveandcomputationallyfriendly seee   VandenOordetal ProjectedBellmanUpdateUsingadiscretedistributionposesaproblem theBellmanupdateTZ andourparametrizationZ almostalwayshavedisjointsupports FromtheanalysisofSection itwouldseemnaturaltominimizetheWassersteinmetric viewedasaloss betweenTZ andZ whichisalsoADistributionalPerspectiveonReinforcementLearningconvenientlyrobusttodiscrepanciesinsupport However asecondissuepreventsthis inpracticewearetypicallyrestrictedtolearningfromsampletransitions whichisnotpossibleundertheWassersteinloss seeProp andtoyresultsintheappendix Instead weprojectthesampleBellmanupdate TZ ontothesupportofZ Figure Algorithm effectivelyreducingtheBellmanupdatetomulticlassclassi cation Let bethegreedypolicyw     EZ Givenasampletransition         wecomputetheBellmanupdate Tzj   zjforeachatomzj thendistributeitsprobabilitypj     totheimmediateneighboursof Tzj Theithcomponentoftheprojectedupdate TZ     is TZ         Xj Tzj VMAXVMIN zi   pj     where baboundsitsargumentintherange     Asisusual weviewthenextstatedistributionasparametrizedbya xedparameter ThesamplelossLx   isthecrossentropytermoftheKLdivergenceDKL TZ     kZ     whichisreadilyminimizede   usinggradientdescent Wecallthischoiceofdistributionandlossthecategoricalalgorithm WhenN asimpleoneparameteralternativeis TZ       TZ     VMIN   wecallthistheBernoullialgorithm Wenotethat whilethesealgorithmsappearunrelatedtotheWassersteinmetric recentwork Bellemareetal hintsatadeeperconnection Algorithm CategoricalAlgorithminputAtransitionxt at rt xt     xt   Pizipi xt     argmaxaQ xt   mi     forj   do Computetheprojectionof Tzjontothesupport zi Tzj rt tzj VMAXVMINbj Tzj VMIN   bj     bbjc   dbje Distributeprobabilityof Tzjml ml pj xt     bj mu mu pj xt   bj   endforoutput Pimilogpi xt at Crossentropyloss EvaluationonAtari GamesTounderstandtheapproachinacomplexsetting weappliedthecategoricalalgorithmtogamesfromtheAr Algorithm computesthisprojectionintimelinearinN cadeLearningEnvironment ALE Bellemareetal WhiletheALEisdeterministic stochasticitydoesoccurinanumberofguises fromstatealiasing learningfromanonstationarypolicy and fromapproximationerrors Weused vetraininggames Fig and testinggames Forourstudy weusetheDQNarchitecture Mnihetal butoutputtheatomprobabilitiespi     insteadofactionvalues andchoseVMAX VMIN frompreliminaryexperimentsoverthetraininggames WecalltheresultingarchitectureCategoricalDQN Wereplacethesquaredloss               byLx   andtrainthenetworktominimizethisloss AsinDQN weuseasimple greedypolicyovertheexpectedactionvalues weleaveasfutureworkthemanywaysinwhichanagentcouldselectactionsonthebasisofthefulldistribution TherestofourtrainingregimematchesMnihetal   includingtheuseofatargetnetworkfor Figure illustratesthetypicalvaluedistributionsweobservedinourexperiments Inthisexample threeactions thoseincludingthebuttonpress leadtotheagentreleasingitslasertooearlyandeventuallylosingthegame Thecorrespondingdistributionsre ectthis theyassignasigni cantprobabilityto theterminalvalue Thesafeactionshavesimilardistributions LEFT whichtrackstheinvaders movement isslightlyfavoured Thisexamplehelpsexplainwhyourapproachissosuccessful thedistributionalupdatekeepsseparatedthelow value losing eventfromthehighvalue survival event ratherthanaveragethemintoone unrealizable expectation Onesurprisingfactisthatthedistributionsarenotconcentratedononeortwovalues inspiteoftheALE sdeterminism butareoftenclosetoGaussians Webelievethisisduetoourdiscretizingthediffusionprocessinducedby VaryingtheNumberofAtomsWebeganbystudyingouralgorithm sperformanceonthetraininggamesinrelationtothenumberofatoms Figure Forthisexperiment weset Fromthedata itisclearthatusingtoofewatomscanleadtopoorbehaviour andthatmorealwaysincreasesperformance thisisnotimmediatelyobviousaswemayhaveexpectedtosaturatethenetworkcapacity Thedifferenceinperformancebetweenthe atomversionandDQNisparticularlystriking thelatterisoutperformedinall vegames andinSEAQUESTweattainstateof theartperformance Asanadditionalpointofthecomparison thesingleparameterBernoullial gorithmperformsbetterthanDQNin gamesoutof andismostnotablymorerobustinASTERIX ForN ourTensorFlowimplementationtrainsatroughly ofDQN sspeed Video http youtu be yFBwyPuO Vg ADistributionalPerspectiveonReinforcementLearningASTERIXQ BERTBREAKOUTPONGSEAQUESTCategorical DQN  returns  returns  returns  returnsDQNBernoulliAverage ScoreTraining Frames  millions Dueling Arch Figure CategoricalDQN Varyingnumberofatomsinthediscretedistribution Scoresaremovingaveragesover millionframes ReturnProbabilityRightLeftRight LaserLeft LaserLaserNoopFigure LearnedvaluedistributionduringanepisodeofSPACEINVADERS Differentactionsareshadeddifferentcolours Returnsbelow whichdonotoccurinSPACEINVADERS arenotshownhereastheagentassignsvirtuallynoprobabilitytothem Oneinterestingoutcomeofthisexperimentwasto ndoutthatourmethoddoespickuponstochasticity PONGexhibitsintrinsicrandomness theexacttimingoftherewarddependsoninternalregistersandistrulyunobserv able Weseethisclearlyre ectedintheagent sprediction Figure over veconsecutiveframes thevaluedistributionshowstwomodesindicatingtheagent sbeliefthatithasyettoreceiveareward Interestingly sincetheagent sstatedoesnotincludepastrewards itcannotevenextinguishthepredictionafterreceivingthereward explainingtherelativeproportionsofthemodes Stateof theArtResultsTheperformanceofthe atomagent fromhereonwards   onthetraininggames presentedinthelastsection isparticularlyremarkablegiventhatitinvolvednoneoftheotheralgorithmicideaspresentinstateof theartagents Wenextaskedwhetherincorporatingthemostcommonhyperparameterchoice namelyasmallertraining couldleadtoevenbetterresults Speci cally weset insteadof furthermore every millionframes weevaluateouragent sperformancewith WecompareouralgorithmtoDQN DoubleDQN vanHasseltetal theDuelingarchitecture Wangetal andPrioritizedReplay Schauletal comparingthebestevaluationscoreachievedduringtraining WeseethatC signi cantlyoutperformstheseotheralgorithms Figures and Infact   surpassesthecurrentstateof theartbyalargemargininanumberofgames mostnotablySEAQUEST Oneparticularlystrikingfactisthealgorithm sgoodperformanceonsparserewardgames forexampleVENTUREandPRIVATEEYE Thissuggeststhatvaluedistributionsarebetterabletopropagaterarelyoccurringevents Fullresultsareprovidedintheappendix Wealsoincludeintheappendix Figure acomparison averagedover seeds showingthenumberofgamesinwhichC strainingperformanceoutperformsfullytrainedDQNandhumanplayers Theseresultscontinuetoshowdramaticimprovements andaremorerepresentativeofanagent saverageperformance Within millionframes   hasoutperformedafullytrainedDQNagenton outof games Thissuggeststhatthefull milliontrainingframes anditsensuingcomputationalcost areunnecessaryforevaluatingreinforcementlearningalgorithmswithintheALE ThemostrecentversionoftheALEcontainsastochasticexecutionmechanismdesignedtowardagainsttrajectoryover tting Speci cally oneachframetheenvironmentrejectstheagent sselectedactionwithprobabilityp AlthoughDQNismostlyrobusttostochasticexecution thereareafewgamesinwhichitsperformanceisreduced OnascorescalenormalizedwithrespecttotherandomandDQNagents   obtainsmeanandmedianscoreimprovementsof and respectively con rmingthebene tsofC beyondthedeterministicsetting ADistributionalPerspectiveonReinforcementLearningFigure IntrinsicstochasticityinPONG MeanMedian     DQNDQN DDQN DUEL PRIOR PR DUEL UNREAL   Figure Meanandmedianscoresacross Atarigames measuredaspercentagesofhumanbaseline     Nairetal Figure Percentageimprovement pergame ofC overDoubleDQN computedusingvanHasseltetal smethod DiscussionInthisworkwesoughtamorecompletepictureofreinforcementlearning onethatinvolvesvaluedistributions WefoundthatlearningvaluedistributionsisapowerfulnotionthatallowsustosurpassmostgainspreviouslymadeonAtari withoutfurtheralgorithmicadjustments Whydoeslearningadistributionmatter Itissurprisingthat whenweuseapolicywhichaimstomaximizeexpectedreturn weshouldseeanydifferenceinperformance Thedistinctionwewishtomakeisthatlearningdistributionsmattersinthepresenceofapproximation Wenowoutlinesomepossiblereasons Reducedchattering OurresultsfromSection highlightedasigni cantinstabilityintheBellmanoptimalityoperator Whencombinedwithfunctionapproximation thisinstabilitymaypreventthepolicyfromconverging whatGordon calledchattering Webelievethegradientbasedcategoricalalgorithmisabletomitigatetheseeffectsbyeffectivelyaveragingthedifferentdistri TheUNREALresultsarenotaltogethercomparable astheyweregeneratedintheasynchronoussettingwithpergamehyperparametertuning Jaderbergetal butions similartoconservativepolicyiteration Kakade Langford Whilethechatteringpersists itisintegratedtotheapproximatesolution Statealiasing Eveninadeterministicenvironment statealiasingmayresultineffectivestochasticity McCallum forexample showedtheimportanceofcouplingrepresentationlearningwithpolicylearninginpartiallyobservabledomains WesawanexampleofstatealiasinginPONG wheretheagentcouldnotexactlypredicttherewardtiming Again byexplicitlymodellingtheresultingdistributionweprovideamorestablelearningtarget Arichersetofpredictions Arecurringthemeinarti cialintelligenceistheideaofanagentlearningfromamultitudeofpredictions Caruana Utgoff Stracuzzi Suttonetal Jaderbergetal Thedistributionalapproachnaturallyprovidesuswitharichsetofauxiliarypredictions namely theprobabilitythatthereturnwilltakeonaparticularvalue Unlikepreviouslyproposedapproaches however theaccuracyofthesepredictionsistightlycoupledwiththeagent sperformance Frameworkforinductivebias Thedistributionalperspectiveonreinforcementlearningallowsamorenaturalframeworkwithinwhichwecanimposeassumptionsaboutthedomainorthelearningproblemitself Inthisworkweuseddistributionswithsupportboundedin VMIN VMAX Treatingthissupportasahyperparameterallowsustochangetheoptimizationproblembytreatingallextremalreturns     greaterthanVMAX asequivalent Surprisingly asimilarvalueclippinginDQNsigni cantlydegradesperformanceinmostgames Totakeanotherexample interpretingthediscountfactor asaproperprobability assomeauthorshaveargued leadstoadifferentalgorithm Wellbehavedoptimization ItiswellacceptedthattheKLdivergencebetweencategoricaldistributionsisarea sonablyeasylosstominimize Thismayexplainsomeofourempiricalperformance Yetearlyexperimentswithalternativelosses suchasKLdivergencebetweencontinuousdensities werenotfruitful inpartbecausetheKLdivergenceisinsensitivetothevaluesofitsoutcomes AcloserminimizationoftheWassersteinmetricshouldyieldevenbetterresultsthanwhatwepresentedhere Inclosing webelieveourresultshighlighttheneedtoaccountfordistributioninthedesign theoreticalorotherwise ofalgorithms ADistributionalPerspectiveonReinforcementLearningAcknowledgementsTheauthorsacknowledgetheimportantroleplayedbytheircolleaguesatDeepMindthroughoutthedevelopmentofthiswork SpecialthankstoYeeWhyeTeh AlexGraves JoelVeness GuillaumeDesjardins TomSchaul DavidSilver AndreBarreto MaxJaderberg MohammadAzar GeorgOstrovski BernardoAvilaPires OlivierPietquin AudrunasGruslys TomStepleton AaronvandenOord andparticularlyChrisMaddisonforhiscomprehensivereviewofanearlierdraft ThanksalsotoMarekPetrikforpointerstotherelevantliterature ReferencesAzar MohammadGheshlaghi Munos   emi andKappen Hilbert Onthesamplecomplexityofreinforcementlearningwithagenerativemodel InProceedingsoftheInternationalConferenceonMachineLearning Bellemare MarcG Naddaf Yavar Veness Joel andBowling Michael Thearcadelearningenvironment Anevaluationplatformforgeneralagents JournalofArti cialIntelligenceResearch Bellemare MarcG Danihelka Ivo Dabney Will Mohamed Shakir Lakshminarayanan Balaji Hoyer Stephan andMunos   emi Thecramerdistanceasasolutiontobiasedwassersteingradients arXiv Bellman RichardE Dynamicprogramming PrincetonUniversityPress Princeton NJ Bertsekas DimitriP andTsitsiklis JohnN NeuroDynamicProgramming AthenaScienti   Bickel PeterJ andFreedman DavidA Someasymptotictheoryforthebootstrap TheAnnalsofStatistics pp Billingsley Patrick Probabilityandmeasure JohnWiley Sons Caruana Rich Multitasklearning MachineLearning Chung KunJenandSobel MatthewJ Discountedmdps Distributionfunctionsandexponentialutilitymaximization SIAMJournalonControlandOptimization Dearden Richard Friedman Nir andRussell Stuart BayesianQlearning InProceedingsoftheNationalConferenceonArti cialIntelligence Engel Yaakov Mannor Shie andMeir Ron Reinforcementlearningwithgaussianprocesses InProceedingsoftheInternationalConferenceonMachineLearning Geist MatthieuandPietquin Olivier Kalmantemporaldifferences JournalofArti cialIntelligenceResearch Gordon Geoffrey Stablefunctionapproximationindynamicprogramming InProceedingsoftheTwelfthInternationalConferenceonMachineLearning Harutyunyan Anna Bellemare MarcG Stepleton Tom andMunos   emi   withoffpolicycorrections InProceedingsoftheConferenceonAlgorithmicLearningTheory Hoffman MatthewD deFreitas Nando Doucet Arnaud andPeters Jan Anexpectationmaximizationalgorithmforcontinuousmarkovdecisionprocesseswitharbitraryreward InProceedingsoftheInternationalConferenceonArti cialIntelligenceandStatistics Jaderberg Max Mnih Volodymyr Czarnecki WojciechMarian Schaul Tom Leibo JoelZ Silver David andKavukcuoglu Koray Reinforcementlearningwithunsupervisedauxiliarytasks ProceedingsoftheInternationalConferenceonLearningRepresentations Jaquette StrattonC Markovdecisionprocesseswithanewoptimalitycriterion Discretetime TheAnnalsofStatistics Kakade ShamandLangford John Approximatelyoptimalapproximatereinforcementlearning InProceedingsoftheInternationalConferenceonMachineLearning Kingma DiederikandBa Jimmy Adam Amethodforstochasticoptimization ProceedingsoftheInternationalConferenceonLearningRepresentations Lattimore TorandHutter Marcus PACboundsfordiscountedMDPs InProceedingsoftheConferenceonAlgorithmicLearningTheory Mannor ShieandTsitsiklis JohnN Meanvarianceopti mizationinmarkovdecisionprocesses McCallum AndrewK Reinforcementlearningwithselectiveperceptionandhiddenstate PhDthesis UniversityofRochester Mnih Volodymyr Kavukcuoglu Koray Silver David Rusu AndreiA Veness Joel Bellemare MarcG Graves Alex Riedmiller Martin Fidjeland AndreasK Ostrovski Georg etal Humanlevelcontrolthroughdeepreinforcementlearning Nature Morimura Tetsuro Hachiya Hirotaka Sugiyama Masashi Tanaka Toshiyuki andKashima Hisashi ADistributionalPerspectiveonReinforcementLearningParametricreturndensityestimationforreinforcementlearning InProceedingsoftheConferenceonUncertaintyinArti cialIntelligence   Morimura Tetsuro Sugiyama Masashi Kashima Hisashi Hachiya Hirotaka andTanaka Toshiyuki Nonparametricreturndistributionapproximationforreinforcementlearning InProceedingsofthe thInternationalConferenceonMachineLearning ICML pp   Nair Arun Srinivasan Praveen Blackwell Sam Alcicek Cagdas Fearon Rory DeMaria Alessandro Panneershelvam Vedavyas Suleyman Mustafa Beattie Charles andPetersen Stigetal Massivelyparallelmethodsfordeepreinforcementlearning InICMLWorkshoponDeepLearning Prashanth LAandGhavamzadeh Mohammad Actorcriticalgorithmsforrisk sensitivemdps InAdvancesinNeuralInformationProcessingSystems Puterman MartinL MarkovDecisionProcesses Discretestochasticdynamicprogramming JohnWiley Sons Inc   osler Uwe   xedpointtheoremfordistributions StochasticProcessesandtheirApplications Schaul Tom Quan John Antonoglou Ioannis andSilver David Prioritizedexperiencereplay InProceedingsoftheInternationalConferenceonLearningRepresentations Sobel MatthewJ Thevarianceofdiscountedmarkovdecisionprocesses JournalofAppliedProbability Sutton RichardS andBarto AndrewG Reinforcementlearning Anintroduction MITPress Sutton     Modayil   Delp   Degris   Pilarski     White   andPrecup   Horde Ascalablerealtimearchitectureforlearningknowledgefromun supervisedsensorimotorinteraction InProceedingsoftheInternationalConferenceonAutonomousAgentsandMultiagentsSystems Tamar Aviv DiCastro Dotan andMannor Shie Learningthevarianceoftherewardto go JournalofMachineLearningResearch Tieleman TijmenandHinton Geoffrey Lecture rmsprop Dividethegradientbyarunningaverageofitsrecentmagnitude COURSERA Neuralnetworksformachinelearning Toussaint MarcandStorkey Amos Probabilisticinferenceforsolvingdiscreteandcontinuousstatemarkovdecisionprocesses InProceedingsoftheInternationalConferenceonMachineLearning Tsitsiklis JohnN Ontheconvergenceofoptimisticpolicyiteration JournalofMachineLearningResearch Utgoff PaulE andStracuzzi DavidJ Manylayeredlearning NeuralComputation VandenOord Aaron Kalchbrenner Nal andKavukcuoglu Koray Pixelrecurrentneuralnetworks InProceedingsoftheInternationalConferenceonMachineLearning vanHasselt Hado Guez Arthur andSilver David DeepreinforcementlearningwithdoubleQlearning InProceedingsoftheAAAIConferenceonArti cialIntelligence Veness Joel Bellemare MarcG Hutter Marcus Chua Alvin andDesjardins Guillaume Compressandcontrol InProceedingsoftheAAAIConferenceonArti cialIntelligence Wang Tao Lizotte Daniel Bowling Michael andSchuurmans Dale Dualrepresentationsfordynamicprogramming JournalofMachineLearningResearch pp Wang Ziyu Schaul Tom Hessel Matteo Hasselt Hadovan Lanctot Marc anddeFreitas Nando Duelingnetworkarchitecturesfordeepreinforcementlearn ing InProceedingsoftheInternationalConferenceonMachineLearning White     Mean variance andprobabilisticcriteriain nitemarkovdecisionprocesses areview JournalofOptimizationTheoryandApplications 