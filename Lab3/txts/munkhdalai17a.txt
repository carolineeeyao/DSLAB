Meta Networks

Tsendsuren Munkhdalai   Hong Yu  

Abstract

Neural networks have been successfully applied
in applications with   large amount of labeled
data  However  the task of rapid generalization
on new concepts with small training data while
preserving performances on previously learned
ones still presents   signi cant challenge to neural network models 
In this work  we introduce   novel meta learning method  Meta Networks  MetaNet  that learns   metalevel knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization  When evaluated on Omniglot and
MiniImageNet benchmarks  our MetaNet models achieve   near humanlevel performance and
outperform the baseline approaches by up to
  accuracy  We demonstrate several appealing
properties of MetaNet relating to generalization
and continual learning 

  Introduction
Deep neural networks have shown great success in several application domains when   large amount of labeled
data is available for training  However  the availability of
such large training data has generally been   prerequisite
in   majority of learning tasks  Furthermore  the standard
deep neural networks lack the ability to continuous learning or incrementally learning new concepts on the     without forgetting or corrupting previously learned patterns  In
contrast  humans can rapidly learn and generalize from  
few examples of the same concept  Humans are also very
good at incremental       continuous  learning  These abilities have been mostly explained by the meta learning      
learning to learn  process in the brain  Harlow   
Previous work on meta learning has formulated the problem as twolevel learning    slow learning of   metalevel

 University

of Massachusetts  MA  USA  CorTsendsuren Munkhdalai  tsend 

respondence
suren munkhdalai umassmed edu 

to 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Overall architecture of Meta Networks 

model performing across tasks and   rapid learning of  
baselevel model acting within each task  Mitchell et al 
  Vilalta   Drissi    The goal of   metalevel
learner is to acquire generic knowledge of different tasks 
The knowledge can then be transferred to the baselevel
learner to provide generalization in the context of   single
task  The base and metalevel models can be framed in  
single learner  Schmidhuber    or in separate learners
 Bengio et al    Hochreiter et al   
In this work we introduce   meta learning model called
MetaNet  for Meta Networks  that supports metalevel continual learning by allowing neural networks to learn and to
generalize   new task or concept from   single example on
the     The overall architecture of MetaNet is shown in
Figure   MetaNet consists of two main learning components    base learner and   meta learner  and is equipped
with an external memory  Learning occurs at two levels
in separate spaces       meta space and task space  The
base learner performs in the input task space whereas the
meta learner operates in   taskagnostic meta space  By
operating in the abstract meta space  the meta learner supports continual learning and performs meta knowledge acquisition across different tasks  Towards this end  the base
learner  rst analyzes the input task  The base learner then
provides the meta learner with   feedback in the form of
higher order meta information to explain its own status in
the current task space  Based on the meta information 

MemorySlow weightsFast weightsMeta weightsSlow weightsFast weightsInputMeta infoFast parameterizationFast parameterizationMemory accessOutputMeta learnerBase learnerMeta Networks

the meta learner rapidly parameterizes both itself and the
base learner so that the MetaNet model can recognize the
new concepts of the input task  Speci cally  the training
weights of MetaNet evolve at different timescales  standard slow weights are updated through   learning algorithm       REINFORCE  tasklevel fast weights are updated within the scope of each task  and examplelevel fast
weights are updated for   speci   input example  Finally
MetaNet equipped with external memory allows for rapid
learning and generalization 
Under the MetaNet framework  it is important to de ne the
types of the meta information which can be obtained from
the learners  While other representations of meta information are also applicable  we use loss gradients as meta information  MetaNet has two types of loss functions with
distinct objectives    representation       embedding  loss
de ned for the good representation learner criteria and  
main  task  loss used for the input task objective 
We extensively studied the performance and the characteristics of MetaNet on oneshot supervised learning  SL 
problems under several different settings  Our proposed
method not only improves the stateof theart results on
the standard benchmarks  but also shows some interesting
properties related to generalization and continual learning 

  Related Work
Our work connects different threads of research in order to
model neural architectures for rapid learning and generalization  Rapid learning and generalization refers to   oneshot learning scenario where   learner is introduced to  
sequence of tasks  where each task entails multiclass classi cation with   single or few labeled example per class 
  key challenge in this setting is that the classes or concepts vary across the tasks  Due to this  oneshot learning
problems have been widely addressed by generative models and metric learning methods  One notable success is
reported by   probabilistic programming approach  Lake
et al    They used speci   knowledge of how pen
strokes are composed to produce characters of different alphabets  Koch   applied Siamese Networks to perform oneshot classi cation  Recently  Vinyals et al   
uni ed the training and testing of   oneshot learner under
the same procedure and developed an endto end differentiable nearest neighbor method for oneshot learning  Santoro et al    proposed   memorybased approach and
trained Neural Turing Machines  Graves et al    for
oneshot learning  although the metalearner and the oneshot learner in this work are not separable explicitly  The
training procedure used by Santoro et al    adapted the
work of Hochreiter et al    in which they use LSTMs
as the metalevel model  More recently an LSTMbased
oneshot optimizer was proposed  Ravi   Larochell   

By taking in the loss  the gradient and the parameters of the
base learner  the meta optimizer was trained to update the
parameters for oneshot classi cation 
  related line of work focuses on building meta optimizers  Hochreiter et al    Maclaurin et al   
Andrychowicz et al    Li   Malik    As the main
interest here is to train an optimization algorithm within
the meta learning framework  these efforts have mainly focused on tasks with large datasets  In contrast  with the absence of large datasets  our experimental setup emphasizes
the dif culties of optimizing   neural network with   large
number of parameters to generalize with limited examples
of   new concept  Our work proposes   novel rapid parameterization approach by employing meta information  By
following the success of the previous work  Mitchell et al 
  Younger et al    Andrychowicz et al   
Ravi   Larochell    we study the meta information
present in the loss gradient of neural nets  Fast weights
and utilizing one neural network to generate parameters
for another neural network have previously been studied
separately  Hinton   Plaut   suggested the usage of
fast weights for rapid learning  Ba et al    recently
used fast weights to replace soft attention mechanism  Fast
weights have also been used to implement recurrent nets
 Schmidhuber        and selfreferential networks
 Schmidhuber        These usages of fast weights
are well motivated by the fact that synapses have dynamics
at many different timescales  Greengard   
The approach proposed by Gomez   Schmidhuber  
is more closely related to our work  They used recurrent
nets to generate fast weights for   singlelayer network
controller  De Brabandere et al    used one network
to generate slow  lter weights for   convolutional neural
net  More recently David Ha   Le   generated slow
weights for recurrent nets  Our MetaNet generates fast
weights at two timescales by operating in meta space  To
integrate the fast weights with the slow weights  we propose   novel layer augmentation approach 
Finally  we note that our MetaNet equipped with an external memory can be seen as   memory augmented neural network  MANN  MANNs have shown promising results on   range of tasks starting from small programming
problems  Graves et al    to largescale language tasks
 Weston et al    Sukhbaatar et al    Munkhdalai
  Yu   

  Meta Networks
MetaNet learns to fast parameterize underlying neural networks for rapid generalizations by processing   higher order meta information  resulting in    exible AI model that
can adapt to   sequence of tasks with possibly distinct in 

Meta Networks

Algorithm   MetaNet for oneshot supervised learning
Require  Support set    cid 
   
   and Training set  xi  yi  
Require  Base learner    Dynamic representation learning function    Fast weight generation functions   and    and Slow
weights                 

     cid 

  

Figure     layer augmented MLP

in oneshot learning setup  the support set contains only
single example per class and thus it is cheap to obtain 

  Meta Learner

The meta learner consists of   dynamic representation
learning function   and fast weight generation functions
  and    The function   has   representation learning objective and constructs embeddings of inputs in each task
space by using tasklevel fast weights  The weight generation functions   and   are responsible for processing
the meta information and generating the example and tasklevel fast weights 
More speci cally  the function   learns the mapping from
the loss gradient     
   derived from the base learner   
    
to fast weights     
  
          
   

 

    

   
   of the support examples    cid 

where   is   neural network with parameter    The fast
weights are then stored in   memory         
   The
memory   is indexed with task dependent embeddings
       cid 
   
   obtained
by the dynamic representation learning function   
The representation learning function   is   neural net parameterized by slow weights   and tasklevel fast weights
   It uses the representation loss lossemb to capture   representation learning objective and to obtain the gradients as
meta information  We generate the fast weights    on   per
task basis as follows 

Li   lossemb        cid 

     cid 
  

      QLi
          

  

 

 
 

  

     cid 
  

     cid 
  

Li   lossemb        cid 

Li   losstask        cid 

Require  Layer augmentation scheme
  Sample   examples from support set
  for         do
 
        QLi
  end for
            
  for         do
 
        WLi
          
     
Store    
 
  cid 
              cid 
 
  
Store   cid 
 
  end for
  Ltrain    
  for         do
 
 
     
 

  in ith position of memory  

  in ith position of index memory  

    sof tmax ai cid  

ri           xi 
ai   attention    ri 
Ltrain   Ltrain   losstask         
    xi  yi 
 Alternatively the base learner can take as input ri instead
of xi 
  end for
  Update   using  Ltrain

put and output distributions  The model consists of two
main learning modules  Figure   The meta learner is responsible for fast weight generation by operating across
tasks while the base learner performs within each task by
capturing the task objective  The generated fast weights
are integrated into both base learner and meta learner to
shift the inductive bias of the learners  We propose   novel
layer augmentation method to integrate the standard slow
weights and the task or example speci   fast weights in  
neural net 
To train MetaNet  we adapt   task formulation procedure by
Vinyals et al    We form   sequence of tasks  where
   
each task consists of   support set    cid 
   and   training set  xi  yi  
   The class labels are consistent for both
support and training sets of the same task  but vary across
distinct tasks  Overall the training of MetaNet consists of
three main procedures  acquisition of meta information 
generation of fast weights and optimization of slow weights 
executed collectively by the base and the meta learner  The
training of MetaNet is described in Algorithm  
To test the model for oneshot SL  we sample another sequence of tasks from   test dataset with unseen classes 
Then the model is deployed to classify test examples based
on its support set  We assume that we have class labels for
the support set during both training and testing  Note that

     cid 

Fast weight layerSlow weight layer Fast weight layerSlow weight layerReLUReLU SoftmaxFast weight layerSlow weight layer ReLUReLUInputOutputMeta Networks

     cid 

where   denotes   neural net parameterized by    that accepts variable sized input  First  we sample   examples
           cid 
   
   from the support set and obtain the
loss gradient as meta information  Then   observes the gradient corresponding to each sampled example and summarizes into the task speci   parameters  We use LSTM for
  although the order of inputs to   does not matter  Alternatively we can take summation or average of the gradients
and use   MLP  However  in our preliminary experiment we
observed that the latter results in   poor convergence 
Once the fast weights are generated  the task dependent ini  
put representations    cid 
   are computed as 
  cid 
              cid 
  

 
where the parameters   and    are integrated using the
layer augmentation method described in Section  
The loss  lossemb does not need to be the same as the main
task loss losstask  However  it should be able to capture
  representation learning objective  We use crossentropy
loss when the support set has only   single example per
class  When there are more than one examples per class
available  contrastive loss  Chopra et al    is   natural
choice for lossemb since both positive and negative samples
can be formed  In this case  we randomly draw   number
of pairs to observe the gradients and the loss is

Li   lossemb        cid 

           cid 

    li 

where li is auxiliary label 

 cid 

li  

if   cid 

       cid 
 
  otherwise

  

 

 

Once the parameters are stored in the memory   and the
memory index   is constructed  the meta learner parameterizes the base learner with the fast weights    
    First it
embeds the input xi in the task space by using the dynamic
representation learning network       Equation   and then
reads the memory with soft attention 

ai   attention    ri 
    norm ai cid  
   

 

 

where attention calculates similarity between the memory
index and the input embedding and we use cosine similarity as attention and norm is   normalization function  for
which we use sof tmax 

losstask  However  unlike standard neural nets    is parameterized by slow weights   and examplelevel fast weights
    The slow weights are updated via   learning algorithm
during training whereas the fast weights are generated by
the meta learner for every input 
The base learner uses   representation of meta information
obtained by using   support set  to provide the meta learner
with feedbacks about the new input task  The meta information is derived from the base learner in form of the loss
gradient information 

Li   losstask        cid 
      WLi

     cid 
  

 
 

     cid 

   
Here Li is the loss for support examples    cid 
     is
the number of support examples in the task set  typically
  single instance per class in the oneshot learning setup 
   is the loss gradient with respect to parameters   and is
our meta information  Note that the loss function losstask
is generic and can take any form  such as   cumulative reward in reinforcement learning  For our oneshot classi 
cation setup we use crossentropy loss  The meta learner
takes in the gradient information    and generates the fast
parameters     as in Equation  
Assuming that the fast weights    
the base learner performs the oneshot classi cation as 

  for input xi are de ned 

   yi xi        

              

    xi 

 
where  yi is predicted output and  xi  
   is an input drawn
from the training set  xi  yi  
   for the current task  Alternatively the base learner can take as input the task speci  
representations  ri  
   produced by the dynamic representation learning network  effectively reducing the number of
MetaNet parameters and leveraging shared representations 
In this case  the base learner is forced to operate in the dynamic task space constructed by   instead of building new
representations from the raw inputs  xi  
  
During training  given output labels  yi  
   we minimize
the crossentropy loss for oneshot SL  The training parameters of MetaNet   consists of the slow weights   and  
and the meta weights   and                         
and jointly updated via   training algorithm such as backpropagation to minimize the task loss losstask  Equation
 
In   similar way  as de ned in the Equation   we can also
parameterize the base learner with tasklevel fast weights 
An ablation experiment on different variation of MetaNet
is reported in Section  

  Base Learner

  Layer Augmentation

The base learner  denoted as    is   function or   neural
net that estimates the main task objective via   task loss

  slow weight layer in the base learner is extended with
its corresponding fast weights for rapid generalization  An

Meta Networks

Table   Oneshot accuracy on Omniglot previous split

Model
Pixel kNN  Kaiser et al   
Siamese Net  Koch   
MANN  Santoro et al   
Matching Nets  Vinyals et al   
Neural Statistician  Edwards   Storkey   
Siamese Net with Memory  Kaiser et al   
MetaNetMetaNet
MetaNet 

 way
 
 
 
 
 
 
 
 
 

 way

 way

 way

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 

 
 
 

 
 

 

 
 
 
 
 
 

example of the layer augmentation approach applied to  
MLP is shown in Figure   The input of an augmented
layer is  rst transformed by both slow and fast weights and
then passed through   nonlinearity       ReLU  resulting
in two separate activation vectors  Finally the activation
vectors are aggregated by an elementwise vector addition 
For the last sof tmax layer  we  rst aggregate two transformed inputs and then normalize for classi cation output 
Intuitively  the fast and slow weights in the layer augmented neural net can be seen as feature detectors operating in two distinct numeric domains  The application of
the nonlinearity maps them into the same domain  which
is   in the case of ReLU so that the activations can be
aggregated and processed further  Our aggregation function here is elementwise sum 
Although it is possible to de ne the base learner with
only fast weights  in our preliminary experiment we found
that the integration of both slow and fast weights with the
layer augmentation approach is essential in convergence
of MetaNet models    MetaNet model relying on   base
leaner with only fast weights were failed to converge and
the best performance of this model was reported to be as
equal as that of   constant classi er that assigns the same
label to every input 

  Results
We carried out oneshot classi cation experiments on three
datasets  Omniglot  MiniImageNet and MNIST  The Omniglot dataset consists of images across   classes with
only   images per class  from   different alphabets  Lake
et al    It also comes with   standard split of   training and   evaluation alphabets  Following  Santoro et al 
  we augmented the training set through     and
  degrees rotations  The images are resized to      
pixels for computational ef ciency  For the experiment on
MiniImageNet data  we evaluated on the same class subset provided by Ravi   Larochell   MNIST images
were used as outof domain data  The training details are
described in Appendix   

  Oneshot Learning Test

In this section we will report four groups of benchmark
experiments  Omniglot previous split  MiniImageNet 
MNIST as outof domain data and Omniglot standard split 

  OMNIGLOT PREVIOUS SPLIT

Following the previous setup Vinyals et al    we split
the Omniglot classes into   and   classes for training and testing  We performed       and  way oneshot classi cation and compared our performance against
the stateof theart results  We also studied three variations
of MetaNet as an ablation experiment in order to show how
fast parameterization affects the network dynamics 
In Table   we compared the performance of our models
with all published models  as baselines  The  rst group
of methods are the previously published models  The next
group is MetaNet variations  MetaNet is the main architecture described in Section   MetaNetis   variant without tasklevel fast weights    in the embedding function
  whereas MetaNet  has additional tasklevel weights for
the base learner in addition to     Our MetaNet model
improves the previous best results by   to   accuracy  As the number of classes increases  from  way to
 way classi cation  overall the performance of the oneshot learners decreases  MetaNet   performance drop is
relatively small  around   while the drop for the other
models ranges from   to   As   result  our model
shows an absolute improvement of   on  way oneshot
task 
Comparing different MetaNet variations 
the additional
tasklevel weights in the base learner  MetaNet  did not
seem to help and in fact had   negative effect on performance  MetaNethowever performed surprisingly well but
still falls behind the MetaNet model as it lacks the dynamic
representation learning function  This performance gap increases when we test them in outof the domain setting
 Appendix   

Meta Networks

Table   Oneshot accuracy on MiniImageNet test set
 way

Model
Finetuning  Ravi   Larochell   
kNN  Ravi   Larochell   
Matching Nets  Vinyals et al   
MetaLearner LSTM  Ravi   Larochell   
MetaNet

     
     
     
     
     

  MINIIMAGENET

The training  dev and testing sets of     and   ImageNet classes  with   examples per class  were provided by Ravi   Larochell   By following Ravi  
Larochell   we sampled   examples per class for
evaluation  By using the dev set  we set an evaluation
checkpoint where only if the model performance exceeds
the previous best result on random   trials produced from
the dev set  we apply the model to another   trials randomly produced from the testing set and report the average
accuracy 
In Table   we present the results of the  way oneshot
evaluation  MetaNet improved the previous result by up to
  accuracy and obtained the best result 

  OMNIGLOT STANDARD SPLIT

Omniglot data comes with   standard split of   training alphabets with   classes and   evaluation alphabets
with   classes  We trained and tested only the standard
MetaNet model in this setup  In order to best match the
evaluation protocol of Lake et al    we form   tasks
 trials  from the evaluation classes to test the model 
In Table   we listed the MetaNet results along with the previous models and human performance  Our MetaNet outperformed the human performance by   slight margin  but
underperformed the probabilistic programming approach 
However  the performance gap is rather small between
these top three baselines  In addition while the probabilistic
programming performs slightly better than MetaNet  our
model does not rely on any extra prior knowledge about
how characters and strokes are composed  Comparing the
results on two Omniglot splits in Tables   and   MetaNet
showed decreasing performances on the standard split  The
later setup seems to be slightly dif cult as the number of
classes in the training set is less   vs   and test
classes are bigger   vs  

 Our code and data will be made available at  https 

bitbucket org tsendeemts metanet

  Generalization Test

We conducted   set of experiments to test the generalization
of MetaNet from multiple aspects  The  rst experiment
tests whether   MetaNet model trained on an Nway oneshot task could generalize to another Kway task  where
   cid     without actually training on the second task 
The second experiment is to test if   meta learner trained
for rapid parameterization of   base learner btrain could
parameterize another base learner beval during evaluation 
The last experimental setup examines whether MetaNet
supports metalevel continual learning 

  NWAY TRAINING AND KWAY TESTING

In this experiment  MetaNet is trained on Nway oneshot
classi cation task and then tested on Kway oneshot tasks 
The number of training and test classes are varied          cid 
   To handle this  we inserted   sof tmax layer into the
base learner during evaluation and then augmented it with
the fast weights generated by the meta learner  If the meta
learner is generic enough  it should be able to parameterize
the new sof tmax layer on the     The new layer weights
remained  xed since no parameter update was performed
for this layer  The Kway test tasks were formed from the
  unseen classes in the test set 
The MetaNet models were trained on one of       and
 way oneshot tasks and evaluated on the rest  In Table
  we summarized the results  As   comparison we also
included some results from Table   which reports accuracy of Nway train and test setting  The MetaNet model
trained on  way tasks obtained   of  way test accuracy which is still   closer match to Matching Network
and higher than Siamese Net trained  way tasks  An interesting  nding is that when   is smaller than         the
model is trained on easier tasks than test ones  we observe
  decreasing performance  Conversely the models trained
on harder tasks              achieved increasing performances when tested on the easier tasks and the performance
is even higher than the ones that were applied to the tasks
with the same level dif culty              For example  the model skilled on  way classi cation improved
the  way oneshot baseline by   showing   ceiling
performance in this setting  We also conducted   preliminary experiment on more extreme testtime classi cation 
MetaNet trained on  way task achieved around   on
 way oneshot classi cation task 
This  exibility in MetaNet is crucial because oneshot
learning usually involves an online concept identi cation
scenario  Furthermore we can empirically obtain   performance lower or upper bound  Particularly the test performance obtained on the tasks with the same level dif culty
that the model was skilled on can be used as   performance
lower or an upper bound depending on   scenario under

Meta Networks

Table   Oneshot accuracy on Omniglot standard split

Model
Human performance  Lake et al   
Pixel kNN  Lake et al   
Af ne model  Lake et al   
Deep Boltzmann Machines  Lake et al   
Hierarchial Bayesian Program Learning  Lake et al   
Siamese Net  Koch   
MetaNet

 way

 way

 way

 way

 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 

 

 

 

 
 
 
 
 
 
 

which the model will be deployed in the future  For example  for the MetaNet model that will deployed under the
      scenario  we can obtain the performance lower
bound by testing it on the       tasks 

  RAPID PARAMETERIZATION OF FIXED WEIGHT

BASE LEARNER

We replaced the entire base learner with   new CNN during evaluation  The slow weights of this network remained
 xed  The fast weights are generated by the meta learner
that is trained to parameterize the old base learner and used
to augmented the  xed slow weights 
We tested   small and   large CNN for the base learner 
The small CNN has    lters and the large CNN has  
 lters  In Figure   the test performances of these CNNs are
compared  The base learner  target CNN  optimized along
within the model performed better than the  xed weight
CNNs  The performance difference between these models is large in earlier training iterations  However  as the
meta learner sees more oneshot learning trials  the test accuracies of the base learners converge  This results show
that MetaNet effectively learns to parameterize   neural net
with  xed weights 

  METALEVEL CONTINUAL LEARNING

MetaNet operates in two spaces  input problem space and
meta  gradient  space 
If the meta space is problem independent  MetaNet should support metalevel continual
learning or lifelong learning  This experiment tests this
in the case of the loss gradient 

Table   Accuracy of MetaNet trained on Nway and tested on Kway oneshot tasks

Test

Train
 way
 way
 way
 way

 way
 
 
 
 

 way

 way

 
 
 
 

 
 
 
 

 way
 
 
 
 

Figure   Comparison of the test performances of the base learners on Omniglot  way classi cation 

Following the previous work on catastrophic forgetting in
neural networks  Srivastava et al    Goodfellow et al 
  Kirkpatrick et al    we formulated two problems in   sequential manner  We  rst trained and tested
the model on the Omniglot sets and then we switched and
continued training on the MNIST data  After training on
  number of MNIST oneshot tasks  we reevaluated the
model on the same Omniglot test set and compare performance    decrease in performance here indicates that
the meta weights   and   of the neural nets   and   are
prone to catastrophic forgetting and the model therefore
does not support continual learning  On the other hand 
an increase in performance indicates that MetaNet supports
reverse transfer learning and continual learning 
We allocated separate parameters for the weights   and  
when we switched the problems so the only meta weights
were updated  We used two threelayer MLPs with   hidden units as the embedding function and the base learner 
The MNIST image and classes were augmented by randomly permuting the pixels  We created   different random shuf es and thus the training set for the second oneshot problem consisted of   classes  We conducted multiple runs and increased the MNIST training trials by multiples of              
in each run giving

 Accuracy The number of trials Target CNNSmall CNNBig CNNMeta Networks

Under the MetaNet framework  an important consideration
is the type of higher order meta information that can be extracted as   feedback from the model when operating on  
new task  One desirable property here is that the meta information should be generic and problem independent  It
should also be expressive enough to explain the model setting in the current task space  We explored the use of loss
gradients as meta information in this work  As shown in
the results  using the gradients as meta information seems
to be   promising direction  MetaNet obtains stateof the
art results on several oneshot SL benchmarks and leads to
  very  exible AI model  For instance  in MetaNet we can
alternate between different sof tmax layers on the    during test  It supports continual learning up to   certain point 
We observed that neural nets with  xed slow weights can
perform well for new task inputs when augmented with the
fast weights  When the slow weights are updated during
training  it learns domain biases resulting in even better performance on identi cation of new concepts within the same
domain  However  one could expect   higher performance
from the  xed weight network when aiming for oneshot
generalization across distant domains 
An interesting future direction would be in exploring   new
type of meta information that is more robust and expressive  and in developing synaptic weights that are capable of
maintaining such higher order information  One could take
inspiration from the meta learning process in the brain and
ask whether the brain operates on some kind of higher order information to generalize across tasks and acquire new
skills 
The rapid parameterization approach presented here has
been shown to be an effective alternative to the direct optimization methods that learn to update network parameters for oneshot generalization  However    problem
this approach poses is the integration of slow and fast
weights  As   solution to this  we presented   simple
layer augmentation method  Although the layer augmentation worked reasonably well  this method becomes dif cult
when   neural net has many types of parameters operating in multiple different timescales  For example    single
base learner equipped with three types of weights  slow 
examplespeci    and tasklevel weights  integrated under
the layer augmentation paradigm could not perform as well
as   simpler one  Therefore    potential extension would be
to train MetaNet so it can discover its own augmentation
schema for ef ciency 
MetaNet can readily be applied to parameterize policies
in reinforcement learning and imitation learning  leading
to an agent with oneshot and meta learning capabilities 
MetaNet based on recurrent networks as underlying learners could lead to useful applications in sequence modeling
and language understanding tasks 

Figure   The difference between the two Omniglot test accuracies obtained before and after training on MNIST task 

more time for MetaNet to adapt its meta weights on the
second problem so that it may forget the knowledge about
Omniglot  Each run was repeated  ve times and we report
the average statistics  For every run  the network and the
optimizer were reinitialized and the training started from
scratch 
In Figure   we plotted the accuracy difference between two
Omniglot test performances obtained before and after training on the MNIST task  The performance improvement  yaxis  after training on the MNIST tasks ranges from  
to   depending on the training time  xaxis  The positive values indicate that the training on the second problem
automatically improves the performance of the earlier task
exhibiting the reverse transfer property  Therefore  we can
conclude that MetaNet successfully performs reverse transfer  At the same time  it is skilled on MNIST oneshot classi cation  The MNIST training accuracy reaches over  
after   MNIST trials  However  reverse transfer happens only up to   certain point in MNIST training  
trials  After that  the meta weights start to forget the Omniglot information  As   result from   trials onwards 
the Omniglot test accuracy drops  Nevertheless even after
  MNIST trials  at which point the MNIST training accuracy reached over   the Omniglot performance drop
was only  

  Discussion and Future Work
Oneshot learning in combination with   meta learning
framework can be   useful approach to address certain neural network drawbacks related to rapid generalization with
small data and continual learning  We present   novel meta
learning method  MetaNet  that performs   generic knowledge acquisition in   meta space and shifts the parameters
and inductive biases of underlying neural networks via fast
parameterization for the rapid generalization 

 The number of MNIST trials Acc differenceMeta Networks

Acknowledgements
We would like to thank the anonymous reviewers and our
colleagues  Jesse Lingeman  Abhyuday Jagannatha and
John Lalor for their insightful comments and suggestions
on improving the manuscript  This work was supported in
part by the grant HL  from the National Institutes of
Health and by the grant    HX  supported by
the Health Services Research   Development of the US
Department of Veterans Affairs Investigator Initiated Research  Any opinions   ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor 

References
Andrychowicz  Marcin  Denil  Misha  Gomez  Sergio 
Hoffman  Matthew    Pfau  David  Schaul  Tom  and
de Freitas  Nando  Learning to learn by gradient descent
by gradient descent  In Advances in Neural Information
Processing Systems  pp     

Ba  Jimmy  Hinton  Geoffrey    Mnih  Volodymyr  Leibo 
Joel    and Ionescu  Catalin  Using fast weights to attend
In Advances In Neural Information
to the recent past 
Processing Systems  pp     

Bengio  Yoshua  Bengio  Samy  and Cloutier  Jocelyn 
Learning   synaptic learning rule 
Universit   de
Montr eal    epartement   informatique et de recherche
op erationnelle   

Chopra  Sumit  Hadsell  Raia  and LeCun  Yann  Learning
  similarity metric discriminatively  with application to
face veri cation  In Computer Vision and Pattern Recognition    CVPR   IEEE Computer Society Conference on  volume   pp    IEEE   

David Ha  Andrew Dai and Le  Quoc    Hypernetworks 

In ICLR    

De Brabandere  Bert  Jia  Xu  Tuytelaars  Tinne  and
Van Gool  Luc  Dynamic  lter networks  In Neural Information Processing Systems  NIPS   

Edwards  Harrison and Storkey  Amos  Towards   neural

statistician  In ICLR    

Graves  Alex  Wayne  Greg  and Danihelka  Ivo  Neural
turing machines  arXiv preprint arXiv   

Greengard  Paul  The neurobiology of slow synaptic trans 

mission  Science     

Harlow  Harry    The formation of learning sets  Psycho 

logical review     

Hinton  Geoffrey   and Plaut  David    Using fast weights
In Proceedings of the ninth
to deblur old memories 
annual conference of the Cognitive Science Society  pp 
   

Hochreiter  Sepp  Younger    Steven  and Conwell  Peter    Learning to learn using gradient descent  In International Conference on Arti cial Neural Networks  pp 
  Springer   

Kaiser  Lukasz  Nachum       Roy  Aurko  and Bengio 
Samy  Learning to remember rare events  In ICLR  
 

Kirkpatrick  James  Pascanu  Razvan  Rabinowitz  Neil 
Veness  Joel  Desjardins  Guillaume  Rusu  Andrei   
Milan  Kieran  Quan  John  Ramalho  Tiago  GrabskaBarwinska  Agnieszka  et al 
Overcoming catastrophic forgetting in neural networks  arXiv preprint
arXiv   

Koch  Gregory 

Siamese neural networks for oneshot
image recognition  PhD thesis  University of Toronto 
 

Lake  Brenden    Salakhutdinov  Ruslan    and Tenenbaum  Josh  Oneshot learning by inverting   compositional causal process  In Advances in neural information
processing systems  pp     

Lake  Brenden    Salakhutdinov  Ruslan  and Tenenbaum 
Joshua    Humanlevel concept learning through probabilistic program induction  Science   
   

Li  Ke and Malik  Jitendra  Learning to optimize  In ICLR

   

Gomez  Faustino and Schmidhuber    urgen  Evolving modIn International
ular fastweight networks for control 
Conference on Arti cial Neural Networks  pp   
Springer   

Maclaurin  Dougal  Duvenaud  David  and Adams  Ryan 
Gradientbased hyperparameter optimization through reIn International Conference on Maversible learning 
chine Learning  pp     

Goodfellow  Ian    Mirza  Mehdi  Xiao  Da  Courville 
Aaron  and Bengio  Yoshua  An empirical investigation
of catastrophic forgetting in gradientbased neural networks  In ICLR    

Mitchell  Tom    Thrun  Sebastian    et al  Explanationbased neural network learning for robot control  Advances in neural information processing systems  pp 
   

Meta Networks

Conference on Representation Learning  ICLR  
San Diego  California  May   In press 

Younger    Steven  Conwell  Peter    and Cotter  Neil   
IEEE Transactions on

Fixedweight online learning 
Neural Networks     

Munkhdalai  Tsendsuren and Yu  Hong  Neural semanIn Proceedings of the  th Conference
tic encoders 
of the European Chapter of the Association for Computational Linguistics  Volume   Long Papers  pp   
  Valencia  Spain  April   Association for Computational Linguistics  URL http www aclweb 
org anthology   

Ravi  Sachin and Larochell  Hugo  Optimization as   model

for fewshot learning  In ICLR    

Santoro  Adam  Bartunov  Sergey  Botvinick  Matthew 
Wierstra  Daan  and Lillicrap  Timothy  Metalearning
with memoryaugmented neural networks  In Proceedings of The  rd International Conference on Machine
Learning  pp     

Schmidhuber     Reducing the Ratio Between Learning
Complexity and Number of Time Varying Variables
in Fully Recurrent Nets  pp   
Springer
   
London  London 
ISBN  
   
 
URL
http dx doi org 
 

doi 

Schmidhuber       neural network that embeds its own
metalevels  In IEEE International Conference on Neural Networks  pp    vol      doi   
ICNN 

Schmidhuber    urgen  Evolutionary principles in selfreferential learning  PhD thesis  Technical University
of Munich   

Schmidhuber    urgen  Learning to control fastweight
memories  An alternative to dynamic recurrent networks  Neural Computation     

Srivastava  Rupesh    Masci  Jonathan  Kazerounian 
Sohrob  Gomez  Faustino  and Schmidhuber    urgen 
In Advances in neural informaCompete to compute 
tion processing systems  pp     

Sukhbaatar  Sainbayar  Weston  Jason  Fergus  Rob  et al 
In Advances in neural

Endto end memory networks 
information processing systems  pp     

Vilalta  Ricardo and Drissi  Youssef    perspective view
and survey of metalearning  Arti cial Intelligence Review     

Vinyals  Oriol  Blundell  Charles  Lillicrap  Tim  Wierstra 
Daan  et al  Matching networks for one shot learning  In
Advances in Neural Information Processing Systems  pp 
   

Weston  Jason  Chopra  Sumit  and Bordes  Antoine  MemIn In Proceedings Of The International

ory networks 

