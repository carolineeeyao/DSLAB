McGan  Mean and Covariance Feature Matching GAN

Youssef Mroueh       Tom Sercu       Vaibhava Goel  

Abstract

We introduce new families of Integral Probability Metrics  IPM  for training Generative Adversarial Networks  GAN  Our IPMs are based on
matching statistics of distributions embedded in
   nite dimensional feature space  Mean and covariance feature matching IPMs allow for stable training of GANs  which we will call McGan  McGan minimizes   meaningful loss between distributions 

  Introduction
Unsupervised learning of distributions is an important
problem  in which we aim to learn underlying features that
unveil the hidden the structure in the data  The classic approach to learning distributions is by explicitly parametrizing the data likelihood and  tting this model by maximizing the likelihood of the real data  An alternative recent approach is to learn   generative model of the data without explicit parametrization of the likelihood  Variational AutoEncoders  VAE   Kingma   Welling    and Generative
Adversarial Networks  GAN   Goodfellow et al    fall
under this category 
We focus on the GAN approach  In   nutshell GANs learn
  generator of the data via   minmax game between the
generator and   discriminator  which learns to distinguish
between  real  and  fake  samples  In this work we focus
on the objective function that is being minimized between
the learned generator distribution    and the real data distribution Pr 
The original work of  Goodfellow et al    showed that
in GAN this objective is the JensenShannon divergence 
 Nowozin et al    showed that other  divergences can
be successfully used  The Maximum Mean Discrepancy
objective  MMD  for GAN training was proposed in  Li

 Equal contribution  AI Foundations  IBM      Watson Research Center  NY  USA  Watson Multimodal Algorithms and
Engines Group 
IBM      Watson Research Center  NY  USA 
Correspondence to  Youssef Mroueh  mroueh us ibm com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

et al    Dziugaite et al    As shown empirically
in  Salimans et al    one can train the GAN discriminator using the objective of  Goodfellow et al    while
training the generator using mean feature matching  An energy based objective for GANs was also developed recently
 Zhao et al    Finally  closely related to our paper 
the recent work Wasserstein GAN  WGAN  of  Arjovsky
et al    proposed to use the Earth Moving distance
 EM  as an objective for training GANs  Furthermore  Arjovsky et al    show that the EM objective has many
advantages as the loss function correlates with the quality
of the generated samples and the mode dropping problem
is reduced in WGAN 
In this paper  inspired by the MMD distance and the kernel
mean embedding of distributions  Muandet et al    we
propose to embed distributions in    nite dimensional feature space and to match them based on their mean and covariance feature statistics  Incorporating  rst and second
order statistics has   better chance to capture the various
modes of the distribution  While mean feature matching
was empirically used in  Salimans et al    we show in
this work that it is theoretically grounded  similarly to the
EM distance in  Arjovsky et al    mean and covariance feature matching of two distributions can be written
as   distance in the framework of Integral Probability Metrics  IPM   Muller    To match the means  we can
use any  cid   norm  hence we refer to mean matching IPM  as
IPM    For matching covariances  in this paper we consider the KyFan norm  which can be computed cheaply
without explicitly constructing the full covariance matrices 
and refer to the corresponding IPM as IPM 
Our technical contributions can be summarized as follows 
   We show in Section   that the  cid   mean feature matching
IPM   has two equivalent primal and dual formulations
and can be used as an objective for GAN training in both
formulations 
   We show in Section   that the parametrization used in
Wasserstein GAN corresponds to  cid  mean feature matching
GAN  IPM  GAN in our framework 
   We show in Section   that the covariance feature
matching IPM  admits also two dual formulations  and can
be used as an objective for GAN training 

McGan  Mean and Covariance Feature Matching GAN

    cid cid cid   
    cid   

     cid cid cid 
     cid 

   Similar to Wasserstein GAN  we show that mean feature
matching and covariance matching GANs  McGan  are stable to train  have   reduced mode dropping and the IPM loss
correlates with the quality of the generated samples 

  Integral Probability Metrics
We de ne in this Section IPMs as   distance between distribution  Intuitively each IPM  nds    critic     Arjovsky
et al    which maximally discriminates between the
distributions 

  IPM De nition
Consider   compact space   in Rd  Let   be   set of
measurable and bounded real valued functions on     Let
      be the set of measurable probability distributions on
    Given two probability distributions              the
Integral probability metric  IPM  indexed by the function
space   is de ned as follows  Muller   

dF          sup

         
   

   

In this paper we are interested in symmetric function spaces
                        hence we can write the IPM in
that case without the absolute value 

dF          sup

         
   

   

 

It is easy to see that dF de nes   pseudometric over
      dF nonnegative  symmetric and satis es the triangle inequality    pseudo metric means that dF         
  but dF            does not necessarily imply       
By choosing   appropriately  Sriperumbudur et al   
  various distances between probability measures can
be de ned 
In the next subsection following  Arjovsky
et al    Li et al    Dziugaite et al    we show
how to use IPM to learn generative models of distributions 
we then specify   special set of functions   that makes the
learning tractable 

  Learning Generative Models with IPM
In order to learn   generative model of   distribution Pr  
      we learn   function

         Rnz      

such that for     pz  the distribution of      is close to the
real data distribution Pr  where pz is    xed distribution on
   for instance         Inz   Let    be the distribution
of          pz  Using an IPM indexed by   function class
  we shall solve therefore the following problem 

min
  

dF  Pr    

 

Hence this amounts to solving the following minmax
problem 

min
  

sup
   

 
  Pr

         
  pz

       

Given samples  xi             from Pr
and samples
 zi             from pz we shall solve the following empirical problem 

min
  

sup
   

 
 

  cid   

   xi   

 
 

  cid   

     zj 

in the following we consider for simplicity       

  Mean Feature Matching GAN
In this Section we introduce   class of functions   having the form  cid       cid  where vector     Rm and    
    Rm   non linear feature map  typically parametrized
by   neural network  We show in this Section that the IPM
de ned by this function class corresponds to the distance
between the mean of the distribution in the   space 

  IPM    Mean Matching IPM
More formally consider the following function space 

Fv               cid       cid cid cid cid     Rm cid   cid      

        Rm       

where  cid cid   is the  cid   norm  Fv   is the space of bounded
linear functions de ned in the non linear feature space induced by the parametric feature map     is typically  
multilayer neural network  The parameter space   is chosen so that the function space   is bounded  Note that for
  given   Fv   is    nite dimensional Hilbert space 
We recall here simple de nitions on dual norms that will be
necessary for the analysis in this Section  Let         
such that  
      By duality of norms we have   cid   cid    

     

 cid   cid    cid   cid   
From Holder inequality we obtain the following bound 

maxv cid   cid     cid      cid  and the Holder inequality   cid cid cid cid      cid cid cid cid   
 cid cid cid      cid cid cid   cid cid cid cid       cid cid cid cid     cid   cid    cid   cid      cid   cid    

To ensure that   is bounded  it is enough to consider   such
that  cid   cid                Given that the space   is
bounded it is suf cient to control the norm of the weights
and biases of the neural network   by regularizing the
 cid   clamping  or  cid  norms  weight decay  to ensure the
boundedness of Fv   

McGan  Mean and Covariance Feature Matching GAN

Figure   Motivating example on synthetic data in     showing how different components in covariance matching can target different
regions of the input space  Mean matching     is not able to capture the two modes of the bimodal  real  distribution   and assigns
higher values to one of the modes  Covariance matching     is composed of the sum of three components         corresponding
to the top three  critic directions  Interestingly  the  rst direction     focuses on the  fake  data    the second direction     focuses on
the  real  data  while the third direction     is mode selective  This suggests that using covariance matching would help reduce mode
dropping in GAN  In this toy example   is    xed random Fourier feature map  Rahimi   Recht    of   Gaussian kernel        
 nite dimensional approximation 

Now that we ensured the boundedness of Fv     we look
at its corresponding IPM 

 

dFv           sup

  Fv  

max

   

       cid     
      cid     
 cid  max
   
   cid         cid    

     

 
   

         
   
   cid 
   cid cid 

       
   
       
   

  max

  max

where we used the linearity of the function class and expectation in the  rst equality and the de nition of the dual
norm  cid cid   in the last equality and our de nition of the mean
feature embedding of   distribution          
    cid   cid    Rm 

       

We see that the IPM indexed by Fv    corresponds to the
Maximum mean feature Discrepancy between the two distributions  Where the maximum is taken over the parameter
set   and the discrepancy is measured in the  cid   sense between the mean feature embedding of   and    In other
words this IPM is equal to the worst case  cid   distance between mean feature embeddings of distributions  We refer
in what follows to dFv   as IPM   

  Mean Feature Matching GAN

We turn now to the problem of learning generative models
with IPM    Setting   to Fv   in Equation   yields
to the following minmax problem for learning generative
models 

min
  

max
 

max

      

        

 

where

          cid     

  Pr

       
  pz

     cid   

or equivalently using the dual norm 

min
  

max

   cid Pr       cid    

 

where        
  pz

     

We refer to formulations   and   as primal and dual formulation respectively 
The dual formulation in Equation   has   simple interpretation as an adversarial learning game  while the feature space   tries to map the mean feature embeddings
of the real distribution Pr and the fake distribution    to
be far apart  maximize the  cid   distance between the mean
embeddings  the generator    tries to put them close one
to another  Hence we refer to this IPM as mean matching
IPM 

  IPM Levelsetsoff   Pkj huj   ihvj   ik uj vjleftandrightsingularvectorsof           IPM Levelsetsoff   hv   iv                       hu   ihv   id hu   ihv   iLevelSetsofc hu   ihv    McGan  Mean and Covariance Feature Matching GAN

We devise empirical estimates of both formulations in
Equations   and   given samples  xi                
from Pr  and  zi                 from pz  The primal formulation   is more amenable to stochastic gradient descent since the expectation operation appears in   linear
way in the cost function of Equation   while it is non linear in the cost function of the dual formulation    inside
the norm  We give here the empirical estimate of the primal formulation by giving empirical estimates          
of the primal cost function 

      min
  

      cid   

max
 

 
 

  cid   

 xi   

   zi cid 

 
 

  cid   

An empirical estimate of the dual formulation can be also
given as follows 

      min
  

max

   cid cid cid cid cid 

 
 

  cid   

 xi   

 
 

  cid   

   zi cid cid cid cid cid  

In what follows we refer to the problem given in     and
    as  cid   Mean Feature Matching GAN  Note that while
    does not need real samples for optimizing the generator      does need samples from real and fake  Furthermore we will need   large minibatch of real data in order
to get   good estimate of the expectation  This makes the
primal formulation more appealing computationally 

  Related Work

We show in this Section that several previous works on
GAN  can be written within the  cid   mean feature matching
IPM  IPM    minimization framework 
   Wasserstein GAN  WGAN   Arjovsky et al    recently introduced Wasserstein GAN  While the main motivation of this paper is to consider the IPM indexed by
Lipchitz functions on     we show that the particular
parametrization considered in  Arjovsky et al    corresponds to   mean feature matching IPM 
Indeed  Arjovsky et al    consider the function set
parametrized by   convolutional neural network with   linear output layer and weight clipping  Written in our notation  the last linear layer corresponds to    and the convolutional neural network below corresponds to   Since  
and   are simultaneously clamped  this corresponds to restricting   to be in the  cid  unit ball  and to de ne in   constraints on the  cid  norms of   In other words  Arjovsky
et al    consider functions in Fv    where      
  Setting       in Equation   and       in Equation
  we see that in WGAN we are minimizing dFv  that
corresponds to  cid  mean feature matching GAN 
   MMD GAN  Let   be   Reproducing Kernel Hilbert
Space  RKHS  with   its reproducing kernel  For any valid

PSD kernel   there exists an in nite dimensional feature
map           such that             cid       cid    
For an RKHS   is noted usually        and satis es the
reproducing proprety 

 

         cid       cid     for all        

has   simple expression 

Setting    cid   cid cid        cid  in Equation   the IPM dF
   cid cid 

       
   

dF         

sup

       cid cid     
   cid cid cid cid cid cid         cid cid cid cid cid cid  

   
 

        is the so called kernel mean
where        
   
embedding  Muandet et al    dF in this case is the so
called Maximum kernel Mean Discrepancy  MMD   Gretton et al      Using the reproducing property MMD
has   closed form in term of the kernel    Note that IPM 
is   special case of MMD when the feature map is  nite
dimensional  with the main difference that the feature map
is  xed in case of MMD and learned in the case of IPM 
 Li et al    Dziugaite et al    showed that GANs
can be learned using MMD with    xed gaussian kernel 
   Improved GAN  Building on the pioneering work of
 Goodfellow et al     Salimans et al    suggested
to learn the discriminator with the binary cross entropy criterium of GAN while learning the generator with  cid  mean
feature matching  The main difference of our IPM  GAN
is that both  discriminator  and  generator  are learned using the mean feature matching criterium  with additional
constraints on  

  Covariance Feature Matching GAN
  IPM  Covariance Matching IPM
As follows from our discussion of mean matching IPM
comparing two distributions amounts to comparing    rst
order statistics  the mean of their feature embeddings  Here
we ask the question how to incorporate second order statistics      covariance information of feature embeddings 
In this Section we will provide   function space   such
that the IPM in Equation   captures second order information  Intuitively   distribution of points represented in
  feature space can be approximately captured by its mean
and its covariance  Commonly in unsupervised learning 
this covariance is approximated by its  rst   principal components  PCA directions  which capture the directions of
maximal variance in the data  Similarly  the metric we de 
 ne in this Section will  nd   directions that maximize
the discrimination between the two covariances  Adding
second order information would enrich the discrimination
power of the feature space  See Figure  

McGan  Mean and Covariance Feature Matching GAN

This intuition motivates the following function space of bilinear functions in    

FU              
 uj vj    Rm orthonormal                     

 cid uj     cid cid vj     cid 

  cid   

Note that the set FU    is symmetric and hence the IPM
indexed by this set  Equation   is well de ned  It is easy
to see that FU    can be written as 

FU              cid   cid       cid   cid cid cid cid 

       Rm      cid     Ik     cid     Ik       

the parameter set   is such that the function space remains
bounded  Let

       
   

     cid 

be the uncentered feature covariance embedding of    It is
easy to see that  
      can be written in terms of      
   
and    

         

    cid   cid       cid   cid      race   cid      
 
   
For   matrix     Rm    we note by       the singular value of                  in descending order  The  
schatten norm or the nuclear norm is de ned as the sum of
singular values   cid   cid 
       We note by      the
kth rank approximation of    We note Om         
Rm     cid     Ik  Consider the IPM induced by this
function set  Let              we have 

   cid  

dFU             sup

  FU   

 
   
     

         
   

     

max

     Om  

 
  max
   
 
     Om  
  max
 

         
   
  race cid   cid           cid 
            
  max
 
   cid           cid 
  max

  cid   

 

where we used the variational de nition of singular values and the de nition of the nuclear norm  Note that     
are the left and right singular vectors of          
Hence dFU    measures the worst case distance between
the covariance feature embeddings of the two distributions 
this distance is measured with the Ky Fan knorm  nuclear
norm of truncated covariance difference  Hence we call
this IPM covariance matching IPM  IPM 

  Covariance Matching GAN

Turning now to the problem of learning   generative model
   of Pr         using IPM  we shall solve 

min
  

dFU     Pr    

this has the following primal formulation 

min
  

      Om  

max

           

 

 

 

max

min
  

  Pr cid   cid       cid   cid 

where                
  pz cid   cid         cid     cid   
   
or equivalently the following dual formulation 
   cid Pr         cid 
where       Ez pz          cid 
The dual formulation in Equation   shows that learning
generative models with IPM  consists in an adversarial
game between the feature map and the generator  when
the feature maps tries to maximize the distance between
the feature covariance embeddings of the distributions  the
generator tries to minimize this distance  Hence we call
learning with IPM  covariance matching GAN 
We give here an empirical estimate of the primal formulation in Equation   which is amenable to stochastic gradient  The dual requires nuclear norm minimization and is
more involved  Given  xi  xi   Pr  and  zj  zj   pz 
the covariance matching GAN can be written as follows 
 

            

max

min
  

      Om  

where               

 
 

  cid   cid   cid xi     cid xi cid 

 
 

 

  cid   cid   cid   zj     cid   zj cid   

  Mean and Covariance Matching GAN

In order to match  rst and second order statistics we propose the following simple extension 

min
  

max

       

     Om  

                      

that has   simple dual adversarial game interpretation

max

   cid         cid    cid Pr         cid 
min
  
where the discriminator  nds   feature space that discriminates between means and variances of real and fake  and
the generator tries to match the real statistics  We can also
give empirical estimates of the primal formulation similar
to expressions given in the paper 

 

McGan  Mean and Covariance Feature Matching GAN

  Algorithms
We present in this Section our algorithms for mean and covariance feature matching GAN  McGan  with IPM   and
IPM 
Mean Matching GAN  Primal    We give in Algorithm
  an algorithm for solving the primal IPM   GAN    
Algorithm   is adapted from  Arjovsky et al    and
corresponds to their algorithm for       The main difference is that we allow projection of   on different  cid   balls 
and we maintain the clipping of   to ensure boundedness of
   
  For example for       projB cid 
For       we obtain the same clipping in  Arjovsky et al 
  projB cid        clip         for      
Dual    We give in Algorithm   an algorithm for solving the dual formulation IPM   GAN     As mentioned
earlier we need samples from  real  and  fake  for training
both generator and the  critic  feature space 
Covariance Matching GAN  Primal    We give in Algorithm   an algorithm for solving the primal of IPM 
GAN  Equation   The algorithm performs   stochastic gradient ascent on          and   descent on   We
maintain clipping on   to ensure boundedness of   and
perform   QR retraction on the Stiefel manifold Om    Absil et al    maintaining orthonormality of   and    

      min 

 
 cid   cid 

Algorithm   Mean Matching GAN   Primal    

Input    to de ne the ball of     Learning rate  nc
number of iterations for training the critic    clipping or
weight decay parameter    batch size
Initialize       
repeat

for       to nc do

Sample   minibatch xi                 xi   Pr
Sample   minibatch zi                 zi   pz
 gv                             
                  RMSProp        gv    
 Project   on  cid   ball    cid        cid   cid      
    projB cid  
    clip        Ensure   is bounded 
end for
Sample zi                 zi   pz
      cid     
          RMSProp     

      zi cid 

  cid  

   

until   converges

  Experiments
We train McGan for image generation with both Mean
Matching and Covariance Matching objectives  We show
generated images on the labeled faces in the wild  lfw 

Algorithm   Mean Matching GAN   Dual    

Input    the matching  cid   norm   Learning rate  nc
number of iterations for training the critic    clipping or
weight decay parameter    batch size
Initialize       
repeat

for       to nc do

      zi 

Sample   minibatch xi                 xi   Pr
Sample   minibatch zi                 zi   pz
    xi     
     
        cid cid  
          RMSProp     
    clip        Ensure   is bounded 

  cid  

  cid  

end for
Sample zi                 zi   pz
Sample xi                 xi   Pr         
    xi     
     
        cid cid  
          RMSProp     

  cid  

  cid  

      zi 

until   converges

Algorithm   Covariance Matching GAN   Primal    

Input    the number of components   Learning rate  nc
number of iterations for training the critic    clipping or
weight decay parameter    batch size
Initialize          
repeat

for       to nc do

Sample   minibatch xi                 xi   Pr
Sample   minibatch zi                 zi   pz
                        
                    RMSProp            
  Project   and   on the Stiefel manifold Om   
Qu  Ru   QR     su   sign diag Ru 
Qv  Rv   QR     sv   sign diag Rv 
    QuDiag su 
    QvDiag sv 
    clip        Ensure   is bounded 

end for
Sample zi                 zi   pz
      
          RMSProp     

  cid  

 

until   converges

    cid      zj       zj cid 

 Huang et al    LSUN bedrooms  Yu et al   
and cifar   Krizhevsky   Hinton    datasets 
It is wellestablished that evaluating generative models is
hard  Theis et al    Many GAN papers rely on   combination of samples for quality evaluation  supplemented
by   number of heuristic quantitative measures  We will
mostly focus on training stability by showing plots of the

McGan  Mean and Covariance Feature Matching GAN

loss function  and will provide generated samples to claim
comparable sample quality between methods  but we will
avoid claiming better sample quality  These samples are all
generated at random and are not cherrypicked 
The design of    and   are following DCGAN principles  Radford et al    with both    and   being
  convolutional network with batch normalization  Ioffe
  Szegedy    and ReLU activations    has output size bs             The inner product can then
equivalently be implemented as conv       or
flatten   Linear       We generate
      images for lfw and LSUN and       images on
cifar  and train with minibatches of size   We follow the
experimental framework and implementation of  Arjovsky
et al    where we ensure the boundedness of   by
clipping the weights pointwise to the range    
Primal versus dual form of mean matching  To illustrate the validity of both the primal and dual formulation 
we trained mean matching GANs both in the primal and
dual form  see respectively Algorithm   and   Samples are
shown in Figure   Note that optimizing the dual form is
less ef cient and only feasible for mean matching  not for
covariance matching  The primal formulation of IPM 
GAN corresponds to clipping         the original WGAN 
while for IPM  we divide   by its  cid  norm if it becomes
larger than   In the dual  for       we noticed little difference between maximizing the  cid  norm or its square 
We observed that the default learning rates from WGAN
    are optimal for both primal and dual formulation 
Figure   shows the loss      
IPM estimate  dropping
steadily for both the primal and dual formulation independently of the choice of the  cid   norm  We also observed
that during the whole training process  samples generated
from the same noise vector across iterations  remain similar in nature  face identity  bedroom style  while details
and background will evolve  This qualitative observation
indicates valuable stability of the training process 
For the dual formulation  Algorithm   we con rmed the
hypothesis that we need   good estimate of  Pr  in order
to compute the gradient of the generator   we needed to
increase the minibatch size of real threefold to      
Covariance GAN  We now experimentally investigate the
IPM de ned by covariance matching  For this section and
the following  we use only the primal formulation      
with explicit uj and vj orthonormal  Algorithm   Figure   and   show samples and loss from lfw and LSUN
training respectively  We use Algorithm   with      
components  We obtain samples of comparable quality to
the mean matching formulations  Figure   and we found
training to be stable independent of hyperparameters like
number of components   varying between   and  

Figure   Samples generated with primal  left  and dual  right 
formulation  in  cid   top  and  cid   bottom  norm      lfw     LSUN 

Figure   Plot of the loss of          WGAN          
during training of lfw  as   function of number of updates to   
Similar to the observation in  Arjovsky et al    training is
stable and the loss is   useful metric of progress  across the different formulations 

   updates         McGan  Mean and Covariance Feature Matching GAN

Figure   lfw samples generated with covariance matching and
plot of loss function  IPM estimate              

Figure   Cifar  Classconditioned generated samples  Within
each column  the random noise   is shared  while within the rows
the GAN is conditioned on the same class  from top to bottom
airplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck 

Table   Cifar  inception score of our models and baselines 
Uncond     Uncond    
     
     
     
     
     
     
     
     
 
     
     

  Sigma
  Sigma
Sigma
WGAN
BEGAN  Berthelot et al   
Impr  GAN  LS   Salimans et al   
Impr  GAN Best  Salimans et al   

Cond    
     
     
     
     

We con rm the improved stability and sample quality of
objectives including covariance matching with inception
scores  Salimans et al    in Table   Samples corresponding to the best inception score  Sigma  are given
in Figure   Using the code released with WGAN  Arjovsky et al    these scores come from the DCGAN
model with   extra layers   deeper generator and
discriminator    More samples are in appendix with combinations of Mean and Covariance Matching  Notice rows
corresponding to recognizable classes  while the noise  
 shared within each column  clearly determines other elements of the visual style like dominant color  across label
conditioning 

  Discussion
We noticed the in uence of clipping on the capacity of the
critic    higher number of feature maps was needed to compensate for clipping  The question remains what alternatives to clipping of   can ensure the boundedness  For
example  we succesfully used an  cid  penalty on the weights
of   Other directions are to explore geodesic distances
between the covariances  Arsigny et al    and extensions of the IPM framework to the multimodal setting
 Isola et al   

Figure   LSUN samples generated with covariance matching and
plot of loss function  IPM estimate              

Covariance GAN with labels and conditioning 
Finally  we conduct experiments on the cifar  dataset 
where we will leverage the additional label information
by training   GAN with conditional generator        
with label          supppplied as onehot vector concatenated with noise    Similar to Infogan  Chen et al 
  and ACGAN  Odena et al    we add  
new output layer      RK   and will write the logits
 cid       cid    RK  We now optimize   combination of
the IPM loss and the crossentropy loss CE             
  log  Softmax cid       cid    The critic loss becomes
  cid xi yi lab CE xi  yi       with
LD           
hyperparameter     We now sample three minibatches
for each critic update    labeled batch for the CE term  and
for the IPM   real unlabeled   generated batch 
The generator loss  with hyperparam     becomes  LG  
        
CE   zi  yi  yi       which
still only requires   single minibatch to compute 

  cid zi pz yi py

 

 

   updates   updates McGan  Mean and Covariance Feature Matching GAN

References
Absil       Mahony     and Sepulchre     Optimization
Algorithms on Matrix Manifolds  Princeton University
Press   

Arjovsky  Martin  Chintala  Soumith  and Bottou  Leon 

Wasserstein gan  ICML   

Arsigny  Vincent  Fillard  Pierre  Pennec  Xavier  and Ayache  Nicholas  Logeuclidean metrics for fast and simple calculus on diffusion tensors  In Magnetic Resonance
in Medicine   

Berthelot  David  Schumm  Tom  and Metz  Luke  Began 
Boundary equilibrium generative adversarial networks 
arXiv   

Chen  Xi  Duan  Yan  Houthooft  Rein  Schulman  John 
InterSutskever  Ilya  and Abbeel  Pieter 
pretable representation learning by information maximizing generative adversarial nets  In NIPS   

Infogan 

Dziugaite  Gintare Karolina  Roy  Daniel    and Ghahramani  Zoubin  Training generative neural networks via
maximum mean discrepancy optimization  In UAI   

Goodfellow  Ian  PougetAbadie  Jean  Mirza  Mehdi  Xu 
Bing  WardeFarley  David  Ozair  Sherjil  Courville 
Aaron  and Bengio  Yoshua  Generative adversarial nets 
In NIPS   

Gretton  Arthur  Borgwardt  Karsten    Rasch  Malte   
Sch olkopf  Bernhard  and Smola  Alexander    kernel
twosample test  JMLR   

Huang  Gary    Ramesh  Manu  Berg  Tamara  and
LearnedMiller  Erik  Labeled faces in the wild   
database for studying face recognition in unconstrained
environments  Technical report   

Ioffe  Sergey and Szegedy  Christian  Batch normalization 
Accelerating deep network training by reducing internal
covariate shift  Proc  ICML   

Isola  Phillip  Zhu  JunYan  Zhou  Tinghui  and Efros 
Alexei    Imageto image translation with conditional
adversarial networks  CVPR   

Muandet  Krikamol  Fukumizu  Kenji  Sriperumbudur 
Bharath  and Schlkopf  Bernhard 
Kernel mean
embedding of distributions    review and beyond 
arXiv   

Muller  Alfred  Integral probability metrics and their generating classes of functions  Advances in Applied Probability   

Nowozin  Sebastian  Cseke  Botond  and Tomioka  Ryota 
fgan  Training generative neural samplers using variational divergence minimization  In NIPS   

Odena  Augustus  Olah  Christopher  and Shlens  Jonathon 
Conditional image synthesis with auxiliary classi er
gans  arXiv   

Radford  Alec  Metz  Luke  and Chintala  Soumith  Unsupervised representation learning with deep convolutional
arXiv 
generative adversarial networks 
 

Rahimi  Ali and Recht  Benjamin  Random features for

largescale kernel machines  In NIPS   

Salimans  Tim  Goodfellow  Ian  Zaremba  Wojciech  Cheung  Vicki  Radford  Alec  Chen  Xi  and Chen  Xi  Improved techniques for training gans  In NIPS   

Sriperumbudur  Bharath    Fukumizu  Kenji  Gretton 
Arthur  Schlkopf  Bernhard  and Lanckriet  Gert      
On integral probability metrics  phi  divergences and binary classi cation   

Sriperumbudur  Bharath    Fukumizu  Kenji  Gretton 
Arthur  Schlkopf  Bernhard  and Lanckriet  Gert      
On the empirical estimation of integral probability metrics  Electronic Journal of Statistics   

Theis  Lucas  Oord    aron van den  and Bethge  Matthias 
ICLR 

  note on the evaluation of generative models 
 

Yu  Fisher  Zhang  Yinda  Song  Shuran  Seff  Ari  and
Xiao  Jianxiong  Lsun  Construction of   largescale image dataset using deep learning with humans in the loop 
arXiv   

Kingma  Diederik    and Welling  Max  Autoencoding

variational bayes  NIPS   

Zhao  Junbo  Mathieu  Michael  and Lecun  Yann  Energy

based generative adversarial networks  ICLR   

Krizhevsky     and Hinton     Learning multiple layers of

features from tiny images  Master   thesis   

Li  Yujia  Swersky  Kevin  and Zemel  Richard  Generative

moment matching networks  In ICML   

Mirza  Mehdi and Osindero  Simon  Conditional genera 

tive adversarial nets  arXiv   

