SPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

Junichiro Hirayama     Aapo Hyv arinen     Motoaki Kawanabe    

Abstract

We present   novel probabilistic framework for
  hierarchical extension of independent component analysis  ICA  with   particular motivation in neuroscienti   data analysis and modeling  The framework incorporates   general subspace pooling with linear ICAlike layers stacked
recursively  Unlike related previous models  our
generative model is fully tractable  both the likelihood and the posterior estimates of latent variables can readily be computed with analytically
simple formulae  The model is particularly simple in the case of complexvalued data since
the pooling can be reduced to taking the modulus of complex numbers  Experiments on electroencephalography  EEG  and natural images
demonstrate the validity of the method 

  Introduction
Linear component analysis and pooling are two fundamental concepts of unsupervised representation or feature
learning on continuousvalued data  The basic method for
linear decomposition is independent component analysis
 ICA   Hyv arinen et al      or sparse coding  Pooling
originates from computational models of  complex cells 
in the visual cortex  Hubel   Wiesel    Adelson  
Bergen    which typically takes the sum of squares
of components or neuronal outputs    pooling  to achieve
invariances in higher features  The combination of the two
concepts have so far found many applications  including
advanced image recognition by deep neural networks 
In the present study  we focus on applications of great
current interest related to neuroscience engineering  such

 RIKEN Center for Advanced Intelligence Project  AIP 
Tokyo  Japan  Advanced Telecommunications Research Institute International  ATR  Kyoto  Japan  Department of Computer Science and HIIT  University of Helsinki  Finland  Gatsby
Computational Neuroscience Unit  University College London  UK  Correspondence to 
Junichiro Hirayama  junichiro hirayama riken jp 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

as electroor magnetoencephalography  EEG MEG  signal analysis and natural image statistics  Hyv arinen et al 
  Related previous studies have longly attempted to
combine together linear component analysis and pooling
and further built them up to hierarchical probabilistic models  Hyv arinen   Hoyer    Hyv arinen et al     
Valpola et al    Karklin   Lewicki    Shan et al 
  Onton   Makeig    Cadieu   Olshausen   
Hirayama et al    Hosoya   Hyv arinen    among
which one of the earliest combination of ICA with pooling was independent subspace analysis  ISA   Hyv arinen
  Hoyer      more general energybased modeling
 EBM  framework       Osindero et al    Salakhutdinov   Hinton      oster   Hyv arinen    Ngiam
et al    has also been popularly used  These developments were somewhat parallel with the rise of general
unsupervised deep learning techniques  see       Bengio
et al    for review  while those neuroscienti   applications speci cally seek simple explanations and interpretations of data  and even twoor threelayer architectures
have been of practical relevance 
However  related previous models were highly intractable 
and they necessarily resorted to approximative or nonconventional methods       for learning and inference 
Such   lack of theoretical transparency  as well as the computational dif culties associated  has hindered their extensive applications and further developments  Speci cally 
hierarchical generative models usually need approximations or numerical methods to evaluate the posterior estimates on latent variables or the likelihood  Bengio et al 
  EBM may avoid approximate posterior computation  while being still hampered by an intractable partition function  normalizing constant  to compute the likelihood  Conventional maximum likelihood  ML  estimation
is thus not easily applicable in both types of models  In
practice  simply stacking together ICA ISA models trained
layerwise  or with  xed lower layers  has often been used
as an alternative       Shan et al    Onton   Makeig 
  Le et al    Cadieu   Olshausen    Hosoya
  Hyv arinen    although its theoretical underpinning
is rather unclear 
Here  we present   simple  fully tractable statistical framework for   hierarchical extension of ICA with an intrinsic pooling mechanism  We will refer to the framework as

SPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

SPLICE  abbreviating stacked pooling and linear components estimation  By fully tractable  we mean that both the
posterior estimates on latent variables and the likelihood
function associated are given by simple  computable  analytical formula without resorting to any approximations  In
this sense  both general hierarchical generative models and
EBMs have only   limited tractability 
Our SPLICE extends ISA so that the subspaces may be dependent of each other via higher layers  latent variables 
the layers can in principle be stacked recursively without
violating the full tractability of the model  In the present
study  we speci cally introduce the basic framework and
  practical learning scheme which combines   layerwise
ICA pretraining with an unsupervised  netuning of the entire layers by nonapproximate ML  As   proof of concept 
we also demonstrate the method with EEG and natural images  as commonly targeted in related previous studies  The
method turns out to have interesting connections to neural
networks  which we also brie   discuss below 

  Proposed Method
  FirstLayer Model

We begin with formulating the generative model for our
SPLICE  Denote by xt observed data vectors     
               either realor complexvalued  consisting of
  entries xit  Each of the   entries is given by   linear
combination of the same number of unknown  rstlayer 
components or sources  collectively denoted as source vector st  Here  we consider the fundamental case where xt
and st are independently and identically distributed        
Omitting sample index   for notational simplicity  we write

    As 

 

where the coef cient matrix    called mixing matrix  is
square and assumed to be invertible  the inverse    
   is called demixing matrix  For convenience  we assume without loss of generality that   and   are zeromean 
by subtracting the sample mean from original data vectors 
Like ISA  we divide the    rstlayer sources into   groups
without overlapping  and denote by      the vector consistj  dj 
Hence      represents   djdimensional subspace in the
original data space  spanned by the corresponding columns
in    Unlike ISA  however  the   source vectors      may
be dependent of each other in our generative model 

ing of the dj sources in the jth group       cid  

  SecondLayer Model

To extend the model to multiple layers by modeling the
dependencies between the subspaces  we introduce an additional  second  layer on the top of the above ISAlike  rst
layer model  Note that we don   count the pooling as   sep 

    General model

    Special case

Figure   Generative model of SPLICE        higher layer
directly gives the squared   norms of lower sources  
within each subspace      An important special case having
one complex source   per subspace 

arate layer  so what we call the second layer is called the
third layer in some previous work 
Speci cally  we assume that source vectors      may be dependent of each other in their  powers  or  energies  as
typically quanti ed by their  squared    norms  cid     cid 
We model the dependency using   linear mixing of additional  secondlayer  sources with pointwise nonlinearities 

 cid     cid       

   cid   cid   

 

 

                  

 
where   monotonic link function Fj         maps
 nonnegative  squared norms into real values  with its in 
    cid  and   cid  are invertible mixing maverse denoted by    
trix and source vector  and   cid      cid  is demixing matrix  of second layer  and    denotes jth entry of   vector 
For later convenience  we denote by   cid      cid   cid   with entries   cid 
To fully specify the generative model  in the present study 
we simply put   sphericality assumption on every          
we assume that the corresponding normalized vector

   the  observed  data vector for the second layer 

           cid     cid 

 

for every    is uniformly distributed on the unit hypersphere  independently of any other random variables 

  Third Layer and Beyond  and   Special Case

An intriguing fact with our model is that it can in principle be extended with any number of layers  Fig      up
to the limit of subspace partitioning  This can be done by
recursively stacking   higher layer   to generate the lower
layer   subspace norms  with the lower layer appropriately
partitioned into subspaces  Note that in the second and further layers  complexvalued variables may not be useful at
least in our current        setting  At the top layer  we simply assume that the sources are mutually independent and

SPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

nonGaussian  throughout our experiments in Section   we
used   typical prior          sech    which corresponds to the conventional tanh nonlinearity in ICA 
On the other hand  if one   goal is primarily to give   simple explanation of data  adding extra layers might overcomplicate the model 
In fact    simpli ed special case
of our model  Fig      having only two layers with
one complexvalued source per subspace        sj   
   cid   cid    may already have   high practical relevance
   
in the context of neuroscienti   data analysis and modeling       Onton   Makeig    Cadieu   Olshausen 
  Hirayama et al    Then the squares  sj  and
arguments arg sj speci cally represent the power  squared
amplitude  and phase of an oscillatory source signal  where
the sphericality assumption reduces to the circularity of sj 
     the phase is uniform  and is independent of the power 

 

  Choice of Intermediate Nonlinearity Fj
The true forms of Fj in   are usually unknown and ideally
they would be learned from the data by either parametric or
nonparametric methods  However  in practice  it is presumably suf cient that they are  xed  as    rst approximation 
so that the computational costs can be reduced 

Logarithm Conventionally  one typical option is the logarithm       Valpola et al    Karklin   Lewicki   
Cadieu   Olshausen        

Fj      ln jq 

 

where    is   nonnegative scale parameter  The scale parameter    can in fact be arbitrarily chosen  because one
cannot determine the true scales of the sources due to the
inherent scaling ambiguity as in ICA  To avoid this ambiguity  we speci cally set    so that

Fj     

 

Gaussianization Another popular choice in the literature
is  radial  Gaussianization  Chen   Gopinath    Shan
et al    Lyu   Simoncelli    generally given by

Fj         jq 

 

where   and    denote the cumulative distribution function  cdf  of standard Gaussian distribution and that of  
certain distribution over   respectively  Gaussianization originally had no generative interpretation but we may
use the principle as an intuitive  adversarial  de nition of
Fj       any nonGaussianity in the data comes from the
nonGaussianity of the secondlayer sources   cid 
   one may
simply set the cdf       of qj    cid     cid  as chisquared
with an appropriate degrees of freedom  so that      is Gaussian when the second layer   cid 
  is  standard  Gaussian  The

scale parameter    can be  xed in the same manner as
above  Figure   illustrates the forms of this type of   and
    for complex sources when dj    

Figure   Forms of Gaussianizationbased nonlinearity  
 left panel  and its inverse      right  for           dj    

  Properties of the Model

Now we will show that the hierarchical probabilistic model
formulated above is in fact fully tractable  To ease exposition  we will give the result only for the twolayer case  but
the generalization with more layers is straightforward 

  THE PDF IS ANALYTICALLY NORMALIZED

First  we show that the probability density function  pdf  of
observed data vector    associated with our model  is analytically normalized       the density has   unit sum without
any intractable normalizing constant  Thus  the likelihood
of our model can easily be evaluated without approximative
or numerical techniques  The lack of this desirable property has long been an obstacle in hierarchical generative or
energybased modeling combined with pooling 
To derive the pdf   rst observe that the linear map    cid   
implies that        ps Wx  det      where ps  denotes the pdf of  rstlayer source vector   and       and
  for realand complex cases  respectively       Adali  
Haykin    sec    Then  the following theorem explicitly relates ps  to the second layer   pdf of  cid     cid 
Theorem   Denote by   the vector having qj    cid     cid 
in the jth entry  Assume that     has the pdf given by
pq            qm  and   unit vectors uj       cid     cid  are independent of   and uniformly distributed in  the Cartesian
products of  the corresponding unit hyperspheres  Then 

 cid 

 

ps      pq cid   cid         cid     cid 

   cid     cid 

 

where    qj      dj  
  dj

 dj dj  if complex 

 

 

 dj dj    if   is real  or

The proof is given in Supplementary Material    Theorem   is   generalization of the result on single sphericallysymmetric random vector       Ollila et al   
The formula   in fact holds for any probabilistic model
pq  of the second layer  Our SPLICE speci cally introduces the model   which resembles  postnonlinear 

    Flogx     expSPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

ICA  Taleb   Jutten    implying that

  cid 

exp cid Hk   cid 

         cid    cid 

pq     

 fj qj  det   cid 

  

  

    cid     cid 

       ln     cid 
      cid 

where Hk   cid 
   are  xed functions that correspond to any typical choice of nonGaussian prior such that
   We denote the entrywise mapping Fj
    Rd and the  rst derivatives of
  the kth transposed row of

collectively by     Rd
Fj by fj  We also denote by   cid 
  cid  and by   dot operator the standard inner product 
Taken together  we eventually obtain the pdf of    which is
analytically normalized 

kjFj cid       cid cid  det   cid 

  cid 

      

  cid 
 cid 
exp cid Hk 
    cid 

  

 

exp Gj cid       cid  det     

 

  

where      consists of only the dj rows in   so that       
       and Gj        ln fj      ln      

  EXACT POSTERIOR ESTIMATE VIA POOLING

Second  we see that our model also allows   simple analytical estimate on latent variables  which was in fact already
implied by the above development  The inverse process can
readily be obtained for the two linear layers by     Wx
and   cid      cid   cid  since we assumed that both   and   cid  are
invertible  The remaining part that links the  rst layer   and
the second layer   cid  is also readily given from Eq    as

    Fj cid     cid    Fj 
  cid 

 si 

 

 cid 

  Sj

for every    where Sj denotes the index set for the sources
belonging to the jthe subspace  Hence the overall transformation from observed   to the toplevel representation   cid  is
given by an analytically very simple form  Although this
consequence is almost obvious from the de nition of our
model  this is still remarkable since previous hierarchical
generative models usually did not possess such   tractability which in fact partly led to the invention of EBM 
Note that the relation   essentially implements an   
pooling operation 
Interestingly  the two demixing layers  interleaved by the pooling layer  constitute   simpli 
 ed multilayer neural network with linear neurons  Thus 
one may also view SPLICE as   principled framework for
unsupervised learning of   multilayer neural network with
pooling layers 
We remark that the framework of SPLICE can even be extended with other ingredients of neural networks without

violating the full analytical tractability  which will be an
interesting open topic for future study  For example  nonlinear activation functions       leaky recti ed linear unit 
and other types of pooling       Lppooling  by introducing
   cid       
  sign    can readily be incorporated at least if the
extra nonlinearity is bijective by itself and the associated
Jacobian determinant is analytically tractable 

  Learning by Maximum Likelihood  SPLICEML 

Next we develop the method for parameter estimation
 learning  in our generative model  The analytically simple form of the pdf   makes conventional maximum likelihood  ML  estimation readily applicable  which theoretically has   number of desirable properties  To obtain an
ML estimate  we simply minimize the sample average of
the corresponding loss function        ln        const 
given for the twolayer case by

  cid 
  cid 

  

  cid 

kjFj cid       cid cid    ln  det   cid 

 cid cid 
 cid cid       cid cid      ln  det   

 

Hk

Gj

   

 

 

  

From the neural network or EBM viewpoint  Eq    is interesting since the loss is associated with not only the output  the top  but also the intermediate  lower  layers  Such
  layerspeci   loss has seldom been used in the literature 
In practice  the logdeterminant terms in   may lead to
  computational dif culty due to the costly matrix inversion when evaluating the gradient  Fortunately  the popular
 stochastic  natural gradient method  Amari et al    is
readily applicable for our model just like ICA  which can
eliminate the need for matrix inversion  In our experiment
 Section   however  we actually used the limitedmemory
BFGS quasiNewton method  Schmidt    with an explicit matrix inversion  as it converged empirically faster in
our setting  results not shown 
Since the objective function is not convex  the optimization needs to start with   good initial estimate not to stack
with poor local optima  We use the following twostep approach that resembles   typical pretraining netuning strategy in the deep learning literature  That is  we  rst perform
  layerwise learning developed below  SPLICELW  and
then optimize the likelihood of the entire layers  SPLICEML  by starting from the layerwise solution  Note that both
steps are unsupervised  in contrast to typical  netuning
strategies in deep neural networks which are supervised 

  Practical Layerwise Learning  SPLICELW 

Our unsupervised layerwise learning scheme combines
ICA with an adaptive subspace partitioning for pool 

SPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

 cid 

 

    cid 

jks cid 

  Hyv arinen    Hirayama et al         sj  
      
   under   conditional Gaussianity assumption  However  this approach usually leads to intractability in learning and inference except for restricted
special cases  Hyv arinen   Hoyer    Hirayama et al 
  Alternatively  two recent studies modeled  sj 
rather than the variance  Cadieu   Olshausen    Hirayama   Hyv arinen    but neither of them implemented subspace pooling with   full tractability 
In practice  many previous studies have rather preferred the
layerwise learning strategy  which stacks together ICA ISA
models trained layerwise  or apply ICA upon  xed lower
layers  Shan et al    Onton   Makeig    Cadieu  
Olshausen    Hosoya   Hyv arinen    the stacked
ISA strategy has also been developed in other application
 eld  Le et al    Our new development may give  
theoretical basis for the previous layerwise approach and
also provides   principled unsupervised  netuning method 

ing    similar approach has previously been studied for
ISA  Szab   et al    see also Hosoya   Hyv arinen 
  Speci cally  we  rst perform an ICA to estimate the
sources sit up to their permutation  and then solve   simple
optimization problem  see below  to assign the sources into
  preset number of subspaces  except for the special case of
dj     which adaptively partition the data space into subspaces  The input to the upper layer can then be computed
by   for which ICA is applied again  For   general number of layers  the procedure is recursively applied 
The idea of the adaptive subspace partitioning scheme is
that our model implies that the sources  correlationin 
squares  ij   corr si sj      cid     are constant  kl
if si   Sk and sj   Sl because of the   sphericality
within each subspace  matrix      ij  thus has   block
structure after an appropriate permutation of rows and
columns  This observation leads to   simple objective function  cid    ZT   cid   with Frobenius norm  to be minimized with respect to      zki  and      kl  where
           is   subspace assignment matrix such that
zki     if and only if source   belongs to subspace   
The problem further reduces to an equivalent maximization

of  cid cid   cid  
  cid  with respect to cid      ZZT    Supplementary Material    where  cid   is necessarily nonnegative
and  cid   cid  

     We thus borrow the idea of orthogonal relaxation from spectral clustering  Yu   Shi    see also
Ding et al    and alternatively solve

 

 cid   VT cid 

     VVT          Rm  
   

 

max

 

which previously appeared in   rather different context  Hirayama et al    Due to the joint orthogonality and
nonnegativity  solution   has at most one nonzero entry in
each column  which readily gives the subspace assignment 
In our experiment below  we used an alternating projected
gradient algorithm to solve   which empirically worked
very well for natural images  Section  

  Related Work
  Natural Image Statistics and EEG MEG Analysis

Our primary motivation for the development of   simple
and tractable hierarchical probabilistic model is in data
analysis and modeling related to neuroscience engineering 
In fact  many preceding studies on natural image statistics
and EEG MEG data analysis developed hierarchical extensions of ICA  while their intractability has hindered extensive applications or further developments 
  conventional approach was to make the second layer
explain the dependency in variances of  rstlayer sources
 Hyv arinen   Hoyer    Hyv arinen et al     
Valpola et al    Karklin   Lewicki    Zhang

  EnergyBased Modeling  EBM 

Another line of research on natural image statistics have
used the energybased modeling  EBM  strategy      
Osindero et al    Salakhutdinov   Hinton   
  oster   Hyv arinen    Ngiam et al    instead of
the hierarchical generative approach  However  EBM suffers from computational dif culties related to an intractable
partition function  as well as limited interpretability since
there are no independent latent variables 
To compare our model with EBM in Section   we introduce an EBM with deterministic hidden units that corresponds to our SPLICE in the twolayer setting  The associated loss function       the negative logpdf up to irrelevant
additive terms  is given by

 cid cid       cid cid cid 

  cid 
kjFj

  lnZ   

 cid 

 cid cid 

LEBM  

Hk

 

 

where        cid  is the partition function to ensure

 cid      dx     We emphasize that partition function  
 cid cid       cid cid  are generally not independent in
 cid 
    cid 

in the EBM is intractable while it is simple and tractable
in SPLICE  Moreover  the Hk now lacks the connection to
the prior pdf     cid 
   

   of independent sources  hence    cid 

kjFj

EBM  which is   clear distinction from SPLICE 

  Nonlinear ICA and Deep Generative Models

From   more general perspective  another important type
of hierarchical extension of ICA is nonlinear ICA using
multilayer neural networks  Almeida    Dinh et al 
  Hyv arinen   Morioka      The difference from
SPLICE  or other related models  is that the theory of
nonlinear ICA basically assumes   bijectivity between ob 

SPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

servations and  nonlinear  independent components  Furthermore  the general nonlinear ICA model is not identi 
able  Hyv arinen   Pajunen     for an alternative approach  see Hyv arinen   Morioka        In contrast 
simulations below indicate that our model is identi able 
although we don   have   formal proof 
In   related context  some authors  Deco   Brauer   
Dinh et al    have pointed out and addressed the computational dif culty associated with the Jacobian determinant of the multilayer neural network  Fortunately  SPLICE
explicitly decomposes the Jacobian determinant into analytically tractable terms  Eq    and the popular natural
gradient technique for ICA can further simplify the computation  Section  
Recently  several new techniques have been made available for learning and inference in generalpurpose hierarchical  deep  generative models on continuous data  such as
variational autoencoder methods and nonclassical learning principles       Kingma   Welling    Kingma
et al    Goodfellow et al    Rezende   Mohamed 
  Hyv arinen   Morioka        These developments mainly seek   computational tractability of learning
and inference  maintaining the complexity  representation
capability  of the model  In contrast  our SPLICE rather
reduces the complexity  while keeping the essence  of the
model to achieve the analytical tractability as well as the
interpretability  with   particular emphasis on the tractable
pooling  The motivations  as well as the target applications 
are therefore quite distinct between the two approaches 

  Experimental Results
In this section  we demonstrate our SPLICE in   simulation
study and with two motivating real datasets 

  Synthetic Data

First  we examined the important special case of SPLICE
 Fig      with   synthetic dataset  and further with   real
EEG dataset  Section   The goal was to demonstrate the
validity of the basic concept of SPLICE       combining
pooling and linear layers in   fully tractable manner  as
well as its practical relevance in exploratory signal analysis 
The twolayer model assumes that both observed   and
 rstlayer source vectors   are complexvalued  where the
pooling operation reduces to taking the squared modulus
of each scalar source variable  the adaptive subspace partitioning  Section   was not necessary in this basic case 
SPLICELW consecutively performed real and complexvalued versions of FastICA  Hyv arinen    Bingham
  Hyv arinen    SPLICEML used the SPLICELW
solution as the initial estimate  For comparison  we also
trained the EBM   by noisecontrastive estimation  Gut 

Figure   Synthetic Data  performance index  Amari et al 
   top  smaller is better  and mean absolute correlations of true and estimated sources  bottom  larger is better  in each of the two layers  left  Layer   right  Layer  
The plot shows the mean and standard deviation of   runs 

mann   Hyv arinen     EBMNCE  using SPLICELW
as the initial estimate  We generated the reference  noise 
dataset for NCE from multivariate Gaussian of the same
mean and covariance as the original  having ten times larger
sample size  All methods used Gaussianizationbased Fj
 Fig    speci cally with           exp          the
cdf of Exponential distribution  and       ln     
  simple simulation was performed to compare the basic
performance of the three methods for blind source separation in each layer  Fig    The dataset consisted of
 dimensional complexvalued vectors xt which we synthesized by our generative model  We generated the toplayer sources   cid 
  from tdistribution of the three degrees of
freedom and every entry in   cid  and  the real and complex
parts of    uniformly in     For simplicity  we used
the same Gaussianizationbased     to generate the data 
As seen in the  gure  both SPLICELW and SPLICEML
clearly outperformed EBM in Layer   which well corresponds to the lack of prior independence in EBM  Section   In Layer   all the methods seem to have correctly
recovered the  rstlayer sources  as indicated by the high
mean absolute correlations  while the two SPLICE methods exhibited improvements in accuracy particularly with
smaller sample sizes  The consistent improvements  or
nondegradation  by SPLICEML over SPLICELW also

Sample Size Performance Index Layer  EBMNCESPLICE LWSPLICEMLSample Size Performance Index Layer  Sample Size Mean Absolute Correlation Layer  Sample Size Mean Absolute Correlation Layer  SPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

demonstrated the effect of unsupervised  netuning 

  EEG Data

To further demonstrate the applicability to exploratory
analysis of neuroimaging signals  we applied the same
methods to   publicly available EEG datasets  Datasets
   Blankertz et al    from the BCI competition IV
 http www bbci de competition iv  The data were measured in four human subjects during   number of trials of   twoclass cued motor imagery task  see Supplementary Material   for the details of data preprocessing 
We eventually obtained the complexvalued data vectors
xt      by concatenating   sensor channels  complex timefrequency spectra   points within  Hz  typical   and   bands  at every time points indexed by    
              As   preprocessing  PCA reduced the dimensionality with the   of total variance kept 
The idea for the analysis was that the amplitude  sj  of oscillatory EEG sources might couple together to represent
higherorder information  in particular  that associated with
the ongoing task states       imagery of two different motor modalities like left and right hands  To verify this  we
evaluated individual secondlayer sources   cid 
   obtained in
the unsupervised manner by each of the three methods  in
terms of their relevance to discriminating the task states 
Speci cally  we calculated AUC  area under the ROC
curve  as the relevance measure  by regarding the withintrial average of every   cid 
  as   single discriminant score
 Fig    We also evaluated the similar score on another
dataset  provided originally for the evaluation purpose in
the competition  by transferring the same model without modi cation  Supplementary Material    For both
datasets  the increase of the fraction of highAUC components   cid 
  by the two SPLICE methods is evident by the
heavier upper tails as well as the QQ plots above the
straight lines  the effect of  netuning was unclear in this
result  The result implies that SPLICE may effectively
discover taskrelated functional couplings of source amplitudes  which will be practically useful to enhance further
explorations of data or help consolidating new hypotheses 

  Natural Images

Finally  we demonstrate the validity of our general SPLICE
model  Fig      using  realvalued  natural images obtained from ImageNet    Deng et al    We followed  Hosoya   Hyv arinen    for basic preprocessing  Image patches were of       pixels  with the pixel
values in each patch normalized to have zero mean and unit
variance  The dimensionality was then reduced to      
by PCA  The logarithmic nonlinearity   was commonly
used in both SPLICE and EBM 

Figure   EEG Data  relevance of individual secondlayer
components   cid 
  to the task state  Left  distributions of AUC
scores by the three methods   median     st    rd
quartiles   cid   percentile  for original and transfer data 
Right  quantilequantile  QQ  plots between SPLICELW
and EBMNCE  yellow line connects  st and  rd quartiles 
extended by red dashed line   cid   percentile 

We  rst quantitatively compared twolayer SPLICE and
EBM  as well as   singlelayer ICA  by their test loglikelihood evaluated with  fold crossvalidation  CV 
for two different sample sizes    Table   The number
of secondlayer sources was commonly set as      
SPLICELW initialized the model parameters and subspace
partitioning in both SPLICEML and EBMNCE  The likelihood of EBM was numerically evaluated with hybrid annealed importance sampling  AIS   Ngiam et al   
SohlDickstein   Culpepper      For   reference  we
also give another result of SPLICE when its partition function     was evaluated numerically by AIS  The table
indicates   superior performance of SPLICEML  followed
by SPLICELW  as compared to ICA or EBMNCE  The
AIS was very accurate in this result  The low performance
of EBM could be attributed to either the model difference
or the relative inef ciency  in sample size  of NCE as compared to the  quasi ML estimators of SPLICEML LW 
The twolayer SPLICE model can be considered   tractable
and generative counterpart to existing models in natural image statistics       Gutmann   Hyv arinen    Hosoya  
Hyv arinen    Note that our second layer actually corresponds to their third layer as we do not count the intermediate pooling layer  In fact  our model qualitatively reproduced local spatial pooling of  rstlayer linear features
 Fig      as well as various types of excitatory inhibitory

 We obtained the Matlab code from https github com SohlDickstein HamiltonianAnnealed ImportanceSampling  We set
the number of intermediate distributions as  

EBMNCESPLICE LWSPLICEML AUCOriginalEBMNCESPLICE LWSPLICEML Transfer SPLICELWQ   PlotOriginal EBMNCE SPLICELWTransferSPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

    First Layer

    Second Layer

    Effect of Increasing Layers

Figure   Natural Images  visualizations and the effect of increasing layers      Examples of  rst layer   feature pooling
learned by twolayer SPLICEML          one pool per row  randomly selected out of   In each row  the leftmost
panel superimposes oriented bars that  iconify   Gutmann   Hyv arinen    the pooled Gaborlike feature detectors  
illustrated on its right      All the   secondlayer feature detectors   cid 
   Each panel visualizes   weighted superposition
 using weights   cid 
kj  separately within positive and negative signs  of the   binarized iconi ed images of pooled  rstlayer
features       the leftmost panels in     Red and blue corresponds to positive and negative signs and their thickness to
absolute values  normalized in each sign to span the color ranges  The global sign of each   cid 
   originally inde nite  was
chosen so that its maximum absolute entry was positive      Test average loglikelihood computed for both SPLICELW
and ML with the numbers of layers     and   The mean and standard deviations by  fold cross validation are shown 

Table   Natural Images  comparison with ICA and EBM 
Test average loglikelihood evaluated by  fold cross validation  mean SD  for different sample sizes   

ICA

Method

Test Avg  LogLikelihood  fold CV 
       
       
       
       
SPLICELW        
     
     
       
SPLICEML
     
     
SPLICEML 
     
     
EBMNCE 
  with   numerically estimated partition function 

patterns on pooled  rstlayer features  Fig      which theoretically models the properties of cortical neurons in the
visual area     Hosoya   Hyv arinen   
Finally  we examined the effect of increasing the number of
layers  Fig              The threelayer SPLICE
model included an additional pooling layer reducing from
  to   inputs to the thirdlayer ICA  the fourlayer model
further added the pooling and ICA layers which reduces
the dimensionality from   to     remarkable increase
of test likelihood by adding the layers is seen in SPLICELW    weak increase was seen but not completely evident
in SPLICEML  We therefore conclude that adding layers
was effective to compensate errors in the pretrained lower
layers  while only two layers  but not one layer  see ICA
in Table   may be almost suf cient to represent the data
when combined with  netuning  This result could be accounted for by either misspeci cation of thirdand fourthlayer models       the type of nonlinearity   or the number

of subspaces  or limited statistical regularities that can be
present in small image patches 

  Conclusion
We introduced SPLICE    novel hierarchical extension of
ICA with an intrinsic pooling mechanism  The striking feature of SPLICE is that the model is fully tractable       both
the posterior estimates on latent variables and the associated pdf or likelihood can be computed with simple analytical formulae  The conceptual simplicity of SPLICE  as
well as its approximationfree nature  will be particularly
useful in exploratory data analysis and modeling  for example  in neuroscienti   contexts  As   proofof concept 
we demonstrated the applicability of the method with EEG
and natural image patches 
So far  the development of hierarchical or deep generative
models has been hampered by intractability and the ensuing computational dif culties  Intriguingly  SPLICE relies
only on conventional principles for statistical modeling and
estimation  and it can easily be extended with an arbitrary
number of layers as well as other typical ingredients of
multilayer neural networks  We hope our developments
will open up new avenues for applications of hierarchical
probabilistic models in unsupervised representation learning of continuousvalued data 

Acknowledgments
This work was partially supported by   contract with
the National Institute of Information and Communica 

 Number of Layers Test Avg  LogLikelihoodSPLICE MLSPLICELWSPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

tions Technology entitled  Development of network dynamics modeling methods for human brain data simulation
systems  Strategic International Collaborative Research
Program  SICORP  from Japan Science and Technology
Agency  JST  and KAKENHI     from Japan Society for the Promotion of Science  JSPS       was funded
by the Academy of Finland  Centreof Excellence in Inverse Problems Research 

References
Adali     and Haykin     Adaptive Signal Processing  Next

Generation Solutions  WileyIEEE Press   

Adelson        and Bergen        Spatiotemporal energy
models for the perception of motion  Journal of the Optical Society of America       

Almeida        MISEP linear and nonlinear ICA based
on mutual information  Journal of Machine Learning
Research     

Amari     Cichocki     and Yang          new learning
In Advances in
algorithm for blind signal separation 
Neural Information Processing Systems    NIPS 
pp     

Bengio     Courville     and Vincent     Representation
learning    review and new perspectives  IEEE Transactions on Pattern Analysis   Machine Intelligence   
   

Bingham     and Hyv arinen       fast  xedpoint algorithm for independent component analysis of complex
valued signals  International Journal of Neural Systems 
   

Blankertz     Dornhege     Krauledat       uller      
and Curio     The noninvasive Berlin braincomputer
interface  Fast acquisition of effective performance in
untrained subjects  NeuroImage     

Cadieu        and Olshausen        Learning intermediatelevel representations of form and motion from natural
movies  Neural Computation     

Ding     Li     Peng     and Park     Orthogonal
nonnegative matrix trifactorizations for clustering 
In
Proceedings of the  th ACM SIGKDD international
conference on Knowledge discovery and data mining
 KDD  pp     

Dinh     Krueger     and Bengio     NICE  Nonlinear
independent components estimation  arXiv 
 

Goodfellow     PougetAbadie     Mirza     Xu    
WardeFarley     Ozair     Courville     and Bengio 
   Generative adversarial nets  In Advances in Neural
Information Processing Systems  volume   pp   
   

Gutmann        and Hyv arinen     Noisecontrastive estimation of unnormalized statistical models  with applications to natural image statistics  Journal of Machine
Learning Research     

Gutmann        and Hyv arinen       threelayer model
of natural image statistics  Journal of PhysiologyParis 
 

Hirayama     and Hyv arinen     Structural equations and
divisive normalization for energydependent component
analysis  In Advances in Neural Information Processing
Systems    NIPS  pp     

Hirayama     Ogawa     and Hyv arinen     Unifying blind
separation and clustering for restingstate EEG MEG
functional connectivity analysis  Neural Compution   
   

Hirayama     Hyv arinen     Kiviniemi     Kawanabe    
and Yamashita     Characterizing variability of modular
brain connectivity with constrained principal component
analysis  PLoS ONE       

Hosoya     and Hyv arinen       hierarchical statistical
model of natural images explains tuning properties in
   The Journal of Neuroscience   
 

Chen        and Gopinath        Gaussianization 

In
Advances in Neural Information Processing Systems  
 NIPS  pp     

Hosoya     and Hyv arinen     Learning visual spatial pooling by strong PCA dimension reduction  Neural Computation     

Deco     and Brauer     Nonlinear higherorder statistical decorrelation by volumeconserving neural architectures  Neural Networks     

Hubel        and Wiesel       Receptive  elds  binocular
interaction and functional architecture in the cat   visual
cortex  Journal of Physiology     

Deng     Berg        Li     and FeiFei  Li  What does
classifying more than   image categories tell us 
In Computer Vision   ECCV  pp     

Hyv arinen     Fast and robust  xedpoint algorithms for
independent component analysis  IEEE Transactions on
Neural Networks     

SPLICE  Fully Tractable Hierarchical Extension of ICA with Pooling

Hyv arinen     and Hoyer        Emergence of phase and
shift invariant features by decomposition of natural images into independent feature subspaces  Neural Computation     

Ngiam     Chen     Koh     and Ng        Learning deep
energy models  In Proceedings of the TwentyEighth International Conference on Machine Learning  ICML 
pp     

Hyv arinen     and Morioka     Nonlinear ICA of temporally dependent stationary sources  In Proc  Arti cial
Intelligence and Statistics  AISTATS     

Hyv arinen     and Morioka     Unsupervised feature extraction by timecontrastive learning and nonlinear ICA 
In Advances in Neural Information Processing Systems
 NIPS     

Hyv arinen     and Pajunen     Nonlinear independent component analysis  Existence and uniqueness results  Neural Networks     

Hyv arinen     Hoyer        and Inki     Topographic
independent component analysis  Neural Computation 
     

Hyv arinen     Karhunen     and Oja    

Independent

Component Analysis  John Wiley   Sons     

Hyv arinen     Hurri     and Hoyer        Natural Image
Statistics     probabilistic approach to early computational vision  SpringerVerlag   

Karklin     and Lewicki          hierarchical Bayesian
model for learning nonlinear statistical regularities in
nonstationary natural signals  Neural Computation   
   

Kingma        and Welling     Autoencoding variational
Bayes  In Proceedings of the International Conference
on Learning Representations  ICLR   

Kingma        Mohamed     Rezende        and Welling 
   Semisupervised learning with deep generative models  In Advances in Neural Information Processing Systems  NIPS  volume   pp     

  oster     and Hyv arinen       twolayer model of natural
stimuli estimated with score matching  Neural Computation     

Le        Zou        Yeung        and Ng        Learning hierarchical invariant spatiotemporal features for action recognition with independent subspace analysis  In
Proceedings of the   IEEE Conference on Computer
Vision and Pattern Recognition  CVPR   pp   
    ISBN  

Lyu     and Simoncelli        Nonlinear extraction of
 Independent Components  of natural images using radial Gaussianization  Neural Computation   
   

Ollila     Tyler        Koivunen     and Poor        Complex elliptically symmetric distributions  Survey  new
IEEE Transactions on Signal
results and applications 
Processing     

Onton     and Makeig     Highfrequency broadband modulations of electroencephalographic spectra  Frontiers in
Neuroscience     

Osindero     Welling     and Hinton        Topographic
product models applied to natural scene statistics  Neural Computation     

Rezende        and Mohamed     Variational Inference with
Normalizing Flows  In Proceedings of the  nd International Conference on Machine Learning  ICML 
volume   pp     

Salakhutdinov     and Hinton     Deep Boltzmann machines  In Proceedings of the  th International Conference on Arti cial Intelligence and Statistics  AISTATS 
volume   of JMLR    CP  pp     

Schmidt     minFunc  unconstrained differentiable multivariate optimization in Matlab  http www cs ubc ca 
 schmidtm Software minFunc html   

Shan     Zhang     and Cottrell        Recursive ICA 
In Advances in Neural Information Processing Systems 
volume   pp     

SohlDickstein     and Culpepper        Hamiltonian annealed importance sampling for partition function estimation  arXiv   

Szab         oczos     and   orincz     Separation theorem
for independent subspace analysis and its consequences 
Pattern Recognition     

Taleb     and Jutten     Source separation in postnonlinear
IEEE Transactions on signal processing   

mixtures 
   

Valpola     Harva     and Karhunen     Hierarchical
models of variance sources  Signal Processing   
   

Yu        and Shi     Multiclass spectral clustering 

In
Proceedings of the Ninth IEEE International Conference
on Computer Vision  ICCV  pp     

Zhang     and Hyv arinen     Source separation and higherorder causal analysis of MEG and EEG  In Proceedings
of the  th Conference on Uncertainty in Arti cial Intelligence  UAI   pp     

