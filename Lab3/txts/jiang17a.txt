Density Level Set Estimation on Manifolds with DBSCAN

Heinrich Jiang  

Abstract

We show that DBSCAN can estimate the connected components of the  density level set     
          given          samples from an unknown density    We characterize the regularity of the level set boundaries using parameter
      and analyze the estimation error under the
Hausdorff metric  When the data lies in RD we

obtain   rate of  cid        which matches
rate of  cid       max  Finally  we pro 

known lower bounds up to logarithmic factors 
When the data lies on an embedded unknown ddimensional manifold in RD  then we obtain  

vide adaptive parameter tuning in order to attain
these rates with no   priori knowledge of the intrinsic dimension  density  or  

  Introduction
DBSCAN  Ester et al    is one of the most popular clustering algorithms amongst practitioners and has had
profound success in   wide range of data analysis applications  However  despite this  its statistical properties have
not been fully understood  The goal of this work is to give
  theoretical analysis of the procedure and to the best of
our knowledge  provide the  rst analysis of density levelset estimation on manifolds  We also contribute ideas to
related areas that may be of independent interest 
DBSCAN aims at discovering clusters which turn out to
be the highdensity regions of the dataset  It takes in two
hyperparameters  minPts and  
It de nes   point as  
corepoint if there are at least minPts sample points in its  
radius neighborhood  The points within the  radius neighborhood of   corepoint are said to be directly reachable
from that corepoint  Then    point   is reachable from  
corepoint   if there exists   path from   to   where each
point is directly reachable from the next point  It is now
clear that this de nition of reachable gives   partitioning of

 Google 

Correspondence to  Heinrich Jiang  hein 

rich jiang gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

the dataset  and remaining points not reachable from any
corepoint are considered noise  This partitioning is the
clustering that is returned by DBSCAN 
The problem of analyzing DBSCAN has recently been explored in  Sriperumbudur   Steinwart    Their analysis is for   modi ed version of DBSCAN and is not focused
on estimating    xed density level  Their results have many
desirable properties  but are not immediately applicable for
what this paper tries to address  Using recent developments
in topological data analysis along with some tools we develop in this paper  we show that it is now possible to analyze the original procedure 
The clusters DBSCAN aims at discovering can be viewed
as approximations of the connected components of the level
sets                where   is the density and   is
some density level  We provide the  rst comprehensive
analysis in tuning minPts and   to estimate the density
level set for   particular level  Here  the density level  
is known to the algorithm while the density remains unknown  Density level set estimation has been studied extensively        Carmichael et al    Hartigan   
Polonik    Cuevas   Fraiman    Walther   
Tsybakov et al    Ba llo et al    Cadre    Willett   Nowak    Biau et al    Rigollet   Vert 
  Maier et al    Singh et al    Rinaldo  
Wasserman    Steinwart    Rinaldo et al   
Steinwart et al    Chen et al    Jiang   
However approaches that obtain stateof art consistency results are largely unpractical       unimplementable  Our
work shows that in actuality  DBSCAN    procedure known
for decades and has since been used widely  can achieve
the strongest known results  Also  unlike much of the existing work  we show that DBSCAN can also recover the
connected components of the level sets separately and bijectively 
Our work begins with the insight that DBSCAN behaves
like an  neighborhood graph  which is different from  but
related to the knearest neighbor graph  The latter has
been heavily used for clustertree estimation  Chaudhuri  
Dasgupta    Stuetzle   Nugent    Kpotufe   von
Luxburg    Chaudhuri et al    Jiang   Kpotufe 
  and in this paper we adapt some of these ideas for
 neighborhood graphs 

Density Level Set Estimation on Manifolds with DBSCAN

Clustertree estimation aims at discovering the hierarchical tree structure of the connectedcomponents as the levels
vary  Balakrishnan et al    extends results by Chaudhuri   Dasgupta   to the setting where the data lies
on   lower dimensional manifold and provide consistency
results depending on the lower dimension and independent
of the ambient dimension  Here we are instead interested
in how to set minPts and   in order to estimate   particular level and provide rates on the Hausdorff distance error  This is different from works on cluster tree estimation
which focuses on how to recover the tree structure rather
than recovering   particular level  In that regard  we also
require density estimation bounds in order to get   handle
on the true densitylevels and the empirical ones 
Dasgupta   Kpotufe   gives us optimal highprobability  nitesample kNN density estimation bounds
which hold uniformly 
this is key to obtaining optimal
levelset estimation rates under the Hausdorff error  Much
of the previous works on density levelset estimation      
 Rigollet   Vert    provide rates under risk measures
such as symmetric set difference  These metrics are considerably weaker than the Hausdorff metric  the latter is   uniform guarantee  There are such bounds for the histogram
density estimator  This allowed Singh et al    to obtain optimal rates under Hausdorff metric  while having
  fully adaptive procedure  This was   signi cant breakthrough for level set estimation  as discussed by Chazal
et al    We believe this to be the strongest consistency results obtained thus far  However    downside is that
the histogram density estimator has little practical value 
Here  aided with the desired bounds on the kNN density
estimator  we can actually obtain similar results to Singh
et al    but with the clearly practical DBSCAN 
We extend the kNN density estimation results of Dasgupta
  Kpotufe   to the manifold case  as the bulk our
analysis is about the more general case that the data lies on
  manifold  Densitybased procedures perform poorly in
highdimensions since the number of samples required increases exponentially in the dimension  the so called curse
of dimensionality  Thus  the consequences of handling the
manifold case are of practical signi cance  Since the estimation rates we obtain depend only on the intrinsic dimension  it explains why DBSCAN can do well in high dimensions if the data has low intrinsic dimension       the manifold hypothesis  Given the modern capacity of systems
to collect data of increasing complexity  it has become ever
more important to understand the feasibility of practical
algorithms in high dimensions 
To analyze DBSCAN  we write minPts and   in terms of
the    unknown manfold dimension     which controls the
density estimator  and   which determines which level to
estimate  We assume knowledge of   with the goal of es 

timating the  level set of the density  We give   range of
  in terms of   and corresponding consistency guarantees
and estimation rates for such choices  We then adaptively
tune   and   in order to attain close to optimal performance
with no   priori knowledge of the distribution  Adaptivity
is highly desirable because it allows for automatic tuning of
the hyperparameters  which is   core tenet of unsupervised
learning  To solve for the unknown dimension  we use an
estimator from Farahmand et al    which we show to
have considerably better  nitesample behavior than previously thought  More details and discussion of related works
is in the main text  We then provide   new method of choosing   such that it will asymptotically approach   value that
provides nearoptimal level set estimation rates 

  Overview
We start by analyzing the procedure under the manifold
assumption  The end of the paper will discuss the fulldimensional setting  The bulk of our contribution lies in
analyzing the former situation  while the analysis of the latter uses   subset of those techniques 

  Section   proves that the clusters returned by DBSCAN are close to the connected components of certain  neighborhood graphs  Lemma   This is signi cant because these graphs can be shown to estimate
density level sets 

  Section   introduces the manifold setting and provides
supporting results including knearest neighbor density estimation bounds  Lemma   and Lemma   that
are useful later on 

  Section   provides   range of parameter settings under which for each true cluster  there exists   corresponding cluster returned by DBSCAN  Lemma   and
Lemma   and   rate for the Hausdorff distance between them  Theorem  

  Section   shows how one can apply DBSCAN   second time to remove false clusters from the  rst application  thus completing   bijection between the estimates and the true clusters  Theorem  

  Section   explains how to adaptively tune the parameters so that they fall within the theoretical ranges  The
main contributions of this section are   stronger result
about   known knearest neighbor based approach to
estimating the unknown dimension  Theorem   and  
new way to tune   to approach an optimal choice of  
 Theorem  

  Section   gives the result when the data lives in RD

without the manifold assumption 

Density Level Set Estimation on Manifolds with DBSCAN

  The connection to neighborhood graphs
This section is dedicated towards the understanding of
the clusters produced by DBSCAN  The algorithm can be
found in  Ester et al    and is not shown here since
Lemma   characterizes what DBSCAN returns 
We have          samples           xn  drawn from  
distribution   over RD 
De nition   De ne the kNN radius of     RD as
rk      inf                          

where         denotes the Euclidean ball of radius   centered at    Let        denote the  neighborhood level
graph of   with vertices          rk        and an
edge between   and   cid  iff        cid     
Remark   This is slightly different from  neighborhood
graph  which includes all vertices  Here we exclude vertices below certain empirical density level       rk       

The next de nition is relevant to DBSCAN and is from  Ester et al    but in the notation of De nition  
De nition   The following is with respect to  xed      
and minPts     

    is   corepoint if rminPts       
    is directly densityreachable from   if            

and   is   corepoint 

    is densityreachable from   if there exists   sequence
            pm     such that pi is directly densityreachable from pi  for              

The following result is paraphrased from Lemmas   and  
from  Ester et al    which characterizes the clusters
learned by DBSCAN 
Lemma    Ester et al    Let   be the clusters returned by DBSCAN minPts    For any corepoint    there
exists       with        On the other hand  for any
       there exists corepoint   such that        cid 
 
  cid  is densityreachable from   

relating the  
We now show the following result
neighborhood level graphs and the clusters obtained from
DBSCAN  Such an interpretation of DBSCAN has been
given in previous works such as Campello et al   
Lemma    DBSCAN and  neighborhood level graphs 
Let   be the clusters obtained from DBSCAN minP ts   
Let   be the connected components of
on   
  minP ts    Then  there exists   oneto one correspence between   and   such that if       and      
correspond  then

           KB          

Proof  Take any        Each point in   is   corepoint
and by Lemma   and the de nition of densityreachable 
each point in   belongs to the same        Thus     
         rk        Next we show that             
rk       
Suppose there exists corepoint       but       and let
       By Lemma   there exists corepoint       such
that all points in   are directly reachable from    Then
there exists   path of corepoints from   to   with pairwise
edges of length at most   The same holds for   to    Thus
there exists such   path of corepoints from   to    which
means that      are in the same CC of   minP ts    contradicting the assumption that       and        Thus 
in fact              rk        The result now follows since   consists of points that are at most   from its
corepoints 
We can now see that DBSCAN   clusterings can be viewed
as the connected components  CCs  of an appropriate  
neighborhood level graph  Using   neighborhood graph to
approximate the levelset has been studied in  Rinaldo  
Wasserman    The difference is that they use   kernel
density estimator instead of   kNN density estimator and
study the convergence properties under different settings 

  Manifold Setting
  Setup

We make the following regularity assumptions which are
standard among works on manifold learning       Baraniuk
  Wakin    Genovese et al    Balakrishnan et al 
 
Assumption     is supported on   where 

    is   ddimensional smooth compact Riemannian
manifold without boundary embedded in compact subset     RD 

  The volume of   is bounded above by   constant 
    has condition number   which controls the cur 

vature and prevents selfintersection 

Let   be the density of   with respect to the uniform measure on   
Assumption     is continuous and bounded 

  Basic Supporting Bounds

The following result bounds the empirical mass of Euclidean balls to the true mass under    It is   direct consequence of Lemma   of Balakrishnan et al   
Lemma    Uniform convergence of empirical Euclidean
balls  Lemma   of Balakrishnan et al    Let   be
  minimal  xed set such that each point in   is at most distance    from some point in     There exists   universal

Density Level Set Estimation on Manifolds with DBSCAN

 

constant    such that the following holds with probability
at least       For all            

          
        
 
        
 

 

  log  
 

  Fn       
  Fn       
 
  Fn     
 
 

 
     
 
 
     
 
 
 
  log    Fn is the empirical

 

where          log 
distribution  and         
Remark   For the rest of the paper  many results are quali ed to hold with probability at least   This is precisely
the event in which Lemma   holds 
Remark   If         then         log   

Next  we need the following bound on the volume of the
intersection Euclidean ball and    this is required to get
  handle on the true mass of the ball under   in later arguments  The upper and lower bounds follow from Chazal
  and Lemma   of Niyogi et al    The proof
is given in the appendix 
Lemma    Ball Volume  If         min       
and       then
vdrd            vold                 vdrd     dr   
where vd is the volume of   unit ball in Rd and vold is the
volume        the uniform measure on   

  kNN Density Estimation

Here  we establish density estimation rates for the kNN
density estimator in the manifold setting  This builds on
work in density estimation on manifolds     
 Hendriks 
  Pelletier    Ozakin   Gray    Kim   Park 
  Berry   Sauer    thus  it may be of independent
interest  The estimator is de ned as follows
De nition    kNN Density Estimator 

fk     

 

    vd   rk      

The following extends previous work of Dasgupta  
Kpotufe   to the manifold case  The proofs can be
found in the appendix 
Lemma    fk upper bound  Suppose that Assumptions  
and   hold  De ne the following which charaterizes how
much the density increases locally in   

         sup

   

sup

  cid        

     cid             

 

 cid 

 cid 

Fix       and       and suppose that        
there exists constant                such that if

    Then

                

  

      

then the following holds with probability at least     uniformly in       and       with              
            

 cid 

 cid 

fk     

            
 

 
provided   satis es vd                   
   
 
Lemma    fk lower bound  Suppose that Assumptions  
and   hold  De ne the following which charaterizes how
much the density decreases locally in   

      
 cid 

 cid 

         sup

   

sup

  cid        

             cid     

 

Fix       and           and suppose          Then
there exists constant                such that if

                

  

      

then with probability at least       the following holds
uniformly for all       and       with              

 cid 

 cid 

            
 

            

fk     

 cid 

 

 cid   

provided   satis es vd                         

 

 
 

       

 
 
Remark   We will often bound the density of points with
low density  In lowdensity regions  there is less data and
thus we require more points to get   tight bound  However 
in many cases   tight bound is not necessary  thus the purposes of   is to allow some slack  The higher the   the
easier it is for the lemma conditions to be satisi ed  In particular  if   is    older continuous                     cid   
        cid  we have                    

  Consistency and Rates
  LevelSet Conditions

Much of the results will depend on the behavior of level
set boundaries  Thus  we require suf cient dropoff at the
boundaries  as well as separation between the CCs at   particular level set  We give the following notion of separation 
De nition        cid  are rseparated in   if there exists  
set   such that every path from   to   cid  intersects   and
supx                  inf       cid       

De ne the following shorthands for distance from   point to
  set  the intersection of   with   neighborhood around  
set under the Euclidean distance  and the largest Euclidean
distance from   point in   set to its closest sample point 

Density Level Set Estimation on Manifolds with DBSCAN

De nition             inf   cid          cid            
                 rn      supc          
We have the following mild assumptions which ensures
that the CCs can be separated from the rest of the density
by suf ciently wide valleys and there is suf cient decay
around the level set boundaries 
Assumption    Separation Conditions  Let       and
   be   CCs of                    There exists
          rs  rc     and           such that the following holds 
For each        there exists AC    connected component
of                          such that 

  AC separates   by   valley  AC does not intersect
with any other CC in    AC and    AC are rsseparated by some SC 

    rc   AC 
   regularity  For       rc    we have

                                         

Remark   We can choose any           The  
regularity assumption appears in       Singh et al   
This is very general and also allows us to make   separate
global smoothness assumption 
Remark   We currently characterize the smoothness       
the Euclidean distance  One could alternatively use the
geodesic distance on    dM        It follows from Proposition   of Niyogi et al    that when              
we have           dM                  Since the distances we deal in our analysis with are of such small order 
these distances can thus essentially be treated as equivalent  We use the Euclidean distance throughout the paper
for simplicity 
Remark   For the rest of this paper  it will be understood
that Assumptions     and   hold 

We can de ne   region which isolates   away from other
clusters of                   
De nition   XC            path   from   to   cid   
  such that     SC    

  Parameter Settings

Fix       and       Let   satisfy the following
Kl    log          Ku    log            cid cid   
  min    and Kl and Ku are positive
where  cid 
constants depending on                      rs  rc
which are implicit in the proofs later in this section 

 cid 

 cid  

The remainder of this section will be to show that DBSCAN minPts    with

minPts         

 

    vd              

 
   

  

will consistently estimate each CC of                 

  Throughout the text  we denote cid    as the clusters re 

turned by DBSCAN under this setting 

  Separation and Connectedness
Take        We show that DBSCAN will return an esti 

mated CC  cid    such that  cid   does not contain any points outside of XC  Then  we show that  cid   contains all the sample

points in    The proof ideas used are similar to that of standard results in cluster trees estimation  they can be found
in the appendix 
Lemma    Separation  There exists Kl suf ciently large
and Ku suf ciently small such that the following holds with

probability at least     Let        There exists  cid    cid   
such that  cid     XC 
exists  cid    cid    such that  cid     XC  then   rn           cid   

Lemma    Connectedness  There exists Kl suf ciently
large and Ku suf ciently small such that the following
holds with probability at least       Let        If there

Remark   These results allow   to have any dimension
between   to   since we reason with   rn    which contains samples  instead of simply   

  Hausdorff Error

We give the estimation rate under the Hausdorff metric 
De nition    Hausdorff Distance 

dHaus      cid    max sup
   

       cid  sup
  cid   cid 

    cid    

Theorem   There exists Kl suf ciently large and Ku suf 
ciently small such that the following holds with probability

at least       For each        there exists  cid    cid    such

dHaus   cid                     

       

that

Proof  For Kl and Ku appropriately chosen  we have
Lemma   and Lemma   hold  Thus we have for       

there exists  cid    cid    such that
  rn           cid    
 cid 

 cid     

 cid 

  XC  
fk      
   
 

 

  We show that dHaus   cid         

De ne     
which involves two directions to show from the Hausdroff

  
 

   

           

metric  that maxx cid                and supx       cid     
We start by proving maxx cid                 De ne     

   

    We have

 cid 

 cid         
       

  
 

 cid   

 

vdn 

 cid      

    

 
 

where the  rst inequality holds when Ku is chosen suf 
ciently small  and the last inequality holds because    
  Hence              Therefore  it suf ces to
      
   
 
show

sup

  XC        

fk             
   
 

 

We have that for      XC                      
         cid  Thus  for any      XC          and
letting      cid          we have

                   

       

 

For Ku chosen suf ciently small  the last equation will be
large enough       of order    vdn    so that the conditions of Lemma   hold  Thus  applying this for each
     XC             we obtain

 cid 

 cid 

    
 

sup

  XC        

fk     

     

         

  for Ku chosen

We have the       
appropriately and the  rst direction follows 

is at most       
   
 

We now turn to the other direction  that supx       cid     
     rn    by de nition of rn and we have that   cid     cid   
error rate of dHaus   cid          max  ignoring

    Let        Then there exists sample point   cid   
Finally  rn         for Kl suf ciently large  and thus
   cid             The result follows 
Remark   When taking       cid cid    we obtain the
logarithmic factors  When           this matches the
known lower bound established in Theorem   of Tsybakov
et al    However  we do not obtain this rate when
      In this case  the density estimation error will be of
order at least      due in part to the error from resolving the geodesic balls with Euclidean balls  This does
not arise in the full dimensional setting  which will be described later 

  Removal of False Clusters
The result of Theorem   guarantees us that for each    

   there exists  cid      cid    that estimates it  In this section  we

Density Level Set Estimation on Manifolds with DBSCAN

gives us the other direction  that each estimate in  cid    corre 

show how   second application of DBSCAN  Algorithm  
can remove the false clusters discovered by the  rst application of DBSCAN with no additional parameters  This
sponds to   true CC in    and thus DBSCAN can identify
with   oneto one correspondence each CC of the levelset 

Algorithm   DBSCAN False CC Removal

As in Section   let minPts     and

 cid 

 cid  

 

   

 

  vd   

   

 

  

 cid 

 cid  

 

 

  

     

De ne    

  vd   

Let  cid    be the clusters returned by DBSCAN minPts   
Let  cid    be the clusters returned by DBSCAN minPts   
Let  cid    be the clusters obtained by merging clusters from
 cid    which are subsets of the same cluster in  cid     
Return  cid   

We state our result below  The proof is less involved and is
in the appendix 
Theorem    Removal of False CC Estimates  De ne    
    supx       XC         which is positive  There exists Kl suf ciently large and Ku suf ciently small depending on   in addition to the constants mentioned in Sec 
      For all  cid      cid    there exists        such that
tion   so that the following holds with probability at least
       

dHaus   cid                     

  Adaptive Parameter Tuning
In this section  we show how to obtain the near optimal
rates by estimating   and adaptively choosing   such that
      cid cid    without knowledge of  

  Determining  

Knowing the manifold dimension   is necessary to tune the
parameters as described in Section   There has been
much work done on estimating the intrinsic dimension as
many learning procedures  including this one  require   as
an input  Such work in intrinsic dimension estimation include  Kegl    Levina   Bickel    Hein   Audibert    Pettis et al    and more recently Farahmand
et al    take   knearest neighbor approach  We work
with the estimate of   dimension at   point proposed in the
latter work 

       

log  

log       rk   

 

The main result of Farahmand et al    gives   highprobability bound for   single sample         Here we

Density Level Set Estimation on Manifolds with DBSCAN

give   highprobability bound under more mild smoothness
assumptions which hold uniformly for all samples above
some densitylevel given our new knowledge of kNN density estimation rates  This may be of independent interest 
Theorem   Suppose that   is    older continuous for
some           Choose       and       Then there exists constants       depending on               such
that if   satis es

      log                   

then with probability at least      

                              
 

 

 

 

       

uniformly for all       with fk       
Proof  We have for       such that if fk        then
              by Lemma   for    chosen appropriately large and    chosen appropriately small 
  log  

log  

log       rk   

log     log fk         
We now try to get   handle on fk          and show it
is suf ciently close to   Applying Lemma   and   with
        
      and       appropriately chosen so that the
 
conditions for the two Lemmas hold  remember that here
we have                     we obtain

 
         
 
         

 
           
 
         
              
 
 
where the last inequality holds when    is chosen suf 
ciently large so that     
  is suf ciently small  On the
other hand  we similarly obtain  for    and    appropriately chosen 

          
          

fk   
      

 

fk   
      

 
 
           
         
              
 

 

 
 
         
         

          
          

It is now clear that by the expansion log             
              and for Kl chosen suf cently large so
that     

  is suf ciently small  we have

 

 cid cid cid cid log

 cid  fk   

      

 cid cid cid cid cid            

 

 

The result now follows by combining this with the earlier
established expression for       as desired 
Remark   In Farahmand et al    it is the case that
      under this setting  we match their bound with an
error rate of      with          being the optimal
choice for    ignoring log factors 

  Determining  

After determining    the next parameter we look at is   
In particular  to obtain the optimal rate  we must choose
      cid cid    without knowledge of   We present  
consistent estimator for  
We need the following de nition  The  rst characterizes how much   varies in balls of   certain radius along
the boundaries of the  level set  where     denotes the
boundary of    The second is meant to be an estimate of
the  rst  which can be computed from the data alone  The
 nal is our estimate of  

Dr   inf

    

sup

        

         

 Dr    

min
   

max

         

        cid 
    logr   Dr   

    fk   

The next is   result of how  Dr   estimates Dr 
Lemma   Suppose that   is    older continuous for
some           Let      cid log   cid  and      
log   
Then there exists positive constants    and   depending on
              rc such that when        then the
following holds with probability at least        

 

 Dr    Dr         log   

Proof sketch  Suppose that the value of Dr is attained at
       and the value of  Dr   is attained at         Let
     be the points that maximize           on         and
        respectively  Let        be the sample points that
maximize   fk    on         and         respectively 
Now  we have

Dr    Dr                     fk   
                  fk               fk   
  max         fk    fk           

Now let   cid  be the closest sample point to   in         Then 
  max      cid    fk   cid  fk                            cid 
   fk      fk   cid   
         
          cid     fk      fk   cid 
On the other hand  we have

         fk   

max

 Dr     Dr       fk               
      fk                           fk   
  max         fk    fk           

Let   cid  be the closest sample point to   in         Then 
  max      cid    fk   cid  fk                            cid 
   fk      fk   cid   
         
          cid     fk      fk   cid 

         fk   

max

Density Level Set Estimation on Manifolds with DBSCAN

                and             such that the following holds  If           then with probability at least

      simulatenously for each        there exists  cid    cid   

such that

dHaus   cid            
spondence between    and cid   

 

 

   max   

Moreover  using Algorithm   there is   oneto one corre 

it

suf ces

to bound maxx                 
Thus
fk         cid       cid fk    fk   cid fk    fk   cid 
First
take        and use Lemma   and   for
maxx                  fk    Using Lemma   we can
show that rn          cid   cid   log        Next we bound
 fk      fk   cid    cid      so we have guarantees on its fk
value  Note that rk   cid    rn   rk      rk   cid    rn 
Let rk   rk   cid  This implies that fk   cid rk rk  
rn     fk      fk   cid rk rk   rn    Now since
rk           we have  fk      fk   cid   cid  log      The
same holds for the bounds related to      cid 
Theorem           in probability  Suppose   is  
  older continuous for some   with          cid  Let
     cid log   cid  and      
log    Then for all      

 

  cid           

 cid 

lim
  

Proof  Based on the  regularity assumption  we have for
    rc 

        Dr        

Combining this with Lemma   we have with probability at
least      

  that

 

           log       Dr                log   

Thus with probability at least        

        log         Dr      log   
        log         Dr      log   

log  

log  

  log    
log  
log    
log  

 

 

It is clear that these expressions go to   as       and the
result follows 
Remark   We can then take        cid   cid    with
 cid    min        for some       so that  cid     cid  for  
suf ciently large and thus   lies in the allowed ranges described in Section   asymptotically  The settings of   and
MinPts are implied by this choice of   and our estimate of
  

  Rates with Datadriven Tuning

Putting this all together  along with Theorems   and  
gives us the following consequence about level set recovery with adaptive tuning  It shows that we can obtain rates
arbitrarily close to those obtained as if the smoothness parameter   and intrinsic dimension were known 
Corollary   Suppose that           and   is  
  older continuous for some           and suppose
the datadriven choices of parameters described in Remark   are used for DBSCAN  For any       there exists

  Full Dimensional Setting
Here we instead take   to be the density of   over the uniform measure on RD  Let

 cid 

 cid  

   

minPts         

 

    vD              

 
   

 

  

where   satis es
Kl    log          Ku    log              

and Kl and Ku are positive constants depending
                     rs  rc 
Then Theorem   and   hold  replacing   with   in Algorithm   for this setting of DBSCAN and thus taking
         gives us the optimal estimation rate of
         straightforward modi cation of Corollary   also holds  This is discussed further in the Appendix 

  Conclusion
We proved that DBSCAN can obtain Hausdorff levelset

recovery rates of  cid        when the data is in RD 
and  cid       max  when the data lies on an em 

bedded ddimensional manifold  The former rate is optimal up to log factors and the latter matches known ddimensional lower bounds for           up to log factors  Moreover  we provided   fully datadriven procedure
to tune the parameters to attain these rates 
This shows that the procedure   ability to recover density
level sets matches the strongest known consistency results
attained for this problem  Furthermore  we developed the
necessary tools and give the  rst analysis of density levelset estimation on manifolds  let alone with   practical procedure such as DBSCAN 
Our density estimation errors however cannot converge

faster than  cid        which is due in part to the error

from resolving geodesic balls with Euclidean balls  Thus
it remains an open problem whether the manifold levelset
rates are minimax optimal when      

Density Level Set Estimation on Manifolds with DBSCAN

Acknowledgements
The author is grateful to Samory Kpotufe for insightful discussions and to the anonymous reviewers for their useful
feedback 

References
Ba llo  Amparo  CuestaAlbertos  Juan    and Cuevas  Antonio  Convergence rates in nonparametric estimation of
level sets  Statistics   probability letters   
 

Balakrishnan     Narayanan     Rinaldo     Singh    
and Wasserman     Cluster trees on manifolds  In Advances in Neural Information Processing Systems  pp 
   

Baraniuk  Richard   and Wakin  Michael    Random projections of smooth manifolds  Foundations of computational mathematics     

Berry  Tyrus and Sauer  Timothy  Density estimation on
manifolds with boundary  Computational Statistics  
Data Analysis     

Biau    erard  Cadre  Beno    and Pelletier  Bruno  Exact
rates in density support estimation  Journal of Multivariate Analysis     

Cadre  Beno    Kernel estimation of density level sets 
Journal of multivariate analysis     

Campello  Ricardo JGB  Moulavi  Davoud  Zimek  Arthur 
and Sander    org  Hierarchical density estimates for
data clustering  visualization  and outlier detection 
ACM Transactions on Knowledge Discovery from Data
 TKDD     

Carmichael     George     and Julius     Finding natural

clusters  Systematic Zoology   

Chaudhuri     and Dasgupta     Rates for convergence for
the cluster tree  In Advances in Neural Information Processing Systems   

Chaudhuri     Dasgupta     Kpotufe     and von
Luxburg     Consistent procedures for cluster tree estimation and pruning  IEEE Transactions on Information
Theory   

Chen  YenChi  Genovese  Christopher    and Wasserman 
Larry  Density level sets  Asymptotics  inference  and
visualization  Journal of the American Statistical Association   justaccepted   

Cuevas     and Fraiman       plugin approach to support

estimation  Annals of Statistics  pp     

Dasgupta     and Kpotufe     Optimal rates for knn density
and mode estimation  In Advances in Neural Information
Processing Systems   

Ester     Kriegel     Sander     and Xu       densitybased algorithm for discovering clusters in large spatial
databases with noise  KDD     

Farahmand     Szepesvari     and Audibert     Manifold 

adaptive dimension estimation  ICML   

Genovese 

Christopher 

PeronePaci co  Marco 
Verdinelli  Isabella  and Wasserman  Larry  Minimax manifold estimation  Journal of machine learning
research   May   

Hartigan     Clustering algorithms  Wiley   

Hein  Matthias and Audibert  JeanYves  Intrinsic dimensionality estimation of submanifolds in rd  ICML   

Hendriks  Harrie  Nonparametric estimation of   probability density on   riemannian manifold using fourier expansions  The Annals of Statistics  pp     

Jiang  Heinrich  Uniform convergence rates for kernel denInternational Conference on Machine

sity estimation 
Learning  ICML   

Jiang  Heinrich and Kpotufe  Samory  Modalset estimation with an application to clustering  Proceedings of the
 th International Conference on Arti cial Intelligence
and Statistics  AISTATS   

Kegl  Balazs  Intrinsic dimension estimation using packing

numbers  NIPS   

Kim  Yoon Tae and Park  Hyun Suk  Geometric structures arising from kernel density estimation on riemannian manifolds  Journal of Multivariate Analysis   
   

Kpotufe     and von Luxburg     Pruning nearest neighbor
In International Conference on Machine

cluster trees 
Learning   

Chazal     An upper bound for the volume of geodesic balls

in submanifolds of euclidean spaces   

Levina  Elizaveta and Bickel  Peter    Maximum likelihood

estimation of intrinsic dimension  NIPS   

Chazal  Fr ed eric  Glisse  Marc  Labru ere  Catherine  and
Michel  Bertrand  Convergence rates for persistence diagram estimation in topological data analysis  Journal of
Machine Learning Research     

Maier  Markus  Hein  Matthias  and von Luxburg  Ulrike 
Optimal construction of knearest neighbor graphs for
identifying noisy clusters  Theoretical Computer Science     

Density Level Set Estimation on Manifolds with DBSCAN

Niyogi  Partha  Smale  Stephen  and Weinberger  Shmuel 
Finding the homology of submanifolds with high con 
dence from random samples  Discrete   Computational
Geometry     

Ozakin  Arkadas and Gray  Alexander    Submanifold

density estimation  pp     

Pelletier  Bruno  Kernel density estimation on riemannian
manifolds  Statistics   probability letters   
   

Pettis     Bailey     and Jain     An intrinsic dimensionIEEE

ality estimator from nearneighbor information 
Transactions on PAMI   

Polonik  Wolfgang  Measuring mass concentrations and
estimating density contour clustersan excess mass approach  The Annals of Statistics  pp     

Rigollet     and Vert     Fast rates for plugin estimators of

density level sets  Bernoulli     

Rinaldo  Alessandro and Wasserman  Larry  Generalized
density clustering  The Annals of Statistics  pp   
   

Rinaldo  Alessandro  Singh  Aarti  Nugent  Rebecca  and
Wasserman  Larry  Stability of densitybased clustering  Journal of Machine Learning Research   Apr 
   

Singh  Aarti  Scott  Clayton  Nowak  Robert  et al  Adaptive hausdorff estimation of density level sets  The Annals of Statistics       

Sriperumbudur  Bharath   and Steinwart  Ingo  Consistency and rates for clustering with dbscan  pp   
   

Steinwart     Adaptive density level set clustering  In  th

Annual Conference on Learning Theory   

Steinwart  Ingo et al  Fully adaptive densitybased cluster 

ing  The Annals of Statistics     

Stuetzle  Werner and Nugent  Rebecca    generalized single linkage method for estimating the cluster tree of  
density  Journal of Computational and Graphical Statistics     

Tsybakov  Alexandre   et al  On nonparametric estimation
of density level sets  The Annals of Statistics   
   

Walther  Guenther  Granulometric smoothing  The Annals

of Statistics  pp     

Willett  RM and Nowak  Robert    Minimax optimal levelset estimation  IEEE Transactions on Image Processing 
   

