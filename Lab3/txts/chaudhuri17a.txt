Active Heteroscedastic Regression

Kamalika Chaudhuri   Prateek Jain   Nagarajan Natarajan  

Abstract

An active learner is given   model class     large
sample of unlabeled data drawn from an underlying distribution and access to   labeling oracle
that can provide   label for any of the unlabeled
instances  The goal of the learner is to  nd  
model       that  ts the data to   given accuracy
while making as few label queries to the oracle as
possible  In this work  we consider   theoretical
analysis of the label requirement of active learning for regression under   heteroscedastic noise
model  where the noise depends on the instance 
We provide bounds on the convergence rates of
active and passive learning for heteroscedastic
regression  Our results illustrate that just like
in binary classi cation  some partial knowledge
of the nature of the noise can lead to signi cant
gains in the label requirement of active learning 

  Introduction
An active learner is given   model class     large sample
of unlabeled data drawn from an underlying distribution Px
and access to   labeling oracle   which can provide   label
for any of the unlabeled instances  The goal of the learner is
to  nd   model       that  ts the data to   given accuracy
while making as few label queries to the oracle as possible 
There has been   lot of theoretical literature on active learning  most of which has been in the context of binary classi cation in the PAC model  Balcan et al    Hanneke    Dasgupta et al    Beygelzimer et al   
Awasthi et al    Zhang and Chaudhuri    For
classi cation  the problem is known to be particularly dif 
cult when there is no perfect classi er in the class that best
 ts the labeled data induced by the oracle   responses  Prior
work in the PAC model has shown that the dif culty of the
problem is alleviated when the  noise  is more benign   for

Authors listed in the alphabetical order  University of California  San Diego  Microsoft Research  India  Correspondence to 
Nagarajan Natarajan  tnanata microsoft com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

example  when there is   ground truth classi er that induces
  labeling and the oracle   responses are perturbed versions
of these labels  Hanneke    Awasthi et al    Zhang
and Chaudhuri    Awasthi et al    corrupted by
certain kinds of noise  In particular  signi cant improvements in label complexity have been obtained under what
is known as the Tsybakov noise conditions  which model
the realistic case of noise that decreases as we move further from the decision boundary 
The case of active learning under regression however is
signi cantly less wellunderstood  In particular  we only
have   theoretical understanding of the two extreme cases
  no noise  as in  no model mismatch  and arbitrary model
mismatch  Chaudhuri et al    show that allowing the
learner to actively select instances for labeling under regression with no model mismatch can only improve the
convergence rates by   constant factor  moreover  in many
natural cases  such as when the unlabeled data is drawn
from   uniform Gaussian  there is no improvement  Sabato
and Munos   look at the other extreme case   when
arbitrary model mismatch is allowed   and provide an algorithm that attempts to  learn  the locations of the mismatch through increasingly re ned partitions of the space 
and then learn   model accordingly  However if the model
mismatch is allowed to be arbitrary  then this algorithm
either requires an extremely re ned partition leading to  
very high running time  or   large number of labels  More
recently  Anava and Mannor   study an online learning approach for estimating heteroscedastic variances and
provide general taskdependent regret bounds  but not exact parameter recovery gaurantees 
In this paper we take   step towards closing this gap in understanding by considering active regression under   realistic yet more benign  noise  model   when the variance of
the label noise depends linearly on the example    Specifically  the oracle   response on an unlabeled example  
is distributed as          
   with             here
  is the unknown vector of regression coef cients and
   is an unknown parameter vector  In classical statistics 
this framework is called heteroscedastic regression  and is
known to arise in econometric and medical applications 
While the usual least squares estimator for heteroscedastic regression is still statistically consistent  we  nd that

Active Heteroscedastic Regression

even in the passive learning case  optimal convergence rates
for heteroscedastic regression are not known  We thus begin with   convergence analysis of heteroscedastic regression for passive learning when the distribution Px over the
unlabeled examples is   spherical Gaussian  in   dimensions  We show that even in this very simple case  the
usual least squares estimator is suboptimal  even when the
noise model    is known to the learner  Instead  we propose   weighted least squares estimator  and show that its

rate of convergence is               when the
noise model is known  and           when it needs

to be estimated from the data  here    denotes the number
of labeled examples used to obtain the estimator  The latter matches the convergence rates of the least squares estimator for the usual homoskedastic linear regression  where
    plays the role of the variance  
We next turn to active heteroscedastic regression and propose   twostage active estimator  We show that when the
noise model is known  the convergence rate of active het 

improvement over passive  However  in the more realistic
case where the noise model is unknown  the rates become

eroscedastic regression is               small
             which improves over the passive

estimator by   factor of    Our results extend to the case
when the distribution Px over unlabeled examples is an arbitrary Gaussian with covariance matrix   and the norm
used is the   norm  Our work illustrates that just like binary classi cation  even   partial knowledge of the nature
of the model mismatch signi cantly helps the label complexity of active learning 
Our work is just    rst step towards   study of active maximum likelihood estimation under controlled yet realistic
forms of noise  There are several avenues for future work 
For simplicity  the convergence bounds we present relate to
the case when the distribution Px is   Gaussian  An open
problem is to combine our techniques with the techniques
of  Chaudhuri et al    and establish convergence rates
for general unlabeled distributions  Another interesting line
of future work is to come up with other  realistic noise models that apply to maximum likelihood estimation problems
such as regression and logistic regression  and determine
when active learning can help under these noise models 
Summary of our main results in this work is given in Table   We conclude the paper presenting simulations supporting our theoretical bounds as well as experiments on
realworld data 

  Problem Setup and Preliminaries
Let   denote instances in Rd  Let Px denote    xed unknown distribution over instances    The response   is gen 

PASSIVE

ACTIVE

MODEL

NOISE
KNOWN

       
       

      
      

    
    

NOISE MODEL ESTIMATED

       
   
      

      

    

Table Summary of our results  Rates for convergence of esti 
  under the heteroscedastic noise model  
Here    is the data dimensionality and   is the number of labeled
examples used for estimation 

mators        
erated according to the model                 where
   denotes instancedependent corruption  and   is the
groundtruth parameter  In this work  we consider the following heteroscedastic model 

 
with   standard parametric model for heteroscedastic noise
given by   linear model 

              

               

 
for some unknown        Each response is independently corrupted via   The goal is to recover   using
instances drawn from Px and their responses sampled from
           
Remark   The noise    can be sampled from any subGaussian distribution with          and bounded second
moment   
        for some constant   For simplicity 
we will consider the Gaussian model  

Our approach is based on maximum likelihood estimator
 MLE  for regression 
In the homoscedastic setting      
        for all   in   MLE is known to give minimax
optimal rates  The standard least squares estimator computed on   iid training instances  xi  yi                    is
given by 

xiyi  

 

and is the solution to the minimization problem 

 LS       
 LS   arg min

 

xixT

       
    

  xi    yi 

In the heteroscedastic setting  it is easy to show that the
standard least squares estimator is consistent 
Remark   Standard least squares estimator is consistent for the heteroscedastic noise model   Assuming
xi   Rd                    are drawn iid from the standard
multivariate Gaussian  we have the rate 

          
    

 LS    

   notable exception is the Stein   estimator that may do better

for high dimensional spaces  Stein et al   

Active Heteroscedastic Regression

While the estimator   is consistent  it does not exploit
the knowledge of the noise model  and does not give better rates even when the noise model    is known exactly 
We look at the maximum likelihood estimator for the heteroscedastic noise   which is given by the weighted least
squares estimator  or sometimes referred to as generalized
least squares estimator 

 GLS       

       

wixixT

wixiyi 

 

where wi    
 xi    When the weights are known  it has
been shown that the weighted estimator is the  correct  estimator to study  in particular  it is the minimum variance
unbiased linear estimator  Theorem   Greene  
However  we do not know of strong learning rate guarantees for the weighted least squares model in general  or in
particular for the model   compared to the ordinary least
squares estimator  This raises two important questions for
which we provide answers in the subsequent sections 

  What are the rates of convergence of the maximum
likelihood estimator for the heteroscedastic model
when the noise model  aka     is unknown 

  Can we achieve   better label requirement via active

learning 

The problem is formally stated as follows  Given   set of
from
  instances                    xm  sampled       
the underlying Px    label budget        and access to
label oracle   that generates responses yi according to the
heteroscedastic noise model   we want an estimator  of
the regression model parameter   such that the estimation
error is small                 

Remark   Existing active regression methods  Sabato
and Munos    Chaudhuri et al    do not consider the heteroscedastic noise model  Note that when   
is known exactly  one can reduce heteroscedastic model to
  homoscedastic model  by scaling instances   and their
responses by        However  we still may not be able
to apply the existing active learning results to the transformed problem  as the modi ed data distribution may no
longer satisfy required nice properties  The resulting estimators do not yield advantages over passive learning  even
in simple cases such as when Px is spherical Gaussian 

Notation 
Id denotes the identity matrix of size    We use
bold letters to denote vectors and capital letters to denote
matrices 

  Basic Idea  Noise Model is Known Exactly
To motivate our approach  we begin with the basic heteroscedastic setting  when    is known exactly in   Even

in this arguably simple setting  the rates for passive and active learning are   priori not clear  and the exercise turns
out to be nontrivial  The results and the analysis here help
gain insights into label complexities achievable via passive
and active learning strategies 
In the standard  passive  learning setting  we sample   instances uniformly from the set   and compute the maximum likelihood estimator given in   with weights set to
wi       xi  The procedure is given in Algorithm
  The resulting estimator is unbiased         GLS     
  Let   denote the diagonal weighting matrix with
Wii   wi  The variance of the estimator is given by 
Var GLS                The question of interest is
if and when the weighted estimator  GLS is qualitatively
better than the ordinary least squares estimator  LS  The

following theorem shows that the variance of the latter  and
in turn the estimation error  can be potentially much larger 
and in particular  the difference between their estimation
errors is at least   factor of dimensionality   
Theorem    Passive Regression With Noise Oracle  Let

   ln  

 GLS denote the estimator in    or the output of Algorithm   where xi       Id         with label budget
       ln   and LS denote the ordinary least squares estimator   There exist positive constants    and       such
that  with probability at least      
nc   both the statements
hold 
          
         
    

    

 GLS    
 LS    

Remark   We present the results for instances sampled
from     Id  for clarity  The estimation error bounds
can be naturally extended to the case of Gaussian distribution with arbitrary covariance matrix  
In this case 
the bounds  in Theorem   for example  continue to hold
for the estimation error measured              

   
         Furthermore  with some calculations  we

can obtain analogous bounds for subGaussian distributions  with distributionspeci   constants featuring in the
resulting bounds 
Remark   In Theorem   when             term is the
lowerorder term  and thus  up to constants  the error of
the weighted least squares estimator is at most      
while that of the ordinary least squares estimator is at
least         Thus  if the noise model is known  the
weighted least squares estimator can give   factor of   improvement in convergence rate 
Remark    Technical challenges  The proofs of key results
in this paper involve controlling quantities such as sum of
ratios of Gaussian random variables  ratios of chisquared

 

 

Active Heteroscedastic Regression

random variables  etc  which do not even have expectation 
let alone higher moments  so  standard concentration arguments cannot be made  However  in many of these cases 
we can show that our error bounds hold with suf ciently
high probability 

The following lemma is key to showing Theorem   the
proof sketch illustrates some of the aforementioned technical challenges  Unlike typical results in this domain 
which bound tr    by providing concentration bounds
for    we bound tr    by providing lower bound on
each eigenvalue of   
Lemma   Let     Rn   where the rows xi are sampled
       from     Id  Assume        ln    Let   denote
  diagonal matrix  with Wii    xi     for  xed    
Rd  Let                denote the eigenvalues of
         Then  with probability at least      
                 
                  
where       and        are constants 
Proof  We give   sketch of the proof here  See Appendix
   for details  To show   lower bound on the smallest
eigenvalue  we  rst show that the smallest eigenvector is
very close to    with suf ciently large probability  To do so 
we exploit the fact that the smallest eigenvalue is at most
     which can be readily seen  For the second part  we
consider the variational characterization of     st singular
value given by 

    and
     ln   for                      

nc  

  

             

max

   dim      

min

      

vT       Xv 

We look at the particular subspace that is orthogonal to   
to get the desired upper bound  One key challenge here is
controlling quantity of the form  
where gi and hi
are iid Gaussian variables  We use   blocking technique
based on the magnitude of     xi  and lower bound the
quantity with just the  rst block  as all the quantities involved are positive  This requires proving   bound on the
order statistics of iid Gaussian random variables  Lemma  
in Appendix   

  
 
  
 

  

Theorem   shows that weighting  clean  instances      
           much more than  noisy  instances yields  
highly accurate estimator of   But can we instead prefer querying labels on instances where we know   priori
the response will be relatively noisefree  This motivates  
simple active learning solution   in principle  if we actually know    we could query the labels of instances with
low noise  and hope to get an accurate estimator  The active

learning procedure is given in Algorithm   Besides label
budget    it takes another parameter   as input  which is  
threshold on the noise level 
We state the convergence for Algorithm   below 
Theorem    Active Regression with Noise Oracle  Con 

   ln  

      

sider the output estimator   of Algorithm   with input label budget        ln    unlabeled set   with         and
xi       Id         and           Then  with probability at least      nc 
     

          

for some constants       and       
Remark   We observe that the estimation error via active
learning  Theorem   goes to    as the size of the unlabeled examples   becomes larger  Note that      is the
error for  dimensional problem and is much better than
     we get from uniform sampling 
Remark   If we have        unlabeled samples  then we
observe that active learning  Theorem   achieves   better convergence rate compared to that of passive learning
 Theorem     the lower order term in case of active learning is      
     The
convergence is superior especially when         as we
also observe in simulations 

     versus passive learning which is      

 

 

nc  

The proof of Theorem   relies on two lemmas stated below 
Lemma   Let     Rn   denote the design matrix
whose rows xi are sampled from   such that they satisfy
 xi           for  xed     Rd  Assume        ln   
Let                  denote the eigenvalues of       
Then  with probability at least      
                  
               
for some constants       and      
Lemma   For each xi      where          de ne gi  
 xi     for any  xed     Rd  Then  with probability at
least exp     
     

     for                      

      gi         

  
 

 

  Estimating Noise  Algorithms and

Guarantees

In this section  we will  rst show that we can obtain  
consistent estimator of    as long as we have   reasonably good estimate of   Let   denote the ordinary least

Algorithm   Passive Regression With Noise Oracle

Active Heteroscedastic Regression

Algorithm   Active Regression With Noise Oracle

Input  Labeling oracle    instances      xi          label budget    noise model   
  Choose   subset   of size   from   uniformly at random from    Query their labels using   
  Estimate  using   on    with wi       xi 
Output   
Input  Labeling oracle    noise model    instances      xi          label budget    noise tolerance  
  Choose   subset   of size at most   from   with expected noise up to the given tolerance        for all xi     
 xi         Query their labels using   
  Estimate   as                      where     Rn   and     Rn  and   is   diagonal matrix with Wii  
   xi   
Output   
squares estimator of   obtained by using   on    labeled instances  chosen        from     Id  The largest
eigenvector of the residualweighted empirical covariance
matrix given by 

 

 
  

    

    

 yi    xi xixT

 

 

 

  
 

gives   suf ciently good estimate of    This is established
formally in the following lemma 
Lemma   Let         log     Then  with probability at least      
converges to   

  the largest eigenvector    of    in  
            

        
    

         

for some positive constant    and expectation is wrt the
randomness in the estimator  
We  rst discuss the implications of using the estimated    in
order to obtain the generalized least square estimator given
in   and then present the active learning solution 

  Weighted Least Squares
We now consider   simple  passive  learning algorithm
for the setting where the noise model is estimated  based
on the weighted least squares solution discussed in Section   We  rst get   good estimator of     as in Lemma
  and then obtain the weighted least squares estimator 

                      where   is the diagonal ma 

trix of inverse noise variances obtained using the estimate
   with   small additive offset   The procedure is presented
in Algorithm  
Remark   Algorithm   can be thought of as   special case
of the wellknown iterative weighted least squares       with
just one iteration  that has been studied in the past  Carroll et al   

It is wellknown heuristic to  rst estimate the weights and
then obtain the weighted estimator in practice  the approach
has been widely in use for decades now in multiple communities including Econometrics and Bioinformatics  Harvey    Greene    However  we do not know of
strong convergence rates for the solution  To our knowledge  the most comprehensive analysis was done by  Carroll et al    Their analysis is not directly applicable to
us for reasons twofold      they focus on using   maximum
likelihood estimate of the parameters in the heteroscedastic
noise model  and does not apply to our noise model   and
 ii  their analysis relies the noise being smooth  for obtaining tighter Taylor series approximation  More importantly 
their analysis conceals   lot of signi cant factors in both  
and    and the resulting statements about convergence rates
are not useful  See Appendix   

Theorem   Consider the output estimator   of Algorithm   with label budget        ln   and offset    
   
   Then  with probability at least      nc 
           ln   

 

 

for some constants       and      
Remark   Note that the above result holds for the speci   choice of   When       we get the weighted
least squares estimator analogous to the one used in Algorithm   However  when estimating weights with       the

resulting estimator   has poor convergence rate  In particular  we observe empirically that the error      

scales as      

   

  Active Regression
We now show that active learning can help overcome the
inadequacy of the passive weighted least squares solution 
The proposed active regression algorithm  presented in Al 

     

Active Heteroscedastic Regression

gorithm   consists of two stages  In the  rst stage  we ob 

selecting instances whose labels are nearly noisefree  To
this end  we sample instances that have suf ciently small

tain an estimate    similar to that in Algorithm   Note that
if    is indeed very close to       serves as   good proxy for
noise              where   is an input parameter to the
algorithm  If    is exact  then the algorithm reduces to the

strategy outlined in Algorithm   Our algorithm follows
the strategy of using   singleround of interaction  in light
of the analysis presented in the passive learning setting  to
achieve   good estimate of the underlying   akin to the active MLE estimation algorithm studied by Chaudhuri et al 
 

Lemma   Let    denote an estimator of    satisfying
            For   given   let   denote   set of
         log   instances sampled from   unlabeled instances    such that      xi      for all xi      and let
yi denote their corresponding labels  Consider the ordinary least squares estimator obtained using        

    xi  
     xi  
Then  with probability at least      
nc  
                
     
for some constants       and      
Remark   The bound in the above theorem recovers the
known variance case discussed in Theorem   where the
estimation error       and the choice of       
   

      

      

     

xixT

xiyi  

Compared to the passive learning error bound in Theorem
  we hope to get leverage   as long we can choose   suf 
 ciently small  and yet guarantee that the number of samples    in Step   of Algorithm   is suf ciently large  The
following theorem shows that this is indeed the case  and
that the proposed active learning solution achieves optimal
learning rate 
Theorem    Active Regression with Noise Estimation 

Consider the output estimator   of Algorithm   with input label budget    unlabeled examples              
and        
   Then  we have  with probability at least
     nc 

 

     

 
for some constants       and      
Remark   We observe that active learning  Theorem  
has the same convergence rate for suf ciently large    as
that of the case when    is known exactly  Theorem  
Note that      and      are lowerorder terms in the
compared bounds 

          

 

  

    

Remark   Unlike in the case when noise model was
known  Theorem   here we can not do better even with
in nite unlabeled examples  The source of trouble is the

estimation error in     so beyond   point even active learn 

ing does not provide improvement  Note that we do not
compute weighted least squares estimator in the  nal step
of Algorithm   unlike in Algorithm   for the same reason 

  Simulations
We now present simulations that support the convergence
bounds developed in this work  The setup is as follows  We sample unlabeled instances               xm from
    Id  Labels are generated according to the heteroscedastic model  yi     xi    gi    xi  where
gi are iid standard Gaussian random variables  We   
        and       We look at how the model estimation error  in case of Algorithms   and        
decays as   function of the label budget            for all
the simulations  We also check the estimation error of the
noise model in case of Algorithms   and  
The results for convergence of model estimation when the
noise model is known are presented in Figure        
In passive learning  the bounds in Theorem   suggest that
when                    
    but once        we
get   convergence of      We observe that the result in Figure       closely matches the given bounds  In
case of active learning  the bounds in Theorem   for the
case when        suggest that we get an error rate of
          
     We observe   similar phenomenon in
the Figure       Turning to the noise estimation setting for
passive learning  we see in Figure       that the estimation
error of   as well as    decay asd    as suggested by

Theorem   for active learning  we see in Figure       that
the estimation error of   is noticeably better  in particular 
better than that of    and approaches    as   becomes
larger than   
We also study the performance of the algorithms on two
realworld datasets from UCI    WINE QUALITY with
      and       and   MSD    subset of the million song dataset  with       and       For each
dataset  we create     traintest split  and learn the best
linear regressor using ordinary least squares  which forms
our   We then sample labels using   and   simulated
heteroscedastic noise    We compare active and passive
learning algorithms on the root mean square error  RMSE 
obtained on the test set  In Figure       we see that active learning with noise estimation gives   signi cant reduction in RMSE early on for WINE QUALITY  We also
see that weighted least squares gives slight bene   over or 

given in the theorem statements

 For better resolution  we plot   rather than  

Active Heteroscedastic Regression

dinary least squares  On MSD dataset   again we observe
that our active learning algorithm consistently achieves  
marginal reduction in RMSE as the number of labeled examples increases 

  Conclusions and Future Work
In conclusion  we consider active regression under   heteroscedastic noise model  Previous work has looked at
active regression either with no model mismatch  Chaudhuri et al    or arbitrary model mismatch  Sabato and
Munos    In the  rst case  active learning provided no
improvement even in the simple case where the unlabeled
examples were drawn from Gaussians  In the second case 
under arbitrary model mismatch  the algorithm either required   very high running time or   large number of labels 
We provide bounds on the convergence rates of active and
passive learning for heteroscedastic regression  Our results
illustrate that just like in binary classi cation  some partial
knowledge of the nature of the noise has the potential to
lead to signi cant gains in the label requirement of active
learning 
There are several avenues for future work  For simplicity 
the convergence bounds we present relate to the case when
the distribution Px over unlabeled examples is   Gaussian 
An open problem is to combine our techniques with the
techniques of  Chaudhuri et al    and establish convergence rates for general unlabeled distributions  Another
interesting line of future work is to come up with other 
realistic noise models that apply to maximum likelihood
estimation problems such as regression and logistic regression  and determine when active learning can help under
these noise models 

 here  the response variable is the year of the song  we make

the response mean zero in our experiments

Active Heteroscedastic Regression

Algorithm   Least Squares with Estimated Weights

Input  Labeling oracle    unlabeled samples      xi          label budget    parameter    offset  
  Draw    examples uniformly at random from   and query their labels   using   
  Estimate  by solving        where     Rm   has xi as rows and     Rm  is the vector of labels 
  Draw   subset   of   examples uniformly at random from    Form     Rn   and     Rn 
  Compute    as the largest eigenvector of the residualweighted empirical covariance given in  
  Set  wi  
  Estimate  by solving                        where   is diagonal matrix with Wii    wi 
Output   

 xi      for xi     

 

Algorithm   Active Regression

Input  Labeling oracle    unlabeled samples      xi          label budget    parameters       
  Draw    examples uniformly at random from   and query their labels   using   
  Estimate  by solving        where     Rm   and     Rm 
  Compute    as the largest eigenvector of    given in  
  Choose   subset   of             instances from   with estimated noise variance up to tolerance          for all
xi       xi          Query their labels using   
  Estimate  as                 where     Rm   and     Rm 
Output   

      
  
Algorithm  

 

 

 

 

 

 

    Algorithm    Active 

Ordinary Least Squares
Algorithm    Passive Learning 
Algorithm    Active Learning 

   
  
Algorithm  

 

 

 
 
 
 
 
 

 

 

 

 

 

 
 

 

 

    Algorithm    Passive 

 

 

 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 
 
 
 

 

    
  
   
         
          

 

 

 

 
 

 

 

 
 
 
 

 

 

 

 

 

 

 

    Algorithm    Active 

    WINE QUALITY

 

 

 
 

 

 

 
 
 
 
 

 

 

 

 

 

 

 

    
       
          

 

 

 
 

 

 

    Algorithm    Passive 

 
 
 
 

 

 

 

 

 

 

 

Ordinary Least Squares
Algorithm    Passive Learning 
Algorithm    Active Learning 

 

 

 

 

 

 

    MSD

Figure Plots       convergence of model   estimation error  when the noise model is known  Plots       convergence of model
  estimation error as well as noise parameter     estimation error  when the noise model is estimated  Plots       RMSE on test
data for two realworld datasets 

Active Heteroscedastic Regression

Sivan Sabato and Remi Munos  Active regression by strati cation  In Advances in Neural Information Processing
Systems  pages    

Charles Stein et al  Inadmissibility of the usual estimator
In
for the mean of   multivariate normal distribution 
Proceedings of the Third Berkeley symposium on mathematical statistics and probability  volume   pages  
   

Chicheng Zhang and Kamalika Chaudhuri 

Beyond
disagreementbased agnostic active learning  In Neural
Information Processing Systems  NIPS   

References
Oren Anava and Shie Mannor  Heteroscedastic sequences 
Beyond gaussianity  In Proceedings of The  rd International Conference on Machine Learning  pages  
 

Pranjal Awasthi  MariaFlorina Balcan  and Philip   
Long  The power of localization for ef ciently learning
linear separators with noise  In Symposium on Theory of
Computing  STOC   New York  NY  USA  May    
June     pages    

Pranjal Awasthi  MariaFlorina Balcan  Nika Haghtalab 
and Hongyang Zhang  Learning and  bit compressed
sensing under asymmetric noise  In Proceedings of the
 th Conference on Learning Theory  COLT   New
York  USA  June     pages    

     Balcan     Beygelzimer  and    Langford  Agnostic
active learning     Comput  Syst  Sci     

   Beygelzimer     Dasgupta  and    Langford  Importance

weighted active learning  In ICML   

Raymond   Carroll  CF Jeff Wu  and David Ruppert  The
effect of estimating weights in weighted least squares 
Journal of
the American Statistical Association   
   

Kamalika Chaudhuri  Sham   Kakade  Praneeth Netrapalli  and Sujay Sanghavi  Convergence rates of active learning for maximum likelihood estimation  In Advances in Neural Information Processing Systems  pages
   

   Dasgupta     Hsu  and    Monteleoni    general agnos 

tic active learning algorithm  In NIPS   

Yehoram Gordon  Alexander Litvak  Carsten Sch utt  and
Elisabeth Werner  On the minimum of several random
variables  Proceedings of the American Mathematical
Society     

William   Greene  Econometric analysis  Prentice Hall 

 

   Hanneke    bound on the label complexity of agnostic

active learning  In ICML   

Andrew   Harvey  Estimating regression models with multiplicative heteroscedasticity  Econometrica  Journal of
the Econometric Society  pages    

Prateek Jain and Ambuj Tewari  Alternating minimization for regression problems with vectorvalued outputs 
In Advances in Neural Information Processing Systems 
pages    

