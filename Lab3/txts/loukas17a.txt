How Close Are the Eigenvectors of the Sample and Actual Covariance

Matrices 

Andreas Loukas  

Abstract

How many samples are suf cient to guarantee
that the eigenvectors of the sample covariance
matrix are close to those of the actual covariance matrix  For   wide family of distributions 
including distributions with  nite second moment and subgaussian distributions supported in
  centered Euclidean ball  we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance
and the number of samples  Our  ndings imply
nonasymptotic concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the nonasymptotic analysis of PCA
and its applications  For instance  they provide
conditions for separating components estimated
from    samples and show that even few samples can be suf cient to perform dimensionality
reduction  especially for lowrank covariances 

  Introduction
The covariance matrix   of an ndimensional distribution
is an integral part of data analysis  with numerous occurrences in machine learning and signal processing 
It is
therefore crucial to understand how close is the sample co 

variance       the matrix  cid   estimated from    nite num 

ber of samples    to the actual covariance matrix  Following developments in the tools for the concentration of
measure   Vershynin    showed that   sample size of
         is up to iterated logarithmic factors suf cient
for all distributions with  nite fourth moment supported in
   Similar results
  centered Euclidean ball of radius   
hold for subexponential  Adamczak et al    and  nite
second moment distributions  Rudelson   
We take an alternative standpoint and ask if we can do
   Ecole Polytechnique   ed erale de Lausanne  Switzerland 
Correspondence to  Andreas Loukas  andreas loukas ep ch 

 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

better when only   subset of the spectrum is of interest 
Concretely  our objective is to characterize how many samples are suf cient to guarantee that an eigenvector and or
eigenvalue of the sample and actual covariance matrices
are  respectively  suf ciently close  Our approach is motivated by the observation that methods that utilize the covariance commonly prioritize the estimation of principal
eigenspaces  For instance  in  local  principal component
analysis we are usually interested in the direction of the
 rst few eigenvectors  Berkmann   Caelli    Kambhatla   Leen    where in linear dimensionality reduction one projects the data to the span of the  rst few eigenvectors  Jolliffe    Frostig et al   
In the nonasymptotic regime  an analysis of these methods hinges on
characterizing how close are the principal eigenvectors and
eigenspaces of the sample and actual covariance matrices 
Our  nding is that the  spectral leaking  occurring in the
eigenvector estimation is strongly concentrated along the

eigenvalue axis  In other words  the eigenvector cid ui of the

sample covariance is far less likely to lie in the span of
an eigenvector uj of the actual covariance when the eigenvalue distance          is large  and the concentration of
the distribution in the direction of uj is small  This agrees
with the intuition that principal components are easier to
estimate  exactly because they are more likely to appear in
the samples of the distribution 
We provide   mathematical argument con rming this phenomenon  Under fairly general conditions  we prove that

 cid 

 cid 

 cid 

 cid 

  
 
 
 

     

  
 

        

and      

 

samples are asymptotically almost surely         suf cient

to guarantee that  cid cid ui  uj cid  and  cid        respectively  is

small for all distributions with  nite second moment  Here 
  is   measure of the concentration of the distribution in
  
the direction of uj  We also attain   high probability bound
for subgaussian distributions supported in   centered Euclidean ball  Interestingly  our results lead to sample estimates for linear dimensionality reduction  and suggest that
linear reduction is feasible even from few samples 
To the best of our knowledge  these are the  rst nonasymptotic results concerning the eigenvectors of the sam 

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

         

Figure   Inner products  cid cid ui  uj cid  are localized       

      samples are needed to approximate    and   

         

         

         

the eigenvalue axis  The phenomenon is shown for MNIST  Much fewer than

ple covariance of nonNormal distributions  Previous studies have intensively investigated the limiting distribution
of the eigenvalues of   sample covariance matrix  Silverstein   Bai    Bai    such as the smallest and largest eigenvalues  Bai   Yin    and the
eigenvalue support  Bai   Silverstein    Eigenvectors and eigenprojections have attracted less attention  the
main research thrust entails using tools from the theory of
largedimensional matrices to characterize limiting distributions  Anderson    Girko    Schott    Bai
et al    and it has limited applicability in the nonasymptotic setting where the sample size   is small and
  cannot be arbitrary large 
Differently  we use techniques from perturbation analysis
and nonasymptotic concentration of measure  However 
in contrast to arguments commonly used to reason about
eigenspaces  Davis   Kahan    Yu et al    Huang
et al    Hunter   Strohmer    our bounds can

characterize weighted linear combinations of  cid cid ui  uj cid  over

  and    and do not depend on the minimal eigenvalue
gap separating two eigenspaces but rather on all eigenvalue
differences  The latter renders them useful to many real
datasets  where the eigenvalue gap is not signi cant but the
eigenvalue magnitudes decrease suf ciently fast 
We also note two recent works targeting the nonassymptotic regime of Normal distributions  Shaghaghi
and Vorobyov recently characterized the  rst two moments
of the subspace projection error    result which implies
sample estimates  Shaghaghi   Vorobyov    but is
restricted to speci   projectors    re ned concentration
analysis for spectral projectors of Normal distributions was
also presented in  Koltchinskii   Lounici    Finally 
we remark that there exist alternative estimators for the
spectrum of the covariance with better asymptotic properties  Ahmed    Mestre    Instead  we here focus
on the standard estimates       the eigenvalues and eigenvectors of the sample covariance 

  Problem Statement and Main Results
Let     Cn be   sample of   multivariate distribution and
denote by               xm the   independent samples used

to form the sample covariance  de ned as
 xp      xp      

  cid 

 cid    

  

 

 

 

where    is the sample mean  Denote by ui the eigenvector
for the eigenvectors cid ui and eigenvalues cid   of  cid    such that
of   associated with eigenvalue     and correspondingly
                    We ask 
that the inner product  cid cid ui  uj cid     cid   
Problem   How many samples are suf cient to guarantee
value gap        cid         is smaller than some constant
  uj  and the eigen 

  with probability larger than  

Clearly  when asking that all eigenvectors and eigenvalues
of the sample and actual covariance matrices are close  we
will require at least as many samples as needed to ensure

that  cid cid       cid       However  we might do better when
is that inner products  cid cid ui  uj cid  are strongly concentrated

only   subset of the spectrum is of interest  The reason

along the eigenvalue axis  To illustrate this phenomenon 
let us consider the distribution constructed by the      
pixel values of digit   in the MNIST database  Figure  
compares the eigenvectors uj of the covariance computed

from all   images  to the eigenvectors  cid ui of the sample covariance matrices  cid   computed from   random subi           we depict at    the average of  cid cid ui  uj cid 
tude of  cid cid ui  uj cid  is inversely proportional to their eigenvalue
gap           ii  Eigenvector cid uj mostly lies in the span of

over   sampling draws  We observe that      The magni 

set of           and   samples  For each

eigenvectors uj over which the distribution is concentrated 
We formalize these statements in two steps 

  Perturbation arguments

First  we work in the setting of Hermitian matrices and notice the following inequality 

Theorem   For Hermitian matrices   and  cid          
with eigenvectors uj and cid ui respectively  the inequality

 cid cid ui  uj cid     cid Cuj cid 

          

     ui uji                     ui uji                     ui uji                     ui uji                holds for sgn          cid     sgn                and

    cid     
The above stands out from standard eigenspace perturbation results  such as the sin  Theorem  Davis   Kahan    and its variants  Huang et al    Hunter  
Strohmer    Yu et al    for three main reasons 
First  Theorem   characterizes the angle between any
pair of eigenvectors allowing us to jointly bound any linear
combination of innerproducts  Though this often proves
handy       Section   it is infeasible using sin type arguments  Second  classical bounds are not appropriate for
  probabilistic analysis as they feature ratios of dependent
random variables  corresponding to perturbation terms  In
the analysis of spectral clustering  this complication was

dealt with by assuming that             cid          Hunter

  Strohmer    We weaken this condition at   cost of
  multiplicative factor 
In contrast to previous work  we
also prove that the condition is met        Third  previous
bounds are expressed in terms of the minimal eigenvalue
gap between eigenvectors lying in the interior and exterior of the subspace of interest  This is   limiting factor
in practice as it renders the results only amenable to situations where there is   very large eigenvalue gap separating
the subspaces  The proposed result improves upon this by
considering every eigenvalue difference 

  Concentration of measure

The second part of our analysis focuses on the covariance
and has   statistical  avor  In particular  we give an answer
to Problem   for various families of distributions 
In the context of distributions with  nite second moment 
we prove in Section   that 

Theorem   For any two eigenvectors cid ui and uj of the

sample and actual covariance respectively  and for any real
number      

  cid cid ui  uj cid          

         
subject to the same conditions as Theorem  

 

 kj

 

 

 cid 

 cid 

For eigenvalues  we have the following corollary 

Corollary   For any eigenvalues    and cid   of   and  cid   

respectively  and for any       we have

    captures the tendency
of the distribution to fall in the span of uj  the smaller the
tail in the direction of uj the less likely we are going to

 

 cid  ki

 cid 

 

    

   
 

   

 cid 
 cid     

 

 cid cid        
Term kj      cid cid xx uj cid 
confuse cid ui with uj 

  

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

 

     

For normal distributions  we have that   

and the number of samples needed for  cid cid ui  uj cid  to be small
     jtr   
    when         and       
is       tr   
 
       tr    Thus for normal distributions 
when
principal components ui and uj with min           
 tr    can be distinguished given   constant number
of samples  On the other hand  estimating    with small
relative error requires       tr      samples and can
thus be achieved from very few samples when    is large 
In Section   we also give   sharp bound for the family of
distributions supported within   ball        cid   cid          
Theorem   For subgaussian distributions supported
within   centered Euclidean ball of radius    there exists
an absolute constant        for any real number      

 cid 
  cid cid ui  uj cid         exp

         ij   

    cid   cid 

 

 

 

 cid 

         
         cid   cid 

and subject to the

where  ij     
same conditions as Theorem  
Above   cid   cid 
is the subgaussian norm  for which we usually have  cid   cid 
      Vershynin    As such  the
theorem implies that  whenever     cid          the sample requirement is with high probability         
   
These theorems solidify our experimental  ndings shown
in Figure   and provide   concrete characterization of the
relation between the spectrum of the sample and actual covariance matrix as   function of the number of samples 
the eigenvalue gap  and the distribution properties  As exempli ed in Section   for linear dimensionality reduction 
we believe that our results carry strong implications for the
nonasymptotic analysis of PCAbased methods 

  Perturbation Arguments
Before focusing on the sample covariance matrix  it helps

to study  cid cid ui  uj cid  in the setting of Hermitian matrices  The
of the form  cid cid ui  uj cid  for any   and    The results are used in

presentation of the results is split in three parts  Section  
starts by studying some basic properties of inner products

Section   to provide    rst bound on the angle between
two eigenvectors  and re ned in Section  

  Basic observations

We start by noticing an exact relation between the angle of
  perturbed eigenvector and the actual eigenvectors of   
Lemma   For every   and   in                the relation

 cid          cid   

  uj   cid  

 cid cid   

    cid     

   Cu cid  holds  

 Though the same cannot be stated about the absolute error

    that is smaller for small    

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

Proof  The proof follows from   modi cation of   standard
argument in perturbation theory  We start from the de ni 

tion  cid   cid ui  cid   cid ui and write

          ui    ui              ui    ui 

Expanded  the above expression becomes

  ui    Cui      ui

 

 

Proof  We rewrite Lemma   as

 cid        cid   
 cid        cid   

We now use the CauchySchwartz inequality
   

 cid 

  uj   

 cid    cid 
  uj      cid 
  cid 

 cid 

 cid 

 

 

   Cu cid 

 cid   
    cid     
  cid 
 cid   
   Cu cid     cid   uj cid 
   
 

   Cu cid 

    cid 

 cid 

 cid 

 

 

  cid 

  cid 

  cid 

where in the last step we exploited Lemma   The proof
concludes by taking   square root at both sides of the inequality 

   Cu cid     cid   uj cid 
   
 

Lemma  
Proof  We  rst notice that   
   Cu cid  is   scalar and equal to
its transpose  Moreover     is Hermitian as the difference
of two Hermitian matrices  We therefore have that

 cid 

   

   Cu cid   

  
   Cu cid   

 cid   Cuj

 cid 

    

    

 cid 

   cid   

 cid   Cuj     

     Cuj    cid Cuj cid 
 

 cid 

matching our claim 

  Re nement

As   last step  we move all perturbation terms to the numerator  at the expense of   multiplicative constant factor 

Theorem   For Hermitian matrices   and  cid          
with eigenvectors uj and cid ui respectively  the inequality
holds for sgn          cid     sgn                and

 cid cid ui  uj cid     cid Cuj cid 

    cid     
Proof  Adding and subtracting    from the left side of the
expression in Lemma   and from de nition we have

          

For     cid      the above expression can be rewritten as

               cid   
 cid cid cid cid    cid 
 cid   
 cid cid cid cid    cid 
 
 cid   

 cid   
  uj   

    max

 cid 

 cid 

    cid     

    cid     
        

  cid 

 cid 

 cid   
    cid     
   Cu cid        cid   
     cid   
  uj 
        

        

   Cu cid 

  uj 

 cid cid cid cid 

 cid cid cid cid 

 

     

  uj   

   Cu cid 

 

     ui    iui      ui 

where we used the fact that Cui    iui to eliminate
    ijuj 
where  ij      
  uj  into   and multiply from the left by
  
    resulting to 

two terms  To proceed  we substitute  ui    cid  
  cid 

  cid 

   Cui  

 iju 

   Cu cid 

 cid 

 iju 

  Cu cid      
  cid 

 cid 

  cid 

    

 iju 

    cid     iu 

  ui     

 iju 

    cid   

 cid 

 cid 

Cancelling the unnecessary terms and rearranging  we have

  cid 

 iu 

  ui                ij

  cid 

    

   Cui  

 iju 

   Cu cid 

 

 cid 

 cid 

    

 iu 

   Cui  

  uj     

equation   becomes

  uj  With this in place 

  uj 
   Cu cid      

At this point  we note that                cid        and
furthermore that  ij    cid   
  ui    cid          cid   
  cid 
  uj     
 cid   
    cid    
all terms other than  cid        cid   
  uj     when    cid     or because     cid             
As the expression reveals   cid cid ui  uj cid  depends on the orientation of cid ui with respect to all other   cid  Moreover  the angles
 as in the sin  theorem  but on every difference cid        

The proof completes by noticing that  in the left hand side 
  uj falloff  either due to

between eigenvectors depend not only on the minimal gap
between the subspace of interest and its complement space

   Cui 

This is   crucial ingredient to   tight bound  that will be
retained throughout our analysis 

  

 

  Bounding arbitrary angles

We proceed to decouple the inner products 

Theorem   For any Hermitian matrices   and  cid    
        with eigenvectors uj and cid ui respectively  we have
that  cid        cid cid ui  uj cid     cid   uj cid 

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

Let us examine the righthand side inequality carefully 
Obviously  when the condition                is not
met  the right clause of   is irrelevant  Therefore  for
                 the bound simpli es to

 cid cid cid cid 

   Cu cid 

    cid     
        

 

 

Similar to the proof of Theorem   applying the CauchySchwartz inequality we have that

 cid   
  uj   
 cid    cid 

 cid   

 cid 

 cid cid cid cid    cid 

 cid 

 cid   

 

  cid 

    cid 
        

 cid 

 

 cid   
  uj   

   

   Cu cid 

 

 cid Cuj cid 
          
 

where in the last step we used Lemma   To  nish the
proof we notice that  due to Theorem   whenever     

       cid         one has
 cid   
 cid        
  uj     cid   uj cid 
           and             cid              for cid         
    when         and for  cid              when

Our bound therefore holds for the union of intervals      

 cid Cuj cid 
          

   cid   uj cid 

          

 

        

Theorem   For any two eigenvectors cid ui and uj of the

sample and actual covariance respectively  with     cid     
and for any real number       we have

  cid cid ui  uj cid          
 cid     

for sgn          cid     sgn                and kj  
 cid   cid cid xx uj cid 

 cid 

         

 cid 

 cid 

  kj

 

 

 

Proof  According to   variant of Tchebichef   inequality  Sarwate    for any random variable   and for any
real numbers       and  

 

  

               Var               
Setting      cid cid ui  uj cid  and       we have
  cid cid ui  uj cid         Var cid cid ui  uj cid      cid cid ui  uj cid 
      cid cid Cuj cid 
 cid 

  cid cid cid ui  uj cid cid 

  

 

 

           

  

 

 

where the last inequality follows from Theorem   We
continue by expanding    using the de nition of the eigenvalue decomposition and substituting the expectation 

  cid cid Cuj cid 

 

 cid     

 

 cid 

   

  
  
  

 cid cid cid Cuj    juj cid 
 cid 
 cid 
   cid        cid        uj
 cid 
 cid 
  cid    uj
 cid 
 cid     
     ju 
  cid    uj
   
  cid xpx 
 cid 

  cid 
  cid xpx 

  xqx 
  

  cid 

  

uj

   

 

uj  

  

 

  

    xx xx  uj
  
 
 
  
    xx xx  uj

 cid 

 cid cid  

uj

 

pxpx 
  

 

 cid 

uj

 

In addition 

  

 cid 
  cid    uj
 cid 

 cid 
  cid 
 cid    cid xqx 
  cid xpx 

    

 

 

 

 

  

 

 
  

  cid  
       

 
  
       
 

 
   
 
 

   

   

and therefore

  cid cid Cuj cid 

 cid         

 

   
    xx xx  uj    
  

 

 

 

 

   

 

    xx xx  uj    
  

 

 
 

  cid cid xx uj cid 

 

 cid     

 

 

 

Putting everything together  the claim follows 

  Concentration of Measure
This section builds on the perturbation results of Section  

to characterize how far any inner product  cid cid ui  uj cid  and
eigenvalue cid   are from the ideal estimates 

   

   

   

Before proceeding  we remark on some simpli cations employed in the following           we will assume that the
mean      is zero  In addition  we will assume the perspective of Theorem   for which the inequality sgn    

     cid     sgn          holds  This event is shown

to occur        when the gap and the sample size are suf 
ciently large  but it is convenient to assume that it happens
almost surely  In fact  removing this assumption is possible
 see Section   but it is largely not pursued here as it
leads to less elegant and sharp estimates 

  Distributions with  nite second moment

Our  rst  avor of results is based on   variant of the
Tchebichef inequality and holds for any distribution with
 nite second moment  though only with moderate probability estimates 

  CONCENTRATION OF EIGENVECTOR ANGLES

We start with the concentration of innerproducts  cid cid ui  uj cid 

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

 

 

  cid  

  cid  

  wij   
 

              

The following corollary will be very useful when applying
our results 
Corollary   For any weights wij and real      

 cid 
   cid 
wij cid cid ui  uj cid     
 cid  and wij  cid    when
 cid     
where kj  cid   cid cid xx uj cid 
    cid     and sgn          cid     sgn               
 cid cid 
  cid   wij cid cid ui  uj cid cid 
 cid cid 
     
wij cid cid ui  uj cid cid   
  cid cid Cuj cid 
 cid 
The claim follows by computing   cid cid Cuj cid 

 cid 
 cid   as before 

Proof  We proceed as in the proof of Theorem  

        

 
and squaring both terms within the probability 

   
  

 

  cid  

  cid  

wij

   

  

 

 

 

 

  EIGENVALUE CONCENTRATION

Though perhaps less sharp than what is currently known
      see  Silverstein   Bai    Bai   Silverstein   
for the asymptotic setting  it might be interesting to observe that   slight modi cation of the same argument can
be used to characterize the eigenvalue relative difference 
and as   consequence the main condition of Theorem  

Corollary   For any eigenvalues    and cid   of   and  cid   

respectively  and for any       we have

 cid cid        
where ki      cid cid xx ui cid 

  

 

 cid 
 cid       

   
 

   

 cid  ki

    

 

 cid 

 

Proof  Directly from the BauerFike theorem  Bauer  
Fike    one sees that

       cid cid Cui    iui cid     cid Cui cid 

 

 cid 

The proof is then identical to that of Theorem  

Using this  we  nd that the event      sgn       cid    

 cid          

sgn                occurs with probability at least
        

           
Therefore  one eliminates the condition from Theorem    statement by relaxing the bound to

  cid cid ui  uj cid           cid cid ui  uj cid                    

        

     

 cid 

   
 

 

 

 

         

   
 

              

 

 

 

 cid 

 cid 

 

  THE INFLUENCE OF THE DISTRIBUTION

As seen by the straightforward inequality   cid cid xx uj cid 
 cid   
 cid  kj connects to the kurtosis of the distribution 
  cid cid   cid 

However  it also captures the tendency of the distribution
to fall in the span of uj 
To see this  we will work with the whitened random vectors           where     denotes the Moore Penrose
pseudoinverse of    In particular 

 

  
     

  
          uj

 cid     
 cid 
 cid       
 cid cid   uj cid 
 cid    cid 
 cid 
 cid   cid cid     cid      

 

 

 

      

    

 

 cid 

where        It is therefore easier to untangle the spaces

spanned by cid ui and uj when the variance of the distribution

along the latter space is small  the expression is trivially
minimized when        or when the variance is entirely
contained along that space  the expression is also small
when        for all    cid     In addition  it can be seen
that distributions with fast decaying tails allow for better

principal component identi cation    cid   cid  is   measure

   

of kurtosis over the direction of uj 
For the particular case of   Normal distribution  we provide
  closedform expression 
Corollary   For   Normal distribution  we have   
        tr   
Proof  For   centered and normal distribution with identity covariance  the choice of basis is arbitrary and the vector        is also zero mean with identity covariance 

Moreover  for every  cid   cid    we can write   cid cid     cid   
  cid cid cid    cid   cid      This implies that
  cid 
  cid cid xx uj cid 
    cid   cid      
 cid cid  
           jtr       
     jtr   
            tr   

 cid     

and  accordingly    

   

 

 cid 

 

  Distributions supported in   Euclidean ball

Our last result provides   sharper probability estimate for
the family of subgaussian distributions supported in   centered Euclidean ball of radius    with their  norm

 cid   cid 

  sup

 
where      is the unit sphere and with the  norm of  
random variable   de ned as

      

 

 cid cid      cid cid 

 cid   cid 

  sup
  

           

 

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

 

 

 

 cid 

where  ij     

      cid     and

         ij   

Our setting is therefore similar to the one used to study covariance estimation  Vershynin    Due to space constraints  we refer the reader to the excellent review article  Vershynin    for an introduction to subgaussian
distributions as   tool for nonasymptotic analysis of random matrices 
Theorem   For subgaussian distributions supported
within   centered Euclidean ball of radius    there exists an
absolute constant    independent of the sample size  such
that for any real number      

fore we will construct   bound with   known tail  As we
saw in Sections   and  

 cid 
  cid cid ui  uj cid         exp
    cid   cid 
sgn          cid     sgn               
         
           cid   cid 
 cid cid ui  uj cid 
  cid cid ui  uj cid                  holds  To proceed there 
 cid cid ui  uj cid     cid Cuj cid 
 cid cid cid   cid  
 cid cid xpx 
   cid  
 cid 
 cid  
 cid 
 cid  

from the simple observation that 
the relation

Proof  We start
for every upper bound   of

  xpx 
        
puj    juj

pxp         

puj    juj 

         

         

        

  xp   

  xp     
 

           

 cid cid cid 

 cid cid 

   

  

  

 

 

 

  

 

  xp cid xp cid 
   
         

 

Assuming further that  cid   cid       and since the numerator
is minimized when  cid xp cid 
  approaches     we can write for
every sample        

 

 cid 

    cid   cid 
   

                
   

 

           
     

 
     

   cid 
 cid 
   cid 
 cid   cid  

               
 jr     

       

 

which is   shifted and scaled version of the random variable
         

    Setting      jr     

    we have
              
         

   

 cid 

  cid cid ui  uj cid          

 cid    cid 
 cid    cid 

  

  

   

   

 cid 
 cid 

                  mt        
                           

 

 

 

  

 

 cid   cid 
 cid 

By Lemma   however  the left hand side is   sum of independent subgaussian variables  Since the summands are
not centered  we expand each         zp         
in terms of   centered subgaussian zp with the same  
norm  Furthermore  by Jensen   inequality and Lemma  

           cid     cid     

 

Therefore  if we set  ij                   
  cid cid ui  uj cid          

 cid    cid 

zp     ij   

         cid   cid 

 

 

  

  

 

           cid   cid 
 cid 

 cid  
Moreover  by the rotation invariance principle  the left hand
side of the last inequality is   subgaussian with  norm
 
    cid zp cid 
smaller than    
         cid   cid 
  for some absolute constant    As  
consequence  there exists an absolute constant    such that
 cid cid cid cid cid cid    cid 
 cid cid cid cid cid     
 cid 
for each      
 cid 
  cid cid ui  uj cid         exp
 cid 

Substituting        ij      we have

           ij    

         
  cid   cid 

  exp

 cid 

 cid 

 cid 

 

  

zp

 

 

 

 

  exp

 

 

  

   cid   cid 
          ij   

    cid   cid 

 

which is the desired bound 
Lemma   If   is   subgaussian random vector and    
      then for every    the random variable         
   
is also subgaussian  with  cid   cid 
Proof  Notice that

   cid   cid 

 
 

   

 cid cid cid cid cid cid 

 cid cid cid cid cid cid    cid 

  

 cid cid cid cid cid cid 

 

 cid   cid 

 cid cid      cid cid 

  sup

      

 
 

   

      
   

   

 
 

  ui   

   

 

 cid   cid 

 

 

  sup
      

 cid cid cid cid cid cid    cid 

  

 

where  for the last inequality  we set     ui 

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

  Application to Dimensionality Reduction
To emphasize the utility of our results  in the following we
consider the practical example of linear dimensionality reduction  We show that   direct application of our bounds
leads to upper estimates on the sample requirement 
In terms of mean squared error  the optimal way to reduce
the dimension of   sample   of   distribution is by projecting it over the subspace of the covariance with maximum
variance  Denote by Ik the diagonal matrix with the  rst
  diagonal entries equal to one and the rest zero  When
the actual covariance is known  the expected energy loss
induced by the Pkx   IkU   projection is

     cid Pkx cid 
  cid   cid 
 

 

 

      
tr   

 

loss Pk   

However  when the projector  cid Pk   Ik cid    is constructed

 

from the sample covariance  we have

  cid cid   cid 

 cid 

 cid 

loss cid Pk   

 

 

 cid 

 cid cid   cid 
     cid cid Pkx cid 
 cid  
        tr Ik cid        cid    
 cid  
       cid 

      cid   

  cid   cid 
 

  uj  

tr   

tr   

 

 

 

 cid 

with the expectation taken over the tobe projected vectors
   but not the samples used to estimate the covariance  After slight manipulation  one  nds that

 cid   
  uj        
tr   

loss cid Pk    loss Pk   
ducing the dimension with  cid Pk one looses either by discard 

The loss difference has an intuitive interpretation  when re 

      cid  

ing useful energy  terms        or by displacing kept components within the permissible eigenspace  terms       
Note also that all terms with       are negative and can
be excluded from the sum if we are satis ed we an upper
estimate 
It is an implication of   and Corollary   that  when its
conditions hold  for any distribution and      

 

 

 cid 

 

loss cid Pk    loss Pk   

 cid 

 cid 

   
   

 

tr   

    
 

mt          

Observe that the loss difference becomes particularly small
whenever   is small      the terms in the sum are fewer and
 ii  the magnitude of each term decreases  due to      
bound of the quantity loss cid Pk    loss Pk 
   similar approach could also be utilized to derive   lower

Figure   The  gure depicts for each    the sample size needed

such that the loss difference loss cid Pk loss Pk  becomes smaller

than some tolerance  We can observe that  in MNIST  linear dimensionality reduction works with fewer than       samples
when the size   of the reduced dimension is small 

This phenomenon is also numerically veri ed in Figure  
for the distribution of the images featuring digit   in
MNIST  total   images with       pixels each 
The  gure depicts for different   how many samples are
required such that the loss difference is smaller than   tolerance threshold  here     and   Each point in the
 gure corresponds to an average over   sampling draws 
The trends featured in these numerical results agree with
our theoretical intuition  Moreover they illustrate that for
modest   the sample requirement is far smaller than   
It is also interesting to observe that for covariance matrices that are  approximately  lowrank  we obtain estimates
reminiscent of compressed sensing  Cand es et al   
in the sense that the sample requirement becomes   function of the nonzero eigenvalues  Though intuitive  with the
exception of  Koltchinskii et al    this dependency of
the estimation accuracy on the rank was not transparent in
known results for covariance estimation  Rudelson   
Adamczak et al    Vershynin   

  Conclusions
The main contribution of this paper was the derivation
of nonasymptotic bounds for the concentration of inner 

products  cid cid ui  uj cid  involving eigenvectors of the sample

and actual covariance matrices  We also showed how these
results can be extended to reason about eigenvalues and we
applied them to the nonasymptotic analysis of linear dimensionality reduction 
We have identi ed two interesting directions for further research  The  rst has to do with obtaining tighter estimates 
Especially with regards to our perturbation arguments  we
believe that our current bounds on inner products could be
sharpened by at least   constant multiplicative factor  The
second direction involves using our results for the analysis of methods that utilize the eigenvectors of the covariance  such that principal component projection and regression  Jolliffe    Frostig et al   

   samplerequirement ofn tolerance tolerance tolerance How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

References
Adamczak  Rados aw  Litvak  Alexander  Pajor  Alain  and
TomczakJaegermann  Nicole  Quantitative estimates of
the convergence of the empirical covariance matrix in
logconcave ensembles  Journal of the American Mathematical Society     

Ahmed  SE  Largesample estimation strategies for eigenvalues of   wishart matrix  Metrika     

Anderson  Theodore Wilbur  Asymptotic theory for principal component analysis  The Annals of Mathematical
Statistics     

Bai  ZD  Methodologies in spectral analysis of large dimensional random matrices    review  Statistica Sinica 
pp     

Bai  ZD and Yin  YQ  Limit of the smallest eigenvalue of  
large dimensional sample covariance matrix  The annals
of Probability  pp     

Bai  ZD  Miao  BQ  Pan  GM  et al  On asymptotics of
eigenvectors of large sample covariance matrix  The Annals of Probability     

Bai  ZhiDong and Silverstein  Jack    No eigenvalues
outside the support of the limiting spectral distribution of
largedimensional sample covariance matrices  Annals
of probability  pp     

Bauer  Friedrich   and Fike  Charles    Norms and exclusion theorems  Numerische Mathematik   
 

Berkmann  Jens and Caelli  Terry  Computation of surface
geometry and segmentation using covariance techniques 
IEEE Transactions on Pattern Analysis and Machine Intelligence     

Cand es  Emmanuel    Li  Xiaodong  Ma  Yi  and Wright 
John  Robust principal component analysis  Journal of
the ACM    June  

Huang  Ling  Yan  Donghui  Taft  Nina  and Jordan 
Michael    Spectral clustering with perturbed data  In
Advances in Neural Information Processing Systems  pp 
   

Hunter  Blake and Strohmer  Thomas 

Performance
analysis of spectral clustering on compressed  incomarXiv preprint
plete and inaccurate measurements 
arXiv   

Jolliffe  Ian  Principal component analysis  Wiley Online

Library   

Jolliffe  Ian      note on the use of principal components

in regression  Applied Statistics  pp     

Kambhatla  Nandakishore and Leen  Todd    Dimension
reduction by local principal component analysis  Neural
computation     

Koltchinskii  Vladimir and Lounici  Karim  Normal approximation and concentration of spectral projectors of
sample covariance  arXiv preprint arXiv 
 

Koltchinskii  Vladimir  Lounici  Karim  et al  Asymptotics
and concentration bounds for bilinear forms of spectral
projectors of sample covariance  In Annales de   Institut
Henri Poincar    Probabilit es et Statistiques  volume  
pp    Institut Henri Poincar     

Mestre  Xavier 

Improved estimation of eigenvalues and
eigenvectors of covariance matrices using their sample
IEEE Transactions on Information Theory 
estimates 
   

Rudelson  Mark  Random vectors in the isotropic position 

Journal of Functional Analysis     

Sarwate 

symmetric

chebyshev
around

Dilip 
for

Twosided
not

inthe
event
equality
mean 
Mathematics Stack Exchange   
URL http math stackexchange com     version 
 

Davis  Chandler and Kahan  William Morton  The rotation
of eigenvectors by   perturbation  III  SIAM Journal on
Numerical Analysis     

Schott  James    Asymptotics of eigenprojections of correlation matrices with some applications in principal components analysis  Biometrika  pp     

Frostig  Roy  Musco  Cameron  Musco  Christopher  and
Sidford  Aaron  Principal component projection withIn Proceedings of
out principal component analysis 
The  rd International Conference on Machine Learning  pp     

Girko     Strong law for the eigenvalues and eigenvectors

of empirical covariance matrices   

Shaghaghi  Mahdi and Vorobyov  Sergiy    Subspace
In

leakage analysis of sample data covariance matrix 
ICASSP  pp    IEEE   

Silverstein  Jack   and Bai  ZD  On the empirical distribution of eigenvalues of   class of large dimensional
random matrices  Journal of Multivariate analysis   
   

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices 

Vershynin  Roman 

Introduction to the nonasymptotic

analysis of random matrices  arXiv   

Vershynin  Roman  How close is the sample covariance
matrix to the actual covariance matrix  Journal of Theoretical Probability     

Yu  Yi  Wang  Tengyao  Samworth  Richard    et al    useful variant of the davis kahan theorem for statisticians 
Biometrika     

