Projectionfree Distributed Online Learning in Networks

Wenpeng Zhang   Peilin Zhao   Wenwu Zhu   Steven       Hoi   Tong Zhang  

Abstract

The conditional gradient algorithm has regained
  surge of research interest in recent years due
to its high ef ciency in handling largescale machine learning problems  However  none of existing studies has explored it in the distributed
online learning setting  where locally light computation is assumed  In this paper  we  ll this gap
by proposing the distributed online conditional
gradient algorithm  which eschews the expensive
projection operation needed in its counterpart algorithms by exploiting much simpler linear optimization steps  We give   regret bound for
the proposed algorithm as   function of the network size and topology  which will be smaller on
smaller graphs or  wellconnected  graphs  Experiments on two largescale realworld datasets
for   multiclass classi cation task con rm the
computational bene   of the proposed algorithm
and also verify the theoretical regret bound 

  Introduction
The conditional gradient algorithm  Frank   Wolfe 
   also known as FrankWolfe  is historically the earliest algorithm for solving general constrained convex optimization problems  Due to its projectionfree property
and ability to handle structural constraints  it has regained
  signi cant amount of interest in recent years  Different
properties concerning the algorithm  such as the sparsity
property  Clarkson    and the primaldual convergence
rate  Jaggi    have been analyzed in details  Many
different algorithm variants  such as the composite variant  Harchaoui et al    the online and stochastic vari 

 Department of Computer Science and Technology  Tsinghua
University  Beijing  China  Arti cial Intelligence Department 
Ant Financial Services Group  Hangzhou  China  School of Information Systems  Singapore Management University  Singapore
 Tencent AI Lab  Shenzhen  China  Correspondence to  Wenwu
Zhu  wwzhu tsinghua edu cn  Wenpeng Zhang  zhangwenpeng gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ants  Hazan   Kale     Hazan     Hazan   Luo 
  faster variants over special types of convex domains 
     spectrahedron  Garber    and polytope  Garber  
Hazan    have also been proposed 
However  despite this great  ourish of research on conditional gradient  its distributed online variant over networks
has rarely been investigated  In comparison to this situation
is the popularity of the variants of its gradient descent and
dual averaging counterparts distributed online gradient
descent  DOGD   Ram et al     Yan et al    and
distributed online dual averaging  DODA   Duchi et al 
   Hosseini et al     Lee et al    which have
been successfully applied in handling largescale streaming data in decentralized computational architectures      
sensor networks and smart phones  Despite the success of
these algorithms  the projection operation required in them
still limits their further applicability in many settings of
practical interests  For example  in matrix learning  Dud  
et al    multiclass classi cation  Hazan   Luo   
and many other related problems  where the convex domain
is the set of all matrices with bounded nuclear norm  the
projection operation amounts to computing the full singular value decomposition  SVD  of   matrix  too expensive
an operation that does not meet the need of locally light
computation in distributed online learning  To avoid this
kind of expensive operation  the distributed online variant
of conditional gradient is desired since it is expected to be
able to eschew the projection operation by using   linear
minimization step instead  In the aforementioned case  the
linear step amounts to  nding the top singular vector of  
matrix  which is at least one order of magnitude simpler 
However  how to design and analyze this variant remains
an open problem 
To  ll this gap  in this work  we present the distributed online conditional gradient  DOCG  algorithm as the desired
variant  This algorithm is   novel extension of the previous
online variant  Hazan    in which each local learner
communicates its dual variables with its neighbors to cooperate with each other  We present highly nontrivial analysis of the regret bound for the proposed algorithm  which
can capture the intuition that the algorithm   regret bound
should be larger on graphs with more nodes and should be
smaller on  wellconnected  graphs       complete graphs 
than on  poorly connected  graphs       cycle graphs  We

Projectionfree Distributed Online Learning in Networks

evaluate the performance of the DOCG algorithm on two
largescale realworld datasets for   multiclass classi cation task  The experimental results show that DOCG runs
signi cantly faster than both DOGD and DODA  This illustrates that although the regret bound for DOCG is in
higher order       than those in order       for DOGD and DODA  its lower computational cost per iteration outweighs the increased number of iterations  making
it overall   faster algorithm  The theoretical results regarding the algorithm   regret bound for different graphs are
also well con rmed 

  Preliminaries
In this section  we  rst give   formal de nition of the distributed online convex optimization problem  and then review the two algorithms that our algorithm are built upon 

  Distributed Online Convex Optimization

Let            denote an undirected graph with vertex
set            and edge set             In distributed online convex optimization de ned over    see the
book  Sayed et al    and the survey  Hazan    for
more details  each node       is associated with   separate agent or learner  At each round             each
learner       is required to generate   decision point xi   
from   convex compact set     Rm  Then the adversary
replies each learner   decision with   convex loss function
ft           and each learner suffers the loss ft   xi   
The communication between learners is speci ed by the
graph    learner   can only communicate directly with its
immediate neighbors                            The
goal of the learners is to generate   sequence of decision
points xi          so that the regret with respect to each
learner   regarding any  xed decision       in hindsight 

  cid 

  cid 

RT  xi     

 ft   xi      ft     

  

  

is sublinear in    
We make the following assumptions 
  each function
ft      is LLipschitz with respect to the    norm  cid cid  on
                     ft        ft          cid       cid    Note
that the Lipschitz condition implies that for any       and
any  ft         ft      we have  cid ft     cid         the
Euclidean diameter of   is bounded by                    
 cid       cid      
Two de nitions are important for deriving useful properties  We say that   function   is  smooth if            
we have

                 cid               cid   

 cid       cid 

 
 

We say that   function   is  strongly convex if         
   we have

                 cid               cid   

 cid       cid 

 
 

   strongly convex function   has   very important property  if      arg minx Kf     then for any        we
have

                 
 

 cid       cid 

  Distributed Online Dual Averaging

In the distributed online dual averaging algorithm  Duchi
et al     Hosseini et al    each learner      
maintains   sequence of iterates xi    and   sequence
of dual variables zi    At each round            
each learner    rst computes the new dual variable zi    
  from   weighted combination of its new subgradient
gi    and other dual variables  zj              received
then updates the iterate xi      
from its neighbors 

via   proximal projection  cid   zi             where
     cid  denotes
 cid          arg minx   cid cid       cid     

      is   positive number from   nonincreasing sequence              is   proximal function and
the projection onto   operator speci ed by      and
     see  Nesterov     Xiao    for more details 
The communication between learners is modeled by   doubly stochastic symmetric matrix     which satis es  
 cid     or         
Pij     only if              
        Pij     and          

   Pij    cid 

           cid  
 cid  
   Pij  cid 

        Pij    

parameters              

Algorithm   Distributed Online Dual Averaging  DODA 
  Input  convex set    maximum round number    
 
  Initialize  xi       zi             
  for           do
 
 
 
 
 
 
  end for

The adversary reveals ft           
Compute subgradients gi       ft   xi           
for Each Learner       do

zi        cid 
xi        cid   zi            

        pijzj   gi   

end for

  Online Conditional Gradient

The standard online conditional gradient algorithm  Hazan
  Kale     Hazan    eschews the computational
expensive projection operation by using   simple linear optimization step instead and is thus much more ef cient for
many computationally intensive tasks 

Projectionfree Distributed Online Learning in Networks

parameters   and    

Algorithm   Online Conditional Gradient  OCG 
  Input  convex set    maximum round number    
 
  Initialize        
  for           do
 
 
 
 
 
  end for

The adversary reveals ft
Compute   subgradient gt    ft xt 
    cid gi     cid     cid       cid 
vt   arg minx    cid Ft xt      cid 
xt    xt      vt   xt 

Ft       cid   

  Distributed Online Conditional Gradient
In this section  we  rst present the proposed distributed online conditional gradient algorithm  and then give the theoretical analysis of its regret bound 

  Algorithm

Algorithm   Distributed Online Conditional Gradient  DOCG 
  Input  convex set    maximum round number    
parameters     and             
 
  Initialize  xi       zi             
  for           do
 
 
 
 
 
 
 
 
  end for

The adversary reveals ft           
Compute subgradients gi       ft   xi           
for Each Learner       do

Ft            cid zi        cid     cid       cid 
vi      arg minx    cid Ft   xi        cid 
xi         xi           vi      xi   

zi        cid 

        pijzj   gi   

end for

 

     
 

  Analysis
Theorem   The DOCG algorithm with parameters     
for any      
 
and any           attains the following regret bound
RT  xi       nLDT  
 
 

     LT   and         

            

  

 

LDT  

 

 

 
 
 
 

 
 
         
         

        
        

        

where      denotes the second largest eigenvalue of matrix   and          denotes the corresponding spectral
gap value 

Remark 
  The regret bound for DOCG is in the
similar order       to that of its centralized variant

OCG  Hazan      Since the connectivity of   graph
is captured by its spectral gap value       Duchi et al 
   Colin et al    the better the connectivity of  
graph is  the larger the spectral gap value will be  it is easy
to verify that this theorem captures the intuition that the
DOCG   regret bound will be larger on larger graphs  the
regret bound will be larger when the node size   is larger
for all     and will be smaller on  wellconnected  graphs
than on  poorly connected  graphs  the regret bound will
be smaller when the spectral gap value is larger for certain
large    
To analyze the regret bound for DOCG  we  rst establish
its connection to DODA  To this end  we consider the following points

  
        arg min

    Ft     

where Ft      are the functions de ned in line   in DOCG 
Actually  these points are exactly the iterates of DODA
with regularization        cid       cid  applied to the
following loss functions

 ft        ft        xi        

     

Note that these loss functions are not the same as the original ft      in DOCG  The reason is that the subgradients
used in the aforementioned DODA are  ft   xi    rather
than  ft     
      Indeed  in this algorithm  the subgradients are evaluated at the points   
      thus the corresponding loss functions  ft      should satisfy

   ft     

         ft   xi   

This clearly holds by de nition of  ft     
Based on the above preparation  we can now present the
following lemma 
Lemma   For any  xed         the following bound holds
for any       and any      

 ft     

  cid 

        ft     

 cid cid xj        

     cid cid   

  cid 

   ft     

         ft     

  cid 

  

    

Proof  By de nition of  ft      and the Lipschitzness of
ft      for any        we have

 cid cid cid   ft        ft     

 cid cid cid      cid cid xj        
     cid cid   

Then by plugging in two auxiliary terms in each difference

LDT  

  

  

ft     

        ft      we obtain

 

  

  

 ft     

 ft     

   ft     

        ft     

         ft     
         ft     
         ft     

  cid 
  cid 
 cid cid cid ft     
    cid 
 cid cid cid 
 cid cid cid   ft        ft     
  cid 
  cid 
  cid 
     cid cid   
 cid cid xj        

   ft     

    

  

  

  

 

 

         ft     
  cid 

 cid cid cid 

     

   ft     

         ft     

  

  

      Let ht        Ft        Ft     

Notice that the last summation in the above bound is exactly the part of the regret of DODA incurred by learner
  with respect to learner    Thus  to proceed  we require
lemmas that allow us to relate the iterates xi    to the iterates   
      and
ht     ht   xi   
In the following  we  rst present  
lemma that establishes the recursion between ht   and
ht   
Lemma   The following recursion between ht   and
ht   holds for any       and any          

ht              ht      

      cid zi         zi   cid cid ht   

  iD 

where zi    denotes the dual variable de ned in DOCG 

Proof  Using the de nitions of ht      and xi       the
fact that Ft      are  smooth and the boundedness of   
we obtain
ht   xi         Ft   xi           vi      xi   

  Ft     

     

  Ft   xi      Ft     

     

        cid Ft   xi      vi      xi   cid 
   

     cid vi      xi   cid 
  Ft   xi      Ft     
     

        cid Ft   xi      vi      xi   cid 
   

  iD 

Projectionfree Distributed Online Learning in Networks

By the optimality of vi    we have

 cid Ft   xi      vi   cid     cid Ft   xi        

     cid   

         ft        ft     

By the convexity of Ft      we have
 cid Ft   xi        

        xi   cid    Ft     

     Ft   xi   

Putting the above three inequalities together  we obtain
ht   xi         Ft   xi      Ft     

     

       Ft     
   

  iD 

        Ft   xi   

           Ft   xi      Ft     

     

   

  iD 

           ht      

  iD 

Next  by de nition of ht   and the optimality of   
we have
ht     Ft   xi         Ft     

        

     

  Ft   xi         Ft     

        

   Ft   xi         Ft   xi      
   Ft     
        

           Ft     
     

  Ft   xi         Ft     

   Ft   xi         Ft   xi      
   Ft     
        

           Ft     

Then  by de nition of Ft      and Ft      we have
Ft        Ft            cid  zi         zi      cid   

Thus 
ht     ht   xi      

      cid  zi         zi    xi      cid 
      cid  zi         zi      
        cid 
  ht   xi      
      cid  zi         zi    xi           
  ht   xi      
      cid zi         zi   cid cid xi           

        cid 

        cid   

The last inequality follows from the CauchySchwarz inequality 
        cid 
Now  we derive the bound for  cid xi           
By de nition  Ft      are  strongly convex and   
       
arg minx   Ft      Thus  using the property of strongly
convex functions  for any        we have
     cid    Ft        Ft     

 cid       

     

Projectionfree Distributed Online Learning in Networks

Analogously  it is easy to deduce that

        cid   cid ht   

 cid xi           

Combining this bound and the above two bounds for ht  
and ht   xi       yields the stated recursion 

To make the above recursion more concrete  it remains to
bound the deviation term  cid zi         zi   cid  which measures the stability of local dual variables over each node 
Lemma   For any       and any             the dual
variables zi    and zi       speci ed in DOCG satisfy
the following bound

 cid zi         zi   cid            
        

 

nL     

Proof  Let     denote the rth power of matrix   and    
ij
denote the jth entry of the ith row of      Then  via   bit
of algebra  we can get the following generalized recursion

zi        

     

ij

zj     

     
ij gj   

  cid 

  cid 

  cid 

  
  gi   

   

  

Clearly  this recursion reduces to the standard dual variable
update in DOCG when        Next  since zj      by
setting       we can obtain

zi        

     
ij gj      gi   

  cid 

  cid 

  

  

  cid 

  cid 

  

  

Then by assuming     to be the identity matrix In  we have

zi   zi     

      

ij         

ij

 gj   gi   

Using the fact that  cid gi   cid       the properties of norm
functions and the symmetry of matrix     we obtain

      

ij         

ij

 cid zi         zi   cid 

 

  

 cid cid cid cid cid cid   cid 
  cid 
  cid 
    cid 
 cid cid      
  cid 
 cid cid      

   

  

  

  

ij         

ij

          

 

 gj      gi   

 cid cid cid cid cid cid 
 cid cid cid cid gj   cid cid     cid gi   cid 
 cid cid      

  

where    

  denotes the ith column of matrix     

Now we try to bound the    norm sum in the above inequality  By plugging in an allones column vector and
then using the properties of norm functions  we obtain

     cid cid 
     cid cid 

 

 

 

  

  

 cid cid 

          

                 

  cid 
 cid cid      
  cid 
 cid cid      
    cid 
 cid cid      
       cid cid   cid cid       
Rn     cid   cid  
 cid   sx      cid          

  

  

 

To proceed  we introduce   useful property of stochastic matrices  Duchi et al    Let          
   xi     denote the ndimensional probability simplex  Then for any positive integer      
and any         the following inequality holds

Taking   to be the ith canonical basis vector ei in Rn  we
have

Note that this inequality also holds for       since

  

 cid   sei      cid          
 cid cid    ei      cid cid   

      

 

   

  

for any         In addition  it is easy to verify that

     cid cid 

Thus  we have

 cid    

       cid     cid   sei      cid 
  cid 
 cid cid      
       cid cid   cid cid       
    cid 

 
                  
 

  

 

 

 

  
        
        
          
        

          
 

  

 

The above equation and the last inequality follow respectively from the summation formula of geometric series and
the fact that          when   is   doubly stochastic matrix  Berman   Plemmons   
Combining the above together yields the stated bound 

Combining the results in Lemma   and Lemma   we can
obtain   more concrete recursion between ht   and ht   
and then deduce the bound for ht   

Lemma   Assume that the parameters    and      in DOCG are chosen such that         
    
  iD  Then the following bound for ht   holds for any
 
      and any          

 

    cid ht    

ht            

     

This lemma can be easily proved using mathematical induction and we place its detailed proof in the Appendix 
Now we can deduce the bound for the deviation between
xi    and   
Lemma   For any  xed         the iterates xi    and
  
      satisfy the following bound
     cid     
 

 cid xi        

  cid 

DT  

  

Proof  As is given in the proof of Lemma   for any       
we have

 cid       

     cid    Ft        Ft     

     

It then follows that
 cid xi        

Ft   xi      Ft     

     

     cid   cid 
 cid ht  

 

    
    
   Dt 

The last inequality follows from the bound in Lemma   and
the last equation follows from the de nition of       Thus 
summing over             we obtain

  cid 

  cid 

 cid xi        

     cid      

  

  

       

       

  

 cid   

  dt 

 cid cid cid cid  

 

 

 
 
 

  

     

 
 

       
 

 

 

 
 

DT  

Note that the dual variables used in the above de nition are
exactly those speci ed in DOCG  Then  as for the deviation between the local dual variable zi    and the global
dual variable       we have the following lemma 
Lemma   For any       and any             the dual
variable zi    de ned in DOCG and their averages      
over all nodes satisfy the following bound

 cid zi           cid   

 

nL

        

 

Two similar bounds for  cid zi           cid  in DODA are reported in  Duchi et al     Hosseini et al    Our
bound is tighter than both of them  The proof is   little bit
similar to that in Lemma   and is presented in detail in the
Appendix 
We can now give the regret bound for the DODA algorithm
in the following lemma 
Lemma   The DODA algorithm with regularization
       cid       cid  and parameters                
    applied to the loss functions  ft      attains the following regret bound

 
            
   xi       
Ra
        

      

 
 

  

Using our tighter bound for  cid zi           cid  this lemma can
be easily deduced from the general regret bound for the DODA algorithm  Hosseini et al    The detailed proof
is presented in the Appendix 
Now  we are ready to prove our theorem 

Proof of Theorem   By plugging in two auxiliary terms
in each difference ft   xi      ft      we have

 ft   xi      ft     

  

  cid 
  cid 
    cid 
  cid 

  

  

 

 

  

Projectionfree Distributed Online Learning in Networks

 ft   xi      ft     

        ft     

        ft     

 ft   xi      ft     

     

 ft     

        ft     

Before proceeding with the  nal proof of Theorem   we
present the regret bound of the DODA algorithm applied to
the loss functions  ft      To this end  we  rst introduce an
auxiliary sequence which are composed of the centralized
averages of dual variables over all nodes at each iteration

  cid 

Using the Lipschitzness of ft      we can obtain the following bound for the  rst summation

 ft   xi      ft     

         

 cid xi        

     cid   

  cid 

       

 
 

zi   

  

  

 Strictly  the norm utilized in them is the general dual norm 

  cid 

  

Projectionfree Distributed Online Learning in Networks

Recall that Lemma   provides the bound for the second
summation  Combining these two bounds together  we
have

 ft   xi      ft     

 cid xi        

     cid      

   ft     

 cid cid xj        
     cid cid 

  cid 

  

  cid 

  

   

 

  cid 
  cid 

  

  

Hence 

RT  xi     

 ft   xi      ft     

         ft     
  cid 

  

  cid 
  cid 
  cid 
  cid 
  cid 
  cid 

  

  

  

    

 

  

  nL

 cid xi        

     cid 

 cid cid xj        
     cid cid 

   ft     

         ft     

  

  

Note that  as the parameters are set to be             
  the last term in the right side is exactly the regret of the
DODA algorithm applied to  ft     
In addition  using
the results in Lemma   the sum of the  rst and the second
terms is further bounded by  nLDT   Thus  we have

RT  xi       nLDT     Ra
   xi   
 
            
 
        

   nLDT    

    

 

 
 

  

 

     
 

 

Let    
     LT     Then via   bit of
analysis  we can verify that the choice of    satis es the
constraint required in Lemma  

  

       cid ht      

 

  iD 

   

        
        

The detailed veri cation is presented in the Appendix 
We thus  nally obtain
RT  xi       nLDT  
 
 

            

 

 

 
 
 
 

 
 
         
         

        
        

        

LDT  

LDT  

  Experiments
To evaluate the performance of the proposed DOCG algorithm  we conduct simulation experiments for   popular
machine learning problem  multiclass classi cation 

  Experimental Setup
Multiclass Classi cation In the distributed online learning setting  the problem is as follows  At each round
            each learner   is presented with   data example ei      Rk which belongs to one of the classes
           and is required to generate   decision mah     Rh   that predicts the class
      xT
trix           xT
label with arg max cid   xT
 cid  ei    Then the adversary reveals
the true class labels yi    and each learner   suffers   con 
 cid 
 cid 
 cid 
vex multivariate logistic loss

exp xT

 cid  ei      xT

ft            log

 

yi   ei   

 

 cid cid yi   

The convex domain of the decision matrices is         
Rh   cid   cid tr     where  cid cid tr denotes the nuclear norm
of matrices  In this case  the linear minimization required
in each iteration of DOCG amounts to compute   matrix   top singular vector  an operation that can be done in
time near linear to the number of nonzeros in the matrix 
whereas the projection onto   operation needed in traditional distributed online algorithms amounts to performing   full SVD  an   hk min       time operation that is
much more expensive 
Datasets We use two multiclass datasets selected from
the LIBSVM  repository with relatively large number of
instances  which is summarized in Table  

dataset
news 

aloi

  features

  classes

  instances

 

 

 

 

 
 

Table   Summary of the multiclass datasets

Network Topology To investigate the in uence of network topology  we conduct our experiments on three types
of graphs  which represent different levels of connectivity 
  Complete graph  This represents the highest level of
connectivity in our experiments  all nodes are connected to each other 

  Cycle graph  This represents the lowest level of connectivity in our experiments  each node has only two
immediate neighbors 

  WattsStrogatz  This random graph generation technique  Watts   Strogatz    has two tunable pa 

 https www csie ntu edu tw  cjlin libsvmtools datasets 

Projectionfree Distributed Online Learning in Networks

Figure   Comparison of DOCG  DOGD and DODA on two
multiclass datasets on   complete graph with   nodes

Figure       Comparison of DOCG on graphs with different
sizes      Comparison of DOCG on graphs with different topology and  xed   nodes  both on the aloi dataset 

rameters  the average degree of the graph   and the
rewiring probability   
In general  the higher the
rewiring probability  the better the connectivity of the
graph  Colin et al    We tune the parameters
      and       to achieve an intermediate level of
connectivity in our experiments 

Compared Algorithms To evaluate the performance
bene   of DOCG over its counterparts with projection operation  we compare it with two classic algorithms  DOGD  Yan et al    and DODA  Hosseini et al   
To verify that performing online conditional gradient in the
distributed setting does not lose much quality compared
with that in the centralized setting  we also compare DOCG with OCG       DOCG with   node 
Parameter Settings We set most of the parameters in
these algorithms as what their corresponding theories suggest  For instance  the parameters      in DOCG are
strictly set to be  
and the learning rates in DOGD are
set to be the typical decaying sequence  
  We use the
method utilized in  Duchi et al    to generate the doubly stochastic matrices and    the nuclear norm bound   to
  throughout 

 

 

  Experimental Results

We measure the running time of the DOGD  DODA and
DOCG algorithms run on   complete graph with   nodes
and see how fast the average losses decrease  From the
results shown in Figure   we can clearly observe that DOCG is signi cantly faster than both DOGD and DODA 
which illustrates the necessity and usefulness of using conditional gradient in distributed online learning 
We then investigate how the number of nodes affects the
performance of DOCG by running experiments on complete graphs with varying number of nodes  From the results shown in Figure     we can make the following
two main observations  First  the average losses decrease

more slowly on larger graphs than on smaller graphs  which
nicely con rms our theoretical results  Second  DOCG is
able to yield comparable results to the centralized OCG 
We  nally test the in uence of network topology on the algorithm   performance  We run experiments on the aforementioned three types of graphs with   nodes using the
aloi dataset  As shown in Figure     graphs with better
connectivity lead to slightly faster convergence  which illustrates good agreement of empirical results with our theoretical predictions 

  Conclusion
In this paper  we propose the distributed online conditional gradient algorithm for projectionfree distributed online learning in networks  We give detailed analysis of the
regret bound for the proposed algorithm  which depends on
both the network size and the network topology  We evaluate the ef cacy of the proposed algorithm on two realworld
datasets for   multiclass classi cation task and  nd that
it runs signi cantly faster than the counterpart algorithms
with projection  The theoretical results regarding the regret
bound for different graphs have also been veri ed 

Acknowledgements
This work is supported by National Program on Key Basic Research Project No   CB  and National
Natural Science Foundation of China Major Project No 
   It is also supported by the National Research
Foundation  Prime Ministers Of ce  Singapore under its
International Research Centres in Singapore Funding Initiative  We thank Zheng Xiong for helping constructing the
networks and thank Wei Liu for his kind help in preparing the submission and the rebuttal  We  nally acknowledge anonymous reviewers for their insightful comments
on comparison and explanation of the regret bound 

Projectionfree Distributed Online Learning in Networks

References
Berman  Abraham and Plemmons  Robert    Nonnegative
Matrices in the Mathematical Sciences  Academic Press 
 

Clarkson  Kenneth    Coresets  sparse greedy approximation  and the frankwolfe algorithm  ACM Transactions
on Algorithms  TALG     

Colin 

Igor  Bellet  Aurelien  Salmon 

Joseph  and
Cl emenc on  St ephan  Gossip dual averaging for decentralized optimization of pairwise functions  In International Conference on Machine Learning  pp   
 

Duchi  John    Agarwal  Alekh  and Wainwright  Martin    Dual averaging for distributed optimization  convergence analysis and network scaling  IEEE Transactions on Automatic Control     

Dud    Miroslav  Malick    er ome  et al  Lifted coordinate
descent for learning with tracenorm regularization  In
International Conference on Arti cial Intelligence and
Statistics  pp     

Hosseini  Saghar  Chapman  Airlie  and Mesbahi  Mehran 
Online distributed optimization via dual averaging 
In
IEEE Conference on Decision and Control  pp   
  IEEE   

Jaggi  Martin  Revisiting frankwolfe  projectionfree
sparse convex optimization  In International Conference
on Machine Learning  pp     

Lee  Soomin  Nedic  Angelia  and Raginsky  Maxim 
Decentralized online optimization with global obarXiv preprint
jectives and local communication 
arXiv   

Nesterov  Yurii  Primaldual subgradient methods for convex problems  Mathematical programming   
   

Ram    Sundhar  Nedi    Angelia  and Veeravalli  Venugopal    Distributed stochastic subgradient projection
algorithms for convex optimization  Journal of optimization theory and applications     

Sayed  Ali   et al  Adaptation  learning  and optimization
over networks  Foundations and Trends   cid  in Machine
Learning     

Frank  Marguerite and Wolfe  Philip 

An algorithm
for quadratic programming  Naval Research Logistics
Quarterly     

Watts  Duncan   and Strogatz  Steven    Collective dynamics of smallworldnetworks  nature   
   

Xiao  Lin  Dual averaging methods for regularized stochastic learning and online optimization  Journal of Machine
Learning Research   Oct   

Yan  Feng  Sundaram  Shreyas  Vishwanathan  SVN  and
Qi  Yuan  Distributed autonomous online learning  Regrets and intrinsic privacypreserving properties  IEEE
Transactions on Knowledge and Data Engineering   
   

Garber  Dan  Faster projectionfree convex optimization
over the spectrahedron  In Advances In Neural Information Processing Systems  pp     

Garber  Dan and Hazan  Elad    linearly convergent variant of the conditional gradient algorithm under strong
convexity  with applications to online and stochastic optimization  SIAM Journal on Optimization   
   

Harchaoui  Zaid 

Juditsky  Anatoli  and Nemirovski 
Arkadi 
Conditional gradient algorithms for normregularized smooth convex optimization  Mathematical
Programming     

Hazan  Elad  Introduction to online convex optimization 
Foundations and Trends   cid  in Optimization   
   

Hazan  Elad and Kale  Satyen  Projectionfree online learning  In International Conference on Machine Learning 
pp     

Hazan  Elad and Luo  Haipeng  Variancereduced and
projectionfree stochastic optimization  In International
Conference on Machine Learning  pp     

