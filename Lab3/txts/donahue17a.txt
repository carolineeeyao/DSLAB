Dance Dance Convolution

Chris Donahue   Zachary    Lipton   Julian McAuley  

Abstract

Dance Dance Revolution  DDR  is   popular
rhythmbased video game  Players perform steps
on   dance platform in synchronization with music as directed by onscreen step charts  While
many step charts are available in standardized
packs  players may grow tired of existing charts 
or wish to dance to   song for which no chart exists  We introduce the task of learning to choreograph  Given   raw audio track  the goal is to
produce   new step chart  This task decomposes
naturally into two subtasks  deciding when to
place steps and deciding which steps to select 
For the step placement task  we combine recurrent and convolutional neural networks to ingest
spectrograms of lowlevel audio features to predict steps  conditioned on chart dif culty  For
step selection  we present   conditional LSTM
generative model that substantially outperforms
ngram and  xedwindow approaches 

Figure   Proposed learning to choreograph pipeline for four seconds of the song Knife Party feat  Mistajam   Sleaze  The pipeline
ingests audio features  Bottom  and produces   playable DDR
choreography  Top  corresponding to the audio 

  Introduction
Dance Dance Revolution  DDR  is   rhythmbased video
game with millions of players worldwide  Hoysniemi 
  Players perform steps atop   dance platform  following prompts from an onscreen step chart to step on the
platform   buttons at speci    musically salient points in
time    player   score depends upon both hitting the correct buttons and hitting them at the correct time  Step charts
vary in dif culty with harder charts containing more steps
and more complex sequences  The dance pad contains up 
down  left  and right arrows  each of which can be in one
of four states  on  off  hold  or release  Because the four arrows can be activated or released independently  there are
  possible step combinations at any instant 

  UCSD Department of Music  San Diego  CA   UCSD Department of Computer Science  San Diego  CA  Correspondence
to  Chris Donahue  cdonahue ucsd edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Step charts exhibit rich structure and complex semantics to
ensure that step sequences are both challenging and enjoyable  Charts tend to mirror musical structure  particular sequences of steps correspond to different motifs  Figure  
and entire passages may reappear as sections of the song
are repeated  Moreover  chart authors strive to avoid patterns that would compel   player to face away from the
screen 
The DDR community uses simulators  such as the opensource StepMania  that allow fans to create and play their
own charts    number of proli   authors produce and disseminate packs of charts  bundling metadata with relevant
recordings  Typically  for each song  packs contain one
chart for each of  ve dif culty levels 
Despite the game   popularity  players have some reasonable complaints  For one  packs are limited to songs with
favorable licenses  meaning players may be unable to dance
to their favorite songs  Even when charts are available 
players may tire of repeatedly performing the same charts 
Although players can produce their own charts  the process
is painstaking and requires signi cant expertise 

Dance Dance Convolution

underrecognized source of annotated data for MIR research  StepMania Online    popular repository of DDR
data  distributes over  Gb of packs with annotations
for more than    songs 
In addition to introducing  
novel task and methodology  we contribute two large public datasets  which we consider to be of notably high quality
and consistency  Each dataset is   collection of recordings
and step charts  One contains charts by   single author and
the other by multiple authors 
For both prediction stages of learning to choreograph  we
demonstrate the superior performance of neural networks
over strong alternatives  Our best model for step placement
jointly learns   convolutional neural network  CNN  representation and   recurrent neural network  RNN  which
integrates information across consecutive time slices  This
method outperforms CNNs alone  multilayer perceptrons
 MLPs  and linear models 
Our bestperforming system for step selection consists of
  conditional LSTM generative model  As auxiliary information  the model takes beat phase    number representing
the fraction of   beat at which   step occurs  Additionally 
the best models receive the time difference  measured in
beats  since the last and until the next step  This model selects steps that are more consistent with expert authors than
the best ngram and  xedwindow models  as measured by
perplexity and pertoken accuracy 

  Contributions

In short  our paper offers the following contributions 

  We de ne learning to choreograph    new task with
realworld usefulness and strong connections to fundamental problems in MIR 

  We introduce two large  curated datasets for benchmarking DDR choreography algorithms  They represent an underrecognized source of music annotations 
  We introduce an effective pipeline for learning to

choreograph with deep neural networks 

  Data
Basic statistics of our two datasets are shown in Table  
The  rst dataset contains   songs choreographed by   single proli   author who works under the name Fraxtil  This
dataset contains  ve charts per song corresponding to increasing dif culty levels  We  nd that while these charts
overlap signi cantly  the lower dif culty charts are not
strict subsets of the higher dif culty charts  Figure   The

 https github com chrisdonahue ddc
 Demonstration showing human choreography alongside out 

put of our method  https youtu be yUc      

Figure     fourbeat measure of   typical chart and its rhythm
depicted in musical notation  Red  quarter notes  Blue  eighth
notes  Yellow  sixteenth notes      jump step      freeze step

In this paper  we seek to automate the process of step chart
generation so that players can dance to   wider variety of
charts on any song of their choosing  We introduce the task
of learning to choreograph  in which we learn to generate
step charts from raw audio  Although this task has previously been approached via adhoc methods  we are the
 rst to cast it as   learning task in which we seek to mimic
the semantics of humangenerated charts  We break the
problem into two subtasks  First  step placement consists
of identifying   set of timestamps in the song at which to
place steps  This process can be conditioned on   playerspeci ed dif culty level  Second  step selection consists of
choosing which steps to place at each timestamp  Running
these two steps in sequence yields   playable step chart 
This process is depicted in Figure  
Progress on learning to choreograph may also lead to advances in music information retrieval  MIR  Our step
placement task  for example  closely resembles onset detection    wellstudied MIR problem  The goal of onset
detection is to identify the times of all musically salient
events  such as melody notes or drum strikes  While not
every onset in our data corresponds to   DDR step  every
DDR step corresponds to an onset  In addition to marking steps  DDR packs specify   metronome click track for
each song  For songs with changing tempos  the exact location of each change and the new tempo are annotated 
This click data could help to spur algorithmic innovation
for beat tracking and tempo detection 
Unfortunately  MIR research is stymied by the dif culty
of accessing large  wellannotated datasets  Songs are often subject to copyright issues  and thus must be gathered by each researcher independently  Collating audio
with separatelydistributed metadata is nontrivial and errorprone owing to the multiple available versions of many
songs  Researchers often must manually align their version of   song to the metadata  In contrast  our dataset is
publicly available  standardized and contains meticulouslyannotated labels as well as the relevant recordings 
We believe that DDR charts represent an abundant and

Dance Dance Convolution

Figure   Five seconds of choreography by dif culty level for the
song KOAN Sound   The Edge from the Fraxtil training set 

second dataset is   larger  multiauthor collection called In
The Groove  ITG  this dataset contains   songs with one
chart per dif culty  except for   songs that lack charts for
the highest dif culty  Both datasets contain electronic music with constant tempo and   strong beat  characteristic of
music favored by the DDR community 

Table   Dataset statistics

Dataset
Num authors
Num packs
Num songs
Num charts
Steps sec
Vocab size

Fraxtil
 
 
    hrs 
    hrs 
 
 

ITG
 
 
    hrs 
    hrs 
 
 

Note that while the total number of songs is relatively
small  when considering all charts across all songs the
datasets contain around   hours of annotations and
  steps  The two datasets have similar vocabulary
sizes   and   distinct step combinations  respectively 
Around   of the steps in both datasets consist of   single  instantaneous arrow 
Step charts contain several invariances  for example interchanging all instances of left and right results in an equally
plausible sequence of steps  To augment the amount of data
available for training  we generate four instances of each
chart  by mirroring left right  up down  or both  Doing so
considerably improves performance in practice 
In addition to encoded audio  packs consist of metadata including   song   title  artist    list of timestamped tempo
changes  and   time offset to align the recording to the tempos  They also contain information such as the chart dif 
 culties and the name of the choreographer  Finally  the
metadata contains   full list of steps  marking the measure
and beat of each  To make this data easier to work with 
we convert it to   canonical form consisting of  beat  time 
step  tuples 

Figure   Number of steps per rhythmic subdivision by dif culty
in the Fraxtil dataset 

The charts in both datasets echo highlevel rhythmic structure in the music  An increase in dif culty corresponds
to increasing propensity for steps to appear at  ner rhythmic subdivisions  Beginner charts tend to contain only
quarter notes and eighth notes  Higherdif culty charts re 
 ect more complex rhythmic details in the music  featuring higher densities of eighth and sixteenth note steps  th 
 th  as well as triplet patterns  th   th   Figure  

  Problem De nition
  step can occur in up to   different locations  subdivisions  within each measure  However  measures contain roughly   steps on average  This level of sparsity
makes it dif cult to uncover patterns across long sequences
of  mostly empty  frames via   single endto end sequential model  So  to make automatic DDR choreography
tractable  we decompose it into two subtasks  step placement and step selection 
In step placement  our goal is to decide at what precise
times to place steps    step placement algorithm ingests
raw audio features and outputs timestamps corresponding
to steps  In addition to the audio signal  we provide step
placement algorithms with   onehot representation of the
intended dif culty rating for the chart 
Step selection involves taking   discretized list of step times
computed during step placement and mapping each of these
to   DDR step  Our approach to this problem involves modeling the probability distribution    mn            mn 
where mn is the nth step in the sequence  Some steps require that the player hit two or more arrows at once    jump 
or hold on one arrow for some duration    freeze  Figure  

  Methods
We now describe our speci   solutions to the step placement and selection problems  Our basic pipeline works
as follows 
  extract an audio feature representation 

BeginnerEasyMediumHardChallenge th th th nd th thDance Dance Convolution

  feed this representation into   step placement algorithm 
which estimates probabilities that   ground truth step lies
within that frame    use   peakpicking process on this
sequence of probabilities to identify the precise timestamps
at which to place steps  and  nally   given   sequence of
timestamps  use   step selection algorithm to choose which
steps to place at each time 

  Audio Representation

Music  les arrive as lossy encodings at  kHz   We decode the audio  les into stereo PCM audio and average
the two channels to produce   monophonic representation 
We then compute   multipletimescale shorttime Fourier
transform  STFT  using window lengths of  ms   ms 
and  ms and   stride of  ms  Shorter window sizes
preserve lowlevel features such as pitch and timbre while
larger window sizes provide more context for highlevel
features such as melody and rhythm  Hamel et al   
Using the ESSENTIA library  Bogdanov et al    we
reduce the dimensionality of the STFT magnitude spectra
to   frequency bands by applying   Melscale  Stevens
et al     lterbank  We scale the  lter outputs logarithmically to better represent human perception of loudness 
Finally  we prepend and append seven frames of past and
future context respectively to each frame 
For  xedwidth methods  the  nal audio representation is  
      tensor  These correspond to the temporal width
of   representing  ms of audio context    frequency
bands  and   different window lengths  To better condition
the data for learning  we normalize each frequency band to
zero mean and unit variance  Our approach to acoustic feature representation closely follows the work of Schl uter  
  ock   who develop similar representations to perform onset detection with CNNs 

  Step Placement

We consider several models to address the step placement
task  Each model   output consists of   single sigmoid unit
which estimates the probability that   step is placed  For
all models  we augment the audio features with   onehot
representation of dif culty 
Following stateof theart work on onset detection
 Schl uter     ock    we adopt   convolutional neural
network  CNN  architecture  This model consists of two
convolutional
layers followed by two fully connected
layers  Our  rst convolutional layer has    lter kernels
that are  wide in time and  wide in frequency  The
second layer has    lter kernels that are  wide in time
and  wide in frequency  We apply    maxpooling after
each convolutional layer  only in the frequency dimension 
with   width and stride of   Both convolutional layers

Figure   CLSTM model used for step placement

use recti ed linear units  ReLU   Glorot et al   
Following the convolutional
layers  we add two fully
connected layers with recti er activation functions and  
and   nodes  respectively 
To improve upon the CNN  we propose   CLSTM model 
combining   convolutional encoding with an RNN that integrates information across longer windows of time  To
encode the raw audio at each time step  we  rst apply two
convolutional layers  of the same shape as the CNN  across
the full unrolling length  The output of the second convolutional layer is      tensor  which we  atten along the
channel and frequency axes  preserving the temporal dimension  The  attened features at each time step then become the inputs to   twolayer RNN 
The CLSTM contains long shortterm memory  LSTM 
units  Hochreiter   Schmidhuber    with forget gates
 Gers   Schmidhuber    The LSTM consists of   layers with   nodes each  Following the LSTM layers  we
apply two fully connected ReLU layers of dimension  
and   This architecture is depicted in Figure   We
train this model using   unrollings for backpropagation
through time 
  chart   intended dif culty in uences decisions both
about how many steps to place and where to place them 
For lowdif culty charts  the average number of steps per
second is less than one  In contrast  the highestdif culty
charts exceed seven steps per second  We trained all models both with and without conditioning on dif culty  and
found the inclusion of this feature to be informative  We

Dance Dance Convolution

Figure   One second of peak picking  Green  Ground truth region     true positive      false positive      false negative     
two peaks smoothed to one by Hamming window      misaligned
peak accepted as true positive by  ms tolerance

concatenate dif culty features to the  attened output of the
CNN before feeding the vector to the fully connected  or
LSTM  layers  Figure   We initialize weight matrices
following the scheme of Glorot   Bengio  

Training Methodology We minimize binary crossentropy with minibatch stochastic gradient descent  For
all models we train with batches of size   scaling down
gradients when their    norm exceeds   We apply  
dropout following each LSTM and fully connected layer 
For LSTM layers  we apply dropout in the input to output
but not temporal directions  following best practices from
 Zaremba et al    Lipton et al    Dai   Le   
Although the problem exhibits pronounced class imbalance
  negatives  we achieved better results training on imbalanced data than with rebalancing schemes  We exclude
all examples before the  rst step in the chart or after the
last step as charts typically do not span the entire duration
of the song 
For recurrent neural networks  the target at each frame is
the ground truth value corresponding to that frame  We
calculate updates using backpropagation through time with
  steps of unrolling  equal to one second of audio or
two beats on   typical track   BPM  We train all networks with earlystopping determined by the area under
the precisionrecall curve on validation data  All models
satisfy this criteria within   hours of training on   single
machine with an NVIDIA Tesla     GPU 

  Peak Picking

Following standard practice for onset detection  we convert sequences of step probabilities into   discrete set of

 For LogReg and MLP  we add dif culty to input layer 

Figure   LSTM model used for step selection

chosen placements via   peakpicking process  First we
run our step placement algorithm over an entire song to assign the probabilities of   step occurring within each  ms
frame  We then convolve this sequence of predicted probabilities with   Hamming window  smoothing the predictions and suppressing doublepeaks from occurring within
  short distance  Finally  we apply   constant threshold to
choose which peaks are high enough  Figure   Because
the number of peaks varies according to chart dif culty  we
choose   different threshold per dif culty level  We consider predicted placements to be true positives if they lie
within    ms window of   ground truth 

  Step Selection

We treat the step selection task as   sequence generation
problem  Our approach follows related work in language
modeling where RNNs are wellknown to produce coherent
text that captures longrange relationships  Mikolov et al 
  Sutskever et al    Sundermeyer et al   
Our LSTM model passes over the ground truth step placements and predicts the next token given the previous sequence of tokens  The output is   softmax distribution over
the   possible steps  As input  we use   more compact
bagof arrows representation containing   features   per
arrow  to depict the previous step  For each arrow  the  
corresponding features represent the states on  off  hold 
and release  We found the bagof arrows to give equivalent

 In DDR  scores depend on the accuracy of   player   step
timing  The highest scores require that   step is performed within
 ms of its appointed time  this suggests that   reasonable algorithm should place steps with an even  ner level of granularity 

 Step FeatsRhythmic FeatsCurr StepNext StepLSTMLSTM   LSTMLSTM   RNNDance Dance Convolution

Table   Results for step placement experiments

Table   Results for step selection experiments

Dataset
Model
Fraxtil
LogReg
Fraxtil
MLP
Fraxtil
CNN
CLSTM Fraxtil
ITG
LogReg
ITG
MLP
CNN
ITG
CLSTM ITG

PPL
 
 
 
 
 
 
 
 

AUC Fscorec Fscorem
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

performance to the onehot representation while requiring
fewer parameters  We add an additional feature that functions as   start token to denote the  rst step of   chart  For
this task  we use an LSTM with   layers of   cells each 
Finally  we provide additional musical context to the step
selection models by conditioning on rhythmic features
 Figure   To inform models of the nonuniform spacing
of the step placements  we consider the following three features     time adds two features representing the time
since the previous step and the time until the next step 
   beat adds two features representing the number of
beats since the previous and until the next step    beat
phase adds four features representing which  th note subdivision of the beat the current step most closely aligns to 

Training Methodology For all neural network models 
we learn parameters by minimizing crossentropy  We train
with minibatches of size   and scale gradients using the
same scheme as for step placement  We use   dropout
during training for both the MLP and RNN models in the
same fashion as for step placement  We use   steps of unrolling  representing an average of   seconds for the easiest charts and   seconds for the hardest  We apply earlystopping determined by average perstep cross entropy on
validation data  All models satisfy this criteria within  
hours of training on   single machine with an NVIDIA
Tesla     GPU 

  Experiments
For both the Fraxtil and ITG datasets we apply    
  splits for training  validation  and test data  respectively  Because of correlation between charts for the same
song of varying dif culty  we ensure that all charts for  
particular song are grouped together in the same split 

  Step Placement

We evaluate the performance of our step placement methods against baselines via the methodology outlined below 

Model
KN 
MLP 
MLP     time
MLP     beat   beat phase
LSTM 
LSTM     time
LSTM     beat   beat phase
LSTM 
LSTM     time
LSTM     beat   beat phase
KN 
MLP 
MLP     time
MLP     beat   beat phase
LSTM 
LSTM     time
LSTM     beat   beat phase
LSTM 
LSTM     time
LSTM     beat   beat phase

Dataset PPL Accuracy
Fraxtil
Fraxtil
Fraxtil
Fraxtil
Fraxtil
Fraxtil
Fraxtil
Fraxtil
Fraxtil
Fraxtil
ITG
ITG
ITG
ITG
ITG
ITG
ITG
ITG
ITG
ITG

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Baselines To establish reasonable baselines for step
placement  we  rst report the results of   logistic regressor  LogReg  trained on  attened audio features  We also
report the performance of an MLP  Our MLP architecture
contains two fullyconnected layers of size   and  
with recti er nonlinearity applied to each layer  We apply
dropout with probability   after each fullyconnected
layer during training  We model our CNN baseline on the
method of Schl uter     ock     stateof theart algorithm for onset detection 

Metrics We report each model   perplexity  PPL  averaged across each frame in each chart in the test data  Using
the sparse step placements  we calculate the average perchart area under the precisionrecall curve  AUC  We average the best perchart Fscores and report this value as
Fscorec  We calculate the micro Fscore across all charts
and report this value as Fscorem 
In Table   we list the results of our experiments for step
placement  For ITG  models were conditioned on not just
dif culty but also   onehot representation of chart author 
For both datasets  the CLSTM model performs the best
by all evaluation metrics  Our models achieve signi cantly
higher Fscores for harder dif culty step charts  On the
Fraxtil dataset  the CLSTM achieves an Fscorec of  
for the hardest dif culty charts but only   for the lowest dif culty  The dif cult charts contribute more to Fscorem calculations because they have more ground truth
positives  We discuss these results further in Section  

Dance Dance Convolution

  Step Selection
Baselines For step selection  we compare the performance of the conditional LSTM to an ngram model  Note
that perplexity can be unbounded when   test set token is
assigned probability   by the generative model  To protect
the ngram models against unbounded loss on previously
unseen ngrams  we use modi ed KneserNey smoothing
 Chen   Goodman    following best practices in language modeling  Mikolov et al    Sutskever et al 
  Speci cally  we train   smoothed  gram model
with backoff  KN  as implemented in Stolcke  
Following the work of Bengio et al    we also compare against    xedwindow  gram MLP which takes  
bagof arrowsencoded steps as input and predicts the next
step  The MLP contains two fullyconnected layers with
  and   nodes and   dropout after each layer during training  As with the LSTM  we train the MLP both
with and without access to side features 
In addition to
the LSTM with   steps of unrolling  we train an LSTM
with   steps of unrolling  These baselines show that the
LSTM learns complex  longrange dependencies  They
also demonstrate the discriminative information conferred
by the  time   beat  and beat phase features 

Metrics We report the average perstep perplexity  averaging scores calculated separately on each chart  We
also report   pertoken accuracy  We calculate accuracy
by comparing the groundtruth step to the argmax over  
model   predictive distribution given the previous sequence
of groundtruth tokens  For   given chart  the per token
accuracy is averaged across time steps  We produce  nal
numbers by averaging scores across charts 
In Table   we present results for the step selection task 
For the Fraxtil dataset  the best performing model was the
LSTM conditioned on both  beat and beat phase  while
for ITG it was the LSTM conditioned on  time  While
conditioning on rhythm features was generally bene cial 
the bene ts of various features were not strictly additive 
Representing  beat and  time as real numbers outperformed bucketed representations 
Additionally  we explored the possibility of incorporating
more comprehensive representations of the audio into the
step selection model  We considered   variety of representations  such as conditioning on CNN features learned from
the step placement task  We also experimented with jointly
learning   CNN audio encoder 
In all cases  these approaches led to rapid over tting and never approached the
performance of the conditional LSTM generative model 
perhaps   much larger dataset could support these approaches  Finally  we tried conditioning the step selection
models on both dif culty and chart author but found these
models to over   quickly as well 

Figure   Top    real step chart from the Fraxtil dataset on the
song Anamanaguchi   Mess  Middle  Onestep lookahead predictions for the LSTM model  given Fraxtil   choreography as input 
The model predicts the next step with high accuracy  errors in
red  Bottom  Choreography generated by conditional LSTM
model 

  Discussion
Our experiments establish the feasibility of using machine
learning to automatically generate highquality DDR charts
from raw audio  Our performance evaluations on both
subtasks demonstrate the advantage of deep neural networks over classical approaches  For step placement  the
best performing model is an LSTM with CNN encoder 
an approach which has been used for speech recognition
 Amodei et al    but  to our knowledge  never for
musicrelated tasks  We noticed that by all metrics  our
models perform better on higherdif culty charts  Likely 
this owes to the comparative class imbalance of the lower
dif culty charts 
The superior performance of LSTMs over  xedwindow
approaches on step selection suggests both that DDR charts
exhibit long range dependencies and that recurrent neural
networks can exploit this complex structure  In addition to
reporting quantitative results  we visualize the step selection model   nextstep predictions  Here  we give the entire
ground truth sequence as input but show the predicted next
step at each time  We also visualize   generated choreography  where each sampled output from the LSTM is fed in
as the subsequent input  Figure   We note the high accuracy of the model   predictions and qualitative similarity of
the generated sequence to Fraxtil   choreography 
For step selection  we notice that modeling the Fraxtil
dataset choreography appears to be easy compared to the
multiauthor ITG dataset  We believe this owes to the distinctiveness of author styles  Because we have so many
step charts for Fraxtil  the network is able to closely mimic
his patterns  While the ITG dataset contains multiple charts
per author  none are so proli   as Fraxtil 

Dance Dance Convolution

We released   public demo  using our most promising models as measured by our quantitative evaluation  Players upload an audio  le  select   dif culty rating and receive  
step chart for use in the StepMania DDR simulator  Our
demo produces   step chart for     minute song in about
  seconds using an NVIDIA Tesla     GPU  At time of
writing    players have produced   step charts with
the demo  We also solicited feedback  on   scale of   for
player  satisfaction  with the demo results  The   respondents reported an average satisfaction of  
  promising direction for future work is to make the selection algorithm audioaware  We know qualitatively that
elements in the ground truth choreography tend to coincide
with speci   musical events  jumps are used to emphasize
accents in   rhythm  freezes are used to transition from regions of high rhythmic intensity to more ambient sections 
DDR choreography might also bene   from an endto end
approach  in which   model simultaneously places steps
and selects them  The primary obstacle here is data sparsity at any suf ciently high feature rate  At  Hz   about
  of labels are null  So in   timesteps of unrolling 
an RNN might only encounter   ground truth steps 
We demonstrate that step selection methods are improved
by incorporating  beat and beat phase features  however
our current pipeline does not produce this information  In
lieu of manual tempo input  we are restricted to using
 time features when executing our pipeline on unseen
recordings  If we trained   model to detect beat phase  we
would be able to use these features for step selection 

  Related Work
Several academic papers address DDR  These include anthropological studies  Hoysniemi    Behrenshausen 
  and two papers that describe approaches to automated choreography  The  rst  called Dancing Monkeys 
uses rulebased methods for both step placement and step
selection    Keeffe    The second employs genetic
algorithms for step selection  optimizing an adhoc  tness
function  Nogaj    Neither establishes reproducible
evaluation methodology or learns the semantics of steps
from data 
Our step placement task closely resembles the classic problem of musical onset detection  Bello et al    Dixon 
  Several onset detection papers investigate modern deep learning methodology  Eyben et al    employ bidirectional LSTMs  BLSTMs  for onset detection 
Marchi et al    improve upon this work  developing
  rich multiresolution feature representation  Schl uter  
  ock   demonstrate   CNNbased approach  against

 http deepx ucsd edu ddc

which we compare  that performs competitively with the
prior BLSTM work  Neural networks are widely used on
  range of other MIR tasks  including musical chord detection  Humphrey   Bello    BoulangerLewandowski
et al      and boundary detection  Ullrich et al   
another transient audio phenomenon 
Our step selection problem resembles the classic natural
language processing task of statistical language modeling  Classical methods  which we consider  include ngram
distributions  Chen   Goodman    Rosenfeld   
Bengio et al    demonstrate an approach to language
modeling using neural networks with  xedlength context 
More recently  RNNs have demonstrated superior performance to  xedwindow approaches  Mikolov et al   
Sundermeyer et al    Sutskever et al    LSTMs
are also capable of modeling language at the character level
 Karpathy et al    Kim et al    While   thorough
explanation of modern RNNs exceeds the scope of this paper  we point to two comprehensive reviews of the literature  Lipton et al    Greff et al    Several papers
investigate neural networks for singlenote melody generation  Bharucha   Todd    Eck    Chu et al   
Hadjeres   Pachet    and polyphonic melody generation  BoulangerLewandowski et al   
Learning to choreograph requires predicting both the timing and the type of events in relation to   piece of music  In
that respect  our task is similar to audio sequence transduction tasks  such as musical transcription and speech
recognition  RNNs currently yield stateof theart performance for musical transcription    ock   Schedl   
BoulangerLewandowski et al      Sigtia et al   
RNNs are widely used for speech recognition  Graves  
Jaitly    Graves et al      Sainath et al   
and the stateof theart method  Amodei et al    combines convolutional and recurrent networks  While our
work is methodologically similar  it differs from the above
in that we consider an entirely different application 

  Conclusions
By combining insights from musical onset detection and
statistical language modeling  we have designed and evaluated   number of deep learning methods for learning to
choreograph  We have introduced standardized datasets
and reproducible evaluation methodology in the hope of
encouraging wider investigation into this and related problems  We emphasize that the sheer volume of available step
charts presents   rare opportunity for MIR  access to large
amounts of highquality annotated data  This data could
help to spur innovation for several MIR tasks  including
onset detection  beat tracking  and tempo detection 

Dance Dance Convolution

Acknowledgements
The authors would like to thank Jeff Donahue  Shlomo
Dubnov  Jennifer Hsu  Mohsen Malmir  Miller Puckette 
Adith Swaminathan and Sharad Vikram for their helpful feedback on this work 
This work used the Extreme Science and Engineering Discovery Environment
 XSEDE   Towns et al    which is supported by
National Science Foundation grant number ACI 
GPUs used for this research were graciously donated by the
NVIDIA Corporation 

References
Amodei  Dario  Anubhai  Rishita  Battenberg  Eric  Case 
Carl  Casper  Jared  Catanzaro  Bryan  Chen  Jingdong 
Chrzanowski  Mike  Coates  Adam  Diamos  Greg  et al 
Deep speech   Endto end speech recognition in english
and mandarin  In ICML   

Behrenshausen  Bryan    Toward    kin  aesthetic of video
gaming the case of dance dance revolution  Games and
Culture   

Bello  Juan Pablo  Daudet  Laurent  Abdallah  Samer 
Duxbury  Chris  Davies  Mike  and Sandler  Mark     
tutorial on onset detection in music signals  IEEE Transactions on speech and audio processing   

Bengio  Yoshua  Ducharme    ejean  Vincent  Pascal  and
Jauvin  Christian    neural probabilistic language
model  JMLR   

Bharucha  Jamshed   and Todd  Peter    Modeling the
perception of tonal structure with neural nets  Computer
Music Journal   

  ock  Sebastian and Schedl  Markus  Polyphonic piano
In

note transcription with recurrent neural networks 
ICASSP   

Bogdanov  Dmitry  Wack  Nicolas    omez  Emilia  Gulati 
Sankalp  Herrera  Perfecto  Mayor  Oscar  Roma  Gerard  Salamon  Justin  Zapata  Jos      and Serra  Xavier 
Essentia  An audio analysis library for music information retrieval  In ISMIR   

BoulangerLewandowski  Nicolas  Bengio  Yoshua  and
Vincent  Pascal  Modeling temporal dependencies in
highdimensional sequences  Application to polyphonic
music generation and transcription  In ICML   

BoulangerLewandowski  Nicolas  Bengio  Yoshua  and
Vincent  Pascal  Audio chord recognition with recurrent
neural networks  In ISMIR     

Chen  Stanley   and Goodman  Joshua  An empirical study
of smoothing techniques for language modeling  Technical Report TR  Harvard University   

Chu  Hang  Urtasun  Raquel  and Fidler  Sanja  Song from
pi    musically plausible network for pop music generation  arXiv   

Dai  Andrew   and Le  Quoc    Semisupervised sequence

learning  In NIPS   

Dixon  Simon  Onset detection revisited  In Proceedings
of the  th International Conference on Digital Audio Effects   

Eck  Douglas     rst look at music composition using lstm
recurrent neural networks  Technical Report IDSIA 
   

Eyben  Florian    ock  Sebastian  Schuller  Bj orn    and
Graves  Alex  Universal onset detection with bidirectional long shortterm memory neural networks  In ISMIR   

Gers  Felix and Schmidhuber    urgen  Recurrent nets that
In International Joint Conference on

time and count 
Neural Networks  IJCNN   

Glorot  Xavier and Bengio  Yoshua  Understanding the dif 
 culty of training deep feedforward neural networks  In
AISTATS   

Glorot  Xavier  Bordes  Antoine  and Bengio  Yoshua 
Deep sparse recti er neural networks  In AISTATS   

Graves  Alex and Jaitly  Navdeep  Towards endto end
In

speech recognition with recurrent neural networks 
ICML   

Graves  Alex  Fern andez  Santiago  Gomez  Faustino  and
Schmidhuber    urgen  Connectionist temporal classi 
cation  labelling unsegmented sequence data with recurrent neural networks  In ICML   

Graves  Alex  Mohamed  Abdelrahman  and Hinton  Geoffrey  Speech recognition with deep recurrent neural
networks  In ICASSP   

Greff  Klaus  Srivastava  Rupesh    Koutn    Jan  Steunebrink  Bas    and Schmidhuber    urgen  Lstm    search
IEEE transactions on neural networks
space odyssey 
and learning systems   

Hadjeres  Ga etan and Pachet  Franc ois 

Deepbach 
for bach chorales generation 

  steerable model
arXiv   

BoulangerLewandowski  Nicolas  Bengio  Yoshua  and
Vincent  Pascal  Highdimensional sequence transduction  In ICASSP     

Hamel  Philippe  Bengio  Yoshua  and Eck  Douglas 
Building musicallyrelevant audio features through multiple timescale representations  In ISMIR   

Dance Dance Convolution

Sigtia  Siddharth  Benetos  Emmanouil  and Dixon  Simon 
An endto end neural network for polyphonic piano muIEEE ACM Transactions on Audio 
sic transcription 
Speech  and Language Processing   

Stevens  Stanley Smith  Volkmann  John  and Newman 
Edwin      scale for the measurement of the psychological magnitude pitch  The Journal of the Acoustical
Society of America   

Stolcke  Andreas  Srilman extensible language modeling

toolkit  In Interspeech   

Sundermeyer  Martin  Schl uter  Ralf  and Ney  Hermann 
Lstm neural networks for language modeling  In Interspeech   

Sutskever  Ilya  Martens  James  and Hinton  Geoffrey   
In

Generating text with recurrent neural networks 
ICML   

Towns  John  Cockerill  Timothy  Dahan  Maytal  Foster  Ian  Gaither  Kelly  Grimshaw  Andrew  Hazlewood  Victor  Lathrop  Scott  Lifka  Dave  Peterson 
Gregory    et al  Xsede  accelerating scienti   discovery  Computing in Science   Engineering   

Ullrich  Karen  Schl uter  Jan  and Grill  Thomas  Boundary
detection in music structure analysis using convolutional
neural networks  In ISMIR   

Zaremba  Wojciech  Sutskever 

and Vinyals 
Recurrent neural network regularization 

Oriol 
arXiv   

Ilya 

Hochreiter  Sepp and Schmidhuber    urgen  Long short 

term memory  Neural computation   

Hoysniemi  Johanna 

International survey on the dance
dance revolution game  Computers in Entertainment
 CIE   

Humphrey  Eric   and Bello  Juan Pablo  Rethinking automatic chord recognition with convolutional neural networks  In ICMLA   

Karpathy  Andrej 

Justin  and FeiFei  Li 
Visualizing and understanding recurrent networks 
arXiv   

Johnson 

Kim  Yoon  Jernite  Yacine  Sontag  David  and Rush 
Alexander    Characteraware neural language models  In Proceedings of the Thirtieth AAAI Conference on
Arti cial Intelligence   

Lipton  Zachary    Berkowitz  John  and Elkan  Charles   
critical review of recurrent neural networks for sequence
learning  arXiv   

Lipton  Zachary    Kale  David    Elkan  Charles  and
Wetzell  Randall  Learning to diagnose with LSTM recurrent neural networks  In ICLR   

Marchi  Erik  Ferroni  Giacomo  Eyben  Florian  Gabrielli 
Leonardo  Squartini  Stefano  and Schuller  Bjorn 
Multiresolution linear prediction based features for audio onset detection with bidirectional lstm neural networks  IEEE   

Mikolov  Tomas  Kara at  Martin  Burget  Lukas  Cernock    Jan  and Khudanpur  Sanjeev  Recurrent neural
network based language model  In Interspeech   

Nogaj  Adam    genetic algorithm for determining optimal
step patterns in dance dance revolution  Technical report 
State University of New York at Fredonia   

  Keeffe  Karl  Dancing monkeys  automated creation of
step  les for dance dance revolution  Technical report 
Imperial College London   

Rosenfeld  Ronald  Two decades of statistical language
modeling  Where do we go from here  Proceedings
of the IEEE   

Sainath  Tara    Weiss  Ron    Senior  Andrew  Wilson 
Kevin    and Vinyals  Oriol  Learning the speech frontend with raw waveform cldnns  In Interspeech   

Schl uter  Jan and   ock  Sebastian  Improved musical onIn

set detection with convolutional neural networks 
ICASSP   

