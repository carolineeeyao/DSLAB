Multilabel Classi cation with Group Testing and Codes

Shashanka Ubaru   Arya Mazumdar  

Abstract
In recent years 
the multiclass and mutlilabel
classi cation problems we encounter in many applications have very large       number
of classes  However  each instance belongs to
only one or few classes       the label vectors
are sparse  In this work  we propose   novel approach based on group testing to solve such large
multilabel classi cation problems with sparse label vectors  We describe various group testing
constructions  and advocate the use of concatenated Reed Solomon codes and unbalanced bipartite expander graphs for extreme classi cation problems  The proposed approach has several advantages theoretically and practically over
existing popular methods  Our method operates on the binary alphabet and can utilize the
wellestablished binary classi ers for learning 
The error correction capabilities of the codes are
leveraged for the  rst time in the learning problem to correct prediction errors  Even if   linearly
growing number of classi ers misclassify  these
errors are fully corrected  We establish Hamming
loss error bounds for the approach  More importantly  our method utilizes   simple prediction algorithm and does not require matrix inversion or
solving optimization problems making the algorithm very inexpensive  Numerical experiments
with various datasets illustrate the superior performance of our method 

  Introduction
In the multilabel classi cation problem  we are given   set
of labeled training data  xi  yi  
   where xi   Rp are
the input features for each data instances and yi       
 Department of Computer Science and Engineering  University of Minnesota at Twin Cities  MN USA   College of Information and Computer Sciences  University of Massachusetts
Amherst  Amherst  MA  USA  Correspondence to  Shashanka
Ubaru  ubaru umn edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

are vectors indicating the corresponding labels  classes  the
data instances belong to  The vector yi has   one in the jth
coordinate if the ith data point belongs to jth class  We
wish to learn   mapping  prediction rule  between the features and the labels  such that  we can predict the class label vector   of   new data point   correctly  Such multilabel classi cation problems occur in many domains such
as text mining  computer vision  music  and bioinformatics  Barutcuoglu et al    Trohidis    Tai   Lin 
  and modern applications involve large number of
labels  Popular applications with many labels include image and video annotation  Wang et al    web page
categorization  Agrawal et al    text and document
categorization  Tsoumakas et al    and others  Bhatia
et al    In most of these applications  the label vectors
yi are sparse  with average sparsity of    cid          each
data point belongs to   few  average   out of    classes 
The multiclass classi cation is an instance of the multilabel classi cation  where all data points belong to only one
of the   classes       
The simple binary classi cation problem  where       and
      is wellstudied  and several ef cient algorithms have
been proposed in the literature    natural approach used
to solve the multiclass              classi cation problem is to reduce the problem into   set of binary classi 
cation problem  and then employ the ef cient binary classi ers to solve the individual problems  Popular methods
based on this approach are  onevs all  allpairs  and the
errorcorrecting output code  ECOC   Dietterich   Bakiri 
  methods  In ECOC method  mdimensional binary
vectors  typically codewords from an error correcting code
with        are assigned to each class  and   binary classi ers are learned  For the jth classi cation  the jth coordinate of the corresponding codeword is used as the binary
label for each class  In the modern applications  where  
is typically very large  this approach is found to be very
ef cient due to the reduction of the class dimension 
The idea of ECOC approach has been extended to the multilabel classi cation  MLC  problem 
In the multiclass
classi cation  using codewords for each class in ECOC is
equivalent to multiplying the code matrix to the label vectors  since the label vectors are basis vectors  Hence  in
the multilabel setting  the reduction from   dimensional label vectors to   dimensional can be achieved by multiply 

Multilabel classi cation via group testing

ing   code matrix     Rm   to the label vector    This
reduction method was analyzed from the compressed sensing point of view in  Hsu et al    with the assumption
of output sparsity         is sparse  with average sparsity   
Using compressed sensing  CS  theory  the results in  Hsu
et al    show that for   linear hypothesis class and under the squared loss    random embedding  random code
matrix  of the classes to         log    dimensions does
not increase the    risk of the classi er  Similarly   Kapoor
et al    discusses MLC using compressed sensing in
the Bayesian framework  However  the CS approach requires solving an optimization problem to recover the label
vector  Constructions with faster recovery algorithms exist  see        Jafarpour et al    but we cannot obtain
   norm results with them 
Alternatively  embedding based approaches have been proposed to reduce the effective number of labels  These methods reduce the label dimension by projecting label vectors
onto   low dimensional space  based on the assumption that
the label matrix                 yn  is lowrank  The various
embedding methods proposed in the literature mainly differ in the way this reduction is achieved  The reduction is
achieved using SVD in  Tai   Lin    while column
subset selection is used in  Bi   Kwok     Zhang  
Schneider    used canonical correlation analysis and
 Chen   Lin    used an SVD approach that leverages
the feature space information   Yu et al    discussed
multilabel classi cation with missing entries and used an
embedding method with   regularized least squares objective  These embedding methods capture the label correlation  and Euclidean distance error guarantees are established  However  the low rank assumption breaks down in
many situations  Bhatia et al    Xu et al        
data is power law distributed  Babbar   Sch olkopf   
The state of the art embedding method called SLEEC
 Sparse Local Embedding for Extreme Classi cation   Bhatia et al    overcomes the limitations of
previous embedding methods by  rst clustering the data
into smaller regions  and then performs local embeddings
of label vectors by preserving distances to nearest label
vectors  However  this method also has many shortcomings  see  Babbar   Sch olkopf    Moreover  most
of these embedding based methods are very expensive 
They involve eigenvalue or singular value decompositions
and matrix inversions  and may require solving convex
optimization problems  all of which become impractical
for very large    In all the embedding methods and the CS
method  the reduced label space is   real space  no longer
binary  Hence we need to use regressors for training and
cannot leverage the ef cient binary classi ers for effective
training for the model  Prediction will also involve rounding thresholding of real values  This is additional work 
and choosing   right threshold is sometimes problematic 

Proposed Approach 
In this paper  we present   novel
reduction approach to solve the MLC problem  Our approach assumes output sparsity  sparse label vectors with
   cid     similar to the CS approach  but reduces   large binary label vector to   binary vector of smaller size  Since
the reduced label vectors are binary  we can use the ef 
cient binary classi ers for effective training for the model 
Our prediction algorithm is extremely simple and does not
involve any matrix inversion or solving optimization algorithm  The prediction algorithm can also detect and correct
errors  Hence  even if   constant fraction of the binary classi ers misclassify  our prediction error will be zero 
Our approach is based on the popular group testing problem  Dorfman    Du   Hwang    In the group
testing problem  we wish to ef ciently identify   small
number   of defective elements in   population of large size
   The idea is to test the items in groups with the premise
that most tests will return negative results  clearing the entire group  Only few    cid    tests are needed to detect the  
defectives  The items can be grouped in either an adaptive
or nonadaptive fashion  In the nonadaptive group testing
scheme  the grouping for each test can be described using
an       binary   entries  matrix   
We make the crucial observation that  the MLC problem
can be solved using the group testing  GT  premise  That
is  the problem of estimating the  few  classes of   data
instance from   large set of classes  is similar to identifying   small set of items from   large set  We consider
  group testing binary matrix   and reduce the label vectors yi   to smaller binary vectors zi using the boolean OR
operation zi       yi  described later  We can now
use binary classi ers on zi for training  The   classi ers
learn to test whether the data belongs to   group  of labels  or not  During prediction  the label vector can be recovered from the predictions of the classi ers using   simple inexpensive algorithm  requiring no matrix inversion or
solving optimization algorithms    low prediction cost is
extremely desirable in real time applications  Depending
on   certain property called       disjunct property of the
group testing matrix    the recovery algorithm can correct
up to  cid   cid  errors in the prediction  We discuss various
constructions for the group testing matrix   which have
the desired       disjunct property  We advocate the use
of concatenated Reed Solomon codes  Kautz   Singleton 
  and unbalanced bipartite expander graphs  Vadhan 
  as the group testing matrix    The optimal number
of binary classi ers required for exact recovery  to form  
      disjunct matrix  will be         logk    However  we show how this can be reduced to         log   
if we tolerate   small   error in the labels recovery 
The idea of grouping the labels helps overcome the issues
most existing methods encounter       when the data has

Multilabel classi cation via group testing

power law distribution  Babbar   Sch olkopf    that is
many labels have very few training instances  which is the
case in most popular datasets  and tail labels  Xu et al 
  Since the classi ers in our approach learn to test for
groups of labels  we will have more training instances per
group yielding effective classi ers  It is well known that
the onevs rest is   highly effective method  expensive 
and recently    doubly  parallelized version of this method
called DiSMEC  Babbar   Sch olkopf    was shown to
be very effective  Our approach is similar to onevs rest 
but the classi ers test for   group of labels  and we require
very few classi ers    log    instead of    Our approach
also resembles the Bloom  lter method  Cisse et al   
which is based on using hash functions to reduce the label
size  However  for Bloom  lters the lower dimension  
can be larger than     log     no bounds are established 
and they may yield many false positives  For proper encoding this method requires clustering of the labels 
We establish Hamming loss error bounds for the proposed
approach  Due to the error correcting capabilities of the
algorithm  even if   fraction of classi ers misclassify  we
can achieve zero prediction error  Numerical experiments
with various datasets illustrate the superior performance of
our group testing approach with different GT matrices  Our
method is extremely inexpensive compared to the CS approach and especially compared to the embedding based
methods  making it very desirable for real time applications
too  The results we obtain using the GT approach are more
accurate compared to the other popular methods  in terms
of Hamming distance  For many examples  the training errors we obtained were almost zero and the test errors were
also quite low 

  Preliminaries
Group testing  Formally  the group testing problem involves identifying an unknown ksparse binary vector    
      such that  supp         where supp          
yi  cid    is called the support of the vector    by performing
  small number of tests  measurements  In the MLC problem  we can view this vector as the sparse label vector   of
the data  indicating the   labels 
  nonadaptive group testing scheme with   tests is described by an       binary matrix    where each row
corresponds to   test  and Aij     if and only if the ith
test includes the jth element  The measured vector   is the
boolean OR operation between the matrix   and the label
vector    The boolean OR operation           can simply be obtained by setting every nonzero entry of the usual
matrixvector product Ay to    and leaving the zero entries
as they are  It can also be thought of as coordinatewise
Boolean OR of the columns of   that correspond to the
nonzero entries of   

De nition    Disjunctness  An       binary matrix  
is called kdisjunct if the support of any of its columns is
not contained in the union of the supports of any other  
columns 

  kdisjunct matrix gives   group testing scheme that identi es any defective set up to size   exactly 
De nition    Error Correction  An       binary matrix
  is called       disjunct         kdisjunct and eerror
detecting  if for every set   of columns of   with         
and        we have  supp           supp          
where      denote the ith column of   

        disjunct matrix can detect up to   errors in the
measurements and can correct up to  cid   cid  errors 
Several
random and deterministic construction of kdisjunct matrices have been proposed in the literature  Kautz   Singleton    Du   Hwang    Matrices from error correcting codes and expander graphs
have also been designed  Dyachkov et al    Ubaru
et al    Cheraghchi    Mazumdar   

  MLC via Group testing
In this section  we present our main idea of adapting the
group testing scheme to the multilabel classi cation problem  MLGT 
Training  Suppose we are given   training instances
 xi  yi  
   where xi   Rp are the input features for
each instances and yi        are corresponding label
vectors  We begin by assuming that each data instance belongs to at most   classes  the label vector   is   sparse 
We consider         disjunct matrix     Rm    We then
compute the reduced measured  label  vectors zi for each
label vectors yi                  using the boolean OR operation zi       yi  We can now train   binary classi ers
 wj  
   with jth entry of zi indicating which class   the ith instance belongs to for the jth
classi er  Algorithm   summarizes our training algorithm 

   based on  xi  zi  

   group testing matrix

Algorithm   MLGT  Training Algorithm

Input  Training data  xi  yi  
    Rm      binary classi er algorithm   
Output    classi ers  wj  
for                  do

  

zi       yi 

end for
for                  do
wj     xi  zij  

  

end for

Prediction  In the prediction stage  given   test data    
Rp  we use the   classi ers  wj  
   to obtain   predicted

Multilabel classi cation via group testing

reduced label vector     We know that     sparse label vector
can be recovered exactly  if the group testing matrix   is  
kdisjunct matrix  With         disjunct matrix        we
can recover the   sparse label vector exactly  even if  cid   cid 
binary classi ers misclassify  using the following decoder 
Decoder   Given   predicted reduced label vector     and
  group testing matrix    set the coordinate position of
   corresponding to                  to   if and only if
 supp     supp        
That is  we set the lth coordinate of    to   if the number
of coordinates that are in the support of the corresponding
column      but are not in the predicted reduced vector    
is less than    The decoder returns the exact label vector even if up to    binary classi ers make errors  Algorithm   summarizes our prediction algorithm 

Algorithm   MLGT  Prediction Algorithm

Input  Test data     Rp  the GT matrix     Rm  
which is       disjunct          classi ers  wj  
  
Output  predicted label    
Compute                    wm   
Set       
for                 do

if  supp     supp         then

 yl    

end if
end for

Note that the prediction algorithm is very inexpensive  requires no matrix inversion or solving optimization  It is
equivalent to an AND operation between   binary sparse
matrix and   binary  likely sparse  vector  which should
cost less than   sparse matrix vector product   nnz     
  kd  where nnz    is the number of nonzero entries of
   It is an interesting future work to design an even faster
prediction algorithm 

  Constructions
In order to recover     sparse label vector exactly  we know
that the group testing matrix   must be   kdisjunct matrix 
With         disjunct matrix  our algorithm can extract the
sparse label vector exactly even if    binary classi ers
make errors  misclassify  Here  we present the results
that will help us construct speci   GT matrices with the
desired properties 

  Random Constructions
Proposition    Random Construction  An       random
binary     matrix   where each entry is   with probability      
     is        log   disjunct with very high
probability  if          log   

If we tolerate   small   fraction of sparsity label misclassi 
cations          errors in the recovered label vector  which
we call  tolerance group testing  then we can follow the
analysis of Theorem   in  Du   Hwang    to show
that it is suf cient to have         log    number of classi ers  Further  we can derive the following result 
Theorem   Suppose we wish to recover     sparse binary
vector     Rd    random binary     matrix   where
each entry is   with probability        recovers      
proportion of the support of   correctly with high probability  for any       with         log    This matrix will
also detect         errors 

The proofs of the proposition and the theorem can be found
in the supplementary 

  Concatenated code based constructions

Kautz and Singleton  Kautz   Singleton    introduced
  twolevel construction in which   qary         ReedSolomon  RS  code is concatenated with   unitweight binary code  The construction starts with   qary         RS
code of length       and replaces the qary symbols in the
codewords by unit weight binary vectors of length    That
is  the qary symbols are replaced as                  
                            This gives us   binary matrix
with             rows  This matrix belongs to   broad
class of error correcting codes called the constant weight
codes  each codeword column has   constant number of
ones    For this KautzSingleton construction          
Proposition    KautzSingleton construction    KautzSingleton construction with    logk   ary ReedSolomon
 RS  code is              logk   disjunct matrix with
        log 

    

Proof    constant weight code matrix is   disjunct maw   cid  where   is the weight and   is
trix with      cid    
the distance of the code   see  Theorem   in  Du  
Hwang    The distance of the qary RS code is    
     logq    Hence  we get       
logq    So  for   kdisjunct matrix  we choose       logk      code with distance   will have         by using Corollary   in  Du
  Hwang    Thus          logq            logk   
                  log 

    

Other code based constructions are given in supplementary 

  Expander graphs

Expander graphs have popularly been used in many applications  for example  in coding theory  Sipser   Spielman    in compressed sensing  Jafarpour et al   
etc  In an expander graph  every small set of vertices  expands 
the are  sparse  yet very  wellconnected   see

Multilabel classi cation via group testing

formal de nition below  With high probability   random
graph is   good expander  Construction of  lossless  expanders have been notoriously dif cult 
De nition    Unbalanced Lossless Expander Graphs   
     unbalanced bipartite expander graph is   bipartite
graph                          where   is the set of
left nodes and   is the set of right nodes  with regular left
degree  cid  such that for any        if         then the set of
neighbors       of   has the size          cid   

The following proposition describe the expander property
of random graphs 
Proposition     random construction of bipartite graphs
           with         with overwhelming probability 
is      lossless  cid regular expander where  cid      log   
with               cid 

The tradeoff of this proposition is close to the best we
can hope for  The proof can be shown by simple random
choice and can be found in  Vadhan    or in  Cheraghchi   
The next de nition and the subsequent two claims are from
 Cheraghchi    First  let us now connect   lossless
expander with disjunct matrix 
De nition     bipartite graph            is called
      disjunct if  for every left vertex       and every
set       such that         and        we have
                

It can be seen that the bipartite adjacency matrix   of  
disjunct graph   is   disjunct matrix 
Proposition   Let   be         disjunct graph with adjacency matrix    Then for every pair of      cid         of  
 sparse vectors  we have             cid       where  
denotes the Hamming distance between vectors 

The following proposition relates expander graphs with
disjunct graphs 
Proposition   Let   be    cid regular      lossless expander  Then  for every           is      cid disjunct
provided that      

 

 cid 

Combining these comments  we get the following 
Proposition    Random Graphs  The adjacency matrix of   randomly constructed bipartite graph is 
with overwhelming probability  kdisjunct with    
     log      More generally  for every         random graphs are       disjunct  with        log     
  with         log         

There is an explicit construction of unbalanced      
lossless expanders for any setting of   and   and is  to
our knowledge  the best possible  in  Capalbo et al   

These constructions yield explicit kdisjunct graphs with
        quasipoly log    Other random constructions
are discussed in the supplementary 
With all the above constructions  we can correct   reasonably large number of   errors by the binary classi ers  The
number of classi ers required for MLGT will be    
     log    which is more than the CS approach where
        log    However  our analysis is for the worst
case  as we saw in Theorem   if we tolerate   small   fraction of error in recovery  we can achieve         log   
for MLGT as well  Moreover  MLGT yields zero prediction error for     sparse label vector even if up to    classi ers misclassify  With MLCS  we only get an   error
guarantees and with respect to  norm  not Hamming distance which is more natural for classi cation 

  Error Analysis
Here we summarize the theoretical error guarantees for
multilabel classi cation using group testing  MLGT 
Theorem   Consider MLGT with an      binary matrix
   and   label vector   with sparsity at most    Suppose
  is       disjunct  and we use Algorithm   during prediction  Let    be the predicted label vector and   denote the
Hamming distance between vectors  If   number of binary
classi ers that make errors in prediction  then we have
  If      cid   cid  then the prediction error            
  If      cid   cid                     Hamming error 
where   is the maximum weight of rows in    In particular  the error rate  average error per class  will be
         
 

If   is   kdisjunct with   error tolerance  then the prediction error will be at most                

Proof  When         we know that the decoding algorithm will still recover the exact label vector due to the error correcting property of the       disjunct matrix  When 
          of the errors are corrected  For every remaining        errors  if   is the maximum weight of rows in
     maximum of   errors can occur in the predicted label 
This is because  the support different         can change
for   maximum of   columns  Hence  the error can be at
         
most          and the error rate will be  
For the kdisjunct matrix with   error tolerance  the decoding algorithm can make up to    errors in addition to
        

Let us see how the errorrate of various group testing constructions translate to MLGT  In the case of   random matrix construction  we have          So  the error rate for
this matrix will be         From proposition   we can
take        log    and        log    Hence  the error rate

Multilabel classi cation via group testing

 

for   random        disjunct matrix will be         log   
for any        log    For any   less than this the error
rate will be zero  Similarly  we can see that the randomized construction of Thm    with         log    rows 
gives the average error rate is          log            for
      log    The error rates of other constructions can be
calculated in the same way  see supplementary 
The above theorem also shows that   Hamming error regret         error in the method   least error possible 
in the binary classi ers will transform linearly to the overall regret of at most          for the MLGT  For the
CS approach  the results in  Hsu et al    show that
     regret     an   norm error regret  in the regressor
   to the overall regret  which is worse 
will translate as
This is because    CS based analysis involves    norm errors and Restricted Isometric Property  RIP  of the compression matrix with respect to    norm  However     error metric is never used for evaluation in practice 
Hamming Loss  Since we operate in the binary  eld  we
derive error  regret  bounds  as well as present experimental results with respect to Hamming loss 
In certain applications we may be interested in only predicting the top
       labels correctly       tagging and recommendation  In such situations  it has been argued that the Hamming loss is not   perfect measure  Jain et al    and
alternate measures such as   recison    de ned later  for
          have been used  Agrawal et al    However  these measures assume there is   ranking amongst the
labels  which can be obtained only when operating in the
real space  Also  these measures ignore the false labels 
Our approach considers labels as just binary vectors  cannot rank  and attempts to predict all labels correctly  hence
Hamming loss  Almost all available mutlilabel datasets
have binary label vectors and do not come with the priority information of labels within the large output classes 
There are recent works which try to rank the labels  rst and
then classify        Jain et al    Chzhen et al   
Hamming loss is nonetheless an interesting error metric for
applications where we need to predict all labels correctly
and require few no false labels  hence is worth analyzing 
Most of the recent popular  embedding  methods tend to
give good results with respect to   recison    but give
poor Hamming errors due to large number of false labels 
Our approach gives very low Hamming loss both theoretically and practically 

  Numerical Experiments
In this section  we illustrate the performance of the proposed group testing approach in the multilabel classi cation problems  MLGT  via several numerical experiments
on various datasets 

Datasets  We use some popular publicly available multilabel datasets in our experiments  All datasets were obtained from The Extreme Classi cation Repository   Bhatia et al    Details about the datasets and the references for their original sources can be found in the repository  Table   in the supplementary gives data details 
Constructions  For MLGT  we consider three different
group testing constructions  The KautzSingleton construction with qary ReedSolomon  RS  codes  where we use
RS codes  MacWilliams   Sloane    with       and
      and       and       To get desired number
of codewords  equal to number of labels  we use appropriate message length  For example  if            
then we use message length of   and if       we use
message length of   We also use two random GT constructions  namely  the random expander graphs and the sparse
random constructions discussed in sec    For MLCS  compressed sensing approach  we again consider three different types compression matrices  namely  random Gaussian
matrices  compressed Hadamard matrices and random expander graphs  expander graphs have been used for CS
too  Jafarpour et al   
Evaluation metrics  Two evaluation metrics are used to
analyze the performances of the different methods  First
is the Hamming loss error  the Hamming distance between
the predicted vector    and the actual label vector           
This metric tells us how close is the recovered vector    is
from the exact label vector    and is more suitable for binary vectors  Hamming loss captures the information of
both correct predictions and false labels  All prediction errors reported  training and test  are Hamming loss errors 
The second metric used is   recison         which is
  popular metric used in MLC literature  Agrawal et al 
  This measures the precision of predicting the  rst
  coordinates  supp        supp      Since we cannot
score the labels  we use     nnz    the output sparsity of
the true label for this measure  This is equivalent to checking whether the method predicted all the labels the data belongs to correctly or not  ignoring misclassi cation  When
  recision   for           are used  one is checking
whether the top     or   labels are predicted correctly  ignoring other and false labels 
MLGT vs MLCS  In the  rst set of experiments  we
compare the performances of the group testing approach
 MLGT  and the compressed sensing approach  MLCS 
using different group testing constructions and different
compression matrices    least squares binary classi er was
used  wj  
   for MLGT  Least squares regression with  cid 
regularization  ridge regression  is used as the regressors
for MLCS and other embedding based methods  Orthogo 

 https manikvarma github io downloads 

XC XMLRepository html

Multilabel classi cation via group testing

Table   Comparison between MLGT and MLCS  Average training and test errors and   recison   
Test Error

Size   Training error

Train    

Data
RCV   
      kmax  
        nt  
       

EURLex   
          
        nt  
       

Delicious 
      kmax  
        nt  
       

AmazonCat   
          
        nt  
       

Wiki             
     nt          

Method

GT  RS code   

GT  expander
GT  sparse rand
CS  Gaussian
CS  Hadamard
CS  Expander

GT  RS code   

GT  expander
GT  sparse rand
CS  Gaussian
CS  Hadamard
CS  Expander
GT  expander
GT  sparse rand
CS  Gaussian
CS  Hadamard
CS  Expander

GT  expander
GT  sparse rand
CS  Gaussian
CS  Hadamard
CS  Expander
GT  expander
CS  Gaussian

GT  RS code   

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Test    

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Table   MLGT     OvsA

Dataset
Medmill
Bibtex

 
 
 

Err
 
 

MLGT
   
 
 

time
  
  

Err
 
 

OvsA
   
 
 

time
  
  

nal Matching Pursuit  OMP   Tropp   Gilbert    was
used for sparse recovery in MLCS  Additional details are
given in supplementary 
Table   compares the performances of MLCS and MLGT
for different CS and GT matrices on different datasets  The
average training and the test errors  Hamming losses  are
reported along with the average   recison   obtained for
training and test data  The methods and the number of
classi ers regressors   used are also listed  For example 
GT RS code    implies MLGT was the method with the
RS code construction with    The number of training
points    test points nt and the number of features   used
in the experiments are reported next to the datasets  kmax
means the maximum sparsity in the data  only those data
points below this sparsity were used  anf bark is the average sparsity  For the latter three datasets  the feature space
was reduced to select only dominant features  data are very
sparse and only few features are prominent 
We observe that  the MLGT method with all the three GT
constructions outperforms the MLCS method 
In most
cases  the training errors  almost zero  and   recison  
 almost one  for MLGT methods are extremely good  This
is because  the binary classi ers are optimally trained on
the reduced binary vectors and since the matrices used

were kdisjunct  we had zero recovery error in most cases 
Hence  the predicted labels for training data were extremely
accurate  The results on test data are also better for MLGT
in almost all cases  The results we obtained for the dataset
Delicious were consistently poor  see supplementary  We
also observed that MLGT is signi cantly faster than MLCS
 as expected  because  MLCS uses an optimization algorithm  OMP  for recovery of labels  see Table   for runtimes 
In the prediction algorithm of MLGT  we have   parameter
   the number of errors the algorithm should try to correct 
The ideal value for   will depend on the GT matrix used 
the values of      and    However  note that we can test for
different values of   at no additional cost  That is  once we
compute the Boolean AND between the predicted reduced
vector and the GT matrix  the dominant operation  we can
get different prediction vectors for   range of   and choose
an   that gives the highest training     
Figure   plots the average training and test errors and average   recison   against the sparsity   of the label vectors
 data with label sparsity   used  obtained for MLGT and
MLCS methods with the three different matrices respectively  seen in Table   The dataset used was RCV   
This dataset has at least   training points and   testing points for each label sparsity ranging from   to   We
observe that the training error for MLGT methods are almost zero and training   recison   almost one   This behavior was seen in Table   as well  Results with test data

Multilabel classi cation via group testing

Figure   Average training and test errors and   recison   versus sparsity   for RCV   for different MLGT and MLCS methods 

Table   Comparisons with embedding methods  Average Test errors 

Dataset
Mediamill             
Bibtex              
Delicious              
RCV               
EurLex             
AmznC             
Wiki             

 
 
 
 
 
 
 
 

MLGT

MLCS

Err
 
 
 
 
 
 
 

time
  
  
  
  
  
 min
  

Err
 
 
 
 
 
 
 

time
  
  
  
  
  
 min
 min

MLCSSP
time
Err
  
 
 
  
  
 
  
 
  
 
 min
 
 
 min

PLST

SLEEC

Err
 
 
 
 
 
 
 

time
  
  
  
  
  
 min
 min

Err
 
 
 
 
 
 
 

time
  
  
  
  
  
 hrs
 min

for MLGT are also impressive  achieving   recison   of
almost   for small   
One vs all  We next compare MLGT against the one versus
all  OvsA  method on two small datasets  Note that OvsA
required   classi ers to be trained  hence is impractical for
larger datasets  and we will need   distributed implementation such as DiSMEC  Babbar   Sch olkopf    Table   gives the results for MLGT and OvsA methods for
two small datasets       nt     and for MLGT
      The table lists the Hamming test errors and    
for the two methods  The table also gives the overall runtimes for the two methods  We note that wrt  to both metrics  MLGT performs better than OvsA  This is due to two
reasons  First  MLGT groups the labels hence has more
training samples per group  yielding better classi ers  Second  the error correction by the prediction algorithm corrects few classi cation errors  Clearly  MLGT is faster than
OvsA  However  OvsA gives better training errors 
Embedding methods  In the next set of experiments  we
compare the performance of MLGT against the popular
embedding based methods  We compare with the following methods  MLCSSP  is an embedding method based
on column subset selection  Bi   Kwok    PLST  is
Principal Label Space Transformation  Tai   Lin   
an embedding method based on SVD  code is made available online by the authors  SLEEC  Sparse Local Embeddings for Extreme Classi cation  Bhatia et al    is
the state of the art embedding method based on clustering
using nearest neighbors and then embedding in the cluster
space  code is made available online by the authors  For
MLGT  we use the random expander graph constructions 
For MLCS  we use random Gaussian matrices  Same least
squares regressor was used in all the latter four methods 

Table   lists the test  Hamming  errors obtained for the different methods on various datasets  We use smaller datasets
since the embedding based methods do not scale well for
large datasets  We also used only   training points and
  test points in each cases  We observe that MLGT outperforms the other methods in most cases  The datasets
have very sparse label  avg  sparsity of around       
but the outputs of MLCSSP and PLST are not very sparse 
Hence  we see high Hamming error for these two methods  since they yield   lot of false labels  Moreover  these
embedding methods are signi cantly more expensive than
MLGT for larger datasets  The runtimes for each method
are also listed in the table 
The runtimes reported  using cputime in Matlab  includes
generation of compression matrices  multiplying the matrix to the label vectors  boolean OR SVD computation 
training the   classi ers  and prediction of   training and
nt test points  SLEEC performs reasonably well on all
datasets  the ideal parameters to be set in this algorithm for
each of these datasets were provided by the authors online 
and gives better     than MLGT for some datasets  For
Delicious dataset  the value of   is high and SLEEC beats
MLGT  However  SLEEC algorithm has many parameters
to set  and for larger datasets  the algorithm is very expensive compared to all other methods 
These experiments illustrate that MLGT performs exceptionally well in practice  The concatenated RS codes and
the bipartite expander graphs constructions proposed are
simple to generate and they exist for large sizes  Hence 
these constructions can be easy applied to extreme classi 
cation problems 

 sparsitykPredictionerrorAveragetrainingerrorsforRCV KGT RS codeGT expanderGT sprandCS GaussianCS HadamardCS expander sparsitykPrecision kAveragetrainingPrecision kforRCV KGT RS codeGT expanderGT sprandCS GaussianCS HadamardCS expander sparsitykPredictionerrorAveragetesterrorsforRCV KGT RS codeGT expanderGT sprandCS GaussianCS HadamardCS expander sparsitykPrecision kAveragetestPrecision kforRCV KGT RS codeGT expanderGT sprandCS GaussianCS HadamardCS expanderMultilabel classi cation via group testing

Acknowledgements
Authors would like to thank Dr  Manik Varma and his team
for making many MLC datasets and codes available online 
This work was supported by NSF under grant NSF CCF 
  NSF CCF  NSF CCF  

References
Agrawal  Rahul  Gupta  Archit  Prabhu  Yashoteja  and
Varma  Manik  Multilabel learning with millions of
labels  Recommending advertiser bid phrases for web
pages  In Proceedings of the  nd international conference on World Wide Web  pp    ACM   

Babbar  Rohit and Sch olkopf  Bernhard  Dismec  Distributed sparse machines for extreme multilabel classi 
 cation  In Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining  pp   
  ACM   

Barutcuoglu  Zafer  Schapire  Robert    and Troyanskaya 
Olga    Hierarchical multilabel prediction of gene
function  Bioinformatics     

Bhatia  Kush  Jain  Himanshu  Kar  Purushottam  Varma 
Manik  and Jain  Prateek  Sparse local embeddings for
extreme multilabel classi cation  In Advances in Neural
Information Processing Systems  pp     

Bi  Wei and Kwok  James Tin Yau  Ef cient multilabel
In  th International
classi cation with many labels 
Conference on Machine Learning  ICML   pp   
   

Capalbo  Michael  Reingold  Omer  Vadhan  Salil  and
Wigderson  Avi  Randomness conductors and constantdegree lossless expanders  In Proceedings of the thiryfourth annual ACM symposium on Theory of computing 
pp    ACM   

Chen  YaoNan and Lin  HsuanTien  Featureaware label
space dimension reduction for multilabel classi cation 
In Advances in Neural Information Processing Systems 
pp     

Cheraghchi  Mahdi  Derandomization and group testing 
In Communication  Control  and Computing  Allerton 
   th Annual Allerton Conference on  pp   
IEEE   

Chzhen  Evgenii  Denis  Christophe  Hebiri  Mohamed 
On the bene ts of output
arXiv preprint

and Salmon  Joseph 
sparsity for multilabel classi cation 
arXiv   

Cisse  Moustapha    Usunier  Nicolas  Artieres  Thierry 
and Gallinari  Patrick  Robust bloom  lters for large

In Advances in Neural
multilabel classi cation tasks 
Information Processing Systems  pp     

Dietterich  Thomas    and Bakiri  Ghulum 

Solving
multiclass learning problems via errorcorrecting output
codes  Journal of arti cial intelligence research   
   

Dorfman  Robert  The detection of defective members of
large populations  The Annals of Mathematical Statistics     

Du        and Hwang       Combinatorial group testing
and its applications  World Scienti     nd edition   

Dyachkov  Arkadii    Macula  Anthony    and Rykov 
Vyacheslav    New applications and results of superimposed code theory arising from the potentialities of
molecular biology  In Numbers  Information and Complexity  pp    Springer   

Hsu  Daniel  Kakade  Sham    Langford  John  and Zhang 
Tong  Multilabel prediction via compressed sensing 
NIPS     

Jafarpour  Sina  Xu  Weiyu  Hassibi  Babak  and Calderbank  Robert  Ef cient and robust compressed sensing
using optimized expander graphs  IEEE Transactions on
Information Theory     

Jain  Himanshu  Prabhu  Yashoteja  and Varma  Manik 
Extreme multilabel loss functions for recommendation 
tagging  ranking   other missing label applications  In
Proceedings of the  nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining 
pp    ACM   

Kapoor  Ashish  Viswanathan  Raajay  and Jain  Prateek  Multilabel classi cation using bayesian compressed sensing  In Advances in Neural Information Processing Systems  pp     

Kautz    and Singleton  Roy  Nonrandom binary superimposed codes  IEEE Transactions on Information Theory 
   

MacWilliams 

Florence

James Alexander 
codes  Elsevier   

Jessie

and Sloane  Neil
The theory of errorcorrecting

Mazumdar     Nonadaptive group testing with random set
of defectives  IEEE Transactions on Information Theory 
  Dec  
ISSN   doi 
 TIT 

Mazumdar  Arya and Mohajer  Soheil  Group testing with
In Communication  Control  and
unreliable elements 
Computing  Allerton     nd Annual Allerton Conference on  pp    IEEE   

Multilabel classi cation via group testing

Sipser  Michael and Spielman  Daniel    Expander codes 
IEEE Transactions on Information Theory   
   

Tai  Farbound and Lin  HsuanTien  Multilabel classi cation with principal label space transformation  Neural
Computation     

Trohidis  Konstantinos  Multilabel classi cation of music into emotions  In  th International Conference on
Music Information Retrieval  pp       

Tropp  Joel   and Gilbert  Anna    Signal recovery from
random measurements via orthogonal matching pursuit 
IEEE Transactions on information theory   
   

Tsfasman  Michael    Vl adu  Serge    and Nogin  Dmitry 
Algebraic geometric codes  basic notions  Number  
American Mathematical Soc   

Tsoumakas  Grigorios  Katakis  Ioannis  and Vlahavas 
Effective and ef cient multilabel classi 
Ioannis 
In
cation in domains with large number of labels 
ECML PKDD   Workshop on Mining Multidimensional Data  MMD   

Ubaru  Shashanka  Mazumdar  Arya  and Barg  Alexander  Group testing schemes from lowweight codeIn Information Theory  ISIT 
words of BCH codes 
  IEEE International Symposium on  pp   
IEEE   

Vadhan  Salil    Pseudorandomness  Foundations and
Trends in Theoretical Computer Science   
 

Wang  Changhu  Yan  Shuicheng  Zhang  Lei  and Zhang 
HongJiang  Multilabel sparse coding for automatic image annotation  In Computer Vision and Pattern Recognition    CVPR   IEEE Conference on  pp 
  IEEE   

Xu  Chang  Tao  Dacheng  and Xu  Chao  Robust extreme
multilabel learning  In Proceedings of the  nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  pp    ACM   

Yu  Hsiangfu  Jain  Prateek  Kar  Purushottam  and
Dhillon  Inderjit  Largescale multilabel learning with
missing labels  In Proceedings of the  st International
Conference on Machine Learning  ICML  pp   
   

Zhang  Yi and Schneider  Jeff    Multilabel output codes
In AISTATS  pp 

using canonical correlation analysis 
   

