Cognitive Psychology for Deep Neural Networks 

  Shape Bias Case Study

Samuel Ritter     David      Barrett     Adam Santoro   Matt    Botvinick  

Abstract

Deep neural networks  DNNs  have advanced
performance on   wide range of complex tasks 
rapidly outpacing our understanding of the nature of their solutions  While past work sought
to advance our understanding of these models 
none has made use of the rich history of problem
descriptions  theories  and experimental methods
developed by cognitive psychologists to study
the human mind  To explore the potential value
of these tools  we chose   wellestablished analysis from developmental psychology that explains
how children learn word labels for objects  and
applied that analysis to DNNs  Using datasets
of stimuli inspired by the original cognitive psychology experiments  we  nd that stateof theart
one shot learning models trained on ImageNet
exhibit   similar bias to that observed in humans  they prefer to categorize objects according to shape rather than color  The magnitude
of this shape bias varies greatly among architecturally identical  but differently seeded models  and even  uctuates within seeds throughout training  despite nearly equivalent classi 
cation performance  These results demonstrate
the capability of tools from cognitive psychology
for exposing hidden computational properties of
DNNs  while concurrently providing us with  
computational model for human word learning 

  Introduction
During the last halfdecade deep learning has signi cantly
improved performance on   variety of tasks  for   review 
see LeCun et al    However  deep neural network
 DNN  solutions remain poorly understood  leaving many

 Equal contribution  DeepMind  London  UK  Correspondence to  Samuel Ritter  ritters google com  David      Barrett  barrett google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

to think of these models as black boxes  and to question
whether they can be understood at all  Bornstein   
Lipton    This opacity obstructs both basic research
seeking to improve these models  and applications of these
models to real world problems  Caruana et al   
Recent pushes have aimed to better understand DNNs 
tailormade loss functions and architectures produce more
interpretable features  Higgins et al    Raposo et al 
  while outputbehavior analyses unveil previously
opaque operations of these networks
 Karpathy et al 
  Parallel to this work  neuroscienceinspired methods such as activation visualization  Li et al    ablation analysis  Zeiler   Fergus    and activation maximization  Yosinski et al    have also been applied 
Altogether  this line of research developed   set of promising tools for understanding DNNs  each paper producing
  glimmer of insight  Here  we propose another tool for
the kit  leveraging methods inspired not by neuroscience 
but instead by psychology  Cognitive psychologists have
long wrestled with the problem of understanding another
opaque intelligent system  the human mind  We contend
that the search for   better understanding of DNNs may
pro   from the rich heritage of problem descriptions  theories  and experimental tools developed in cognitive psychology  To test this belief  we performed   proofof 
concept study on stateof theart DNNs that solve   particularly challenging task  oneshot word learning  Specifically  we investigate Matching Networks  MNs   Vinyals
et al    which have stateof theart oneshot learning
performance on ImageNet and we investigate an Inception
Baseline model  Szegedy et al     
Following the approach used in cognitive psychology  we
began by hypothesizing an inductive bias our model may
use to solve   word learning task  Research in developmental psychology shows that when learning new words 
humans tend to assign the same name to similarly shaped
items rather than to items with similar color  texture  or
size  To test the hypothesis that our DNNs discover this
same  shape bias  we probed our models using datasets
and an experimental setup based on the original shape bias
studies  Landau et al   

Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

Our results are as follows    Inception networks trained on
ImageNet do indeed display   strong shape bias    There
is high variance in the bias between Inception networks
initialized with different random seeds  demonstrating that
otherwise identical networks converge to qualitatively different solutions    MNs also have   strong shape bias  and
this bias closely mimics the bias of the Inception model
that provides input to the MN    By emulating the shape
bias observed in children  these models provide   candidate
computational account for human oneshot word learning 
Altogether  these results show that the technique of testing
hypothesized biases using probe datasets can yield both expected and surprising insights about solutions discovered
by trained DNNs 

  Inductive Biases  Statistical Learners and

Probe Datasets

disparity between   and              cid 

Before we delve into the speci cs of the shape bias and
oneshot word learning  we will describe our approach in
the general context of inductive biases  probe datasets  and
statistical learning  Suppose we have some data  yi  xi  
  
where yi      xi  Our goal is to build   model of the
data    to optimize some loss function   measuring the
   yi     xi 
Perhaps this data   is images of ImageNet objects to be
classi ed  images and histology of tumors to be classi ed
as benign or malignant  Kourou et al    or medical
history and vital measurements to be classi ed according
to likely pneumonia outcomes  Caruana et al   
  statistical learner such as   DNN will minimize   by
discovering properties of the input   that are predictive of
the labels    These discovered predictive properties are  in
effect  the properties of   for which the trained model has
an inductive bias  Examples of such properties include the
shape of ImageNet objects  the number of nodes of   tumor 
or   particular constellation of blood test values that often
precedes an exacerbation of pneumonia symptoms 
Critically  in realworld datasets such as these  the discovered properties are unlikely to correspond to   single feature of the input    instead they correspond to complex
conjunctions of those features  We could describe one of
these properties using   function      which  for example 
returns the shape of the focal object given an ImageNet im 

 The use of behavioral probes to understand neural network
function has been extensively applied within psychology itself 
where neural networks have been employed as models of human
brain function  Rumelhart et al    Plaut et al    Rogers
  McClelland    Mareschal et al  To our knowledge  work
applying behavioral probes to DNNs in machine learning has been
quite limited  we only are aware of Zoran et al    and Goodfellow et al    who used psychophysicslike experiments to
better understand image processing models 

age  or the number of nodes given   scan of tumor  Indeed 
one way to articulate the dif culty in understanding DNNs
is to say that we often can   intuitively describe these conjunctions of features      although we often have numerical representations in intermediate DNN layers  they re often too arcane for us to interpret 
We advocate for addressing this problem using the following hypothesisdriven approach  First  propose   property
hp    that the model may be using  Critically  it   not necessary that hp    be   function that can be evaluated using
an automated method  Instead  the intention is that hp   
is   function that humans       ML researchers and practitioners  can intuitively evaluate  hp    should be   property that is believed to be relevant to the problem  such as
object shape or number of tumor nodes 
After proposing   property  the next step is to generate predictions about how the model should behave when given
various inputs  if in fact it uses   bias with respect to the
property hp    Then  construct and carry out an experiment wherein those predictions are tested  In order to execute such an experiment  it typically will be necessary to
craft   set of probe examples   that cover   relevant portion of the range of hp    for example   variety of object
shapes  The results of this experiment will either support
or fail to support the hypothesis that the model uses hp   
to solve the task  This process can be especially valuable in
situations where there is little or no training data available
in important regions of the input space  and   practitioner
needs to know how the trained model will behave in that
region 
Psychologists have developed   repertoire of such hypotheses and experiments in their effort to understand the human mind  Here we explore the application of one of these
theoryexperiment pairs to state of the art oneshot learning
models  We will begin by describing the historical backdrop for the human oneshot word learning experiments
that we will then apply to our DNNs 

  The problem of word learning  the solution

of inductive biases

Discussions of oneshot word learning in the psychological literature inevitably begin with the philosopher       
Quine  who broke this problem down and described one
of its most computationally challenging components  there
are an enormous number of tenable hypotheses that  
learner can use to explain   single observed example  To
make this point  Quine penned his nowfamous parable of
the  eld linguist who has gone to visit   culture whose language is entirely different from our own  Quine   
The linguist is trying to learn some words from   helpful
native  when   rabbit runs past  The native declares  gava 

Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

gai  and the linguist is left to infer the meaning of this new
word  Quine points out that the linguist is faced with an
abundance of possible inferences  including that  gavagai 
refers to rabbits  animals  white things  that speci   rabbit 
or  undetached parts of rabbits  Quine argues that indeed
there is an in nity of possible inferences to be made  and
uses this conclusion to bolster the assertion that meaning
itself cannot be de ned in terms of internal mental events 
Contrary to Quine   intentions  when this example was introduced to the developmental psychology community by
Macnamara   it spurred them not to give up on the
idea of internal meaning  but instead to posit and test for
cognitive biases that enable children to eliminate broad
swaths of the hypothesis space  Bloom      variety of
hypothesiseliminating biases were then proposed including the whole object bias  by which children assume that
  word refers to an entire object and not its components
 Markman    the taxonomic bias  by which children
assume   word refers to the basic level category an object
belongs to  Markman   Hutchinson    the mutual exclusivity bias  by which children assume that   word only
refers to one object category  Markman   Wachtel   
the shape bias  with which we are concerned here  Landau
et al    and   variety of others  Bloom    These
biases were tested empirically in experiments wherein children or adults were given an object  or picture of an object  along with   novel name  then were asked whether the
name should apply to various other objects 
Taken as   whole  this work yielded   computational level
 Marr    account of word learning whereby people
make use of biases to eliminate unlikely hypotheses when
inferring the meaning of new words  Other contrasting
and complementary approaches to explaining word learning exist in the psychological literature  including association learning  Regier    Colunga   Smith    and
Bayesian inference  Xu   Tenenbaum    We leave
the application of these theories to deep learning models
to future work  and focus on determining what insight can
be gained by applying   hypothesis elimination theory and
methodology 
We begin the present work with the knowledge that part
of the hypothesis elimination theory is correct  the models
surely use some kind of inductive biases since they are statistical learning machines that successfully model the mapping between images and object labels  However  several
questions remain open  What predictive properties did our
DNNs  nd  Do all of them  nd the same properties  Are
any of those properties interpretable to humans  Are they
the same properties that children use  How do these biases
change over the course of training 
To address these questions  we carry out experiments analogous to those of Landau et al    This enables us to

test whether the shape bias     human interpretable feature
used by children when learning language   is visible in the
behavior of MNs and Inception networks  Furthermore we
are able to test whether these two models  as well as different instances of each of them  display the same bias  In the
next section we will describe in detail the oneshot word
learning problem  and the MNs and Inception networks we
use to solve it 

  Oneshot word learning models and

training

  Oneshot word learning task

The oneshot word learning task is to label   novel data example            novel probe image  with   novel class label
           new word  after only   single example  More
speci cally  given   support set      xi  yi            
of images xi and their associated labels yi  and an unlabelled probe image     the oneshot learning task is to identify the true label of the probe image    from the support set
labels  yi           

     arg max

 

          

 

We assume that the image labels yi are represented using  
onehot encoding and that            is parameterised by  
DNN  allowing us to leverage the ability of deep networks
to learn powerful representations 

  Inception  baseline oneshot learning model

In our simplest baseline oneshot architecture    probe image    is given the label of the nearest neighbour from the
support set 

      

         arg min

 xi yi  

    xi      

 

where   is   distance function  The function   is parameterised by Inception   one of the best performing ImageNet
classi cation models  Szegedy et al      Speci cally 
  returns features from the last layer  the softmax input  of
  pretrained Inception classi er  where the Inception classi er is trained using rmsprop  as described in Szegedy
et al      section   With these features as input and
cosine distance as the distance function  the classi er in
equation   achieves   accuracy on oneshot classi cation on the ImageNet dataset  Vinyals et al    Henceforth  we call the Inception classi er together with the
nearestneighbor component the Inception Baseline  IB 
model 

 Unlike Quine  we use   pragmatic de nition of meaning    
human or model understands the meaning of   word if they assign
that word to new instances of objects in the correct category 

Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

 cid 

  Matching Nets model architecture and training

We also investigate   stateof theart oneshot learning
architecture called Matching Nets  MN   Vinyals et al 
  MNs are   fully differentiable neural network architecture with stateof theart one shot learning performance
on ImageNet   oneshot labelling accuracy 
MNs are trained to assign label    to probe image    according to equation   using an attention mechanism   acting on
image embeddings stored in the support set   

     xi   

ed          xi   
  ed          xj    

 

 

where   is   cosine distance and where   and   provide
contextdependent embeddings of    and xi  with context
   The embedding   xi     is   bidirectional LSTM
 Hochreiter   Schmidhuber    with the support set  
provided as an input sequence  The embedding          is
an LSTM with   readattention mechanism operating over
the entire embedded support set  The input to the LSTM
is given by the penultimate layer features of   pretrained
deep convolutional network  speci cally Inception  as in
our baseline IB model described above  Szegedy et al 
   
The training procedure for the oneshot learning task is critical if we want MNs to classify   probe image    after viewing only   single example of this new image class in its
support set  Hochreiter et al    Santoro et al   
To train MNs we proceed as follows    At each step of
training  the model is given   small support set of images
and associated labels 
In addition to the support set  the
model is fed an unlabelled probe image       The model
parameters are then updated to improve classi cation accuracy of the probe image    given the support set  Parameters are updated using stochastic gradient descent with
  learning rate of     After each update  the labels
 yi            in the training set are randomly reassigned
to new image classes  the label indices are randomly permuted  but the image labels are not changed  This is  
critical step  It prevents MNs from learning   consistent
mapping between   category and   label  Usually  in classi cation  this is what we want  but in oneshot learning we
want to train our model for classi cation after viewing  
single inclass example from the support set  Formally  our
objective function is 

 ES      

   cid 

      

    EC  

log           

   

where   is the set of all possible labelings of our classes   
is   support set sampled with   class labelling       and
  is   batch of probe images and labels  also with the same
randomly chosen class labelling as the support set 

Next we will describe the probe datasets we used to test for
the shape bias in the IB and MNs after ImageNet training 

  Data for bias discovery
  Cognitive Psychology Probe Data

The Cognitive Psychology Probe Data  CogPsyc data  that
we use consists of   images of objects  Figure   The
images are arranged in triples consisting of   probe image    shapematch image  that matches the probe in colour
but not shape  and   colormatch image  that matches the
probe in shape but not colour  In the dataset there are  
triples  each shown on   different backgrounds  giving  
total of   triples 
The images were generously provided by cognitive psychologist Linda Smith  The images are photographs of
stimuli used previously in shape bias experiments conducted in the Cognitive Development Lab at Indiana University  The potentially confounding variables of background content and object size are controlled in this dataset 

  Probe Data from the wild

We have also assembled   realworld dataset consisting of
  images of objects   triples  collected using Google
Image Search  Again  the images are arranged in triples
consisting of   probe    shapematch and   colourmatch 
For the probe image  we chose images of real objects that
are unlikely to appear in standard image datasets such as
ImageNet  In this way  our data contains the irregularity
of the real world while also probing our models  properties
outside of the image space covered in our training data  For
the shapematch image  we chose an object with   similar
shape  but with   very different colour  and for the colourmatch image  we chose an object with   similar colour  but
with   very different shape  For example  one triple consists of   silver tuning fork as the probe    silver guitar capo
as the colour match  and   black tuning fork as the shape
match  Each photo in the dataset contains   single object
on   white background 
We collected this data to strengthen our con dence in the
results obtained for the CogPsych dataset and to demonstrate the ease with which such probe datasets can be constructed  One of the authors crafted this dataset solely using Google Image Search in the span of roughly two days 
work  Our results with this dataset  especially the fact that
the bias pattern over time matches the results from the well
established CogPsych dataset  support the contention that
DNN practitioners can collect effective probe datasets with
minimal time expenditure using readily available tools 

  The CogPsyc dataset

is available at http www 

indiana edu cogdev SB testsets html

Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

colour match

shape match

probe

Figure   Example images
from the Cognitive Psychology
Dataset  see section   The data consists of image triples  rows 
each containing   colour match image  left column    shape
match image  middle column  and   probe image  right column 
We use these triples to calculate the shape bias by reporting the
proportion of times that   model assigns the shape match image
class to the probe image  This dataset was supplied by cognitive
psychologist Linda Smith  and was designed to control for object
size and background 
  Results
  Shape bias in the Inception Baseline Model

First  we measured the shape bias in IB  we used   pretrained Inception classi er  with   top  accuracy  to
provide features for our nearestneighbour oneshot classi er  and probed the model using the CogPsyc dataset 
Speci cally  for   given probe image     we loaded the
shapematch image xs and corresponding label ys  along
with the colourmatch image xc and corresponding label
yc into memory  as the support set      xs  ys   xc  yc 
We then calculated    using Equation   Our model assigned
either yc or ys to the probe image  To estimate the shape
bias Bs  we calculated the proportion of shape labels assigned to the probe 

Bs         ys 

 

where   is an expectation across probe images and   is the
Dirac delta function 
We ran all IB experiments using both Euclidean and cosine
distance as the distance function  We found that the results
for the two distance functions were qualitatively similar  so
we only report results for Euclidean distance 
We found the shape bias of IB to be Bs     Similarly  the shape bias of IB using our realworld dataset was
Bs     Together  these results strongly suggest that IB

trained on ImageNet has   stronger bias towards shape than
colour 
Note that  as expected  the shape bias of this model is qualitatively similar across datasets while being quantitatively
different   largely because the datasets themselves are quite
different 
Indeed  the datasets were chosen to be quite
different so that we could explore   broad space of possibilities  In particular  our CogPsyc dataset backgrounds
have much larger variability than our realworld dataset
backgrounds  and our realworld dataset objects have much
greater variability than the CogPsyc dataset objects 

  Shape bias in the Matching Nets Model

Next  we probed the MNs using   similar procedure  We
used the IB trained in the previous section to provide the
input features for the MN as described in section  
Then  following the training procedure outlined in section
  we trained MNs for oneshot word learning on ImageNet  achieving stateof theart performance  as reported
in  Vinyals et al    Then  repeating the analysis
above  we found that MNs have   shape of bias Bs    
using our CogPsyc dataset and   bias of Bs     using the
realworld dataset  It is interesting to note that these bias
values are very similar to the IB bias values 

  Shape bias statistics  within models and across

models

The observation of   shape bias immediately raises some
important questions  In particular    Does this bias depend on the initial values of the parameters in our model 
  Does the size of the shape bias depend on model performance    When does shape bias emerge during training
  before model convergence or afterwards    How does
shape bias compare between models  and within models 
To answer these questions  we extended the shape bias
analysis described above to calculate the shape bias in  
population of IB models and in   population of MN models
with different random initialization  Figs    and  
  We  rst calculated the dependence of shape bias on the
initialization of IB  Fig    Surprisingly  we observed  
strong variability  depending on the initialization  For the
CogPsyc dataset  the average shape bias was Bs    
with standard deviation  Bs     at the end of training
and for the realworld dataset the average shape bias was
Bs     with  Bs    
  Next  we calculated the dependence of shape bias on
model performance  For the CogPsych dataset  the correlation between bias and classi cation accuracy was    
  with tn      pone tail     and for the
realworld dataset  the correlation was       with
tn      pone tail     Therefore   uctuations

Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

Figure   Shape bias across models with different initialization seeds  and within models during training calculated using the CogPsyc
dataset      The shape bias Bs of   Inception models is calculated throughout training  yellow lines    strong shape bias emerges
across all models    bias value Bs     indicates   shape bias and Bs     indicates   colour bias  Two examples are highlighted here
 blue and red lines  for clarity      The shape bias  uctuates strongly within models during training by up to three standard deviations 
    The distribution of bias values  calculated at the start  blue  middle  red  and end  yellow  of training  Bias variability is high at the
start and end of training  Here  these distributions are calculated using kernel density estimates from all shape bias measurements from
all models within the indicated window 

Figure   Classi cation accuracy of all   Inception models evaluated on   test set during training on ImageNet  same models as in
Figure     All   Inception network seeds achieve near identical
test accuracy  overlapping yellow lines 

in the bias cannot be accounted for by  uctuations in classi cation accuracy  This is not surprising  because the classi cation accuracy of all models was similar at the end of
training  while the shape bias was variable  This demonstrates that models can have variable behaviour along important dimensions       bias  while having the same performance measured by another       accuracy 
  Next we explored the emergence of the shape bias during training  Fig        Fig        At the start of training  the average shape bias of these models was Bs  
  with standard deviation  Bs     on the CogPsyc dataset and Bs     with  Bs     on the
realworld dataset  We observe that   shape bias began
to emerge very early during training  long before convergence 
  Finally  we compare shape bias within models during
training  and between models at the end of training  During training  the shape bias within IB  uctuates signi 

Figure   Scatter plot showing Matching Network  MN  bias as
  function of Inception bias  Each MN receives input through
an Inception model  Each point in this scatter plot is the bias
of   MN and the bias of the Inception model providing input to
that particular MN  In total  the bias values of   MN models are
plotted  some dots are overlapping 

cantly  Fig       Fig      In contrast  the shape bias does
not  uctuate during training of the MN  Instead  the MN
model inherits its shape bias characteristics at the start of
training from the IB that provides it with input embeddings
 Fig    and this shapebias remains constant throughout
training  Moreover  there is no evidence that the MN and
corresponding IB bias values are different from each other
 paired ttest        Note that we do not  netune the
Inception model providing input while training the MN  We
do this so that we can observe the shapebias properties of
the MN independent of the IB model properties 

Training Epochs     Converged Performance  Top Training Epochs Training Epochs Training Epochs       zscore Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

Figure   Shape bias across models with different initialization seeds  and within models during training calculated using the realworld
dataset      The shape bias Bs of   Inception models is calculated throughout training  yellow lines    strong shape bias emerges
across all models  Two examples are highlighted here  blue and red lines  for clarity      The shape bias  uctuates strongly within
models during training      The distribution of bias values  calculated at the start  blue  middle  red  and end  yellow  of training  Bias
variability is high at the start and end of training 

  Discussion
    shape bias case study

Our psychologyinspired approach to understanding DNNs
produced   number of insights  Firstly  we found that both
IB and MNs trained on ImageNet display   strong shape
bias  This is an important result for practitioners who routinely use these models   especially for applications where
it is known   priori that colour is more important than
shape  As an illustrative example  if   practitioner planned
to build   oneshot fruit classi cation system  they should
proceed with caution if they plan to use pretrained ImageNet models like Inception and MNs because fruit are often de ned according to colour features rather than shape 
In applications where   shape bias is desirable  as is more
often the case than not  this result provides reassurance
that the models are behaving sensibly in the presence of
ambiguity 
The second surprising  nding was the large variability in
shape bias  both within models during training and across
models  depending on the randomly chosen initialisation
of our model  This variability can arise because our models are not being explicitly optimised for shape biased categorisation  This is an important result because it shows that
not all models are created equally   some models will have
  stronger preference for shape than others  even though
they are architecturally identical and have almost identical
classi cation accuracy 
Our third  nding   that MNs retain the shape bias statistics of the downstream Inception network   demonstrates
the possibility for biases to propagate across model components  In this case  the shape bias propagates from the
Inception model through to the MN memory modules  This
result is yet another cautionary observation  when combin 

ing multiple modules together  we must be aware of contamination by unknown properties across modules  Indeed 
  bias that is benign in one module might only have   detrimental effect when combined later with other modules 
  natural question immediately arises from these results  
how can we remove an unwanted bias or induce   desirable bias  The biases under consideration are properties of
an architecture and dataset synthesized together by an optimization procedure  As such  the observation of   shapebias is partly   result of the statistics of natural imagelabellings as captured in the ImageNet dataset  and partly  
result of the architecture attempting to extract these statistics  Therefore  on discovering an unwanted bias    practitioner can either attempt to change the model architecture
to explicitly prevent the bias from emerging  or  they can attempt to manipulate the training data  If neither of these are
possible   for example  if the appropriate data manipulation
is too expensive  or  if the bias cannot be easily suppressed
in the architecture  it may be possible to do zeroth order
optimization of the models  For example  one may perform
posthoc model selection either using early stopping or by
selecting   suitable model from the set of initial seeds 
An important caveat to note is that behavioral tools often
do not provide insight into the neural mechanisms  In our
case  the DNN mechanism whereby model parameters and
input images interact to give rise to   shape bias have not
been elucidated  nor did we expect this to happen  Indeed 
just as cognitive psychology often does for neuroscience 
our new computational level insights can provide   starting
point for research at the mechanistic level  For example 
in future work it would be interesting to use gradientbased
visualization or neuron ablation techniques to augment the
current results by identifying the mechanisms underlying
the shape bias  The convergence of evidence from such

       Training Epochs Training Epochs Training Epochs Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

introspective methods with the current behavioral method
would create   richer account of these models  solutions to
the oneshot word learning problem 

come increasingly useful for understanding deep reinforcement learning agents as they learn to solve increasingly
complex tasks 

  Conclusion
In this work  we have demonstrated how techniques from
cognitive psychology can be leveraged to help us better understand DNNs  As   case study  we measured the shape
bias in two powerful yet poorly understood DNNs   Inception and MNs  Our analysis revealed previously unknown
properties of these models  More generally  our work leads
the way for future exploration of DNNs using the rich body
of techniques developed in cognitive psychology 

Acknowledgements
We would like to thank Linda Smith and Charlotte Wozniak for providing the Cognitive Psychology probe dataset 
Charles Blundell for reviewing our paper prior to submission  Oriol Vinyals  Daan Wierstra  Peter Dayan  Daniel
Zoran  Ian Osband and Karen Simonyan for helpful discussions  James Besley for legal assistance  and the DeepMind
team for support 

  Modelling human word learning

There have been previous attempts to model human word
learning in the cognitive science literature  Colunga  
Smith    Xu   Tenenbaum    Schilling et al 
  Mayor   Plunkett    However  none of these
models are capable of oneshot word learning on the scale
of realworld images  Because MNs both solve the task
at scale and emulate hallmark experimental  ndings  we
propose MNs as   computationallevel account of human
oneshot word learning  Another feature of our results supports this contention  in our model the shape bias increases
dramatically early in training  Fig      similarly  humans
show the shape bias much more strongly as adults than as
children  and older children show the bias more strongly
than younger children  Landau et al   
As   good cognitive model should  our DNNs make testable
predictions about wordlearning in humans  Speci cally 
the current results predict that the shape bias should vary
across subjects as well as within   subject over the course
of development  They also predict that for humans with
adultlevel oneshot word learning abilities  there should
be no correlation between shape bias magnitude and oneshot word learning capability 
Another promising direction for future cognitive research
would be to probe MNs for additional biases in order to
predict novel computational properties in humans  Probing
  model in this way is much faster than running human behavioural experiments  so   wider range of hypotheses for
human word learning may be rapidly tested 

  Cognitive Psychology for Deep Neural Networks

Through the oneshot learning case study  we demonstrated
the utility of leveraging techniques from cognitive psychology for understanding the computational properties of
DNNs  There is   wide ranging literature in cognitive psychology describing techniques for probing   spectrum of
behaviours in humans  Our work here leads the way to the
study of arti cial cognitive psychology   the application of
these techniques to better understand DNNs 
For example  it would be useful to apply work from the
massive literature on episodic memory  Tulving    to
the recent  urry of episodic memory architectures  Blundell et al    Graves et al    and to apply techniques from the semantic cognition literature  Lamberts  
Shanks    to recent models of concept formation  Higgins et al    Gregor et al    Raposo et al   
More generally  the rich psychological literature will be 

Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

References
Bloom  Paul  How children learn the meanings of words 

MIT press Cambridge  MA   

Blundell  Charles  Uria  Benigno  Pritzel  Alexander  Li 
Yazhe  Ruderman  Avraham  Leibo  Joel    Rae  Jack 
Wierstra  Daan  and Hassabis  Demis  Modelfree
arXiv preprint arXiv 
episodic control 
 

Bornstein  Aaron  Is arti cial intelligence permanently inscrutable  Despite new biologylike tools  some insist
interpretation is impossible  Nautilus   

Caruana  Rich  Lou  Yin  Gehrke  Johannes  Koch  Paul 
Sturm  Marc  and Elhadad  Noemie  Intelligible models
for healthcare  Predicting pneumonia risk and hospital
In Proceedings of the  th ACM
 day readmission 
SIGKDD International Conference on Knowledge Discovery and Data Mining  pp    ACM   

Colunga  Eliana and Smith  Linda    From the lexicon to
expectations about kinds    role for associative learning 
Psychological review     

Goodfellow  Ian  Lee  Honglak  Le  Quoc    Saxe  Andrew 
and Ng  Andrew    Measuring invariances in deep netIn Advances in neural information processing
works 
systems  pp     

Graves  Alex  Wayne  Greg  Reynolds  Malcolm  Harley 
Tim  Danihelka  Ivo  GrabskaBarwi nska  Agnieszka 
Colmenarejo  Sergio   omez  Grefenstette  Edward  Ramalho  Tiago  Agapiou  John  et al  Hybrid computing
using   neural network with dynamic external memory 
Nature     

Gregor  Karol  Besse  Frederic  Rezende  Danilo Jimenez 
Danihelka  Ivo  and Wierstra  Daan  Towards concepIn Advances In Neural Information
tual compression 
Processing Systems  pp     

Higgins  Irina  Matthey  Loic  Glorot  Xavier  Pal  Arka 
Uria  Benigno  Blundell  Charles  Mohamed  Shakir 
and Lerchner  Alexander  Early visual concept learnarXiv preprint
ing with unsupervised deep learning 
arXiv   

Hochreiter  Sepp and Schmidhuber    urgen  Long shortterm memory  Neural computation   
 

Karpathy  Andrej  Johnson  Justin  and FeiFei  Li  Visualizing and understanding recurrent networks  arXiv
preprint arXiv   

Kourou  Konstantina  Exarchos  Themis    Exarchos  Konstantinos    Karamouzis  Michalis    and Fotiadis  Dimitrios    Machine learning applications in cancer prognosis and prediction  Computational and structural
biotechnology journal     

Lamberts  Koen and Shanks  David  Knowledge Concepts

and Categories  Psychology Press   

Landau  Barbara  Smith  Linda    and Jones  Susan    The
importance of shape in early lexical learning  Cognitive
development     

LeCun  Yann  Bengio  Yoshua  and Hinton  Geoffrey  Deep

learning  Nature     

Li  Jiwei  Chen  Xinlei  Hovy  Eduard  and Jurafsky  Dan 
Visualizing and understanding neural models in nlp 
arXiv preprint arXiv   

Lipton  Zachary    The mythos of model interpretability 

arXiv preprint arXiv   

Macnamara  John  Cognitive basis of language learning in

infants  Psychological review     

Mareschal  Denis  French  Robert    and Quinn  Paul     
connectionist account of asymmetric category learning
in early infancy  Developmental psychology 

Markman  Ellen    Constraints children place on word

meanings  Cognitive Science     

Markman  Ellen   and Hutchinson  Jean    Children  
sensitivity to constraints on word meaning  Taxonomic
versus thematic relations  Cognitive psychology   
   

Markman  Ellen   and Wachtel  Gwyn    Children   use of
mutual exclusivity to constrain the meanings of words 
Cognitive psychology     

Marr  David  Vision    computational investigation into
the human representation and processing of visual information  henry holt and co  Inc  New York  NY   
 

Mayor  Julien and Plunkett  Kim    neurocomputational
account of taxonomic responding and fast mapping in
early word learning  Psychological review   
 

Hochreiter  Sepp  Younger    Steven  and Conwell  Peter    Learning to learn using gradient descent  In International Conference on Arti cial Neural Networks  pp 
   

Plaut  David    McClelland  James    Seidenberg  Mark   
and Patterson  Karalyn  Understanding normal and impaired word reading  computational principles in quasiregular domains  Psychological review     

Cognitive Psychology for Deep Neural Networks    Shape Bias Case Study

Zeiler  Matthew   and Fergus  Rob  Visualizing and understanding convolutional networks  In European Conference on Computer Vision  pp     

Zoran  Daniel  Isola  Phillip  Krishnan  Dilip  and Freeman 
William    Learning ordinal relationships for midlevel
vision  In Proceedings of the IEEE International Conference on Computer Vision  pp     

Quine  Willard Van Orman  Word and object  MIT press 

 

Raposo  David  Santoro  Adam  Barrett  David      Pascanu  Razvan  Lillicrap  Timothy  and Battaglia  Peter 
Discovering objects and their relations from entangled
scene representations  arXiv preprint arXiv 
 

Regier  Terry  The human semantic potential  Spatial language and constrained connectionism  MIT Press   

Rogers  Timothy   and McClelland  James    Semantic
cognition    parallel distributed processing approach 
MIT press   

Rumelhart  David    McClelland 

James    Group 
PDP Research  et al  Parallel distributed processing  volume   IEEE   

Santoro  Adam  Bartunov  Sergey  Botvinick  Matthew 
Wierstra  Daan  and Lillicrap  Timothy  Metalearning
with memoryaugmented neural networks  In Proceedings of The  rd International Conference on Machine
Learning  pp     

Schilling  Savannah    Sims  Clare    and Colunga 
Eliana  Taking development seriously  Modeling the interactions in the emergence of different word learning
biases  In CogSci   

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 
In Proceedings of
Going deeper with convolutions 
the IEEE Conference on Computer Vision and Pattern
Recognition  pp       

Szegedy  Christian  Vanhoucke  Vincent  Ioffe  Sergey 
Shlens  Jonathon  and Wojna  Zbigniew  Rethinking
the inception architecture for computer vision  arXiv
preprint arXiv     

Tulving  Endel  Elements of episodic memory   

Vinyals  Oriol  Blundell  Charles  Lillicrap  Timothy 
Kavukcuoglu  Koray  and Wierstra  Daan  MatcharXiv preprint
ing networks for one shot learning 
arXiv   

Xu  Fei and Tenenbaum  Joshua    Word learning as
bayesian inference  Psychological review   
 

Yosinski 

Jason  Clune 

Thomas  and Lipson  Hod 
networks through deep visualization 
arXiv   

Jeff  Nguyen  Anh  Fuchs 
Understanding neural
arXiv preprint

