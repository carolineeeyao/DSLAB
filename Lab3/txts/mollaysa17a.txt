Regularising Nonlinear Models Using Feature Sideinformation

Amina Mollaysa     Pablo Strasser     Alexandros Kalousis  

Abstract

Very often features come with their own vectorial
descriptions which provide detailed information
about their properties  We refer to these vectorial descriptions as feature sideinformation  In
the standard learning scenario  input is represented as   vector of features and the feature sideinformation is most often ignored or used only
for feature selection prior to model  tting  We
believe that feature sideinformation which carries information about features intrinsic property
will help improve model prediction if used in  
proper way during learning process  In this paper  we propose   framework that allows for the
incorporation of the feature sideinformation during the learning of very general model families
to improve the prediction performance  We control the structures of the learned models so that
they re ect features  similarities as these are de 
 ned on the basis of the sideinformation  We
perform experiments on   number of benchmark
datasets which show signi cant predictive performance gains  over   number of baselines  as  
result of the exploitation of the sideinformation 

  Introduction
Sideinformation in machine learning is   very general term
used in very different learning scenarios with quite different connotations  Nevertheless  generally it is understood as any type of information  other than the learning instances  which can be used to support the learning
process  typically such information will live in   different
space than the learning instances  Examples include learning with privileged information  Vapnik   Izmailov   
in which during training   teacher provides additional in 

 Equal contribution  University of Applied Sciences  Western
Switzerland  University of Geneva  Correspondence to  Amina
Mollaysa  maolaaisha aminanmu hesge ch  Pablo Strasser
 pablo strasser hesge ch  Alexandros Kalousis  Alexandros Kalousis hesge ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

formation for the learning instances  this information is not
available in testing 
In metric learning and clustering  it
has been used to denote the availability of additional similarity information on instances       pairs of similar and
dissimilar instances   Xing et al    In this paper we
focus on sideinformation describing the features  We will
consider learning problems in which we have additional information describing the properties and or the relations of
the features  The features will have their own vectorial descriptions in some space in which we will describe their
properties 
Real world problems with such properties are very common  For example in drug ef ciency prediction problems 
and more general in chemical formulae property prediction problems  drugs formulae are collections of molecules 
Each molecule comes with its own description  for example
in terms of its physiochemical properties  and or its molecular structure  In language modeling  words are features
and the words  semantic and syntactic properties are their
sideinformation  In image recognition  pixels are features
and their position is the sideinformation  and so on  Similar ideas also appear in tasks such as matrix completion  robust PCA and collaborative  ltering  Rao et al    Chiang et al      There one seeks low rank matrix
decompositions in which the component matrices are constrained to follow relationships given by sideinformation
matrices  typically matrices which contain user and item
descriptors 
Despite the prevalence of such problems  there has been
surprisingly limited work on learning with feature sideinformation  Krupka et al    used the feature sideinformation to perform feature selection as   preprocessing step prior to any modelling or learning  More interestingly  Krupka   Tishby    exploit the feature sideinformation directly within the learning process  by forcing features that have similar sideinformation to have  
similar weight within   SVM model  One can think of
this as   model regularisation technique in which we force
the model structure       the feature parameters  to re ect
the feature manifold as this is given by the feature sideinformation 
In the same work the authors also provide
an adhoc way to apply the same idea for nonlinear models  more precisely polynomials of low degree  However
the solution that they propose requires an explicit construc 

Regularising Nonlinear Models Using Feature Sideinformation

tion of the different nonlinear terms  as well as appropriate
de nitions of the feature sideinformation that is associated
with them  These de nitions are handcrafted and depend
on the speci   application problem  Beyond this adhoc approach it is far from clear how one could regularise general
nonlinear models so that they follow the feature manifold 
In this paper we present   method for the exploitation of
feature sideinformation in nonlinear models  The main
idea is that the learned model will treat in   similar manner
features that are similar  Intuitively  exchanging the values
of two very similar features should only have   marginal effect on the model output  This is straightforward for linear
models since we have direct access to how the model treats
the features       the feature weights  In such   case one can
design regularisers as Krupka   Tishby    did which
force the feature weights to re ect the feature manifold  An
obvious choice would be to apply   Laplacian regulariser
to the linear model  where the Laplacian is based on the
feature similarity  Such regularisers have been previously
used for parameter shrinkage but only in the setting of linear models where one has direct access to the model parameters  Huang et al    However  in general nonlinear
models we no longer have access to the feature weights  the
model parameters are shared between the features and we
cannot disentangle them 
We present   regulariser which forces the learned model
to be invariant symmetric to relative changes in the values of similar features  It directly re ects the intuition that
small changes in the values of similar features should have
  small effect on the model output  The regulariser relies
on   measure of the model output sensitivity to changes in
all possible pairs of features  The model sensitivity measure quanti es the norm of the change of the model output
under all possible relative changes of the values of two features  We compute this norm by integrating over the relative changes and the data distribution  Integrating over the
relative changes is problematic we thus give two ways to
approximate the sensitivity measure  In the  rst approach
we rely on    rst order Taylor expansion of the learned
model under which the sensitivity measure boils down to
the squared norm of the difference of the partial derivatives
of the model with respect to the input features  Under this
approach the regulariser  nally boils down to the application of   Laplacian regulariser on the Jacobian of the model 
In the second approach we rely on sampling and data augmentation to generate instances with appropriate relative
changes over different feature pairs  We approximate the
value of the regulariser only on the augmented data 
We implement the above ideas in the context of neural networks  nevertheless it is relatively straightforward to use
them in other nonlinear models such as SVMs and kernels 
We experiment on   number of text classi cation datasets in

which the sideinformation is the word vec representation
of the words  We compare against   number of baselines
and we show signi cant performance improvements 

  Learning Symmetric Models with Respect

to Feature Similarity

We consider supervised learning settings in which  in addition to the classical data matrix           containing  
instances and   features  and the target matrix           
we are also given   matrix            the ith row of which 
denoted by zi  contains   description of the ith feature 
We call   the feature sideinformation matrix  Note how
the   matrix is  xed and independent of the training instances  As in the standard supervised setting  instances 
xi     Rd  are drawn       from some nonobserved
probability distributions        and targets  yi     Rm 
are assigned according to some nonobserved conditional
distribution            Rm 
In the standard setting
we learn   mapping from the input to the output        
Rd       Rm using the      matrices by optimizing
In this paper we learn the inputsome loss function   
output mapping using in addition to the       matrices the
feature sideinformation   matrix 
We bring the feature sideinformation in the learning process through the feature similarity matrix     Rd   which
we construct from   as follows  Given two features      
with zi  zj  sideinformation vectors the Sij element of
  contains their similarity given by some similarity function  We will denote by           the Laplacian of the
similarity matrix      is the diagonal degree matrix with
Dii    Sij 

We use the similarity and the Laplacian matrices to constraint the learned model to treat in   similar manner features that have similar sideinformation  This is relatively
straightforward with linear models such as WXT     
Rm    and can be achieved through the introduction of
the Laplacian regulariser Tr  WLWT     ij       
    Sij in the objective function where     is the ith
column vector of   containing the model parameters associated with the ith feature   Huang et al    The Laplacian regulariser forces the parameter vectors of the features
to cluster according to the feature similarity 
However in nonlinear models such neat separation of the
model parameters is not possible since these are shared between the different input features  In order to achieve the
same effect we will now operate directly on the model output  We will do so by requiring that the change in the
model   output is marginal if we change the relative proportion of two very similar features  Concretely  let   and
  be such features  and ei  ej  be the ddimensional unit
vectors with the ith and jth dimensions respectively equal

Regularising Nonlinear Models Using Feature Sideinformation

over all instances drawn from       as well as for all  
vectors that satisfy the equality constraint eq     natural
measure of the degree to which the constraint holds for the
feature pair      is given by 

Rij            iei    jej 
      iei    jej 
SijI        dx

 

where        if                    and   otherwise 
Since we want to de ne   regulariser that accounts for all
feature pairs and their similarities we simply have 

    ij

Rij 

 

Calculating the regularizer is problematic due to the presence of the    function that selects the   subspace over
which the integration is performed  In the next two sections we will give two ways to approximate it  The  rst
one will be analytical relying on the  rst order Taylor expansion of     and its Jacobian  The second one stochastic  essentially performing data augmentation and de ning
  regularisation term along the lines of eq   

  An analytical Approximation
We will use the  rst order Taylor expansion of     to simplify the squared term in eq    by removing the   variable 
We will start by using the  rst order Taylor expansion to
approximate the value of       iei    jej  at  

      iei    jej              iei    jej 

       Rm   is the Jacobian of     evaluated at    Then
plugging the Taylor expansion in eq   we get 

            ei               ej   

 
Sij

 

and since                   we have                   
and eq   becomes 

     ei       ej                   

 
Sij

 

where       is the mdimensional partial derivative of
    with respect to the ith input feature  Using eq   we
can approximate Rij as follows 

Rij                   SijP    dx

Figure The blue dot is some given instance     The two axes
are the ith and jth features  If the two features are on the limit
identical then the model   output is constant along the line de ned
as       iei    jej             where   is some constant 

to one  We want that 

 

      iei    jej          iei    jej 
                  such that                  
Equation   states that as long as the total contribution of
the       features is kept  xed  the model   output should be
left almost unchanged  The exact equality will hold when
the       are on the limit identical       Sij     More
general the level of the model   change should re ect the
similarity of the       features  thus   more accurate reformulation of equation   is 

      iei    jej          iei    jej   

 
Sij

 

                  such that                  
Thus the norm of the change in the model output  that
we get when we alter the relative proportion of two features   and    while keeping their total contribution  xed 
should be inversely proportional to the features similarity 
     large similarity  small output change  The result is that
the model is symmetric to similar features and its output
does not depend on the individual contributions values of
two similar features but only on their total contribution 
In  gure   we visualise the effect of the model constraint
given in eq    Given some instance   and two features
      that are on the limit identical the constraint forces
the model output to be constant on the line de ned by
     iei    jej               for some given       
We can think of the whole process as the model clustering
together  to some latent factor  features that have very high
similarity  The latent factor captures the original features
total contribution leaving the model   output unaffected to
relative changes in their values 
To unclutter notation we will de ne the vector    
               We want the constraint of eq    to be valid

Regularising Nonlinear Models Using Feature Sideinformation

from which we get the following approximation of the
   regulariser 

      ij                SijP    dx

             SijP    dx

     ij
    Tr     LJT        dx

 

which is the local linear approximation of the original regulariser eq   on the input instances  Since we only have
access to the training sample and not to       we will get
the sample estimate of eq    given by

data augmentation as we will see in the next section  Note
also that the presence of the Jacobian in the objective function means that if we optimise it using gradient descent we
will need to compute second order partial derivatives which
come with an increasing computational cost 

    stochastic Approximation
Instead of using the  rst order Taylor expansion to simplify the squared term required by the regulariser we can
use sampling to approximate it  Concretely for   given
feature pair        and   given instance   we randomly
       
     
sample   quadruples    
    such that
 
       
   
       
                 which we use to
generate   new instance pairs as follows 

       

     

 

 

 

       ij   
      ij
    

   xk       xk Sij
   xk       xk Sij

Tr   xk LJT xk 

So the sample based estimate of the regulariser is   sum
of Laplacian regularisers applied on the Jacobian of each
one of the training samples 
It forces the partial derivatives of the model with respect to the input  or equivalently the model   sensitivity to the input features  to re ect
the features similarity in the local neighborhood around
each training point  Or in other words it will constrain the
learned model in   small neighborhood around each training point to have similar slop in the dimensions that are
associated with similar features  Note that if       Wx
then   xk      and Tr   xk LJT xk  reduces to the
standard Tr WLWT  Laplacian regulariser on the columns
of   associated with the input features  Adding the sample based estimate of the regulariser to the loss function we
get the  nal objective function which we minimize with
    giving the following minimization problem under the
analytical approximation 

min

    

  yk   xk      

Tr   xk LJT xk 

 

The approximation of the requlariser is only effective locally around each training point since it relies on  rst order
Taylor expansion  When the learned function is highly nonlinear  it can force model invariance only to small relative
changes in the values of two similar features  However  as
the size of the relative changes increases and we move away
from the local region the approximation is no longer effective  The regulariser will not be powerful enough to make
the invariance hold away from the training points  If we
want   less local approximation we can either use higher
order Taylor approximation which is computationally prohibitive or rely on   more global approximation through

           

       

  ei      
  ej
  ei      
  ej

 

We can now use the training sample and the sampling process to get an estimate of Rij  by 

     
 xk      

 xk      
  ei      

  ei      

  ej 

  ej Sij

and of the  nal regulariser    by 

       ij      
 xk      

 xk      
  ei      

  ej Sij
So the  nal optimization problem will now become 

  ei      

  ej 

min

    

  yk   xk         

 

 

Note that the new instances appear only in the regulariser
and not in the loss  The regulariser will penalise models
which do not have the invariance property with respect to
pairs of similar features  In practice when computing    
we do not want to go through all the pairs of features but
only through the most similar  We do not want to spend
sampling time on data augmentation for dissimilar pairs
since for these there is no effective constraint on the values of the model   output  So we simplify the sum run only
over the pairs of similar features  One motivation for the
stochastic approach was the fact that the analytical one relies in an approximation which is only effective locally in
the neighborhood of each learning instance  In the stochastic approach we have control on the size of the neighborhood over which the constraint is enforced through the Euclidean norm of the change vector        the larger its

Regularising Nonlinear Models Using Feature Sideinformation

value the larger the neighborhood  The smaller the neighborhood the closer we are to the local behavior of the analytical approximation  We should note here that the sampling of stochastic approximation will naturally blend with
the stochastic gradient optimization that we will use to optimize our objective functions 

  Optimization
We learn   with   standard feed forward neural network
with sigmoid activation functions applied on the hidden
layers using stochastic gradient descent  The objective
function of the analytical approach contains the Jacobian
of the model with respect to its input  Calculating the gradient over this results in the introduction of second order
partial derivatives of the model with respect to the inputs
and the model parameters  Bishop    gave   backpropagation algorithm for the exact calculation of the Hessian
of the loss of   multilayer perceptron  We have adapted
this algorithm so that we can compute the gradient of objective functions that contain the Jacobian with respect to
the input features  we give the complete gradient calculation procedure in the appendix 
We will give now the computational complexity of each
the two methods  We will denote by   the number of
layers    the output dimension of the network  hk the
number of hidden units of the kth layer and we will de 
 ne hmax   max hk                     The computational complexity of computing the gradient for   single instance of the objective function of the analytical approach
is               for networks with   single hidden layer
and           
max      for networks
with more than one hidden layers 
To reduce this computational complexity in our experiments we sparsify   by
keeping only the entries correponding to top   biggest
elements and zero out the rest 
The complexity now becomes               for one layer networks and      
max  for networks with more than
    
one layer  The computational complexity of the stochastic
approach is         
max               hmax         
while the computational complexity for standard feed forward network is         
  Related Work
Krupka   Tishby    use feature sideinformation  they
call it metafeatures  within   linear SVM model  They
force the SVM   weights to be similar for features that have
similar sideinformation  They achieve that through the introduction of   Gaussian prior on the feature weight vector 
The covariance matrix of the Gaussian is   function of the
features similarity  The authors show how to extend their
approach from linear to polynomial models  However  their
approach requires explicit calculation of all the higher or 

max       hmax     

max                

max         

der terms limiting its applicability to low order polynomials  Similar work is the Laplacianbased feature regularisation   Huang et al    which constraints the feature
weights to re ect relations that are given by the Laplacian 
The Laplacian matrix is constructed from available domain
knowledge  what here we call feature sideinformation  It
can also be constructed from the data  for example as  
function of the feature correlation matrix 
The Laplacian Regulariser  LR  has connections to some
Lassobased regularisers  With LR feature weights re ect
the feature similarity given by the side information  The
fused  graph  and group lasso variants  Tibshirani et al 
  Yuan   Lin    bring in and out of the model
feature groups  here feature similarity is whether features
belong in the same group or not   similarity  given by
domain knowledge  If yes their weight values will either
be all zero  out of the model  or different than zero  in the
model  but not necessarily the same  Fused lasso in addition to that pushes subsequent  connected  features to have
similar weights being even more similar to LR  Such lassobased regularisers are also applicable only in linear models
where we have   clear grasp of how each feature is treated
by the model  It is not clear how to use them on nonlinear
models  The analytical approach can be adapted easily for
such regularisers making the approach interesting relevant
for regularisers other than the Laplacian  On   similar note
Rosasco et al    applied lasso regularisation on the Jacobian of kernelbased models to learn nonlinear models
that are sparse with respect to the input features 
The Taylor expansion in the analytical approximation of the
regulariser brings in the Jacobian of the model  Regularisers that use the Jacobian have previously been successfully
used to control the stability robustness of models to noisy
inputs  Relevant work includes contractive auto encoders 
 Rifai et al      and the work of  Zhai   Zhang   
 Rifai et al      use the Frobenius of the Jacobian at the
input instances to force the model to be relatively constant
in small neighbors around the input instances  Such   regulariser introduces invariance to small input variations 
Optimizing the Jacobian in networks with more than one
layer is cumbersome  thus very often the stochastic approach is preferred over the analytic       Zhai   Zhang 
  Denoising autoencoders   Vincent et al    follow the stochastic paradigm and require that small random
variations in the inputs have only   limited effect on the
model output  Zheng et al    used Gaussian perturbations to stabilise the network   output with respect to
variations in the input  essentially augmenting the training
data  Regularisers on higher order derivatives  Hessian  are
also used   Rifai et al      in such cases the stochastic approach is the only choice due to the prohibitive cost
of optimizing the Hessian term 
Data augmentation is

Regularising Nonlinear Models Using Feature Sideinformation

also used when we have additional prior knowledge on the
instance structures to which the models should be invariant  In imaging problems such structures include translations  rotations  scalings etc   Simard et al    Decoste
  Sch olkopf     Corduneanu   Jaakkola    learn
models with builtin invariance by imposing class invariance in dense regions of the feature space  we impose invariance to directions determined by the side information 
Closer to our work are  Krupka   Tishby    and the
works that use Laplacian based regularisers for model regularisation        Huang et al    However to the best
of our knowledge all previous work was strictly limited to
linear models  We show how to apply such regularisers and
constraints to general classes of nonlinear models 

  Experiments
We will experiment and evaluate our regularisers in two
settings    synthetic and   real world one  We will compare
the analytical and the stochastic regulariser  which we will
denote by AN and ST respectively  against popular regularisers used with neural networks  namely   and Dropout
 Srivastava et al    over different network architectures  In the real world datasets we also give the results of
the Word Mover   Distance  WMD   Kusner et al   
which makes direct use of the sideinformation to compute
document distances  Obviously our regularisers and WMD
have an advantage over   and dropout since it exploit sideinformation which   and dropout do not 
We trained both the analytical and the stochastic models 
as well as all baselines against which we compare  using
Adam  Kingma   Ba    We used          
        for one hidden layer networks  and    
  for the networks with more hidden layers  We initialize all networks parameters using  Glorot   Bengio 
  Due to the large computational complexity of the
analytical approach we set the minibatch size   to  ve 
For the stochastic model  as well as for all the baseline
models  we set the minibatch size to   For the analytical model we set the maximum number of iterations to
  For the stochastic model we set the maximum number of iterations to   for the one layer networks and
to   for networks with more layers  We used early
stopping where we keep   of the training data as the
validation set  Every  ve parameter updates we compute
the validation error  Training terminates either if we reach
the maximum iteration number or the validation error keeps
increasing more than ten times in   row 
In the stochastic approach we do the sampling for the generation of the instance pairs within the stochastic gradient descent process  Concretely for each instance   in  
mini batch we randomly chose   feature pair       from

the set of similar feature pairs  We sample   quadruple
               from   respecting the constraint        
        from which we generate the respective instance
pairs  We repeat the process   times each time sampling  
new feature pair       and   new quadruple  We    the set
of similar feature pairs to be the top   of most similar
feature pairs  Thus within each minibatch of size   we
generate      instance pairs and we accumulate the norm
of the respective model output differences in the objective 
In the experiments we set      

  Arti cial Datasets
We design   simple data generation process in order to test
the performance of our regularisers when the data generation mechanism is compatible with the assumptions of
our models  We randomly generate an instance matrix
    Rn   by uniformly sampling instances from Rd  We
create    feature clusters as follows  To each one of these
clusters we initially assign one of the original input features
without replacement  We assign randomly and uniformly
the remaining    features to the clusters  We use the feature clusters to de ne   latent space where every feature
cluster gives rise to   latent feature  The value of each latent
feature is the sum of the values of the features that belong to
its cluster  On the latent space representation of the training
data we apply   linear transformation that projects the latent space to   new space with lower dimensionality    On
this lower dimensionality space we apply an elementwise
sigmoid and the  nal class assignment is given by the index
of the maximum sigmoid value  The similarity Sij of the
      features of the original space is   if they ended up in the
same cluster and   otherwise  We set            
      We will call this dataset    The generating procedure gave   very sparse   matrix with only   of its
entries being nonzero  Each feature had an average of  
similar features  We used   instances for training and
the rest for testing  During training   of the instances
are used for the validation set  We train all algorithms on
the original input space  For all regularisers we used   network with single hidden layer with   hidden units  We
tune the hyperparameters based on the performance on the
validation set  We select the   hyperparameters of of AN 
ST  and   from                    we select the   of
dropout from           We set the   in the
augmentation process  that controls the size of the neigborhood within which the output constraints should hold  to
one 
Both the analytical and the stochastic regulariser bring performance improvements of roughly   when compared
to the   regulariser and to Dropout  results in table   In
 gure   we plot the error on the validation set as   function
of epoch number and the real time for the four regularisers 
The analytical and stochastic regulariser require consider 

Regularising Nonlinear Models Using Feature Sideinformation

ably less epochs to converge and that to signigicantly lower
error values than either   or dropout  However  since the
computational complexity of each step is larger  the real
time to convergence is larger than that of   and dropout 

using exactly the same protocol as in    The results are
also given in table   As we see the classi cation error of
both ST and AN increases as the dataset sparsity increases
and it approaches that of the standard regularisers 

ST
AN
 
Dropout

Dataset
  
  
  

 Sij    

ST
   
   
   

AN
 
 
 

  Dropout
 
 
 

 
 
 

Table Classi cation error    of the different regularizers  and
  of non zero elements of the similarity matrix   for the three
arti cial datasets 

  Real World Datasets
We evaluated both approaches on the eight document classi cation datasets used in  Kusner et al    We removed all the words in the SMART stop word list  Salton
  Buckley    Documents are represented as bag of
words  To speed up training  we removed words that appear
very few times over all the documents of   dataset  Concretely  in  NEWS we reduce the dictionary size by removing words with   frequency less or equal to three  In the
OHSUMED and CLASSIC datasets we remove words with
frequency one and the in REUTER dataset words with frequency equal or less than two  As feature sideinformation
we use the word vec representation of the words which
have   dimensionality of    Mikolov et al   
In
table   we give   description of the  nal datasets on which
we experiment including the number of classes     and average number of unique words per document 

Date set
BBCsport
Twitter
Classic
Amazon
 NEWS
Recipe
Ohsumed
Reuter

 
 
 
 
 
 
 
 
 

  Unique words avg   
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

Table  Dataset description

We compute the similarity matrix   from the word vect
word representations using the heat kernel with bandwidth
parameter        the similarity of       features is given by 
   zi   zj    zi   zj  We select   so that
Sij   exp   
roughly   of the entries of the similarity matrix are in
    interval 
For those datasets without   prede ned train test split
 BBCsport  Twitter  Classic  Amazon  Recipe  we use  vefold cross validation and report the average error  We control the statistical signi cance of the results using the MacNemar   test    value  We tune the hyperparame 

 
 
 
 
 

 
 

 
 
 
 
 

 
 

 
 

 
 

 
 

 

 

 

 

 

 

 

 

 

 

ST
AN
 
Dropout

 

 

 

 

epoch number

 

 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 

 

 

 

 

       

real time  seconds 

Figure Validation set error for the different regularisers  as  
function of epoch number  left  as   function of realtime  right 

The regularizer we propose constrains the model structure
by forcing the model to re ect the feature similarity as
these are given in the similarity matrix  Thus we expect the
structure of the similarity matrix to have an impact on the
performance of the regulariser  To see that let us consider
the trivial case in which   is diagonal In this case the input
and the latent spaces are equivalent  and the regulariser has
no effect since there are no similarity constraints  If all features are identical       Sij          then the latent space
will have   dimensionality of one  and the regulariser has
the strongest effect 
To explore this dependency we generate two additional
synthetic datasets where we use the same generating mechanism as in    but vary the proportion of features we cluster together to generate latent factors  Concretely in the
synthetic dataset we will call    we randomly select   set
  of    features over which we will perform clustering
to de ne latent factors  We use the remaining set   of
   features directly as they are in the latent space  We
cluster the features of the   set to    clusters latent factors  making sure that as in    each cluster has at least one
feature in it  As   result the  nal latent space has   dimensionality of               To generate the class
assignments we proceed as in    To generate the third
dataset     we select    features to generate   and the
remaining for    We now cluster the features in   to   
clusters  again making sure that there is at least one feature pre cluster  The dimensionality of the latent space is
now                class assignments are generated
as above  We used the same values for         as in   
As we move from    to    we reduce the number of features that are similar to other features  thus we increase the
sparsity of    For    and    the percentage of nonzero
elements is   and   respectively  compared to
  we had in    So    is the datasets that has most
constraints while    is the one with the least constraints 
We apply the different regularisers in these two datasets

Regularising Nonlinear Models Using Feature Sideinformation

ters with threefold inner cross validation  We select the
   of AN  ST  and   from           we select the   of dropout from           We did
  series of experiments in which we varied the number of
hidden layers  Due to the computational complexity of the
backprogation for the AN regulariser we only give results
for the single layer architecture 
In the  rst set of experiments we use   neural network with
one hidden layer and   hidden units  we give the results
in table   ST is signi cantly better than the AN in six out
of the eight datasets  signi cantly worse once  and equivalent in one dataset  ST is signi cantly better than the  
in six out of the eight daasets  while it is equivalent in one 
Compared to dropout it is four times signi cantly better
and two times signi cantly worse 
When we increase the number of hidden layers to two with
  and   units on the  rst and second layer ST method
is signi cantly better compared to   three times  signi 
cantly worse three times  while there is no signi cant difference in two datasets    similar picture emerges with
respect to Dropout with ST being signi cantly better three
times  signi cantly worse twise  while in three cases there
is no signi cant difference  We give the detailed results in
table  

Dataset
BBCsport
Twitter
Classic
Amazon
 NEWS
Recipe
Ohsumed
Reuter

ST
 
 
 
 
 
 
 
 

AN
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

Dropout WMD
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

Table Classi cation error    with one hidden layer NNs  AN 
analytical approach  ST  stochastic approach  WMD results are
from  Kusner et al    The   and   signs give the signi 
cance test results of the comparison of the performance of   given
regulariser to those of the regularisers in the subsequent columns 
indicating respectively signi cantly better  worse  no difference 
WMD is not included in the signi cance comparison since at the
time of the experiments we did not have acces to the code 

Dataset
BBCsport
Twitter
Classic
Amazon
 NEWS
Recipe
Ohsumed
Reuter

ST
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

Dropout WMD
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Table Classi cation error    with two hidden layers network 
Table interpretation as in table  

  Conclusion and Future Work
Many real world applications come with additional information describing the properties of the features  Despite
that  quite limited attention has been given to such setting 
In this paper we develop   regulariser that exploits exactly
such information for general nonlinear models  It relies on
the simple intuition that features which have similar properties should be treated by the learned model in   similar manner  The regulariser imposes   stability constraint
over the model output  The constraint forces the model to
produce similar outputs for instances the feature values of
which differ only on similar features  We give two ways
to approximate the value of the regulariser  An analytical
one which boils down to the imposition of   Laplacian regulariser on the Jacobian of the learned model with respect
to the input features and   stochastic one which relies on
sampling 
We experiment with neural networks with the two approximations of the regulariser and compare their performance to well established model regularisers  namely  
and dropout  on arti cial and real world datasets  In the
arti cial datasets  for which we know that they match the
assumptions of our regulariser we demonstrate signi cant
performance improvements  In the real world datasets the
performance improvements are less striking  One of the
main underlying assumptions of our model is that the feature sideinformation is indeed relevant for the learning
problem  when this is indeed the case we will have performance improvements  If it is not the case then the regulariser will not be selected  as   result of the tuning of the
  parameter 
Along the same lines we want to perform   more detailed
study on how the structure of the similarity matrix  namely
its sparsity and the underlying feature cluster structure  determines the regularisation strength of our regulariser  It is
clear that   sparse similarity matrix will lead to   rather limited regularisation effect since only few features will be affected  This points to the fact that the regulariser should be
used together with more traditional sparsity inducing regularisers  especially in the case of   sparse feature similarity matrix  Finally since we use the feature information
through   similarity function it might be the case that the
similarity function that we are using is not appropriate and
better results can be obtained if we also learn the feature
similarity  We leave this for future work 

Acknowledgement This work was supported by SNSF
     RAWFIE   ISNET  HSTS 

Regularising Nonlinear Models Using Feature Sideinformation

References
Bishop  Christopher  Exact calculation of the hessian matrix for the multilayer perceptron  Neural Computation 
  January  

Bishop  Christopher    Neural networks for pattern recog 

nition  Oxford university press   

Chiang  KaiYang  Hsieh  ChoJui  and Dhillon  Inderjit   
Matrix completion with noisy side information  In Advances in Neural Information Processing Systems  pp 
   

Chiang  KaiYang  Hsieh  ChoJui  and Dhillon  EDU Inderjit    Robust principal component analysis with side
information  In Proceedings of The  rd International
Conference on Machine Learning  pp     

Corduneanu  Adrian and Jaakkola  Tommi  On information
regularization  In Proceedings of the Nineteenth conference on Uncertainty in Arti cial Intelligence  pp   
  Morgan Kaufmann Publishers Inc   

Decoste  Dennis and Sch olkopf  Bernhard  Training invariant support vector machines  Machine Learning   
    ISSN   doi     
 

Glorot  Xavier and Bengio  Yoshua  Understanding the dif 
 culty of training deep feedforward neural networks  In
Aistats  volume   pp     

Huang  Jian  Ma  Shuangge  Li  Hongzhe  and Zhang  CunHui  The sparse laplacian shrinkage estimator for highdimensional regression  The Annals of Statistics   
    ISSN  

Kingma  Diederik and Ba 

Jimmy 
method for stochastic optimization 
arXiv   

Adam 

 
arXiv preprint

Krupka  Eyal and Tishby  Naftali 

Incorporating prior
knowledge on features into learning 
In International
Conference on Arti cial Intelligence and Statistics  pp 
   

Krupka  Eyal  Navot  Amir  and Tishby  Naftali  Learning to select features using their properties  Journal of
Machine Learning Research   Oct   

Kusner  Matt    Sun  Yu  Kolkin  Nicholas    and Weinberger  Kilian    From word embeddings to document
distances  In Proceedings of the  nd International Conference on Machine Learning  ICML   pp   
   

Mikolov  Tomas  Sutskever  Ilya  Chen  Kai  Corrado 
Greg    and Dean  Jeff  Distributed representations of
words and phrases and their compositionality 
In Advances in neural information processing systems  pp 
   

Rao  Nikhil  Yu  HsiangFu  Ravikumar  Pradeep    and
Dhillon  Inderjit    Collaborative  ltering with graph
information  Consistency and scalable methods  In Advances in Neural Information Processing Systems  pp 
   

Rifai  Salah  Mesnil  Gr egoire  Vincent  Pascal  Muller 
Xavier  Bengio  Yoshua  Dauphin  Yann  and Glorot 
Xavier  Higher order contractive autoencoder  In Machine Learning and Knowledge Discovery in Databases
  European Conference  ECML PKDD   Athens 
Greece  September     Proceedings  Part II  pp 
     

Rifai  Salah  Vincent  Pascal  Muller  Xavier  Glorot 
Xavier  and Bengio  Yoshua  Contractive autoencoders 
Explicit invariance during feature extraction 
In Proceedings of the  th international conference on machine learning  ICML  pp       

Rosasco  Lorenzo  Villa  Silvia  Mosci  So    Santoro 
Matteo  and Verri  Alessandro  Nonparametric sparsity
and regularization  Journal of Machine Learning Research     

Salton  Gerard and Buckley  Christopher  Termweighting
approaches in automatic text retrieval  Information processing   management     

Simard  Patrice    Victorri  Bernard  LeCun  Yann  and
Denker  John    Tangent prop     formalism for specifying selected invariances in an adaptive network 
In
Moody  John    Hanson  Stephen Jose  and Lippmann 
Richard  eds  Advances in Neural Information Processing Systems    NIPS Conference  Denver  Colorado 
USA  December     pp    Morgan Kaufmann   

Srivastava  Nitish  Hinton  Geoffrey    Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan  Dropout 
  simple way to prevent neural networks from over tting 
Journal of Machine Learning Research   
   

Tibshirani  Robert  Saunders  Michael  Rosset  Saharon 
Zhu  Ji  and Knight  Keith  Sparsity and smoothness via
the fused lasso  Journal of the Royal Statistical Society 
Series    Statistical Methodology     

Vapnik  Vladimir and Izmailov  Rauf  Learning using privileged information  Similarity control and knowledge

Regularising Nonlinear Models Using Feature Sideinformation

transfer 
   

Journal of Machine Learning Research   

Vincent  Pascal  Larochelle  Hugo  Lajoie  Isabelle  Bengio  Yoshua  and Manzagol  PierreAntoine  Stacked denoising autoencoders  Learning useful representations in
  deep network with   local denoising criterion  Journal
of Machine Learning Research     

Xing  Eric    Ng  Andrew    Jordan  Michael    and Russell  Stuart    Distance metric learning with application
to clustering with sideinformation  In Advances in Neural Information Processing Systems    Neural Information Processing Systems  NIPS   December  
  Vancouver  British Columbia  Canada  pp   
   

Yuan  Ming and Lin  Yi  Model selection and estimation in
regression with grouped variables  Journal of the Royal
Statistical Society  Series    Statistical Methodology 
   

Zhai  Shuangfei and Zhang  Zhongfei 

Manifold
regularized discriminative neural networks 
CoRR 
abs    URL http arxiv org 
abs 

Zheng  Stephan  Song  Yang  Leung  Thomas  and Goodfellow  Ian   
Improving the robustness of deep neural networks via stability training  In   IEEE Conference on Computer Vision and Pattern Recognition 
CVPR   Las Vegas  NV  USA  June     pp 
   

