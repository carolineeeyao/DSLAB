Strong NPHardness for Sparse Optimization with Concave Penalty Functions

Yichen Chen   Dongdong Ge   Mengdi Wang   Zizhuo Wang   Yinyu Ye   Hao Yin  

Abstract

Consider the regularized sparse minimization
problem  which involves empirical sums of loss
functions for   data points  each of dimension
   and   nonconvex sparsity penalty  We prove
that  nding an   nc dc optimal solution to
the regularized sparse optimization problem is
strongly NPhard for any             such that
            The result applies to   broad class
of loss functions and sparse penalty functions 
It suggests that one cannot even approximately
solve the sparse optimization problem in polynomial time  unless     NP 

Keywords  Nonconvex optimization   Computational
complexity   NPhardness   Concave penalty   Sparsity

Introduction

 
We study the sparse minimization problem  where the objective is the sum of empirical losses over input data and  
sparse penalty function  Such problems commonly arise
from empirical risk minimization and variable selection 
The role of the penalty function is to induce sparsity in the
optimal solution       to minimize the empirical loss using
as few nonzero coef cients as possible 
Problem   Given the loss function  cid           cid    
penalty function        cid     and regularization parameter
      consider the problem

  cid 

 cid cid aT

     bi

min
  Rd

 cid     

  cid 

   xj   

  

  

where                 an     Rn                    bn  
  Rn are input data 

 Princeton University  NJ  USA  Shanghai University of Finance and Economics  Shanghai  China  University of Minnesota 
MN  USA  Stanford University  CA  USA  Correspondence to 
Mengdi Wang  mengdiw princeton edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

We are interested in the computational complexity of Problem   under general conditions of the loss function  cid  and
the sparse penalty    In particular  we focus on the case
where  cid  is   convex loss function and   is   concave penalty
with   unique minimizer at   Optimization problems with
convex  cid  and concave   are common in sparse regression 
compressive sensing  and sparse approximation    list of
applicable examples of  cid  and   is given in Section  
For certain special cases of Problem   it has been shown
that  nding an exact solution is strongly NPhard  Huo  
Chen    Chen et al    However  these results have
not excluded the possibility of the existence of polynomialtime algorithms with small approximation error 
 Chen
  Wang    established the hardness of approximately
solving Problem   when   is the    norm 
In this paper  we prove that it is strongly NPhard to approximately solve Problem   within certain optimality error  More precisely  we show that there exists   lower
bound on the suboptimality error of any polynomialtime
deterministic algorithm  Our results apply to   variety of
optimization problems in estimation and machine learning 
Examples include sparse classi cation  sparse logistic regression  and many more  The strong NPhardness of approximation is one of the strongest forms of complexity
result for continuous optimization  To our best knowledge 
this paper gives the  rst and strongest set of hardness results for Problem   under very general assumptions regarding the loss and penalty functions 
Our main contributions are threefold 

  We prove the strong NPhardness for Problem   with
general loss functions  This is the  rst results that
apply to the broad class of problems including but
not limited to  least squares regression  linear model
with Laplacian noise  robust regression  Poisson regression  logistic regression  inverse Gaussian models 
etc 

  We present   general condition on the sparse penalty
function   such that Problem   is strongly NPhard 
The condition is   slight weaker version of strict concavity  It is satis ed by typical penalty functions such
as the Lq norm          clipped    norm  SCAD 
etc  To the best of our knowledge  this is the most gen 

Strong NPHardness for Sparse Optimization with Concave Penalty Functions

eral condition on the penalty function in the literature 
  We prove that  nding an    nc dc optimal solution to Problem   is strongly NPhard  for any        
    such that             Here the    hides parameters that depend on the penalty function    which
is to be speci ed later 
It illustrates   gap between
the optimization error achieved by any tractable algorithm and the desired statistical precision  Our proof
provides    rst uni ed analysis that deals with   broad
class of problems taking the form of Problem  

Section   summarizes related literatures from optimization 
machine learning and statistics  Section   presents the key
assumptions and illustrates examples of loss and penalty
functions that satisfy the assumptions  Section   gives the
main results  Section   discusses the implications of our
hardness results  Section   provides   proof of the main
results in   simpli ed setting  The full proofs are deferred
to the appendix 

  Background and Related Works
Sparse optimization is   powerful machine learning tool for
extracting useful information for massive data  In Problem
  the sparse penalty serves to select the most relevant variables from   large number of variables  in order to avoid
over tting  In recent years  nonconvex choices of   have received much attention  see  Frank   Friedman    Fan
  Li    Chartrand    Candes et al    Fan  
Lv    Xue et al    Loh   Wainwright    Wang
et al    Fan et al   
Within the optimization and mathematical programming
community  the complexity of Problem   has been considered in   number of special cases   Huo   Chen     rst
proved the hardness result for   relaxed family of penalty
functions with    loss  They show that for the penalties in
   hardthresholded  Antoniadis   Fan    and SCAD
 Fan   Li    the above optimization problem is NPhard   Chen et al    showed that the   Lp minimization is strongly NPhard when         At the same time 
 Bian   Chen    proved the strongly NPhardness for
another class of penalty functions  The preceding existing
analyses mainly focused on  nding an exact global optimum to Problem   For this purpose  they implicitly assumed that all the input and parameters involved in the reduction are rational numbers with    nite numerical representation  otherwise  nding   global optimum to   continuous problem would be always intractable    recent technical report  Chen   Wang    proves the hardness of
obtaining an  optimal solution when   is the    norm 
Within the theoretical computer science community  there
have been several early works on the complexity of sparse

recovery  beginning with  Arora et al     Amaldi  
Kann    proved that the problem min cid   cid    Ax     
is not approximable within   factor  log    for any      
 Natarajan    showed that  given         and    the
problem min cid   cid     cid Ax     cid      is NPhard   Davis
et al    proved   similar result that for some given
      and       it is NPcomplete to  nd   solution
  such that  cid   cid      and  cid Ax     cid      More recently 
 Foster et al    studied sparse recovery and sparse linear regression with subgaussian noises  Assuming that the
true solution is Ksparse  it showed that no polynomialtime  randomized  algorithm can  nd        log  dsparse
solution   with  cid Ax   cid 
    dC       with high probability  where         are arbitrary positive scalars  Another
work  Zhang et al    showed that under the Gaussian
linear model  there exists   gap between the mean square
loss that can be achieved by polynomialtime algorithms
and the statistically optimal mean squared error  These two
works focus on estimation of linear models and impose distributional assumptions regarding the input data  These results on estimation are different in nature with our results
on optimization 
In contrast  we focus on the optimization problem itself 
Our results apply to   variety of loss functions and penalty
functions  not limited to linear regression  Moreover  we do
not make any distributional assumption regarding the input
data 
There remain several open questions  First  existing results
mainly considered least square problems or Lq minimization problems  Second  existing results focused mainly on
the    penalty function  The complexity of Problem   with
general loss function and penalty function is yet to be established  Things get complicated when   is   continuous
function instead of the discrete    norm function  The
complexity for  nding an  optimal solution with general
 cid  and   is not fully understood  We will address these questions in this paper 

  Assumptions
In this section  we state the two critical assumptions that
lead to the strong NPhardness results  one for the penalty
function    the other one for the loss function  cid  We argue that these assumptions are essential and very general 
They apply to   broad class of loss functions and penalty
functions that are commonly used 

  Assumption About Sparse Penalty
Throughout this paper  we make the following assumption
regarding the sparse penalty function   
Assumption   The penalty function    satis es the fol 

Strong NPHardness for Sparse Optimization with Concave Penalty Functions

lowing conditions 

     Monotonicity     is nondecreasing on    
 ii   Concavity  There exists       such that    is con 

cave but not linear on      

In words  condition  ii  means that the concave penalty   
is nonlinear  Assumption   is the most general condition
on penalty functions in the existing literature of sparse optimization  Below we present   few such examples 

  In variable selection problems  the    penalization
           cid  arises naturally as   penalty for the
number of factors selected 

    natural generalization of the    penalization is the
Lp penalization        tp where          
The corresponding minimization problem is called the
bridge regression problem  Frank   Friedman   

  To obtain   hardthresholding estimator  Antoniadis  
Fan   use the penalty functions           
       with       where       max     
denotes the positive part of   

  Any penalty function that belongs to the folded concave penalty family  Fan et al    satis es the conditions in Theorem   Examples include the SCAD
 Fan   Li    and the MCP  Zhang     
whose derivatives on     are   cid 
             
     
    respectively  where             and      

           

and   cid 

    

  

  The conditions in Theorem   are also satis ed by the
clipped    penalty function  Antoniadis   Fan   
Zhang                 min      with      
This is   special case of the piecewise linear penalty
function 

 cid     

  Assumption About Loss Function
We state our assumption about the loss function  cid 
Assumption   Let   be an arbitrary constant  For any
interval     where                there exists
    cid    bi  has

       and     Qk such that         cid  

the following properties 

         is convex and Lipschitz continuous on    
 ii       has   unique minimizer    in    
 iii  There exists               and        such that

when         we have

               

  

    

 iv        bi  

   can be represented in   log

bits 

 

 

 

struct   function       cid  

Assumption   is   critical  but very general  assumption
regarding the loss function  cid       Condition     requires
convexity and Lipschitz continuity within   neighborhood 
Conditions  ii   iii  essentially require that  given an interval     one can arti cially pick            bk to coni   cid    bi  such that   has its
unique minimizer in     and has enough curvature near
the minimizer  This property ensures that   bound on the
minimal value of      can be translated to   meaningful
bound on the minimizer    The conditions      ii   iii 
are typical properties that   loss function usually satis es 
Condition  iv  is   technical condition that is used to avoid
dealing with in nitelylong irrational numbers  It can be
easily veri ed for almost all common loss functions 
We will show that Assumptions   is satis ed by   variety
of loss functions  An  incomplete  list is given below 

  In the least squares regression  the loss function has

  cid 

 cid aT

      bi

 cid 

 

      

                if      

if          

the form

where             and      

  Another family of penalty functions which bridges the
   and    penalties are the fraction penalty functions

      

      

     

with        Lv   Fan   

  The family of logpenalty functions 

      

 

log     

log       

with       also bridges the    and    penalties  Candes et al   

  

Using our notation  the corresponding loss function
is  cid                 For all     we choose an
arbitrary   cid        We can verify that       
 cid      cid  satis es all the conditions in Assumption  

  In the linear model with Laplacian noise  the negative

loglikelihood function is

  cid 

 cid cid aT

      bi

 cid cid   

  

So the loss function is  cid                As in the case
of least squares regression  the loss function satisfy

Strong NPHardness for Sparse Optimization with Concave Penalty Functions

Assumption   Similar argument also holds when we
consider the Lq loss        with      

  In robust regression  we consider the Huber loss  Huber    which is   mixture of    and    norms 
The loss function takes the form

 cid   
       
           

   

         

for            
otherwise 

for some       where     aT    We then verify that
Assumption   is satis ed  For any interval     we
pick an arbitrary         and let         cid      
We can see that      satis es all the conditions in Assumption  

  In Poisson regression  Cameron   Trivedi    the

negative loglikelihood minimization is

  cid 

    

  

 exp aT

    bi aT

  log              min
  Rd

min
  Rd
We now show that  cid         ey         satis es Assumption   For any interval     we choose   and
  such that              Note that          
                Also     is bounded
by eM   Thus       can be chosen to be polynomial
in  cid     cid  by letting      cid     cid  and  
be some number less than     eM   Then  we choose
    cid    bi   
    ey          Let us verify Assumption        iv 
are straightforward by our construction  For  ii  note
that      take its minimum at ln      which is inside
    by our construction  To verify  iii  consider
the second order Taylor expansion of      at ln     

      and     Zk such that       cid  

                

    ey
 

            
 

    

We can see that  iii  is satis ed  Therefore  Assumption   is satis ed 

  In logistic regression 
function minimization is

  cid 

the negative loglikelihood

         cid 

bi   aT

    

min
  Rd

log    exp aT

  

  

that       cid  

We claim that the loss function  cid         log   
exp         satis es Assumption   By   similar argument as the one in Poisson regression  we can verify
    cid    bi      log    exp      qy
where           
  
      and      are polynomial
     
in  cid cid  satis es all the conditions in Assumption   For  ii  observe that  cid       take its minimum

at     ln    
ond order Taylor expansion at     ln    
is

       To verify  iii  we consider the sec 
       which

                

 

    ey 

      

where         Note that ey is bounded by eM  
which can be computed beforehand  As   result   iii 
holds as well 

  In the mean estimation of inverse Gaussian models
 McCullagh    the negative loglikelihood function minimization is

 bi  cid aT

  cid 

  

min
  Rd

       
bi

 

  
 

the loss function  cid        
Now we show that
   
satis es Assumption   By setting the
derivative to be zero with regard to    we can see that
  take its minimum at         Thus for any    
 
we choose   cid           
  We can see
that         cid      cid  satis es all the conditions in Assumption  

   

 

  In the estimation of generalized linear model under the
exponential distribution  McCullagh    the negative loglikelihood function minimization is

min
  Rd

  log              min
  Rd

bi
aT
   

  log aT

    

By setting the derivative to   with regard to    we can
see that  cid          
    log   has   unique minimizer at
       Thus by choosing   cid        appropriately 
we can readily show that         cid      cid  satis es all
the conditions in Assumption  

To sum up  the combination of any loss function given in
Section   and any penalty function given in Section  
results in   strongly NPhard optimization problem 

  Main Results
In this section  we state our main results on the strong NPhardness of Problem   We warm up with   preliminary
result for   special case of Problem  
Theorem      Preliminary Result  Let Assumption  
hold  and let    be twice continuously differentiable in
  Then the minimization problem

  cid 

  

  xj 

 

 cid Ax     cid  

     

min
  Rn

is strongly NPhard 

Strong NPHardness for Sparse Optimization with Concave Penalty Functions

The result shows that many of the penalized least squares
problems        Fan   Lv    while enjoying small
estimation errors  are hard to compute 
It suggests that
there does not exist   fully polynomialtime approximation
scheme for Problem   It has not answered the question 
whether one can approximately solve Problem   within certain constant error 
Now we show that it is not even possible to ef ciently approximate the global optimal solution of Problem   unless
          Given an optimization problem minx        
we say that   solution    is  optimal if        and        
inf              

Theorem    Strong NPHardness of Problem   Let Assumptions   and   hold  and let             be arbitrary such that             Then it is strongly NPhard to  nd           nc dc optimal solution of Problem   where   is the dimension of variable space and
    mint               

 

 

The nonapproximable error in Theorem   involves the
constant   which is determined by the sparse penalty function    In the case where   is the    norm function  we can
take       In the case of piecewise linear    penalty  we
have              In the case of SCAD penalty  we
have      
According to Theorem   the nonapproximable error    
    nc dc  is determined by three factors      properties of
the regularization penalty      ii  data size    and  iii  dimension or number of variables    This result illustrates  
fundamental gap that can not be closed by any polynomialtime deterministic algorithm  This gap scales up when either the data size or the number of variables increases  In
Section   we will see that this gap is substantially larger
than the desired estimation precision in   special case of
sparse linear regression 

Theorems   and   validate the longlasting belief that optimization involving nonconvex penalty is hard  More importantly  Theorem   provide lower bounds for the optimization error that can be achieved by any polynomialtime
algorithm  This is one of the strongest forms of hardness
result for continuous optimization 

  An Application and Remarks
In this section  we analyze the strong NPhardness results
in the special case of linear regression with SCAD penalty
 Problem   We give   few remarks on the implication of
our hardness results 

  Hardness of Regression with SCAD

Penalty

  xj 

 

  

min

 

 
 

 cid Ax     cid 

Let us try to understand how signi cant
is the nonapproximable error of Problem   We consider the special
case of linear models with SCAD penalty  Let the input
data        be generated by the linear model             
where    is the unknown true sparse coef cients and   is  
zeromean multivariate subgaussian noise  Given the data
size   and variable dimension    we follow  Fan   Li 
  and obtain   special case of Problem   given by

     

  cid 
      cid      an

where      cid log       Fan   Li    showed that the
 cid    where an  

optimal solution    of problem   has   small statistical
error        cid       cid 
max   cid 
   cid     Fan et al    further showed
  log doptimal solution to  
that we only need to  nd  
to achieve such   small estimation error 
However  Theorem   tells us that it is not possible to compute an    noptimal solution for problem   in polynomial
time  where               by letting            
  In the special case of problem   we can verify that
      and          log      As   result  we see
that

              cid cid   log   

   

       

 

for high values of the dimension    According to Theorem
  it is strongly NPhard to approximately solve problem
  within the required statistical precision
  log    This
result illustrates   sharp contrast between statistical properties of sparse estimation and the worstcase computational
complexity 

 

illustrated by the preceding analysis 

  Remarks on the NPHardness Results
the nonAs
approximibility of Problem   suggests that computing the
sparse estimator is hard  The results suggest   fundamental con ict between computation ef ciency and estimation accuracy in sparse data analysis  Although the results seem negative  they should not discourage researchers
from studying computational perspectives of sparse optimization  We make the following remarks 

  Theorems     are worstcase complexity results 
They suggest that one cannot  nd   tractable solution
to the sparse optimization problems  without making
any additional assumption to rule out the worstcase
instances 

  Our results do not exclude the possibility that  under
more stringent modeling and distributional assump 

Strong NPHardness for Sparse Optimization with Concave Penalty Functions

tions  the problem would be tractable with high probability or on average 

In short  the sparse optimization Problem   is fundamentally hard from   purely computational perspective  This
paper together with the prior related works provide   complete answer to the computational complexity of sparse optimization 

  Proof of Theorem  
In this section  we prove Theorem   The proof of Theorems   is deferred to the appendix which is based on
the idea of the proof in this section  We construct  
polynomialtime reduction from the  partition problem
 Garey   Johnson    to the sparse optimization problem  Given   set   of    integers          the three partition problem is to determine whether   can be partitioned
into   triplets such that the sum of the numbers in each
subset is equal  This problem is known to be strongly NPhard  Garey   Johnson    The main proof idea bears
  similar spirit as the works by Huo   Chen   Chen
et al    and Chen   Wang   The proofs of all
the lemmas can be found in the appendix 
We  rst illustrate several properties of the penalty function
if it satis es the conditions in Theorem  
Lemma   If      satis es the conditions in Theorem  
then for any       and any               tl      we have
             tl    min            tl      
Lemma   If      satis es the conditions in Theorem  
then there exists           such that    is concave
but not linear on     and is twice continuously differentiable on       Furthermore  for any            let
    min                 Then for any        
      and any               tl such that          tl       we
have

             tl             

only if  ti           for some   while  tj      for all    cid    
where            

   

 

In our proof of Theorem   we will consider the following
function

                                    

with         where   is an arbitrary  xed rational number
in       We have the following lemma about     
Lemma   If      satis es the conditions in Theorem  
      and   satis es the properties in Lemma   then
there exist       and       such that for any       and
          the following properties are satis ed 
    cid cid 

        for any          

       has   unique global minimizer       

     

  Let     min                     then for
any         we have                 only if
              where      is the minimal value
of     

Lemma   If      satis es the conditions in Theorem  
      and   satis es the properties in Lemma   then
there exist       such that for any       the following
properties are satis ed 

    cid 

        for any           and   cid 
any            

        for

       has   unique global minimizer           

     

  Let     min                then for any        

we have                 only if           

By combining the above results  we have the following
lemma  which is useful in our proof of Theorem  
Lemma   Suppose      satis es the conditions in Theorem   and   satis es the properties in Lemma   Let
     and      be as de ned in Lemma   and Lemma
  respectively for the case       and       Then we can
 nd   and   such that for any                  tl     

  tj       

tj    

      

 cid 

  cid 

  

  cid 

  

 cid cid cid cid cid cid    cid 

  

 cid cid cid cid cid cid    cid 

  

 cid cid cid cid cid cid  

tj

  

     

 cid cid cid cid cid cid    cid 

 cid cid cid cid cid cid  
 cid   
      
 cid cid cid cid cid cid  

 cid cid cid cid cid cid    cid 

   

  

 cid cid cid cid cid cid  

Moreover  let     min
      
where    is de ned in Lemma   then for any        
we have

 

 

     

  tj     

tj

tj    

          

 
holds only if  ti            for some   while  tj     
for all    cid    

Proof of Theorem   We present   polynomial
time reduction to problem   from the  partition problem 
For any given instance of the  partition problem with
                     we consider the minimization problem
minx       in the form of   with      xij         

Strong NPHardness for Sparse Optimization with Concave Penalty Functions

       

               where

 cid cid cid cid cid     cid 
  cid 
bixij      cid 
 cid cid cid cid cid cid 
    cid 
   cid 

  

  

  

  

  

 
 

 

bixi 

xij    

 cid cid cid cid cid cid  

xij

 

 cid cid cid cid cid  
 cid cid cid cid cid cid  

 cid cid cid cid cid cid 
  cid 
   cid 
  cid 

  

 
 

   cid 

  

   

  

  

  xij 

Note that the lower bounds     and   only depend on
the penalty function    we can choose       and    
  if       or       and       if       such that
   and    are both rational numbers  Since   is
also rational  all the coef cients of       are of  nite size
and independent of the input size of the given  partition
instance  Therefore  the minimization problem minx      
has polynomial size with respect to the given  partition
instance 
For any    by Lemma  

  xij       

xij

 cid cid cid cid cid cid    cid 

  

 cid cid cid cid cid cid  

 

 cid    cid 

  

                 cid 
 cid cid cid cid cid cid    cid 

     

  

  

 cid cid cid cid cid cid   cid 

xij    

            

Now we claim that there exists an equitable partition to the
 partition problem if and only if the optimal value of      
is smaller than                where   is speci ed later 
On one hand  if   can be equally partitioned into   subsets 
then we de ne

 cid      

xij  

 

if bi belongs to the jth subset 
otherwise 

It can be easily veri ed that these xij   satisfy        
           Then due to   we know that these xij  
provide an optimal solution to       with optimal value
          
On the other hand  suppose the optimal value of       is
           and there is   polynomialtime algorithm
that solves   Then for

 cid 

 
   bi

 cid  
 cid   

 

    min

    min

where

 cid 

   

and

        

    min     

 

 
            

 

        

 

 

 cid 

   

 

 

we are able to  nd   nearoptimal solution   such that
                       within   polynomial time of
log  and the size of       which is polynomial with
respect to the size of the given  partition instance  Now
we show that we can  nd an equitable partition based on
this nearoptimal solution  By the de nition of          
               implies

 cid cid cid cid cid cid    cid 

  

 cid cid cid cid cid cid  

 cid cid cid cid cid cid    cid 

  

 cid cid cid cid cid cid  

 

  xij     

     

xij

xij    

  

         

                  

  cid 

According to Lemma   for each                     implies that there exists   such that  xik            and
 xij      for any    cid     Now let

 cid      

 

yij  

if  xik           
if  xij     

 

 cid cid cid cid cid 

yi 

    

  bi

We de ne   partition by assigning bi to the jth subset Sj if
yij        Note that this partition is wellde ned since
for each    by the de nition of   there exists one and only
one yik        while the others equal   Now we show
that this is an equitable partition 
Note that for any                  the difference between the
sum of the jth subset and the  rst subset is

 cid cid cid cid cid   

 

bi

bi

 

  

  

  

  

  

 

Sj

  

Sj

  

yij

biyi 

    

    

    

 cid 

 cid 

 cid cid cid cid cid cid 
 cid cid cid cid cid cid 

bi    yij   xij 

By triangle inequality  we have

bi  cid 

bi  cid 
   cid 

 cid cid cid cid cid cid   
 cid cid cid cid cid     cid 
 cid cid cid cid cid cid   

  bi      cid 
 cid cid cid cid cid     cid 
biyij      cid 
 cid     cid 
 cid cid cid cid cid     cid 
bixij      cid 
know that cid cid cid cid cid    cid 
 cid cid cid cid cid          
 cid cid cid cid cid cid 
 cid 
 cid 
  cid 
bi  cid 
 cid 
Now since bi   are all integers  we must have cid 
 cid 

bixij     cid 
 cid cid cid cid cid cid   

bi    yi    xi   

Therefore  we have

    

bixi 

 
 

bi  

 

  

  

  

Sj

  

  

 

  

  

bi

 

Sj
bi  which means that the partition is equitable 

  

bixi 

By the de nition of yij  we have  yij   xij      for any
      for the last term  since                        we

   

bi  

 cid 

 

 cid cid cid cid cid 

Strong NPHardness for Sparse Optimization with Concave Penalty Functions

References
Amaldi     and Kann     On the approximability of minimizing nonzero variables or unsatis ed relations in linear systems  Theoretical Computer Science   
   

Antoniadis     and Fan     Regularization of wavelet approximations  Journal of the American Statistical Association     

Arora     Babai     Stern     and Sweedy     The hardness
of approximate optima in lattices  codes  and systems
In Foundations of Computer Sciof linear equations 
ence    Proceedings   th Annual Symposium on 
pp    IEEE   

Bian     and Chen     Optimality conditions and complexity for nonlipschitz constrained optimization problems 
Preprint   

Cameron        and Trivedi        Regression analysis
of count data  volume   Cambridge university press 
 

Candes     Wakin     and Boyd     Enhancing sparsity by
reweighted    minimization  Journal of Fourier Analysis and Applications     

Chartrand     Exact reconstruction of sparse signals via
Signal Processing Letters 

nonconvex minimization 
IEEE     

Fan 

   Liu     Sun     and Zhang    

TAC
for sparse learning  Simultaneous control of algoritharXiv preprint
mic complexity and statistical error 
arXiv   

Foster     Karloff     and Thaler     Variable selection is

hard  In COLT  pp     

Frank        and Friedman          statistical view of some
chemometrics regression tools  Technometrics   
   

Garey        and Johnson         Strong NPcompleteness
results  Motivation  examples  and implications  Journal
of the ACM  JACM     

Huber        Robust estimation of   location parameter  The
Annals of Mathematical Statistics     

Huo     and Chen     Complexity of penalized likelihood
estimation  Journal of Statistical Computation and Simulation     

Loh       and Wainwright        Regularized Mestimators
with nonconvexity  Statistical and algorithmic theory for
In Advances in Neural Information Prolocal optima 
cessing Systems  pp     

Lv     and Fan       uni ed approach to model selection
and sparse recovery using regularized least squares  The
Annals of Statistics       

McCullagh     Generalized linear models  European Jour 

nal of Operational Research     

Chen     Ge     Wang     and Ye     Complexity of unconstrained      Lp minimization  Mathematical Programming     

Natarajan        Sparse approximate solutions to linear
systems  SIAM journal on computing   
 

Chen     and Wang     Hardness of approximation for
sparse optimization with    norm  Technical Report 
 

Davis     Mallat     and Avellaneda     Adaptive greedy
approximations  Constructive approximation   
   

Fan     and Li     Variable selection via nonconcave peJournal
nalized likelihood and its oracle properties 
of the American Statistical Association   
   

Wang     Liu     and Zhang     Optimal computational
and statistical rates of convergence for sparse nonconvex learning problems  Annals of statistics   
 

Xue     Zou     Cai     et al  Nonconcave penalized
composite conditional likelihood estimation of sparse
ising models  The Annals of Statistics   
 

Zhang       Nearly unbiased variable selection under
minimax concave penalty  The Annals of Statistics   
     

Fan     and Lv       selective overview of variable selection
in high dimensional feature space  Statistica Sinica   
   

Zhang     Analysis of multistage convex relaxation for
sparse regularization  Journal of Machine Learning Research       

Fan     Xue     and Zou     Strong oracle optimality
of folded concave penalized estimation  The Annals of
Statistics     

Zhang     Wainwright        and Jordan        Lower
bounds on the performance of polynomialtime algorithms for sparse linear regression  In COLT   

