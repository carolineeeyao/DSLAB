The Statistical Recurrent Unit

Junier    Oliva   Barnab as   oczos   Jeff Schneider  

Abstract

Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been
shown to be highly effective in   myriad of applications  We develop an ungated unit  the statistical recurrent unit  SRU  that is able to learn long
term dependencies in data by only keeping moving averages of statistics  The SRU   architecture is simple  ungated  and contains   comparable number of parameters to LSTMs  yet  SRUs
perform favorably to more sophisticated LSTM
and GRU alternatives  often outperforming one
or both in various tasks  We show the ef cacy
of SRUs as compared to LSTMs and GRUs in an
unbiased manner by optimizing respective architectures  hyperparameters for both synthetic and
realworld tasks 

  Introduction
The analysis of sequential data has long been   staple in
machine learning  Domain areas like natural language
 Zaremba et al    Vinyals et al    speech  Graves
et al    Graves   Jaitly    music  Chung et al 
  and video  Donahue et al    processing have
recently garnered much attention  While the study of sequences itself is broad and may be extended to general
functional analysis  Ramsay   Silverman    most recent success has been from neural network based models 
especially from recurrent architectures 
Recurrent networks are dynamical systems that represent
time recursively  For example  the simple recurrent unit
 Elman    contains   hidden state that itself depends on
the previous hidden state  However  training such networks
has been observed to be dif cult in practice due to exploding and vanishing gradients when propagating error gradients through time  Hochreiter et al    While explod 

 Machine Learning Department  Carnegie Mellon University 
Pittsburgh  PA  USA  Correspondence to  Junier    Oliva  joliva cs cmu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ing gradients can be mitigated with techniques like gradient
clipping and normalization  Pascanu et al    vanishing gradients may be harder to deal with  As   result  sophisticated gated architectures like LongShort Term Memory  LSTM  networks  Hochreiter   Schmidhuber   
and Gated Recurrent Unit  GRU  networks  Cho et al 
  have been developed  These gated architectures contain  memory cells  along with gates to control how much
they decay through time thereby aiding the networks  ability to learn long term dependencies in sequences 
Notwithstanding 
there are still challenges in capturing
long term dependencies in gated architectures  Le et al 
 
In this paper we present   simple ungated architecture  the Statistical Recurrent Unit  that often outperforms these more complicated alternatives  Although
the SRU keeps only simple moving averages of summary
statistics  its novel architecture makes it more adept than
previous gated units for capturing long term information in
sequences and comparing them across different windows
of time  For instance  the SRU  unlike traditional recurrent units  can obtain   multitude of viewpoints of the past
by simple linear combinations of only   few averages  We
shall illustrate the ef cacy of the SRU below using both
realworld and synthetic sequential data tasks 
The structure of the paper is as follows   rst we detail the
architecture of the SRU as well as provide several key intuitions and insights for its design  after  we describe our
experiments comparing the SRU to popular gated alternatives  and we perform    dissective  study of the SRU 
gaining further understanding of the unit by exploring how
various hyperparameters affect performance   nally  we
discuss conclusions from our study 

  Model
The SRU maintains long term sequential dependencies in  
rather intuitive fashion through summary statistics  As the
name implies  statisticians often employ summary statistics
when trying to represent   dataset  Quite naturally then  we
look to an algorithm that itself learns to represent data seen
previously in much the same vein as   neural statistician
 Edwards   Storkey   
Of course  unlike with unordered        samples  simply
averaging statistics of sequential points will lose valuable

The Statistical Recurrent Unit

temporal information  The SRU maintains sequential information in two ways   rst  we generate recurrent statistics
that depend on   context of previously seen data  second 
we generate moving averages at several scales  allowing
the model to distinguish the type of data seen at different
points in the past  We expound on these methods for creating temporallyaware statistics below 
We shall see that the statistical design of the SRU yields  
powerful yet simple model that is able to analyze sequential
data and  on the     create summary statistics for learning
over sequences  Furthermore  through the use of ReLUs
and exponential moving averages  the SRU is able to mitigate vanishing gradient issues that are common to many
recurrent units 

  Recurrent Statistics

 cid  

We consider an input sequence of real valued points
              xT   Rd  As seen in the second row of Table   we can compute   vector of statistics  xi    RD
for each point  Here  each vector  xi  is independent of
other points xj for    cid     One may then average these
    xi  to produce summary statisvectors as      
 
tics of the sequence  This approach amounts to treating
the sequence as   set of        points drawn form some distribution and marginalizing out time  Clearly  here one
will lose temporal information that will be useful for many
sequence related ML tasks 
It is interesting to note that
global average pooling operations have gained   lot of recent traction in convolutional networks  Lin et al   
Iandola et al    Analogously to the        statistic approach  global averaging will lose spatial information  yet
the highlevel summary statistics provide an effective representation  Still  not marginalizing out time should provide   more robust approach for sequence tasks  thus we
consider the following methods for producing statistics 
First  we provide temporal information whilst still utilizing
averages through recurrent statistics that also depend on the
values of previous points  see third row of Table   That
is  we compute our statistics on the ith point xi not only
as   function of xi  but also as   function of the previous
statistics of xi   cid     which itself depends on  cid    etc 

ing illustrative example where xi      and statistics

 cid                   xi         

 cid                      xi         

 
 

 cid  

That is  one records the ith input in the ith index  When averaged the statistics will be  
    cid                      
 
the complete sequence  Such recurrent statistics will undoubtedly suffer from the curse of dimensionality  Hence 
we consider   more restrictive model of recurrent statistics
which we expound on below  
Second  we provide even more temporal information by
considering summary statistics at multiple scales  As   simple hypothetical example  consider taking multiple means
across separate time windows  for instance taking means
over indices   then over indices   etc  Such an
approach   will illustrate how summary statistics evolve
through time 

 cid 

 cid cid 

 

 cid 

 cid 

 cid cid 

 

 cid 

           

             

         

 

In practice  we shed light on the dynamics of statistics
through time by using several averages of the same summary statistics  The SRU will use exponential moving averages       cid       to compute means  hence  we
consider multiple weights by taking the exponential means
at various scales              as shown in the last row of
Table   Later we show that this multiscaled approach is
capable of   combinatorial number of viewpoints of past
statistics through simple linear combinations 

Table   Methods for keeping statistics of sequences 

inputs

      

statistics

recurrent
statistics

              xT

                 xT  

       cid         cid           xT    cid   

recurrent

multiscaled

statistics

   

 

   cid      

 

   cid   

 

   
     cid      

     cid   

 cid         cid   cid         cid       

 

  Update Equations

where   is   function for producing statistics given the
current point and previous statistics  and  cid  is   constant
initial vector for convention  We note that from   general
standpoint if given    exible model and enough dimensions  then recurrent summary statistics like   can perfectly encode ones sequence  Take for instance the follow 

We have discussed in broad terms how one may create
temporallyaware summary statistics through multiscaled
recurrent statistics  Below  we cover speci cally how the
SRU creates and uses summary statistics for sequences 
Recall that our input is   sequence of ordered points 
              xt   Rd  Throughout  we apply an element 

The Statistical Recurrent Unit

wise nonlinearity     which we take to be the ReLU
 Jarrett et al    Nair   Hinton         
max    The SRU operates via exponential moving averages      Rs   kept at various scales        
              where          These moving averages 
  are of recurrent statistics     that are dependent
not only on the current input but also on features of averages      The moving averages are then concatenated
as                   and used to create an output  
  that is fed upwards in the network 

  Intuitions from Mean Map Embeddings

The design of the SRU is deliberately chosen to allow for
long term dependencies to be learned  To better elucidate
the design and its intuition  let us take   brief excursion to
another use of  summary  statistics in machine learning for
the representation of data  mean map embeddings  MMEs 
of distributions  Smola et al    At its core  the concept of MMEs is that one may embed  and thereby represent    distribution through statistics  such as moments 
The MME for   distribution   given   positive semide nite
kernel   is 

      EX          

 

where    are the reproducing kernel Hilbert space  RKHS 
features of    which may be in nite dimensional  To represent   set                 yn  iid    one would use an
empirical mean version of the MME 

  cid 

  

      

 
 

   yi 

 

Numerous works have shown success in representing distributions and sets through MMEs  Muandet et al   
One interpretation for the design of SRUs is that we are
modifying MME   for use on sequences  Of course  one
way of applying MMEs directly on sequences is to simply
ignore the noni      nature of sequences and treat points as
comprising   set  This however loses important sequential
information  as previously mentioned  Below we discuss
the speci   modi cations we make from traditional MMEs
and the bene ts they yield 

  DATADRIVEN STATISTICS

First  we note the clear analogue between the mean embedding of   set            and the moving average    
The moving averages   are clearly serving as summary
statistics of previously seen data  However  the statistics
we are averaging for       are not comprised of apriori RKHS features as is typical with MMEs  but rather
are learned nonlinear features  This has the bene   of using datadriven statistics  and may be interpreted as using  
linear kernel in the learned features 

  RECURSIVE STATISTICS FROM THE PAST

Second  recall that typical MMEs use statistics that depend
only on   single point          As aforementioned this
is  ne for        data  however it loses sequential information when averaged  Instead  we wish to assign statistics
that depend on the data we have seen so far  since it provides context for one   current point in the sequence  For
instance  one may want to have   statistic that keeps track
of the difference between the current point and the mean

Figure   Graphical representation of the SRU  Solid lines indicate   dependence on the current value of   node  Dashed lines
indicate   dependence on the previous value of   node  We see
that both the current point xt as well as   summary of the previous data rt are used to make statistics     which in turn are used in
moving averages      nally an output ot is feedforward through
the rest of the network 

We detail the update equations for the SRU below  and in
Figure  

rt    

 cid 

              cid 
 cid 
   rt        xt     cid 
 cid 
             cid 

           

     

 

      
        

ot    

 

 

 

 

In practiced we noted that it suf ces to use only   few   
such as              
It is worth noting that previous work has considered capturing recurrent information at various timescales in the
past  For instance  Koutnik et al    considers an RNN
scheme that divides the hidden state into different modules for use at different frequencies  Furthermore  exponential averages in recurrent units have been considered
previously        Mikolov et al    Bengio et al   
Jaeger et al    However  such works are more akin to
ungated GRUs since they consider only one scale per feature  limiting the views available per statistic to just one 
The use of ReLUs in recurrent units has also been recently
explored by Le et al    however there no statistics
are kept and their use is limited to the simple RNN when
initialized in   special manner 

The Statistical Recurrent Unit

of previous data  We provide   context based on previous data by making the statistics considered at time      
    function not only of xt but also of             xt 
through rt   rt may be interpreted as   condensation of
the sequence seen so far  and allows us to keep sequential
information even through an averaging operation 

  MULTISCALED STATISTICS

Third  the use of multiscaled moving averages of statistics
gives the SRU   simple and powerful rich view of past data
that is unique to this recurrent unit  In short  by keeping
moving averages at different scales               we are
able to uncover differences in statistics at various times in
the past  Note that we may unroll moving averages as 

         cid                      cid 

 

 

Thus    smaller   weighs current statistics more than
older statistics  hence    concatenated vector    
              itself provides   multiscale view of
statistics through time  see Figure   For instance  keeping statistics for short and long terms pasts already yields
information on the evolution of the sequence through time 

Figure   We may unroll the moving average updates as   To
visualize the different emphasis in the past that varying   has on
statistics we plot the values of weights in moving averages      
    for   points in the past across rows  We see that alpha
values closer to   focus only on the recent past  where values close
to   maintain an emphasis on the distant past as well 

  Viewpoints of the Past

An interesting and useful property of keeping multiple
scales for each statistic is that one can obtain   combinatorial number of viewpoints of the past through simple linear
combinations of ones statistics  For instance  for properly
chosen wj  wk      wj    wk    provides an aggregate of statistics from the past for          Figure   Of
course  more complicated linear combinations may be performed to obtain richer viewpoints that are comprised of
multiple windows  Furthermore  by using   linear projection of our statistics     as we do with ot   we are able to
compute output features of combined viewpoints of several
statistics 
This kind of multiviewpoint perspective of previously seen
data is dif cult to produce in traditional gated recurrent

Figure   We visualize the power of taking linear combinations
of    for providing different viewpoints into past data  In row
  we show the effective weights that would be used for weighing
statistics    if one considers       we
see that this is equivalent to considering only statistics from the
distant past  Similarly  we show the effective weights when taking
    and     on rows   and
  respectively  We see that these linear combinations amount to
considering viewpoints concentrated at various points in the past 
Lastly its worth noting that more complicated linear combinations
may lead to even richer views on previous statistics  for instance 
we show      
    on row   which
concentrates on the statistics of the distant and very recent past 
but deemphasizes statistics of data from less recent past 

units since they must encode where in the sequence they
currently are and then store an activation on separate nodes
per each viewpoint for future use  SRUs  on the other hand 
only need to take simple linear combinations to capture various viewpoints in the past  For example  as shown above 
statistics from just the distant past are available via   simple subtraction of two moving averages  Figure   row  
Such   windowed view would require   gated unit to learn
to stop averaging after   certain point in the sequence  and
the corresponding statistic would not yield any information
outside of this window  In contrast  each statistic kept by
the SRU provides   combinatorial number of varying perspectives in the past through linear combinations and their
multiscaled nature 

  Vanishing Gradients

As previously mentioned  it has been shown that vanishing gradients make learning recurrent units dif cult due
to an inability to propagate error gradients through time 
Notwithstanding its simple ungated structure  the SRU
features several safeguards to alleviate vanishing gradients 
First  units and statistics are comprised of ReLUs  ReLUs
have been observed to be easier to train for general deep
networks  Nair   Hinton    and have had success in
recurrent units  Le et al    Intuitively  ReLUs allow
for the propagation on error on positive inputs without saturation and vanishing gradients as with traditional sigmoid
units  The ability of the SRU to use ReLUs  without any
special initialization  makes it especially adept at learning
long term dependencies through time 
Furthermore  the explicit moving average of statistics al 

lows for longer term learning  Consider the following
derivative of the error signal          an element
of the unit   moving averages when         

 
  

 

The Statistical Recurrent Unit

 cid 

 cid 

  Synthetic Recurrent Unit Generated Data

First we provide evidence that traditional gated units have
dif culties capturing the same type of multiscale recurrent
statistic based dependencies that the SRU offers  We show
the relative inef ciency of traditional gated units at learning long term dependencies of statistics by considering   
synthetic data from   ground truth SRU 

 

iid        and xt
We begin the sequences with   
is the results of   projection of ot  We generate   total of
  points per sequence for   training sequences   
validation sequences  and   testing sequences 
The ground truth statistical recurrent unit has three statistics      the positive part of inputs     the negative
part of inputs     and an internal statistic     We use
               Denote  
       
   
     
as the moving averages using   for each respective statistic  The internal statistic   does not get used
 through rt   in updating the statistics for     or    
  is itself updated as 

zt    zt 
     
 

 cid 
 cid 
 cid         
 cid 
 cid 
 cid 
 cid 
 cid 
     
     
         
ot  cid xt   xt  vT

where each of the summands are rt features  Furthermore
we have ot       

     

              vT

  

 cid   

 

 

 

 

 

 

 

where vj   where initialized and  xed as  vj  
       
    Finally the next point is generated as 

xt     xt     xt    wT ot 

iid 

iid        and

where   was initialized and  xed as     
ot  are the last   dimensions of ot 
After the ground truth SRU was constructed we generated
the training  validation  and testing sequences  As can be
seen in Figure   the sequences follow   simple pattern 
at the start negative values are quickly pushed to zero and
positive values follow   parabolic line until hitting zero  at
which point they slope downward depending on initial values  While simple  it is clear that trained recurrent units
must be able to hold longterm information since all sequences converge at one point and future behaviour depends on initial values 
 cid 
We look to minimize the mean of squared errors
 MSE 
the loss we consider per sequence is
that
    xt    pt  where pt is the output of the net 
 
 
work after being fed xt  We conducted   trials of hyperparameter optimization as described above and obtained
the following results in Table  

is 

 cid 
 cid 

 cid 
 cid 

 cid 

 

  
 
  

 cid 

 

 

 

 

 
 

 
  

 cid 

  
 
 

 cid 

 

 cid 

  
 
 

 cid 

 

 

   

 

 

 

 

That is  the factor   directly controls the decay of the error
signal through time  Thus  by including an   explicitly near
          the decay for that moving average can be
made minuscule for the lengths of sequences in ones data 
Also  it is interesting to note that  with   large   near  
SRUs with ReLUs can implement part of the functionality
of   gate  remembering  by carrying through the previous
moving average  
    when the corresponding statistic
     has be zeroed out   The other functionality of  
gate  forgetting  can be had by including an   near   if the
ReLU statistic is not zeroed out  then the moving average
for   small   will  forget  the previous value 

  Experiments
We compared the performance of the SRU  to two popular
gated recurrent units  the GRU and LSTM unit  All experiments were performed in Tensorflow  Abadi et al 
  and used the standard implementations of GRUCell
and BasicLSTMCell for GRUs and LSTMs respectively 
In order to perform   fair  unbiased comparison of the recurrent units and their hyperparameters  which greatly affect performance  Bergstra   Bengio    we used the
Hyperopt  Bergstra et al    hyperparameter optimization package  We believe that such an approach gives
each algorithm   fair shot to succeed without injecting biases from experimenters or imposing gross restrictions on
architectures considered 
In all experiments we used SGD for optimization using gradient clipping  Pascanu et al    with   norm of   on all
algorithms  Unless otherwise speci ed   trials were performed to search over the following hyperparameters on  
validation set  one  initial learning rate the initial learning rate used for SGD  in range of  exp   
two  lr decay the multiplier to multiply the learning
rate by every    iterations  in range of     three 
dropout keep rate  percent of output units that are
kept during dropout  in range     four  num units
number of units for recurrent unit  in             In addition  the following two parameters were searched over
for the SRU  num stats  the dimensionality of    
in             summary dims  the dimensionality of
    in            

 See

recurrent for code 

https github com junieroliva 

The Statistical Recurrent Unit

Figure     sequences generated from the ground truth SRU
model 

Table   MSEs for synthetically generated dataset 

SRU GRU
 
 

LSTM
 

Error

Not surprisingly  the SRU performs far better than traditional gated recurrent units  This suggests that the types
of longterm statistical relationships captured by the SRU
are indeed different than those of traditional recurrent units 
As previously mentioned  the SRU is able to obtain   multitude of different views from its statistics    task that traditional units achieve less ef ciently since they must devote
one whole memory cell per viewpoint and statistic  As we
show below  the SRU is able to outperform traditional gated
units in long term problems even for real data that is not
generated from its model class 

  MNIST Image Classi cation

Next we explore the ability of recurrent units to use longterm dependencies in ones data with   synthetic task using   real dataset  It has been observed that LSTMs perform poorly in classifying   long pixelby pixel sequence
of MNIST digits  Le et al    In this synthetic task 
each   grayscale MNIST digit image is  attened and
observed as   sequence                where xi      
 see Figure   The task is  based on the output observed after feeding    through the network  to classify the digit of
the corresponding image in             Hence  we project
the output after    of each recurrent unit to   dimensions and use   softmax activation 
We report the hyperparameter optimized results below in
Table   due to resource constraints each trial consisted
only of    training iterations  We see that the SRU is
able to outperform both GRUs and LSTMs  Given the
long length and dependencies of pixel sequences in this
experiment  it is not surprising that SRUs  abilities to capture longterm dependencies are aiding it to achieve   much
lower error 

Table   Test error rate for MNIST pixel sequence classi cation 

Error Rate

SRU GRU LSTM
 
 

 

Figure   Right  example MNIST       image  which is taken
as   pixelby pixel sequence of length   unrolled as shown in
yellow  Left  example pixel sequences for     and   digit images 

  DISSECTIVE STUDY

the architecture 

Next  we study the behavior of the statistical recurrent unit with   dissective study where we vary sevWe consider
eral parameters of
variants to   base model with 
num stats 
  dims  num units  We keep the parameters
initial learning rate  lr decay  xed at the the
optimal values found     respectively  unless we  nd
no learning  in which case we also try learning rates of  
and  

The need for multiscaled recurrent statistics  Recall
that we designed the statistics used by the SRU expressly
to capture long term time dependencies in sequences  We
did so both with recurrent statistics       statistics that themselves depend on previous points  statistics  and with multiscaled averages  We show below that both of these timedependent design choices are vital to capturing long term
dependencies in data  Furthermore  we show that the use
of ReLU statistics lends itself to better learning 
We explored the impact that timedependent statistics had
on learning by  rst considering naive        summary statistics for sequences  This was achieved by using   dims 
and           Here no pastdependent context
is used for statistics       we used       type statistics as is
typical for unordered sets  Furthermore  the use of   single
scale   near   means that all of the points  statistics will
be weighted nearly identically   regardless of index  We
optimized the SRU when using no recurrent statistics and
  single scale  iid  when using recurrent statistics with  
single scale  recur  and when using no recurrent statistics with multiple scales  multi  We report errors below
in Table  

Table   Test error rate for MNIST pixel sequence classi cation 

Error Rate

iid
 

recur

multi

 

 

The Statistical Recurrent Unit

Predictably  we cannot learn by simply keeping        type
statistics of pixel values at   single scale  Furthermore  we
 nd that only using recurrent statistics  recur  in the SRU
is not enough  It is interesting to note  however  that keeping        statistics at multiple scales is able to predict digits
with limited success  This lends evidence for the need of
both recurrent statistics and multiple scales 
Next  we explored the effects of the scales at which
we keep our statistics by varying from        
          considering        
                      We see in
Table   that additional  longer scales aid our learning for
this dataset  This is not very surprising given the long term
nature of the pixel sequences 

Table   Test error rate for MNIST pixel sequence classi cation 

 

Error Rate

     

       

 

 

Lastly  we considered the use of nonReLU statistics by
changing the elementwise nonlinearity       to be
the hyperbolic tangent       tanh  We postulated that
the use of ReLUs would help our learning since they have
been observed to better handle the problem of vanishing
gradients  We  nd evidence of this when swapping ReLUs
for hyperbolic tangent units in SRUs  we get an error rate
of   when using hyperbolic tangent units  Although the
previous uses of ReLUs in RNN required careful initialization  Le et al    SRUs are able to use ReLUs for better
learning without any special considerations 

Dimension of recurrent summary  Next we explore the
effect of varying the number of dimensions used for the recurrent summary of statistics rt   We consider   dims
in       As previously discussed rt provides   context based on past data so that the SRU may produce noni      statistics as it moves along   sequences  As one would
expect the dimensionality of rt will limit the information
 ow from the past and values that are too small will hinder
performance  It is also interesting to see that after enough
dimensions  there are diminishing returns to adding more 

Table   Test error rate varying recurrent summary rt 

  dims
Error Rate

 

 

 
 

 
 

Table   Test error rate varying number of units 

num stats
 
 
 
 

units
 
 
 
 

Error Rate

  Polyphonic Music Modeling

Henceforth we consider real data and sequence learning
tasks  First  we used the polyphonic music datasets from
BoulangerLewandowski et al    Each timestep is  
binary vector representing the notes played at the respective
timestep  Since we were required to predict binary vectors
we used the elementwise sigmoid        the binary vector of notes xt  was modeled as    pt  where pt is the
output after feeding xt  and previous values            xt 
through the recurrent network 
It is interesting to note in Table   that the SRU is able to outperform one of the traditional gated units in every dataset
and it outperforms both in two datasets 

Table   Test negative loglikelihood for polyphonic music data 

SRU
Data set
 
JSB
Muse
 
Nottingham  
 
Piano

GRU LSTM
 
 
 
 
 
 
 
 

  ElectronicaGenre Music MFCC

In the following experiment we modeled the Mel frequency cepstrum coef cients  MFCCs  in   dataset of
nearly     scraped    sound clips of electronicagenre
songs  MFCCs are perceptually based spectral features
positioned logarithmically on the mel scale  which approximates the human auditory system   response    uller 
  We looked to model the   realvalued coef cients
using the recurrent units  by modeling xt  as   projection
of the output of   recurrent unit after being fed            xt 

Table   Testset MSEs of MFCC Music data 

SRU
 

GRU LSTM
 
 

Error

As can be seen in Table   SRUs again are outperforming
gated architectures and are especially beating GRUs by  
wider margin 

Number of statistics and outputs  Finally  we vary the
number of statistics num stats  and outputs units  Interestingly the SRU seems robust to the number of outputs
propagated in the network  However  performance is considerably affected by the number of statistics considered 

  Climate Data

Next we consider weather data prediction using the North
America Regional Reanalysis  NARR  Project  NAR  The
dataset provides   longterm set of consistent climate data
on   regional scale for the North American domain  The

The Statistical Recurrent Unit

period of the reanalyses is from October   to the present
and analyses were made   times daily   hour intervals 
We take our input sequences to be yearlong sequences of
weather variables in   location for the year        an input sequence will be     length sequence of weather
variables at   given lat lon coordinate  We considered
the following   variables  pres        pressure  pa 
tcdc  total cloud cover   rh    relative humidity
     tmpsfc  surface temperature     snod  snow
depth surface     ugrd      component of wind   
above ground  vgrd      component of wind    above
ground  The variables were standardized  see Figure   for
example sequences 

Figure   Two example sequences for weather variables at distinct
locations for the year  

Below we see results using     training location sequences and     validation and testing instances  Again 
we look to model the next point in   sequence as   projection of the output of the recurrent unit after feeding the
previous points  One may see in Table   that SRUs and
LSTMs perform nearly identically  perhaps the cyclical nature of climate data was bene cial to the gated units 

Table   Test MSEs for weather data 

SRU
 

GRU LSTM
 
 

Error

  SportVu NBA Tracking data

Finally  we look to predict the positions of National Basketball Association  NBA  players based on previous court positions during   play  Optical tracking data for this project
were provided by STATS LLC from their SportVU product
and obtained from  NBA  The data are composed of   and
  coordinates for each of the ten players and the ball  We
again minimize the squared norm of errors for predictions 

Table   Testset MSEs of NBA data 
LSTM
 

GRU
 

SRU
 

Error

We observed   large margin of improvement for SRUs over
gated architectures in Table   that is reminiscent of the
synthetic data experiment in   This suggests that this

Figure   Example player ball      positions for two plays 

dataset contains long term dependencies that the SRU is
able to exploit 

  Discussion
We believe that the use of summary statistics has been
underexplored in modern recurrent units  Although recent
studies in convolutional networks have considered global
average pooling  which is essentially using highlevel summary statistics to represent images  there has been little exploration of summary statistics for modern recurrent networks  To this end we introduce the Statistical Recurrent
Unit    novel architecture that seeks to capture long term
dependencies in data using only simple moving averages
and recti edlinear units 
The SRU was motivated by the success of meanmap embeddings for representing unordered datasets  and may be
interpreted as an alteration of MMEs for sequential data 
The main modi cations are as follows   rst  the SRU uses
datadriven statistics unlike typical MMEs  which will use
RKHS features from an apriori selected class of kernels 
second  SRUs will use recurrent statistics that are dependent not only on   current point  but on previous points 
statistics through   condensation of kept moving averages 
third  the SRU will keep moving averages at various scales 
We provide evidence that the combination of these modi 
 cations yield much better results than any one of them in
isolation 
The resulting recurrent unit is especially adept for capturing long term dependencies in data and readily has access
to   combinatorial number of viewpoints of past windows
through simple linear combinations  Moreover  it is interesting to note that even though the SRU is gateless  it
may implement part of both  remembering  and  forgetting  functionalities through ReLUs and moving averages 
We showed empirically that the SRU is better equipped that
traditional gated units for long term dependencies via synthetic and realworld data experiments 

Acknowledgements

research is

This
DESC  and NSF grant IIS 

supported in part by DOE grant

The Statistical Recurrent Unit

References
Ncep north american regional

reanalysis 

https 

 data noaa gov dataset ncepnorth 
americanregional reanalysisnarr 
for topresent  Accessed   

Nba movement data 

https github com 
sealneaward nbamovement data  Accessed 
 

Abadi  Mart    Barham  Paul  Chen  Jianmin  Chen 
Zhifeng  Davis  Andy  Dean  Jeffrey  Devin  Matthieu 
Ghemawat  Sanjay  Irving  Geoffrey  Isard  Michael 
et al  Tensor ow    system for largescale machine
In Proceedings of the  th USENIX Sympolearning 
sium on Operating Systems Design and Implementation
 OSDI  Savannah  Georgia  USA   

Bengio  Yoshua  BoulangerLewandowski  Nicolas  and
Pascanu  Razvan  Advances in optimizing recurrent
networks  In Acoustics  Speech and Signal Processing
 ICASSP    IEEE International Conference on  pp 
  IEEE   

Bergstra  James and Bengio  Yoshua  Random search
for hyperparameter optimization  Journal of Machine
Learning Research   Feb   

Bergstra  James  Komer  Brent  Eliasmith  Chris  Yamins 
Dan  and Cox  David    Hyperopt    python library for
model selection and hyperparameter optimization  Computational Science   Discovery     

BoulangerLewandowski  Nicolas  Bengio  Yoshua  and
Vincent  Pascal  Modeling temporal dependencies in
highdimensional sequences  Application to polyphonic
arXiv preprint
music generation and transcription 
arXiv   

Cho  Kyunghyun  Van Merri enboer  Bart  Bahdanau 
Dzmitry  and Bengio  Yoshua  On the properties of neural machine translation  Encoderdecoder approaches 
arXiv preprint arXiv   

Chung  Junyoung  Gulcehre  Caglar  Cho  KyungHyun 
and Bengio  Yoshua  Empirical evaluation of gated recurrent neural networks on sequence modeling  arXiv
preprint arXiv   

Donahue  Jeffrey  Anne Hendricks  Lisa  Guadarrama 
Sergio  Rohrbach  Marcus  Venugopalan  Subhashini 
Saenko  Kate  and Darrell  Trevor  Longterm recurrent convolutional networks for visual recognition and
In Proceedings of the IEEE conference
description 
on computer vision and pattern recognition  pp   
   

Edwards  Harrison and Storkey  Amos  Towards   neural

statistician  arXiv preprint arXiv   

Elman  Jeffrey    Finding structure in time  Cognitive sci 

ence     

Graves  Alex and Jaitly  Navdeep  Towards endto end
In

speech recognition with recurrent neural networks 
ICML  volume   pp     

Graves  Alex  Mohamed  Abdelrahman  and Hinton  Geoffrey  Speech recognition with deep recurrent neural networks  In Acoustics  speech and signal processing  icassp    ieee international conference on  pp 
  IEEE   

Hochreiter  Sepp and Schmidhuber    urgen  Long shortterm memory  Neural computation   
 

Hochreiter  Sepp  Bengio  Yoshua  Frasconi  Paolo  and
Schmidhuber    urgen  Gradient  ow in recurrent nets 
the dif culty of learning longterm dependencies   

Iandola  Forrest    Han  Song  Moskewicz  Matthew   
Ashraf  Khalid  Dally  William    and Keutzer  Kurt 
Squeezenet  Alexnetlevel accuracy with    fewer
arXiv preprint
parameters and    mb model size 
arXiv   

Jaeger  Herbert  Luko sevi cius  Mantas  Popovici  Dan  and
Siewert  Udo  Optimization and applications of echo
state networks with leakyintegrator neurons  Neural
networks     

Jarrett  Kevin  Kavukcuoglu  Koray  LeCun  Yann  et al 
What
is the best multistage architecture for object
recognition  In Computer Vision    IEEE  th International Conference on  pp    IEEE   

Koutnik  Jan  Greff  Klaus  Gomez  Faustino  and Schmidhuber  Juergen    clockwork rnn  pp     

Le  Quoc    Jaitly  Navdeep  and Hinton  Geoffrey     
simple way to initialize recurrent networks of recti ed
linear units  arXiv preprint arXiv   

Lin  Min  Chen  Qiang  and Yan  Shuicheng  Network in

network  arXiv preprint arXiv   

Mikolov  Tomas  Joulin  Armand  Chopra  Sumit  Mathieu 
Michael  and Ranzato  Marc Aurelio  Learning longer
memory in recurrent neural networks  arXiv preprint
arXiv   

Muandet  Krikamol  Fukumizu  Kenji  Sriperumbudur 
Bharath  and Sch olkopf  Bernhard  Kernel mean embedding of distributions    review and beyonds  arXiv
preprint arXiv   

The Statistical Recurrent Unit

  uller  Meinard  Information retrieval for music and mo 

tion  volume   Springer   

Nair  Vinod and Hinton  Geoffrey    Recti ed linear units
improve restricted boltzmann machines  In Proceedings
of the  th international conference on machine learning
 ICML  pp     

Pascanu  Razvan  Mikolov  Tomas  and Bengio  Yoshua 
On the dif culty of training recurrent neural networks 
ICML      

Ramsay       and Silverman       Applied functional data
analysis  methods and case studies  volume   Springer
New York   

Smola  Alex  Gretton  Arthur  Song  Le  and Sch olkopf 
Bernhard    hilbert space embedding for distributions 
In International Conference on Algorithmic Learning
Theory  pp    Springer   

Vinyals  Oriol  Kaiser   ukasz  Koo  Terry  Petrov  Slav 
Sutskever  Ilya  and Hinton  Geoffrey  Grammar as  
In Advances in Neural Information
foreign language 
Processing Systems  pp     

Zaremba  Wojciech  Sutskever  Ilya  and Vinyals  Oriol 
Recurrent neural network regularization  arXiv preprint
arXiv   

