StronglyTyped Agents are Guaranteed to Interact Safely

David Balduzzi  

Abstract

As arti cial agents proliferate  it is becoming increasingly important to ensure that their interactions with one another are wellbehaved  In this
paper  we formalize   commonsense notion of
when algorithms are wellbehaved  an algorithm
is safe if it does no harm  Motivated by recent
progress in deep learning  we focus on the speci   case where agents update their actions according to gradient descent  The paper shows
that that gradient descent converges to   Nash
equilibrium in safe games  The main contribution is to de ne stronglytyped agents and show
they are guaranteed to interact safely  thereby
providing suf cient conditions to guarantee safe
interactions    series of examples show that
strongtyping generalizes certain key features of
convexity  is closely related to blind source separation  and introduces   new perspective on classical multilinear games based on tensor decomposition 

 First  do no harm 

  Introduction
Recent years have seen rapid progress on core problems
in arti cial intelligence such as object and voice recognition  Hinton   et al    Krizhevsky et al   
playing video and board games  Mnih et al    Silver et al    and driving autonomous vehicles  Zhang
et al    As arti cial agents proliferate  it is increasingly important to ensure their interactions with one another  with humans  and with their environment are safe 
Concretely  the number of neural networks being trained
and used is growing rapidly  There are enormous and increasing economies of scale that can likely be derived from
treating them as populations   rather than as isolated algorithms  How to ensure interacting neural networks cooperate effectively  When can weights trained on one problem

 Victoria University of Wellington  New Zealand  Correspon 

dence to  David Balduzzi  dbalduzzi gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

be adapted to another without adverse effects  The problems fall under mechanism design    branch of game theory
 Nisan et al    However  neural nets differ from humans in that they optimize clear objectives using gradient
descent  The setting is thus more structured than traditional
mechanism design 

Safety  The  rst contribution of the paper is formalize
safety as   criterion on how agents interact  We propose
  basic notion of safety based on the commonsense principle that agents should do no harm to one another  More
formally  each agent optimizes an objective whose value
depends on the actions of the agent and the actions of the
rest of the population    game is safe if the actions chosen
by each agent do no  in nitesimal  harm to any other agent 
where harm is measured as increased loss 
The key simplifying assumption in the paper is to take gradient descent as   computational primitive  Balduzzi 
  Questions about mechanism design are sharpened under the assumption that agents use gradient descent  The assumption holds broadly since the key driver
of progress in arti cial intelligence is deep learning  which
uses gradient descent to optimize complicated objective
functions composed from simple differentiable modules
 LeCun et al   
  weakness of the approach is that it conceives safety more
narrowly than  for example  Amodei et al    which is
concerned with societal risks arising from arti cial intelligence  We argue that   necessary foundational component
of the broader AIsafety project is to clarify exactly what
safety entails when the objectives of the agents and the algorithms they employ are precisely speci ed 

Stronglytyped games  The second contribution is to introduce type systems suited to multiagent optimization
problems  that is  games  We build on the typed linear
algebra introduced in Balduzzi   Ghifary   The
nomenclature is motivated by an analogy with types in the
theory of programming  Type systems are used to prevent
untrapped errors  errors that go unnoticed and cause arbitrary behavior later on  when running   program  Cardelli 
    program is safe if it does not cause untrapped errors  Type systems can enforce safety by statically rejecting
all programs that are potentially unsafe 

StronglyTyped Agents are Guaranteed to Interact Safely

The idea underlying types in programming is that  like
should interact with like  Typed linear algebra  de nition   formalizes  like interacts with like  in the simplest
possible way   by  xing an orthogonal basis  Section   introduces   wider class of games than in the literature and
de nes safety  Theorem   shows that gradient descent converges to   Nash equilibrium in safe games  Section   extracts the key ingredients required for safe gradients from
two warmup examples  The ingredients are simultaneous
diagonalization      
the existence of   shared latent orthogonal basis  and monotonic covariation      
that the
derivatives of the objectives have the same sign in the latent coordinate system  The main result  theorem   is that
stronglytyped games are guaranteed to be safe 

Implications  Safety and strongtyping generalize key
properties of convexity  Convexity is of course the gold
standard for wellbehaved gradients  We uncover latent
types and demonstrate safety of Newton   method  natural
gradient and mirror descent  see sections      and   
The main theme of sections   and   is disentangling latent
factors  We show that strongtyping in quadratic games is
closely related to blind source separation  Section   analyzes classical Nplayer games  The analysis yields   new
perspective on classical games based on tensorSVD that is
closely related to independent component analysis 
Sections   and    switch to neural networks and analyze two biologically plausible variants of backpropagation
 Balduzzi et al    Lillicrap et al    We show that
the main results of the papers are to prove the respective
algorithms are safe 

Scope and related work  This paper lays the foundations
of safety in gradientbased optimization  Applications are
deferred to future work 
The literature on safety is mostly focused on problems arising in reinforcement learning  for example ensuring agents
avoid dangerous outcomes  Turchetta et al    Amodei
et al    Berkenkamp et al    Gradients are typically not available in reinforcement learning problems  We
study interactions between algorithms with clearly de ned
objectives that utilize gradientbased optimization  which
gives   more technical perspective 
The idea of   population of neural networks solving multiple related tasks is developed in Fernando et al   
which uses genetic algorithms to adapt components to new
tasks  However  they repeatedly reinitialize components to
undo the damage done by the genetic algorithm  Our work
is intended  ultimately  to help design algorithms that detect
and avoid damaging updates    recent survey paper argues
the brain optimizes   family of complementary loss functions  Marblestone et al    without considering how

the complementarity of the loss functions could be checked
or enforced 
The idea of investigating gametheoretic and mechanism
design questions speci   to certain classes of algorithms is
introduced in Rakhlin   Sridharan   Syrgkanis et al 
  The papers consider how convergence in games can
be accelerated if the players use variants of mirror descent 

Terminology 
If       then   is positive  if     then it
is strictly positive     not necessarily square  matrix   is
diagonal if dij     for all       and similarly for tensors 
Vectors are columns  The inner product is hv  wi       
  Safety
  Types and orthogonal projections
Let us recall some basic facts about orthogonal projections 
Let         be   vector space equipped with an inner
product  An orthogonal projection is   linear transform
          that is
   idempotent        and
   selfadjoint             hv       for any            
Lemma   Let   denote an        matrix with orthogonal columns            pk  Then the        matrix PP   
Pk
   pihpi     Pk
  is an  orthogonal  projection 
Lemma   If two orthogonal projections   and   commute
then their product is an orthogonal projection 

   pip 

Proof  Let         If         then

Checking selfadjointness is an exercise 

                             
   is   DDe nition     type TV             
dimensional vector space with an inner product and orthogonal projections            such that         
for       andPR
        IV is the identity  Type TV has
dimension   and rank   

  full rank type         is equivalent to   vector space
equipped with an orthogonal basis  Lower rank types are
less rigid  and can be thought of as vector spaces equipped
with generalized orthogonal coordinates 

  Safe games
De nition     game consists of   type TV   feasible set
       players                     losses           
and an assignment                of players to projections 

StronglyTyped Agents are Guaranteed to Interact Safely

The type structure and assignments specify the coordinates
controlled by each player  On round    player   chooses
  
      and updates the joint action via
wt    wt       

   where wt  wt      

Updates leaving the feasible set can be mapped back into
it  see section    The projection     speci es the coordinates of the jointaction vector that player   can modify 
Example   In   block game actions        QN
   RDn
decompose as                 wN   where the nth player
can modify the coordinates in wn  The orthogonal projections                   wn            form   rankN type
with         for all         
Example   In an open game the type has rank TV      
so the single projection is the identity and         for all
   Every player can modify all the coordinates 

Block games coincide with the standard de nition of  
game in the literature  Open games arise below when considering Newton   method  natural gradients  mirror descent and neural networks 
The goal of each player is to minimize its loss  Safety is the
condition that no player   updates harm any other player 
De nition   It is safe for player   to choose   
      if
it does no in nitesimal harm to any player

    

       wt      for all         

  game is safe if it is safe for players to use gradient descent       if choosing   
         wt  is safe for all   
It is worth getting   degenerate case out of the way    block
game is decomposable if player     loss only depends on
the actions it controls  Intuitively    decomposable game is
  independent optimization problems  More formally 
Lemma     block game is decomposable if        
   mw  for all   and    Decomposable games are safe 

Proof  Since    is selfadjoint  we have that          
           Decomposability implies             
when        so

                              

 

which is always positive 

 

if      
else

  Convergence
  block game is convex if the feasible set   is compact
and convex and the losses           are convex in
the coordinates controlled by the respective players  Nash
equilibria are guaranteed to exist in convex block games
 Nash    However   nding them is often intractable

 Daskalakis et al    We show gradient descent converges to   Nash equilibrium in safe convex games 
Theorem   Gradient descent converges to   Nash equilibrium in safe convex games with smooth losses 

Proof  Introduce potential function       PN

      where        are strictly positive  Then

       

          mi  

 nD           mE  

NXn 
                  

     

since safety implies the crossterms are nonnegative  The
players  updates therefore converge to either   critical point
of   or to the boundary of the feasible set  Suppose gradient descent converges to the interior of    Eq   implies
that if       then              for all    By convexity of the losses  the critical point is   minimizer of each
loss with respect to that player   actions  implying it is  
Nash equilibrium    similar argument holds if gradient descent converges to the boundary  see section   
Example    convergence in   safe constrained game  Consider   twoplayer block game with                 and
                where player  controls   and player 
  controls    Introduce feasible set                  
            The game is convex and safe  The set of
Nash equilibria is the bottomleft quadrant of the boundary                      and             Gradient
descent with positive combinations of         
   and
   always converges to   Nash equilibrium 
        
  simple game that does not converge is the following zerosum game  which is related to generative adversarial networks  Goodfellow   
Example    convergence requires positivity  Consider the
twoplayer block game          xy and           xy
where player  controls   and player  controls    The
Nash equilibrium is the origin              However 
gradient descent does not converge  Observe that      
   so           
   
        
and            
     The  ow            
rotates around the origin  No positive combination of
     and      converges to the origin 
  StronglyTyped Games
Strongtyping is based two key ideas  diagonalization and
positivity  Diagonalization is an important tool in applied
mathematics  The Fourier transform simultaneously diagonalizes differentiation and convolution 

   and           

        

  

   df
dx            

and                        

StronglyTyped Agents are Guaranteed to Interact Safely

The SVD diagonalizes any matrix    MP      Finally 
the Legendre transform      max        diagonalizes the in mal convolution

             for         min

        
Diagonalization  nds   latent orthogonal basis that is more
mathematically amenable than the naturally occurring coordinate system  Strongtyping is based on an extension of
diagonalization to nonlinear functions  Before diving in 
we recall the basics of simultaneous diagonalization 

Symmetric matrices  Any symmetric matrix   factorizes as       DP where   is orthogonal and   is diagonal    collection            AN of symmetric matrices
is simultaneously diagonalizable iff the matrices commute 
in which case Ai     DiP where Di is diagonal and  
determines   common latent coordinate system  or type 

Arbitrary matrices  The diagonalization of an       
matrix   is     PDQ  where   and   are orthogonal
        and         matrices and   is positive diagonal 
  collection of matrices is simultaneously diagonalizable
if Ai   PDiQ  for all      necessary condition for simultaneous diagonalizability is that

  Aj and AiA 
  

  are symmetric for all      

 

Next  we work through two examples where diagonalization and   positivity condition imply safety 

  Warmup  When are twoplayer games safe 
To orient the reader  we consider   minimal example which
illustrates most of the main ideas of the paper  twoplayer
bilinear games  von Neumann   Morgenstern    Consider   twoplayer block game with loss functions

           Aw and

           Bw

  viAij

 

 wj

 
 vi

and projections                and      The
   

gradients are       Pij wjAij
           and                  The game is
safe if
                Bw    
hr             BA      
Safety requires that     and BA  are positive semidefinite  Any square matrix decomposes into symmetric and
antisymmetric components     Ms   Ma    
      
      
          where   Maw     for all    Thus 
  square matrix is positive semide nite iff its symmetric
component is positive semide nite 

and
for all   and   

We therefore restrict to when     and BA  are symmetric  Recalling   we further suppose that   and   are
simultaneously diagonalizable and obtain 
Lemma     twoplayer game is safe if     PDQ  and
    PEQ  where   and   are orthogonal matrices   
and   are diagonal  and DE    
Proof  The assumptions imply that
                 Bw                  
and hr                           
  Warmup  When is Newton   method safe 
It was observed in Dauphin et al    that applying Newton   method to neural networks is problematic because it
is attracted to saddle points and can increase the loss on
nonconvex problems  We reformulate their observation in
the language of safety 
Consider   single player open game with twice differentiable loss           and projection     IV   Newton  
method optimizes   via weight updates
wt    wt      with             wt    wt 
where Hij       
    is the Hessian and       
Lemma   If   is strictly convex then Newton   method is
safe       hH              for all   
Proof  Factorize the Hessian at wt as     PDP  If   is
strictly convex then       and so

 wi wj

hH            hD                   

as required 

Two features are noteworthy      the transform       
diagonalizes the secondorder Taylor expansion of  

compare                        
with                          

   

 
 

   

 
 

and  ii  the proof hinges on the positivity of    Sections   
and    extend the approach to show the natural gradient
 Amari    and mirror descent  Raskutti   Mukherjee 
  are safe using the Legendre transform 

  Stronglytyped games are safe
We apply the lessons from the warmups to de ne   factorization of nonlinear functions 

StronglyTyped Agents are Guaranteed to Interact Safely

satisfying

   simultane 

            fL   

  fl   Rpl      

De nition   The functions             
ously factorize if there is   triple
 Pl  
  
  gn   RL      
        gn     
Lw  for all  
where Pl are    pl matrices whose columns jointly form
an orthogonal basis of   and fl and gn are differentiable 
and the gn   covary monotonically   gm
 fl
The projections      PlP 
  de ne   type structure on    
Intuitively  the outputs zl   fl   
     are latent factors
computed from the inputs   such that each zl is independent of the others   independence is enforced by the projections     Monotonic covariation of the functions gn with
respect to the factors zl plays the same role as positivity in
twoplayer games and Newton   method 
   is stronglytyped if the
De nition   Game  TV     
loss functions admit   simultaneous factorization whose
projections       PlP 
    
Theorem   Stronglytyped games are safe 

   commute with     

 fl    

  

 gn

Proof  Commutativity implies there is   basis  ei  
   for
  that simultaneously diagonalizes the projections    
and      Express elements of   as             vD  in the
basis  Safety then reduces to showing
            ni         ei 

 gn
 vi    

 gm
 vi

 

Observe that  fk
 vi
tions of orthogonal coordinates  It follows that

 fl
 vi

 gm
 vi

 gn
 vi

 

    if       since fk and fl are func 
 vi 

 gm
 fk

 gn
 fl

 fl

   LXk 
LXl 

 

 gm
 fl

 fk

 vi     LXl 
 fl    fl
 vi 

 gn

   

since the gn   covary monotonically 

Strongtyping is   suf cient but not necessary condition for
safety  More general de nitions can be proposed according
to taste  De nition   is easy to check  covers the basic examples below  and incorporates the concrete intuition developed in the warmups 

  Comparison with potential games
The proof of theorem   suggests that safe games are related
to potential games  Monderer   Shapley   
In our

notation    block game is   weighted potential game if there
exists   potential function   and scalar weights       
satisfying

              nv                 nv 
for all          and         
We provide two counterexamples to show that stronglytyped games are distinct from potential games 
Example      stronglytyped game that is not   potential
game  Let                       and         
              The block game with projections onto
  and   is stronglytyped but is not   weighted potential
game 
Example      potential game that
is not safe  Let
         xy and          xy       with projections
onto   and    The game is   potential game but is not safe
because

                                       

can be negative 

  Quadratic Games
Given   collection of       matrices       
   and Dvectors       the corresponding quadratic game has loss
functions

       

 
 

                

We assume the matrices      are symmetric without loss
of generality 

  Open quadratic games
In an open quadratic game  each player updates the entire
joint action 
Corollary   An open quadratic game is safe if there is
an orthogonal        matrix    diagonal matrices     
such that              and Dvector   such that
and              

       PD     

We derive corollaries   and   from theorem   Alternate 
direct proofs are provided in appendix   

Proof  Let fi          
Then

    bi  and gn     PD
Dw 

            fD   

        gn     

where pi are the columns of    is stronglytyped 

       

 

  zi 

StronglyTyped Agents are Guaranteed to Interact Safely

The Hessian of    is            The conditions of corollary   can be reformulated as     the Hessians of the losses
commute   mH       nH   for all   and    and  ii 
the Newton steps for the losses coincide  when the Hessians are nonsingular 

  
  

      

                          

 

Newton step

  

 

Example  Disentangling latent factors  An important
problem in machine learning is disentangling latent factors
 Bengio    Basic methods for tackling the problem
include PCA  canonical correlation analysis  CCA  and independent component analysis  ICA  We show how the
factorization in corollary   can arise  in nature  as   variant of blind source separation 
Suppose   signal on   channels is recorded for   timepoints giving         matrix    Assume the observations
combine   independent latent signals      MS where  
is an         matrix representing the latent signal and  
is   mixing matrix 
Blind source separation is concerned with recovering the
latent signals  The covariance of the signal is     XX 
Factorize     PDP  and let           Although this
may not recover the original signal              in general 
it does disentangle   into uncorrelated factors 

          XX       

Finally  recall that  nding the  rst principal component can
be formulated as the constrained maximization problem 

argmax

  Aw 

   kwk 
there
generated

are   sets of observations
Now suppose
orthogonal
                
mixing matrix acting on different sets of  potentially
rescaled  latent signals         PS    Finding the  rst
principle components of the signals reduces to solving the
constrained optimization problems

single

by

 

  argmax

   kwk 

             

  

 

Corollary   implies that   is   safe  Note the corollary implies the optimization problems have compatible gradients 
not that they share   common solution  In general there are
many Nash equilibria  analogous to example  

Quadratic games and linear regression  The blind
source separation example assumes that the linear terms
     in the loss are zero 
If the linear term is nonzero
then linear regression is   special case of minimizing the
quadratic loss  Safety then relates to searching for weights
that simultaneously solve linear regression problems on
multiple datasets 

  Block Quadratic Games
The block quadratic game has losses as above  however
the action space decomposes into             wN   with corresponding projections  Block decompose the components
of the loss as

      BB 

    
 
 
    
   

  

      
 
      

   

 CCA and       BB 

    
 
 
    
 

 CCA  

Corollary     block quadratic game is safe if there are 
           orthogonal   with Pmn     for       
 ii          matrix   with Rn  diagonal for all   
 iii  diagonal       matrices      with             
 iv  and   Dvector  

such that        PRD       

and

             for all   

The notation Pmn and     refers to blocks in the rows and
columns of   and columns of   

 

       

Proof  Let pi denote the columns of   and gn     
PL
  zl  Given    construct Pl by concatenating the
columns pi of   for which the corresponding entries of Ril
are nonzero and let rl be the vector containing the nonzero
entries of      De ne fl xl      
  xl  Then

    xl

        gn     

            fL   

    bl     
Lw 

It is an exercise to check the game is stronglytyped 

Example  Disentangling latent factors  We continue
the discussion of blind source separation and safety  Suppose that the mixing matrix decomposes into blocks

     

  
MN 

 CA

The blocks generate multiple views on   single latent signal 
 Kakade   Foster    McWilliams et al    Benton
et al    The nth view is Mn   
As in the example in section   now suppose there are  
sets of observed signals generated from   sets of latent signals  Each agent attempts to  nd the principal component
speci   to its view on its set of observations  Corollary  
implies that the problems

  argmax

 wn kwnk 

             

  

StronglyTyped Agents are Guaranteed to Interact Safely

can be safely optimized using gradient descent if the mixing matrix has the block form

Mn    Pnn   Rn 

where Pnn is orthogonal and Rn  is diagonal 
In
other words  if the views are generated by rescaling and
changingthe basis of the latent signals 
The open and block settings share   common theme  Safe
disentangling requires observed signals that are generated
by   single  structured  mixing process applied to  arbitrary  sets of independent latent signals  The same phenomenon arises in multiplayer games  resulting in tensor
decompositions that generalize ICA 

  MultiPlayer Games and TensorSVD
  classic Nplayer strategic game consists in  nite actionn  An      Enumerate
the elements of each set as An    Dn  and encode the
losses as             DN  tensors

sets An and losses         QN
An                                where       Dn 
Given   collection of   such tensors  de ne the corresponding multilinear game  as

              wN     An          wN

                    wN     

 

  DNX    

The classic example is when actions are drawn from the
  wn   

Dnsimplex  Dn    wn   RDn   PDn
  and wn      for all  
We now recall the orthogonal tensor decomposition or tensor SVD  Zhang   Golub    Chen   Saad     
tensor admits   tensorSVD if it can be written in the form

   

LXl 

dl     

    uN

               UN

where Un is    Dn     matrix with orthogonal columns
and   is   diagonal               tensor 
Corollary     multilinear game is safe if it admits   simultaneous tensorSVD

                     UN

where the diagonals have the same sign coordinatewise 

 We use the nmode product notation     see de Lathauwer

et al   

       

  zl and fl     Qn xn  DeProof  Let gn     PL
 ne Pl as the         matrix whose nth column is un
in
the block of rows corresponding to wn and zero elsewhere 
Then
        gn     

            fL   

and the game is stronglytyped 

Lw 

 

Not all tensors admit   tensorSVD  However  all tensors
do admit   higherorder SVD  de Lathauwer et al   
Section    explains why simultaneous HOSVD does not
guarantee safety and the stronger tensorSVD is required 

Example  Disentangling latent factors Suppose   is  
latent signal with independent nonGaussian coordinates 
We observe     PS     where   is           mixing
matrix and   is Gaussian noise  By whitening the signal
as   preprocessing step  one can ensure the columns of  
are orthogonal  ICA recovers   from the cumulants of   
see Hyv arinen et al    The main insight is that the
 thorder cumulant tensor admits   tensorSVD 
                cum xi  xj  xk  xl 

  Xo      
 Xr

PioPjpPkqPlr   cum so  sp  sq  sr 
PirPjrPkrPlr   kurt sr 

since cum so  sp  sq  sr      unless               because the signals are independent  The expression can be
written               where diagonal tensor
  speci es the kurtosis of the latent signal  In other words 
computing the tensorSVD recovers the mixing matrix and
allows to recover the latent signal up to basic symmetries 
Following the same prescription as the examples above  if
there are   sets of observations generated from   latent
signals by the same mixing matrix  then the resulting cumulant tensors satisfy corollary  

  Biologically Plausible Backpropagation
Our ultimate goal is to apply strongtyping to safely optimize neural nets with multiple loss functions  Marblestone
et al    Doing so requires constructing variants of
backprop that allow the propagation of multiple error signals  First steps in this direction have been taken with biologically plausible models of backprop that introduce additional degrees of freedom into the algorithm 

Feedback alignment
is   recent algorithm with comparable empirical performance to backprop  It is also more
biologically plausible since it loosens backprop   requirement that forwardand backpropagating weights are sym 

StronglyTyped Agents are Guaranteed to Interact Safely

metric  Lillicrap et al    The main theoretical result
of the paper  see their supplementary information  is
Theorem  Let  BP       denote the error backpropagated one layer of the neural network  Under certain conditions  the error signal computed by feedback alignment 
       Be  satis es

               where    

and    is the pseudoinverse of   

Proof  See theorem   of Lillicrap et al   
Corollary   Under the same conditions  feedback alignment is safe 

Proof  We require to check         BPi     Applying
the theorem obtains

        BPi               ei        WW    ei 

Observe that WW  is an orthogonal projection by standard properties of the pseudoinverse so

        BPi        WW    WW ei    

as required 

In fact  Lillicrap et al    provide experimental and theoretical evidence that feedback alignment learns to align
the feedforward weights with the pseudoinverse of the
backconnections  In other words  they argue that feedback
alignment learns safe gradients 
Another variant of backprop is kickback  which loosens
backprop   requirement that there are distinct forwardand
backward signals  Balduzzi et al    Kickback truncates backprop   error signals so that the network learns
from just the feedforward sweep together with scalar error
signals  One of the main results of Balduzzi et al    is
that kickback is safe  see section   

  Conclusion
Backprop provides   generalpurpose tool to train con gurations of differentiable modules that share   single objective  However  effectively training populations of neural
networks on potentially con icting tasks  such that they automatically exploit synergies and avoid damaging incompatibilities  such as unlearning old features that are not useful on   new task  requires fundamentally new ideas 
  key piece of the puzzle is to develop type systems that
can be used to     guarantee when certain optimizations can
be safely performed jointly and  ii   ag potential con icts
so that the incompatible optimization problems can be separated  The paper provides    rst step in this direction 

From   different perspective  convex methods have played
an enormous role in optimization yet their relevance to
deep learning is limited  The approach to strongtyping
developed here is inspired by and extends certain features
of convexity  One of the goals of this paper is to carve
out some of the key concepts underlying convex geometry
and reassemble them into   more  exible  but still powerful framework  The proposed de nition of strongtyping
should be considered    rst and far from  nal attempt 
  large class of natural examples is generated by imposing
strongtyping on simple quadratic and multilinear games 
It turns out that  in these settings  strongtyping yields
the same matrix and tensor decompositions that arise in
blind source separation and independent component analysis  where multiple latent signals are mixed by the same
structured process  An important future direction is to disentangle nonlinear latent factors 

Strongtyping and safety in neural nets  We conclude
by discussing the relevance of the framework to neural networks  Firstly  neural nets and strongtyping have many of
the same ingredients  neural nets combine linear algebra
 matrix multiplications and convolutions  with monotonic
functions  sigmoids 
tanhs  recti ers  and maxpooling
amongst others  Recti ers and sigmoids have the additional feature that their outputs are always positive 
Secondly  there is   deeper connection between recti ers
and strongtyping  Recti ers are orthogonal projections
on weights        zeroes out the columns wl of   for
which   
        Recti ers are more sophisticated projections than we have previously considered because they are
contextdependent  The columns that are zeroed out depend on   and    the recti erprojection takes   and  
as parameters  compare remarks   and   in the appendix 
Representation learning in recti er networks can thus be
recast as learning parameterized type structures  An interesting future direction is to consider tensorswitching networks  Tsai et al    which decouple   neuron   decision to activate from the information it passes along  for  
recti er  both depend on     
Finally  it has long been known that the brain does not use
backprop  Crick    One possibility is that backprop is
the optimal deep learning algorithm which  unfortunately 
evolution failed to stumble upon  Another is that there are
evolutionary advantages to not using backpropagation  For
example  it has been argued that the brain optimizes multiple loss functions  Marblestone et al    Does jointly
optimizing or satis cing multiple objectives require learning mechanisms with more degrees of freedom than backprop  Balduzzi et al    Lillicrap et al    Safety
and strongtyping provide the tools needed to frame and
investigate the question 

StronglyTyped Agents are Guaranteed to Interact Safely

Acknowledgements
  am grateful to Stephen Marsland and James Benn for useful discussions 

References
Amari     Natural Gradient Works Ef ciently in Learning  Neural

Comp     

Amari     Information Geometry and Its Applications  Convex
In Nielsen  Frank  ed 

Function and Dually Flat Manifold 
Emerging Trends in Visual Computing   

Amodei  Dario  Olah  Chris  Steinhardt  Jacob  Christiano  Paul 
Schulman  John  and Man    Dan  Concrete Problems in AI
Safety  In arXiv   

Balduzzi     Vanchinathan     and Buhmann     Kickback cuts
Backprop   redtape  Biologically plausible credit assignment
in neural networks  In AAAI   

Balduzzi  David  Grammars for Games    GradientBased 
GameTheoretic Framework for Optimization in Deep Learning  Frontiers in Robotics and AI     

Balduzzi  David and Ghifary  Muhammad  StronglyTyped Re 

current Neural Networks  In ICML   

Bengio  Yoshua  Deep Learning of Representations  LookIn Dediu  AdrianHoria  Mart nVide  Carlos 
ing Forward 
Mitkov  Ruslan  and Truthe  Bianca  eds  Statistical Language and Speech Processing  Springer   

Benton  Adrian  Khayrallah  Huda  Gujral  Biman  Reisinger 
Drew  Zhang  Sheng  and Arora  Raman  Deep Generalized
Canonical Correlation Analysis  In arXiv   

Berkenkamp     Moriconi     Schoellig     and Krause     Safe
learning of regions of attraction for uncertain  nonlinear systems with gaussian processes  In IEEE CDC   

Bubeck    ebastien  Convex Optimization  Algorithms and Complexity  Foundations and Trends in Machine Learning   
   

Cardelli  Luca  Type Systems  In Handbook of Computer Science

and Engineering  CRC Press   

Chen  Jie and Saad  Yousef  On the tensor SVD and the optimal
low rank orthogonal approximation of tensors  SIAM    Matrix
Anal  Appl     

Crick  Francis  The recent excitement about neural networks  Na 

ture     

Daskalakis  Constantinos  Goldberg  Paul    and Papadimitriou 
Christos  The Complexity of Computing   Nash Equilibrium 
SIAM    Computing     

Dauphin  Yann  Pascanu  Razvan  Gulcehre  Caglar  Cho 
Kyunghyun  Ganguli  Surya  and Bengio  Yoshua 
Identifying and attacking the saddle point problem in highdimensional
nonconvex optimization  In NIPS   

de Lathauwer  Lieven  de Moor  Bart  and Vandewalle  Joos 
  multilinear singular value decomposition  SIAM    Matrix
Anal  Appl     

Fernando     Banarse     Blundell     Zwols     Ha     Rusu    
Pritzel     and Wierstra     PathNet  Evolution Channels Gradient Descent in Super Neural Networks  In arXiv 
 

Goodfellow  Ian    NIPS   Tutorial  Generative Adversarial

Networks  In arXiv   

Hinton    and et al  Deep Neural Networks for Acoustic Modeling in Speech Recognition  The Shared Views of Four Research Groups  IEEE Signal Proc Magazine     

Hyv arinen  Aapo  Karhunen  Juha  and Oja  Erkki  Independent

Component Analysis  John Wiley   Sons   

Kakade  Sham and Foster  Dean    Multiview Regression Via

Canonical Correlation Analysis  In COLT   

Krizhevsky     Sutskever     and Hinton       Imagenet classi 
cation with deep convolutional neural networks  In Advances
in Neural Information Processing Systems  NIPS   

LeCun  Yann  Bengio  Yoshua  and Hinton  Geoffrey  Deep learn 

ing  Nature     

Lillicrap  Timothy    Cownden  Daniel  Tweed  Douglas    and
Ackerman  Colin    Random feedback weights support error
backpropagation for deep learning  Nature Communications   
   

Marblestone  Adam    Wayne  Greg  and Kording  Konrad   
Towards an Integration of Deep Learning and Neuroscience 
Front  Comput  Neurosci     

McWilliams  Brian  Balduzzi  David  and Buhmann  Joachim 
Correlated random features for fast semisupervised learning 
In NIPS   

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David  and et al 
Humanlevel control through deep reinforcement learning  Nature       

Monderer  Dov and Shapley  Lloyd    Potential Games  Games

and Economic Behavior     

Nash  John    Equilibrium Points in nPerson Games  Proc Natl

Acad Sci           

Nisan  Noam  Roughgarden  Tim  Tardos   Eva  and Vazirani  Vijay  eds  Algorithmic Game Theory    Cambridge University Press  Cambridge 

Rakhlin  Alexander and Sridharan  Karthik  Optimization  learn 

ing  and games with predictable sequences  In NIPS   

Raskutti    and Mukherjee     The Information Geometry of Mirror Descent  IEEE Trans  Inf  Theory     

Silver  David  Huang  Aja  and et al  Mastering the game of go
with deep neural networks and tree search  Nature   
     

Syrgkanis  Vasilis  Agarwal  Alekh  Luo  Haipeng  and Schapire 
Robert  Fast Convergence of Regularized Learning in Games 
In NIPS   

Tsai  ChuanYung  Saxe  Andrew  and Cox  David  Tensor

Switching Networks  In NIPS   

StronglyTyped Agents are Guaranteed to Interact Safely

Turchetta  Matteo  Berkenkamp  Felix  and Krause  Andreas  Safe
Exploration in Finite Markov Decision Processes with Gaussian Processes  In NIPS   

von Neumann  John and Morgenstern  Oskar  Theory of Games
and Economic Behavior  Princeton University Press  Princeton
NJ   

Zhang     Kahn     Levine     and Abbeel     Learning deep control policies for autonomous aerial vehicles with mpcguided
policy search  In ICRA   

Zhang  Tong and Golub  Gene    Rankone approximation to
higher order tensors  SIAM    Matrix Anal  Appl   
   

