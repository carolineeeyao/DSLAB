NearOptimalDesignofExperimentsviaRegretMinimizationZeyuanAllen Zhu YuanzhiLi AartiSingh YiningWang AbstractWeconsidercomputationallytractablemethodsfortheexperimentaldesignproblem wherekoutofndesignpointsofdimensionpareselectedsothatcertainoptimalitycriteriaareapproximatelysatis ed Ouralgorithm ndsa approximateoptimaldesignwhenkisalinearfunctionofp incontrast existingresultsrequirektobesuperlinearinp Ouralgorithmalsohandlesallpopularoptimalitycriteria whileexistingonesonlyhandleoneortwosuchcriteria Numericalresultsonsyntheticandreal worlddesignproblemsverifythepracticaleffectivenessoftheproposedalgorithm IntroductionExperimentaldesignisanimportantprobleminstatisticsandmachinelearningresearch Pukelsheim Consideralinearregressionmodely     whereX Rn pisapoolofndesignpoints yistheresponsevector isapdimensionalunknownregressionmodelandwisavectorofi     noisevariablessatisfyingEwi andEw   Theexperimentaldesignproblemistoselectasmallsubsetofrows     designpoints XSfromthedesignpoolXsothatthestatisticalpowerofestimating ismaximizedfromnoisyresponseySontheselecteddesignsXS Asanexample consideramaterialsynthesisapplicationwherepisthenumberofvariables     temperature pressure duration thatarehypothesizedtoaffectthequalityofthesynthesizedmaterialandnisthetotalnumberofcombinationsofdifferentparametersofexperimentalcondi tions Asexperimentsareexpensiveandtimeconsuming Authornameslistedinalphabeticorder MicrosoftResearch Redmond USA PrincetonUniversity Princeton USA CarnegieMellonUniversity Pittsburgh USA Correspondenceto YiningWang yiningwa cs cmu edu Proceedingsofthe thInternationalConferenceonMachineLearning Sydney Australia PMLR Copyright bytheauthor   onewishestoselectk cid nexperimentalsettingsfromXthatarethemoststatisticallyef cientforestablishingamodelthatconnectsexperimentalparameterswithsynthesizedmaterialquality   Theexperimentaldesignproblemisalsorelatedtomanymachinelearningtasks suchaslinearbandits Deshpande Montanari Huangetal diversitysampling Kulesza Taskar andactivelearning Maetal Chaudhurietal Hazan Karnin Balcan Long Wang Singh Sincestatisticalef ciencycanbemeasuredinvariousways thereexistanumberofoptimalitycriteriatoguidetheselectionofexperiments WereviewsomeoptimalitycriteriainSec andinterestedreadersarereferredtoSec of Pukelsheim foracomprehensivereview Typically anoptimalitycriterionisafunctionf     Rthatmapsfromthepdimensionalpositivede niteconetoarealnumber Theexperimentaldesignproblemcanthenbeformulatedasacombinatorialoptimizationproblem     argminS           SXS whereSiseitherasetoramultisetofsizek andXS Rk pisformedbystackingtherowsofXthatareinS TheconstraintsetS     isde nedasfollows Withreplacement       Smultiset         Underthissetting XSmaycontainduplicaterowsofthedesignpoolX Withoutreplacement       Sstandardset         Underthissetting XSonlycontainsdistinctrowsofthedesignpoolX The withreplacement settingisclassicalinstatisticsliterature wherethemultiplemeasurementsinywithrespecttothesamedesignpointleadtodifferentvalueswithstatisticallyindependentnoise The withoutreplacement setting ontheotherhand ismorerelevantinmachinelearningapplications becauselabelsarenotlikelytochangeifthesamedatapoint     thesameimage isconsideredtwice Finally itisworthpointingoutthatthe withreplacement settingiseasier becauseitcanbereduced inpolynomialtime tothe withoutreplacement settingbyreplicatingeachrowofXforktimes Formanypopularchoicesoff theexactoptimizationNearoptimaldesignofexperimentsviaregretminimizationTable Comparisonwithexistingresultsoncomputationallyef cientexperimentaldesign Analgorithmproducesasubset Sofsizek andtheapproximationratioisde nedasf   SX   minS Sb         SXS forr   TOPTdenotesthetimecomplexityforsolvingthecontinuousconvexoptimizationprobleminEq Withoutreplacement Deterministic TimecomplexityConstraints CriteriaApprox ratioPipageroundingYesYesTOPT           pD     Bouhtouetal YesNoTOPT         pD       Avron Boutsidis YesYesO         pAn       Wangetal NoNoTOPT   nk     andr plogp     Wangetal YesYesTOPT               Thispaper Eq NoYesTOPT   nkp                   Thispaper Eq YesYesTOPT   nkp     pA         GO Thispaper Eq YesYesTOPT   nkp                     In   and wehidelogarithmicdependencyovern pandk probleminEq isNPhard   ivril MagdonIsmail Cern   Hlad   Inthispaper weproposeacomputationallytractablealgorithmthatapproximatelycomputesEq forawiderangeofoptimalitycriteria andunderveryweakconditionsonn kandp Belowisourmaintheorem Theorem Supposeb     pandletf     Rbearegularoptimalitycriterion cf De nition Thereexistsapolynomialtimealgorithmthatoutputs   Sb     foranyinputmatrixX Rn pwithfullcolumnrank and Ssatis esthefollowing Forb withreplacement thereexistsanabsoluteconstantC suchthat forany ifk     thenf   SX   minS           SXS Forb withoutreplacement andany thereexistsconstantC dependingonlyon suchthat ifk pthenf   SX     minS           SXS Moreover for wehaveC Forb withoutreplacement andany ifk rsatisfyk randr   thenf   SX   minS           SXS Weinterpretthesigni canceofTheorem asfollows Underaverymildconditionofk   ourpolynomialtimealgorithm ndsaset     ofsizek withobjectivevaluef   SX   beingatmostO aconstanttimestheoptimum SeeEq Ifreplacement   oroversampling     isallowed theapproximationratiocanbetightenedto forarbitrarilysmall SeeEq and Inallofthethreecases weonlyrequirektogrowlinearlyinp Recallthatk pisnecessarytoensurethesingularityofX SX   Incontrast nopolynomialtimealgorithmhasachievedO approximationintheregimek     fornonsubmodularoptimalitycri teria     AandV optimality underthewithoutreplacementsetting Ouralgorithmworksforanyregularoptimalitycriterion Tothebestofourknowledge noknownpolynomialtimealgorithmcanachievea approximationfortheD andToptimalitycriteria orevenanO approximationfortheEandG optimalitycriteria SeeTable foracomparison ThekeyideabehindourproofofTheorem isaregretminimizationcharacterizationoftheleasteigenvalueofpositivesemide nite PSD matrices Similarideasweredevelopedin AllenZhuetal Silvaetal toconstructef cientalgorithmsforlinearsizedgraphsparsi ers Inthispaperweadopttheregretminimizationframeworkandpresentnovelpotentialfunctionanalysisforthespeci capplicationofexperimentaldesign NotationsS pisthepositivede niteconeofp pmatrices ap psymmetricmatrixAbelongstoS pifandonlyifv Av forallv Rp ForsymmetricmatricesAandB wewriteA cid Bifv       forallv Rp TheinnerproducthA Biisde nedashA Bi tr     Ppi   AijBij WeusekAk supv Rp kAvk kvk todenotethespectralnorm andkAkF qPpi     ij phA AitodenoteNearoptimaldesignofexperimentsviaregretminimizationtheFrobeniusnormofA ForA cid wewriteB   astheuniqueB cid thatsatis esB   ForadesignpoolX Rn   weusexi RptodenotetheithrowofX Weuse min   fortheleast smallest singularvalueofaPSDmatrixA RelatedworkExperimentaldesignisanoldtopicinstatisticsresearch Pukelsheim Fedorov Computationallyef cientexperimentaldesignalgorithms withprovableguarantees are however alessstudied eld Inthecaseofsubmodularoptimalitycriteria     DandT optimality theclassicalpipageroundingmethod Ageev Sviridenko Horeletal Ravietal combinedwithsemide niteprogrammingresultsincomputationallyef cientalgorithmsthatenjoyaconstantapproximationratio Bouhtouetal improvestheapproximationratiowhenkisverycloseton Deshpande Rademacher Lietal consideredpolynomialtimealgo rithmsforsamplingfromaDoptimalitycriterion Thesealgorithmsarenotapplicabletononsubmodularcriteria suchasA   EorG optimality FortheparticularAoptimalitycriterion Avron Boutsidis proposedagreedyalgorithmwithanapproximationratioofO     withrespecttof     Itwasshownthatintheworstcasemin   kf   SXS             andhencetheboundistight However forgeneraldesignpoolmin   kf   SXS couldbefarsmallerthanO           makingthetheoreticalresultspowerlessinsuchscenarios Wangetal consideredavariantofthegreedymethodandshowedanapproximationratioquadraticindesigndimensionpandindependentofpoolsizen Wangetal derivedalgorithmsbasedoneffectiveresistancesampling Spielman Srivastava thatattain approximationratioifk plogp andrepetitionsofdesignpointsareallowed Thealgorithmfundamentallyreliesonthecapabilityof reweighting repeating designpointsandcannotbeadaptedtothemoregeneral withoutreplacement setting Naivesamplingbasedmethodswereconsideredin Wangetal Chaudhurietal Dhillonetal whichalsoachieve approximationbutrequiresthesubsetsizektobemuchlargerthantheconditionnumberofX Arelatedhoweverdifferenttopicislowrankmatrixcol umnsubsetselectionandCURapproximation whichseekscolumnsubsetCandrowsubsetRsuchthatkX CC XkFand orkX CURkFareminimized Drineasetal Boutsidis Woodruff Wang Singh   Drineas Mahoney Wang Zhang Wang Singh   Theseproblemsareunsupervisedinnatureanddonotingeneralcorrespondtostatisticalpropertiesundersupervisedregressionsettings Pilanci Wainwright Raskutti Mahoney Woodruff consideredfastmethodsforsolvingordinaryleastsquares OLS problems Theyarecomputationallyorientedandtypicallyrequireknowledgeofthefullresponsevectory whichisdifferentfromtheexperimentaldesignproblem RegularcriteriaandcontinuousrelaxationWestartwiththede nitionofregularoptimalitycriteria De nition Regularcriteria Anoptimalitycriterionf     Risregularifitsatis esthefollowigproperties Convexity               forall andA       Monotonicity IfA cid Bthenf       Reciprocalmultiplicity   tA       forallt andA     Almostalloptimalitycriteriausedintheexperimentaldesignliteratureareregular Belowwelistafewpopularexamples theirstatisticalimplicationscanbefoundin Pukelsheim Aoptimality Average fA ptr Doptimality Determinant fD det   Toptimality Trace fT   tr Eoptimality Eigenvalue fE     Voptimality Variance fV ntr     Goptimality fG maxdiag     The         criteriaconcernestimatesofregressioncoef cientsandthe     criteriaareaboutinsamplepredictions Allcriterialistedaboveareregular NotethatforDoptimalitytheproxyfunctiongD logdet isconsideredtosatisfytheconvexityproperty Inaddition bythestandardarithmeticinequalitywehavethatfT fD fA fEandthatfV fG AlthoughexactoptimizationofthecombinatorialproblemEq isintractable itisneverthelesseasytosolveacontinuousrelaxationofEq giventheconvexitypropertyinDe nition Weconsiderthefollowingcontinuousoptimizationproblem   argmin     nXi ixix                     Thispropertycouldberelaxedtoallowaproxyfunctiong     Rbeingconvex whereg               NearoptimaldesignofexperimentsviaregretminimizationThek   rconstraintmakessureonlyrrowsofXare selected wherer kisaparameterthatcontrolsthedegreeofoversampling The   constraintenforcesthateachrowofXis selected atmostonceandisonlyapplicabletothewithoutreplacementsetting   Eq isarelaxationoftheoriginalcombinatorialproblemEq whichweformalizebelow Fact Forb wehavef Pni     xix   minS Sb         SXS Inaddition becauseofthemonotonicitypropertyoffthesumconstraintmustbind Fact Forb itholdsthatPni       ProofsofFacts and arestraightforwardandareplacedinthesupplementarymaterial BoththeobjectivefunctionandtheconstraintsetinEq areconvex andhenceitcanbeef cientlysolvedtoglobaloptimalitybyconventionalconvexoptimizationalgorithms Inparticular fordifferentiablefwesuggestthefollowingprojectedgradientdescent PGD procedure   PC cid         cid wherePC   argminy Ckx yk istheprojectionoperatorontothefeasiblesetC Rp               and     isasequenceofstepsizestypicallychosenbybacktrackinglinesearch Whenfisnotdifferentiableeverywhere projectedsubgradientdescentcouldbeusedwitheithercon stantordiminishingstepsizes Wedeferdetailedgradientcomputationstothesupplementarymaterial Itwasshownin Wangetal Suetal thattheprojectionoperatorPC   couldbeef cientlycomputeduptoprecision inO nlog kxk operations Sparsi cationviaregretminimizationTheoptimalsolution ofEq doesnotnaturallyleadtoavalidapproximationofthecombinatorialprobleminEq becausethenumberofnonzerocomponentsin mayfarexceedk Theprimaryfocusofthissectionistodesignef cientalgorithmsthatsparsifytheoptimalsolution intos     withreplacement ors   withoutreplacement whileatthesametimeboundingtheincreaseintheobjective Duetothemonotonicityandreciprocalmultiplicitypropertiesoff itsuf cesto ndasparsi ersthatsatis es nXi sixix   cid  nXi ixix   forsomeconstant ByDe nition Eq immediatelyimpliesf Pni sixix     Pni ixix   Thekeyideabehindouralgorithmisaregretminimizationinterpretationoftheleasteigen valueofapositivede nitematrix whicharisesfromrecentprogressinthespectralgraphsparsi cationliterature Silvaetal AllenZhuetal Intherestofthissection weadoptthenotationthat diag andS diag   bothbeingn nnonnegativediagonalmatrices WealsouseItodenotetheidentitymatrix whosedimensionshouldbeclearfromthecontext ThewhiteningtrickConsiderthelineartransformxi     xi xi ItiseasytoverifythatPni   xi       Suchatransformisusuallyreferredtoaswhitening becausethesamplecovarianceofthetransformeddataistheidentitymatrix De neW Pni si xi     Wethenhavethefollowing Proposition For   cid Iifandonlyif Pni sixix   cid Pni ixix   Proof ThepropositionholdsbecauseW cid Iifandonlyif           cid     andthat           XSX Proposition showsthat withoutlossofgenerality wemayassumePni ixix         ThequestionofprovingW XSX cid IisthenreducedtolowerboundingthesmallesteigenvalueofW RecallthatWcanbewrittenasasumofrank PSDmatricesW Pkt Ft whereFt xix iforsomei   InthenextsectionwegiveanovelcharacterizationoftheleasteigenvalueofWfromaregretminimizationperspective TheproblemoflowerboundingtheleasteigenvalueofWcanthenbereducedtoboundingtheregretofaparticularFollow TheRegularized Leader FTRL algorithm whichisamucheasiertaskasFTRLadmitsclosedformsolutions SmallesteigenvalueasregretminimizationWe rstreviewtheconceptofregretminimizationinaclassicallinearbanditsetting Let     Rp     cid tr   beanactionspacethatconsistsofpositivesemi de nitematricesofdimensionpandunittracenorm Considerthelinearbanditproblem whichoperatesinkiterations Atiterationt theplayerchoosesanactionAt   afterwards   reference actionFt cid isobservedandthelosshFt Atiisincurred Theobjectiveoftheplayeristominimizehis herregret   At kt kXt hFt Ati infU pkXt hFt Ui Nearoptimaldesignofexperimentsviaregretminimizationwhichisthe excessloss of At kt comparedtothesingleoptimalactionU pinhindsight knowingallthereferenceactions Ft kt ApopularalgorithmforregretminimizationisFollowThe RegularizedLeader FTRL alsoknowntobeequivalenttoMirrorDescent MD McMahan whichsolvesforAt argminA           hF Ai Herew   isaregularizationtermand isaparameterthatbalancesmodel ttingandregularization Fortheproofofourpurposeweadoptthe regularizerw   tr   introducedin AllenZhuetal whichleadstotheclosedformsolutionAt  ctI       wherect RistheuniqueconstantthatensuresAt   Thefollowinglemmafrom AllenZhuetal boundstheregretofFTRLusingtheparticular regularizer Lemma Theorem of AllenZhuetal specializedto regularization Suppose rank Ft andlet At kt beFTRLsolutionsde nedinEq If hFt   ti forallt thenR At kt kXt hFt Ati infU pkXt hFt Ui kXt hFt AtihFt   ti hFt   ti   NowconsidereachFt xitx ittobetheouterproductofadesignpointselectedfromthedesignpoolX OneremarkableconsequenceofLemma isthat inordertolowerboundthesmallesteigenvalueofPkt Ft whichbyde nitionisinfU phPkt Ft Ui itsuf cestolowerboundPkt hFt Ati BecauseAtadmitsclosedformexpres sioninEq choosingasequenceof Ft kt withlargePkt hFt Atibecomesamuchmoremanageableanalyticaltask whichweshallformalizeinthenextsection ProofofTheorem ReorganizingtermsinLemma weobtaininfU pkXt hFt Ui kXt hFt Ati hFt   ti   Theknearoptimaldesignpointsareselectedinasequen tialmanner Let   Sb     bethesetofselecteddesignpointsatorpriortoiterationt andde neFt xitx it whereitisthedesignpointselectedatiterationt De nealso   Pt   Pi txix   We rstconsiderthewithreplacementsettingb Lemma SupposePni ixix   Iwhere   andPni     Thenfor   kwehavethatmaxi   hxix   Ati hxix     ti     Proof Recallthattr At andPni ixix     Subsequently Pni ihxix   Ati Ontheotherhand wehavethatPni   hxix     ti Pni   tr         tr           Here   isduetotheoptimizationconstraintthatk     and   isbecausetr             pk       ppk At   pptr At   where isthevectorofalleigenvaluesofaPSDmatrix Combiningbothinequalitieswehavethatmaxi   hxix   Ati hxix     ti Pni ihxix   AtiPni   hxix     ti wheretherighthandsideislowerboundedby     Letit argmaxi   hxix   Ati hxix     tibethedesignpointselectedatiterationt CombiningEq andLemma   Xi kxix   cid cid kr     cid   ToproveEq set   Becausek       wehavethatkr       WithC therighthandsideislowerboundedby Eq isthusprovedbecause Wenextconsiderthewithoutreplacementsettingb Lemma Fixarbitrary andsupposePni ixix   Iwhere   andPni     Thenforall     maxi   hxix   Ati hxix     ti min         Proof Ononehand wehavePi   ihxix   Ati   hAt         trh   ctI     ct trh   ctI   ct tr       ct   Here   isduetoPni ixix   Iand     isduetohAt Ii tr At and   isprovedintheproofofLemma Because   ctI cid weconcludethatct min   andthereforePi   ihxix   Ati min     Ontheotherhand Pi     hxix     ti Nearoptimaldesignofexperimentsviaregretminimizationr pbythesameargumentasintheproofofLemma Subsequently maxi   hxix   Ati hxix     ti Pi   ihxix   AtiPi     hxix     ti min         Letit argmaxi   hxix   Ati hxix     ti CombiningEq andLemma with wehavethat   cid  kXt             where   min   WearenowreadytoproveEqs inTheorem ProofofEq Notethat   cid supu min cid               cid   Eq canbeprovedbyacaseanalysis ifu tforsome   kthen min   min     otherwise         forall     Supposek   pforsome andlet     where issomeparametertobespeci edlater Eq thenyields   cid   Because itispossibletoselect suchthatC Finally for and wehaveC Eq isthusproved ProofofEq Let beaparametertobespeci edlater andde ne     ixix iand       ixix   Let SbeconstructedsuchthatitincludesallpointsinS     plustheresultingsetbyrunningAlgorithm ontheremainingweightssmallerthan withsubsetsizek       De ne                 and               Let Pi Sxix ibethesamplecovarianceoftheselectedsubset Bythede nitionof SandLemma togetherwiththewhiteningtrick Sec on wehave cid supu minnu         cid supu minnu       oI wherethesecondlineholdsbecause Iandu Nowset andnotethatk     byde nitionofS Subsequently     andk rfor impliesthat     whichyieldsu andhencef   SX         XS Eq isthusproved Algorithm Nearoptimalexperimentaldesign Input designpoolX Rn   budgetparametersk     algorithmicparameter SolvetheconvexoptimizationproblemEq withparameters Let betheoptimalsolution Whitening       diag   Initialization fort tokdo ct FINDCONSTANT Pi   xix   At ctI Pi   xix   Ifb then     else       it argmaxi thxix   Ati hxix     ti     it endfor Output     Algorithm FINDCONSTANT   Initialization   min   cu   while   cu do     cu Iftr cI   thenc   elsecu   endwhile Output     cu OurproofofTheorem isconstructiveandyieldsacomputationallyef cientiterativealgorithmwhich ndssubset   Sb     thatsatis estheapproximationresultsinTheorem InAlgorithm wegiveapseducodedescriptionofthealgorithm whichmakesuseofabinarysearchroutine Algorithm that ndstheuniqueconstantctforwhichtr At tr ctI Pi   xix   NotethatforEq tobevalid itisnecessarytorunAlgorithm ontheremainingsetof afterincludingallpointsxiwith   in   ExtensiontogeneralizedlinearmodelsTheexperimentalalgorithmpresentedinthispapercouldbeeasilyextendedbeyondthelinearregressionmodel ForthispurposeweconsidertheGeneralizedLinearModel GLM whichassumesthaty xi           wherep isaknowndistributionand isanunknownpdimensionalregressionmodel Examplesincludethelogisticregressionmodelp     exp   exp   thePossioncountmodelp yi     exp yx       andmanyothers LetS Sb     bethesetofselecteddesignpointsfromX Undertheclassicalstatisticsregime NearoptimaldesignofexperimentsviaregretminimizationTable Simulationresultsonsyntheticdataofsizen andk Uniformsamplingandweightedsamplingarerunfor independenttrialsandthemedianobjectiveisreported Inf meansthesamplecovarianceX SXSdoesnotbelongtoS           fAfDfTfEfVfGfAfDfTfEfVfGUNIFORMSAMPLING WEIGHTEDSAMPLING InfInf Inf FEDOROV SEXCHANGE runningtime secs ALGORITHM runningtime secs         UNIFORMSAMPLING WEIGHTEDSAMPLING Inf FEDOROV SEXCHANGE runningtime secs ALGORITHM runningtime secs themaximumlikelihood ML estimator ML argmin Pi Slogp yi     isasymptoticallyef cient anditsasymptoticvarianceequalstheFisher sinformationI XS Xi SEy     cid logp   xi cid       Xi SEy   cid logp       cid xix   Herethesecondequalityisduetothesuf ciencyofx   inaGLM Notethatforthelinearregressionmodely     theMLestimatoristheordinaryleastsquares OLS   SXS XSySanditsFisher sinformationequalsthesamplecovarianceX SXS Theexperimentaldesignproblemcanthenbeformalizedasfollows minS Sb         XS minS Sb       Xi Sziz   zi   Ey   cid logp yi     cid       Suppose isa pilot estimateof obtainedfromauniformlysampleddesignsubsetS AnearoptimaldesignsetS canthenbeconstructedbyminimizingEq using       SuchanapproachwasadoptedinsequentialdesignandactivelearningforMLestimators Chaudhurietal Khurietal however withouralgorithmthequalityofS isgreatlyimproved UnderverymildconditionsE logp   logp isnonnegative VanderVaart NumericalresultsWecomparetheproposedmethodwithseveralbaselinemethodsonbothsyntheticandrealworlddatasets Weonlyconsidertheharder withoutreplacement setting whereeachrowofXcanbeselectedatmostonce MethodsandtheirimplementationWecompareouralgorithmwiththreesimpleheuristicmethodsthatapplytoalloptimalitycriteria Uniformsampling SissampleduniformlyatrandomwithoutreplacementfromthedesignpoolX Weightedsampling rsttheoptimalsolution ofEq iscomputedwithr   afterwards Sissampledwithoutreplacementaccordingtothedistributionspeci edby   Recallthat Wangetal provedthatweightedsamplingworkswhenkissuf cientlylargecomparedtop cf Table Fedorov sexchange Miller Nguyen thealgorithmstartswitharandomsubsetS Sb     anditerativelyexchangestwocoordinatesi       suchthattheobjectiveisminimizedaftertheexchange Thealgorithmterminatesifnosuchexchangecanreducetheobjective orTiterationsarereached AllalgorithmsareimplementedinMATLAB exceptfortheFedorov sexchangealgorithm whichisimplementedinCduetoef ciencyconcerns WealsoapplytheShermanMorrisonformula   uu     uu       uandthematrixdeterminantlemmadet   uu Fact ensuresthat kisavalidprobabilitydistribution NearoptimaldesignofexperimentsviaregretminimizationTable ResultsontheMinnesotawindspeeddataset       MSEisde nedasq nky     fVMSEfGMSEUNIFORMSAMPLING WEIGHTEDSAMPLING FEDOROV SEXCHANGE runningtime secs ALGORITHM runningtime secs FULLSAMPLEOLS       det   toacceleratecomputationsofrank updatesofmatrixinverseanddeterminant Foruniformsamplingandweightedsampling wereportthemedianobjectiveof indpendenttrials WeonlyreporttheobjectiveforonetrialofFedorov sexchangemethodduetotimeconstraints ThemaximumnumberofiterationsTforFedorov sexchangeissetatT Wealwayssetk rintheoptimizationproblemEq anddetailsofsolvingEq areplacedintheappendix InAlgorithm weset oursimiluationssuggestthatthealgorithmisnotsensitiveto SyntheticdataWesynthesizea designpoolXasfollows   cid XA XB cid XAisa randomGaussianmatrix rescaledsothattheeigenvaluesofX AXAsatisfyaquadraticdecay     AXA   XBisa Gaussianmatrixwithi     standardNormalvariables BothXAandXBhavecomparableFrobeniusnorm InTable wereportresultsonall optimalitycriteria fA fD fT fE fV fG fork         Wealsoreporttherunningtime measuredinseconds ofAlgorithm andtheFedorov sexchangealgorithm Theothertwosamplingbasedalgorithmsareveryef cientandalwaysterminatewithinonesecond WeobservethatouralgorithmhasthebestperformanceforfEandfG whilestillachievingcomparableresultsfortheotheroptimalitycriteria Itisalsorobustwhenkissmallcomparedtop whilesamplingbasedmethodsoccasionallyproducedesignsthatarenotevenfullrank Finally Algorithm iscomputationallyef cientandterminateswithinsecondsforallsettings TheMinnesotawindspeeddatasetTheMinnesotawinddatasetcollectswindspeedinformationacrossn locationsinMinnesota USAforaperiodof months forthepurposeofthisexperiment weonlyusewindspeeddataforonemonth The locationsareconnectedwith bidirectionalroads whichformann nsparseunweightedundirectedgraphG LetL diag   Gbethen nLaplacianofG wheredisavectorofnodedegrees andletV Rn pbeanorthonormaleigenbasiscorrespondingtothesmallestpeigenvaluesofL Chenetal showsthattherelativelysmoothwindspeedsignaly Rncanbewellapproximatedbyusingonlyp graphLaplacianbasis InTable wecomparethemeansquareerror MSE forpredictiononthefulldesignpoolV MSE   nky     Becausetheobjectiveispredictionbased weonlyconsiderthetwopredictionrelatedcriteria fV tr     andfG maxdiag     Thesubsetsizekissetask   whichismuchsmallerthann WeobservethatAlgorithm consistentlyoutperformstheotherheuristicmethods andissoef cientthatitsrunningtimeisnegligible Itisalsointerestingthatbyusingk samplesAlgorithm alreadyachievesanMSEthatiscomparabletotheOLSontheentiren designpool ConcludingremarksandopenquestionsWeproposedacomputationallyef cientalgorithmthatapproximatelycomputesoptimalsolutionsfortheexperimen taldesignproblem withnearoptimalrequirementonk     thenumberofexperimentstochoose Inparticular weobtainedaconstantapproximationundertheveryweakconditionk   anda approximationifreplacementorover samplingisallowed Ouralgorithmworksforallregularoptimalitycriteria Animportantopenquestionistoachieve relativeapproximationratiounderthe propersampling regimek   orthe slightoversampling regimek   forthewithoutreplacementmodel Itwasshownin Wangetal thatasimplegreedymethodachieves approximationratioforAandV optimalityprovidedthatk   Whethersuchanalysiscanbeextendedtootheroptimalitycriteriaandwhetherthep termcanbefurtherreducedtoanearlinearfunctionofpremainopen AnotherpracticalquestionistodevelopfastconvergingoptimizationmethodsforthecontinuousprobleminEq especiallyforcriteriathatarenotdifferentiablesuchastheEandG optimality wheresubgradientmethodshaveveryslowconvergencerate AcknowledgementThisworkissupportedbyNSFgrantsCAREERIIS andCCF WethankAdamsWeiYuforprovidinganef cientimplementationoftheprojectionstep andotherusefuldiscussions NearoptimaldesignofexperimentsviaregretminimizationReferencesAgeev AlexanderAandSviridenko MaximI Pipagerounding Anewmethodofconstructingalgorithmswithprovenperformanceguarantee JournalofCombinatorialOptimization AllenZhu Zeyuan Liao Zhenyu andOrecchia Lorenzo Spectralsparsi cationandregretminimizationbeyondmatrixmultiplicativeupdates InProceedingsofAnnualSymposiumontheTheoryofComputing STOC Avron HaimandBoutsidis Christos Fastersubsetselectionformatricesandapplications SIAMJournalonMatrixAnalysisandApplications Balcan MariaFlorinaandLong PhilipM Activeandpassivelearningoflinearseparatorsunderlogconcavedistributions InProceedingsofAnnualConferenceonLearningTheory COLT Bouhtou Mustapha Gaubert Stephane andSagnol Guillaume Submodularityandrandomizedroundingtechniquesforoptimalexperimentaldesign ElectronicNotesinDiscreteMathematics Boutsidis ChristosandWoodruff DavidP OptimalCURmatrixdecompositions InProceedingsofAnnualSymposiumontheTheoryofComputing STOC Cern   MichalandHlad   Milan TwocomplexityresultsonCoptimalityinexperimentaldesign ComputationalOptimizationandApplications Chaudhuri Kamalika Kakade Sham Netrapalli Praneeth andSanghavi Sujay Convergenceratesofactivelearningformaximumlikelihoodestimation InProceedingsofAdvancesinNeuralInformationProcessingSystems NIPS Chen Siheng Varma Rohan Singh Aarti andKova cevi   Jelena Signalrepresentationsongraphs Toolsandapplications arXivpreprintarXiv   ivril AliandMagdonIsmail Malik Onselectingamaximumvolumesub matrixofamatrixandrelatedproblems TheoreticalComputerScience Deshpande AmitandRademacher Luis Ef cientvolumesamplingforrow columnsubsetselection InProceedingsofAnnualConferenceonFoundationsofComputerScience FOCS Deshpande YashandMontanari Andrea Linearbanditsinhighdimensionandrecommendationsystems InProceedingsofAnnualAllertonConferenceonCommunica tion Control andComputing Allerton Dhillon Paramveer Lu Yichao Foster DeanP andUngar Lyle Newsubsamplingalgorithmsforfastleastsquaresregression InProceedingsofAdvancesinNeuralInformationProcessingSystems NIPS Drineas PetrosandMahoney MichaelW OntheNystr ommethodforapproximatingagrammatrixforimprovedkernelbasedlearning JournalofMachineLearningResearch Drineas Petros Mahoney MichaelW andMuthukrishnan   RelativeerrorCURmatrixdecompositions SIAMJournalonMatrixAnalysisandApplications Fedorov ValeriiVadimovich Theoryofoptimalexperiments Elsevier Hazan EladandKarnin Zohar Hardmarginactivelinearregression InProceedingsofInternationalConferenceonMachineLearning ICML Horel Thibaut Ioannidis Stratis andMuthukrishnan   Budgetfeasiblemechanismsforexperimentaldesign InProceedingsofLatinAmericanSymposiumonTheoreticalInformatics LATIN Huang Ruitong Lattimore Tor Gy orgy Andr as andSzepesv ari Csaba Followingtheleaderandfastratesinlinearprediction Curvedconstraintsetsandotherregularities InProceedingsofAdvancesinNeuralInformationProcessingSystems NIPS Khuri Andre Mukherjee Bhramar Sinha Bikas andGhosh Malay Designissuesforgeneralizedlinearmodels areview StatisticalScience Kulesza AlexandTaskar Ben Determinantalpointprocessesformachinelearning FoundationsandTrendsR cid inMachineLearning Li Chengtao Jegelka Stefanie andSra Suvrit Polynomialtimealgorithmsfordualvolumesampling arXivpreprintarXiv Ma Yifei Garnett Roman andSchneider Jeff optimalityforactivelearningonGaussianrandom elds InProceedingsofAdvancesinNeuralInformationProcessingSystems NIPS McMahan HBrendan Followthe regularizedleaderandmirrordescent EquivalencetheoremsandL regularization InProcedingsofInternationalConferenceonArti cialIntelligenceandStatistics AISTATS Miller AlanandNguyen NamKy AFedorovexchangealgorithmfordoptimaldesign JournaloftheRoyalStatisticalSociety SeriesC AppliedStatistics NearoptimaldesignofexperimentsviaregretminimizationPilanci MertandWainwright MartinJ IterativeHessiansketch Fastandaccuratesolutionapproximationforconstrainedleastsquares JournalofMachineLearningResearch Pukelsheim Friedrich Optimaldesignofexperiments SIAM Raskutti GarveshandMahoney Michael Astatisticalperspectiveonrandomizedsketchingforordinaryleastsquares arXivpreprintarXiv Ravi SathyaN Ithapu VamsiK Johnson SterlingC andSingh Vikas Experimentaldesignonabudgetforsparselinearmodelsandapplications InProceedingsofInternationalConferenceonMachineLearning ICML Silva MarcelK Harvey NicholasJA andSato CristianeM Sparsesumsofpositivesemide nitematrices ACMTransactionsonAlgorithms Spielman DanielAandSrivastava Nikhil Graphsparsi cationbyeffectiveresistances SIAMJournalonComputing Su Hao Yu AdamsWei andLi FeiFei Ef cienteuclideanprojectionsontotheintersectionofnormballs InProceedingsofInternationalConferenceonMachineLearning ICML VanderVaart AadW Asymptoticstatistics volume Cambridgeuniversitypress Wang ShusenandZhang Zhihua ImprovingCURmatrixdecompositionandtheNystr omapproximationviaadaptivesampling JournalofMachineLearningResearch Wang YiningandSingh Aarti Anempiricalcomparisonofsamplingtechniquesformatrixcolumnsubsetse lection InProceedingsofAnnualAllertonConferenceonCommunication Control andComputing Allerton   Wang YiningandSingh Aarti Provablycorrectactivesamplingalgorithmsformatrixcolumnsubsetselectionwithmissingdata arXivpreprintarXiv   Wang YiningandSingh Aarti Noiseadaptivemargin basedactivelearningandlowerboundsundertsybakovnoisecondition InProceedingsofAAAIConferenceonArti cialIntelligence AAAI Wang Yining Yu WeiAdams andSingh Aarti Oncomputationallytractableselectionofexperimentsinregres sionmodels arXivpreprints arXiv Woodruff DavidP Sketchingasatoolfornumericallinearalgebra FoundationsandTrendsR cid inTheoreticalComputerScience 