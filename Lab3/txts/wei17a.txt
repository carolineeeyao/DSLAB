SourceTarget Similarity Modelings for MultiSource Transfer Gaussian

Process Regression

Pengfei Wei     Ramon Sagarna     Yiping Ke     YewSoon Ong     ChiKeong Goh    

Abstract

  key challenge in multisource transfer learning is to capture the diverse interdomain similarities 
In this paper  we study different approaches based on Gaussian process models to
solve the multisource transfer regression problem  Precisely  we  rst investigate the feasibility
and performance of   family of transfer covariance functions that represent the pairwise similarity of each source and the target domain  We
theoretically show that using such   transfer covariance function for general Gaussian process
modelling can only capture the same similarity
coef cient for all the sources  and thus may result in unsatisfactory transfer performance  This
leads us to propose TCM SStack  an integrated
strategy incorporating the bene ts of the transfer covariance function and stacking  Extensive experiments on one synthetic and two realworld datasets  with learning settings of up to  
sources for the latter  demonstrate the effectiveness of our proposed TCM SStack 

  Introduction
Transfer learning  TL  methods show specially appealing
for realworld applications where the data from the target
domain is scarce but   good amount of data from another
source domain is available  With research efforts largely
con ned to the singlesource setting  Pan et al    Wei
et al    Zhou et al    an increasing amount of
studies are contributing to   realistic applicability of TL by
addressing the multisource scenario  mainly for classi ca 

 School of Computer Science and Engineering  Nanyang
Technological University  Singapore  RollsRoyce Nanyang
Technological University Corporate Lab  RollsRoyce Advanced Technology Centre  Singapore 
Correspondence to 
Pengfei Wei  Pwei   ntu edu sg  Ramon Sagarna  saramon ntu edu sg 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tion  Tommasi et al    Fang et al    Bhatt et al 
  The problem of regression  however  has been much
less studied  despite of the variety of realworld domains
in which it arises  for instance wi  or indoor signal location  Pan et al    biological data analysis  Lam et al 
  or mechanical system design  Ghosh et al   
In this work  we concentrate on multisource transfer regression  MSTR  based on Gaussian process  GP  models 
All the way through  the TL community has been paying
attention to modeling the similarity between different domains so that only the source knowledge that is helpful
for the target domain is transferred  This is because designing   TL method based on the assumption that domains
are mutually relevant may lead to negative transfer  Pan  
Yang    Similarity capture is particularly crucial in
multisource TL as the transfer capacity to the target task
may differ considerably across the diverse source domains 
Thus  TL methods that are capable of tuning the strength of
the knowledge transfer to the similarity of the domains are
attracting increasing interest  Luo et al    Wang et al 
  AlStouhi   Reddy   
As regards to MSTR    key issue is to capture the diverse
SourceTarget  ST  similarities  The relatively few efforts
to date have focused on ensemble methods  Particularly  an
amount of works rely on the boosting strategy due to its capability to capture  negrained ST similarities be weighting the contribution of train instances individually  Dai
et al    Pardoe   Stone    Yao   Doretto   
However  as outlined in  AlStouhi   Reddy    such
an instancebased similarity strategy in boosting has shown
issues with slow premature weights convergence that have
seriously penalized the computational cost or the transfer
performance  Another type of ensemble strategy for multisource transfer is stacking  Wolpert    Pardoe and
Stone propose   metamodel that aggregates the predictions
of several base models previously learned with each source
in isolation  Pardoe   Stone      The aggregation is
done by assigning each base model   model importance  In
this case  the ST similarities can be captured through the
model importance  However  in such stackingbased methods  the base models suffer from   lack of consideration of
the dependencies between the different source domains 

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

Another popular idea to model the ST similarity is to
construct   transfer covariance function that relates two
data points from distinct domains through the similarity
coef cients  Bonilla et al    Williams et al   
Such idea has been proposed in multitask learning  Bonilla
et al    where each task pair is assigned   particular
similarity coef cient  Note  however  that multitask learning differs from the TL problem in that the former aims at
improving performance across all the domains while the
objective of the latter focuses on the target domain only 
Nevertheless  the idea of transfer covariance function is referential for the TL problem  In  Cao et al      single
source transfer covariance function  TCSS  was proposed 
In the corresponding transfer covariance matrix  one similarity coef cient was assigned to the ST block to model
the interdomain similarity    GP with such TCSS  called
GPTCSS  was then trained for the transfer task 
When generalizing to MSTR  one may naturally consider
  multisource transfer covariance function  TCM    with
different similarity coef cients attached to distinct ST
blocks in the corresponding transfer covariance matrix  In
this work  we investigate the feasibility of such covariance
function  We theoretically prove that   general GP with
TCM    GPTCM    fails to capture the similarity diversities of various ST domain pairs  Although TCM   intends
to utilize different similarity coef cients  the learnt GPTCM   would give the same similarity coef cient for all the
ST domain pairs  The generalization error bounds of the
learnt GPTCM   show that this coef cient is taking effect
in every source domain  Considering the diverse ST similarities between the sources and the target  this may jeopardize the transfer performance  especially when the number of sources increases  Moreover  the learning of GPTCM   rapidly poses   computational issue with increasing amounts of source domains as usually      computations are required to evaluate   model for   data points 
The unsatisfactory performance of GPTCM   leads us to
exploit the transfer covariance function in another way 
Considering that both the stacking strategy and the transfer covariance function can model the ST similarity and
using the transfer covariance function at the base models
would therefore add  exibility to the similarity capture capability of the stacking approach  we propose to integrate
them into one uni ed model  Speci cally  we  rst discuss TCSSStack    method that simply stacks GPTCSS
base models  TCSSStack alleviates the computational issue of GP since it allows to stretch the number of sources
due to its        cost for   sources with   points each 
However  TCSSStack still suffers from the aforementioned
limitation of conventional stacking  Thus  we propose  
more involved TCM SStack  Two salient features make
TCM SStack signi cantly different from TCSSStack      it
associates the similarity coef cients in the base GPTCSS

with the model importance during learning  and  ii  it learns
the model importance and the base GPTCSS jointly  By
doing so  on the one hand  TCM SStack further reduces the
computational cost by lowering the number of optimization
variables  On the other hand  although the similarity coef 
cient in TCM SStack represents bivariate ST similarity relations  they are elicited by pondering all the interdomain
dependencies  In the experiments  we show the superiority of TCM SStack on the transfer performance compared
to TCSSStack  GPTCM    and other MSTR methods 

  Related Work
  main challenge in MSTR is to precisely capture the diverse ST transfer capacities across the different sources 
Ensemble approaches  Dai et al    which can provide an explicit   negrained similarity capture  are widely
used to handle the MSTR problems  In  Pardoe   Stone 
  TrAdaBoost    was proposed    boosting based algorithm that weights the contribution of train instances individually  and thus delivers   model accounting for the ST
similarities for every instance  However  such boostinglike methods suffer from slow premature convergence issues that tremendously jeopardize the transfer performance
 AlStouhi   Reddy    Pardoe and Stone also introduced   multisource transfer stacking in which base models are pretrained in different source domains separately 
and   metamodel is trained by aggregating the outputs of
the base models  Pardoe   Stone    By doing so  the
ST similarities are captured at metamodel level by the
learnt model importance  Although the stacking methods
show success in some MSTR problems  they have the limitation that interdomain dependencies between sources are
ignored at the base models 
At the other end of the spectrum are transfer covariance
function representing   multivariate similarity relation over
sources and target domains    popular representative of
this family is the work by Bonilla et al 
 Bonilla et al 
  on multitask learning  where   freeform kernel relates each pair of tasks  Apart from the difference of the application domain  multitask learning versus TL  this kind
of models often imply  tting an increasingly large number of hyperparameters      
in the freeform kernel  this
number grows as            where   is the number of
sources  Motivated by  Bonilla et al     Cao et al 
  develops another transfer covariance function for the
single source transfer 
In this work  we  rst describe   family of transfer covariance functions  and investigate their feasibility for MSTR 
With the theoretical analysis showing the unsatisfactory
performance of such transfer covariance function  we propose to unify the ST similarity capture of stacking and the
transfer covariance function  To the best of our knowledge 

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

this is the  rst work that analyzes the feasibility and performance of such family of transfer covariance functions for
MSTR  and further combines them with stacking 

  Problem Statement
We denote   domain set for MSTR as           where
     Si              is   set of source domains and
  is the target domain  All source domain data and few
target domain data are labeled  Denote the data matrix
and its corresponding label vector in each source domain
Si as   Si    RnSi   and   Si    RnSi   Likewise  we
represent the target data set with            Tl    Tu 
where   Tl    RnTl   is the labeled target data matrix and
  Tu    RnTu   is the unlabeled one  We further de ne
  Tl    RnTl as the label vector for   Tl  Moreover  we
assume nTl  cid  min nS      nSN   nTu  Our objective is
to utilize    Si    Si  
   and    Tl    Tl  to predict
labels for   Tu 
We use the GP model for this regression task  We denote
the underlying latent function between the inputs   and the
outputs   as    and the noise variance as   Thus    denotes the function vector over      GP model de nes  
Gaussian distribution over the functions             in
which   is the mean vector and   is the covariance matrix which is positive semide nite  PSD  or equivalently
denoted as    cid    Usually   is assumed to be   and
thus the GP model is completely speci ed by   given  
covariance function which is parameterized by  

  GP with Transfer Covariance Function
In this section  we analyze the transfer performance of GP
using   speci   family of transfer covariance function 

  Transfer Covariance Function for MultiSource

Since the GP model is speci ed by    one straightforward way to achieve the knowledge transfer across multiple
source domains and the target domain is to design   transfer covariance function for multisource  Different from  
classical GP which uses    xed covariance function for the
data from different domains  we focus on the covariance
function of the form  TCM   

   ik      cid        Si      cid         

or             cid      Si 

       cid   

       cid 

otherwise 

 
where    is any valid covariance function  and    is the
metric measuring the similarity between the source Si and
the target     Through the learning     is expected to capture the different transfer strengths in different ST domain
pairs  Those highly targetrelated sources will play   more

important role in transfer  while those completely targetunrelated sources will not be considered  However  to guarantee the GP model is always valid  any covariance matrix
   constructed by    should be PSD  Theorem   gives
the suf cient and necessary condition for   PSD   
Theorem   Let KDiDj  Di Dj      denote   covariance
matrix for points in Di and Dj    Gram matrix

 

  KS SN
 
  KSN SN
     KT SN

 KS  

 

   KSNT

KT  
is PSD for any covariance matrix   in the form

    

 

KSNS 
 KT   

  KS   
  KS   

 

KSNS 
KT   

   

 

 

 

  KS SN KS  
 
 
  KSN SN KSNT
  KT SN KT  

if and only if            and        

Proof  Necessary condition  Let    be   PSD matrix  We
use KSS to represent the sourcesto sources block matrix 
and KST   KST   to represent the sourcesto target block
matrix in   and    respectively  Thus  we have 

 cid  KSS KST

KTST KT  

 cid 

   

      

 cid  KSS KST  

KTST   KT  

 cid 

 

Since   is PSD  according to the Schur complement theorem  Zhang    we have 

     KSS cid KSS  KST    
KT     KTST  cid KSS KST  cid   

where  cid KSS is the generalized inverse of KSS  By rewriting  cid KSS as   block matrix using  cid KSiSj as the element  we

 

further derive eq    and eq    as 

 

     

 

  

  

  

  

  cid 
KS       cid 
KS Si cid KSiSj KSjT
  cid 
KSNT     cid 
KSNSi cid KSiSj KSjT
KT       cid 
  cid 
KT Si cid KSiSj KSjT  cid   
 KS       cid 
   KSNT     cid 

 jKS Si cid KSiSj KSjT
 jKSNSi cid KSiSj KSjT

  cid 
  cid 

  

  

  

  

 

 

  

  

     

Likewise  for the PSD matrix    we have the following
two Schur complement derivations 

 

 

 

 

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

 cid 

 cid  KSS

 cid 

 cid    

 

the noisy training data is             where

    

 KST
 KTST KT  

     

 

   IT  

IS       SN

and    is   diagonal block matrix with the diagonal block
ISNSN  According to  Raselements    
mussen    if the GP prior is correctly speci ed  the
generalization error at   point is also the posterior variance
at such point  Speci cally  for GPTCM    the posterior
variance at the target point xt is 
    xt     Si   Si
  ktt   kT tC      

  
   XT       

 

where kT      kTSt  kTT    kSt  kT    is the vector of covariances between    Si  
          and xt  and ktt is
the prior variance at xt  Wherever it is not misleading 
we will simplify the posterior variance expression using
  
    Si
        The generalization error for the target domain can be obtained by averaging eq    over xt 

  
   XT       
  
    xt   Si
         xt dxt 

 

To derive the generalization error bounds for GPTCM   
we  rst rewrite

 cid 
      Si   Si
 
 cid   KSS       
 cid   ISS
 cid 

KTST
 
IT  

KST

KT        IT  

 

  Thus  the posterior variance

 cid 

      

 

where    
at point xt becomes 
    Si
where kT

   jKT Si cid KSiSj KSjT  cid   

  cid 

  

  

KT       cid 
  cid 
  cid 
  cid 
  cid 

  

  

  

  

 

Combining eq    and eq    we get 

       KS Si cid KSiSj KSjT
        KSNSi cid KSiSj KSjT

 

     

 

 

Since Eq    must hold for all PSD    we induce    
           Based on such conclusion  we combine eq 
  and eq   

     KT         cid   

where     KT       cid 

  cid 

 

KT Si cid KSiSj KSjT   Since eq 

  

  

  must hold for all PSD KT   and PSD    we resolve
that      
Suf cient condition  Let                and      
According to the Theorem   in  Cao et al    we obtain
that    is   PSD matrix 
To sum up  we conclude that if    is   PSD matrix    
should satisfy            and        
From Theorem   we can see that         which means
  highly targetrelated source results in   full transfer of
KSi     but   completely targetunrelated source results in
  zero block matrix  This indicates the adaptiveness of    
However  Theorem   also shows that    can just give
one similarity coef cient for all ST domain pairs to ensure
the validity of GPTCM    Such single similarity coef 
cient compromises the diverse similarities between different ST domain pairs  This violates the original intention of
   which is to distinguish the similarity diversity between
different ST domain pairs 

  Generalization Bounds of GPTCM  
To investigate the effect of   single compromised similarity coef cient on the performance of GPTCM    we derive its generalization error bounds 
In  Chai    an
earlier analysis can be found for the generalization errors
and learning curves in multitask learning  speci cally  two
learning tasks with the same noise variance  Our investigation is different from that work however as we are working
on   TL setting  and more importantly  on multiple sources
with different noise variances 
We denote the single compromised similarity coef cient as
      
  and the noise variance for different domains as  
     SN      Thus  the transfer covariance matrix of

  
          ktt   kT

   Si

  
       kt 
 

     kTSt  kTT    and
 Si
  
       
 

 cid   KSS       

KTST

 cid 

 

KST

KT        IT  

Note that the above derivation excludes the situation where
      When       all the source domains are unrelated
to the target domain  and thus no knowledge is transferred 
This is easy to verify by plugging       into eq   
Further  we observe that    is equal for   and   so we
only investigate the case         For eq    we further
decompose it as 
 Si
 

 cid  KSS KST

 cid     ISS
 cid  KSS       
 cid 

 cid          ISS  

  
       

KTST KT  

   IT  

     
                               

 cid 

 cid 

 cid 

 

 

 

 

 

 

 

 

 

 

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

 cid  KSS       

 cid 

 

 

and     

where           

 cid          ISS  

 cid 

 

 

  Eq    unveils that the posterior
variance of having instances from different source domains
is equivalent to the posterior variance of having those instances from target domain with two additional correlated
noise terms     and    This shows us the two main factors that matter in the transfer performance  namely  the
ST similarity and the noise variances  To further analyze
how the ST similarity affects the transfer performance  we
focus on one factor and    the other  Assuming that all the
sources are totally related to the target             and consequently  the noise variance for each source becomes  Si
 
we de ne the difference 
  
              Si

  
       
To obtain the upper  lower  bound of     Si
we are interested in those  
  for all the target points 
Proposition   Let   and   be the maximum and mini 
       
   Si
mum eigenvalues of KSS   
        for every source Si 
and  
Si
         
  
Then  for all the target data points      
Si
    Si
  
       

  
       
  that make          

  
              

        Si

   Si

Si  
Si

 
Si

 
Si

 

Proof  By applying eq    we have 
              Si
        Si
  
  
       
           Si
   Si
  
  
       kt
  kT
To make       for all the target data points  we need
           Si
to prove  Si
  
  
        is
PSD  which means 

 Si
 
 
where  cid 
block elements    
 

  
           Si
 Si

 cid       KSS    cid 

         cid   
  
 cid 

  
       
          
 

  
         cid   Si

 cid   

 

  is   diagonal block matrix with the diagonal

IS       SN
     KSS    cid 

ISNSN 
          cid   

 
Note that   is   monotonically increasing function of  Si
thus we take the minimum  Si
Si to
be the smallest upper bound of     Si
  
        Sim 
        to construct
ilarly  we have  
Si
  
the largest lower bound of     Si
       

        as  

   Si

 

  
  
              
 
       
Si
              
  
  
       
Si
  
  
              Si
         
  
       

Proposition   gives the lower and upper bounds of the pos 
  
terior variance     Si
        By applying eq   
we readily obtain the generalization error bounds 
Corollary   Let
    Si
    Si
Then      Si
    Si
Proposition   serves to demonstrate that   takes effect in
every source on the  nal transfer performance  With the
assumption that different source domains have different ST similarities with the target domain    single   that works
for every sources has   great dif culty capturing such ST similarity diversity  This leads us to exploit the transfer
covariance function in another way 

  Transfer Covariance Function Stacking
Considering the effectiveness showed by the stacking strategy for MSTR  Pardoe   Stone    we propose   framework that can integrate the capability for ST similarity capture of both the transfer covariance function and stacking 
We  rst introduce TCSSStack    conventional way of stacking the transfer covariance function  Then  we design  
more involved stackinginspired approach that overcomes
some limitations of the conventional stacking method 

  Conventional Transfer Stacking TCSSStack
Motivated by the fact that both the stacking strategy and
the transfer covariance function can model the ST similarity and using the transfer covariance function at the
base models would therefore add  exibility to the similarity capture capability of the stacking approach  we propose   TCSSStack method 
In TCSSStack  we  rst train
multiple GPTCSS models using each Si and    denoted
as     Si           
   and then apply the conventional
stacking strategy to combine their predictions  Given   target point    the  nal prediction is given by 

 cid  

 cid  

  

KSS  cid        cid 
     

   

 

 
   Si

   Si
     

     Si
   Si

 

  or every Si

       

 if  Si            

        

  

          or every Si

where    are coef cients learned by minimizing the least
square error on the target labeled data 

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

There are two major issues for the above model    Since
each    Si     is pretrained separately  the parameters learnt
for each    Si     do not take the interdomain dependencies between different source domains into account 
 
Both    and    re ect the ST domain similarity  However  TCSSStack takes them as two different variables and
learns them separately  Intuitively  the model importance
   should be positively correlated with the similarity coef 
 cient     For example  the prediction of   GPTCSS using
an unrelated source is less trustful  and should be assigned
  smaller coef cient in the stacking 

 cid  

  Improved Transfer Stacking TCM SStack
To overcome the above issues  we propose   new transfer
stacking model  TCM SStack  as follows 

 

      

          Si              

  

for the ith source       cid  

where    refers to the similarity coef cient in the GPTCSS
        is the normalization
term  and      is any function preserving the monotonicity
of     so that it coordinates the model importance and the
similarity coef cient  This also reduces the search efforts
by lowering the number of free parameters to     Moreover  instead of pretraining    Si           separately  we
jointly learn    Si             for all the source domains 
By doing so  the multiple GPTCSS models are learned
together with the dependencies between multiple sources
taken into account 
Notice that the model in eq    allows for multiple options to characterize the relative importance of GPTCSS
models through    In this paper  we use   simple function            However  the absolute value function is
not smooth at the origin  Thus  we use   smooth function
studied in  Yong    to approximate it as follows 

       Ln 

  
   

 

 
 

     

   

 
 

We set       which is the best approximation stated in
 Yong    Since Theorem   also tells us           
we propose to de ne             bi           
and bi     as in  Cao et al    Then  we conduct the
learning by minimizing the squared errors 

 cid nTl

  

min

     bi  

  

   Tl 
        Tl 

 

 

 

 

 

In the optimization  we propose to use the conjugate gradient method  Other optimization methods can also be applied to solve this objective function 

  Complexity Analysis

As in usual GP model training  the computational time
complexity of each    Si     is dominated by the calculation

Table   Amazon review products dataset  

Top category

Source domains

Beauty Health Grocery

Beauty  Grocery Food  Health

Electronics

Electronic  Of ce Product  Kindle Store

Home Garden Tool
Movies Music Game

Kitchen  Pet Supplies

CD Vinyl  Digital Music  Video Games

Movies TV

Target domain

Clothing Shoes Jewelry
Cellphone Accessory

Tools Home Improvements

of the inverse of its covariance matrix         nTl  nSi  
Considering nTl  cid  nSi and assuming nS       
nSN   nS  the evaluation of   TCM SStack model takes
then           Notice that by following the stacking strategy of eq    the training involves the steps of learning
each    Si     and subsequently learning the    coef cients 
The latter calls for some crossvalidation approach to evaluate   metamodel  as the    Si     from the previous step
have been induced using the target data  Pardoe   Stone 
 
In the extreme case of leaveone out  this would
take   nTl         Even if we also choose   leaveone out
validation to solve eq    the cost of   TCM SStack model
evaluation would be lower  since the  rst step of stacking
is not required  On the other side  by following TrAdaBoost    or the GPTCM   approach    GP model evaluation requires     nS   which even exceeds the cost
for TCM SStack using leaveone out whenever    
nTl 

 

  Experimental Study
In the following experimental study we aim at two main
goals      to assess the ability of TCM SStack in capturing
interdomain similarity  and  ii  to evaluate its predictive
effectiveness compared to other approaches 

  Experiment Setting

All the GPs herein build upon   standard squared exponential covariance function  The hyperparameters of each
method are optimized using the conjugate gradient implementation from the gpml package  Rasmussen   Nickisch 
  For each search  we allow   maximum of   evaluations  The reported results correspond to the model providing the best objective function value over   independent
runs with random initial solutions each  We use one synthetic dataset and two realworld datasets 
Synthetic dataset  We consider   linear function        
        where         and   is   zeromean Gaussian
wT
noise term  as the target  We use this function to generate
  points as target test data  and   points as target train
       
data  For the source task  we use         wT
where    is   random  uctuation vector and   is the variable controlling the similarity between   and    higher  
indicates lower similarity  to generate   points for each
source with different  
Amazon reviews  We extract the raw data containing  

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

product reviews from  McAuley et al    and categorize the products into four top categories according to the
Amazon website  Products in the same category are conceptually similar  Each product is taken as   domain  and
we select one as target from each category  see Table  
Reviews in each domain are represented by the count features and are labeled using stars in the set          
UJIIndoorLoc  The building location dataset covers three
buildings of Universitat Jaume   with four  oors each
 TorresSospedra et al    We build   domains by
taking the location data from each  oor of each building as
  domain  The  rst  oor of each building is taken as the
target  Domains from the same building are taken as similar  The received signal strength intensity from   wireless access points is used as the features  and the location
represented by the latitude and longitude is taken as label 

  Domain Similarity Capture

We  rst elucidate the ability of TCM SStack in capturing
the diverse ST similarities through the    coef cients  To
rationalize the assessment  we use the synthetic dataset
and consider   variety of problems covering   broad spectrum of TL settings  Precisely  we build four scenarios
of             sources  In each scenario  we specify six problems  each given by   different combination of
sources  Three problems represent settings in which all the
sources are equally similar to the target  with high      
medium       and low       similarity strength 
The other three problems re ect diverse ST similarities 
Each source is given by     randomly sampled from the set
                  and with replacement  We
enforce the three problems to be different and avoid all the
sources to be equal  We show the results in Figure  
In the  gure  the  rst three problems of each scenario are
the cases with equal ST similarities  It can be observed in
the bar plots on the left hand side that the   values learnt
by TCM SStack are strictly reversecorrelated with the prede ned   values  which indicates an accurate capture of the
high  medium and low strengths of ST similarity  We further observe from the black dots that GPTCM   is also able
to strongly coordinate   with   single compromised   This
is because all the sources share the same   with the target 
and thus can be regarded as   single larger source 
The remaining three problems of each scenario re ect diverse similarities across source domains  In this case  Figure   shows that the   values of TCM SStack re ect the relative differences of   across different sources fairly well
in general  The learnt   is generally reversecorrelated
with the prede ned   values  but it is not strict and tight 
For instance  in problem   of the  sources scenario or
in the problem   of the  sources scenario  such reversecorrelated relations do not hold in all the sources  This is

  Sources

  Sources

  Sources

  Sources

Figure   Results on six problem settings for each scenario of    
  and   sources  Bar plots on the left show the correspondence
between each problem similarity   and the similarity captured
by the model   bars for TCM SStack and black dots for GPTCM    Bar plots on the right show the RMSE for TCM SStack
 red  and GPTCM    green 

because  although the learnt    only represent the bivariate
ST similarities  each of them is speci ed during learning
by considering its in uence relative to the rest of similarity coef cients       the interdomain dependencies between
different sources are taken into account during the learning
of the     Thus  in some cases  the learnt    may not strictly
approximate the real ST similarities  However     are always learnt to guarantee the outcome of the best transfer
performance  That is the reason why the above two cases

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

the different amounts of source domains  This showcases
the capability of TCM SStack to transfer knowledge from
various sources with different ST similarities 
For the other baselines  we observe that TrAdaBoost   
gives the poorest results due to the premature weights convergence issue  If  in addition  we consider the high computational cost for the models involved  TrAdaBoost   
does not seem to be   good choice for MSTR  especially
when the number of source domains is large  As for GPTCM    it presents   steadily inferior performance than
TCM SStack  Overall  the outcomes are in line with those
in the synthetic dataset  offering further support to the superiority of TCM SStack to GPTCM    Notice that  since
the current benchmark was generated randomly  it is likely
  scenario to comprise diverse problem settings  Therefore 
capturing the diverse similarities through   single   coef 
cient may compromise the performance of GPTCM    As
opposed to GPTCM    TCM SStack offers more robust performance improvements 
Finally 
the comparison with the other stackingbased
methods exposes the bene ts of the two salient features of
TCM SStack  Both TCSSStack and  Stacking are beaten
by TCSSStackJoint and TCM SStack  Since these two sets
of methods only differ in the joint learning of the parameters  the outcomes point at the bene ts of bringing in
the interdomain dependencies of the other sources during the learning  On the other side  the results for  
Stacking and TCM SStack are better or comparable to those
by TCSSStack and TCSSStackJoint  repectively  This provides support to the correlation of the model importance
with the similarity coef cients  which allows to specify
the model by estimating fewer hyperparameters while preserving the similarity capture capability 

  Conclusions
We investigate   family of transfer covariance functions
that represent the pairwise similarity between each source
and the target domain for the MSTR problem  We prove
that  GPTCM      Gaussian process with such   transfer
covariance function can only capture the same similarity
coef cient for all the sources  By further analyzing the generalization errors of GPTCM    we conclude the bounds
depend on the single similarity coef cient  which may penalize the transfer performance  As an alternative  we propose TCM SStack  an approach that integrates the transfer
covariance function and the stacking strategy into one uni 
 ed model  TCM SStack aligns the ST similarity coef 
 cients with the model importance and jointly learns the
base models  Extensive experiments on one synthetic and
two realworld datasets  with learning settings of up to  
sources for the latter  show the superiority of TCM SStack
to other MSTR methods 

Figure   Comparison results on the two datasets 

still achieve satisfactory transfer performance in terms of
RMSE  By contrast  we  nd that GPTCM   only gives  
tradeoff value of   over the diverse   values  The right
hand side of the  gure shows   consistently lower RMSE
for TCM SStack than for GPTCM   in all the problems  In
particular    dramatic improvement is observed by utilizing TCM SStack for the diverse problems   These results
indicate the superiority of TCM SStack over GPTCM   for
MSTR  We further verify this conclusion on the two realworld datasets in the next section 

  Performance on RealWorld Datasets

We compare TCM SStack with several MSTR approaches 
namely  Tradaboost    GPTCM    TCSSStack    variant
of TCSSStack with joint learning of model importance coef cient and   which we call TCSSStackJoint  and   variant of TCSSStack using the learnt   as the model importance which we call  Stacking  The evaluation comprises
both the Amazon and the UJIIndoorLoc datasets  For each
source domain we sample   points uniformly at random
for training  Likewise  train and test data from each target domain are obtained by sampling   points and  
points  respectively  For the Amazon dataset  we generated   set of problems by using each target domain in Table
  and by randomly choosing   number of source domains
subsets  More precisely  for each scenario of     and  
sources  ten different source combinations were randomly
constructed  In addition    scenario of   sources with all
the source domains was selected  Thus  we construct  
transfer problems for each scenario of     and   sources 
and   problems for the scenario of   sources  For the UJIIndoorLoc dataset  we generate the transfer problems in  
similar way to the Amazon dataset described above 
In Figure   we show the average RMSE results over all
the problems in each scenario for the two datasets  Overall  TCM SStack is the winner among all the baselines on
the two datasets  improving the transfer performance across

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

Acknowledgments
This work was conducted within the RollsRoyce Nanyang Technological University Corporate Lab with support
from the National Research Foundation  NRF  Singapore
under the Corp Lab University Scheme 
It is also partially supported by the School of Computer Science and
Engineering at Nanyang Technological University 

References
AlStouhi  Samir and Reddy  Chandan  Adaptive boosting
for transfer learning using dynamic updates  Machine
Learning and Knowledge Discovery in Databases  pp 
   

Bhatt  Himanshu Sharad  Rajkumar  Arun  and Roy  Shourya  Multisource iterative adaptation for crossdomain
classi cation  In Proceedings of the TwentyFifth International Joint Conference on Arti cial Intelligence  pp 
  AAAI Press   

Bonilla  Edwin    Chai  Kian    and Williams  Christopher  Multitask gaussian process prediction  In Platt 
      Koller     Singer     and Roweis         eds 
Advances in Neural Information Processing Systems  
pp    Curran Associates  Inc   

Cao  Bin  Pan  Sinno Jialin  Zhang  Yu  Yeung  DitYan 
In Proand Yang  Qiang  Adaptive transfer learning 
ceedings of the TwentyFourth AAAI Conference on Arti 
 cial Intelligence  AAAI  pp    AAAI Press 
 

Chai  Kian    Generalization errors and learning curves
for regression with multitask gaussian processes 
In
Advances in neural information processing systems  pp 
   

Dai  Wenyuan  Yang  Qiang  Xue  GuiRong  and Yu 
Yong  Boosting for transfer learning  In Proceedings of
the  th International Conference on Machine Learning 
ICML   pp    New York  NY  USA   
ACM  ISBN  

Fang  Min  Guo  Yong  Zhang  Xiaosong  and Li  Xiao 
Multisource transfer learning based on label shared subspace  Pattern Recognition Letters     

Ghosh  Sayan  Jacobs  Ryan  and Mavris  Dimitri    Multisource surrogate modeling with bayesian hierarchical regression  In  th AIAA NonDeterministic Approaches
Conference  pp     

Lam  Kari    Westrick  Zachary      uller  Christian   
Christiaen  Lionel  and Bonneau  Richard  Fused regression for multisource gene regulatory network inference 
PLOS Computational Biology     

Luo  Ping  Zhuang  Fuzhen  Xiong  Hui  Xiong  Yuhong 
and He  Qing  Transfer learning from multiple source
domains via consensus regularization  In Proceedings of
the  th ACM conference on Information and knowledge
management  pp    ACM   

McAuley  Julian  Targett  Christopher  Shi  Qinfeng  and
van den Hengel  Anton  Imagebased recommendations
In Proceedings of the  th
on styles and substitutes 
International ACM SIGIR Conference on Research and
Development in Information Retrieval  pp    ACM 
 

Pan  Sinno Jialin and Yang  Qiang    survey on transfer learning  Knowledge and Data Engineering  IEEE
Transactions on     

Pan  Sinno Jialin  Kwok  James    and Yang  Qiang  Transfer learning via dimensionality reduction  In AAAI  volume   pp     

Pan  Sinno Jialin  Tsang  Ivor    Kwok  James    and
Yang  Qiang  Domain adaptation via transfer component analysis  IEEE Transactions on Neural Networks 
   

Pardoe  David and Stone  Peter  Boosting for regression
transfer  In Frnkranz  Johannes and Joachims  Thorsten
 eds  Proceedings of the  th International Conference
on Machine Learning  ICML  pp    Omnipress   

Rasmussen  Carl Edward  Gaussian processes for machine

learning  Citeseer   

Rasmussen  Carl Edward and Nickisch  Hannes  Gaussian
processes for machine learning  gpml  toolbox  Journal
of Machine Learning Research    December   ISSN  

Tommasi  Tatiana  Orabona  Francesco  and Caputo  Barbara  Learning categories from few examples with multi
IEEE transactions on patmodel knowledge transfer 
tern analysis and machine intelligence   
 

TorresSospedra  Joaqu    Montoliu  Ra ul  Us    Adolfo Mart nez  Avariento  Joan    Arnau  Tomas   
BeneditoBordonau  Mauri  and Huerta  Joaqu    Ujiindoorloc    new multibuilding and multi oor database
for wlan  ngerprintbased indoor localization problems 
In International Conference on Indoor Positioning and
Indoor Navigation  pp    IEEE   

Wang  Qifan  Ruan  Lingyun  and Si  Luo  Adaptive
knowledge transfer for multiple instance learning in image classi cation  In AAAI  pp     

SourceTarget Similarity Modelings for MultiSource Transfer Gaussian Process Regression

Wei  Pengfei  Ke  Yiping  and Goh  Chi Keong  Deep nonlinear feature coding for unsupervised domain adaptation  In In Proceedings of the TwentyFifth International
Joint Conference on Arti cial Intelligence  pp   
  AAAI Press   

Williams  Christopher  Klanke  Stefan  Vijayakumar  Sethu  and Chai  Kian    Multitask gaussian process
learning of robot inverse dynamics  In Advances in Neural Information Processing Systems  pp     

Wolpert  David    Stacked generalization  Neural Networks    February   ISSN  

Yao  Yi and Doretto  Gianfranco  Boosting for transfer
learning with multiple sources  In Computer vision and
pattern recognition  CVPR    IEEE conference on 
pp    IEEE   

Yong  Longquan  Uniform smooth approximation functions for absolute value function  Mathematics in practice and theory  pp     

Zhang  Fuzhen  The Schur complement and its applications  volume   Springer Science   Business Media 
 

Zhou  Joey Tianyi  Pan  Sinno Jialin  Tsang  Ivor    and
Ho  ShenShyang  Transfer learning for crosslanguage
text categorization through active correspondences construction  In AAAI  pp     

