Localto Global Bayesian Network Structure Learning

Tian Gao   Kshitij Fadnis   Murray Campbell  

Abstract

We introduce   new localto global structure
learning algorithm  called graph growing structure learning  GGSL  to learn Bayesian network
 BN  structures  GGSL starts at    random  node
and then gradually expands the learned structure
through   series of local learning steps  At each
local learning step  the proposed algorithm only
needs to revisit   subset of the learned nodes 
consisting of the local neighborhood of   target  and therefore improves on both memory and
time ef ciency compared to traditional global
structure learning approaches  GGSL also improves on the existing localto global learning
approaches by removing the need for con ictresolving ANDrules  and achieves better learning accuracy  We provide theoretical analysis
for the local learning step  and show that GGSL
outperforms existing algorithms on benchmark
datasets  Overall  GGSL demonstrates   novel
direction to scale up BN structure learning while
limiting accuracy loss 

  Introduction
Bayesian networks have been used in classi cation  Aliferis et al    feature selection  Gao et al    latent
variable discovery  Lazic et al    Gao   Ji     
and knowledge discovery  Spirtes et al    Gao   Ji 
  in various domains  Ott et al    However  due
to its NPhard nature  Chickering et al    exact BN
structure learning on directed acyclic graphs  DAG  faces
scalability issues 
In this paper  we consider   localto global approach to
learn the Bayesian network structure  starting from the local graph structure of one node and then gradually expanding the graph based on already learned structures 

 IBM Thomas    Watson Research Center  Yorktown
Heights  NY   USA  Correspondence to  Tian Gao
 tgao us ibm com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

The predominant exact scorebased structure learning algorithms adopt the global approach and focus on better scoring criteria  Acid et al    Brenner   Sontag    or more ef cient search procedures  Chickering    Koivisto   Sood    Silander   Myllymaki 
  Jaakkola et al    Cussens    Yuan   Malone    to navigate the intractable search space of possible directed acyclic graphs over all the variables present 
Despite such progress  the practical usage of these algorithms is still limited when there are large numbers of variables  Many approximations  Scanagatta et al    constraints  de Campos et al    Chen et al    and
assumptions  Nie et al    are utilized to alleviate time
and memory complexity 
Instead of searching the entire DAG space for all the variables at the same time  the localto global approach limits the size of the space by learning   local structure with
only   limited number of variables  These variables consists of potential candidates for local structures  usually de 
 ned by the parentchild  PC  or the Markov Blanket  MB 
 Pearl    set of   target node in   DAG  Many local
learning algorithms  Koller   Sahami    Tsamardinos et al    Fu   Desmarais    iteratively query
new variables to update and learn the local structure  either PC  MB set or both  for one speci   target variable 
Many constraintbased and scorebased local learning algorithms have been proposed and shown to have promising performances in practice  Aliferis et al    Using
learned local structures  some prior works have proposed to
combine the local structures for the global structure  Several works  Margaritis   Thrun    Pellet   Ellisseeff 
  have proposed algorithms to identify MBs of every node in the graph  rst  and then connect the MBs in
  maximally consistent way to learn the global structure
of   BN  Both constraintbased  Tsamardinos et al   
and scorebased localto global structure learning methods
 Niinimaki   Parviainen    have been proposed  This
localto global approach has the bene   of improving the
exact structure learning ef ciency  as at each step only  
small number of variables are expected to be used  although
the accuracy has not been very competitive 
We aim to improve the accuracy of the localto global approach and propose   new localto global structure learning
algorithm  The algorithm starts the local learning at one

Localto Global BN Structure Learning

variable  rst  and then iteratively applies the local learning procedure to its neighbors and so on  gradually expanding the learned graph with minimal repeated learning 
GGSL ef ciently grows local graphs to global graphs without considering the traditional the ANDrule  or   consistency check on learned local neighborhoods  It uses only
the necessary variables for each local learning step to learn
and resolve any possible con icts among local structures 
hence improving ef ciency over global learning algorithms
and improving accuracy over existing localto global algorithms with the ANDrule 
Notation  We use capital letters  such as        to represent
variables  small letters  such as       to represent values
of variables  and bold letters  such as    MB  to represent
variable sets      represents the size of   set          and
      represent independence and dependence between
  and     respectively 

  Technical Preliminaries
Let   denote   set of random variables    Bayesian Network for   is represented by   pair       The network
structure   is   directed acyclic graph with nodes corresponding to the random variables in    If   directed edge
exists from node   to node   in      is   parent of  
and   is   child of    The parameters   indicate the conditional probability distribution of each node       given
its parents  Moreover  let   path between two nodes   and
  in   be any sequence of nodes between them such that
any successive nodes are connected by   directed edge  and
no node appears in the sequence twice    directed path
of   DAG is   path with nodes       Vn  such that  for
           Vi is   parent of Vi  If there is   directed path
from   to     then   is an ancestor of   and   is   descendant of    If   and   have   common child and they
are not adjacent    and   are spouses of each other  Three
nodes        and   form   Vstructure if node   has two
incoming edges from   and    forming           
and   is not adjacent to      is   collider if   has two incoming edges from   and   in   path    path   from node
  to   is blocked by   set of nodes    if any of following
holds true    There is   noncollider node in   belonging
to      There is   collider node   on   such that neither  
nor any of its descendants belong to    Otherwise    from
  to   is unblocked or active 
The Local Markov Condition  Pearl    states   node
in   BN is independent of its nondescendant nodes  given
its parents  It enables the recovery of   distribution    in
term of independence relationships  from   known DAG
     DAG   and   joint distribution   are faithful to each
other if all and only the conditional independencies true in
  are entailed by    Pearl    The faithfulness condition enables us to recover   DAG   from   distribution  

to completely characterize   
  Markov Blanket of   target variable     MBT   is the
minimal set of nodes conditioned on which all other nodes
are independent of     denoted as       MBT     
         MBT  Given independently and identically distributed         samples   from an unknown distribution
   represented by   faithful but unknown DAG    to   
local structure learning is to  nd the PC or MB of   target
node in    To avoid symbol confusion  we use   to represent any learned DAG and use    to represent the ground
truth DAG  Under the faithfulness assumption between   
and    the PC and MB of   target node is uniquely identi able  Pearl    For example  in Figure     nodes  
and   form PCB  MBB contains its parent node    its
child    and its spouse    All other nodes        and  
are independent of    given MBB  due to blocked paths 

Figure   Sample Bayesian Network     Dark node   is the target
node and the shaded nodes       and   are the Markov Blanket
of         learned DAG is shown  where the dashed edges can
have different orientations while still being Markov Equivalent to
the DAG in   

Scorebased structure learning algorithms rely on some
score criteria   to learn   best tting DAG   for data   
Score         of   BN DAG structure   measures the
goodness of    of   on    Let   be any BN structure
and   cid  be the same structure as   but with an edge from
  node   to   node    Let PaG
  be the parent set of   in
   Score   is locally consistent if  as the size of the data
  goes to in nity  the following two properties hold true 
  if       PaG
   then               cid     and   if
      PaG
   then               cid     In addition    is
score equivalent if Markov equivalent DAGs have the same
score    is decomposable if it is   sum of each node   individual score that depends on only this node and its parents 
Commonly used Bayesian score criteria  such as BDeu  are
decomposable  consistent  locally consistent  Chickering 
  and score equivalent  Heckerman et al    We
assume the Markov condition  faithfulness condition  and
the in nite data size hold in the theoretical analysis part of

ADEBFHCADEBFHC Localto Global BN Structure Learning

the paper 
Lastly  one of the main concepts in the topologybased MB
algorithms is the symmetry constraint  or the ANDrule 
Lemma   ANDRule  For   node   to be adjacent to  
in    both of the following statements hold true    must
be in the PC set of   and   must be in the PC set of        
    PCG

  and     PCG
  

The localto global BN structure learning algorithms generally use the ANDrule to enforce consistency between
different learned local structures to obtain   global DAG
 Margaritis   Thrun    Local structure learning algorithms also employ it to guarantee soundness  Niinimaki  
Parviainen   

  Localto Global BN Structure Learning
We will  rst introduce local BN structure learning and provide some new theoretical guarantees  then propose   novel
procedure to expand the local graph to the global graph 
including some consistency guarantee and the proposed
GGSL algorithm 

  Local Structure Learning

learning approach  rst uses

loThe localto global
cal structure learning algorithms  either constraintbased
 Tsamardinos et al    or scorebased  Niinimaki  
Parviainen    to discover the PC set or the Markov
Blanket of the target  The arguably stateof art algorithms
to  nd the local structure of Bayesian network use   scorebased framework  Gao   Ji    shown in Algorithm  
LocalLearn 
In Algorithm   subroutine BNStructLearn learns an
optimal DAG over   set of variables in the data  and
can use any exact global BN structure learning algorithm 
Subroutine findPC and findSpouse extract   variable
     PC set  by  nding parent set   and children set   
and spouse set given the adjacency matrix of   graph   
LocalLearn  rst sequentially learns the PC set by repeatedly using BNStructLearn on   set of nodes   containing the target node     its current PC set PCT   and one
new query variable    Then it uses   similar procedure to
learn the spouse set and update the PC set  PCG
  is guaranteed to contain all the true positive PC nodes of    
Lemma   Preservation of True Positive PCs  Niinimaki
  Parviainen    Let    be the faithful DAG of distribution   over    and   be the DAG learned by exact BN
structure learning algorithms over the subset of variables
ZL     at the last iteration of Step   of Algorithm   Let
  be the learned PC set of the target   in   and PC 
PCG
 
be the PC set of   in    Under the faithfulness and in nite
data assumption  PC 

    PCG
   

Algorithm   LocalLearn
Input  dataset    target node  
 step    nd the PC set  
PCT                
while   is nonempty do
choose                   
             PCT  
    BNStructLearn     DZ 
PCT   PT   CT   findPC         

end while
 step   remove false PC nodes and  nd spouses 
ST             PCT  
while   is nonempty do

choose                   
             PCT   ST  
    BNStructLearn     DZ 
PCT   PT   CT   findPC         
ST   findSpouse         
end while
Return  MB   PT   CT   ST  

However  PCG
  may contain false positive PC nodes  Aliferis et al    as shown in Figure   of  Niinimaki  
Parviainen    The iterative nature of Algorithm   can
potentially violate the faithfulness assumption during the
learning  due to absent variables  Previous analysis  Niinimaki   Parviainen    Gao   Ji    conjectured
the soundness and completeness of LocalLearn  Here
we provide   new theoretical proofs of these results in the
LocalLearn  
Lemma   Preservation of Dependence Relationships between   and the Learned PC Set  Let    be the global
faithful DAG of   for the entire variable set    and   be
the DAG learned by exact BN structure learning algorithms
over   subset of variables of    VG  present at the last iteration of Step   of Algorithm   Then in    every variable
in the learned PC set PCG
  is dependent of the target   
conditioned on any subset of the ground truth PC set      
            PCG
Proof  If     PC 
    then the lemma holds automatically  Else if    cid  PC 
    assuming        such that
             where     VG       Then  one of the
following two cases must hold in          or        
If        since each node in ChildrenX   SpousesX
is either   not saved during the iterative procedure  or  
saved as   node of PCG
    in which case it forms   fully
connected subgraph with   and   and can be changed to  
ParentX  Then                    PaG
    by MB de 
nition  Since                           by assumption
             then    cid  PaX since         must contain PaG
   Then by local consistency removing the edge
      will increase the score  which contradicts the as 

       PC 

       

Localto Global BN Structure Learning

sumptions  If         since                         
by assumption  then using   similar argument     cid  PaG
and removing the edge       will increase the score 
which contradicts the assumption  Hence              
Since     PC 

                  the lemma holds 

 

Lemma   shows that the false PC nodes consist of only
descendents of     which is the same as Lemma   of  Gao
  Ji    but without conjectured results 
Lemma   PC False Positive Identity  Let PCG
  be the
learned PC set of the target   in the learned graph   from
Step   of Algorithm   and PC 
  be the ground truth PC
set in    The false positives   in PCG
  consist of only
             Des 
descendants of   in    denoted as Des 
   
    PCG

    PC 
   

  consists of the entire PC 

Proof  By Lemma   PCG
  and
some false positives    We show     Des 
    According
to Lemma     node       is conditionally dependent of
        in    In the last iteration
  given any     PC 
of Step   PCG
  must contain the true positive parents of  
    PC 
    PCG
    as Pa 
Pa 
  by Lemma   Therefore 
      Pa 
    However  by the Markov condition  all the
nondescendant nodes   are independent of   given Pa 
 
in    Hence nondescendants   cannot be in PCG
  by the
last iteration  Thus      Des 
   

For the sake of complete discussion  we include the following property  showing the existence of unblocked paths 
Lemma   Coexistence Between Descendants and
Spouses in ScoreBased PC Search  In the learned   from
Step   of Algorithm   the only false positives   in PCG
  beT  due to an unblocked
long to the descendants of     Des 
path between   and its descendants via   Vstructure    
Child   Spouse in   

Proof  Lemma   shows the  rst part of the lemma is true 
and we just need to show the second part holds  Assuming
false positive PC nodes   exist  let         Des 
    then
Lemma   shows that             PC 
    For   to
exist        PC 
  must be true  Since PC 
  must be
present in all paths from   to   in    in the last iteration
of the scorebased PC search the dependence between  
and   occurs only if PC 
  unblocks some paths from   to
   This can only happen when there is   collider node in
PC 
  is through
an unblocked path that contains   Vstructure     child  
spouse in   

    Hence  the only way   can exist in PCG

Proof  Step   of Algorithm   returns all of the true positive
parents  children  and some descendants  if they exist  by
Lemma   and Lemma   The tasks left are to add the true
positive spouses and remove false positive PC nodes      
the nonchild descendants  from PCT  
First  we show LocalLearn will  nd all of the true positive spouses  In Step   LocalLearn learns   structure
with the target  the current PC set  the current spouse set 
and one query variable             PCT   At each
step  PCT is   set that has been found to be dependent
or conditionally dependent of the target  Since PCT includes and will always include the true positive PC set by
Lemma   true positive spouses are conditionally dependent of   given PCT   Following   similar logic as Lemma
    
  must directly connect to the true positive children of
    Because Step   of Algorithm   queries every variable
in   and   
  must be dependent of nonadjacent   given
PC 
    to capture the correct independence relationships
  must be included in
with the locally consistent score    
ST  
Secondly  we show false positive PCs and spouses will be
removed  Since all the true positive PC nodes and spouses
are present in the last iteration  the false positive PC nodes
 the nonMB descendants by Lemma   if exist  should
be adjacent to the true positive PC nodes and spouses in
   However  the DAG obtained from   by removing the
edge from false positives PC to   would score higher by
capturing the same independence relationships       the descendants are dependent of children and spouse nodes of
  and independent of   given the true positive MB set 
and the dependence relationships between the true positive
MB set and     but with fewer edges  Therefore  all false
positive PC nodes will be removed from PCT   Similarly 
since all the true positive PC nodes and spouses are present
in the last iteration  if there exist false positive spouses
    SG
      would be parents to some true positive
  in   and          If so    should be
children nodes   
  as well in   as every path between    and
adjacent to   
  must go through   
    However  the DAG obtained from
  by removing the edge from   to   would score higher
than   by capturing the same independence relationships
but with less edges  Therefore  there will not be any false
positive spouse nodes 
Thus  PCT and   will contain all and only the true PC and
spouses  Their union contains all and only the MB nodes 
Therefore  LocalLearn is sound and complete 

      

We show the consistency results of LocalLearn 
Theorem   Under the in nite data and faithfulness assumption  LocalLearn  nds all and only the Markov
Blanket nodes of the target node 

Note that the learned parent set   and children set   from
LocalLearn themselves may not be sound or complete 
due to Markov equivalence  even though their joint set is
sound and complete 

  Localto Global Learning

Algorithm   Graph Growing Structure Learning

Localto Global BN Structure Learning

Using the local structures  one alternative approach to
global structure learning is to learn the local structures of
all nodes and then combine them to construct the global
graph structure from the existing local graphs  Many algorithms  Margaritis   Thrun    Niinimaki   Parviainen     rst use the ANDrule to resolve the potential
con icts among different local structures between adjacent
variables  and then use the Meek rules  Meek    to obtain the  nal DAG  The ANDrule seems arbitrary in the
learning results and can introduce errors if one of the neighbors learns   wrong local graph  hence affecting the accuracy of the  nal global graph  One naive way to solve such
con icts is to rerun the subroutine BNStructLearn on
the variable set containing both neighbor sets to redetermine the existence of the edges  However  the procedure is
inef cient as it repeats learning for local structures of every edge after repeating for every node  relearning the same
parts of the graph multiple times 
We propose to anchor the learning at one target variable
    and then grow the graph by gradually expanding it 
It is made possible as the LocalLearn does not require neighbors  local structure to learn      neighborhood
correctly  unlike previous algorithms  We only use the
necessary variables at each iteration to expand the graph
while keeping other parts of the learned graph  xed  The
proposed graph growing structure learning  GGSL  algorithm is shown in Algorithm   Starting from an empty
graph  GGSL iteratively updates the global graph by using LocalLearn to learn the local structure of one target
variable   at   time  Each local learning uses the set consisting of the current local variables of the target variable
in the learned graph   and variables that are not in the local structures of any variable  Then GGSL updates   with
learned results using updateGraph  It runs until all but
one variable is not learned and has four main steps 
Step   GGSL chooses one target variable   at each iteration   rst chosen randomly and then based on query set
   which contains adjacent nodes of the already queried
variables in the graph      can be maintained as   regular
queue  rst in   rst out  Queried variable set   keeps all
the learned     and prevents repeated learning 
Step   GGSL uses LocalLearn shown in Algorithm  
to  nd the local structure of the target variable over the
query set    This step resolves the potential edge con icts
through ef cient learning  avoiding the simple ANDrule
used by other localto global structure learning algorithms
The main difference between each run of LocalLearn is
that the previous      will not be considered  unless they are
in the local structure of the current target variable  Hence
the max possible variable set size decreases over iterations 
improving the memory ef ciency 

Input  data    size    variable set  
           
    zeros       
repeat

if    cid          cid    then

      pop 
    the next unqueried variable in  

else

end if
    GGSL               
add adjacent nodes of   in   to  
         

until               
    PDAGto DAG   
Return   

Algorithm   GGSL Subroutine

Input  data    target variable     current DAG    variable set    variable set  
PC         findPC       
Sp   findSpouse       
        PC   Sp           PC  Sp    
DZ     of the set  
MB        Sp   LocalLearn DZ     
    updateGraph             Sp 
Return   

Step   Subroutine updateGraph  shown in Algorithm   performs the following check to enforce graph
consistency  First  it checks if any directed parent in the
existing   is learned as   child from the local learned children set    If so  it corrects the children into parents  Secondly  it checks if any directed child in the existing   is
wrongly learned as   parent from    If so  it corrects the
parents into children  Lastly  the algorithm checks if any  
and   nodes can have   different edge direction without introducing new or destroying existing Vstructures  if so  all
these   and   are marked as undirected  Otherwise  they
are marked as directed edges  There checks are needed to
ensure the alreadyoriented edges remain unchanged  as the
earlier runs of local structure learning uses more variables
and hence are more likely to be correct  Lastly  to orient
the newly learned spouse set Sp is straightforward  due to
the de nitive nature of Vstructures 
Step   After repeating the  rst three steps for all but one
variable  the last step of GGSL is to obtain   DAG given all
the directed and undirected edges in the graph  Applying
the conventional rules  Meek    to convert   partial directed acyclic graph  PDAG  to completely directed acyclic
graph is suf cient  with an option to convert to DAG if desired 

Algorithm   updateGraph

Localto Global BN Structure Learning

Input  current DAG    target     learned parent    child
set    spouse set Sp
 Step   update the PC set 
for all   in   do

if   con icts with directed edge       in   then

add   into   and remove   from  

end if
end for
for all   in   do

if   con icts with directed edge       in   then

add   into   and remove   from  

end if
end for
if         then
else if directed edge                    cannot be
reversed without destroying existing or introducing new
Vstructures then

orient   and   accordingly as   directed edge in  

orient      accordingly as   directed edge in  

else

Figure   An example BN for Lemma  
If the query order is
                   the edge       would be not
guaranteed to orient correctly until   is queried and hence the
post processing is needed 

orient      accordingly as   undirected edge in  

end if
 Step   update the spouse set 
Orient Sp and   to their children accordingly as directed
edges in  
Return   

Lemma   Requirement of Postprocessing   Subroutine
PDAGto DAG is required in   localto global learning
system to correctly orient DAG   to its Markov equivalent
class in Algorithm  

  simple example  shown in Figure   would justify
Lemma   If the target   is chosen with the following order              and    then the direction of edges       
       and       can be set in both directions and hence
are labeled as undirected edges by Algorithm   When
node   becomes the target  then edge       is known 
Other edges      and       have to be checked and corrected  Without subroutine PDAGto DAG to propagate
the changes back  the learned DAG is not guaranteed to be
the correct completely partially directed DAG  CPDAG 
We show Algorithm   is sound and complete 
Theorem   Soundness and Completeness  Under the in 
 nite data and faithfulness assumption  Algorithm   GGSL
learns and directs all and only the correct edges in the underlying DAG    up to the Markov equivalent class of   

Proof  For the  rst target variable    GGSL  nds the correct local structure of one target variable by Theorem  
The updated DAG   is sound and complete for    Starting from the second iteration       since the learned graph

  is shown to be correct  if any variable in the target variable Ti   MB set has been queried or saved  they must exist
in the PC and Sp variable  hence in   of Algorithm  
With the rest of variables complementing the PC and Sp
variables  all the true positive PC set and spouse set of Ti
must be included in the set   of Algorithm   By Theorem   again  the local structure learned must be sound and
complete  Hence  at each iteration of Algorithm   the updated DAG   must be sound and complete for all Tis as
well  Since the PDAGto DAG subroutine is also proven
to orient the correct DAG from PDAG  Meek    the
result DAG   at the end of Algorithm   must be in the
Markov equivalent class of   

  Case Study

Applying Algorithm   to Figure    would result in the following procedure  assuming the query order for target  
is                         When        
LocalLearn  nds   and   in the local structure of  
with the query set of all variables  with undirected edges
between them   and update    With         the query set
of variables contains its local structure   from   and the
rest of variables  LocalLearn  nds   and   as     local structure  and updates    With         LocalLearn
 nds the same   and   and does not make new update of
   With         LocalLearn  nds   and update  
with directed edge       in    Similarly  LocalLearn
 nds the directed edge       and       due to the de nitive Vstructure and update   accordingly  shown in Figure     when Ti     and     PDAGto DAG takes   and
produces   possible DAG in Figure    

CDEFABLocalto Global BN Structure Learning

  Implementation Optimization

While Algorithm   is theoretically sound and complete 
in practice further optimizations in the algorithmic procedure can be implemented  First 
inside the iterative
LocalLearn procedure  variables in the existing PC and
Sp set of Ti should be queried  rst  These true positive
MB set variables could potentially reduce the queried set
size to BNStructLearn at each iteration by removing
nonMB set of variables early in the process  Using this
procedure is similar to the idea behind an improved version of LocalLearn  Gao   Ji    which is shown
to improve accuracy and reduce computational time  Secondly  using the same concept  one can also keep track of
which variables are removed from Ti   local structure after
adding each queried variable   for target Ti   local learning procedure  hence forming   separation set  or sepset  of
  from     Querying variables from the sepset  rst when
  becomes the target could also reduce the potential query
set variable size to LocalLearn  as sepset variables are
more likely to be adjacent variables of   

  Complexity and Performance Discussion

The exact global learning complexity varies depending
on the algorithm  but is exponential in the worst case 
For example  Dynamic programming approaches  Silander   Myllymaki    cost          where   is the
total number of variables present  The most expensive
step of localto global approach is each iteration of local
learning using LocalLearn  which costs          using the same dynamic programming approach  Repeating
for all the variable present  the localto global approach
takes          in the worst case  Niinimaki   Parviainen    Gao   Ji    As one can see  in the case
where the local structure of one target variable includes
all other variable  such as in the Naive Bayesian model 
the localto global approach would match the complexity
of the global approach  However  by the iterative nature
of the learning and the fact that the number of query variables decreases at later stages of learning  the expected running time is much lower for the localto global learning
approach  If we assume   uniform distribution on the local neighbor size of each node in   network of   nodes 
then the expected time complexity of the proposed GGSL
                   If we assume  
to be the maximum size of local neighbors  then the average complexity would be        which can lead to   big
performance gain       
Our algorithm is   scorebased algorithm  as it can use
any one of existing scorebased optimal learner as the
BNStructLearn subroutine  Theoretical optimality of
the proposed algorithm holds only under standard assumptions 
In real datasets  when the faithfulness assumption

approach is   cid  

is violated or estimated probabilistic distributions are estimated incorrectly due to insuf cient data  the performances
of all the algorithms  global and local  are not guaranteed  During the learning procedure of the GGSL algorithm  even with   smaller query set of variables  the information about edge existence and orientation with suf 
cient data does not decrease compared to the global learning methods  if the local variables around each edge are
all present  Hence  reducing the query set size would not
affect performance with suf cient data  although the information loss does happen in practice  On the other hand 
the estimation performance on the number of data samples
 
is known to be sensitive to the number of parameters  The
   where   is the stanstandard error of estimation is    
dard deviation  Due to the smaller set of variable present 
the computation of Bayesian scores could be more accurate
in practice when the sample size is limited  The tradeoff
between the information loss and estimation error varies
among different datasets 

  Experiments
We compare the proposed algorithms with both global and
localto global learning methods  Speci cally  we compare
our results with global methods  Dynamic Programming
 DP  structure learning  Silander   Myllymaki   
Constrained Structure Learning  CSL   de Campos et al 
  GOBNILP  Cussens et al    and local methods
Scorebased Local Learning  SLL     Niinimaki   Parviainen    with three different BNStructLearn as
above  DP  CSL  and GOBNILP  denoted as SLL CDP 
SLL CCSL  and SLL CGOBNILP  We use the existing
implementation of DP  in MATLAB  CSL  in    and
GOBNILP  in    and implement our algorithms in MATLAB  We test the algorithms on benchmark BN datasets
from the BN repository  using the datasets provided from
existing works Tsamardinos et al    We run the algorithms with   samples of each dataset   times  and
compare BDeu scores of each algorithm  along with the
standard deviation  shown in Table   We also compare
the algorithms on   synthetic  node network for DP as
BNStructLearn so algorithms can return results within
the time limit  We report the running time  the entire algorithmic time  including data access  score computation and
structure search  of each algorithm  along with the standard deviation  shown in Table   with the maximum running time of   hours  The experiments are conducted on  
machine with Intel      GHz with   GB memory 
Due to memory limitation  the DP method can fail to  nish  CSL and GOBNILP have parameters that can control

 http www bnlearn com bnrepository 
  The time results are different from results on the GOBNILP
website  which represent the times of  nding the optimal structure
given already computed scores 

Localto Global BN Structure Learning

Table   Learning Scores for Different BN Structure Learning Algorithms on Different Datasets 

DATA SET

VARIABLE SIZE

 BN
ALARM
CHILDREN
HAILFINDER
CHILDREN 

 
 
 
 
 

DATA SET

VARIABLE SIZE

ALARM
CHILDREN
HAILFINDER
CHILDREN 

 
 
 
 

DATA SET

VARIABLE SIZE

DP
   
OOM
OOM
OOM
OOM

SLL CDP
   
   
 
 
 
SLL CCSL
 
 
 
   
SLL CGOB
   
DNF
 
     
   
DNF
 
DNF
DNF  DID NOT FINISH

CSL
     
     
   
   
GOBNILP

GGSLDP
     
   
   
     
   
GGSLCSL
   
   
   
   
GGSLGOB
 
   
   
   

ALARM
CHILDREN
HAILFINDER
CHILDREN 

memory usages 

 
 
 
 

OOM  OUT OF MEMORY 

Table   Time Taken 
Learning Algorithms on Different Datasets 

in seconds  for Different BN Structure

Data set
DP
 
 BN
OOM
Alarm
Children
OOM
Children  OOM
Hail nder OOM

SLL CDP
 
 
 
 
 

Data set

Alarm
Children
Children 
Hail nder

CSL
 
   
 
 

SLL CCSL
     
 
 
 

GGSLDP
   
   
   
 
   

GGSLCSL
   
   
 
 

Data set

GOBNILP
   hr
Alarm
 
Children
Children     hr
Hail nder    hr

SLL CGOB GGSLGOB
   
 
   
   
OOM  Out of Memory 

 
   
   
   

As one can see from Table   GGSL improves the learning scores by   signi cant margin over the SLL   algorithm  with different BNStructLearn in all four datasets
tested  except one case in ALARM with GOBNILP  It can
even compete with global structure learning approaches
in some cases  GGSL outperforms the global learning
methods in CHILDREN datasets with CSL and GOBNILP 
Ef ciencywise  from Table   using DP  GGSL is more ef 

 cient than SLL   and can achieve one than one order of
speedup  Using CSL and GOBNILP  GGSL has more than
one order of magnitude speedups in   out of   datasets
when compared with global learning method  However 
GGSL   running time is generally slower to SLL   algorithm using CSL and GOBNILP  It is faster than SLL  
on HAILFINDER with CSL  and is slightly slower in
the other testing cases  We speculate that the difference
in speed gains across different algorithms is mainly the
code base of BNStructLearn  where the extra checking Step   in GGSL can take proportionally longer time if
BNStructLearn is implemented in   

  Discussion and Conclusion
We have proposed   novel graph expanding learning algorithm to learn BN structure  We strengthen the existing
local structure learning analysis  justifying its soundness
when the traditional faithfulness condition fails with absent variables  and propose   new localto global approach
to combine the local structures ef ciently  Experiments
have shown that the proposed GGSL improves the accuracy over existing localto global algorithms and improves
ef ciency over existing global algorithms  both by   significant margin  In addition  GGSL can work with any exact
scorebased BN learning algorithm and achieve consistent
performance gain  The iterative nature of GGSL can have
many applications  such as online BN structure learning
with streaming data  Future work could study how GGSL
would work with constraintbased  van Beek   Hoffmann 
  Gao   Ji      and approximated BN structure
learning algorithms as the BNStructLearn routines 

Localto Global BN Structure Learning

Acknowledgements
We thank Dennis Wei and anonymous reviewers for inspiration and helpful comments 

References
Acid  Silvia  de Campos  Luis    and Castellano  Javier   
Learning bayesian network classi ers  Searching in  
space of partially directed acyclic graphs  Machine
Learning     

Aliferis  Constantin    Statnikov  Alexander  Tsamardinos 
Ioannis  Mani  Subramani  and Koutsoukos  Xenofon   
Local causal and markov blanket induction for causal
discovery and feature selection for classi cation part   
Algorithms and empirical evaluation  Journal of Machine Learning Research  pp    Jan  

Brenner  Eliot and Sontag  David  Sparsityboost    new
scoring function for learning bayesian network structure 
arXiv preprint arXiv   

Chen  Eunice YuhJie  Shen  Yujia  Choi  Arthur  and Darwiche  Adnan  Learning bayesian networks with ancestral constraints  In Advances in Neural Information Processing Systems  pp     

Chickering  David Maxwell  Optimal structure identi cation with greedy search  Journal of Machine Learning
Research   

Chickering  David Maxwell  Meek  Christopher  and Heckerman  David  Largesample learning of bayesian networks is nphard  CoRR  abs   

Cussens  James  Bayesian network learning with cutting
In Proceedings of the TwentySeventh Conferplanes 
ence Annual Conference on Uncertainty in Arti cial Intelligence  UAI  pp    Corvallis  Oregon 
  AUAI Press 

Cussens  James  Haws  David  and Studen    Milan  Polyhedral aspects of score equivalence in bayesian network
structure learning  Mathematical Programming  pp   
   

de Campos  Cassio    Zeng  Zhi  and Ji  Qiang  Structure learning of Bayesian networks using constraints  In
ICML   Proceedings of the  th Annual International
Conference on Machine Learning  pp    New
York  NY  USA    ACM 

Fu  Shunkai and Desmarais  Michel    Fast markov blanket discovery algorithm via local learning within single
pass  In Proceedings of the Canadian Society for computational studies of intelligence   st conference on Advances in arti cial intelligence  Canadian AI  Berlin 
Heidelberg    SpringerVerlag 

Gao  Tian and Ji  Qiang  Local causal discovery of direct
causes and effects  In Advances in Neural Information
Processing Systems  pp     

Gao  Tian and Ji  Qiang  Constrained local latent variIn Proceedings of the TwentyFifth Inable discovery 
ternational Joint Conference on Arti cial Intelligence 
IJCAI  pp    AAAI Press     

Gao  Tian and Ji  Qiang  Ef cient markov blanket discovery and its application  IEEE Transactions on Cybernetics  pp       

Gao  Tian and Ji  Qiang  Ef cient scorebased markov
blanket discovery  International Journal of Approximate
Reasoning     

Gao  Tian  Wang  Ziheng  and Ji  Qiang  Structured feature selection  In Proceedings of the IEEE International
Conference on Computer Vision  pp     

Heckerman  David  Geiger  Dan  and Chickering  David   
Learning bayesian networks  The combination of knowledge and statistical data  Machine learning   
   

Jaakkola  Tommi  Sontag  David  Globerson  Amir  and
Meila  Marina  Learning bayesian network structure using lp relaxations   

Koivisto  Mikko and Sood  Kismat  Exact bayesian structure discovery in bayesian networks  The Journal of Machine Learning Research     

Koller  Daphne and Sahami  Mehran  Toward optimal feaIn ICML   pp    Morgan

ture selection 
Kaufmann   

Lazic  Nevena  Bishop  Christopher  and Winn  John 
Structural expectation propagation  sep  Bayesian
structure learning for networks with latent variables  In
Arti cial Intelligence and Statistics  pp     

Margaritis  Dimitris and Thrun  Sebastian  Bayesian netIn Advances
work induction via local neighborhoods 
in Neural Information Processing Systems   pp   
  MIT Press   

Meek  Christopher  Strong completeness and faithfulness
In Proceedings of the Eleventh
in bayesian networks 
conference on Uncertainty in arti cial intelligence  pp 
  Morgan Kaufmann Publishers Inc   

Nie  Siqi  Mau    Denis    De Campos  Cassio    and
Ji  Qiang  Advances in learning bayesian networks of
bounded treewidth  In Advances in Neural Information
Processing Systems  pp     

Localto Global BN Structure Learning

Niinimaki  Teppo and Parviainen  Pekka  Local structure
disocvery in bayesian network  In Proceedings of Uncertainy in Arti cal Intelligence  Workshop on Causal
Structure Learning  pp     

Ott  Sascha  Imoto  Seiya  and Miyano  Satoru  Finding
optimal models for small gene networks  In Paci   symposium on biocomputing  volume   pp     

Pearl  Judea  Probabilistic reasoning in intelligent systems 
networks of plausible inference  Morgan Kaufmann Publishers  Inc    edition   

Pellet  JeanPhilippe and Ellisseeff  Andre  Using markov
blankets for causal structure learning  Journal of Machine Learning Research   

Scanagatta  Mauro  de Campos  Cassio    Corani  Giorgio 
and Zaffalon  Marco  Learning bayesian networks with
thousands of variables  In Advances in Neural Information Processing Systems  pp     

Silander  Tomi and Myllymaki  Petri    simple approach
for  nding the globally optimal bayesian network structure  In Proceedings of the TwentySecond Annual Conference on Uncertainty in Arti cial Intelligence  UAI 
pp     

Spirtes  Peter  Glymour  Clark    and Scheines  Richard 
Computation  Causation  and Discovery  AAAI Press 
 

Tsamardinos 

Ioannis  Aliferis  Constantin  Statnikov 
Alexander  and Statnikov  Er  Algorithms for large
In In The  th Interscale markov blanket discovery 
national FLAIRS Conference  St  pp    AAAI
Press   

Tsamardinos  Ioannis  Brown  LauraE  and Aliferis  ConstantinF  The maxmin hillclimbing bayesian network
structure learning algorithm  Machine Learning   
   

van Beek  Peter and Hoffmann  HellaFranziska  Machine learning of bayesian networks using constraint
programming  In International Conference on Principles
and Practice of Constraint Programming  pp   
Springer   

Yuan  Changhe and Malone  Brandon  Learning optimal
bayesian networks    shortest path perspective   
   

