Framebased Data Factorizations

Sebastian Mair   Ahc ene Boubekki     Ulf Brefeld  

Abstract

Archetypal Analysis is the method of choice
to compute interpretable matrix factorizations 
is represented as   convex
Every data point
factors 
combination of
     points on the
boundary of the convex hull of the data  This
renders computation inef cient  In this paper  we
show that the set of vertices of   convex hull  the
socalled frame  can be ef ciently computed by
  quadratic program  We provide theoretical and
empirical results for our proposed approach and
make use of the frame to accelerate Archetypal
Analysis 
The novel method yields similar
reconstruction errors as baseline competitors but
is much faster to compute 

  Introduction
Matrix factorizations are used in many different learning
tasks  including clustering  classi cation  recommendation 
text and social network analysis 
image denoising and
hyperspectral
imaging  consequently    great deal of
approaches to factor   matrix into two or more matrices
have been proposed 
  common limitation of matrix factorization techniques 
is   lack of interpretability of the resulting
however 
factors 
This observation also holds for nonnegative
matrix factorization  Paatero   Tapper    Lee  
Seung     NMF  Although NMF constrains data 
matrices  and factors to be nonnegative  the remaining
factors may lie far from other data points and do not
necessarily contribute to interpretability  Recently 
the
focus on interpretable factors led to stochastic NMFs
for clusterings  Arora et al      and convex
constraints  Ding et al   
However  computing interpretable factors is actually an
 old  problem that has been introduced by Cutler and

 Leuphana University    uneburg  Germany  German Institute
for Educational Research  Frankfurt am Main  Germany 
Correspondence to  Sebastian Mair  mair leuphana de 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Breiman   Their idea is to represent every data
point as   convex combination of factors  the socalled
archetypes  Since the archetypes are themselves restricted
to be convex combinations of data points  they need to lie
on the boundary of the convex hull of the data  hence  they
are either close to actual data points or mixtures thereof 
see Figure   An archetypebased factorization yields two
stochastic matrices and naturally interpretable factors for
arbitrary data matrices  Variants of Archtypal Analysis
 AA  are for instance introduced in   rup and Hansen
  Chen et al    and Bauckhage et al   
The computation of convex hulls is generally demanding 
For AA    straight forward idea is to make use of      
precomputed  vertices of the convex hull of the data  We
will refer to the set of vertices as the frame  Given
the frame  computing the archetypes reduces to   search
within the frame  We propose to go one step further and
approximate AA by restricting the whole optimization to
the frame   rstly  we compute the frame and secondly 
we compute the archetypes on the frame  and just on the
frame  Naturally  the result is   factorization of the frame
itself  The factorization can then be extended to the all
data points by recomputing the weights in hindsight  The
idea is similar to Thurau et al 
  who propose to
approximate the frame using twodimensional projections 
unfortunately  their approach adds an unnecessary layer
Ideally  given an
of approximation to the problem 
appropriate approximation of the frame 
the bulk of
the remaining data points should be reconstructed either
lossless or at only   small cost 
Standard approaches to compute the convex hull
like
Quickhull  Barber et al    become infeasible for
dimensionalities larger than three because of dispensable
triangulations  Discarding the triangulations leads to linear
programming  LP based solutions  Dul     Helgason 
  Ottmann et al    Dul       opez    which
test whether   point athand is part of the frame  These
approaches require adequate preprocessing as duplicates
may cause false negatives 
In this paper  we     show that the exact frame can be
computed by   quadratic program  QP   ii  reduce the
optimization to an existing algorithm and  iii  provide
theoretical and empirical justi cations for the developed
method 

Framebased Data Factorizations

Algorithm   Archetypal Analysis  AA 

Input  data matrix    number of archetypes  
Output  factor matrices        BX  ABX     
    initial guess of archetypes on  
while not converged do
for                   do

 cid   cid ai   xi cid 

 

ai   argmin

 cid ai cid aij 
end for
       cid     cid  
for                   do

bk   argmin

 cid bk cid bkj 

end for
    BX
end while

 cid   cid bk   zk cid 

 

where  cid cid   denotes the Frobenius norm  The optimization
problem is nonconvex in   and   but convex in   for  
 xed   and vice versa  Usually  it is solved by iteratively
solving for   and   as outlined in Algorithm   Cutler and
Breiman   prove that the archetypes            zp lie on
the boundary   conv     of the convex hull of the data  
for            From   geometrical point of view  AA
yields an approximation of the convex hull of size    Every
point inside this polygon can be reconstructed in   lossless
way and every point outside will be projected onto it  The
optimization of the RSS will minimize the reconstruction
error 

  

  Framebased Factorizations
  Preliminaries
We consider   discrete data set                 xn  
consisting of   data points in   dimensions       xi   Rd
for                    The convex hull conv     is the
intersection of all convex sets containing     Furthermore 
conv     is the set of all convex combinations of points in
    The intersection of   with the boundary of         
      conv     is called the frame    Hence  the frame
consists of the extreme points of     Those points cannot be
represented as convex combinations of other points rather
than themselves  Note that the frame   and the data set  
yield the same convex hull       conv      conv     By
        we refer to the size of the frame and we call the
portion of points in   belonging to the frame    the frame
density  In the remainder we assume       
There are some straightforward consequences that will
become handy in the remainder  When the inner product
between two points is maximized  one of the points is an
extreme point and thus belongs to the frame of the data 

Figure   Comparison of factorizations computed with NMF
 green squares  and AA  red stars  While the former may place
the factors far from data  Archetypal Analysis uses data points on
the boundary of the convex hull and yields an intuitive solution 

The remainder is structured as follows  Section   reviews
Archetypal Analysis and Section   contains the main
contribution    new computational method for  nding the
frame and   framebased matrix factorization  Section  
reports on empirical results and Section   introduces an
application as an autoencoder  Section   concludes 

  Archetypal Analysis
Let                 xn  
   be   data set consisting of   data
points in   dimensions and     Rn   be the data matrix 
The goal of Archetypal Analysis  AA   Cutler   Breiman 
  is to  nd   factorization of the data

    ABX   AZ 

 
where     Rn        BX   Rp   are the factors
and       is the latent dimensionality  The matrix  
contains the weights ai   Rp                   of the
convex combinations of the   archetypes            zp which
are stacked in the matrix        

    

xi  

 ai     zk 

 ai      

 ai      

  

  

The archetypes zk                   are themselves convex
combinations of data points  The matrix   holds the
weights bk   Rn                   of those convex
combinations and selects the archetypes from the given
data set    The representation is as follows 

    

zk  

 bk     xi 

 bk      

 bk      

  

  

The matrices   and   are rowstochastic due to their
constraints  The factorization is obtained by minimizing
the residual sum of squares  RSS 
min RSS       cid     AZ cid 

     cid     ABX cid 
   

  cid 

  cid 

  cid 

  cid 

Framebased Data Factorizations

Algorithm   FrameAA

Input  data     number of archetypes      
Output  factor matrices        BH  ABH     
    Frame    
    frame matrix
       ArchetypalAnalysis      
        Rn  
for                   do

ai   argmin

 cid ai cid aij 

end for

 cid   cid ai   xi cid 

 

Lemma   Let   be    nite set of discrete points  then

        

 cid      cid cid      

argmax

  cid  

Proof  Linearity and convexity of the inner product imply
that its maximum is realized by an extreme point of the
domain  Since the domain is     the maximum belongs to
its frame   

In addition  every point of the domain lies in the span of
some points on the frame 
Proposition   Every point   of   can be written as  
convex combination of at most       points from the frame
  of    

Proof  See Brondsted  

  Motivation

The idea of the proposal is as follows  Due to the fact that
the archetypes lie on the boundary of the convex hull  it is
possible to restrict the search of the archetypes to the frame 
However  we intend to go one step further  Since the frame
  and the data   yield the same convex hull  we restrict
the factorization in Equation   to the matrix     Rq  
of stacked points of the frame      

    ABH   AZ 

The idea is depicted in Figure   Although Archetypal
Analysis is only computed on the frame it often yields
almost identical archetypes as AA computed on the whole
data set as illustrated in the right subplot of Figure   The
reduction of points yields an accelerated computation 
Assuming   low frame density          cid       suf cient
approximation of the frame  and therefore the convex hull 
will also cover most of the points in the interior of its
induced polygon  On the other hand  if the frame density is
high  the problem will reduce to standard AA in the limit
      and the speed up will vanish  Based on the nature of

Figure   Approximation of Archetypal Analysis 

the problem  we claim that AA makes no sense in scenarios
of high frame densities  This is because almost all points
will be projected unless   is chosen very high which is
usually not the case 
After computing the factorization by AA  we have to
recompute the weight matrix     Rq   for all data points
using the optimized archetypes within    This procedure
is called FrameAA and presented in Algorithm   Note
that the idea does not only apply for standard Archetypal
Analysis as presented in Cutler and Breiman   but
also all variants thereof    rup   Hansen    Chen
et al    Bauckhage et al   
Until now  we assumed that the frame   or equivalently
the frame matrix     Rq    as required in the  rst line
of the algorithm  is already given  In the following section 
we present   novel algorithm for ef ciently computing the
frame of   discrete data set 

  Representing   Single Point
Let   be the frame of   and     Rd   be the design
matrix of     The idea is as follows  For every        
we aim to solve the linear system Xs     subject to the
constraints that the solution   is nonnegative  sums up to
one  and uses only points from the frame      

     

Xs    
si        cid         si  cid      xi     
Theorem   reduces this to   leastsquares approach 
Theorem   Let   be the frame of         Rd   be

the design matrix of           and       cid   cid   cid cid   

 

      as well as         cid   cid    Rd  be the
augmented versions of   and    Then  the following
problems have the same solutions 
    Xs           si        cid         si  cid      xi     
 ii   Xs            si       si  cid      xi     
 iii  argmins 

        si  cid      xi     

 cid   Xs      cid 

 

Proof      is equivalent to  ii  as it integrates the constraint
 cid       into the system of linear equations  Proposition  

Framebased Data Factorizations

Algorithm   Lawson and Hanson   active set algorithm

Input  design matrix    and right hand side   
Output  solution   with si       
     
                  
     
                cid      Xs 
while    cid    and            do
    argmaxj  wj          
           
           
 XP                 
    argminz  cid   XP        cid 
        
              zj      
while          do
sj zj

         

 

sj

    min 
                   
for                   do

if       and  si      then

           
           
 XP         

end if
end for
    argminz  cid   XP        cid 
        
end while
     
                cid      Xs 

 

end while

assures that     has   solution  Hence  the minimum of
 cid   Xs      cid 
  is always zero and   solution of  iii  is also
 
  solution to  ii  and    

If the condition that the positive positions of   refer to
points on the frame is dropped  then problem

    argmins         

 cid   Xs      cid 

 

 
 

 

is equivalent to the nonnegative least squares  NNLS 
problem which is   special case of   QP  The activeset
method from Lawson and Hanson   is proven to yield
  leastsquares estimate of Equation   As the minimum
is zero  it also gives   solution to the linear problem up
to the condition that the points contributing to the solution
belong to the frame  The method of Lawson and Hanson
  called NNLS is outlined in Algorithm  
However  until now it remains unclear whether the positive
elements of the solution refer to points on the frame  The
following theorem shows that this is actually the case 

Theorem   Algorithm   from Lawson and Hanson  
solves the problem in Equation  

Proof  Let Algorithm   solve the problem in Equation  
Denote by                 cid      Xs  the negative
gradient with respect to    The points    xi          
involved in the solution are selected via

    argmaxj  wj          

  argmaxj cid     cid      Xs cid 
  argmaxj   cid xj     cid      cid 

           
si    cid xi   xj cid           

  

Lemma   assures that those points belong to the frame 
Since there is no other way of selecting points  all of them
belong to the frame  Hence  the condition si  cid      xi  
  is satis ed  The claim follows with Theorem  

The following proposition from Lawson and Hanson
  characterizes the solution of this method 
Proposition    KuhnTucker conditions    vector     Rn
  if and only if there
is   solution for          
exists   vector     Rn and   partitioning of the integers
from   to       into subsets   and   such that with    
            cid      Xs  it holds that

 cid   Xs      cid 

si     for       
wi     for       

si     for       
wi     for       

Proposition   states that no more than       extreme points
are needed to de ne   point 
In addition  Algorithm  
chooses iteratively one extreme point after another  As it is
guaranteed to lower the error in each iteration  Lawson  
Hanson    it terminates with at most    nonnegative
positions and yields   sparse convex combination 

  Computing the Frame

Until now  we represented   single point as   sparse convex
combination of points on the frame  By doing this for every
point xi                     in the data set  we obtain the
frame   of     Algorithm   summarizes this procedure 
called NNLSFrame  and Corollary   proves this claim 
Corollary   Algorithm   yields the frame   of    

Proof  Theorem   ensures that the positive positions of
every solution sk                     refers to points on the
frame  Taking the union of those positions recovers the
frame indices   since every extreme point de nes itself and
therefore its index is part of the union 

that Algorithm   already realizes

Note
  matrix
factorization for the special case        This is also
stated in the following corollary 

Framebased Data Factorizations

Algorithm   NNLSFrame
Input  design matrix   
Output  indices of ext  points   and weight matrix  
     
        Rn  
for                   do
sk   NNLS       xk 
Pk                           sk        
        Pk
         sk

end for

Corollary   Let   and   and be the result of Algorithm  
Then it holds that

        XW              is   factorization of the
design matrix   with          Rp   being   nonnegative stochastic matrix and        Rd   being
the submatrix of   containing only the frame of    

 ii  the factorization above is lossless      

 cid               cid 

     

  Complexity Analysis

First 

   XP      cid 

There are two main computations within the NNLS method
in Algorithm  
the negative gradient is being
computed  which has   complexity of          
Second  an unconstrained leastsquares problem    
argminz  cid   XP        cid 
  is being solved  The latter can
be rewritten as             cid 
     and has  
complexity of               where the average size
of   is  
         Since no more than        points are
suf cient for the solution  compare Theorem   the while
loop is being executed at most        times  Therefore 
the complexity of the NNLS method is   NNLS   
         
                   on average  The
complexity of NNLSFrame presented in Algorithm   is
      NNLS        
                   Actually 
the multiplier is less than   since points which are already
known to be extreme can be omitted  In contrast  the fastest
LPbased approach  Dul       opez    so far scales in
      LPd     dn    where LPr   is the runtime of  
LP with   equality constraints and   nonnegative variables 

  Computing the Frame Ef ciently

One way to speed up the frame computation is   divide and
conquer approach  The underlying principle is stated in the
following lemma 
Lemma   Let   and   be nonempty discrete sets  then

conv          conv cid  conv      conv   cid 

Algorithm   Divide and Conquer strategy of NNLSFrame

   cid  

Input  data     number of splits      
Output  indices of extreme points  
     
for                   do

         with                   for    cid   

Ek   NNLSFrame      
        Ek

end for
    NNLSFrame XE  

Figure   Percentage of discovered extreme points versus
percentage of iterations on synthetic data with       points
in       dimensions 

The idea is as follows  Let                     be   random
partition of     The size nk of every subset       should
be signi cantly smaller than the size of          nk  cid    
The assumption of having   pairwise disjunction is not
necessary but reasonable 
Instead of the whole data set
    Algorithm   is now executed on every subset       for
                   Finally  the frame of   is obtained by
merging the frames of every subset and run the proposed
approach again on it  The procedure is summarized in
Algorithm   The forloops of Algorithms     and   can
be trivially parallelized 

  Experiments
  Computing the Frame

For the experiments in this section we want to control the
frame density  Hence  use the same synthetic data  as in
Dul   and   opez   which was generated according
to   procedure described in   opez   The data
consists of             data points in
            dimensions with   frame density of
          percent respectively 

 http www people vcu edu jdula 

FramesAlgorithms 

Framebased Data Factorizations

Figure   Timing results on synthetic data with       points 

The third experiment is about the divide and conquer
approach introduced in Section   As shown in Figure  
the runtime is halved for every con guration using only
three partitions  The takehome message is  that even  
small number of partitions can lead to   substantial speed
up  Note that those partitions can be processed in parallel 

Table   Data sets and their frame size and density 

data set
 Dul       opez   
Banking 
 Dul       opez   
Banking 
 Epifanio et al   
USAFSurvey
 Horton   Nakai   
yeast
 Dul       opez   
Banking 
 Vinu     
SpanishSurvey
swissheads  Cutler   Breiman   
skel 
 Heinz et al   
 Cutler   Breiman   
ozone

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

frame density
 
 
 
 
 
 
 
 
 

  Matrix Factorization

We now compare the framebased Archetypal Analysis 
FrameAA  FAA  to several baselines including standard
Archetypal Analysis  Cutler   Breiman     AA 
ConvexHullNMF  Thurau et al   
 CHNMF 
ConvexNMF  Ding et al     CNMF  and standard
NMF  Lee   Seung    The  rst two methods are
implemented in Python  For CHNMF and CNMF we
use pymf   and for NMF we use scikitlearn   Table  
depicts the data sets used for this experiment  The frame
sizes and densities are computed with our proposed NNLSFrame method 
Table   reports on the results for the choice of       in
terms of reconstruction error measured with the Frobenius
norm  We obtain similar results for          
The number of iterations executed per algorithm is  xed
to   in order to obtain fair results  We use random
initializations for all algorithms and report on averages
over   repetitions  As seen in Table   FAA yields
similar errors as AA  The lowest errors are achieved with
NMF  The baseline CHNMF  which approximates the

 http pypi python org pypi PyMF
 http scikitlearn org

Figure   Timing results for the divide and conquer approach on
synthetic data with       points in       dimensions 

In contrast

The  rst experiment is about the discovery of extreme
to LPbased approaches  Dul    
points 
Helgason    Ottmann et al    which discover up
to one extreme point per iteration  NNLSFrame  nds up
to       extreme points  This is   result of Theorem   and
illustrated in Figure   The graph shows the percentage
of discovered extreme points against the percentage of
iterations conducted on   synthetic data set with    
  points in   dimensions with various frame densities 
The lower the frame density the faster the frame is being
discovered  Even for   fairly high frame density of  
the discovery is faster than approximately linear as one
could expect from an LPbased approach  This is depicted
as   dashed line  Note that if an approximation of the frame
is suf cient  the computation could be stopped 
In our second experiment we show that  nding the frame is
faster than using LPbased approaches  The two baselines
  and Dul   and
are taken from Ottmann et al 
Helgason   We make use of the implementation by
Dul   and   opez   and implement our approach in
the same programming language for compatibility  We test
on data sets of sizes             and of
dimensionality             Figure   shows timing
results for       The  gure shows that our approach
is always faster than the baselines  irrespectively of the
con guration  We obtain the same result on the remaining
data set sizes           which are not shown 

Framebased Data Factorizations

Table   Average Frobenius norm reported on   repetitions with
random initializations for         indicates that the method
could not be executed due to negative data 

data set
Banking 
Banking 
USAFSurvey
yeast
Banking 
SpanishSurvey
swissheads
skel 
ozone

FAA
 
 
 
 
 
 
 
 
 

AA CHNMF CNMF
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

NMF
 
 
 
 
 
 
 
 
 

frame instead of computing it exactly  yields higher error
of approximately   while CNMF is even worse 
In
summary  the error of FAA is similar to standard AA and
much better than CHNMF as well as CNMF 
Usually  it is apriori not known how to choose the latent
dimensionality      standard approach is to choose  
with respect to the socalled elbow criterion which requires
several runs for different values of   to be executed 
In
such   scenario our approximative approach is particularly
bene cial  FrameAA requires the computation of the
frame   before archetypes can be located  However  once
the frame is complete 
it can be used for  nding any
number of archetypes  Hence  it is interesting to see the
cumulative time taken by the methods when evaluating
several con gurations  say                    
This is depicted in Figure   for the USAFSurvey data set 
We report again on averages as well as standard errors on
  repetitions executed on random initializations  FrameAA turns out fastest at the  rst evaluation of       despite
the computation of the frame  Since the frame is static for
  data set  it can be reused for all remaining computations
and FrameAA clearly outperforms its peers  Note that
the divide and conquer strategy proposed in Algorithm  
leads to an additional speedup that is not shown here 
The reconstruction error is shown in the bottom part of
Figure   Once again  FrameAA yields   very similar
error as standard AA  which is computed on the whole data
set  while CNMF and CHNMF baselines perform much
worse 

  Autoencoders
We consider   prototypical application of FrameAA as an
autoencoder similar to the approach of Bauckhage et al 
  However  instead of focusing on the reconstruction 
we are interested in the quality of the learned embedding
given by the weights of the convex combinations 
For this exemplary task we make use of the mnist data

Figure   Cumulative time for several evaluations of   as well as
reconstruction error for USAFSurvey data 

set  It consists of handwritten images of the digits zero to
nine which are of size       rendering the problem  
dimensional  We split every image into       subimages
yielding   patches per image  Those patches are stackedup to form   new design matrix  For lack of space we focus
on   subtask and aim to separate digit   from digit   The
task is thus phrased as   standard binary classi cation task 
We sample   images per class  The  nal design matrix is
of size       and its frame density is approximately
  The design matrix is then factorized as described
in Section   After the factorization  every patch is
described by   weights contained within the matrix    The
weights of the convex combinations are then vectorized
yielding an embedding of size        per image 
As baseline we employ   deep learning autoencoder that
uses   convolutional layer  LeCun et al    to learn
intermediate representations of images  To perform   fair
comparison  the convolutional layer learns    lters of the
same size as FrameAA  that is       and stride      
This layer has   sigmoid activation function and is followed
by an upsampling layer that recovers the original image
size  The last layer is another convolutional layer with one
 lter of size       stride one  and sigmoid activation  We
optimize the network using squared loss as in the matrix
factorization  We compute an embedding analogously to
FrameAA by representing an image by its output of the
 rst convolutional layer  yielding the same embedding size
of        
For every obtained embedding  an SVM is trained on    
              archetypes lters meaning that every image is
now represented by   vector of size                  
Note that the last one is identical to the original image
size  We deploy an RBF kernel and the parameters   and
  are optimized on               grid respectively  For

Framebased Data Factorizations

Figure   Archetypes  left  and  lters  right  for      

we computed the frame by leveraging the well known
activeset method for nonnegative least squares problems
called NNLS  We provided   theoretical underpinning for
our approach and conducted   series of experiments to
compare the computation of the frame with two LPbased
approaches to show our competitiveness 
Moreover  we proposed an approximation of Archetypal
Analysis called FrameAA by restricting the optimization
of the archetypes to the frame  We showed that if   small
number of data points approximated the frame suf ciently 
the resulting approximation of AA on the whole data set
turned out appropriate  This approximation does not only
work for Archetype Analysis but also for all variants of
it like for instance   rup and Hansen   Chen et al 
  and Bauckhage et al   
Empirically  we showed that the error of FrameAA is only
slightly higher than standard Archetypal Analysis but was
often much faster once the frame has been computed  We
leveraged this observation and proposed an ef cient model
selection strategy to  nd the optimal number of archetypes 
We proposed to compute the frame only once and then reused it for all subsequent computations for other numbers
of archetypes  We observed an enormous acceleration in
scenarios with low frame densities 
Low frame densities were attained when only small
portions of
On the other
hand  high frame densities diminish this speedup and
the computational time converges to that of   standard
Archetypal Analysis  However  we argued that these
scenarios are not suited for Archetypal Analyses in the  rst
case 
Furthermore  we compared FrameAA to several stateof theart baselines  FrameAA either outperformed its
competitors or performed onpar  On the example of
an autoencoder  FrameAA led to nicely interpretable
classi cations in contrast to convolutional neural networks 

the data were extreme 

Acknowledgements

This work has been funded in parts by the German Federal
Ministry of Education and Science BMBF under grant
QQM LSA   

Figure   Predictive performance in terms of accuracy and
reconstruction error in terms of Frobenius norm for various
embedding sizes 

the hyperparameter search we use   hold out set consisting
of another   images per class that are disjoint from the
training set 
To evaluate the predictive performance we sample another
  test images per class and obtain their embeddings using
the trained approaches described above  The results are
depicted in Figure   The top part of the  gure depicts the
reconstruction errors in terms of the Frobenius norm  The
curve of FrameAA has   typical elbowstructure as one
would expect  while the error of the net is almost static 
Although the net achieves smaller reconstruction errors 
Figure    bottom  shows that the SVM effectively leverages
the representation learned by FAA  The archetypebased
embedding leads to more accurate classi cation models 
The learnt archetypes lters are shown in Figure   for the
case of       yielding an embedding of size      
The archetypes in the left part of the  gure resembles
edge and border detectors while the  lters on the right
appear random  Moreover  every patch is represented
as   convex combination of the shown archetypes  That
is  the classi cation of   patch can be explained by the
corresponding weights to render the results interpretable 

  Conclusion
We proposed   novel method for computing the frame of  
data set  The frame is   subset of the data set containing
only the extreme points       the vertices of the convex hull
of the data  While standard approaches like Quickhull were
infeasible for scenarios with more than three dimensions 

Framebased Data Factorizations

Horton  Paul and Nakai  Kenta 

  probabilistic
cellular
In Ismb  volume  

the

system for

classi cation
localization sites of proteins 
pp     

predicting

Lawson  Charles   and Hanson  Richard    Solving least

squares problems  volume   SIAM   

LeCun  Yann  Boser  Bernhard  Denker 

John   
Henderson  Donnie  Howard  Richard    Hubbard 
Wayne  and Jackel  Lawrence    Backpropagation
applied to handwritten zip code recognition  Neural
computation     

Lee  Daniel   and Seung    Sebastian  Learning the parts
of objects by nonnegative matrix factorization  Nature 
   

Lopez  FranciscoJavier  Generating random points  or
vectors  controlling the percentage of them that are
extreme in their convex  or positive  hull  Journal of
Mathematical Modelling and Algorithms   
 

  rup  Morten and Hansen  Lars Kai  Archetypal analysis
for machine learning  In Machine Learning for Signal
Processing  MLSP    IEEE International Workshop
on  pp    IEEE   

Ottmann  Thomas  Schuierer  Sven  and Soundaralakshmi 
Subbiah 
Enumerating extreme points in higher
dimensions  Nordic Journal of Computing   
   

Paatero  Pentti and Tapper  Unto 

Positive matrix
factorization    nonnegative factor model with
optimal utilization of error estimates of data values 
Environmetrics     

Thurau  Christian  Kersting  Kristian  Wahabzada 
Mirwaes  and Bauckhage  Christian 
Convex nonnegative matrix factorization for massive datasets 
Knowledge and information systems   
 

Vinu    Guillermo  Anthropometry  An   package for
analysis of anthropometric data  Journal of Statistical
Software     

References
Arora  Raman  Gupta  Maya  Kapila  Amol 

and
Fazel  Maryam  Clustering by leftstochastic matrix
factorization  In Proceedings of the  th International
Conference on Machine Learning  ICML  pp   
   

Arora  Raman  Gupta  Maya    Kapila  Amol  and Fazel 
Maryam  Similaritybased clustering by leftstochastic
Journal of Machine Learning
matrix factorization 
Research     

Barber    Bradford  Dobkin  David    and Huhdanpaa 
Hannu  The quickhull algorithm for convex hulls  ACM
Transactions on Mathematical Software  TOMS   
   

Bauckhage     Kersting  HF  Thurau     et al  Archetypal
In Workshop New
 

analysis as an autoencoder 
Challenges in Neural Computation   pp 
Citeseer   

Brondsted  Arne  An introduction to convex polytopes 
volume   Springer Science   Business Media   

Chen  Yuansi  Mairal 

Julien  and Harchaoui  Zaid 
Fast and robust archetypal analysis for representation
In Proceedings of the IEEE Conference on
learning 
Computer Vision and Pattern Recognition  pp   
   

Cutler  Adele and Breiman  Leo  Archetypal analysis 

Technometrics     

Ding  Chris HQ  Li  Tao  and Jordan  Michael

  
Convex and seminonnegative matrix factorizations 
IEEE transactions on pattern analysis and machine
intelligence     

Dul    Jos     and Helgason  Richard      new procedure
for identifying the frame of the convex hull of  
 nite collection of points in multidimensional space 
European Journal of Operational Research   
   

Dul    Jos     and   opez  Francisco    Competing outputsensitive frame algorithms  Computational Geometry 
   

Epifanio 

Irene  Vinu       and Alemany  Sandra 
Archetypal analysis 
for estimating
boundary cases in multivariate accommodation problem 
Computers   Industrial Engineering   
 

contributions

Heinz  Grete  Peterson  Louis    Johnson  Roger   
and Kerk  Carter    Exploring relationships in body
Journal of Statistics Education   
dimensions 
 

