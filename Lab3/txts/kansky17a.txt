Schema Networks  Zeroshot Transfer with   Generative Causal Model of

Intuitive Physics

Ken Kansky Tom Silver David      ely Mohamed Eldawy Miguel   azaroGredilla Xinghua Lou

Nimrod Dorfman Szymon Sidor Scott Phoenix Dileep George

Abstract

tasks 

The recent adaptation of deep neural networkbased methods to reinforcement learning and
planning domains has yielded remarkable
progress on individual
Nonetheless 
progress on taskto task transfer remains limited 
In pursuit of ef cient and robust generalization 
we introduce the Schema Network  an objectoriented generative physics simulator capable
of disentangling multiple causes of events and
reasoning backward through causes to achieve
goals  The richly structured architecture of the
Schema Network can learn the dynamics of an
environment directly from data  We compare
Schema Networks with Asynchronous Advantage ActorCritic and Progressive Networks on  
suite of Breakout variations  reporting results on
training ef ciency and zeroshot generalization 
consistently demonstrating faster  more robust
learning and better transfer  We argue that
generalizing from limited data and learning
causal relationships are essential abilities on the
path toward generally intelligent systems 

  Introduction
  longstanding ambition of research in arti cial intelligence is to ef ciently generalize experience in one scenario
to other similar scenarios  Such generalization is essential
for an embodied agent working to accomplish   variety of
goals in   changing world  Despite remarkable progress on
individual tasks like Atari   games  Mnih et al   
Van Hasselt et al    Mnih et al    and Go  Silver
et al      the ability of stateof theart models to transfer learning from one environment to the next remains lim 

All authors af liated with Vicarious AI  California  USA  Correspondence to  Ken Kansky  ken vicarious com  Tom Silver
 tom vicarious com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Variations of Breakout  From top left  standard version 
middle wall  half negative bricks  offset paddle  random target 
and juggling  After training on the standard version  Schema Networks are able to generalize to the other variations without any
additional training 

ited  For instance  consider the variations of Breakout illustrated in Fig    In these environments the positions of objects are perturbed  but the object movements and sources
of reward remain the same  While humans have no trouble
generalizing experience from the basic Breakout to its variations  deep neural networkbased models are easily fooled
 Taylor   Stone    Rusu et al   
The modelfree approach of deep reinforcement learning
 Deep RL  such as the DeepQ Network and its descendants is inherently hindered by the same feature that makes
it desirable for singlescenario tasks  it makes no assumptions about the structure of the domain  Recent work has
suggested how to overcome this de ciency by utilizing
objectbased representations  Diuk et al    Usunier
et al    Such   representation is motivated by the

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

wellacknowledged Gestalt principle  which states that the
ability to perceive objects as   bounded  gure in front of
an unbounded background is fundamental to all perception
 Weiten    Battaglia et al    and Chang et al 
  go further  de ning hardcoded relations between
objects as part of the input 
While objectbased and relational representations have
shown great promise alone  they stop short of modeling
causality   the ability to reason about previous observations
and explain away alternative causes    causal model is essential for regression planning  in which an agent works
backward from   desired future state to produce   plan
 Anderson    Reasoning backward and allowing for
multiple causation requires   framework like Probabilistic
Graphical Models  PGMs  which can natively support explaining away  Koller   Friedman   
Here we introduce Schema Networks     generative model
for objectoriented reinforcement learning and planning 
Schema Networks incorporate key desiderata for the  exible and compositional transfer of learned prior knowledge to new settings    Knowledge is represented with
 schemas    local causeeffect relationships involving one
or more object entities    In   new setting  these causeeffect relationships are traversed to guide action selection 
and   The representation deals with uncertainty  multiplecausation  and explaining away in   principled way  We
 rst describe the representational framework and learning
algorithms and then demonstrate how action policies can
be generated by treating planning as inference in   factor graph  We evaluate the endto end system on Breakout variations and compare against Asynchronous Advantage ActorCritic        Mnih et al    and Progressive Networks  PNs   Rusu et al    the latter of which
extends     explicitly to handle transfer  We show that
the structure of the Schema Network enables ef cient and
robust generalization beyond these Deep RL models 

  Related Work
The  eld of reinforcement learning has witnessed signi 
cant progress with the recent adaptation of deep learning
methods to traditional frameworks like Qlearning  Since
the introduction of the Deep Qnetwork  DQN   Mnih
et al    which uses experience replay to achieve
humanlevel performance on   set of Atari   games 
several innovations have enabled faster convergence and
better performance with less memory  The asynchronous
methods introduced by Mnih et al    exploit multiple agents acting in copies of the same environment  combining their experiences into one model  As the Asyn 

 We borrow the term  schema  from Drescher   whose
schema mechanism inspired the early development of our model 

chronous Advantage ActorCritic       is the best among
these methods  we use it as our primary comparison 
Modelfree Deep RL models like     are unable to
substantially generalize beyond their training experience
 Jaderberg et al    Rusu et al    To address this
limitation  recent work has attempted to introduce more
structure into neural networkbased models  The Interaction Network  Battaglia et al     IN  and the Neural
Physics Engine  NPE   Chang et al    use objectlevel
and pairwise relational representations to learn models of
intuitive physics  The primary advantage of these models
is their amenability to gradientbased methods  though such
techniques might be applied to Schema Networks as well 
Schema Networks offer two key advantages  latent physical properties and relations need not be hardcoded  and
planning can make use of backward search  since the model
can distinguish different causes  Furthermore  neither INs
nor NPEs have been applied in RL domains  Progress in
modelbased Deep RL has thus far been limited  though
methods like Embed to Control  Watter et al    Value
Iteration Networks  Tamar et al    and the Predictron
 Silver et al      demonstrate the promise of this direction  However  these approaches do not exploit the objectrelational representation of INs or NPEs  nor do they incorporate   backward model for regression planning 
Schema Networks build upon the ideas of the ObjectOriented Markov Decision Process  OOMDP  introduced
by Diuk et al     see also Scholz et al    Related frameworks include relational and  rstorder logical
MDPs  Guestrin et al      These formalisms  which
harken back to classical AI   roots in symbolic reasoning 
are designed to enable robust generalization  Recent work
by Garnelo et al    on  deep symbolic reinforcement
learning  makes this connection explicit  marrying  rstorder logic with deep RL  This effort is similar in spirit to
our work with Schema Networks  but like INs and NPEs  it
lacks   mechanism to learn disentangled causes of the same
effect and cannot perform regression planning 
Schema Networks transfer experience from one scenario
to other similar scenarios that exhibit repeatable structure
and substructure  Taylor   Stone    Rusu et al 
  show how     can be augmented to similarly exploit common structure between tasks via Progressive Networks  PNs    PN is constructed by successively training
copies of     on each task of interest  With each new
task  the existing network is frozen  another copy of    
is added  and lateral connections between the frozen network and the new copy are established to facilitate transfer of features learned during previous tasks  One obvious
limitation of PNs is that the number of network parameters
must grow quadratically with the number of tasks  However  even if this growth rate was improved  the PN would

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

still be unable to generalize from biased training data without continuing to learn on the test environment  In contrast 
Schema Networks exhibit zeroshot transfer 
Schema Networks are implemented as probabilistic graphical models  PGMs  which provide practical inference and
structure learning techniques  Additionally  inference with
uncertainty and explaining away are naturally supported by
PGMs  We direct the readers to  Koller   Friedman   
and  Jordan    for   thorough overview of PGMs  In
particular  early work on factored MDPs has demonstrated
how PGMs can be applied in RL and planning settings
 Guestrin et al     

  Schema Networks
  MDPs and Notation

The traditional formalism for the Reinforcement Learning
problem is the Markov Decision Process  MDP  An MDP
  is    vetuple               where   is   set of states 
  is   set of actions                   is the probability of transitioning from state          to          after action                           is the probability
of receiving reward          after executing action     
while in state      and         is the rate at which future
rewards are exponentially discounted 

  Model De nition

  Schema Network is   structured generative model of an
MDP  We  rst describe the architecture of the model informally  An image input is parsed into   list of entities  which
may be thought of as instances of objects in the sense of
OOMDPs  Diuk et al    All entities share the same
collection of attributes  We refer to   speci   attribute of
  speci   entity as an entityattribute  which is represented
as   binary variable to indicate the presence of that attribute
for an entity  An entity state is an assignment of states to
all attributes of the entity  and the complete model state is
the set of all entity states 
  grounded schema is   binary variable associated with
  particular entityattribute in the next timestep  whose
value depends on the present values of   set of binary
entityattributes  The event that one of these present entityattributes assumes the value   is called   precondition of the
grounded schema  When all preconditions of   grounded
schema are satis ed  we say that the schema is active  and
it predicts the activation of its associated entityattribute 
Grounded schemas may also predict rewards and may be
conditioned on actions  both of which are represented as
binary variables  For instance    grounded schema might
de ne   distribution over Entity     position  attribute at
time   conditioned on Entity     position  attribute at
time   and the action  UP  at time   Grounded schemas

Figure   Architecture of   Schema Network  An ungrounded
schema is   template for   factor that predicts either the value
of an entityattribute     or   future reward     based on entity
states and actions taken in the present  Selftransitions     predict
that entityattributes remain in the same state when no schema is
active to predict   change  Selftransitions allow continuous or
categorical variables to be represented by   set of binary variables
 depicted as smaller nodes  The grounded schema factors  instantiated from ungrounded schemas at all positions  times  and entity
bindings  are combined with selftransitions to create   Schema
Network    

are instantiated from ungrounded schemas  which behave
like templates for grounded schemas to be instantiated at
different times and in different combinations of entities 
For example  an ungrounded schema could predict the  position  attribute of Entity   at time       conditioned on
the  position  of Entity   at time   and the action  UP 
at time    this ungrounded schema could be instantiated at
time       with       and       to create the grounded
schema described above  In the case of attributes like  position  that are inherently continuous or categorical  several
binary variables may be used to discretely approximate the
distribution  see the smaller nodes in Figure     Schema
Network is   factor graph that contains all grounded instantiations of   set of ungrounded schemas over some window
of time  illustrated in Figure  
We now formalize the Schema Network factor graph  For
simplicity  suppose the number of entities and the number of attributes are  xed at   and   respectively  Let
Ei refer to the ith entity and let    
    refer to the jth attribute value of the ith entity at time    We use the notation
    
       
      to refer to the state of the ith en 

        

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

template 

          

let AND      vn     cid  

and OR      vn         cid  

tity at time    The complete state of the MDP modeled by
the network at time   is then             
    Actions and rewards are also represented with sets of binary
variables  denoted      and      respectively    Schema
Network for time   will contain the variables in          
     and     
Let    denote the variable for grounded schema      
is bound to   speci   entityattribute      and activates it
when the schema is active  Multiple grounded schemas
can predict the same attribute  and these predictions are
For binary variables
combined through an OR gate 
     vn 
      vi    
        vi    
  grounded schema is connected to its precondition
entityattributes with an AND factor  written as     
AND         iH  jH      for   entityattribute preconditions and an optional action    There is no restriction on
how many entities or attributes from   single entity can be
preconditions of   grounded schema 
An ungrounded schema  or
is represented
as    Ex    ExH     AND             xH  yH  
where xh determines the relative entity index of the hth
precondition and yh determines which attribute variable is
the precondition  The ungrounded schema is   template
that can be bound to multiple speci   entities and locations
to generate grounded schemas 
  subset of attributes corresponds to discrete positions 
These attributes are treated differently from all others 
whose semantic meanings are unknown to the model 
When   schema predicts   movement to   new position 
we must inform the previously active position attribute to
be inactive unless there is another schema that predicts it
to remain active  We introduce   selftransition variable to
represent the probability that   position attribute will remain active in the next time step when no schema predicts
  change from that position  We compute the selftransition
variable as        AND      si    for entity  
and position attribute    where the set    includes all
schemas that predict the future position of the same entity
  and include si   as   precondition 
With these terms de ned  we may now compute
the transition function  which can be factorized as
          
An entityattribute is active at the next time step if either
  schema predicts it to be active or if its selftransition vari 
        OR         kQ       
able is active  Ti       
where   kQ are the indices of all grounded schemas that
predict si   

                    cid  

 cid  

   Ti       

   

  

   

  Construction of Entities and Attributes

In practice we assume that   vision system is responsible for detecting and tracking entities in an image 
It is
therefore largely up to the vision system to determine what
constitutes an entity  Essentially any trackable image feature could be an entity  which most typically includes objects  their boundaries  and their surfaces  Recent work has
demonstrated one possible method for unsupervised entity
construction using autoencoders  Garnelo et al    Depending on the task  Schema Networks could learn to reason  exibly at different levels of representation  For example  using entities from surfaces might be most relevant
for predicting collisions  while using one entity per object
might be most relevant for predicting whether it can be controlled by an action  The experiments in this paper utilize
surface entities  described further in Section  
entity attributes can be provided by the
Similarly 
vision system  and these attributes typically include 
color appearance  surface edge orientation  object category  or partof an object category      
frontleft tire 
For simplicity we here restrict the entities to have fully observable attributes  but in general they could have latent attributes such as  bounciness  or  magnetism 

  Connections to Existing Models

Schema Networks are closely related to ObjectOriented
MDPs  OOMDPs   Diuk et al    and Relational
MDPs  RMDPs   Guestrin et al      However  neither OOMDPs nor RMDPs de ne   transition function
with an explicit OR of possible causes  and traditionally
transition functions have not been learned in these models 
In contrast  Schema Networks provide an explicit OR to
reason about multiple causation  which enables regression
planning  Additionally  the structure of Schema Networks
is amenable to ef cient learning 
Schema Networks are also related to the recently proposed
Interaction Network  IN   Battaglia et al    and Neural
Physics Engine  NPE   Chang et al    At   high level 
INs  NPEs  and Schema Networks are much alike   objects
are to entities as relations are to schemas  However  neither INs nor NPEs are generative and hence do not support
regression planning from   goal through causal chains  Because Schema Networks are generative models  they support more  exible inference and search strategies for planning  Additionally  the learned structures in Schema Networks are amenable to human interpretation  explicitly factorizing different causes  making prediction errors easier to
relate to the learned model parameters 

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

  Learning and Planning in Schema

Networks

In this section we describe how to train Schema Networks
      learn its structure  from interactions with an environment  as well as how they can be used to perform planning 
Planning is not only necessary at test time to maximize reward  but also can be used to improve exploration during
the training procedure 

  Training Procedure

 

Given   series of actions  rewards and images  we represent
each possible action and reward with   binary variable  and
we convert each image into   set of entity states  The number of entities is allowed to vary between adjacent frames 
accounting for objects appearing or moving out of view 
Given   dataset of entity states over time  we preprocess the
entity states into   representation that is more convenient
for learning  For   entities observed over   timesteps  we
wish to predict    
    on the basis of the attribute values of
the ith entity and its spatial neighbors at time        for
          and             The attribute values of
    
and its neighbors can be represented as   row vector
of length      where   is the number of attributes and
      is the number of neighbor positions of each entity 
determined by    xed radius  Let            cid 
be
the arrangement of all such vectors into   binary matrix 
with         and   cid         Let          be  
binary vector such that if row   in   refers to     
  then
yr      
      Schemas are then learned to predict   from  
using the method described in Section  
While gathering data  actions are chosen by planning using
the schemas that have been learned so far  This planning
algorithm is described in Section   We use an  greedy
approach to encourage exploration  taking   random action at each timestep with small probability  We found no
need to perform any additional policy learning  and after
convergence predictions were accurate enough to allow for
successful planning  As shown in Section   since learning only involves understanding the dynamics of the game 
transfer learning is simpli ed and there is no need for policy adaptation 

 

  Schema Learning

Structure learning in graphical models is   well studied
topic in machine learning  Koller   Friedman    Jordan    To learn the structure of the Schema Network 
we cast the problem as   supervised learning problem over
  discrete space of parameterizations  the schemas  and
then apply   greedy algorithm that solves   sequence of LP
relaxations  See Jaakkola et al    for further work on

applying LP relaxations to structure learning 
With   and   de ned above  the learning problem is to  nd
  mapping

    fW       XW cid 

where all the involved variables are binary and operations
follow Boolean logic  addition corresponds to ORing  and
overlining to negation           cid   is   binary
matrix  with each column representing one  ungrounded 
schema for at most   schemas  The elements set to   in
each schema represent an existing connection between that
schema and an input condition  see Fig    The outputs
of each individual schema are ORed to produce the  nal
prediction 
We would like to minimize the prediction error of Schema
Networks while keeping them as simple as possible    suitable objective function is

min

    cid  

 
 

     fW           

 

where the  rst term computes the prediction error  the second term estimates the complexity and parameter   controls the tradeoff between both  This is an NPhard problem for which we cannot hope to  nd an exact solution 
except for very small environments 
We consider   greedy solution in which linear programming  LP  relaxations are used to  nd each new schema 
Starting from the empty set  we greedily add schemas
 columns to     that have perfect precision and increase
recall for the prediction of    See Algorithm   in the
Supplementary 
In each successive iteration  only the
inputoutput pairs for which the current schema network
is predicting an output of zero are passed  This procedure
monotonically decreases the prediction error of the overall
schema network  while increasing its complexity  The process stops when we hit some prede ned complexity limit 
In our implementation  the greedy schema selection produces very sparse schemas  and we simply set   limit to
the number of schemas to add  For this algorithm to work 
no contradictions can exist in the input data  such as the
same input appearing twice with different labels  Such
contradictions might appear in stochastic environments and
would not be artifacts in real environments  so we preprocess the input data to remove them 

  Planning as Probabilistic Inference

The full Schema Network graph  Fig    provides   probabilistic model for the set of rewards that will be achieved by
  sequence of actions  Finding the sequence of actions that
will result in   given set of rewards becomes then   MAP

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

inference problem  This problem can be addressed approximately using maxproduct belief propagation  MPBP   Attias    Another option is variational inference  Cheng
et al    use variational inference for planning but resort to MPBP to optimize the variational free energy functional  We will follow the  rst approach 
Without loss of generality  we will consider the present
time step to be       The state  action and reward variables
for       are observed  and we will consider inference over
the unobserved variables in   lookahead window of size 
                     
     Since the Schema Network is built
exclusively of compatibility factors that can take values  
or   any variable assignment is either impossible or equally
probable under the joint distribution of the graph  Thus  if
we want to know if there exists any global assignment that
activates   binary variable  say  variable     
  signaling positive reward at some future time       we should look at
the maxmarginal        
      It will be   if no global
assignment compatible with both the SN and existing observations can lead to activate the reward  or   if it is feasible  Similarly  we will be interested in the maxmarginal
       
           whether it is feasible to  nd   con guration that avoids   negative reward 
At   highlevel  planning proceeds as follows  Identify feasible desirable states  activating positive rewards and deactivating negative rewards  clamp their value to our desires by adding   unary potential to the factor graph  and
then  nd the MAP con guration of the resulting graph 
The MAP con guration contains the values of the action
variables that are required to reach our goal of activating deactivating   variable  We can also look at   to see
how the model  imagines  the evolution of the entities until they reach their goal state  Then we perform the actions
found by the planner and repeat  We now explain each of
these stages in more detail 

Potential feasibility analysis First we run   feasibility
analysis  To this end    forward pass MPBP from time  
to time   is performed  This provides    coarse  approximation to the desired maxmarginals for every variable 
Because the SN graph is loopy  MPBP is not exact and the
forward pass can be too optimistic  announcing the feasibility of states that are unfeasible  Actual feasibility will
be veri ed later  at the backtracking stage 

 In contrast with MDPs 

the reward is discounted with  
rolling square window instead of an exponentially weighted one 
 To illustrate the problem  consider the case in which it is feasible for an entity to move at time   to position   or position  
 but obviously not both  and then some reward is conditioned on
that type of entity being in both positions    single forward pass
will not handle the entanglement properly and will incorrectly report that such reward is also feasible 

Choosing   positive reward goal state We will choose
the potentially feasible positive reward that happens soonest within our explored window  clamp its state to   and
backtrack  see below  to  nd the set of actions that lead to
it  If backtracking fails  we will repeat for the remaining
potentially feasible positive rewards 

Avoiding negative rewards Keeping the selected positive reward variable clamped to    if it was found in the
previous step  we now repeat the same procedure on the
negative rewards  Among the negative rewards that have
been found as potentially feasible to turn off  we clamp to
zero as many negative rewards as we can  nd   jointly satisfying backtrack  If no positive reward was feasible  we
backtrack from the earliest predicted negative reward 

Backtracking This step is akin to Viterbi backtracking 
  message passing backward pass that  nds   satisfying
con guration  Unlike the HMM for which the Viterbi algorithm was designed  our model is loopy  so   standard
backward pass is not enough to  nd   satisfying con guration  although can help to  nd good candidates  We combine the standard backward pass with   depth rst search
algorithm to  nd   satisfying con guration 

  Experiments
We compared the performance of Schema Networks      
and PNs  Progressive Networks  on several variations of
the game Breakout  The chosen variations all share similar dynamics  but the layouts change  requiring different
policies to achieve high scores    diverse set of concepts
must be learned to correctly predict object movements and
rewards  For example  when predicting why rewards occur 
the model must disentangle possible causes to discover that
reward depends on the color of   brick but is independent
of the ball   velocity and position where it was hit  While
these causal relationships are straightforward for humans to
recover  we have yet to see any existing approach for learning   generative model that can recover all of these dynamics without supervision and transfer them effectively 
Schema Networks rely on an input of entity states instead
of raw images  and we provided the same information to
    and PNs by augmenting the three color channels of the
image with   additional channels  Four of these channels
indicated the shape to which each pixel belongs  including
shapes for bricks  balls  and walls  Another   channels
indicated the positions of parts of the paddle  where each
part consisted of   single pixel  To reduce training time 
we did not provide     and PN with part channels for objects other than the paddle  since these are not required to
learn the dynamics or predict scores  Removing irrelevant
inputs could only give     and PN an advantage  since

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

    Mini Breakout Learning Rate

    Middle Wall Learning Rate

Figure   Comparison of learning rates      Schema Networks and     were trained for    frames in Mini Breakout  Plot shows the
average of   training attempts for Schema Networks and the best of   training attempts for      which did not converge as reliably     
PNs and Schema Networks were pretrained on    frames of Standard Breakout  and then training continued on    additional frames
of the Middle Wall variation  We show performance as   function of training frames for both models  Note that Schema Networks are
ignoring all the additional training data  since all the required schemas were learned during pretraining  For Schema Networks  zeroshot
transfer learning is happening 

the input to Schema Networks did not treat any object differently  Schema Networks were provided separate entities
for each part  pixel  of each object  and each entity contained   attributes corresponding to the available part labels   for bricks    for the paddle    for walls  and   for
the ball  Only one of these part attributes was active per
entity  Schema Networks had to learn that some attributes 
like parts of bricks  were irrelevant for prediction 

  Transfer Learning

This experiment examines how effectively Schema Networks and PNs are able to learn   new Breakout variation
after pretraining  which examines how well the two models can transfer existing knowledge to   new task  Fig    
shows the learning rates during    frames of training on
Mini Breakout  In   second experiment  we pretrained on
Large Breakout for    frames and continued training on
the Middle Wall variation  shown in Fig      Fig     shows
that PNs require signi cant time to learn in this new environment  while Schema Networks do not learn anything
new because the dynamics are the same 

Rather than comparing transfer with additional training using PNs  in these variations we can compare zeroshot generalization by training     only on Standard Breakout 
Fig   be shows some of these variations with the following
modi cations from the training environment 

  Offset Paddle  Fig      The paddle is shifted upward

by   few pixels 

  Middle Wall  Fig        wall is placed in the middle
of the screen  requiring the agent to aim around it to
hit the bricks 

  Random Target  Fig        group of bricks is
destoyed when the ball hits any of them and then reappears in   new random position  requiring the agent to
delibarately aim at the group 

  Juggling  Fig      enlarged from actual environment
to see the balls  Without any bricks  three balls are
launched in such   way that   perfect policy could juggle them without dropping any 

  ZeroShot Generalization

Many Breakout variations can be constructed that all involve the same dynamics  If   model correctly learns the
dynamics from one variation  in theory the others could
be played perfectly by planning using the learned model 

Table   shows the average scores per episode in each
Breakout variation  These results show that     has failed
to recognize the common dynamics and adapt its policy accordingly  This comes as no surprise  as the policy it has
learned for Standard Breakout is no longer applicable in
these variations  Simply adding an offset to the paddle is

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

Table   ZeroShot Average Score per Episode Average of the   best out of   training attempts for      and average of   training
attempts for Schema Networks      was trained on    frames of Standard Breakout  hence its zeroshot scores for Standard Breakout
are unknown  while Schema Networks were trained on    frames of Mini Breakout  Episodes were limited to   frames for all
variations  In every case the average Schema Network scores are better than the best     scores by more than one standard deviation 

Standard Breakout Offset Paddle Middle Wall
     
    Image Only
   
     
    Image   Entities    
     
Schema Networks

     
     
     

     

Random Target

Juggling

           
           
           

Table   Average Score per Episode on Half Negative Bricks
    and Schema Networks were trained on    frames of Random Negative Bricks  both to convergence  Testing episodes were
limited to   frames  Negative rewards are sometimes unavoidable  resulting in higher variance for all methods 

    Image Only
    Image   Entities
Schema Networks

Half Negative Bricks

     
     
     

suf cient to confuse      which has not learned the causal
nature of controlling the paddle with actions and controlling the ball with the paddle  The Middle Wall and Random
Target variations illustrate that Schema Networks are aiming to deliberately cause positive rewards from ballbrick
collisions  while     struggles to adapt its policy accordingly  The Juggling variation is particularly challenging 
since it is not clear which ball to respond to unless the
model understands that the lowest downwardmoving ball
is the most imminent cause of   negative reward  By learning and transferring the correct causal dynamics  Schema
Networks outperform     in all variations 

  Testing for Learned Causes

To better evaluate whether these models are truly learning
the causes of rewards  we designed one more zeroshot generalization experiment  We trained both Schema Networks
and     on   Mini Breakout variation in which the color
of   brick determines whether   positive or negative reward
is received when it is destroyed  Six colors of bricks provide   reward  and two colors provide   reward  Negative
bricks occurred in random positions   of the time during training  Then during testing  the bricks were arranged
into two halves  with all positive colored bricks on one half
and negative colored bricks on the other  see Fig      If
the causes of rewards have been correctly learned  the agent
should prefer to aim for the positive half whenever possible 
As Table   shows  Schema Networks have correctly learned

from random arrangements which brick colors cause which
rewards  preferring to aim for the positive half during testing  while     demonstrates no preference for one half or
the other  achieving an average score near zero 

  Discussion and Conclusion
In this work we have demonstrated the promise of Schema
Networks with strong performance on   suite of Breakout
variations 
Instead of learning policies to maximize rewards  the learning objective for Schema Networks is designed to understand causality within these environments 
The fact that Schema Networks are able to achieve rewards
more ef ciently than stateof theart modelfree methods
like     is all the more notable  since high scores are  
byproduct of learning an accurate model of the game 
The success of Schema Networks is derived in part from
the entitybased representation of state  Our results suggest
that providing Deep RL models like     with such   representation as input can improve both training ef ciency and
generalization  This  nding corroborates recent attempts
 Usunier et al    Garnelo et al    Chang et al 
  Battaglia et al    to incorporate object and relational structure into neural networkbased models 
The environments considered in this work are conceptually
diverse but also simpli ed in   number of ways with respect to the real world  states  actions  and rewards are all
discretized as binary random variables  the dynamics of the
environments are deterministic  and there is no uncertainty
in the observed entity states  In future work we plan to address each of these limitations  adapting Schema Networks
to continuous  stochastic domains 
Schema Networks have shown promise toward multitask
transfer where Deep RL struggles  This transfer is enabled
by explicit causal structures  which in turn allow for planning in novel tasks  As progress in RL and planning continues  robust generalization from limited experience will
be vital for future intelligent systems 

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

Acknowledgements
Special thanks to Eric Purdy and Ramki Gummadi for useful insights and discussions during the preparation of this
work 

References
Anderson  John    Cognitive psychology and its implications  WH Freeman Times Books Henry Holt   Co 
 

Attias  Hagai  Planning by probabilistic inference  In AIS 

TATS   

Battaglia  Peter  Pascanu  Razvan  Lai  Matthew  Rezende 
Danilo Jimenez  et al 
Interaction networks for learning about objects  relations and physics  In Advances in
Neural Information Processing Systems  pp   
 

Chang  Michael    Ullman  Tomer  Torralba  Antonio  and
Tenenbaum  Joshua      compositional objectbased approach to learning physical dynamics  arXiv preprint
arXiv   

Cheng  Qiang  Liu  Qiang  Chen  Feng  and Ihler  Alexander    Variational planning for graphbased mdps 
In
Advances in Neural Information Processing Systems  pp 
   

Diuk  Carlos  Cohen  Andre  and Littman  Michael   
An objectoriented representation for ef cient reinforcement learning  In Proceedings of the  th international
conference on Machine learning  pp    ACM 
 

Drescher  Gary    Madeup minds    constructivist ap 

proach to arti cial intelligence  MIT press   

Garnelo  Marta  Arulkumaran  Kai  and Shanahan  Murray 
Towards deep symbolic reinforcement learning  arXiv
preprint arXiv   

Guestrin  Carlos  Koller  Daphne  Gearhart  Chris  and
Kanodia  Neal  Generalizing plans to new environments
In Proceedings of the  th interin relational mdps 
national joint conference on Arti cial intelligence  pp 
  Morgan Kaufmann Publishers Inc     

Guestrin  Carlos  Koller  Daphne  Parr  Ronald  and
Venkataraman  Shobha  Ef cient solution algorithms
for factored mdps  Journal of Arti cial Intelligence Research       

Jaakkola  Tommi    Sontag  David  Globerson  Amir 
Meila  Marina  et al  Learning bayesian network strucIn AISTATS  pp   
ture using lp relaxations 
 

Jaderberg  Max  Mnih  Volodymyr  Czarnecki  Wojciech Marian  Schaul  Tom  Leibo  Joel    Silver 
David  and Kavukcuoglu  Koray  Reinforcement learning with unsupervised auxiliary tasks  arXiv preprint
arXiv   

Jordan  Michael Irwin  Learning in graphical models  vol 

ume   Springer Science   Business Media   

Koller  Daphne and Friedman  Nir  Probabilistic graphical

models  principles and techniques  MIT press   

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David 
Rusu  Andrei    Veness  Joel  Bellemare  Marc   
Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas   
Ostrovski  Georg  et al  Humanlevel control through
deep reinforcement learning  Nature   
   

Mnih  Volodymyr  Badia  Adria Puigdomenech  Mirza 
Mehdi  Graves  Alex  Lillicrap  Timothy  Harley  Tim 
Silver  David  and Kavukcuoglu  Koray  Asynchronous
In Proceedmethods for deep reinforcement learning 
ings of The  rd International Conference on Machine
Learning  pp     

Rusu  Andrei    Rabinowitz  Neil    Desjardins  Guillaume  Soyer  Hubert  Kirkpatrick  James  Kavukcuoglu 
Koray  Pascanu  Razvan  and Hadsell  Raia  Progressive neural networks  arXiv preprint arXiv 
 

Scholz  Jonathan  Levihn  Martin  Isbell  Charles  and
Wingate  David    physicsbased model prior for objectoriented mdps  In Proceedings of the  st International
Conference on Machine Learning  ICML  pp   
   

Silver  David  Huang  Aja  Maddison  Chris    Guez 
Arthur  Sifre  Laurent  Van Den Driessche  George 
Schrittwieser  Julian  Antonoglou  Ioannis  Panneershelvam  Veda  Lanctot  Marc  et al  Mastering the game of
go with deep neural networks and tree search  Nature 
     

Silver  David  van Hasselt  Hado  Hessel  Matteo  Schaul 
Tom  Guez  Arthur  Harley  Tim  DulacArnold  Gabriel 
Reichert  David  Rabinowitz  Neil  Barreto  Andre  et al 
The predictron  Endto end learning and planning  arXiv
preprint arXiv     

Tamar  Aviv  Levine  Sergey  Abbeel  Pieter  WU  YI  and
Thomas  Garrett  Value iteration networks  In Advances
in Neural Information Processing Systems  pp   
   

Taylor  Matthew   and Stone  Peter  Transfer learning for
reinforcement learning domains    survey  Journal of
Machine Learning Research   Jul   

Schema Networks  Zeroshot Transfer with   Generative Causal Model of Intuitive Physics

Usunier  Nicolas  Synnaeve  Gabriel  Lin  Zeming  and
Chintala  Soumith  Episodic exploration for deep deterministic policies  An application to starcraft micromanagement tasks  arXiv preprint arXiv   

Van Hasselt  Hado  Guez  Arthur  and Silver  David  Deep
reinforcement learning with double qlearning  In AAAI 
pp     

Watter  Manuel  Springenberg  Jost  Boedecker  Joschka 
and Riedmiller  Martin  Embed to control    locally linear latent dynamics model for control from raw images 
In Advances in Neural Information Processing Systems 
pp     

Weiten     Psychology  Themes and Variations  PSY  
General Psychology Series  Cengage Learning   
ISBN  
URL https books 
google com books id   tznfeTxV   

