Learning Hierarchical Features from Deep Generative Models

Shengjia Zhao   Jiaming Song   Stefano Ermon  

Abstract

Deep neural networks have been shown to be
very successful at learning feature hierarchies in
supervised learning tasks  Generative models  on
the other hand  have bene ted less from hierarchical models with multiple layers of latent variables  In this paper  we prove that hierarchical
latent variable models do not take advantage of
the hierarchical structure when trained with some
existing variational methods  and provide some
limitations on the kind of features existing models can learn  Finally we propose an alternative
architecture that does not suffer from these limitations  Our model is able to learn highly interpretable and disentangled hierarchical features
on several natural image datasets with no taskspeci   regularization 

  Introduction
  key property of deep feedforward networks is that they
tend to learn learn increasingly abstract and invariant representations at higher levels in the hierarchy  Bengio   
Zeiler   Fergus    In the context of image data  low
levels may learn features corresponding to edges or basic
shapes  while higher levels learn more abstract features 
such as object detectors  Zeiler   Fergus   
Generative models with   hierarchical structure  where
there are multiple layers of latent variables  have been
less successful compared to their supervised counterparts    nderby et al   
In fact  the most successful generative models often use only   single layer of latent variables  Radford et al    van den Oord et al 
  and those that use multiple layers only show modest
performance increases in quantitative metrics such as loglikelihood    nderby et al    Bachman    Because of the dif culties in evaluating generative models

 Stanford University 

Zhao
 tsong stanford edu 

 zhaosj stanford edu 

Correspondence to 
Jiaming

Shengjia
Song

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Left  Body parts feature detectors only carry   small
amount of information about an underlying image  yet  it is suf 
cient for   con dent classi cation as   face  Right  if   hierarchical generative model attempts to reconstruct an image based on
these highlevel features  it could generate inconsistent images 
even when each part can be perfectly generated  Even though this
 face  is clearly absurd  Google cloud platform classi cation API
can identify with   con dence that this is   face 

 Theis et al    and the fact that adding network layers
increases the number of parameters  it is not always clear
whether the improvements truly come from the choice of
  hierarchical architecture  Furthermore  the capability of
learning   hierarchy of increasingly complex and abstract
features has only been demonstrated to   limited extent 
with feature hierarchies that are not nearly as rich as the
ones learned by feedforward networks  Gulrajani et al 
 
Part of the problem is inherent and unavoidable for any
generative model  The heart of the matter is that while
highly invariant and local features are often suf cient for
classi cation  generative modeling requires preservation of
details  as illustrated in Figure   In fact  most latent features in   generative model of images cannot even demonstrate scale and translation invariance  The size and location of   subpart often has to be dependent on the other
subparts  For example  an eye should only be generated
with the same size as the other eye  at symmetric locations
with respect to the center of the face  with appropriate distance between them  The inductive biases that are directly
encoded into the architecture of convolutional networks is

insu cid cient for generationsu cid cient for classi cid cationLearning Hierarchical Features from Deep Generative Models

not suf cient in the context of generative models 
On the other hand  other problems are associated with speci   models or design choices  and may be avoided with
alternative training criteria and architectures  The goal of
this paper is to provide   deeper understanding of the design and performance of common hierarchical latent variable models  We focus on variational models  though it
should be possible to generalize most of the conclusions
to adversarially trained models that support inference  Dumoulin et al    Donahue et al    In particular  we
study two classes of models with   hierarchical structure 
  Stacked hierarchy  The  rst type we study is characterized by recursively stacking generative models on top
of each other  Most existing models    nderby et al 
  Gulrajani et al    Bachman    Kingma et al 
  belong to this class  We show that these models have
two limitations  First  we show that if these models can
be trained to optimality  then the bottom layer alone contains enough information to reconstruct the data distribution  and the layers above the  rst one can be ignored  This
result holds under fairly general conditions  and does not
depend on the speci   family of distributions used to de 
 ne the hierarchy       Gaussian  Second  we argue that
many of the building blocks commonly used to construct
hierarchical generative models are unlikely to help us learn
disentangled features 
  Architectural hierarchy  Motivated by these limitations  we turn our attention to single layer latent variable
models  We propose an alternative way to learn disentangled hierarchical features by crafting   network architecture that prefers to place highlevel features on certain parts
of the latent code  and lowlevel features in others  We
show that this approach  called Variational Ladder Autoencoder  allows us to learn very rich feature hierarchies
on natural image datasets such as MNIST  SVHN  Netzer
et al    and CelebA  Liu et al    in contrast 
generative models with   stacked hierarchical structure fail
to learn such features 

  Problem Setting
We consider   family of latent variable models speci ed
by   joint probability distribution         over   set of
observed variables   and latent variables    The family of models is assumed to be parametrized by   Let
     denote the marginal distribution of    We wish to
maximize the marginal loglikelihood      over   dataset
                      drawn from some unknown underlying distribution pdata    Formally we would like to
maximize

  cid 

  

log       

 

log       

which is nonconvex and often intractable for complex generative models  as it involves marginalization over the latent variables   
We are especially interested in unsupervised feature learning applications  where by maximizing   we hope to discover   meaningful representation for the data   in terms
of latent features given by       

  Variational Autoencoders

  popular solution  Kingma   Welling    Jimenez
Rezende et al   
for optimizing the intractable
marginal likelihood   is to optimize the evidence lower
bound  ELBO  by introducing an inference model       
parametrized by    

log        Eq     log           log       

  Eq     log          KL       cid     
          

 

where KL is the KullbackLeibler divergence 

  Hierarchical Variational Autoencoders

  hierarchical VAE  HVAE  can be thought of as   series
of VAEs stacked on top of each other  It has the following
hierarchy of latent variables                 zL  in addition
to the observed variables    We use the conventional notation where    represents the lowest layer  closest to    and
zL the top layer  Using chain rule  the joint distribution
                zL  can be factored as follows

                zL          

    cid   cid   zL 

 

where   cid  indicates    cid    zL  and         
            zL  Note that this factorization via chainrule is
fully general  In particular it accounts for recent models
that use shortcut connections  Kingma et al    Bachman    where each hidden layer   cid  directly depends
on all layers above it    cid  We shall refer to this fully
general formulation as autoregressive HVAE 
Several models assume   Markov independence structure
on the hidden variables 
leading to the following simpler factorization  Jimenez Rezende et al    Gulrajani
et al    Kaae   nderby et al   

                cid 

    cid   cid   zL 

 

  

 We omit the dependency on   and   for the remainder of the

paper 

  cid 

 cid 

  cid 

Learning Hierarchical Features from Deep Generative Models

We refer to this common but more restrictive formulation
as Markov HVAE 
For the inference distribution        we do not assume
any factorized structure to account for complex inference techniques used in recent work  Kaae   nderby
et al    Bachman    We also denote          
pdata         
Both        and        are jointly optimized  as before in
Equation   to maximize the ELBO objective

LELBO   Epdata   Eq     log       

Epdata   KL           

  cid 

 

Eq     log   zl                

 

  

where we de ne         zL        denotes the entropy
of   distribution  and the expectation over pdata    is estimated by the samples in the training dataset  This can be
interpreted as stacking VAEs on top of each other 

  Limitations of Hierarchical VAEs
  Representational Ef ciency

One of the main reasons deep hierarchical networks are
widely used as function approximators is their representational power  It is well known that certain functions can
be represented much more compactly with deep networks 
requiring exponentially less parameters compared to shallow networks  Bengio et al    However  we show
that under ideal optimization of LELBO  HVAE models do
not lead to improved representational power  This is because for   well trained HVAE    Gibbs chain on the bottom
layer  which is   single layer model  can be used to recover
pdata    exactly 
We  rst show this formally for Markov HVAE with the following proposition

Proposition   LELBO in Eq  is globally maximized
as   function of        and        when LELBO  
   pdata   
If LELBO is globally maximized for  
Markov HVAE  the following Gibbs sampling chain converges to pdata    if it is ergodic

            
    
               
   

 

  Epdata   log     

 cid 

 cid 

Proof of Proposition   We notice that
 cid 
LELBO   Epdata         

 cid 
  Epdata         
   Epdata   KL             

log

       
      

log
      
      

  KL pdata            pdata   

when         cid 

By nonnegativity of KLdivergence  and the fact that KL
divergence is zero if an only if the two distributions are
identical  it can be seen that this is uniquely optimized
         dz   pdata    and             
       and the optimum is

  
ELBO      pdata   

This also implies that   

        

      pdata   

    

        

 

Because the following Gibbs chain converges to pdata   
when it is ergodic

            
    
               
   
    with         

We can replace         
chain still converges to pdata   

    using   and the

Therefore under the assumptions of Proposition   we
can sample from pdata    without using the latent code
      zL  at all  Hence  optimization of the LELBO
objective and ef cient representation are con icting  in the
sense that optimality implies some level of redundancy in
the representation 
We demonstrate that this phenomenon occurs in practice 
even though the conditions of Proposition   might not be
met exactly       the objective is not globally optimized 
We train   factorized three layer VAE in Equation   on
MNIST by optimizing the ELBO criteria from Equation
  We use   model where each conditional distribution is  
factorized Gaussian     cid   cid       cid   cid   cid   cid 
where  cid  and  cid  are deep neural networks  We compare 
the samples generated by the Gibbs chain in Equation  
with samples generated by ancestral sampling with the entire model in Figure   We observe that the Gibbs chain
generates samples  left panel  with similar visual quality
as ancestral sampling with the entire model  right panel 
even though the Gibbs chain only used the bottom layer of
the model 
This problem can be generalized to autoregressive HVAEs 
One can sample from pdata    without using     cid   cid  for
     cid       at all  We prove this in the Appendix 

Learning Hierarchical Features from Deep Generative Models

  Feature learning

Another signi cant advantage of hierarchical models for
supervised learning is that they learn rich and disentangled
hierarchies of features  This has been demonstrated for example using various visualization techniques  Zeiler   Fergus    However  we show in this section that typical
HVAEs do not enjoy this property 
Recall that we think of        as    probabilistic  feature detector  and        as an approximation to       
It might therefore be natural to think that   might learn
hierarchical features similarly to   feedforward network
      cid        zL  where higher layers correspond
to higher level features that become increasingly abstract
and invariant to nuisance variations  However  if     cid   cid 
maps low level features to high level features  then the reverse mapping     cid   cid  maps high level features to likely
low level subfeatures  For example  if zL correspond to
object classes  then   zL zL  could represent the distribution over object subparts given the object class 
Suppose we train LELBO in Equation   to optimality  we
would have

       pdata                   

Recall that

          pdata         
                                 

Comparing the two we see that

                 

if the joint distributions are identical  then any conditional
distribution would also be identical  which implies that for
any   cid      cid   cid        cid   cid 
For the majority of models the conditional distributions
    cid   cid  belong to   very simple distribution family such
as parameterized Gaussians  Kingma   Welling   
 Jimenez Rezende et al     Kaae   nderby et al 
   Kingma et al    Therefore for   perfectly
optimized LELBO in the Gaussian case  the only type of
feature hierarchy we can hope to learn is one under which
    cid   cid  is also Gaussian  This limits the hierarchical representation we can learn  In fact  the hierarchies we observe
for feedforward models  Zeiler   Fergus    require
complex multimodal distributions to be captured  For example  the distribution over object subparts for an object
category is unlikely to be unimodal and cannot be well approximated with   Gaussian distribution 
More generally  as shown in  Zhao et al    even when
LELBO is not globally optimized  optimizing LELBO
encourages     cid   cid  and     cid   cid  to match 
Since

    cid   cid  typically belongs to some parameterized distribution family such as Gaussians  this encourages     cid   cid 
to belong to that distribution family as well 
We experimentally validate these intuitions in Figure  
where we train   three layer Markov HVAE with factorized
Gaussian conditionals     cid   cid  on MNIST and SVHN 
Details about the experimental setup are explained in the
Appendix  As suggested in  Kingma   Welling    we
reparameterize the stochasticity in     cid   cid  using   separate noise variable  cid           and implicitly rewrite
the original conditional distribution as

  cid     cid   cid     cid   cid   cid   cid 

where  cid  indicates elementwise product  We    the value
of    to   random sample from        at all layers    
     cid       cid          except for one  and observe the
variations in   generated by randomly sampling  cid  We
observe in Figure   that only very minor variations correspond to lower layers  Left and center panels  and almost all the variation is represented by the top layer  Right
panel  More importantly  no notable hierarchical relationship between features is observed 

  Variational Ladder Autoencoders
Given the limitations of hierarchical architectures described in the previous section  we focus on an alternative
approach to learn   hierarchy of disentangled features 
Our approach is to de ne   simple distribution with no
hierarchical structure over the latent variables       
       zL  For example  the joint distribution      can
be   white Gaussian  Instead we encourage the latent code
     zL to learn features with different levels of abstraction by carefully choosing the mappings        and       
between input   and latent code    Our approach is based
on the following intuition 
Assumption  If zi is more abstract than zj  then the inference mapping   zi    and generative mapping when other
layers  denoted as      are  xed     zi             requires   more expressive network to be captured 
This informal assumption suggests that we should use neural networks of different level of expressiveness to generate
the corresponding features  the more abstract features require more expressive networks  and vice versa  We loosely
quantify expressiveness with depth of the network  Based
on these assumptions we are able to design an architecture
that disentangles hierarchical features for many natural image datasets 

Learning Hierarchical Features from Deep Generative Models

Figure   Left  Samples obtained by running the Gibbs sampling chain in Proposition   using only the bottom layer of    layer recursive
hierarchical VAE  Right  samples generated by ancestral sampling from the same model  The quality of the samples is comparable 
indicating that the bottom layer contains enough information to reconstruct the data distribution 

Figure     hierarchical three layer VAE with Gaussian conditional distributions   zl zl  does not learn   meaningful feature hierarchy
on MNIST and SVHN when trained with the ELBO objective  Left panel  Samples generated by sampling noise   at the bottom layer 
while holding   and   constant  Center panel  Samples generated by sampling noise   at the middle layer  while holding   and  
constant  Right panel  Samples generated by sampling noise   at the top layer  while holding   and   constant  For both MNIST
and SVHN we observe that the top layer represents essentially all the variation in the data  right panel  leaving only very minor local
variations for the lower layers  left and center panels  Compare this with the rich hierarchy learned by our VLAE model  shown in
Figures   and  

  Model De nition

We decompose the latent code into subparts    
             where    is related to   via   shallow network  and increase the depth of the network depth up to
level    so that zL is related to   via   deep network  In
particular  we share parameters with   ladderlike architecture  Valpola    Pezeshki et al    Because of this
similarity we denote this architecture as Variational Ladder Autoencoder  VLAE  Formally  our model  shown in
Figure   is de ned as follows
  Generative Network                zL  is  
simple prior on all latent variables  We choose it as  
standard Gaussian        The conditional distribution

                  zL  is de ned implicitly as 

 zL   fL zL 
   cid      cid   cid    cid   cid             
             

 
 
 

where   cid  is parametrized as   neural network  and    cid  is an
auxiliary variable we use to simplify the notation    is  
distribution family parameterized by      In our experiments we use the following choice for   cid 

   cid      cid   cid    cid   cid 

 
where   denotes concatenation of two vectors  and
  cid    cid  are neural networks  We choose   as    xed variance factored Gaussian with mean given by          

Learning Hierarchical Features from Deep Generative Models

subparts of the latent code  Meanwhile  because it is essentially   singlelayer  at model  our VLAE does not exhibit
the problems we have identi ed with traditional hierarchical VAE described in Section  
Ladder Variational Autoencoders  LVAE  on the other
hand  utilize the ladder architecture from the inference encoding side 
its generative model is   standard
HVAE  While the ladder inference network performs better
than the one used in the original HVAE  ladder variational
autoencoders still suffer from the problems we discussed
in Section   The difference is between our model  VLAE 
and LVAE is illustrated in Figure  
An additional advantage over ladder variational autoencoders  and more generally HVAEs  is that our de nition
of the generative network Equ  allows us to select  
much richer family of generative models    Because for
HVAE the LELBO optimization requires the evaluation of
log     cid   cid  shown in Equ    reparameterized HVAE
has to inject noise into the network in   way that corresponds to   conditional distribution with   tractable loglikelihood  For example    HVAE can inject noise  cid  by

  cid     cid   cid     cid   cid   cid   cid 

 

only because this corresponds to Gaussian conditional distributions   zl zl  In comparison  for VLAE we only
require evaluation of log          zL  so except for the
bottom layer   we can combine noise using arbitrary black
box functions   cid 

  Experiments
We train VLAE over several datasets and visualize the semantic meaning of the latent code    According to our assumptions  complex  highlevel information will be learned
by latent codes at higher layers  whereas simple  lowlevel
features will be represented by lower layers 
In Figure   we visualize generation results from MNIST 
where the model is    layer VLAE with   dimensional
latent code     at each layer  The visualizations are generated by systematically exploring the    latent code for
one layer  while randomly sampling other layers  From
the visualization  we see that the three layers encode stroke
width  digit width and tilt and digit identity respectively 
Remarkably  the semantic meaning of   particular latent
code is stable with respect to the sampled latent codes from
other layers  For example  in the second layer  the left side
represents narrow digits whereas the right side represents
wide digits  Sampling latent codes at other layers will control the digit identity  but this will have no in uence on

 Code is available at https github com ermongroup Variational 

LadderAutoencoder

Figure   Inference and generative models for VLAE  left  and
LVAE  right  Circles indicate stochastic nodes  and squares are
deterministically computed nodes  Solid lines with arrows denote
conditional probabilities  solid lines without arrows denote deterministic mappings  dash lines indicates regularization to match
the prior      Note that in VLAE  we do not attempt to regularize the distance between   and    

  Inference Network  For the inference network  we
choose        as

  cid      cid   cid 
  cid       cid   cid   cid   cid 

 
 
where  cid             cid   cid   cid  are neural networks  and
       
  Learning  For learning we use the ELBO criterion as in
Equation  
       Eq     log          KL       cid       
where               denotes the prior for    This is
tractable if   has tractable log likelihood       when   is
  Gaussian 
This is essentially the inference and learning framework for
  onelayer VAE  the hierarchy is only implicitly de ned by
the network architecture  therefore we call this    at hierarchy model  Motivated by our earlier theoretical results 
we do not use additional layers of latent variables 

  Comparison with Ladder Variational

Autoencoders

Our architecture resembles the ladder variational autoencoder  LVAE     nderby et al    However the two
models are very different  The purpose of our architecture
is to connect subparts of the latent code with networks of
different expressive power  depth  the model is encouraged to place highlevel  complex features at the top  and
lowlevel  simple features at the bottom  in order to reach
lower reconstruction error with latent codes of the same
capacity  Empirically  this allows the network to learn disentangled factors of variation  corresponding to different

     xh       xz       xh   xz   Learning Hierarchical Features from Deep Generative Models

Figure   VLAE on MNIST  Generated digits obtained by systematically exploring the    latent code from one layer  and randomly
sampling from other layers  Left panel  The  rst  bottom  layer encodes stroke width  Center panel  the second layer encodes digit
width and tilt  Right panel  the third layer encodes  mostly  digit identity  Note that the samples are not of stateof theart quality only
because of the restricted  dimensional latent code used to enable visualization 

Figure   VLAE on SVHN  Each sub gure corresponds to images generated when  xing latent code on all layers except for one 
which we randomly sample from the prior distribution  From left to right the random sampled layer go from bottom layer to top layer 
Left panel  The bottom layer represents color schemes  Centerleft panel  the second layer represents shape variations of the same
digit  Centerright panel  the third layer represents digit identity  interestingly these digits have similar style although having different
identities  Right panel  the top layer represents the general structure of the image 

Figure   VLAE on CelebA  Each sub gure corresponds to images generated when  xing latent code on all layers except for one 
which we randomly sample from the prior distribution  From left to right the random sampled layer go from bottom layer to top layer 
Left panel  The bottom layer represents ambient color  Centerleft panel  the second bottom layer represents skin and hair color 
Centerright panel  the second top layer represents face identity  Right panel  the top layer presents pose and general structure 

Learning Hierarchical Features from Deep Generative Models

the width  This is interesting given that width is actually
correlated with the digit identity  for example  digit   is
typically thin while digit   is mostly wide  Therefore  the
model will generate more zeros than ones if the latent code
at the second layer corresponds to   wide digit  as shown in
the visualization 
Next we evaluate VLAE on the Street View House Number  SVHN  Netzer et al    dataset  where it is signi cantly more challenging to learn interpretable representations since it is relatively noisy  containing certain digits
which do not appear in the center  However  as is shown in
Figure   our model is able to learn highly disentangled features through    layer ladder  which includes color  digit
shape  digit context  and general structure  These features
are highly disentangled  since the latent code at the bottom
layer controls color  modifying the code from other three
layers while keeping the bottom layer  xed will generate
  set of images with the same general tone  Moreover  the
latent code learned at the top layer is the most complex
one  which captures rich variations lower layers cannot accurately represent 
Finally  we display compelling results from another challenging dataset  CelebA  Liu et al    which includes
  celebrity images  These images are highly varied
in terms of environment and facial expressions  We visualize the generation results in Figure   As in the SVHN
model  the latent code at the bottom layer learns the ambient color of the environment while keeping the personal
details intact  Controlling other latent codes will change
the other details of the individual  such as skin color  hair
color  identity  pose  azimuth  more complicated features
are placed at higher levels of the hierarchy 

  Discussions
Training hierarchical deep generative models is   very challenging task  and there are two main successful families
of methods  One family de nes the  destruction  and reconstruction of data using   prede ned process  Among
them  LapGANs  Denton et al    de ne the process
as repeatedly downsampling  and Diffusion Nets  SohlDickstein et al    de nes   forward Markov chain that
converts   complex data distribution to   simple  tractable
one  Without having to perform inference  this makes training much easier  but it does not provide latent variables for
other downstream tasks  unsupervised learning 
Another line of work focuses on learning   hierarchy of
latent variables by stacking single layer models on top of
each other  Many models also use more  exible inference
techniques to improve performance    nderby et al   
Dinh et al    Salimans et al    Rezende   Mohamed    Li et al    Kingma et al    How 

ever we show that there are limitations to stacked VAEs 
Our work distinguishes itself from prior work by explicitly
discussing the purpose of learning such models  the advantage of learning   hierarchy is not in better representation
ef ciency  or better samples  but rather in the introduction
of structure in the features  such as hierarchy or disentanglement  This motivates our method  VLAE  which justi es our intuition that   reasonable network structure can
be  by itself  highly effective at learning structured  disentangled  representations  Contrary to previous efforts on
hierarchical models  we do not stack VAEs on top of each
other  instead we use    at  approach 
Our experimental results resemble those obtained with InfoGAN  Chen et al    both frameworks learn disentangled representations from the data in an unsupervised
manner  The InfoGAN objective  however  explicitly maximizes the mutual information between the latent variables
and the observation  whereas in VLAE  this is achieved
through the reconstruction error objective which encourages the use of the latent code  Furthermore we are able
to explicitly disentangle features with different level of abstractness 

  Conclusions
In this paper  we discussed the potential practical value
of learning   hierarchical generative model over   nonhierarchical one  We show that under some assumptions
little can be gained in terms of representation ef ciency or
sample quality  We further show that traditional HVAE
models have trouble learning structured features  Based
on these insights  we consider an alternative to learning
structured features by leveraging the expressive power of
  neural network  Empirical results show that we can learn
highly disentangled features 
One limitation of VLAE is the inability to learn structures other than hierarchical disentanglement  Future work
should consider more principled ways of designing architectures that allow for learning features with more complex
structures 

  Acknowledgement
This research was supported by Intel Corporation  NSF
      and Future of Life Institute  

References
Bachman  Philip  An architecture for deep  hierarchical generative models  In Advances In Neural Information Processing
Systems  pp     

Bengio  Yoshua  Learning deep architectures for ai  Found 

Learning Hierarchical Features from Deep Generative Models

Trends Mach  Learn    January   ISSN  
  doi    URL http dx doi 
org 

Radford  Alec  Metz  Luke  and Chintala  Soumith  Unsupervised
representation learning with deep convolutional generative adversarial networks  arXiv preprint arXiv   

Bengio  Yoshua et al  Learning deep architectures for ai  Founda 

tions and trends   cid  in Machine Learning     

Rezende        and Mohamed     Variational inference with nor 

malizing  ows  arXiv preprint arXiv   

Salimans     Kingma        Welling     et al  Markov chain
monte carlo and variational inference  Bridging the gap  In International Conference on Machine Learning  pp   
 

SohlDickstein     Weiss        Maheswaranathan     and Ganguli     Deep unsupervised learning using nonequilibrium thermodynamics  arXiv preprint arXiv   

  nderby        Raiko     Maal         nderby        and
Winther     Ladder variational autoencoders  In Advances In
Neural Information Processing Systems  pp     

Theis  Lucas  Oord    aron van den  and Bethge  Matthias   
note on the evaluation of generative models  arXiv preprint
arXiv   

Valpola  Harri  From neural pca to deep unsupervised learning 
Adv  in Independent Component Analysis and Learning Machines  pp     

van den Oord  Aaron  Kalchbrenner  Nal  Espeholt  Lasse 
Vinyals  Oriol  Graves  Alex  et al  Conditional image generation with pixelcnn decoders  In Advances in Neural Information Processing Systems  pp     

Zeiler  Matthew   and Fergus  Rob  Visualizing and understanding convolutional networks  In Computer vision ECCV  
pp    Springer   

Zhao     Song     and Ermon     Towards Deeper Understanding
of Variational Autoencoding Models  ArXiv eprints  February
 

Chen     Duan     Houthooft     Schulman     Sutskever    
and Abbeel     Infogan  Interpretable representation learning
by information maximizing generative adversarial nets  In Advances in Neural Information Processing Systems  pp   
   

Denton        Chintala     Fergus     et al  Deep generative
image models using   laplacian pyramid of adversarial networks  In Advances in neural information processing systems 
pp     

Dinh     Krueger     and Bengio     Nice  Nonlinear independent components estimation  arXiv preprint arXiv 
 

Donahue  Jeff  Kr ahenb uhl  Philipp  and Darrell  Trevor  Adversarial feature learning  arXiv preprint arXiv   

Dumoulin  Vincent  Belghazi 

Ishmael  Poole  Ben  Lamb 
Alex  Arjovsky  Martin  Mastropietro  Olivier  and Courville 
arXiv preprint
Aaron  Adversarially learned inference 
arXiv   

Gulrajani 

Ishaan  Kumar  Kundan  Ahmed  Faruk  Taiga 
Adrien Ali  Visin  Francesco    azquez  David  and Courville 
Aaron    Pixelvae    latent variable model for natural images 
CoRR  abs    URL http arxiv org 
abs 

Jimenez Rezende     Mohamed     and Wierstra     Stochastic
Backpropagation and Approximate Inference in Deep Generative Models  ArXiv eprints  January  

Kaae   nderby     Raiko     Maal       Kaae   nderby     and
Winther     Ladder Variational Autoencoders  ArXiv eprints 
February  

Kingma       and Welling     AutoEncoding Variational Bayes 

ArXiv eprints  December  

Kingma  Diederik    Salimans  Tim  and Welling  Max 

Improving variational inference with inverse autoregressive  ow 
arXiv preprint arXiv   

Li     Zhu     and Zhang     Learning to generate with memory 

arXiv preprint arXiv   

Liu  Ziwei  Luo  Ping  Wang  Xiaogang  and Tang  Xiaoou  Deep
learning face attributes in the wild  In Proceedings of International Conference on Computer Vision  ICCV   

Netzer  Yuval  Wang  Tao  Coates  Adam  Bissacco  Alessandro 
Wu  Bo  and Ng  Andrew    Reading digits in natural images
with unsupervised feature learning  In NIPS workshop on deep
learning and unsupervised feature learning  volume   pp 
   

Pezeshki  Mohammad  Fan  Linxi  Brakel  Philemon  Courville 
Aaron  and Bengio  Yoshua  Deconstructing the ladder network architecture  arXiv preprint arXiv   

