Learning to Discover CrossDomain Relations

with Generative Adversarial Networks

Taeksoo Kim   Moonsu Cha   Hyunsoo Kim   Jung Kwon Lee   Jiwon Kim  

Abstract

While humans easily recognize relations between data from different domains without any
supervision  learning to automatically discover
them is in general very challenging and needs
many groundtruth pairs that illustrate the relations  To avoid costly pairing  we address the
task of discovering crossdomain relations when
given unpaired data  We propose   method based
on generative adversarial networks that learns
to discover relations between different domains
 DiscoGAN  Using the discovered relations  our
proposed network successfully transfers style
from one domain to another while preserving key
attributes such as orientation and face identity 

  Introduction
Relations between two different domains  the way in which
concepts  objects  or people are connected  arise ubiquitously  Crossdomain relations are often natural to humans 
For example  we recognize the relationship between an English sentence and its translated sentence in French  We
also choose   suit jacket with pants or shoes in the same
style to wear 
Can machines also achieve   similar ability to relate two
different image domains  This question can be reformulated as   conditional image generation problem  In other
words   nding   mapping function from one domain to the
other can be thought as generating an image in one domain given another image in the other domain  While this
problem tackled by generative adversarial networks  GAN 
 Isola et al    has gained   huge attention recently 
most of today   training approaches use explicitly paired
data  provided by human or another algorithm 
This problem also brings an interesting challenge from  

 SK TBrain  Seoul  South Korea  Correspondence to  Taeksoo

Kim  jazzsaxma   sktbrain com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

    Learning crossdomain relations without any extra label

    Handbag images  input    Generated shoe images  output 

    Shoe images  input    Generated handbag images  output 

Figure   Our GANbased model trains with two independently
collected sets of images and learns how to map two domains without any extra label  In this paper  we reduce this problem into generating   new image of one domain given an image from the other
domain      shows   highlevel overview of the training procedure
of our model with two independent sets       handbag images and
shoe images      and     show results of our method  Our method
takes   handbag  or shoe  image as an input  and generates its
corresponding shoe  or handbag  image  Again  it is worth noting
that our method does not take any extra annotated supervision and
can selfdiscover relations between domains 

learning point of view  Explicitly supervised data is seldom available and labeling can be labor intensive  Moreover  pairing images can become tricky if corresponding
images are missing in one domain or there are multiple best
candidates  Hence  we push one step further by discovering
relations between two visual domains without any explic 

Disco GANINPUTOUTPUTINPUTOUTPUTLearning to Discover CrossDomain Relations with Generative Adversarial Networks

Figure   Three investigated models      standard GAN  Goodfellow et al        GAN with   reconstruction loss      our proposed
model  DiscoGAN  designed to discover relations between two unpaired  unlabeled datasets  Details are described in Section  

itly paired data 
In order to tackle this challenge  we introduce   model that
discovers crossdomain relations with GANs  DiscoGAN 
Unlike previous methods  our model can be trained with
two sets of images without any explicit pair labels  Figure
    and does not require any pretraining  Our proposed
model can then take an image in one domain and generate its corresponding image in another domain  see Figure     The core of our model is based on two different
GANs coupled together   each of them ensures our generative functions can map each domain to its counterpart
domain    key intuition we rely on is to constrain all images in one domain to be representable by images in the
other domain  For example  when learning to generate shoe
image from handbag image  we force this generated image
to be an imagebased representation of the handbag image
 and hence reconstruct the handbag image  through   reconstruction loss  and to be as close to images in the shoe
domain as possible through   GAN loss  We use these two
properties to encourage the mapping between two domains
to be well covered on both directions       encouraging oneto one rather than manyto one or oneto many  In the experiments section  we show that this simple intuition discovers common properties and styles of two domains very
well 
Both experiments on toy domain and real world image
datasets support the claim that our proposed model is wellsuited for discovering crossdomain relations  When translating data points between simple  dimensional domains
and between face image domains  our DiscoGAN model
was more robust to the mode collapse problem compared

to two other baseline models of Figures    and     It also
learns the bidirectional mapping between two image domains  such as faces  cars  chairs  edges and photos  and
successfully apply the mapping in image translation  Translated images consistently change speci   attributes such
as hair color  gender and orientation while maintaining all
other components 

  Model
We now formally de ne crossdomain relations and present
the problem of learning to discover such relations in two
different domains  Standard GAN model and   similar variant model with additional components are investigated for
their applicability for this task  Limitations of these models are then explained  and we propose   new architecture
based on GANs that can be used to discover crossdomain
relations 

  Formulation
Relation is mathematically de ned as   function GAB that
maps elements from domain   to elements in its codomain
  and vice versa for GBA  In fully unsupervised setting 
GAB and GBA can be arbitrarily de ned  To  nd   meaningful relation  we need to impose   condition on the relation of interest  Here  we constrain relation to be   oneto 
one correspondence  bijective mapping  That means GAB
is the inverse mapping of GBA 
The range of function GAB   the complete set of all possible resulting values GAB xA  for all xA   in domain
    should be contained in domain   and similarly for

       Learning to Discover CrossDomain Relations with Generative Adversarial Networks

Figure   Illustration of our models on simpli ed one dimensional domains      ideal mapping from domain   to domain   in which the
two domain   modes map to two different domain   modes      GAN model failure case      GAN with reconstruction model failure
case 

GBA xB 
We now relate these constraints to objective functions  Ideally  the equality GBA   GAB xA    xA should be satis ed  but this hard constraint is dif cult to optimize and
  relaxed soft constraint is more desirable  For this reason  we minimize the distance   GBA   GAB xA  xA 
where any form of metric function such as       or Huber loss can be used  Similarly  we also need to minimize
  GAB   GBA xB  xB 
Guaranteeing that GAB maps to domain   is also very
dif cult
to optimize  We relax this constraint as follows  we instead minimize generative adversarial
loss
 ExA PA
 log DB GAB xA  Similarly  we minimize
 ExB  PB
 log DA GBA xB  
Now  we explore several GAN architectures to learn with
these loss functions 

  Notation and Architecture

 

 

 

We use the following notations in sections below    gen 
  Rh    
erator network is denoted as GAB   Rh    
 
subscripts denote the input and output domains  and superscripts denote the input and output image size  The discrim 
      and
inator network is denoted as DB   Rh    
the subscript   denotes that it discriminates images in domain    Notations GBA and DA are used similarly 
Each generator takes an image of size       and feeds it
through an encoderdecoder pair  The encoder part of each
generator is composed of convolution layers  each followed
by leaky ReLU  Maas et al    Xu et al    The
decoder part is composed of deconvolution layers  followed
by   ReLU  and outputs   target domain image of size    
     The number of convolution and deconvolution layers
ranges from four to  ve depending on the domain 
The discriminator is similar to the encoder part of the generator  In addition to the convolution layers and leaky Re 

LUs  the discriminator has an additional convolution layer 
and    nal sigmoid to output   scalar output between    

  GAN with   Reconstruction Loss

We  rst consider   standard GAN model  Goodfellow
et al    for the relation discovery task  see Figure    
Originally    standard GAN takes random Gaussian noise   
encodes it into hidden features   and generates images in
domains such as MNIST  We make   slight modi cation to
this model to    our task  the model we use takes in image
as input instead of noise 
In addition  since this architecture only learns one mapping
from domain   to domain    we add   second generator
that maps domain   back into domain    see Figure    
We also add   reconstruction loss term that compares the
input image with the reconstructed image  With these additional changes  each generator in the model can learn mapping from its input domain to output domain and discover
relations between them 
  generator GAB translates input image xA from domain  
into xAB in domain    The generated image is then translated into   domain   image xABA to match the original input image  Equation     Various forms of distance functions  such as MSE  cosine distance  and hingeloss  can be
used as the reconstruction loss    Equation   The translated output xAB is then scored by the discriminator which
compares it to   real domain   sample xB  

xAB   GAB xA 
xABA   GBA xAB     GBA   GAB xA  

    GBA   GAB xA   xA  
   ExA PA

 log DB GAB xA 

LCON STA
LGANB

 
 
 
 

The generator GAB receives two types of losses     reconstruction loss LCON STA
 Equation   that measures how
well the original input is reconstructed after   sequence of

Domain ADomain BGABGABGABGBALCONSTAGABGBALCONSTA     GABGAB   Learning to Discover CrossDomain Relations with Generative Adversarial Networks

two generations  and   standard GAN generator loss LGANB
 Equation   that measures how realistic the generated image is in domain    The discriminator receives the standard
GAN discriminator loss of Equation  

LGAB

  LGANB

  LCON STA

LDB

    ExB  PB
  ExA PA

 log DB xB  
 log    DB GAB xA  

 

 

During training  the generator GAB learns the mapping
from domain   to domain   under two relaxed constraints 
that domain   maps to domain    and that the mapping
on domain   is reconstructed to domain    However  this
model lacks   constraint on mapping from   to    and these
two conditions alone does not guarantee   crossdomain relation  as de ned in section   because the mapping satisfying these constraints is onedirectional  In other words 
the mapping is an injection  not bijection  and oneto one
correspondence is not guaranteed 
Consider two possibly multimodal image domains   and
   Figure   illustrates the two multimodal data domains
on   simpli ed onedimensional representation  Figure   
shows the ideal mapping from input domain   to domain
   where each mode of data is mapped to   separate mode
in the target domain  Figure     in contrast  shows the
mode collapse problem    prevalent phenomenon in GANs 
where data from multiple modes of   domain map to   single mode of   different domain  For instance  this case is
where the mapping GAB maps images of cars in two different orientations into the same mode of face images 
In some sense  the addition of   reconstruction loss to  
standard GAN is an attempt to remedy the mode collapse
problem  In Figure     two domain   modes are matched
with the same domain   mode  but the domain   mode can
only direct to one of the two domain   modes  Although
the additional reconstruction loss LCON STA
forces the reconstructed sample to match the original  Figure     this
change only leads to   similar symmetric problem  The reconstruction loss leads to an oscillation between the two
states and does not resolve modecollapsing 

  Our Proposed Model  Discovery GAN

Our proposed GAN model for relation discovery   DiscoGAN   couples the previously proposed model  Figure    
Each of the two coupled models learns the mapping from
one domain to another  and also the reverse mapping for
reconstruction  The two models are trained together simultaneously  The two generators GAB   and the two generators GBA   share parameters  and the generated images
xBA and xAB are each fed into separate discriminators LDA
and LDB   respectively 

One key difference from the previous model is that input
images from both domains are reconstructed and that there
are two reconstruction losses  LCON STA and LCON STB  
 

  LGANA

  LCON STB

LG   LGAB
  LGANB

  LGBA
  LCON STA

LD   LDA

  LDB

 

As   result of coupling two models  the total generator loss
is the sum of GAN loss and reconstruction loss for each
partial model  Equation   Similarly  the total discriminator loss LD is   sum of discriminator loss for the two discriminators DA and DB  which discriminate real and fake
images of domain   and domain    Equation  
Now  this model is constrained by two LGAN losses and
two LCON ST losses  Therefore   bijective mapping is
achieved  and   oneto one correspondence  which we de 
 ned as crossdomain relation  can be discovered 

  Experiments
  Toy Experiment

To empirically demonstrate our explanations on the differences between   standard GAN    GAN with reconstruction loss and our proposed model  DiscoGAN  we designed an illustrative experiment based on synthetic data in
 dimensional domains   and    Both source  domain   
and target  domain    data samples are drawn from Gaussian mixture models 
In Figure   the leftmost  gure shows the initial state of
the toy experiment where all the domain   modes map to
almost   single point because of initialization of the generator  For all plots the target domain    plane is shown
with target domain modes marked with black      Colored
points represent samples from domain   that are mapped
to domain    and each color denotes samples from each
domain   mode  In this case  the task is to discover crossdomain relations between domains   and    and translate
samples from  ve domain   modes into domain    which
has ten modes spread around the arc of   circle 
We use   neural network with three linear layers each followed by   ReLU nonlinearity as the generator  For the
discriminator we use  ve linear layers each followed by  
ReLU  except for the last layer which is switched out with
  sigmoid that outputs   scalar       The colored background shows the output value of the discriminator DB 
which discriminates real target domain samples from synthetic translated samples from domain    The contour lines
show regions of same discriminator value 
  iterations of training were performed  and due to the

Learning to Discover CrossDomain Relations with Generative Adversarial Networks

   

   

   

   

Figure   Toy domain experiment results  The colored background shows the output value of the discriminator      marks denote different
modes in domain    and colored circles indicate samples of domain   mapped to domain    where each color corresponds to   different
mode in domain        ten target  domain    modes and initial translations      results from standard GAN model      GAN with
reconstruction loss      our proposed DiscoGAN model 

domain simplicity our model often converged much earlier 
Illustrations of translated samples in Figure   show very
different behavior depending on the model used 
In the baseline  standard GAN  case  many translated
points of different colors are located around the same domain   mode  For example  navy and lightblue colored
points are located together  as well as green and orange colored points  This result illustrates the modecollapse problem of GANs since points of multiple colors  multiple  
domain modes  are mapped to the same domain   mode 
The baseline model still oscillate around domain   modes
throughout the iterations 
In the case of GAN with   reconstruction loss  the collapsing problem is less prevalent  but navy  green and lightblue
points still overlap at   few modes  The contour plot also
demonstrates the difference from baseline  regions around
all domain   modes are leveled in   green colored plateau
in the baseline  allowing translated samples to freely move
between modes  whereas in the case with reconstruction
loss the regions between domain   modes are clearly separated 
In addition  both this model and the standard GAN model
fail to cover all modes in domain   since the mapping from
domain   to domain   is injective  Our proposed DiscoGAN model  on the other hand  is able to not only prevent
modecollapse by translating into distinct wellbounded regions that do not overlap  but also generate domain   samples in all ten modes as the mappings in our model is bijective  It is noticeable that the discriminator for domain  
is perfectly fooled by translated samples from domain  
 white regions near   domain modes 
Although this experiment is limited due to its simplicity 
the results clearly support the superiority of our proposed
model over other variants of GANs 

  Real Domain Experiment

To evaluate whether our DiscoGAN successfully learns underlying relationships between domains  we trained and
tested our model using several imageto image translation
tasks that require the use of discovered crossdomain relations between source and target domains 
In each real domain experiment  all input images and translated images were size of           For training  we
employed learning rate of   and used the Adam optimizer  Kingma   Ba    with       and    
  We applied Batch Normalization  Ioffe   Szegedy 
  to all convolution and deconvolution layers except
the  rst and the last layers  and applied weight decay regularization coef cient of   and minibatch of size  
All computations were conducted on   single machine with
an Nvidia Titan   Pascal GPU and an Intel    Xeon   
   CPU 

  CAR TO CAR  FACE TO FACE

We used   Car dataset  Fidler et al    which consists
of rendered images of    car models with varying azimuth
angles at   intervals  We split the dataset into train set
and test set  and again split the train set into two groups 
each of which is used as   domain and   domain samples  In addition to training   standard GAN model    GAN
with   reconstruction model and   proposed DiscoGAN
model  we also trained   regressor that predicts the azimuth
angle of   car image using the train set  To evaluate  we
translated images in the test set using each of the three
trained models  and azimuth angles were predicted using
the regressor for both input and translated images  Figure
  shows the predicted azimuth angles of input and translated images for each model  In standard GAN and GAN
with reconstruction    and     most of the red dots are
grouped in   few clusters  indicating that most of the input images are translated into images with same azimuth

Learning to Discover CrossDomain Relations with Generative Adversarial Networks

Figure   Car to Car translation experiment  Horizontal and vertical axes in the plots indicate predicted azimuth angles of input and
translated images  where the angle of input image ranges from   to   RMSE with respect to ground truth  blue lines  are shown
in each plot  Images in the second row are examples of input car images ranging from   to   at   intervals  Images in the third
row are corresponding translated images      plot of standard GAN     GAN with reconstruction     DiscoGAN  The angles of input and
output images are highly correlated when our proposed DiscoGAN model is used  Note the angles of input and translated car images are
reversed with respect to         mirror images  for     and    

angles  and that these models suffer from mode collapsing
problem  Our proposed DiscoGAN     on the other hand 
shows strong correlation between predicted angles of input
and translated images  indicating that our model successfully discovers azimuth relation between the two domains 
In this experiment  the input and translated images either
have positive correlation where they have the same range
of azimuth angles     or negative correlation where they
have opposite range of angles    and    
Next  we use   Face dataset  Paysan et al    shown in
Figure     in which the face images vary in azimuth rotation from   to   Similar to previous Car to Car experiment  input images in the   to   rotation range
generated output images either in the same range  from  
  to   or the opposite range  from   to   when
our proposed model was used     We also trained   standard GAN and   GAN with reconstruction loss for comparison  When   standard GAN and GAN with reconstruction
loss were used  the generated images do not vary as much
as the input images in terms of rotation  In this sense  similar to what has been shown in previous Car to Car experiment  the two models suffered from mode collapse 

  FACE CONVERSION

wellpreserved while   single desired attribute  gender  is
changed  Similarly  in Figures    and    hair color conversion and conversion between with and without eyeglasses
are successfully performed 

Figure   Face to Face translation experiment      input face images from   to       results from   standard GAN     results
from GAN with   reconstruction loss     results from our DiscoGAN  Here our model generated images in the opposite range 
from   to  

We also applied the face attribute conversion task on
CelebA and Facescrub dataset  Liu et al    Ng   Winkler    where   speci   feature  such as gender or hair
color  varies between two domains and other facial features
are preserved  The results are listed in Figure  
In Figure     we can see that various facial features are

  CHAIR TO CAR  CAR TO FACE

We also investigated the case where the discrepancy between two domains is large  and there is   single shared
feature between two domains     rendered images of chair
 Aubry et al    and the previously used car and face
datasets  Fidler et al    Paysan et al    were used

               Learning to Discover CrossDomain Relations with Generative Adversarial Networks

Figure       Translation of gender in Facescrub dataset      Blond to black and black to blond hair color conversion in CelebA dataset 
    Eyeglasses to no eyeglasses  no eyeglasses to eyeglasses conversion in CelebA dataset 

Figure   Discovering relations of images from visually very different object classes      chair to car translation  DiscoGAN is trained
on chair and car images     car to face translation  DiscoGAN is trained on car and face images  Our model successfully pairs images
with similar orientation 

in this task  All three datasets vary along the azimuth rotation  Figure   shows the results of imageto image translation from chair to car and from car to face datasets  The
translated images clearly match the rotation features of the
input images while preserving visual features of car and
face domain respectively 

  EDGES TO PHOTOS

Edgesto photos is an interesting task as it is    toN problem  where   single edge image of items such as shoes
and handbags can generate multiple colored images of such
items  In fact  an edge image can be colored in in nitely
many ways  We validated that our DiscoGAN performs
very well on this type of imageto image translation task
and generate realistic photos of handbags  Zhu et al   
and shoes  Yu   Grauman    The generated images

    Chair to Car    Car to FaceInputOutputInputOutputLearning to Discover CrossDomain Relations with Generative Adversarial Networks

years  network models such as LAPGAN  Denton et al 
  and DCGAN  Radford et al    and improved
training techniques  Salimans et al    Arjovsky et al 
  has been explored  More recent GAN works are described in  Goodfellow   
Several methods were developed to generate images
based on GANs  Conditional Generative Adversarial Nets
 cGANs   Mirza   Osindero    used MNIST digit
class label as an additional information to both generator and discriminator and can generate digit images of the
speci ed class  Similarly  Dosovitskiy et al    showed
that GAN can generate images of objects based on speci 
 ed characteristic codes such as color and viewpoint  Other
approaches instead used conditional features from   completely different domain for image generation  For example  Reed et al    used encoded text description of images as the conditional information to generating images
that match the description 
In addition to using conditional information such as class
labels and text encodings  several works in the  eld of
imageto image translation used images of one domain to
generate images in another domain   Isola et al   
translated blackand white images to colored images by
training on paired blackand white and colored image data 
Similarly  Taigman et al    translated face images to
emojis by providing image features from pretrained face
recognition module as conditional input to   GAN 
Several recent papers  Tong et al    Chen et al   
Makhzani    used similar architectures which reconstruct the original input by successively applying   mapping and its inverse mapping  and encourage the reconstruction to match the original input 
The most similar work to ours  Mathieu et al    proposed an architecture that combined variational autoencoder  Kingma et al    with generative adversarial network for factor decomposition and domain transfer 

  Conclusion
This paper presents   learning method to discover crossdomain relations with   generative adversarial network
called DiscoGAN  Our approach works without any explicit pair labels and learns the relations between datasets
from very different domains  One possible future directions
is to extend DiscoGAN to handle mixed modalities such as
text and image 

References
Arjovsky     Chintala     and Bottou     Wasserstein

GAN  In arXiv preprint arXiv   

Figure   Edges to photos experiment  Our model is trained on  
set of object sketches and colored images and learns to generate
corresponding photos or sketches      colored handbag images are
generated from handbag sketches      colored shoe images are
generated from shoe sketches      handbag sketches are generated
from colored handbag images 

are presented in Figure  

  HANDBAG TO SHOES  SHOES TO HANDBAG

Finally  we investigated the case with two domains that are
visually very different  where shared features are not explicit even to humans  We trained   DiscoGAN using previously used handbags and shoes datasets  not assuming
any speci   relation between those two  In the translation
results shown in Figure   our proposed model discovers
fashion style as   related feature between the two domains 
Note that translated results not only have similar colors and
patterns  but also have similar level of fashion formality as
the input fashion item 

  Related Work
Recently    novel method to train generative models named
Generative Adversarial Networks  GANs   Goodfellow
et al    has been developed    GAN is composed of
two modules     generator   and   discriminator    The
generator   objective is to generate  synthesize  data samples whose distribution closely matches that of real data
samples while the discriminator   objective is to distinguish between real and generated samples  The two models
  and    formulated as   twoplayer minimax game  are
trained simultaneously 
Researchers have studied GANs vigorously in the past few

         INPUTOUTPUTINPUTOUTPUTINPUTOUTPUTLearning to Discover CrossDomain Relations with Generative Adversarial Networks

Aubry     Maturana     Efros        Russell     and
Sivic     Seeing    chairs  Exemplar partbased     
alignment using   large dataset of cad models  In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  CVPR   

Chen  Xi  Chen  Xi  Duan  Yan  Houthooft  Rein  Schulman  John  Sutskever  Ilya  and Abbeel  Pieter 
Infogan  Interpretable representation learning by information
maximizing generative adversarial nets   

Denton        Chintala     Szlam     and Fergus     Deep
generative image models using   laplacian pyramid of
adversarial networks  In Advances in Neural Information
Processing Systems  NIPS   

Dosovitskiy     Springenberg        and Brox     Learning
to generate chairs with convolutional neural networks 
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  CVPR   

Fidler     Dickinson     and Urtasun        object detection and viewpoint estimation with   deformable   
cuboid model  In Advances in Neural Information Processing Systems  NIPS   

Goodfellow     PougetAbadie     Mirza     Xu    
WardeFarley     Ozair     Courville     and Bengio 
   Generative adversarial nets  In Advances in Neural
Information Processing Systems  NIPS   

Goodfellow  Ian    NIPS   tutorial  Generative adverIn arXiv preprint arXiv 

sarial networks 
 

Ioffe     and Szegedy     Batch normalization  Accerlerating deep network training by reducing internal covariate
shift  In arXiv preprint arXiv   

Isola     Zhu     Zhou     and Efros       

Imageto 
image translation with conditional adversarial networks 
In arXiv preprint arXiv   

Kingma        and Ba     Adam    method for stochastic
In Proceedings of the  rd International
optimization 
Conference on Learning Representations  ICLR   

Kingma          and Welling     Autoencoding variational
bayes  In Proceedings of the  rd International Conference on Learning Representations  ICLR   

Liu     Luo     Wang     and Tang     Deep learning
face attributes in the wild  In Proceedings of the IEEE
International Conference on Computer Vision  ICCV 
 

Maas        Hannun        and Ng        Recti er nonlinearities improve neural network acoustic models  In Proceedings of the  th International Conference on Machine Learning  ICML   

Makhzani     Shlens    Jaitly    Goodfellow    Frey   

Adversarial autoencoders   

Mathieu  Michael    Zhao  Junbo Jake  Zhao  Junbo 
Ramesh  Aditya  Sprechmann  Pablo  and LeCun  Yann 
Disentangling factors of variation in deep representation
using adversarial training   

Mirza     and Osindero     Conditional generative adver 

sarial nets  In arXiv preprint arXiv   

Ng        and Winkler       datadriven approach to cleaning large face datasets  In Proceedings of the IEEE International Conference on Image Processing  ICIP   

Paysan     Knothe     Amberg     Romdhani     and Vetter          face model for pose and illumination invariIn Proceedings of the  th IEEE
ant face recognition 
International Conference on Advanced Video and Signal
based Surveillance  AVSS  for Security  Safety and Monitoring in Smart Environments   

Radford     Metz     and Chintala     Unsupervised representation learning with deep convolutional generative
adversarial networks  In Proceedings of the  th International Conference on Learning Representations  ICLR 
 

Reed     Akata     Yan     Logeswaran     Schiele    
and Lee     Generative adversarial text to image synthesis  In Proceedings of the  rd International Conference
on Machine Learning  ICML   

Salimans     Goodfellow     Zaremba     Cheung    
Radford     and Chen    
Improved techniques for
training gans  In Advances in Neural Information Processing Systems  NIPS   

Taigman     Polyak     and Wolf     Unsupervised
In arXiv preprint

crossdomain image generation 
arXiv   

Tong     Li     Jacob        Bengio     and Li     Mode
regularized generative adversarial networks  In Proceedings of the  rd International Conference on Learning
Representations  ICLR   

Xu     Wang        Chen  and Li     Empirical evaluation of recti ed activations in convolutional network  In
arXiv preprint arXiv   

Yu     and Grauman     Finegrained visual comparisons
In Computer Vision and Pattern

with local learning 
Recognition  CVPR  June  

Zhu  JunYan  Kr ahenb uhl  Philipp  Shechtman  Eli  and
Efros  Alexei    Generative visual manipulation on the
In Proceedings of European
natural image manifold 
Conference on Computer Vision  ECCV   

