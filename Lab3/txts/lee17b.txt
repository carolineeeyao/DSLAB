Con dent Multiple Choice Learning

Kimin Lee   Changho Hwang   KyoungSoo Park   Jinwoo Shin  

Abstract

Ensemble methods are arguably the most trustworthy techniques for boosting the performance
of machine learning models  Popular independent ensembles  IE  relying on na ve averaging voting scheme have been of typical choice
for most applications involving deep neural networks  but they do not consider advanced collaboration among ensemble models  In this paper 
we propose new ensemble methods specialized
for deep neural networks  called con dent multiple choice learning  CMCL  it is   variant of
multiple choice learning  MCL  via addressing
its overcon dence issue  In particular  the proposed major components of CMCL beyond the
original MCL scheme are     new loss       con 
 dent oracle loss   ii  new architecture       feature sharing and  iii  new training method      
stochastic labeling  We demonstrate the effect of
CMCL via experiments on the image classi cation on CIFAR and SVHN  and the foregroundbackground segmentation on the iCoseg  In particular  CMCL using   residual networks provides   and   relative reductions in
the top  error rates from the corresponding IE
scheme for the classi cation task on CIFAR and
SVHN  respectively 

  Introduction
Ensemble methods have played   critical role in the
machine learning community to obtain better predictive
performance than what could be obtained from any of
the constituent
learning models alone       Bayesian
model parameter averaging  Domingos    boosting
 Freund et al    and bagging  Breiman    Recently  they have been successfully applied to enhancing
the power of many deep neural networks         of

 School of Electrical Engineering  Korea Advanced Institute
of Science and Technology  KAIST  Daejeon  Repulic of Korea 
Correspondence to  Jinwoo Shin  jinwoos kaist ac kr 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

top  bestperforming teams on ILSVRC challenge  
 Krizhevsky et al    employ ensemble methods  They
are easy and trustworthy to apply for most scenarios 
While there exists   long history on ensemble methods 
the progress on developing more advanced ensembles specialized for deep neural networks has been slow  Despite continued efforts that apply various ensemble methods such as bagging and boosting to deep models  it has
been observed that traditional independent ensembles  IE 
which train models independently with random initialization achieve the best performance  Ciregan et al    Lee
et al    In this paper  we focus on developing more
advanced ensembles for deep models utilizing the concept
of multiple choice learning  MCL 
The MCL concept was originally proposed in  GuzmanRivera et al    under the scenario when inference procedures are cascaded 
    First  generate   set of plausible outputs 
    Then  pick the correct solution form the set 
For example   Park   Ramanan    Batra et al   
proposed humanpose estimation methods which produce
multiple predictions and then re ne them by employing  
temporal model  and  Collins   Koo    proposed  
sentence parsing method which reranks the output of an
initial system which produces   set of plausible outputs
 Huang   Chiang    In such scenarios  the goal of
the  rst stage     is generating   set of plausible outputs
such that at least one of them is correct for the second stage
         human operators  Under this motivation  MCL has
been studied  GuzmanRivera et al      Lee et al 
  where various applications have been demonstrated 
     image classi cation  Krizhevsky   Hinton    semantic segmentation  Everingham et al    and image
captioning  Lin et al     
It trains an ensemble of
multiple models by minimizing the socalled oracle loss 
only focusing on the most accurate prediction produced by
them  Consequently  it makes each model specialized for
  certain subset of data  not for the entire one similarly as
mixtureof expert schemes  Jacobs et al   
Although MCL focuses on the  rst stage     in cascaded
scenarios and thus can produce diverse plausible outputs  it
might be not useful if one does not have   good scheme for

Con dent Multiple Choice Learning

the second stage     One can use   certain average voting
scheme of the predictions made by models for     but MCL
using deep neural networks often fails to make   correct
decision since each network tends to be overcon dent in
its prediction  Namely  the oracle error loss of MCL is low 
but its top  error rate might be very high 
Contribution  To address the issue  we develop the concept of con dent MCL  CMCL  that does not lose any bene   of the original MCL  while its target loss and architecture are redesigned for making the second stage     easier 
Speci cally  it targets to generate   set of diverse plausible
con dent predictions from which one can pick the correct
one using   simple average voting scheme  To this end  we
 rst propose   new loss function  called con dent oracle
loss  for relaxing the overcon dence issue of MCL  Our
key idea is to additionally minimize the KullbackLeibler
divergence from   predictive distribution to the uniform
one in order to give con dence to nonspecialized models 
Then  CMCL that minimizes the new loss can be ef ciently
trained like the original MCL for certain classes of models
including neural networks  via stochastic alternating minimization  Lee et al    Furthermore  when CMCL is
applied to deep models  we propose two additional regularization techniques for boosting its performance  feature
sharing and stochastic labeling  Despite the new components  we note that the training complexity of CMCL is
almost same to that of MCL or IE 
We apply the new ensemble model trained by the new
training scheme for several convolutional neural networks  CNNs  including VGGNet  Simonyan   Zisserman    GoogLeNet  Szegedy et al    and ResNet
 He et al    for image classi cation on the CIFAR
 Krizhevsky   Hinton    and SVHN  Netzer et al 
  datasets  and fullyconvolutional neural networks
 FCNs   Long et al    for foregroundbackground segmentation on the iCoseg dataset  Batra et al    First 
for the image classi cation task  CMCL outperforms all
baselines       the traditional IE and the original MCL 
in top  error rates 
In particular  CMCL of   ResNet
with   layers provides   and   relative reductions in the top  error rates from the corresponding IE
on CIFAR  and SVHN  respectively  Second  for the
foregroundbackground segmentation task  CMCL using
multiple FCNs with   layers also outperforms all baselines
in top  error rates  Each model trained by CMCL generates highquality solutions by specializing for speci   images while each model trained by IE does not  We believe that our new approach should be of broader interest
for many deep learning tasks requiring high accuracy 
Organization  In Section   we introduce necessary backgrounds for multiple choice learning and the corresponding
loss function  We describe the proposed loss and the corre 

sponding training scheme in Section   Section   provides
additional techniques for the proposed ensemble model 
Experimental results are reported in Section  

  Preliminaries
  Multiple Choice Learning

In this section  we describe the basic concept of multiple choice learning  MCL   GuzmanRivera et al   
  Throughout this paper  we denote the set             
by     for positive integer    The MCL scheme is   type
of ensemble learning that produces diverse outputs of high
quality  Formally  given   training dataset      xi  yi   
         xi       yi      we consider an ensemble of  
models                     fM   For some taskspeci   loss
function  cid            the oracle loss over the dataset   is
de ned as follows 

LO      

min
     

 cid   yi  fm  xi   

 

while the traditional independent ensemble  IE  loss is

  cid 

  

  cid 

 cid 

  

     

LE      

 cid   yi  fm  xi   

 

If all models have the same capacity and one can obtain
the  global  optimum of the IE loss with respect to the
model parameters  then all trained models should produce
the same outputs                    fM   On the other hand 
the oracle loss makes the most accurate model optimize the
loss function  cid            for each data    Therefore  MCL
produces diverse outputs of high quality by forcing each
model to be specialized on   part of the entire dataset 
Minimizing the oracle loss   is harder than minimizing
the independent ensemble loss   since the min function is
  noncontinuous function  To address the issue   GuzmanRivera et al    proposed an iterative block coordinate
decent algorithm and  Dey et al    reformulated this
problem as   submodular optimization task in which ensemble models are trained sequentially in   boostinglike
manner  However  when one considers an ensemble of
deep neural networks  it is challenging to apply these methods since they require either costly retraining or sequential
training  Recently   Lee et al    overcame this issue
by proposing   stochastic gradient descent  SGD  based
algorithm  Throughout this paper  we primarily focus on
ensembles of deep neural networks and use the SGD algorithm for optimizing the oracle loss   or its variants 

  Oracle Loss for Top  Choice

The oracle loss   used for MCL is useful for producing diverse plausible outputs  but it is often inappropriate for ap 

Con dent Multiple Choice Learning

    Multiple choice learning  MCL 

    Con dent MCL  CMCL 

    Independent ensemble  IE 

Figure   Classwise test set accuracy of each ensemble model trained by various ensemble methods  One can observe that most models
trained by MCL and CMCL become specialists for certain classes while they are generalized in case of traditional IE 

plications requiring   single choice       top  error  This
is because ensembles of deep neural networks tend to be
overcon dent in their predictions  and it is hard to judge  
better solution from their outputs  To explain this in more
detail  we evaluate the performance of ensembles of convolutional neural networks  CNNs  for the image classi cation task on the CIFAR  dataset  Krizhevsky   Hinton 
  We train ensembles of   CNNs  two convolutional
layers followed by   fullyconnected layer  using MCL 
We also train the models using traditional IE which trains
each model independently under different random initializations  Figure   summarizes the classwise test set accuracy of each ensemble member  In the case of MCL  most
models become specialists for certain classes  see Figure
    while they are generalized in the case of traditional
IE as shown in Figure     However  as expected  each
model trained by MCL signi cantly outperforms for its
specialized classes than that trained by IE  For choosing
  single output  similar to  Wan et al    Ciregan et al 
  one can average the output probabilities from ensemble members trained by MCL  but the corresponding
top  classi cation error rate is often very high       see
Table   in Section   This is because each model trained
by MCL is overcon dent for its nonspecialized classes 
To quantify this  we also compute the entropy of the predictive distribution on the test data and use this to evaluate the quality of con dence uncertainty level  Figure    
reports the entropy extracted from the predictive distribution of one of ensemble models trained by MCL  One can
observe that it has low entropy as expected for its specialized classes       classes that the model has   test accuracy higher than   However  even for nonspecialized
classes  it also has low entropy  Due to this  with respect
to top  error rates  simple averaging of models trained by
MCL performs much worse than that of IE  Such issue typically occurs in deep neural networks since it is well known
that they are poor at quantifying predictive uncertainties 
and tend to be easily overcon dent  Nguyen et al   

  Con dent Multiple Choice Learning
  Con dent Oracle Loss

In this section  we propose   modi ed oracle loss for relaxing the issue of MCL described in the previous section 
Suppose that the mth model outputs the predictive distribution             given input    where    denotes the
model parameters  Then  we de ne the con dent oracle
loss as the following integer programming variant of  

  cid 

  cid 

  

  

LC      min

vm
 

 cid 

   cid   yi           xi 
vm

 cid 

        vm

    DKL         cid           xi 

subject to

  cid 
vm
     
  
       
vm

   

   
   
         

where DKL denotes the KullbackLeibler  KL  divergence 
      is the uniform distribution    is   penalty parameter 
and vm
is    ag variable to decide the assignment of xi to
 
the mth model  By minimizing the KL divergence from
the predictive distribution to the uniform one  the new loss
forces the predictive distribution to be closer to the uniform
one       zero con dence  on nonspecialized data  while
those for specialized data still follow the correct one  For
example  for classi cation tasks  the most accurate model
for each data is allowed to optimize the classi cation loss 
while others are forced to give less con dent predictions by
minimizing the KL divergence  We remark that although
we optimize the KL divergence only for nonspecialized
data  one can also do it even for specialized data to regularize each model  Pereyra et al   

AirplaneAutomobileBirdCatDeerDogFrogHorseShipTruck                                                                                                     Con dent Multiple Choice Learning

    MCL

    CMCL

    IE with AT

    Feature sharing

Figure   Histogram of the predictive entropy of model trained by     MCL     CMCL and     IE on CIFAR  and SVHN test data  In
the case of MCL and CMCL  we separate the classes of CIFAR  into specialized       classes that model has   classwise test accuracy
higher than   and nonspecialized  others  classes  In the case of IE  we follow the proposed method by  Lakshminarayanan et al 
 
train an ensemble of   models with adversarial training  AT  and measure the entropy using the averaged probability      
averaging output probabilities from   models      Detailed view of feature sharing between two models  Grey units indicate that they
are currently dropped  Masked features passed to   model are all added to generate the shared features 

  Stochastic Alternating Minimization for Training

Algorithm   Con dent MCL  CMCL 

In order to minimize the con dent oracle loss   ef 
ciently  we use the following procedure  GuzmanRivera
et al    which optimizes model parameters     and
assignment variables  vm
  Fix     and optimize  vm
   

    alternatively 

  Fix  vm

Under  xed model parameters     the objective
    is decomposable with respect to assignments
 vm

    and it is easy to  nd optimal  vm
   

    and optimize    
Under  xed assignments  vm
   
the objective    
is decomposable with respect to model parameters
    and it requires each model to be trained independently 

The above scheme iteratively assigns each data to   particular model and then independently trains each model only
using its assigned data  Even though it monotonically decreases the objective  it is still highly inef cient since it
requires training each model multiple times until assignments  vm
    converge  To address the issue  we propose
deciding assignments and update model parameters to the
gradient directions once per each batch  similarly to  Lee
et al    In other words  we perform   single gradientupdate on parameters in Step   without waiting for their
convergence to    local  optimum  In fact   Lee et al   
show that such stochastic alternating minimization works
well for the oracle loss   We formally describe   detailed training procedure as the  version   of Algorithm  
and we will introduce the alternative  version   later  This
direction is complementary to ours  and we do not explore
in this paper 

Input  Dataset      xi  yi    xi       yi      and
penalty parameter  
Output  Ensemble of   trained models
repeat

Let       be   uniform distribution
Sample random batch      
for       to   do

Compute the loss of the mth model 

   
Lm

DKL         cid    cid        xi 

 cid 
 cid   cid  
   cid   yi       yi   xi   

 xi  yi     

end for
for       to   do
for       to     do

if the mth model has the lowest loss then

else

Compute the gradient of the training loss
 cid   yi       yi   xi          
  version   exact gradient  
Compute the gradient of the KL divergence
 DKL         cid           xi          
  version   stochastic labeling  
  log      cid yi   xi  using  cid yi          where
Compute the gradient of the cross entropy loss
 cid yi        

end if
end for
Update the model parameters

end for

until convergence

CIFAR   specialized CIFAR   nonspecialized SVHN  unseen Fraction Entropy CIFAR   specialized CIFAR   nonspecialized SVHN  unseen Fraction Entropy CIFAR   seen SVHN  unseen Fraction Entropy Hidden Feature  Hidden Feature  Masked Feature  Masked Feature  Shared Feature Shared Feature Con dent Multiple Choice Learning

  Effect of Con dent Oracle Loss

Similar to Section   we evaluate the performance of the
proposed training scheme using   CNNs for image classi cation on the CIFAR  dataset  As shown in Figure
    ensemble models trained by CMCL using the exact
gradient       version   of Algorithm   become specialists for certain classes  For specialized classes  they show
the similar performance compared to the models trained by
MCL       minimizing the oracle loss   which considers
only specialization  see Figure     For nonspecialized
classes  ensemble members of CMCL are not overcon 
dent  which makes it easy to pick   correct output via simple voting averaging  We indeed con rm that each model
trained by CMCL has not only low entropy for its specialized classes  but also exhibits high entropy for nonspecialized classes as shown in Figure    
We also evaluate the quality of con dence uncertainty level
on unseen data using SVHN  Netzer et al    Somewhat surprisingly  each model trained by CMCL only using
CIFAR  training data exhibits high entropy for SVHN
test data  whereas models trained by MCL and IE are overcon dent on it  see Figure     and     We emphasize that our method can produce con dent predictions signi cantly better than the proposed method by  Lakshminarayanan et al    which uses the averaged probability of ensemble models trained by IE to obtain high quality
uncertainty estimates  see Figure    

  Regularization Techniques
In this section  we introduce advanced techniques for reducing the overcon dence and improving the performance 

  Feature Sharing

We  rst propose   feature sharing scheme that stochastically shares the features among member models of CMCL
to further address the overcon dence issue  The primary
reason why deep learning models are overcon dent is that
they do not always extract general features from data  For
examples  assume that some deep model only trains frogs
and roses for classifying them  Although there might exist
many kinds of features on their images  the model might
make   decision based only on some speci   features      
colors  In this case   red  apples can be classi ed as rose
with high con dence  Such an issue might be more severe
in CMCL  and MCL  compared to IE since members of
CMCL are specialized to certain data  To address the issue 
we suggest the feature ensemble approach that encourages
each model to generate meaningful abstractions from rich
features extracted from other models 
Formally  consider an ensemble of   neural networks with
  hidden layers  We denote the weight matrix for layer

  and   cid 

   respectively 

 cid  of model          and  cid th hidden feature of model
  by   cid 
Instead of sharing the
whole units of   hidden feature  we introduce random binary masks determining which units to be shared with other
models  We denote the mask for layer  cid  from model   to
nm   Bernoulli  which has the same dimenm as  cid 
sion with   cid 
   we use       in all experiments  Then 
the  cid th hidden feature of model   with sharing  cid     th
hidden features is de ned as follows 

   cid 

 

   cid 

  cid 

         

 cid 

  cid  

       

nm  cid    cid 
 cid 

 

   

   

where  cid  denotes elementwise multiplication and   is the
activation function  Figure     illustrates the proposed
feature sharing scheme in an ensemble of deep neural networks  It makes each model learn more generalized features by sharing the features among them  However  one
might expect that it might make each model over tted due
to the increased number of parameters that induces   single
prediction       the statistical dependencies among outputs
of models increase  which would hurt the ensemble effect 
In order to handle this issue  we introduce the randomness
in sharing across models in   similar manner to DropOut
 Srivastava et al    using the random binary masks
  In addition  we propose sharing features at lower layers
since sharing the higher layers might over   the overall networks more  For example  in all experiments with CNNs in
this paper  we commonly apply feature sharing for hidden
features just before the  rst pooling layer  We also remark
that such feature sharing strategies for better generalization
have also been investigated in the literature for different
purposes  Misra et al    Rusu et al   

  Stochastic Labeling

For more ef ciency in minimizing the con dent oracle loss 
we also consider   noisy unbiased estimator of gradients of
the KL divergence with Monte Carlo samples from the uniform distribution  The KL divergence from the predictive
distribution to the uniform distribution can be written as
follows 

 cid 
 cid 

DKL         cid            
     
          

      log        cid 

      log

 

 

 

      log           

 

 

Hence  the gradient of the above KL divergence with respect to the model parameter   becomes
 cid DKL         cid                EU    cid log           

Con dent Multiple Choice Learning

From the above  we induce the following noisy unbiased
estimator of gradients with Monte Carlo samples from the
uniform distribution 
  EU    cid log             cid     
 

 cid log     ys     

 cid 

 

where ys         and   is the number of samples  This
random estimator takes samples from the uniform distribution       and constructs estimates of the gradient using
them  In other words   cid log     ys      is the gradient of
the cross entropy loss under assigning   random label to   
This stochastic labeling provides ef ciency in implementation computation and stochastic regularization effects  We
formally describe detailed procedures  as the version   of
Algorithm  

  Experiments
We evaluate our algorithm for both classi cation and
foregroundbackground segmentation tasks using CIFAR 
   Krizhevsky   Hinton    SVHN  Netzer et al 
  and iCoseg  Batra et al    datasets  In all experiments  we compare the performance of CMCL with those
of traditional IE and MCL using deep models  We provide
the more detailed experimental setups including model architectures in the supplementary material 

  Image Classi cation
Setup  The CIFAR  dataset consists of   training
and   test images with   image classes where each
image consists of       RGB pixels  The SVHN dataset
consists of   training and   test images  We
preprocess the images with global contrast normalization
and ZCA whitening following  Ian    Goodfellow   Bengio    Zagoruyko   Komodakis    and do not use
any data augmentation  Using these datasets  we train various CNNs       VGGNet  Simonyan   Zisserman   
GoogLeNet  Szegedy et al    and ResNet  He et al 
  Similar to  Zagoruyko   Komodakis    we use
the softmax classi er  and train each model by minimizing
the crossentropy loss using the stochastic gradient descent
method with Nesterov momentum 
For evaluation  we measure the top  and oracle error rates
on the test dataset  The top  error rate is calculated by averaging output probabilities from all models and predicting
the class of the highest probability  The oracle error rate is
the rate of classi cation failure over all outputs of individual ensemble members for   given input       it measures
whether none of the members predict the correct class for

available

at https github com 

 Our

is
chhwang cmcl 

code

 We do not use the extra SVHN dataset for training 

Ensemble
Method

Feature
Sharing

Stochastic
Labeling

IE

MCL

CMCL

 
 
 
 cid 
 cid 

 
 
 
 
 cid 

Oracle

Error Rate
 
 
 
 
 

Top 

Error Rate
 
 
 
 
 

Table   Classi cation test set error rates on CIFAR  using various ensemble methods 

an input  While   lower oracle error rate suggests higher
diversity    lower oracle error rate does not always bring
  higher top  accuracy as this metric does not reveal the
level of overcon dence of each model  By collectively
measuring the top  and oracle error rates  one can grasp
the level of specialization and con dence of   model 
Contribution by each technique  Table   validates contributions of our suggested techniques under comparison
with other ensemble methods IE and MCL  We evaluate an
ensemble of  ve simple CNN models where each model
has two convolutional layers followed by   fullyconnected
layer  We incrementally apply our optimizations to gauge
the stepwise improvement by each component  One can
note that CMCL signi cantly outperforms MCL in the top 
  error rate even without feature sharing or stochastic labeling while it still provides   comparable oracle error rate 
By sharing the  st ReLU activated features  the top  error rates are improved compared to those that employ only
con dent oracle loss  Stochastic labeling further improves
both error rates  This implies that stochastic labeling not
only reduces computational burdens but also provides regularization effects 
Overlapping  As   natural extension of CMCL  we also
consider picking   specialized models instead of having
only one specialized model  which was investigated for
original MCL  GuzmanRivera et al    Lee et al 
  This is easily achieved by modifying the constraint
       where   is an overlap parameter that controls training data overlap between the models 
This simple but natural scheme brings extra gain in top 
performance by generalizing each model better  Table  
compares the performance of various ensemble methods
with varying values of    Under the choice of      
CMCL of   CNNs provides   relative reduction in
the top  error rates from the corresponding IE  Somewhat
interestingly  IE has similar error rates on ensembles of
both   and   CNNs  which implies that the performance
of CMCL might be impossible to achieve using IE even if
one increases the number of models in IE 
Largescale CNNs  We now evaluate the performance of
our ensemble method when it is applied to largerscale
CNN models for image classi cation tasks on CIFAR 

    as cid  

   vm

Con dent Multiple Choice Learning

Ensemble Method  

Ensemble Size      

Oracle Error Rate Top  Error Rate

Ensemble Size      

Oracle Error Rate Top  Error Rate

IE

MCL

CMCL

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
   
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
   

Table   Classi cation test set error rates on CIFAR  with varying values of the overlap parameter   explained in Section   We
use CMCL with both feature sharing and stochastic labeling  Boldface values in parentheses represent the relative reductions from the
best results of MCL and IE 

Model Name

VGGNet 

GoogLeNet 

ResNet 

Ensemble
Method
   single 

IE

MCL
CMCL
   single 

IE

MCL
CMCL
   single 

IE

MCL
CMCL

CIFAR 

Oracle Error Rate Top  Error Rate

SVHN

Oracle Error Rate Top  Error Rate

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
   
 
 
 
   
 
 
 
   

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
   
 
 
 
   
 
 
 
   

Table   Classi cation test set error rates on CIFAR  and SVHN for various largescale CNN models  We train an ensemble of  
models  and use CMCL with both feature sharing and stochastic labeling  Boldface values in parentheses indicate relative error rate
reductions from the best results of MCL and IE 

and SVHN datasets  Speci cally  we test VGGNet  Simonyan   Zisserman    GoogLeNet  Szegedy et al 
  and ResNet  He et al    We share the nonlinear activated features right before the  rst pooling layer 
     the  th   nd  and  st ReLU activations for ResNet
with   layers  VGGNet with   layers  and GoogLeNet
with   layers  respectively  This choice is for maximizing the regularization effect of feature sharing while
minimizing the statistical dependencies among the ensemble models  For all models  we choose the best hyperparameters for con dent oracle loss among the penalty parameter               and the overlapping
parameter           Table   shows that CMCL consistently outperforms all baselines with respect to the top 
error rate while producing comparable oracle error rates to
those of MCL  We also apply the feature sharing to IE as reported in Figure     Even though the feature sharing also
improves the performance of IE  CMCL still outperforms
IE  CMCL provides   relative reduction of the top 

error rate from the IE with feature sharing under the choice
of       We also remark that IE with feature sharing has similar error rates as the ensemble size increases 
while CMCL does not       the gain is more signi cant for
CMCL  This implies that feature sharing is more effectively working for CMCL 

  ForegroundBackground Segmentation

In this section  we evaluate if ensemble models trained with
CMCL produce highquality segmentation of foreground
and background of an image with the iCoseg dataset  The
foregroundbackground segmentation is formulated as  
pixellevel classi cation problem with   classes        
 background  or    foreground  To tackle the problem  we
design fully convolutional networks  FCNs  model  Long
et al    based on the decoder architecture presented in
 Radford et al    The dataset consists of   groups of
related images with pixellevel ground truth on foregroundbackground segmentation of each image  We only use im 

Con dent Multiple Choice Learning

Figure   Prediction results of foregroundbackground segmentation for   few sample images    test error rate is shown below each
prediction  The ensemble models trained by CMCL and MCL generate highquality predictions specialized for certain images 

   

   

   

Figure       Top  error rate on CIFAR  We train an ensemble of   ResNets with   layers  and apply feature sharing  FS  to IE
and CMCL      Top  error rate and     oracle error rate on iCoseg by varying the ensemble sizes  The ensemble models trained by
CMCL consistently improves the top  error rate over baselines 

ages that are larger than       pixels  For each class 
we randomly split   and   of the data into training
and test sets  respectively  We train on       resized
images using the bicubic interpolation  Keys    Similar to  GuzmanRivera et al    Lee et al    we
initialize the parameters of FCNs with those trained by IE
for MCL and CMCL  For all experiments  CMCL is used
with both feature sharing and stochastic labeling 
Similar to  GuzmanRivera et al    we de ne the percentage of incorrectly labeled pixels as prediction error
rate  We measure the oracle error rate       the lowest error
rate over all models for   given input  and the top  error
rate  The top  error rate is measured by following the predictions of the member model that has   lower pixelwise
entropy       picking the output of   more con dent model 
For each ensemble method  we vary the number of ensemble models and measure the oracle error rate and test error
rate  Figure     and     show both top  and oracle error rates for all ensemble methods  We remark that the ensemble models trained by CMCL consistently improves the
top  error rate over baselines  In an ensemble of   models 
we  nd that CMCL achieve up to   relative reduction

in the top  error rate from the corresponding IE  As shown
in Figure   an individual model trained by CMCL generates highquality solutions by specializing itself in speci   images       model   is specialized for  lobster  while
model   is specialized for  duck  while each model trained
by IE does not 

  Conclusion
This paper proposes CMCL    novel ensemble method of
deep neural networks that produces diverse plausible con 
 dent prediction of high quality  To this end  we address
the overcon dence issues of MCL  and propose   new
loss  architecture and training method  In our experiments 
CMCL outperforms not only the known MCL  but also the
traditional IE  with respect to the top  error rates in classi cation and segmentation tasks  The recent trend in the
deep learning community tends to make models bigger and
wider  We believe that our new ensemble approach brings  
refreshing angle for developing advanced largescale deep
neural networks in many related applications 

         InputGround truthIE model  CMCL model  Prediction error rate         Prediction error rate         IE model  CMCL model  MCL model  MCL model  CMCLIE with FSIE without FS Top  error rate  Ensemble size   MCLCMCLIETop  error rate  Ensemble size   MCLCMCLIEOracle error rate  Ensemble size   Con dent Multiple Choice Learning

Acknowledgements
This work was supported in part by the ICT     Program
of MSIP IITP  Korea  under   Research on
Adaptive Machine Learning Technology Development for
Intelligent Autonomous Digital Companion    
   High Performance Big Data Analytics Platform Performance Acceleration Technologies Development  and
by the National Research Council of Science   Technology  NST  grant by the Korea government  MSIP   No 
CRC ETRI 

References
Batra  Dhruv  Kowdle  Adarsh  Parikh  Devi  Luo  Jiebo 
and Chen  Tsuhan  icoseg  Interactive cosegmentation
with intelligent scribble guidance  In Computer Vision
and Pattern Recognition  CVPR  pp    IEEE 
 

Batra  Dhruv  Yadollahpour  Payman  GuzmanRivera  Abner  and Shakhnarovich  Gregory  Diverse mbest solutions in markov random  elds  In European Conference
on Computer Vision  ECCV  pp    Springer   

Breiman  Leo  Bagging predictors  Machine learning   

   

Ciregan  Dan  Meier  Ueli  and Schmidhuber    urgen 
Multicolumn deep neural networks for image classi 
In Computer Vision and Pattern Recognition
cation 
 CVPR  pp    IEEE   

Collins  Michael and Koo  Terry  Discriminative reranking
for natural language parsing  Computational Linguistics 
   

Dey  Debadeepta  Ramakrishna  Varun  Hebert  Martial 
and Andrew Bagnell     Predicting multiple structured
In International Conference on
visual interpretations 
Computer Vision  ICCV  pp     

Domingos  Pedro  Bayesian averaging of classi ers and
the over tting problem  In International Conference on
Machine Learning  ICML  volume   pp   
 

Everingham  Mark  Van Gool  Luc  Williams  Christopher KI  Winn  John  and Zisserman  Andrew  The pascal visual object classes  voc  challenge  International
journal of computer vision     

Freund  Yoav  Schapire  Robert  and Abe       short introduction to boosting  JournalJapanese Society For Arti 
 cial Intelligence     

GuzmanRivera  Abner  Batra  Dhruv  and Kohli  Pushmeet  Multiple choice learning  Learning to produce

In Advances in neural inmultiple structured outputs 
formation processing systems  NIPS  pp   
 

GuzmanRivera  Abner  Kohli  Pushmeet  Batra  Dhruv 
and Rutenbar  Rob    Ef ciently enforcing diversity
In International
in multioutput structured prediction 
Conference on Arti cial Intelligence and Statistics  AISTATS  volume   pp     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition 
In Computer Vision and Pattern Recognition  CVPR 
 

Huang  Liang and Chiang  David  Better kbest parsing 
In Proceedings of the Ninth International Workshop on
Parsing Technology  pp    Association for Computational Linguistics   

Ian    Goodfellow  David WardeFarley  Mehdi Mirza
Aaron Courville and Bengio  Yoshua  Maxout networks  In International Conference on Machine Learning  ICML  pp     

Ioffe  Sergey and Szegedy  Christian  Batch normalization 
Accelerating deep network training by reducing internal
covariate shift  In International Conference on Machine
Learning  ICML  pp     

Jacobs  Robert    Jordan  Michael    Nowlan  Steven    and
Hinton  Geoffrey    Adaptive mixtures of local experts 
Neural computation   

Keys  Robert  Cubic convolution interpolation for digiIEEE transactions on acoustics 
tal image processing 
speech  and signal processing     

Kingma  Diederik and Ba  Jimmy  Adam    method for
stochastic optimization  In International Conference on
Learning Representations  ICLR   

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple

layers of features from tiny images   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In Advances in neural information processing
systems  NIPS  pp     

Lakshminarayanan  Balaji  Pritzel  Alexander  and Blundell  Charles  Simple and scalable predictive uncertainty
estimation using deep ensembles  NIPS Workshop on
Bayesian Deep Learning   

Lee  Stefan  Purushwalkam  Senthil  Cogswell  Michael 
Crandall  David  and Batra  Dhruv  Why   heads are
better than one  Training   diverse ensemble of deep networks  arXiv preprint arXiv   

Con dent Multiple Choice Learning

Lee  Stefan  Prakash  Senthil Purushwalkam Shiva 
Cogswell  Michael  Ranjan  Viresh  Crandall  David 
and Batra  Dhruv  Stochastic multiple choice learning for
training diverse deep ensembles  In Advances in neural
information processing systems  NIPS  pp   
 

Lin  Min  Chen  Qiang  and Yan  Shuicheng  Network in
network  In International Conference on Learning Representations  ICLR     

Simonyan  Karen and Zisserman  Andrew  Very deep convolutional networks for largescale image recognition  In
International Conference on Learning Representations
 ICLR   

Srivastava  Nitish  Hinton  Geoffrey    Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan  Dropout 
  simple way to prevent neural networks from over tJournal of Machine Learning Research   
ting 
   

Lin  TsungYi  Maire  Michael  Belongie  Serge  Hays 
James  Perona  Pietro  Ramanan  Deva  Doll ar  Piotr  and
Zitnick    Lawrence  Microsoft coco  Common objects
in context  In European Conference on Computer Vision
 ECCV  pp    Springer     

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 
Going deeper with convolutions  In Computer Vision and
Pattern Recognition  CVPR   

Long  Jonathan  Shelhamer  Evan  and Darrell  Trevor 
Fully convolutional networks for semantic segmentation 
In Computer Vision and Pattern Recognition  CVPR 
pp     

Wan  Li  Zeiler  Matthew    Zhang  Sixin  LeCun  Yann 
and Fergus  Rob  Regularization of neural networks usIn International Conference on Maing dropconnect 
chine Learning  ICML  pp     

Zagoruyko  Sergey and Komodakis  Nikos  Wide residIn British Machine Vision Conference

ual networks 
 BMVC   

Misra  Ishan  Shrivastava  Abhinav  Gupta  Abhinav  and
Hebert  Martial  Crossstitch networks for multitask
In Computer Vision and Pattern Recognition
learning 
 CVPR  pp     

Netzer  Yuval  Wang  Tao  Coates  Adam  Bissacco 
Alessandro  Wu  Bo  and Ng  Andrew    Reading digits in natural images with unsupervised feature learning 
NIPS Workshop on Deep Learning and Unsupervised
Feature Learning     

Nguyen  Anh  Yosinski  Jason  and Clune  Jeff  Deep neural networks are easily fooled  High con dence predicIn Computer Vision
tions for unrecognizable images 
and Pattern Recognition  CVPR  pp     

Park  Dennis and Ramanan  Deva  Nbest maximal decoders for part models  In International Conference on
Computer Vision  ICCV  pp    IEEE   

Pereyra  Gabriel  Tucker  George  Chorowski  Jan  Kaiser 
 ukasz  and Hinton  Geoffrey  Regularizing neural networks by penalizing con dent output distributions 
In
International Conference on Learning Representations
 ICLR   

Radford  Alec  Metz  Luke  and Chintala  Soumith  Unsupervised representation learning with deep convolutional
generative adversarial networks  In International Conference on Learning Representations  ICLR   

Rusu  Andrei    Rabinowitz  Neil    Desjardins  Guillaume  Soyer  Hubert  Kirkpatrick  James  Kavukcuoglu 
Koray  Pascanu  Razvan  and Hadsell  Raia  Progressive neural networks  arXiv preprint arXiv 
 

