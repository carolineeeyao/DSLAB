Nystr om Method with Kernel Kmeans  Samples as Landmarks

Dino Oglic     Thomas   artner  

Abstract

We investigate  theoretically and empirically  the
effectiveness of kernel Kmeans  samples as
landmarks in the Nystr om method for lowrank
approximation of kernel matrices  Previous empirical studies  Zhang et al    Kumar et al 
  observe that the landmarks obtained using
 kernel  Kmeans clustering de ne   good lowrank approximation of kernel matrices  However 
the existing work does not provide   theoretical
guarantee on the approximation error for this approach to landmark selection  We close this gap
and provide the  rst bound on the approximation error of the Nystr om method with kernel Kmeans  samples as landmarks  Moreover  for
the frequently used Gaussian kernel we provide
  theoretically sound motivation for performing
Lloyd re nements of kernel Kmeans  landmarks in the instance space  We substantiate our
theoretical results empirically by comparing the
approach to several stateof theart algorithms 

  Introduction
We consider the problem of  nding   good lowrank approximation for   given symmetric and positive de nite matrix 
Such matrices arise in kernel methods  Sch olkopf   Smola 
  where the data is often  rst transformed to   symmetric and positive de nite matrix and then an offthe shelf
matrixbased algorithm is used for solving classi cation
and regression problems  clustering  anomaly detection  and
dimensionality reduction  Bach   Jordan    These
learning problems can often be posed as convex optimization problems for which the representer theorem  Wahba 
  guarantees that the optimal solution can be found in
the subspace of the kernel feature space spanned by the
instances  To  nd the optimal solution in   problem with  
instances  it is often required to perform   matrix inversion

 Institut   ur Informatik III  Universit at Bonn  Germany  School
of Computer Science  The University of Nottingham  United Kingdom  Correspondence to  Dino Oglic  dino oglic unibonn de 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

or eigendecomposition which scale as   cid   cid  To overcome

this computational shortcoming and scale kernel methods
to large scale datasets  Williams   Seeger   have proposed to use   variant of the Nystr om method  Nystr om 
  for lowrank approximation of kernel matrices  The
approach is motivated by the fact that frequently used kernels have   fast decaying spectrum and that small eigenvalues can be removed without   signi cant effect on the
precision  Sch olkopf   Smola    For   given subset
of   landmarks  the Nystr om method  nds   lowrank ap 

proximation in time   cid         cid  and kernel methods with
scale as   cid   cid  In practice     cid    and the approach can

the lowrank approximation in place of the kernel matrix

scale kernel methods to millions of instances 
The crucial step in the Nystr om approximation of   symmetric and positive de nite matrix is the choice of landmarks
and an optimal choice is   dif cult discrete combinatorial
problem directly in uencing the goodness of the approximation  Section     large part of the existing work has 
therefore  focused on providing approximation guarantees
for different landmark selection strategies  Following this
line of research  we propose to select landmarks using the
kernel Kmeans  sampling scheme  Arthur   Vassilvitskii    and provide the  rst bound on the relative
approximation error in the Frobenius norm for this strategy
 Section   An important part of our theoretical contribution is the  rst complete proof of   claim by Ding   He
  on the relation between the subspace spanned by
optimal Kmeans centroids and left singular vectors of the
feature space  Proposition   While our proof covers the
general case  that of Ding   He   is restricted to data
matrices with piecewise constant right singular vectors 
Having given   bound on the approximation error for the
proposed landmark selection strategy  we provide   brief
overview of the existing landmark selection algorithms and
discuss our work in relation to approaches directly comparable to ours  Section   For the frequently used Gaussian
kernel  we also theoretically motivate the instance space
Lloyd re nements  Lloyd    of kernel Kmeans 
landmarks  The results of our empirical study are presented
in Section   and indicate   superior performance of the
proposed approach over competing methods  This is in
agreement with the previous studies on Kmeans centroids
as landmarks by Zhang et al    and Kumar et al   

Nystr om Method with Kernel Kmeans  Samples as Landmarks

  Nystr om Method
In this section  we review the Nystr om method for lowrank approximation of kernel matrices  The method was
originally proposed for the approximation of integral eigenfunctions  Nystr om    and later adopted to lowrank
approximation of kernel matrices by Williams   Seeger
  We present it here in   slightly different light by following the approach to subspace approximations by Smola
  Sch olkopf    
Let   be an instance space and              xn 
an independent sample from   Borel probability measure
de ned on     Let   be   reproducing kernel Hilbert space
with   positive de nite kernel                Given  
set of landmark points         zm   not necessarily
  subset of the sample  the goal is to approximate kernel
functions    xi  for all         using linear combinations
of the landmarks  This goal can be formally stated as

 cid cid cid cid cid cid    xi      cid 

  

  cid 

  

min

 Rm  

 cid cid cid cid cid cid 

 

   ih  zj 

 

 

Let   denote the kernel matrix over all samples and landmarks and let HZ denote the block in this matrix corresponding to the kernel values between the landmarks  Additionally  let hx denote   vector with entries corresponding to
the kernel values between an instance   and the landmarks 
After expanding the norm  the problem is transformed into

min

 Rm  

Hii      cid 

xi

      cid 

  HZ    

 

  cid 

  

where    denotes the ith column of   Each summand in
the optimization objective is   convex function depending
only on one column of   Hence  the optimal solution is

      

  HZ    

From here it then follows that the optimal approximation
 HX   of the matrix HX using landmarks   is given by

 HX     HX ZH 

  HZ    

While the problem of computing the optimal projections of
instances to   subspace spanned by the landmarks is convex and solvable in closed form  see above  the problem
of choosing the best set of landmarks is   combinatorial
problem that is dif cult to solve  To evaluate the effectiveness of the subspace spanned by   given set of landmarks
it is standard to use the Schatten matrix norms  Weidmann 
  The Schatten pnorm of   symmetric and positive def 
     where
       are eigenvalues of   and       For       the
Schatten pnorm is equal to the operator norm and for      

inite matrix   is de ned as  cid   cid      cid  

     
   

it is equal to the Frobenius norm  The three most frequently
used Schatten norms are         and for these norms
the following inequalities hold 

 cid   cid    max

    

 
   

tr    cid       cid   cid 

 cid cid 

 cid 

 

     tr        cid   cid   

 

 cid 

 

From Eq    and   it follows that for   subspace spanned
by   given set of landmarks    the  norm approximation
error of the optimal projections onto this space is given by

      tr HX     tr   HX     

 cid cid cid HX    HX  

 cid cid cid 

 

The latter equation follows from the properties of trace
and the fact that     HX    HX   is   symmetric and
positive de nite matrix with  ij    cid   xi     xj cid  

and    xi       xi   cid  

  ih  zk 

    

For   good Nystr om approximation of   kernel matrix it is
crucial to select the landmarks to reduce the error in one of
the frequently used Schatten pnorms      

    

arg min

  span        

 cid cid cid HX    HX  

 cid cid cid  

 

Let us denote with VK and    the top   eigenvectors and
eigenvalues of the kernel matrix HX  Then  at the lowrank
approximation    
    the Schatten pnorm
error attains its minimal value  Golub   van Loan   

      VK KV  cid 

  Kernel Kmeans  Samples as Landmarks
We start with   review of Kmeans clustering  Lloyd   
and then give the  rst complete proof of   claim stated
in Ding   He   and Xu et al    on the relation
between the subspace spanned by the top        left singular vectors of the data matrix and that spanned by optimal
Kmeans centroids  Building on   result by Arthur   Vassilvitskii   we then bound the relative approximation
error in the Frobenius norm of the Nystr om method with
kernel Kmeans  samples as landmarks 
Let the instance space     Rd and let   denote the number
of clusters  In Kmeans clustering the goal is to choose  
set of centers           cK  minimizing the potential

     

 cid       cid   

min
   

 cid     ck cid   

where Pk                    ck  is   clustering cell
and           denotes the centroid assignment function  For   clustering cell Pk the centroid is computed as
 Pk 
   In the remainder of the section  we denote

 cid 

  Pk

  cid 

 cid 

  

  Pk

 cid 

   

Nystr om Method with Kernel Kmeans  Samples as Landmarks

with     Rn   the cluster indicator matrix of the clustering   such that pij    
nj when instance xi is assigned
to centroid cj  and pij     otherwise  Here nj denotes the
number of instances assigned to centroid cj  Without loss
of generality  we assume that the columns of the data matrix

    Rd   are centered instances      cid  
       cid cid     XP    cid cid cid 

Now  using the introduced notation we can write the clustering potential as  Ding   He    Boutsidis et al   

   xi      

   

Denoting with pi the ith column in   we have that it holds
  cid 
  pj    ij  where  ij     if       and otherwise  ij    
Hence  it holds that    cid     IK and   is an orthogonal
projection matrix with rank    Let   denote the family of
all possible clustering indicator matrices of rank    Then 
the Kmeans optimization problem is equivalent to the constrained lowrank approximation problem

 cid cid     XP    cid cid cid 

   

      arg min
   

From here  using the relation between the squared Schatten
 norm and the matrix trace we obtain

tr cid   cid   cid    tr cid    cid   cid XP cid   

 

      arg min
   

  

 

problem can be reduced  As cid  

In the remainder of the section  we refer to the constrained
optimization objective from Eq    as the discrete problem  For this problem  Ding   He   observe that the
 
set of vectors       pK    
   is linearly dependent   
is   vector of ones  and that the rank of the optimization
nipi      there exists
  linear orthonormal transformation of the subspace basis
given by the columns of   such that one of the vectors in
 
the new basis of the subspace spanned by   is   
   Such
transformations are equivalent to   rotation of the subspace 
Let     RK   denote an orthonormal transformation
matrix such that the vectors  pi  
   with
qK    
     This is equivalent to requiring that the Kth coli       for
            Moreover  from         and   cid     IK it
follows that

umn in   is rK  cid cid    

   map to  qi  
 cid cid 

     cid  nK

and   cid 

 

  cid       cid    cid         cid     IK 

tr cid   cid 

Hence  if we denote with QK  the matrixblock with the
 rst        columns of   then the problem from Eq   
can be written as  Ding   He    Xu et al   
  
    cid XQK 
    
  cid 
  QK    IK 
 
          qK  
 

QK Rn   

arg max

 cid 

    

  

While   is an orthonormal indicator sparse matrix of rank
     is   piecewise constant and in general nonsparse orthonormal matrix of the same rank  The latter optimization
problem can be relaxed by not adding the structural con 
 
straints         and qK     
   The resulting optimization problem is known as the Rayleigh Ritz quotient      
see   utkepohl    and in the remainder of the section we
refer to it as the continuous problem  The optimal solution
to the continuous problem is  up to   rotation of the basis 
de ned by the top        eigenvectors from the eigendecomposition of the positive de nite matrix   cid   and the
optimal value of the relaxed optimization objective is the
sum of the eigenvalues corresponding to this solution  As
the continuous solution is  in general  not sparse  the discrete problem is better described with nonsparse piecewise
constant matrix   than with sparse indicator matrix    
Ding   He   and Xu et al    have formulated
  theorem which claims that the subspace spanned by optimal Kcentroids is in fact the subspace spanned by the
top        left singular vectors of    The proofs provided in these works are  however  restricted to the case
when the discrete and continuous relaxed version of the
optimization problem match  We address here this claim
without that restriction and amend their formulation accordingly  For this purpose  let            cK 
be   centroids specifying an optimal Kmeans clustering       minimizing the potential  The between clusi projects any vector
      to   subspace spanned by the centroid vectors      
   denote the Kth eigenvalue of       cid   and assume
the eigenvalues are listed in descending order    proof of
the following proposition is provided in Appendix   
Proposition   Suppose that the subspace spanned by optimal Kmeans centroids has   basis that consists of left
singular vectors of    If the gap between the eigenvalues
    and    is suf ciently large  see the proof for explicit
de nition  then the optimal Kmeans centroids and the top
       left singular vectors of   span the same subspace 
Proposition   In contrast to the claim by Ding   He  
and Xu et al    it is possible that no basis of the
subspace spanned by optimal Kmeans centroids consists
of left singular vectors of    In that case  the subspace
spanned by the top        left singular vectors is different
from that spanned by optimal Kmeans centroids 
Let           cid  be an SVD decomposition of   and denote
with UK the top   left singular vectors from this decomposition  Let also   
  denote the dual matrix of UK and
        UK  the clustering potential given by the projections of   and    onto the subspace UK 
Proposition   Let HK denote the optimal rank   approximation of the Gram matrix       cid   and let    be an

ter scatter matrix      cid  
Sx    cid  

 cid   cid 
    cid  ci   span      cK  Let also

   nicic cid 

   ni

Nystr om Method with Kernel Kmeans  Samples as Landmarks

optimal Kmeans clustering of    Then  it holds
     cid     HK cid            UK   

Let us now relate Proposition   to the result from Section  
where we were interested in  nding   set of landmarks spanning the subspace that preserves most of the variance of
the data in the kernel feature space  Assuming that the
conditions from Proposition   are satis ed  the Nystr om
approximation using optimal kernel Kmeans centroids as
landmarks projects the data to   subspace with the highest
possible variance  Hence  under these conditions optimal
kernel Kmeans landmarks provide the optimal rank    
reconstruction of the kernel matrix  However  for   kernel
Kmeans centroid there does not necessarily exist   point
in the instance space that maps to it  To account for this
and the hardness of the kernel Kmeans clustering problem  Aloise et al    we propose to approximate the
centroids with kernel Kmeans  samples  This sampling
strategy iteratively builds up   set of landmarks such that
in each iteration an instance is selected with probability
proportional to its contribution to the clustering potential
in which previously selected instances act as cluster centers  For   problem with   instances and dimension    the
strategy selects   landmarks in time    Knd 
Before we give   bound on the Nystr om approximation with
kernel Kmeans  samples as landmarks  we provide  
result by Arthur   Vassilvitskii   on the approximation
error of the optimal clustering using this sampling scheme 
Theorem    Arthur   Vassilvitskii   If   clustering
  is constructed using the Kmeans  sampling scheme
then the corresponding clustering potential       satis es

             ln              

where    is an optimal clustering and the expectation is
taken with respect to the sampling distribution 

Having presented all the relevant results  we now give  
bound on the approximation error of the Nystr om method
with kernel Kmeans  samples as landmarks    proof of
the following theorem is provided in Appendix   
Theorem   Let   be   kernel matrix with    nite rank
factorization          
      Denote with HK the
optimal rank   approximation of   and let  HK be the
Nystr om approximation of the same rank obtained using
kernel Kmeans  samples as landmarks  Then  it holds

 cid 

 
   ln          

           

 cid 

 cid cid      HK cid 

 cid     HK cid 

 

with         UK  cid   HK cid  where UK denotes the top
  left singular vectors of       and    an optimal kernel
Kmeans clustering with        clusters 

Corollary   When         UK     
then the additive term       
    cid 

 cid cid      HK cid 

 cid 

ln  

 

       cid     HK cid 
      and
 

 cid 

     

 

 

 cid     HK cid 

The given bound for lowrank approximation of symmetric
and positive de nite matrices holds for the Nystr om method
with kernel Kmeans  samples as landmarks without
any Lloyd iterations  Lloyd    To obtain even better
landmarks  it is possible to  rst sample candidates using
the kernel Kmeans  sampling scheme and then attempt
  Lloyd re nement in the instance space  motivation for
this is provided in Section   If the clustering potential
is decreased as   result of this  the iteration is considered
successful and the landmarks are updated  Otherwise  the
re nement is rejected and current candidates are selected as
landmarks  This is one of the landmark selection strategies
we analyze in our experiments       see Appendix   
Let us now discuss the properties of our bound with respect to the rank of the approximation  From Corollary  
it follows that the bound on the relative approximation error increases initially  for small    with ln   and then
decreases as   approaches    This is to be expected as  
larger   means we are trying to  nd   higher dimensional
subspace and initially this results in having to solve   more
dif cult problem  The bound on the lowrank approximation error is  on the other hand  obtained by multiplying
with  cid     HK cid  which depends on the spectrum of the
kernel matrix and decreases with    In order to be able to
generalize at all  one has to assume that the spectrum falls
rather sharply and typical assumptions are             
tion   Bach    It is simple to show that for      
      and              such falls are sharper than ln  
 Corollary   Appendix    Thus  our bound on the lowrank
approximation error decreases with   for sensible choices
of the kernel function  Note that   similar stateof theart
bound on the relative approximation error by Li et al   
exhibits worse behavior and grows linearly with   

with       or        cid   bi cid  with             see Sec 

  Discussion
We start with   brief overview of alternative approaches
to landmark selection in the Nystr om method for lowrank
approximation of kernel matrices  Following this  we focus on   bound that is the most similar to ours  that of
KDPP Nystr om  Li et al    Then  for the frequently
used Gaussian kernel  we provide   theoretically sound
motivation for performing Lloyd re nements of kernel Kmeans  landmarks in the instance space instead of the
kernel feature space  These re nements are computationally
cheaper than those performed in the kernel feature space
and can only improve the positioning of the landmarks 

Nystr om Method with Kernel Kmeans  Samples as Landmarks

  Related Approaches

As pointed in Sections   and   the choice of landmarks
is instrumental for the goodness of the Nystr om lowrank
approximations  For this reason  the existing work on the
Nystr om method has focused mainly on landmark selection
techniques with theoretical guarantees  These approaches
can be divided into four groups     random sampling  ii 
greedy methods  iii  methods based on the Cholesky decomposition  iv  vector quantization       Kmeans clustering 
The simplest strategy for choosing the landmarks is by uniformly sampling them from   given set of instances  This
was the strategy that was proposed by Williams   Seeger
  in the  rst paper on the Nystr om method for lowrank
approximation of kernel matrices  Following this  more sophisticated nonuniform sampling schemes were proposed 
The schemes that received   lot of attention over the past
years are the selection of landmarks by sampling proportional to column norms of the kernel matrix  Drineas et al 
  diagonal entries of the kernel matrix Drineas   Mahoney   approximate leverage scores  Alaoui   Mahoney    Gittens   Mahoney    and submatrix
determinants  Belabbas   Wolfe    Li et al   
From this group of methods  the approximate leverage score
sampling and the KDPP Nystr om method  see Section  
are considered stateof theart methods in lowrank approximation of kernel matrices 
The second group of landmark selection techniques are
greedy methods    wellperforming representative from
this group is   method for sparse approximations proposed
by Smola   Sch olkopf   for which it was later independently established  Kumar et al    that it performs
very well in practice second only to Kmeans clustering 
The third group of methods relies on the incomplete
Cholesky decomposition to construct   lowrank approximation of   kernel matrix  Fine   Scheinberg    Bach
  Jordan    Kulis et al    An interesting aspect of
the work by Bach   Jordan   and that of Kulis et al 
  is the incorporation of side information labels into
the process of  nding   good lowrank approximations of  
given kernel matrix 
Beside these approaches  an in uential ensemble method
for lowrank approximation of kernel matrices was proposed by Kumar et al    This work also contains an
empirical study with   number of approaches to landmark
selection  Kumar et al    also note that the landmarks
obtained using instance space Kmeans clustering perform
the best among nonensemble methods 

  KDPP Nystr om Method

The  rst bound on the Nystr om approximation with landmarks sampled proportional to submatrix determinants was

given by Belabbas   Wolfe   Li et al    recognize this sampling scheme as   determinantal point process
and extend the bound to account for the case when   landmarks are selected to make an approximation of rank       
That bound can be formally speci ed as  Li et al   

 cid 

 cid cid      HK cid 

 cid     HK cid 

 

       

         

 

      

 

For        the bound can be derived from that of Belabbas
  Wolfe  Theorem     by applying the inequalities
between the corresponding Schatten pnorms 
The bounds obtained by Belabbas   Wolfe   and Li
et al    can be directly compared to the bound from
Corollary   From Eq    for           we get that
the expected relative approximation error of the KDPP

Nystr om method scales like   cid  

      cid  For   good

 

 

worst case guarantee on the generalization error of learning
with Nystr om approximations  see       Yang et al   
the parameter   scales as
   Plugging this parameter
estimate into Eq    we see that the upper bound on the ex 
 
pected error with kernel Kmeans  landmarks scales like
   
  ln    and that with KDPP landmarks like      
Having compared our bound to that of KDPP landmark
selection  we now discuss some speci cs of the empirical
study performed by Li et al    The crucial step of
that landmark selection strategy is the ability to ef ciently
sample from   KDPP  To achieve this  the authors have proposed to use   Markov chain with   worst case mixing time
linear in the number of instances  The mixing bound holds
provided that   datadependent parameter satis es   condition which is computationally dif cult to verify  Section  
Li et al    Moreover  there are cases when this condition is not satis ed and for which the mixing bound does not
hold  In their empirical evaluation of the KDPP Nystr om
method  Li et al    have chosen the initial state of the
Markov chain by sampling it using the Kmeans  scheme
and then run the chain for   iterations  While the
choice of the initial state is not discussed by the authors  one
reason that this could be   good choice is because it starts the
chain from   high density region  To verify this hypothesis 
we simulate the KDPP Nystr om method by choosing the
initial state uniformly at random and run the chain for    
and     steps  Section   Our empirical results indicate
that starting the KDPP chain with Kmeans  samples is
instrumental for performing well with this method in terms
of runtime and accuracy  Figure   Li et al    Moreover  for the case when the initial state is sampled uniformly
at random  our study indicates that the chain might need at
least one pass through the data to reach   region with good
landmarks  The latter is computationally inef cient already
on datasets with more than     instances 

Nystr om Method with Kernel Kmeans  Samples as Landmarks

  Instance Space Kmeans Centroids as Landmarks

 cid cid       cid     cid       cid cid 

We  rst address the approach to landmark selection based
on Kmeans clustering in the instance space  Zhang et al 
  and then give   theoretically sound motivation for why
these landmarks work well with the frequently used Gaussian kernel  The outlined reasoning motivates the instance
space Lloyd re nements of kernel Kmeans  samples
and it can be extended to other kernel feature spaces by
following the derivations from Burges  
The only existing bound for instance space Kmeans landmarks was provided by Zhang et al    However  this
bound only works for kernel functions that satisfy
                               
 
for all                and   dataand kerneldependent constant          In contrast to this  our bound holds for all
kernels over Euclidean spaces  The bound given by Zhang
et al    is also   worst case bound  while ours is  
bound in the expectation  The type of the error itself is also
different  as we bound the relative error and Zhang et al 
  bound the error in the Frobenius norm  The disadvantage of the latter is in the sensitivity to scaling and such
bounds become loose even if   single entry of the matrix
is large  Li et al    Having established the difference
in the type of the bounds  it cannot be claimed that one is
sharper than the other  However  it is important to note that
the bound by Zhang et al   Proposition     contains the
full clustering potential       multiplied by  
    as  
term and this is signi cantly larger than the rank dependent
term from our bound       see Theorem  
Burges   has investigated the geometry of kernel feature spaces and   part of that work refers to the Gaussian
kernel  We review the results related to this kernel feature
space and give an intuition for why Kmeans clustering in
the instance space provides   good set of landmarks for the
Nystr om approximation of the Gaussian kernel matrix  The
reasoning can be extended to other kernel feature spaces
as long as the manifold onto which the data is projected
in the kernel feature space is    at Riemannian manifold
with the geodesic distance between the points expressed in
terms of the Euclidean distance between instances       see
Riemmannian metric tensors in Burges   
The frequently used Gaussian kernel is given by

            cid           cid    exp cid cid     cid cid   

 

where the feature map       is in nite dimensional and for
  subset   of the instance space     Rd also in nitely
continuously differentiable on    As in Burges   we
denote with   the image of   in the reproducing kernel
Hilbert space of    The image   is         dimensional
surface in this Hilbert space  As noted by Burges  

the image   is   Hausdorff space  Hilbert space is   metric
space and  thus    Hausdorff space  and has   countable
basis of open sets  the reproducing kernel Hilbert space
of the Gaussian kernel is separable  So  for   to be  
differentiable manifold  Boothby    the image   needs
to be locally Euclidean of dimension       
We assume that our set of instances   is mapped to   differentiable manifold in the reproducing kernel Hilbert space
   On this manifold   Riemannian metric can be de ned
and  thus  the set   is mapped to   Riemannian manifold   
Burges   has showed that the Riemannian metric tensor induced by this kernel feature map is gij    ij
    where
 ij     if       and zero otherwise               This
form of the tensor implies that the manifold is  at 
From the obtained metric tensor  it follows that the squared
geodesic distance between two points       and       on  
is equal to the  scaled Euclidean distance between   and  
in the instance space       dS                  cid     cid 
For   cluster Pk  the geodesic centroid is   point on   that
minimizes the distance to other cluster points  centroid in
the Kmeans sense  For our instance space  we have that

  
    arg min

  Rd

 cid       cid      

   

 
 Pk 

 cid 

  Pk

 cid 

  Pk

  

Thus  by doing Kmeans clustering in the instance space
we are performing approximate geodesic clustering on the
manifold onto which the data is embedded in the Gaussian
kernel feature space  It is important to note here that   centroid from the instance space is only an approximation to
the geodesic centroid from the kernel feature space   the
preimage of the kernel feature space centroid does not necessarily exist  As the manifold is  at  geodesic centroids are
 good  approximations to kernel Kmeans centroids  Hence 
by selecting centroids obtained using Kmeans clustering
in the instance space we are making   good estimate of
the kernel Kmeans centroids  For the latter centroids  we
know that under the conditions of Proposition   they span
the same subspace as the top        left singular vectors
of    nite rank factorization of the kernel matrix and  thus 
de ne   good lowrank approximation of the kernel matrix 

  Experiments
Having reviewed the stateof theart methods in selecting landmarks for the Nystr om lowrank approximation
of kernel matrices  we perform   series of experiments to
demonstrate the effectiveness of the proposed approach
and substantiate our claims from Sections   and   We
achieve this by comparing our approach to the stateof theart in landmark selection   approximate leverage score sampling  Gittens   Mahoney    and the KDPP Nystr om
method  Belabbas   Wolfe    Li et al   

Nystr om Method with Kernel Kmeans  Samples as Landmarks

Figure   The  gure shows the lift of the approximation error in the Frobenius norm as the bandwidth parameter of the Gaussian kernel
varies and the approximation rank is  xed to       The lift of   landmark selection strategy indicates how much better it is to
approximate the kernel matrix with landmarks obtained using that strategy compared to the uniformly sampled ones 

Figure   The  gure shows the time it takes to select landmarks via different schemes together with the corresponding error in the
Frobenius norm while the bandwidth of the Gaussian kernel varies and the approximation rank is  xed to      

Before we present and discuss our empirical results  we
provide   brief summary of the experimental setup  The
experiments were performed on   realworld datasets available at the UCI and LIACC repositories  Each of the selected
datasets consists of more than     instances  Prior to
running the experiments  the datasets were standardized to
have zero mean and unit variance  We measure the goodness of   landmark selection strategy with the lift of the
approximation error in the Frobenius norm and the time
needed to select the landmarks  The lift of the approximation error of   given strategy is computed by dividing the
error obtained by sampling landmarks uniformly without
replacement  Williams   Seeger    with the error of
the given strategy  In contrast to the empirical study by Li
et al    we do not perform any subsampling of the
datasets with less than     instances and compute the
Frobenius norm error using full kernel matrices  On one
larger dataset with more than     instances the memory
requirements were hindering our parallel implementation
and we  therefore  subsampled it to     instances  ctslice dataset  Appendix    By performing our empirical
study on full datasets  we are avoiding   potentially negative
in uence of the subsampling on the effectiveness of the
compared landmark selection strategies  time consumed 

and the accuracy of the approximation error  Following previous empirical studies  Drineas   Mahoney    Kumar
et al    Li et al    we evaluate the goodness of
landmark selection strategies using the Gaussian kernel and
repeat all experiments   times to account for their nondeterministic nature  We refer to       as the bandwidth
of the Gaussian kernel and in order to determine the bandwidth interval we sample     instances and compute their
squared pairwise distances  From these distances we take
the inverse of   and   percentile values as the right and
left endpoints  To force the kernel matrix to have   large
number of signi cant spectral components       the Gaussian kernel matrix approaches to the identity matrix  we
require the right bandwidth endpoint to be at least   From
the logspace of the determined interval we choose   evenly
spaced values as bandwidth parameters  In the remainder of
the section  we summarize our  ndings with   datasets and
provide the complete empirical study in Appendix   
In the  rst set of experiments  we    the approximation
rank and evaluate the performance of the landmark selection
strategies while varying the bandwidth of the Gaussian kernel  Similar to Kumar et al    we observe that for most
datasets at   standard choice of bandwidth   inverse median
squared pairwise distance between instances   the princi 

 log loglift   aileronskernelKmeans leveragescores uniformsketch KDPP MCsteps kernelKmeans withrestarts leveragescores Kdiagonalsketch KDPP MCsteps log   parkinsons log   elevators log   ujil log   calhousing timeinsecondslogFrobeniuserror   aileronskernelKmeans leveragescores uniformsketch KDPP MCsteps uniformkernelKmeans withrestarts leveragescores Kdiagonalsketch KDPP MCsteps timeinseconds   parkinsons timeinseconds   elevators timeinseconds   ujil timeinseconds   calhousingNystr om Method with Kernel Kmeans  Samples as Landmarks

Figure   The  gure shows the improvement in the lift of the approximation error measured in the Frobenius norm that comes as   result
of the increase in the rank of the approximation  The bandwidth parameter of the Gaussian kernel is set to the inverse of the squared
median pairwise distance between the samples 

pal part of the spectral mass is concentrated at the top  
eigenvalues and we set the approximation rank      
Figure   demonstrates the effectiveness of evaluated selection strategies as the bandwidth varies  More precisely  as
the log value of the bandwidth parameter approaches to
zero the kernel matrix is close to being the identity matrix 
thus  hindering lowrank approximations  In contrast to this 
as the bandwidth value gets smaller the spectrum mass becomes concentrated in   small number of eigenvalues and
lowrank approximations are more accurate  Overall  the
kernel Kmeans  sampling scheme performs the best
across all   datasets  It is the best performing method on
  of the considered datasets and   competitive alternative
on the remaining ones  The improvement over alternative
approaches is especially evident on datasets ailerons and
elevators  The approximate leverage score sampling is on
most datasets competitive and achieves   signi cantly better
approximation than alternatives on the dataset calhousing 
The approximations for the KDPP Nystr om method with
    MC steps are more accurate than that with    
steps  The low lift values for that method seem to indicate
that the approach moves rather slowly away from the initial state sampled uniformly at random  This choice of the
initial state is the main difference in the experimental setup
compared to the study by Li et al    where the KDPP
chain was initialized with Kmeans  sampling scheme 
Figure   depicts the runtime costs incurred by each of the
sampling schemes 
It is evident that compared to other
methods the cost of running the KDPP chain with uniformly chosen initial state for more than     steps results
in   huge runtime cost without an appropriate reward in the
accuracy  From this  gure it is also evident that the approximate leverage score and kernel Kmeans  sampling are
ef cient and run in approximately the same time apart from
the dataset ujil  see also ctslice  Appendix    This dataset
has more than   attributes and it is time consuming for
the kernel Kmeans  sampling scheme  our implementation does not cache precompute the kernel matrix  While

on such large dimensional datasets the kernel Kmeans 
sampling scheme is not as fast as the approximate leverage score sampling  it is still the best performing landmark
selection technique in terms of the accuracy 
In Figure   we summarize the results of the second experiment where we compare the improvement in the approximation achieved by each of the methods as the rank of the
approximation is increased from   to   The results indicate that the kernel Kmeans  sampling achieves the best
increase in the lift of the approximation error  On most of
the datasets the approximate leverage score sampling is competitive  That method also performs much better than the
KDPP Nystr om approach initialized via uniform sampling 
As the landmark subspace captured by our approach depends
on the gap between the eigenvalues and that of the approximate leverage score sampling on the size of the sketch
matrix  we also evaluate the strategies in   setting where
  landmarks are selected in order to make   rank      
approximation of the kernel matrix  Similar to the  rst experiment  we    the rank to       and in addition to the
already discussed case with       we consider cases with
      ln   and       ln    Due to space restrictions  the
details of this experiment are provided in Appendix    The
results indicate that there is barely any difference between
the lift curves for the kernel Kmeans  sampling with
      ln   and       ln   landmarks  In their empirical
study  Gittens   Mahoney   have observed that for uniformly selected landmarks          and          ln   
the average rank   approximation errors are within      
of the optimal rank   approximation errors  Thus  based
on that and our empirical results it seems suf cient to take
  ln   landmarks for an accurate rank   approximation
of the kernel matrix  Moreover  the gain in the accuracy for
our approach with       ln   landmarks comes with only
  slight increase in the time taken to select the landmarks 
Across all datasets  the proposed sampling scheme is the
best performing landmark selection technique in this setting 

 rankloglift   aileronskernelKmeans leveragescores uniformsketch KDPP MCsteps kernelKmeans withrestarts leveragescores Kdiagonalsketch KDPP MCsteps rank   parkinsons rank   elevators rank   ujil rank   calhousingNystr om Method with Kernel Kmeans  Samples as Landmarks

Acknowledgment  We are grateful for access to the University of Nottingham High Performance Computing Facility 

References
Alaoui  Ahmed    and Mahoney  Michael    Fast randomized
kernel ridge regression with statistical guarantees  In Advances
in Neural Information Processing Systems    

Aloise  Daniel  Deshpande  Amit  Hansen  Pierre  and Popat 
Preyas  NPhardness of Euclidean sumof squares clustering 
Machine Learning   

Arthur  David and Vassilvitskii  Sergei  Kmeans  The advantages of careful seeding  In Proceedings of the Eighteenth
Annual ACMSIAM Symposium on Discrete Algorithms   

Kulis  Brian  Sustik    aty as  and Dhillon  Inderjit  Learning lowrank kernel matrices  In Proceedings of the  rd International
Conference on Machine Learning   

Kumar  Sanjiv  Mohri  Mehryar  and Talwalkar  Ameet  Sampling
methods for the Nystr om method  Journal of Machine Learning
Research   

Li  Chengtao  Jegelka  Stefanie  and Sra  Suvrit  Fast DPP sampling for Nystr om with application to kernel methods 
In
Proceedings of the  rd International Conference on Machine
Learning   

Lloyd  Stuart  Least squares quantization in PCM  IEEE Transac 

tions on Information Theory   

  utkepohl  Helmut  Handbook of Matrices  Wiley   

Bach  Francis    Sharp analysis of lowrank kernel matrix approximations  In Proceedings of the  th Annual Conference on
Learning Theory   

Nystr om  Evert   

 Uber die praktische Au osung von Integralgleichungen mit Anwendungen auf Randwertaufgaben  Acta
Mathematica   

Sch olkopf  Bernhard and Smola  Alexander    Learning with
kernels  Support vector machines  regularization  optimization 
and beyond  MIT Press   

Smola  Alexander    and Sch olkopf  Bernhard  Sparse greedy
matrix approximation for machine learning  In Proceedings of
the  th International Conference on Machine Learning   

Wahba  Grace  Spline models for observational data  SIAM   

Weidmann  Joachim  Linear operators in Hilbert spaces  Springer 

Verlag   

Williams  Christopher       and Seeger  Matthias  Using the
Nystr om method to speed up kernel machines  In Advances in
Neural Information Processing Systems    

Xu  Qin  Ding  Chris  Liu  Jinpei  and Luo  Bin  PCAguided

search for Kmeans  Pattern Recognition Letters   

Yang  Tianbao  Li  Yufeng  Mahdavi  Mehrdad  Jin  Rong  and
Zhou  ZhiHua  Nystr om method vs random Fourier features 
  theoretical and empirical comparison  In Advances in Neural
Information Processing Systems    

Zhang  Kai  Tsang  Ivor    and Kwok  James   

Improved
Nystr om lowrank approximation and error analysis  In Proceedings of the  th International Conference on Machine Learning 
 

Bach  Francis    and Jordan  Michael    Predictive lowrank
decomposition for kernel methods  In Proceedings of the  nd
International Conference on Machine Learning   

Belabbas  Mohamed    and Wolfe  Patrick    Spectral methods
in machine learning  New strategies for very large datasets 
Proceedings of the National Academy of Sciences of the USA 
 

Boothby  William    An introduction to differentiable manifolds

and Riemannian geometry  Academic Press   

Boutsidis  Christos  Drineas  Petros  and Mahoney  Michael   
Unsupervised feature selection for the Kmeans clustering problem  In Advances in Neural Information Processing Systems  
 

Burges  Christopher       Geometry and invariance in kernel based

methods  In Advances in Kernel Methods  MIT Press   

Ding  Chris and He  Xiaofeng  Kmeans clustering via principal
component analysis  In Proceedings of the  st International
Conference on Machine Learning   

Drineas  Petros and Mahoney  Michael    On the Nystr om method
for approximating   Gram matrix for improved kernelbased
learning  Journal of Machine Learning Research   

Drineas  Petros  Kannan  Ravi  and Mahoney  Michael    Fast
Monte Carlo algorithms for matrices II  Computing   lowrank
approximation to   matrix  SIAM Journal on Computing   

Fine  Shai and Scheinberg  Katya  Ef cient SVM training using
lowrank kernel representations  Journal of Machine Learning
Research   

Gittens  Alex and Mahoney  Michael    Revisiting the Nystr om
method for improved largescale machine learning  Journal
Machine Learning Research   

Golub  Gene    and van Loan  Charles    Matrix Computations 

Johns Hopkins University Press   

Kanungo  Tapas  Mount  David    Netanyahu  Nathan    Piatko 
Christine    Silverman  Ruth  and Wu  Angela      local
search approximation algorithm for Kmeans clustering 
In
Proceedings of the Eighteenth Annual Symposium on Computational Geometry   

