Differentiable Imitation Learning for Sequential Prediction

Deeply AggreVaTeD 

Wen Sun   Arun Venkatraman   Geoffrey    Gordon   Byron Boots      Andrew Bagnell  

Abstract

Recently  researchers have demonstrated stateof theart performance on sequential prediction
problems using deep neural networks and Reinforcement Learning  RL  For some of these
problems  oracles that can demonstrate good performance may be available during training  but
are not used by plain RL methods  To take advantage of this extra information  we propose AggreVaTeD  an extension of the Imitation Learning  IL  approach of Ross   Bagnell  
AggreVaTeD allows us to use expressive differentiable policy representations such as deep
networks  while leveraging trainingtime oracles to achieve faster and more accurate solutions with less training data  Speci cally  we
present two gradient procedures that can learn
neural network policies for several problems  including   sequential prediction task and several
highdimensional robotics control problems  We
also provide   comprehensive theoretical study
of IL that demonstrates that we can expect up
to exponentiallylower sample complexity for
learning with AggreVaTeD than with plain RL
algorithms  Our results and theory indicate that
IL  and AggreVaTeD in particular  can be   more
effective strategy for sequential prediction than
plain RL 

  Introduction
  fundamental challenge in arti cial intelligence  robotics 
and language processing is sequential prediction  to reason 
plan  and make   sequence of predictions or decisions to
minimize accumulated cost  achieve   longterm goal  or

 Robotics Institute  Carnegie Mellon University  USA
 Machine Learning Department  Carnegie Mellon University 
USA  College of Computing  Georgia Institute of Technology 
USA  Correspondence to  Wen Sun  wensun cs cmu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

optimize for   loss acquired only after many predictions 
Although conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems  researchers are beginning to utilize Reinforcement Learning  RL  methods to achieve even
higher performance  Ranzato et al    Bahdanau et al 
  Li et al    In sequential prediction tasks  future predictions often depend on the history of previous
predictions  thus    poor prediction early in the sequence
can lead to high loss  cost  for future predictions  Viewing
the predictor as   policy   deep RL algorithms are able to
reason about the future accumulated cost in sequential prediction problems  These approaches have dramatically advanced the stateof theart on   number of problems including highdimensional robotics control tasks and video and
board games  Schulman et al    Silver et al   
In contrast with general reinforcement learning methods 
imitation learning and related sequential prediction algorithms such as SEARN  Daum   III et al    DaD
 Venkatraman et al    AggreVaTe  Ross   Bagnell 
  and LOLS  Chang et al      reduce the sequential prediction problems to supervised learning by leveraging    near  optimal costto go oracle that can be queried
for the next  near best prediction at any point during training  Speci cally  these methods assume access to an oracle that provides an optimal or nearoptimal action and the
future accumulated loss    the socalled costto go  For
robotics control problems  this oracle may be   human expert guiding the robot during the training phase  Abbeel  
Ng    or the policy from an optimal MDP solver  Ross
et al    Kahn et al    Choudhury et al   
that is either too slow to use at test time or leverages information unavailable at test time  For sequential prediction problems  an oracle can be constructed by optimization       beam search  or by   clairvoyant greedy algorithm  Daum   III et al    Ross et al    Rhinehart
et al    Chang et al      that  given the training
data   ground truth  is nearoptimal on the taskspeci   performance metric       cumulative reward  IoU  Unlabeled
Attachment Score  BLEU 

Expert  demonstrator  and oracle are used interchangeably 

Differential Imitation Learning for Sequential Prediction

We stress that the oracle is only required to be available
during training  Therefore  the goal of IL is to learn   policy
  with the help of the oracle      during the training
session  such that   achieves similar or better performance
at test time when the oracle is unavailable  In contrast to
IL  reinforcement learning methods often initialize with  
random policy   or costto go estimate    that may be far
from optimal  The optimal policy  or costto go  must be
found by exploring  often with random actions 
  classic family of IL methods is to collect data from running the demonstrator or oracle and train   regressor or
classi er via supervised learning  These methods  Abbeel
  Ng    Syed et al    Ratliff et al    Ziebart
et al    Finn et al    Ho   Ermon    learn
either   policy   or     from    xedsize dataset precollected from the oracle  Unfortunately  these methods
exhibit   pernicious problem  they require the training and
test data to be sampled from the same distribution  despite
the fact they explicitly change the sample policy during
training  As   result  policies learned by these methods can
fail spectacularly  Ross   Bagnell    Interactive approaches to IL such as SEARN  Daum   III et al   
DAgger  Ross et al    and AggreVaTe  Ross   Bagnell    interleave learning and testing to overcome the
data mismatch issue and  as   result  work well in practical applications  Furthermore  these interactive approaches
can provide strong theoretical guarantees between training
time loss and test time performance through   reduction to
noregret online learning 
In this work  we introduce AggreVaTeD    differentiable
version of AggreVaTe  Aggregate Values to Imitate  Ross
  Bagnell    which allows us to train policies with
ef cient gradient update procedures  AggreVaTeD extends
and scales interactive IL for use in sequential prediction
and challenging continuous robot control tasks  We provide two gradient update procedures    regular gradient
update developed from Online Gradient Descent  OGD 
 Zinkevich    and   natural gradient update  Kakade 
  Bagnell   Schneider    which is closely related to Weighted Majority  WM   Littlestone   Warmuth 
    popular noregret algorithm that enjoys an almost
dimensionfree property  Bubeck et al   
AggreVaTeD leverages the oracle to learn rich polices that
can be represented by complicated nonlinear function approximators  Our experiments with deep neural networks
on various robotics control simulators and on   dependency parsing sequential prediction task show that AggreVaTeD can achieve expertlevel performance and even
superexpert performance when the oracle is suboptimal 
  result rarely achieved by noninteractive IL approaches 
     the regret bound depends on polylog of the dimension

of parameter space 

The differentiable nature of AggreVaTeD additionally allows us to employ Recurrent Neural Network policies      
Long ShortTerm Memory  LSTM   Hochreiter   Schmidhuber    to handle partially observable settings      
observe only partial robot state  Empirical results demonstrate that by leveraging an oracle  IL can learn much faster
than RL 
In addition to providing   set of practical algorithms  we
develop   comprehensive theoretical study of IL on discrete MDPs  We construct an MDP that demonstrates exponentially better sample ef ciency for IL than any RL algorithm  For general discrete MDPs  we provide   regret
upper bound for AggreVaTeD with WM  which shows IL
can learn dramatically faster than RL  We provide   regret
lower bound for any IL algorithm  which demonstrates that
AggreVaTeD with WM is nearoptimal 
To summarize the contributions of this work    AggreVaTeD allows us to handle continuous action spaces and
employ recurrent neural network policies for Partially Observable Markov Decision Processes  POMDPs    understanding IL from   perspective that is related to policy gradient allows us to leverage advances from the wellstudied RL policy gradient literature       gradient variance reduction techniques  ef cient natural gradient computation    we provide   new sample complexity study
of IL and compare to RL  showing that we can expect up to
exponentially lower sample complexity  Our experimental
and theoretical results support the proposition 

Imitation Learning is   more effective strategy
than Reinforcement Learning for sequential prediction with nearoptimal costto go oracles 

  Preliminaries
  Markov Decision Process consists of   set of states  actions  that come from   policy  cost  loss  and   model
that transitions states given actions 
Interestingly  most
sequential prediction problems can be framed in terms
of MDPs  Daum   III et al    The actions are the
learner         RNN    predictions  The state is then the result of all the predictions made so far       the dependency
tree constructed so far or the words translated so far  The
cumulative cost is the performance metric such as  negative  UAS  received at the end  horizon  or after the  nal
prediction  For robotics control problems  the robot   con 
 guration is the state  the controls       joint torques  are
the actions  and the cost is related to achieving   task      
distance walked 
Formally     nitehorizon Markov Decision Process  MDP 
is de ned as                  Here    is   set of   states
and   is   set of   actions  at time step    Pt is the transition
dynamics such that for any st      st       at     

Differential Imitation Learning for Sequential Prediction

Pt st st  at  is the probability of transitioning to state
st  from state st by taking action at at step      is the
cost distribution such that   cost ct at step   is sampled from
Ct st  at  Finally  we denote  ct st  at  as the expected
cost    as the initial distribution of states  and        as
the  nite horizon  max length  of the MDP 
We de ne   stochastic policy   such that for any state
                 where     is   Adimensional
simplex  conditioned on state                outputs the
probability of taking action   at state    The distribution of
trajectories                    aH  sH  is determined by
  and the MDP  and is de ned as

         

 at st Pt st st  at 

HYt 

The distribution of the states at time step    induced by running the policy   until    is de ned  st 
   st      si ai     

 ai si Pi si si  ai 

  Yi 

   

  

     

Note that the summation above can be replaced by an integral if the state or action space is continuous  The average

state distribution        PH
The expected average cost of   policy   can be de ned with
respect to   or    
   
       HXt 
 ct st  at   

           

       

HXt 

    

 

 ct        

We de ne the stateaction value   
for policy   at time step   as 

               costto go 

  

   st  at     ct st  at   

 

  Pt st at     

  

       

where the expectation is taken over the randomness of the
policy   and the MDP 
We de ne   as the expert policy       human demonstrators  search algorithms equipped with groundtruth  and
           as the expert   costto go oracle  We emphasize
that   may not be optimal           arg min   
Throughout the paper  we assume            is known or can
be estimated without bias       by rolling out   starting
from state    applying action    and then following   for
      steps 
When   is represented by   function approximator  we use
the notation   to represent the policy parametrized by
    Rd        In this work we speci cally consider optimizing policies in which the parameter dimension   may
be large  We also consider the partially observable setting
in our experiments  where the policy          ot   

is de ned over the whole history of observations and actions  ot is generated from the hidden state st  We use
both LSTM and Gated Recurrent Unit  GRU   Chung et al 
  based policies where the RNN   hidden states provide   compressed feature of the history  To our best knowledge  this is the  rst time RNNs are employed in an IL
framework to handle partially observable environments 

  Differentiable Imitation Learning
Policy based imitation learning aims to learn   policy  
that approaches the performance of the expert   at test
time when   is no longer available  In order to learn rich
policies such as LSTMs or deep networks  Schulman et al 
  we derive   method related to policy gradients for
imitation learning and sequential prediction  To do this  we
leverage the reduction of IL and sequential prediction to
online learning as shown in  Ross   Bagnell    to learn
policies represented by expressive differentiable function
approximators 
The fundamental idea in Ross   Bagnell   is to use
  noregret online learner to update policies using the following loss function at each episode   

     

 
 

HXt 

   

 

  st 

      st      

 

 

st    

The loss function intuitively encourages the learner to
 nd   policy that minimize the expert   costto go under
the state distribution resulting from the current learned
policy    
Speci cally  Ross   Bagnell   suggest an algorithm named AggreVaTe  Aggregate Values
to Imitate  that uses Followthe Leader  FTL   ShalevShwartz et al   
     
arg min Pn
       where   is   prede ned convex
policy set  When     is strongly convex with respect to
  and       after   iterations AggreVaTe with FTL can
 nd   policy   with 

to update policies 

               ln       

 

      min PN

where       PN
         Note
that        and the above inequality indicates that   can
outperform   when   is not  locally  optimal           
  Our experimental results support this observation 
  simple implementation of AggreVaTe that aggregates the
values  as the name suggests  will require an exact solution
to   batch optimization procedure in each episode  When
  is represented by large  nonlinear function approximators  the arg min procedure generally takes more and more
computation time as   increases  Hence an ef cient incremental update procedure is necessary for the method to
scale 

Differential Imitation Learning for Sequential Prediction

To derive an incremental update procedure  we can take
one of two routes  The  rst route  suggested already by
 Ross   Bagnell    is to update our policy with an
incremental noregret algorithm such as weighted majority
 Littlestone   Warmuth    instead of with   batch algorithm like FTRL  Unfortunately  for rich policy classes
such as deep networks  noregret learning algorithms may
not be available         deep network policy is nonconvex
with respect to its parameters  So instead we propose  
novel second route  we directly differentiate Eq    yielding an update related to policy gradient methods  We work
out the details below  including   novel update rule for IL
based on natural gradients 
Interestingly  the two routes described above yield almost
identical algorithms if our policy class is simple enough 
     for   tabular policy  AggreVaTe with weighted majority yields the natural gradient version of AggreVaTeD described below  And  the two routes yield complementary
theoretical guarantees  the  rst route yields   regret bound
for simpleenough policy classes  while the second route
yields convergence to   local optimum for extremely  exible policy classes 

  Online Gradient Descent
For discrete actions  the gradient of      Eq    with
respect to the parameters   of the policy is

      

 
 

HXt 

 
  
st  

  Xa     st        st    

 
For continuous action spaces  we cannot simply replace the
summation by integration since in practice it is hard to evaluate            for in nitely many    so  instead  we use importance weighting to reformulate     Eq    as

     

 

 
 

HXt 
   
     

 

       
         
     st  at 

 
       
 at st   
 at st     

  
 

HXt 

          

 

 

See Appendix   for the derivation of the above equation 
With this reformulation  the gradient with respect to   is

      

 

 
     

     st  at 

  at st   
 at st     

HXt 
   ln at st          st  at 

 

     
HXt 

The above gradient computation enables   very ef cient
update procedure with online gradient descent       
      nr      where    is the learning rate 

  Policy Updates with Natural Gradient Descent
We derive   natural gradient update procedure for imitation
learning inspired by the success of natural gradient descent
in RL  Kakade    Bagnell   Schneider    Schulman et al    Following  Bagnell   Schneider   
we de ne the Fisher information matrix      using trajectory likelihood 

 
   

      

        log          log         
 
where    log      is the gradient of the log likelihood of the trajectory   which can be computed as
PH
      log at st  Note that this representation is
equivalent to the original Fisher information matrix proposed by  Kakade    Now  we can use Fisher information matrix together with the IL gradient derived in the
previous section  Eq    to compute the natural gradient
as            which yields   natural gradient
update              nI         
Interesting  as we mentioned before  when the given MDP
is discrete and the policy class is in   tabular representation 
AggreVaTe with Weighted Majority  Littlestone   Warmuth    yields an extremely similar update procedure
as AggreVaTeD with natural gradient  Due to space limitation  we defer the detailed similarity between AggreVaTe
with Weighted Majority and AggreVaTeD with natural gradient to Appendix    As Weighted Majority can speed
up online learning       almost dimension free  Bubeck
et al    and AggreVaTe with Weighted Majority enjoys strong theoretical guarantees on the performance of
the learned policy  Ross   Bagnell    this similarity
provides an intuitive explanation why we can expect AggreVaTeD with natural gradient to speed up IL and learn  
high quality policy 

  SampleBased Practical Algorithms
In the previous section  we derived   regular gradient update procedure and   natural gradient update procedure for
IL  Note that all of the computations of gradients and Fisher
information matrices assumed it was possible to exactly
compute expectations including Es    and Ea      In
this section  we provide practical algorithms where we approximate the gradients and Fisher information matrices
using  nite samples collected during policy execution 

  Gradient Estimation and Variance Reduction
We consider an episodic framework where given   policy
   at episode    we roll out      times to collect   trajectories    
      For gradient       we can compute an unbiased estimate

    for            

     si  

    ai  

Differential Imitation Learning for Sequential Prediction

HK

      

       
using    
KXi 
 
KXi 

      

HK

 

    

        si  
 

HXt Xa         si  
HXt 
        si  
 si  
    ln   ai  
 

 

 

  ai  

   

 

for discrete and continuous setting respectively 
When we can compute          we can replace      si  
    
 
by the stateaction advantage function      si  
      
 
     si  
    which leads to the following unt
biased and variancereduced gradient estimation for continuous action setting  Greensmith et al   

             si  

      

 

HK

KXi 

HXt 

    ln   ai  

 

        si  
 si  
 

  ai  

   

 

In fact  we can use any baselines to reduce the variance by
replacing      st  at  by      st  at      st  where   st   
      is   actionindependent function 
Ideally   st 
should be some function approximator that approximates
   st 
In our experiments  we test linear function approximator        wT    which is online learned using   
rollout data 
The Fisher information matrix  Eq    is approximated as 

 

KXi 

    

       

  SnST
   

    log          log       
 
where  for notation simplicity  we denote Sn as       matrix where the     th column is     log      HpK 
Namely the Fisher information matrix is represented by  
sum of   rankone matrices  For large policies represented
by neural networks         and hence         low rank
matrix  One can  nd the descent direction    by solving
            for    using Conjugate
the linear system SnST
Gradient  CG  with    xed number of iterations  which is
equivalent to solving the above linear systems using Partial
Least Squares  Phatak   de Hoog    This approach
is used in TRPO  Schulman et al    The difference is
that our representation of the Fisher matrix is in the form
  and in CG we never need to explicitly compute or
of SnST
store SnST
  which requires    space and time  Instead  we
only compute and store Sn    Kd  and the total computational time is still        The learningrate for natural
   

gradient descent can be chosen as       KL   rT
such that KL                  KL     

  

Algorithm   AggreVaTeD  Differentiable AggreVaTe 
  Input  The given MDP and expert   Learning rate

  Initialize policy  

    Schedule rate                 
learning 

 either random or supervised

 

  for       to   do
  Mixing policies                      
Starting from   roll out by executing    on the
 
given MDP to generate   trajectories    
   
Using    and    
    Eq    Eq    Eq    or CG 
Update                  

 
  end for
  Return  the best hypothesis          on validation 

      compute the descent direction

Figure   The binary tree structure MDP    

  Differentiable Imitation Learning  AggreVaTeD
Summarizing the above discussion  we present the differentiable imitation learning framework AggreVaTeD  in Alg   
At every iteration    the roll out policy    is   mix of the
expert policy   and the current policy     with mixing
rate                at every step  with probability
     picks   and picks    otherwise  This mixing strategy with the decay rate was  rst introduced in  Ross et al 
  for IL  and later on was used in sequence prediction
 Bengio et al    In Line   one can either choose Eq   
or the corresponding variance reduced estimation Eq    to
perform regular gradient descent  and choose CG to perform natural gradient descent  AggreVaTeD is extremely
simple  we do not need to perform any data aggregation
      we do not need to store all      from all previous iterations  the computational complexity of each policy update scales in     
When we use nonlinear function approximators to represent the polices  the analysis of AggreVaTe from  Ross  
Bagnell    will not hold  since the loss function    
is not convex with respect to parameters   Nevertheless 
as we will show in experiments  in practice AggreVaTeD
is still able to learn   policy that is competitive with  and
sometimes superior to  the oracle   performance 

Differential Imitation Learning for Sequential Prediction

  Quantify the Gap  An Analysis of IL vs RL
How much faster can IL learn   good policy than RL  In
this section we quantify the gap on discrete MDPs when IL
can   query for an optimal    or   query for   noisy but
unbiased estimate of    To measure the speed of learning 
we look at the cumulative regret of the entire learning pron          smaller
regret rate indicates faster learning  Throughout this section  we assume the expert   is optimal  We consider
 nitehorizon  episodic IL and RL algorithms 

cess  de ned as RN  PN

  Exponential Gap
We consider an MDP   shown in Fig    which is   depthK binary treestructure with            states and two
actions al  ar  goleft and goright  The transition is deterministic and the initial state     root  is  xed  The cost
for each nonleaf state is zero  the cost for each leaf is      
sampled from   given distribution  possibly different distributions per leaf  Below we show that for    IL can be
exponentially more sample ef cient than RL 
Theorem   For    the regret RN of any  nitehorizon 
episodic RL algorithm is at least 

  RN      pSN  

 

The expectation is with respect to random generation of
cost and internal randomness of the algorithm  However 
for the same MDP    with the access to    we show IL
can learn exponentially faster 
Theorem   For the MDP    AggreVaTe with FTL can
achieve the following regret bound 

RN     ln    

 

Fig    illustrates the intuition behind the theorem  Assume
during the  rst episode  the initial policy   picks the rightmost trajectory  bold black  to explore  We query from
the costto go oracle    at    for al and ar  and learn that
     al         ar  This immediately tells us that
the optimal policy will go left  black arrow  at    Hence
the algorithm does not have to explore the right subtree
 dotted circle 
Next we consider   more dif cult setting where one can
only query for   noisy but unbiased estimate of          by
rolling out    nite number of times  The above halving
argument will not apply since deterministically eliminating
nodes based on noisy estimates might permanently remove
good trajectories  However  IL can still achieve   polylog
regret with respect to    even in the noisy setting 
Theorem   With only access to unbiased estimate of   
for the MDP    AggreVaTeD with WM can achieve the

following regret with probability at least      

RN      ln   pln      pln    

 

The detailed proofs of the above three theorems can be
found in Appendix       respectively 
In summary  for
MDP    IL is is exponentially faster than RL 
  Polynomial Gap and NearOptimality
We next quantify the gap in general discrete MDPs and also
show that AggreVaTeD is nearoptimal  We consider the
harder case where we can only access an unbiased estimate
of       for any   and stateaction pair  The policy   is represented as   set of probability vectors            for all
      and                          
Theorem   With access to unbiased estimates of      
AggreVaTeD with WM achieves the regret upper bound 

RN     HQe

maxpS ln     

 

Here Qe
max is the maximum costto go of the expert  The
total regret shown in Eq    allows us to compare IL algorithms to RL algorithms  For example  the Upper Con 
dence Bound  UCB  based  nearoptimal optimistic RL algorithms from  Jaksch et al    speci cally designed
for ef cient exploration  admit regret    HSpHAN  
leading to   gap of approximately pHAS compared to the
regret bound of imitation learning shown in Eq   
We also provide   lower bound on RN for the       case
which shows the dependencies on         are tight 
Theorem   There exists an MDP     such that  with
only access to unbiased estimates of    any  nitehorizon
episodic imitation learning algorithm must have 

  RN      pS ln      

 

The proofs of the above two theorems regarding general
MDPs can be found in Appendix      In summary for discrete MDPs  one can expect at least   polynomial gap and
  possible exponential gap between IL and RL 

  Experiments
We evaluate our algorithms on robotics simulations from
OpenAI Gym  Brockman et al    and on Handwritten Algebra Dependency Parsing  Duyck   Gordon   
We report reward instead of cost  since OpenAI Gym by
default uses reward and dependency parsing aims to maximize UAS score  As our approach only promises there
If
max       then the expert is no better than   random policy

max is   constant compared to   

Qe
of which the costto go is around    

Here we assume Qe

Differential Imitation Learning for Sequential Prediction

    Cartpole

    Acrobot

    Acrobot  POMDP 

    Hopper

    Walker

Figure   Performance  cumulative reward   on yaxis  versus number of episodes    on xaxis  of AggreVaTeD  blue and green 
experts  red  and RL algorithms  dotted  on different robotics simulators 

exists   policy among all of the learned polices that can
perform as well as the expert  we report the performance
of the best policy so far  max        For regular gradient descent  we use ADAM  Kingma   Ba   
which is    rstorder noregret algorithm  and for natural
gradient  we use CG to compute the descent direction  For
RL we use REINFORCE  Williams    and Truncated
Natural Policy Gradient  TNPG   Duan et al   

  Robotics Simulations
We consider CartPole Balancing  Acrobot Swingup  Hopper and Walker  For generating an expert  similar to previous work  Ho   Ermon    we used   Deep QNetwork
 DQN  to generate    for CartPole and Acrobot       to
simulate the settings where    is available  while using
the publicly available TRPO implementation to generate
  for Hopper and Walker to simulate the settings where
one has to estimate    by MonteCarlo roll outs  

Discrete Action Setting We use   onelayer   hidden units  neural network with ReLu activation functions
to represent the policy   for the Cartpole and Acrobot
benchmarks  The value function    is obtained from the
DQN  Mnih et al    and represented by   multilayer
fully connected neural network  The policy   is initialized with common ReLu neural network initialization techniques  For the scheduling rate     we set all       
namely we did not rollin using the expert   actions during training  We set the number of roll outs       and
horizon       for CartPole and       for Acrobot 
Fig     and    shows the performance averaged over  
random trials of AggreVaTeD with regular gradient descent and natural gradient descent  Note that AggreVaTeD
outperforms the experts  performance signi cantly  Natural gradient surpasses the expert by   in Acrobot and
  in Cartpole  Also  for Acrobot swingup  at horizon       with high probability   randomly initialized
neural network policy won   be able to collect any reward
signals  Hence the improvement rates of REINFORCE and
TNPG are slow  In fact  we observed that for   short horizon such as       REINFORCE and Truncated Natural
Gradient often even fail to improve the policy at all  failed

  times among   trials  On the contrary  AggreVaTeD
does not suffer from the delayed reward signal issue  since
the expert will collect reward signals much faster than  
randomly initialized policy 
Fig     shows the performance of AggreVaTeD with an
LSTM policy   hidden states  in   partially observed
setting where the expert has access to full states but the
learner has access to partial observations  link positions 
RL algorithms did not achieve any improvement while AggreVaTeD still achieved   of the expert   performance 
In Appendix    we provide extra experiments on partial
observable CartPole with GRUbased policies  where we
demonstrate that even in partial observable setting  AggreVaTeD can learn RNN polices that outperform experts 

Continuous Action Setting We test our approaches on
two robotics simulators with continuous actions    the
   Walker and   the Hopper from the MuJoCo physics
simulator  Following the neural network settings described
in Schulman et al    the expert policy   is obtained
from TRPO with one hidden layer   hidden states  which
is the same structure that we use to represent our policies
  We set       and       We initialize   by
collecting   expert demonstrations and then maximize the
likelihood of these demonstrations       supervised learning  We use   linear baseline        wT   for RL and IL 
Fig     and    show the performance averaged over   random trials  Note that AggreVaTeD outperforms the expert
in the Walker by   while achieving   of the expert  
performance in the Hopper problem  After   iterations 
we see that by leveraging the help from experts  AggreVaTeD can achieve much faster improvement rate than the
corresponding RL algorithms  though eventually we can
expect RL to catch up  In Walker  we also tested AggreVaTeD without linear baseline  which still outperforms the
expert but performed slightly worse than AggreVaTeD with
baseline as expected 

  Dependency Parsing on Handwritten Algebra
We consider   sequential prediction problem 
transitionbased dependency parsing for handwritten algebra with raw
image data  Duyck   Gordon    The parsing task

Differential Imitation Learning for Sequential Prediction

ArcEager AggreVaTeD  LSTMs  AggreVaTeD  NN 
Regular
Natural

 
 

 
 

SLRL  LSTMs 
   
 

SLRL NN  RL  LSTMs  RL  NN 
 
 

 
 

 
 

DAgger

SL  LSTMs 

SL  NN 

Random

 

 

 

 

Table   Performance  UAS  of different approaches on handwritten algebra dependency parsing  SL stands for supervised learning using
expert   samples  maximizing the likelihood of expert   actions under the sequences generated by expert itself  SLRL means RL with
initialization using SL  Random stands for the initial performances of random policies  LSTMs and NN  The performance of DAgger
with Kernel SVM is from  Duyck   Gordon   

for algebra is similar to the classic dependency parsing
for natural language  Chang et al      where the problem is modelled in the IL setting and the stateof theart is
achieved by AggreVaTe with FTRL  using Data Aggregation  The additional challenge here is that the inputs are
handwritten algebra symbols in raw images  We directly
learn to predict parse trees from low level image features
 Histogram of Gradient features  HoG  During training 
the expert is constructed using the groundtruth dependencies in training data  The full state   during parsing consists of three data structures  Stack  Buffer and Arcs  which
store raw images of the algebraic symbols  Since the sizes
of stack  buffer and arcs change during parsing    common approach is to featurize the state   by taking the features of the latest three symbols from stack  buffer and arcs
       Chang et al      Hence the problem falls into
the partially observable setting  where the feature   is extracted from state   and only contains partial information
about    The dataset consists of   sets of handwritten
algebra equations  We use   for training    for validation  and   for testing  We include an example of
handwritten algebra equations and its dependency tree in
Appendix    Note that different from robotics simulators
where at every episode one can get fresh data from the simulators  the dataset is  xed and sample ef ciency is critical 
The RNN policy follows the design from  Sutskever et al 
  It consists of two LSTMs  Given   sequence of algebra symbols   the  rst LSTM processes one symbol at
  time and at the end outputs its hidden states and memory         summary of   The second LSTM initializes its
own hidden states and memory using the outputs of the  rst
LSTM  At every parsing step    the second LSTM takes the
current partial observation ot  ot consists of features of the
most recent item from stack  buffer and arcs  as input  and
uses its internal hidden state and memory to compute the
action distribution       ot      conditioned on history 
We also tested reactive policies constructed as fully connected ReLu neural networks  NN   onelayer with  
hidden states  that directly maps from observation ot to action    where ot uses the most three recent items  We use
variance reduced gradient estimations  which give better
performance in practice  The performance is summarised
in Table   Due to the partial observability of the problem  AggreVaTeD with   LSTM policy achieves signi 
cantly better UAS scores compared to the NN reactive pol 

    Validation

    Test

Figure   UAS  yaxis  versus number of iterations    on xaxis 
of AggreVaTeD with LSTM policy  blue and green  experts  red 
on validation set and test set for ArcEager Parsing 
icy and DAgger with   Kernelized SVM  Duyck   Gordon 
  Also AggreVaTeD with   LSTM policy achieves
  of optimal expert   performance  Fig    shows the improvement rate of regular gradient and natural gradient on
both validation set and test set  Overall we observe that
both methods have similar performance  Natural gradient
achieves   better UAS score in validation and converges
slightly faster on the test set but also achieves   lower UAS
score on test set 

  Conclusion
We introduced AggreVaTeD    differentiable imitation
learning algorithm which trains neural network policies for
sequential prediction tasks such as continuous robot control
and dependency parsing on raw image data  We showed
that in theory and in practice IL can learn much faster
than RL with access to optimal costto go oracles  The IL
learned policies were able to achieve expert and sometimes
superexpert levels of performance in both fully observable
and partially observable settings  The theoretical and experimental results suggest that IL is signi cantly more effective than RL for sequential prediction with near optimal
costto go oracles 

Acknowledgement
This research was supported in part by ONR    
 

References
Abbeel  Pieter and Ng  Andrew    Apprenticeship learning via
inverse reinforcement learning  In ICML  pp    ACM   

Bagnell    Andrew and Schneider  Jeff  Covariant policy search 

Differential Imitation Learning for Sequential Prediction

IJCAI   

Bahdanau  Dzmitry  Brakel  Philemon  Xu  Kelvin  Goyal 
Anirudh  Lowe  Ryan  Pineau  Joelle  Courville  Aaron  and
Bengio  Yoshua  An actorcritic algorithm for sequence prediction  arXiv preprint arXiv   

Bengio  Samy  Vinyals  Oriol  Jaitly  Navdeep  and Shazeer 
Noam  Scheduled sampling for sequence prediction with recurrent neural networks  In NIPS   

Brockman  Greg  Cheung  Vicki  Pettersson  Ludwig  Schneider 
Jonas  Schulman  John  Tang  Jie  and Zaremba  Wojciech 
Openai gym  arXiv preprint arXiv   

Bubeck    ebastien  CesaBianchi  Nicolo  et al  Regret analysis
of stochastic and nonstochastic multiarmed bandit problems 
Foundations and Trends    in Machine Learning   

Bubeck    ebastien et al  Convex optimization  Algorithms and
complexity  Foundations and Trends    in Machine Learning 
 

Chang  KaiWei  He  He  Daum   III  Hal  and Langford 
John  Learning to search for dependencies  arXiv preprint
arXiv     

Chang  Kaiwei  Krishnamurthy  Akshay  Agarwal  Alekh 
Daume  Hal  and Langford  John  Learning to search better
than your teacher  In ICML     

Choudhury  Sanjiban  Kapoor  Ashish  Ranade  Gireeja  Scherer 
Sebastian  and Dey  Debadeepta  Adaptive information gathering via imitation learning  RSS   

Chung  Junyoung  Gulcehre  Caglar  Cho  KyungHyun  and
Empirical evaluation of gated recurrent
arXiv preprint

Bengio  Yoshua 
neural networks on sequence modeling 
arXiv   

Daum   III  Hal  Langford  John  and Marcu  Daniel  Search 

based structured prediction  Machine learning   

Duan  Yan  Chen  Xi  Houthooft  Rein  Schulman  John  and
Abbeel  Pieter  Benchmarking deep reinforcement learning for
continuous control  In ICML   

Duyck  James   and Gordon  Geoffrey    Predicting structure in
handwritten algebra data from low level features  Data Analysis Project Report  MLD  CMU   

Finn  Chelsea  Levine  Sergey  and Abbeel  Pieter  Guided cost
learning  Deep inverse optimal control via policy optimization 
In ICML   

Greensmith  Evan  Bartlett  Peter    and Baxter  Jonathan  Variance reduction techniques for gradient estimates in reinforcement learning  JMLR   

Ho  Jonathan and Ermon  Stefano  Generative adversarial imita 

tion learning  In NIPS   

Kahn  Gregory  Zhang  Tianhao  Levine  Sergey  and Abbeel 
Pieter  Plato  Policy learning using adaptive trajectory optimization  arXiv preprint arXiv   

Kakade  Sham    natural policy gradient  NIPS   

Kakade  Sham and Langford  John  Approximately optimal ap 

proximate reinforcement learning  In ICML   

Kingma  Diederik and Ba  Jimmy  Adam    method for stochas 

tic optimization  arXiv preprint arXiv   

Li  Jiwei  Monroe  Will  Ritter  Alan  Galley  Michel  Gao  Jianfeng  and Jurafsky  Dan  Deep reinforcement learning for dialogue generation  arXiv preprint arXiv   

Littlestone  Nick and Warmuth  Manfred    The weighted majority algorithm  Information and computation   
 

Mnih  Volodymyr et al  Humanlevel control through deep rein 

forcement learning  Nature   

Phatak  Aloke and de Hoog  Frank  Exploiting the connection
between pls  lanczos methods and conjugate gradients  alternative proofs of some properties of pls  Journal of Chemometrics 
 

Ranzato  Marc Aurelio  Chopra  Sumit  Auli  Michael  and
Zaremba  Wojciech  Sequence level training with recurrent
neural networks  ICLR    

Ratliff  Nathan    Bagnell    Andrew  and Zinkevich  Martin   

Maximum margin planning  In ICML   

Rhinehart  Nicholas  Zhou  Jiaji  Hebert  Martial  and Bagnell 
  Andrew  Visual chunking    list prediction framework for
regionbased object detection  In ICRA  IEEE   

Ross  St ephane and Bagnell     Andrew  Ef cient reductions for

imitation learning  In AISTATS  pp     

Ross  Stephane and Bagnell    Andrew  Reinforcement and imitation learning via interactive noregret learning  arXiv preprint
arXiv   

Ross  St ephane  Gordon  Geoffrey    and Bagnell    Andrew   
reduction of imitation learning and structured prediction to noregret online learning  In AISTATS   

Ross  Stephane  Zhou  Jiaji  Yue  Yisong  Dey  Debadeepta  and
Bagnell  Drew  Learning policies for contextual submodular
prediction  In ICML   

Schulman 

John  Levine  Sergey  Abbeel  Pieter 

Jordan 
Michael    and Moritz  Philipp  Trust region policy optimization  In ICML  pp     

ShalevShwartz  Shai et al  Online learning and online convex
optimization  Foundations and Trends    in Machine Learning 
 

Hochreiter  Sepp and Schmidhuber    urgen  Long shortterm

memory  Neural computation     

Silver  David et al  Mastering the game of go with deep neural

networks and tree search  Nature   

Jaksch  Thomas  Ortner  Ronald  and Auer  Peter  Nearoptimal

regret bounds for reinforcement learning  JMLR   

Sutskever  Ilya  Vinyals  Oriol  and Le  Quoc    Sequence to

sequence learning with neural networks  In NIPS   

Differential Imitation Learning for Sequential Prediction

Syed  Umar  Bowling  Michael  and Schapire  Robert    Appren 

ticeship learning using linear programming  In ICML   

Venkatraman  Arun  Hebert  Martial  and Bagnell    Andrew  Improving multistep prediction of learned time series models 
AAAI   

Williams  Ronald    Simple statistical gradientfollowing algorithms for connectionist reinforcement learning  Machine
learning   

Ziebart  Brian    Maas  Andrew    Bagnell    Andrew  and Dey 
Anind    Maximum entropy inverse reinforcement learning 
In AAAI   

Zinkevich  Martin  Online Convex Programming and Generalized

In nitesimal Gradient Ascent  In ICML   

