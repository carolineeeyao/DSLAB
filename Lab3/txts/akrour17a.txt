Local Bayesian Optimization of Motor Skills

Riad Akrour   Dmitry Sorokin   Jan Peters     Gerhard Neumann    

Abstract

Bayesian optimization is renowned for its sample ef ciency but its application to higher dimensional tasks is impeded by its focus on global
optimization  To scale to higher dimensional
problems  we leverage the sample ef ciency of
Bayesian optimization in   local context  The
optimization of the acquisition function is restricted to the vicinity of   Gaussian search distribution which is moved towards high value areas of the objective  The proposed informationtheoretic update of the search distribution results
in   Bayesian interpretation of local stochastic search  the search distribution encodes prior
knowledge on the optimum   location and is
weighted at each iteration by the likelihood of
this location   optimality  We demonstrate the
effectiveness of our algorithm on several benchmark objective functions as well as   continuous
robotic task in which an informative prior is obtained by imitation learning 

  Introduction
Recent advances in deep reinforcement learning  supported
by the ability of generating and processing large amounts
of data  allowed impressive achievements such as playing
Atari at human level  Mnih et al    or mastering the
game of Go  Silver et al    In robotics however  sample complexity is paramount as sample generation on physical systems cannot be sped up and can cause wear and
damage to the robot when excessive  Kober et al   
Relying on   simulator to carry the learning will inevitably
result in   reality gap  since mechanical forces such as
stiction are hard to accurately model  However    policy
learned in   simulated environment can still be valuable
provided the availability of   sample ef cient algorithm to

 CLAS IAS  TU Darmstadt  Darmstadt  Germany  Max
Planck Institute for Intelligent Systems    ubingen  Germany  LCAS  University of Lincoln  Lincoln  United Kingdom  Correspondence to  Riad Akrour  riad robotlearning de 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

carry an additional optimization phase on the physical system 
Bayesian optimization is best known as   blackbox global
optimizer  Brochu et al    Shahriari et al    It
was shown to be ef cient for several function landscapes
 Jones    real world scenarios such as the automatic
tuning of machine learning algorithms  Bergstra et al 
  Snoek et al    Feurer et al    or robotics and
control  Lizotte et al    Wilson et al    Calandra
et al    and several of its variants have convergence
guaranties to   global optimum  Vazquez   Bect   
Bull    Its ef ciency stems from two key principles   
probabilistic modeling of the objective function and   sampling procedure that fully exploits this model  However 
as the dimensionality of the task increases  nonstationarity
effects of the objective or the noise function  see Shahriari et al    Sec       for   discussion of these effects  are exacerbated  rendering the modeling of the objective function challenging  An additional dif culty stemming from the increase in dimensionality is the tendency
of Bayesian optimization to overexplore  which was experimentally observed in      Brochu et al    Several
recent approaches trying to scale Bayesian optimization to
higher dimensions assume additional structure of the objective function  In Snoek et al    it is assumed that stationarity of the objective function can be recovered through
the use of   parametric family of mappings  While it is assumed that the objective function has   lower intrinsic dimension in Djolonga et al    Wang et al    can
be decomposable into   sum of lower dimensional functions in Kandasamy et al    or   combination of both
hypothesis in Li et al   
In this paper  we assume prior knowledge on the location
of the optimum given by an initial solution and   con 
 dence on the optimality thereof and leverage Bayesian
optimization in   local manner to improve over this solution  We are especially interested in the application of our
algorithm to the optimization of motor skills since    evaluating the policy return is expensive on physical systems and
will likely dominate the computational budget of the optimization process  as such  sample ef cient algorithms such
as Bayesian optimization are desirable ii  robotics applications are typically high dimensional and global optimization might be prohibitively expensive iii  an initial solution

Local Bayesian Optimization of Motor Skills

Algorithm   Local Bayesian Optimization of Motor Skills

    stepsize   en 

Input  Initial policy          
tropy reduction rate  
Output  Policy   
Fit  Gaussian cid pn from local samples of   cid 
for       to   do

Optimize        arg min gn   
Bayesian Update     cid cid     cid 
    cid 
Evaluate  xn from local samples of   cid 
Dn   Dn     xn  yn 
 

end for

   Sec   
 Sec   
   Sec   
 Sec   

can often be obtained through the use of imitation learning  Argall et al    or by   preliminary optimization
on   surrogate model such as   simulator 

  Related work
Our algorithm can be seen as   local stochastic search
algorithm akin to Covariance Matrix Adaptation  CMAES   Hansen   Ostermeier    crossentropy  Mannor et al    or MOdelbased Relative Entropy
 MORE   Abdolmaleki et al    Local stochastic
search algorithms typically maintain   Gaussian search distribution from which samples are generated  the objective
function is evaluated and the search distribution is updated 
As in Bayesian optimization  they are of particular use
when the gradient of the objective function is unknown 
Their use as   blackbox optimization routine is gaining
popularity in the machine learning community       in reinforcement learning  Thomas et al    or even for hyperparameter tuning  Bergstra et al    and the optimization of the acquisition function  Wang et al    of global
Bayesian optimization 
Our algorithm shares the same general structure as local stochastic search algorithms and additionally learns
   probabilistic  model of the objective function  Modeling the objective function was already explored in the
stochastic search literature    surrogate function is learned
in  Loshchilov et al    using SVMRank  and is optimized using CMAES for   few iterations  yielding an
update of the search distribution without requiring additional function evaluations  While in MORE  Abdolmaleki
et al      local quadratic approximation of the objective function yields the new mean and covariance of the
Gaussian search distribution upon an informationtheoretic
update  Unlike these algorithms  we do not optimize the
learned  probabilistic  model  but derive from it      
  cid    the probability of   being optimal  Our search distribution is then updated such as to minimize the KullbackLeibler  KL  divergence to         cid    Compared to
these surrogate assisted local stochastic search algorithms
 Loshchilov et al    Abdolmaleki et al    the
transformation of the optimization landscape  minimizing
the KLdivergence to         cid    instead of the objective function  facilitates learning of the surrogate model by
lowering the variance in poorly performing regions  as illustrated in Fig   
To approximate         cid    we rely on   probabilistic
modeling of the objective function and to select the next
point to sample we locally optimize an acquisition function  As such  our algorithm can also be seen as Bayesian
optimization where the usual box constraint is moved towards   high value area of the objective function to restrict
exploration 

In reinforcement learning  probabilistic modeling was used
to      learn   transition model  Deisenroth   Rasmussen 
  or the policy gradient  Ghavamzadeh et al   
with Gaussian processes  Closer to our work  the use of
an adaptive box constraint was explored in Bayesian optimization to ensure   safe optimization of   robot controller
 Berkenkamp et al    Englert   Toussaint   
Considering safety is crucial for motor skill learning on
physical systems to prevent the evaluation of  dangerous 
parameters  Both approaches restrict exploration to an initial safe region of the parameter space that is incrementally
expanded using additional problem assumptions  Without
such assumptions our algorithm cannot guarantee safety
but its local nature is expected to dampen the potential risk
of global Bayesian optimization 

  Local Bayesian optimization
Let     Rd  cid    be an objective function  For example
      can be the expected reward of   robot controller parameterized by     Rd  We assume that the algorithm only
has access to noisy evaluations               where    
     
    is Gaussian noise of unknown deviation     The
algorithm will produce   sequence               xN   yN  
 cid 
of parameterevaluation pairs and the goal is to minimize
       cid    yi for some global
the cumulative regret  
 
maximizer   cid  of    The cumulative regret emphasizes
the inherent cost in evaluating   bad parameter  potentially
causing wear and damage to the robot 
Prior knowledge on an optimum   location   cid  is given
to the algorithm by   Gaussian distribution    
     
    In what follows we will indistinctly refer to
  as   search distribution or   policy following the terminology of the stochastic search and reinforcement learning
 RL  communities  In an RL context  an informative prior
can often be obtained from human generated data or from  
simulator  Speci cally  we assume that the mean   of  
is obtained by imitation learning if nearoptimal demonstrations are available or by   preliminary optimization on
  less accurate but inexpensive model of the system dy 

Local Bayesian Optimization of Motor Skills

    Objective function   

    Search distribution 

    Quad  approx  of   

    GP approx  of   

    Gaussian approx 
    cid   

of

Figure   The bene ts of minimizing the KL divergence to     cid    instead of maximizing   local approximation of the objective
function       The objective function   has two modes illustrated by the red  higher mode  and blue  lower mode  stars     The search
distribution   draws samples in the area of both modes  The samples and their evaluation are stored in       The quadratic approximation
of    minimizing the empirical mean squared error on    captures variations of   even in low value areas and result in   poor orientation
for the search distribution update as illustrated by the model   optimum  magenta star in     On the other side  the Gaussian process
approximation of    shown in     is con dent enough in its approximation for     cid    to sample mainly around the higher mode  red
star  see     As   result  the Gaussian approximation of     cid    only focuses on high value areas and results in   better direction for
the search distribution update than the quadratic model of   

namics  Whereas   is   hyperparameter of the algorithm 
manually set in our experiments  and expressing the con 
dence in the optimality of  
The search distribution    is updated by solving the optimization problem formally de ned in Sec    The
objective of the optimization problem is to minimize the
KL divergence between    and         cid Dn  the probability of   cid  being optimal according to the data set of
parameterevaluation pairs Dn  Solving this problem results in   Bayesian update  as shown in Alg    where the
prior       on the optimality of   is weighted by the likelihood         cid Dn  of   being optimal according to Dn 
Letting the likelihood         cid Dn  be denoted by   cid 
    

the  rst step of the algorithm is to    cid pn    Gaussian approx 

imation of   cid 
   Sec    Subsequently    dual function is
optimized  Sec    to make sure that the search distribution moves slowly towards   cid  as new evaluations are collected  Modulating the Bayesian update with the dual parameters   and   is important since Dn is initially empty
  not initially informative  Finally    new evaluations
and   cid 
of   is requested by selecting xn from the previously generated samples of   cid 
The next subsections give   detailed presentation of both
the search distribution update and the sampling procedure
from   cid 
  

  and the process is iterated 

  Search distribution update

The search distribution in our algorithm is updated such as
to minimize the KL divergence between    and   cid 
   The
resulting optimization problem is closely related to the one
solved by the MORE algorithm  Abdolmaleki et al   
In the next subsections  we will  rst formalize our search
distribution update before brie   describing the search distribution update of MORE and showing how their deriva 

tions can be used to obtain our search distribution update 

  THE OPTIMIZATION PROBLEM

The search distribution is updated such that its KL divern is minimized  Since future evaluations of  
gence          cid 
will be performed around the updated search distribution 
it becomes critical to control the change of distribution between iterations by constraining the aforementioned minimization problem  These constraints will ensure that the
exploration is not reduced too fast or that the mean is not
moved too quickly from the initial solution   The resulting optimization problem is given by

arg min

 

subject to

KL   cid    cid 
  
KL   cid     
         

   
   

 
 

where KL    cid        cid       log     
gence between   and   and         cid       log     dx

     dx is the KL diver 

is the entropy of    The hyperparameters   and   respectively bound the change in distribution and the reduction in
entropy between successive iterations 
The use of the KL divergence to constrain the update is
widespread in the reinforcement learning community  Peters et al    Schulman et al    When the search
distributions   and    are of Gaussian form  the KL divergence in Eq    is impacted by three factors  On one side 
by the change in entropy between the two distributions 
having   direct impact on the exploration rate  On the other
side  by the displacement of the mean and the rotation of
the covariance matrix not impacting the exploration rate 
To better control the exploration  we choose to decouple

 Local Bayesian Optimization of Motor Skills

the reduction in entropy from the KL constraint 
It was
shown in  Abdolmaleki et al    that the additional entropy constraint can lead to signi cantly better solutions at
the expense of   slower start 
The optimization problem de ned in this section is closely
related to the one solved by MORE  In fact  when the inequality   is replaced by the equality constraint       
       for both algorithms then the two problems coincide  while only   small modi cation of the dual function
is necessary otherwise  For the sake of clarity and to keep
the paper selfcontained  we will brie   introduce MORE
before showing how we can reuse their derivation of the
search distribution update in our algorithm 

  MODELBASED RELATIVE ENTROPY SEARCH

MORE  Abdolmaleki et al    is   local stochastic
search algorithm where the search distribution       is updated by solving the following constrained problem

 cid 

arg max

 

subject to

        dx

KL   cid     
         

   
   

 
 

An analytic solution of the problem is obtained by locally
approximating   with the quadratic model

Rn         
 

xT Rnx   xT rn   rn 

learned by linear regression from the data set Dn  Letting
the search distribution                     at iteration
  be parameterized by the mean    and covariance matrix
    the aforementioned optimization problem yields the
closed form update where the new mean and covariance
are given by

      cid     cid cid cid 

 
       cid     cid   

    Rn

 cid cid 

 cid   

 
 

       rn

 cid   

where  cid  and  cid  are the Lagrange multipliers of the constraints   and   respectively  and are obtained by minimizing the dual function gn    by gradient descent  Abdolmaleki et al   
As can be seen in Eq    the new covariance matrix is  
tradeoff between the old covariance and the local curvature of the objective function   where the tradeoff parameters are computed in order to satisfy both constraints
of the optimization problem  As such  it is appropriate to
use the covariance matrix of the search distribution in the
kernel function for the local approximation of   when using GP regression 
We additionally de ne MORE with equality constraint as
  variant of MORE where the inequality constraint in  

is replaced with the equality constraint            
  forcing the reduction in entropy at each iteration to be
exactly   This modi cation will not change the shape of
the update but only the Lagrange multipliers  that can be
obtained by simply alleviating the constraint       in the
minimization of gn   

  OPTIMIZATION PROBLEMS  EQUIVALENCE

We now show that the optimization problem in our algorithm can be phrased as the optimization problem solved
by MORE for the equality entropy constraint  while only  
small modi cation of the dual minimization is required for
the inequality entropy constraint  The equivalence of the
optimization problems will allow us to use Eq    and   to
update our search distribution 
Proposition   The optimization problem in Sec    can
be reduced to the optimization problem in   for the objective function     log   cid 
  when both problems enforce an
exact entropy reduction constraint on  

Proof  We  rst rephrase the problem in Sec    as the
maximization over   of

 KL   cid    cid 

             

where we switched the sign of the KL divergence term and
added the constant term          These modi cations
will not change the value of the stationary points of the
Lagrangian  The resulting Lagrangian is

        

    log   cid 

    dx KL   cid     

                    

with dual variables       and       and where we have
split the term KL   cid    cid 
   into the expected logdensity of
  and the entropy    of     MORE formulation with
  cid 
similar entropy and KL divergence constraints and where
the objective is to maximize the logdensity log   cid 
  yields
the Lagrangian

 cid 

 cid 

  cid       

    log   cid 

    dx KL   cid     
                

Since we have no constraint on   it is easy to see that
the dual variable minimizing the dual of the  rst problem
 cid  and of the second  MORE  problem  cid  are related by
 cid     cid      and both problems will result in the same
update of  
Intuitively  the minimization of KL   cid    cid 
   can be reduced
to the maximization  in expectation of   of the logdensity

Local Bayesian Optimization of Motor Skills

  because the equality constraint              
log   cid 
annihilates the effect of the additional entropy term   
coming from the KL objective 
From Proposition   and following the derivations in  Abdolmaleki et al    the search distribution     solution of the optimization problem in Sec    is given by

         

 cid 

 cid cid     cid 
  

 

 cid cid   

 

where  cid  and  cid  are the Lagrange multipliers related to the
KL and entropy constraints respectively and minimizing
the dual function gn    We refer the reader to Sec   
in  Abdolmaleki et al    for the de nition of gn   
When the entropy constraint is the inequality in   instead
of an equality  the Lagrange multipliers for our update and
the MORE update may differ  However   cid  and  cid  can still
be obtained by the minimization of the same gn    with
the additional constraint      
Note that the new search distribution     as de ned in
Eq    is not necessarily Gaussian because of the multiplin by   Gaussian
cation by   cid 

distribution cid pn  log cid pn will be   quadratic model and Eq   

   However  by approximating   cid 

and   can be used to obtain   Gaussian    

  Approximating the argmax distribution

Gaussian  cid pn to samples of   cid 

To obtain   closed form update of the Gaussian search distribution in Eq    we will approximate   cid 
  by  tting  
  as shown in Fig      To
   we use Thompson sampling
generate samples from   cid 
 Chapelle   Li    Russo   Roy    from   probabilistic model of the objective function   
The probabilistic model of   follows from both   Gaussian process  GP  prior and   Gaussian likelihood assumption  We use in this paper the squared exponential kernel
kn xi  xj      exp xi   xj    
   xi   xj  with
hyperparameters   and   and    the covariance matrix
of     The resulting model has hyperparameter vector
            where  
  is the noise variance of the
likelihood function as previously de ned  Samples from
  are generated by    sampling   hyperparameter vecp cid 
tor   from the posterior distribution   Dn  using slice
sampling  Murray   Adams    ii  sampling   func 

tion from the GP posterior   cid   Dn    and iii  returning
the argmax of  cid   
The computational complexity of evaluating  cid   is cubical in
inversion  As such  the exact maximization of  cid   can prove
approximating  cid   with   linear function  see for example

the number of requested evaluations as it involves   matrix

to be challenging  Prior work in the literature considered

Hern andezLobato et al    Sec    and globally

maximizing the linear surrogate 
In our local optimization context  we follow   more straightforward approach
by generating samples from     and returning the sample

with maximal value of  cid    The rational behind searching
the argmax of  cid   in the vicinity of    is that samples from
   are likely to have high  cid   value since    is updated such

that the KL divergence          cid 
  is minimized  The repeated process of drawing points from     drawing their
value from the GP posterior and selecting the point with
highest value will constitute   data set   cid 
  containing local
samples from   cid 
  
Once samples from   cid 
  and  cid 
    cid 
mean and covariance of the samples in   cid 

set  cid pn      cid 
  are generated and stored in   cid 
   Because  cid pn
   where  cid 
is Gaussian  log cid pn is quadratic and the search distribution

   we
  are the sample

update in Eq    yields   Gaussian distribution     with
covariance and mean as de ned in Eq    and Eq    respectively with Rn    cid 
 

  and rn    cid 

 cid 
  

 

  Sample generation

The function   is initially evaluated at   point    drawn
from the prior distribution   In subsequent iterations   
point xn is randomly selected from   cid 
   the set of samples

used in the computation of cid pn  Sec   

Experimentally  we noticed that the exploration in our algorithm is heavily in uenced by the centering of the values
 yi  
  in Dn  Three variants of our algorithm are initially
evaluated with different target values of the GP  The target
values are obtained by subtracting from yi either the max
the min or the mean of  yi  
    Since the GP modeling of
  has   zero mean prior  the extreme case where the max
 resp  the min  is subtracted from the data results in an optimistic  resp  pessimistic  exploration strategy considering
that the objective function in unexplored areas have values higher  resp  lower  in expectation than the best  resp 
worst  evaluation so far 

  Experiments
We initially investigate in this section the impact of the
target centering  Sec    on the explorationexploitation
tradeoff of our algorithm  We then compare our algorithm to two stateof theart model based optimizers  the
global Bayesian optimizer and the local ModelBased Relative Entropy Search  Abdolmaleki et al    The algorithms are compared on several continuous function benchmarks as well as   simulated robotics task 
Benchmarks  Variants of our algorithm are  rst compared
on randomly generated smooth   dimensional objective
functions  We then conduct   comparison to the stateof 
theart on the COmparing COntinuous optimisers  COCO 

Local Bayesian Optimization of Motor Skills

testbed on the   functions     to      we refer the reader to
http coco gforge inria fr  for an illustration and the mathematical de nition of each function  We
chose to split the experimentation between the unimodal
and the multimodal categories of the testbed  The unimodal category is representative of the informed initialization hypothesis that only requires local improvements 
While the multimodal category assesses the robustness
of our algorithm to more complex function landscapes 
which can be encountered in practice despite the informed
initialization if        too wide variance  
  is initially set 
We vary the dimension of the COCO functions from   to
  while the robotics task evaluates our algorithm on    
dimensional setting 
Algorithms  In what follows  we will refer to our algorithm as LBayesOpt  We rely on the GPStuff library  Vanhatalo et al    for the GP implementation and the posterior sampling of hyperparameters  We use the BayesOpt
library  MartinezCantin    for global Bayesian optimization with   similar to LBayesOpt squared exponential kernel and MCMC sampling of hyperparameters and
an additional Automatic Relevance Determination step executed every   samples  In the experiments we evaluate
BayesOpt with both Expected Improvement and Thompson Sampling acquisition functions 
In all of the experiments  LBayesOpt and MORE will
share the same initial policy  stepsize   entropy reduction   and will sample ten points per iteration  We choose
to use an equality constraint for the entropy reduction for
both algorithms  As   result  both LBayesOpt and MORE
will have the same entropy at every iteration and any difference in performance will be attributed to   better location of the mean  adaptation of the covariance matrix or
sampling procedure rather than   faster reduction in exploration  In all but the last experiment           while for
the robotics experiment with an initial solution learned by
imitation learning we set   more aggressive step size and
entropy reduction          
Evaluation criterion  The performance metric in RL
is typically given by the average return       

 cid            dx while in Bayesian optimization it is typ 

ically determined by the minimal evaluation min     yi
reached at iteration    When the evaluations are noisy the
minimum evaluation is not   robust performance metric 
nor an appropriate criterion for the algorithm to select the
 cid 
returned optimizer  In order to have   common evaluation
criterion  all the approaches are seen as multiarmed bandit
       cid 
algorithms and we use the cumulative regret  
 
yi as the evaluation criterion  The cumulative regret of
 global  Bayesian optimizers is expected to be asymptotically lower than that of local optimizers as it always  nds
the global maximum given suf ciently many evaluations 

    Sample objective function 

    Cumulative regret 

Figure   Three variants of LBayesOpt  Sec    and MORE
evaluated on   randomly generated objective functions  The min
variant results in   contained exploration and has the lowest cumulative regret during the  rst   function evaluations 

Conversely  tradingoff global optimality for fast local improvements might result in   lower regret for local optimizers when the evaluation budget is moderate 

  Exploration variants

In this  rst set of experiments  we evaluate the different exploration strategies resulting from three different centering
methods of the   values in Dn  We compare these three
variants of LBayesOpt on   randomly generated two dimensional Gaussian mixture objective functions  see Fig 
   for an illustration  We chose these functions as they
are cheap to evaluate  easy to approximate by   GP and
their multimodal nature is appropriate for evaluating the
explorationexploitation tradeoff of the three variants 
As hypothesized in Sec    the cumulative regret in Fig 
   shows that the min variant exhibits the lowest exploration and reduces the regret faster than the other optimizers  Yet  when compared to MORE it manages to converge
to better local optima in   out of the   randomly generated
objectives while MORE converges to   better optimum in
one of the   remaining objectives  Note that MORE manages to decrease the regret faster than our algorithm during
the  rst   evaluations  However  the sampling scheme relying on the Thompson sampling acquisition function and
the convergence to higher modes gives the advantage to the
LBayesOpt variants after the initial   evaluations  In the
remainder of the experimental section only the min variant
of our algorithm will be considered 

  Stateof theart benchmark comparisons

We compare our algorithm to MORE and Bayesian optimization on the COCO testbed  We form two sets each containing   objective functions  The  rst one includes unimodal functions      to     while the second one includes
multimodal function with an adequate      to     and  
weak      to     global structure  Each function has  
global optimum in       where   is the dimension of
the objective function that we vary in the set      

 Local Bayesian Optimization of Motor Skills

    Multimodal objectives  dim    

    Multimodal objectives  dim    

    Multimodal objectives  dim    

    Unimodal objectives  dim    

    Unimodal objectives  dim    

    Unimodal objectives  dim    

Figure   Evaluation on unimodal and multimodal functions of varying dimension of the COCO testbed of four algorithms  LBayesOpt  MORE and global Bayesian optimization using either Expected Improvement  EI  or Thompson Sampling  TS  acquisition
function  Bayesian optimization is   global optimizer and its combination with Thompson sampling is as such   zero regret algorithm 
However  when evaluation budget is moderate     the local optimization performed by LBayesOpt yields faster improvements
than Bayesian optimization when the objective function is unimodal or when the dimensionality of the problem increases 

The bounding box      is provided to Bayesian optimization while for the local stochastic search algorithms
we set the initial distribution to             Note that
this is not an informed initialization and none of the functions had their optimum on the null vector 
Fig    shows the performance of the four algorithms on the
multimodal  top row  and unimodal  bottom row  function sets  On the multimodal set of functions and when
      Bayesian optimization with Thompson sampling
proves to be an extremely ef cient bandit algorithm for
uncovering the highest reward point with   minimal number of evaluations  On the contrary  both local stochastic
search algorithms struggle to improve over the initial performance  Upon closer inspection  this appears to be especially true for functions with weak global structure such
as     We hypothesize that for these highly multimodal
functions  both model based stochastic search algorithms
learn poor quadratic models  when either approximating  
or   cid 
   The performance gap between Bayesian optimization and our algorithm reduces however as the dimensionality of the problem increases 
On the unimodal functions set  our algorithm reduces signi cantly faster the regret than Bayesian optimization  As

the dimension of the objective function increases from
      to       more evaluations are required by
Bayesian optimization to reach our algorithm  Compared
to MORE  and since the objectives are unimodal  the use
of an acquisition function is the main driving factor for the
faster decrease of the regret  Note that even if the functions
are unimodal  both local search algorithms are not necessarily zero if the decrease in entropy is too fast 
Both LBayesOpt and BayesOpt TS rely on the Thompson
sampling acquisition function for selecting the next point
to evaluate  While the acquisition function is maximized
on the full support of the objective in the case of Bayesian
optimization  it is only optimized in the vicinity of the current search distribution by our algorithm  The experiments
on the COCO testbed show that when the function landscape enables the learning of an appropriate update direction for moving the search distribution  the adaptive strategy employed by our algorithm can be more ef cient than
the global search performed by Bayesian optimization 

  Robot ball in the cup

The task   objective is for the Barrett robot arm to swing
the ball upward and place it in the cup  Fig      The

Local Bayesian Optimization of Motor Skills

promising region of the objective function  The constant
reduction of the entropy of the search distribution ensures
that the objective function is not modeled and optimized on
the entirety of its domain  Compared to  global  Bayesian
optimization  we experimentally demonstrated on several
continuous optimization benchmarks that it results in faster
improvements over the initial solution  at the expense of
global optimality  This property is especially useful when
an initial informative solution is available and only requires
to be locally improved 
The computational cost of the search distribution update
in our algorithm is signi cantly higher than most local
stochastic search algorithms  This cost mainly arises from
the full Bayesian treatment of the modeling of the objective
function    If the evaluation of   is cheap    better performance per second is obtained by less expensive stochastic
search algorithms where the additional computational budget can be spent in running additional randomized restarts
of the algorithms  Auger   Hansen    However  if the
optimization cost is dominated by the evaluation of    the
probabilistic modeling proved to be more sample ef cient
on several benchmarks by actively selecting the next point
to evaluate  As   result  when   is expensive to evaluate
our algorithm is expected to have better per second performance than stateof theart stochastic search algorithms 
The search distribution update proposed in this paper is
well founded and results in an interpretable update  At each
iteration the current search distribution is simply weighted
by         cid Dn  the probability of   being optimal
according to the current data set  Future work can further improve the sample ef ciency of our algorithm in at
least three ways  First  if the objective function is upper
bounded and the bound is known  we expect that the integration of an additional constraint               for all  
to lead to   more accurate probabilistic modeling and   better explorationexploitation tradeoff  Secondly  the search
distribution update is phrased as the minimization of the
Iprojection of         cid Dn  which has the property of
focusing on one mode of the distribution  Bishop   
However  the Gaussian approximation of         cid Dn 
can average over multiple modes if the GP is unsure about
which of them is the highest  We expected that   better update direction can be obtained if   clustering algorithm can
detect the highest mode from samples of         cid Dn 
Finally and perharps most interestingly  we expect our algorithm to be able to scale to signi cantly higher dimensional policies in an RL setting if   trajectory data kernel is
used  Wilson et al    Speci cally  distance between
policies can be measured by the similarity of actions taken
in similar states  The local nature of our algorithm will additionally ensure that such similarity is evaluated on states
that are likely to be reached by the evaluated policies 

    Robot ball in   cup 

    Cumulative regret 

Figure   LBayesOpt and MORE locally optimizing an imitation
learning policy  The   DoF robot is controlled by     parameters
policy  LBayesOpt is initially slower but is better at  netuning
the policy later on  Plots are averaged over   runs 

optimization is performed on the   weights of the forcing function of   Dynamical Movement Primitive  DMP 
Ijspeert   Schaal   controlling the   joints of the robot 
The initial forcing function weights   are learned by linear regression from   single demonstrated trajectory that
successfully swings the ball up but where the ball lands at
circa  cm from the cup  We compare the performance
of MORE and LBayesOpt in optimizing the initial policy            using the same hyperparameters  The
challenge of the task  in addition to the dimension of the
action space  stems from the two exploration regimes required by the exploration scheme  While initially   signi 
cant amount of noise needs to be introduced to the parameters to get the ball closer to the cup  successfully getting the
ball in the cup requires   more careful tuning of the forcing
function 
Fig     shows the performance of both MORE and LBayesOpt on the robot ball in   cup task  MORE has  
better initial sample ef ciency and gets the ball closer to
the cup at   faster pace than LBayesOpt  However  the acquisition function based sampling scheme of our algorithm
was more ef cient for discovering parameters that successfully put the ball in the cup and results in   lower regret  averaged over   runs  after   evaluations  The experiment
shows that for such high dimensional tasks  our algorithm
was better at tuning the policy only when the entropy of the
search distribution was signi cantly reduced  This might
be due to the low correlation between Euclidean distance
between parameters and difference in reward  One promising direction for future work in   reinforcement learning
context is to use kernels based on trajectory data distance
instead of parameter distance in euclidian space  Wilson
et al   

  Discussion
The algorithm presented in this paper can be seen as
Bayesian optimization where the usual box constraint is rotated  shrunk and moved at each iteration towards the most

Local Bayesian Optimization of Motor Skills

Acknowledgments
The research leading to these results was funded by
the DFG Project LearnRobotS under the SPP   Autonomous Learning 

References
Abdolmaleki  Abbas  Lioutikov  Rudolf  Peters  Jan   
Lau  Nuno  Pualo Reis  Luis  and Neumann  Gerhard  Modelbased relative entropy stochastic search 
In Advances in Neural Information Processing Systems
 NIPS  pp    Curran Associates  Inc   

Argall  Brenna  Chernova  Sonia  Veloso  Manuela   
and Browning  Brett    survey of robot learning from
demonstration  Robotics and Autonomous Systems   
   

Auger  Anne and Hansen  Nikolaus    restart cma evoluIn IEEE
tion strategy with increasing population size 
Congress on Evolutionary Computation  volume   pp 
  IEEE   

Bergstra  James  Bardenet    emi  Bengio  Yoshua  and
  egl  Bal azs  Algorithms for hyperparameter optimization  In Advances in Neural Information Processing Systems  NIPS  pp     

Berkenkamp  Felix  Schoellig  Angela    and Krause  Andreas  Safe controller optimization for quadrotors with
gaussian processes  In Robotics and Automation  ICRA 
  IEEE International Conference on  pp   
IEEE   

Bishop  Christopher    Pattern Recognition and Machine

Learning  SpringerVerlag New York   

Brochu  Eric  de Freitas  Nando  and Ghosh  Abhijeet 
Active preference learning with discrete choice data 
In Advances in Neural Information Processing Systems
 NIPS  pp     

Brochu  Eric  Cora  Vlad    and de Freitas  Nando   
tutorial on bayesian optimization of expensive cost functions  with application to active user modeling and hierarchical reinforcement learning  CoRR  abs 
 

Bull        Convergence rates of ef cient global optimiza 

tion algorithms     

Calandra  Roberto  Seyfarth  Andr    Peters  Jan  and
Deisenroth  Marc Peter  Bayesian optimization for learning gaits under uncertainty   an experimental comparison
on   dynamic bipedal walker  Ann  Math  Artif  Intell 
   

Chapelle  Olivier and Li  Lihong  An empirical evaluation
of thompson sampling  In Advances in Neural Information Processing Systems  NIPS  pp    Curran
Associates  Inc   

Deisenroth     and Rasmussen     PILCO    ModelBased and DataEf cient Approach to Policy Search  In
International Conference on Machine Learning  ICML 
pp     

Djolonga  Josip  Krause  Andreas  and Cevher  Volkan 
Highdimensional gaussian process bandits  In Advances
in Neural Information Processing Systems  NIPS  pp 
   

Englert  Peter and Toussaint  Marc  Combined optimization and reinforcement learning for manipulations skills 
In Robotics Science and Systems    

Feurer  Matthias  Klein  Aaron  Eggensperger  Katharina 
Springenberg  Jost Tobias  Blum  Manuel  and Hutter 
Frank  Ef cient and robust automated machine learning 
In Advances in Neural Information Processing Systems
 NIPS  pp     

Ghavamzadeh  Mohammad  Engel  Yaakov  and Valko 
Michal  Bayesian policy gradient and actorcritic algorithms  Journal of Machine Learning Research   
   

Hansen  Nikolaus and Ostermeier  Andreas  Completely
derandomized selfadaptation in evolution strategies 
Evolutionary Computation     

Hern andezLobato  Jos   Miguel  Hoffman  Matthew   
and Ghahramani  Zoubin  Predictive entropy search
for ef cient global optimization of blackbox functions 
In Advances in Neural Information Processing Systems
 NIPS  pp     

Ijspeert     and Schaal     Learning Attractor Landscapes
for Learning Motor Primitives  In Advances in Neural
Information Processing Systems    NIPS  MIT Press 
Cambridge  MA   

Jones  Donald      taxonomy of global optimization methods based on response surfaces     Global Optimization 
   

Kandasamy  Kirthevasan  Schneider  Jeff    and   oczos 
Barnab as  High dimensional bayesian optimisation and
bandits via additive models  In International Conference
on Machine Learning  ICML  pp     

Kober     Bagnell     Andrew  and Peters     Reinforcement
learning in robotics    survey  International Journal of
Robotics Research  July  

Local Bayesian Optimization of Motor Skills

Li  ChunLiang  Kandasamy  Kirthevasan    oczos 
Barnab as  and Schneider  Jeff    High dimensional
bayesian optimization via restricted projection pursuit
models  In Proceedings of the  th International Conference on Arti cial Intelligence and Statistics  AISTATS
  Cadiz  Spain  May     pp     

Lizotte  Daniel    Wang  Tao  Bowling  Michael    and
Schuurmans  Dale  Automatic gait optimization with
In IJCAI   Proceedgaussian process regression 
ings of the  th International Joint Conference on Arti 
cial Intelligence  Hyderabad  India  January    
pp     

Loshchilov  Ilya  Schoenauer  Marc  and Sebag  Mich ele 
Bipopulation CMAES agorithms with surrogate modIn Genetic and Evolutionary
els and line searches 
Computation Conference  GECCO   Amsterdam  The
Netherlands  July     Companion Material Proceedings  pp     

Mannor  Shie  Rubinstein  Reuven  and Gat  Yohai  The
Cross Entropy method for Fast Policy Search  In Proceedings of the  th International Conference on Machine Learning   ICML   pp    Washington  DC  USA   

MartinezCantin  Ruben  Bayesopt    bayesian optimization library for nonlinear optimization  experimental design and bandits  Journal of Machine Learning Research
 JMLR    January  
ISSN  
 

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David 
Rusu  Andrei    Veness  Joel  Bellemare  Marc   
Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas   
Ostrovski  Georg  Petersen  Stig  Beattie  Charles  Sadik 
Amir  Antonoglou  Ioannis  King  Helen  Kumaran 
Dharshan  Wierstra  Daan  Legg  Shane  and Hassabis 
Demis  Humanlevel control through deep reinforcement
learning  Nature       

Murray  Iain and Adams  Ryan    Slice sampling covariance hyperparameters of latent gaussian models  In
Advances in Neural Information Processing Systems
 NIPS  pp     

Peters       ulling     and Altun     Relative Entropy PolIn Proceedings of the  th National Conicy Search 
ference on Arti cial Intelligence  AAAI  AAAI Press 
 

Russo  Daniel and Roy  Benjamin Van  Learning to optimize via posterior sampling  Math  Oper  Res   
   

Schulman  John  Levine  Sergey  Jordan  Michael  and
Abbeel  Pieter  Trust Region Policy Optimization  International Conference on Machine Learning  ICML  pp 
   

Shahriari  Bobak  Swersky  Kevin  Wang  Ziyu  Adams 
Ryan    and de Freitas  Nando  Taking the human out of
the loop    review of bayesian optimization  Proceedings of the IEEE     

Silver  David  Huang  Aja  Maddison  Chris    Guez 
Arthur  Sifre  Laurent  van den Driessche  George 
Schrittwieser  Julian  Antonoglou  Ioannis  Panneershelvam  Veda  Lanctot  Marc  Dieleman  Sander 
Grewe  Dominik  Nham  John  Kalchbrenner  Nal 
Sutskever  Ilya  Lillicrap  Timothy  Leach  Madeleine 
Kavukcuoglu  Koray  Graepel  Thore  and Hassabis 
Demis  Mastering the game of Go with deep neural networks and tree search  Nature    January  

Snoek  Jasper  Larochelle  Hugo  and Adams  Ryan   
Practical bayesian optimization of machine learning algorithms  In Advances in Neural Information Processing
Systems  NIPS  pp     

Snoek  Jasper  Swersky  Kevin  Zemel  Richard    and
Adams  Ryan    Input warping for bayesian optimization
of nonstationary functions  In International Conference
on Machine Learning  ICML  pp     

Thomas 

Philip    Theocharous  Georgios 

Ghavamzadeh  Mohammad 
icy improvement 
Machine Learning  ICML  pp     

and
High con dence polIn International Conference on

Vanhatalo  Jarno  Riihim aki  Jaakko  Hartikainen  Jouni 
Jyl anki  Pasi  Tolvanen  Ville  and Vehtari  Aki  Gpstuff 
Bayesian modeling with gaussian processes  Journal of
Machine Learning Research  JMLR   
April   ISSN  

Vazquez     and Bect     Convergence properties of the
expected improvement algorithm with  xed mean and
covariance functions     of Statistical Planning and Inference     

Wang  Ziyu  Hutter  Frank  Zoghi  Masrour  Matheson 
David  and de Freitas  Nando  Bayesian optimization
in   billion dimensions via random embeddings     Artif 
Intell  Res   JAIR     

Wilson  Aaron  Fern  Alan  and Tadepalli  Prasad  Using
trajectory data to improve bayesian optimization for reinforcement learning  Journal of Machine Learning Research     

