Analytical Guarantees on Numerical Precision of Deep Neural Networks

Charbel Sakr Yongjune Kim Naresh Shanbhag

Abstract

The acclaimed successes of neural networks often overshadow their tremendous complexity 
We focus on numerical precision     key parameter de ning the complexity of neural networks 
First  we present theoretical bounds on the accuracy in presence of limited precision 
Interestingly  these bounds can be computed via the
backpropagation algorithm  Hence  by combining our theoretical analysis and the backpropagation algorithm  we are able to readily determine the minimum precision needed to preserve accuracy without having to resort to timeconsuming  xedpoint simulations  We provide
numerical evidence showing how our approach
allows us to maintain high accuracy but with
lower complexity than stateof theart binary networks 

  Introduction
Neural networks have achieved stateof theart accuracy on
many machine learning tasks  AlexNet  Krizhevsky et al 
  had   deep impact   few years ago in the ImageNet
Large Scale Visual Recognition Challenge  ILSVRC  and
triggered intensive research efforts on deep neural networks  Recently  ResNet  He et al    has outperformed humans in recognition tasks 
These networks have very high computational complexity 
instance  AlexNet has   million parameters and   neurons  Krizhevsky et al   
Its convolutional
layers alone require   million
multiplyaccumulates  MACs  per       image   
MACs pixel  and   million weights  Chen et al   
Deepface   network involves more than   million parameters  Taigman et al    ResNet is    layer

For

The authors are with the University of Illinois at UrbanaIL   USA 
Champaign      Main St  Urabna 
Correspondence
Charbel Sakr  sakr illinois edu 
Yongjune Kim  yongjune illinois edu  Naresh Shanbhag
 shanbhag illinois edu 

to 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

deep residual network  This high complexity of deep
neural networks prevents its deployment on energy and
resourceconstrained platforms such as mobile devices and
autonomous platforms 

  Related Work

One of the most effective approaches for reducing resource
utilization is to implement  xedpoint neural networks  As
mentioned in  Lin et al      there are two approaches
for designing  xedpoint neural networks    directly train
   xedpoint neural network  and   quantize   pretrained
 oatingpoint neural network to obtain    xedpoint network 
As an example of  xedpoint training  Gupta et al   
showed that  bit  xedpoint representation incurs little
accuracy degradation by using stochastic rounding    more
aggressive approach is to design binary networks such as
Kim   Smaragdis   which used bitwise operations to
replace the arithmetic operations and Rastegari et al   
which explored optimal binarization schemes  BinaryConnect  Courbariaux et al    trained networks using
binary weights while BinaryNet  Hubara et al     
trained networks with binary weights and activations 
Although these  xedpoint training approaches make it
possible to design  xedpoint neural networks achieving
excellent accuracy  training based on  xedpoint arithmetic
is generally harder than  oatingpoint training since the optimization is done in   discrete space 
Hence  in this paper  we focus on the second approach
that quantizes   pretrained  oatingpointing network to  
 xedpoint network  This approach leverages the extensive work in training stateof theart  oatingpoint neural
networks such as dropout  Srivastava et al    maxout
 Goodfellow et al    networkin network  Lin et al 
  and residual learning  He et al    to name  
few  In this approach  proper precision needs to be determined after training to reduce complexity while minimizing the accuracy loss  In  Hwang   Sung    exhaustive search is performed to determine   suitable precision
allocation  Recently  Lin et al      offered an analytical
solution for nonuniform bit precision based on the signalto quantizationnoise ratio  SQNR  However  the use of
nonuniform quantization step sizes at each layer is dif 

Analytical Guarantees on Numerical Precision of Deep Neural Networks

cult to implement as it requires multiple variable precision
arithmetic units 
In addition to  xedpoint
implementation  many approaches have been proposed to lower the complexity of
deep neural networks in terms of the number of arithmetic
operations  Han et al    employs   threestep training
method to identify important connections  prune the unimportant ones  and retrain on the pruned network  Zhang
et al    replaces the original convolutional layers by
smaller sequential layers to reduce computations  These
approaches are complementary to our technique of quantizing   pretrained  oatingpoint neural network into    xedpoint one 
In this paper  we obtain analytical bounds on the accuracy of  xedpoint networks that are obtained by quantizing
  conventionally trained  oatingpoint network  Furthermore  by de ning meaningful measures of    xedpoint
network   hardware complexity viz 
computational and
representational costs  we develop   principled approach to
precision assignment using these bounds in order to minimize these complexity measures 

  Contributions

Our contributions are both theoretical and practical  We
summarize our main contributions 

  We derive theoretical bounds on the misclassi cation
rate in presence of limited precision and thus determine analytically how accuracy and precision tradeoff with each other 
  Employing the theoretical bounds and the backpropagation algorithm  we show that proper precision
assignments can be readily determined while maintaining accuracy close to  oatingpoint networks 
  We analytically determine which of weights or activations need more precision  and we show that typically the precision requirements of weights are greater
than those of activations for fullyconnected networks
and are similar to each other for networks with shared
weights such as convolutional neural networks 
  We introduce computational and representational
costs as meaningful metrics to evaluate the complexity
of neural networks under  xed precision assignment 
  We validate our  ndings on the MNIST and CIFAR 
datasets demonstrating the ease with which  xedpoint networks with complexity smaller than stateof theart binary networks can be derived from pretrained  oatingpoint networks with minimal loss in
accuracy 

It is worth mentioning that our proposed method is general
and can be applied to every class of neural networks such as
multilayer perceptrons and convolutional neural networks 

  FixedPoint Neural Networks
  Accuracy of FixedPoint and FloatingPoint

Networks

For   given  oatingpoint neural network and its  xedpoint counterpart we de ne    the  oatingpoint error
probability pe       Pr   Yf    cid      where  Yf   is the output
of the  oatingpoint network and   is the true label    the
 xedpoint error probability pe       Pr   Yf    cid      where
 Yf   is the output of the  xedpoint network    the mismatch probability between  xedpoint and  oatingpoint
pm   Pr   Yf    cid   Yf    Observe that 
pe       pe       pm

 

The righthand side represents the worst case of having no
overlap between misclassi ed samples and samples whose
predicted labels are in error due to quantization  We provide   formal proof of   in the supplementary section 
Note that pe     is   quantity of interest as it characterizes
the accuracy of the  xedpoint system  We employ pm as
  proxy to pe     because it brings in the effects of quantization into the picture as opposed to pe     which solely
depends on the algorithm  This observation was made in
 Sakr et al    and allowed for an analytical characterization of linear classi ers as   function of precision 

  FixedPoint Quantization

The study of  xedpoint systems and algorithms is well established in the context of signal processing and communication systems  Shanbhag      popular example is the
least mean square  LMS  algorithm for which bounds on
precision requirements for input  weights  and updates have
been derived  Goel   Shanbhag    In such analyses 
it is standard practice  Caraiscos   Liu    to assume
all signed quantities lie in     and all unsigned quantities lie in     Of course  activations and weights can be
designed to satisfy this assumption during training    Bbit  xedpoint number af   would be related to its  oatingpoint value   as follows 

af         qa

 

 cid   

where qa is the quantization noise which is modeled as
an independent uniform random variable distributed over

 cid  where         is the quantization step

     

 

 Caraiscos   Liu   

  Complexity in FixedPoint

We argue that the complexity of    xedpoint system has
two aspects  computational and representational costs  In
what follows  we consider activations and weights to be
quantized to BA and BW bits  respectively 

Analytical Guarantees on Numerical Precision of Deep Neural Networks

The computational cost is   measure of the computational
resources utilized for generating   single decision  and is
de ned as the number of   bit full adders    As    full
adder is   canonical building block of arithmetic units  We
assume arithmetic operations are executed using the commonly used ripple carry adder  Knauer    and BaughWooley multiplier  Baugh   Wooley    architectures
designed using   As  Consequently  the number of   As
used to compute   Ddimensional dot product of activations and weights is  Lin et al     
DBABW         BA   BW    cid log   cid       
Hence  an important aspect of the computational cost of  
dot product is that it is an increasing function of the product
of activation precision  BA  weight precision  BW   and
dimension    
We de ne the representational cost as the total number of
bits needed to represent all network parameters       both
activations and weights  This cost is   measure of the storage complexity and communications costs associated with
data movement  The total representational cost of    xedpoint network is 

    BA       BW

 
bits  where   and   are the index sets of all activations
and weights in the network  respectively  Observe that the
representational cost is linear in activation and weight precisions as compared to the computational cost 
Equations       illustrate that  though computational and
representational costs are not independent  they are different  Together  they describe the implementation costs associated with   network  We shall employ both when evaluating the complexity of  xedpoint networks 

  Setup

Here  we establish notation  Let us consider neural networks deployed on   Mclass classi cation task  For
  given input  the network would typically have   numerical outputs  zi  
   and the decision would be     
zi  Each numerical output is   function of
arg max
weights and activations in the network 

   

zi      ah     wh      

 

for                  where ah denotes the activation indexed
by   and wh denotes the weight indexed by    When activations and weights are quantized to BA and BW bits  respectively  the output zi is corrupted by quantization noise
qzi so that 

zi   qzi      ah   qah     wh   qwh      

 

 

      

where qah and qwh are the quantization noise terms of the
activation ah and weight wh  respectively  Here   qah    
are independent uniformly distributed random variables

on cid    
distributed random variables on cid    

 cid  and  qwh     are independent uniformly
 cid  with     

 BA  and       BW  
In quantization noise analysis  it is standard to ignore crossproducts of quantization noise terms as their contribution is
negligible  Therefore  using Taylor   theorem  we express
the total quantization noise at the output of the  xedpoint
network as 

      

 

 cid 

   

qzi  

 cid 

   

qah

 zi
 ah

 

qwh

 zi
 wh

 

 

Note that the derivatives in   are obtained as part of the
backpropagation algorithm  Thus  using our results  it
is possible to estimate the precision requirements of deep
neural networks during training itself  As will be shown
later  this requires one additional backpropagation iteration to be executed after the weights have converged 

  Bounds on Mismatch Probability
  Second Order Bound

We present our  rst result  It is an analytical upper bound
on the mismatch probability pm between    xedpoint neural network and its  oatingpoint counterpart 
Theorem   Given BA and BW   the mismatch probability pm between    xedpoint network and its  oatingpoint
counterpart is upper bounded as follows 

pm    
 
 

 

    cid 

  
  cid   Yf  

 

 
 
 

 

   

 

 Ah

 
 cid cid cid cid 
 cid cid cid cid   Zi    Yf  

 cid cid cid cid   Zi    Yf  
 cid 
 Zi      Yf  
    cid 
 cid 
 Zi      Yf  

   

 

 

 wh

  
  cid   Yf  

 cid cid cid cid 

 

   

where expectations are taken over   random input and
 Ah       Zi  

   and  Yf   are thus random variables 

Proof  The detailed proof can be found in the supplementary section  Here  we provide the main idea and the intuition behind the proof 
The

 cid  for any pair of outputs zi

Pr cid zi   qzi   zj   qzj

evaluating

and zj where zj   zi  Equivalently  we need to evaluate

proof

heart

lies

the

of

in

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Pr cid qzi   qzj   zj   zi

 cid  But from   we have 

qzi   qzj  

 zi   zj 

 ah

 

qah

 cid 

   

 cid 

   

 zi   zj 

 wh

 

qwh

 

 

having

symmetric

linear
qzi   qzj

combination of quanis   zero mean
distribu 

In   we have
 
tization noise terms 
random variable
tion 
 
Chebyshev   inequality  Indeed  from   the variance of
qzi   qzj is given by 

This means that Pr cid qzi   qzj   zj   zi
 cid   
  Pr cid qzi   qzj     zj   zi cid  which allows us to use
 cid cid cid cid 
 cid cid cid cid   zi   zj 
 cid cid cid cid 
 cid cid cid cid   zi   zj 
 cid 
Pr cid zi   qzi   zj   qzj
 cid cid cid 
 cid cid cid   zi zj  
 cid cid cid   zi zj  
   

 cid 

 cid 
 cid cid cid 

 cid 

 cid 

 
 
 

 
 
 

so that

   

 

   

   

   

 wh

 ah

 wh

 

 

 ah

   
 
 zi   zj 

 

 

As explained in the supplementary section  it is possible to
obtain to   from   using standard probabilistic arguments 

  and  

Before proceeding  we point out that the two expectations in   are taken over   random input but the weights
 wh     are frozen after training and are hence deterministic 
Several observations are to be made  First notice that the
    This
mismatch probability pm increases with  
is to be expected as smaller precision leads to more mismatch  Theorem   says   little bit more 
the mismatch
probability decreases exponentially with precision  because
      BA  and       BW  
Note that the quantities in the expectations in   can be
obtained as part of   standard backpropagation procedure 
Indeed  once the weights are frozen  it is enough to perform one forward pass on an estimation set  which should
have statistically signi cant cardinality  record the numerical outputs  perform one backward pass and probe all relevant derivatives  Thus    can be readily computed 
Another practical aspect of Theorem   is that this operation needs to be done only once as these quantities do not
depend on precision  Once they are determined  for any
given precision assignment  we simply evaluate   and
combine it with   to obtain an estimate  upper bound  on
the accuracy of the  xedpoint instance  This way the precision necessary to achieve   speci   mismatch probability is obtained from   trained  oatingpoint network  This

clearly highlights the gains in practicality of our analytical
approach over   trialand error based search 
Finally    reveals   very interesting aspect of the tradeoff
between activation precision BA and weight precision BW  
We rewrite   as 

where

and

pm    

AEA    

  EW

  
  cid   Yf  

    cid 
    cid 

  
  cid   Yf  

 Ah

 cid cid cid cid   Zi    Yf  
 cid cid cid cid   Zi    Yf  

 cid 
   
 Zi      Yf  
 cid 
   
 Zi      Yf  

 wh

 

 

 

 

 cid cid cid cid 
 cid cid cid cid 

 

 
   

EA    

EW    

The  rst term in   characterizes the impact of quantizing
activations on the overall accuracy while the second characterizes that of weight quantization  It might be the case
that one of the two terms dominates the sum depending on
the values of EA and EW   This means that either the activations or the weights are assigned more precision than
necessary  An intuitive  rst step to ef ciently get   smaller
upper bound is to make the two terms of comparable order 
  EW which is
That can be made by setting  
equivalent to

AEA    

BA   BW   round

log 

 

 cid 

 cid 

 cid  EA

EW

where round  denotes the rounding operation  This is an
effective way of taking care of one of the two degrees of
freedom introduced by  
  natural question to ask would be which of EA and EW
is typically larger  That is to say  to whom  activations or
weights  should one assign more precision  In deep neural networks  there are more weights than activations   
trend particularly observed in deep networks with most layers fully connected  This trend  though not as pronounced 
is also observed in networks with shared weights  such as
convolutional neural networks  However  there exist   few
counterexamples such as the networks in  Hubara et al 
    and  Hubara et al      It is thus reasonable to
expect EW   EA  and consequently the precision requirements of weights will  in general  be more than those of
activations 
One way to interpret   is to consider minimizing the upper bound in   subject to BA BW     for some constant

Analytical Guarantees on Numerical Precision of Deep Neural Networks

   Indeed  it can be shown that   would be   necessary
condition of the corresponding solution  This is an application of the arithmeticgeometric mean inequality  Effectively    is of particular interest when considering computational cost which increases as   function of the product
of both precisions  see Section  

  Tighter Bound

We present   tighter upper bound on pm based on the Chernoff bound 
Theorem   Given BA and BW   the mismatch probability pm between    xedpoint network and its  oatingpoint
counterpart is upper bounded as follows 

pm    

        Yf    

       Yf   

 

       Yf   

 

 

 

where  for    cid    

    cid 
 cid 

  
  cid   Yf  

 cid 

        

      
Ah

 

  
 

      
Ah

   
 Zi   Zj 

 Ah

 cid 

 Zi   Zj 
   

 cid 

 cid   

      
wh
 Zi   Zj 

 

 cid 
 cid 

        
wh

 

  
 
             
Ah
             
Ah

sinh

sinh

             
wh
             
wh

 cid 
 cid 

   

   

 wh

 

 

       

 

 

       

 

 

 cid 
 cid 
 cid 

and

         

      
Zj   Zi

 

Proof  Again  we leave the technical details for the supplementary section  Here we also provide the main idea and
intuition 
As

 cid    Pr cid qzi   qzj   zj   zi

Pr cid zi   qzi   zj   qzj

in Theorem   we shall

focus on evaluating

 cid  for

any pair of outputs zi and zj where zj   zi  The key
difference here is that we will use the Chernoff bound in
order to account for the complete quantization noise statistics  Indeed  letting     zj   zi  we have 

Pr cid qzi   qzj     cid      tvE cid 
et qzi qzj  cid 

sinh  tda   

 cid 

 

et qzi qzj  cid 
 cid 

for any       We show that 

  cid 

sinh  tdw   

   

tda  

   

tdw  

 zi zj  

and dw       
 

 zi zj  

 wh

  This

 ah

where da       
 
yields 

Pr cid qzi   qzj     cid 
    tv  cid 

sinh  tda   

   

tda  

 cid 

   

sinh  tdw   

tdw  

 

 

We show that the righthand side is minimized over positive values of   when 

 cid 
     da     cid 

  

   

     dw     

Substituting this value of   into   and using standard
probabilistic arguments  we obtain  

 

 

       Yf   

The  rst observation to be made is that Theorem   indicates
that  on average  pm is upper bounded by an exponentially
decaying function of the quantity       Yf    for all    cid   Yf   up
to   correction factor        Yf   
  This correction factor is   product of terms typically centered around    each
      for small    On the other
term is of the form sinh   
hand        Yf    by de nition  is the ratio of the excess con 
 dence the  oatingpoint network has in the label  Yf   over
the total quantization noise variance re ected at the output 
           Yf    is the SQNR  Hence  Theorem   states that the
tolerance of   neural network to quantization is  on average  exponentially decaying with the SQNR at its output 
In terms of precision  Theorem   states that pm is bounded
by   double exponentially decaying function of precision
 that is an exponential function of an exponential function 
Note how this bound is tighter than that of Theorem  
This double exponential relationship between accuracy and
precision is not too surprising when one considers the problem of binary hypothesis testing under additive Gaussian
noise  Blahut    scenario  In this scenario  it is wellknown that the probability of error is an exponentially decaying function of the signalto noise ratio  SNR  in the
highSNR regime  Theorem   points out   similar relationship between accuracy and precision but it does so using
rudimentary probability principles without relying on highSNR approximations 
While Theorem   is much tighter than Theorem   theoretically  it is not as convenient to use  In order to use Theorem
  one has to perform   forwardbackward pass and select
relevant quantities and apply   for each choice of BA
and BW   However    lot of information      
the derivatives  can be reused at each run  and so the runs may be
lumped into one forwardbackward pass  In   sense  the
complexity of computing the bound in Theorem   lies between the evaluation of   and the complicated conventional trialand error based search 
We now illustrate the applications of these bounds 

Analytical Guarantees on Numerical Precision of Deep Neural Networks

   

 cid  EW

   

EA

    FX Sim denotes  xed point simulations 

Figure   Validity of bounds for MNIST when      BW   BA and     BW   BA     as dictated by    EA     and
EW     so that log 
  Simulation Results
We conduct numerical simulations to illustrate both the validity and usefulness of the analysis developed in the previous section  We show how it is possible to reduce precision
in an aggressive yet principled manner  We present results
on two popular datasets  MNIST and CIFAR  The metrics we address are threefold 

It
  epochs using   dropout   epochs overall 
appears from the original dropout work  Srivastava et al 
  that the typical   dropout fraction works best for
very wide multilayer perceptrons  MLPs    to  
hidden units  For this reason  we chose to experiment with
smaller dropout fractions 
The only preprocessing done is to scale the inputs between
  and   We used ReLU activations with the subtle addition of   right recti er for values larger than    as discussed in Section   The resulting activation is also called
  hard sigmoid  We also clipped the weights to lie in    
at each iteration  The resulting test error we obtained in
 oatingpoint is  
Figure   illustrates the validity of our analysis 
Indeed 
both bounds  based on Theorems       successfully upper bound the test error obtained through  xedpoint simulations  Figure       demonstrates the utility of   Indeed  setting BW   BA allows us to reduce the precision
to about   or   bits before the accuracy start degrading  In
addition  under these conditions we found EA     and
    Thus  setting
EW     so that log 
BW   BA     as dictated by   allows for more aggressive precision reduction  Activation precision BA can
now be reduced to about   or   bits before the accuracy degrades  To compute the bounds  we used an estimation set
of   random samples from the dataset 
We compare our results with SQ which used    
  architecture on  bit  xedpoint activations and
weights    stochastic rounding scheme was used to compensate for quantization  We also compare our results with
BN with             architecture on
binary quantities    stochastic rounding scheme was also
used during training 
Table   shows some comparisons with related works in
terms of accuracy  computational cost  and representational

 cid  EW

EA

  Accuracy measured in terms of test error 
  Computational cost measured in    As  see Section
  Representational cost measured in bits  see Section

    was used to compute    As per MAC 

    was used 

We compare our results to similar works conducting similar experiments    the work on  xedpoint training with
stochastic quantization  SQ   Gupta et al    and   BinaryNet  BN   Hubara et al     

  DNN on MNIST

First  we conduct simulations on the MNIST dataset for
handwritten character recognition  LeCun et al   
The dataset consists of    training samples and    test
samples  Each sample consists of an image and   label  Images are of size       pixels representing   handwritten
digit between   and   Labels take the value of the corresponding digit 
In this  rst experiment  we chose an architecture of    
                     hidden layers  each of  
units  We  rst trained the network in  oatingpoint using
the backpropagation algorithm  We used   batch size of
  and   learning rate of   with   decay rate of  
per epoch  We restore the learning rate every   epochs 
the decay rate makes the learning rate vary between  
and   We train the  rst   epochs using   dropout 
the second   epochs using   dropout  and the third

   bits       Test errorAB   bits         CDTest errorAnalytical Guarantees on Numerical Precision of Deep Neural Networks

Precision Assignment

Test error  

Computational
Cost     As 

Representational
Cost   bits 

Floatingpoint

   
   
   
   

SQ      Gupta et al   
BN      Hubara et al     

 
 
 
 
 
 
 

   
 
 
 
 
 
 

   
 
 
 
 
 
 

Table   Results for MNIST  Comparison of accuracy  computational cost  and representational cost with stateof theart
related works  Chosen precision assignments are obtained from Figure  

cost  For comparison  we selected four notable design options from Figures        
   Smallest  BA  BW   such that BW   BA and
In this case

   Smallest  BA  BW   such that BW   BA and
In this case

pm     as bounded by Theorem  
 BA  BW        
pm     as bounded by Theorem  
 BA  BW        
   Smallest  BA  BW   such that BW   BA     as dictated by   and pm     as bounded by Theorem
  In this case  BA  BW        
   Smallest  BA  BW   such that BW   BA     as dictated by   and pm     as bounded by Theorem
  In this case  BA  BW        

As can be seen in Table   the accuracy is similar across
all design options including the results reported by SQ
and BN  Interestingly  for all four design options  our network has   smaller computational cost than BN  In addition  SQ   computational cost is about   that of BN
      The greatest reduction in computational
cost is obtained for   precision assignment of     corresponding to     and   reduction compared to
BN       and SQ       respectively 
The corresponding test error rate is of   Similar
trends are observed for representational costs  Again  our
four designs have smaller representational cost than even
BN  BN itself has   smaller representational cost than
SQ       Note that   precision assignment of    
yields   and   smaller representational costs than
BN       and SQ       respectively  The
corresponding test error rate is  
The fact that we are able to achieve lesser computational
and representational costs than BN while maintaining similar accuracy highlights two important points  First  the
width of   network severely impacts its complexity  We
made our network four times as narrow as BN   and still
managed to use eight times as many bits per parameter
without exceeding BN   complexity  Second  our results illustrate the strength of numbering systems  speci cally  the

Figure   Validity of bounds for CIFAR when BW   BA
which is also dictated by    EA     and EW  
    FX Sim denotes  xed
  so that log 
point simulations 

EA

 cid  EW

strength of  xedpoint representations  Our results indicate
that   correct and meaningful multibit representation of
parameters is better in both complexity and accuracy than
   bit unstructured allocation 

  CNN on CIFAR  

We conduct   similar experiment on the CIFAR  dataset
 Krizhevsky   Hinton    The dataset consists of
   color images each representing airplanes  automobiles  birds  cats  deers  dogs  frogs  horses  ships  and
trucks     of these images constitute the training set  and
the    remaining are for testing  SQ   architecture on
this dataset is   simple one  three convolutional layers  interleaved by max pooling layers  The output of the  nal
pooling layer is fed to    way softmax output layer  The
reported accuracy using  bit  xedpoint arithmetic is  
  test error  BN   architecture is   much wider and
deeper architecture based on VGG  Simonyan   Zisserman    The reported accuracy of the binary network
is an impressive   which is of benchmarking quality
even for full precision networks 
We adopt   similar architecture as SQ  but leverage re 

   bits       Test errorABAnalytical Guarantees on Numerical Precision of Deep Neural Networks

Precision Assignment

Test error  

Computational
Cost     As 

Representational
Cost   bits 

Floatingpoint

   
   

SQ      Gupta et al   
BN      Hubara et al     

 
 
 
 
 

   
 
 
 
 

   
 
 
 
 

Table   Results for CIFAR  Comparison of accuracy  computational cost  and representational cost with stateof theart
related works  Chosen precision assignments are obtained from Figure  

cent advances in convolutional neural networks  CNNs 
research  It has been shown that adding networks within
convolutional layers  in the  Network in Network  sense 
as described in  Lin et al    signi cantly enhances
accuracy  while not incurring much complexity overhead 
Hence  we replace SQ   architecture by   deep one which
we describe as                                
                        
  where    denotes       kernels     denotes      
kernels  they emulate the networks in networks       
denotes       max pooling  and     denotes fully connected components  As is customary for this dataset  we
apply zerophase component analysis  ZCA  whitening to
the data before training  Because this dataset is   challenging one  we  rst  netune the hyperparameters  learning
rate  weight decay rate  and momentum  then train for  
epochs  The best accuracy we reach in  oating point using
this  layer deep network is  
Figure   shows the results of our  xedpoint simulation and
analysis  Note that  while both bounds from Theorems  
and   still successfully upper bound the test error  these are
not as tight as in our MNIST experiment  Furthermore  in
this case    dictates keeping BW   BA as EA    
    The fact that
and EW     so that log 
EW   EA is expected as there are typically more weights
than activations in   neural network  However  note that in
this case the contrast between EW and EA is not as sharp
as in our MNIST experiment  This is mainly due to the
higher weight to activation ratio in fully connected DNNs
than in CNNs 
We again select two design options 
   Smallest  BA  BW   such that BW   BA and
In this case

 cid  EW

EA

pm     as bounded by Theorem  
 BA  BW        
pm     as bounded by Theorem  
 BA  BW        

   Smallest  BA  BW   such that BW   BA and
In this case

Table   indicates that BN is the most accurate with  
test error  Interestingly  it has lesser computational cost but
more representational cost than SQ  This is due to the dependence of the computational cost on the product of BA

and BW   The least complex network is ours when setting
 BA  BW         and its test error is   which
is already   large improvement on SQ in spite of having
smaller computational and representational costs  This network is also less complex than that of BN 
The main take away here is that CNNs are quite different
from fully connected DNNs when it comes to precision requirements  Furthermore  from Table   we observe that
BN achieves the least test error  It seems that this better
accuracy is due to its greater representational power rather
than its computational power  BN   representational cost
is much higher than the others as opposed to its computational cost 

  Conclusion
In this paper we analyzed the quantization tolerance of neural networks  We used our analysis to ef ciently reduce
weight and activation precisions while maintaining similar
 delity as the  oatingpoint initiation  Speci cally  we obtained bounds on the mismatch probability between    xedpoint network and its  oatingpoint counterpart in terms
of precision  We showed that   neural network   accuracy
degradation due to quantization decreases double exponentially as   function of precision  Our analysis provides  
straightforward method to obtain an upper bound on the
network   error probability as   function of precision  We
used these results on real datasets to minimize the computational and representational costs of    xedpoint network
while maintaining accuracy 
Our work addresses the general problem of resource constrained machine learning  One take away is that it is imperative to understand the tradeoffs between accuracy and
complexity  In our work  we used precision as   parameter to analytically characterize this tradeoff  Nevertheless 
additional aspects of complexity in neural networks such as
their structure and their sparsity can also be accounted for 
In fact  more work can be done in that regard  Our work
may be viewed as    rst step in developing   uni ed and
principled framework to understand complexity vs  accuracy tradeoffs in deep neural networks and other machine
learning algorithms 

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Acknowledgment
This work was supported in part by Systems on Nanoscale
Information fabriCs  SONIC  one of the six SRC STARnet
Centers  sponsored by MARCO and DARPA 

References
Baugh  Charles   and Wooley  Bruce      two   comIEEE
plement parallel array multiplication algorithm 
Transactions on Computers     

Blahut  Richard    Fast algorithms for signal processing 

Cambridge University Press   

Caraiscos  Christos and Liu  Bede    roundoff error analysis of the lms adaptive algorithm  IEEE Transactions on
Acoustics  Speech  and Signal Processing   
 

Chen     et al  Eyeriss  An energyef cient recon gurable
accelerator for deep convolutional neural networks  In
  IEEE International SolidState Circuits Conference  ISSCC  pp    IEEE   

Courbariaux  Matthieu et al  Binaryconnect  Training deep
neural networks with binary weights during propagaIn Advances in Neural Information Processing
tions 
Systems  pp     

Goel     and Shanbhag     Finiteprecision analysis of the
pipelined strengthreduced adaptive  lter  Signal Processing  IEEE Transactions on     

Goodfellow  Ian   et al  Maxout networks  ICML    

   

Gupta     et al  Deep Learning with Limited Numerical
In Proceedings of The  nd International
Precision 
Conference on Machine Learning  pp     

Hubara  Itay et al  Binarized neural networks 

In Advances in Neural Information Processing Systems  pp 
     

Hwang  Kyuyeon and Sung  Wonyong  Fixedpoint feedforward deep neural network design using weights     
and    In Signal Processing Systems  SiPS    IEEE
Workshop on  pp    IEEE   

Kim     and Smaragdis     Bitwise Neural Networks 

arXiv preprint arXiv   

Knauer  Karl  Ripplecarry adder  June     US Patent

 

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple

layers of features from tiny images   

Krizhevsky  Alex et al  Imagenet classi cation with deep
In Advances in neural

convolutional neural networks 
information processing systems  pp     

LeCun  Yann  Cortes  Corinna  and Burges  Christopher JC  The MNIST database of handwritten digits 
 

Lin  Darryl et al  Fixed point quantization of deep convolutional networks  In Proceedings of The  rd International Conference on Machine Learning  pp   
   

Lin  Min et al  Network in network 

arXiv   

arXiv preprint

Lin  Yingyan et al  Variationtolerant architectures for convolutional neural networks in the near threshold voltage
regime  In Signal Processing Systems  SiPS    IEEE
International Workshop on  pp    IEEE     

Rastegari  Mohammad et al  Xnornet  Imagenet classi 
 cation using binary convolutional neural networks  In
European Conference on Computer Vision  pp   
Springer   

Han  Song et al  Learning both weights and connections
for ef cient neural network  In Advances in Neural Information Processing Systems  NIPS  pp   
 

Sakr  Charbel et al  Minimum precision requirements for
the SVMSGD learning algorithm  In Acoustics  Speech
and Signal Processing  ICASSP    IEEE International Conference on  IEEE   

He  Kaiming et al  Deep residual learning for image
recognition  In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pp   
 

Hubara  Itay  Courbariaux  Matthieu  Soudry  Daniel  ElYaniv  Ran  and Bengio  Yoshua  Quantized neural networks  Training neural networks with low
arXiv preprint
precision weights and activations 
arXiv     

Shanbhag  Naresh    Energyef cient machine learning
in silicon    communicationsinspired approach  arXiv
preprint arXiv   

Simonyan  Karen and Zisserman  Andrew  Very deep convolutional networks for largescale image recognition 
arXiv preprint arXiv   

Srivastava  Nitish et al  Dropout    simple way to prevent
neural networks from over tting  Journal of Machine
Learning Research     

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Taigman  Yaniv et al  Deepface  Closing the gap to humanlevel performance in face veri cation  In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition  pp     

Zhang  Xiangyu et al  Accelerating very deep convoluIEEE
tional networks for classi cation and detection 
Transactions on Pattern Analysis and Machine Intelligence   

