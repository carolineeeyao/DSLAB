Tensor Decomposition via Simultaneous Power Iteration

PoAn Wang   ChiJen Lu  

Abstract

Tensor decomposition is an important problem
with many applications across several disciplines  and   popular approach for this problem
is the tensor power method  However  previous
works with theoretical guarantee based on this
approach can only  nd the top eigenvectors one
after one  unlike the case for matrices 
In this
paper  we show how to  nd the eigenvectors simultaneously with the help of   new initialization
procedure  This allows us to achieve   better running time in the batch setting  as well as   lower
sample complexity in the streaming setting 

  Introduction
Tensors have long been successfully used in several disciplines  including neuroscience  phylogenetics  statistics 
signal processing  computer vision  and data mining  They
are used to model multirelational or multimodal data  and
their decompositions often reveal some underlying structures behind the observed data  See  Kolda   Bader   
for   survey of such results  Recently  they have found
applications in machine learning  particularly for learning
various latent variable models  Anandkumar et al   
Chaganty   Liang    Anandkumar et al     
One popular decomposition method in such applications
is the CP  Candecomp Parafac  decomposition  which decomposes the given tensor as   sum of rankone components  This is similar to the singular value decomposition
 SVD  of matrices  and   popular approach for SVD is the
power method  which is wellunderstood and has nice theoretical guarantee  As tensors can be seen as generalization
of matrices to higher orders  one would hope that   natural generalization of the power method to tensors could
inherit the success from the matrix case  However  the situation turns out to be much more complicated for tensors
the discussion in  Anandkumar et al     
 see     

 Academia Sinica  Taiwan  Correspondence to  PoAn Wang

 poanwang iis sinica edu tw 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

and in fact several problems related to tensor decomposition are known to be NPhard  Hillar   Lim    Nevertheless  when the given tensor has some additional structure  the tensor decomposition problem becomes tractable
again  In particular  for tensors having orthogonal decomposition  Anandkumar et al      provided an ef cient
algorithm based on the tensor power method with theoretical guarantee  Still  as we will discuss later in Section   the
seemingly subtle change of going from matrices to tensors
makes some signi cant differences for the power method 
The  rst is that while the matrix power method can guarantee that   randomly selected initial vector will almost surely
converge to the top singular vector  we have much less control of where the convergence goes in the tensor case  Consequently  most previous works based on the tensor power
method with theoretical guarantee  such as  Anandkumar
et al        Wang   Anandkumar    require much
more complicated procedures  In particular  they can only
 nd the top   eigenvectors one by one  each time with the
power method applied to   modi ed tensor  de ated from
the original tensor according to the previously found vectors  Moreover  to  nd each vector  they need to sample
several initial vectors and apply the power method on all
of them  before selecting just one from them  In contrast 
algorithms for matrices such as  Mitliagkas et al   
Hardt   Price    are much simpler  as they can  nd
the   vectors simultaneously by applying the power method
only on   random initial vectors  The second difference  on
the other hand  has   bene cial effect  which allows the tensor power method to converge exponentially faster than the
matrix one when starting from good initial vectors  Then  
natural question is  can we inherit the best of both worlds 
Namely  is it possible to have   simple algorithm which can
 nd the   eigenvectors of   tensor simultaneously and converge faster than that for matrices 

Our Results  As in previous works  we consider the
slightly harder scenario in which we only have access to
  noisy version of the tensor we want to decompose  This
arises in applications such as learning latent variable models  in which the tensor we have access to is obtained from
some empirical average of the observed data  Our main
contribution is to answer the above question af rmatively 
First  we consider the batch setting in which we assume

Tensor Decomposition via Simultaneous Power Iteration

that the given noisy tensor is stored somewhere and can be
accessed whenever we want to  In this setting  we identify   suf cient condition such that if we have   initial vectors satisfying this condition  then we can apply the tensor
power method on them simultaneously  which will come
within some distance   to the eigenvectors in   log log  
   
iterations  with parameters related to eigenvalues considered as constant  To apply such   result  we need an ef 
 cient way to  nd such initial vectors  We show how to
do this by choosing   good direction to project the tensor
down to   matrix while preserving the eigengaps  and then
applying the matrix power method for only   few iterations
just to obtain vectors meeting that suf cient condition  The
number of iterations needed here is only   log    independent of   where   is the dimension of the eigenvectors 
The result stated above is for orthogonal tensors  On the
other hand  it is known that an nonorthogonal tensor with
linearly independent eigenvectors can be converted into an
orthogonal one with the help of some whitening matrix 
However  previous works usually pay little attention on
how to  nd such   whitening matrix ef ciently  According to  Anandkumar et al      one way is via SVD on
some second moment matrix  but doing this using the matrix power method would take longer to converge compared
to the tensor power method which would then be applied on
the whitened tensor  Our second contribution is to provide
an ef cient way to  nd   whitening matrix  by simply applying only one iteration of the matrix power method 
While most previous works on tensor decomposition focus
on the batch setting  storing even   tensor of order three requires     space  which is infeasible for   large    We
show to avoid this in the streaming setting  with   stream
of data arriving one at   time  which is the only source of
information about the tensor  We provide   streaming algorithm using only   kd  space  which is the smallest possible  just enough to store the   eigenvectors of dimension
   To achieve an approximation error   the total number of
samples we need is   kd log      

  log   log  

   

Related Works There is   huge literature on tensor decomposition  and it is beyond the scope of this paper to
give   comprehensive survey  Thus  we only compare our
results to the most related ones  particularly those based
on the power method  While different works may focus
on different aspects  we are most interested in understanding how the error parameter   affects various performance
measures  having in mind   small  
First  the batch algorithm of Anandkumar et al     
using   better analysis in  Wang   Anandkumar   
runs in time about      log   log log  
    which can be
made to run in     log log  
    iterations in parallel  while
ours are     log log  
    and   log log  
    respectively  On

the other hand  one advantage of their algorithm is that its
running time does not depend on the eigengaps  while ours
has the dependence hidden above as some constant 
In the streaming setting  Wang   Anandkumar  
provided an algorithm using   dk log    memory and
    samples  while ours only uses   dk  memO   
  log   
ory and     
    samples  Nevertheless  the
  log   log log  
sample complexity of Wang   Anandkumar   is also
independent of the eigengaps  while ours has the dependence hidden above as   constant factor 
As one can see  our algorithms  which  nd the   eigenvectors simultaneously  allow us to save   factor of   in the
time complexity and the sample complexity  although our
bounds may become worse when the eigengaps are small 
Thus  our algorithms can be seen as new options for users
to choose from  depending on the data they are given 
Although not directed related  let us also compare to previous works on SVD  Two related ones  both based on the
simultaneous matrix power method  are the batch algorithm
of  Hardt   Price    which converges in   log  
    iterations  and the streaming algorithm of  Li et al   
which requires     
    samples  Both bounds
are worse than ours and also depend on the eigengaps 
Thus  although one approach for orthogonal tensor decomposition is to reduce it to   matrix SVD problem  this does
not appear to result in better performance than ours 
Finally  comparisons of the tensor power method with other
approaches can be found in works such as  Anandkumar
et al      Wang   Anandkumar    For example 
the online SGD approach of  Ge et al    works only
for tensors of even orders and its sample complexity has  
poor dependency on the dimension   

  log   log  

Organization of the paper  First  we provide some preliminaries in Section   Then we present our batch algorithm for orthogonal and symmetric tensors of order three
in Section   and then for general orthogonal tensors in Section   In Section   we introduce our whitening procedure
for nonorthogonal but symmetry tensors  Finally  in Section   we present our algorithm for the streaming setting 
Due to the space limitation  we will move all our proofs to
the appendix in the supplementary material 

  Preliminaries
Let us  rst introduce some notations and de nitions which
we will use later  Let   denote the set of real numbers
and   the set of positive integers  Let       denote the
standard normal distribution with mean   and variance  

 We use   different input distribution from theirs  The bound

listed here is modi ed from theirs according to our distribution 

Tensor Decomposition via Simultaneous Power Iteration

 cid     cid 
 cid   cid    using the convention that  

and let        for        denote the dvariate one which
has each of its   dimensions sampled independently from
      For        let     denote the set              For
  vector    let  cid   cid  denote its    norm  For        let Id
denote the       identity matrix  For   matrix     Rd   
let Ai  for         denote its ith column  and let Ai    for
        be the jth entry of Ai  Moreover  for   matrix   
let   cid  denote its transpose  and de ne its norm as  cid   cid   
maxx Rk
Tensors are the focus of our paper  which can be seen as
generalization of matrices to higher orders  For simplicity of presentation  we will use symmetric tensors of order
three as examples in the following de nitions    real tensor   of order three can be seen as an threedimensional
array in Rd      for some        with its          th
entry denoted as Ti      For such   tensor   and three
matrices     Rd        Rd        Rd    let
            be the tensor in Rm      with its          
         Ti   kAa iBb jCc    The norm
of   tensor   we will use is the operator norm   cid   cid   
maxx     Rd

th entry de ned as cid 

          
 cid   cid cid   cid cid   cid   

     

The tensor decomposition problem 
there is   tensor   with some unknown decomposition

In this problem 

 cid 

    

   

     ui   ui   ui 

with        and ui   Rd for any         Then given some
        and         our goal is to  nd    and  ui with
             and  cid ui   ui cid      for every        

We will assume that cid 

    

       and                    

 

As in previous works  we consider   slightly harder version of the problem  in which we only have access to some
noisy version of     instead of the noiseless     We will
consider the following two settings  In the batch setting 
we have access to some            for the whole time 
for some perturbation tensor   In the streaming setting 
we have   stream of data points             arriving one by
one  which provide the only information we have about    
with each      Rd allowing us to compute some     with
mean               In this streaming setting  we are particularly interested in the case of   large   which prohibits one
to store   tensor of size    in memory 

Power Method  Matrices versus Tensors  Note that  
tensor of order two is just   matrix  and   popular approach for decomposing matrices is the socalled power

method  which works as follows  Suppose we are given
          ui   ui  with nonnegative           and orthonormal vectors            ud 
     ci   ui 
usually chosen randomly  and then repeatedly performs the
update                 which results in

        matrix      cid 
The power method starts with some     cid 
 cid 
 cid  

      cid    ui  

 cid    ui 

 cid 

      

 cid 

  cid 

ici

  

    

    

Note that for any    cid    as        the coef cient   
ici will
soon become much smaller than the coef cient   
    if   
is not too small  which is likely to happen for   randomly
chosen    This has the effect that after normalization 
    cid     cid  approaches    quickly 

Now consider   tensor      cid 
          ui   ui   ui 
with nonnegative           and orthonormal
 cid 
The tensor version of the power
vectors            ud 
method again starts from   randomly chosen     
     ci   ui  but now repeatedly performs the update

          Id            which in turn results in

 cid 

    

 cid 

      cid    ui  

  cid 

 cid 

    

      

  

 

 

 ici     ui 

The coef cient of each ui now has   different form from
the matrix case  and this leads to the following two effects 
First  one now has much less control on what     cid     cid 
converges to  In fact  it can converge to any ui  cid     if
ui has the largest value of    ci  which happens with  
good probability if    is not much smaller than   Consequently  to  nd the top   vectors            uk  previous
works based on the power method all need much more
complicated procedures  Anandkumar et al      compared to those for matrices  as discussed in the introduction 
On the other hand  the different form of      has the bene 
cial effect that the convergence is now exponentially faster
than in the matrix case  More precisely  if    ci       cj 
than the gap between the coef cients  ici   and  jcj  
is now ampli ed much faster  We will show how to inherit
this nice property of faster convergence but at the same time
avoid the dif culty discussed above 

  Orthogonal and Symmetric Tensors of

Order Three

In this section  we focus on the special case in which the
tensors to be decomposed are orthogonal  symmetric  and
of order three  Formally  there is an underlying tensor

 cid 

    

   

     ui   ui   ui 

Tensor Decomposition via Simultaneous Power Iteration

Algorithm   Robust tensor power method

 cid 

Input  Tensor      Rd     and parameters            
Initialization Phase 
Sample            wL     
 
Compute       
    
 
Compute          Id  Id     
Factorize     as          by QR decomposition 
for       to   do

             
    Id  wj  wj 

          

Compute                   
Factorize       as              by QR decomposition 

end for
Tensor Power Phase 
Let           
for       to   do
          
Compute      
Factorize       as             by QR decomposition 
and          uj   uj   uj          

end for
Output   uj        

        Id      

      

 

 

 

with orthonormal vectors ui   and real      satisfying the
condition   Then given         and         our goal
is to  nd approximates to those    and ui within distance  
but we only have access to some noisy tensor           
for some symmetric perturbation tensor  
Our algorithm is given in Algorithm   which consists of
two phases  the initialization phase and the tensor power
phase  The main phase is the tensor power phase  which
we will discuss in detail in Subsection   For our tensor power phase to work  it needs to have   good starting
point  This is provided by the initialization phase  which
we will discuss in detail in Subsection   Through these
two subsections  we will prove Theorem   below  which
summarizes the performance of our algorithm  according
to the following parameters of the tensor 

    min
    

     
 
 
 

  

and     min
    

        

 

 

 

 

dk

  log    and       log   

Theorem   Suppose       
  and the perturbation tensor
  for   small
has the bound  cid cid    min   
      
 
   
enough constant   Then for some         
  log       
    
    our Algorithm  
  log  
with high probability will output  ui and    with  cid ui ui cid   
  and              for every        
Let us make some remarks about the theorem  First  the  
samples are used to compute    and     which can be done
in   parallel way  Second  our parameter   is related to  
parameter  cid    mini   
used in  Hardt   Price 
  and it is easy to verify that      cid  Thus  our algorithm for tensors converges in     
  log  
   
rounds  which is faster than the     
 cid  log  
   

  log     log   
 cid  log      

     

  

rounds of  Hardt   Price    for matrices  Note that
our dependence on the error parameter   is exponentially
smaller than that of  Hardt   Price    which means
that for   small   we can decompose tensors much faster
than matrices  Finally  compared to previous works on
tensors  our convergence time  for   small   is about
  log log  
    while those in  Anandkumar et al     
Wang   Anandkumar    are at least    log log  

   

  Our Robust Tensor Power Method

                

The tensor power phase of our Algorithm   is based on
our version of the tensor power method  which works as
follows  At each step    we maintain         matrix     
with columns     
  as our current estimators for
           uk  which is obtained by updating the previous estimators with the following two operations 
The main operation is to apply the noisy tensor    on them
simultaneously to get         matrix       with its jth column computed as      
  which

equals  cid 

  

    

for    

     Id      
             cid 

       

 

      

 

 

 

      

        Id      
 cid 
      
  cid 
 cid 

 cid    ui      
 cid 

      
  This implies that
      
  cid 

    cid 

   

 

 

 

   
   

 

which shows the progress made by this operation 
The second operation is to orthogonalize       as

                   

by the QR decomposition via the GramSchmidt process 
to obtain         matrix      with columns     
                
   
which then become our new estimators  As we will show
in Lemma   below  given   small enough  cid cid  if we start
with   fullrank    then each      also has full rank and
consists of orthonormal columns  and each      is invertible  Moreover  although we apply the QR decomposition
on the whole matrix       to obtain the matrix      it has
the effect that for any         the  rst   columns of     
can be seen as obtained from the  rst   columns of      
by   QR decomposition  This property is needed in our
Lemma   and Theorem   below to guarantee the simultaneous convergence of     
 
Before stating Lemma   which guarantees the progress we
make at each step  let us prepare some notations  rst  For
        matrix   and some         let      denote the
      matrix containing the  rst   columns of    Let  
denote the       matrix with the target vector ui as its   th
column  For         matrix   and some         de ne

to ui for every        

 cid cid cid   cid 

 cid cid cid   cid          cid 

cosm      min
  Rm

              

Tensor Decomposition via Simultaneous Power Iteration

   Id  Id         cid 

method for only   few steps just to make the tangents less
than one  Although we could continue applying the matrix
power method till reaching the much smaller target bound
  this would take exponentially longer than by switching
to the tensor power method as we actually do 
As mentioned above  we would  rst like to project the tensor    down to   matrix    naive approach is to sample
  random vector    and take the matrix     Id  Id       
        ui   ui  However  this
may mess up the gaps between eigenvalues  which are
needed to guarantee the convergence rate of the matrix
power method  The reason is that as each   cid 
     has mean
zero  the coef cient      cid 
      also has mean zero and thus
has   good chance of coming very close to others  To preserve the gaps  we would like to have   cid 
      for
each   with high probability  To achieve this  let us  rst
imagine sampling   random     Rd from        and
computing the vector          Id        which is close to

          cid 

         cid 

 cid 
              Id         cid 

Then one can show that for every        cid 
      iui  and

   Id         

    

     cid 

       ui 

         so that

                        cid 
  cid 

         

However  we want the gappreserving guarantee to be in
high probability  instead in expectation  Thus we go further
by sampling not just one  but some number   of vectors
           wL independently from the distribution       
and then taking the average

    Id  wj  wj 

 

 cid 

    

    

 
 

The following lemma shows that such      is likely to have
           which we prove in Appendix   
  cid 
Lemma   Suppose we have            with  cid cid     
    
Then for some         
  log    the vector    computed
according to   with high probability satis es

        for every        

 

 cid cid   cid 

         

 cid cid     

 

 cid    cos 

which equals the cosine of the   th principal angle between the column spaces of      and      let sinm     

     and let us use as the error measure

tanm      sinm    cosm   

More information about the principal angles can be found
in        Golub   Van Loan    Then we have the
following lemma  which we prove in Appendix   
Lemma   Fix any         and       Let    
      
the       matrix with    
  th column  and suppose

    denote
  as its

 

     Id      
 cid 

 cid 
      

  cos 

 

 cid cid cid     

   

 

 

for some       Then for     maxi       
  
  max      tan 
tanm        max

   
    we have

 cid 
      

 

 cid cid cid        min
 cid 

Observe that the guarantee provided by the lemma above
has   similar form as that in  Hardt   Price    for
matrices  The main difference is that here in the tensor
case  we have the error measure essentially squared after each step  which has the following two implications 
First  to guarantee that the error is indeed reduced  we need
tanm      to be small enough  say  less than one  unlike in the matrix case  Next  if we indeed have   small
enough tanm      then the error can be reduced in  
much faster rate than in the matrix case  Another difference is that here we provide the guarantee for all the   sub 
    for         instead of just one matrix     
matrices     
This allows us to show the simultaneous convergence of
to the target vector ui for every        
each column     
 
as given in the following  which we prove in Appendix   
Theorem   For any         
   
there exists some
      log   
    such that the following holds  Suppose the perturbation is bounded by  cid cid     
 
and we
start from some initial    with tanm        for every         Then for any        with  ui       
and          ui   ui   ui  we have  cid ui    ui cid      and
             for every        
Note that the convergence rate guaranteed by the theorem
above is exponentially faster than that in  Hardt   Price 
  for matrices  assuming that we indeed can have such
  good initial    to start with  In the next subsection  we
show how it can be found ef ciently 

  log  

 

 

 

  Initialization Procedure

With this     we compute the matrix          Id  Id     
As shown by the following lemma  which we prove in Appendix       is close to   matrix with ui   as eigenvectors
and good gaps between eigenvalues 
Lemma   Suppose we have            Then for any   
satisfying the condition   in Lemma   the matrix     
    Id  Id      can be decomposed as

 cid 

    

Our approach for  nding   good initialization is to project
the tensor down to   matrix and apply the matrix power

    

     ui   ui    

Tensor Decomposition via Simultaneous Power Iteration

as that in the previous section  Note that this approach has
the eigenvalues decreased exponentially in      different
approach avoiding this is to compute    directly as

 cid      Id  Id  wj          wj cid 

 

 cid 

    

for some      with              for         and    
 Id  Id      with  cid   cid     cid cid 
With such   matrix     we next apply the matrix power
method of  Hardt   Price    to  nd good approximates to its eigenvectors  More precisely  we sample an
initial matrix       Rd   by choosing each of its column independently according to the distribution       
and factorize it as                by QR decomposition via the GramSchmidt process  Then at step      
we multiply the previous estimate       by    to obtain
                   factorize it as                     
by QR decomposition via the GramSchmidt process  and
then take the orthonormal       as the new estimate  The
following lemma shows the number of steps needed to  nd
  good enough      
Lemma   Suppose we are given   matrix    having the
decomposition described in Lemma   with  cid   cid     
dk
for   small enough constant   Then there exists some
        
  log    such that with high probability  we have
tanm           for every         whenever       
This together with the previous two lemmas guarantee that
given            with  cid cid    min   
  for   small
enough constant   we can obtain with high probability  
good       which can be used as the initial    for our
tensor power phase  Combining this with Theorem   in the
previous subsection  we then have our Theorem   given at
the beginning of the section 

      

dk

  General Orthogonal Tensors
In the previous section  we consider tensors which are orthogonal  symmetric  and of order three  In this section  we
show how to extend our results for general orthogonal tensors  to deal with higher orders  rst and then asymmetry 

  HigherOrder Tensors

To handle orthogonal and symmetric tensors of any order  only the initialization procedure needs to be modi ed 
First  for tensors of any odd order    straightforward modi cation is as follows  Take for example   tensor of order
       Now we simply compute

    Id  wj          wj 

    

With such   vector     one can show that the matrix

with    copies of wj  and similarly to Lemma   one can
         ui 

show that    is likely to be close to the vector cid 
with        copies of     is close to the matrix cid 

 
       
ui   ui  similarly to Lemma   Then the rest is the same
 

         Id  Id                 

 cid 

    

 
 

    

 
 

 cid 

which is close to

 
 

 
 

    

    
 
 

 cid 
 cid 

 cid 
 cid 
be close to the matrix cid 

    
 
 

    

 
 

    

 

 

    Id  Id  wj          wj 

 cid   cid 
 cid   cid 

  wj

  wj

 cid      ui   ui
 cid      ui   ui 

 cid 

    

    

 
 

Then one can again show that such   matrix    is likely to

      

    ui   ui 

To handle tensors of even orders 
the initialization is
slightly different but the idea is similar  Given   tensor of
order     we again sample vectors            wL as before 
but now we compute the matrix directly as

    Id  Id  wj          wj 

the matrix    is likely to be close to the matrix cid 

with        copies of wj  As before  one can show that
         
ui   ui  Then again we can apply the matrix power method
on    and obtain   good initialization for the tensor power
method as before  Note that now the eigenvalues are no
longer squared  and the previous requirement on  cid cid  can
be slightly relaxed  with the dependence on   being replaced by  

  Asymmetric Tensors

For simplicity of presentation  let us focus on the third order case  the extension to higher orders is straightforward 
That is  now the underlying tensor has the form

 cid 

    

   

     ai   bi   ci 

with nonnegative      satisfying the condition   together
with three sets of orthonormal vectors of ai    bi    and ci   
As before  we only have access to   noisy version    of    
The main modi cation of our algorithm is again to the initialization procedure  but the idea is also similar  To  nd  
good initial matrix   for ai    we sample            wL independently from        and now compute the matrix

 cid      Id  Id  wj cid cid      Id  Id  wj cid cid 

 

    

 
 

 cid 

    

Tensor Decomposition via Simultaneous Power Iteration

As before  it is not hard to show that

      cid 

 cid 
 cid   cid 
which is close to the matrix cid 

    

    

 
 

 
 

  wj

 cid    ai   ai 

      

    ai   ai with high
probability  From the matrix     we can again apply the
matrix power method to  nd   good initial matrix    Similarly  we can  nd good initial matrices   and   for bi  
and ci    respectively 
Next  with such matrices  we would like to apply the tensor
power method  which we modify as follows  Now at each
step    we take previous estimates                
and compute      
 
        
  Id       
  Id  for
        followed by orthonormalizing                  
to obtain the new estimates                 via QRdecomposition  It is not hard to show that the resulting algorithm has   similar convergence rate as our Algorithm  

      Id      
       

            

      

       

       

 

 

 

 

 

 

 

 

  Nonorthogonal but Symmetric Tensors
In the previous section  we consider general orthogonal tensors  which can be asymmetric  In this section  we consider nonorthogonal tensors which are symmetric  We
remark that for some latent variable models such as the
multiview model  the corresponding asymmetric tensors
can be converted into symmetric ones  Anandkumar et al 
    so that our result here can still be applied  For
 cid 
simplicity of exposition  let us again focus on the case of
order three  so that the given tensor has the form    
        vi vi vi  but the vectors vi   are no longer assumed to be orthogonal to each other  Still we assume them
to be linearly independent  and we again assume without
loss of generality that  cid   cid      and  cid vi cid      for each    In
addition  let us assume  as in previous works  that       
for          
Following  Anandkumar et al      we would like to
whiten such   tensor   into an orthogonal one  so that we
can then apply our Algorithm   More precisely  our goal is
to  nd         matrix   such that the tensor             
becomes orthogonal  As in  Anandkumar et al     
assume that we also have available   matrix

 cid 

    

   

     vi   vi 

Then for   whitening matrix  it suf ces to  nd some  

 This assumption is not necessary  We assume it just to simplify the  rst step of our algorithm given below  Without it  we
can simply replace that step by the matrix power method used in
our Algorithm   which takes more steps but can still do the job 
 More generally  the weights    in   are allowed to differ
from those in     but for simplicity we assume they are the same 

such that    cid       Ik  The reason is that

    cid    cid vi

 cid   cid    cid vi

 cid   

 cid 

    

Ik      cid      

which implies that the vectors
orthonormal  Then the tensor              equals

 iW  cid vi  for         are

 

 
  

 cid 

    

 cid cid 

 iW  cid vi

 cid 

 

 cid 

    

         cid vi   

which has an orthogonal decomposition 
According to  Anandkumar et al      one way to
 nd such     is to do the spectral decomposition of  
as      cid  with eigenvectors as columns of    and let
         
    However  we will not take this approach 
because  nding   good approximate to   by the matrix
power method would take longer to converge than the tensor power method which we will later apply to the whitened
tensor  Our key observation is that it suf ces to  nd       
matrix   such that the matrix       cid     is invertible 
since we can then let     QP    

  and have

   cid            

    cid   QP    

    Ik 

 
 

 

  ui if   has orthonormal columns 

With such       the tensor              becomes orthogonal  so that we can decompose it  to obtain       
and
  
 iW  cid vi  from which we can recover       
and
ui  
vi    iQP  
As before  we consider   similar setting in which we only
have access to   noisy            for some symmetric
perturbation matrix   in addition to the noisy tensor     
    Then our algorithm for  nding the whitening matrix
consists of the following two steps 
  Sample   random matrix     Rd   with orthonormal
columns  compute            and factorize it as     
     by   QR decomposition 

  Compute        cid       and output              

  as

the whitening matrix 

We analyze our algorithm in the following  First note that
  is computed in the same way as we compute     in
Algorithm   and with         we are likely to have
tank        so that the matrix       cid     is invertible  Formally  we have the following  which we prove in
Appendix   
Lemma   Suppose  cid   cid       
for   small enough constant   Then with high probability we have  max      
  and  min         
   

dk

 To apply our Algorithm   we need to scale it properly  say

 
     to make its norm at most one 

by   factor of

Tensor Decomposition via Simultaneous Power Iteration

         

         

 cid     min    max      
   

Next  with   small enough  cid   cid  if   is invertible  then so
is      and moreover  we have       
    This is shown
in the following  which we prove in Appendix   
Lemma   Fix any         and suppose we have
 min         and  cid   cid      Then    is invertible and
 cid        
Then  with   good       
    we can obtain   good    and have
                    close to              which has an orthogonal decomposition  This is shown in the following  which
we prove in Appendix   
Theorem   Fix any         
    and suppose we have
 cid cid     
    and  cid   cid      min     
  for  
small enough constant   Then with high probability we
have  cid                                    cid     

   
  
 

dk

 
 

  Streaming setting
In the previous sections  we consider the batch setting in
which the tensor    is assumed to be stored somewhere
which can be accessed whenever we want to  However 
storing such   tensor  say of order three  requires   space
complexity of     which becomes impractical even for
  moderate value of    In this section  we study the possibility of achieving   space complexity of   kd  which is the
least amount of memory needed just to store the   vectors
in Rd  More precisely  we consider the streaming setting 
in which there is   stream of vectors             arriving
one at   time  We assume that each vector   is sampled independently from some distribution over Rd  with  cid   cid     
and some function     Rd   Rd     such that

               and given           Rd      Id       

can be computed in      space 

we can choose   proper size for    In fact  to save the total number of samples  we can follow the approach of  Li
et al    by choosing different sizes in different iterations of the matrix or tensor power method 
Following  Wang   Anandkumar    let us take the
speci   case with                  as   concrete example
and focus on the orthogonal case studied in Section   it is
not hard to convert other algorithms of ours to the streaming setting  One can show that in this speci   case  we have
      iui so that there is   more ef cient way to
 nd   vector    for producing the matrix    in the initialization phase  Formally  we have the following lemma  which
we prove in Appendix   
Lemma   There is an algorithm using      space and
   log  
    samples to  nd some      Rd satisfying the condition   in Lemma   with high probability 

      cid 

With such   vector     we can then use the streaming algorithm of  Li et al    to  nd   good initial matrix   for
the later tensor power phase  Formally  we have the following lemma  which we prove in Appendix   
Lemma   Given    from Lemma   we can use   kd 
space and   
  samples to  nd some     Rd  
with tanm        for any         with high probability 

kd log  
 

 

Having such   matrix    we can proceed to the tensor
power phase  Borrowing again the idea from  Li et al 
  let us partition the incoming data into blocks of increasing sizes  with the   th block Jt used to carry out one
tensor power iteration      
  for
        of Algorithm   with           Jt 
 
       Instead of preparing this        and then computing each      
 
we now go through  Jt  steps of updates 

 cid 
          Id      
      
 Jt

 

 

Such   function   is known to exist for some latent variable models  Ge et al    Wang   Anandkumar   
Given such   function  our algorithms in previous sections
can all be converted to work in the streaming setting using
  kd  space  This is because all our operations involving tensors have the form     Id        for some        Rd 
which can be realized as

 cid 

 cid 

 cid 

   

 
   

 cid 

   

 
   

  xt 

 Id         

   xt   Id         

for   collection   of samples  with the righthand side above
clearly computable in   kd  space  Then depending on
the distance we want between         
      xt  and    

 cid 

 This also includes the initialization phase in which we now do
not store the matrix    explicitly but instead replace the operation
   Zi by     Id  Zi     

 cid 

 cid 

  For     Jt do       

         

     Jt     cid 

      

 

     

The block sizes are chosen carefully to keep  cid             cid 
small enough so that we can have tanm      decreased in
  desirable rate  Here  we choose the parameters

     max

   

 
 

and  Jt   

   log dt 

 
 

 

 

for   large enough constant    to make the condition  
in Lemma   hold with high probability so that we have
tanm            In Appendix    we summarize our
algorithm and prove the following theorem 
Theorem   Given         
    with high probability we
can  nd      ui with           cid   ui   ui cid      for any    
    using   kd  space and   
 
samples 

   

log   log   

kd log  
 

  log  

 

   

Tensor Decomposition via Simultaneous Power Iteration

References
Anandkumar  Animashree  Hsu  Daniel    and Kakade 
Sham      method of moments for mixture models and
hidden markov models  In COLT  volume   pp     

Anandkumar  Animashree  Ge  Rong  Hsu  Daniel 
Kakade  Sham    and Telgarsky  Matus  Tensor decompositions for learning latent variable models  Journal of
Machine Learning Research       

Anandkumar  Animashree  Ge  Rong  and Janzamin  Matensor decomposiarXiv preprint

jid 
tion via alternating rank  updates 
arXiv     

Guaranteed nonorthogonal

Chaganty  Arun Tejasvi and Liang  Percy  Estimating
latentvariable graphical models using moments and
likelihoods  In ICML  pp     

Ge  Rong  Huang  Furong  Jin  Chi  and Yuan  Yang  Escaping from saddle pointsonline stochastic gradient for
tensor decomposition  In Proceedings of The  th Conference on Learning Theory  pp     

Golub  Gene    and Van Loan  Charles    Matrix ComJohn Hopkins University Press   rd edition 

putation 
 

Hardt  Moritz and Price  Eric  The noisy power method   
meta algorithm with applications  In Advances in Neural
Information Processing Systems  pp     

Hillar  Christopher   and Lim  LekHeng  Most tensor
problems are NPhard  Journal of the ACM  JACM   
   

Kolda  Tamara   and Bader  Brett    Tensor decompositions and applications  SIAM review   
 

Li  ChunLiang  Lin  HsuanTien  and Lu  ChiJen  Rivalry
of two families of algorithms for memoryrestricted
streaming PCA  In Proceedings of the  th International
Conference on Arti cial Intelligence and Statistics  AISTATS  pp     

Mitliagkas  Ioannis  Caramanis  Constantine  and Jain  Prateek  Memory limited  streaming PCA  In Advances in
Neural Information Processing Systems  pp   
 

Wang  Yining and Anandkumar  Anima  Online and
In Addifferentially private tensor decomposition 
vances in Neural Information Processing Systems  pp 
   

