Toward Ef cient and Accurate Covariance Matrix

Estimation on Compressed Data

Xixian Chen     Michael    Lyu     Irwin King    

Abstract

Estimating covariance matrices is   fundamental technique in various domains  most notably
in machine learning and signal processing  To
tackle the challenges of extensive communication costs  large storage capacity requirements 
and high processing time complexity when handling massive highdimensional and distributed
data  we propose an ef cient and accurate covariance matrix estimation method via data compression  In contrast to previous dataoblivious
compression schemes  we leverage   dataaware
weighted sampling method to construct
lowdimensional data for such estimation  We rigorously prove that our proposed estimator is unbiased and requires smaller data to achieve the
same accuracy with specially designed sampling
distributions  Besides  we depict that the computational procedures in our algorithm are ef cient 
All achievements imply an improved tradeoff between the estimation accuracy and computational
costs  Finally  the extensive experiments on synthetic and realworld datasets validate the superior property of our method and illustrate that it
signi cantly outperforms the stateof theart algorithms 

  Introduction
Covariance matrices play   fundamental role in machine
learning and statistics owing to their capability to retain the
secondorder information of data samples  Feller   
For example  Principal Component Analysis  PCA  along

 Shenzhen Research Institute  The Chinese Univer 
 Department of
sity of Hong Kong  Shenzhen  China 
Computer Science and Engineering  The Chinese University of Hong Kong  Shatin       Hong Kong 
CorreXixian Chen  xxchen cse cuhk edu hk 
spondence to 
Michael    Lyu  lyu cse cuhk edu hk 
Irwin King
 king cse cuhk edu hk 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

with its extensions  Zou et al    Linear Discriminant Analysis  LDA  and Quadratic Discriminant Analysis
 QDA   Anzai    are powerful for dimension reduction
and denoising  which require the estimation of   covariance
matrix from   given collection of data points  Other prominent examples include Generalized Least Squares  GLS 
regression that requires the estimation of the noise covariance matrix  Kariya   Kurata    Independent Component Analysis  ICA  that relies on prewhitening based on
the covariance matrix  Hyv arinen et al    and Generalized Method of Moments  GMM   Hansen    that
improves the effectiveness by   precise covariance matrix 
Many practical applications also rely on covariance matrix directly  Bartz    In biology  gene relevance networks and gene association networks are straightforwardly
inferred from the covariance matrix  Butte et al   
Sch afer   Strimmer    In modern wireless communications  protocols optimize the bandwidth based on covariance estimates  Tulino   Verd      In array signal processing  the capon beamformer linearly combines
the sensors to minimize the noise in the signal  which is
closely related to the portfolio optimization on covariance
matrices  Abrahamsson et al    For policy learning
in the  eld of robotics  it requires reliable estimates of the
covariance matrix between policy parameters  Deisenroth
et al   
Calculation of   covariance matrix usually requires enormous computational resources in the form of communication and storage because large and highdimensional data
are now routinely gathered at an exploding rate from many
distributed remote sites  such as sensor networks  surveillance  and distributed databases  Haupt et al    Shi
et al    Ha   Barber    In particular  high communication cost of transmitting the distributed data from
the remote sites to the fusion center         destination to
conduct complex data analysis tasks  will require tremendous bandwidth and power consumption  Srisooksai et al 
  AbbasiDaresari   Abouei    Formally  given
  data matrix     Rd   with   features and   instances
 cid  
collected from the remote sites  the covariance matrix is
  XXT      xT  
computed in the fusion center by    cid   
   xi   Rd  Feller    For simplicity
where       
 

Toward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

of discussion  we temporarily assume the empirical mean is
zero              The covariance matrix can be written as
  XXT consequently  Azizyan et al    Then  it
     
takes   nd  communication burden to transmit data from
numerous remote sites to the fusion center to form the full
data set      nd  storage in total to store   in remote
sites  and   nd    storage with   nd  time to calculate
  in the fusion center  When       cid    the overall cost is
prohibitively expensive for practical scenarios like wireless
sensors which have narrow transmission bandwidth  limited storage  and low power supply 
To tackle such computational challenges  compressed data
can be leveraged to estimate the covariance matrix  which
essentially has roots in compressed sensing  One solution
is to process each data point by multiplying it with   single projection matrix     Rd   whose entry follows the
Gaussian distribution      
     Mahoney    Thus 
storing ST   and the estimated covariance matrix requires
  mn      space in total  sending ST   to the fusion center incurs     mn  communication cost  and calculating
ST   and the covariance matrix estimator  
  SST XXT SST
takes   mdn               md  time  This method
substantially reduces all computational costs if    cid       
Note that synchronizing only   seed between remote sites
and the fusion center allows pseudorandom number generators to reconstruct an identical    which avoids sending
  directly and imposes   negligible computational burden 
However  the example solution has two critical drawbacks 
The  rst is that the operations on the Gaussian matrix is
inef cient  One could use   sparse projection matrix  Li
et al    structured matrix  Ailon   Chazelle    or
sampling matrix  Drineas et al      to achieve   better
tradeoff between computational cost and estimation precision  The second problem is that applying   single projection matrix to all data points cannot consistently estimate
the covariance matrix       the estimator cannot converge
to the actual covariance matrix even if the sample size  
grows to in nity with    xed  This issue is demonstrated
both theoretically and empirically in  Azizyan et al   
and also brie   described in  Gleichman   Eldar   
Anaraki   Hughes    Anaraki   Becker   
In this paper  we thus adopt   distinct projection matrices
for   data vectors  Azizyan et al    Anaraki   Hughes 
  Anaraki   Becker    Anaraki    to achieve
consistent covariance matrix estimation  and construct  
speci   sampling matrix to increase both its ef ciency and
accuracy  On the whole  we do not make statistical assumptions on the distributed data     Rd   with       cid   
nor do we impose structural assumptions on the covariance
matrix   such as being lowrank or sparse  Our goal is to
compress data and recover   ef ciently and accurately  and
the contributions in our work are summarized as follows 

  First  in contrast to all existing methods  Azizyan
et al    Anaraki   Hughes    Anaraki  
Becker    Anaraki    that are based on dataoblivious projection matrices  we propose to estimate
the covariance matrix based on the data compressed
by   weighted sampling scheme  This strategy is dataaware with   capacity to explore the most important
entries  Hence  we require considerably fewer entries
to achieve an equal estimation accuracy 

  Second  we provide error analysis for the derived unbiased covariance estimator  which rigorously demonstrates that our method can compress data to   much
smaller volume than other methods  The proofs also
indicate our probability distribution is speci cally designed to render   covariance matrix estimation based
on the compressed data as accurate as possible 

  Third  we specify our method by an ef cient algorithm
whose computational complexity is superior to other
methods  By additionally considering the best tradeoff
between the estimation accuracy and the compression
ratio  our algorithm ultimately incurs   signi cantly
lower computational cost than the other methods 

  Finally  we validate our method on both synthetic and
realworld datasets  which demonstrates   better performance than the other methods 

The remainder of this paper is organized as follows  In Section   we review the prior work  In Section   we present
our method along with theoretical analysis and emphasize
its achievements  In Section   we provide extensive empirical results  and in Section   we conclude the whole work 

  xi  Because Si ST

  Related Work
There have been several investigations of ways to achieve
accurate covariance matrix estimation from the lowdimensional compressed observations constructed by applying   distinct projection matrix  Si  
     Rd  
     Rd 
to each data vector  xi  
The work
 Qi   Hughes    adopts   Gaussian matrix
of
  xi  and recovers them by
to compress data via ST
  Si ST
is   strictly
Si ST
mdimensional orthogonal projection drawn uniformly
at random 
it can capture the information of all entries in each data vector uniformly and substantively 
  up to
Then   
 
  known scaling factor is expected to constitute accurate
and consistent covariance matrix estimation  This estimator can be modi ed to an unbiased one  and its error analysis is thoroughly provided in  Azizyan et al    However    Gaussian matrix is dense and unstructured  which
imposes an extra computational burden  Also  many matrix inversions take   considerable amount of time  and

  Si ST

  Si ST

  Si ST

 

 cid  

   Si ST

  xixT

  Si ST

Toward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

 cid  

  xixT

  SiST
 

 

   SiST

  xi   Rd because SiST

the whole square matrix has to be loaded into the memis thus proory  Biased estimator  
 
posed in  Anaraki   Hughes    to improve the ef 
 ciency by avoiding matrix inversions and assigning Si
to be   sparse matrix  This method is less accurate because SiST
  approximates only an mdimensional random
orthogonal projection  Its another disadvantage is that the
result only holds for data samples under statistical assumptions  Based on  Anaraki   Hughes    another study
proposes an unbiased estimator  Anaraki    but it still
adopts an unstructured sparse matrix that is insuf ciently
computationef cient and fails to provide the error bounds
to characterize the estimation error versus the compression
ratio  Recently  sampling matrices Si   Rd   constructed
via uniform sampling without replacement have been employed  Anaraki   Becker    This approach is ef 
cient  but it only results in poor accuracy if data are compressed directly by ST
is an mdimensional orthogonal projection drawn only from   deterministic orthogonal spaces coordinates  and the      
entries of each vector are removed  To avoid sacri cing much accuracy  use of the computationally ef cient
Hadamard matrix  Tropp    before sampling has also
been proposed in  Anaraki   Becker    It  attens out
whole entries  particularly those with large magnitudes  to
all coordinates to ensure that poor uniform sampling with  
small sampling size still obtains some information among
all entries  However  the Hadamard matrix involves deterministic orthogonal projection and is unable to capture
the information uniformly in all coordinates of each vector 
which results in the need for numerous samples to achieve
suf cient accuracy   Anaraki   Becker    constitutes
the current state of the art in the tradeoff between the estimation accuracy and computational ef ciency  Throughout the paper  we group the foregoing representative methods into GaussInverse  Azizyan et al    Qi   Hughes 
  Sparse  Anaraki   Hughes    Anaraki   
and UniSampleHD  Anaraki   Becker    and the unbiased estimators produced by these methods are adopted
in the subsequent theoretical and empirical comparisons 
  number of other methods have been proposed to recover
covariance matrix from compressed data  Chen et al   
BioucasDias et al    Dasarathy et al    Cai et al 
  These methods are only applicable to lowrank 
sparse  or statisticallyassumed covariance matrices 
Interesting work has also been done in the area of lowrank
matrix approximation via randomized techniques  In addition to simply embedding the data   into space spanned
by   single random projection matrix      representative
study  Halko et al    improves approximation accuracy by replacing the random projection matrix   with  
lowdimensional dataaware matrix XS cid  where   cid  is   random projection matrix  However    has to be lowrank 

and computing XS cid  requires one extra pass through all entries in    It is not suitable for our settings  where we do
not impose structural assumptions on the covariance matrix  nor do we fully observe all data  Moreover   Azizyan
et al    demonstrates both theoretically and empirically that   single projection matrix for all data points cannot consistently and accurately estimate the covariance matrix  The problem also exist in  Wu et al    Mroueh
et al    aiming for   fast approximation of matrix products in   single pass  which only results in an inconsistent
covariance matrix estimation and suits the lowrank case 
Among randomized techniques  it is also worth brie  
discussing sampling approaches in matrix approximation 
Literature in  Drineas et al      Papailiopoulos et al 
  Woodruff    Holodnak   Ipsen    proposes
to leverage column sampling in which the sampling probabilities in the sampling matrix are either the column norms
or leverage scores  Other work  Woodruff    Achlioptas   Mcsherry    Achlioptas et al    performs
elementwise sampling on the entire matrix based on the
relative magnitudes over all data entries  These researches
employ different sampling distributions to sample entries
in   matrix  However  they have to observe all data fully
to calculate the sampling distributions  which also requires
one or more extra passes  In addition  their sampling probabilities are designed for matrix approximation  which cannot be trivially extended to covariance matrix estimation
because the exact covariance matrix in our setting cannot be
calculated in advance  Note that although the uniform sampling in matrix approximation is   simple onepass algorithm  it performs poorly on many problems because usually there exists structural nonuniformity in the data which
has been veri ed in  Anaraki   Becker   

  Our Approach
In this section  we  rst introduce the de nition and background to our overall work  We then justify and present our
method of data compression and covariance matrix estimation  followed by the primary results and analysis 

  Preliminaries
Let     denote   set of integers                Given   matrix     Rd    for                 we let xi   Rd denote
the ith column of    and xji denote the       th element of
  or jth element of xi  Let  Xt  
   denote the set of matrices                Xk  and xji   denote the       th element of Xt  Let XT denote the transpose of    and Tr   
denote its trace  Let     denote the absolute value of    Let
 cid   cid  and  cid   cid   denote the spectral norm and Frobenius
    xj     for
      be the  cid qnorm of     Rd  Let      be   square

norm of    respectively  Let  cid   cid      cid  

Toward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

diagonal matrix with the elements of vector   on the main
diagonal  and      also be   square diagonal matrix whose
main diagonal has only the main diagonal elements of   

  Method and Algorithm

As discussed previously  GaussInverse  Azizyan et al 
  Qi   Hughes    and Sparse  Anaraki   Hughes 
  Anaraki    suffer from de ciencies in either
computational ef ciency or estimation accuracy  whereas
UniSampleHD  Anaraki   Becker    is less accurate
but offers   good tradeoff between estimation accuracy and
computational ef ciency  We thus propose the adoption of
     Rd   to compress
weighted sampling matrices  Si  
  xi and then backproject the compressed data
data via ST
into the original space via SiST
  xi  The recovered data
is then used for covariance matrix estimation as shown in
Eq    Hence    high computational ef ciency is maintained  Although Si removes at least     entries from the
ith vector  the remainders can be the most informative and
are retained  With the carefully designed sampling probabilities  our unbiased estimator Ce performs as accurately
as or more accurately than its counterparts asymptotically
in terms of matrix spectral norm  cid Ce     cid  Note we
have not quanti ed the error in any other entrywise norm
      the Frobenius norm  that could be uninformative on
the quality of the approximate invariant subspace and unstable regarding the additive random error  Anaraki   
Achlioptas et al    Gittens   

Algorithm   The proposed algorithm 
Input 

Data     Rd    sampling size    and          
Estimated covariance matrix Ce   Rd   
Rn with  

Output 
  Initialize     Rm        Rm        Rn  and    
  for all         do
 

Load xi into memory  let vi    cid xi cid   cid  

    xki 

   cid  

     
ki

and wi    cid xi cid 
for all         do

 
 

Pick tji       with pki     tji         
        

  and let yji   xtjii

ki
wi

 xki 
vi

 

   and   to the fusion center 

  Pass the compressed data    sampling indices      
  for all         do
 
 
 

Initialize Si   Rd   and     Rd   with  
for all         do
Let ptjii    
 
mptjii

  Compute Ce as de ned in Eq    by using  Si  

  and stjij    

       

 yji 
vi

  
ji
wi

  

      and   

We here summarize our method in Algorithm   In   nutshell  we employ   weighted sampling that is able to explore the most important entries to reduce estimation error
 cid Ce     cid  Steps   to   in our proposed algorithm show
how to compress distributed data in many remote sites 
In step   each entry is retained with probability proportional to the combination of its relative absolute value and
square value  and such sampling probability is designed to
make  cid Ce     cid  as small as possible  Step   shows the
communication procedure  and steps   to   reveal how to
construct an unbiased covariance matrix estimator in the
fusion center from compressed data  In many computing
cases  it is possible to manipulate vectors of length     
in memory  and thus when compressing data via weighted
sampling  only one pass is required to move data from the
external space to memory  Hence  our algorithm is also
applicable to streaming data  For   covariance matrix den XXT      xT   we can exactly calculate
 ned as      
   xi in the fusion center via       
   uj 
      
 
   are from    cid    remote sites  and uj   Rd
where  xi  
is the summation of all data vectors in the jth remote site
before being compressed  Doing so makes no deviation on
the following error analysis and imposes only   negligible
computational burden 

 cid  

 cid  

 

  Primary Provable Results

  

  xi  
   and  ST

   and   is able to obtain  ST

In this part  we introduce the proposed covariance matrix
In Algorithm   we employ          and  
estimator 
to calculate  Si  
It can be veri ed that using only
 Si  
   Thus  we
describe our estimator via  Si  
  xi  
   in the
following theorem  which shows our estimator is unbiased 
Theorem   Assume     Rd   and the sampling size
           Sample   entries from each xi   Rd with replacement by running Algorithm   Let  pki  
   and Si  
Rd   denote the sampling probabilities and sampling matrix  respectively  Then  the unbiased estimator for the target covariance matrix      
  XXT can
 
be recovered as

 cid  
Ce    cid     cid   
 cid  
where    Ce        cid       
 cid  
and  cid     

   
  SiST
  xixT
     bi  with

   SiST
  SiST

nm  
  SiST

   xixT

     

  xixT

 

  

bki  

 

   pki

 

nm  
 

  xixT

  SiST

Note that at most   entries in each bi have to be calculated
because each SiST
  has at most   nonzero entries in its diagonal  Now  having achieved the above unbiased estimator Ce  we analyze its properties  We precisely
upper bound the estimation error for the original estimator
  in the matrix spectral norm 
Theorem   Given     Rd   and the sampling size    
       let   and Ce be de ned as in Theorem   If the

Toward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

 cid 

    
sampling probabilities satisfy pki    
 cid xi cid 
with           for all         and         then with
probability at least          

 xki 
 cid xi cid 

ki

 

 cid Ce     cid    log 

 

 

 

  
 

  
 

  
 

 cid   cid xi cid 
 cid 
   cid cid  
        cid xi cid 

 cid 
  log 
     cid xi cid 
    log   nd
        cid xi cid 
 cid xi cid 
    
 cid xi cid 
      cid 

 xix 

nm 

  

 

 

 

 

 

 

 

  and

   cid  

where     maxi   
 cid xi cid 

 cid 

   cid xi cid 

  
 cid xi cid 
    

 

 

 

ki

ki

 xki 
 cid xi cid 

          
 cid xi cid 

  large   and   work against the accuracy of Ce  Accordingly  our sampling probabilities are designed to make
  and   as small as possible to improve the accuracy  In
the proof of Theorem   we also show that the selection of
 cid  
 xki  
    xki   used for constructing the sampling
        in
is necessary and

probability pki    
suf cient to make the error bound considerably tight 
Furthermore    balances the performance by  cid norm
based sampling  xki 
and  cid norm based sampling   
 
 cid xi cid 
 cid xi cid 
 cid  sampling penalizes small entries more than  cid  sampling 
Hence  cid  sampling is more likely to select larger entries to
decrease error  However  as seen from the proof in the appendix  different from  cid  sampling   cid  sampling is unstable
and sensitive to small entries  and it can make estimation
error incredibly high if extremely small entries are picked 
Hence  if   varies from   to   the estimation error will decrease and then increase  which is also empirically veri ed
in the appendix 
The error bound in Theorem   involves many datadependent quantities  whereas our primary interest lies in
studying the tradeoff between the computational ef ciency
and estimation accuracy by employing weighted sampling
to compress data and estimate covariance matrix  To clarify  we modify Theorem   and make the bound explicitly
dependent on       and   with the constraint           
Corollary   Given     Rd   and the sampling size    
       let   and Ce be created by Algorithm   De ne
 cid xi cid 
   and  cid xi cid      for all
 cid xi cid 
        Then  with probability at least           we have

    with          

where        
logarithmic factors on              and  

       

nm      

The formulation above explores the fact

that    
  by the CauchySchwarz inequality 

 cid xi cid cid xi cid     

 cid Ce     cid    min cid  

 cid 

 cid  

   

   
 

 cid 

   

 cid 

     

   
 
  cid   cid 

 cid   

 cid   
 cid 
 cid   
 cid cid   cid 
nm   and  cid    hides the

nm
  cid   cid 
nm

 cid 

   

 

 

 

 

 

 

nd cid   cid 

     cid   cid    maxi     cid xi cid 

Before proceeding  we make several remarks to make  
comparison with the following representative work  GaussInverse  UniSampleHD  and Sparse  The  rst two methods
provide error analysis without assuming data distribution 
which is shown in  Azizyan et al    Anaraki   Becker 
  and illustrated in our appendix  In the following remarks  only our method is sensitive to   and we also employ the fact that  
   
    to simplify all asymptotic bounds 
Remark   Eq    with    
  indicates the error bound
for our estimator Ce in the worst case  where the magnitudes of each entry in all of the input data vectors are the
same       highly uniformly distributed  Even in this case 
nm  

our error bound has   leading term of order min cid  
 cid    cid   cid 

 cid      
 cid  which is the

 
same as GaussInverse ignoring logarithmic factors 
In
contrast  as the magnitudes of the entries in each data vector become uneven    gets smaller  leading to   tighter error bound than that in GaussInverse  Furthermore  when
most of the entries in each vector xi have very low magnitudes  the summation of these magnitudes will be comparable to   particular constant  This situation is typical because in practice only   limited number of features in each input data dominate the learning performance  Hence    turns to    and Eq    becomes

 cid  which is
min cid  
of at least cid      As explained in the next section  Gauss 

tighter than the leading term of GaussInverse by   factor

       cid   
 cid     

 cid    cid   cid 

 cid cid   cid 

 cid      

 cid   

nm      

 cid     

nm      

 cid  

 cid  

     

 cid 

 cid 

nm

nm

 

 

 

 

 

nm

nm

 

   

 cid 

nm    

nm       

nm       

 cid   

 cid    cid   cid 

   and at least     when       

Inverse also lacks computational ef ciency 
Remark   As our target is to compress data to   smaller
  that is not comparable to   in practice          
can be approximately regarded as      Then  the error
 
which is asymptotically worse than our bound  When  
is suf ciently large  the leading term of its error becomes
  which can be weaker than the

 cid      
of UniSampleHD is  cid  
 cid    cid   cid 
 cid   
 cid 
 cid 
 cid  
leading term in our method by   factor of   to cid     when
 cid    cid   cid 

the error of UniSampleHD becomes  cid  
 cid   
factor of cid          when    

However  if   is suf ciently close to    which is not meaningful for practical usage                will hold and
nm  
  This bound may slightly outperform ours by  
   but is still worse
than ours when        These results also coincide with
the fact that UniSampleHD adopts uniform sampling without replacement combined with the Hadamard matrix  but
we employ weighted sampling with replacement 
Remark   The Sparse method  which employs   sparse

 cid      

nm  

 cid 

 

   
 

nm

Toward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

matrix for each Si  is not suf ciently accurate as demonstrated in our experiments  Moreover  there is no error analysis available for its unbiased estimator to characterize the
estimation error versus the compression ratio 

Thus far  we have not made statistical nor structural assumptions concerning the input data or covariance matrix
to derive our provable results  However  motivated by  Azizyan et al    it is also straightforward to extend our
results to the statistical data and   lowrank covariance matrix estimation  The derived results below are polynomially
equivalent to those in GaussInverse  Azizyan et al   
Corollary   shows the  lowrank  covariance matrix estimation on Gaussian data  and Corollary   indicates the derived covariance estimator also guarantees the accuracy of
the principal components regarding the subspace learning 
Corollary   Given     Rd          and an unknown
population covariance matrix Cp   Rd   with each column vector xi   Rd        generated from the Gaussian distribution     Cp  Let Ce be constructed by Algorithm  
with the sampling size            Then  with probability
at least              
 cid Ce   Cp cid 

 
Additionally  assuming rank Cp     with probability at
least               we have

 cid    

   cid  

 cid Cp cid 

 cid 

 cid 

 
 

nm

 
 

 

 

 cid  rd

 cid  

 cid 

 cid 

 cid 

 

 cid Ce     Cp cid 

 

 
 

 
 

 

rd
nm

nm

 cid Cp cid 

Let  cid 

     cid  

 
where  Ce   is the solution to minrank      cid     Ce cid 

and  
Corollary   Given          Cp and Ce as de ned
in Corollary  
   uiuT
   
   being the leading
  eigenvectors of Cp and Ce  respectively  Denote by   
the kth largest eigenvalue of Cp  Then  with probability at
least              

and  cid    hides the logarithmic factors on               
  and  cid cid 
 cid  
 cid cid cid 
   cid 
where the eigengap              and  cid    hides the

   and  ui  
 cid    

  with  ui  

        

 cid Cp cid 

    ui  uT

 cid 

   cid 

 cid  

 
 

 cid 

 

 
 

 

nm

 

 

 

logarithmic factors on                and  

The proof details of all our theoretical results are relegated to the appendix  We leverage the Matrix Bernstein
inequality  Tropp    and establish the error bound of
our proposed estimator on an arbitrary sampling probability in order to determine which sampling probability brings
the best estimation accuracy  The employment of the Matrix Bernstein inequality involves controlling the range and

variance of all zeromean random matrices  whose derivations differ from those in  Azizyan et al    Anaraki
  Becker    because of different data compression
schemes  Moreover  to obtain the desired tight bound for
the range and variance  we precisely provide   group of
closedform equalities or concentration inequalities for various quantities  see our proposed Lemma   and Lemma  
along with their proofs in the appendix 

  Computational Complexity

Recall that we have   data samples in the ddimensional
space  and let   be the target compressed dimension  Ren XXT   the computational comgarding estimating      
parisons between our method and the representative baseline methods are presented in Table   in which Standard method means that we compute   directly without
data compression  For the de nition of covariance matrix
  XXT      xT   extra computational costs         gd 
     
storage    gd  communication cost  and   nd  time  must
be added to the last four compression methods in the table 
where    cid    is the number of the entire remote sites  All
detailed analysis is relegated to the appendix 
TG and TS in Table   represent
the time taken to
generate the Gaussian matrices and sparse matrices by
fast pseudorandom number generators like Mersenne
Twister  Matsumoto   Nishimura    which can be
enormous  Anaraki   Becker    and proportional to
nmd and nd  respectively  up to certain small constants 
Hence  our method can be regarded as the most ef cient
when   is large  Furthermore  by using the smallest   to
obtain the same estimation accuracy as the other methods 
our approach incurs the least computational burden 

  Empirical Studies
In this section  we empirically verify the properties of the
proposed method and demonstrate its superiority  We compare its estimation accuracy with that of GaussInverse 
Sparse  and UniSampleHD  We also report the time comparisons 
We run all algorithms on both synthetic and realworld
datasets whose largest dimension is around and below  
Such dimension is not very high in modern data analysis 
but this limitation is due to that reporting the estimation
error by calculating the spectral norm of   covariance matrix with its size larger than       will take intolerable
amount of memory and time  The parameter selection on  
is deferred to the appendix  and we empirically set      
To allow   fair comparison of the time consumptions measured by FLOPS  we implement all algorithms in    and
run them in   single thread mode on   standard workstation
with Intel CPU GHz and  GB RAM 

Toward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

Table   Computational costs on the storage  communication  and time 

Storage

Communication

Method
Standard

GaussInverse

  nd     
  nm     
  nm     
UniSampleHD   nm     
  nm     

Our method

Sparse

  nd 
  nm 
  nm 
  nm 
  nm 

Time
  nd 

  nmd   nm     nd    TG

      nm    TS
  nd log     nm 

  nd   nm log     nm 

  Experiments on Synthetic Datasets

To clearly examine the performance  we compare all methods on six synthetic datasets   Xi 
       
                and     
   which are generated based on the generative model  Liberty    Speci cally  given   matrix
    Rd   from such model  it is formally de ned as
    UFG  where     Rd   de nes the signal column
space with UT     Ik         the square diagonal matrix
    Rk   contains the diagonal entries fii       
that gives linearly diminishing signal singular values  and
    Rk   is the signal coef cient with gij         that
is the Gaussian distribution  We let         then setting       and       completes the creation of
data    For    it is de ned as DX  where each entry in
the square diagonal matrix   is de ned by dii       and
   is randomly sampled from the integer set   Regarding    it is constructed in the same way as    except that
  now becomes an identity matrix  Next   Xi 
   follow
the same generation strategy of    except for the   and   

Figure   Accuracy comparisons of covariance matrix estimation
on synthetic datasets  The estimation error is measured by
 cid Ce     cid cid   cid  with Ce calculated by all compared methods 
and cf       is the compression ratio 

In Figure   we plot the relative estimation error averaged
over ten runs with its standard deviation versus the naive
compression ratio cf        Note that   large cf is not
necessary for practical usage  and our aim is to compress
data to   smaller volume  In Figure   we report the running
time taken in both the compressing and recovering stages 
which preliminarily depicts the ef ciency of the different
methods and indicates how much power should be spent in
the practical computation 

 

 

Generally  our method displays the least error and deviation for all datasets and its error decreases dramatically
with an increase at   small cf  This observation indicates
that our method can achieve suf cient estimation accuracy
by using substantially fewer data entries than the other
   the magnitudes of the
methods  For         
data entries are highly uniformly distributed  and thus our
method can be regarded as uniform sampling with replacement  which may perform slightly worse than UniSampleHD and GaussInverse if cf becomes large enough  After allowing the magnitudes to vary within   moderately
   our method considerlarger range in         
ably outperforms the other three methods  Its improvement
comes from that only our method is sensitive to   and  
smaller   produces   tighter result  as demonstrated by Re 
 
marks   and   However  the error of each method in   
 
   is larger than that in   
   respectively  It is be 

   cid cid   cid           
   cid cid   cid           
cause of that almost all methods are sensitive to    cid cid   cid 
and the error  cid Ce     cid cid   cid  increases when    cid cid   cid 
though the   and    cid cid   cid  in    are approximately equal

rises  Such phenomenon is demonstrated via dividing numerous error bounds in Remarks   and   by  cid   cid  Our
method also achieves the best performance in    Al 

with those in    yet the proved error bounds with Remarks   and   reveal that   larger   in    will lead to
smaller estimation errors given the same cf  Finally  our
method also achieves the best accuracy when the dimension
  increases in both    and    Besides  taking more data
      enlarging    as suggested by    can be considered to
reduce the error in    and    Note that GaussInverse
has not been run on    since it costs enormous time 

Figure   Time comparisons of covariance matrix estimation on
synthetic datasets  Rescaled time results from the running time
normalized by that spent in the Standard way of calculating    
XXT    without data compression  and it is plotted in log scale 

Turning to GaussInverse  it becomes highly accurate when

    Error         GaussInverseSparseUniSample HDOur methodm   Error         GaussInverseSparseUniSample HDOur methodm   Error         GaussInverseSparseUniSample HDOur methodm   Error         GaussInverseSparseUniSample HDOur methodm   Error         GaussInverseSparseUniSample HDOur methodm   Error         SparseUniSampleHDOur methodm   Rescaled Time         GaussInverseSparseUniSample HDOur methodStandardm   Rescaled Time         GaussInverseSparseUniSample HDOur methodStandardm   Rescaled Time         GaussInverseSparseUniSample HDOur methodStandardToward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

cf increases but requires much more time than Standard
 see Figure   so that its usage might be ruled out in practice  However  GaussInverse remains   good choice when
we are in urgent need of reducing the storage and communication burden  Sparse  which has no error analysis of its
unbiased estimator  generally performs less accurately than
the others but requires less time than Standard  UniSampleHD is ef cient while it still consumes more time than our
method  Also  its accuracy is inferior to our method especially when cf is small  In conclusion  our method is capable of compressing data to   very small size while guaranteeing both estimation accuracy and computational ef 
ciency 

  Experiments on Realworld Datasets

In the second set of experiments  we use nine publicly
available realworld datasets  Chang   Lin    Blake
  Merz    Amsaleg    some of which are gathered from many distributed sensors  Their statistics are
displayed in Figure   We again compare the estimation
accuracy of the proposed method against the other three
approaches  As can be seen from the  gure  our method
consistently exhibits superior accuracy over all cf       
and its error decreases dramatically when cf grows  The
error of the other three methods also decreases with cf but
is still large at   small cf  Besides  our method enjoys the
least deviation  In summary  these results con rm that our
method can compress data to the lowest volume with the
best accuracy  thereby substantially reducing storage  communication  and processing time cost in practice 

Figure   Convergence rates of our method for the settings in
Corollaries   and  

 

As con rmed in Figure     large   bene ts the estimation
accuracy  Thus  we study its effect more quantitatively  We
conduct experiments following the settings as de ned in
 
Corollaries   and   and their results in Eqs    clearly
  convergence rate if    cid 
show that the errors decay in  
   We run our method on another two synthetic datasets
 Xt 
     Rd   that follow the ddimensional multivariate normal distribution     Cpt  where the       th element of Cp    Rd   is       and Cp    Rd   is
  lowrank matrix that satis es minrank      cid     Cp cid 
We take                        
          and vary   from   to   In Figure   the top three plots report the errors as de ned in the
LHS of Eqs    respectively  Then  dividing such errors by  
The observation that the curves in plots       are roughly
 
 at validates that the error bounds induced by our method
  convergence rate  which
decay rapidly with   in the  
coincides with Eqs   
In addition to the fast error
convergence for the lowrank matrix Cp  our method can
also obtain an increasingly better estimation accuracy for  
highrank covariance matrix Cp  if we enlarge    which is
displayed in plot     Besides  considering the omitted plot
where the eigengap       of Cp  decreases with    the
fact that the errors in plot     increase with   also coheres
with Eq    To conclude  our method also adapts well to
 
the speci   settings in Corollaries   and   and all induced
error bounds indeed satisfy    

  obtains the bottom three plots accordingly 

Figure   Accuracy comparisons of covariance matrix estimation
on realworld datasets 

  Conclusion
In this paper  we describe   weighted sampling method for
accurate and ef cient calculation of an unbiased covariance matrix estimator  The analysis demonstrates that our
method can employ   smaller data volume than the other
approaches to achieve an equal accuracy  and is highly ef 
cient regarding the communication  storage  and processing
time  The empirical results of the algorithm   application to
both synthetic and realworld datasets further support our
analysis and demonstrate its signi cant improvements over
other stateof theart methods 
Compared with the samplingwith replacement scheme in
this paper  we seek to make more achievements via  
samplingwithout replacement scheme in the future work 
Analyzing the corresponding unbiased estimator will pose
signi cant technical challenges in this research direction 

  convergence rate 

  Error                                                     Rescaled Error           Error                    Rescaled Error        Error                                                                       Rescaled Error          Error DailySports       GaussInverseSparseUniSample HDOur methodm   Error Arcene       GaussInverseSparseUniSample HDOur methodm   Error Slice       GaussInverseSparseUniSample HDOur methodm   Error Cifar       GaussInverseSparseUniSample HDOur methodm   Error Gist         GaussInverseSparseUniSample HDOur methodm   Error Mnist       GaussInverseSparseUniSample HDOur methodm   Error Isolet       GaussInverseSparseUniSample HDOur methodm   Error           GaussInverseSparseUniSample HDOur methodm   Error UJIIndoorLoc       GaussInverseSparseUniSample HDOur methodToward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

Acknowledgments
We truly thank Akshay Krishnamurthy for the fruitful discussions and interpretations on  Azizyan et al    We
also thank Yuxin Su for the help on the experiments  The
work described in this paper was fully supported by the
National Natural Science Foundation of China  Project
No    the Research Grants Council of the Hong
Kong Special Administrative Region  China  No  CUHK
  and No  CUHK   of the General Research Fund  and   Microsoft Research Asia Collaborative Research Program  Project No  FY RESTHEME 

References
AbbasiDaresari     and Abouei     Toward clusterbased
weighted compressive data aggregation in wireless sensor networks  Ad Hoc Networks     

Abrahamsson     Selen     and Stoica     Enhanced covariance matrix estimators in adaptive beamforming  In
Acoustics  Speech and Signal Processing    ICASSP
  IEEE International Conference on  volume   pp 
II  IEEE   

Achlioptas     and Mcsherry     Fast computation of lowrank matrix approximations  Proceedings of the annual
ACM symposium on Theory of computing     

Achlioptas     Karnin        and Liberty     Nearoptimal
In Advances in
entrywise sampling for data matrices 
Neural Information Processing Systems  pp   
 

Ailon     and Chazelle     The fast johnsonlindenstrauss
SIAM

transform and approximate nearest neighbors 
Journal on Computing     

Amsaleg     Datasets for approximate nearest neighbor

search   

Anaraki     Estimation of the sample covariance matrix
IET Signal Process 

from compressive measurements 
ing   

Anaraki     and Becker     Preconditioned data sparsi cation for big data with applications to pca and kmeans 
IEEE Transactions on Information Theory   

Anaraki     and Hughes     Memory and computation ef 
In Procient pca via very sparse random projections 
ceedings of the  st International Conference on Machine Learning  ICML  pp     

Anzai     Pattern Recognition   Machine Learning  Else 

vier   

Azizyan     Krishnamurthy     and Singh     Extreme
compressive sampling for covariance estimation  arXiv
preprint arXiv   

Bartz     Advances in highdimensional covariance matrix

estimation   

BioucasDias     Cohen     and Eldar        Covalsa  Covariance estimation from compressive measurements using alternating minimization  In Signal Processing Conference  EUSIPCO    Proceedings of the  nd European  pp    IEEE   

Blake       and Merz       UCI repository of machine

learning databases   

Butte        Tamayo     Slonim     Golub        and Kohane        Discovering functional relationships between
rna expression and chemotherapeutic susceptibility using relevance networks  Proceedings of the National
Academy of Sciences     

Cai        Zhang     et al  Rop  Matrix recovery via rankone projections  The Annals of Statistics   
 

Chang  ChihChung and Lin  ChihJen  Libsvm    library
for support vector machines  ACM Transactions on Intelligent Systems and Technology  TIST     

Chen     Chi     and Goldsmith     Exact and stable covariance estimation from quadratic sampling via convex
programming   

Dasarathy     Shah     Bhaskar        and Nowak       
Sketching sparse matrices  covariances  and graphs via
tensor products  Information Theory  IEEE Transactions
on     

Deisenroth        Neumann     Peters     et al    survey
on policy search for robotics  Foundations and Trends in
Robotics     

Drineas     Kannan     and Mahoney        Fast monte
carlo algorithms for matrices    Approximating matrix
multiplication  SIAM Journal on Computing   
     

Drineas     Mahoney        and Muthukrishnan     Subspace sampling and relativeerror matrix approximation 
In Approximation  Randomization  and Combinatorial
Optimization     

Feller     introduction to probability theory and its appli 

cations  vol  ii an   

Gittens     The spectral norm error of the naive nystrom

extension  arXiv preprint arXiv   

Toward Ef cient and Accurate Covariance Matrix Estimation on Compressed Data

Gleichman     and Eldar        Blind compressed sensing  Information Theory  IEEE Transactions on   
   

Qi     and Hughes        Invariance of principal components under lowdimensional random projection of the
data  In Image Processing  IEEE   

Ha     and Barber        Robust pca with compressed data 
In Advances in Neural Information Processing Systems 
pp     

Sch afer     and Strimmer     An empirical bayes approach to inferring largescale gene association networks  Bioinformatics     

Shi     Tang     Xu     and Moscibroda     Correlated
In UAI  pp 

compressive sensing for networked data 
   

Srisooksai     Keamarungsi     Lamsrichan     and
Araki     Practical data compression in wireless sensor
networks    survey  Journal of Network and Computer
Applications     

Tropp        Improved analysis of the subsampled randomized hadamard transform  Advances in Adaptive Data
Analysis       

Tropp        An introduction to matrix concentration inequalities  Foundations and Trends in Machine Learning     

Tulino        and Verd       Random matrix theory and
wireless communications  volume   Now Publishers
Inc   

Woodruff        Sketching as   tool for numerical linear

algebra  arXiv preprint arXiv   

Wu     Bhojanapalli     Sanghavi     and Dimakis    
In Advances In
Single pass pca of matrix products 
Neural Information Processing Systems  pp   
 

Zou     Hastie     and Tibshirani     Sparse principal component analysis  Journal of computational and
graphical statistics     

Halko     Martinsson     and Tropp        Finding structure with randomness  Probabilistic algorithms for constructing approximate matrix decompositions  SIAM review     

Hansen        Large sample properties of generalized
method of moments estimators  Econometrica  Journal
of the Econometric Society  pp     

Haupt     Bajwa        Rabbat     and Nowak     Compressed sensing for networked data  IEEE Signal Processing Magazine     

Holodnak        and Ipsen        Randomized approximation
of the gram matrix  Exact computation and probabilistic
bounds  SIAM Journal on Matrix Analysis and Applications     

Hyv arinen     Karhunen     and Oja     Independent component analysis  volume   John Wiley   Sons   

Kariya     and Kurata     Generalized least squares  John

Wiley   Sons   

Li     Hastie        and Church        Very sparse random
projections  In Proceedings of the  th ACM SIGKDD
international conference on Knowledge discovery and
data mining  ACM   

Liberty     Simple and deterministic matrix sketching 
In Proceedings of the  th ACM SIGKDD international
conference on Knowledge discovery and data mining 
pp    ACM   

Mahoney     Randomized algorithms for matrices and
data  Foundations and Trends in Machine Learning   
   

Matsumoto     and Nishimura     Mersenne twister 
   dimensionally equidistributed uniform pseudorandom number generator  ACM Transactions on Modeling and Computer Simulation   

Mroueh     Marcheret     and Goel     Cooccuring directions sketching for approximate matrix multiply  arXiv
preprint arXiv   

Papailiopoulos     Kyrillidis     and Boutsidis     Provable deterministic leverage score sampling  In Proceedings of the  th ACM SIGKDD international conference
on Knowledge discovery and data mining  pp   
ACM   

