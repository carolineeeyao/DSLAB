Faster Principal Component Regression

and Stable Matrix Chebyshev Approximation

Zeyuan AllenZhu     Yuanzhi Li    

Abstract

We solve principal component regression  PCR 
up to   multiplicative accuracy   by reducing

the problem to  cid    blackbox calls of ridge

regression  Therefore  our algorithm does not require any explicit construction of the top principal components  and is suitable for largescale
In contrast  previous result rePCR instances 

quires  cid    such blackbox calls  We obtain

this result by developing   general stable recurrence formula for matrix Chebyshev polynomials  and   degreeoptimal polynomial approximation to the matrix sign function  Our techniques
may be of independent interests  especially when
designing iterative methods 

  Introduction
In machine learning and statistics  it is often desirable to
represent   largescale dataset in   more tractable  lowerdimensional form  without losing too much information 
One of the most robust ways to achieve this goal is through
principal component projection  PCP 

PCP  project vectors onto the span of the top prin 

cipal components of the   matrix 

It is wellknown that PCP decreases noise and increases ef 
 ciency in downstream tasks  One of the main applications
is principal component regression  PCR 

PCR  linear regression but restricted to the sub 

space of top principal components 

Classical algorithms for PCP or PCR rely on   principal
component analysis  PCA  solver to recover the top principal components  rst  with these components available  the

 Equal contribution  

Future version of this paper shall
be found at https arxiv org abs 
 Microsoft Reseaerch  Princeton University  Correspondence
to  Zeyuan AllenZhu  zeyuan csail mit edu  Yuanzhi Li
 yuanzhil cs princeton edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tasks of PCP and PCR become trivial because the projection matrix can be constructed explicitly 
Unfortunately  PCA solvers demand   running time that
at least linearly scales with the number of top principal
components chosen for the projection  For instance  to
project   vector onto the top   principal components of
  highdimensional dataset  even the most ef cient Krylovbased  Musco   Musco    or Lanczosbased  AllenZhu   Li      methods require   running time that is
proportional to           times the input matrix
sparsity  if the Krylov or Lanczos method is executed for
  iterations  This is usually computationally intractable 
  Approximating PCP Without PCA
In this paper  we propose the following notion of PCP approximation  Given   data matrix     Rd cid    with singular values no greater than   and   threshold       we
say that an algorithm solves    approximate PCP if  
informally speaking and up to   multiplicative   error 
it projects  see Def    for   formal de nition 

  eigenvector   of   cid   with value in cid       cid  to  
  eigenvector   of   cid   with value in cid     cid  to  cid 
  eigenvector   of   cid   with value in cid         
 cid  to  anywhere between  cid  and  

Such   de nition also extends to    approximate PCR
 see Def   
It was  rst noticed by Frostig et al   Frostig et al   
that approximate PCP and PCR be solved with   running
time independent of the number of principal components
above threshold   More speci cally  they reduced    
approximate PCP and PCR to

  cid  log cid  blackbox calls of

any ridge regression subroutine

where each call computes    cid          for some vector    Our main focus of this paper is to quadratically
improve this performance and reduce PCP and PCR to

 Ridge regression is often considered as an easyto solve machine learning problem  using for instance SVRG  Johnson  
Zhang    one can usually solve ridge regression to an  
accuracy with at most   passes of the data 

  cid  log cid  blackbox calls of

any ridge regression subroutine

where each call again computes    cid          
Remark   Frostig et al  only showed their algorithm
satis es the properties   and   of    approximation
 but not
the property   and thus their proof was
only for matrix   with no singular value in the range

 cid     cid      This is known as the eigengap

assumption  which is rarely satis ed in practice  Musco  
Musco    In this paper  we prove our result both with
and without such eigengap assumption  Since our techniques also imply the algorithm of Frostig et al  satis es
property   throughout the paper  we say Frostig et al  solve
   approximate PCP and PCR 
  From PCP to Polynomial Approximation
The main technique of Frostig et al 
struct   polynomial
sgn             

is to conto approximate the sign function

sgn     

 cid         
 cid cid     cid cid                

       

In particular  given any polynomial      satisfying

 cid cid        sgn   cid cid                    

 
 
the problem of    approximate PCP can be reduced to
computing the matrix polynomial      for        cid    
     cid          cf  Fact   In other words 
  to project any vector     Rd to top principal compo 

nents  we can compute      instead  and

for each evaluation of Su for some vector   

  to compute      we can reduce it to ridge regression
Remark   Since the transformation from   cid   to   is
not linear  the  nal approximation to the PCP is   rational function  as opposed to   polynomial  over   cid    We
restrict to polynomial choices of    because in this way 
the  nal rational function has all the denominators being
  cid         thus reduces to ridge regressions 
Remark   The transformation from   cid   to   ensures
that all the eigenvalues of   cid   in the range      
roughly map to the eigenvalues of   in the range    
Main Challenges  There are two main challenges regarding the design of polynomial     
  EFFICIENCY  We wish to minimize the degree    
deg      because the computation of      usually
requires   calls of ridge regression 

  STABILITY  We wish      to be stable  that is      
must be given by   recursive formula where if we make
 cid  error in each recursion  due to error incurred from
ridge regression  the  nal error of      must be at
most  cid    poly   

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Remark   Ef cient routines such as SVRG  Johnson  
Zhang    solve ridge regression and thus compute Su
for any     Rd  with running times only logarithmically in
 cid  Therefore  by setting  cid     poly    one can blow
up the running time by   small factor   log    in order to
obtain an  accurate solution for     
The polynomial      constructed by Frostig et al 
comes from truncated Taylor expansion 
It has degree

  cid  log cid  and is stable  This   dependency lim 

its the practical performance of their proposed PCP and
PCR algorithms  especially in   high accuracy regime  At
the same time 
  the optimal degree for   polynomial to satisfy even

only   is  cid  log cid   Eremenko   Yuditskii 

   

 

Frostig et al  were unable to  nd   stable polynomial
matching this optimal degree and left it as open question 
  Our Results and Main Ideas
We provide an ef cient and stable polynomial approximation to the matrix sign function that has   nearoptimal degree    log  At   high level  we construct  

for some       then we set              
which approximates sgn   

no singular point on     so we can apply Chebyshev approximation theory to obtain some      of degree
   log  satisfying

 cid 
polynomial      that approximately equals cid    
 cid  has
To construct      we  rst note that  cid    
 cid cid cid       cid           
 cid cid cid cid      for every        
This can be shown to imply cid cid        sgn   cid cid      for every
      cid           
 cid 

            so   is satis ed 
In order to prove     we prove   separate lemma 
for every            

 

 

 

 

 

Note that this does not follow from standard Chebyshev
theory because Chebyshev approximation guarantees are
only with respect to        
This proves the  EFFICIENCY  part of the main challenges
discussed earlier  As for the  STABILITY  part  we prove  
general theorem regarding any weighted sum of Chebyshev
polynomials applied to matrices  We provide   backward
recurrence algorithm and show that it is stable under noisy

nomial      of degree   cid  log cid  satisfying   How 

 Using degree reduction  Frostig et al  found an explicit poly 

ever  that polynomial is unstable because it is constructed monomial by monomial and has exponentially large coef cients in front
of each monomial  Furthermore  it is not clear if their polynomial
satis es the  

 We proved   general lemma which holds for any function

whose all orders of derivatives are nonnegative at      

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

computations  This may be of independent interest 
For interested readers  we compare our polynomial     
with that of Frostig et al  in Figure  

  Related Work

There are   few attempts to reduce the cost of PCA when
solving PCR  by for instance approximating the matrix
AP  where    is the PCP projection matrix  Chan  
Hansen    Boutsidis   MagdonIsmail    However  they cost   running time that linearly scales with the
number of principal components above  
  signi cant number of papers have focused on the lowrank case of PCA  Musco   Musco    AllenZhu  
Li        and its online variant  AllenZhu   Li 
    Unfortunately  all of these methods require   running time that scales at least linearly with respect to the
number of top principal components 
More related to this paper is work on matrix sign function  which plays an important role in control theory and
quantum chromodynamics  Several results have addressed
Krylov methods for applying the sign function in the socalled Krylov subspace  without explicitly constructing
any approximate polynomial  van den Eshof et al   
Schilders et al    However  Krylov methods are not
   approximate PCP solvers  and there is no supporting
stability theory behind them  Other iterative methods have
also been proposed  see Section   of textbook  Higham 
  For instance  Schur   method is   slow one and
also requires the matrix to be explicitly given  The Newton   iteration and its numerous variants        Nakatsukasa
  Freund    provide rational approximations to the
matrix sign function as opposed to polynomial approximations  Our result and Frostig et al   Frostig et al    differ from these cited works  because we have only accessed
an approximate ridge regression oracle  so ensuring   polynomial approximation to the sign function and ensuring its
stability are crucial 
Using matrix Chebyshev polynomials to approximate matrix functions is not new  Perhaps the most celebrated example is to approximate    using polynomials on    used
in the analysis of conjugate gradient  Shewchuk    Independent from this paper  Han et al   Han et al   
used Chebyshev polynomials to approximate the trace of
the matrix sign function       Tr sgn    which is similar but   different problem  Also  they did not study the

 We anyways have included Krylov method in our empirical
evaluation section and shall discuss its performance there  see the
full version of this paper 

 Their paper appeared online two months before us  and we

became aware of their work in March  

 In particular  their degree of the Chebyshev polynomial is

  cid log    log  log cid  in the language of this
paper  in contrast  we have degree   cid  log cid 

case when the matrixvector multiplication oracle is only
approximate  like we do in this paper  or the case when  
has eigenvalues in the range    
Roadmap 
In Section   we provide notions for this paper and basics for Chebyshev polynomials  In Section  
we formally de ne approximate PCP and PCR  and reduce
PCR to PCP  In Section   we show   general lemma for
In Section   we design our
Chebyshev approximations 
polynomial approximation to sgn   
In Section   we
show how to stably compute Chebyshev polynomials  In
Section   we state our main theorems regarding PCP and
PCR  In Section   we provide empirical evaluations 
  Preliminaries
We denote by           the indicator function for event
   by  cid   cid  or  cid   cid  the Euclidean norm of   vector    by   
the MoorePenrose pseudoinverse of   symmetric matrix
   and by  cid   cid  its spectral norm  We sometimes use  cid  
to emphasize that   is   vector 
Given   symmetric       matrix   and any        
         is the matrix function applied to    which
to Udiag                  Dd   cid  if    
is equal
Udiag            Dd   cid  is its eigendecomposition 
Throughout the paper  matrix   is of dimension   cid      
We denote by  max    the largest singular value of   
Following the tradition of  Frostig et al    and keeping
the notations light  we assume without loss of generality
that  max        We are interested in PCP and PCR
problems with an eigenvalue threshold        
Throughout the paper  we denote by             
  the eigenvalues of   cid    and by                Rd
the eigenvectors of   cid   corresponding to              
We denote by    the projection matrix   
 
                         cid  where   is the largest index satisfying        In other words     is   projection matrix
to the eigenvectors of   cid   with eigenvalues    
De nition   The principal component projection  PCP 
of     Rd at threshold   is       
De nition   The principal component regression  PCR 
of regressand     Rd cid 

at threshold   is

     arg miny Rd  cid AP       cid 
        cid       cid     

or equivalently

  Ridge Regression
De nition     blackbox algorithm ApxRidge        
is an  approximate ridge regression solver  if for every    
Rd  it satis es  cid ApxRidge          cid       cid   
 cid   cid 
Ridge regression is equivalent to solving wellconditioned
linear systems  or minimizing strongly convex and smooth
objectives          

    cid   cid              cid   

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

    degree  

    degree  

    degree  

Figure   Comparing our polynomial       orange solid curve  with that of Frostig et al   blue dashed curve 

Remark   There is huge literature on ef cient algorithms
solving ridge regression  Most notably 
  Conjugate gradient  Shewchuk    or accelerated
gradient descent  Nesterov    gives fastest fullgradient methods 

  SVRG  Johnson   Zhang    and its acceleration
Katyusha  AllenZhu    give the fastest stochasticgradient method  and

  NUACDM  AllenZhu et al    gives the fastest

coordinatedescent method 

The running time of   is   nnz    log 
where nnz    is time to multiply   to any vector  The
running times of   and   depend on structural properties of   and are always faster than  
Because the best complexity of ridge regression depends
on the structural properties of    following Frostig et al 
we only compute our running time in terms of the  number
of blackbox calls  to   ridge regression solver 

  Chebyshev Polynomials
De nition   Chebyshev polynomials of  st and  nd kind
are  Tn      and  Un      where
        
Tn           Tn      Tn   
                    Un           Un      Un   
Fact    Trefethen   
nUn    for       and

         

It satis es

 

  cos   arccos   
         Tn     
 cid cid    cid 
 cid 
      cid    cid    
      cid   cid 
In particular  when      
 cid 
      cid     cid    cid 
Tn     
 cid cid    
Un     

cosh   arccosh   
   cosh   arccosh   

dxTn     
if        
if      
if      

      cid   cid 

 
 
 
 

 
      

De nition   For function       whose domain contains     its degreen Chebyshev truncated series and
degreen Chebyshev interpolation are respectively

pn     

akTk    and qn     

ckTk     

  

  

  cid 

  cid 

 cid   
  cid 

          

 

          

     Tk   
 
      

dx

 

 cid xj

 cid Tk

  cid xj

 cid   
 cid        is the jth Cheby 

  

where ak  

ck  

Above  xj   cos cid     

  

shev point of order   

     

The following lemma is known as the aliasing formula for
Chebyshev coef cients 
Lemma    cf  Theorem   of  Trefethen    Let
  be Lipschitz continuous on     and  ak ck  be de 
 ned in Def    then

 

cn   an         

               
for every                      
ck   ak  ak    ak                  
De nition   For every       let    be the ellipse   of
foci   with major radius        This is also known as

Bernstein ellipse with parameter        cid     

  and

The following lemma is the main theory regarding Chebyshev approximation 
Lemma    cf  Theorem   and   of  Trefethen 
  Suppose       is analytic on    and           
on    Let pn    and qn    be the degreen Chebyshev
truncated series and Chebyshev interpolation of       on
    Then 
  max
  
  max
  

 cid cid     cid   
 cid cid     cid   
          and  ak       cid       cid     cid   for      

      pn     
      qn     

 
 

 

 

  

  

 

 

  Approximate PCP and PCR
We formalize our notions of approximation for PCP and
PCR  and provide   reduction from PCR to PCP 
  Our Notions of Approximation
that Frostig et al   Frostig et al    work
Recall
only with matrices   that satisfy the eigengap assumption 
is    has no singular value in the range

that

 Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

 cid     cid      Their approximation guaran 

tees are very straightforward 
  an output   is  approximate for PCP on vector   if

 cid     cid     cid cid 

  an output   is  approximate for PCR with regressand

  if  cid       cid     cid   cid 

Unfortunately  these notions are too strong and impossible
to satisfy for matrices that do not have   large eigengap
around the projection threshold  
In this paper we propose the following more general  but
yet very meaningful  approximation notions 
De nition   An algorithm    is    approximate
PCP for threshold   if for every     Rd

 cid       cid cid cid     cid cid 
   cid cid   
   cid cid         cid cid     cid cid 
     such that       cid           cid  it satis es

 cid         cid     cid     cid     cid cid 

Intuitively  the  rst property above states that  if projected
to the eigenspace with eigenvalues above       then
   and   are almost identical  the second property states
that  if projected to the eigenspace with eigenvalues below
      then    is almost zero  and the third property
states that  for each eigenvector    with eigenvalue in the
range             the projection  cid     cid  must
be between   and  cid     cid   but up to an error  cid cid 
Naturally     itself is      approximate PCP 
We propose the following notion for approximate PCR 
De nition   An algorithm      is    approximate
PCR for threshold   if for every     Rd cid 

   cid cid           cid cid     cid   cid 

   cid AC        cid     cid Ax      cid     cid   cid 
where         cid       cid   is the exact PCR solution for threshold      
The  rst notion states that the output          has nearly
no correlation with eigenvectors below threshold    
and the second states that the regression error should be
nearly optimal with respect to the exact PCR solution but
at   different threshold      
Relationship to Frostig et al  Under eigengap assumption  our notions are equivalent to Frostig et al 
Fact
singular

 cid     cid      then

If   has

  Def    is equivalent to  cid        cid      cid cid 
  Def    implies  cid        cid      cid   cid  and

value

 

no

 cid        cid      cid   cid  implies Def   

in

Above          cid       cid   is the exact PCR solution 

  Reductions from PCR to PCP
If the PCP solution         cid    is computed exactly 
then by de nition one can compute    cid    which gives
  solution to PCR by solving   linear system  However  as
pointed by Frostig et al   Frostig et al    this computation is problematic if   is only approximate  The following approach has been proposed to improve its accuracy by
Frostig et al 
   compute     cid         where      is   polyno 

mial that approximates function

 

   

Frostig et al  picked        pm     cid  

This is   good approximation to    cid    because the
   is exactly   
composition of functions
      xt which
is   truncated Taylor series  and used the following procedure to compute sm   pm   cid        

   and

 

 

if      log   

         cid   

     ApxRidge        
         sk             ApxRidge      sk   

 
Above    is an approximate PCP solver and ApxRidge is
an approximate ridge regression solver  Under eigengap
assumption  Frostig et al   Frostig et al    showed
Lemma    PCRto PCP  For  xed            
lie in
let   be   matrix whose singular values
    approximate ridge regression solver  and let   be
    approximate PCP solver  Then  proce 

 cid cid     cid   cid cid       cid  Let ApxRidge be any

    
any       
dure   satis es
 cid sm   cid       cid   cid     cid   cid 
Unfortunately  the above lemma does not hold without
eigengap assumption 
In this paper  we    this issue by
proving the following analogous lemma 
Lemma    gap free PCRto PCP  For  xed      
    and         let   be   matrix whose singular values are no more than   Let ApxRidge be any
    approximate ridge regression solver  and   be
    
    approximate PCP solver  Then  proceany       
 cid 
dure   satis es 
 cid       sm cid     cid   cid    and

 cid 

 cid Asm     cid     cid     cid       cid       cid     cid   cid 

if      log 

Note that the conclusion of this lemma exactly corresponds to the two properties in our Def    The proof
of Lemma   is not hard  but requires   very careful case
analysis by decomposing vectors   and each sk into three
components  each corresponding to eigenvalues of   cid   in
the range         and    
 Recall from Fact   that this requirement is equivalent to

 
saying that  cid        cid        
    cid cid 

 

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

 

Proof of Lemma   Denoting by          cid    
 cid 
  and it satis es           cid  in    Applying
 cid log  
 cid 
    log  
 cid 

we know that       is analytic on ellipse    with    
Lemma   we know that when      
it satis es          qn       

lemma an immediate consequence of our

Lemma   with        cid    
 cid 
 cid 
    qn        cid       

Lemma   For every                       
and         we have

The next

 

 

 

 

 

We defer the details to the full version 
  Chebyshev Approximation Outside    
Classical Chebyshev approximation theory  such as
Lemma   only talks about the behaviors of pn    or
gn    on interval     However  for the purpose of this
paper  we must also bound its value for       We prove
the following general lemma in the full version  and believe
it could be of independent interest   we denote by        
the kth derivative of   at   
Lemma   Suppose       is analytic on    and for every                 Then  for every        letting
pn    and qn    be be the degreen Chebyshev truncated
series and Chebyshev interpolation of       we have
          

    pn       qn                   

  Our Polynomial Approximation of sgn   
For  xed         we consider the degreen Chebyk  ckTk    of the function

shev interpolation qn     cid  
       cid    
 cid 

 cid  on     Def    tells us that
 cid         
 cid cid 
 cid cid 
 cid 
 cid        

          

        cos

  cid 

ck  

     

     

  cos

  

 

 

     

and

deg gn           

Our  nal polynomial to approximate sgn    is therefore
gn        qn   
We prove the following theorem in this section 
Theorem   For every                 choosing       our function gn          qn           
satis es that as long as      
    then  see also
Figure  
   gn      sgn        for every              
  gn          for every         and gn     

    for every        

Note that our degree       cid  log cid  is nearsatisfy even only the  rst item is  cid  log cid   Ere 

optimal  because the minimum degree for   polynomial to

log  

 

menko   Yuditskii      However  the results of
 Eremenko   Yuditskii      are not constructive 
and thus may not lead to stable matrix polynomials 
We prove Theorem   by  rst establishing two simple
lemmas 
The following lemma is   consequence of
Lemma  
Lemma   For every         and         if
     

 cid log  

 cid  then

 

    log  
        

 

         qn         

are now ready to prove

Proof of Theorem   We
Theorem  
  When               it satis es              
    Therefore  applying Lemma   we have whenever      
  it satis es       
   qn        This further implies
 gn   sgn       xqn   xf    
                     qn                 

     

  log  

log  

 

  When         it satis es                      

Applying Lemma   we have for all        
    gn          qn                         
and similarly for         it satis es     gn     
 cid 
 

  Bound on Chebyshev Coef cients  We also give an
upper bound to the coef cients of polynomial qn    Its
proof can be found in the full version  and this upper bound
shall be used in our  nal stability analysis 
Lemma    coef cients of qn 

 cid  
of          cid    
Let qn     
   ckTk    be the degreen Chebyshev interpolation

 cid  on     Then  for all    

              

 

 ci      cid      

 cid 

 

 cid 

 cid  

       

     

  Stable Computation of Matrix Chebyshev

Polynomials

that

In this section we show that any polynomial
is
  weighted summation of Chebyshev polynomials with
bounded coef cients  can be stably computed when applied
to matrices with approximate computations  We achieve
so by  rst generalizing Clenshaw   backward method to
matrix case in Section   in order to compute   matrix
variant of Chebyshev sum  and then analyze its stability in
Section   with the help from Elloit   forwardbackward
transformation  Elliott   

Remark   We wish to point out that although Chebyshev
polynomials are known to be stable under error when computed on scalars  Gil et al    it is not immediately
clear why it holds also for matrices  Recall that Chebyshev polynomials satisfy Tn       xTn      Tn   
In the matrix case  we have Tn       MTn     
Tn    where     Rd is   vector  If we analyzed this
formula coordinate by coordinate  error could blow up by  
factor   per iteration 
In addition  we need to ensure that the stability theorem
holds for matrices   with eigenvalues that can exceed  
This is not standard because Chebyshev polynomials are
typically analyzed only on domain    
  Clenshaw   Method in Matrix Form
Consider any computation of the form

Tk   cid ck   Rd

  

 cid sN  

 
where     Rd   is symmetric and each  cid ck is in Rd   Note
that for PCP and PCR purposes  we it suf ces to consider
      is   scalar and     Rd is    xed
 cid ck     cid 
vector for all    However  we need to work on this more
general form for our stability analysis 
Vector sN can be computed using the following procedure 
Lemma    backward recurrence   cid sN    cid     cid    where

   where   cid 

  cid 

 cid bN      cid 

 cid bN    cid cN  

and

Inexact Clenshaw   Method in Matrix Form

                         cid br      cid br     cid br     cid cr   Rd  
 
We show that  if implemented using the backward recurrence formula  the Chebyshev sum of   can be stably
computed  We de ne the following model to capture the
error with respect to matrixvector multiplications 
De nition    inexact backward recurrence  Let   be
an approximate algorithm that satis es  cid     Mu cid   
 cid   cid  for every     Rd  Then  de ne inexact backward
recurrence to be

 cid bN        cid bN    cid cN  
                       cid br      cid cid br 
and de ne the output as cid sN  cid        cid   

 cid   cid br     cid cr   Rd  

and

The following theorem gives an error analysis to our inexact backward recurrence  We prove it in full version  and
the main idea of our proof is to convert each error vector of
  recursion of the backward procedure into an error vector
corresponding to some original  cid ck 
Theorem    stable Chebyshev sum  For every    
   suppose the eigenvalues of   are in        and suppose there are parameters CU     CT           Cc  

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

 cid 

 cid               

  satisfying                      
   cid cid ck cid    Cc
Then  if the inexact backward recurrence in Def    is
applied with      

 cid cid sN    cid sN cid               CT    CU Cc  

 Tk   CT   
 Uk   CU   

  we have

   CU

 

 cid 

  Algorithms and Main Theorems for PCP

and PCR

 

where        cid    

We are now ready to state our main theorems for PCP and
PCR  We  rst note   simple fact 
Fact  
        sgn   
     cid            cid          cid        
In other words  for every vector     Rd 
the exact
PCP solution    is the same as computing      
  sgn   
  Thus  we can use our polynomial gn    introduced in Section   and compute gn      sgn    Finally  in order to compute gn    we need to multiply   to
deg gn  vectors  whenever we do so  we call perform ridge
regression once 
Since the highlevel structure of our PCP algorithm is very
clear  due to space limitation  we present the pseudocodes
of our PCP and PCR algorithms in the full version 

 

  Our Main Theorems

We  rst state our main theorem under the eigengap assumption  in order to provide   direct comparison to that
of Frostig et al   Frostig et al   
Theorem    eigengap assumption  Given     Rd cid  
and           assume that the singular values of  
    Rd and     Rd cid 

are in the range  cid       cid        Given

  denote by

       and         cid       cid  

the exact PCP and PCR solutions  and by ApxRidge any
 cid approximate ridge regression solver  Then 
  QuickPCP outputs   satisfying  cid     cid     cid cid  with

  cid  log  
log cid     cid  log  
  cid  log  
log cid     cid  log  

 cid  oracle calls to ApxRidge as long as
 cid  oracle calls to ApxRidge  as long as

  QuickPCR outputs   satisfying  cid      cid     cid   cid  with

 cid 
 cid 

 

 

 

 

    for PCP and   log  

In contrast  the number of ridgeregression oracle calls was
  log  
    for PCR in
 Frostig et al    We include the proof of Theorem  
in the full version 
We state our theorem without the eigengap assumption 

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Theorem    gapfree  Given     Rd cid           
and         assume that  cid   cid      Given
    Rd and     Rd cid 
  and suppose ApxRidge is an  cid 
approximate ridge regression solver  then
  QuickPCP outputs   that is    approximate PCP

 cid  oracle calls to ApxRidge as long
with   cid  log  
 cid 
as log cid     cid  log  
 cid  oracle calls to ApxRidge as
with   cid  log  
long as elog cid     cid  log  

  QuickPCR outputs   that is    approximate PCR

 

 cid 

 

 

 

We make    nal remark here regarding the practical usage
of QuickPCP and QuickPCR 
Remark   Since our theory is for    approximations
that have two parameters  the user in principle has to feed
in both   and   where   is the degree of the polynomial
approximation to the sign function  In practice  however  it
is usually suf cient to obtain    approximate PCP and
PCR  Therefore  our pseudocodes allow users to set      
and thus ignore this parameter   in such   case  we shall
use     log     which is equivalent to setting      
because       log 
  Experiments
We provide empirical evaluations in the full version of this
paper 
  Conclusion
We summarize our contributions 
  We put forward approximate notions for PCP and PCR
that do not rely on any eigengap assumption  Our notions reduce to standard ones under the eigengap assumption 
  We design nearoptimal polynomial approximation
  We develop general stable recurrence formula for matrix Chebyshev polynomials  as   corollary  our     
can be applied to matrices in   stable manner 
  We obtain faster  provable PCAfree algorithms for

     to sgn    satisfying   and  

PCP and PCR than known results 

References
AllenZhu  Zeyuan  Katyusha  The First Direct Accelera 

tion of Stochastic Gradient Methods  In STOC   

AllenZhu  Zeyuan and Li  Yuanzhi  LazySVD  Even
Faster SVD Decomposition Yet Without Agonizing
Pain  In NIPS     

AllenZhu  Zeyuan and Li  Yuanzhi  Doubly Accelerated
Methods for Faster CCA and Generalized Eigendecomposition  In Proceedings of the  th International Conference on Machine Learning  ICML    

AllenZhu  Zeyuan  Richt arik  Peter  Qu  Zheng  and Yuan 
Yang  Even faster accelerated coordinate descent using
nonuniform sampling  In ICML   

Boutsidis  Christos and MagdonIsmail  Malik 

Faster
SVDtruncated regularized leastsquares  In   IEEE
International Symposium on Information Theory  pp 
  IEEE   

Chan  Tony   and Hansen  Per Christian  Computing truncated singular value decomposition least squares solutions by rank revealing QRfactorizations  SIAM Journal
on Scienti   and Statistical Computing   
 

Elliott  David  Error analysis of an algorithm for summing
certain  nite series  Journal of the Australian Mathematical Society     

Eremenko  Alexandre and Yuditskii  Peter  Uniform approximation of sgn   by polynomials and entire funcJournal   Analyse Math ematique   
tions 
   

Eremenko  Alexandre and Yuditskii  Peter  Polynomials of
the best uniform approximation to sgn     on two intervals  Journal   Analyse Math ematique   
 

Frostig  Roy  Musco  Cameron  Musco  Christopher  and
Sidford  Aaron  Principal Component Projection Without Principal Component Analysis  In ICML   

Gil  Amparo  Segura  Javier  and Temme  Nico    Numerical Methods for Special Functions  Society for
Industrial and Applied Mathematics  jan  
ISBN
 
doi   
URL
http epubs siam org doi 
abs http 
 epubs siam org doi book 
 

Han  Insu  Malioutov  Dmitry  Avron  Haim  and Shin 
Jinwoo  Approximating the spectral sums of largescale matrices using chebyshev approximations  arXiv
preprint arXiv   

AllenZhu  Zeyuan and Li  Yuanzhi  First Ef cient Convergence for Streaming kPCA    Global  GapFree  and
NearOptimal Rate  ArXiv eprints  abs 
July    

Higham     Functions of Matrices  Society for Industrial and Applied Mathematics    doi   
  URL http epubs siam org 
doi abs 

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Johnson  Rie and Zhang  Tong  Accelerating stochastic gradient descent using predictive variance reduction 
In Advances in Neural Information Processing Systems 
NIPS   pp     

Musco  Cameron and Musco  Christopher  Randomized
block krylov methods for stronger and faster approximate singular value decomposition  In NIPS  pp   
   

Nakatsukasa  Yuji and Freund  Roland    Computing
fundamental matrix decompositions accurately via the
matrix sign function in two iterations  The power of
SIAM Review   
zolotarev   functions 
 

Nesterov  Yurii  Introductory Lectures on Convex Programming Volume    Basic course  volume    Kluwer Academic Publishers    ISBN  

Schilders  Wilhelmus      Van der Vorst  Henk    and
Rommes  Joost  Model order reduction  theory  research
aspects and applications  volume   Springer   

Shewchuk  Jonathan Richard  An introduction to the conjugate gradient method without the agonizing pain   

Trefethen  Lloyd    Approximation Theory and Approxi 

mation Practice  SIAM   

van den Eshof  Jasper  Frommer  Andreas  Lippert  Th 
Schilling  Klaus  and van der Vorst  Henk    Numerical
methods for the qcdd overlap operator     signfunction
and error bounds  Computer Physics Communications 
   

