Differentially Private Ordinary Least Squares

Or Sheffet  

Abstract

Linear regression is one of the most prevalent
techniques in machine learning  however  it is
also common to use linear regression for its explanatory capabilities rather than label prediction  Ordinary Least Squares  OLS  is often used
in statistics to establish   correlation between
an attribute       gender  and   label      
income  in the presence of other  potentially correlated  features  OLS assumes   particular model
that randomly generates the data  and derives tvalues   representing the likelihood of each real
value to be the true correlation  Using tvalues 
OLS can release   con dence interval  which is
an interval on the reals that is likely to contain
the true correlation  and when this interval does
not intersect the origin  we can reject the null hypothesis as it is likely that the true correlation
is nonzero  Our work aims at achieving similar guarantees on data under differentially private estimators  First  we show that for wellspread data  the Gaussian JohnsonLindenstrauss
Transform  JLT  gives   very good approximation of tvalues  secondly  when JLT approximates Ridge regression  linear regression with
  regularization  we derive  under certain conditions  con dence intervals using the projected
data  lastly  we derive  under different conditions 
con dence intervals for the  Analyze Gauss  algorithm  Dwork et al   

  Introduction
Since the early days of differential privacy  its main goal
was to design privacy preserving versions of existing techniques for data analysis  It is therefore no surprise that several of the  rst differentially private algorithms were machine learning algorithms  with   special emphasis on the
ubiquitous problem of linear regression  Kasiviswanathan

 Computing Science Dept  University of Alberta  Edmonton
AB  Canada  This work was done when the author was at Harvard University  supported by NSF grant CNS  Correspondence to  Or Sheffet  osheffet ualberta ca 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

et al    Chaudhuri et al    Kifer et al    Bassily et al    However  all existing body of work on
differentially private linear regression measures utility by
bounding the distance between the linear regressor found
by the standard nonprivate algorithm and the regressor
found by the privacypreserving algorithm  This is motivated from   machinelearning perspective  since bounds
on the difference in the estimators translate to error bounds
on prediction  or on the loss function  Such bounds are
 highly  interesting and nontrivial  yet they are of little use
in situations where one uses linear regression to establish
correlations rather than predict labels 
In the statistics literature  Ordinary Least Squares  OLS 
is   technique that uses linear regression in order to infer
the correlation between   variable and an outcome  especially in the presence of other factors  And so  in this paper  we draw   distinction between  linear regression  by
which we refer to the machine learning technique of  nding
  speci   estimator for   speci   loss function  and  Ordinary Least Squares  by which we refer to the statistical inference done assuming   speci   model for generating the
data and that uses linear regression  Many argue that OLS
is the most prevalent technique in social sciences  Agresti
  Finlay    Such works make no claim as to the labels of   new unlabeled batch of samples  Rather they aim
to establish the existence of   strong correlation between
the label and some feature  Needless to say  in such works 
the privacy of individuals  data is   concern 
In order to determine that   certain variable xj is positively
 resp  negatively  correlated with an outcome    OLS assumes   model where the outcome   is   noisy version of
  linear mapping of all variables          xxx      with  
denoting random Gaussian noise  for some predetermined
and unknown   Then  given many samples  xxxi  yi  OLS
establishes two things      when  tting   linear function
to best predict   from xxx over the sample  via computing
  yixxxi  the coef cient    is positive
 resp  negative  and  ii  inferring  based on     that the
true    is likely to reside in     resp     In fact  the
crux in OLS is by describing    using   probability distribution over the reals  indicating where    is likely to fall 
derived by computing tvalues  These values take into account both the variance in the data as well as the variance of
the noise    Based on this probability distribution one can

   cid cid 

  xxxixxxT
 

 cid 

 cid 

 For example  imagine we run linear regression on   certain

Differentially Private Ordinary Least Squares

de ne the  con dence interval   an interval   centered
at    whose likelihood to contain    is     Of particular
importance is the notion of rejecting the nullhypothesis 
where the interval   does not contain the origin  and so
one is able to say with high con dence that    is positive
 resp  negative  Further details regarding OLS appear in
Section  
In this work we give the  rst analysis of statistical inference for OLS using differentially private estimators  We
emphasize that the novelty of our work does not lie in the
differentiallyprivate algorithms  which are  as we discuss
next  based on the JohnsonLindenstrauss Transform  JLT 
and on additive Gaussian noise and are already known to
be differentially private  Blocki et al    Dwork et al 
  Instead  the novelty of our work lies in the analyses of the algorithms and in proving that the output of the
algorithms is useful for statistical inference 

The Algorithms  Our  rst algorithm  Algorithm   is an
adaptation of Gaussian JLT  Proving that this adaptation
remains    differentially private is straightforward  the
proof appears in Appendix    As described  the algorithm takes as input   parameter    in addition to the
other parameters of the problem  that indicates the number
of rows in the JLmatrix  Later  we analyze what should
one set as the value of    Our second algorithm is taken

Algorithm   Outputting   private JohnsonLindenstrauss
projection of   matrix 

 cid 

Input    matrix     Rn   and   bound       on the
  norm of any row in   
Privacy parameters         
Parameter   indicating the number of rows in the resulting matrix 
Set                
 
Sample     Lap    and let  min    denote the
smallest singular value of   
if  min                   ln 

 cid cid   ln      ln 

Sample        matrix   whose entries are       samples from   normal Gaussian 
return RA and  matrix unaltered 
Let   cid  denote the result of appending   with the   dmatrix wId   
Sample               matrix   whose entries are
      samples from   normal Gaussian 
returnRA cid  and  matrix altered 

then

else

 

 

end if

    yyy  which results in   vector   with coordinates        
  Yet while the column    contains many    and     the
column    is mostly populated with zeros 
In such   setting 
OLS gives that it is likely to have       whereas no such
guarantees can be given for  

verbatim from the work of Dwork et al   We de 

Algorithm    Analyze Gauss  Algorithm of Dwork et
al  

Input    matrix     Rn   and   bound       on the
  norm of any row in   
Privacy parameters         
    symmetric        matrix with upper triangle en 

tries sampled       from   cid 

      ln 

 

 cid 

 

return ATA     

liberately focus on algorithms that approximate the  ndmoment matrix of the data and then run hypothesistesting
by postprocessing the output  for two reasons  First  they
enable sharing of data  and running unboundedly many
hypothesistests  Since  we do not deal with OLS based
on the private singleregression ERM algorithms  Chaudhuri et al    Bassily et al    as such inference requires us to use the Fisherinformation matrix of the loss
function   but these algorithms do not minimize   private
lossfunction but rather prove that outputting the minimizer
of the perturbed lossfunction is private  This means that
differentiallyprivate OLS based on these ERM algorithms
requires us to devise new versions of these algorithms 
making this   second step in this line of work   After  rst
understanding what we can do using existing algorithms 
We leave this approach   as well as performing private hypothesis testing using   PTRtype algorithm  Dwork   Lei 
   output merely reject   don treject decision without justi cation  or releasing only relevant tests judging
by their pvalues  Dwork et al      for future work 

Our Contribution and Organization  We analyze the
performances of our algorithms on   matrix   of the form
        yyy  where each coordinate yi is generated according to the homoscedastic model with Gaussian noise  which
is   classical model in statistics  We assume the existence
of   vector        for every   we have yi    Txxxi   ei and
ei is sampled       from      
We study the result of running Algorithm   on such data
in the two cases  where   wasn   altered by the algorithm
and when   was appended by the algorithm  In the former
case  Algorithm   boils down to projecting the data under
  Gaussian JLT  Sarlos   has already shown that the
JLT is useful for linear regression  yet his work bounds
the   norm of the difference between the estimated re 
 Researcher   collects the data and uses the approximation of
the  ndmoment matrix to test some OLS hypothesis  but once
the approximation is published researcher   can use it to test for
  completely different hypothesis 

 This model may seem objectionable  Assumptions like the
noise independence   meaned or sampled from   Gaussian distribution have all been called into question in the past  Yet due to
the prevalence of this model we see    to initiate the line of work
on differentially private Least Squares with this Ordinary model 

Differentially Private Ordinary Least Squares

        

gression before and after the projection  Following Sarlos 
work  other works in statistics have analyzed compressed
linear regression  Zhou et al    Pilanci   Wainwright 
      However  none of these works give con dence
intervals based on the projected data  presumably for three
reasons  Firstly  these works are motivated by computational speedups  and so they use fast JLT as opposed to our
analysis which leverages on the fact that our JLmatrix is
composed of       Gaussians  Secondly  the focus of these
works is not on OLS but rather on newer versions of linear
regression  such as Lasso or when   lies in some convex
set  Lastly  it is evident that the smallest con dence interval is derived from the data itself  Since these works do
not consider privacy applications   actually   Zhou et al 
  Pilanci   Wainwright      do consider privacy
applications of the JLT  but quite different than differential
privacy  they assume the analyst has access to the data itself  and so there was no need to give con dence intervals
for the projected data  Our analysis is therefore the  rst  to
the best of our knowledge  to derive tvalues   and therefore achieve all of the rich expressivity one infers from tvalues  such as con dence bounds and nullhypotheses rejection   for OLS estimations without having access to  
itself  We also show that  under certain conditions  the sample complexity for correctly rejecting the nullhypothesis
 
increases from   certain bound     without privacy  to  
bound of         
  ATA  with privacy  where
     denotes the condition number of the matrix    This
appears in Section  
In Section   we analyze the case Algorithm   does append
the data and the JLT is applied to   cid  In this case  solving
the linear regression problem on the projected   cid  approximates the solution for Ridge Regression  Tikhonov   
Hoerl   Kennard   
In Ridge Regression we aim
to solve minzzz
we penalize vectors whose   norm is large  In general  it
is not known how to derive tvalues from Ridge regression 
and the literature on deriving con dence intervals solely
from Ridge regression is virtually nonexistent 
Indeed 
prior to our work there was no need for such calculations 
as access to the data was  in general  freely given  and so
deriving con dence intervals could be done by appealing
back to OLS  We too are unable to derive approximated
tvalues in the general case  but under additional assumptions about the data   which admittedly depend in part on
 cid cid  and so cannot be veri ed solely from the data   we
show that solving the linear regression problem on RA cid  allows us to give con dence intervals for     thus correctly
determining the correlation   sign 
In Section   we discuss the  Analyze Gauss  algorithm  Dwork et al    that outputs   noisy version of
  covariance of   given matrix using additive noise rather
than multiplicative noise  Empirical work  Xi et al   
shows that Analyze Gauss   output might be nonPSD if
the input has small singular values  and this results in truly

 cid cid 
  yi   zzzTxxxi      cid zzz cid cid  which means

bad regressors  Nonetheless  under additional conditions
 that imply that the output is PSD  we derive con dence
bounds for Dwork et al    Analyze Gauss  algorithm  Finally  in Section   we experiment with the heuristic of
computing the tvalues directly from the outputs of Algorithms   and   We show that Algorithm   is more  conservative  than Algorithm   in the sense that it tends to
not reject the nullhypothesis until the number of examples is large enough to give   very strong indication of rejection  In contrast  Algorithm   may wrongly rejects the
nullhypothesis even when it is true 

Discussion  Some works have already looked at the intersection of differentially privacy and statistics  Dwork  
Lei    Smith    Chaudhuri   Hsu    Duchi
et al    Dwork et al     especially focusing on robust statistics and rate of convergence  But only   handful
of works studied the signi cance and power of hypotheses
testing under differential privacy  without arguing that the
noise introduced by differential privacy vanishes asymptotically  Vu   Slavkovic    Uhler et al    Wang
et al    Rogers et al    These works are experimentally promising  yet they     focus on different statistical tests  mostly Goodnessof Fit and Independence testing   ii  are only able to prove results for the case of simple
hypothesistesting    single hypothesis  with an ef cient
datageneration procedure through repeated simulations  
  cumbersome and time consuming approach  In contrast 
we deal with   composite hypothesis  we simultaneously
reject all    with sign     cid  sign      by altering the
con dence interval  or the critical region 
One potential reason for avoiding con denceinterval analysis for differentially private hypotheses testing is that it
does involve revisiting existing results  Typically  in statistical inference the sole source of randomness lies in the
underlying model of data generation  whereas the estimators themselves are   deterministic function of the dataset 
In contrast  differentially private estimators are inherently
random in their computation  Statistical inference that considers both the randomness in the data and the randomness
in the computation is highly uncommon  and this work  to
the best of our knowledge  is the  rst to deal with randomness in OLS hypothesis testing  We therefore strive in our
analysis to separate the two sources of randomness   as
in classic hypothesis testing  we use   to denote the bound
on any bad event that depends solely on the homoscedastic model  and use   to bound any bad event that depends
on the randomized algorithm   Thus  any result which is
originally of the form  reject the nullhypothesis  is now
converted into   result  reject the null hypothesis 

 Or any randomness in generating the feature matrix   which
standard OLS theory assumes to be  xed  see Theorems  
and  

Differentially Private Ordinary Least Squares

   and taking the quantity   cid cid cid    It is  

 cid cid     
         thus it is   common pracknown fact that Tk
tice to apply Gaussian tail bounds to the Tkdistribution
when   is suf ciently large 

Differential Privacy  In this work  we deal with input in
the form of       dmatrix with each row bounded by  
  norm of    Two inputs   and   cid  are called neighbors if
they differ on   single row 
De nition    Dwork et al      An algorithm ALG
which maps        matrices into some range   is    
differential privacy it holds that Pr ALG          
  Pr ALG   cid           for all neighboring inputs   and
  cid  and all subsets       

  Preliminaries and OLS Background

Notation  Throughout this paper  we use lowercase letters to denote scalars       yi or ei  boldboldbold characters to
denote vectors  and UPPERcase letters to denote matrices  The ldimensional all zero vector is denoted     and
the     mmatrix of all zeros is denoted       We use eee
to denote the speci   vector yyy      in our model  and
though the reader may  nd it   bit confusing but hopefully
clear from the context   we also use eeej and eeek to denote
elements of the natural basis  unit length vector in the direction of coordinate   or    We use     to denote the privacy parameters of Algorithms   and   and use   and   to
denote con dence parameters  referring to bad events that
hold          and     resp  based on the homoscedastic
model or the randomized algorithm resp  We also stick to
the notation from Algorithm   and use   to denote the positive scalar for which         
 
throughout this paper  We use standard notation for SVD
composition of   matrix              its singular values
and its MoorePenrose inverse     

 cid cid   ln    ln 
 cid 

Gaussians give that Pr          cid ln     

The Gaussian distribution 
  univariate Gaussian
      denotes the Gaussian distribution whose mean
is   and variance   Standard concentration bounds on
for any        
      multivariate Gaussian      
for some positive semide nite   denotes the multivariate Gaussian distribution where the mean of the jth coordinate is the    and the covariance between coordinates
  and   is       The PDF of such Gaussian is de ned
only on the subspace colspan    matrix Gaussian distribution  denoted    Ma    Ia        has mean    independence among its rows and variance   for each of
its columns  We also require the following property of
Gaussian random variables  Let   and   be two random
Gaussians                and           where
     
       for some    then for any       we have
  Prx             Prx             cPrx            
 
 see Proposition   

Additional Distributions  We denote by Lap  the
Laplace distribution whose mean is   and variance is  
kdistribution  where   is referred to as the deThe  
grees of freedom of the distribution  is the distribution
over the   norm squared of the sum of   independent
normal Gaussians  That is  given                  Xk  
      it holds that  
def                 Xk   
      Ik    and  cid cid     
   Existing tail bounds on
  distribution  Laurent   Massart    give that
the  
The TkPr
distribution  where   is referred to as the degrees of freedom of the distribution  denotes the distribution over the
reals created by independently sampling           and

   cid  ln cid         

 cid cid cid     

 

Background on OLS  For the unfamiliar reader  we give
here   very brief overview of the main points in OLS  Further details  explanations and proofs appear in Section   
We are given   observations  xxxi  yi  
   where     xxxi  
Rp and yi      We assume the existence of     Rp     
the label yi was derived by yi    Txxxi   ei where ei  
      independently  also known as the homoscedastic
Gaussian model  We use the matrix notation where   denotes the         feature matrix and yyy denotes the labels 
We assume   has full rank 
The parameters of the model are therefore   and   which
we set to discover  To that end  we minimize minzzz  cid yyy  
Xzzz cid  and have
       TX   Tyyy      TX          eee           eee  
    yyy               eee            eee         XX  eee  
And then for any coordinate   the tvalue  which
is
is

the quantity     

def 

 

according

    
      cid cid 
 
   
to Tn pdistribution 

 cid  and   satisfying         

distributed
    
PDFTn      dx
Pr
for any measurable        Thus      describes the
likelihood of any      for any       we can now give
an estimation of how likely it is to have         which
is PDFTn         and this is known as ttest for the
value    In particular  given           we denote   
as the number for which the interval        contains
  probability mass of       from the Tn pdistribution 
And so we derive   corresponding con dence interval   
centered at    where         with con dence of level of
     
Of particular importance is the quantity   
 cid cid cid 

def      
 since if there is no correlation between xj

   
   TX 

 

  

 cid 

   TX 

 cid 
   cid 

 

 
   

and   then the likelihood of seeing    depends on the ratio
of its magnitude to its standard deviation  As mentioned
         then rather than viewing
earlier  since Tk

Differentially Private Ordinary Least Squares

 

 
 

Let   be the number         cid   

this    as sampled from   Tn pdistribution  it is common
to think of    as   sample from   normal Gaussian      
This allows us to associate    with   pvalue  estimating the
event    and    have different signs  Speci cally  given
        we  reject the null hypothesis if       
    dx    
This means we  reject the null hypothesis when        
We now lower bound the number of       sample points
needed in order to  reject the null hypothesis  This bound
is our basis for comparison between standard OLS and the
differentially private version 
Theorem   Fix any positive de nite matrix     Rp  
and any        
    Fix parameters     Rp and  
 cid    Let   be   maand   coordinate          
trix whose   rows are       samples from       and
yyy be   vector where yi        is sampled       from
      Fix         Then                  we
have that OLS        con dence interval has length
    
for some suf ciently large constant    Furthermore 
there exists   constant    such that                 
OLS  correctly  rejects the null hypothesis provided    
  where   
max

 cid   min  provided           ln 
 cid 
is the number for which cid      

PDFTn      dx        

      ln        

   
 
 min 

    

 cid 

 
 
 

  OLS over Projected Data
In this section we deal with the output of Algorithm  
in the special case where Algorithm   outputs matrix
unaltered and so we work with RA 
To clarify  the setting is as follows  We denote         yyy 
the columnwise concatenation of the          matrix
  with the nlength vector yyy 
 Clearly  we can denote
any column of   as yyy and any subset of the remaining
columns as the matrix    We therefore denote the output
RA    RX  Ryyy  and for simplicity we denote     RX
and           We denote the SVD decomposition of
            So   is an orthonormal basis for the columnspan of   and as   is fullrank   is an orthonormal basis
for Rp  Finally  in our work we examine the linear regression problem derived from the projected data  That is  we
denote
       TRTRX RX   Ryyy         RX Reee

 

   

 
     

 cid cid    with

     

  Ryyy    

   RX 

 

We now give our main theorem  for estimating the tvalues
based on   and  

 Theorem   also illustrates how we  separate  the two
sources of privacy  In this case    bounds the probability of bad
events that depend to sampling the rows of    and   bounds the
probability of   bad event that depends on the sampling of the yyy
coordinates 

Theorem   Let   be          matrix  and parameters     Rp and   are such that we generate the vector
yyy        eee with each coordinate of eee sampled independently from       Assume Algorithm   projects the
matrix         yyy  without altering it  Fix        
and          ln  Fix coordinate    Then we have
that              deriving   and   as in Equations  
and   the pivot quantity        
has  
distribution   satisfying   aPDFTr         PDFD     
eaPDFTr      ax  for any        where we denote    
   
     

   TRTRX 

    

 cid 

 
   

 

The implications of Theorem   are immediate  all estimations one can do based on the tvalues from the true data
   yyy  we can now do based on    modulo an approximation
factor of exp     
      In particular  Theorem   enables us
to deduce   corresponding con dence interval based on  
Corollary   In the same setting as in Theorem       
        we have the following  Fix any        
    Let    
denote the number      the interval     contains  
     
probability mass of the Tr pdistribution  Then Pr    

 cid       ea          

 cid 

   TRTRX 

   

           

 cid 

 cid 

 

   

   

   

   

 

 

 cid 

 cid cid 
   

   TX 

   TX 

 cid 
 cid   cid cid 

         TRTRX 
therefore

We compare the con dence interval of Corollary  
to the con dence interval of the standard OLS model 
      As   is   JLwhose length is   
matrix  known results regarding the JL transform give
that  cid cid       cid cid  and that
     
We
that
 cid cid 
  
 
   
   TX 

 cid cid 
 cid 
have
 
 cid        
   TRTRX 
   TRTRX 
         
  So for values of  
        we get that the con dence interval
for which  
of Theorem   is   factor of  
 larger than
the standard OLS con dence interval  Observe that when
      which is the common case  the dominating

factor is cid              This bound intuitively makes

 cid 
 cid 
 cid     

 cid     

 cid 

 cid 

   

   

sense  we have contracted   observations to   observations  hence our model is based on con dence intervals
derived from Tr   rather than Tn   
In the supplementary material we give further discussion 
in which we compare our work to the more straightforward
bounds one gets by  plugging in  Sarlos  work   and
we also compare ourselves to the bounds derived from alternative works in differentially private linear regression 

  

 Moreover 

this

interval

is

the interval       contains  
   

essentially optimal 
der  
    probThen Pr    

the Tr pdistribution 
         

   TRTRX 

 
   

note        
ability mass of

 cid              

 cid 

 cid 

Differentially Private Ordinary Least Squares

 cid 

   ln       ln cid 

min   

    

  This discussion cul 

min
minates in the following corollary 
Corollary   Denoting  cid LB       
thus conclude that if          

 cid cid LB 

   min    we
and    

 cid 

 

 cid 

  then the result of Theorem  

   ln       ln cid 

    

min   

 

Rejecting the Null Hypothesis  Due to Theorem  
we can mimic OLS  technique for rejecting the null hyand repothesis 

     we denote      

 cid 

  

 

   TRTRX 

 
   

ject the nullhypothesis if indeed the associated     denot 
     
        is below
ing pvalue of the slightly truncated  
     
     
      Much like Theorem   we now establish  
lower bound on   so that       we end up  correctly  rejecting the nullhypothesis 
Theorem   Fix   positive de nite matrix     Rp   
Fix parameters     Rp and       and   coordinate  
         cid    Let   be   matrix whose   rows are sampled
      from         Let yyy be   vector      yi       
is sampled       from       Fix         and
        Then there exist constants         
and    such that when we run Algorithm   over     yyy 
with parameter                    we  correctly 
reject
the null hypothesis using           Algorithm  
returns matrix unaltered and we can estimate    
     
      provided
and verify that
  and    
        max
where
   
       

   
   min       ln 
 
min min        ln 

indeed            
  

      de ned      PrX Tr            
PrX          

   
         
   

 cid 
 cid 

     
     

 cid 

 cid 

     

   
 

max

  

 

 cid 

 cid     

  Setting the Value of    Deriving   Bound on  
Comparing the lower bound on   given by Theorem   to
the bound of Theorem   we have that the datadependent
bound of  
should now hold for   rather than
   Yet  Theorem   also introduces an additional dependency between   and    we require         
 min   
 since otherwise we do not have  min     cid    and Algorithm   might alter   before projecting it  and by de nition

   is proportional to cid   ln  This is precisely the

      

   min 

focus of our discussion in this subsection  We would like
to set     value as high as possible   the larger   is  the
more observations we have in RA and the better our con 
dence bounds  that depend on Tr    are   while satisfying
     

 

 

 cid 

if each sample point

is drawn       xxx  
Recall
        then each sample  xxxi   yi  is sampled from
          for    de ned in the proof of Theorem  
that is      
  So  Theorem   gives the lower bound          
and the following lower bounds on          and
  which means    
     

 cid 
 cid     

 
   

  ln ln 
 min   

   min 

   

 cid 

 cid 

 cid 

 

 

  

 

 min min   
that

 cid cid LB 

 cid 
 cid 

 

   ln 

 min     

holds by setting     min

 

 

 

   

 cid 

 cid 

 cid 

    ln 

 cid cid 

 cid cid LB 

It is interesting to note that when we know     we also
have   bound on    Recall    
the variance of the
Gaussian  xxx      Since every sample is an independent
draw from           then we have an upper bound of
     log np max    So our lower bound on    using
    to denote the condition number of     is given by
    max
 
Observe  overall this result is similar in nature to many
other results in differentially private learning  Bassily et al 
  which are of the form  without privacy  in order to
achieve   total loss of     we have   sample complexity
bound of some    and with differential privacy the sam 

ple complexity increases to       cid    However 

 cid cid LB 

 min    but not to        max   

there     subtlety here worth noting   cid LB  is proportional
to
 min      The additional
dependence on  max follows from the fact that differential
privacy adds noise proportional to the upper bound on the
norm of each row 

 

 

 cid 

 cid 

    Id  

  Projected Ridge Regression
We now turn to deal with the case that our matrix does not
pass the ifcondition of Algorithm   In this case  the matrix is appended with       dmatrix which is wId    Denoting   cid   
we have that the algorithm  
output is RA cid  Similarly to before  we are going to denote
          and decompose         yyy  with     Rn   and
yyy   Rn  with the standard assumption of yyy        eee and
ei sampled       from       We now need to introduce
some additional notation  We denote the appended matrix
and vectors   cid  and yyy cid         cid       cid  yyy cid  And so  using the
output RA cid  of Algorithm   we solve the linear regression
problem derived from  

  Ryyy cid       we set

  RX cid  and  

 cid       cid TRTRX cid RX cid   Ryyy cid 
 cid     

   Ryyy cid    RX cid cid 

 

Sarlos  results   regarding the Johnson Lindenstrauss transform give that  when   has suf ciently
many rows  solving the latter optimization problem gives
  good approximation for
the optimization problem      arg minzzz  cid yyy cid      cid zzz cid   
arg minzzz

 cid cid yyy   Xzzz cid      cid zzz cid cid  The latter problem is

the solution of

Differentially Private Ordinary Least Squares

known as the Ridge Regression problem  Invented in the
    Tikhonov    Hoerl   Kennard    Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coef cients are too large  It is also
often applied in the case where   doesn   have full rank
or is close to not having fullrank  one can show that the
minimizer         TX     Ip     Tyyy is the unique
solution of the Ridge Regression problem and that the RHS
is always wellde ned 
While the solution of the Ridge Regression problem might
have smaller risk than the OLS solution  it is not known
how to derive tvalues and or reject the null hypothesis under Ridge Regression  except for using   to manipulate
   back into        TX   Tyyy and relying on OLS 
In fact  prior to our work there was no need for such analysis  For con dence intervals one could just use the standard
OLS  because access to   and yyy was given 
Therefore  much for the same reason  we are unable to derive tvalues under projected Ridge Regression  Clearly 
there are situations where such con dence bounds simply
cannot be derived Nonetheless  under additional assumptions about the data  our work can give con dence intervals
for     and in the case where the interval doesn   intersect
the origin   assure us that sign cid 
     sign          
This is detailed in the supplementary material 
To give an overview of our analysis  we  rst discuss  
model where eee   yyy      is  xed       the data is  xed
and the algorithm is the sole source of randomness  and
prove that in this model  cid cid cid  is as an approximation to  
Theorem   Fix     Rn   and yyy      De ne    
   yyy and          XX  yyy  Let RX cid      cid  and Ryyy cid 
denote the result of applying Algorithm   to the matrix    
    yyy  when the algorithm appends the data with        
matrix  Fix   coordinate   and any         When
computing  cid  and  cid  as in   we have that           
         cid TM cid 
  contains

  it holds that       cid 

where   cid 
      mass of the Tr pdistribution 
However  our goal remains to argue that  cid 
  serves as   good
approximation for     To that end  we combine the standard OLS con dence interval   which says that       
    over the randomness of picking eee in the homoscedastic model we have              cid cid 
      with
the con dence interval of Theorem   above  and denot 
   TX 
    cid TM cid 
ing       
we have that Pr cid 
                       And
 Note  The na ve approach of using RX cid  and Ryyy cid  to interpolate RX and Ryyy and then apply Theorem   using these estimations of RX and Ryyy ignores the noise added from appending the
matrix   into   cid  and therefore leads to inaccurate estimations of
the tvalues 

  denotes the number such that    cid 

 cid cid cid cid   

        cid 

      cid 
 cid 

 cid cid 
   

 cid cid cid 
   

 cid 

 cid 

 cid 

    cid 

 cid 

   TX 

 
   

   

   

 

 cid 

so  in summary  in Section   we give conditions under
which the length of the interval   is dominated by the
  cid 
    factor derived from Theorem  

    cid TM cid 

 cid cid cid 
   

 

 

 cid 

   log 

the output of Algorithm   as

  Con dence Intervals for  Analyze Gauss 
In this section we analyze the  Analyze Gauss  algorithm
of Dwork et al   Algorithm   works by adding random Gaussian noise to ATA  where the noise is symmetric
with each coordinate above the diagonal sampled       from
      with      
  Using the same notation for   submatrix of   as     yyy  as before  we denote

 cid 
   cid 
  Thus 
  TX  cid   Tyyy
 cid yyyTX  cid yyyTyyy
 cid cid 
 cid   cid   Tyyy and
we approximate   and  cid cid  by cid   
 cid cid cid   cid yyyTyyy      cid yyyT   cid   cid 
  TX cid  resp  We now argue
that it is possible to use  cid   and  cid cid cid  to get   con dence
          min   TX     cid   ln  Under the
 cid cid cid 

interval for    under certain conditions 
Theorem   Fix          
       
homoscedastic model  given   and   if we assume also
that  cid cid      and  cid cid     cid   TX   Tyyy cid       then
                 it holds that

 cid cid cid  is at most
 cid cid cid    cid  
 cid 
       cid   ln     cid 
 cid 

    Assume that there exists

   cid 

 
   

  TX

  TX

  TX

 cid 

   

 

 

ln 
 

 cid 

     

 cid 
  TX

 
      ln      

   

where   is       an upper bound on    details appear in
the Supplementary material 
Note that the assumptions that  cid cid      and  cid cid     
are fairly benign once we assume each row has bounded
  norm  The key assumption is that   TX is wellspread 
Yet in the model where each row in   is sampled      
from       this assumption merely means that   is large
enough   namely  that      

 
 min   

  ln 

 

  Experiment  tValues of Output

Goal  We set to experiment with the outputs of Algorithms   and   While Theorem   guarantees that computing the tvalue from the output of Algorithm   in the
matrix unaltered case does give   good approximation of the tvalue   we were wondering if by computing
the tvalue directly from the output we can     get   good
approximation of the true  nonprivate  tvalue and     get
the same  higherlevel conclusion  of rejecting the nullhypothesis  The answers are  as ever  mixed  The two main

Differentially Private Ordinary Least Squares

observations we do notice is that both algorithms improve
as the number of examples increases  and that Algorithm  
is more conservative then Algorithm  

Setting  We tested both algorithms in two settings  The
 rst is over synthetic data  Much like the setting in Theorems   and     was generated using       independent normal Gaussian features  and yyy was generated using
the homoscedastic model  We chose        
so the  rst coordinate is twice as big   the second but
of opposite sign  and moreover  yyy is independent of the
 rd feature  The variance of the label is also set to  
and so the variance of the homosedastic noise equals to
              The number of observations
  ranges from       to      
The second setting is over reallife data  We ran the two
algorithms over diabetes dataset collected over ten years
  taken from the UCI repository  Strack et al 
  We truncated the data to   attributes  sex  binary 
age  in buckets of   years  number medications  numeric 
  and   diagnosis  numeric    Naturally  we
added    th column of all   intercept  Omitting any entry with missing or nonnumeric values on these nine attributes we were left with       entries  which we
shuf ed and fed to the algorithm in varying sizes   from
        to         Running OLS over the entire
  observation yields          
and tValues of      

The Algorithms  We ran   version of Algorithm   that uses
  DPestimation of  min  and  nds the largest   the we can
use without altering the input  yet if this   is below   then
it does alter the input and approximates Ridge regression 
We ran Algorithm   verbatim  We set       and    
  We repeated each algorithm   times 

and the tvalue from Algorithm   is   factor of cid   

Results  We plot the tvalues we get from Algorithms  
and   and decide to reject the nullhypothesis based on tvalue larger than    which corresponds to   fairly conservative pvalue of   Not surprisingly  as   increases 
the tvalues become closer to their expected value   the tvalue of Analyze Gauss is close to the nonprivate tvalue
  smaller
as detailed above  see after Corollary   As   result 
when the nullhypothesis is false  Analyze Gauss tends to
produce larger tvalues  and thus reject the nullhypothesis 
for values of   under which Algorithm   still does not reject  as shown in Figure     This is exacerbated in real
data setting  where its actual least singular value     is
fairly small in comparison to its size       
However  what is fairly surprising is the case where the
nullhypothesis should not be rejected   since       
 in the synthetic case  or its nonprivate tvalue is close
to    in the realdata case  Here  the Analyze Gauss  tvalue approximation has fairly large variance  and we still

get fairly high  in magnitude  tvalues  As the result  we
falsely reject the nullhypothesis based on the tvalue of
Analyze Gauss quite often  even for large values of    This
is shown in Figure     Additional  gures  including plotting the distribution of the tvalue approximations  appear
in the supplementary material 
The results show that tvalue approximations that do not
take into account the inherent randomness in the DPalgorithms lead to erroneous conclusions  One approach
would be to follow the more conservative approach we advocate in this paper  where Algorithm   may allow you to
get true approximation of the tvalues and otherwise reject the nullhypothesis only based on the con dence interval  of Algorithm   or   not intersecting the origin  Another approach  which we leave as future work  is to replace the    distribution with   new distribution  one that
takes into account the randomness in the estimator as well 
This  however  has been an open and longstanding challenge since the  rst works on DP and statistics  see  Vu
  Slavkovic    Dwork   Lei    and requires we
move into nonasymptotic hypothesis testing 

    Synthetic data  coordinate  

    Synthetic data  coordinate  

Figure   Correctly and Wrongly Rejecting the NullHypothesis

Differentially Private Ordinary Least Squares

Acknowledgements
The bulk of this work was done when the author was  
postdoctoral fellow at Harvard University  supported by
NSF grant CNS  and also an unpaid collaborator
on NSF grant   The author wishes to wholeheartedly thank Prof  Salil Vadhan  for his tremendous help in
shaping this paper  The author would also like to thank
Prof  Jelani Nelson and the members of the  Privacy Tools
for Sharing Research Data  project at Harvard University  especially James Honaker  Vito   Orazio  Vishesh
Karwa  Prof  Kobbi Nissim and Prof  Gary King  for many
helpful discussions and suggestions  as well as Abhradeep
Thakurta for clarifying the similarity between our result 
Lastly the author thanks the anonymous referees for many
helpful suggestions in general and for   reference to  Ullman    in particular 

References
Agresti     and Finlay     Statistical Methods for the Social

Sciences  Pearson    Hall   

Bassily     Smith     and Thakurta     Private empirical
risk minimization  Ef cient algorithms and tight error
bounds  In FOCS   

Blocki     Blum     Datta     and Sheffet     The
JohnsonLindenstrauss transform itself preserves differential privacy  In FOCS   

Chaudhuri  Kamalika and Hsu  Daniel    Convergence rates
for differentially private statistical estimation  In ICML 
 

Chaudhuri  Kamalika  Monteleoni  Claire  and Sarwate 
Anand    Differentially private empirical risk minimization  Journal of Machine Learning Research     

Duchi  John    Jordan  Michael    and Wainwright  MarIn

tin    Local privacy and statistical minimax rates 
FOCS  pp     

Dwork     and Lei     Differential privacy and robust statis 

tics  In STOC   

Dwork  Cynthia  Kenthapadi  Krishnaram  McSherry 
Frank  Mironov  Ilya  and Naor  Moni  Our data  ourselves  Privacy via distributed noise generation  In EUROCRYPT     

Dwork  Cynthia  Mcsherry  Frank  Nissim  Kobbi  and
Smith  Adam  Calibrating noise to sensitivity in private
data analysis  In TCC     

Dwork  Cynthia  Talwar  Kunal  Thakurta  Abhradeep  and
Zhang  Li  Analyze gauss   optimal bounds for privacy preserving principal component analysis  In STOC 
 

Dwork  Cynthia  Su  Weijie  and Zhang  Li  Private false

discovery rate control  CoRR  abs   

Hoerl        and Kennard        Ridge regression  Biased
estimation for nonorthogonal problems  Technometrics 
   

Kasiviswanathan     Lee     Nissim     Raskhodnikova 
   and Smith     What can we learn privately  In FOCS 
 

Kifer  Daniel  Smith  Adam    and Thakurta  Abhradeep 
Private convex optimization for empirical risk minimization with applications to highdimensional regression  In
COLT   

Laurent     and Massart     Adaptive estimation of  
quadratic functional by model selection  The Annals of
Statistics       

Ma        and Zarowski  Christopher    On lower bounds
for the smallest eigenvalue of   hermitian positivede nite matrix  IEEE Transactions on Information Theory     

Muller  Keith    and Stewart  Paul    Linear Model Theory  Univariate  Multivariate  and Mixed Models  John
Wiley   Sons  Inc   

Pilanci     and Wainwright     Randomized sketches of
convex programs with sharp guarantees  In ISIT     

Pilanci  Mert and Wainwright  Martin    Iterative hessian
sketch  Fast and accurate solution approximation for
constrained leastsquares  CoRR  abs     

Rao     Radhakrishna  Linear statistical inference and its

applications  Wiley   

Rogers  Ryan    Vadhan  Salil    Lim  HyunWoo  and
Gaboardi  Marco  Differentially private chisquared hypothesis testing  Goodness of    and independence testing  In ICML  pp     

Rudelson  Mark and Vershynin  Roman  Smallest singular
value of   random rectangular matrix  Comm  Pure Appl 
Math  pp     

Sarl os     Improved approx  algs for large matrices via ran 

dom projections  In FOCS   

Sheffet     Private approximations of the  ndmoment matrix using existing techniques in linear regression  CoRR 
abs    URL http arxiv org 
abs 

Smith  Adam    Privacypreserving statistical estimation
with optimal convergence rates  In STOC  pp   
 

Differentially Private Ordinary Least Squares

Strack     DeShazo     Gennings     Olmo     Ventura 
   Cios     and Clore     Impact of HbA   measurement on hospital readmission rates  Analysis of  
clinical database patient records  BioMed Research International    pages   

Tao     Topics in Random Matrix Theory  American Math 

ematical Soc   

Thakurta  Abhradeep and Smith  Adam  Differentially private feature selection via stability arguments  and the robustness of the lasso  In COLT   

Tikhonov        Solution of incorrectly formulated problems and the regularization method  Soviet Math  Dokl 
   

Uhler  Caroline  Slavkovic  Aleksandra    and Fienberg  Stephen    Privacypreserving data sharing for
Journal of Privacy
genomewide association studies 
and Con dentiality    Available at  http 
repository cmu edu jpc vol iss 

Ullman     Private multiplicative weights beyond linear

queries  In PODS   

Vu     and Slavkovic     Differential privacy for clinical

trial data  Preliminary evaluations  In ICDM   

Wang  Yue  Lee  Jaewoo  and Kifer  Daniel  Differentially private hypothesis testing  revisited  CoRR 
abs   

Xi     Kantarcioglu     and Inan     Mixture of gaussian models and bayes error under differential privacy 
In CODASPY  ACM   

Zhou     Lafferty     and Wasserman     Compressed re 

gression  In NIPS   

