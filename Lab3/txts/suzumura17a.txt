Selective Inference for Sparse HighOrder Interaction Models

Shinya Suzumura   Kazuya Nakagawa   Yuta Umezu   Koji Tsuda     Ichiro Takeuchi    

Abstract

Finding statistically signi cant highorder interactions in predictive modeling is important but
challenging task because the possible number of
highorder interactions is extremely large      
   
In this paper we study feature selection and statistical inference for sparse highorder interaction models  Our main contribution
is to extend recently developed selective inference framework for linear models to highorder
interaction models by developing   novel algorithm for ef ciently characterizing the selection
event for the selective inference of highorder interactions  We demonstrate the effectiveness of
the proposed algorithm by applying it to an HIV
drug response prediction problem 

  Introduction
Finding statistically reliable highorder interaction features
in predictive modeling has been important challenging task 
For example  in   biomedical study  cooccurrence of multiple mutations in multiple genes may have   signi cant in 
 uence on   response to   drug even if occurrence of single
mutation in each of these genes has no in uence  Manolio   Collins    Cordell      major challenge
in prediction modeling with highorder interaction features
is the exponentially expanded feature space  If one has  
dataset with   original variables and takes into account in 

teractions up to order    the model has      cid  

 cid  
 cid 

 

 
features       for                     Unless
both   and   are fairly small    is extremely large  Feature selection and statistical inference in such an extremely
highdimensional model are challenging both computationally and statistically 
  common approach to highdimensional modeling is to
consider   sparse model         model only with   selected

 Nagoya Institute of Technology  Nagoya  Japan  University
of Tokyo  Tokyo  Japan  RIKEN  Tokyo  Japan  Correspondence
to  Ichiro Takeuchi  takeuchi ichiro nitech ac jp 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Example of the tree structure among highorder interaction features when       and      

subset of features  In the past two decades  considerable
amount of studies have been done on sparse modeling and
feature selection in highdimensional models 
In these
studies    variety of feature selection algorithms such as
marginal screening  Fan   Lv    orthogonal matching pursuit  Pati et al    LASSO  Tibshirani   
and their various extensions have been developed  On the
other hand  statistical inference for sparse models  hypothesis testing or con dence interval computation of the  tted coef cients  have not been deeply studied until very
recently  The main challenge in statistical inference of
sparse models is that  if the data is used for selecting  
subset of features  this selection event must be taken into
account in the following inference stage  Otherwise  the
inference results are distorted by socalled selection bias 
and false positive errors cannot be controlled at desired
levels  This problem is refereed to as selective inference
or post selection inference  Benjamini   Yekutieli   
Benjamini et al    Berk et al    After the seminal
work by Lee et al    signi cant progress has been recently made on selective inference for sparse linear models
 Fithian et al      Lee   Taylor    Fithian et al 
  Tian   Taylor    Taylor   Tibshirani   
Yang et al    Barber   Cand es   
In this paper  we study feature selection and statistical inference for sparse highorder interaction models  Unfortunately  neither existing feature selection methods nor existing selective inference methods can be applied to sparse
highorder interaction models because the computational
costs of these existing methods at least linearly depend on
the number of features    The main contribution in this
paper is to develop computationally ef cient algorithms for
these two tasks when the original variables are represented
in       Our main idea is to exploit the underlying tree
structure of highorder interaction features as depicted in

Selective Inference for Sparse HighOrder Interaction Models

Figure  
In feature selection tasks  it allows us to ef 
ciently identify interaction features that have no chance to
be selected  In statistical inference tasks  it allows us to ef 
 ciently identify interaction features that do not affect the
results of the selective inference 
We demonstrate the effectiveness of the proposed methods
through numerical experiments both on synthetic and real
datasets 
In the latter  we apply the proposed method to
HIV dataset in  Rhee et al    where the goal is to identify statistically signi cant highorder interactions of multiple gene mutations that are signi cantly associated with
HIV drug responses 

Related works and our contributions Methods for ef 
ciently  nding highorder interaction features and properly
evaluating their statistical signi cances have long been desired in many scienti   studies 
In the past decade  feature selection for interaction models has been studied in the context of sparse learning  Choi
et al    Hao   Zhang    Bien et al    None
of these works have   special computational trick for handling exponentially large number of interaction features 
which makes their empirical evaluations restricted upto
second order interactions  One commonly used heuristic
in the context of interaction modeling is to introduce  
prior knowledge such as strong heredity assumption where 
     an interaction term      would be selected only when
both of    and    are selected  Such   heuristic restriction is helpful for reducing the number of interaction terms
to be considered  However  in many scienti   studies  researchers are primarily interested in  nding interactions
even when their main effects alone do not have any association with the response  The idea of considering   tree structure among interaction features has been commonly used  
data mining literature  Kudo et al    Saigo et al   
Nakagawa et al    However  it is dif cult to properly
assess the statistical signi cances of the selected features
by these mining techniques 
One traditional approach to assessing the statistical significances of selected features is multiple testing correction
 MTC  In the context of DNA microarray studies  many
MTC procedures for highdimensional data have been proposed  Tusher et al    Dudoit et al    An MTC
approach for statistical evaluation of highorder interaction features was recently studied in  Terada et al   
LlinaresL opez et al      main drawback of MTC is
that they are highly conservative when the number of candidate features increases  Another common approach is datasplitting  DS   Fithian et al      In DS approach  we
split the data into two subsets  and use one for feature selection and another for statistical inference  which enables
us to remove the selection bias  However  performances

of DS approach is clearly weak both in selection and inference stages because only   part of the available data is
used in each stage  In addition  it is quite annoying that
different set of features would be selected if data is splitted
differently  Recently  much attention has been paid to selective inference for sparse linear models  The basic idea
of selective inference is to make inferences conditional on
  feature selection event  Lee et al    recently proposed   practical selective inference framework for   class
of feature selection algorithms 
The main contribution in this paper is to extend the selective inference framework into sparse highorder interaction
models by introducing novel computational algorithms  To
the best of our knowledge  there are no other existing works
for sparse highorder interaction models in which the statistical signi cances of the  tted coef cients are properly
evaluated in nonasymptotic sense 

Notations We use the following notations in the remainder  For any natural number    we de ne    
 
               vector and   matrix is denoted such as
    Rn and     Rn    respectively  The index function is written as     which returns   if   is true  and  
otherwise  The sign function is written as sgn    which
returns   if       and   otherwise  An       identity
matrix is denoted as In 

  Preliminaries
  Problem setup
Consider   regression problem with   response       and
ddimensional original covariates                 zd cid  by the
following highorder interaction model up to rth order

   jr zj    zjr    

 

   jr    

  cid cid jr

where    are the coef cients and   is   random noise  We
assume that each original covariate zj          is de ned in
  domain     Here  values   and   respectively might
be interpreted as the existence and the nonexistence of
  certain property  and values between them indicate the
 degree  of existence  Highorder interaction features thus
represent coexistence of multiple properties  For example 
if we are interested in interactions among age  body mass
index  BMI  and   mutation in   certain gene  we may code

   

   zj   

     zj zj 

 cid 

    
     

 cid 

 cid 

         

  cid   

Selective Inference for Sparse HighOrder Interaction Models

some covariates as

   

 BMI        
 

zj   
zj     mutation in the gene 

if BMI    
if BMI      
if BMI    

 cid  
 cid  features  Let us write the mapping from the

Then       an interaction term zj  zj  represents the coexistence of high BMI and   mutation in the gene 
 cid 
The highorder interaction model Eq  has in total    
original covariates                 zd cid    Rd to the highorder interaction features                 xD cid    RD as
                    cid         

   

 

                      zd               zd zd 

          zk          zd   zd cid 

Then  the highorder interaction model Eq  is simply
written as   Ddimensional linear model

     cid                DxD 

where              are   coef cients corresponding to
               jr in Eq  Since   highorder interaction
feature is   product of original covariates de ned in    
the range of each feature xj          is also    
The original training set is denoted as  zi  yi          
       while the expanded training set is written as
 xi  yi                  The latter is also denoted as
                  Rn where each row of   is xi   Rd
and each element of   is yi  Furthermore  the jth column
of   is written as              We denote the pseudo
inverse of   as          cid     cid 
Our goal is to identify statistically signi cant highorder interaction terms that have large impacts on the response  
by identifying regression coef cients    which are significantly deviated from zero  Unfortunately  since the number of coef cients    to be  tted would be far greater than
the sample size    traditional leastsquare estimation theory cannot be used for making statistical inferences on the
 tted model  We thus consider  rst to perform feature selection and then to make statistical inference only for the
selected features based on selective inference approach 

  Selective inference for sparse linear models

In this section  we brie   review the selective inference
framework for sparse linear models developed by Lee et al 
  Selective inference is developed for two stage
methods  where   subset of features is selected in the  rst
stage  and inferences are made only on the selected features
in the second stage    key  nding by Lee et al    is

that  if the  rst selection stage is described as   linear selection event  then exact statistical inference of the  tted
coef cients conditional on the selection event can be done 
Consider   linear regression model            where
    RD is the true coef cients and   is distributed according to        with known variance  

Feature selection stage Suppose that  in the  rst feature
selection stage    subset of features         are selected 
The selective inference framework in Lee et al    can
be applied to feature selection algorithms whose selection
process can be characterized by   set of linear inequalities
in the form of Ay     with   certain matrix   and   certain vector   that do not depend on    This type of selection event is called   linear selection event  In the selective
inference framework  inferences are made conditional on
the selection event  It means that  in the case of   linear
selection event  we only care about the cases where   is
observed in   polytope Pol           Rn   Ay     
In Lee   Taylor   and Lee et al    marginal
screening  OMP and LASSO are shown to be linear selection events  indicating that the selective inference framework can be applied to statistical testing of the selected
features by these algorithms 

Statistical inference stage Consider   hypothesis testing
for the jth selected feature in  

               vs              cid   

 

The leastsquare estimator of the linear model only with the
selected features   is written as         cid   XS   cid     
If we consider the case where   is NOT selected from the
data       independent of    then  under the null hypothesis
   the sampling distribution of each  tted coef cient is
           
For twosided test at level   if the critical values  cid  and
   are chosen to be the lower and the upper   points
of the sampling distribution in Eq  then the type   error
at level   is controlled as

         cid   XS 
jj  

     where  

 

Pr          cid        

 

On the other hand  when   is selected from the data as
we consider here  we would like to control the following
selective type   error

          
          

Pr          cid     
 Pr          cid     

         is selected 
          Pol         
by appropriately selecting the adjusted critical values  cid     
 
    where the selection event    is selected  is
and       

Selective Inference for Sparse HighOrder Interaction Models

written as     Pol    in the case of   linear selection
event  Lee et al    derived how to compute these
adjusted critical values as formally stated in the following
lemma 
Lemma   If the critical values are computed as

                     
 cid     
      
                     

 

   

 

   

 
     

   

   

then the selective type   error is controlled as in Eq   
where         
is the cumulative distribution function of  
 
truncated Normal distribution TN               
               
               

        
       

 

and the truncation points are obtained  by using the observed      and    as

                      cid   XS 
jj  

where      min

                  

                        cid   XS 
jj  

where      max

                  

   

   cid ej   Pol   

   
   cid ej   Pol   

The proof of Lemma   is is presented in Appendix   although it is easily proved by using the results in Lee et al 
  See Lee et al    for more general statement
about the selective inference framework 
Eq  indicates that the truncation points are obtained by
considering the interval where the test statistic      can
move within the polyhedron Pol    Figure   schematically illustrates that  when we make inferences conditional
on   linear selection event    the sampling distribution is
de ned within the polytope Pol    and it follows   truncated normal distribution when   is normally distributed 
Unfortunately  we cannot directly apply this selective inference framework to highorder interaction models because
the polytope Pol    is characterized by extremely large
number of linear inequalities  and the optimization problems in Eq  are hard to solve 

  Feature selection for interaction models
In this section  we present two feature selection algorithms
for highorder interaction models  Since the number of features   is extremely large  existing feature selection algorithms for linear models cannot be directly applied to interaction models  In this paper  we study marginal screening
 MS  and orthogonal matching pursuit  OMP  as examples
of feature selection algorithms 

Figure   An illustration of polyhedral lemma  The polyhedron
represents the selection event and truncation points can be comS  cid ej  In adputed by optimizing   along with the direction     
dition  critical values can be obtained by computing Eq  and
the red region shows the rejection region of the test in Eq 

  MS for interaction models

Consider selecting the top   interaction features from all
the   interaction features that have marginal strong correlations with the response  Noting that each feature is
de ned in     and the value indicates  the degree of 
the existence of   certain property  we consider   score
              for each of the   features  and select the top
  cid 
  features according to their absolute scores    cid 
      We denote the index set of the selected   features by    and that
of the unselected            features by              
Since   is extremely large  we cannot compute the score
for each interaction feature  We exploit the tree structure
among interaction patterns as depicted in Figure  
De nition    Descendant features  For each         let
Des          be the set of features corresponding to the
descendant nodes in the tree including   itself 
Lemma   Consider an interaction feature             
whose indices are represented in   tree structure as depicted in Figure  Then  for any node         in the tree 

xijyi   cid 

xijyi

  yi 

   

   cid 

  yi 

   jy    max
for all      Des   

The proof of Lemma   is presented in Appendix   
Lemma   tells that  for   descendant feature               
    Des    an upper bound of the absolute score    cid 
  
  
can be computed based on its parent feature     
We note that this simple upper bound has been used in some
data mining studies such as Saigo et al    Kudo et al 

yL                         ejSelective Inference for Sparse HighOrder Interaction Models

  Nakagawa et al    When we search over the
tree  if the upper bound in Eq  is smaller than the current kth largest score at   certain node    then we can quit
searching over its descendant nodes      Des   
As pointed out in Lee   Taylor   feature selection
processes of marginal screening is   linear selection event 
     characterized by   set of linear constraints  The event
that   features in   are selected  and    features in    are
   cid            cid   
not selected is rephrased as    cid 
             Then  the above
        Let sj   sgn   cid 
feature selection event is rewritten with the sign constraints
of the selected features by the following        constraints

           cid 

 sjx cid 

                 

 sjx         cid cid               cid           
 sjx         cid cid               cid           

   
   
   
These constraints are written as Ay     with   matrix
               Unfortunately   nding  min and  max
by naively solving the optimization problems in Eq  is
computationally dif cult because the polyhedron Pol    is
characterized by the extremely large number of constraints 
For example  when                     the
number of linear inequalities that de nes the polyhedron
Pol    is             

  OMP for interaction models

Orthogonal matching pursuit  OMP  is   wellknown iterative feature selection method  Pati et al    At each
iteration  the most correlated feature with the residual of
the current model which is  tted via leastsquares method
by using the features selected in earlier steps 
Consider again selecting   interaction features by OMP 
Let               be the sequence of the indices of the selected features from step   to step   for         and de 
 ne Sh                 Before step       we have
already selected   features          Sh  Using these   fea 
 cid 
tures  the current ndimensional model output is written as
 Sh        where the coef cients  Sh           
    
are estimated by leastsquares method  Denoting by  Sh
the     matrix whose jth column is      the least square
estimates are written as  Sh      Sh           Sh   cid   
 Sh    Then  at the       step  we consider the correlation between the residual vector rh        Sh
 Sh and
  feature     cid  for   cid     Sh  and  nd the one that maximizes
the absolute correlation    cid 
   cid rh  among them  Here  since
the number of remaining features    Sh          is still
extremely large  it is hard to compute all these       correlations  To overcome this dif culty  we can simply use
Lemma   just by replacing   with the current residual rh 
Speci cally  for   descendant feature           Des    an

upper bound of    cid 
  

rh  is given as

   cid 

  rh   

   cid 
   rh    max

xijrh     cid 

  rh   

   

xijrh  

At each iteration  when we search over the tree  if the upper
bound is smaller than the current largest correlation  then 
in the same way as the case of MS  we can quit searching
over its descendant nodes   cid    Des   
It is also pointed out in Lee   Taylor   that   feature
selection process of OMP is linear selection event  At step
   the event that the    th feature is selected is formulated
as    cid 
 Sh  
Sh
set of linear inequalities with respect to  

   cid rh  for all   cid     Sh  Let PSh   In  

  Then  the above selection event is rewritten as  

   rh       cid 

       cid 

   
   
   

                cid cid PSh         cid     Sh 
                cid cid PSh         cid     Sh 
   PSh      
the OMP is characterized by cid 
   rh  By combining all the linear
where        sgn   cid 
selection events in   steps  the entire selection event of
               linear inequalities in Rn  In practice  it is computationally intractable to handle these extremely large number of linear
inequalities 

  Selective inference for interaction models
In this section  we present an ef cient selective inference
algorithm for highorder interaction models  which is our
main contribution 
The discussion in   suggests that it would be hard to compute critical values for selective inference in Eq  because
the selection event     Pol    is characterized by extremely large number of inequalities  Our basic idea for addressing this computational dif culty is to note that most of
the inequalities actually do not affect the results of the selective inference  and   large portion of them can be identi 
 ed by exploiting the antimonotonicity properties de ned
in the tree structure among highorder interaction features 

  Marginal screening

We consider   trees for each of the   selected features 
Each tree consists of   set of nodes corresponding to each of
the nonselected features   cid        For   pair       cid          
the   cid th node in the jth tree corresponds to the linear inequalities Eqs    and     When we search over these  
trees  we introduce   novel pruning strategy by deriving  
condition such that  if the   cid th node in the jth tree satis es
certain conditions  then all the        cid th inequalities for all

Selective Inference for Sparse HighOrder Interaction Models

   cid    Desj   cid  are guaranteed to be irrelevant to the selective inference results because they do not affect the optimal
solutions in Eq  where we de ne Desj   cid  be all the
features corresponding to the descendant node of   cid  in the
jth tree 
Lemma   Let         
mization problems in   are respectively written as

   cid ej  The solutions of the opti 

       min   
       max   

       
       

   
       
   
       

where

   
   

    

min
 cid 
       

 cid 

 sj          cid   

   
   

    

max
 cid 
       

 cid 

 sj          cid   

   
   

   
   

   
    min
    
    

sj   cid 

    

min
 cid 
       
 sj         cid   

 cid 

    

max
 cid 
       
 sj         cid   

 cid 
sjx cid 
    
sjx cid 
    

 sjx         cid cid  
 sjx         cid cid 

 sjx         cid cid  
 sjx         cid cid 
 sjx         cid cid  
 sjx         cid cid 
 sjx         cid cid  
 sjx         cid cid 

 

 

 

 

   

   

   

   

     

    max
    
    

sj   cid 

sjx cid 
    
sjx cid 
    

 

  yi 

      

  yi 

      

      

     
    sjx cid 

    
    sjx cid 

    
    sjx cid 

The proof of Lemma   is presented in Appendix   
Lemma   For any triplet       cid     cid             Desj   cid 
xij cid yi    sjx         cid cid       
xij cid yi    sjx         cid cid       
xij cid      sjx         cid cid     
xij cid      sjx         cid cid     
xij cid yi    sjx         cid cid       
xij cid yi    sjx         cid cid   
xij cid      sjx         cid cid     
xij cid      sjx         cid cid     

 cid 
 cid 
 cid 
 cid 
        cid 
        cid 
        cid 
        cid 

    
    sjx cid 

    
    sjx cid 

     
    sjx cid 

     
    sjx cid 

     
    sjx cid 

      

   

  yi 

  yi 

    

    

    

    

The proof of Lemma   is presented in Appendix   

Theorem       Consider solving the optimization problem
in Eq    and let    
  be the current optimal solution 
     we know that the optimal    
  is at least no greater
than    
      

   
       
   
       
is true  then the        cid th constraint in Eq      for any
      cid     cid             Desj   cid  does not affect the optimal
solution in Eq   

    If
             
       

          
          

          
          

       
        

      

 ii  Next  consider solving the optimization problem in
Eq    and let    
             
       

  be the current optimal solution  If
   
       
   
       
is true  then the        cid th constraint in Eq      for any
      cid     cid             Desj   cid  does not affect the optimal
solution in Eq   

          
          

          
          

       
        

 iii  Furthermore  consider solving the optimization problem in Eq    and let    
  be the current optimal solution  If
     

   
       
   
       
is true  then the        cid th constraint in Eq      for any
      cid     cid             Desj   cid  does not affect the optimal
solution in Eq   

              
        

          
          

          
          

        
       

 iv  Finally  consider solving the optimization problem in
Eq    and let    
              
        

  be the current optimal solution  If
   
       
   
       

          
          

          
          

        
       

     

is true  then the        cid th constraint in Eq      for any
      cid     cid             Desj   cid  does not affect the optimal
solution in Eq   

The proof of Theorem   is presented in Appendix  Note
that all the conditions in Theorem   can be checked at the
  cid th node in each tree  If the conditions are satis ed as
the   cid th node  then one can skip searching over its subtree  It allows us to perform selective inference for highorder interaction models even the number of constraints
that de nes the selection event is extremely large  As we
demonstrate in the experiment section  these pruning conditions are quite effective in practice  For example  we can
perform selective inference for an interaction models with
                    in   few seconds 

Selective Inference for Sparse HighOrder Interaction Models

  Orthogonal matching pursuit  OMP 

As we discuss in the previous section  the selection event
at each iteration of OMP has same form as MS  Therefore 
we can derive similar pruning conditions as in Theorem  
for OMP  Due to the space limitation  we deffer the corresponding lemma and the theorem for OMP in Appendix   

  Experiments
We demonstrate the performance of the selective inference
for highorder sparse interaction models by numerical experiments on synthetic datasets and   real dataset 

  Experiments on synthetic datasets

First  we compared selective inference  select  with
naive  naive  and datasplitting  split  on synthetic
datasets  In naive  the critical values of the selected  
features were naively computed without any selection bias
correction mechanisms as in Eq    In split  the dataset
was  rst divided into two equally sized sets  and one of
them was used for selection stage  and the other was used
for inference stage  Note that the errors controlled by these
methods are individual false positive rate for each of the
selected features  although naive actually cannot control
it  we applied Bonferroni correction within the   selected
features       we reject the hypothesis in Eq    with the
signi cance level    where       and we refer this
error as familywise false positive rates  FWFPRs 
The synthetic dataset was generated as follows  In the experiments for comparing FWFPRs  we generated the training instances  zi  yi           independently at random
for each         The original covariates zi were randomly
generated so that it contains           on average  where
        is an experimental parameter for representing
the sparsity of the dataset  while the response yi was randomly generated from   Normal distribution       In
the experiments for comparing true positive rates  TPRs 
the response yi was randomly generated from   Normal
distribution           where  for each row of    
is de ned as  zi            in the experiments for MS 
 zi                          in the experiments for
OMP  We investigated the performances by changing various experimental parameters  We set the baseline parameters as                              
      and      

  FALSE POSITIVE RATES

Figure   shows the FWFPRs when varying the number of
transactions                   the number of original covariates                   In all cases  the
FWFPRs of naive were far greater than the desired signi cance level       indicating that the selection bias

                   

                   

MS

                   

                   

OMP

Figure   False positive rates  FPRs 

                   

MS

                   

                   

                   

OMP

Figure   True positive rates  TPRs 

is harmful  The FWFPRs of the other two approaches
select and split were successfully controlled 

  TRUE POSITIVE RATES

Figure   shows the TPRs of select and split  we omit
naive because it cannot control FPRs  Here  TPRs are
de ned as the probability of  nding truly correlated interaction features 
In all the setups  the TPRs of select
were much greater than split  Note that the performances of split would be worse than select both in
the selection and the inference stages  The risk of failing to
select truly correlated features in split would be higher
than select because only half of the data would be used
in the selection stage  Similarly  the statistical power in the
inference stage in split would be smaller than select
because the sample size is smaller 

           The number of instances          Familywise error  FWFPRs naiveselectsplit      The number of features          Familywise error  FWFPRs naiveselectsplit           The number of instances          Familywise error  FWFPRs naiveselectsplit      The number of features          Familywise error  FWFPRs naiveselectsplit            True positive rates  TPRs     The number of instances nselectsplit            True positive rates  TPRs The number of features dselectsplit            True positive rates  TPRs     The number of instances nselectsplit            True positive  rates  TPRs The number of features dselectsplitSelective Inference for Sparse HighOrder Interaction Models

with computational trick

MS

     
 
     
 
     
 
     
     
     
     
 
     
 
     
     
     
     

     
 
     
 
     
 
     
 
     
     
     
 
     
 
     
 
     
     
     

 
 
 
 
 
 
 
 
 
 
 
 

Table   Computation times  sec 

OMP

without computational trick
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
    day
    day
    day
    day
    day
    day
    day
    day

with computational trick

     
 
     
     
     
     
     
     
 
     
     
     
    day
    day

     
 
     
 
     
 
     
     
     
     
 
     
     
     
     
     

without computational trick
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
    day
    day
    day
    day
    day
    day
    day
    day

Table   The numbers of signi cant highorder interactions of
multiple mutations in HIV datasets 

MS

OMP

Data  st  nd  rd  th Time   

 st  nd  rd  th Time   

NNRTI       

NRTI       

 
 
 

PI       

dlv      
efv      
nvp      

 tc      
abc      
azt      
         
ddi      
tdf      

apv      
atv      
idv      
lpv      
nfv      
rtv      
sqv      

 

 

 
 
 
 
 

 
 
 
 
 
 
 

 

 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 

 

 

 

 

 
 

 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 

  COMPUTATIONAL EFFICIENCY

Table   shows the computation times in seconds for the selective inference approach with and without the computational tricks described in   for various values of the number of transactions                   the number of
original covariates                   and the sparsity rates          we terminated the search if the
time exceeds   day  It can be observed from the table that 
if we use the computational trick  the selective inferences
can be conducted with reasonable computational costs except for         and       cases with OMP  When the
computational trick was not used  the cost was extremely
large  Especially when the number of original covariates  
is larger than   we could not complete the search within
  day  From the results  we conclude that computational
trick described in   is indispensable for selective inferences for sparse highorder interaction models 

  Application to HIV drug resistance data

We applied the selective inference approach to HIV  sequence data obtained from Stanford HIV Drug Resistance

Database  Rhee et al    The goal here is to  nd statistically signi cant highorder interactions of multiple mutations  up to       order interactions  that are highly associated with the drug resistances  We selected      
features  and evaluated the statistical signi cances of these
features by the selective inference framework  Table  
shows the numbers of  st   nd   rd and  th order interactions that were statistically signi cant after Bonferroni
correction       signi cance level is set to be    with
       there were no statistically signi cant  th order interactions 
Figure   shows the degree of signi cances in the form of
adjusted pvalues after Bonferroni correction in increasing
order on idv and     datasets by MS and OMP scenario 
respectively  These results indicate that the selective inference approach could successfully identify statistically signi cant highorder interactions of multiple mutations 

    idv dataset  MS 

        dataset  OMP 

Figure   The list of Bonferroniadjusted selective pvalues of
      selected highorder interactions of multiple mutations
on two HIV datasets 

                                                                                                                                              MAdjusted pvalue                                                                                                                                                    EAdjusted pvalueSelective Inference for Sparse HighOrder Interaction Models

Acknowledgements
This work was partially supported by MEXT KAKENHI
JST CREST  JPMJCR 
       
JPMJCR  RIKEN Center
for Advanced Intelligence Project  and JST support program for starting up
innovationhub on materials research by information integration initiative 

References
Barber  Rina Foygel and Cand es  Emmanuel      knockoff  lter for highdimensional selective inference  arXiv
preprint arXiv   

Benjamini  Yoav and Yekutieli  Daniel  False discovery
rate adjusted multiple con dence intervals for selected
parameters  Journal of the American Statistical Association     

Benjamini  Yoav  Heller  Ruth  and Yekutieli  Daniel  Selective inference in complex research  Philosophical
Transactions of the Royal Society of London    Mathematical  Physical and Engineering Sciences   
   

Berk  Richard  Brown  Lawrence  Buja  Andreas  Zhang 
Kai  and Zhao  Linda  Valid postselection inference 
The Annals of Statistics     

Bien     Taylor        and Tibshirani       LASSO for hierarchical interactions  Journal of The Royal Statistical
Society       

Choi       Li     and Zhu     Variable selection with the
strong heredity constraint and its oracle property  Journal of the American Statistical Association   
   

Cordell  Heather    Detecting gene gene interactions that
underlie human diseases  Nature Reviews Genetics   
   

Dudoit  Sandrine  Shaffer  Juliet Popper  and Boldrick  Jennifer    Multiple hypothesis testing in microarray experiments  Statistical Science  pp     

Fan     and Lv     Sure independence screening for ultrahigh dimensional feature space  Journal of The Royal
Statistical Society       

Fithian  William  Sun  Dennis  and Taylor  Jonathan  Optimal inference after model selection  arXiv preprint
arXiv     

Fithian  William  Sun  Dennis  and Taylor  Jonathan  OparXiv preprint

timal inference after model selection 
arXiv     

Fithian  William  Taylor  Jonathan  Tibshirani  Robert  and
Tibshirani  Ryan  Selective sequential model selection 
arXiv preprint arXiv   

Hao  Ning and Zhang  Hao Helen 

Interaction screening
for ultrahighdimensional data  Journal of the American
Statistical Association     

Kudo     Maeda     and Matsumoto     An application of
boosting to graph classi cation  In Advances in Neural
Information Processing Systems   

Kudo  Taku  Maeda  Eisaku  and Matsumoto  Yuji  An application of boosting to graph classi cation  In Advances
in neural information processing systems  pp   
 

Lee  Jason   and Taylor  Jonathan    Exact post model
selection inference for marginal screening  In Advances
in Neural Information Processing Systems  pp   
 

Lee  Jason    Sun  Dennis    Sun  Yuekai  Taylor 
Jonathan    et al  Exact postselection inference  with
application to the lasso  The Annals of Statistics   
   

LlinaresL opez  Felipe  Sugiyama  Mahito  Papaxanthos 
Laetitia  and Borgwardt  Karsten  Fast and memoryef cient signi cant pattern mining via permutation testIn Proceedings of the  th ACM SIGKDD Intering 
national Conference on Knowledge Discovery and Data
Mining  pp    ACM   

Manolio  Teri   and Collins  Francis    Genes  environment  health  and disease  facing up to complexity  Human heredity     

Nakagawa  Kazuya  Suzumura  Shinya  Karasuyama 
Masayuki  Tsuda  Koji  and Takeuchi  Ichiro  Safe pattern pruning  An ef cient approach for predictive pattern
mining  In Proceedings of the  nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining  pp    ACM   

Pati  Yagyensh Chandra  Rezaiifar  Ramin  and Krishnaprasad  PS  Orthogonal matching pursuit  Recursive
function approximation with applications to wavelet decomposition  In Signals  Systems and Computers   
  Conference Record of The TwentySeventh Asilomar Conference on  pp    IEEE   

Rhee  SooYon  Gonzales  Matthew    Kantor  Rami  Betts 
Bradley    Ravela  Jaideep  and Shafer  Robert    Human immunode ciency virus reverse transcriptase and
protease sequence database  Nucleic acids research   
   

Selective Inference for Sparse HighOrder Interaction Models

Saigo     Uno     and Tsuda     Mining complex genotypic features for predicting hiv  drug resistance  Bioinformatics     

Taylor  Jonathan and Tibshirani  Robert  Postselection
arXiv

inference for   penalized likelihood models 
preprint arXiv   

Terada  Aika  OkadaHatakeyama  Mariko  Tsuda  Koji 
and Sese  Jun  Statistical signi cance of combinatorial
regulations  Proceedings of the National Academy of
Sciences     

Tian  Xiaoying and Taylor  Jonathan  Asymptotics of selective inference  arXiv preprint arXiv   

Tibshirani     Regression shrinkage and selection via the
lasso  Journal of the Royal Statistical Society  Series   
   

Tusher  Virginia Goss  Tibshirani  Robert  and Chu 
Gilbert  Signi cance analysis of microarrays applied to
the ionizing radiation response  Proceedings of the National Academy of Sciences     

Yang  Fan  Barber  Rina Foygel  Jain  Prateek  and Lafferty 
John  Selective inference for groupsparse linear models 
arXiv preprint arXiv   

