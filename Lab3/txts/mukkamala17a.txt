Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Mahesh Chandra Mukkamala     Matthias Hein  

Abstract

 

Adaptive gradient methods have become recently
very popular 
in particular as they have been
shown to be useful in the training of deep neural networks 
In this paper we have analyzed
RMSProp  originally proposed for the training
of deep neural networks  in the context of onT  type reline convex optimization and show
gret bounds  Moreover  we propose two variants SCAdagrad and SCRMSProp for which
we show logarithmic regret bounds for strongly
convex functions  Finally  we demonstrate in the
experiments that these new variants outperform
other adaptive gradient techniques or stochastic
gradient descent in the optimization of strongly
convex functions as well as in training of deep
neural networks 

  Introduction
There has recently been   lot of work on adaptive gradient
algorithms such as Adagrad  Duchi et al    RMSProp
 Hinton et al    ADADELTA  Zeiler    and
Adam  Kingma   Bai    The original idea of Adagrad to have   parameter speci   learning rate by analyzing the gradients observed during the optimization turned
out to be useful not only in online convex optimization
but also for training deep neural networks  The original
analysis of Adagrad  Duchi et al    was limited to the
case of all convex functions for which it obtained   datadependent regret bound of order   
    which is known
to be optimal  Hazan    for this class  However    lot
of learning problems have more structure in the sense that
one optimizes over the restricted class of strongly convex
functions  It has been shown in  Hazan et al    that
one can achieve much better logarithmic regret bounds for
the class of strongly convex functions 

 

 Department of Mathematics and Computer Science  Saarland
University  Germany  IMPRSCS  Max Planck Institute for Informatics  Saarbr ucken  Germany   Correspondence to  Mahesh
Chandra Mukkamala  mmahesh chandra gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

The goal of this paper is twofold  First  we propose SCAdagrad which is   variant of Adagrad adapted to the
strongly convex case  We show that SCAdagrad achieves
  logarithmic regret bound for the case of strongly convex
functions  which is datadependent  It is known that such
bounds can be much better in practice than data independent bounds  Hazan et al   McMahan    Second  we analyze RMSProp which has become one of the
standard methods to train neural networks beyond stochastic gradient descent  We show that under some condi 
 
tions on the weighting scheme of RMSProp  this algorithm
    regret bound  In fact  it
achieves   datadependent   
turns out that RMSProp contains Adagrad as   special case
for   particular choice of the weighting scheme  Up to our
knowledge this is the  rst theoretical result justifying the
usage of RMSProp in online convex optimization and thus
can at least be seen as theoretical support for its usage in
deep learning  Similarly  we then propose the variant SCRMSProp for which we also show   datadependent logarithmic regret bound similar to SCAdagrad for the class of
strongly convex functions  Interestingly  SCAdagrad has
been discussed in  Ruder    where it is said that  it
does not to work  The reason for this is that SCAdagrad
comes along with   damping factor which prevents potentially large steps in the beginning of the iterations  However  as our analysis shows this damping factor has to be
rather large initially to prevent large steps and should be
then monotonically decreasing as   function of the iterations in order to stay adaptive  Finally  we show in experiments on three datasets that the new methods are competitive or outperform other adaptive gradient techniques as
well as stochastic gradient descent for strongly convex optimization problem in terms of regret and training objective
but also perform very well in the training of deep neural
networks  where we show results for different networks and
datasets 

  Problem Statement
We  rst need some technical statements and notation and
then introduce the online convex optimization problem 

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

  cid 

    

 cid cid      cid  

  Notation and Technical Statements
We denote by      the set              Let     Rd   be  
symmetric  positive de nite matrix  We denote as

 cid      cid      cid    Ay cid   

Aijxiyj 

 cid   cid    

 cid      cid     cid 

Note that the standard Euclidean inner product becomes
  xiyi    cid      cid   While we use here the general notation for matrices for comparison to the literature 
All positive de nite matrices   in this paper will be diagonal matrices  so that the computational effort for computing
inner products and norms is still linear in    The CauchySchwarz inequality becomes   cid      cid      cid   cid    cid   cid     We
further introduce the elementwise product   cid   of two vectors  Let        Rd  then     cid        aibi for                 
Let     Rd   be   symmetric  positive de nite matrix 
    Rd and     Rd   convex set  Then we de ne the
weighted projection    

      of   onto the set   as

   

        arg min

   

 cid       cid 
   

 

It is wellknown that the weighted projection is unique and
nonexpansive 
Lemma   Let     Rd   be   symmetric  positive de 
nite matrix and     Rd be   convex set  Then

 cid cid    

           

     cid cid      cid       cid    

Lemma   For any symmetric  positive semide nite matrix     Rd   we have

 cid    Ax cid     max   cid      cid    tr   cid      cid 

 

where  max    is the maximum eigenvalue of matrix  
and tr    denotes the trace of matrix    

  Problem Statement

In this paper we analyze the online convex optimization
setting  that is we have   convex set   and at each round we
get access to    sub gradient of some continuous convex
function ft          At the tth iterate we predict       
and suffer   loss ft    The goal is to perform well with
respect to the optimal decision in hindsight de ned as

  cid 

  

    arg min

  

ft 

The adversarial regret at time       is then given as

  cid 

  

       

 ft      ft 

  cid 

We assume that the adversarial can choose from the class
of convex functions on    for some parts we will specialize
this to the set of strongly convex functions 

De nition   Let   be   convex set  We say that   function           is  strongly convex  if there exists     Rd
with        for                 such that for all          

                 cid             cid     cid       cid 

diag 

           cid             cid   

   yi   xi 

  

Let     mini       then this function is  strongly convex  in the usual sense  that is

                 cid             cid       cid       cid   

Note that the difference between our notion of componentwise strong convexity and the usual de nition of strong
convexity is indicated by the bold font versus normal font 
We have two assumptions 

     It holds supt   cid gt cid      which implies the
existence of   constant    such that supt   cid gt cid   
  
    

It holds supt   cid      cid      which implies the existence of   constant    such that
supt   cid      cid      

 
One of the  rst methods which achieves the optimal regret
    for convex problems is online projected
bound of   
gradient descent  Zinkevich    de ned as

      PC      tgt 

 

 

where       
is the stepsize scheme and gt is    sub 
gradient of ft at     With       
    online projected gradient descent method achieves the optimal   log     regret
bound for stronglyconvex problems  Hazan et al   
We consider Adagrad in the next subsection which is one of
the popular adaptive alternative to online projected gradient
descent 

  Adagrad for convex problems

In this section we brie   recall the main result for the Adagrad  The algorithm for Adagrad is given in Algorithm   If
the adversarial is allowed to choose from the set of all possible convex functions on     Rd  then Adagrad achieves
 
    as shown in  Duchi et al 
the regret bound of order   
  This regret bound is known to be optimal for this
class  see     
 Hazan    For better comparison to
our results for RMSProp  we recall the result from  Duchi

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Algorithm   Adagrad

Input                         Rd
for       to   do
gt    ft   
vt   vt     gt  cid  gt 
 
vt      
At   diag 
        At
 

 cid        

  gt

 cid 

end for

et al    in our notation  For this purpose  we introduce
the notation                       gT       where gt   is the
ith component of the gradient gt   Rd of the function ft
evaluated at    

Theorem    Duchi et al    Let Assumptions      
hold and let    be the sequence generated by Adagrad in
Algorithm   where gt    ft    and ft         is an
arbitrary convex function  then for stepsize       the regret
is upper bounded as

          
 

 cid       cid     

 cid       cid   

  

  

This can be seen as follows   rst note that vT    cid  

The effective steplength of Adagrad is on the order of  
 
     
   
 
vt     

and thus  At  is   diagonal matrix with entries
Then one has

 

  cid 

  cid 

   

   ii  

 

 cid cid  
 cid   

 
 

 

     

       
 

 cid  

     

       

 

 

 

Thus an alternative point of view of Adagrad  is that it has  
decaying stepsize  
but now the correction term becomes
the running average of the squared derivatives plus   vanishing damping term  However  the effective stepsize has
to decay faster to get   logarithmic regret bound for the
strongly convex case  This is what we analyze in the next
section  where we propose SCAdagrad for strongly convex
functions 

  Strongly convex Adagrad  SCAdagrad 
The modi cation SCAdagrad of Adagrad which we propose in the following can be motivated by the observation
that the online projected gradient descent  Hazan et al 
  uses stepsizes of order         
    in order to achieve
the logarithmic regret bound for strongly convex functions 
In analogy with the derivation in the previous section  we
     But now we modify  At 

still have vT      cid  

     

and set it as   diagonal matrix with entries
one has

 

vt    

  Then

   

   ii  

 cid  

     

 

 
 

 
 

        

 cid  

 
     

        
 

 

 

Again  we have in the denominator   running average of
the observed gradients and   decaying damping factor  In
    in SCthis way  we get an effective stepsize of order     
Adagrad  The formal method is presented in Algorithm
  As just derived the only difference of Adagrad and SCAdagrad is the de nition of the diagonal matrix At  Note

Algorithm   SCAdagrad

Input                         Rd
for       to   do
gt    ft   
vt   vt     gt  cid  gt 
Choose              element wise
At   diag vt    diag   
        At
  gt
 

 cid        

 cid 

end for

also that we have de ned the damping factor    as   function of   which is also different from standard Adagrad 
The constant   in Adagrad is mainly introduced due to numerical reasons in order to avoid problems when gt   is
very small for some components in the  rst iterations and
is typically chosen quite small            For SCAdagrad the situation is different  If the  rst components
                are very small  say of order   then the update
is
which can become extremely large if    is chosen
to be small  This would make the method very unstable
and would lead to huge constants in the bounds  This is
probably why in  Ruder    the modi cation of Adagrad where one  drops the squareroot  did not work   
good choice of    should be initially roughly on the order
    starts to grow 

of   and it should decay as vt    cid  

     

  

 

This is why we propose to use

          vt   

                

for             as   potential decay scheme as it satis 
 es both properties for suf ciently large   and   chosen
on the order of   Also  one can achieve   constant decay scheme for               We will come back to
this choice after the proof  In the following we provide the
regret analysis of SCAdagrad and show that the optimal
logarithmic regret bound can be achieved  However  as it is
datadependent it is typically signi cantly better in practice
than dataindependent bounds 

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

  to denote the inner product            cid 

  Analysis
For any two matrices        Rd    we use the notation
  AijBij 
Note that         tr AT   
Lemma    Lemma    Hazan et al    Let      be
positive de nite matrices  let    cid     cid    then

 cid 

 

               log

 

 cid   

 cid 

   

where     denotes the determinant of the matrix  
Lemma   Let Assumptions       hold  then for    
  and At     as de ned in the SCAdagrad algorithm we
have 

 cid gt    

  gt

  cid 

  

 cid      cid 
    cid 

  

log

  cid 

  

  

 cid cid       cid        

 cid 

  
           
 cid       cid        

Theorem   Let Assumptions       hold and let    be
the sequence generated by the SCAdagrad in Algorithm
  where gt    ft    and ft         is an arbitrary
 strongly convex function     Rd
  where the stepsize
ful lls     maxi  
  Furthermore  let        and
                            then the regret of SCAdagrad can be upper bounded for       as

  
  

           tr diag 

  cid 

  

 

 
 

 

 cid          

   

inf
     

 

 cid cid       cid        

 cid 

  cid 

  

 

 
 

log

 

 

 cid       cid        

          

  

 cid 

For constant                             and         then
the regret of SCAdagrad is upper bounded as

            
 

 

 
 

  cid 

  

log

 cid cid       cid     

 cid 

 

 

For  strongly convex function choosing       
tain the above mentioned regret bounds 

  we ob 

Note that the  rst and the last term in the regret bound
can be upper bounded by constants  Only the second term
depends on     Note that  cid       cid         and as    is
monotonically decreasing  the second term is on the order
of   log     and thus we have   logarithmic regret bound 
As the bound is datadependent  in the sense that it depends
on the observed sequence of gradients  it is much tighter
than   dataindependent bound 

The bound includes also the case of   nondecaying damping factor                  While   rather large
constant damping factor can work well  we have noticed
that the best results are obtained with the decay scheme

          vt   

                

where                 which is what we use in the experiments  Note that this decay scheme for         is
adaptive to the speci   dimension and thus increases the
adaptivity of the overall algorithm  For completeness we
also give the bound specialized for this decay scheme 

Corollary   In the setting of Theorem   choose       
   vt   for                 for some               Then
the regret of SCAdagrad can be upper bounded for      
as

        dD 

 

 
 

   
 

log       

  cid 
 cid  log       cid    cid 

 cid cid       cid        cid        cid cid 
      cid        cid cid 

 

 cid 

log

  

 

 

  

Unfortunately  it is not obvious that the regret bound for our
decaying damping factor is better than the one of   constant
damping factor  Note  however that the third term in the regret bound of Theorem   can be negative  It thus remains
an interesting question for future work  if there exists an
optimal decay scheme which provably works better than
any constant one 

  RMSProp and SCRMSProp
RMSProp is one of the most popular adaptive gradient
algorithms used for the training of deep neural networks
 Schaul et al    Dauphin et al    Daniel et al 
  Schmidhuber    It has been used frequently in
computer vision  Karpathy   FeiFei        
to train
the latest InceptionV  network  Szegedy et al       
Note that RMSProp outperformed other adaptive methods
like Adagrad order Adadelta as well as SGD with momentum in   large number of tests in  Schaul et al    It
has been argued that if the changes in the parameter update
are approximately Gaussian distributed  then the matrix At
can be seen as   preconditioner which approximates the diagonal of the Hessian  Daniel et al    However  it is
fair to say that despite its huge empirical success in practice and some  rst analysis in the literature  there is so far
no rigorous theoretical analysis of RMSProp  We will analyze RMSProp given in Algorithm   in the framework of
of online convex optimization 

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Algorithm   RMSProp

Input                               Rd
for       to   do
gt    ft   
vt    tvt           gt  cid  gt 
and       
Set       
 
 
At   diag 
vt     tI
        At
 

 cid      tA 

  gt

 cid 

 

end for

First  we will show that RMSProp reduces to Adagrad for
  certain choice of its parameters  Second  we will prove
for the general convex case   regret bound of   
    similar to the bound given in Theorem   It turns out that
the convergence analysis requires that in the update of the
weighted cumulative squared gradients  vt    it has to hold

 

     
 

            
 

 

for some           This is in contrast to the original
suggestion of  Hinton et al    to choose       
It will turn out later in the experiments that the constant
choice of    leads sometimes to divergence of the sequence 
whereas the choice derived from our theoretical analysis
always leads to   convergent scheme even when applied to
deep neural networks  Thus we think that the analysis in
the following is not only interesting for the convex case but
can give valuable hints how the parameters of RMSProp
should be chosen in deep learning 
Before we start the regret analysis we want to discuss the
sequence vt in more detail  Using the recursive de nition
of vt  we get the closed form expression

vt    

  cid 
  cid 
  one gets  vt    cid  

       

  

telescoping

    

 kg 

    

 cid  

 
 

  
product

    

ones

  
    

    
gets

With         
and

 cid  

    

the
using
  
     
  and thus
    
     

 cid  

 

vt      
 
If one uses additionally the stepsize scheme       
and
      
  then we recover the update scheme of Adagrad 
see   as   particular case of RMSProp  We are not aware
of that this correspondence of Adagrad and RMSProp has
been observed before 
The proof of the regret bound for RMSProp relies on the
following lemma 

 

Lemma   Let Assumptions    and    and suppose that
  for some           and       Also
   

            

  

 

 

 

 cid 

 

     then

  
   
  vt    

for       suppose cid            
 cid cid   vT    
  cid 
       
      Also for       suppose cid            
 cid cid   vT    
  cid 

Corollary   Let Assumptions       hold and suppose
  for some           and
that      
     and
 cid 

 cid         

              

 cid gt    

set       

  cid 

  then

    

    

 

   

 

 

  gt

  
 

  

 

  

With the help of Lemma   and Corollary   we can now
state the regret bound for RMSProp 

for some       and    

Theorem   Let Assumptions       hold and let    be
the sequence generated by RMSProp in Algorithm   where
gt    ft    and ft         is an arbitrary convex funct           
tion and       
 
     then the regret of RMSProp can be upper bounded
for       as

for some           Also for       let cid          
 cid 
       cid    

 cid cid   vT    

 cid    cid 

     

    

 

 

 

 

 

 

  

Note that for           
  where
RMSProp corresponds to Adagrad we recover the regret
bound of Adagrad in the convex case  see Theorem   up
to the damping factor  Note that in this case

    that is       and       

 cid   vT    

 cid cid cid cid    cid 

       cid       cid   
  

  

  SCRMSProp

Similar to the extension of Adagrad to SCAdagrad  we
present in this section SCRMSProp which achieves   logarithmic regret bound 
Note that again there exist choices for the parameters of
SCRMSProp such that it reduces to SCAdagrad  The correspondence is given by the choice

          
 

      

    

  
 

 

 
 

 

 cid  

     

    with the same
for which again it follows vt      
 
argument as for RMSProp  Please see Equation   for the
correspondence  Moreover  with the same argument as for
SCAdagrad we use   decay scheme for the damping factor
                 for              

     vt  

        

 

 

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds
        log    
inf       jvj          

  cid 

 
 

 

  

Algorithm   SCRMSProp

Input                           Rd
for       to   do
gt    ft   
vt    tvt           gt  cid  gt 
Set        
At   diag vt      
        At
 

 cid      tA 

  gt

 cid 

end for

  where             for         and       

 

Note that the regret bound reduces for       to that of SCAdagrad  For             comparison between the bounds
is not straightforward as the vt   terms cannot be compared 
It is an interesting future research question whether it is
possible to show that one scheme is better than the other
one potentially dependent on the problem characteristics 

  

 cid 

 cid     xt   

  Experiments
The idea of the experiments is to show that the proposed
algorithms are useful for standard learning problems in
both online and batch settings  We are aware of the fact
that in the strongly convex case online to batch conversion is not tight  Hazan   Kale    however that does
not necessarily imply that the algorithms behave generally suboptimal  We compare all algorithms for   strongly
convex problem and present relative suboptimality plots 
  where    is the global optimum  as well
log 
as separate regret plots  where we compare to the best optimal parameter in hindsight for the fraction of training
points seen so far  On the other hand RMSProp was originally developed by  Hinton et al    for usage in deep
 
learning  As discussed before the  xed choice of    is
not allowed if one wants to get the optimal   
    regret
bound in the convex case  Thus we think it is of interest to
the deep learning community  if the insights from the convex optimization case transfer to deep learning  Moreover 
Adagrad and RMSProp are heavily used in deep learning
and thus it is interesting to compare their counterparts SCAdagrad and SCRMSProp developed for the strongly convex case also in deep learning  The supplementary material
contains additional experiments on various neural network
models 
Datasets  We use three datasets where it is easy  dif 
cult and very dif cult to achieve good test performance 
just in order to see if this in uences the performance  For
this purpose we use MNIST   training samples   
classes  CIFAR    training samples    classes 
and CIFAR    training samples    classes  We
refer to  Krizhevsky    for more details on the CIFAR
datasets 
Algorithms  We compare   Stochastic Gradient Descent
 SGD   Bottou    with      decaying stepsize for
the strongly convex problems and for nonconvex problems
we use   constant learning rate    Adam  Kingma   Bai 
    is used with step size decay of       
for strongly
convex problems and for nonconvex problems we use  
constant stepsize    Adagrad  see Algorithm   remains
the same for strongly convex problems and nonconvex

 

The analysis of SCRMSProp is along the lines of SCAdagrad with some overhead due to the structure of vt 
Lemma   Let       
de ned in SCRMSProp  then it holds for all      

         
              
  cid 

 cid    cid vT         

  and At as

  cid 

 cid 

 cid 

 cid     

log

  gt

 cid gt    
  cid 
  cid 
  cid 

  

  

  

  
 

  

 

 

 
 

 
 

 

  

  

                  

tvt          
        log    
inf       jvj          

  this reduces to

  for some

 cid cid 

Note that for       and the choice        
the result of Lemma  
Lemma   Let       
          Then it holds 

  and      

              

  cid 

  

 
 

   
 

 

 cid 
 tAt   cid diag       tdiag   
  cid 
  cid 
  cid 

       
                  

        log    
inf       jvj          

tvt          

  

  

 
 

  

  mini   

and      

  with       

              

Theorem   Let Assumptions       hold and let    be
the sequence generated by SCRMSProp in Algorithm  
where gt    ft    and ft         is an arbitrary  
strongly convex function     Rd
  for some
       
  for some    
  and assume             
      Furthermore  set        
and                             then the regret of SCRMSProp can be upper bounded for       as
 cid    vT         
 cid 
  cid 
 cid 

           tr diag 

 cid          

  cid 

 
 

  

log

 

  

 

 

   

 

          

 

 tvt           

 

 
 

inf
     

  

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

    CIFAR 

    CIFAR 

    MNIST

Figure   Relative Suboptimality vs Number of Epoch for   Regularized Softmax Regression

    CIFAR 

    CIFAR 

    MNIST

Figure   Regret  log scale  vs Dataset Proportion for Online   Regularized Softmax Regression

 

problems    RMSProp as proposed in  Hinton et al   
is used for both strongly convex problems and nonconvex
problems with                 RMSProp  Ours  is
and           
used with stepsize decay of       
   
In order that the parameter range is similar to the original
RMSProp  Hinton et al    we    as       for all
experiment  note that for       RMSProp  Ours  is equivalent to Adagrad    SCRMSProp is used with stepsize
  and       as RMSProp  Ours    SCAdagrad
      
is used with   constant stepsize   The decaying damping factor for both SCAdagrad and SCRMSProp is used
with             for convex problems and we use
            for nonconvex deep learning problems  Finally  the numerical stability parameter   used in
Adagrad  Adam  RMSProp is set to   as it is typically
recommended for these algorithms 
Setup  Note that all methods have only one varying parameter  the stepsize   which we choose from the set of
          for all experiments  By this
setup no method has an advantage just because it has more
hyperparameters over which it can optimize  The optimal
rate is always chosen for each algorithm separately so that
one achieves either best training objective or best test performance after    xed number of epochs 

Strongly Convex Case   Softmax Regression  Given the
training data  xi  yi      and let yi       we      linear model with cross entropy loss and use as regularization
the squared Euclidean norm of the weight parameters  The
objective is then given as

 cid 

  cid 

  

   

 cid   cid 

 cid 

log

  cid 

  

 cid  

xi byi

yi

   
      

  xi bj

        
 

All methods are initialized with zero weights  The regularization parameter was chosen so that one achieves the
best prediction performance on the test set  The results are
shown in Figure   We also conduct experiments in an online setting  where we restrict the number of iterations to
the number of training samples  Here for all the algorithms 
we choose the stepsize resulting in best regret value at the
end  We plot the Regret   in log scale   vs dataset proportion seen  and as expected SCAdagrad and SCRMSProp
outperform all the other methods across all the considered
datasets  Also  RMSProp  Ours  has   lower regret values
than the original RMSProp as shown in Figure  
Convolutional Neural Networks  Here we test    layer
CNN with two convolutional    lters of size       and
one fully connected layer   hidden units followed by
  dropout  The activation function is ReLU and after

 Epoch  Relative Suboptimality  log scale  SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Epoch  Relative Suboptimality  log scale  SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Epoch  Relative Suboptimality  log scale  SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Dataset proportion  Regret  log scale SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Dataset proportion  Regret  log scale SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Dataset proportion  Regret  log scale SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC AdagradVariants of RMSProp and Adagrad with Logarithmic Regret Bounds

    CIFAR 

    CIFAR 

    MNIST

Figure   Test Accuracy vs Number of Epochs for  layer CNN

    Training Objective

    Test Accuracy

Figure   Plots of ResNet  on CIFAR  dataset

the last convolutional layer we use maxpooling over      
  window and   dropout  The  nal layer is   softmax
layer and the  nal objective is crossentropy loss  This is
  pretty simple standard architecture and we use it for all
datasets  The results are shown in Figure   Both RMSProp
 Ours  and SCAdagrad perform better than all the other
methods in terms of test accuracy for CIFAR  dataset  On
both CIFAR  and MNIST datasets SCRMSProp is very
competitive 
Residual Network  We also conduct experiments for
ResNet  network proposed in  He et al      where
the residual blocks are used with modi cations proposed
in  He et al      on CIFAR  dataset  We report the
results in Figures   SCAdagrad  SCRMSProp and RMSProp  Ours  have the best performance in terms of test
Accuracy and RMSProp  Ours  has the best performance
in terms of training objective along with Adagrad 
We also test all the algorithms on   simple  layer Multilayer perceptron which we include in the supplementary material  Given these experiments  we think that SCRMSProp and SCAdagrad are valuable new adaptive gradient techniques for deep learning 

  Conclusion
We have analyzed RMSProp originally proposed in the
deep learning community in the framework of online convex optimization  We show that the conditions for convergence of RMSProp for the convex case are different than
what is used by  Hinton et al    and that this leads
to better performance in practice  We also propose variants SCAdagrad and SCRMSProp which achieve logarithmic regret bounds for the strongly convex case  Moreover  they perform very well for different network models
and datasets and thus they are an interesting alternative to
existing adaptive gradient schemes  In the future we want
to explore why these algorithms perform so well in deep
learning tasks even though they have been designed for the
strongly convex case 

Acknowledgements
We would like to thank Shweta Mahajan and all the reviewers for their insightful comments 

 Epoch  Test Accuracy SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Epoch  Test Accuracy SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Epoch  Test Accuracy SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Epoch  Training Objective SGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC Adagrad Epoch  Test AccuracySGDAdamAdagradRMSPropRMSProp  Ours SCRMSPropSC AdagradVariants of RMSProp and Adagrad with Logarithmic Regret Bounds

Krizhevsky     Learning multiple layers of features from
tiny images  Technical report  University of Toronto 
 

McMahan    Brendan    survey of algorithms and
arXiv preprint

analysis for adaptive online learning 
arXiv   

Ruder     An overview of gradient descent optimization

algorithms  preprint  arXiv     

Schaul     Antonoglou     and Silver     Unit tests for

stochastic optimization  In ICLR   

Schmidhuber     Deep learning in neural networks  An

overview  Neural Networks         

Srivastava  Nitish  Hinton  Geoffrey    Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan  Dropout 
  simple way to prevent neural networks from over tJournal of Machine Learning Research   
ting 
   

Szegedy     Ioffe     and Vanhoucke    

Inceptionv 
inceptionresnet and the impact of residual connections
on learning  In ICLR Workshop     

Szegedy     Vanhoucke     Ioffe     Shlens     and Wojna     Rethinking the inception architecture for computer vision  In CVPR     

Zeiler        ADADELTA  An adaptive learning rate

method  preprint  arXiv     

Zinkevich     Online convex programming and general 

ized in nitesimal gradient ascent  In ICML   

References
Bottou     Largescale machine learning with stochastic
gradient descent  In Proceedings of COMPSTAT 
pp    Springer   

Daniel     Taylor     and Nowozin     Learning step size
controllers for robust neural network training  In AAAI 
 

Dauphin     de Vries     and Bengio     Equilibrated
adaptive learning rates for nonconvex optimization  In
NIPS   

Duchi     Hazan     and Singer     Adaptive subgradient
methods for online learning and stochastic optimization 
Journal of Machine Learning Research   
 

Hazan    

Introduction to online convex optimization 
Foundations and Trends in Optimization   
 

Hazan     and Kale     Beyond the regret minimization barrier  optimal algorithms for stochastic stronglyconvex
optimization  Journal of Machine Learning Research 
   

Hazan     Agarwal     and Kale     Logarithmic regret algorithms for online convex optimization  Machine
Learning     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition  In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp       

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Identity mappings in deep residual networks  In
European Conference on Computer Vision  pp   
Springer     

Hinton     Srivastava     and Swersky     Lecture   
    separate  adaptive learning rate for each connection 
Slides of Lecture Neural Networks for Machine Learning   

Karpathy     and FeiFei     Deep visualsemantic alignIn CVPR 

ments for generating image descriptions 
 

Kingma        and Bai        Adam    method for stochastic

optimization  ICLR   

Koushik  Jayanth and Hayashi  Hiroaki 

stochastic gradient descent with feedback 
preprint arXiv   

Improving
arXiv

