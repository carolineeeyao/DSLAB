Deep Spectral Clustering Learning

Marc    Law   Raquel Urtasun   Richard    Zemel    

Abstract

Clustering is the task of grouping   set of examples so that similar examples are grouped into
the same cluster while dissimilar examples are
in different clusters  The quality of   clustering depends on two problemdependent factors
which are    the chosen similarity metric and
ii  the data representation  Supervised clustering approaches  which exploit labeled partitioned
datasets have thus been proposed  for instance to
learn   metric optimized to perform clustering 
However  most of these approaches assume that
the representation of the data is  xed and then
learn an appropriate linear transformation  Some
deep supervised clustering learning approaches
have also been proposed  However  they rely on
iterative methods to compute gradients resulting
in high algorithmic complexity 
In this paper 
we propose   deep supervised clustering metric learning method that formulates   novel loss
function  We derive   closedform expression
for the gradient that is ef cient to compute  the
complexity to compute the gradient is linear in
the size of the training minibatch and quadratic
in the representation dimensionality  We further
reveal how our approach can be seen as learning spectral clustering  Experiments on standard
realworld datasets con rm stateof theart Recall   performance 

  Introduction
Clustering is   widely used technique with applications in
machine learning  statistics  speech processing  computer
vision 
It consists in grouping   set of examples so that
 similar  examples are in the same cluster while  dissimilar  examples are in different clusters  In most applications 
the vector representation of examples is given as input and

 Department of Computer Science  University of Toronto 
Toronto  Canada  CIFAR Senior Fellow  Correspondence to 
Marc    Law  law cs toronto edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Our approach learns   nonlinear embedding function in
  supervised way so that elements in the same category  here with
the same color  are organized into the same cluster 

  key step is then to determine an appropriate similarity
metric so that similar and dissimilar objects can be easily
identi ed  In some cases  experts with domain knowledge
may help determine an appropriate distance metric  However  in highdimensional problems  determining an effective metric becomes increasingly dif cult even for an expert  and standard metrics such as the Euclidean distance
can lead to very poor results 
Many approaches that learn an appropriate similarity metric  Xing et al    Lajugie et al    Law et al 
  or   nonlinear embedding function  Schroff et al 
  Sohn    Song et al    in   supervised way
have thus been proposed  They assume the availability
of   training dataset that  shares the same metric  as the
test dataset that they want to partition       the datasets
represent the same concepts such as birds species or car
models  As both datasets share the same metric  the
model learned from training examples is expected to correctly compare test examples  The approaches can roughly
be divided into four groups based on two criteria  semisupervised supervised setting and shallow deep architecture 
In the semisupervised setting  Xing et al   
Chopra et al    the training data is given as   small set
of example pairs that are expected to be in the same or different clusters  On the other hand  supervised approaches
 Lajugie et al    Law et al    assume the availability of labeled datasets for which the ground truth partitions are provided during training    model is then learned
so that some clustering algorithm will produce   partition
similar to the ground truth partition on the training dataset 
The supervised clustering setting can be seen as the spe 

cluster centerRepresentationLearningInitialRepresentationof ExamplesLearned Representationneighborhood of centerDeep Spectral Clustering Learning

cial case of semisupervised setting where all the pairwise
similarity relations between training examples are given  In
particular  supervised clustering is   speci   classi cation
problem where the model is learned so that the representations of training examples are closer to the representative
vector of their category than to the representative vector of
any other category  In this paper  we propose   novel metric
learning approach whose rationale is illustrated in Fig   
In particular  we leverage deep nonlinear and hierarchical
architectures to learn complex representations that better
re ect similarity relations among examples 
Many deep metric learning approaches  Schroff et al 
  Song et al    extend the ideas introduced in the
shallow metric learning literature  Xing et al    Weinberger et al    to learn deep neural networks on large
datasets  The main dif culty is how to implement these
ideas so that they are scalable and avoid memory bottleneck  For instance  many approaches have proposed hard
negative mining strategies to limit the number of training
constraints    deep supervised clustering approach was
proposed in  Song et al    to compute its gradient 
the approach uses an iterative greedy algorithm whose algorithmic complexity is high 
Contributions  In this paper  we propose   metric learning
framework that optimizes an embedding function so that
the learned representations of similar examples are grouped
into the same cluster  and dissimilar examples are in different clusters  To this end  we relax the problem of partitioning   dataset with Bregman divergences  Banerjee et al 
  and we formulate our problem so that the gradient
is ef cient to compute  In particular  the gradient can be
expressed in closedform  and the algorithmic complexity
to compute it is linear in the size of the training minibatch
and quadratic in the representation dimensionality  which
is better than the complexity of existing iterative methods 
Our method is also simple to implement and obtains stateof theart performance on standard datasets 

  Preliminaries
In this section  we provide some technical background
about clustering  and set up the notation throughout  We reformulate in matrix form the problem of clustering   set of
examples with Bregman divergences  Banerjee et al   
and write it as an optimization problem        one variable
in Eq   
Notation  We note  cid      cid    tr AB cid  the Frobenius
inner product where   and   are realvalued matrices  and
vector of all ones with appropriate dimensionality     is the
MoorePenrose pseudoinverse of    ri    is the relative
interior of the set   

 cid   cid     cid tr AA cid  the Frobenius norm of      is the

Data  We consider that we are given   set of   examples
     fn     which may be represented as   single matrix           fn cid         In the following  we consider that     Rd  and thus       Rn   
Bregman divergence  Any Bregman divergence         
ri          is de ned as                      
 cid         cid  where           is   continuouslydifferentiable and strictly convex function        Rd
represents the gradient vector of   at    The most commonly used Bregman divergences for clustering are the
squared Euclidean distance de ned with        cid   cid 
  the
KLdivergence  Dhillon et al    or the ItakuraSaito
distance  Buzo et al    We refer the reader to  Banerjee et al    Nielsen   Nock    for details 
Clustering  Partitioning the   observations in    
      fn cid        into   clusters is equivalent to determining an assignment matrix             such that
 Yic     if fi is assigned to cluster   and   otherwise  In this
paper  we assume that each example is assigned to one and
only one cluster       the sum of each row of    is   and that
there is no empty cluster  which corresponds to adding the
constraint rank            Therefore     is in the following
set of assignment matrices 

                                 rank           

We assume as in  Banerjee et al    that each cluster  
is represented by   single representative vector zc   ri   
and that an example fi is assigned to cluster   only if
    cid       fi  zc      fi  zb  where    is the chosen
Bregman divergence  To simplify the notations  we consider that zc      and we concatenate the   representative
vectors into   single matrix           zk cid         The
problem of partitioning the   examples in         with   
can then be formulated as minimizing the energy function 

  cid 

  cid 

 Yic     fi  zc 

 

min

               

  

  

 

min

               

 cid    cid       cid               cid 

where we have used the de nition of Bregman divergences
and we note             an cid              
       an cid    Rn is the concatenation into  
single vector of the different  ai       and      
     an cid    Rn   is the concatenation into
  single matrix of the different gradients  ai    Rd 
Banerjee et al    demonstrated that  for any value of
            the minimizer of zc in Eq    is unique and
is the mean vector of all the examples in   assigned to
cluster   if and only if    is   Bregman divergence 
In
matrix form  this means that           cid           cid          

Deep Spectral Clustering Learning

is the unique global minimizer of   in Eq    We then
de ne the set                             and rewrite Eq   
as   minimization problem        one variable 

 cid        cid   CF      cid      CF   CF  cid   

min
    

As an illustration 
if    is the squared Euclidean distance  then Eq    reduces to   cid   cid     cid   CF cid     cid    
 CF     CF cid     cid   cid   cid   CF cid     cid     CF cid     cid      CF cid 
We then obtain the formulation mentioned in  Lajugie
et al   Section   of the usual kmeans algorithm 

 cid      CF cid    max
    

min
    

 cid           cid cid 

 

by using the fact that all the matrices in   are orthogonal
projection matrices       symmetric and idempotent  we
then have          cid   CF cid    tr           cid    tr   CF    cid 

  Deep Spectral Clustering Learning
In this section  we introduce our method that we call Deep
Spectral Clustering Learning  DSCL  We  rst relax the
clustering problem in Eq    and consider the set of solutions of the relaxed problem as the prediction function of
our model  Then  we present our large margin supervised
clustering problem and its ef cient solver  Finally  we explain the connection of our approach with spectral clustering learning 

  Constraint Relaxation
Optimizing Eq    is   NPhard problem  Aloise et al 
  we then approximate it  Following  Shi   Malik 
  Zha et al    Ng et al    Peng   Wei   
we now present   spectral relaxation of this problem  In
particular  we extend the domain of Eq    so that the set
of solutions of the resulting problem can be formulated in
  convenient way 
We propose to relax the constraint        in Eq    with
the constraint      Cn   where the set Cn   includes   and
is de ned as the set of       rankk orthogonal projection matrices Cn            Rn                   cid   
    rank           The set of solutions of the resulting
relaxed version of Eq    can then be formulated 

         arg max
   Cn  

 cid   CF      cid      CF   CF  cid   

where the terms that do not depend on    are omitted 
The solutions in Eq    can be found in closedform  Let us
note     rank     we show in the supplementary material
that if        then the set of solutions of Eq    is         
       Cn      CF               Cn                 

     cid       cid    Cn           cid       In particular  if
       then      cid      and we have                
In the following  we consider only the case where the dimensionality   of training examples is not greater than   so
that the property       is satis ed  Nonetheless  if       
the set of solutions can be considered as                

  Structured output prediction
In the following  we consider that we are given   examples xi             images  and an embedding function
          whose set of parameters is   and such that
             xi    fi       fi can be the output of
  convolutional neural network  All these representations
are concatenated into   single matrix           fn cid   
     We are also given   ground truth assignment matrix
          which indicates the desired partition for the  
examples  In other words  let               be the ground
truth clustering matrix of     we would like to learn the embedding function   so that the matrix predicted with the
 relaxed  clustering problem          Cn   in Eq    is as
close as possible to the ground truth clustering matrix   
Different evaluation metrics such as purity  rand index and
normalized mutual information exist to evaluate clustering  see  Manning et al     Chapter   for details 
As in  Hubert   Arabie    Bach   Jordan    Lajugie et al    Law et al    we choose the Frobenius norm which has many advantages  As explained in
 Lajugie et al   Section   unlike rand index  the
Frobenius norm between clustering  orthogonal projection 
matrices takes into account the size of the different clusters
      its value is not dominated by the performance on the
largest clusters  and thus optimizes intraclass variance that
is   rescaled indicator 

  PROBLEM FORMULATION

In the following  we consider that the matrix   is   variable  that depends on   The problem of minimizing the
discrepancy between the ground truth clustering       of
the matrix   and the  relaxed  clustering    predicted with
       in Eq    can be formulated as the following empirical risk minimization problem 

min
     

max
         

 cid        cid 

 

As all the matrices in        have the same rank  we can
rewrite Eq     cid        cid     cid   cid     cid     cid      tr      
where  cid     cid    tr        rank          is   constant 
Eq    is then equivalent to the problem 

max
     

min
         

tr      

 

Deep Spectral Clustering Learning

Algorithm   Deep Spectral Clustering Learning  DSCL 
input   Set of training examples       images  in     embedding function   number of iterations   learning rates      
  for iteration       to   do
 
 
 
 
  end for

Randomly sample   training examples      xn    
Create representation matrix           fn cid                         fi    xi 
Create rescaled gradient                  cid   see Eq    where           is the desired partition matrix of the   examples
Update the set of parameters   of   by exploiting the rescaled gradient   and perform gradient step with learning rate   

which is naturally lower bounded by  see details in supplementary material 

max
     

min
         

tr    CF       max
     

tr CF    

 

where we use the fact that                CF            
We note that Eq    equals Eq    if rank         
The dif culty of optimizing the problem in Eq    is that it
depends on both   and     If we assume that rank     is  
constant  with   not necessarily full rank  in Eq    then
the problem is differentiable  Golub   Pereyra    and
the gradient of Eq             is 

                    cid 

 

where   is the identity matrix  Details on the gradient can
be found in the supplementary material  Our matrix   is
always full rank in our experiments  so the constant rank
condition along the iterations is satis ed 

  LOW ALGORITHMIC COMPLEXITY

We now show that in addition to having   closedform expression  computing our gradient    is ef cient  the complexity to compute it is linear in   and quadratic in   
We note               the ground truth partition matrix
where the matrix           is given  let yc be the cth
column of     then the cth column of     cid  can be written
    yc  The complexity to compute     cid  is linear
max   cid 
in   due to the sparsity of         cid  is also sparse       it
contains   nonzero elements  We can then write   
  as 

 

 cid              cid         cid cid 

  
 

   

 
where   indicates      matrices which are computed ef 
ciently due to the sparsity of     The complexity to compute
    is   nd min               nd  as we assume       

  DIRECT LOSS MINIMIZATION

Our method computes in closedform the gradient of the
structured output prediction problem in Eq    when
rank          and its algorithmic complexity is low 

 In our experiments  since the number of rows of   is greater

than its number of colums    has full column rank 

To the best of our knowledge  although we exploit classic spectral relaxation results  our method is the  rst approach that includes the closedform solution of the relaxed
kmeans problem  see Section   within   large margin
method for structured output  Even Mahalanobis metric
learning methods  Lajugie et al    cannot use such  
simpli cation due to the nature of their model  and the formulation of their subgradient is then different 
Including the
relaxed
kmeans algorithm results in   simple problem formulation  see Eq    The resulting large margin optimization
problem is easy to optimize as we do not have to perform
lossaugmented inference during training 

closedform solution of

the

  Learning deep models
Our method can be used to learn neural networks with conventional gradientbased methods by exploiting chain rule 
Our approach is illustrated in Algorithm  
In detail  we note   minibatch matrix           fn cid   
    the concatenation into   single matrix of the   different embedding representations  xi    fi  where  for instance            is   neural network embedding function and xi     is an image  We can rewrite Eq    as
  minimization problem by de ning our loss function as  
function of the minibatch representation matrix 

Loss           tr CF    

 
which is nonnegative and where               is the
ground truth clustering matrix of     The neural network  
is then learned via backpropagation as illustrated in Algorithm         using stochastic gradient descent with rescaled
gradient    where   is de ned in Eq   
We note that the structure of the learned neural network
is not limited to the case where   is Rd  For instance 
if the goal is to partition probability distributions with
KLdivergence  the set   can be constrained to be the ddimensional simplex               cid      by using  
softmax regression at the last layer of the neural network 
Regression problem  It is worth noting that    is also the
gradient          of the nonlinear least squares problem 

max
     

   
 

 cid           cid 

 

Deep Spectral Clustering Learning

where we assume that the rank of   is constant  otherwise 
the problem is not differentiable  Our solver can then be
seen as   gradientbased solver for the nonlinear regression
problem that iteratively decreases the distance  cid         cid 
In particular  the spectral relaxation proposed in Section  
allows to write the set of predicted clusterings        as  
function of       and the choice of the Frobenius norm to
compare clusterings makes our problem similar to   least
squares problem 
Given the least squares formulation of Eq    one can see
that our problem focuses more on the similarity between
the matrices   and       than between   and the individual matrix     Our problem can then be seen as   spectral
clustering algorithm as explained in the following 

  Connection with spectral clustering
We now explain how our proposed method can be seen as
learning spectral clustering in   supervised way 
As explained in  Bach   Jordan    Von Luxburg 
  spectral clustering does not refer to one particular method but to   family of methods        Shi   Malik    that partition   dataset by exploiting the leading
eigenvectors of   similarity matrix  They rely on the eigenstructure of   similarity matrix to partition examples into
disjoint clusters  with examples in the same cluster having
high similarity and examples in different clusters having
low similarity  In our case  as can be seen in Eq    the
 kernel  similarity matrix is                 cid  where
    Rn   is   matrix whose columns are the leftsingular
vectors of   corresponding to its nonzero singular values
and where     rank     By de nition  these leftsingular
vectors of   are also the   leading eigenvectors of   
It is worth noting that our approach is different from classic
spectral clustering approaches that perform clustering by
exploiting some Laplacian matrix of the similarity matrix 
Our approach directly performs clustering by exploiting the
leading eigenvectors of the similarity matrix   
By increasing the value tr CF       tr CU   cid  at
each backpropagation iteration  the leading eigenvectors of
    cid      become more similar to the leading eigenvectors of    As   and         cid  are both orthogonal
projection matrices  in the ideal case  tr CU   cid  is maximized when the column space of one of the two matrices   or      cid  is included in the column space of the
In particular  if rank      rank     cid 
other matrix 
then tr CU   cid  is maximized iff         cid   Fan   
In this ideal case  we have arg max      cid          cid cid       
which corresponds to the solution of Eq    when replacing   by    in other words  partitioning the rows of   with
kmeans will return the desired clustering matrix   
It is then clear that comparing the similarity between the

rows of         using spectral clustering  is at least as relevant as comparing the rows of   to partition the dataset 
In our experiments  our method obtains better performance
when the leftsingular vectors of the test set matrix are used
for partitioning rather than the learned features 

  Experiments
The goal of metric learning is to learn   metric that can
be used to compare new examples  possibly from categories that were not in the training dataset  In this context 
the model is learned on   training dataset and tested on  
dataset that shares the same  semantical  metric and represents similar concepts  This allows to evaluate the generalization ability of the model  Following the experimental
protocol described in  Song et al      we evaluate
our method on the following  negrained datasets and use
the exact same train test splits 
  The CaltechUCSD Birds  CUB  dataset  Wah
et al    is composed of   images of birds from
  different species categories  We split the  rst   categories for training   images  and the rest for test
  images 
  The CARS  dataset  Krause et al    is composed
of   images of cars from   model categories  The
 rst   categories are used for training   images  the
rest for test   images 
  The Stanford Online Product  Song et al    dataset is
composed of   images from   online product
categories  It is partitioned into   images from  
categories for training and   images from   categories for test 
In these experiments  the categories for training and the categories for testing are disjoint although they belong to the
same context       they all represent birds  cars or products 
This makes the problem challenging as deep models may
over   on the training categories  In the same way as the
baselines  we then perform early stopping 

  Implementation details
We closely follow the experimental setup described in
 Song et al      In particular  we implemented
our method with the Tensor ow package  Abadi et al 
  and used the Inception  Szegedy et al    network with batch normalization  Ioffe   Szegedy   
pretrained on ImageNet ILSVRC  CLS  Russakovsky
et al    we  netuned the network on the training
datasets  We perform two types of  netuning 

 For instance  our model obtains more than   performance
for all the evaluation metrics on the training categories after  
iterations 

Deep Spectral Clustering Learning

Algorithm   DSCL Normalized Spectral Clustering
input   Test set Ft     nt  nt is the number of test examples
  Create Mt   Rnt   by mean centering Ft
  Create     rank Mt 
  Create Ut         unt  cid    Rnt        MtM
  Create           tnt  cid    Rnt            ti    cid ui cid 
  partition the rows of   into   clusters with kmeans

 
    UtU cid 
 
ui

  endto end  we  netune the model and update the parameters of all the layers of the neural network during backpropagation  In this case  we perform   iterations of gradient descent 
  last layer  we freeze all the parameters of the neural
network  pretrained on ImageNet  except those in the last
layer  Only the parameters in the last layer of the model are
updated  We perform   iterations of gradient descent 
In both cases  we remove the softmax function at the end
of the last layer 
The images are  rst resized to square size       and
cropped at       For the dataset augmentation  we
use   random horizontal mirroring for training and   single
center crop for test  As in  Song et al    and unlike
 Sohn    we use   single crop per image 
We ran our experiments on   single Tesla    GPU with
 GB RAM  and used   standard Stochastic Gradient optimizer  Our batch size is set to                
our method backpropagates the loss in Eq    for all
the examples in the batch  As Inception is   large model
and to    into memory  we iteratively compute submatrices Fi       and concatenate them into   single matrix
   cid         Our method then computes
        cid 
   cid    Rn   de ned
 cid  
the gradient matrix        cid 
in Eq    and minimizes the loss function  cid     cid   
  cid Fi Gi cid  with gradient descent where    is  xed 
 
Different
        are tested in  Song et al   
it is reported that   does not play   crucial role 
In our
case  we then set our dimensionality   to be equal to the
number of categories   for the Birds and Cars datasets 
For the Products dataset which contains more than    categories  we observed as in  Sohn    that the larger the
value of    the better the results  We then set       in
order to be fair with the other models and randomly subsample   training categories 

         cid 

        cid 

dimensionality

embedding

values  

  Partitioning   test dataset
Partitioning   test dataset Xt         xnt cid      nt
 where xi     is an image in these experiments  and nt is
the number of test examples  is done by  rst computing the
test representation matrix Ft          xnt cid   

  nt where   is the learned embedding function  and then
applying   partition algorithm  The partition algorithm can
either be   standard clustering algorithm as described in
 Banerjee et al    or can exploit the connection of our
method with spectral clustering as explained in Section  
The spectral clustering  SC  algorithm that we use  which is
inspired by  Ng et al    is illustrated in Algorithm  
we  rst mean center Ft  the mean of each column of the
resulting matrix Mt is zero  then we extract the matrix Ut
that contains the leading leftsingular vectors of the resulting matrix Mt  the rows of Ut are then  cid normalized and
partitioned with the usual kmeans algorithm 
To test our method without spectral clustering  we  cid 
normalize the output representations as done in  Song et al 
  and then apply the usual kmeans algorithm with the
squared Euclidean distance 

the

of

also

report

  Quantitative results
We compare our method to current stateof theart metric
learning approaches in Tables   to   by evaluating the Normalized Mutual Information  NMI  and Recall   performances  In particular  Recall  is   useful metric in zeroshot learning contexts  it allows to assign the category of
  test image to the category of its nearest neighbor  it then
evaluates the generalization performance of   model to new
similar concepts  The scores for the following baselines
 Schroff et al    Song et al    Sohn    Song
et al    are reported from  Song et al    where the
methods are tested in   similar setup  As explained in the
related work  Section   the NMIbased approach  Song
et al    is the only baseline that is explicitly learned to
optimize   clustering criterion 
We
vanilla
GoogLeNet Inception features pretrained on ImageNet 
and GoogLeNet features  netuned for classi cation with
softmax regression 
In both cases  we report the results
obtained with Spectral Clustering  SC  as illustrated in
Algorithm   and without SC  as explained in the last
paragraph of Section   We report
the scores for
 cid normalized features as they obtain better performance
than unnormalized features in our experiments 
Recognition performance  Our spectral clustering approach obtains stateof theart Recall   performance on
all the datasets  Only the method in  Song et al   
obtains better NMI performance on Birds and Products 
this can be expected as the model in  Song et al    is
speci cally learned to optimize the NMI evaluation metric 
Our choice to optimize the Frobenius norm which allows
to compute   gradient in closedform then seems to be  
good tradeoff for scalability as it obtains competitive NMI
results and stateof theart Recall   performance 

performance

the

Deep Spectral Clustering Learning

Table   NMI and Recall   evaluation on the Birds  CUB 
  dataset

Method

Triplet       Schroff et al   
Lifted struct  Song et al   
Npairs  Sohn   
NMIbased  Song et al   

Vanilla GoogLeNet  with SC 
Vanilla GoogLeNet  without SC 
Softmax regression  with SC 
Softmax regression  without SC 

Ours  endto end with SC 
Ours  endto end without SC 
Ours  last layer with SC 
Ours  last layer without SC 

NMI

 
 
 
 
 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 

 
 
 
 

Table   NMI and Recall   evaluation on the Cars  dataset

Method

Triplet       Schroff et al   
Lifted struct  Song et al   
Npairs  Sohn   
NMIbased  Song et al   

Vanilla GoogLeNet  with SC 
Vanilla GoogLeNet  without SC 
Softmax regression  with SC 
Softmax regression  without SC 

Ours  endto end with SC 
Ours  endto end without SC 
Ours  last layer with SC 
Ours  last layer without SC 

NMI

 
 
 
 

 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 

 
 
 
 

Table   NMI and Recall   evaluation on the Products  Stanford Online Products  dataset

Method

Triplet       Schroff et al   
Lifted struct  Song et al   
Npairs  Sohn   
NMIbased  Song et al   

Vanilla GoogLeNet  with SC 
Vanilla GoogLeNet  without SC 
Softmax regression  with SC 
Softmax regression  without SC 

Ours  endto end with SC 
Ours  endto end without SC 
Ours  last layer with SC 
Ours  last layer without SC 

NMI

 
 
 
 
 
 
 
 

 
 
 
 

  

 
 
 
 

 
 
 
 
 
 
 
 

  

  

 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

The usual kmeans algorithm applied on the representations of our learned model obtains worse results than our
proposed spectral clustering  SC  approach  It obtains better performance than most baselines on Birds and Cars  but
also poor results on the Products dataset  This may be explained by the fact that there are only   images per category in this dataset  which is   times less than for the
other datasets  Some categories also contain only   images 
which is hard to generalize on  Our approach is then more
appropriate when the categories are large       more than  
images  than when there are many  very  small clusters 

We also note that when our model updates only the last
layer during  netuning  it obtains stateof theart performance on Birds and Cars  but not as good as our fully
learned model on the Products dataset  The strong results
on the former two is likely due to their small size  fewer
than    training images  which makes learning all the
layers prone to over tting  Learning in all layers seems
bene cial on larger datasets such as Products 
We observe that most baselines  Schroff et al    Sohn 
  Song et al    and our spectral method obtain
comparable results on the Products dataset  There is then
not   clear way to learn deep models in contexts with small
categories  On the other hand  there is   huge gap in
Recall   performance on the other datasets between approaches that optimize clustering and approaches that do
not  The largest performance gap is observed on the Cars
dataset which contains the largest number of images per
category  more than  
It is also worth noting that unlike usual softmax regression
for classi cation that promotes centroids to be onehot vectors in the ideal case  our approach takes as input the current representations and tries to group similar examples together without  xing the desired centroids  It can then be
easily combined with other approaches 
Training time  Once the matrix representation of the minibatch         has been computed  computing the gradient
  takes   second  with       and       Since
we backpropagate our loss for all the examples in the minibatch  each iteration takes about   seconds due to the large
architecture of Inception  Nevertheless  multiple GPUs can
be used in parallel to speed up training 
Qualitative results  tSNE  Van Der Maaten    plots
are available in the supplementary material 

  Related work
As explained in Section   many approaches have been proposed to learn   similarity metric  Xing et al    BarHillel et al    Lajugie et al    Law et al   
    or   nonlinear embedding function  Schroff et al 
  Song et al    optimized to perform clustering 
However  most of them belong to the semisupervised setting  Xing et al    BarHillel et al    Schroff et al 
       they are designed to learn from small sets of
pairwise or tripletwise relations and do not account for
the global clustering performance on the training dataset 
In pairwise approaches  the model is given pairs of examples  xi  xj  which are either similar       the examples are
in the same category  or dissimilar       the examples are
in different categories  the model is then learned so that
the distances between representations of similar objects
are smaller than the distances between dissimilar objects 

Deep Spectral Clustering Learning

In the tripletwise approach  Schultz   Joachims   
      
Weinberger et al    triplets of examples  xi    
   
are provided and the model is learned so that the distance
between the representations of the pair  xi    
    is smaller
than for the pair  xi    
    We focus on the supervised clustering setting which considers all the possible similar and
dissimilar pairs and takes into account the global clustering
structure of the training dataset  here   minibatch 
In the shallow metric learning literature   Lajugie et al 
  Law et al    learn   linear transformation in the
supervised clustering setting so that the partition obtained
when using kmeans on   dataset is as close as possible to
the desired partition  However  they both consider that the
data representation is  xed and are limited by the complexity of their linear model  Moreover  their solvers are very
different from ours  Lajugie et al    propose an extension of the structural SVM  Tsochantaridis et al   
for Mahalanobis distances  and their projected subgradient
method thus has high complexity  Law et al    propose   closedform solver but the method is limited to the
case where there is   single training dataset and they do not
provide gradientbased strategies to optimize other types of
models such as neural networks 
In the deep learning literature  Song et al    proposed
to approximate   loss function that considers all the positive
and negative pairs  To this end  they iteratively randomly
sample   few similar pairs  and then actively add their dif 
cult neighbors to the training minibatch  This idea is similar to the idea of active set of constraints used in  Weinberger   Saul    Law et al      to limit the number of active constraints       the number of constraints that
have nonzero subgradients  and be able to optimize over
large numbers of triplets  Although their approach considers most of the similarity relations  it is not optimized to
group all the similar examples into the same unique cluster 
Indeed  each category can be divided in multiple subclusters as explained in  Song et al   
Sohn   proposed to tackle the problem of slow convergence of tripletwise approaches  caused by hard negative mining  by optimizing losses over       tuplets  In
particular  an ef cient batch construction method is proposed to require    examples instead of the na ve        
to build   tuplets of length        The loss function recruits multiple distances between dissimilar examples from different categories and approximates the ideal
loss that minimizes the distances between similar examples
while maximizing the distances between dissimilar examples  This approach considers the global structure of the
representation of minibatches better than tripletwise approaches  However  although it does take into account most
distances in the minibatch  it does not explicitly optimize
the model so that it obtains good clustering performance 

In the deep metric learning literature  the most similar approach to ours is  Song et al    They select for
each category one unique example that will be the medoid
      representative example  The choice of the medoids
is not discussed and may be problematic if there is noise
in the labels  In contrast  our centroids are learned so that
they are the mean vectors of the training examples  Indeed 
our set of centroids is written           where         is
the representation of our minibatch and    is implicitly included in the formulation of the set Cn    Our optimization
problem then learns   data representation such that training
examples are projected close to their respective centroids 
Moreover  the gradient in  Song et al    requires an iterative greedy algorithm and each iteration of their greedy
algorithm has higher complexity than the complexity of
computing our gradient  we set the dimensionality   of
our learned representations to be    Each iteration of the
greedy algorithm is linear in the size of the minibatch and
cubic in the number of categories in the minibatch 
Deep learning was also used in  Ionescu et al    to
learn   convolutional neural network optimized for clustering  However  it is applied to unsupervised image segmentation with normalized cuts  Shi   Malik    We are
interested in this paper in the supervised clustering setting
where the ground truth partition is provided  Another deep
clustering approach was proposed in  Hershey et al   
However  their model is not optimized to be robust to the
size of the different clusters as discussed in Section  

  Conclusion
We have presented   novel deep learning approach optimized for the supervised clustering task  Our method is
simple to implement and scalable thanks to its low algorithmic complexity  It also obtains stateof theart recall  
performance on different standard  negrained datasets 
Future work includes improving our proposed solver by exploiting the results in  Ionescu et al    which take into
account the structure of the neural network instead of using
  standard stochastic gradient descent solver 

Acknowledgments  We thank David Duvenaud  Stavros
Tsogkas  Dimitris Vlitas and the anonymous reviewers for their
helpful comments  This work was supported by Samsung and the
Intelligence Advanced Research Projects Activity  IARPA  via
Department of Interior Interior Business Center  DoI IBC  contract number   PC  The      Government is authorized
to reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon 
Disclaimer  The views and conclusions contained herein are
those of the authors and should not be interpreted as necessarily representing the of cial policies or endorsements  either expressed or implied  of IARPA  DoI IBC  or the      Government 

Deep Spectral Clustering Learning

References
Abadi  Mart    Agarwal  Ashish  Barham  Paul  Brevdo 
Eugene  Chen  Zhifeng  Citro  Craig  Corrado  Greg   
Davis  Andy  Dean  Jeffrey  Devin  Matthieu  et al  Tensor ow  Largescale machine learning on heterogeneous
distributed systems  arXiv preprint arXiv 
 

Aloise  Daniel  Deshpande  Amit  Hansen  Pierre  and
Popat  Preyas  Nphardness of euclidean sumof squares
clustering  Machine learning     

Bach  Francis and Jordan  Michael  Learning spectral clus 

tering  NIPS     

Banerjee  Arindam  Merugu  Srujana  Dhillon  Inderjit   
and Ghosh  Joydeep  Clustering with bregman divergences  Journal of machine learning research   Oct 
   

BarHillel  Aharon  Hertz  Tomer  Shental  Noam  and
Weinshall  Daphna 
Learning   Mahalanobis metric from equivalence constraints  Journal of Machine
Learning Research     

Buzo  Andr es  Gray     Gray     and Markel  John  Speech
coding based upon vector quantization  IEEE Transactions on Acoustics  Speech  and Signal Processing   
   

Chopra  Sumit  Hadsell  Raia  and LeCun  Yann  Learning
  similarity metric discriminatively  with application to
face veri cation  In Computer Vision and Pattern Recognition    CVPR   IEEE Computer Society Conference on  volume   pp    IEEE   

Dhillon  Inderjit    Mallela  Subramanyam  and Kumar 
Rahul    divisive informationtheoretic feature clustering algorithm for text classi cation  Journal of machine
learning research   Mar   

Fan  Ky  On   theorem of weyl concerning eigenvalues
of linear transformations    Proceedings of the National
Academy of Sciences of the United States of America   
   

Golub  Gene   and Pereyra  Victor  The differentiation
of pseudoinverses and nonlinear least squares problems
whose variables separate  SIAM Journal on numerical
analysis     

Hershey  John    Chen  Zhuo  Le Roux  Jonathan  and
Watanabe  Shinji  Deep clustering  Discriminative embeddings for segmentation and separation  In Acoustics 
Speech and Signal Processing  ICASSP    IEEE International Conference on  pp    IEEE   

Hubert  Lawrence and Arabie  Phipps  Comparing parti 

tions  Journal of classi cation     

Ioffe  Sergey and Szegedy  Christian  Batch normalization 
Accelerating deep network training by reducing internal
covariate shift  arXiv preprint arXiv   

Ionescu  Catalin  Vantzos  Orestis  and Sminchisescu  Cristian  Matrix backpropagation for deep networks with
structured layers  In Proceedings of the IEEE International Conference on Computer Vision  pp   
 

Krause  Jonathan  Stark  Michael  Deng  Jia  and FeiFei 
Li     object representations for  negrained categorizaIn Proceedings of the IEEE International Contion 
ference on Computer Vision Workshops  pp   
 

Lajugie     Bach     and Arlot     Large margin metric
learning for constrained partitioning problems  Proc  International Conference on Machine Learning   

Law  Marc Teva  Yu  Yaoliang  Cord  Matthieu  and Xing 
Eric Poe  Closedform training of mahalanobis distance
for supervised clustering  In CVPR  IEEE   

Law  Marc Teva  Thome  Nicolas  and Cord  Matthieu 
Learning   distance metric from relative comparIJCV   
isons between quadruplets of images 
     
doi 
 
   URL http dx doi org 
   

ISSN  

Law  Marc Teva  Yu  Yaoliang  Urtasun  Raquel  Zemel 
Richard  and Xing  Eric Poe  Ef cient multiple instance
metric learning using weakly supervised data  In Computer Vision and Pattern Recognition  CVPR     

Manning  Christopher    Raghavan  Prabhakar  Sch utze 
Hinrich  et al  Introduction to information retrieval  volume   Cambridge university press Cambridge   

Ng  Andrew    Jordan  Michael    Weiss  Yair  et al  On
spectral clustering  Analysis and an algorithm  In NIPS 
volume   pp     

Nielsen  Frank and Nock  Richard  Sided and symmetrized
IEEE transactions on Information

bregman centroids 
Theory     

Peng     and Wei     Approximating kmeans type clustering via semide nite programming  SIAM Journal on
Optimization     

Russakovsky  Olga  Deng  Jia  Su  Hao  Krause  Jonathan 
Satheesh  Sanjeev  Ma  Sean  Huang  Zhiheng  Karpathy  Andrej  Khosla  Aditya  Bernstein  Michael  et al 

Deep Spectral Clustering Learning

Imagenet large scale visual recognition challenge  International Journal of Computer Vision   
 

Schroff  Florian  Kalenichenko  Dmitry  and Philbin 
James  Facenet    uni ed embedding for face recognition and clustering  In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp 
   

Weinberger  Kilian    Blitzer  John  and Saul  Lawrence 
Distance metric learning for large margin nearest neighbor classi cation  Advances in neural information processing systems     

Xing  Eric    Jordan  Michael    Russell  Stuart  and Ng 
Andrew    Distance metric learning with application to
clustering with sideinformation  In NIPS  pp   
 

Schultz  Matthew and Joachims  Thorsten  Learning   disIn NIPS  vol 

tance metric from relative comparisons 
ume   pp     

Zha  Hongyuan  He  Xiaofeng  Ding  Chris  Gu  Ming  and
Simon  Horst    Spectral relaxation for kmeans clustering  In NIPS  pp     

Shi  Jianbo and Malik  Jitendra  Normalized cuts and image segmentation  IEEE Transactions on pattern analysis and machine intelligence     

Sohn  Kihyuk  Improved deep metric learning with multiclass npair loss objective  In Advances in Neural Information Processing Systems  pp     

Song  Hyun Oh  Xiang  Yu  Jegelka  Stefanie  and
Savarese  Silvio  Deep metric learning via lifted strucIn Proceedings of the IEEE
tured feature embedding 
Conference on Computer Vision and Pattern Recognition  pp     

Song  Hyun Oh  Jegelka  Stefanie  Rathod  Vivek  and
Murphy  Kevin  Deep metric learning via facility loIn Computer Vision and Pattern Recognition
cation 
 CVPR   

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 
In Proceedings of
Going deeper with convolutions 
the IEEE Conference on Computer Vision and Pattern
Recognition  pp     

Tsochantaridis  Ioannis  Joachims  Thorsten  Hofmann 
Thomas  and Altun  Yasemin  Large margin methods for
structured and interdependent output variables  Journal
of machine learning research   Sep   

Van Der Maaten  Laurens  Accelerating tsne using treebased algorithms  Journal of machine learning research 
   

Von Luxburg  Ulrike    tutorial on spectral clustering 

Statistics and computing     

Wah  Catherine  Branson  Steve  Welinder  Peter  Perona 
Pietro  and Belongie  Serge  The caltechucsd birds 
  dataset   

Weinberger  Kilian   and Saul  Lawrence    Distance metric learning for large margin nearest neighbor classi cation  JMLR     

