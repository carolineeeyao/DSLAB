Risk Bounds for Transferring Representations With and Without FineTuning

Daniel McNamara   MariaFlorina Balcan  

Abstract

  popular machine learning strategy is the transfer of   representation         feature extraction
function  learned on   source task to   target task 
Examples include the reuse of neural network
weights or word embeddings  We develop suf 
cient conditions for the success of this approach 
If the representation learned from the source task
is  xed  we identify conditions on how the tasks
relate to obtain an upper bound on target task risk
via   VC dimensionbased argument  We then
consider using the representation from the source
task to construct   prior  which is  netuned using target task data  We give   PACBayes target task risk bound in this setting under suitable
conditions  We show examples of our bounds
using feedforward neural networks  Our results
motivate   practical approach to weight transfer 
which we validate with experiments 

  Introduction
  widely used machine learning technique is the transfer
of   representation learned from   source task  for which
labeled data is abundant  to   target task  for which labeled
data is scarce  This may be effective if the tasks approximately share an intermediate representation  For example 
  features learned from an image of   human face to pre 

dict age may also be useful for predicting gender

  word embeddings learned to predict word contexts

may also be useful for part of speech tagging

  features learned from  nancial data to predict loan default may also be useful for predicting insurance fraud 
Often   representation is learned by   different organization
that may have greater access to data  computational and human resources  Examples are the Google word vec package  Mikolov et al    and downloadable pretrained

 The Australian National University and Data  Canberra  ACT  Australia  Carnegie Mellon University  Pittsburgh  PA  USA  Correspondence to  Daniel McNamara
 daniel mcnamara anu edu au 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

neural networks  Under this  representationas aservice 
model    user may expect to access the representation itself 
as well as information about its performance on the source
task data on which it was trained  We aim to convert this
into   guarantee of the usefulness of the representation on
other tasks  which is known in advance without the effort
or cost of testing the representation on the target task   
Our analysis also covers the case where the source task is
constructed from unlabeled data  as in neural network unsupervised pretraining 
We consider two approaches to transferring   representation learned from   source task to   target task  as shown in
Figure   We may either treat the representation as  xed  or
we may narrow the class of representations considered on
the target task  which we refer to as  netuning  The  xed
option may be attractive when very little labeled target task
data is available and hence over tting is   strong concern 
while the advantage of  netuning is relatively greater hypothesis class expressiveness 
Let      and   be sets known as the input  output and
feature spaces respectively  Let   be   class of representations  where           for         Let   be   class
of specialized classi ers  where           for       
Let the hypothesis class                       
such that            Let hS  hT         be the labeling functions and PS  PT be the input distributions for
source task   and target task   respectively  We consider
the setting         Let the risk of   hypothesis
  on   and   be RS      Ex PS  hS     cid       and
RT       Ex PT  hT      cid       respectively  Let  RS   
and  RT     be the corresponding empirical      
training
set  risks  We have mS labelled points for   and mT labelled points for     Let dH be the VC dimension of   
The remainder of the paper is structured as follows  In Section   we introduce related work  In Sections   and   we
analyze the cases where the transferred representation is
 xed and  netuned respectively 
In Section   we apply
the results and use them to motivate and test   practical approach to weight transfer in neural networks  We conclude
in Section   and defer more involved proofs to Section  

 See

http code google com archive   
word vec 
http caffe berkeleyvision org 
model zoo and http vlfeat org matconvnet 
pretrained for examples 

Risk Bounds for Transferring Representations With and Without FineTuning

  Background
Empirical studies have shown the success of transferring
representations between tasks  Donahue et al    Hoffman et al    Girshick et al    Socher et al   
Bansal et al    Word embeddings learned on   source
task have been shown  Qu et al    to perform better
than unigram features on target tasks such as part of speech
tagging  and comparably or better than embeddings  netuned on the target task  Yosinski et al 
  learned
neural network weights using half of the ImageNet classes 
and then learned the other classes with   neural network
initialized with these weights   nding   bene   compared
to random initialization only with target task  netuning 
The transfer of representations  both with and without  netuning  is widely and successfully used 
Previous work on domain adaptation  BenDavid et al 
  Mansour et al    Germain et al    has considered learning   hypothesis   on   and reusing it on
    bounding RT     using RS     measured with labeled
source data  and some notion of similarity between PS and
PT  measured with additional unlabeled target data  Such
results motivate   joint optimization using labeled source
and unlabeled target data  Ganin et al    Long et al 
  to learn separate mappings fS  fT         which
make the induced distributions in the feature space   similar  and   hypothesis           learned from the source
labels which can be reused on     This approach assumes
the tasks become the same if their input distributions can be
aligned  We consider   relaxation where the tasks are more
weakly related but some representation step can be transferred  We consider learning           on    reusing it
on     and then learning gT         from   small amount
of labeled target data  Given the widespread use of  downloadable  representations  where   and gT are learned separately and there is no joint optimization over source and
target data  this is   realistic setting 
Work on lifelong learning relates the past performance of
  representation over many tasks to its expected future
performance  For   representation       we construct
                        Suppose there is   distribution over tasks  known as an environment  Assume several
tasks from this environment have been sampled  and that
for each task some hypothesis in       has been selected
and its empirical risk evaluated  Previous work has provided bounds on the difference between the average empirical risk and the expected risk of the best hypothesis in
      for   new task drawn from the environment  Such
bounds have been given by measuring the complexity of
  and   using covering numbers  Baxter      variant of the growth function  Galanti et al    and  
distributiondependent measure known as Gaussian complexity  Maurer et al    All of these bounds rely on

Learn representation

from scratch

Transfer representation

without  netuning

Transfer representation

with  netuning

 

 

 

  

 

 

  

  

 

Figure     comparison of approaches to learning   representation
on   target task  where the search space in each case is the shaded
area  Learning from scratch  we search   representation class  
for   good representation         Without  netuning  we     
representation    learned from the source task  With  netuning 
we narrow the search to        near     which still contains   
known past performance on   large number of tasks  In
practice  however  representations such as neural network
weights or word embeddings are often learned using only  
single source task  which is the setting we consider 
  Representation Fixed by Source Task
Suppose labeled source data is abundant  labeled target data
is scarce  and we believe the tasks share   representation 
  natural approach to leveraging the source data is to learn
 gS          on    from which we assume we may extract          then conduct empirical risk minimization
over                           on   yielding  gT      
Theorem   upperbounds RT  gT        using four terms   
function   measuring   transferrability property obtained
analytically from the problem setting  the empirical risk
 RS gS        the generalization error of   hypothesis in  
learned from mS samples  and the generalization error of
  hypothesis in   learned from mT samples  The value
of the theorem is that if             RS gS        is  
small constant  mS  cid  mT and dH  cid  dG  we improve on
the VC dimensionbased bound for learning   from scratch
by avoiding the generalization error of   hypothesis in  
learned from mT samples  Furthermore  we do not settle
for bounding RT  gT        in terms of  RT  gT        which
may be large  The theorem can be used to select   given

 Pentina   Lampert   extend this analysis to stochastic hypotheses       distributions over deterministic hypotheses 
where for each task we learn   posterior given   prior and training
data  The quality of the prior affects the learner   performance 
The study proposes using source tasks to learn    hyperposterior 
  distribution over priors which is sampled to give   prior for each
task  Such   hyperposterior may focus the learner on   representation shared across tasks  The study gives   PACBayes bound
on the expected risk of using   hyperposterior to learn   new task
drawn from the environment  in terms of the average empirical
risk obtained using the hyperposterior to learn the source tasks 
 This is not possible with knowledge of  gS      alone  but in
the case of feedforward neural networks which we focus on     is
known if the weights learned on   are known 
 We have mS  cid  mT if labeled source task data is abundant
while labeled target task data is scarce  and dH  cid  dG if we simplify target task learning by substantially reducing the hypothesis
space to be searched 

Risk Bounds for Transferring Representations With and Without FineTuning

mT

 

mS

    arg min

   

RT           With probability at

 cid   dG log emT  dG  log 

 cid   dH log emS  dH   log 

several options  While we refer to   in   general form  we
give an example in Section   and expect that others exist 
Theorem   Let           be   nondecreasing function  Suppose PS  PT   hS  hT         have the property that
 gS      min
RT              RS gS        Let  gT  
   
 RT           Then with probability at least      
arg min
   
over pairs of training sets for tasks   and     RT  gT       
     RS gS           
   
 
Proof  Let   
least      
RT  gT       
   RT  gT           
            
   RT    
  RT    
            
   RS gS           
     RS gS           
 

 cid   dG log emT  dG  log 
 cid   dG log emT  dG  log 
 cid   dG log emT  dG  log 
 cid   dG log emT  dG  log 
 cid   dH log emS  dH   log 

 cid   dG log emT  dG  log 

   

mT

mT

mT

mT

mS

mT

 

 cid     log em    log 

Using   training points and   hypothesis class of VC dimension    with probability at least       for all hypotheses   simultaneously  the risk      and empirical risk      
satisfy                
 Mohri
et al    For   this yields the  rst and third inequalities
with probability at least      
  For    because   is nondecreasing  this yields the  fth inequality with probability
at least      
  Applying the union bound achieves the desired result  The second inequality is by the de nition of  gT
and the fourth inequality follows from our assumption 

 

  Neural Network Example with Fixed

Representation

In Theorem   we give an example of the property required
by Theorem   which is speci   to   particular problem setting  We consider   neural network with   single hidden
layer  see Figure   We propose transferring the lowerlevel weights  corresponding to     learned on    so that
only the upperlevel weights  corresponding to    have to
be learned on     We want to show    is also useful for    
RT           since
we expect this may be feasible analytically as in our example in
Section   However  because we only observe  RS gS        in
Theorem   we use this to bound RS gS        and then apply  

 We de ne   by relating RS gS        to min
   

Learn  

from scratch

Learn  gS     

on  

Transfer    from   

learn  gT on  

  

  

 gS

 gT

Figure   Neural network example learning   from scratch  left 
and with weights transferred from    right  Thin blue and thick
red lines show weights trained on   and   respectively  Under
certain assumptions using weight transfer yields low risk on    

To do this  we assume that some lowerlevel weights perform well on both tasks  which is clearly   necessary condition for the speci      we are transferring to perform well
on both tasks  We also assume PS and PT have the relative rotation invariance property and that the upperlevel
weights have  xed magnitude  This is so that   point   for
which        contributes to the risk on   cannot be  hidden 
from the risk of using    on    either through low PS   
or low magnitude upperlevel weights  Hence RS gS       
reliably indicates the usefulness of    on    
Let     Rn and     Rk  Let   be the function class such
that                              wk      where wi   Rn
for                      is an odd function  and
  is the dot product  Let   be the function class such that
       sign        where           Suppose     
   gS  gT     such that max RS gS     RT  gT         
Let                                   wk      Given wi and
 wi  pick nonzero constants    and    such that  wi   
    wi    iwi  and wi       wi    iwi      Let   be  
     matrix with rows                    wk      wk 
 kwk  Suppose   is full rank  Suppose       cid  such that
             cid  PT       cPS   cid  for some      
which we call relative rotation invariance and implies PS
and PT have the same support  If   is an orthogonal matrix then       cid  such that          cid  PT       cPS   cid 
Theorem   Let       cR          Then  gS     
min
   

RT              RS gS       
                   Examples are tanh  sign and identity 
 To see that this condition is necessary  consider the following
example where   is not full rank  Let             hS  
sign    and hT   sign    For                         
gS      sign         and gT       sign         we have
RS gS         RT  gT           On   we learn               
           and  gS      sign         so that RS gS           
but in general min
   

RT               since    ignores   

 For example  PS and PT are spherical Gaussians  For   zeromean multivariate Gaussian distribution this is achieved by the
whitening transformation             where the columns
of   and entries of the diagonal matrix   are the eigenvectors and
eigenvalues of the distribution   covariance matrix respectively 

Risk Bounds for Transferring Representations With and Without FineTuning

  Representation FineTuned on Target Task
Consider learning  gS      on    and then using    and
RS gS        to  nd          as in Figure   Let  hg   be
  stochastic hypothesis         distribution over    associated with                   is the mode of  hg     We
propose learning   with the hypothesis class  HG      
 hg                     and the prior    gS       Learning   from scratch we assume that we would instead use
 HG      hg                   and some  xed prior
       HG     Let RT        
  PT      hT      cid      
and compute  RT     on the training set distribution of    
In Theorem   we show that if    is  small enough  so that
all       HG     have   small KL divergence from    gS      
we may apply   PACBayes bound to the generalization
error of hypotheses in  HG     involving four terms    function   measuring   transferrability property  the empirical
risk  RS gS        the generalization error of   hypothesis
in   learned from mS points  and   weak dependence on
mT   The value of the theorem is that if           
 RS gS       is   small constant  and mS  cid  mT   we improve
on the PACBayes bound for  HG   and        is useful if
it is also  large enough  in the sense that  hgT       HG    
such that RT  hgT          Here   quanti es how large the
   we search on   must be in order to be  large enough 
in terms of RS gS        While in general such an    and  
may not exist  we give an example in Section  
Theorem   Let           be nondecreasing  Suppose given        and RS gS        estimated from   
it is possible to construct    with the property     
 HG       KL     gS          RS gS        Then with
probability at least       over pairs of training sets for
 cid 
tasks   and           HG       RT        RT      

 cid   dH log emS  dH   log 

   RS  gS      

 log  mT  

 mT  

KL     gS      log  mT  

 cid 
 cid   RS  gS      log  mT  

Proof  With probability at least      
RT    
   RT      
   RT      
The  rst inequality holds with probability at least      
 
 ShalevShwartz   BenDavid    The second inequality holds by assumption  Furthermore  RS gS         
 RS gS           
with probability at least      
   Mohri et al    and   is nondecreasing  The result follows from the union bound 

 cid   dH log emS  dH   log 

 mT  

mS

 

 Using the restricted deterministic hypothesis class         
                     such that            and   VC dimensionbased bound may not improve on    since possibly dG       dH 

mS
 mT  

 

  Neural Network Example with FineTuning

We transfer and  netune weights in   feedforward neural network with one hidden layer to instantiate the property required by Theorem   We learn   deterministic hypothesis of this type on   and obtain   estimated lowerlevel weight vectors  wi  Learning   we now consider only
lowerlevel weights near  wi  corresponding to      On   we
learn   stochastic hypothesis formed by taking   deterministic network and adding independent sources of spherical
Gaussian noise to the lowerlevel weights and sign ipping
noise to the upperlevel weights  The KL divergence between two of the stochastic hypotheses is expressed using
the angles between their lowerlevel weights  and   quantity computable from their upperlevel weights 
We want to prove that we can construct such an    to successfully learn     To do this  we assume some lowerlevel
weights wi perform well on both   and     We make   
 small enough  by only including lowerlevel weights with
small angles to  wi  and  large enough  by using the risk
observed using  wi on   to provide an upper bound on the
angle between each pair wi and  wi  Our assumptions ensure that poor  wi cannot be  hidden  from the risk on    either through low PS density in the region of disagreement
between wi and  wi  or through low magnitude higherlevel
weights  Hence we know that searching    will include wi 
Let     Rn and     Rk  where   is odd 
Let   be the function class
such that        
 sign                 sign wk      where wi   Rn
for           
Let   be the function class
such that        sign        where          
Let Bv be   distribution on      such that for
  cid    Bv        cid   
   vj         cid 
   vj   where
  cid 
        Let  hg       cid      cid  such that   cid    cid 
   
            cid 
   wi      Suppose         gS  gT     such
Bv
that max RS gS       RT  hgT          Let         
 sign                   sign   wk       wi   wi  be the angle
between wi and  wi  and assume        wi      De ne  
as in Section   Let PS have the rotation invariance property       cid  such that              cid  PS      cPS   cid 
for some      
Theorem   Given    and RS gS        estimated from   
            wi         wi   wi     max  Let
    
       
Then  hgT       HG     such that RT  hgT          and
      HG       KL     gS          RS gS       
 Assuming that the lowerlevel weight vectors are of  xed
magnitude  which is no loss of model expressiveness since we
use the sign activation function at the hidden layer 

let  max    cid        RS gS            and     

    cos  max                    log 

  cid 

    cid 

  

  

 

Risk Bounds for Transferring Representations With and Without FineTuning

  Applications
We show the utility of the risk bounds  and present   novel
technique and experiments motivated by our theorems 
  Using the Risk Bounds

The results described yield tighter bounds on risk when
transferring representations from    compared to learning
  from scratch  Examples are shown in Figure  
We set       For the top part  we use the example
from Section   and set             Learning  
from scratch with    we use the bound from Mohri et al 
  used previously  The VC dimension of   network
of     edges using the sign activation is      log    
 ShalevShwartz   BenDavid    where in our case
      nk    We use dH       log     in the chart  Transferring   representation from   to   without  netuning 
we consider the limit        RS gS            mS    
and hence       by Theorem   Furthermore  dG    
since   is  nite and hence dG   log       ShalevShwartz
  BenDavid    We use the bound from Theorem  
For the bottom part  we use the example from Section  
  Learning   from
and set      
scratch we use the stochastic hypothesis class  hg        
  such that    wi             and   prior     where
   wi     and          is arbitrary  Hence we
have the bound KL              
    which becomes
tight for large    We apply the PACBayes bound  ShalevShwartz   BenDavid    used previously  Transferring
  representation from   and  netuning on     we consider
the limit        RS gS            mS     We have
KL     gS          
  by Theorem   We use the bound from
Theorem  

             

  FineTuning through Regularization
We relax the hard constraint on    from Section   by using   modi ed loss function  which we  nd performs better
in practice  Let yi and  yi be the label and prediction respectively for the ith training point  In   fullyconnected
feedforward neural network with   layers of weights  let
      be the jth weight matrix         be its estimate from
   excluding weights for bias units in both cases  and  
be the entrywise   norm    typical loss function   used
for training is composed of the sum of training set log loss
and    regularization on the weights 

 Note that VC dimension risk bounds are known for being
rather loose  while PACBayesian bounds are tighter and hence
yield nontrivial results in higher dimensions with fewer samples 
 This class is as expressive as  HG   but by setting  wi     
the KL divergence of all hypotheses from any prior is bounded 
allowing   fair comparison to  HG       The choice of     minimizes
worst case KL divergence to   hypothesis in the class 

Figure     comparison of risk bounds compared to learning  
from scratch  without  netuning  top  and with  netuning  bottom  The two charts use different parameters  see Section  
      
 

 yi log  yi       yi  log     yi   

  cid 

  cid 

  

 
 

We replace the regularization penalty with  

 

 

  

  cid 

  

 

   

 

               

   

   

 

      
 

This penalizes estimates of   far from the representation
learned on    Since we expect the tasks to share   lowlevel representation       edge detectors for vision  word
embeddings for text  but be distinct at higher levels      
image components for vision  topics for text  we set  
to be   decreasing function  while   controls standard
   regularization  The technique is novel to our knowledge  although other approaches to transferring regularization between tasks exist  Evgeniou   Pontil    Raina
et al    Argyriou et al    Ghifary et al   

  Experiments

We experiment on basic image and text classi cation
tasks  We show that learning algorithms motivated by
our theoretical results can help to overcome   scarcity of
labeled target task data  Note that we do not replicate the
conditions speci ed in our theorems  nor do we attempt extensive tuning to achieve stateof theart performance 

 Basing our approach on   we follow the convention that
weights connected to bias units are excluded from the regularization penalty  However  the inclusion of these weights in the
                term of   is   plausible variant 

 The MNIST and   Newgroups datasets are available at
http yann lecun com exdb mnist and http 
qwone com jason Newsgroups respectively 

Risk Bounds for Transferring Representations With and Without FineTuning

   

     

We randomly partition label classes into sets    and   
where           We construct    by randomly picking from    up to    
  then randomly picking
from    such that           We let   be the task of
distinguishing between    and    and   be that of distinguishing    and    Constructing    and    as disjunctions of classes means that the class labels are   perfect
representation shared between   and    
We compare the accuracy on   of four options 

  learn   from scratch  BASE 
  transfer    from     netune   and train   on   using

   FINETUNE    

  transfer    from   and     train   on    FIX    
  transfer  gS      from   and     FIX  gS      

We use                        
mT     and the sigmoid activation function  For
MNIST we use raw pixel intensities              network and mS     For NEWSGROUPS we use TFIDF
weighted counts of most frequent words             
network and mS     We use conjugate gradient optimization with   iterations 
The results are shown in Table   When the tasks are nonidentical  FINETUNE    is mostly the strongest but performs better on MNIST  FIX    outperforms BASE when
      and hence the tasks are similar  While FIX    outperforms FIX  gS      when the tasks are nonidentical on
MNIST  on NEWSGROUPS there is no evidence of bene   
When the tasks are identical  FIX  gS      is the strongest 
It appears that learning an MNIST digit requires   dense
weight vector and so      tends to encode single digits 
which helps transferrability  However  it appears that since
we may learn   newsgroup with   sparse weight vector 
     tends to encode disjunctions of newsgroups which
somewhat reduces transferrability  When transferring representations does work   netuning using the regularization
penalty proposed in   improves performance 

 For MNIST there are   label classes and for   Newgroups
there are  
In both cases the classes are approximately balanced  Note that we ignore the hierarchical structure of the  
Newsgroups classes  which likely contributes to the lower accuracies reported for all methods for this dataset relative to MNIST 
      logistic regression with    regularization and     xed 
 Used to isolate the bene   of transferring    rather than  gS      
 We explored tuning   to lift the performance of BASE on
MNIST  but found that the results did not materially improve  Potentially     and     in   could be tuned with cross validation on the target task 
 For       hS   hT   We do not consider       since that
is equivalent to     with the de nitions of    and    swapped 

Table   Evaluation of transferring representations  Entries are the
test set accuracy of the technique  row  for the task  column  averaged over   trials  with the best result for each task bolded 

TECHNIQUE

MNIST     

 

 

 

NEWSGROUPS     
 
 

 

BASE
FINETUNE   
FIX   
FIX  gS     

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

  Conclusion
We developed suf cient conditions for the successful transfer of representations both with and without  netuning 
This is   step towards   principled explanation of the empirical success achieved by such techniques    promising
direction for future work is generalizing the neural network
architectures considered       using multiple hidden layers 
and relaxing the distributional assumptions required  Furthermore  in the  netuning case it may be possible to upper
bound the target task generalization error of hypotheses in
                              such that            using another measure such as the Rademacher complexity of
         eliminating the need for stochastic hypotheses 
We proposed   novel form of regularization for neural network training motivated by our theoretical results 
which penalizes divergence from source task weights and
is stricter for lowerlevel weights  We validated this technique through applications to image and text classi cation 
Future directions include experiments on more challenging
tasks using deeper and more tailored network architectures
      convolutional neural networks 
  Additional Proofs
We provide complete proofs of Theorems   and   For
brevity  we drop the explicit dependence of        hS and
hT on   in our notation where the meaning is clear 
  Proof of Theorem  
Proof  Let gS      sign vS      gT       sign vT     
 gS      sign vS       gT       sign      vS      where
    vS   vT        and   is the elementwise product 
It is suf cient to show RT  gT          cRS gS        
RT  gT       
    rx PT  hT      vS         
    rx PT  hT     vS             vS          vS           
  rx PT  hT     vS             vS          vS         
    rx PT  hT     vS        
  rx PT      vS          vS         

Risk Bounds for Transferring Representations With and Without FineTuning

        rx PT      vS          vS         
      cP rx PS  vS      vS         
          rx PS  hS  vS          hSvS        
  rx PS  hS  vS          hSvS        
          rx PS  hS  vS              rx PS  hSvS        
  cRS gS                
The third and  nal inequalities are due to the shared representation assumption in the problem statement  The fourth
inequality holds by Lemma   The remaining lines apply
simple rules of probability 
Lemma   Suppose       cid  such that              cid 
PT       cPS   cid  Let                              Then
  rx PT                          cP rx PS                   
Proof  Suppose there is an invertible map Rn   Rn yielding   cid  on input    such that                  cid  and
                                      cid           cid  Then the result
follows since PT       cPS   cid  by assumption  Furthermore  if   is an orthogonal matrix           cid 
Such   map is   cid                           where     
               dk  dk  We have     wi     cid    diwi     and
    wi iwi   cid    di    wi iwi    and hence  wi   cid   
di  wi     for         cid    Therefore 
                            
                             
           cid               
           cid           cid 
The  rst equality is   property of the elementwise and dot
products  For the second equality    wi   cid      diwi     
dia wi      since   is an odd function  Similarly  for the
third equality     wi     cid      di  wi        dia   wi     

  Proof of Theorem  
Proof of  hgT       HG     such that RT  hgT         
Recall that wi are the weight vectors for   and  wi are
those for     Observe that for any wi such that wi    wi    
we have  wi    wi     and  visign wi       
visign wi      Combining this with the assumption
max RS gS       RT  gT           we conf   gS  gT   
clude         gS  gT     such that     wi    wi     and
max RS gS       RT  hgT         
Let gS      sign vS      and  gS      sign vS      Let
  be   rotation invariant distribution for       To prove
 hgT       HG       by the de nition of  HG     it is suf cient

to show    wi   wi     cid        RS gS           

min

max

 wi   wi 
 
   

 
 

    rx    vS     vS         
    rx    vS      vS         
  cP rx PS  vS      vS         
      rx PS  hSvS         hS  vS         
  rx PS  hSvS         hS  vS         
      rx PS  hSvS             rx PS  hS  vS         
       RS gS       
The  rst inequality holds by Lemma   The second inequality holds by Lemma   using the fact     wi    wi    
The third inequality uses the rotation invariance of PS  The
following two lines use basic laws of probability  The  nal
inequality uses the assumption RS gS          

Proof of       HG       KL     gS          RS gS       
For any  hg      HG       KL hg     gS      
  cid 

 KL    wi          wi        KL Bv   vS  

 

  

The KL divergence of   product distribution is the sum of
the KL divergences of its component distributions  We upper bound both terms and apply the de nition of  

  cid 

  

 wi       wi     wi   wi  cos wi   wi 

  

KL    wi          wi     

 wi    wi 

  cid 
  cid 
  cid 
      cos cid        RS gS           

    cos wi   wi 

  

  

   
 

   
 

   
 

   
The  rst equality uses the KL divergence of Gaussian distributions  The second equality uses the law of cosines  The
third equality is because    wi       wi      by construction  The inequality follows by the de nition of    and
the fact that     cos  is nondecreasing for        
KL Bv   vS  

    cid 

  

 cid  

 

 cid pi          log 

pi      
   ipk  

                     log 
The  rst inequality uses the de nition of Bv to express
KL Bv   vS   The equality is   simpli cation 

   

 

Risk Bounds for Transferring Representations With and Without FineTuning

                                         

                                

                         

      

   cid 
   cid 
   cid 
   cid 

      

max

 
 

 wi   wi 
 
   

 fj                 

    rx                    

Lemma   Suppose   is odd                      such
that     wi    wi     and   is rotation invariant with      
Then
Proof  Let                   vj  vj          vk  and de ne
    and      similarly  Let          rx    
                    
                      
                                                    
                   
    vjfjv            fj
                   
     vjfjv              fj
    vjfjv              fj
                   
     vjfjv              fj
    vjfjv              fj
                   
     vjfjv              fj
    vjfjv              fj
                       fj
                       fj
       wj    wj  
       wj    wj  
   

 fj                 
 fj                 
 fj                 
 fj    

 fj                 
 fj                 

 fj                 

 fj                 

 cid   

  

 cid   

 

 

 

     
   
 wi   wi 
 
  max
   

 
 

 

The third inequality follows since   is rotation invariant
and wj    wj     The third and  fth equalities use rotation
invariance  The  nal equality uses rotation invariance and
the fact that   is odd  The fourth inequality is   standard
lower bound for the central binomial coef cient  The other
lines use basic simpli cations and laws of probability 
Lemma   Suppose   is odd                         
such that     wi    wi     and   is rotation invariant with       Then   rx                      
  rx                     
Proof  Let          rx     and      Ex     Let
              rx                       fk     fk          
Let            and                                 
                         Assume     cid     if        then
   fi     and
the lemma clearly holds  Let          
   Let                             

let     min
  di 
                                      fl    

  cid 

  

 cid   cid cid 

  cid 

 cid  

 

 cid cid    

   

 cid  The

Let        

   

  

  cid       cid 

 fi   sign         while

term   counts coordinates where vi
  counts those where vifi   sign         and fi    fi 
                                            
                                          
    

      
                                                         
                                              

      
   
The second equality uses linearity of expectation  The third
equality uses the law of total expectation and the de nition
of     
The fourth equality holds since           

                            
                         
                                  due to the

   cid 
     cid 
     cid 

   

   

   

rotation invariance of    
The  fth equality holds by expanding   linearity of expectation  and   similar argument to the previous equality
to show                                                          
The sixth equality holds by the rotation invariance of   and
the fact that   is odd 
For the  nal inequality  the right hand term is nonnegative
since                      and   is nonincreasing  The left
hand term is also nonnegative due to the rotation invariance assumption and the fact that     wi    wi    
Acknowledgements
Daniel McNamara was   visitor at Carnegie Mellon University during the period of this research  supported by  
Fulbright Postgraduate Scholarship 
This work was supported in part by NSF grants CCF 
  CCF  IIS  and   Microsoft Research Faculty Fellowship 
We thank the anonymous reviewers for their useful comments 

Risk Bounds for Transferring Representations With and Without FineTuning

References
Argyriou  Andreas  Evgeniou  Theodoros  and Pontil  Massimiliano  Convex multitask feature learning  Machine
Learning     

Bansal  Mohit  Gimpel  Kevin  and Livescu  Karen  Tailoring continuous word representations for dependency
parsing  In Association for Computational Linguistics 
pp     

Hoffman  Judy  Guadarrama  Sergio  Tzeng  Eric    Hu 
Ronghang  Donahue  Jeff  Girshick  Ross  Darrell 
Trevor  and Saenko  Kate  LSDA  Large scale detection
through adaptation  In Advances in Neural Information
Processing Systems  pp     

Long  Mingsheng  Cao  Yue  Wang  Jianmin  and Jordan 
Michael    Learning transferable features with deep
In International Conference on
adaptation networks 
Machine Learning  pp     

Baxter  Jonathan    model of inductive bias learning  Journal of Arti cial Intelligence Research   
 

Mansour  Yishay  Mohri  Mehryar  and Rostamizadeh  Afshin  Domain adaptation  Learning bounds and algorithms  In Conference on Learning Theory   

BenDavid  Shai  Blitzer  John  Crammer  Koby  Kulesza 
Alex  Pereira  Fernando  and Vaughan  Jennifer Wortman    theory of learning from different domains  Machine Learning     

Donahue  Jeff  Jia  Yangqing  Vinyals  Oriol  Hoffman 
Judy  Zhang  Ning  Tzeng  Eric  and Darrell  Trevor  DeCAF    deep convolutional activation feature for generic
visual recognition  In International Conference on Machine Learning  pp     

Evgeniou  Theodoros and Pontil  Massimiliano  Regularized multitask learning  In International Conference on
Knowledge Discovery and Data Mining  pp   
 

Galanti  Tomer  Wolf  Lior  and Hazan  Tamir    theoretical framework for deep transfer learning  Information
and Inference   

Ganin  Yaroslav  Ustinova  Evgeniya  Ajakan  Hana  Germain  Pascal  Larochelle  Hugo  Laviolette  Franc ois 
Marchand  Mario  and Lempitsky  Victor  Domainadversarial training of neural networks  Journal of Machine Learning Research     

Germain  Pascal  Habrard  Amaury  Laviolette  Franc ois 
and Morvant  Emilie    PACBayesian approach for domain adaptation with specialization to linear classi ers 
In International Conference on Machine Learning  pp 
   

Ghifary  Muhammad  Kleijn    Bastiaan  and Zhang 
Mengjie  Domain adaptive neural networks for object
recognition  In Paci   Rim International Conference on
Arti cial Intelligence  pp    Springer   

Girshick  Ross  Donahue  Jeff  Darrell  Trevor  and Malik 
Jitendra  Rich feature hierarchies for accurate object detection and semantic segmentation  In IEEE Conference
on Computer Vision and Pattern Recognition  pp   
   

Maurer  Andreas  Pontil  Massimiliano  and RomeraParedes  Bernardino  The bene   of multitask representation learning  Journal of Machine Learning Research 
   

Mikolov  Tomas  Sutskever  Ilya  Chen  Kai  Corrado 
Greg    and Dean  Jeff  Distributed representations of
In Adwords and phrases and their compositionality 
vances in Neural Information Processing Systems  pp 
   

Mohri  Mehryar  Rostamizadeh  Afshin  and Talwalkar 
Ameet  Foundations of Machine Learning  MIT Press 
 

Pentina  Anastasia and Lampert  Christoph      PACBayesian bound for Lifelong Learning  In International
Conference on Machine Learning  pp     

Qu  Lizhen  Ferraro  Gabriela  Zhou  Liyuan  Hou  Weiwei  Schneider  Nathan  and Baldwin  Timothy  Big data
small data  in domain outof domain  known word unknown word  the impact of word representation on sequence labelling tasks  In Conference on Computational
Natural Language Learning  pp     

Raina  Rajat  Ng  Andrew    and Koller  Daphne  Constructing informative priors using transfer learning 
In
International Conference on Machine Learning  pp 
   

ShalevShwartz  Shai and BenDavid  Shai  Understanding
Machine Learning  From Theory to Algorithms  Cambridge University Press   

Socher  Richard  Ganjoo  Milind  Manning  Christopher   
and Ng  Andrew  Zeroshot learning through crossmodal transfer  In Advances in Neural Information Processing Systems  pp     

Yosinski  Jason  Clune  Jeff  Bengio  Yoshua  and Lipson 
Hod  How transferable are features in deep neural networks  In Advances in Neural Information Processing
Systems  pp     

