Identifying Best Interventions through Online Importance Sampling

Rajat Sen     Karthikeyan Shanmugam     Alexandros    Dimakis   Sanjay Shakkottai  

Abstract

Motivated by applications in computational advertising and systems biology  we consider the
problem of identifying the best out of several
possible soft interventions at   source node  
in an acyclic causal directed graph  to maximize
the expected value of   target node    located
downstream of     Our setting imposes    xed
total budget for sampling under various interventions  along with cost constraints on different
types of interventions  We pose this as   best arm
identi cation bandit problem with   arms where
each arm is   soft intervention at    and leverage
the information leakage among the arms to provide the  rst gap dependent error and simple regret bounds for this problem  Our results are  
signi cant improvement over the traditional best
arm identi cation results  We empirically show
that our algorithms outperform the state of the
art in the Flow Cytometry dataset  and also apply our algorithm for model interpretation of the
Inceptionv  deep net that classi es images 

UserIntention 

Use 

Hidden Variables 

Ad inventory 

CTR Prediction 
Algorithm 

User Query  

Qu 

Predicted Click 
Through rate 

Ad Placement 
 Design 

Actual User Clicks 

  

Ads chosen 

Bids Chosen 

Pricing 

Revenue 

  

Figure   Computational advertising example borrowed from
 Bottou et al    Various observable and hidden variables
are shown  The topology of the causal graph is known  however
the strengths of most interactions are unknown    clickrate scoring algorithm predicts future user click through rates from users 
search queries and the set of ads relevant to the user query chosen from an ad inventory  The algorithm   output determines the
ads displayed  as well as the display style  and through   complex causal graph   nally determines actual revenue  The part of
the network in bold   distribution of user queries and matching
ad keywords is known  including strengths  and the input output
characteristics of several candidate  randomized  clickrate scoring algorithms are known  The objective is to choose the best
algorithm that maximizes the revenue  target in bold red 

  Introduction
Causal graphs  Pearl      are useful for representing
causal relationships among interacting variables in large
systems  Bottou et al    Over the last few decades 
causal models have found use in computational advertising  Bottou et al    biological systems  Meinshausen et al    sociology  Blalock    agriculture  SplawaNeyman et al    and epidemiology  Joffe
et al    There are two important questions commonly
studied with causal graphs      How to learn   directed
causal graph that encodes the pattern of interaction among
components in   system  casual structure learning   Pearl 
      and  ii  Using previously acquired  partial  knowledge about the causal graph structure  how to estimate

 Equal contribution  The University of Texas at Austin  IBM
Thomas    Watson Research Center  Correspondence to  Rajat
Sen  rajat sen utexas edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

and or to optimize the effect of   new intervention on other
variables  optimization   Bottou et al    Joffe et al 
  Kemmeren et al    Bonneau et al    Krouk
et al    Here  an intervention is   forcible change to
the value of   variable in   system  The change either alters
the relationship between the parental causes and the variable  or decouples it from the parental causes entirely  Our
focus is on optimizing over   given set of interventions 
An illustrative example includes online advertising  Bottou
et al    where there is   collection of clickthrough
rate scoring algorithms that provide an estimate of the probability that an user clicks on an ad displayed at   speci  
position  The interventions occur through the choice of
clickthrough rate scoring algorithm  the algorithm choice
directly impacts ad placement and pricing  and through  
complex network of interactions  affects the revenue generated through advertisements  The revenue is used to determine the best scoring algorithm  optimize for the best
intervention  see Figure   Another example is in biological generegulatory networks  Bonneau et al    where
  large number of genomes interact amongst each other and

Identifying Best Interventions through Online Importance Sampling

also interact with environmental factors  The objective here
is to understand the best perturbation of some genomes in
terms of its effect on the expression of another subset of
genomes  target  in cellular systems 
This paper focuses on the following setting  We are given
apriori knowledge about the structure and strength of interactions over   small part of the causal graph  In addition 
there is freedom to intervene  from   set of allowable interventions  at   certain node in the known part of the graph 
and collect data under the chosen intervention  further we
can alter the interventions over time and observe the corresponding effects  Given   set of potential interventions
to optimize over  the key question of interest is  How to
choose the best sequence of   allowable interventions in
order to discover which intervention maximizes the expectation of   downstream target node 
Determining the best intervention in the above setting can
be cast as   best arm identi cation bandit problem  as noted
in  Lattimore et al    The possible interventions to
optimize over are the arms of the bandit  while the sample
value of the target node under an intervention is the reward 
More formally  suppose that   is   node in   causal
graph         as shown in Fig    with the parents
of   denoted by pa     In Fig      corresponds to
the clickthrough rate and its parents are userquery and
adschosen  This essentially means that   is causally
determined by   function of pa     and some exogenous random noise  This dependence is characterized
by the conditional      pa     Then    soft  intervention mathematically corresponds to changing this conditional probability distribution      probabilistically forcing   to certain states given its parents 
In the computational advertising example  the interventions correspond
to changing the click through rate scoring algorithm     
  click through rate ads chosen  user query  whose
inputoutput characteristics are wellstudied  Further  suppose that the effect of an intervention is observed at  
node   which is downstream of   in the topological order            refer to Fig    Then  our key question
is stated as follows  Given   collection of interventions
      pa           PK    pa      nd the best intervention among these   possibilites that maximizes      
under    xed budget of    intervention  observation  pairs 

  Main Contributions
 Successive Rejects Algorithm  We provide an ef cient
successive rejects multiphase algorithm  The algorithm
uses clipped importance sampling  The clipper level is set
adaptively in each phase in order to tradeoff bias and vari 

 Formally if node   has parents       then this distribution
is the conditional                         for all         

        PK 

       

 

Layers

 

Figure   Illustration of our setting  The soft interventions modify
     pa     The various conditionals and the marginal of pa    
is assumed to be known from prior data  The target variable   lies
further downstream in the unknown portions of the causal graph 

   

ance  Our procedure yields   major improvement over the
algorithm in  Lattimore et al     both in theoretical
guarantees and in practice  which sets the clippers and allocates samples in   static manner 
 Gap Dependent Error Guarantees under Budget Constraints  In the classic best arm identi cation problem  Audibert   Bubeck    Audibert et al  derive gap dependent bounds on the probability of error given    xed sample
budget  Speci cally  let     be the ith largest gap  difference  in the expected reward from that of the best arm      
  is the difference between the best arm expected reward and the second best reward  Then  it has been shown
in  Audibert   Bubeck    that the number of samples
needed scales as  upto poly log factors  maxi   
In our setting    fundamental difference from the classical
best arm setting  Audibert   Bubeck    is the information leakage across the arms       samples from one arm can
inform us about the expected value of other arms because
of the shared causal graph  We show that this information
leakage yields signi cant improvement both in theory and
practice  We derive the  rst gap dependent  gaps between
the expected reward at the target under different interventions  bounds on the probability of error in terms of the
number of samples     cost budget   on the relative fraction of times various arms are sampled and the divergences
between soft intervention distributions of the arms 
In our result  upto poly log factors  the factor   is replaced
by the  effective variance  of the estimator for arm         
we obtain  with informal notation  maxi  
       can
be much smaller than pi  the corresponding term in the results of  Audibert   Bubeck    Our theoretical guarantees quantify the improvement obtained by leveraging information leakage  which has been empirically observed
in  Bottou et al    We discuss in more detail in Sections   about how these guarantees can be exponentially
better than the classical ones  We also derive gap dependent
simple regret bounds  Section  

   

Identifying Best Interventions through Online Importance Sampling

 Novel fdivergence measure for analyzing Importance
Sampling   We provide   novel analysis of clipped importance sampling estimators  where pairwise fdivergences
between the distributions  Pk    pa     for   carefully
chosen function      see Section    act as the  effective
variance  term in the analysis for the estimators  similar to
Bernstein   bound  Bennett   
 Extensive Empirical Validation  We demonstrate that
our algorithm outperforms the prior works  Lattimore et al 
  Audibert   Bubeck    on the Flow Cytometry
dataset  Sachs et al     in Section   We exhibit
an innovative application of our algorithm for model interpretation of the Inception Deep Network  Szegedy et al 
  for image classi cation  refer to Section  
Remark   The techniques in this paper can be directly
applied to more general settings like     the intervention
source      can be   collection of nodes     and the changes
affect the distribution      pa    where pa    is the
union of all the parents   ii  the importance sampling can
be applied at   directed cut separating the sources and
the targets  provided the effect of the interventions  on the
nodes forming the cut can be estimated  Moreover  our
techniques can be applied without the complete knowledge
of the source distributions  We explain the variations in
more detail in Section   in the appendix 

  Related Work
The problem lies at the intersection of causal inference and
best arm identi cation in bandits  There have been many
studies on the classical best arm identi cation in the bandit
literature  both in the  xed con dence regime  Kaufmann
et al    Gabillon et al    and in the  xed budget setting  Audibert   Bubeck    Chen   Li   
Jamieson et al    Carpentier   Locatelli   
It
was shown recently in  Carpentier   Locatelli    that
the results of  Audibert   Bubeck    are optimal  The
key difference from our work is that  in these models  there
is no information leakage among the arms 
There has been   lot of work  Mooij et al    Hyttinen
et al    Eberhardt    Hauser     uhlmann   
Spirtes et al    Pearl      Loh     uhlmann   
Shanmugam et al    on learning casual models from
data and or experiments and using it to estimate causal
strength questions of the counterfactual nature  One notable work that partially inspired our work is  Bottou et al 
  where the causal graph underlying   computational
advertising system  like in Bing  Google etc 
is known
and the primary interest is to  nd out how   change in the
system would affect some other variable 
At the intersection of causality and bandits   Lattimore
et al    is perhaps most relevant to our setting  It studies the problem of identifying the best hard interventions

on multiple variables  among many  provided the distribution of the parents of the target is known under those
interventions  Simple regret bound of order   pT   was
derived  We assume soft interventions that affect the mechanism between    source  node and its parents  far away
from the target  similar to the case of computational advertising considered in  Bottou et al    Further  we
derive the  rst gap dependent bounds  that can be exponentially small in     generalizing the results of  Audibert  
Bubeck    Our formulation can handle general budget
constraints on the bandit arms and also recover the problem
independent bounds of  Lattimore et al     orderwise 
Budget constraints in bandit settings have been explored
before in  Abernethy et al    Slivkins   
In the context of machine learning  importance sampling
has been mostly used to recondition input data to adhere
to conditions imposed by learning algorithms  Sugiyama
et al    Li et al    Zhang   Zhao   

  Problem Setting
In this work  we consider the problem of identifying the
best soft intervention       the one that maximizes the expected value of   certain target variable  The problem setting is best illustrated in Figure   Consider   causal graph
       that speci es directed causal relationships between
the variables    We provide   short introduction to causal
graphs in Section    in the appendix  for the sake of completeness  Let   be   target random variable which is
downstream in the graph    the expected value of this target
variable is the quantity of interest  Consider another random variable   along with its parents pa     We assume
that there are   possible soft interventions  Each soft intervention is   distinct conditional distribution that dictates
the relationship pa           During   soft intervention
                          the conditional distribution of   given its parents is set to Pk    pa     and all
other relationships in the causal graph are unchanged 
It
distributions
Pk    pa     and marginals for pa     for        
are known from past experiments or existing domain
knowledge  We only observe samples of      and pa    
while the rest of the variables in the causal graph may be
unobserved under different interventions  For simplicity
we assume that the variables    pa     are discrete while
the target variable   may be continuous discrete and has
bounded support in     Further  we assume that the
various conditionals       Pk     pa     are absolutely
continuous with respect to each other 
In the case of
discrete distributions  the nonzero supports of these distributions are identical  However  our algorithm can be easily
generalized for continuous distributions on   and pa    
 as in our experiments in Section   In this setting  we

conditional

the

is

assumed

that

Identifying Best Interventions through Online Importance Sampling

are interested in the following natural question  Which of
the   soft interventions yield the highest expected value
of the target        and what is the misidenti cation
error that can be achieved with    nite total budget   for
samples  
Remark   Although we may know apriori the joint distribution of pa     and   under different interventions  how
the change affects another variable   in the causal graph
is unknown and must be learnt from samples  The task is to
transfer prior knowledge to identify the best intervention 

PK

Bandit Setting  The   different soft interventions can
be thought of as the   arms of   bandit problem  Let
the reward of arm   be denoted by       Ek      where
Ek      is the expected value of   under the soft intervention when the conditional distribution of   given its parents
pa     is set to Pk    pa      soft intervention    while
keeping all other things in   unchanged  We assume that
there is only one best arm  Let    be the arm that yields
the highest expected reward and   be the value of the
corresponding expected reward           arg maxk   
and         Let the optimality gap of the kth arm be
de ned as              We shall see that the these
gaps      
   and the relationship between distributions
 Pk    pa      
   are important parameters in the problem  Let   min         
Fixed Budget for Samples  In this paper  we work under
the  xed budget setting of best arm identi cation  Audibert   Bubeck    Let Tk be the number of times the
kth intervention is used to obtain samples  We require that
  be the fraction of times the

   Tk       Let      Tk

kth intervention is played 
Additional Cost Budget on Interventions 
In the context of causal discovery  some interventions require   lot
more resources or experimental effort than the others  We
 nd such examples in the context of online advertisement
design  Bottou et al    Therefore  we introduce two
variants of an additional cost constraint that in uences the
choice of interventions      Dif cult arm budget     Some
arms are deemed to be dif cult  Let        be the
set of dif cult arms  We require that the total fraction of
times the dif cult arms are played does not exceed       
Pk            ii  Cost Budget     This is the most

general budget setting that captures the variable costs of
sampling each arm  Slivkins    We assume that there
is   cost ck associated with sampling arm    It is required
that the average cost of sampling does not exceed   cost
   ck                  ck  along with
the total budget   completely de nes this budget setting  It
should be noted that    is   special case of   
We note that unless otherwise stated  we work with the
most general setting in    We state some of our results

budget    ie PK 

in the setting    for clearer exposition 
Objectives  There are two main quantities of interest 
 Probability of Error  This is the probability of failing to
identify the best soft intervention  arm  Let          be the
arm that is predicted to be the best arm at the end of the experiment  Then the probability of error          Audibert
  Bubeck    Carpentier   Locatelli    is given by 

                        

 Simple Regret  Another important quantity that has been
analyzed in the best arm identi cation setting is the simple
regret  Lattimore et al    The simple regret is given

by          Pk     kP             

  Our Main Results
In this section we provide our main theoretical contributions  In Section   we provide   successive rejects style
algorithm that leverages the information leakage via importance sampling  Then  we provide theoretical guarantees on
the probability of misidenti cation          and simple
regret          for our algorithm in Section   In order
to explain our algorithm and our results formally  we  rst
describe several key ideas in our algorithm and introduce
important de nitions in Section  

  De nitions
Quantifying Information Leakage  Observe that      

Pk      pa       By weighting samples
Ek       Ek hY Pk    pa    
with the correct ratio of conditional probabilities Pk  and
Pk  it is possible to use samples under intervention   
to estimate the mean under intervention    This is the basic
idea behind importance sampling which has been used in
similar settings  Lattimore et al    Bottou et al   
However  the variance of this estimator depends on the ratio
of Pk  to      We use   speci   fdivergence measure
between Pk  and      to quantify the variance  This
fdivergence measure can be calculated analytically or estimated from empirical data  without access to full distributions  using techniques like that of    erezCruz   
De nition   Let     be   nonnegative convex function
such that         For two joint distributions pX         
and qX           and the associated conditionals  the conditional fdivergence Df  pX   kqX     is given by 
Df  pX   kqX       EqX   hf  pX         
qX            
Recall that Pi is the conditional distribution of node  
given the state of its parents pa     Thus  Df  PikPj  is
the conditional fdivergence between the conditional distributions Pi and Pj  Now we de ne some logdivergences

Identifying Best Interventions through Online Importance Sampling

that are are crucial in the our analysis 
De nition    Mij measure  Let          exp   
We de ne the following logdivergence measure  Mij  
    log    Df PikPj             
Aggregating Interventional Data  We describe an ef 
cient estimator of Ek              that combines available samples from different arms  This estimator adaptively weights samples depending on the relative Mij measures  and also uses clipping to control variance by introducing bias  The estimator is given by  
Suppose we obtain    samples from arm         Let the
total number of samples from all arms be denoted by  
Further  let us index all the samples by             and
Tk           be the indices of all the samples collected
from arm    Let Xj    denotes the sample collected for
random variable   under intervention    at time instant   
Finally  let Zk  Pj       Mkj  We denote the estimate

    is an indicator of the level of con dence

of    by     
desired  Our estimator is 

    
   

 
Zk

KXj Xs Tj

 

Mkj

Yj   

Pk Vj   pa        
Pj Vj   pa        
 

  Pk Vj   pa        
Pj Vj   pa             log Mkj   

In other words      
  is the weighted average of the clipped
samples  where the samples from arm   are weighted by
 Mkj and clipped at   log Mkj  The choice of   controls the biasvariance tradeoff which we will adaptively
change in our algorithm  This adaptive clipping is crucial
in deriving gap dependent bounds 

  Algorithm
Now  we describe our main algorithmic contribution   Algorithm   and   Algorithm   starts by having all the  
arms under consideration and then proceeds in phases  possibly rejecting one or more arms at the end of each phase 
At every phase  Estimator   with   phase speci   choice
of the   parameter       controlling bias variance tradeoff 
is applied to all arms under consideration  Using   phase
speci   threshold on these estimates  some arms are rejected at the end of each phase    random arm among the
ones surviving at the end of all phases is declared to be the
optimal  We now describe the duration of various phases 
Recall the parameters     Total sample budget available
and     average cost budget constraint  Let        

 We note that the authors in  Lattimore et al    discuss the
possibility of   multiphase approach  where clipper levels could
change across phases  However  they do not pursue this direction
 no speci   algorithm or results  as their objective is to derive gap
independent bounds  minimax regret 

dlog     log  pTe  Let log      Pn
     We
will have an algorithm with       phases numbered by
                Let     be the total number of samples in phase   We set            llog       for    
          Note thatPl             Let   be the set
of arms remaining to compete with the optimal arm at the
beginning of phase   which is continuously updated 
Let     be the samples allocated to arm   in phase  
Let    
    be the vector consisting of entries      The
   
vector of allocated samples          
    is decided by Algo 
   
rithm   Intuitively  an arm that provides suf cient information about all the remaining arms needs to be given more
budget than other less informative arms  This allocation depends on the average budget constraints and the relative log
divergences between the arms  De nition   Algorithm  
formalizes this intuition  and ensures that variance of the
worst estimator  of the form   for the arms in   is as
good as possible  quanti ed in Theorem   and Lemma  

Algorithm   Successive Rejects with Importance Sampling      SRISv    Given total budget   and the cost
budget    along with ci    picks the best arm 
  SRIS   Mkj     
         
  Form the matrix     RK   such that Akj    
  for       to       do
 

      ALLOCATE                Algo 
   
   
rithm  
Use arm        times and collect samples
       pa    
for      do

Let  Yk be the estimator for arm   as in   calculated with  Mkj        and the samples
obtained in Line  

 

 
 

 

Mkj

 Yk 

return  the arm in   

end for
 
Let  YH   arg maxk  
 
                 YH    Yk      
if         then
 
 
end if
 
  end for
  return    randomly chosen arm from   
Remark   Note that Line   uses only the samples acquired in that phase  Clearly    natural extension is to modify the algorithm to reuse all the samples acquired prior to
that step  We give that variation in Algorithm   We prove
all our guarantees for Algorithm   We conjecture that the
second variation has tighter guarantees  dropping   multiplicative log factor  in the sample complexity requirements 

The inverse of the maximal objective of the LP in Algorithm   acts as effective standard deviation uniformly for

Identifying Best Interventions through Online Importance Sampling

Algorithm   Successive Rejects with Importance Sampling      SRISv    Given total budget   and the cost
budget    along with    picks the best arm 
  Identical to Algorithm   except for Line   where all
samples acquired in all the phases till that Line is used 

all the estimators for the remaining arms in    It is analogous to the variance terms appearing in Bernstientype concentration bounds  refer to Lemma   in the appendix 
De nition   The effective standard deviation for budget   and arm set        is de ned as        
        from Algorithm   with input   and arm set   
Algorithm   Allocate   Allocates   given budget   among
the arms to reduce variance 
  ALLOCATE             
  Solve the following LP 

 

     

 

           max
KXi 

ci       and

    

    

min
   

 

KXj 

            

  Assign              

  Theoretical Guarantees
We state our main results as Theorem   and Theorem  
which provide guarantees on probability of error and simple regret respectively  Our results can be interpreted as  
natural generalization of the results in  Audibert   Bubeck 
  when there is information leakage among the arms 
This is the  rst gap dependent characterization 
Theorem    Proved formally as Theorem   Let  
    Let   be the effective standard deviation as
min
    
in De nition   The probability of error for Algorithm  
satis es 

              log  exp 
     pT   Here 

 

   Hlog        

when the budget for the total number of samples is   and

     max
    

log           
      blog   

  

and             log   

of arms whose distance from the optimal arm is roughly at
most twice that of arm   

      is the set

 

 

 

       

Comparison with the result in  Audibert   Bubeck 
  Let                          
the set
of arms which are closer to the optimal than arm    Let
     max
  The result in  Audibert   Bubeck 
    
  can be stated as  The error in  nding the optimal

arm is bounded as               exp     

log       

Our work is analogous to the above result  upto poly log
factors  except that    appears instead of     In Section   
 in the appendix  we demonstrate through simple examples that         can be signi cantly smaller than
         the corresponding term in       above  even

when there are no average budget constraints  Moreover 
our results can be exponentially better in the presence of
average budget constraints  examples in Section    Now
we present our bounds on simple regret in Theorem  
Theorem    Proved formally as Theorem   Let   be
the effective standard deviation as in De nition   The simple regret of Algorithm   when the number of samples is  
satis es 

         

 
pT
    Xk   

   pT

                    pTo  
   log   

    exp 

   Hklog      

 

 

log   

     

            and

      blog   

Here   Hk   max       
            log   
Comparison with the result in  Lattimore et al   
In  Lattimore et al    the simple regret scales as
  pT   and does not adapt to the gaps  We provide gap
dependent bounds that can be exponentially better than that
of  Lattimore et al     when      are not too small and
the  rst term in   is zero  More over our bounds generalize to gap independent bounds that match   pT   Further details are provided in Section     in the appendix 
We defer the theoretical analysis to Section    Theorem  
and Theorem   are subparts of our main technical theorem
 Theorem   which is proved in Section   

  Empirical Validation
We empirically validate the performance of our algorithms
in two real data settings  In Section   we study the empirical performance of our algorithm on the  ow cytometry
dataset  Sachs et al    In Section   we apply our
algorithms for the purpose of model interpretability of the
Inception Deep Network  Szegedy et al    in the context of image classi cation  In Section    in the appendix 

Identifying Best Interventions through Online Importance Sampling

we include more experiments  In the appendix we empirically show that our divergence metric is fundamental and
replacing it with other divergences is suboptimal 

  Flow Cytometry DataSet
The  ow cytometry dataset  Sachs et al     extensively used for validating causal inference algorithms  consists of multivariate measurements of protein interactions
in   single cell  under different experimental conditions
 soft interventions  Our experiments are aimed at identifying the best intervention among many  given some ground
truth about the causal graph  For  this purpose we borrow
the causal graph from Fig      in  Mooij   Heskes   
 shown in Fig      and consider it to be the ground truth 
Parametric linear models have been popularly used for
causal inference on this dataset  Meinshausen et al   
Cho et al    We      GLM gamma model  Hardin
et al    between the activation of each node and its parents in Fig     using the observational data  In Section   
 in the appendix  we provide further details showing that
the sampled distributions in the  tted model are extremely
close to the empirical distributions from the data  The soft
interventions signifying the arms are generated by changing the distribution of   source node pkc in the GLM  The
objective is to identify the intervention that yields the highest output at the target node erk  We provide empirical results for two sets of interventions at the source node  Both
these experiments have been performed with   arms each
representing different distributions at pkc 
Budget Restriction  The experiments are performed in the
budget setting    where all arms except arm   are deemed
to be dif cult  We plot our results as   function of the total
samples     while the fractional budget of the dif cult arms

    is set to  pT   Therefore  we havePk  Tk   pT  

This essentially belongs to the case when there is   lot of
data that can be acquired for   default arm while any new
change requires signi cant cost in acquiring samples 
Competing Algorithms  We test our algorithms on different problem parameters and compare with related prior
work  Audibert   Bubeck    Lattimore et al   
The algorithms compared are     SRISv  Algorithm   introduced in Section   The divergences  Df Pi Pj  are
estimated from sampled data using techniques from    erezCruz     ii SRISv  Algorithm   as detailed in Section    iii  SR  Successive Rejects Algorithm from  Audibert   Bubeck    adapted to the budget setting  The
division of the total budget   into    phases is identical 
 The activations of the node erk have been scaled so that the
mean is less than one  Note that the marginal distribution still has
an exponential tail  and thus does not strictly adhere to our boundedness assumption on the target variable  However  the experiments suggest that our algorithms still perform extremely well 

while the individual arm budgets are decided in each phase
according to the budget restrictions   iv  CR  Algorithm  
from  Lattimore et al    The optimization problem for
calculating the mixture parameter   is not ef ciently solvable for general distributions and budget settings  Therefore  the mixture proportions are set by Algorithm  
In these experiments  the budget restrictions imply that arm
  can be pulled much more than the other arms  Intuitively
the divergences of the arms from arm   as well as the gap
  de nes the hardness of identi cation  Fig     represents
  dif cult scenario where the divergences Mk      for
many arms  large divergences imply low information leakage  and        small   increases hardness  In Fig    
 easier scenario  the divergences Mk      for most arms
while the gap is same as before  We see that SRISv  outperforms all the other algorithms by   large margin  especially in the low sample regime 

  Interpretability of Inception Deep Network
In this section we use our algorithm for model interpretation of the pretrained Inceptionv  network  Szegedy
et al    for classifying images  Model Interpretation
essentially addresses 
 why does   learning model classify in   certain way  which is an important question for
complicated models like deep nets  Ribeiro et al   
When an RGB image is fed to Inception  it produces an
ordered sequence of   labels       drums   sunglasses 
and generally the top  labels are an accurate description
of the objects in the image  To address interpretability  we
segment the image into   number of superpixels segments
 using segmentation algorithms like SLIC  Achanta et al 
  and infer which superpixels encourage the neural
net to output   certain label  henceforth referred to as labelI       drum  in topk             and to what extent 
Given   mixture distribution over the superpixels of an image  Figure       few superpixels are randomly sampled
from the distribution with replacement  Then   new image
is generated where all other superpixels of the original image are blurred out except the ones selected  This image
is then fed to Inception  and it is observed whether labelI appears within the topk labels    mixture distribution
is said to be   good interpretation for labelI if there is  
high probability that labelI appears for an image generated
by this mixture distribution  To empirically test the goodness of   mixture distribution  we would generate  using
this mixture distribution    number of random images  and
determine the fraction of images for which labelI appears 
  large fraction indicates that the mixture distribution is  
good interpretation of labelI 
Motivated by the above discussion  we generate   large
number   of mixture distributions  with the goal of

Identifying Best Interventions through Online Importance Sampling

pip 

pip 

plcg

pkc  Source 

pka

akt

mek

jnk

  

erk  Target 

raf

 

 

 

 

 

 

 

 

  below SR

  below CR

SRISv 
SRISv 
SR
CR

SR
SRISv 
SRISv 
CR

  below SR

  below CR

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    Causal Graph for Cytometry
Data  Mooij   Heskes   

    Simple Regret when divergences
Mk   are high and gap   is small

    Simple Regret when divergences
Mk   are low and gap   is small

Figure   Performance of various algorithms on the cytometry data under different scenarios  The results are averaged over the course of
  independent experiments  The total sample budget   is plotted on the xaxis  The budget for all arms other than arm   is constrained
to be less than pT   Here       The performance improvement is especially evident in the low sample regime  For  example in    
SRISv  provides more than   improvement over CR and SR  It is signi cant in biological data where number of samples is generally
small  SR does not use information leakage while the static clipper in CR cannot adapt to high divergences like in    

 nding the one that best interprets labelI  To highlight the
applicability of our algorithm  we allow images to be generated for only   of these mixture distributions  in other
words  most of the mixture distributions cannot be directly
tested  Nevertheless  we determine the best from among the
entire collection of mixture distributions  counterfactuals 
Speci cally in our experiments  we consider the image in
Figure     partition it into   superpixels  and generate images from mixture distributions by sampling   superpixels
 with replacement  We generate   arm distributions
which lie in the  dimensional simplex but have sparse
support  sparsity of   in our examples  The support of
these distributions are randomly generated by techniques
like markov random walk  encourages contiguity  random
choice  etc  as detailed in Section    in the appendix 
We are only allowed to sample using   different set of  
arms that are dense distributions chosen uniformly at random from the  dimensional simplex  The distributions
are generated in   manner which is completely agnostic to
the image content  The total sample budget      is  
Figure   shows images in which the segments are weighted
in proportion to the optimal distribution  obtained by
SRISv  for the interpretation of three different labels  This
showcases the true counterfactual power of the algorithm 
as the set of arms that can be optimized over are disjoint
from the arms that can be sampled from  Moreover the
sample budget is less than the number of arms  This is
an extreme special case of budget setting    We see that
our algorithm can generate meaningful interpretations for
all the labels with relatively less number of runs of Inception  Even sampling   times from each of the arms to be
optimized over would require     runs of Inception for
  single image and label  while we use only   runs by
leveraging information leakage 
Conclusion  In this paper we provide the  rst gap depen 

    Original Image

    InterpretationDrums

    InterpretationTigerCat

    InterpretationSunglasses

Figure   Interpretation for different top labels for the image in
    Image courtesy  cat  The best mixture distribution generates
an image where the highlighted superpixels are most indicative of
  label  For example  in     we see the drums highlighted  while in
       different mixture distribution  sunglasses are in the focus 

dent error and simple regret bounds for identifying the best
soft intervention at   source node     the one that maximizes
the expected value of   downstream target node  These
bounds are   generalization of the classical best arm identi cation bounds  Audibert   Bubeck    when there
is information leakage among the arms  We test our algorithm empirically on the  ow cytometry dataset and also
use it for interpretability of Inceptionv  deep net 

Acknowledgements
This work is partially supported by NSF grants CNS 
  CCF       
ARO grants   NF    NF 
  NF  and the US DoT supported DSTOP
Tier   University Transportation Center 

Identifying Best Interventions through Online Importance Sampling

References
Image source  http bit ly hviIwP  Accessed 

 

Abernethy  Jacob  Chen  Yiling  Ho  ChienJu  and Waggoner  Bo  Lowcost learning via active data procurement  In Proceedings of the Sixteenth ACM Conference
on Economics and Computation  pp    ACM 
 

Achanta  Radhakrishna  Shaji  Appu  Smith  Kevin  Lucchi  Aurelien  Fua  Pascal  and   usstrunk  Sabine  Slic
superpixels  Technical report   

Audibert  JeanYves and Bubeck    ebastien  Best arm
identi cation in multiarmed bandits 
In COLT th
Conference on Learning Theory  pp       

Bennett  George  Probability inequalities for the sum of
independent random variables  Journal of the American
Statistical Association     

Blalock  Hubert    Causal models in the social sciences 

Transaction Publishers   

Bonneau  Richard  Facciotti  Marc    Reiss  David   
Schmid  Amy    Pan  Min  Kaur  Amardeep  Thorsson  Vesteinn  Shannon  Paul  Johnson  Michael    Bare 
  Christopher  et al    predictive model for transcriptional control of physiology in   free living cell  Cell 
   

Bottou    eon  Peters  Jonas  Candela  Joaquin Quinonero 
Charles  Denis Xavier  Chickering  Max  Portugaly 
Elon  Ray  Dipankar  Simard  Patrice    and Snelson  Ed 
Counterfactual reasoning and learning systems  the example of computational advertising  Journal of Machine
Learning Research     

Carpentier  Alexandra and Locatelli  Andrea  Tight  lower 
bounds for the  xed budget best arm identi cation bandit
problem  arXiv preprint arXiv   

Chen  Lijie and Li  Jian  On the optimal sample comarXiv preprint

plexity for best arm identi cation 
arXiv   

Cho  Hyunghoon  Berger  Bonnie  and Peng  Jian  Reconstructing causal biological networks through active
learning  PloS one       

Eberhardt  Frederick  Almost optimal intervention sets for
causal discovery 
In Proceedings of  th Conference
in Uncertainty in Arti cial Intelligence  UAI  pp   
   

Gabillon  Victor  Ghavamzadeh  Mohammad  and Lazaric 
Alessandro  Best arm identi cation    uni ed approach
to  xed budget and  xed con dence 
In Advances in
Neural Information Processing Systems  pp   
 

Hardin  James William  Hilbe  Joseph    and Hilbe 
Joseph  Generalized linear models and extensions  Stata
press   

Hauser  Alain and   uhlmann  Peter  Two optimal strategies for active learning of causal networks from interventional data  In Proceedings of Sixth European Workshop
on Probabilistic Graphical Models   

Hyttinen  Antti  Eberhardt  Frederick  and Hoyer  Patrik 
Experiment selection for causal discovery  Journal of
Machine Learning Research     

Jamieson  Kevin    Malloy  Matthew  Nowak  Robert   
and Bubeck    ebastien  lil ucb  An optimal exploration
algorithm for multiarmed bandits  In COLT  volume  
pp     

Joffe  Michael  Gambhir  Manoj  ChadeauHyam  Marc 
and Vineis  Paolo  Causal diagrams in systems epidemiology  Emerging themes in epidemiology     

Kaufmann  Emilie  Capp    Olivier  and Garivier  Aur elien 
On the complexity of best arm identi cation in multiarmed bandit models  The Journal of Machine Learning
Research   

Kemmeren  Patrick  Sameith  Katrin  van de Pasch 
Loes AL  Benschop  Joris    Lenstra  Tineke    Margaritis  Thanasis    Duibhir  Eoghan  Apweiler  Eva 
van Wageningen  Sake  Ko  Cheuk    et al  Largescale genetic perturbations reveal regulatory networks
and an abundance of genespeci   repressors  Cell   
   

Krouk  Gabriel  Lingeman  Jesse  Colon  Amy Marshall 
Coruzzi  Gloria  and Shasha  Dennis  Gene regulatory
networks in plants  learning causality from time and perturbation  Genome biology     

Lattimore  Finnian  Lattimore  Tor  and Reid  Mark   
Causal bandits  Learning good interventions via causal
inference  In Advances In Neural Information Processing Systems  pp     

Li  Lihong  Chu  Wei  Langford  John  and Wang  Xuanhui 
Unbiased of ine evaluation of contextualbandit based
news article recommendation algorithms 
In Proceedings of the fourth ACM international conference on Web
search and data mining  pp    ACM   

Identifying Best Interventions through Online Importance Sampling

Loh  PoLing and   uhlmann  Peter  Highdimensional
learning of linear causal networks via inverse covariance
estimation  Journal of Machine Learning Research   
   

Sugiyama  Masashi  Krauledat  Matthias  and      zller 
KlausRobert  Covariate shift adaptation by importance
weighted cross validation  Journal of Machine Learning
Research   May   

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 
Going deeper with convolutions 
In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition  pp     

Zhang  Tong and Zhao  Peiling  Stochastic optimization
with importance sampling for regularized loss minimization  arXiv preprint arXiv   

Meinshausen  Nicolai  Hauser  Alain  Mooij  Joris    Peters  Jonas  Versteeg  Philip  and   uhlmann  Peter  Methods for causal inference from gene perturbation experiments and validation  Proceedings of the National
Academy of Sciences     

Mooij  Joris and Heskes  Tom  Cyclic causal discovery from continuous equilibrium data  arXiv preprint
arXiv   

Mooij  Joris    Peters  Jonas  Janzing  Dominik  Zscheischler  Jakob  and Sch olkopf  Bernhard  Distinguishing
cause from effect using observational data  methods and
benchmarks  Journal of Machine Learning Research   
   

Pearl  Judea  Causality  Models  Reasoning and Inference 

Cambridge University Press     

Pearl  Judea  Causality  Cambridge university press 

   

  erezCruz  Fernando  Kullbackleibler divergence estimation of continuous distributions  In Information Theory 
  ISIT   IEEE International Symposium on  pp 
  IEEE   

Ribeiro  Marco Tulio  Singh  Sameer  and Guestrin  Carlos 
Why should   trust you  Explaining the predictions of
any classi er  In Proceedings of the  nd ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining  pp    ACM   

Sachs  Karen  Perez  Omar  Pe er  Dana  Lauffenburger 
Douglas    and Nolan  Garry    Causal proteinsignaling
networks derived from multiparameter singlecell data 
Science     

Shanmugam  Karthikeyan  Kocaoglu  Murat  Dimakis 
Alexandros    and Vishwanath  Sriram  Learning causal
graphs with small interventions  In Advances in Neural
Information Processing Systems  pp     

Slivkins  Aleksandrs  Dynamic ad allocation  Bandits with

budgets  arXiv preprint arXiv   

Spirtes  Peter  Glymour  Clark  and Scheines  Richard 
Causation  Prediction  and Search    Bradford Book 
 

SplawaNeyman  Jerzy  Dabrowska  DM  Speed  TP  et al 
On the application of probability theory to agricultural
experiments  essay on principles  section   Statistical
Science     

