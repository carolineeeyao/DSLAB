On The Projection Operator to   Threeview Cardinality Constrained Set

Haichuan Yang   Shupeng Gui   Chuyang Ke   Daniel Stefankovic   Ryohei Fujimaki   Ji Liu  

Abstract

The cardinality constraint is an intrinsic way to
restrict the solution structure in many domains 
for example  sparse learning  feature selection 
and compressed sensing  To solve   cardinality constrained problem 
the key challenge is
to solve the projection onto the cardinality constraint set  which is NPhard in general when
there exist multiple overlapped cardinality constraints  In this paper  we consider the scenario
where the overlapped cardinality constraints satisfy   Threeview Cardinality Structure  TVCS 
which re ects the natural restriction in many applications  such as identi cation of gene regulatory networks and taskworker assignment problem  We cast the projection into   linear programming  and show that for TVCS  the vertex
solution of this linear programming is the solution for the original projection problem  We
further prove that such solution can be found
with the complexity proportional to the number
of variables and constraints  We  nally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method 

  Introduction
The cardinality constraint is an intrinsic way to restrict
the solution structure in many real problems  for example 
sparse learning  Olshausen   Field    feature selection  Zhang    and compressed sensing  Candes et al 
  The generic cardinality constrained optimization
can be expressed as

min
  Rp
subject to

     
kwgk    sg       

   

   

 University of Rochester  Rochester  NY  USA  NEC 
Cupertino  CA  USA  Correspondence to  Haichuan Yang
   yang rochester edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

where   is the optimization variable    is an index subset
of                wg is the subvector of   indexed
by    kwgk  denotes the cardinality of the subvector      
the number of nonzeros in wg    is the hyper set of all
prede ned groups  and          is the upper bound vector   sg     refers to the upper bound of the sparsity over
group    Objective   is the loss function which could be
de ned with different form according to the speci   application  The problem   refers to   nonconvex optimization
 NPhard  due to the cardinality constraint  Some ef cient
iterative methods such as IHT  Yuan et al    CoSaMP
 Needell   Tropp    GradMP  Nguyen et al   
and their variants can guarantee to solve the original problem under some mild conditions    key component in all
of these methods is the projection operator

           arg min
      

kw   vk 

 

to the conwhere        denotes the feasible set
straint     While in some special case  for example 
        the projection is trivial  it is quite challenging 
especially when   includes multiple overlapped index sets
 even NPhard in some cases 
In this paper  we consider the scenario where the overlapped cardinality constraints     satisfy   Threeview
Cardinality Structure  TVCS 
 Threeview Cardinality Structure
De nition  
 TVCS  For        the hyper set   consisting of subsets
of     admits the TVCS structure if the following conditions
are satis ed 

  There exists   partition       and    such that    

            
          
  All element sets in    have no overlap 
  All element sets in    have no overlap 

This de nition basically requires that   can be partitioned
into three hyper sets       and    and overlaps can only
happen between element sets in different hyper sets     is

On The Projection Operator to   Threeview Cardinality Constrained Set

 

 

 

 
 
 

 

 
 
 
 

 

 

 

 

 

 

 
 
 
 
 

 

 

 
 

 

 

 

 

 

 
 
 
 
 

 

 
 
 
 
 

 

 

 

 

 
 
 
 
 

 

 
 
 
 
 

 

Figure   Two examples of groups for TVCS model  The  rst
model has                                       
       The second model organizes elements as matrix     
  Each row and column is   group  and             
                                   

usually used to restrict the overall sparsity  Figure   provides two examples of   for TVCS 
The TVCS model is motivated from some important applications  for example  in recommendation  taskworker
assignment  and bioinformatics 

  Online recommendation  Suppose we want to recommend   certain number of books  among   books 
to   customer   corresponding to the    based sparsity constraint  Among the selected books  we want to
maintain some diversities   the recommended books
by the same author should not exceed   certain number     based sparsity constraint  and about the same
topic should not exceed   certain number either    
based sparsity constraint  One can refer to the top
graph in Figure      is grouped by authors and    is
grouped by topics 

  Taskworker assignment  Suppose we have   bunch
of tasks and workers  and we want to assign the tasks
to workers  For example  in crowdsourcing  we usually assign several different workers to each task since
we want to use the answers from multiple workers
to improve the accuracy  On the other hand  each
worker is usually assigned to multiple tasks so there
is    many to many  relationship in this assignment 

The goal is to pursue the optimal assignment under
  certain criteria in crowdsourcing  while satisfying
some restrictions  For example  the total assignments
should be bounded by the total budget  corresponding
to    the total cost of assignments to   single worker
cannot exceed   certain threshold  corresponding to
   and the total cost of assignments on   single task
cannot exceed   certain threshold  corresponding to
   Let   be the assignment matrix  and its rows
are indexed by workers and the columns are indexed
by tasks  These constraints can be illustrated by the
bottom graph in Figure  

  Identi cation of gene regulatory networks  The essential goal of identifying gene regulatory network is
to identify   weighted directed graph  which can be
represented by   square matrix   with          
elements in total where   is the number of vertices 
  sparse network constraint is to restrict the indegree
and outdegree for each vertex  which corresponds to
the sparsity in each row and column of    

To solve the TVCS constrained projection   we show
an interesting connection between the projection and   linear programming  LP  that the vertex solution to this linear
programming is an integer solution which solves the original problem 
To  nd an integer solution to such LP ef ciently  we formulate it into   feasibility problem  and further an equivalent quadratic convex optimization  By using the rounding
technique  we can avoid solving the exact solution of this
LP problem  We propose an iterative algorithm to solve
it and each iteration can be completed in linear time  We
also show that the iterate linearly converges to the optimal
point  Finally  the proposed TVCS model is validated by
the synthetic experiment and two important and novel applications in identi cation of gene regulatory networks and
task assignment problem of crowdsourcing 

  Related Works
Recent years have witnessed many research works in the
 eld of structured sparsity and groupbased sparsity  Yuan
  Lin   introduced the group LASSO  which pursues groupwise sparsity that restricts the number of groups
for the selected variables  Jenatton et al    construct
  hierarchical structure over the variables and use group
LASSO with overlapped groups to solve it  Exclusive
LASSO  Zhou et al    Kong et al    was proposed for the exclusive group sparsity which can be treated
as relaxing our cardinality constraints to convex regularizations  In  Kong et al    the authors discussed the
overlapping situation and tried to solve the problem using
convex relaxation  which is different from our approach 

On The Projection Operator to   Threeview Cardinality Constrained Set

Besides the aforementioned works  some proposed more
general models to cover various sparsity structures  Bach
et al    extended the usage of   norm relaxation to
several different categories of structures  And recently 
another generalization work  El Halabi   Cevher   
proposed convex envelopes for various sparsity structures 
They built the framework by de ning   totally unimodular penalty  and showed how to formulate different sparsity
structures using the penalty  The work above concentrated
on using convex relaxation to control the sparsity 
Besides using convex relaxation  there are also some works
focusing on projectionbased methods  When the exact
projection operator was provided  Baraniuk et al   
extended the traditional IHT and CoSaMP methods to general sparsity structures  In this work  the authors also introduced the projection operator for block sparsity and tree
sparsity  Cevher et al    investigated cluster sparsity
and they applied dynamic programming to solve the projection operator for their sparsity model  Hegde et al   
introduced    spike trains  signal model  which is also related to exclusive group sparsity  Its groups always have
consecutive coordinates  and each group cannot contain
more than one nonzero element  To solve the projection
problem of their model  they showed the basic feasible solutions of the relaxed linear programming  LP  are always
integer points  In our work  we also use LP to solve the projection problem  But our model de nes the group structure
differently and aims at different applications 
In addition  there are some works for the cases without an
ef cient exact projection operator  Hegde et al       
Nguyen et al    This is meaningful since the projection operator for complex structured sparsity often involves
solving complicated combinatorial optimization problems 
Hegde et al      discussed how to guarantee convergence if using approximate projection in IHT and CoSaMP
for compressive sensing  They proved that the convergence
needs    head approximation  to project the update  gradient  before applying it  Hegde et al      proposed
  general framework to formulate   series of models as  
weighted graph  and designed an ef cient approximate projection operator for the models  Nguyen et al    applied the approximate projectionbased IHT and CoSaMP
to general convex functions and stochastic gradients 

  Preliminary  GradMP and IHT

Frameworks

cardinality constrained problem  Other methods like hard
thresholding pursuit  HTP  also follows similar steps and
has been shown to be effective both empirically and theoretically  Yuan et al    The procedures of IHT and
GradMP for our model are shown in Algorithms   and  
where supp  is the support set of the argument vector 
Therefore  one can see that the ef ciency of both algorithms relies on the computation of the gradient and the
projection  To avoid the expensive computation of the gradient  GradMP and IHT can be extended to the stochastic
versions  Nguyen et al    by assigning   the stochastic
gradient at the gradient computation step 
Both Algorithms   and    and their stochastic variants 
guarantee some nice properties  the iterate converges to  
small ball surrounding the true solution at   linear rate under certain RIPtype conditions  Nguyen et al    and
the radius of such ball converges to zero when the number
of samples goes to in nity 

Algorithm   Iterative Hard Thresholding 
Input  Sparsity parameter   
Result  Problem solution wt 
Initialize         
while stop criterion is not met do

    rf  wt   
zt   wt       
wt          zt   
         

  Gradient computation
  Gradient descent
  Projection

end

Algorithm   Gradient Matching Pursuit 
Input  Sparsity parameter   
Result  Problem solution wt 
Initialize         
while stop criterion is not met do

    rf  wt 
    supp         
        supp wt   
zt   arg minsupp           
optimization
wt          zt   
         

end

  Gradient computation

  Subspace selection
  Subspace

  Projection

This section brie   reviews two commonly used algorithm
frameworks to solve the cardinality constrained optimization  
iterative hard thresholding  IHT   Yuan et al 
  Nguyen et al    and gradient matching pursuit  GradMP   Nguyen et al       the general
version of CoSaMP  Needell   Tropp    for solving

  common component in Algorithms   and   is the projection operator  If all the groups except     in   do not
overlap each other  the projection problem can be easily
solved by sequential projections  Yang et al    But
for those cases involving overlapped groups  it is generally
challenging to solve them ef ciently 

On The Projection Operator to   Threeview Cardinality Constrained Set

  Projection Operator
This section introduces how to solve the essential projection step  Note that the projection onto   nonconvex set
is NPhard in general  By utilizing the special structure
of TVCS  we show that the projection can be solved ef 
ciently  Due to the page limitation  all proofs are provided
in the supplementary material 

  LP Relaxation
Firstly  we can cast the projection problem   to an equivalent integer linear programming problem  ILP  according
to Lemma  
Lemma   The projection problem   is equivalent to the
following integer linear programming problem  ILP 

 

max

hv  xi
subject to Ax    

        

 

where    is applying elementwise square operation on
vector      is           matrix which is de ned as 

   
  

 

where               whose rows represent the indicator vector of each group        and   
Each row in   corresponds to one group   from    For
example  Cij     if the jth coordinate is in the ith group 
otherwise Cij     The  rst row   corresponds to the
overall sparsity        
It is NPhard to solve an ILP in general  One common way
to handle such ILP is making   linear programming  LP 
relaxation  In our case  we can use   box constraint    
     to replace the integer constraint          

 

max

hv  xi
subject to Ax    

        

 

However  there is no guarantee that   general ILP can be
solved via its LP relaxation  because the solution of the relaxed LP is not always integer  Although one can make  
rounding to the LP solution and acquire   integer solution 
such solution is not guaranteed to be optimal  or even feasible  to the original ILP 
Fortunately  due to the special structure of our TVCS
model  we  nd that its relaxed LP has some nice properties
which make it possible to get the optimal solution of the
ILP ef ciently  The following theorem reveals the relationship between the ILP problem and the relaxed LP problem 

Theorem   Given   satisfying     CS  all the vertices of
the feasible set to   are integer points  Furthermore  there
is an optimal solution on the vertex that solves the ILP  

This theorem suggests that  nding   vertex solution of the
relaxed LP can solve the original projection problem onto
  TVCS    The proof basically shows that matrix    for
TVCS  is   totally unimodular matrix  Papadimitriou  
Steiglitz    We provide the detailed proof in the supplementary material 

  Linearly Convergent Algorithm for Projection

Operator onto TVCS

To  nd   solution on the vertex  one can use the Simplex
method  Although Simplex method guarantees to  nd an
optimal solution on the vertex and could be very ef cient in
practice  it does not have   deterministic complexity bound 
In the IHT and GradMP algorithms  projection operator
is only   subprocedure in one iteration  Hence  we are
usually supposed to solve lots of instances of problem  
Simplex might be ef cient practically  but its worst case
may lead to exponential time complexity  Papadimitriou
  Steiglitz    In this section  the integer solution to
the linear programming can be found within the complexity proportional to the number of variables and constraints 

Equivalent Feasibility Problem Formulation  The dual
of LP problem   can be written as 

min

 
subject to

       yi
                   

 

Since the duality gap of LP is zero  combining the primal
LP   and dual LP   we can formulate an equivalent
problem      
the feasibility problem over the following
constraints 

 nd     

subject to        yi   hv  xi

             
       
  
 

           

Iterative Algorithm  The feasibility problem with linear
constraints above is equivalent to the following optimiza 

On The Projection Operator to   Threeview Cardinality Constrained Set

tion problem 

min
   

 
 

        yi   hv  xi 
 
 
                    
   Ax       
 
 

subject to                
where     is the elementwise hinge operator       it transforms each element zi to max zi   
This is   convex optimization problem with   quadratic objective and box constraints  We adopt the projected gradient descent to solve this problem  and show it converges
linearly 
Theorem   For the optimization problem with the form

min

 

          Az          kBz   bk 

subject to      

Theorem   we have the linear convergence rate       so
the number of iterations we need is
 

    log 

 kz       
iterations 

 

 kz    

Therefore  we claim that we can obtain the solution    by
rounding after log 
Secondly  we show that the computation complexity in
each iteration is linear with dimensionality   and the
amount of groups     Since each column of   contains at
most   nonzero elements  the complexity of the matrix multiplications in computing the gradient of   is          
Together with other computation  the complexity for each
iteration is          
  Empirical Study
This section will validate the proposed method on both synthetic data and two practical applications  crowdsourcing
and identi cation of gene regulatory networks 

where          Cz      the projected gradient descent
algorithm zt      zt    rf  zt  has   linear convergence rate with some        depending on   and   

kzt    Pz zt      kzt   Pz zt   

where Pz  is the projection onto the optimal solution set 
Notice that the objective function   in Theorem   is not
necessarily strongly convex  which means the well recognized linear convergence conclusion from the strong convexity is not applicable here 
Theorem   mainly applies Hoffman   Theorem  Hoffman 
  to show that   is an optimal strongly convex function
 Liu   Wright    This leads to   linear convergence
rate 
    where   is the HoffThe convergence rate        
man constant  Hoffman    that depends on      and
is always positive    is the Lipschitz continuous gradient
constant  More details are included in the supplementary
materials 
To show the complexity of this algorithm  we  rstly count
how many iterations we need  Since we know that we can
just make   rounding  to the result xt when we attain kxt 
          Let            represent all the variables
in   Because kzt       kzt        kxt      we
can do the rounding safely when kzt           where
      are the optimal points of this problem  According to
 Acute readers may notice that the convergent point may be on
the face of the polytope in some cases instead of vertex  However 
we can add   small random perturbation to ensure the optimal
point to be vertices with probability  

  Linear Regression and Classi cation on Synthetic

Data

In this section  we validate the proposed method with linear regression objective and squared hinge objective  classi cation  on synthetic data  Let     Rpp pp be   matrix     and    are de ned as groups with all rows and all
columns respectively  The linear regression loss is de ned
asPn
  hXi  wi   yi  and the squared hinge loss is de 
 ned asPn
   max    yihXi  wi  where   is the total
number of training samples  Xi and yi are the features and
label of the ith sample respectively 
In the linear regression experiment  the true model     
Rpp pp is generated from the following procedure  generate   random vector and apply the projection operator to
get   support set which satisfy our sparsity constraints  the
elements of positions in support set are drawn from standard normal distribution    is  xed as   and   is gradually increased  The group sparsity upper bounds sg for
       and        are uniformly generated from the integers in the range pp  The overall sparsity upper bound
is set by   min Pg   
sg  Each Xi   is an
pp   pp        Gaussian random matrix  yi is generated
from yi   hXi   wi   ei  where ei is the        Gaussian
random noise drawn from       We compare the
proposed methods to bilevel exclusive sparsity with nonoverlapped groups  rowwise or columnwise   Yang et al 
  overall sparsity  Needell   Tropp    and exclusive LASSO  Kong et al    For fairness we project
the  nal result of all the compared methods to satisfy all
constraints  All the experiments are repeated   times and
we use the averaged result  We use selection recall and successful recovery rate to evaluate the performance  Selection

sg Pg   

On The Projection Operator to   Threeview Cardinality Constrained Set

recall is de ned as  supp   supp        wk  where    is the
optimization result  Successful recovery rate is the ratio of
the successful feature selection      supp      supp     
to the total number of repeated experiments  In Figure  
we can observe that our model with all sparsity constraints
always have the best performance  While the performance
of exclusive LASSO and our method is comparable when
the number of samples are very limited  our method outperforms exclusive LASSO when the number of samples
increases 
For classi cation experiments  we use the same settings of
sparsity with linear regression  Here we set       and
change   from   to   The true model    and feature matrices are generated by the same way as the linear
regression experiment  The class label yi       is
got by yi   signhXi   wi  Besides the selection recall  we
also compare the classi cation error  In Figure   we can
see that the superiority of our method is even more signi 
cant in the classi cation experiment  Although the overall
sparsity has the lowest selection recall  it still has   similar
classi cation error with the methods that consider row or
column groups 

Feature selection recall with  sample

ours
row
col
overall
exclusive LASSO

 

 

 
 

 
 
 
 

 

 

Successful Recovery rate with  sample

ours
row
col
overall
exclusive LASSO

 

 

 

 

 

 

 

 

 

 
 

 
 
 
 
 
 
 
 

 

 

 

 sample

 

 

 
 

 

 

 sample

 

 

    Selection recall 

    Successful recovery rate 

Figure   Selection recall and successful recovery rate for least
square loss 

Feature selection recall with  sample

Classification error with  sample

 

 

 

 

 

 

 

 
 

 
 
 
 

ours
row
col
overall

ours
row
col
overall

 

 

 

 

 
 
 
 
 

 

 
 

 
 

 
 
 
 
 
 
 
 
 

 

 

 

 sample

 

 

 

 

 

 

 sample

    Selection recall 

    Classi cation error 

Figure   Selection recall and classi cation error for squared
hinge loss 

  Application in Crowdsourcing
This section applies the proposed method to the workertask assignment problem in crowdsourcing  Take the im 

age labeling task as an example  Given   workers and  
images  each image can be assigned to multiple workers
and each worker can label multiple images  The predicted
label for each image is decided by all the labels provided
by the assigned workers and the quality of each worker on
the image  The goal is to maximize the expected prediction
accuracy based on the assignment  Let           
be the assignment matrix       Xij     if assign the ith
worker to jth task  otherwise Xij               
is the corresponding quality matrix  which is usually estimated from the golden standard test  Ho et al    The
whole formulation is de ned to maximize the average of
the expected prediction accuracy over   tasks over   TVCS
constraint 

 

 
 

max

 

subject to

Eacc          

mXj 
nXi 
Xij   sworker   
mXj 
Xi  

Xij   stask   
Xij   stotal            

where Eacc  is the expected prediction accuracy  sworker
is the  worker sparsity       the largest number of assigned
workers for each task  and stask is the  task sparsity      
each worker can be assigned with at most stask tasks  and
stotal is the total sparsity to control the budget       the maximal number of assignment  In image labeling task  we assume that each image can only have two possible classes
and the percentage of images in each class is one half  We
use the Bayesian rule to infer the predicted labels given the
workers  answer  Here we consider the binary classi cation task  Let yj       be the true label of the jth task
and  yj be the prediction given labels by selected workers 
    

 yj  

 

if   yj                  yj             
otherwise

where  Yij is the ith worker   predication on jth task  Set
   contains the indices of the selected workers for jth
task       Xij            and Xi             
Then Eacc           can be de ned in the following 
Eacc               yj     yj      yj     yj    
By this way  the expected accuracy will not be continuous 
so we use smooth function to approximate the expected accuracy and adopt the stochastic gradient with the proposed

On The Projection Operator to   Threeview Cardinality Constrained Set

projection operator to optimize it  Due to the space limitation  the detailed derivation of the objective formulation
can be found in the supplemental material 
We conduct experiment for crowdsourcing task assignment
on synthetic data  Speci cally  we generate the quality matrix   from uniformly random distribution with interval
    The prior probability   yj     and   yj    
are set as   for all the tasks 
To avoid evaluating the expectation term  we apply the
stochastic iterative hard thresholding framework  Nguyen
et al    Each iteration we get     yj     and     yj    
by sampling based on            Yij    yj       Qij 
    Yij    yj       Qij  Then we can get   stochastic
gradient based on the sampled     
Besides the proposed formulation   we evaluate the random assignment algorithm and the Qbased linear programming  Ho et al    The random assignment algorithm widely used in practice is the most straightforward
approach  given the total assignment budget stotal and the
restrictions  sworker and stask  for workers and tasks  randomly assign tasks to the workers  The Qbased linear programming uses the linear combination of Qij over   to evaluate the overall accuracy on task   for simpler formulation 
In addition  it does not consider the restriction on tasks 
thus it may assign lots of workers to   dif cult task  To
make   fair comparison  the task restriction is added into
this method  To get the assignment result which satis es
the task and worker restriction  we use our projection operator in the other methods too 
We evaluate the experiments on different value of
stask  sworker by setting them as different ratios of the total number of tasks and workers  The overall sparsity is
set by the same way as in Section   To measure the
performance  we compare the sampled expected accuracy 
The samples            are independent to the samples used
in training  Figure   shows the comparison of the expected accuracy of the three approaches  We can observe
that the accuracy increases with larger ratio       more assignments  The random assignment strategy needs more
assignments to get the same accuracy compared with the
other two methods 

  Application in Identi cation of Gene Regulatory

Networks

In this section  we apply the projection operator to the identi cation of gene regulatory networks  GRN 

    dif cult  task means that all workers  qualities are low on

this task 

Accuracy with Ratio

Accuracy with Ratio

 

 

 

 

 

 

 

 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 

 

 

 

 

 

ours
random
Qbased

ours
random
Qbased

 

 

 
Ratio

 

 

 

 

 
Ratio

 

 

                 

                 

Figure   Expected accuracy of crowdsourced classi cation 

Background  Gene regulatory network represents the relations between different genes  which plays important
roles in biological processes and activities by controlling
the expression level of RNAs  There is   wellknown
biological competition named DREAM challenge about
identifying GRN  Based on the time series gene expression data which are RNAs  level along time sequence 
contestants are required to recover the whole gene network
of given size  One popular way to infer GRN is to utilize
the sparse property of GRN       one gene in the network
is only related to   small number of genes and we already
know that there exists no relationship between some genes 
Therefore  the amount of edges connecting to one vertex is
far less than the dimension of the graph  It is   practical
case of rowwise and columnwise sparsity for matrix  We
could apply the projection operator to constrain the number of edges related to each vertex to identify the whole
network  Recently  the dynamic Bayesian network  DBN 
 Zou   Conzen    is supposed to be an effective model
to recover GRNs  The RNAs  level of all genes in GRN at
time   is stored in gene expression vector xt   RN  where
each entry corresponds to one gene respectively and   is
the number of genes in GRN  Hence  We de ne the total
amount of time points in the experiment as     Gene activity model is usually assumed to be

xt      xt   et 

                 

where     RN   is the covariance matrix of GRN and
et   RN is Gaussian white noise  Then the difference of
RNA levels between time points       and         yt    
RN is de ned as follows 
yt     xt    xt      xt   et 
                 
where            is the true sparse Nby   matrix 
Therefore  the GRN is only considered between different
genes and we eliminate edges whose start and end vertex
are the same  We de ne that                 yT     
RN    and                 xT    RN    The
objective function is

        

 
 kY      Xk 

   

 
 

  Xt 

  xt  xt     xtk 

Our Method

GENIE 

TIGRESS

CLR

PCC

ARACNE
MINET

On The Projection Operator to   Threeview Cardinality Constrained Set

SN

 
 
 
 
 
 
 

SP

 
 
 
 
 
 
 

ACC

 
 
 
 
 
 
 

Fmeasure

 
 
 
 
 
 
 

MCC

 
 
 
 
 
 
 

AUC

 
 
 
 
 
 
 

Table   Performance evaluation of our method and six other stateof art methods

Timecourse Gene Expression Data  To evaluate our
method  we employ GeneNetWeaver  Marbach et al   
Schaffter et al    the of cial DREAM Challenge tool
for timeseries expression data generation  With typical
gene network structure and ordinary differential equation
 ODE  models  GeneNetWeaver will produce the timecourse gene expression data at prespeci ed time points 
In the simulation studies  we control the size of gene network to be       vertexes and the gene expression data
are generated under   Gaussian white noise 
The network is shown in Figure   In this Figure  it is clear
that one gene only has   few connections to other genes 
Therefore  the GRN is sparse and we are able to restrict the
indegree and outdegree of every vertex by representing
the network as   matrix and controlling the sparsity within
each row and column 

Performance evaluation  Six commonlyused criteria
are considered to measure the performance       sensitivity  SN  speci city  SP  accuracy  ACC  Fmeasure 
Matthews correlation coef cient  MCC  and the Area Under ROC Curve  AUC 

TP

SN  

ACC  

SP  
TP   TN

TP   FN  
TP   FP   TN   FN  

TN

TN   FP  

Fmeasure  

    SN   SP
SN   SP  

 

TP   TN   FP   FN

MCC  

  TP   FP TP   FN TN   FP TN   FN 

where TP and TN denote the true positive and true negative  and FP and FN denote the false positive and false
negative  respectively  With these criteria  we compare the
performance of our method with six representative algorithms  including PCC  ARACNE  Margolin et al   
CLR  Faith et al    MINET  Meyer et al    GENIE   HuynhThu et al    TIGRESS  Haury et al 
  The results are summarized in Table   Our method
outperforms other six stateof art methods  the AUC of our
method achieve   higher which is far more than other
methods    out of   different measure show that our method
has signi cant advantage compared to other algorithms 

Figure   This gene regulatory network contains   vertexes
which are represented by blue circles  One edge starts at gene
  and ends at gene   if gene   has in uence on gene  

  Conclusion
This paper considers the TVCS constrained optimization 
motivated by the intrinsic restrictions for many important
applications  for example  in bioinformatics  recommendation system  and crowdsourcing  To solve the cardinality
constrained problem  the key step is the projection onto the
cardinality constraints  Although the projection onto the
overlapped cardinality constraints is NPhard in general 
we prove that if the TVCS condition is satis ed the projection can be reduced to   linear programming  We further
prove that there is an iterative algorithm which  nds an integer solution to the linear programming within time comR   where   is the distance from
plexity           log 
the initial point to the optimization solution and      
is the convergence rate  We  nally use synthetic experiments and two interesting applications in bioinformatics
and crowdsourcing to validate the proposed TVCS model 

 

Acknowledgements
This project is supported in part by the NSF grant CNS 
  and the NEC fellowship 

On The Projection Operator to   Threeview Cardinality Constrained Set

References
Bach  Francis  Jenatton  Rodolphe  Mairal  Julien  Obozinski  Guillaume  et al  Structured sparsity through convex
optimization  Statistical Science     

Baraniuk  Richard    Cevher  Volkan  Duarte  Marco   
and Hegde  Chinmay  Modelbased compressive sensing  Information Theory  IEEE Transactions on   
   

Candes  Emmanuel    Romberg  Justin    and Tao  Terence  Stable signal recovery from incomplete and inaccurate measurements  Communications on pure and
applied mathematics     

Cevher  Volkan  Indyk  Piotr  Hegde  Chinmay  and Baraniuk  Richard    Recovery of clustered sparse signals from compressive measurements  Technical report 
DTIC Document   

El Halabi  Marwa and Cevher  Volkan    totally unimodular view of structured sparsity 
In Proceedings of the
Eighteenth International Conference on Arti cial Intelligence and Statistics  pp     

Faith  Jeremiah    Hayete  Boris  Thaden  Joshua   
Mogno  Ilaria  Wierzbowski  Jamey  Cottarel  Guillaume  Kasif  Simon  Collins  James    and Gardner 
Timothy    Largescale mapping and validation of escherichia coli transcriptional regulation from   compendium of expression pro les  PLoS biol     
 

Haury  AnneClaire  Mordelet  Fantine  VeraLicona 
Paola  and Vert  JeanPhilippe  Tigress  trustful inference of gene regulation using stability selection  BMC
systems biology     

Hegde  Chinmay  Duarte  Marco    and Cevher  Volkan 
Compressive sensing recovery of spike trains using  
structured sparsity model  In SPARS Signal Processing with Adaptive Sparse Structured Representations 
 

Hegde  Chinmay  Indyk  Piotr  and Schmidt  Ludwig  Approximation algorithms for modelbased compressive
sensing  Information Theory  IEEE Transactions on   
     

Hegde  Chinmay  Indyk  Piotr  and Schmidt  Ludwig   
nearlylinear time framework for graphstructured sparsity 
In Proceedings of the  nd International Conference on Machine Learning  ICML  pp   
   

Ho  ChienJu 

Jabbari  Shahin  and Vaughan 

Jennifer Wortman  Adaptive task assignment for crowdsourced classi cation  In Proceedings of The  th International Conference on Machine Learning  pp   
 

Hoffman  Alan    On approximate solutions of systems of
linear inequalities  In Selected Papers Of Alan   Hoffman  With Commentary  pp     

HuynhThu  Vn Anh 

Irrthum  Alexandre  Wehenkel 
Louis  and Geurts  Pierre  Inferring regulatory networks
from expression data using treebased methods  PloS
one       

Jenatton  Rodolphe  Mairal  Julien  Obozinski  Guillaume 
and Bach  Francis  Proximal methods for hierarchical
sparse coding  Journal of Machine Learning Research 
 Jul   

Kong  Deguang  Fujimaki  Ryohei  Liu  Ji  Nie  Feiping 
and Ding  Chris  Exclusive feature learning on arbitrary
structures via   norm  In Advances in Neural Information Processing Systems  pp     

Liu  Ji and Wright  Stephen    Asynchronous stochastic
coordinate descent  Parallelism and convergence properties  SIAM Journal on Optimization   
 

Marbach  Daniel  Schaffter  Thomas  Mattiussi  Claudio 
and Floreano  Dario  Generating realistic in silico gene
networks for performance assessment of reverse engineering methods  Journal of computational biology   
   

Margolin  Adam    Nemenman  Ilya  Basso  Katia  Wiggins  Chris  Stolovitzky  Gustavo  Favera  Riccardo   
and Califano  Andrea  Aracne  an algorithm for the
reconstruction of gene regulatory networks in   mammalian cellular context  BMC bioinformatics   Suppl
     

Meyer  Patrick    La tte  Frederic  and Bontempi  Gianluca  minet  Ar bioconductor package for inferring large
transcriptional networks using mutual information  BMC
bioinformatics     

Needell  Deanna and Tropp  Joel    Cosamp  Iterative signal recovery from incomplete and inaccurate samples 
Applied and Computational Harmonic Analysis   
   

Nguyen  Nam  Chin  Sang  and Tran  Trac      uni ed
iterative greedy algorithm for sparsity constrained optimization   

On The Projection Operator to   Threeview Cardinality Constrained Set

Nguyen  Nam  Needell  Deanna  and Woolf  Tina  Linear convergence of stochastic iterative greedy algorithms
with sparse constraints  arXiv preprint arXiv 
 

Olshausen  Bruno   and Field  David    Sparse coding with
an overcomplete basis set    strategy employed by   
Vision research     

Papadimitriou  Christos   and Steiglitz  Kenneth  Combinatorial optimization  algorithms and complexity 
Courier Corporation   

Schaffter  Thomas  Marbach  Daniel  and Floreano  Dario 
Genenetweaver  in silico benchmark generation and performance pro ling of network inference methods  Bioinformatics     

Yang  Haichuan  Huang  Yijun  Tran  Lam  Liu  Ji  and
Huang  Shuai  On bene ts of selection diversity via
bilevel exclusive sparsity  In Computer Vision and Pattern Recognition  CVPR    IEEE Conference on 
IEEE   

Yuan  Ming and Lin  Yi  Model selection and estimation in
regression with grouped variables  Journal of the Royal
Statistical Society  Series    Statistical Methodology 
   

Yuan  Xiaotong  Li  Ping  and Zhang  Tong  Gradient hard
thresholding pursuit for sparsityconstrained optimization  In Proceedings of The  st International Conference on Machine Learning  pp     

Yuan  Xiaotong  Li  Ping  and Zhang  Tong  Exact recovery of hard thresholding pursuit  In Advances in Neural
Information Processing Systems  pp     

Zhang  Tong  On the consistency of feature selection using greedy least squares regression  Journal of Machine
Learning Research   Mar   

Zhou  Yang  Jin  Rong  and Hoi  Steven CH  Exclusive
lasso for multitask feature selection  In AISTATS  volume   pp     

Zou  Min and Conzen  Suzanne      new dynamic
bayesian network  dbn  approach for identifying gene
regulatory networks from time course microarray data 
Bioinformatics     

