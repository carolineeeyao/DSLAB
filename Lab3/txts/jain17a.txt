Scalable Generative Models for Multilabel Learning with Missing Labels

Vikas Jain     Nirbhay Modhe     Piyush Rai  

Abstract

We present   scalable  generative framework for
multilabel learning with missing labels  Our
framework consists of   latent factor model for
the binary label matrix  which is coupled with
an exposure model to account for label missingness       whether   zero in the label matrix is
indeed   zero or denotes   missing observation 
The underlying latent factor model also assumes
that the lowdimensional embeddings of each label vector are directly conditioned on the respective feature vector of that example  Our generative framework admits   simple inference procedure  such that the parameter estimation reduces to   sequence of simple weighted leastsquare regression problems  each of which can be
solved easily  ef ciently  and in parallel  Moreover  inference can also be performed in an online fashion using minibatches of training examples  which makes our framework scalable for
large data sets  even when using moderate computational resources  We report both quantitative and qualitative results for our framework on
several benchmark data sets  comparing it with  
number of stateof theart methods 

  Introduction
Multilabel learning  Gibaja   Ventura      is the
problem of assigning to an object   subset of labels from  
potentially very large label vocabulary  Prabhu   Varma 
  Jain et al    Babbar   Sch olkopf   
In
contrast to binary or multiclass classi cation  in multilabel learning  each example is associated with   binary
label vector  potentially very large  denoting the presence absence  relevance irrelevance  of each label  Multilabel learning has applications in several domains such as
computer vision  Wang et al    computational adver 

 Equal contribution

 Department of Computer Science and
Enginerring  IIT Kanpur  Kanpur   UP  India  Correspondence to  Vikas Jain  vikasj iitk ac in  Nirbhay Modhe
 nirbhaym iitk ac in  Piyush Rai  piyush cse iitk ac in 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tising and recommender systems  Prabhu   Varma   
Jain et al    etc 
Several stateof theart methods for multilabel learning are
based on certain structural assumptions on the binary label matrix  Some of the key structural assumptions that
have been used in prior work include lowrank assumption  Yu et al    locally lowrank assumption  Bhatia et al    and lowrank plus sparse assumption  Xu
et al    and clusters topics of labels assumption  Ciss  
et al    Rai et al    Models based on these assumptions are broadly dubbed as embedding based methods for multilabel learning and offer two key advantages 
  The relatedness correlation among labels can be easily
modeled captured  and   the label vector for each example can be represented as   lowdimensional embedding 
which faciliates developing computationally scalable models for multilabel learning    more detailed discussion of
prior work is provided in the Related Work section 
Despite the considerable recent interest and progress on
the problem of multilabel learning  Yu et al    Bhatia
et al    Wang et al    Ciss   et al      number
of important issues still remain  One of such issues  especially for the embdding based methods  is the ambiguity
regarding the zeros vs unobserved  missing  entries in the
binary label vector of each example  Since  in practice  the
true value   for only   small subset of all the labels can
be obtained  the zeros in the label vector do not necessarily
represent negative labels    typical heuristic employed by
multilabel learning algorithms is to simply treat all such
the zeros in the label vector as are true negatives  Yu et al 
  Another heuristic is to assign different weights to
the zeros and ones in the binary label matrix  Yu et al 
  which is inspired by matrix factorization based collaborative  ltering models that learn from implicit  binary 
feedback data  Hu et al    However    more principled strategy to address this issue is highly desirable 
Another important desideratum is scalability  especially in
the case of extreme multilabel learning problems  Prabhu
  Varma    Jain et al    Babbar   Sch olkopf 
  which are characterized by   massive number of labels  features  and examples  Although   number of recent
multilabel learning models have been proposed that can
scale to largescale problems  these models usually require

Scalable Generative Models for Multilabel Learning with Missing Labels

large computational resources to truly scale to massive data
sets  Babbar   Sch olkopf    Bhatia et al    Jain
et al    Moreover  most of the scalable multilabel
learning algorithms only operate in batch setting and are
usually not designed to work  Prabhu   Varma    Bhatia et al    Jain et al    in online settings with
continuous stream of training examples 
In this paper  we present   scalable  generative framework
for multilabel learning  that not only bring to bear the
modeling  exibility of probabilistic  generative models for
the multilabel learning problem  Kapoor et al    Rai
et al    but is also designed to handle the abovementioned challenges in   principled way  Our framework
is based on   latent factor model for the binary label matrix 
and has the following distinguishing aspects    It naturally handles the issue of missing vs negative labels via  
principled generative model with   exposure model  Liang
et al    for the label matrix    It is accompanied by
  simple and scalable inference procedure  both via Gibbs
sampling and via fast point estimation  and   Inference
can also be easily performed in an online fashion  enabling
us to apply it on largescale problems  even when using
moderate computational resources 

  The Model
In the multilabel learning problem  we assume that we are
given   training examples                 xN   yN   with
xn   RD and yn                          We will denote                 xN    RN   to be the feature matrix
and                 yN           to be the label matrix  Given training data        the goal in multilabel
learning is to learn   model that can predict the label vector
          for   new test input      RD 
Note that an entry yn cid      in the label matrix   may not
necessarily mean   negative label but could simply mean
that this label is missing  and its true value could be   or  
As we shall show  our generative model can infer the missingness of   label yn cid      by associating another binary
latent variable    cid   called exposure variable  These exposure variables will be incorporated in   latent factor model
 Sec    for the label matrix   and are jointly learned
along with the rest of the model parameters 

  An Exposurebased Latent Factor Model for the

Binary Label Matrix

We model the binary label matrix   using   latent factor model  Speci cally  we assume that each training example                 is associated with   latent factor
un   RK and each label  cid                is associated
with   latent factor   cid    RK  We further condition un
on the feature vector xn   RD of example   by as 

suming that the prior distribution of un is conditioned on
xn  as   un xn       un Wxn   
  IK  Here     
            wK cid    RK   which denotes the matrix of regression weights that map the feature vector xn to the mean
of the Gaussian prior on un  We further assume   zeromean Gaussian prior     cid         cid   
  IK  on label
latent factors   cid   cid                 Note that  although we do
not consider it here  our model can also be easily extended
to incorporate label features  if available  by conditioning
Gaussian prior on   cid  on those label features  in the same
manner we condition the prior on un on input features 

Figure   Our generative model in plate notation  Note  Hyperparameters not shown for brevity 

The complete generative story for each label yn cid  of the binary label matrix   is given by
un xn      un Wxn   
  cid         cid   
  IK 
   cid    Bernoulli   cid 
yn cid   

Bernoulli cid yn cid   cid 

    cid cid   

if    cid     

  IK 

 
 
 

 

 cid 

 

if    cid     

where           exp    denotes the logistic function  Note that we have associated   binary exposure latent variable    cid  with each label yn cid  such that    cid     
implies that yn cid  is   because it is missing  not exposed 
and    cid      implies that yn cid  is exposed  and could be  
or   depending on the outcome of the Bernoulli draw  In
Eq      denotes   pointmass at zero  which means that  if
   cid      then yn cid  is zero with probability   Otherwise  we
draw the observed label yn cid  from   Bernoulli distribution
as yn cid    Bernoulli yn cid   cid 
    cid  Note that  effectively 
each yn cid  is being modeled using   mixture of two distributions     Bernoulli with probability given by the sigmoid
   cid 

    cid  and   pointmass at  

    cid cid         cid   yn cid       

yn cid       cid Bern cid yn cid   cid 

Note that the latent variable    cid  decides which of the two
distributions from this mixture generates yn cid  Figure  
shows our model in the plate notation  Also note that if
yn cid      then    cid      with probability   and therefore    cid 
only needs to be inferred for entries for which yn cid     

Scalable Generative Models for Multilabel Learning with Missing Labels

The generative model speci ed in Eq   has two additional parameters                  wK cid    RK   which
denotes the matrix of regression weights that map each
input feature vector xn to the corresponding latent factor
un   RK  and   probability parameter    cid        which
denotes the probability of the label yn cid  being exposed  but
note that yn cid  can be   or   depending on the outcome of
Bernoulli yn cid   cid 
    cid  We refer to    cid  as the exposure
probability of label  cid  for example   
We assume each regression weight vector wk to have  
Gaussian prior       wk      wk   
  ID  Note that
the spherical covariance of this prior can also be replaced
by   more  exible diagonal covariance  which will give the
model ability to perform feature selection 
For the exposure probability    cid  we consider two types of
priors  In the  rst case  we simply assume    cid     cid     
which means that the probability that   label  cid  is observed
is the same for all the examples       the label exposure for
the label  cid  is global  not example speci    In this case 
we assume   Beta prior on  cid        cid    Beta   
In the second case  we assume access to some contexual
information  often available in applications such as recommender systems  that we may have for each examplelabel
pair      cid  in form of some given covariates    cid    RM  
Given these covariates  we model the label exposure prob 
   cid  where     RM is   vector of
ability as    cid     
regression coef cients  We assume   Gaussian prior on  
              
  Inference
Although the generative model speci ed in Eq    is not
readily conjugate because the logisticBernoulli likelihood
is not conjugate to the Gaussian prior on the latent factors  we can leverage dataaugmentation techniques  Polson et al    to make the model locally conjugate  This
enables us to develop   simple Gibbs sampling algorithm
for doing inference in our model  The conjugacy also allows us to design an online expectation maximization  EM 
algorithm  Capp     Moulines    which enables us to
apply our model on largescale problems 
We handle the nonconjugate logisticBernoulli likelihood
using the   olyagamma augmentation technique  Polson
et al    which is based on the following identity

  IM  

 cid 

 cid   

exp cid cid      

 exp  

   exp  

 

    exp      
where            and      PG      denotes
the   olyagamma distribution  Polson et al    This
identity allows us to write any likelihood of the form
 exp  
 exp         Bernoulli  binomial  negativebinomial 
as   Gaussian distribution  when conditioned on   PG random variable     PG      Speci cally  using PG augmentation  we can write the logisticBernoulli likelihood

 cid 

from Eq    as   Gaussian when conditioned on    cid   
    cid  In particular     cid      cid 
PG    cid 
    cid  conditioned on
   cid  becomes   Gaussian
    cid   cid    exp
   cid   cid     
 

 
where    cid    yn cid      This likelihood with the Gaussian
priors on the latent factors un and   cid  results in Gaussian
posteriors on un and   cid  When doing EM  this also leads to
subproblems that are like least square regression problems 

   cid 
  cid 

 cid 

  Gibbs Sampling

Using the PG augmentation  we can derive the posterior
distributions of all the latent variables in our model  and
perform Gibbs sampling for doing inference in our model 
Due to conjugacy  the inference updates are straightforward
to derive as are summarized below 
Sampling    cid  Note that if yn cid      then    cid      with
probability one  and therefore need not be inferred  For
yn cid      we sample    cid  from the posterior
    cid           cid   cid 
    cid 
    cid               cid     

 
 
Sampling    cid  For the case when    cid     cid      with
Beta    prior on each  cid  the posterior will be

    cid    Beta   

  cid 

   cid            cid 

  

  

   cid 

 

 cid 

 cid     cid   cid   cid   cid 

 un    cid  
given by  un    un cid  

Note that  if we parameterize each    cid  as    cid     
   cid 
where    cid  is the interaction feature vector for the examplelabel pair  and the regresssion weight   is assumed to have
  Gaussian prior  the model is not conjugate  However 
using the PG augmentation allows us to easily derive  
closedform Gaussian posterior for  
Sampling un  Given the PG variables          cid  
 cid 
and the other latent variables  the posterior of un will be
un      un un    un   where the covariance is given by
 cid     uIK  and the mean is
 cid     cid   cid   cid     uWxn  Note
that if   label  cid  is inferred as not exposed for example   
        cid      it does not contribute to the update of un 
Sampling   cid  Given  cid       cid  
variables  the posterior   cid  will be   cid         cid   cid 

where covariance    cid     cid  
     cid   cid  

   and the other latent
     cid 
     vIK 
      cid   cid unu cid 
      cid   cid un  Note that
and the mean    cid 
if an example   is inferred as not exposed to label  cid      
   cid      it does not contribute to the update of   cid 
Sampling    Each row  wk  
   of the regression
weights matrix                 wK cid    RK   will have
  Gaussian posterior given by wk      wk wk
   wk  
where covariance  wk      cid      wID  the mean
 wk

   wk    cid    and                 uN     RK    

Scalable Generative Models for Multilabel Learning with Missing Labels

  Scalable Inference via EM and Online EM

Although the Gibbs sampler  Sec    is easy to derive and
implement in practice  sampling tends to be slow in practice and convergence may be slow  We therefore present
an online expectation maximization algorithm  Capp    
Moulines    for doing ef cient inference in our model 
We  rst show the batch EM updates for our model parameters and then describe the online EM algorithm which can
process the training data in small minibatches of examples 
and results in faster convergence in practice 

  THE EM ALGORITHM

The EM algorithm for our model alternates between computing the expectations of the local latent variables  namely
the   olyagamma variables    cid  and the binary exposure
latent variables    cid  in the   step  and then using these expectations to estimate the other model parameters un    cid 
   and exposure probabilities    cid  in the   step 
The   Step  The   step involves computing the expectations of the latent variables    cid  and    cid  given the current values of the other model parameters un    cid     and
   cid  estimated in the previous   step  The   step update
equations are given below 

  Expectations of   olyagamma variables    cid       cid 
are known to be available in closed form  Scott   Sun 
  and are given by
   cid        cid   cid   

 cid     cid 

 cid 

 

tanh

 

   cid 

 

    cid  is computed using the estimates
  Expectations of each of the binary exposure variables

where    cid      cid 
of un and vm from the previous   step 
   cid       cid  are given by
pn cid        cid   cid   

   cid   cid 

   cid   cid           cid 

 

The   Step  Given the expectations of the latent variables computed in the   step  the   step maximizes the
following expected complete data loglikelihood plus logprior terms  which we denote as              where
 cid     and        cid       cid 
     un  
   cid       cid   cid 

          cid  

 cid 

    cid 

                  
 

pn cid 

  cid 

 

log Bernoulli pn cid   cid      

 un   Wxn 

    

   cid           

log Beta   cid   

 

 cid 

  cid 

Note that the  rst term in the objective function given in
Eq    is due to the logistic likelihood transformed into  
Gaussian  using PG augmentation  This term is akin to
  weighted least squares objective where each label being

 cid 
  cid 

  cid 

   cid 

  cid 
 cid 

  

 cid    cid 
 cid cid  

associated with   weight pn cid        cid   cid  Intuitively  in
the  rst term  the contribution of each label yn cid  to the loglikelihood gets modulated based on its expected exposure 
Maximizing                     each of the model parameters             xing the rest  yields closedform
updates for each of these  The updates are as follows 
  Estimating each of the latent factors  un  
weighted ridgeregression problem with solution

   is  

 cid 

 cid    cid 
where  un    cid  

un    un

 cid 

pn cid   cid   cid     uWxn

 
 cid     uIK  Note
   are all independent of

 cid  pn cid   cid   cid   cid 

that the updates for  un  
each other and are easily parallelizable 

  Estimating each of the label latent factors    cid  
  weighted ridgeregression problem with solution

 cid  is

 cid 

 

 cid 

  

  cid       cid 

pn cid   cid un
   pn cid   cid unu cid 
where    cid   
 
     vIK
Again  note that the updates for  vn  
 cid  are all independent of each other and are easily parallelizable 
  Estimating the regression weight matrix   is equivalent to solving   vectorvalued linear regression problem un   Wxn      with the following updates

  cid       cid      wID   cid   

 
Note that solving Eq    exactly requires inverting  
      matrix which will be expensive for large   
However  the EM algorithm does not require solving
for   exactly in each   step  We therefore solve
for   ef cient using gradient based methods  such
as conjugategradient  CG  method  Bertsekas   
which allows us to also leverage the sparsity in the
feature matrix    Typically    small number of CG
iterations are suf cient in practice 

 cid 

 cid   

   cid  

   pn cid     
             

  Given pn cid  from the   step  the updates for    cid  for the
case when    cid     cid      is simply the MAP solution
 
For the other case when each    cid  in modeled as    cid   
   cid  with   Gaussian prior on   estimating  
 
reduces to solving   regression problem with the training data being    cid  pn cid       cid  where    cid  is the
given feature vector for the inputlabel pair     cid  and
pn cid  is estimated in the   step  Ignoring the prior term
 equivalent to  cid  regularizer on   we can estimate  
iteratively using gradientdescent updates

         
   

 

 cid 

   cid    pn cid   cid 

 

 cid 

  cid 

where   denotes the learning rate 

Scalable Generative Models for Multilabel Learning with Missing Labels

  ONLINE EM

The EM algorithm described in Section   is more ef 
cient than the Gibbs sampler described in Section   It is
also highly parallelizable since the updates for     
   and
   cid  
 cid  can be easily parallelized  and solve for   ef 
ciently using CG updates  However  it is   batch procedure
and requires going over the entire training data in every iteration  For largescale multilabel learning problems  which
are characterized by large       and    the batch setting
may not be feasible in practice  especially when having access to moderate computational resources and storage 
We therefore present an ef cient online version of the EM
algorithm for our model which allows it to scale up to
massivesized data sets even on machines with moderate
hardware  As we show in our experiments  this enables us
to apply our model to be run ef ciently on massive data
sets       one of the data sets we experiment with has more
than    examples with about    features per example 
even on   standard laptop with very moderate hardware 
The online EM algorithm works by maintaining suf cient
statistics of all the model parameters and updates these
suf cient statistics with every minibatch of data  For
each minibatch of training examples  the   step computes
the relevant expectations associated with these observations and then uses the expectations to update the suf cient
statistics of the parameters to be estimated in the   step 
 cid  
For example  noting that the suf cient statistics for updating the label latent factors   cid        are given by    
   pn cid   cid unu cid 
   pn cid   cid un  we
can update   and   using   small minibatch containing Nb
examples as  xn  yn Nb

   vIK and    cid  

   as follows

                      tA new 
                      tb new 

 
 

where   new     cid Nb
  new     cid Nb

   pn cid   cid unu cid 

     vIK  and
   pn cid   cid un are computing using only the
current minibatch  The suf cient statistics of the other
model parameters can also be updated in the same manner  Here    is   decaying learning rate  or   forgetting
factor  which also acts as   tradeoff between the contribution from the old suf cient statistics computed thus far
and the suf cient statistics contribution from the new minibatch of data  We set               with        and   to
be close to    Capp     Moulines   
  PREDICTION

Given   new test input    we  rst predict its latent factor
     RK as Wx  and then predict each entry of its label
vector    as     cid      cid       cid 
If we are only
interested in the top few labels  fast search methods such as
maximum inner product search  Fraccaro et al    can
be used to reduce the computational cost at test time 

    cid 

  Related Work
  prominent line of work on multilabel learning has been
based on models that learn   lowdimensional embedding
of the label vectors  Chen   Lin    Yu et al    Rai
et al    Bhatia et al    Note that this amounts to
assuming that the label matrix is lowrank 
Since many realworld data sets have   large number of
rare labels  sometimes the lowrank assumption may not
be appropriate  To address this issue   Bhatia et al   
proposed   method which assumes the label matrix to be
locally lowrank  One way to impose this assumption is to
learn embeddings that only try to preserve distances in  
small neighborhood of each example  Another approach to
handle the rare labels is to assume that the label matrix is  
sum of   lowrank and   sparse matrix  Xu et al   
Note that our latent factor model is equivalent to imposing
  lowrank assumption on the label matrix  and is therefore similar in spirit to the labelembedding approaches 
However  unlike the existing labelembedding based approaches  our generative framework has   principled mechanism to handle infer the unobserved labels  Moreover 
none of the existing labelembedding methods can work
in online fashion  and scaling up these methods to largescale problems requires large computational resources  In
addition  our model readily allows incorporating the label
features  if available  by   simple modi cation to the prior
on the label latent factors 
Apart from the labelembedding based multilabel learning methods 
treebased methods for multilabel learning  Agrawal et al    Prabhu   Varma    Jain
et al    are also popular due to being fast at test time 
especially when the number of labels is large  However 
these models usually have high training costs and cannot
be trained easily in an online fashion  unlike our model 
On the other hand  for faster predictions at test time  our
framework model can easily be adapting by replacing the
Gaussian prior on the label latent factors   cid  by   von MisesFisher prior  Fraccaro et al    which naturally facilitates using maximum innerproduct search techniques 
without the requirement of any postprocessing 
Among other models to address the missing labels problem in multilabel learning  recently   Kanehira   Harada 
  proposed   ranking based framework for learning
from positive and unlabeled data in the context of multilabel learning  Although this is similar in spirit to our
model in terms of not treating the unobserved labels as zeros  the approach in  Kanehira   Harada    is fundamentally different than ours  Moreover  their setting is not
amenable to online learning  nor does it leverage the lowrank structure of label matrices with   huge number of labels  Other approaches that try to handle missing labels in 

Scalable Generative Models for Multilabel Learning with Missing Labels

clude  Bucak et al    which uses group LASSO adaptation of   multilabel ranking objective  and  Kong et al 
  which learns   model using   positive and unlabeled
 PU  stochastic gradient descent procedure  However  it
works in batch setting  uses stacking to leverage label correlations  and does not scale to large number of labels 
Oneclass matrix factorization  OCMF  is also an approach  Yu et al    to solve the missing labels problem by assigning different  but  xed  weights to the ones
and zeros  In contrast to this method  our generative framework can learn the weight for each label by modeling these
weights as latent variables  In another recent work   Liang
et al    proposed an exposure model for recommender
system problems posed as matrix factorization of implicit
feedback data  Their approach of modeling the exposure
similar in spirit to our framework 
Some of the early works on generative models for multilabel learning problems include models speci cally designed for image annotation problems  Barnard et al   
Feng et al    Other recent attempts on doing multilabel learning in more general problem settings include
models such as Bayesian compressive sensing  Kapoor
et al    and multilabel learning using Bayesian nonnegative matrix factorization  Rai et al    However 
these models do not have   mechanism to distinguish between unobserved and negative labels  have complicated
inference  and do not scale to largescale problems 
Our generative framework is also amenable for various interesting extensions  For example  it can be be extended
to   mixture of latent factor models  which can handle the
situation when the label matrix is not lowrank but   mixture of several lowrank matrices  Note that such an extension would be   fully generative counterpart of the model
in  Bhatia et al    which learns   locally lowrank
model but has to rely on an adhoc clustering step beforehand  which is known to be unstable in practice  Bhatia
et al    Another nice aspect of our framework is that
is naturally allows active learning  Kapoor et al    Vasisht et al    where we can selectively ask for most
informative labels for an unannotated example  Moreover 
our framework is  exible and inference in our model can
be performed in   fully Bayesian manner       MCMC or
variational inference  as well as fast point estimation methods such as  online  EM  that we used in this work 
To summarize  our generative framework offers    exible way to model the label generation mechanism for
realworld multilabel data sets  which most of the existing models currently lack  We can model label missingness observability rigorously under our framework and infer the model parameters easily using   simple inference
procedure  Moreover  the simplicity of the inference procedure makes it easy to design scalable inference algorithms 

such as online EM for our model  which enables updating
the model whenever fresh training data is available  This is
in contrast to some of the other stateof theart multilabel
learning methods  which although scalable  Bhatia et al 
  Prabhu   Varma    Jain et al    are not
suitable to be applied in such online settings 

  Experiments
We evaluate our framework on   number of benchmark
data sets and compare it with several stateof theart methods  Our baselines include both labelembedding methods
as well as treebased methods  The statistics of data sets
we use in our experiments are summarized in Table  

Dataset
Bibtex

Mediamill
Eurlex  
Movielens

RCV

Wikipedia

 
 
 
 
 
 
 

Ntest
 
 
 
 
 
 

 
 
 
 
 

 
 

 
 
 
 
 
 
 

Table   Dataset used for the experiments with their properties 
   number of features     number of labels     number of training examples  Ntest  number of test examples
We report both quantitative results  in terms of label prediction accuracies  as well as some qualitative results  namely
looking at the relationship of empirical label frequencies
and label exposure  Note that the label frequency for  
given label denotes how many examples had this label as
  while label exposure  cid        in general refers to
how popular the label  cid  is 
In our experiments  we compare with the following stateof theart baselines 

  LEML  This is   lowrank embedding based multilabel learning model  Yu et al    LEML assumes the label matrix   to be modeled as     UV
where     XW  LEML considers various types of
loss functions such as squared loss  logistic loss  hinge
loss  etc  Interestingly  note that LEML with logistic
loss can be seen as   special nonprobabilistic case of
our model when also considering        and the
label exposure model turned off 

  BCS  Bayesian Compressive Sensing  BCS  is   generative model  Kapoor et al    for the label vector  It assumes   compressive sensing model for the
label vectors and is essentially   lowrank model 

  FastXML  This is   fast treebased multilabel learning model which uses an ensemble of trees  Prabhu  
Varma   

  PfasterXML  This is an extension of FastXML and
uses propensityweighted scores to improve performance on rare labels  Jain et al   

Scalable Generative Models for Multilabel Learning with Missing Labels

  PDSparse  This model takes   different approach
as compared to labelembedding methods and uses  
marginmaximizing loss for the multilabel learning
problem  Yen et al   

For the baselines  the reported results are either obtained
using publicly available implementations  with the recommended hyperparameter settings  or the publicly known
best results  We refer to our model as GenEML  for Generative Exposurebased model for Multilabel Learning 
Hyperparameter Settings  For our model  we set the hyperparameters    and    to   which works well on
all the data sets we experimented with  We select the other
two hyperparameters    and    number of latent factors 
using crossvalidation  On small mediumscale data  both
EM and online EM perform comparably and we only report
the results using online EM  On large data sets  we only use
online EM  On the small and mediumscale data  we however also show   separate experiment comparing EM and
online EM for our model in terms of convergence speed
versus accuracy  For the conjugate gradient  CG  method
used by the   step of our inference algorithm  we run   iterations  which was found to be suf cient  For online EM 
for each data set  we use minibatch sizes of   and  
and report the one which gives better results 

  Quantitative Results

  BENEFIT OF EXPOSURE MODEL

In our  rst experiment  we assess the bene   of using the
exposure model  For this  we apply our model with and
without exposure on   synthetic data set  For this experiment  we generate   synthetic data set with   
   and    and use varying degrees of exposure
probabilities  cid                for the
different labels  cid                We also create   test set
with   test examples 
The results are shown in Table   As the results show 
our model with exposure turned on outperforms the model
when the exposure is turned off  This clearly demonstrate
the bene   of the exposure model when   signi cant fraction of labels are missing       not exposed  Our model
also outperforms LEML which does not have   mechanism
to model label exposure 

GenEML

 
 
 

  
  
  

GenEML    

Exposure

 
 
 

LEML

 
 
 

Table   Precision   values on synthetic data obtained by our
model with and without using the label exposure

  PREDICTION ACCURACIES

In our next set of experiments  in Table   we compare
our model  with exposure on  with the other baselines 
in terms of Precision  Precision  and Precision 
scores  As Table   shows  our model outperforms the other
baselines in most of the cases  except for the RCV and
Wikipedia data  on which our model is outperformed by
LEML and or PfasterXML  Note  however  that these stateof theart baselines use batch inference methods whereas
we only ran our model in the online setting on   moderate  
core processor with  GB RAM  Moreover  our results may
further improve with   more careful hyperparameter tuning
 including selection of minibatch size  The point of the
largescale data experiment was to mainly show that the our
model can be feasibly run on such largescale data sets  on
standard machines with moderate computational resources 
Most of the other existing models for multilabel learning
are infeasible to run under such restrictive settings 

  BATCH EM VS ONLINE EM

The online version of our EM algorithm is scalable and
faster than its batch counterpart  Fig   shows that online
EM converges faster and to   precision score which is very
similar to the batch EM on Bibtex and Mediamill datasets 
Furthermore  online inference is also more effecient 
storagewise  due the need of maintaining just the suf cient
statistics as in Eq   for the updates of each latent factor
un and   cid  For very large datasets  the size of the the suf 
 cient statistics          covariance matrix  for updating
the regression weight matrix   might not be feasible to
store and update  Therefore  we use cheap   rstorder gradient based updates for  nding an approximate solution to
the update equation of   in each iteration of the EM algorithm  note that we need not solve for   exactly  the EM
algorithm just requires   few steps of updates for   in the
  step  This further reduces the memory requirement of
our model  while also speeding up inference due to faster
computation of gradients as compared to CG updates 

Figure   Convergence time comparison of the batch and online
EM algorithm for inference in our model
  Qualitative Results

Finally  we do some qualitative analyses of our model   behavior  We investigate whether the global frequency of
  label necessarily correlates to its exposure probability 

Scalable Generative Models for Multilabel Learning with Missing Labels

FastXML

PfasterXML

PDSparse

BCS WSABIE
 
 
 

Dataset

Bibtex

Mediamill

Eurlex  

Movielens

RCV  

Wiki  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 

 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

LEML GenEML
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Table   Performance Comparison using Prec   of the model with other baselines  The   denotes that either these results were not
available or the method was infeasible to run on that data set  On the largescale data sets  RCV and Wiki  our model was run using
online EM based inference 

While it may be the case for some data sets where high label frequency implies   high inferred label exposure probability       see Fig    for Bibtex and Mediamill data  it
need not be the case with other data sets  For example 
for Movielens data  each usermovie  examplelabel  pair
has an some context information  user and movie features 
available for it  As we show in Fig   the inferred exposure probability  which depends on the context features  of
the same movie  label  indeed turns out to be different for
different users  examples 
Fig    shows the plot of inferred exposure probabilities  nl
for two users  one female  one male  plotted against the
label frequencies  movie popularities 

Figure   Inferred label exposure probabilities for Bibtex and Mediamill data sets
As Fig    shows  our model infers that    popular movie
 shown in red dot in Fig   has   high exposure probability for the left user  Female    Healthcare Doctor  while
it has   low exposure probability for the right user  Male 
  artist  This example illustrates that   high label frequency does not necessarily imply   high exposure probability  which can be context  user in this case  dependent 
  Conclusion
We presented    exible and scalable generative framework
for multilabel learning  Our framework is based on   latent

Figure   Inferred userspeci   label exposure probabilties for
two users on Movielens data set  Female    Healthcare Doctor
 left  and Male    artist  right  with label frequency for each
label  The red circle shows the most frequent movie  Hairspray  Comedy  Drama  and its exposure probability for
both the users 

factor model for the label matrix and does not assume that
the zeros in the label matrix are necessarily negative labels 
We use   set of label exposure latent variables to model this 
and infer these exposure probabilities from data  Incorporating these latent variables leads to improve multilabel
classi cation accuracies  and also enables doing interesting
qualitative analyses  Our model admits   simple inference
procedure which can be implemeted using Gibbs sampling
or EM  We further develop   highly scalable online EM
algorithm for performing inference in our model  which allows our model to be applied on largescale data sets  even
on standard machines with moderate hardware  The generative framework makes it easy to extend our model in many
interesting ways  For example  it can be extended to   mixture of latent factor models  which will allow handling the
cases where   single lowrank model does not adequately
capture the structure of the label matrix 
Acknowledgements  PR acknowledges support from Extreme
Classi cation research grant from Microsoft Research India 
DSTSERB Early Career Research Award  Dr  Deep Singh and
Daljeet Kaur Fellowship  and ResearchI Foundation  IIT Kanpur 

Scalable Generative Models for Multilabel Learning with Missing Labels

Liang  Dawen  Charlin  Laurent  McInerney  James  and Blei 
In

David    Modeling user exposure in recommendation 
WWW   

Polson  Nicholas    Scott  James    and Windle  Jesse  Bayesian
inference for logistic models using   olya gamma latent variables  Journal of the American Statistical Association   
   

Prabhu  Yashoteja and Varma  Manik  FastXML    fast  accurate
and stable treeclassi er for extreme multilabel learning  In
KDD   

Rai  Piyush  Hu  Changwei  Henao  Ricardo  and Carin 
Lawrence  Largescale bayesian multilabel learning via topicbased label embeddings  In NIPS   

Scott  James   and Sun  Liang  Expectationmaximization for

logistic regression  arXiv preprint arXiv   

Vasisht  Deepak  Damianou  Andreas  Varma  Manik  and
Kapoor  Ashish  Active learning for sparse bayesian multilabel
classi cation  In KDD   

Wang  Jiang  Yang  Yi  Mao  Junhua  Huang  Zhiheng  Huang 
Chang  and Xu  Wei  CNNRNN    uni ed framework for
multilabel image classi cation  In CVPR   

Xu  Chang  Tao  Dacheng  and Xu  Chao  Robust extreme multi 

label learning  In KDD   

Yen  Ian EH  Huang  Xiangru  Zhong  Kai  Ravikumar  Pradeep 
and Dhillon  Inderjit    PDsparse    primal and dual sparse
approach to extreme multiclass and multilabel classi cation  In
ICML   

Yu  HsiangFu  Jain  Prateek  Kar  Purushottam  and Dhillon  Inderjit    Largescale multilabel learning with missing labels 
In ICML   

Yu  HsiangFu  Huang  HsinYuan  Dhillon  Inderjit    and Lin 
ChihJen    uni ed algorithm for oneclass structured matrix
factorization with side information  In AAAI   

References
Agrawal  Rahul  Gupta  Archit  Prabhu  Yashoteja  and Varma 
Manik  Multilabel learning with millions of labels  Recommending advertiser bid phrases for web pages  In WWW   

Babbar     and Sch olkopf     DiSMECdistributed sparse machines for extreme multilabel classi cation  In WSDM   

Barnard  Kobus  Duygulu  Pinar  Forsyth  David  Freitas 
Nando de  Blei  David    and Jordan  Michael    Matching
words and pictures  JMLR   

Bertsekas  Dimitri    Nonlinear programming  Athena scienti  

Belmont   

Bhatia  Kush  Jain  Himanshu  Kar  Purushottam  Varma  Manik 
and Jain  Prateek  Sparse local embeddings for extreme multilabel classi cation  In NIPS   

Bucak  Serhat Selcuk  Jin  Rong  and Jain  Anil    Multilabel

learning with incomplete class assignments  In CVPR   

Capp    Olivier and Moulines  Eric 

Online expectation 
maximization algorithm for latent data models  Journal of the
Royal Statistical Society  Series    Statistical Methodology 
 

Chen  YaoNan and Lin  HsuanTien  Featureaware label space
In NIPS 

dimension reduction for multilabel classi cation 
 

Ciss    Moustapha  AlShedivat  COM Maruan  and Bengio 
Samy  Adios  Architectures deep in output space  In ICML 
 

Feng  SL  Manmatha  Raghavan  and Lavrenko  Victor  Multiple
bernoulli relevance models for image and video annotation  In
CVPR   

Fraccaro  Marco  Paquet  Ulrich  and Winther  Ole 

Indexable
probabilistic matrix factorization for maximum inner product
search  In AAAI   

Gibaja  Eva and Ventura  Sebasti an  Multilabel learning    review of the state of the art and ongoing research  Wiley Interdisciplinary Reviews  Data Mining and Knowledge Discovery 
 

Gibaja  Eva and Ventura  Sebasti an    tutorial on multilabel

learning  ACM Comput  Surv   

Hu  Yifan  Koren  Yehuda  and Volinsky  Chris  Collaborative

 ltering for implicit feedback datasets  In ICDM   

Jain  Himanshu  Prabhu  Yashoteja  and Varma  Manik  Extreme
multilabel loss functions for recommendation  tagging  ranking   other missing label applications  In KDD   

Kanehira  Atsushi and Harada  Tatsuya  Multilabel ranking from

positive and unlabeled data  In CVPR   

Kapoor  Ashish  Viswanathan  Raajay  and Jain  Prateek  MulIn

tilabel classi cation using bayesian compressed sensing 
NIPS   

Kong  Xiangnan  Wu  Zhaoming  Li  LiJia  Zhang  Ruofei  Yu 
Philip    Wu  Hang  and Fan  Wei  Largescale multilabel
learning with incomplete label assignments  In SDM   

