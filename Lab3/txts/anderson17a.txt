An Ef cient  SparsityPreserving  Online Algorithm for LowRank

Approximation

David Anderson     Ming Gu    

Abstract

Lowrank matrix approximation is   fundamental
tool in data analysis for processing large datasets 
reducing noise  and  nding important signals 
In this work  we present   novel truncated LU
factorization called SpectrumRevealing LU
 SRLU  for effective lowrank matrix approximation  and develop   fast algorithm to compute
an SRLU factorization  We provide both matrix
and singular value approximation error bounds
for the SRLU approximation computed by our
algorithm  Our analysis suggests that SRLU is
competitive with the best lowrank matrix approximation methods  deterministic or randomized  in both computational complexity and approximation quality  Numeric experiments illustrate that SRLU preserves sparsity  highlights important data features and variables  can be ef 
ciently updated  and calculates data approximations nearly as accurately as possible  To the best
of our knowledge this is the  rst practical variant
of the LU factorization for effective and ef cient
lowrank matrix approximation 

  Introduction
Lowrank approximation is an essential data processing
technique for understanding large or noisy data in diverse
areas including data compression  image and pattern recognition  signal processing  compressed sensing  latent semantic indexing  anomaly detection  and recommendation
systems  Recent machine learning applications include
training neural networks  Jaderberg et al    Kirkpatrick et al    second order online learning  Luo
et al    representation learning  Wang et al   
and reinforcement learning  Ghavamzadeh et al   

 Equal contribution

 University of California  BerkeCorrespondence to  David Anderson  davidander 

ley 
son berkeley edu  Ming Gu  mgu berkeley edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Additionally    recent trend in machine learning is to include an approximation of second order information for
better accuracy and faster convergence  Krummenacher
et al   
In this work  we introduce   novel lowrank approximation
algorithm called SpectrumRevealing LU  SRLU  that can
be ef ciently computed and updated  Furthermore  SRLU
preserves sparsity and can identify important data variables
and observations  Our algorithm works on any data matrix 
and achieves an approximation accuracy that only differs
from the accuracy of the best approximation possible for
any given rank by   constant factor 
The major innovation in SRLU is the ef cient calculation
of   truncated LU factorization of the form

     

 cid   
 cid cid      

In  

 cid   cid   
 cid 

 cid 

     
     
 

    

   

  
 
        

 cid   
   cid   cid   

  

 

def

where   and   are judiciously chosen permutation matrices  The LU factorization is unstable  and in practice
is implemented by pivoting  interchanging  rows during
factorization       choosing permutation matrix   For
the truncated LU factorization to have any signi cance 
nevertheless  complete pivoting  interchanging rows and

columns  is necessary to guarantee that the factors  cid   and
 cid   are wellde ned and that their product accurately repre 

sents the original data  Previously  complete pivoting was
impractical as   matrix factorization technique because it
requires accessing the entire data matrix at every iteration 
but SRLU ef ciently achieves complete pivoting through
randomization and includes   deterministic followup procedure to ensure   hight quality lowrank matrix approximation  as supported by rigorous theory and numeric experiments 

 The truncated SVD is known to provide the best lowrank
matrix approximation  but it is rarely used for large scale practical
data analysis  See   brief discussion of the SVD in supplemental
material 

An Ef cient Algorithm for LowRank Approximation

  Background on the LU factorization

Algorithm   presents   basic implementation of the LU factorization  where the result is stored in place such that the
upper triangular part of   becomes   and the strictly lower
triangular part becomes the strictly lower part of    with
the diagonal of   implicitly known to contain all ones  LU
with partial pivoting  nds the largest entry in the ith column
from row   to   and pivots the row with that entry to the ith
row  LU with complete pivoting  nds the largest entry in
the submatrix Ai       and pivots that entry to Ai    It
is generally known and accepted that partial pivoting is suf 
 cient for general  realworld data matrices in the context
of linear equation solving 

Algorithm   The LU factorization
  Inputs  Data matrix     Rm  
  for           min       do
 
 
 
 
  Ai         Ai              
  end for

Perform row and or column pivots
for               do

Ak     Ak   Ai  

end for

Perform column pivots

Algorithm   Crout LU
  Inputs  Data matrix     Rm    block size  
  for                min            do
 
  Aj           
 
 
 
  Aj            
 
  end for

Apply Algorithm   on Aj        
Apply the row pivots to other columns of  

Aj                  

Aj                 

Line   of Algorithm   is known as the Schur update  Given
  sparse input  this is the only step of the LU factorization
that causes  ll  As the algorithm progresses   ll will compound and may become dense  but the LU factorization 
and truncated LU in particular  generally preserves some 
if not most  of the sparsity of   sparse input    numeric
illustration is presented below 
There are many variations of the LU factorization  In Algorithm   the Crout version of LU is presented in block
form  The column pivoting entails selecting the next  
columns so that the inplace LU step is performed on  
nonsingular matrix  provided the remaining entries are not
all zero  Note that the matrix multiplication steps are the
bottleneck of this algorithm  requiring   mnb  operations
each in general 

The LU factorization has been studied extensively since
long before the invention of computers  with notable results from many mathematicians  including Gauss  Turing 
and Wilkinson  Current research on LU factorizations includes communicationavoiding implementations  such as
tournament pivoting  Khabou et al    sparse implementations  Grigori et al    and new computation
of preconditioners  Chow   Patel      randomized
approach to ef ciently compute the LU factorization with
complete pivoting recently appeared in  Melgaard   Gu 
  These results are all in the context of linear equation
solving  either directly or indirectly through an incomplete
factorization used to precondition an iterative method  This
work repurposes the LU factorization to create   novel ef 
cient and effective lowrank approximation algorithm using
modern randomization technology 

  Previous Work
  LowRank Matrix Approximation  LRMA 

Previous work on lowrank data approximation includes
the Interpolative Decomposition  ID   Cheng et al   
the truncated QR with column pivoting factorization  Gu  
Eisenstat    and other deterministic column selection
algorithms  such as in  Batson et al   
Randomized algorithms have grown in popularity in recent
years because of their ability to ef ciently process large
data matrices and because they can be supported with rigorous theory  Randomized lowrank approximation algorithms generally fall into one of two categories  sampling
algorithms and black box algorithms  Sampling algorithms
form data approximations from   random selection of rows
and or columns of the data  Examples include  Deshpande et al    Deshpande   Vempala    Frieze
et al    Mahoney   Drineas     Drineas et al 
  showed that for   given approximate rank      ran 

domly drawn subset   of       cid   log    log  cid 
      cid   log    log  cid  rows of the data  and set 

columns of the data    randomly drawn subset   of
ting       AR  then the matrix approximation error
 cid    CUR cid   is at most   factor of       from the optimal
rank   approximation with probability at least     Black
box algorithms typically approximate   data matrix in the
form

    QT QA 

where   is an orthonormal basis of the random projection
 usually using SVD  QR  or ID  The result of  Johnson  
Lindenstrauss    provided the theoretical groundwork
for these algorithms  which have been extensively studied
 Clarkson   Woodruff    Halko et al    Martinsson et al    Papadimitriou et al    Sarlos   
Woolfe et al    Liberty et al    Gu    Note

An Ef cient Algorithm for LowRank Approximation

that the projection of an mby   data matrix is of size mby cid  for some oversampling parameter  cid       and   is
the target rank  Thus the computational challenge is the
orthogonalization of the projection  the random projection
can be applied quickly  as described in these works    previous result on randomized LU factorizations for lowrank
approximation was presented in  Aizenbud et al   
but is uncompetitive in terms of theoretical results and computational performance with the work presented here 
For both sampling and black box algorithms the tuning parameter   cannot be arbitrarily small  as the methods become meaningless if the number of rows and columns sampled  in the case of sampling algorithms  or the size of the
random projection  in the case of black box algorithms 
surpasses the size of the data    common practice is      
 

  Guaranteeing Quality

Rankrevealing algorithms  Chan    are LRMA algorithms that guarantee the approximation is of high quality
by also capturing the rank of the data within   tolerance
 see supplementary materials for de nitions  These methods  nevertheless  attempt to build an important submatrix
of the data  and do not directly compute   lowrank approximation  Furthermore  they do not attempt to capture
all positive singular values of the data   Miranian   Gu 
  introduced   new type of highquality LRMA algorithms that can capture all singular values of   data matrix
within   tolerance  but requires extra computation to bound
approximations of the left and right null spaces of the data
matrix  Rankrevealing algorithms in general are designed
around   de nition that is not speci cally appropriate for
LRMA 
  key advancement of this work is   new de nition of high
quality lowrank approximation 

De nition     rankk truncated LU factorization is
spectrumrevealing if

                  

 cid cid cid    cid   cid  
 cid cid cid 
 cid cid   cid  

  

 cid   

and

      

          

for           and            and            are
bounded by   low degree polynomial in       and   

De nition   has precisely what we desire in an LRMA 
and no additional requirements  The constants            
and            are at least   for any rankk approximation
by  Eckart   Young    This work shows theoretically
and numerically that our algorithm  SRLU  is spectrumrevealing in that it always  nds such    and    often with
           in practice 

Algorithm   TRLUCP
  Inputs  Data matrix     Rm    target rank    block
size    oversampling parameter        random Gaus 

sian matrix     Rp   cid   and  cid   are initially   matri 

ces

  Calculate random projection       
  for                      do
 

Perform column selection algorithm on   and swap
columns of  

Perform block LU with partial row pivoting and
swap rows of  

 
 

Update block column of cid  
Update block row of  cid  

 
 
  end for

Update  

  LowRank and Other Approximations in Machine

Learning

Lowrank and other approximation algorithms have appeared recently in   variety of machine learning applications  In  Krummenacher et al    randomized lowrank approximation is applied directly to the adaptive optimization algorithm ADAGRAD to incorporate variable dependence during optimization to approximate the full matrix version of ADAGRAD with   signi cantly reduced
computational complexity  In  Kirkpatrick et al     
diagonal approximation of the posterior distribution of previous data is utilized to alleviate catastrophic forgetting 

  Main Contribution  SpectrumRevealing

LU  SRLU 

Our algorithm for computing SRLU is composed of two
subroutines  partially factoring the data matrix with randomized complete pivoting  TRLUCP  and performing
swaps to improve the quality of the approximation  SRP 
The  rst provides an ef cient algorithm for computing  
truncated LU factorization  whereas the second ensures the
resulting approximation is provably reliable 

  Truncated Randomized LU with Complete

Pivoting  TRLUCP 

Intuitively  TRLUCP performs deterministic LU with partial row pivoting for some initial data with permuted
columns  TRLUCP uses   random projection of the Schur
complement to cheaply  nd and move forward columns
that are more likely to be representative of the data  To accomplish this  Algorithm   performs an iteration of block
LU factorization in   careful order that resembles Crout LU
reduction  The ordering is reasoned as follows  LU with
partial row pivoting cannot be performed until the needed

An Ef cient Algorithm for LowRank Approximation

columns are selected  and so column selection must  rst
occur at each iteration  Once   block column is selected   
partial Schur update must be performed on that block column before proceeding  At this point  an iteration of block
LU with partial row pivoting can be performed on the current block  Once the row pivoting is performed    partial
Schur update of the block of pivoted rows of   can be performed  which completes the factorization up to rank       
Finally  the projection matrix   can be cheaply updated to
prepare for the next iteration  Note that any column selection method may be used when picking column pivots
from    such as QR with column pivoting  LU with row
pivoting  or even this algorithm can again be run on the
subproblem of column selection of    The  op count of
TRLUCP is dominated by the three matrix multiplication
steps  lines     and   The total number of  ops is

  TRLUCP    pmn                            

Note the transparent constants  and  because matrix multiplication is the bottleneck  this algorithm can be implemented ef ciently in terms of both computation as well
as memory usage  Because the output of TRLUCP is
only written once  the total number of memory writes is
              Minimizing the number of data writes by
only writing data once signi cantly improves ef ciency because writing data is typically one of the slowest computational operations  Also worth consideration is the simplicity of the LU decomposition  which only involves three
types of operations  matrix multiply  scaling  and pivoting 
By contrast  stateof theart calculation of both the full and
truncated SVD requires   more complex process of bidiagonalization  The projection   can be updated ef ciently
to become   random projection of the Schur complement
for the next iteration  This calculation involves the current
progress of the LU factorization and the random matrix  
and is described in detail in the appendix 

  SpectrumRevealing Pivoting  SRP 

TRLUCP produces highquality data approximations for
almost all data matrices  despite the lack of theoretical
guarantees  but can miss important rows or columns of the
data  Next  we develop an ef cient variant of the existing
rankrevealing LU algorithms  Gu   Eisenstat    Miranian   Gu    to rapidly detect and  if necessary  correct
any possible matrix approximation failures of TRLUCP 
Intuitively  the quality of the factorization can be tested by
searching for the next choice of pivot in the Schur complement if the factorization continued and determining if
the addition of that element would signi cantly improve
the approximation quality  If so  then the row and column
with this element should be included in the approximation
and another row and column should be excluded to maintain rank  Because TRLUCP does not provide an updated

Schur complement  the largest element in the Schur complement can be approximated by  nding the column of  
with largest norm  performing   Schur update of that column  and then picking the largest element in that column 
Let   be this element  and  without loss of generality  assume it is the  rst entry of the Schur complement  Denote 

   

 cid  
  

   

 

 

   

 

    
sT
 
 
     

    

   

Next  we must  nd the row and column that should be replaced if the row and column containing   are important 
Note that the smallest entry of      may still lie in an
important row and column  and so the largest element of
the inverse should be examined instead  Thus we propose
de ning

 cid   

 cid  

 cid cid     

 cid 

 

 

  

def
 

and testing

 cid  

 
   cid max    
 

 

for   tolerance parameter       that provides   control of
accuracy versus the number of swaps needed  Should the
test fail  the row and column containing   are swapped with
 
the row and column containing the largest element in  
   
Note that this element may occur in the last row or last col 
 
umn of  
    indicating only   column swap or row swap
respectively is needed  When the swaps are performed 
the factorization must be updated to maintain truncated LU
form  We have developed   variant of the LU updating algorithm of  Gondzio    to ef ciently update the SRLU
factorization 
SRP can be implemented ef ciently  each swap requires
 
at most             operations  and  cid  
   cid max can be
quickly and reliably estimated using  Higham   Relton 
  An argument similar to that used in  Miranian  

Gu    shows that each swap will increase cid cid det cid   

 cid cid cid 

by   factor at least    hence will never repeat  At termination  SRP will ensure   partial LU factorization of the form
  that satis es condition   We will discuss spectrumrevealing properties of this factorization in Section  
It is possible to derive theoretical upper bounds on the
worst number of swaps necessary in SRP  but in practice 
this number is zero for most matrices  and does not exceed
      in the most pathological data matrix of dimension at
most   we can contrive 
SRLU can be used effectively to approximate second order information in machine learning  SRLU can be used
as   modi cation to ADAGRAD in   manner similar to the

An Ef cient Algorithm for LowRank Approximation

Algorithm   SpectrumRevealing Pivoting  SRP 

  Input  Truncated LU factorization      cid   cid    toler 

ance      
 
   cid max      do
  while  cid  
 

 

Set   to be the largest element in    or  nd an approximate   using   
Swap row and column containing   with row and
 
column of largest element in  
 
Update truncated LU factorization

 
  end while

lowrank approximation method in  Krummenacher et al 
  Applying the initialization technique in this work 
SRLU would likely provide an ef cient and accurate adaptive stochastic optimization algorithm  SRLU can also become   fullrank approximation  lowrank plus diagonal 
by adding   diagonal approximation of the Schur complement  Such an approximation could be appropriate for improving memory in arti cial intelligence  such as in  Kirkpatrick et al    SRLU is also   freestanding compression algorithm 

  The CUR Decomposition with LU

  natural extension of truncated LU factorizations is
  CURtype decomposition for increased accuracy  Mahoney   Drineas   

     cid  

 cid cid     cid   cid cid  

   cid LM cid   

def

    

As with standard CUR  the factors  cid   and  cid   retain  much

of  the sparsity of the original data  while   is   small 
kby   matrix  The CUR decomposition can improve the
accuracy of an SRLU with minimal extra needed memory 
Extra computational time  nevertheless  is needed to calculate      more ef cient  approximate CUR decomposition
can be obtained by replacing   with   high quality approximation  such as an SRLU factorization of high rank  in the
calculation of   

  The Online SRLU Factorization
Given   factored data matrix     Rm   and new observa 

     
     

tions    
decomposition takes the form

     cid   
 cid 

 cid    

 

   
 

   

 cid    Rs    an augmented LU
   

      

 

 

  
  

 
Snew
  and Snew            

 

where          
     An
SRLU factorization can then be obtained by simply performing correcting swaps  For   rank  update  at most  

swap is expected  although examples can be constructed
that require more than one swap  which requires at most
              ops  By contrast  the URV decomposi 

tion of  Stewart    is   cid   cid  while SVD updating
requires   cid        min        cid  operations in general 
   cid  for   numerical approxor   cid        min        log 

imation with the fast multipole method 

  Theoretical Results for SRLU

Factorizations

  Analysis of General Truncated LU Decompositions
Theorem   Let    denote the ranks truncated SVD for
       cid        Then for any truncated LU factorization
with Schur complement   
 cid    

 

 cid     cid   cid             

   cid   cid   cid     cid   cid 
 cid 
   cid cid   cid  
   cid   cid 
 cid   
 cid 
 cid cid   cid  
   cid LM cid   cid     cid   cid 
   cid LM cid   cid      cid   cid    

   

 cid cid   cid  

 cid   cid 

      

 cid    

 cid    

  

   

Theorem   For   general rankk truncated LU decomposition  we have for all           

for any norm  and

 cid    

           

and

Theorem   CUR Error Bounds 

Theorem   simply concludes that the approximation is accurate if the Schur complement is small  but the singular
value bounds of Theorem   are needed to guarantee that
the approximation retains structural properties of the original data  such as an accurate approximation of the rank
and the spectrum  Furthermore  singular values bounds can
be signi cantly stronger than the more familiar norm error
bounds that appear in Theorem   Theorem   provides  
general framework for singular value bounds  and bounding the terms in this theorem provided guidance in the design and development of SRLU  Just as in the case of deterministic LU with complete pivoting  the sizes of

 cid   cid 

   cid   cid   

range from moderate to small for almost all
and
data matrices of practical interest  They  nevertheless  cannot be effectively bounded for   general TRLUCP factorization  implying the need for Algorithm   to ensure that

 cid   cid 

   cid   cid   

An Ef cient Algorithm for LowRank Approximation

these terms are controlled  While the error bounds in Theorem   for the CUR decomposition do not improve upon
the result in Theorem   CUR bounds for SRLU speci 
cally will be considerably stronger  Next  we present our
main theoretical contributions 

  Analysis of the SpectrumRevealing LU

Decomposition
 

Theorem    SRLU Error Bounds  For       and    
mn  SRP produces   rankk SRLU factorization
      
with

 cid    

   cid cid   cid  

   cid   cid   cid             
 cid 

 cid           

 cid 

 

           
     

 cid    

Theorem   is   special case of Theorem   for SRLU factorizations  For   data matrix with   rapidly decaying spectrum  the righthand side of the second inequality is close
to           substantial improvement over the sharpness
of the bounds in  Drineas et al   
Theorem    SRLU Spectral Bound  For            SRP
produces   rankk SRLU factorization with

      

 cid cid   cid  
for       cid mnk    cid 

           
      

    

 cid          

 cid 

 cid 

 cid 

     

       
      

While the worst case upper bound on   is large  it is
dimensiondependent  and   and   may be chosen so that
     
is arbitrarily small compared to   In particular  if
      
  is the numeric rank of    then the singular values of the
approximation are numerically equal to those of the data 
These bounds are problemspeci   bounds because their
quality depends on the spectrum of the original data  rather
than universal constants that appear in previous results  The
bene   of these problemspeci   bounds is that an approximation of data with   rapidly decaying spectrum is guaranteed to be highquality  Furthermore  if         is not
small compared to        then no highquality lowrank
approximation is possible in the   and Frobenius norms 
Thus  in this sense  the bounds presented in Theorems  
and   are optimal 
Given   highquality rankk truncated LU factorization 
Theorem   ensures that   lowrank approximation of rank
 cid  with  cid      of the compressed data is an accurate rank cid 
approximation of the full data  The proof of this theorem
centers on bounding the terms in Theorems   and   Experiments will show that   is small in almost all cases 
Stronger results are achieved with the CUR version of
SRLU 

   cid LM cid   cid           
   cid LM cid   cid              

Theorem  

 cid    

and

 cid    
where           
         kmn 

 

mn  is the same as in Theorem   and

 cid 

         cid   cid 

  then

Theorem   If  

 cid cid LM cid  

 cid         
 cid 
for       cid mnk    cid  and   is an input parameter control 

 cid          

           

     

      

ling   tradeoff of quality vs  speed as before 

As before  the constants are small in practice  Observe that
for most real data matrices  their singular values decay with
increasing    For such matrices this result is signi cantly
stronger than Theorem  

  Experiments
  Speed and Accuracy Tests

In Figure   the accuracy of our method is compared to
the accuracy of the truncated SVD  Note that SRLU did
not perform any swaps in these experiments   CUR  is
the CUR version of the output of SRLU  Note that both
methods exhibits   convergence rate similar to that of the
truncated SVD  TSVD  and so only   constant amount of
extra work is needed to achieve the same accuracy  When
the singular values decay slowly  the CUR decomposition
provides   greater accuracy boost 
In Figure   the runtime of SRLU is compared to that of the truncated SVD  as
well as Subspace Iteration  Gu    Note that for Subspace Iteration  we choose iteration parameter       and
do not measure the time of applying the random projection  in acknowledgement that fast methods exist to apply  
random projection to   data matrix  Also  the block size implemented in SRLU is signi cantly smaller than the block
size used by the standard software LAPACK  as the size of
the block size affects the size of the projection  See supplement for additional details  All numeric experiments were
run on NERSC   Edison  For timing experiments  the truncated SVD is calculated with PROPACK 
Even more impressive  the factorization stage of SRLU becomes arbitrarily faster than the standard implementation
of the LU decomposition  Although the standard LU decomposition is not   lowrank approximation algorithm  it
is known to be roughly   times faster than the SVD  Demmel    See appendix for details 

An Ef cient Algorithm for LowRank Approximation

    Spectral Decay    

    Spectral Decay    

Figure   Accuracy experiment on random     matrices with different rates of spectral decay 

    Rank  Factorizations

    Time vs  Truncation Rank

Figure   Time experiment on various random matrices  and   time experiment on       matrix with varying truncation ranks 

Table   Errors of lowrank approximations of the given target
rank  SRLU is measured using the CUR version of the factorization 

DATA

 

Gaus 

SRFT

Dual BCH

SRLU

  PIn 
deter 
lc  
lc  

 
 
 
 

 
 
 

 
 
 

 
 
 

 
 
 
 

Next  we compare SRLU against competing algorithms  In
 Ubaru et al    errorcorrecting codes are introduced
to yield improved accuracy over existing random projection lowrank approximation algorithms  Their algorithm 
denoted Dual BCH  is compared against SRLU as well as
two other random projection methods  Gaus  which uses
  Gaussian random projection  and SRFT  which uses  
Fourier transform to apply   random projection  We test the
spectral norm error of these algorithms on matrices from
the sparse matrix collection in  Davis   Hu   
In Table   results for SRLU are averaged over   experiments  Using tuning parameter       no swaps were
needed in all cases  The matrices being tested are sparse
matrices from various engineering problems    PIn 
is   by   deter  is   by   and
lp ceria    abbreviated lc    is   by   Note

that SRLU    more ef cient algorithm  provides   better approximation in two of the three experiments  With   little
extra oversampling    practical assumption due to the speed
advantage  SRLU achieves   competitive quality approximation  The oversampling highlights an additional and
unique advantage of SRLU over competing algorithms  if
more accuracy is desired  then the factorization can simply
continue as needed 

  Sparsity Preservation Tests

The SRLU factorization is tested on sparse  unsymmetric
matrices from  Davis   Hu    Figure   shows the
sparsity patterns of the factors of an SRLU factorization
of   sparse data matrix representing   circuit simulation
 oscil dcop  as well as   full LU decomposition of the
data  Note that the LU decomposition preserves the sparsity of the data initially  but the full LU decomposition becomes dense  Several more experiments are shown in the
supplement 

  Towards Feature Selection

An image processing example is now presented to illustrate
the bene   of highlighting important rows and columns selection 
In Figure   an image is compressed to   rank 
  approximation using SRLU  Note that the rows and
columns chosen overlap with the astronaut and the planet 
implying that minimal storage is needed to capture the

An Ef cient Algorithm for LowRank Approximation

words terms  and an initial SRLU factorization of rank  
was performed on the  rst    documents  The initial factorization contained none of the top  ve words  but  after adding the remaining documents and updating  the top
three were included in the approximation  The fourth and
 fth words  market  and  california  have high covariance
with at least two of the three top words  and so their inclusion may be redundant in   lowrank approximation 

  Conclusion
We have presented SRLU    lowrank approximation
method with many desirable properties  ef ciency  accuracy  sparsitypreservation  the ability to be updated  and
the ability to highlight important data features and variables  Extensive theory and numeric experiments have illustrated the ef ciency and effectiveness of this method 

Acknowledgements
This research was supported in part by NSF Award CCF 
 

References
Aizenbud     Shabat     and Averbuch     Randomized lu decomposition using sparse projections  CoRR 
abs   

Batson     Spielman        and Srivastava     Twiceramanujan sparsi ers  SIAM Journal on Computing   
   

Chan        Rank revealing qr factorizations  Linear alge 

bra and its applications     

Cheng     Gimbutas     Martinsson       and Rokhlin 
   On the compression of low rank matrices  SIAM   
Scienti   Computing     

Chow     and Patel     Finegrained parallel incomplete
lu factorization  SIAM    Scienti   Computing   
 

Clarkson        and Woodruff        Low rank approximation and regression in input sparsity time  CoRR 
abs   

Davis        and Hu     The university of  orida sparse matrix collection  ACM Transactions on Mathematical Software      URL http www cise 
ufl edu research sparse matrices 

Demmel     Applied Numerical Linear Algebra  SIAM 

 

      and   patterns of   lowrank factorization

      and   patterns of the full factorization

Figure   The sparsity patterns of the   and   matrices of   rank
  SRLU factorization  followed by the sparsity pattern of the  
and   matrices of   full LU decomposition of the same data  For
the SRLU factorization  the green entries compose the lowrank
approximation of the data  The red entries are the additional data
needed for an exact factorization 

black background  which composes approximately two
thirds of the image  While this result cannot be called feature selection per se  the rows and columns selected highlight where to look for features  rows and or columns are
selected in   higher density around the astronaut  the curvature of the planet  and the storm front on the planet 

    Original

    SRLU

    Rows and Cols 

Figure   Image processing example 
image
 NASA    rank  approximation with SRLU  and   highlight
of the rows and columns selected by SRLU 

The original

  Online Data Processing

Online SRLU is tested here on the Enron email corpus
 Lichman    The documents were initially reversesorted by the usage of the most common word  and then
reversesorted by the second most  and this process was
repeated for the  ve most common words  the top  ve
words were used signi cantly more than any other  so that
the most common words occurred most at the end of the
corpus  The data contains   documents and  

An Ef cient Algorithm for LowRank Approximation

Deshpande     and Vempala     Adaptive sampling and fast
lowrank matrix approximation  In APPROXRANDOM 
volume   of Lecture Notes in Computer Science  pp 
  Springer   

Deshpande     Rademacher     Vempala     and Wang 
   Matrix approximation and projective clustering via
volume sampling  Theory of Computing   
 

Drineas     Mahoney        and Muthukrishnan    
Relativeerror cur matrix decompositions  SIAM    Matrix Analysis Applications     

Eckart     and Young     The approximation of one matrix
by another of lower rank  Psychometrika   
 

Frieze        Kannan     and Vempala     Fast montecarlo algorithms for  nding lowrank approximations    
ACM     

Ghavamzadeh     Lazaric     Maillard       and
Munos     Lstd with random projections  In NIPS  pp 
   

Gondzio     Stable algorithm for updating dense lu factorization after row or column exchange and row and column addition or deletion  Optimization    journal of
Mathematical Programming and Operations Research 
pp     

Grigori     Demmel     and Li        Parallel symbolic
factorization for sparse lu with static pivoting  SIAM   
Scienti   Computing     

Gu     Subspace iteration randomization and singular
value problems  SIAM    Scienti   Computing   
 

Gu     and Eisenstat        Ef cient algorithms for computing   strong rankrevealing qr factorization  SIAM   
Sci  Comput     

Halko     Martinsson       and Tropp        Finding
structure with randomness  Probabilistic algorithms for
constructing approximate matrix decompositions  SIAM
Review     

Higham        and Relton        Estimating the largest
entries of   matrix    URL http eprints 
ma man ac uk 

Jaderberg     Vedaldi     and Zisserman     Speeding up
convolutional neural networks with low rank expansions 
CoRR  abs   

Johnson        and Lindenstrauss     Extensions of lipschitz mappings into   hilbert space  Contemporary
Mathematics     

Khabou     Demmel     Grigori     and Gu     Lu factorization with panel rank revealing pivoting and its communication avoiding version  SIAM    Matrix Analysis
Applications     

Kirkpatrick     Pascanu     Rabinowitz        Veness 
   Desjardins     Rusu        Milan     Quan    
Ramalho     GrabskaBarwinska     Hassabis    
Clopath     Kumaran     and Hadsell     Overcoming catastrophic forgetting in neural networks  Proceedings of the National Academy of Sciences   
   

Krummenacher     McWilliams     Kilcher     Buhmann        and Meinshausen     Scalable adaptive
stochastic optimization using random projections 
In
NIPS  pp     

Liberty     Woolfe     Martinsson       Rokhlin     and
Tygert     Randomized algorithms for the lowrank approximation of matrices  Proceedings of the National
Academy of Sciences     

Lichman     UCI machine learning repository    URL

http archive ics uci edu ml 

Luo     Agarwal     CesaBianchi     and Langford    
Ef cient second order online learning by sketching  In
NIPS  pp     

Mahoney        and Drineas     Cur matrix decompositions for improved data analysis  Proceedings of the
National Academy of Sciences     

Martinsson       Rokhlin     and Tygert       randomized algorithm for the approximation of matrices  Tech 
Rep  Yale University  Department of Computer Science 
   

Melgaard     and Gu     Gaussian elimination with randomized complete pivoting  CoRR  abs 
 

Miranian     and Gu     Stong rank revealing lu factorizations  Linear Algebra and its Applications   
 

Nasa celebrates   years of
spacewalkURL https www nasa gov image 

NASA 
ing 
feature nasacelebrates yearsof 
spacewalking  Accessed on August    
Published on June     Original photograph from
February    

An Ef cient Algorithm for LowRank Approximation

Papadimitriou        Raghavan     Tamaki     and Vempala     Latent semantic indexing    probabilistic analysis     Comput  Syst  Sci     

Sarlos    

Improved approximation algorithms for large
matrices via random projections  In FOCS  pp   
IEEE Computer Society   

Stewart        An updating algorithm for subspace tracking  IEEE Trans  Signal Processing   
 

Ubaru     Mazumdar     and Saad     Low rank approximation using error correcting coding matrices  In ICML 
volume   pp     

Wang        Mehdad     Radev        and Stent      
lowrank approximation approach to learning joint embeddings of news stories and images for timeline summarization  pp     

Woolfe     Liberty     Rokhlin     and Tygert       fast
randomized algorithm for the approximation of matrices 
Applied and Computational Harmonic Analysis   
   

