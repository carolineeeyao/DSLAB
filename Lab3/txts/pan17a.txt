Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

with Applications to Filtering and Control

Yunpeng Pan     Xinyan Yan     Evangelos    Theodorou     Byron Boots    

Abstract

ization 

Sparse Spectrum Gaussian Processes  SSGPs 
are   powerful tool for scaling Gaussian processes  GPs  to large datasets  Existing SSGP
algorithms for regression assume deterministic
inputs  precluding their use in many realworld
robotics and engineering applications where accounting for input uncertainty is crucial  We
address this problem by proposing two analytic
momentbased approaches with closedform expressions for SSGP regression with uncertain inputs  Our methods are more general and scalable than their standard GP counterparts  and are
naturally applicable to multistep prediction or
uncertainty propagation  We show that ef cient
algorithms for Bayesian  ltering and stochastic
model predictive control can use these methods 
and we evaluate our algorithms with comparative
analyses and both realworld and simulated experiments 

  Introduction
The problem of prediction under uncertainty  appears in
many  elds of science and engineering that involve sequential prediction including state estimation  Ko   Fox 
  Deisenroth et al    time series prediction  Girard et al    stochastic process approximation  Archambeau et al    and planning and control  Deisenroth et al    Pan et al    In these problems  uncertainty can be found in both the predictive models and the
model   inputs  Formally  we are often interested in  nding
the probability density of   prediction    given   distribution      and   probabilistic model        By marginalInstitute of Technology  Atlanta  Georgia 
InCorrespondence to  Yunpeng Pan

USA  School of Aerospace Engineering  School of
teractive Computing 
 ypan gatech edu 

 Georgia

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

 cid 

      

           dx 

 

Unfortunately  computing this integral exactly is often intractable  In this paper  we tackle   subfamily of   where 
  the probabilistic model is learned from data and speci 
 ed by   sparse spectrum representation of   Gaussian process  SSGP  and   the input   is normally distributed  We
show that analytic expressions of the moments of      can
be derived and that these are directly applicable to sequential prediction problems like  ltering and control 
  Related work
Gaussian Process  GP  regression with uncertain inputs
has been addressed by Candela et al    Girard et al 
  and extended to the multivariate outputs by Kuss
  These methods have led to the development of
many algorithms in reinforcement learning  Rasmussen  
Kuss    Deisenroth et al    Bayesian  ltering
 Ko   Fox    Deisenroth et al    and smoothing
 Deisenroth et al    However  these approaches have
two major limitations    they are not directly applicable
to large datasets  due to the polynomial time complexity
for exact inference  Williams   Rasmussen    and  
analytic moment expressions  when used  are restricted to
squared exponential  SE  kernels  Kuss    and cannot
be generalized to other kernels in   straightforward way 
  common method for approximating largescale kernel
machines is through random Fourier features  Rahimi  
Recht    The key idea is to map the input to   lowdimensional feature space yielding fast linear methods  In
the context of GP regression  GPR  this idea leads to the
sparse spectrum GPR  SSGPR  algorithm    azaroGredilla
et al    SSGP has been extended in   number of ways
for      
incremental model learning  Gijsberts   Metta 
  and largescale GPR  Dai et al    Yan et al 
  However  to the best of our knowledge  prediction
under uncertainty for SSGPs has not been explored  Although there are several alternative approximations to exact
GP inference including approximating the posterior distribution using inducing points        Snelson   Ghahramani 
  Titsias    Cheng   Boots    comparing different GP approximations is not the focus of this paper 

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

  Applications
We consider two key problems that are widely encountered
in robotics and engineering  Bayesian  ltering and stochastic model predictive control 
The goal of Bayesian  ltering is to infer   hidden system state through the recursive application of Bayes  rule 
Wellknown frameworks for Bayesian  ltering include unscented Kalman Filtering  UKF  particle  ltering  PF  extended Kalman  ltering  EKF  and assumed density  ltering  ADF  GPbased Bayesian  ltering with SE kernels
has been developed for these frameworks by  Ko   Fox 
  Deisenroth et al    We extend this work with
highly ef cient SSGPbased EKF and ADF algorithms 
The goal of stochastic model predictive control  MPC  is
to  nd  nite horizon optimal control at each time instant 
Due to the high computational cost of GP inference and
realtime optimization requirements in MPC  most GPbased control methods  Deisenroth et al    Pan  
Theodorou    Kupcsik et al    are restricted to
episodic reinforcement learning tasks  To cope with this
challenge  we present an SSGPbased MPC algorithm that
is fast enough to perform probabilistic trajectory optimization and model adaptation onthe   
  Our contributions

  We propose two approaches to prediction under uncertainty in SSGPs with closedform expressions for
the predictive distribution  Compared to previous GP
counterparts  our methods    are more scalable  and
  can be generalized to any continuous shiftinvariant
kernels with   Fourier feature representation 
  We demonstrate successful applications of the proposed approaches by presenting scalable algorithms
for   recursive Bayesian  ltering and   stochastic
model predictive control via probabilistic trajectory
optimization 

The rest of the paper is organized as follows  In   we give
an introduction to SSGPs  which serves as our probabilistic model  Derivation and expressions of the two proposed
prediction methods are detailed in   Applications to  ltering and control  and experimental results are presented
in   and   respectively  Finally   concludes the paper 
  Sparse Spectral Representation of GPs
Consider the task of learning the function     Rd     
given IID data      xi  yi  
   with each pair related by
 
             

         
  

where   is IID additive Gaussian noise  Gaussian process regression  GPR  is   principled way of performing
Bayesian inference in function space  assuming that function   has   prior distribution     GP       with mean

function     Rd     and kernel     Rd   Rd     
Without loss of generality  we assume          Exact GPR is challenging for large datasets due to its     
time and      space complexity  Williams   Rasmussen 
  which is   direct consequence of having to store and
invert an       Gram matrix 
Random features can be used to form an unbiased approximation of continuous shiftinvariant kernel functions  and
are proposed as   general mechanism to accelerate largescale kernel machines  Rahimi   Recht    via explicitly mapping inputs to lowdimensional feature space 
Based on Bochner   theorem  the Fourier transform of  
continuous shiftinvariant positive de nite kernel        cid 
is   proper probability distribution    assuming        cid 
is properly scaled  Rahimi   Recht   

       cid   

  ej        cid    

        cid        

 

 cid 

 cid     

 cid 

 

 cid           cid  where random fre 

where       ej      and we can see that        cid  only
depends on the lag vector separating   and   cid      cid  Equation   leads to an unbiased  nite sample approximation
of           cid     
quencies     
   are drawn IID from    Utilizing the
fact that   can be replaced by sinusoidal functions since
both    and        cid  are reals  and concatenating features
    
   into   succinct vector form  an approximation for
       cid  is expressed as

 

       

 SE  kernel 

 cid       cid cid 

       cid            cid       

           sin  

 
     
            
           cos  
  
where    is   scaling coef cient  For the commonly
       cid   
used Squared Exponential
             and     
  exp   
 
   
   where the coef cient    and the diagonal matrix   are
the hyperparameters  examples of kernels and corresponding spectral densities can be found in Table  
In accordance with this feature map   Sparse Spectrum
GPs are de ned as follows
De nition   Sparse Spectrum GPs  SSGPs  are GPs with
kernels de ned on the  nitedimensional and randomized
feature map    

       cid            cid     

        cid 

 

where the function   is the Kronecker delta function 

The second term in   accounts for the additive zero mean
Gaussian noise in   if the goal is to learn the correlation between   and   directly as in our case of learning the
probabilistic model        instead of learning the latent
function   

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

Because of the explicit  nitedimensional feature map  
each SSGP is equivalent to   Gaussian distribution over the
weights of features          Assuming that prior distribution of weights   is          and the feature map is
 xed  after conditioning on the data      xi  yi  
   the
posterior distribution of   is  

         

nA 

                   

nI 

 

which can be derived through Bayesian linear regression 
In   the column vector   and the matrix   are

speci ed by the data         cid   
 cid   

 cid        
       xn cid  Consequently  the posterior distri 

bution over the output   in   at   test point   is exactly
Gaussian  in which the posterior variance explicitly captures the model uncertainty in prediction with input   

     

yn

                   

     

  cid   cid 

    

 

This Bayesian linear regression method for SSGP is proposed in   azaroGredilla et al    Its time complexity
is   nm       which is signi cantly more ef cient than
standard GPR        when    cid    

Remark It   worth noting that the methods proposed in
this paper are not tied to speci   algorithms for SSGP
regression such as Bayesian linear regression    azaroGredilla et al    but able to account for any SSGP
with speci ed feature weights distribution   where posterior   and   can be computed by any means  Variations on
  include sparse approximations by   low rank plus diagonal matrix  or iterative solutions by optimization methods
like doubly stochastic gradient descent  Dai et al   
  Prediction under Uncertainty
Two methods for prediction under uncertainty are presented under two conditions    the uncertain input is normally distributed            and   probabilistic models are in the form of   speci ed by SSGPs  Despite
these conditions  evaluating the integral in   is still intractable 
In this work  we approximate the true predictive distribution      by   Gaussian distribution with moments that are analytically computed through    exact moment matching  and   linearization of posterior mean function  Closedform expressions for predictive mean  variance  covariance  and inputprediction crosscovariance are
derived  We consider multivariate outputs by utilizing con 

ance is identity since                   cid     wwT    cid cid   

   is the identity matrix with proper size  The prior covari 
       wwT    cid  and              cid            cid   see
  in Rasmussen   Kuss   for details 
 Conditioning on data   is omitted       in      for simplic 

ity in notation 

ditionally independent scalar models for each output dimension       assuming for outputs in different dimension
ya and yb    ya  yb        ya     yb    Discussions on
this assumption can be found in Appendix   For notational simplicity  we suppress the dependency of     on
   and treat   as   scalar by default 
  Exact moment matching  SSGPEMM 
We derive the closedform expressions for exact moments 
  the predictive mean        the predictive variance Var  
and covariance Cov ya  yb  which in the multivariate case
correspond to the diagonal and offdiagonal entries of the
predictive covariance matrix  and   the crosscovariance
between input and prediction Cov      
Using the expressions for SSGP     and the law of
total expectation  the predictive mean becomes

                   cid    cid        

 cid  

 

 

 cid 

  

    

         cos  

         

         sin  

    

where                  and in the nested expectation
         the outer expectation is over the input distribution              and the inner expectation is over the
conditional distribution         
By observing   we see that the expectation of sinusoids
under the Gaussian distribution is the key to computing the
predictive mean  Thus  we state the following proposition 
Proposition   The expectation of sinusoids over multivariate Gaussian distributions                Rd 
               
    can
be computed analytically 

 cid      cid 

   det    

  exp   

  cos        exp   
 
  sin        exp   
 

 cid cid 
 cid cid 

  cos    

  sin    

To prove it  we invoke Euler   formula to transform the lefthand side to complex domain  apply identities involving
quadratic exponentials  and then convert back to real numbers  see Appendix   for details  In Proposition   the
expectations depend on the mean and variance of the input
Gaussian distribution  Intuitively  after passing   Gaussian
distributed input through   sinusoidal function  the expectation of the output is equal to passing the mean of the input through the sinusoid  and then scaling it by   constant
exp   
  which depends on the variance of the input 
Expectations are smaller with larger input variance due to
the periodicity of sinusoids 
The exact moments are then derived using Proposition  
By the law of total variance  the predictive variance is

 cid cid 

Var       Var        Var       

  Tr cid   cid                

   

     

 

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

 

 cid 

 sc  ss

 
 
 
 
 
 
 
 
 

 cc

ij  

 ss

ij  

where   is de ned as the expectation of the outer product
of feature vectors over input distribution      Speci cally 
we compute   by applying the productto sum trigonometric identities 

 cid cc  cs

  cid   cid       
 cid   cid cos            cid      cid cos            cid cid   
 cid   cid cos            cid      cid cos            cid cid   
 cid   cid sin            cid      cid sin            cid cid   

 cs
ij  
where  cc   ss   cs are       matrices  and       
             on whose terms Proposition   can be directly applied 
Next  we derive the covariance for different output dimensions for multivariate prediction  These correspond to the
offdiagonal entries of the predictive covariance matrix 
We show that  despite the conditional independence assumption for different outputs given   deterministic input 
outputs become coupled with uncertain inputs  Using the
law of total covariance  the covariance is
Cov ya  yb    Cov    ya      yb   

       ya      yb     ya   yb 
    

   ab       

        

       

 

where matrix  ab is the expectation of the outer product
of feature vectors corresponding to different feature maps
       for outputs ya  yb  computed similarly as in  
with corresponding random frequencies     and the scaling coef cient      Vectors    and    are the corresponding weight vectors for ya and yb   Compared to
the expression for the variance of   single output in   the
term    Cov ya    Cov yb    that is included in the law
of total covariance is neglected due to the assumption of
conditional independence of different outputs   so  
does not have the corresponding  rst two terms in  
Finally  we compute the crosscovariance between input
and each output dimension  Invoking the law of total covariance 

Cov         Cov          

                         
           

 

where matrix   is the expectation of the outer product of
the input   and the feature vector     over input distribution          

             cid  
         cid cos  

  

      cid      

        

    
 

         cid cos  

 

        
 

 cid   
      cid   

Kernel
Gaussian
Laplacian
Mat ern

       cid 
 cid       cid cid 
exp   
exp cid       cid cid 
 
           

   

  

     

 cid  
 cid     cid cid 

  

 

   

    

   
Table   Examples of continuous shiftinvariant positivede nite kernels and their corresponding spectral densities 
where    
     is   modi ed Bessel function 
and        

 cid     cid cid 

   

 

 

 cid 

 

 
     
 cid 

where                  We state the following proposition
to compute each column in   consisting of expectations of
the product sinusoidal functions and inputs 

Proposition   The expectation of the multiplication of sinusoids and linear functions over multivariate Gaussian
distributions            can be computed analytically 

  cid cos       cid   cid   cos     cid         sin     
  cid sin       cid   cid   sin     cid     cid   cos     cid   

where the righthand side expectations have analytical expressions  Proposition  

To prove it  we  nd an expression for   cid aT   cos     cid 
  cid   cos     cid  by setting   to consist of indicator vec 

for any   
through the complex domain trick used to
prove Proposition   Next 
is extended to
tors  see Appendix   for details  Applying Proposition
  and   we complete the derivation of Cov       in  

the result

Remark In summary  SSGPEMM computes the exact
posterior moments  This is equivalent to expectation propagation  Minka    by minimizing the KullbackLeibler
divergence between the true distribution and its Gaussian approximation with respect to the natural parameters 

where   is the number of features    is the output dimension  and   is the input dimension  The most computationally demanding part is constructing matrices  ab   for

SSGPEMM   computation complexity is   cid       cid 
each output pair  where each requires   cid     cid 
  cid       cid  time complexity  SSGPEMM is more ef 

Compared to the multivariate momentmatching approach
for GPs  GPEMM   Girard et al    Kuss    with
cient when    cid     Moreover  our approach is applicable to any positivede nite continuous shiftinvariant kernel with different spectral densities  see examples in Table
  while previous approaches like GPEMM  Kuss   
are only derived for squared exponential  SE  or polynomial kernels  Next we introduce   more computationally
ef cient but less accurate approach that avoids the computation of  ab   

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

  Linearization  SSGPLin 

An alternative approach to computing the exact moments
of the predictive distribution is based on the linearization
of the posterior mean function in   at the input mean  

                          

      

 

 cid   cid cid   cid 

 

           sin  

where    denotes taking the derivative of function  
at   Given the de nition of   in      can be found
by chain rule     
       
   cos  
Utilizing the linearized posterior mean function   the
predictive moments can be approximated  The predictive
mean approximation is

   
     

       

     

                   
and the predictive variance approximation is

Var       Var        Var       
  Var      Var       
   

  cid cid 

     

                

 

 

and the approximate covariance between output dimension
  and   is
Cov ya  yb    Cov    ya      yb   

  Ma                

    

 

 cid 

    cid  

    

  Ma    

     

where Ma and Mb are de ned as   in   except that
they correspond to feature maps    and     Notice that the
assumption of conditional independence between different
outputs is invoked here again  cf   
Finally  the crosscovariance between the input and output
can be approximated as

Cov         Cov          

    cid                cid 

 

        

Unlike SSGPEMM  which computes exact moments
  this linearizationbased approach SSGPLin computes an approximation of the predictive moments 
In

contrast to SSGPEMM     cid       cid  computational comO cid   kd cid  as   direct consequence of avoiding the con 

plexity  the computation time of SSGPLin is reduced to

struction of     in SSGPEMM   which makes
SSGPLin more ef cient
than SSGPEMM  especially
when the output dimension is high 
Both SSGPEMM and SSGPLin are applicable to   general family of kernels  See Table   for   comparison between our methods and GPEMM  Girard et al   
Kuss   
In the next section  we compare these approaches in applications of  ltering and control 

SSGPLin

continuous shiftinvariant kernels

SSGPEMM
                 mk   
continuous shiftinvariant kernels

Method
GPEMM
Time
        
Applicable
SE or polynokernels
mial kernels
Table   Comparison of our proposed methods and GPEMM  Girard et al    Kuss    in terms of computational complexity and generalizability 
  Applications
We focus on the application of the proposed methods to
Bayesian  ltering and predictive control  We begin by introducing GaussMarkov models  which can be expressed
by the following discretetime nonlinear dynamical system 

  

yt     xt      
   

            
  
            
  

xt       xt  ut      
   

 
 
where xt   Rd is state  ut   Rr is control  yt   Rk is
    Rd is IID process noise 
observation or measurement    
    Rk is IID measurement noise  and subscript   denotes
  
discrete time index  We call the probabilistic models  
and   the dynamics and observation models  and the corresponding deterministic functions   and   the dynamics
and observation functions 
We consider scenarios where   and   are unknown but

  dataset      cid xt  ut  xt   

 cid  is pro 

    xt  yt  

vided  The probabilistic models speci ed by SSGPs can
be learned from the dataset  and then used to model the
dynamics and observation     More concretely  the
dynamics model   xt xt  ut  is learned using state transition pairs  xt  ut  xt   
     and the observation model
  yt xt  is learned separately from stateobservation pairs
 xt  yt  
  Bayesian  ltering
The task of Bayesian  ltering is to infer the posterior distribution of the current state of   dynamical system based
on the current and past noisy observations        nding
  xt    where the notation xt   denotes the random variable xt            ys  Due to the Markov property of the process         xt            xt    xt xt  in GaussMarkov
models    xt    can be computed recursively through alternating prediction step and correction step 
  PREDICTION STEP  xt      xt   
In the prediction step  xt    is propagated through the
dynamics model   xt xt  ut 

  

  xt     

  xt xt  ut   xt    dxt 

 cid 

which can be viewed as prediction under uncertainty
  Suppose that   xt                   
the dynamics 
with learned SSGP representation for
Gaussian approximations of the output    xt     
              can be obtained by either SSGPEMM

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

  using     and   or SSGPLin   using
    and  
  CORRECTION STEP  xt      xt   
The correction step conditions xt    on the current observation yt using Bayes  rule 

  yt xt     xt   

 cid    yt xt     xt    dxt

 

  xt     

 
In the preceding prediction step  we obtain   xt     
              which serves as   prior on xt in this
correction step  Due to the intractability of the integral in
the denominator  to apply Bayes  rule we  rst seek Gaussian approximations for the joint distribution  as in the previous work on Bayesian  ltering relying on GPs  Deisenroth et al    Ko   Fox   

 cid xt   
Invoking   yt     cid    yt xt     xt    dxt  the mo 

 cid cid     

 cid       

 cid cid 

 xy
  

   

yt   

 

 

  
xy

 cid 

 

 cid 

  

ments         and  xy in the joint Gaussian approximation
can be computed as the predictive mean  predictive covariance  and inputprediction crosscovariance  for the observation model   yt xt  with input   xt    using SSGPEMM or SSGPLin  Having all terms in   determined 
we condition xt    exactly on current observation yt 

                xy  
                xy  

 

 xy 

          

 

This Gaussian approximation   xt                    is
then used as input to the prediction step  Thus  we have
shown that starting from              by consecutively applying prediction and correction steps presented
above  we recursively obtain state estimates for xt    and
xt    Rather than using    nite samplebased approximation
such as in the GPUKF  Ko   Fox    the Gaussian approximations of the full densities   xt    and   xt    are
propagated 
Algorithm   SSGPADF and SSGPEKF
  Model learning  collect dataset    and learn SSGP dy 

namics and observations models  

  Initialization  set prior     
  for             do
 

Prediction  compute       and         by either
SSGPEMM   or SSGPLin  
  Measurement  make an observation yt 
 

Correction  compute      and      according to  
by either SSGPEMM   or SSGPLin  

  end for
We summarize the resulting  ltering algorithm SSGPADF  assumed density  ltering  and SSGPEKF  extended
Kalman  ltering  based on SSGPEMM and SSGPLin 
respectively  in Algorithm   These are analogs of GPADF
 Deisenroth et al    and GPEKF  Ko   Fox   

  Stochastic Model Predictive Control
The stochastic model predictive control  MPC  problem is
to choose   control sequence that minimizes the expected
cost  provided   xt 

  cid   xt      

  xt    ut   cid 

  cid 
        argmin
ut    

    cid 

 

at each time step  subject to stochastic system dynamics
  where function     Rd     and     Rd   Rr    
are the  nal and running cost respectively  There are two
main challenges to applying MPC in practice    MPC requires an accurate dynamics model for multistep prediction  and   online optimization is very computationally expensive  For clarity in presentation  we will assume that the
state is fully observable henceforth 
Algorithm   MPC via probabilistic trajectory optimization
  of ine optimization    online optimization 
  Model learning  collect dataset    and learn SSGP dy 

namics model  

  Initialization  set       and estimate     
  Trajectory optimization  perform trajectory optimiza 

tion in belief space  obtain   cid 

       

  repeat
 

Policy execution  apply onestep control   cid 
   to the
system and move one step forward  update       
  Model adaptation  incorporate new data and update

 

SSGP dynamics model 
Trajectory optimization  perform reoptimization
Initialize with the previwith the updated model 
       
ously optimized trajectory and obtain new   cid 

  until Task terminated

  MPC VIA PROBABILISTIC TRAJECTORY

OPTIMIZATION

We address the aforementioned challenges by employing
  combination of prediction under uncertainty and trajectory optimization  More precisely  we use SSGPEMM or
SSGPLin to ef ciently obtain approximate Gaussian distribution over trajectory of states and perform trajectory optimization in the resultant Gaussian belief space based on
differential dynamic programming  DDP   Abbeel et al 
  Tassa et al    Note that DDPrelated methods
require computation of  rst and second order derivatives of
the dynamics and cost  Our analytic moment expressions
provide   robust and ef cient way to compute these derivatives  Details are omitted due to space limit  but they can
be found in Appendix  
Within the SSGP framework  we may incrementally update the posterior distribution over the feature weights  
  given   new sample without storing or inverting the
matrix   explicitly  Instead we keep track of its upper triangular Cholesky factor     RT    Gijsberts   Metta 
  Given   new sample    rank  update is applied to

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

SSGPADF

 
 

Method
  Lx

SSGPEKF GPADF
 
 
 
 

GPEKF
 
 
RM SE
Table   Comparison of our methods with GPADF
 Deisenroth et al    and GPEKF  Ko   Fox   
in terms of average   Lx  negative loglikelihood  of the
ground truth states given estimates and RMSE  rootmean 
square error  Lower values are better  The results correspond to the  ltering task in sec  

the Cholesky factor    which requires      time  To
cope with timevarying systems and to make the method
more adaptive  we employ   forgetting factor        
such that the impact of the previous samples decays exponentially in time  Ljung   
Our proposed MPC algorithm  summarized in Algorithm
  is related to several algorithms and differs in both model
and controller learning  First  SSGPs are more robust to
modeling error than Locally Weighted Projection Regression  LWPR  used in iLQGLD  Mitrovic et al    See
  numerical comparison in  Gijsberts   Metta    Second  we ef ciently propagate uncertainty in multistep prediction which is crucial in MPC  In contrast  AGPiLQR
 Boedecker et al    drops the input uncertainty and
uses subset of regressors  SoRGP  which lacks   principled way to select reference points  In addition  PDDP  Pan
  Theodorou    uses GPs which are computationally
expensive for online optimization  Two deep neural networks are used for modeling in  Yamaguchi   Atkeson 
  which make it dif cult to perform online incremental learning  as we do here 
  Experimental Results
  Bayesian  ltering
     ONESTEP FILTERING
We consider   synthetic dynamical system with ground 
    and observation       
truth dynamics          
  sin    with        and        in   in  
similar setting to Deisenroth et al    We compare the
performance of four  lters  SSGPADF  SSGPEKF  GPADF  Deisenroth et al    and GPEKF  Ko   Fox 
  All models are trained using   samples  However  for SSGP models  only   random Fourier features
of   SE kernel are used  Figure   illustrates the comparison of  ltered state distribution of   typical realization 
We evaluate the methods by computing   Lx  the negative
loglikelihood of the ground truth samples in the  ltered
distribution  and RMSE  rootmean square error between
 ltered mean and ground truth samples  See Table   for  
detailed comparison  Our methods SSGPADF and SSGPEKF are able to offer close performance with their full GP
counterparts but with greatly reduced computational cost 
See Appendix   for further discussions on the comparison between SSGPADF and SSGPEKF 

        

  RECURSIVE FILTERING
We next consider   state estimation task in highspeed autonomous driving on   dirt track  Figure     The goal is
to recursively estimate the state of an autonomous rallycar
given noisy measurements  The vehicle state consists of
linear velocities    and    heading rate  and roll angle  in
body frame  Controls are steering and throttle  Measurements are collected by wheel speed sensors  This  ltering
task is challenging because of the complex nonlinear dynamics and the amount of noise in the measurements  We
do not use any prior model of the car  but learn the model
from ground truth estimates of vehicle state generated by
integrating GPS and IMU data via iSAM   Kaess et al 
    samples are collected from wheel speed sensors and ground truth state estimates from iSAM  for training  Because of the sample size  it is too computationally
expensive to use GPbased  lter such as GPADF  Deisenroth et al    Instead  we use SSGPADF to perform
  recursive  ltering steps which correspond to   seconds of highspeed driving  Filtered distributions using  
features are shown in Figure     and Figure    shows the
mean and twice the standard deviation of   Lx over six  
seconds driving with different number of features  Surprisingly  only need   small number of features is necessary for
satisfactory results 

  Model Predictive Control
  TRACKING   MOVING TARGET
We consider the Puma  robotic arm and quadrotor systems with dynamics model speci ed by SSGPs  For both
tasks the goal is to track   moving target  In addition  the
true system dynamics vary online  which necessitates both
online optimization and model update  as we do here  See
Appendix   for detailed task descriptions  Results in
terms of cost   xt  ut  are shown in Figure   Figure   
shows that our methods outperform iLQGLD  Mitrovic
et al    and AGPiLQR  Boedecker et al    The
similarities and differences between these methods have
been discussed in   Figure    shows that model update
is necessary and more features could improve performance 

  AUTONOMOUS DRIFTING
We study the control of an autonomous car during extreme
operating conditions  powerslide  The task is to stabilize
the vehicle to   speci ed steadystate using purely longitudinal control during highspeed cornering  This problem
has been studied in Velenis et al    where the authors
developed   LQR control scheme based on   physicsbased
dynamics model  We apply our MPC algorithm to this task
without any prior model knowledge and   data points
generated by the model in Velenis et al    SSGPLin
is used for multistep prediction  Results and comparison
to Velenis et al    are illustrated in Figure  

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

    GPADF   data points 

    SSGPADF   features 

    GPEKF   data points 

    SSGPEKF   features 

Figure   Black points are ground truth states  red areas are  lter distributions for     GPADF  Deisenroth et al   
    GPEKF  Ko   Fox    our proposed methods     SSGPADF and     SSGPEKF  The xaxis is the mean of initial
belief      which is randomly distributed in     and yaxis shows the mean and twice the standard deviation of
 ltered distribution        after observing   

    Autonomous driving task

    Filtered distribution vs  ground truth

      Lx vs    of features

Figure   Recursive  ltering task for highspeed autonomous driving  Figure     shows trajectories of all the states of    
seconds continuous driving   steps  where blue lines are the ground truth  and red lines and red areas are the mean
and twice the standard deviation of the  ltered distributions respectively  In     the red line and area are the mean and
twice the standard deviation of   Lx over six   seconds driving with varying number of features 

  Discussion and Conclusion
We introduced two analytic momentbased approaches to
prediction under uncertainty in sparse spectrum Gaussian
processes  SSGPs  Compared to their full GP counterparts  our methods are more general  they are applicable
to any continuous shiftinvariant kernel  They also scale
to larger datasets by leveraging random features with frequencies sampled from the spectral density of   given kernel  see Table     Although we adopt the name SSGP 
our proposed methods are not tied to speci   model learning methods such as linear Bayesian regression    azaroGredilla et al    They can be applied to any SSGP
with   speci ed feature weight distribution   and   and
  can be computed via different approaches  For example    can be iteratively computed by methods like doubly
stochastic gradient descent  Dai et al    We studied
the application of the proposed methods to Bayesian  ltering and model predictive control  Our methods directly
address the challenging aspects of these problems  model
uncertainty and realtime execution constraints  We evaluated our algorithms on realworld and simulated examples
and showed that SSGPEMM   and SSGPLin  
are accurate alternatives to their full GP counterparts when
learning from large amounts of data 
Acknowledgements
This work was supported by NSF NRI awards   and
 

Figure   Comparison of the drifting performance using  
 left     middle  and    right  random features  Blue
lines are the solution provided in Velenis et al    Performance improves with   larger number of features  and
with   moderate number of features  MPC with SSGPLin
behaves very closely to the ground truth solution 

    Robotic arm task cost

    Quadrotor task cost

Figure   Cost comparison for arm and quadrotor tasks 

 GPADF SSGPADF GPEKF SSGPEKF Start Start       Start             Time step CostOur method  SSGPEMM Our method  SSGPLin iLQGLD  LWPR AGPiLQR  SoRGP Time step CostWith model adaptation   feat With model adaptation   feat Without model adaptation   feat RHDDP with known modelPrediction under Uncertainty in Sparse Spectrum Gaussian Processes

References
Abbeel     Coates     Quigley     and Ng        An application
of reinforcement learning to aerobatic helicopter  ight  NIPS 
   

Archambeau  Cedric  Cornford  Dan  Opper  Manfred  and
ShaweTaylor  John  Gaussian process approximations of
stochastic differential equations  Gaussian Processes in Practice     

Kuss  Malte  Gaussian process models for robust regression  classi cation  and reinforcement learning  PhD thesis  Technische
Universit at   

  azaroGredilla     Qui noneroCandela     Rasmussen       
and FigueirasVidal        Sparse spectrum Gaussian process
regression  The Journal of Machine Learning Research   
   

Ljung  Lennart  System identi cation  Springer   

Boedecker     Springenberg  JT  Wul ng     and Riedmiller    
Approximate realtime optimal control based on sparse Gaussian process models  In ADPRL   pp    IEEE   

Minka  Thomas      family of algorithms for approximate
Bayesian inference  PhD thesis  Massachusetts Institute of
Technology   

Candela     Quinonero  Girard     Larsen     and Rasmussen 
      Propagation of uncertainty in Bayesian kernel modelsapplication to multiplestep ahead forecasting  In IEEE International Conference on Acoustics  Speech  and Signal Processing  IEEE   

Incremental variational
Cheng  ChingAn and Boots  Byron 
In Proceedings of Adsparse Gaussian process regression 
vances in Neural Information Processing Systems    NIPS 
 

Dai  Bo  Xie  Bo  He  Niao  Liang  Yingyu  Raj  Anant  Balcan 
MariaFlorina    and Song  Le  Scalable kernel methods via
doubly stochastic gradients  In Advances in Neural Information Processing Systems  pp     

Deisenroth     Fox     and Rasmussen     Gaussian processes
for dataef cient learning in robotics and control  IEEE Transsactions on Pattern Analysis and Machine Intelligence   
   

Deisenroth  Marc Peter  Huber  Marco    and Hanebeck  Uwe   
In ProAnalytic momentbased Gaussian process  ltering 
ceedings of the  th annual international conference on machine learning  pp    ACM   

Deisenroth  Marc Peter  Turner  Ryan Darby  Huber  Marco   
Hanebeck  Uwe    and Rasmussen  Carl Edward  Robust  ltering and smoothing with Gaussian processes  IEEE Transactions on Automatic Control     

Gijsberts     and Metta     Realtime model learning using incremental sparse spectrum Gaussian process regression  Neural
Networks     

Girard     Rasmussen       QuinoneroCandela     and MurraySmith     Gaussian process priors with uncertain inputs application to multiplestep ahead time series forecasting  In NIPS 
 

Kaess  Michael  Johannsson  Hordur  Roberts  Richard  Ila 
Viorela  Leonard  John    and Dellaert  Frank 
isam  Incremental smoothing and mapping using the bayes tree  The International Journal of Robotics Research     

Ko     and Fox     Gpbayes lters  Bayesian  ltering using gaussian process prediction and observation models  Autonomous
Robots     

Kupcsik     Deisenroth       Peters     Loh  AP  Vadakkepat 
   and Neumann     Modelbased contextual policy search for
dataef cient generalization of robot skills  Arti cial Intelligence   

Mitrovic     Klanke     and Vijayakumar     Adaptive optimal
feedback control with learned internal dynamics models 
In
From Motor Learning to Interaction Learning in Robots  pp 
  Springer   

Pan     and Theodorou     Probabilistic differential dynamic programming  In Advances in Neural Information Processing Systems  NIPS  pp     

Pan  Yunpeng  Theodorou  Evangelos  and Kontitsis  Michail 
Sample ef cient path integral control under uncertainty  In Advances in Neural Information Processing Systems  pp   
   

Rahimi     and Recht     Random features for largescale kernel machines  In Advances in neural information processing
systems  pp     

Rasmussen     and Kuss     Gaussian processes in reinforcement

learning  In NIPS  volume   pp     

Snelson     and Ghahramani     Sparse Gaussian processes using

pseudoinputs  NIPS     

Tassa     Erez     and Smart        Receding horizon differential

dynamic programming  In NIPS   

Titsias  Michalis    Variational learning of inducing variables in
In AISTATS  volume   pp   

sparse gaussian processes 
   

Velenis     Frazzoli     and Tsiotras     Steadystate cornering
equilibria and stabilisation for   vehicle during extreme operating conditions  International Journal of Vehicle Autonomous
Systems     

Williams        and Rasmussen       Gaussian processes for

machine learning  MIT Press   

Yamaguchi  Akihiko and Atkeson  Christopher    Neural networks and differential dynamic programming for reinforceIn   IEEE International Conment learning problems 
ference on Robotics and Automation  ICRA  pp   
IEEE   

Yan  Xinyan  Xie  Bo  Song  Le  and Boots  Byron  Largescale
Gaussian process regression via doubly stochastic gradient descent  The ICML Workshop on LargeScale Kernel Learning 
 

