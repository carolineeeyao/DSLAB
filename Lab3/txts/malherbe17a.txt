Global optimization of Lipschitz functions

  edric Malherbe   Nicolas Vayatis  

Abstract

The goal of the paper is to design sequential
strategies which lead to ef cient optimization of
an unknown function under the only assumption
that it has    nite Lipschitz constant  We  rst
identify suf cient conditions for the consistency
of generic sequential algorithms and formulate
the expected minimax rate for their performance 
We introduce and analyze    rst algorithm called
LIPO which assumes the Lipschitz constant to
be known  Consistency  minimax rates for LIPO
are proved  as well as fast rates under an additional   older like condition  An adaptive version
of LIPO is also introduced for the more realistic
setup where the Lipschitz constant is unknown
and has to be estimated along with the optimization  Similar theoretical guarantees are shown
to hold for the adaptive algorithm and   numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with
respect to stateof theart methods over typical
benchmark problems for global optimization 

  Introduction
In many applications such as complex system design or
hyperparameter calibration for learning systems  the goal
is to optimize the output value of an unknown function
with as few evaluations as possible  Indeed  in such contexts  evaluating the performance of   single set of parameters often requires numerical simulations or crossvalidations with signi cant computational cost and the operational constraints impose   sequential exploration of the
solution space with small samples  Moreover  it can generally not be assumed that the function has good properties such as linearity or convexity  This generic problem of sequentially optimizing the output of an unknown
and potentially nonconvex function is often referred to as

 CMLA  ENS Cachan  CNRS  Universit   ParisSaclay   
 name cmla ens 

Cachan  France  Correspondence to 
cachan fr 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

global optimization  Pint er    blackbox optimization
 Jones et al    or derivativefree optimization  Rios  
Sahinidis    There is   large number of algorithms
based on various heuristics which have been introduced
in order to solve this problem such as genetic algorithms 
modelbased methods or Bayesian optimization  We focus
here on the smoothnessbased approach to global optimization  This approach is based on the simple observation that 
in many applications  the system presents some regularity
with respects to the input  In particular  the use of the Lipschitz constant   rst proposed in the seminal works of  Shubert    Piyavskii    initiated an active line of research and played   major role in the development of many
ef cient global optimization algorithms such as DIRECT
 Jones et al    MCS  Huyer   Neumaier    or
SOO  Preux et al    Convergence properties of global
optimization methods have been developed in the works of
 Valko et al    Munos    under local smoothness
assumptions  but  up to our knowledge  such properties
have not been considered in the case where only the global
smoothness of the function can be speci ed  An interesting question is how much global assumptions on regularity
which cover in some sense local assumptions may improve
the convergence of the latter  In this work  we address the
following questions       nd the limitations and the best
performance that can be achieved by any algorithm over the
class of Lipschitz functions and  ii  design ef cient and optimal algorithms for this class of problems  Our contribution with regards to the above mentioned works is twofold 
First  we introduce two novel algorithms for global optimization which exploit the global smoothness of the function and display good performance in typical benchmarks
for optimization  Second  we show that these algorithms
can achieve faster rates of convergence on globally smooth
problems than the previously known methods which only
exploit the local smoothness of the function  The rest of the
paper is organized as follows  In Section   we introduce
the framework and give generic results about the convergence of sequential algorithms  In Section   we introduce
and analyze the LIPO algorithm which requires the knowledge of the Lipschitz constant  In Section   the algorithm
is extended to the case where the Lipschitz constant is unknown and the adaptive algorithm is compared to existing
methods in Section   All proofs can be found in the Supplementary Material provided as   separate document 

Global optimization of Lipschitz functions

  Setup and preliminary results
  Setup and notations
Setup  Let     Rd be   compact and convex set with nonempty interior and let           be an unknown function
which is only supposed to admit   maximum over its input domain     The goal in global optimization consists in
 nding some point

  cid    arg max

   

     

with   minimal amount of function evaluations  The standard setup involves   sequential procedure which starts
by evaluating the function       at an initial point   
and then selects at each step       an evaluation
point Xt      depending on the previous evaluations
                   Xt     Xt  and receives the evaluation
of the unknown function    Xt  at this point  After  
iterations  we consider that the algorithm returns an evaluation point     with      arg mini      Xi  which has
recorded the highest evaluation  The performance of the
algorithm over the function   is then measured after   iterations through the difference between the value of the true
maximum and the highest evaluation observed so far 

max
            max

   

   Xi 

The analysis provided in the paper considers that the number   of evaluation points is not  xed and it is assumed that
function evaluations are noiseless  Moreover  the assumption made on the unknown function   throughout the paper
is that it has    nite Lipschitz constant        

                        cid       cid       cid 

 cid        cid       

     
  Rd    cid       cid 

note by  cid   cid     cid  

Before starting the analysis  we point out that similar settings have also been studied in  Munos    Malherbe
et al    and that  Valko et al    Grill et al   
considered the noisy scenario 
Notations  For all                 xd    Rd  we dei   the standard  cid norm and
by              cid 
 cid       the ball
centered in   of radius       For any bounded set
    Rd  we de ne its innerradius as rad       max    
           such that              its diameter as
diam       max     cid      cid       cid 
 cid  and we denote by
     its volume where   stands for the Lebesgue measure  Lip                                    cid   
     cid       cid 
 cid          cid        denotes the class of kk  Lip    denotes
the set of Lipschitz continuous functions        stands for
the uniform distribution over   bounded measurable domain          for the Bernoulli distribution of parameter
      for the standard indicator function taking values in
    and the notation       means that the random variable   has the distribution   

Lipschitz functions de ned on   and cid 

  Preliminary results

In order to design ef cient procedures  we  rst investigate
the best performance that can be achieved by any algorithm
over the class of Lipschitz functions 
Sequential algorithms and optimization consistency  We
describe the sequential procedures that are considered here
and the corresponding concept of consistency in the sense
of global optimization 

De nition    SEQUENTIAL ALGORITHM  The class of
optimization algorithms we consider  denoted in the sequel
by    contains all the algorithms      At    completely
described by 
    distribution    taking values in   which allows to

generate the  rst evaluation point              

  An in nite collection of distributions  At    taking values in   and based on the previous evaluations which de ne the iteration loop       Xt   
At                   Xt     Xt 

Note that this class of algorithms also includes the deterministic methods in which case the distributions  At   
are degenerate  The next de nition introduces the notion of
asymptotic convergence 

De nition    OPTIMIZATION CONSISTENCY    global
optimization algorithm   is said to be consistent over  
set   of realvalued functions admitting   maximum over
  if and only if

        max

   

   Xi 

  max

         

 

where            Xn denotes   sequence of   evaluations
points generated by the algorithm   over the function   

Asymptotic performance  We now investigate the minimal conditions for   sequential algorithm to achieve asymptotic convergence  Of course  it is expected that   global
optimization algorithm should be consistent at least for the
class of Lipschitz functions and the following result reveals
  necessary and suf cient condition  NSC  in this case 

Proposition    CONSISTENCY NSC    global optimization algorithm   is consistent over the set of Lipschitz functions if and only if

    

   Lip    sup
   

    cid Xi     cid 
min

 

   

 cid 

  crucial consequence of the latter proposition is that the
design of any consistent method ends up to covering the
whole input space regardless of the function values  The
example below introduces the most popular space lling
method which will play   central role in our analysis 

Global optimization of Lipschitz functions

Example    PURE RANDOM SEARCH  The Pure Random
Search  PRS  consists in sequentially evaluating the function over   sequence of points                uniformly
and independently distributed over the input space     For
this method    simple union bound indicates that for all
      cid  and         we have with probability at least
 cid   
 cid  ln        ln   
      and independently of the function values 

 

sup
   

min
    cid Xi   cid    diam    

 

 

In addition to this result  we point out that the covering rate
of any method can easily be shown to be at best of order
      and thus subject to to the curse of dimensionality by means of covering arguments  Keeping in mind
the equivalence of Proposition   we may now turn to the
nonasymptotic analysis 
Finitetime performance  We investigate here the best
performance that can be achieved by any algorithm with
   nite number of function evaluations  We start by casting   negative result stating that any algorithm can suffer  at
any time  an arbitrarily large loss over the class of Lipschitz
functions 

Proposition   Consider any global optimization algo 
 cid 
rithm    Then  for any constant       arbitrarily large 
any       cid  and        
there exists   function
    
   Lip    only depending on             for which
we have with probability at least      
         max

    max
   

    Xi 

   

This result might however not be very surprising since the
class of Lipschitz functions includes functions with  nite 
but arbitrarily large variations  When considering the subclass of functions with  xed Lipschitz constant  it becomes
possible to derive  nitetime bounds on the minimax rate
of convergence 

Proposition    MINIMAX RATE  adapted from  Bull 
  For any Lipschitz constant       and any       cid 
the following inequalities hold true 
             

 cid 

 cid 

   
inf
    sup
  Lip   

 

max
            max

   

   Xi 

               
where      rad              diam          and the

expectation is taken over   sequence of   evaluation points
           Xn generated by the algorithm   over   

 

We point out that this minimax rate of convergence of order       can still be achieved by any method with
an optimal covering rate of order        Observe indeed that since    maxx           maxi      Xi     
     supx   mini    cid     Xi cid  for all     Lip   
then an optimal covering rate necessarily implies minimax
ef ciency  However  as it can be seen by examining the
proof of Proposition   provided in the Supplementary Material  the functions constructed to prove the limiting bound
of       are spikes which are almost constant everywhere and do not present   large interest from   practical
perspective  In particular  we will see in the sequel that one
can design 
   An algorithm with  xed constant      which achieves
minimax ef ciency and also presents exponentially
decreasing rates over   large subset of functions  as
opposed to space lling methods  LIPO  Section  

II    consistent algorithm which does not require the
knowledge of the Lipschitz constant and presents
comparable performance as when the constant   is assumed to be known  AdaLIPO  Section  

  Optimization with  xed Lipschitz constant
In this section  we consider the problem of optimizing an
unknown function   given the knowledge that     Lip   
for   given      
  The LIPO Algorithm
The inputs of the LIPO algorithm  Algorithm   are   number   of function evaluations    Lipschitz constant      
the input space   and the unknown function    At each
iteration         random variable Xt  is sampled uniformly over the input space   and the algorithm decides
whether or not to evaluate the function at this point  Indeed  it evaluates the function over Xt  if and only if
the value of the upper bound on possible values      
   cid  mini      Xi      cid     Xi cid  evaluated at this
point and computed from the previous evaluations is at least
equal to the value of the best evaluation observed so far
maxi      Xi  As an example  the computation of the
decision rule of LIPO is illustrated in Figure  

Algorithm   LIPO             
  Initialization  Let           
  Evaluate            
  Iterations  Repeat while      
  Let Xt         
  If min
   
  Evaluate    Xt           
  Output  Return     where      arg maxi      Xi 

    Xi         cid Xt    Xi cid    max

   Xi 

   

Global optimization of Lipschitz functions

More formally  the mechanism behind this rule can be explained using the active subset of consistent functions previously considered in active learning  see        Dasgupta 
  and  Hanneke   

De nition    CONSISTENT FUNCTIONS  The active subset of kLipschitz functions consistent with the unknown
function   over   sample                    Xt     Xt  of
      evaluations is de ned as follows 
Fk          Lip                        Xi       Xi 
Indeed  one can recover from this de nition the subset of
points which can actually maximize the function   
De nition    POTENTIAL MAXIMIZERS  Using the same
notations as in De nition   we de ne the subset of poten 
 cid 
tial maximizers estimated over any sample       evaluations with   constant       as follows 
Xk    

             Fk   such that     arg max

 cid 

    

   

 

We may now provide an equivalence which makes the link
with the decision rule of the LIPO algorithm 
Lemma   If Xk   denotes the set of potential maximizers
de ned above  then we have the following equivalence 

   

   Xi         cid     Xi cid    max

    Xk     min
Hence  we deduce from this lemma that the algorithm only
evaluates the function over points that still have   chance to
be maximizers of the unknown function 

   Xi 

   

Remark    EXTENSION TO OTHER SMOOTHNESS ASSUMPTIONS  It is important to note the proposed optimization scheme could easily be extended to   large number of sets of globally and locally smooth functions by
slightly adapting the decision rule  For instance  when
            cid  is unique and     
  cid      
         cid             cid   cid     denotes the set of functions
locally smooth around their maxima with regards to any
semimetric  cid              previously considered in
 Munos      straightforward derivation of Lemma  
directly gives that the decision rule applied in Xt  would

simply consists in testing whether maxi      Xi   
mini      Xi     cid Xt  Xi  However  since the purpose of this work is to design fast algorithms for Lipschitz
functions  we will only derive convergence results for the
version of the algorithm stated above 
  Convergence analysis

We start with the consistency property of the algorithm 

Proposition    CONSISTENCY  For any Lipschitz constant       the LIPO algorithm tuned with   parameter
  is consistent over the set kLipschitz functions      

     Lip    max

   

   Xi 

 

  max

         

The next result shows that the value of the highest evaluation observed by the algorithm is always superior or equal
in the usual stochastic ordering sense to the one of   PRS 

Proposition    FASTER THAN PURE RANDOM SEARCH 
Consider the LIPO algorithm tuned with any constant    
  Then  for any     Lip    and       cid  we have that
  cid 
       

 cid 

 cid 

     cid 

    cid 

max
   

   Xi     

max
   

      

            cid 

where            Xn is   sequence of   evaluation points
generated by LIPO and   cid 
  is   sequence of   independent random variables uniformly distributed over    
Based on this result  one can easily derive    rst  nitetime
bound on the difference between the value of the true maximum and its approximation 
Corollary    UPPER BOUND  For any     Lip       
 cid   
 cid  ln 
  cid  and         we have with probability at least      
max
            max

   Xi        diam      

   

 

 

 

This bound which assesses the miminax optimality of LIPO
stated in Proposition   does however not show any improvement over PRS and it cannot be signi cantly improved without any additional assumption as shown below 
Proposition   For any       cid  and         there exists   function      Lip    only depending on   and   for
which we have with probability at least      
         max

    rad      

 cid   

  max
   

 cid   

    Xi 

   

 

 

Figure   Left   Lipschitz function    sample of   evaluations and
the upper bound          cid  mini      Xi      cid     Xi cid 
in grey  Right  the set of points Xk                      
maxi      Xi  which satisfy the decision rule 

As announced in Section   one can nonetheless get
tighter polynomial bounds and even an exponential decay
by using the following condition which describes the behavior of the function around its maximum 

XXk  Global optimization of Lipschitz functions

Figure   Three onedimensional functions satisfying Condition  
with        Left         Middle  and        Right 

Condition    DECREASING RATE AROUND THE MAXIMUM    function           is     decreasing around
its maximum for some              if 
  The global optimizer   cid      is unique 
  For all         we have that 

     cid                  cid       cid cid 
   

This condition  already considered in the works of  Zhigljavsky   Pint er    and  Munos    captures how
fast the function decreases around its maximum  It can be
seen as   local onesided   older condition which can only
be met for       when   is assumed to be Lipschitz  As
an example  three functions satisfying this condition with
different values of   are displayed on Figure  
Theorem    FAST RATES  Let     Lip    be any Lipschitz function satisfying Condition   for some           
  Then  for any       cid  and         we have with
probability at least      
max
            max

   Xi        diam    

   

 cid 
 cid 

 

 cid 
 cid   

 

 

  

     

     

exp

  Ck   

  ln 

ln          

 
 

    Ck   

          
ln          

where Ck        maxx    cid       cid cid       

The last result we provide states an exponentially decreasing lower bound 
Theorem    LOWER BOUND  For any     Lip    satisfying Condition   for some              and any       cid 
and         we have with probability at least      
   rad        

 cid 
    ln ln 

   cid 

   

  max

            max

   

   Xi 

  discussion on these results can be found in the next section where LIPO is compared with similar algorithms 

  Comparison with previous works

The Piyavskii algorithm  Piyavskii    is   Lipschitz method with  xed       consisting in sequentially evaluating the function over   point Xt   
arg maxx   mini      Xi         cid     Xi cid  maximizing
the upper bound displayed on Figure  
 Munos   
also proposed   similar algorithm  DOO  which uses   hierarchical partitioning of the space in order to sequentially
expand and evaluate the function over the center of   partition which has the highest upper bound computed from
  semimetric  cid  set as input  Up to our knowledge  only
the consistency of the Piyavskii algorithm was proven in
 Mladineo    and  Munos    derived  nitetime
bounds for DOO with the use of weaker local assumptions  To compare our results  we thus considered DOO
tuned with  cid      cid       cid       cid 
 cid  over          partitioned into    dary tree of hypercubes and with   belonging to the sets of globally smooth functions      Lip       
          Lip    satisfying Condition   with         
 cid 
and      
                           cid           
    cid       cid cid 
  The results of the comparison can be found
in Table  
In addition to the novel lower bounds and
the rate over Lip    we were able to obtain similar upper bounds as DOO over    uniformly better rates for the
functions in  
  with      
and   similar exponenital rate  up to   constant factor  when
      Hence  when   is only known to be kLipschitz 
one thus should expect the algorithm exploiting the global
smoothness  LIPO  to perform asymptotically better or at
least similarly to the one using the local smoothness  DOO 
or no information  PRS  However  keeping in mind that
the constants are not necessarily optimal  it is also interesting to note that the term          appearing in both the
exponential rates of LIPO and DOO tends to suggest that
if   is also known to be locally smooth for some   cid   cid    
then one should expect an algorithm exploiting the local
smoothness   cid  to be asymptotically faster than the one using the global smoothness   in the case where      

 cid 
  locally equivalent to  cid   cid      cid 

   

   

LIPO

DOO

 cid 
 

       

   

   

Piyavskii

PRS

      
 

      
     

 cid 
   
OP  

 cid 
   
OP  

   
      
   
 

Algorithm
    Lip   
Consistency
Upper Bound
            
Upper bound
Lower bound
      cid 
Upper bound
Lower bound
      cid 
Upper bound    
Lower bound
Table   Comparison of the results reported over the difference
maxx           maxi      Xi  in Lipschitz optimization 
Dash symbols are used when no results could be found 

                
 

   
      
   
 

         
 
   

      
     

      
 

   
   

   
   

   
   

   
   

   
   

   

   

OP  
    

OP  
    

  

     

   

 
  ln 

  

   

   

  

  

OP  
    

 cid 
 

 
 

 
 

       

  ln 

 

 Global optimization of Lipschitz functions

  Optimization with unknown Lipschitz

constant

unknown function   in the class cid 

In this section  we consider the problem of optimizing any

   Lip   

  The adaptive algorithm
The AdaLIPO algorithm  Algorithm   is an extension of
LIPO which involves an estimate of the Lipschitz constant
and takes as input   parameter         and   nondecreasing sequence of Lipschitz constant ki   de ning   meshgrid of          such that               with ki      
ki  The algorithm is initialized with   Lipschitz constant
    set to   and alternates randomly between two distinct
phases  exploration and exploitation  Indeed  at step       
  Bernoulli random variable Bt  of parameter   driving
this tradeoff is sampled  If Bt      then the algorithm
explores the space by evaluating the function over   point
uniformly sampled over     Otherwise  if Bt      the
algorithm exploits the previous evaluations by making an
iteration of the LIPO algorithm with the smallest Lipschitz
constant of the sequence  kt which is associated with   subset of Lipschitz functions that probably contains    step abbreviated in the algorithm by Xt        kt    Once an
evaluation has been made  the Lipschitz constant estimate
 kt is updated 

Remark    EXAMPLES OF MESHGRIDS  Several sequences of Lipschitz constants with various shapes such as
ki      sgn    ln       sgn    or        for some      
could be considered to implement the algorithm  In particular  we point out that with these sequences the computation of the estimate is straightforward  For instance 
when ki           we have  kt        it where it  
 cid ln maxi cid       Xj       Xl cid Xj   Xl cid  ln     cid 
  Convergence analysis
Lipschitz constant estimate  Before starting the analysis
of AdaLIPO  we  rst provide   control on the Lipschitz
constant estimate based on   sample of random evaluations
that will be useful to analyse its performance  In particular  the next result illustrates the purpose of using   discretization of Lipschitz constant instead of   raw estimate
of the maximum slope by showing that  given this estimate 
  small subset of functions containing the unknown function can be recovered in    nitetime 

Then 

Proposition   Let   be any nonconstant Lipschitz funcif  kt denotes the Lipschitz constant estition 
mate of Algorithm   computed with any increasing sequence ki   de ning   meshgrid of    over   sample
                   Xt     Xt  of       evaluations where
           Xt are uniformly and independently distributed

Algorithm   ADALIPO       ki          
  Initialization  Let           
  Evaluate                    
  Iterations  Repeat while      
  Let Bt        
  If Bt       Exploration 
  Let Xt         
  If Bt       Exploitation 
  Let Xt        kt    where   kt   denotes the set
  of potential maximizers introduced in De nition  
 cid 
  computed with   set to  kt
  Evaluate    Xt           
  Let  kt   inf
  Output  Return     where      arg maxi      Xi 

    Xi       Xj 
 cid Xi   Xj cid    ki

ki     max
  cid  

 cid 

  cid 

 cid 
over     we have that
    Lip kt 
 cid 
where the coef cient

    ki cid       

              ki cid   cid   cid 
 cid 

              

 cid        cid 

  ki cid   

   

with   cid    min             Lip ki  is strictly positive 
Remark    MEASURE OF GLOBAL SMOOTHNESS  The
coef cient     ki cid    which appears in the lower bound
 cid cid   cid 
of Proposition   can be seen as   measure of the global
smoothness of the function   with regards to ki cid    Indeed 
     Xi    Xi cid   cid   
observing that  cid   cid 
      ki cid    it is easy to see
ki cid   cid Xi     cid   cid   cid 
that   records the ratio of volume the product space     
where   is witnessed to be at least ki cid    Lipschitz 

  
 

Remark    DENSITY OF THE SEQUENCE  As   direct
consequence of the previous remark  we point out that
the density of the sequence ki    captured here by    
supi   ki    ki ki has opposite impacts on the maximal deviation of the estimate and its convergence rate 
Indeed  since   is involved in both the following upper
bounds on the deviation  limt   kt     cid   cid      where
  cid    sup             Lip    and on the coef cient
    ki cid            cid      we deduce that using  
sequence with   small   reduces the bias but also the convergence rate through   small coef cient     ki cid 

Analysis of AdaLIPO  Given the consistency equivalence
of Proposition   one can directly obtain the following
asymptotic result 

Proposition    CONSISTENCY  The AdaLIPO algorithm
tuned with any parameter         and any sequence of

Global optimization of Lipschitz functions

Lipschitz constant ki   covering    is consistent over the
set of Lipschitz functions      

    

   Lip    max

   

   Xi 

 

  max

         

 cid 

The next result provides    rst  nitetime bound on the difference between the maximum and its approximation 

Proposition    UPPER BOUND  Consider AdaLIPO
 cid 
tuned with any         and any sequence ki   de ning   meshgrid of    Then  for any nonconstant    
   Lip    any       cid  and         we have with
probability at least      
 cid  ln 
 cid   
max
            max

   Xi    diam      

 cid   

 cid   

  ln 

   

 

 

ki cid   

 

 

  ln        ki cid 

 

 

where     ki cid    and   cid  are de ned as in Proposition  

This result might be misleading since it advocates that doing pure exploration gives the best rate       when      
However  as Proposition   provides us with the guarantee that     Lip kt  within    nite number of iterations
where  kt denotes the Lipschitz constant estimate  one can
recover faster convergence rates similar to the one reported
for LIPO where the constant   is assumed to be known 

Theorem    FAST RATES  Consider the same assumptions as in Proposition   and assume in addition that the
function   satis es Condition   for some             
Then  for any       cid  and         we have with probability at least      
 cid 
            max
max
ki cid    exp

   Xi    diam    
  ln   ki cid        ln 
    

 cid 

  ln 

   

 

 cid 
 cid 

 

exp

 Cki cid     

         ln 

  ln          

 

    Cki cid     

             
  ln          
where Cki cid          maxx    cid       cid cid 

 

 cid 

 

     

 cid   

  

 

     

 ki cid     

This bound shows the precise impact of the parameters  
and ki   on the convergence of the algorithm  In particular 
it illustrates the complexity of the exploration exploitation
tradeoff through   constant term and   convergence rate
which are inversely correlated to the exploration parameter
and the density of the sequence of Lipschitz constants 

  Comparison with previous works

The DIRECT algorithm  Jones et al    is   Lipschitz
algorithm with unknown constant which uses   deterministic splitting technique of the search space to evaluate the
function on subdivisions of the space that have recorded
the highest evaluation among all subdivisions of similar
size  Moreover   Munos    generalized DIRECT in
  broader setting by extending DOO to any unknown and
arbitrary local semimetric  With regards to these works 
we proposed an alternative stochastic strategy which directly relies on the estimation of the Lipschitz constant and
thus only presents guarantees for globally smooth functions  However  as far as we know  only the consistency
property of DIRECT was shown in  Finkel   Kelley   
and  Munos    derived convergence rates of the same
order as for DOO  except that the best rate they derive is
 
of order      
   to be compared with the fast rate of
AdaLIPO which is of order       cn  The conclusion of
the comparison thus remains the same as in Section   exploiting the global smoothness instead of just the local one
allows to derive faster algorithms in the some cases where
the unknown function is indeed globally smooth 
  Experiments
We compare here the empirical performance of AdaLIPO
with  ve stateof theart global optimization methods 
Algorithms  BAYESOPT   MartinezCantin    is  
Bayesian optimization algorithm which uses   distribution
over functions to build   surrogate model of the unknown
function  The parameters of the distribution are estimated
during the optimization process  CMAES Hansen   
is an evolutionary algorithm which samples the new evaluation points according to   multivariate normal distribution
with mean vector and covariance matrix computed from the
previous evaluations  CRS Kaelo   Ali    is   variant of PRS including local mutations which starts with  
random population and evolves these points by an heuristic rule  MLSL Kan   Timmer    is   multistart algorithm performing   series of local optimizations starting from points randomly chosen by   clustering heuristic that helps to avoid repeated searches of the same local optima  DIRECT Jones et al    and PRS were
previously introduced  For   fair comparison  the tuning
parameters were all set to default and AdaLIPO was constantly used with   parameter   set to   and   sequence
ki             xed by an arbitrary rule of thumb   
Data sets  Following the steps of  Malherbe   Vayatis 
  we  rst studied the task of estimating the regularization parameter   and the bandwidth   of   gaussian kernel
ridge regression minimizing the empirical mean squared
 In Python   from  BayesOpt  MartinezCantin   

 CMA    Hansen    and  NLOpt  Johnson   

Global optimization of Lipschitz functions

Yacht

 
 
 
 
 
 

 
 

 

 
 
 
 
 
 

 
 

 

 
 
 
 
 
 

 
 

 

           

     cid  

   
   
   
   
   
   
   
   
   
   
   
   
   
 
   
   
   
   
   
   
 

Deb   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Housing
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
 

Sphere
   
   
   
   
   
 
 
   
   
   
   
   
 
 
   
   
   
   
   
 
 

AutoMPG BreastCancer Concrete
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
 
 

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
 

HolderTable Rosenbrock LinearSlope
   
   
   
   
   
   
   
   
   
   
   
   
   
 
   
   
   
   
   
   
   
   
   
   
   
   
 
   
   
   
   
   
   
   
   
   
   
   
   
   
   
 

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
 

Problem
AdaLIPO
BayesOpt
CMAES
CRS
DIRECT
MLSL
PRS
AdaLIPO
BayesOpt
CMAES
CRS
DIRECT
MLSL
PRS
AdaLIPO
BayesOpt
CMAES
CRS
DIRECT
MLSL
PRS
Table   Results of the numerical experiments  The table displays the number of evaluations required by each method to reach the
speci ed target  mean   standard deviation  In bold  the best result obtained in terms of average of function evaluations 
error of the predictions over    fold cross validation
with real data sets  The optimization was performed over
 ln  ln              with  ve data sets from
the UCI Machine Learning Repository  Lichman   
AutoMPG  Breast Cancer Wisconsin  Prognostic  Concrete slump test  Housing and Yacht Hydrodynamics  We
then compared the algorithms on   series of  ve synthetic
problems commonly met in standard optimization benchmark taken from  Jamil   Yang    Surjanovic   Bingham    HolderTable  Rosenbrock  Sphere  LinearSlope and Deb    This series includes multimodal and
nonlinear functions as well as illconditioned and wellshaped functions with   dimensionality ranging from   to  
  complete description of the test functions of the benchmark can be found in the Supplementary Material 
Protocol and performance metrics  For each problem
and each algorithm  we performed       distinct runs
with   budget of     function evaluations  For each
target parameter         and   we have collected the stopping times corresponding to the number of
evaluations required by each method to reach the speci ed
target      min                           
    ftarget   
where min      by convention           
   de 
  
notes the evaluations made by   given method on the kth
run with       and the target value is set to ftarget     
maxx        
 
       The normalization of the target to the average
value prevents the performance measures from being dependent of any constant term in the unknown function  In
practice  the average was estimated from   Monte Carlo
sampling of   evaluations and the maximum by taking
the best value observed over all the sets of experiments 
Based on these stopping times  we computed the average
and standard deviation of the number of evaluations rek       and

Results  Results are collected in Table   Due to space
constraints  we only make few comments  First  we point
out that the proposed method displays very competitive results over most of the problems of the benchmark  except
on the nonsmooth DebN  where most methods fail  In
particular  AdaLIPO obtains several times the best performance for the target   and    see       BreastCancer 
HolderTable  Sphere  and experiments Linear Slope and
Sphere also suggest that  in the case of smooth functions  it
can be robust against the dimensionality of the input space 
However  in some cases  the algorithm can be witnessed
to reach the   target with very few evaluations while
getting more slowly to the   target  see       Concrete 
Housing  This problem is due to the instability of the Lipschitz constant estimate around the maxima but could certainly be solved with the addition of   noise parameter that
would allow the algorithm be more robust against local perturbations  Additionally  investigating better values for  
and ki as well as alternative covering methods such as LHS
 Stein    could also be promising approaches to improve its performance  However  an empirical analysis of
the algorithm with these extensions is beyond the scope of
the paper and will be carried out in   future work 

  Conclusion
We introduced two novel strategies for global optimization 
LIPO which requires the knowledge of the Lipschitz constant and its adaptive version AdaLIPO which estimates
the constant during the optimization process    theoretical analysis is provided and empirical results based on synthetic and real problems have been obtained demonstrating
the performance of the adaptive algorithm with regards to
existing stateof theart global optimization methods 

quired to reach the target             cid  

 

          dx    cid 
 cid 

 cid maxx          

 

Global optimization of Lipschitz functions

References
Bull  Adam    Convergence rates of ef cient global optimization algorithms  The Journal of Machine Learning
Research     

Dasgupta  Sanjoy  Two faces of active learning  Theoretical

Computer Science     

Finkel  Daniel   and Kelley  CT  Convergence analysis of
the direct algorithm  Optimization Online Digest   

Grill  JeanBastien  Valko  Michal  and Munos    emi 
Blackbox optimization of noisy functions with unIn Neural Information Processing
known smoothness 
Systems   

Hanneke  Steve  Rates of convergence in active learning 

The Annals of Statistics     

Hansen  Nikolaus  The cma evolution strategy    comparing review  In Towards   New Evolutionary Computation  pp    Springer   

Hansen  Nikolaus  The cma evolution strategy    tutorial  Retrieved May     from http www 
lri fr hansen cmaesintro html   

Huyer  Waltraud and Neumaier  Arnold  Global optimization by multilevel coordinate search  Journal of Global
Optimization     

Jamil  Momin and Yang  XinShe    literature survey of
benchmark functions for global optimization problems 
International Journal of Mathematical Modelling and
Numerical Optimisation     

Johnson  Steven    The NLopt nonlinearoptimization
package  Retrieved May     from http 
abinitio mit edu nlopt   

Jones  Donald    Perttunen  Cary    and Stuckman 
Bruce    Lipschitzian optimization without the lipschitz
constant  Journal of Optimization Theory and Applications     

Jones  Donald    Schonlau  Matthias  and Welch 
William    Ef cient global optimization of expensive
blackbox functions  Journal of Global Optimization 
   

Kaelo  Professor and Ali  Montaz  Some variants of the
controlled random search algorithm for global optimization  Journal of Optimization Theory and Applications 
   

Kan  AHG Rinnooy and Timmer  Gerrit    Stochastic
global optimization methods part    Clustering methods 
Mathematical Programming     

Lichman  Moshe  UCI machine learning repository   

URL http archive ics uci edu ml 

Malherbe    edric and Vayatis  Nicolas 

approach to global optimization 
arXiv   

  ranking
arXiv preprint

Malherbe    edric  Contal  Emile  and Vayatis  Nicolas   
ranking approach to global optimization  In In Proceedings of the  st International Conference on Machine
Learning  pp     

MartinezCantin  Ruben  Bayesopt    bayesian optimization library for nonlinear optimization  experimental design and bandits  The Journal of Machine Learning Research     

Mladineo  Regina Hunter  An algorithm for  nding the
global maximum of   multimodal  multivariate function 
Mathematical Programming     

Munos    emi  From bandits to montecarlo tree search 
The optimistic principle applied to optimization and
planning  Foundations and Trends   cid  in Machine Learning     

Pint er    anos    Global optimization in action  Scienti  

American     

Piyavskii  SA  An algorithm for  nding the absolute extremum of   function  USSR Computational Mathematics and Mathematical Physics     

Preux  Philippe  Munos    emi  and Valko  Michal  Bandits
In Evolutionary Compuattack function optimization 
tation  CEC    IEEE Congress on  pp   
IEEE   

Rios  Luis Miguel and Sahinidis  Nikolaos    Derivativefree optimization    review of algorithms and comparison of software implementations  Journal of Global Optimization     

Shubert  Bruno      sequential method seeking the global
maximum of   function  SIAM Journal on Numerical
Analysis     

Stein  Michael  Large sample properties of simulations
using latin hypercube sampling  Technometrics   
   

Surjanovic  Sonja and Bingham  Derek  Virtual library of
simulation experiments  Test functions and datasets  Retrieved May     from http www sfu ca 
 ssurjano   

Valko  Michal  Carpentier  Alexandra  and Munos    emi 
In In
Stochastic simultaneous optimistic optimization 
Proceedings of the  th International Conference on
Machine Learning  pp     

Global optimization of Lipschitz functions

Zhigljavsky       and Pint er       Theory of Global Random Search  Mathematics and its Applications  Springer
Netherlands    ISBN  

