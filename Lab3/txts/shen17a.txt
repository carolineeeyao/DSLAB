On the Iteration Complexity of Support Recovery

via Hard Thresholding Pursuit

Jie Shen   Ping Li  

Abstract

  Blumensath   Davies    Bouchot et al   

Recovering the support of   sparse signal from
its compressed samples has been one of the most
important problems in high dimensional statistics  In this paper  we present   novel analysis for
the hard thresholding pursuit  HTP  algorithm 
showing that it exactly recovers the support of
an arbitrary ssparse signal within       log  
iterations via   properly chosen proxy function 
where   is the condition number of the problem 
In stark contrast to the theoretical results in the
literature  the iteration complexity we obtained
holds without assuming the restricted isometry
property  or relaxing the sparsity  or utilizing the
optimality of the underlying signal  We further
extend our result to   more challenging scenario 
where the subproblem involved in HTP cannot be
solved exactly  We prove that even in this setting 
support recovery is possible and the computational complexity of HTP is established  Numerical study substantiates our theoretical results 

  Introduction

In the last two decades  pursuing   sparse representation
for high dimensional data has become one of the most signi cant problems in machine learning  To seek   sparse
solution    large body of work is devoted to ef cient methods  including the convex formulation  for instance  basis pursuit  Chen et al    and the Lasso  Tibshirani 
  as well as greedy pursuits       orthogonal matching pursuit  Pati et al   
iterative hard thresholding  Daubechies et al    and hard thresholding pursuit  HTP   Foucart    along with elegant theoretical
understanding on parameter estimation and support recovery in either ideal setting or noisy scenario  Cand es   Tao 
  Wainwright    Tropp   Gilbert    Cai et al 

 Rutgers University  Piscataway  New Jersey  USA  Jie Shen 

js rutgers edu  Ping Li  pingli stat rutgers edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Compared to parameter estimation       bounding the  
distance between the solution and the desired sparse signal  support recovery is   much more challenging task and
it usually requires more stringent conditions  See Tropp
  Zhao   Yu   Yuan   Lin   Zhang
  for some early results and Nguyen   Tran  
Loh   Wainwright   for more recent developments 
Nevertheless  if the support of   signal can be predicted by
  method  then the solution returned by the method immediately enjoys the oracle property       with optimal statistical rate  Wainwright    Thereby  support recovery has
received broad attention in recent years  Osher et al   
Wang et al    Bouchot et al   

In this work  we follow the research line with   particular
interest in the hard thresholding pursuit algorithm  which
exhibits encouraging performance among many machine
learning applications  The algorithm was originally presented by Foucart   for recovering the true signal in
compressed sensing  Donoho    Yuan et al   
suggested using the HTP algorithm for general sparsityconstrained machine learning problems  and they showed
that the solution obtained from HTP converges with   geometric rate  Very recently    rigorous theoretical analysis on
when HTP guarantees support recovery was independently
carried out by Bouchot et al    and Yuan et al   
In Bouchot et al    they considered the compressed
sensing problem and illustrated that HTP recovers the support of the true signal in  nite iterations if the restricted
isometry property  RIP  condition holds  Cand es   Tao 
  Yuan et al    showed that in some situations 
HTP eventually terminates and guarantees support recovery without assuming the RIP condition 

Although these appealing theoretical results characterize
the behavior of HTP in particular regimes  it turns out
that   thorough understanding on when HTP identi es the
support of an arbitrary sparse signal is missing in the literature  To be more precise  the RIP condition used in
Bouchot et al    amounts to imposing   small condition number for the underlying problem  which may not be
practical for machine learning applications where the condition number usually grows with the sample size  To guar 

Support Recovery of Hard Thresholding Pursuit

antee the support recovery of an ssparse signal  Yuan et al 
  required that the signal of interest is the unique
global minimizer of   sparsityconstrained program  which
invokes the RIP condition  or that HTP maintains denser
iterates  This poses an interesting question of whether HTP
is able to recover the support without the RIP assumption 
or the optimality of the signal  or the relaxed sparsity 

In addition  an insightful analysis on the performance of
HTP in   realistic scenario is missing  For concreteness 
recall that HTP proceeds as follows 

 HTP  bt    xt       xt 
 HTP  St    supp cid bt    cid   

 HTP  xt    arg min

     

supp       

where       is   step size  supp cid bt    cid  denotes the sup 

port of the   largest absolute elements of bt  and      
is   properly chosen function  For general machine learning problems  we are only guaranteed with  approximate
solutions in the third step       for all      
     

   xt       xt 

 

 

where xt 
is the global minimizer of       restricted on
St  Related to the inexact solutions    natural question
to ask is how the accuracy parameter   affects the recovery
performance of HTP  additively or progressively 

Another issue coming up with the inexact iterates is that
the usually employed stopping criterion St    St may
not be valid  which makes part of the analysis in Yuan et al 
  not applicable to this setting  Note that when exact
solutions are available  HTP becomes stationary as soon as
the detected support does not change  since the solutions
are entirely determined by the support  Yuan et al   
made use of this feature to establish theoretical guarantee
for HTP  However  allowing approximate iterates quickly
changes the premise because many stochastic solvers      
stochastic gradient descent  introduce randomness  rendering  HTP  outputs different results even restricted on the
same support set 

  Contribution

We make the following contribution in this paper  First 
suppose that  HTP  has exact solutions  we show that under very mild conditions  HTP either terminates early or
guarantees support recovery of an arbitrary ssparse signal
within       log   iterations  Then we move on to the inexact case  and prove that under the RIP condition or using
  relaxed sparsity  support recovery with the same iteration complexity holds provided that the optimization error
  is small compared to the magnitude of the target signal 
As   consequence  we present the  rst bound on the computational complexity of HTP  For concreteness  we relate

our deterministic results to two prevalent statistical models 
and show that the conditions involved in our theorems can
be met with high probability 

We also revisit the role of       of the HTP algorithm  Previous work  for example  Jain et al    tends to treat
      as an objective function  the choice of which depends
on the underlying problem and the signal  and views HTP
as an optimization procedure towards the optimal solution 
Interestingly  we  nd that       behaves more like   proxy
function that guides HTP to the target signal  Hence  to recover   signal  we have many more choices of       as far
as it satis es the conditions to be present  see Section  

From   high level  the paper shares the same merit of
Bouchot et al    Yuan et al         recovering  
sparse signal  Hence  part of our proof is inspired by their
work  Yet  we establish novel RIPfree results based on  
more careful analysis for the problem structure  See   detailed comparison in Section  

  Notation

Throughout the paper  we use bold lowercase letters      
   to denote   column vector  The support of   vector  
is denoted by supp     whereas that of the largest   absolute elements is denoted by supp        Both kvk  and
 supp     are used to count the nonzeros in    Suppose
that                    is an index set  then for     Rd    
can either be explained as an  dimensional vector or  
ddimensional vector with the elements outside of   set to
zero  The Euclidean norm of   vector   is denoted by kvk 
We write boldface capital letters          for matrices  and
the transpose is denoted by   
The ssparse vector      Rd is the target signal we aim to
recover  and we reserve the capital letter   for its support 
We de ne  xmin     as the absolute value of the smallest
element  in magnitude  of  xS   Rs  With   slight abuse
of the notation   kF     should be explained as the vector
consisting of the top   elements  in magnitude  of       
rather than the kth component of       

  Roadmap

The remainder of the paper is organized as follows  Section   introduces the problem setting and some preliminary
results that the main theorems build on  Section   presents
the main results of this paper with   detailed comparison
In Section   we specialize our
to closely related work 
results to two concrete statistical models    proof sketch
of the main results is given in Section   Next  we verify our theoretical results with extensive numerical study
in Section   and Section   concludes the paper  Technical
lemmas and the full proof are deferred to the appendix  see
the supplementary  le 

Support Recovery of Hard Thresholding Pursuit

  Problem Setup and Preliminary Results

In this section  we introduce the problem setting and some
preliminary consequences on which our main results build 

To be clear  the target signal      Rd we consider in this

paper is only endowed with sparsity 

Our analysis depends on the following two properties of the
function      

De nition     differentiable function       is said to be
restricted strongly convex  RSC  with parameter mK    
if for all vectors   and    with kx            
                                 
De nition     differentiable function       is said to be
restricted smooth  RSS  with parameter MK     if for all
vectors   and    with kx            
                                 

MK
  kx         

mK
  kx         

In particular  we require that the RSC condition holds at
sparsity level       and the RSS condition holds at sparsity
level     respectively  That is 

          is mk srestricted strongly convex 

          is   krestricted smooth 

Note that the RSC and RSS conditions are now standard and are widely utilized for establishing performance
guarantees for   variety of popular algorithms  See  for
example  Negahban et al    Agarwal et al   
Jain et al    and Loh   Wainwright   For simplicity  throughout the paper we write     mk   and
         We also denote         which is actually
the  restricted  condition number of the problem 

The  rst result states that if  HTP  outputs exact solutions 
then HTP decreases the function value with   geometric
rate before the stopping criterion       St    St  is met 
Formally  we have the following proposition 

Proposition   Consider the HTP algorithm with exact solutions in  HTP  Assume     and     pick       
in  HTP  and set       in  HTP  Then before HTP
terminates  it holds that for all      

where

   xt             cid    xt         cid   
     
       

          

     

holds even for    xt              It is also worth men 

tioning that by the proposition  we can deduce

   xt               cid              cid   

However  the above inequality does not imply the conver 

gence of     xt    since    xt          is not bounded

from below  Rather  it is invoked to establish parameter
estimation for HTP 

The following proposition shows that when the conditions
in Prop    are satis ed  we have an accurate estimate on the
signal in the   metric 

Proposition   Assume same conditions as in Prop   
Then before HTP terminates  the following holds for      

 cid cid xt      cid cid   

   cid cid         cid cid   

where   is given in Prop   

 
      sF       

In the literature    variety of work has established theoretical guarantees on parameter estimation  either under
the RIP condition  Bouchot et al    or by relaxing the
sparsity  Yuan et al    In contrast  neither of the conditions are assumed in Prop    owing to   careful analysis

on the connection between     xt  and     See the supple 

mentary  le for the proof  However  we point out that such
an appealing behavior is not guaranteed if  HTP  does not
output exact solutions  and in this case  we have to relax
the sparsity or use the RIP condition  In particular  let

xt

    arg min
supp      

     

and consider that  HTP  outputs xt obeying

supp cid xt cid    St     xt       xt

     

 

Note that this is   realistic scenario because even for simple functions             is the logistic loss  convex solvers
only ensure  approximate solutions  The major issue coming up with the  approximate solutions is that the gradient
of       evaluated at xt does not vanish on the support St 
which makes our technical analysis of Prop    invalid  Yet 
we can still bound it under proper conditions 

Lemma   Assume     and   Then at any iteration
      we have

    

 cid cid        xt cid cid   

Based on the lemma  we show the following RIPbased result for parameter estimation 

Remark  Note that we did not assume the optimality of   
with respect to the function       In other words  Prop   

Proposition   Consider the HTP algorithm with inexact
solutions   Suppose that the condition number      

Support Recovery of Hard Thresholding Pursuit

and set       in  HTP  Then picking        with
              guarantees

 cid cid xt      cid cid           cid cid         cid cid 

 
      sF       

 

    

 

 

As the RIP condition is hard to ful ll for many machine
learning problems  Jain et al    proposed to relax the

sparsity parameter       cid   cid  in order to alleviate it 

Shen   Li   further showed that by relaxing the sparsity    stochastic solver is able to produce an accurate solution for sparsityconstrained programs  Inspired by their
interesting work  we derive the following result for HTP 

Proposition   Consider the HTP algorithm with inexact
      Then

solutions   Pick        and let            
 cid cid xt      cid cid   

   cid cid         cid cid 
      sF          

 

 

 

      

where

       

          

 

 

  Main Results

This section is dedicated to   deterministic analysis on the
performance of HTP  We  rst treat the exact case      
 HTP  outputs exact solutions  along with   detailed comparison with previous work in the literature  Then we
demonstrate that even when  HTP  is solved approximately up to an  accuracy  support recovery is still possible provided that   is small enough compared to the magnitude of the target signal 

The following theorem is one of the main results in the paper  It justi es that under proper conditions  HTP recovers
the support of    using  nite iterations 

Theorem   Consider the HTP algorithm with exact solutions in  HTP  Assume     and     Pick        in
 HTP  and       in  HTP  Then HTP either terminates
early  or recovers the support of    using at most

tmax  cid    log  
iterations  provided that for some constant        

  log     

   cid   xk   

log 

log 

 

     

 xmin  

  

    sF       

 

Above  the quantity   is given by

       

          

     

     

In the theorem  we recall that  xmin is the minimum absolute value of the nonzeros of     Below we discuss the
important messages conveyed by the theorem and contrast
our result to prior work  For ease of exposition  we write

       for some constant         and it quickly
indicates that            
Iteration complexity  We remind that the  rst term in  
plays the most crucial role  since it upper bounds the other
two for suf ciently large   In the regime where   itself is
bounded by   constant from above  the iteration complexity
is simply explained as      xk  Asymptotically  we can
show that the iteration complexity is dominated by   log  
as   tends to in nity  that is 

tmax        xk    log    

This follows from   simple calculation on the Taylor expansion of log  at the point       with   being replaced
with         Note that the number of iterations we
obtained for support recovery is as few as that for accurate
parameter estimation  see Prop    It is also worth mentioning that the linear dependency on the sparsity of    is
nearly optimal  because in the worst case HTP may take
several steps to pick only one correct support 

Conditions  We also emphasize that the condition   is
now ubiquitous for analyzing the support recovery performance  The quantity  xmin involved is natural  because
  signal with large magnitude is easier to recover than
those with small or vanishing components  To see why
    sF      is used to lower bound the magnitude of    
let us consider the compressed sensing problem as an example  Suppose that we observe the response vector   
which obeys             for   given design matrix  
and some noise    In order to recover the true parameter
    we may choose       as the leastsquares  of which the
derivative evaluated at        is given by

                             

Then the RIP condition asserts that

    sF                 kek  

where            is the        th restricted isometry
constant  Cand es   Tao    Therefore  imposing the
condition   amounts to distinguishing the true signal from
the observation noise 

Comparison to prior work  We contrast our result to the
stateof theart work of Yuan et al    To recover  
sparse signal     Yuan et al    required the condition
number       which might be too restrictive to general machine learning problems where the condition number grows with sample size  In addition  support recovery
was established only for   carefully chosen              

Support Recovery of Hard Thresholding Pursuit

Table   Comparison to previous work on HTPstyle algorithm  We present the  rst support recovery guarantee for an arbitrary
sparse signal without assuming the RIP condition or relaxing the sparsity 

Result

Target sparse signal RIPfree No sparsity relaxation

Support recovery

Foucart  
Yuan et al   
Jain et al   
Bouchot et al   
Yuan et al    Theorem  
Yuan et al    Theorem  
Proposed Theorem  

true signal
arbitrary

optimal solution

true signal

optimal solution

arbitrary
arbitrary

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

must be the unique global minimizer of       subject to
  sparsity constraint  see Theorem   therein  Such   requirement dramatically excludes many popular and simple
choices of       For example  let us again examine the
compressed sensing problem  With the presence of noise 
it is almost impossible for    to be the global optimum of
        ky   Axk  Hence  one cannot apply the theoret 

ical result of Yuan et al    to justify the performance
of HTP  In comparison  our theorem ensures that support
recovery is possible as far as the selected       ful lls the
condition   Though Theorem   in Yuan et al   
does not assume the RIP condition or the optimality of   
with respect to       it requires   relaxed sparsity parame 

ter       cid   cid  whereas the proposed Theorem   asserts

that       suf ces  We also note that iteration complexity
was not provided by Yuan et al    in the relaxed sparsity case  whereas we clearly state the dependency on all
the parameters 

Compared with Bouchot et al    it is not hard to see
that the problem considered here is more general  since we
aim to recover an arbitrary sparse signal while they targeted
the true parameter of compressed sensing  Bouchot et al 
  also imposed the RIP condition that is not invoked
here  Jain et al      presented HTPstyle algorithms with analysis on parameter estimation  but   guarantee on support recovery was not considered  We summarize
the comparison in Table  

Weakness  We remark that though Theorem   is free of the
RIP condition and the relaxed sparsity  it implicitly requires
that HTP should not terminate too early  Otherwise  HTP
may fail to recover the support  We believe that it is   very
interesting future direction to give   lower bound on the
iteration complexity of HTP  In the sequel  we strengthen
our result by providing suf cient conditions which prevent
HTP from early stopping 

In particular  we move on to the practical scenario where
the results to be established also apply to the exact case 
As   reminder  due to the assumption      HTP  is virtually solving   convex program  Yet  since       is   gen 

eral function   HTP  can only be solved approximately by 
     gradient descent  Nesterov    stochastic gradient descent  Bottou   Bousquet    or the more recent
variance reduced variant  Johnson   Zhang    The
question to ask is  whether support recovery is possible under such    noisy  setting  and how the optimization accuracy   enters the conditions for this end 

The following theorem presents an af rmative answer 
though the RIP condition is assumed 

the HTP algorithm with  
Theorem   Consider
approximate solutions in  HTP  Assume     and    
Suppose that the condition number       Pick    
   with               and set       in  HTP 
Then HTP recovers the support of    using at most
     xk 
tmax    log  
iterations  provided that for some constant        
    

log     

     

log 

log 

 

 

 xmin  

  

    sF       

 
  

Above  the quantity   is given by

             

Since the condition number is assumed to be well bounded 
it follows that the iteration complexity is   constant multiple of the sparsity            xk  By examining the  xmin
condition   we  nd that the optimization error   does not
propagate in   progressive manner  Rather  it enters the
condition as an additive error  By comparing   to   the
exact case  one may argue that   is more stringent because
it requires  xmin          sF      while   imposes
 xmin          sF       Yet  we point out that Theorem   is based on the RIP condition             So it
is not appropriate to examine the asymptotic behavior for
the condition  

Finally  we study under which RIPfree conditions can
HTP guarantee support recovery in the face of approximate
solutions  We have the following result 

Support Recovery of Hard Thresholding Pursuit

the HTP algorithm with  
Theorem   Consider
approximate solutions in  HTP  Assume     and    
Pick        and let            
    in  HTP  Then
HTP recovers the support of    using at most
     xk 
tmax      log  
iterations  provided that for some constant        

  log     

log 

log 

 

     

 xmin  

  

    sF     

       

      

    

 

 

 

Above  the quantity   is given by

       

          

 

     

To be clear  due to sparsity relaxation  Theorem   only enIn Yuan et al 

  they showed that under the condition

sures support inclusion           Stmax 
 xmin                    

 

 

HTP terminates with output xt satisfying supp  xt      
   However  the iteration number   was not given  Either 
it is not clear how large the difference               is 
where    is the global ssparse minimizer of       and we
recall that    is an arbitrary signal 
In contrast to Theorem   the quantity   here is multiplied
by the condition number   which will consume more computational resources in order to ful ll the condition  This is
not surprising because enlarging the support increases the
chance of detecting the support but as   price  it also introduces more noise  Fortunately  under the RSC and RSS
assumptions   rst order solvers converges linearly  For instance  after     log  steps  gradient descent guarantees an  approximate solution 

In view of the existing results from convex optimization  Nesterov    together with Theorem   we can
show that the total computational complexity of HTP is

 cid        log        log cid     log  

 
To see this  note that  HTP  consumes       operations
and  HTP  costs      log    Using gradient descent
to solve  HTP  results in   complexity       log 
Combining them together and noting       cid   cid  we obtain the above 

We point out that though Theorem   and Theorem   need to
know the sparsity    one can set   to be   quantity smaller
than    In this case  it follows from our analysis that HTP
recovers the support of the topk elements  Interested readers may refer to Lemma   for more details 

  Statistical Results

In this section  we relate our main results  Theorem   to
Theorem   to concrete statistical models  In particular  we
study two prevalent models 
the sparse linear regression
and the sparse logistic regression 

The sparse linear regression model is in essence the one
considered in the compressed sensing community  It assumes that the given response vector   obeys             
for   known design matrix      true sparse parameter     to
be estimated  and an unknown noise    In order to estimate
the signal     many researchers       Jain et al    considered the following formulation 

min
  Rd

        ky   Axk         kxk      

and attempted to prove that the  near  optimal solution of
the above program is close enough to     Yet  it turns
out that we can use more  exible functions           
        ky   Axk      kxk  To see this  by standard

results       Vershynin   Shen   Li   we are
guaranteed that when the entries of   and those of   are
       subgaussian 

holds with high probability  where   is the sample size 

    sF          cid pN         log   cid        xk
Hence  by picking       cid pN         log   cid  we have
    sF      vanishes as   increases  In light of such an
observation and our theorems  speci cally the  xmin conditions  we  nd that it is not the sparsityconstrained program matters  Rather  it is   properly chosen       that
guides HTP to the target signal 

The logistic regression model is used for binary classi 
 cation 
It has been shown in   number of work  see 
     Yuan et al    that     sF      is bounded from
above by   cid pN         log   cid  with high probability 

assuming the data is        subgaussian  Again  we can
add an   regularizer to the logistic loss to make it strongly
convex  without loss of the support recovery guarantee 

Relating these statistical results to our theorems  we conclude that the  xmin conditions involved can be satis ed
with high probability as soon as the sample size   grows
with         log    Moreover  under the same conditions 
the condition number   is well bounded from above  say
      implying   constant iteration complexity      xk 
and   fast computation  see the complexity in   We also
remark that in light of the many more choices of       the
function       essentially acts as   proxy that guides HTP
to the target signal  rather than an objective function being
optimized by HTP 

  Proof Sketch

Si     Thus 

Support Recovery of Hard Thresholding Pursuit

Our main results  Theorem   to Theorem   are proved by
mathematical induction  The key idea is partitioning the
support set   into several disjoint subsets               SK
according to the magnitude of the elements  Zhang   
Bouchot et al    Then we show that after   few itera 

tions  say    HTP identi es the  rst subset            Sn   

Given this  we further examine how many iterations are
needed to include the  rst two subsets  And we inductively
show that after             ni steps  the support set produced by HTP contains the  rst   number of subsets      
            Si   Sn   ni   We then show that each
ni is small  and the sum of them is upper bounded by  
multiple of   xk  Hence  two components are important to
this end  First  we need to construct the subsets properly 
and second  we need to offer an estimate on the ni   which
should be small enough 

Without loss of generality  suppose that the elements of   
are arranged in descending order  Then each subset Si is
inductively constructed as follows 

Si    si              si            

where        and for all            si is de ned as the
largest index such that

 xsi   

 

 cid cid xsi cid cid   

Note that the constant   can be replaced with any other
quantity smaller than   Since si is the largest one  it follows that

 xsi   

which immediately implies

 

 cid cid xsi cid cid   
   xsi  Si   

 

where

 cid cid   si   cid cid 
Xj 

Si    

   

    Si     

Then we show that given the above and the condition     
     Si    Sn   ni    as soon as HTP decreases
the distance to    with   geometric rate  which is the theme
of Section   we are guaranteed that             Si  
Sn   ni   Here  ni is given by

 xsi         nipSi      

for some parameters     and   Now assuming    
 xmin    xsi  implies that ni is as small as the logarithm of

tmax  

 

Xi 

ni  

 

Xi 

log Si       log

 
 

 

Xi 

Si   

The result follows by doing some calculation on the sum of
Si       See the full proof in the supplementary  le 

  Numerical Study

The HTP algorithm has been studied for several years and
has found plenty of successful applications  There is also  
large volume of empirical study       Bouchot et al   
showing that HTP performs better in terms of computational ef ciency and parameter estimation than compressive sampling matching pursuit  Needell   Tropp   
subspace pursuit  Dai   Milenkovic    iterative hard
thresholding  Blumensath   Davies    to name   few 
Hence  the focus of our numerical study is to verify the
theoretical  ndings in Section  

Data  In order to investigate the performance of HTP with
both the exact and inexact solutions  we consider the linear regression model               where    is    
dimensional vector with   tunable sparsity    The elements
in the design matrix   and the noise   are        normal
variables  The response   is an    dimensional vector  For
  certain sparsity level    the support of    is chosen uniformly and the nonzero components of    are        normal
variables  If not speci ed  we set       and      

Evaluation metric  In the experiments  we are mainly interested in examining the percentage of successful support
recovery and the iteration number that guarantees it  We
mark   trial as success if before HTP terminates  there is  
solution xt satisfying supp  xt    supp     Otherwise 
we mark it as failure  The iteration number is counted only
for those success trials and we report the averaged result 

Solvers  We choose the leastsquares loss as the proxy
function       for which an exact solution can be computed in  HTP  We also implement the gradient descent  GD  algorithm to approximately solve  HTP  In
order to produce solutions with different accuracy   we run
the GD algorithm with   various number of gradient oracle
calls  In this way  we are able to examine how   affects
support recovery through the number of oracle calls 

Other settings  The step size   in HTP is  xed as    
  We use the true sparsity for the sparsity parameter   in
 HTP  For each con guration of sparsity  we generate
  independent copies of     Hence  all the experiments
are performed with   trials 

  notable aspect of our theoretical results is that after
      log   iterations  HTP captures the support  For the
purpose of justi cation  we vary the sparsity   from   to  

Support Recovery of Hard Thresholding Pursuit

 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 
 

 

Exact
GD 
GD 
GD 
GD 
GD 

 

 
 non zeros

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 
 

 

Exact
GD 
GD 
GD 
GD 
GD 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 

 

 
 non zeros

 

 

 

 

 

 

 

Exact
GD 
GD 
GD 
GD 
GD 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 
 

 

 

Exact
GD 
GD 
GD 
GD 
GD 
 

 

 

 

 measurements

 

 

 measurements

Figure   Iteration number and percentage of success against
the sparsity  The number of measurements       GD    
means we run the gradient descent algorithm for   steps  As predicted by our theorem  the iteration number is nearly proportional
to the sparsity  left panel  Note that using approximate solutions
does not affect the iteration complexity  From the right panel 
we observe that gradient descent with   steps already ensures
comparable performance to the exact solution  possibly due to the
geometric convergence rate of gradient descent 

and plot the curve of the iteration number used to identify
the support against the true sparsity    Note that we use the
same design matrix for all trials  hence    xed condition
number   The result is recorded in the left panel of Figure   As predicted by our theorem  the iteration number is
 almost  linear with the sparsity  Interestingly  we also  nd
that HTP uses far fewer steps than expected  For example 
to recover the support of    sparse signal    iterations
suf ce in average  suggesting possible improvement of our
theorems in special cases  Also note that for   given sparsity level  applying an inexact solver for  HTP  does not
increase the iteration number of HTP  This is not surprising since our theorem states that the optimization error in
 HTP  only enters the  xmin condition  In other words  it
only affects the percentage of success as shown in the right
panel of Figure   Thanks to the linear convergence of gradient descent  it turns out that using   calls of gradient
oracle guarantees an appealing performance 

Next  we tune the number of measurements   from   to
  and study the support recovery performance against
the choice of     Here  the sparsity level   is  xed to
      With the subgaussian design  standard result
shows that the condition number can be upper bounded
by          log           log   
See  for example  Jain et al    This indicates that the condition
number is inversely proportional to   after   proper shifting  and hence the iteration number  The curves on the left
panel of Figure   matches our assertion  In the right panel 
  phase transition emerges  Donoho   Tanner    That
is  above   certain threshold  here the threshold is   support recovery is guaranteed with high probability while below that threshold  we have no hope to estimate the signal  We also  nd that when suf cient measurements are
available  running GD with   gradient oracle calls already
brings desirable performance 

Figure   Iteration number and percentage of success against
the number of measurements  The sparsity       GD    
means we run the gradient descent algorithms for   steps  The
left panel shows that the more measurements we have  the faster
we detect the support  The rationale is that the condition number becomes smaller with additional measurements  and by our
theorem  we need fewer iterations  The right panel shows   phase
transition phenomenon  when we have   or more measurements 
HTP guarantees support recovery with high probability while support recovery is impossible if we do not have suf cient samples 
Again  running GD with   gradient oracle calls produces similar
result with the exact solution 

We remind that in Figure   and Figure   some values of
 iterations are not plotted  For example  we do not have
the iteration number for GD  in Figure   when      
This is simply because all the trials are marked as failure 
See the associated percentage of success curve 

Now let us return to the  xmin condition of Theorem       
Eq    From Figure   and Figure   we conclude that
as far as the optimization error is small enough  HTP with
inexact iterates behaves comparably to that with exact solutions  For example  the  GD  curve  black solid  and
the  Exact  curve  red dashed  in these two  gures actually
lie on top of each other even the RIP condition is not met
 small   or large    This suggests that the relaxed sparsity
condition in Theorem   may not be vital 

  Conclusion and Future Work

In this paper  we have studied the iteration complexity of
the hard thresholding pursuit algorithm for recovering the
support of an arbitrary ssparse signal  We have shown that
if the iterates of HTP are exact solutions  HTP recovers
the support within       log   iterations where   is the
condition number 
In   more practical machine learning
setting  we have proved that even with inexact solutions 
support recovery is still possible with the same iteration
bound  We have also investigated two popular statistical
models  and have established probabilistic arguments under
the standard subgaussian design  The numerical study has
con rmed the correctness of our theoretical  ndings 

Orthogonal to the present work  an interesting direction for
future study is establishing   lower bound on the iteration
complexity of HTP for support recovery  It is also interesting to investigate the performance on realistic datasets 

Support Recovery of Hard Thresholding Pursuit

Acknowledgements

The work is supported in part by NSFBigdata 
and NSFIII  We thank the anonymous reviewers for pointing out several related work and for suggesting
improvement on the proof sketch and the experiments  We
also thank Jian Wang and Jing Wang for valuable discussion on the work 

References

Agarwal  Alekh  Negahban  Sahand  and Wainwright  Martin    Fast global convergence of gradient methods for
highdimensional statistical recovery  The Annals of
Statistics     

Blumensath  Thomas and Davies  Mike    Iterative hard
thresholding for compressed sensing  Applied and Computational Harmonic Analysis     

Bottou    eon and Bousquet  Olivier  The tradeoffs of large
scale learning  In Proceedings of the  st Annual Conference on Neural Information Processing Systems  pp 
   

Bouchot  JeanLuc  Foucart  Simon  and Hitczenko  Pawel 
Hard thresholding pursuit algorithms  number of iterations  Applied and Computational Harmonic Analysis 
   

Cai  Tony    Wang  Lie  and Xu  Guangwu  New bounds
for restricted isometry constants  IEEE Trans  Information Theory     

Cand es  Emmanuel    and Tao  Terence  Decoding by linear
programming  IEEE Trans  Information Theory   
   

Chen  Scott Shaobing  Donoho  David    and Saunders 
Michael    Atomic decomposition by basis pursuit 
SIAM Journal on Scienti   Computing   
 

Dai  Wei and Milenkovic  Olgica  Subspace pursuit for
compressive sensing signal reconstruction  IEEE Trans 
Information Theory     

Daubechies  Ingrid  Defrise  Michel  and Mol  Christine De  An iterative thresholding algorithm for linear
inverse problems with   sparsity constraint  Communications on Pure and Applied Mathematics   
   

Donoho  David    Compressed sensing  IEEE Trans  In 

formation Theory     

Donoho  David    and Tanner  Jared  Precise undersampling theorems  Proceedings of the IEEE   
   

Foucart  Simon  Hard thresholding pursuit  An algorithm
for compressive sensing  SIAM Journal on Numerical
Analysis     

Jain  Prateek  Tewari  Ambuj  and Dhillon  Inderjit    Orthogonal matching pursuit with replacement 
In Proceedings of the  th Annual Conference on Neural Information Processing Systems  pp     

Jain  Prateek  Tewari  Ambuj  and Kar  Purushottam  On iterative hard thresholding methods for highdimensional
Mestimation  In Proceedings of the  th Annual Conference on Neural Information Processing Systems  pp 
   

Johnson  Rie and Zhang  Tong  Accelerating stochastic
gradient descent using predictive variance reduction  In
Proceedings of the  th Annual Conference on Neural
Information Processing Systems  pp     

Loh  PoLing and Wainwright  Martin    Support recovery
without incoherence    case for nonconvex regularization  CoRR  abs   

Needell  Deanna and Tropp  Joel    CoSaMP  Iterative
signal recovery from incomplete and inaccurate samples 
Applied and Computational Harmonic Analysis   
   

Negahban  Sahand  Ravikumar  Pradeep  Wainwright 
Martin    and Yu  Bin    uni ed framework for highdimensional analysis of    estimators with decomposable regularizers 
In Proceedings of the  rd Annual
Conference on Neural Information Processing Systems 
pp     

Nesterov  Yurii  Introductory lectures on convex optimization  volume   Springer Science   Business Media 
 

Nguyen  Nam    and Tran  Trac    Robust lasso with missing and grossly corrupted observations  IEEE Trans  Information Theory     

Nguyen  Nam    Needell  Deanna  and Woolf  Tina  Linear convergence of stochastic iterative greedy algorithms
with sparse constraints  CoRR  abs   

Osher  Stanley  Ruan  Feng  Xiong  Jiechao  Yao  Yuan 
and Yin  Wotao  Sparse recovery via differential inclusions  Applied and Computational Harmonic Analysis 
   

Support Recovery of Hard Thresholding Pursuit

Pati  Yagyensh    Rezaiifar  Ramin  and Krishnaprasad 
Perinkulam    Orthogonal matching pursuit  Recursive function approximation with applications to wavelet
decomposition 
In Conference Record of The TwentySeventh Asilomar Conference on Signals  Systems and
Computers  pp    IEEE   

Shen  Jie and Li  Ping    tight bound of hard thresholding 

CoRR  abs   

Tibshirani  Robert  Regression shrinkage and selection via
the Lasso  Journal of the Royal Statistical Society  Series    Methodological  pp     

Tropp  Joel    Greed is good  algorithmic results for
sparse approximation  IEEE Trans  Information Theory 
   

Tropp  Joel    and Gilbert  Anna    Signal recovery
from random measurements via orthogonal matching
pursuit  IEEE Trans  Information Theory   
   

Vershynin  Roman 

Introduction to the nonasymptotic
analysis of random matrices  CoRR  abs 
 

Wainwright  Martin   

Sharp thresholds for highdimensional and noisy sparsity recovery using  
constrained quadratic programming  Lasso 
IEEE
Trans  Information Theory     

Wang  Jian  Kwon  Suhyuk  Li  Ping  and Shim  Byonghyo 
Recovery of sparse signals via generalized orthogonal
matching pursuit    new analysis  IEEE Trans  Signal
Processing     

Yuan  Ming and Lin  Yi  On the nonnegative garrotte estimator  Journal of the Royal Statistical Society  Series  
 Statistical Methodology     

Yuan  XiaoTong  Li  Ping  and Zhang  Tong  Gradient
hard thresholding pursuit for sparsityconstrained optimization  In Proceedings of the  st International Conference on Machine Learning  pp     

Yuan  XiaoTong  Li  Ping  and Zhang  Tong  Exact recovery of hard thresholding pursuit  In Proceedings of the
 th Annual Conference on Neural Information Processing Systems  pp     

Zhang  Tong  On the consistency of feature selection using greedy least squares regression  Journal of Machine
Learning Research     

Zhang  Tong  Sparse recovery with orthogonal matching
pursuit under RIP  IEEE Trans  Information Theory   
   

Zhao  Peng and Yu  Bin  On model selection consistency of
lasso  Journal of Machine Learning Research   
   

