Robust Submodular Maximization 

  NonUniform Partitioning Approach

Ilija Bogunovic   Slobodan Mitrovi     Jonathan Scarlett   Volkan Cevher  

Abstract

We study the problem of maximizing   monotone
submodular function subject to   cardinality constraint    with the added twist that   number of
items   from the returned set may be removed 
We focus on the worstcase setting considered in
 Orlin et al    in which   constantfactor approximation guarantee was given for       pk 
In this paper  we solve   key open problem
raised therein  presenting   new Partitioned Robust  PRO  submodular maximization algorithm
that achieves the same guarantee for more general          Our algorithm constructs partitions consisting of buckets with exponentially
increasing sizes  and applies standard submodular optimization subroutines on the buckets in order to construct the robust solution  We numerically demonstrate the performance of PRO in
data summarization and in uence maximization 
demonstrating gains over both the greedy algorithm and the algorithm of  Orlin et al   

  Introduction
Discrete optimization problems arise frequently in machine
learning  and are often NPhard even to approximate  In the
case of   set function exhibiting submodularity  one can ef 
 ciently perform maximization subject to cardinality con 

straints with     

  factor approximation guarantee  Ap 

plications include in uence maximization  Kempe et al 
  document summarization  Lin   Bilmes   
sensor placement  Krause   Guestrin    and active
learning  Krause   Golovin    just to name   few 

 LIONS  EPFL  Switzerland  LTHC  EPFL  Switzerland 
Correspondence to  Ilija Bogunovic  ilija bogunovic ep ch 
Slobodan Mitrovi    slobodan mitrovic ep ch 
Jonathan
Scarlett
Volkan
Cevher
 volkan cevher ep ch 

 jonathan scarlett ep ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

In many applications of interest  one requires robustness in
the solution set returned by the algorithm  in the sense that
the objective value degrades as little as possible when some
elements of the set are removed  For instance      in in uence maximization problems    subset of the chosen users
may decide not to spread the word about   product   ii 
in summarization problems    user may choose to remove
some items from the summary due to their personal preferences   iii  in the problem of sensor placement for outbreak
detection  some of the sensors might fail 
In situations where one does not have   reasonable prior
distribution on the elements removed  or where one requires robustness guarantees with   high level of certainty 
protecting against worstcase removals becomes important 
This setting results in the robust submodular function maximization problem  in which we seek to return   set of cardinality   that is robust with respect to the worstcase removal of   elements 
The robust problem formulation was  rst introduced in
 Krause et al    and was further studied in  Orlin
et al    In fact   Krause et al    considers   more
general formulation where   constantfactor approximation
guarantee is impossible in general  but shows that one can
match the optimal  robust  objective value for   given set
size at the cost of returning   set whose size is larger by  
logarithmic factor  In contrast   Orlin et al    designs
an algorithm that obtains the  rst constantfactor approximation guarantee to the above problem when       pk 
  key difference between the two frameworks is that the
algorithm complexity is exponential in   in  Krause et al 
  whereas the algorithm of  Orlin et al    runs in
polynomial time 
Contributions  In this paper  we solve   key open problem
posed in  Orlin et al    namely  whether   constantfactor approximation guarantee is possible for general    
     as opposed to only       pk  We answer this question in the af rmative  providing   new Partitioned Robust
 PRO  submodular maximization algorithm that attains  
constantfactor approximation guarantee  see Table   for
comparison of different algorithms for robust monotone
submodular optimization with   cardinality constraint 

Robust Submodular Maximization    NonUniform Partitioning Approach

Algorithm

Max  Robustness

Cardinality

SATURATE  KRAUSE ET AL   

OSU  ORLIN ET AL   

PROGREEDY  OURS 

Arbitrary
  pk 
    

      log   log   

 

 

Oracle Evals 
exponential in  

  nk 
  nk 

Approx 

 
 
 

Table   Algorithms for robust monotone submodular optimization with   cardinality constraint  The proposed algorithm is ef cient and
allows for greater robustness 

Achieving this result requires novelty both in the algorithm
and its mathematical analysis  While our algorithm bears
some similarity to that of  Orlin et al    it uses   novel
structure in which the constructed set is arranged into partitions consisting of buckets whose sizes increase exponentially with the partition index    key step in our analysis
provides   recursive relationship between the objective values attained by buckets appearing in adjacent partitions 
In addition to the above contributions  we provide the  rst
empirical study beyond what is demonstrated for      
in  Krause et al    We demonstrate several scenarios
in which our algorithm outperforms both the greedy algorithm and the algorithm of  Orlin et al   

  Problem Statement
Let   be   ground set with cardinality           and let    
        be   set function de ned on     The function  
is said to be submodular if for any sets           and
any element             it holds that

                                      

We use the following notation to denote the marginal gain
in the function value due to adding the elements of   set  
to the set   

                             

In the case that   is   singleton of the form     we adopt
the shorthand         We say that   is monotone if for any
sets           we have                and normalized
if        
The problem of maximizing   normalized monotone submodular function subject to   cardinality constraint      

max

     

 

       
has been studied extensively 
  celebrated result
of  Nemhauser et al    shows that   simple greedy
algorithm that starts with an empty set and then iteratively adds elements with highest marginal gain provides
         approximation 

 
 
   
   
   
      
      
      

      mins            
 
 
 

 
 
 
 
 

     

     
     
 
  

Table   Function   used to demonstrate that GREEDY can perform arbitrarily badly 

In this paper  we consider the following robust version of
  introduced in  Krause et al   

max

       

min

      

         

 

We refer to   as the robustness parameter  representing the
size of the subset   that is removed from the selected set
   Our goal is to  nd   set   such that it is robust upon
the worst possible removal of   elements       after the removal  the objective value should remain as large as possible  For       our problem reduces to Problem  
The greedy algorithm  which is nearoptimal for Problem   can perform arbitrarily badly for Problem   As
an elementary example  let us                and      
and consider the nonnegative monotone submodular function given in Table   For       the greedy algorithm selects        The set that maximizes mins                
      is        For this set  mins                
           while for the greedy set the robust objective
value is   As   result  the greedy algorithm can perform
arbitrarily worse 
In our experiments on realworld data sets  see Section  
we further explore the empirical behavior of the greedy solution in the robust setting  Among other things  we observe that the greedy solution tends to be less robust when
the objective value largely depends on the  rst few elements selected by the greedy rule 

Robust Submodular Maximization    NonUniform Partitioning Approach

Related work   Krause et al    introduces the following generalization of  

max

       

min

      

fi   

 

where fi are normalized monotone submodular functions 
The authors show that this problem is inapproximable in
general  but propose an algorithm SATURATE which  when
applied to   returns   set of size       log   log   
whose robust objective is at least as good as the optimal
sizek set  SATURATE requires   number of function evaluations that is exponential in   making it very expensive to
run even for small values  The work of  Powers et al   
considers the same problem for different types of submodular constraints 
Recently  robust versions of submodular maximization
have been applied to in uence maximization 
In  He  
Kempe    the formulation   is used to optimize  
worstcase approximation ratio  The con dence interval
setting is considered in  Chen et al    where two runs
of the GREEDY algorithm  one pessimistic and one optimistic  are used to optimize the same ratio  By leveraging
connections to continuous submodular optimization   Staib
  Jegelka    studies   related continuous robust budget
allocation problem 
 Orlin et al    considers the formulation in   and
provides the  rst constant  factor approximation result  valid for       pk  The algorithm proposed therein 
which we refer to via the authors surnames as OSU  uses
the greedy algorithm  henceforth referred to as GREEDY 
as   subroutine       times  On each iteration  GREEDY is
applied on the elements that are not yet selected on previous
iterations  with these previouslyselected elements ignored
in the objective function  In the  rst   runs  each solution is
of size   log    while in the last run  the solution is of size
     log    The union of all the obtained disjoint solutions
leads to the  nal solution set 

  Applications
In this section  we provide several examples of applications where the robustness of the solution is favorable  The
objective functions in these applications are nonnegative 
monotone and submodular  and are used in our numerical
experiments in Section  
Robust in uence maximization  The goal in the in uence
maximization problem is to  nd   set of   nodes        
targeted set  in   network that maximizes some measure
of in uence  For example  this problem appears in viral
marketing  where companies wish to spread the word of  
new product by targeting the most in uential individuals in
  social network  Due to poor incentives or dissatisfaction
with the product  for instance  some of the users from the

targeted set might make the decision not to spread the word
about the product 
For many of the existing diffusion models used in the literature       see  Kempe et al    given the targeted set
   the expected number of in uenced nodes at the end of
the diffusion process is   monotone and submodular function of    He   Kempe    For simplicity  we consider
  basic model in which all of the neighbors of the users in
  become in uenced  as well as those in   itself 
More formally  we are given   graph            where  
stands for nodes and   are the edges  For   set    let      
denote all of its neighboring nodes  The goal is to solve the
robust dominating set problem       to  nd   set of nodes  
of size   that maximizes

min

 RS RS        RS          RS 

 

where RS     represents the users that decide not to
spread the word  The nonrobust version of this objective
function has previously been considered in several different
works  such as  Mirzasoleiman et al      and  NorouziFard et al   
Robust personalized image summarization  In the personalized image summarization problem    user has   collection of images  and the goal is to  nd   images that are
representative of the collection 
After being presented with   solution  the user might decide
to remove   certain number of images from the representative set due to various reasons       bad lighting  motion
blur  etc  Hence  our goal is to  nd   set of images that
remain good representatives of the collection even after the
removal of some number of them 
One popular way of  nding   representative set in   massive
dataset is via exemplar based clustering       by minimizing
the sum of pairwise dissimilarities between the exemplars
  and the elements of the data set     This problem can be
posed as   submodular maximization problem subject to  
cardinality constraint  cf   Lucic et al   
Here  we are interested in solving the robust summarization
problem       we want to  nd   set of images   of size  
that maximizes

min

 RS RS  
 

is

 

               RS      
reference

element

and        

    Pv   mins           is the kmedoid loss func 

where   
 
tion  and where         measures the dissimilarity between
images   and   
Further potential applications not covered here include robust sensor placement  Krause et al    robust protection of networks  Bogunovic   Krause    and robust
feature selection  Globerson   Roweis   

Robust Submodular Maximization    NonUniform Partitioning Approach

  Algorithm and its Guarantees
  The algorithm
Our algorithm  which we call the Partitioned Robust  PRO 
submodular maximization algorithm  is presented in Algorithm   As the input  we require   nonnegative monotone
submodular function             the ground set of elements     and an optimization subroutine    The subroutine          takes   cardinality constraint    and   ground
set of elements     Below  we describe the properties of  
that are used to obtain approximation guarantees 
The output of the algorithm is   set       of size   that is
robust against the worstcase removal of   elements  The
returned set consists of two sets    and    illustrated in
Figure      is obtained by running the subroutine   on
             ignoring the elements already placed into   
and is of size       
We refer to the set    as the robust part of the solution set   
It consists of dlog        partitions  where every partition
       dlog     consists of     ie buckets Bj     
       ie  In partition    every bucket contains    
elements  where        is   parameter that is arbitrary
for now  we use     log    in our asymptotic theory  but
our numerical studies indicate that even       works well
in practice  Each bucket Bj is created afresh by using the
subroutine   on       prev  where   prev contains all
elements belonging to the previous buckets 
The following proposition bounds the cardinality of    and
is proved in the supplementary material 
Proposition   Fix       and        The size of the
robust part    constructed in Algorithm   is

     

dlog  eXi 

    ie         log      

This proposition reveals that the feasible values of        

  We will

those with        can be as high as     

later set       log     thus permitting all          up
to   few logarithmic factors  In contrast  we recall that the
algorithm OSU proposed in  Orlin et al    adopts  
simpler approach where   robust set is used consisting of
  buckets of equal size   log    thereby only permitting the
scaling       pk 
We provide the following intuition as to why PRO succeeds
despite having   smaller size for    compared to the algorithm given in  Orlin et al    First  by the design of
the partitions  there always exists   bucket in partition   that
at most    items are removed from  The bulk of our analysis is devoted to showing that the union of these buckets
yields   suf ciently high objective value  While the earlier

Algorithm   Partitioned Robust Submodular optimization
algorithm  PRO 
Require  Set                 algorithm  
Ensure  Set       such that      
           
  for       to dlog    do
 
 
 
                       
             
  return  

Bj               
          Bj

for       to     ie do

buckets have   smaller size  they also have   higher objective value per item due to diminishing returns  and our
analysis quanti es and balances this tradeoff  Similarly 
our analysis quanti es the tradeoff between how much the
adversary can remove from the  typically large  set    and
the robust part   

  Subroutine and assumptions
PRO accepts   subroutine   as the input  We consider  
class of algorithms that satisfy the  iterative property  de 
 ned below  We assume that the algorithm outputs the  nal
set in some speci   order             vk  and we refer to vi
as the ith output element 

De nition   Consider   normalized monotone submodular set function   on   ground set     and an algorithm   
Given any set       and size    suppose that   outputs
an ordered set             vk  when applied to     and de ne
Ai                   vi  for        We say that   satis es
the  iterative property if

   Ai          Ai      

 
 

max
   

     Ai    

 

Intuitively    states that in every iteration  the algorithm
adds an element whose marginal gain is at least     fraction of the maximum marginal  This necessarily requires
that      
Examples  Besides the classic greedy algorithm  which
satis es   with         good candidate for our
subroutine is THRESHOLDINGGREEDY  Badanidiyuru  
Vondr ak    which satis es the  iterative property
with         This decreases the number of function
evaluations to      log   
STOCHASTICGREEDY  Mirzasoleiman et al      is
another potential subroutine candidate  While it is unclear whether this algorithm satis es the  iterative property  it requires an even smaller number of function eval 

Robust Submodular Maximization    NonUniform Partitioning Approach

uations  namely      log   We will see in Section   that PRO performs well empirically when used
with this subroutine  We henceforth refer to PRO used
along with its appropriate subroutine as PROGREEDY 
PROTHRESHOLDING GREEDY  and so on 
Properties  The following lemma generalizes   classical
property of the greedy algorithm  Nemhauser et al   
Krause   Golovin    to the class of algorithms satisfying the  iterative property  Here and throughout the paper 
we use OPT        to denote the following optimal set for
nonrobust maximization 

OPT          argmax
       

     

Lemma   Consider   normalized monotone submodular
function             and an algorithm              
that satis es the  iterative property in   Let Al     denote the set returned by the algorithm       after   iterations  Then for all          
   Al             

       OPT       

 

We will also make use of the following property  which is
implied by the  iterative property 

Proposition   Consider   submodular set function    
        and an algorithm   that satis es the  
iterative property for some       Then  for any      
and element               we have

              

        

 

 

 

Intuitively    states that the marginal gain of any nonselected element cannot be more than   times the average
objective value of the selected elements  This is one of the
rules used to de ne the  nice class of algorithms in  Mirrokni   Zadimoghaddam    however  we note that in
general  neither the  nice nor  iterative classes are   subset of one another 

  Main result  Approximation guarantee
For the robust maximization problem  we let OPT          
denote the optimal set 

OPT             argmax
       

         
Moreover  for   set    we let     denote the minimizer

      

min

      argmin
      

         

 

With these de nitions  the main theoretical result of this
paper is as follows 

partitions

 

     

 

 

  
  

 

 

buckets

     

   

 

         

Figure   Illustration of the set             returned by PRO 
The size of     is       and the size of     is given in Proposition   Every partition in    contains the same number of
elements  up to rounding 

Theorem   Let   be   normalized monotone submodular function  and let   be   subroutine satisfying the  
iterative property  For   given budget   and parameters
 log    and      log       PRO returns  
       
set   of size   such that

 

             

 

 dlog              
     
 dlog              
     
   
     OPT               OPT       

 

 

where     and   OPT        are de ned as in  

  log    and     log     then we

In addition  if         
have the following as      
                    

           
     OPT               OPT       

 

In particular  PROGREEDY achieves an asymptotic
approximation factor of at
least   and PROTHRESHOLDING GREEDY with parameter   achieves an
asymptotic approximation factor of at least      
This result solves an open problem raised in  Orlin et al 
  namely  whether   constantfactor approximation
guarantee can be obtained for          as opposed to

Robust Submodular Maximization    NonUniform Partitioning Approach

only       pk 

In the asymptotic limit  our constant
factor of   for the greedy subroutine matches that of
 Orlin et al    but our algorithm permits signi cantly
 higher robustness  in the sense of allowing larger   values  To achieve this  we require novel proof techniques 
which we now outline 

  Highlevel overview of the analysis
The proof of Theorem   is provided in the supplementary material  Here we provide   highlevel overview of the
main challenges 
Let   denote   cardinality  subset of the returned set  
that is removed  By the construction of the partitions  it is
easy to verify that each partition   contains   bucket from
which at most    items are removed  We denote these by
           Bdlog     and write EBi       Bi  Moreover  we
de ne             and            
We establish the following lower bound on the  nal objective function value 

            max                             

   dlog      

 Bi   EBi 

 

The arguments to the  rst and third terms are trivially seen
to be subsets of        and the second term represents the
utility of the set    subsided by the utility of the elements
removed from   
The  rst two terms above are easily lower bounded by
convenient expressions via submodular and the  iterative
property  The bulk of the proof is dedicated to bounding the
third term  To do this  we establish the following recursive
relations with suitablyde ned  small  values of    

 

       
 Bi   EBi       
 Bi   EBi     jf     
  EBj  
    

     

       Bj 
 Bi   EBi   

Intuitively  the  rst equation shows that the objective value
from buckets                 with removals cannot be too
much smaller than the objective value in bucket   without
removals  and the second equation shows that the loss in
bucket   due to the removals is at most   small fraction of
the objective value from buckets              The proofs of
both the base case of the induction and the inductive step
make use of submodularity properties and the  iterative
property  cf  De nition  
Once the suitable lower bounds are obtained for the terms
in   the analysis proceeds similarly to  Orlin et al 

  Speci cally  we can show that as the second term
increases  the third term decreases  and accordingly lower
bound their maximum by the value obtained when the two
are equal    similar balancing argument is then applied to
the resulting term and the  rst term in  
The condition    
 log    follows directly from Proposition   namely  it is   suf cient condition for       
as is required by PRO 

 

  Experiments
In this section  we numerically validate the performance of
PRO and the claims given in the preceding sections  In particular  we compare our algorithm against the OSU algorithm proposed in  Orlin et al    on different datasets
and corresponding objective functions  see Table   We
demonstrate matching or improved performance in   broad
range of settings  as well as observing that PRO can be
implemented with larger values of   corresponding to  
greater robustness  Moreover  we show that for certain realworld data sets  the classic GREEDY algorithm can perform
badly for the robust problem  We do not compare against
SATURATE  Krause et al    due to its high computational cost for even   small  
Setup  Given   solution set   of size    we measure the performance in terms of the minimum objective value upon the
worstcase removal of   elements       minZ           
   Unfortunately  for   given solution set     nding such  
set   is an instance of the submodular minimization problem with   cardinality constraint  which is known to be
NPhard with polynomial approximation factors  Svitkina
  Fleischer    Hence  in our experiments  we only
implement the optimal  adversary        removal of items 
for small to moderate values of   and    for which we use
  fast    implementation of branchand bound 
Despite the dif culty in implementing the optimal adversary  we observed in our experiments that the greedy adversary  which iteratively removes elements to reduce the
objective value as much as possible  has   similar impact
on the objective compared to the optimal adversary for the
data sets considered  Hence  we also provide   largerscale
experiment in the presence of   greedy adversary  Throughout  we write OA and GA to abbreviate the optimal adversary and greedy adversary  respectively 
In our experiments  the size of the robust part of the solution set           is set to     and   log   for OSU and
PRO  respectively  That is  we set       in PRO  and
similarly ignore constant and logarithmic factors in OSU 
since both appear to be unnecessary in practice  We show

 This can be seen by noting that for submodular   and any

                             remains submodular 

Robust Submodular Maximization    NonUniform Partitioning Approach

both the  raw  objective values of the solutions  as well
as the objective values after the removal of   elements  In
all experiments  we implement GREEDY using the LAZYGREEDY implementation given in  Minoux   
The objective functions shown in Table   are given in
Section   For the exemplar objective function  we use
          ks   vk  and let the reference element    be the
zero vector  Instead of using the whole set     we approximate the objective by considering   smaller random subset
of   for improved computational ef ciency  Since the objective is additively decomposable and bounded  standard
concentration bounds       the Chernoff bound  ensure that
the empirical mean over   random subsample can be made
arbitrarily accurate 
Data sets  We consider the following datasets  along with
the objective functions given in Section  

  EGOFACEBOOK  This network data consists of social circles  or friends lists  from Facebook forming an
undirected graph with   nodes and   edges 
  EGOTWITTER  This dataset consists of   social circles from Twitter  forming   directed graph
with   nodes and   edges  Both EGOFACEBOOK and EGOTWITTER were used previously
in  Mcauley   Leskovec   

  TINY   and TINY    We used two Tiny Images
data sets of size    and    consisting of images
each represented as    dimensional vector  Torralba et al    Besides the number of images 
these two datasets also differ in the number of classes
that the images are grouped into  We shift each vectors to have zero mean 

  CMMOLECULES  This dataset consists of  
small organic molecules  each represented as    
dimensional vector  Each vector is obtained by processing the molecule   Coulomb matrix representation
 Rupp    We shift and normalize each vector to
zero mean and unit norm 

Dataset
Tiny  
Tiny  

CMMolecules

Network

egoFacebook
egoTwitter

 

dimension

 

   
   
 
  nodes
 
   

 
 
 

  edges
   

     

Exemplar
Exemplar
Exemplar

 

DomSet
DomSet

Table   Datasets and corresponding objective functions 

Results  In the  rst set of experiments  we compare PROGREEDY  written using the shorthand PROGR in the legend  against GREEDY and OSU on the EGOFACEBOOK
and EGOTWITTER datasets  In this experiment  the dominating set selection objective in   is considered  Figure  
    and     show the results before and after the worstcase
removal of       elements for different values of    In
Figure       and     we show the objective value for  xed
      and       respectively  while the robustness
parameter   is varied 
GREEDY achieves the highest raw objective value  followed by PROGREEDY and OSU  However  after the
worstcase removal  PROGREEDY OA outperforms both
OSUOA and GREEDYOA  In Figure       and    
GREEDYOA performs poorly due to   high concentration
of the objective value on the  rst few elements selected by
GREEDY  While OSU requires         PRO only requires
      log   and hence it can be run for larger values of  
      see Figure       and     Moreover  in Figure      
and     we can observe that although PRO uses   smaller
number of elements to build the robust part of the solution
set  it has better robustness in comparison with OSU 
In the second set of experiments  we perform the same
type of comparisons on the TINY  and CMMOLECULES
datasets  The exemplar based clustering in   is used as the
objective function  In Figure       and     the robustness
parameter is  xed to       and       respectively  while
the cardinality   is varied  In Figure       and     the cardinality is  xed to       and       respectively  while
the robustness parameter   is varied 
Again  GREEDY achieves the highest objective value  On
the TINY  dataset  GREEDYOA  Figure       and    
has   large gap between the raw and  nal objective  but it
still slightly outperforms PROGREEDY OA  This demonstrates that GREEDY can work well in some cases  despite failing in others  We observed that it succeeds here
because the objective value is relatively more uniformly
spread across the selected elements  On the same dataset 
PROGREEDY OA outperforms OSUOA  On our second
dataset CMMOLECULES  Figure       and     PROGREEDY OA achieves the highest robust objective value 
followed by OSUOA and GREEDYOA 
In our  nal experiment  see Figure       we compare
the performance of PROGREEDY against two instances of
PROSTOCHASTIC GREEDY with       and      
 shortened to PROST in the legend  seeking to understand
to what extent using the more ef cient stochastic subroutine impacts the performance  We also show the performance of OSU  In this experiment  we          and
vary   We use the greedy adversary instead of the optimal
one  since the latter becomes computationally challenging
for larger values of  

Robust Submodular Maximization    NonUniform Partitioning Approach

    egoFacebook       

    egoFacebook       

    egoTwitter       

 

 

 
 

 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

 
 

 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

PRoGr
OSU
Greedy
PRoGr   OA
OSU   OA
Greedy   OA

 
 

 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Cardinality  

Robustness  

Cardinality  

    egoTwitter       

 

 

 
 

 
 
 

 
 
 
 

 

 

 

 

 

 
 

 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

    Tiny       

 

 

    Tiny       

 
 

 
 
 

 
 
 
 

 

 

 

 

 

 

 

 
 
Robustness  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Cardinality  

Robustness  

    CMMolecules       

    CMMolecules       

 

 

 

 

 

 

 
 

 
 
 

 
 
 
 

 

 

 

 

 
 
Robustness  

 
 

 
 
 

 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Cardinality  

 

 

    Tiny       

 
 

 
 
 

 
 
 
 

 

 

 

 

 

 

 

PRoGr
OSU
PRoSt      
PRoSt      
PRoGr   GA
OSU   GA
PRoSt   GA      
PRoSt   GA      

 

 

 

 

 

 

 

 

 

 

Robustness  

Figure   Numerical comparisons of the algorithms PROGREEDY  GREEDY and OSU  and their objective values PROOA  OSUOA
and GREEDYOA once   elements are removed  Figure     shows the performance on the larger scale experiment where both GREEDY
and STOCHASTICGREEDY are used as subroutines in PRO 

In Figure       we observe   slight decrease in the objective value of PROSTOCHASTIC GREEDY due to the
stochastic optimization  On the other hand  the gaps between the robust and nonrobust solutions remain similar 
or even shrink  Overall  we observe that at least in this example  the stochastic subroutine does not compromise the
quality of the solution too signi cantly  despite having  
lower computational complexity 

  Conclusion
We have provided   new Partitioned Robust  PRO  submodular maximization algorithm attaining   constantfactor approximation guarantee for general          thus

resolving an open problem posed in  Orlin et al   
Our algorithm uses   novel partitioning structure with partitions consisting of buckets with exponentially decreasing
size  thus providing    robust part  of size    poly log    
We have presented   variety of numerical experiments
where PRO outperforms both GREEDY and OSU    potentially interesting direction for further research is to understand the linear regime  in which        for some constant         and in particular  to seek   constantfactor
guarantee for this regime 
Acknowledgment  This work was supported in part by
the European Commission under Grant ERC Future Proof 
SNF   and SNF CRSII  and  EPFL
Fellows   Horizon   

Robust Submodular Maximization    NonUniform Partitioning Approach

References
Badanidiyuru  Ashwinkumar and Vondr ak  Jan  Fast algorithms for maximizing submodular functions  In ACMSIAM Symp  Disc  Alg   SODA  pp     

Bogunovic  Ilija and Krause  Andreas  Robust protection
of networks against cascading phenomena  Tech  Report
ETH   urich   

Chen  Wei  Lin  Tian  Tan  Zihan  Zhao  Mingfei  and
Zhou  Xuren  Robust in uence maximization  arXiv
preprint arXiv   

Globerson  Amir and Roweis  Sam  Nightmare at test time 
robust learning by feature deletion  In Int  Conf  Mach 
Learn   ICML   

He  Xinran and Kempe  David  Robust in uence maximization  In Int  Conf  Knowledge Discovery and Data
Mining  KDD  pp     

Kempe  David  Kleinberg  Jon  and Tardos   Eva  Maximizing the spread of in uence through   social network 
In Int  Conf  on Knowledge Discovery and Data Mining
 SIGKDD   

Krause  Andreas and Golovin  Daniel  Submodular function maximization  Tractability  Practical Approaches
to Hard Problems     

Krause  Andreas and Guestrin  Carlos  Nearoptimal observation selection using submodular functions  In Conf 
Art  Intell   AAAI   

Krause  Andreas  McMahan    Brendan  Guestrin  Carlos 
and Gupta  Anupam  Robust submodular observation selection  Journal of Machine Learning Research   Dec 
   

Lin  Hui and Bilmes  Jeff    class of submodular functions
for document summarization  In Assoc  for Comp  Ling 
Human Language TechnologiesVolume    

Lucic  Mario  Bachem  Olivier  Zadimoghaddam  Morteza 
and Krause  Andreas  Horizontally scalable submodular
maximization  In Proc  Int  Conf  Mach  Learn   ICML 
 

Mcauley  Julian and Leskovec  Jure  Discovering social circles in ego networks  ACM Trans  Knowl  Discov  Data 
 

Minoux  Michel  Accelerated greedy algorithms for maximizing submodular set functions  In Optimization Techniques  pp    Springer   

Mirrokni  Vahab and Zadimoghaddam  Morteza  Randomized composable coresets for distributed submodular
maximization  In ACM Symposium on Theory of Computing  STOC   

Mirzasoleiman  Baharan  Badanidiyuru  Ashwinkumar 
Karbasi  Amin  Vondr ak  Jan  and Krause  Andreas 
Lazier than lazy greedy 
In Proc  Conf  Art  Intell 
 AAAI     

Mirzasoleiman  Baharan  Karbasi  Amin  Badanidiyuru 
Ashwinkumar  and Krause  Andreas  Distributed submodular cover  Succinctly summarizing massive data  In
Adv  Neur  Inf  Proc  Sys   NIPS  pp       

Nemhauser  George    Wolsey  Laurence    and Fisher 
Marshall    An analysis of approximations for maximizing submodular set functionsi  Mathematical Programming     

NorouziFard  Ashkan  Bazzi  Abbas  Bogunovic  Ilija 
El Halabi  Marwa  Hsieh  YaPing  and Cevher  Volkan 
An ef cient streaming algorithm for the submodular
cover problem 
In Adv  Neur  Inf  Proc  Sys   NIPS 
 

Orlin  James    Schulz  Andreas    and Udwani  Rajan 
Robust monotone submodular function maximization  In
Int  Conf  on Integer Programming and Combinatorial
Opt   IPCO  Springer   

Powers  Thomas  Bilmes  Jeff  Wisdom  Scott  Krout 
David    and Atlas  Les  Constrained robust submodular optimization  NIPS OPT  workshop   

Rupp  Matthias  Machine learning for quantum mechanics
in   nutshell  Int  Journal of Quantum Chemistry   
   

Staib  Matthew and Jegelka  Stefanie 

Robust budget allocation via continuous submodular functions 
http people csail mit edu stefje 
papers robust budget pdf   

Svitkina  Zoya and Fleischer  Lisa  Submodular approximation  Samplingbased algorithms and lower bounds 
SIAM Journal on Computing     

Torralba  Antonio  Fergus  Rob  and Freeman  William   
  million tiny images    large data set for nonparametric object and scene recognition  IEEE Trans  Patt  Ana 
Mach  Intel     

