Minimax Regret Bounds for Reinforcement Learning

Mohammad Gheshlaghi Azar   Ian Osband     mi Munos  

Abstract

 

HSAT          

 

 

bound of  cid   
ous known bound  cid   HS
 cid   

We consider the problem of provably optimal
exploration in reinforcement learning for  nite
horizon MDPs  We show that an optimistic
modi cation to value iteration achieves   regret
    where
  is the time horizon    the number of states   
the number of actions and   the number of timesteps  This result improves over the best previAT   achieved by the
UCRL  algorithm of Jaksch et al    The
key signi cance of our new results is that when
           and SA      it leads to   regret of
 
HSAT   that matches the established lower
HSAT   up to   logarithmic factor 
bound of  
Our analysis contains two key insights  We use
careful application of concentration inequalities
to the optimal value function as   whole  rather
than to the transitions probabilities  to improve
scaling in    and we de ne Bernsteinbased  exploration bonuses  that use the empirical variance of the estimated values at the next states  to
improve scaling in   

 

  Introduction
We consider the reinforcement learning  RL  problem of an
agent interacting with an environment in order to maximize
its cumulative rewards through time  Burnetas   Katehakis    Sutton   Barto    We model the environment as   Markov decision process  MDP  whose transition dynamics are unknown from the agent  As the agent
interacts with the environment it observes the states  actions and rewards generated by the system dynamics  This
leads to   fundamental trade off  should the agent explore
poorlyunderstood states and actions to gain information
and improve future performance  or exploit its knowledge
to optimize shortrun rewards 

 DeepMind  London  UK  Correspondence to  Mohammad

Gheshlaghi Azar  mazar google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

The most common approach to this learning problem is
to separate the process of estimation and optimization 
In this paradigm  point estimates of the unknown quantities are used in place of the unknown parameters and  
plan is made with respect to these estimates  Naive optimization with respect to these point estimates can lead
to premature exploitation and so may never learn the optimal policy  Dithering approaches to exploration      
 greedy  address this failing through random action selection  However  as this exploration is not directed the
resultant algorithms may take exponentially long to learn
 Kearns   Singh    In order to learn ef ciently it is
necessary that the agent prioritizes potentially informative
states and actions  To do this  it is important that the agent
maintains some notion of its own uncertainty 
In some
sense  given any prior belief  the optimal solution to this
exploration exploitation dilemma is given by the dynamic
programming in the extended Bayesian belief state  Bertsekas    However  the computational demands of this
method become intractable for even small problems  Guez
et al    while  nite approximations can be arbitrarily
poor  Munos   
To combat these failings  the majority of provably ef cient
learning algorithms employ   heuristic principle known as
optimism in the face of uncertainty  OFU  In these algorithms  each state and action is afforded some  optimism 
such that its imagined value is as high as statistically plausible  The agent then chooses   policy under this optimistic view of the world  This allows for ef cient exploration since poorlyunderstood states and actions are afforded higher optimistic bonus  As the agent resolves its
uncertainty  the effects of optimism will reduce and the
agent   policy will approach optimality  Almost all reinforcement learning algorithms with polynomial bounds on
sample complexity employ optimism to guide exploration
 Kearns   Singh    Brafman   Tennenholtz   
Strehl et al    Dann et al   
An alternative principle motivated by the Thompson sampling  Thompson    has emerged as   practical competitor to optimism  The algorithm posterior sampling reinforcement learning  PSRL  maintains   posterior distribution for MDPs and  at each episode of interaction  follows   policy which is optimal for   single random sample
 Strens    Previous works have argue for the potential

Minimax Regret Bounds for Reinforcement Learning

bene ts of such PSRL methods over existing optimistic approaches  Osband et al    Osband   Van Roy     
but they come with guarantees on the Bayesian regret only 
However   very recent work  Agrawal   Jia    have
shown that an optimistic version of posterior sampling  using   max over several samples  achieves   frequentist reSAT    for large     in the more general

gret bound  cid    

 

setting of weakly communicating MDPs 
In this paper we present   conceptually simple and computationally ef cient approach to optimistic reinforcement
learning in  nitehorizon MDPs and report results for the
frequentist regret  Our algorithm  upper con dence bound
value iteration  UCBVI  is similar to modelbased interval estimation  MBIEEB   Strehl   Littman    with  
delicate alteration to the form of the  exploration bonus 
In particular UCBVI replaces the universal scalar of the
bonus in MBIEEB with the empirical variance of the nextstate value function of each stateaction pair  This alter 

ation is essential to improve the regret bound from  cid     
to  cid   
bound  cid   
length and   is the total number of timesteps  and where  cid  
and SA     this bound is  cid   

Our key contribution is to establish   high probability regret
    where   is the number of states    is the number of actions    is the episode

ignores logarithmic factors  Importantly  for           
HSAT   which matches
the established lower bound for this problem  up to logarithmic factors  Osband   Van Roy      This positive
result is the  rst of its kind and helps to address an ongoing
question about where the fundamental lower bounds lie for
reinforcement learning in  nite horizon MDPs  Bartlett  
Tewari    Dann   Brunskill    Osband   Van Roy 
    Our re ned analysis contains two key ingredients 

HSAT          

  
 

 

 

 

  We use careful application of Bernstein and Freedman inequalities  Bernstein    Freedman   
to the concentration of the optimal value function directly  rather than building con dence sets for the
transitions probabilities and rewards  like in UCRL 
 Jaksch et al    and UCFH  Dann   Brunskill 
 

  We use empiricalvariance exploration bonuses based
on Bernstein   inequality  which together with   recursive Bellmantype Law of Total Variance  LTV  provide tight bounds on the expected sum of the variances
of the value estimates  in   similar spirit to the analysis
from  Azar et al    Lattimore   Hutter   

 In fact the lower bound of  Jaksch et al    is for the more
general setting of the weakly communicating MDPs and it doesn  
directly apply to our setting  But   similar approach can be used
to prove   lower bound of same order for the  nitehorizon MDPs 
as it is already used in  Osband   Van Roy     

 

At   high level  this work addresses the noted shortcomings
of existing RL algorithms  Bartlett   Tewari    Jaksch
et al    Osband   Van Roy      in terms of dependency on   and    We demonstrates that it is possible
to design   simple and computationally ef cient optimistic
algorithm that simultaneously address both the loose scaling in   and   to obtain the  rst regret bounds that match
the  
We should be careful to mention the current limitations of
our work  each of which may provide fruitful ground for future research  First  we study the setting of episodic   nite
horizon MDPs and not the more general setting of weakly
communicating systems  Bartlett   Tewari    Jaksch
et al    Also we assume that the horizon length  
is known to the learner  Further  our bounds only improve

HSAT   lower bounds as   becomes large 

over previous scaling  cid   HS

AT   for            

 

We hope that this work will serve to elucidate several of the
existing shortcomings of exploration in the tabular setting
and help further the direction of research towards provably
optimal exploration in reinforcement learning 

  Problem formulation
In this section  we brie   review some notation  as well as
some standard concepts and de nitions from the theory of
Markov decision processes  MDPs 
Markov Decision Problems We consider the problem of
undiscounted episodic reinforcement learning  RL   Bertsekas   Tsitsiklis    where an RL agent interacts
with   stochastic environment and this interaction is modeled as   discretetime MDP  An MDP is   quintuple
 cid              cid  where   and   are the set of states and
actions    is the state transition distribution  The function
           cid  is   realvalued function on the stateaction
space and the horizon   is the length of episode  We denote by          and         the probability distribution
over the next state and the immediate reward of taking action   at state    respectively  The agent interacts with the
environment in   sequence of episodes  The interaction between the agent and environment at every episode         
is as follows  starting from xk      which is chosen by
the environment  the agent interacts with the environment
for   steps by following   sequence of actions chosen in
  and observes   sequence of nextstates and rewards until the end of episode  The initial state xk  may change
arbitrarily from one episode to the next  We also use the
notation  cid     cid  for the  cid  norm throughout this paper 
Assumption    MDP Regularity  We assume   and   are
 nite sets with cardinalities       respectively  We also
assume that the immediate reward         is deterministic

 We write     for                    

Minimax Regret Bounds for Reinforcement Learning

and belongs to the interval    

 

  as    

also de ned as             def   cid 
        def   cid 

In this paper we focus on the setting where the reward function   is known  but extending our algorithm to unknown
stochastic rewards poses no real dif culty 
The policy during an episode is expressed as   mapping
                 The value    
        denotes the value function at every step                  
and state       such that    
      corresponds to the expected sum of       rewards received under policy  
starting from xh          Under Assumption   there
exists always   policy   which attains the best possible
values  and we de ne the optimal value function    
      def 
      for all       and       The policy  
sup     
at every step   de nes the state transition kernel    
  and
        def                
the reward function   
      def              for all        For every
and   
    are
          the rightlinear operators    and    
                    for
            
all                and     
for all        respectively  The Bellman operator for
the policy   at every step       and        is de ned
as     
        We also de ne
the stateaction Bellman operator for all               
as             def                        and the optimality Bellman operator for all       as           def 
maxa              For ease of exposition  we remove
the dependence on   and             writing     for
            and   for       when there is no possible confusion 
We measure the performance of the learner over     KH
steps  by the regret Regret    de ned as
   xk        
   

        def    

Regret    def 

            

  cid 

   xk 

       

  

where    is the control policy followed by the learner at
episode    Thus the regret measures the expected loss of
following the policy produced by the learner instead of the
optimal policy  So the goal of learner is to follow   sequence of policies                such that Regret    is
as small as possible 

  Upper con dence bound value iteration
In this section we introduce two variants of the algorithm
that we investigate in this paper  We call the algorithm upper con dence bound value iteration  UCBVI  UCBVI is

 For rewards in  Rmin  Rmax  simply rescale these bounds 
 In this paper we will often substitute    KH to highlight
various dependencies relative to the existing literature  This
equivalence should be kept in mind by the reader 

an extension of value iteration which guarantees that the
resultant value function is    highprobability  upper con 
 dence bound  UCB  on the optimal value function  This
algorithm is related to the model based interval estimation
 MBIEEB  algorithm  Strehl   Littman    Our key
contribution is the precise design of the upper con dence
sets  and the analysis which lead to tight regret bounds 
UCBVI  described in Algorithm   calls UCBQ values
 Algorithm   which returns UCBs on the Qvalues computed by value iteration using an empirical Bellman operator to which is added   con dence bonus bonus  We consider two variants of UCBVI depending on the structure of
bonus  which we present in Algorithms   and  

Algorithm   UCBVI

Initialize data      
for episode                   do

Qk     UCB       values   
for step                 do

Take action ak     arg maxa Qk   xk      
Update          xk    ak    xk   

end for

end for

Algorithm   UCBQ values
Require  Bonus algorithm bonus  Data  

   cid   cid   cid HI   cid        cid        cid      
    Nk       
 xi   ai   xi   HI xi       ai       

Compute  for all               

Nk         cid 
Nk       cid 
          cid 
Estimate  cid Pk          Nk       

  cid 
Let              Nk         
Initialize Vk          for all          
for             do
for           do
if        then

for all       

bk          bonus cid Pk Vk   Nk   cid 
Qk          min cid Qk         
          cid PkVk          bk       cid 

Nk     

    

else

Qk           

end if
Vk        maxa   Qk       

end for

end for
return Qvalues Qk  

The  rst of these UCBVICH is based upon ChernoffHoeffding   concentration inequality  considers UCBVI
with bonus   bonus  bonus  is   very simple bound
which only assumes that values are bounded in      We

Minimax Regret Bounds for Reinforcement Learning

 

 

already achieve   regret bound of  cid    

will see in Theorem   that this very simple algorithm can
SAT   thus improving the best previously known regret bounds from  
  dependence  The intuition for this improved
  to  
Sdependence is that our algorithm  as well as our analysis  does not consider con dence sets on the transition
dynamics            like UCRL  and UCFH do  but instead directly maintains con dence intervals on the optimal value function  This is crucial as  for any given       
the transition dynamics are Sdimensional whereas the Qvalue function is onedimensional 

will converge to the variance of     Now we need to make
sure our estimates Vk   are optimistic       that they upper
bound    
    at all times  This is achieved by adding an additional bonus  last term in         which guarantees that
we upper bound the variance of     Now  using an iterative  Bellmantype  Law of Total Variance  we have  see
proof  that the sum of the nextstate variances of      over
  time steps   which is related to the sum of the exploration bonuses over   steps  is bounded by the variance
of the Hsteps return  Thus the size of the bonuses built
by UCBVIBF are constrained over the   steps  And we
prove that the sum of those bonuses do not grow linearly
  only  This is the key for our improved
in   but in
dependence from   to

 

 

  

 

  Main results
In this section we present the main results of the paper 
which are upper bounds on the regret of UCBVICH and
UCBVIBF algorithms  We assume Assumption   holds 
Theorem    Regret bound for UCBVICH  
Consider   parameter      
UCBVICH is bounded      at least       by

 
Regret           
where     ln HSAT  
For     HS   and SA     this bound translates to  
SAT   where     KH is the total

regret bound of  cid    

SAK         AL 

Then the regret of

number of timesteps at the end of episode   
Theorem   is signi cant in that  for large     it improves
the regret dependence from   to
   compared to the best
known bound of  Jaksch et al    The main intuition
for this improved Sdependence is that we bound the estimation error of the nextstate value function directly  instead of the transition probabilities  More precisely  ink         Vk   
         cid cid Vk   cid   as is done in  Jaksch et al 
   instead
 for which   bound with no dependence on   can be
achieved since     is deterministic  and handle carefully

stead of bounding the estimation error  cid     
by  cid cid     
  for example  we bound  cid     
the correction term  cid     

          Vk         

             

  

 

Our second result  Theorem   demonstrates that we can
improve upon the Hdependence by using   more re ned 
BernsteinFriedman type  exploration bonus 
Theorem    Regret bound for UCBVIBF  
Consider   parameter      
UCBVIBF is bounded            by

Then the regret of

Regret       HL
    

SAK         AL 
 

KL 

 

where     ln HSAT  

Algorithm   bonus 

Require   cid Pk       Nk      
 cid   

           HL
return  

Nk      where     ln SAT  

However  the loose form of UCB given by UCBVICH
does not look at the value function of the next state  and
just consider it as being bounded in      However 
much better bounds can be obtained by looking at the variance of the next state values  Our main result relies upon
UCBVI with bonus   bonus  which we refer to as
UCBVIBF as it relies on BernsteinFreedman   concentration inequalities to build the con dence set  UCBVIBF
builds upon the intuition for UCBVICH but also incorporates   variancedependent exploration bonus  This leads to
tighter exploration bonuses and an improved regret bound

 

HSAT  

of  cid   
Require   cid Pk       Vk    Nk    cid 

Algorithm   bonus 

   

 LVarY  cid Pk     Vk      

 cid 
 cid cid cid cid   cid 
  cid Pk       

 

Nk     

 cid 

min

  cid 
Nk     

 HL

 Nk     

 

 cid        AL 

      

    

 cid cid 

      

where   ln SAT  
return  

Compared to UCBVIBF here we use   bonus built from
the empirical variance of the estimated next values  The
idea is that if we had knowledge of the optimal value    
we could build tight con dence bounds using the variance
of the optimal value function at the next state in place of the
loose bound of    Since however     is unknown  here we
use as   surrogate the empirical variance of the estimated
values  As more data is gathered  this variance estimate

Minimax Regret Bounds for Reinforcement Learning

 

translates to   regret bound of  cid   
            our bound is  cid   

We note that for            and SA     this bound
HSAT   This result
 
is particularly signi cant since  for   large enough      
 
HSAT   which matches
the established lower bound  
HSAT   of  Jaksch et al 
  Osband   Van Roy      up to logarithmic factors 
The key insight is to apply concentration inequalities to
bound the estimation errors and the exploration bonuses in
terms of the variance of     at the next state  We then use
the fact that the sum of these variances is bounded by the
variance of the return  see      Munos   Moore   
Azar et al    Lattimore   Hutter    which shows
  instead of linthat the estimation errors accumulate as
early in    thus implying the improved Hdependence 

 

Computational ef ciency
Theorems   and   guarantee the statistical ef ciency of
UCBVI  In addition  both UCBVICH and UCBVIBF
are computationally tractable  Each episode both algorithms perform an optimistic value iteration with computational cost of the same order as solving   known MDP  In
fact  the computational cost of these algorithms can be further reduced by only selectively recomputing UCBVI after suf ciently many observations  This technique is common to the literature  Jaksch et al    Dann   Brunskill 
  and would not affect the    statistical ef ciency  The
computational cost of this variant of UCBVI then amounts

to  cid   SA min SA      min       as it only needs to update the model  cid   SA  times  Jaksch et al   

Weakly communicating MDPs
In this short paper we focus on the setting of  nite horizon
MDPs  By comparison  previous optimistic approaches to
exploration  such as UCRL  provide bounds for the more
general setting of weakly communicating MDPs  Jaksch
et al    Bartlett   Tewari    However  we believe
that much of the insight from the UCBVI algorithm  and
its analysis  will carry over to this more general setting using existing techniques such as  the doubling trick   Jaksch
et al   

  Proof sketch
Here we provide the sketch proof of our results  The full
proof is deferred to the appendix 

  Sketch Proof of Theorem  
Let      Vk        
         be the event under which
all computed Vk   values are upper bounds on the optimal
value function  Using backward induction on    and standard concentration inequalities  one can prove that   holds
with high probability  see Lem    in the appendix  To

 

 

  

def     

simplify notations in this sketch of proof we will not make
the numerical constants explicit  and instead we will denote by  cid    numerical constant which can vary from line
to line  The exact values of these constants are provided
in the full proof  We will also make use of simpli ed notations  such as using   to represent the logarithmic term
    ln cid HSAT  
The cumulative regret at episode   is Regret    def 
   xk  De ne  cid Regret    def 
   xk  Under   we have
and cid    
Regret       cid Regret    so we now bound  cid Regret   
def  Vk         
De ne     
    Thus
  Vk      bk              

 cid 
 cid 
   xk        
        
     Vk xk        
        
        cid        cid     
          Vk           cid        bk   
   cid     
The dif culty in bounding  cid     
Vk    and  cid     
          Vk    is that both
are random variables and are not independent  the value function Vk    computed at      
may depend on the samples collected from state xh   
thus   straightforward application of ChernoffHoeffding
  this issue is addressed by bounding it by  cid cid     
 CH  inequality does not work here 
In  Jaksch et al 
   
     cid cid Vk   cid  at the price of an additional
The main contribution of our  cid    
Instead of directly bounding  cid     
 cid     
            
and deal with the correction term  cid     

SAT   bound  which
  factor compared to the previous bound of
removes  
 Jaksch et al    is to handle this term more properly 
         Vk    we bound
   using straightforward application of CH
   is deterministic 
          Vk     

  factor since    

 which removes the

 cid        cid     
      cid        bk     ek   
          Vk         
def   cid     
             
def   cid     xk    we have
              xk     cid     
 cid        cid     

  xk    is the estimation
where ek  
error of the optimal value function at the next state  De   

ing cid    

   
   We have

  

 

  

 

 

 

       bk     ek   

where     

def            xk           xk   

bound on the correction term  cid     
   
Step  
          xk    Using Bernstein   inequality    
this term is bounded by

        xk   

 cid  

        xk   nk  

         

 cid SHL
nk  

 

 cid 

 cid 

 

Minimax Regret Bounds for Reinforcement Learning

def  Nk xk       xk    Now considering only
where nk  
the   such that         xk   nk      cid       and since
              xk    is

             cid      then  cid     

bounded by

 cid 

 cid 
NK      cid 

nk  

 

 cid 
 cid 

   

Step   Bounding the exploration bonuses  cid 

Using the pigeonhole principle  we have

    bk   

bk      cid HL

 

   

   

  

 

   cid HL

   cid HL

 cid   
Step   Bounding on the estimation errors cid 
Using CH         we have ek      cid     
 cid   

    ek   
 CH 
 cid  
  Thus this bound on the estimation errors are
of the same order as the exploration bonuses  which is the
reason we choose those bonuses 

             

SAT  

 

nk  

  

 cid       

 cid SHL
nk  

 cid 

 
 

      

        

      xk   xk   nk  

 cid  

 cid SHL
nk  

 

 cid       
 cid cid 
 cid   cid  
 cid 

nk  

 cid       
 

def 

          xk   

 cid     

      xk   xk   

where     
 
The sum over the neglected   such that         xk   nk    
 cid      contributes to an additional term

        xk   

 

 

 cid cid         xk   nk hL

 cid 

 

  

   

 

   

   

   

 
 

 cid 

 
 
  

term  and the smaller order

Neglecting this
term
 cid SHL nk    for now  by the pigeonhole principle we
can prove that these terms contribute to the  nal regret by
  constant at most  cid   AH     we have

 cid        cid 
   cid 
 cid 

 cid cid                      bk     ek  
 cid  
 cid 
  cid 
 cid 

              bk     ek  

              bk     ek   

The regret is thus bounded by

We now bound those   terms 

 cid cid 
 cid Regret       cid cid 
 cid 
         and  cid 
lead to   regret of  cid    
 cid 
    bk   and the estimation errors cid 
Step   Bounding the martingales  cid 
 cid 
 cid 

It is easy to check that
         are sums of martingale differences  which are bounded using Azuma   inequality  and
    without dependence on the
size of state and action space  The leading terms in the regret bound comes from the sum of the exploration bonuses

          Using Azuma   inequality we deduce
 cid     

         and

 Az   cid  

 cid 

    ek   

 Az 

    

 

 

 

   

    

    

   

   

           cid SH   

nk  

 

Putting everything together  Plugging   and   into
   and adding the smaller order term  we deduce

Regret     cid Regret   cid cid  

SAK        AL cid 

 
   

 

  Sketch Proof of Theorem  

The proof of Theorem   relied on proving by   straightforh        hold
ward induction over   that      Vk        
with high probability  In the case of exploration bonuses
de ned by 

  

 cid cid LV
 cid 
 cid cid cid cid  min
 cid 

       
Nk      
empirical Bernstein

 cid Vk      cid 
   cid  
 cid cid 
 cid cid      AL cid 
 cid cid 

 
Nk      

 

additional bonus

 cid HL
Nk      

 

 cid Pk       
            
  cid 

 cid 

 

 cid 
 cid 

 

bk          

the backward induction over   is not straightforward  Indeed  if the Vk    are upper bounds on    
   it is not necessarily the case that the empirical variance of Vk    are
upper bound on the empirical variance of    
   However
we can prove by  backward  induction over   that Vk    is
suf ciently close to    
   to guarantee that the variance of
those terms are suf ciently close to each other so that the
additional bonus  additional bonus in   will make sure
that Vk   is still an upperbound on    
    More precisely 
de ne the set of indices 

 

      hist

def                                  

                   

def   Vi        

Minimax Regret Bounds for Reinforcement Learning
                   hist 
and the event     
Our induction is the following 
 cid  SAL
  Assume that      holds 
        cid  
   cid Pk     
 

Then we prove that
      
  cid 

 Vk         

  We

 cid Vk      cid   
 cid    
     cid 

 cid      AL cid 

deduce

that
 cid Pk       
          
  cid 

   cid Pk     
so the additional bonus compensates for the possible
variance difference  Thus Vk        
  and      
holds 

   

   

 

 

So in order to prove that all values computed by the
algorithm are upper bounding     we just need to
prove that under       we have  Vk         
      
min cid    SL
            which is obtained by deN cid 
riving the following regret bound on

def 

 Vi          

  xi     xi        

 cid Rk     

 cid   
 cid 

   

 cid 

   cid HL

SAN cid 

 
Indeed  since  Vi     is   decreasing sequence in    we have

      

        cid Rk       cid 

      

 Vk         

 cid 

   cid HL

SA   cid 

      

Once we have proven that        all computed values are
upper bounds on           event   then we prove that under
  the following regret bound holds 
 
Regret       cid Regret       cid HL

SAK        AL 
 
The proof of   relies on the same derivations as those
used for proving   The only two differences being that
    HK is replaced by   cid 
       the number of times  
 
state   was reached at time       up to episode    and  ii 
  factor which comes from the fact that
the additional
at any episode    cid 
       can only tick once  whereas the
total number of transitions from   during any episode can
be as large as    The full proof of   will be given in
details in the appendix  We now give   proof sketch of  
under  
Similar steps used for proving Theorem   apply  The main
difference compared to Theorem   is the bound on the sum
of the exploration bonuses and the estimation errors  which
we consider in Steps   and   below  This is where we can
  factor  The use of the Bernstein inequalremove the
ity makes it possible to bound both of those terms in terms
of the expected sum of variances  under the current policy

 

   at any episode    of the nextstate values  for that policy  and then using recursively the Law of Total Variance
to conclude that this quantity is nothing but the variance of
the returns  This step is detailed now  For simplicity of the
exposition of this sketch we neglect second order terms 

 

  

 

 

   

 

 cid 

   

nk  

 cid  

Nk      

Nk      

 cid cid 

 cid 

   cid  

  where

main term

   xk   

second order term

 cid Vk   

By CauchySchwarz 

   xk   
nk  

Step   Bounding the sum of exploration bonuses bk   

 cid Vk      cid 
 cid 
 cid 

 cid  
We have cid 
 cid 
bk      cid 
 cid 
 cid cid cid cid  min
 cid cid      AL cid 
 cid Pk       
            
  cid 
 cid cid 
 cid 
 cid 
 cid cid 
 cid 
    cid Vk   
the main term is bounded by
 cid Vk      cid 
Since  cid 
def 
 
   cid  
 
 
the term cid 
    cid Vk   
 cid SA ln     by the pigeonhole principle  we now focus on
We now prove that  cid Vk    is close to    
 cid     
     cid  by bounding the following
quantity cid Vk         
   cid         
        cid      Vk   
     cid         
                cid          
                 
 ii   cid                 
 cid cid 
 cid 
 cid 
 cid 
 cid 

 cid cid 
            

               

           

   xk   

           

  

  

    

 cid 

 ak   

    

    

   

nk  

   cid 

    

    

def 

 

 

  

  

 cid    

 
nk  

 

 

        

     Using similar argument as those used in  Jaksch et al    we have
that

where     holds since under       Vk        
 ii  holds due to Chernoff Hoeffding 

Step     bounding cid 
    cid Vk      
ak        cid cid             cid     cid    cid 
pigeonhole principle cid 

def  Nk xk       xk    Thus from the

    ak      cid     

 where nk  

SL nk   

AT   

 

  

 

and

Now   cid 

    is bounded as

       HP     Vk         
  cid 
   

   HP    cid     

Thus using Azuma   inequality 

 cid 

   

  cid 

   

 Az    

 cid 

   

 cid         cid    

 

   

           cid    

 

    

       of  

  def   cid 
Step     bounding cid 
For any episode      cid 

where   is de ned as an upperbound on the pseudo regret 
   an upper bound on the

    bk     ek     cid  

 

 

   

   

 Dominant term 

exact derivation in the appendix  we deduce

    xk       xk   cid  which

is thus bounded by     Finally  using Freedman    Fr  ink    by its expectation  see the

   
    
   
    Hk  is the expected sum
   
of variances of the value function    
      at the next state
          xk    under the true transition model for the
current policy    recursive application of the law of total variance  see      Munos   Moore    Azar et al 
  Lattimore   Hutter    shows that this quantity is
nothing else than the variance of the return  sum of   re 

wards  under policy       cid cid 
equality to bound cid 
  cid cid 
 cid 
       cid 
Thus  using     and the bounds on cid  ak   and cid    cid 
we deduce that cid 
 cid 
estimation errors cid 

bk      cid   cid             SA 

Step   Bounding the sum of estimation errors
    ek    We now use Bernstein inequality to bound the

    Hk
   
 

         cid    

   cid    

   

    

 cid 

    

    

 

 

   

   

   

 

 

  xk   

ek    

   

 cid 
   cid 

   

   

 cid     
             
 cid   

 cid 

    
nk  

   cid  HL
nk  

 

 cid    
     cid  Now  in  

    

def  VY        xk   

where   
very similar way as in Step   above  we relate   
to    

     and use the Law of total variance to bound
   

     by HT and deduce that

 cid 

    

   

Minimax Regret Bounds for Reinforcement Learning

 cid 

ek      cid   cid             SA 

   

 

HSAT      SAL  This implies  

From   we see that      cid   cid             SA thus

 
     cid  
So the reason we are able to remove the
  factor from
the regret bound comes from the fact that the sum  over  
steps  of the variances of the next state values  which de 
 ne the amplitude of the con dence intervals  is at most
bounded by the variance of the return 
Intuitively this
 
means that the size of the con dence intervals do not add
  only  Although
up linearly over   steps but grows as
the sequence of estimation errors are not independent over
time  we are able to demonstrate   concentration of measure phenomenon that shows that those estimation errors
concentrate as if they were independent 

  

 

 

 

  Conclusion
In this paper we re ne the familiar concept of optimism
in the face of uncertainty  Our key contribution is the design and analysis of the algorithm UCBVIBF   which addresses two key shortcomings in existing algorithms for optimistic exploration in  nite MDPs  First we apply   concentration to the value as   whole  rather than the transition
   Next we
estimates  this leads to   reduction from   to
apply   recursive law of total variance to couple estimates
across an episode  rather than at each time step individually  this leads to   reduction from   to
Theorem   provides the  rst regret bounds which  for suf 
 ciently large     match the lower bounds for the problem
HSAT   up to logarithmic factors  It remains an open
problem whether we can match the lower bound using this
approach for small     We believe that the higher order term

 cid   
can be improved from  cid           to  cid   HS    by   more

  which can be improved to

careful analysis         more extensive use of FreedmanBernstein inequalities  The same applies to the term of order  
These results are particularly signi cant because they help
to estabilish the informationtheoretic lower bound of reinforcement learning at  
HSAT    Osband   Van Roy 
    whereas it was suggested in some previous work
SAT   Moving from
that lowerbound should be of   
this bigpicture insight to an analytically rigorous bound is
nontrivial  Although we push many of the technical details
to the appendix  our paper also makes several contributions
in terms of analytical tools that may be useful in subsequent
work  In particular we believe that the way we construct the
exploration bonus and con dence intervals in UCBVICH
is novel to the literature of RL  Also the constructive approach in the proof of UCBVICH  which bootstraps the
regret bounds to prove that Vk hs are ucbs  is another analytical contribution of this paper 

HT  

 

 

 

 

Minimax Regret Bounds for Reinforcement Learning

Acknowledgements
The authors would like to thank Marc Bellemare and all the
other wonderful colleagues at DeepMind for many hours of
discussion and insight leading to this research  We are also
grateful for the anonymous reviewers for their helpful comments and for  xing several mistakes in an earlier version
of this paper 

References
Agrawal  Shipra and Jia  Randy  Posterior sampling for
reinforcement learning  worstcase regret bounds  arXiv
preprint arXiv   

Azar  Mohammad Gheshlaghi  Munos    mi  and Kappen 
Hilbert    Minimax pac bounds on the sample complexity of reinforcement learning with   generative model 
Machine learning     

Bartlett  Peter    and Tewari  Ambuj  REGAL    regularization based algorithm for reinforcement learning in
In Proceedings of the
weakly communicating MDPs 
 th Conference on Uncertainty in Arti cial Intelligence
 UAI  pp    June  

Bernstein     Theory of probability   

Bertsekas        Dynamic Programming and Optimal
Control  volume    Athena Scienti    Belmount  Massachusetts  third edition   

Bertsekas        and Tsitsiklis        NeuroDynamic Programming  Athena Scienti    Belmont  Massachusetts 
 

Brafman  Ronen    and Tennenholtz  Moshe  Rmax    
general polynomial time algorithm for nearoptimal reinforcement learning  Journal of Machine Learning Research     

Bubeck    bastien and CesaBianchi  Nicol  Regret analysis of stochastic and nonstochastic multiarmed bandit
problems  CoRR  abs    URL http 
 arxiv org abs 

Bubeck    bastien  Munos    mi  Stoltz  Gilles  and
Szepesv ri  Csaba  Xarmed bandits  Journal of Machine Learning Research     

Burnetas  Apostolos   and Katehakis  Michael    Optimal
adaptive policies for markov decision processes  Mathematics of Operations Research     

CesaBianchi     and Lugosi     Prediction  Learning  and
Games  Cambridge University Press  New York  NY 
USA   

Dann  Christoph and Brunskill  Emma  Sample complexity
of episodic  xedhorizon reinforcement learning  In Advances in Neural Information Processing Systems   

Dann  Christoph  Lattimore  Tor  and Brunskill  Emma 
Ubeva more practical algorithm for episodic rl with
nearoptimal pac and regret guarantees  arXiv preprint
arXiv   

Freedman  David    On tail probabilities for martingales 

the Annals of Probability  pp     

Guez  Arthur  Silver  David  and Dayan  Peter 

Scalable and ef cient bayesadaptive reinforcement learning
based on montecarlo tree search  Journal of Arti cial
Intelligence Research  pp     

Jaksch     Ortner     and Auer     Nearoptimal regret
bounds for reinforcement learning  Journal of Machine
Learning Research     

Kearns  Michael    and Singh  Satinder    Nearoptimal
reinforcement learning in polynomial time  Machine
Learning     

Lattimore  Tor and Hutter  Marcus  PAC bounds for dis 

counted MDPs  CoRR  abs   

Maurer  Andreas and Pontil  Massimiliano  Empirical
bernstein bounds and sample variance penalization  stat 
   

Munos     and Moore    

In uence and variance of  
Markov chain   Application to adaptive discretizations in
optimal control  In Proceedings of the  th IEEE Conference on Decision and Control   

Munos    mi  From bandits to MonteCarlo Tree Search 
The optimistic principle applied to optimization and
planning  Foundations and Trends   cid  in Machine Learning     

Osband  Ian and Van Roy  Benjamin  On lower bounds for

regret in reinforcement learning  stat       

Osband  Ian and Van Roy  Benjamin  Why is posterior
sampling better than optimism for reinforcement learning  arXiv preprint arXiv     

Osband  Ian  Russo  Dan  and Van Roy  Benjamin   more 
ef cient reinforcement learning via posterior sampling 
In Advances in Neural Information Processing Systems 
pp     

Strehl  Alexander   and Littman  Michael      theoretical
analysis of modelbased interval estimation  In Proceedings of the  nd international conference on Machine
learning  pp    ACM   

Minimax Regret Bounds for Reinforcement Learning

Strehl  Alexander   and Littman  Michael    An analysis
of modelbased interval estimation for markov decision
processes  Journal of Computer and System Sciences   
   

Strehl  Alexander    Li  Lihong  Wiewiora  Eric  Langford  John  and Littman  Michael    PAC modelfree
reinforcement learning  In ICML  pp     

Strens  Malcolm         Bayesian framework for reinforce 

ment learning  In ICML  pp     

Sutton  Richard and Barto  Andrew  Reinforcement Learn 

ing  An Introduction  MIT Press  March  

Thompson       On the likelihood that one unknown probability exceeds another in view of the evidence of two
samples  Biometrika     

Weissman  Tsachy  Ordentlich  Erik  Seroussi  Gadiel 
Verdu  Sergio  and Weinberger  Marcelo   
Inequalities for the    deviation of the empirical distribution 
HewlettPackard Labs  Tech  Rep   

