Geometry of Neural Network Loss Surfaces via Random Matrix Theory

Jeffrey Pennington   Yasaman Bahri  

Abstract

Understanding the geometry of neural network
loss surfaces is important for the development of
improved optimization algorithms and for building   theoretical understanding of why deep
learning works  In this paper  we study the geometry in terms of the distribution of eigenvalues
of the Hessian matrix at critical points of varying
energy  We introduce an analytical framework
and   set of tools from random matrix theory that
allow us to compute an approximation of this distribution under   set of simplifying assumptions 
The shape of the spectrum depends strongly on
the energy and another key parameter    which
measures the ratio of parameters to data points 
Our analysis predicts and numerical simulations
support that for critical points of small index  the
number of negative eigenvalues scales like the  
power of the energy  We leave as an open problem an explanation for our observation that  in
the context of   certain memorization task  the
energy of minimizers is wellapproximated by
the function  

       

  Introduction
Neural networks have witnessed   resurgence in recent
years  with   smorgasbord of architectures and con gurations designed to accomplish ever more impressive tasks 
Yet for all the successes won with these methods  we have
managed only   rudimentary understanding of why and in
what contexts they work well  One dif culty in extending our understanding stems from the fact that the neural
network objectives are generically nonconvex functions
in highdimensional parameter spaces  and understanding
their loss surfaces is   challenging task  Nevertheless  an
improved understanding of the loss surface could have  
large impact on optimization  Saxe et al  Dauphin et al 

 Google Brain  Correspondence to  Jeffrey Pennington  jpen 

nin google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Choromanska et al    Neyshabur et al   
architecture design  and generalization  Keskar et al 

  Related work

There is no shortage of prior work focused on the loss
surfaces of neural networks  Choromanska et al   
and Dauphin et al    highlighted the prevalence of
saddle points as dominant critical points that plague optimization  as well as the existence of many local minima
at low loss values  Dauphin et al    studied the distribution of critical points as   function of the loss value
empirically and found   trend which is qualitatively similar to predictions for random Gaussian landscapes  Bray
  Dean    Choromanska et al    argued that the
loss function is wellapproximated by   spinglass model
studied in statistical physics  thereby predicting the existence of local minima at low loss values and saddle points
at high loss values as the network increases in size  Goodfellow et al  observed that loss surfaces arising in practice tend to be smooth and seemingly convex along lowdimensional slices  Subsequent works  Kawaguchi   
Safran   Shamir    Freeman   Bruna    have furthered these and related ideas empirically or analytically 
but it remains safe to say that we have   long way to go
before   full understanding is achieved 

  Our contributions

One shortcoming of prior theoretical results is that they are
often derived in contexts far removed from practical neural network settings   for example  some work relies on
results for generic random landscapes unrelated to neural
networks  and other work draws on rather tenuous connections to spinglass models  While there is   lot to be gained
from this type of analysis  it leaves open the possibility that
characteristics of loss surfaces speci   to neural networks
may be lost in the more general setting  In this paper  we
focus narrowly on the setting of neural network loss surfaces and propose an analytical framework for studying the
spectral density of the Hessian matrix in this context 
Our bottomup construction assembles an approximation
of the Hessian in terms of blocks derived from the weights 
the data  and the error signals  all of which we assume to

Geometry of Neural Networks

be random variables  From this viewpoint  the Hessian
may be understood as   structured random matrix and we
study its eigenvalues in the context of random matrix theory  using tools from free probability  We focus on singlehidden layer networks  but in principle the framework can
accommodate any network architecture  After establishing
our methodology  we compute approximations to the Hessian at several levels of re nement  One result is   prediction that for critical points of small index  the index scales
like the energy to the   power 

  Preliminaries
Let       Rn    and       Rn    be weight matrices of   singlehidden layer network without biases  Denote by     Rn   the input data and by     Rn   the
targets  We will write           for the preactivations 
We use     max    to denote the ReLU activation 
which will be our primary focus  The network output is

 yi   

   

ik    

      

 

and the residuals are ei     yi    yi  We use Latin indices
for features  hidden units  and outputs  and   to index examples  We consider mean squared error  so that the loss
 or energy  can be written as 

  cid 

  

        

 
  

  
    

 

    cid 

  

    cid 

  

The Hessian matrix is the matrix of second derivatives of
the loss with respect to the parameters              
 
where              It decomposes into two pieces 
            where    is positive semide nite and
where    comes from second derivatives and contains all
of the explicit dependence on the residuals  Speci cally 

 

       
 

   yi 
 

   yi 
 

   
 

 JJ      

 

where we have introduced the Jacobian matrix     and 

    cid 

  

ei 

 cid     yi 

 cid 

 

       
 

 

 

We will almost exclusively consider square networks with
                 We are interested in the limit of
large networks and datasets  and in practice they are typically of the same order of magnitude    useful charac 

 Although we additionally assume the random variables are
independent  our framework does not explicitly require this assumption  and in principle it could be relaxed in exchange for
more technical computations 

terization of the network capacity is the ratio of the number of parameters to the effective number of examples 
       mn         As we will see    is   critical
parameter which governs the shape of the distribution  For
instance  eqn    shows that    has the form of   covariance matrix computed from the Jacobian  with   governing
the rank of the result 
We will be making   variety of assumptions throughout this
work  We distinguish primary assumptions  which we use
to establish the foundations of our analytical framework 
from secondary assumptions  which we invoke to simplify
computations  To begin with  we establish our primary assumptions 

  The matrices    and    are freely independent   

property we discuss in sec   

  The residuals are        normal random variables with
tunable variance governed by   ei          This
assumption allows the gradient to vanish in the large
  limit  specifying our analysis to critical points 

  The data features are        normal random variables 
  The weights are        normal random variables 

We note that normality may not be strictly necessary  We
will discuss these assumptions and their validity in sec   

  Primer on Random Matrix Theory
In this section we highlight   selection of results from the
theory of random matrices that will be useful for our analysis  We only aim to convey the main ideas and do not
attempt   rigorous exposition  For   more thorough introduction  see        Tao   

  Random matrix ensembles

The theory of random matrices is concerned with properties of matrices   whose entries Mij are random variables 
The degree of independence and the manner in which the
Mij are distributed determine the type of random matrix
ensemble to which   belongs  Here we are interested
primarily in two ensembles  the real Wigner ensemble for
which   is symmetric but otherwise the Mij are        and
the real Wishart ensemble for which     XX   where
Xij are        Moreover  we will restrict our attention to
studying the limiting spectral density of    For   random
matrix Mn   Rn    the empirical spectral density is de 
 ned as 

 Mn   

 
 

       Mn   

 

  cid 

  

 In our context of squared error  each of the    targets may be

considered an effective example 

where the    Mn                   denote the   eigenvalues
of Mn  including multiplicity  and     is the Dirac delta
function centered at    The limiting spectral density is
de ned as the limit of eqn    as       if it exists 

For   matrix   of the real Wigner matrix ensemble
whose entries Mij         Wigner   computed
its limiting spectral density and found the semicircle law 

 SC       

     

if      
otherwise

 

 

 cid   

 
 

 

Similarly  if     XX   is   real Wishart matrix with
    Rn   and Xij           then the limiting spectral density can be shown to be the MarchenkoPastur distribution  Mar cenko   Pastur   

 MP       

if      
 
          otherwise

 cid 

 

 

 

where         and 

 cid         

   

 

       cid   

 

The Wishart matrix   is low rank if       which explains
the delta function density at   in that case  Notice that there
is an eigenvalue gap equal to   which depends on  

  Free probability theory

Suppose   and   are two random matrices  Under what
conditions can we use information about their individual
spectra to deduce the spectrum of        One way of
analyzing this question is with free probability theory  and
the answer turns out to be exactly when   and   are freely
independent  Two matrices are freely independent if

Ef        fk   gk       

 

whenever fi and gi are such that

Efi          Egi         

 
Notice that when       this is equivalent to the de nition of classical independence  Intuitively  the eigenspaces
of two freely independent matrices are in  generic position   Speicher         they are not aligned in any special way  Before we can describe how to compute the spectrum of        we must  rst introduce two new concepts 
the Stieltjes transform and the   transform 

  THE STIELTJES TRANSFORM
For           the Stieltjes transform   of   probability
distribution   is de ned as 

 cid 

Geometry of Neural Networks

from which   can be recovered using the inversion formula 

       
 

lim
 

Im          

 

  THE   TRANSFORM
Given the Stieltjes transform   of   probability distribution   the   transform is de ned as the solution to the
functional equation 

 
The bene   of the   transform is that it linearizes free convolution  in the sense that 

    

     

RA     RA   RB  

 
if   and   are freely independent  It plays   role in free
probability analogous to that of the log of the Fourier
transform in commutative probability theory 

  cid     cid   

 

The prescription for computing the spectrum of      
is as follows    Compute the Stieltjes transforms of   
and       From the Stieltjes transforms  deduce the  
transforms RA and RB    From RA     RA   RB 
compute the Stieltjes transform GA    and   Invert the
Stieltjes transform to obtain      

  Warm Up  Wishart plus Wigner
Having established some basic tools from random matrix
theory  let us now turn to applying them to computing the
limiting spectral density of the Hessian of   neural network
at critical points  Recall from above that we can decompose the Hessian into two parts              and that
     JJ       Let us make the secondary assumption
that at critical points  the elements of   and    are       
normal random variables  In this case  we may take    to
be   real Wishart matrix and    to be   real Wigner matrix 
We acknowledge that these are strong requirements  and
we will discuss the validity of these and other assumptions
in sec   

  Spectral distribution

We now turn our attention to the spectral distribution of  
and how that distribution evolves as the energy changes 
For this purpose  only the relative scaling between    and
   is relevant and we may for simplicity take         and
     

  Then we have 

 

       MP     

         SC 

     

 
which can be integrated using eqn    to obtain GH  and
GH  Solving eqn    for the   transforms then gives 

 

      

   
     

dt  

 

RH     

 

      

  RH          

 

 

Geometry of Neural Networks

         

         

         

Figure   Spectral distributions of the Wishart   Wigner approximation of the Hessian for three different ratios of parameters to data
points    As the energy   of the critical point increases  the spectrum becomes more semicircular and negative eigenvalues emerge 

We proceed by computing the   transform of   

RH   RH    RH   

 

      

      

 

so that  using eqn    we  nd that its Stieltjes transform
GH solves the following cubic equation 

    

            

            GH            
The correct root of this equation is determined by the
asymptotic behavior GH      as        Tao   
From this root  the spectral density can be derived through
the Stieltjes inversion formula  eqn    The result is illustrated in       For small   the spectral density resembles
the MarchenkoPastur distribution  and for small enough
  there is an eigenvalue gap  As   increases past   critical value     the eigenvalue gap disappears and negative
eigenvalues begin to appear  As   gets large  the spectrum
becomes semicircular 

  Normalized index
Because it measures the number of descent directions 
one quantity that is of importance in optimization and in
the analysis of critical points is the fraction of negative
eigenvalues of the Hessian  or the index of   critical point 
  Prior work  Dauphin et al    Choromanska et al 
  has observed that the index of critical points typically grows rapidly with energy  so that critical points with
many descent directions have large loss values  The precise
form of this relationship is important for characterizing the
geometry of loss surfaces  and in our framework it can be
readily computed from the spectral density via integration 

     

              

          

 
Given that the spectral density is de ned implicitly through
equation eqn    performing this integration analytically

 Here we ignore degenerate critical points for simplicity 

 cid   

 

 cid   

 

is nontrivial  We discuss   method for doing so in the supplementary material  The full result is too long to present
here  but we  nd that for small  

 cid cid cid cid        

  

 cid cid cid cid 

 

 

       

where the critical value of  

    

 
 

                   

 

is the value of the energy below which all critical points are
minimizers  We note that    can be found in   simpler way 
it is the value of   below which eqn    has only real roots
at            it is the value of   for which the discriminant
of the polynomial in eqn    vanishes at       Also  we
observe that    vanishes cubically as   approaches  

      
 

                

 

The   scaling in eqn    is the same behavior that was
found in  Bray   Dean    in the context of the  eld
theory of Gaussian random functions  As we will see later 
the   scaling persists in   more re ned version of this calculation and in numerical simulations 

  Analysis of Assumptions
  Universality

There is   wealth of literature establishing that many properties of large random matrices do not depend on the details
of how their entries are distributed       many results are
universal  For instance  Tao   Vu   show that the
spectrum of Wishart matrices asymptotically approaches
the MarcenkoPastur law regardless of the distribution of
the individual entries  so long as they are independent  have
mean zero  unit variance  and  nite kth moment for      
Analogous results can be found for Wigner matrices  Aggarwal  Although we are unaware of any existing analy 

 Geometry of Neural Networks

  RESIDUALS ARE        RANDOM NORMAL
First  we note that ei          is consistent with the
de nition of the energy   in eqn    Furthermore  because
the gradient of the loss is proportional to the residuals  it
vanishes in expectation       as       which specializes
our analysis to critical points  So this assumptions seems
necessary for our analysis  It is also consistent with the priors leading to the choice of the squared error loss function 
Altogether we believe this assumption is fairly mild 

  DATA ARE        RANDOM NORMAL

This assumption is almost never strictly satis ed  but it is
approximately enforced by common preprocessing methods  such as whitening and random projections 

  WEIGHTS ARE        RANDOM NORMAL

Although the        assumption is clearly violated for   network that has learned any useful information  the weight
distributions of trained networks often appear random  and
sometimes appear normal  see              of the supplementary material 

  Secondary assumptions

In sec    we introduced two assumptions we dubbed secondary  and we discuss their validity here 

    AND    ARE        RANDOM NORMAL
Given that   and    are constructed from the residuals  the
data  and the weights   variables that we assume are       
random normal   it is not unreasonable to suppose that their
entries satisfy the universality conditions mentioned above 
In fact  if this were the case  it would go   long way to validate the approximations made in sec    As it turns out  the
situation is more complicated  both   and    have substructure which violates the independence requirement for
the universality results to apply  We must understand and
take into account this substructure if we wish to improve
our approximation further  We examine one way of doing
so in the next section 

  Beyond the Simple Case
In sec    we illustrated our techniques by examining the
simple case of Wishart plus Wigner matrices  This analysis
shed light on several qualitative properties of the spectrum
of the Hessian matrix  but  as discussed in the previous section  some of the assumptions were unrealistic  We believe
that it is possible to relax many of these assumptions to produce results more representative of practical networks  In
this section  we take one step in that direction and focus on
the speci   case of   singlehidden layer ReLU network 

Figure   Testing the validity of the free independence assumption by comparing the eigenvalue distribution of          in
blue  and      QH QT  in orange  for randomly generated orthogonal    The discrepancies are small  providing support for
the assumption  Data is for   trained singlehidden layer ReLU
autoencoding network with   hidden units and no biases on  
      downsampled  grayscaled  whitened CIFAR  images 

ses relevant for the speci   matrices studied here  we believe that our conclusions are likely to exhibit some universal properties   for example  as in the Wishart and Wigner
cases  normality is probably not necessary 
On the other hand  most universality results and the tools
we are using from random matrix theory are only exact
in the limit of in nite matrix size  Finitesize corrections
are actually largest for small matrices  which  counterintuitively  means that the most conservative setting in
which to test our results is for small networks  So we
will investigate our assumptions in this regime  We expect
agreement with theory only to improve for larger systems 

  Primary assumptions

In sec    we introduced   number of assumptions we
dubbed primary and we now discuss their validity 

  FREE INDEPENDENCE OF    AND   
Our use of free probability relies on the free independence
of    and    Generically we may expect some alignment
between the eigenspaces of    and    so that free independence is violated  however  we  nd that this violation
is often quite small in practice  To perform this analysis 
it suf ces to examine the discrepancy between the distribution of         and that of      QH QT   where   is an
orthogonal random matrix of Haar measure  Chen et al 
  Fig    show minimal discrepancy for   network
trained on autoencoding task  see the supplementary material for further details and additional experiments  More
precise methods for quantifying partial freeness exist  Chen
et al    but we leave this analysis for future work 

           QH QT Geometry of Neural Networks

  Product Wishart distribution and   
In the singlehidden layer ReLU case  we can write    as 

where its offdiagonal blocks are given by the matrix   

 cid 

  
 

 

    

   ab cd  

 

  

 cid   
 cid 
  cid 

 
 

  

 

 
 

 

ei 

 

    
cd

   yi 
    
ab

ec ad   

    xb   

 

  cid 

 

Here it is understood that the matrix    is obtained from
the tensor    ab cd by independently vectorizing the  rst
two and last two dimensions   ad is the Kronecker delta
function  and   is the Heaviside theta function  As discussed above  we take the error to be normally distributed 

       yc   cid           

   

cj    

ec   cid cid 

 

 

We are interested in the situation in which the layer width
  and the number of examples   are large  in which case
   
     can be interpreted as   mask that eliminates half of
the terms in the sum over   If we reorder the indices so
that the surviving ones appear  rst  we can write 

      
 

 ad

ec xb   

In      xT  

 
 

where we have written    and    to denote the     
  matrices
derived from   and   by removing the vanishing half of
their columns 
Owing to the block offdiagonal structure of   
the
squares of its eigenvalues are determined by the eigenvalues of the product of the blocks 

    

   

 

   In      xT    xT     

 

The Kronecker product gives an nfold degeneracy to each
eigenvalue  but the spectral density is the same as that of

     

      xT    xT     

 

It follows that the spectral density of    is related to that
of   

            

 

Notice that   is   Wishart matrix where each factor is itself
  product of real Gaussian random matrices  The spectral
density for matrices of this type has been computed using

Figure   Spectrum of    at initialization  using       downsampled  grayscaled  whitened CIFAR  in   single layer ReLU autoencoder with   hidden units  Error signals have been replaced
by noise distributed as                  Theoretical prediction  red  matches well  Left       right      

the cavity method  Dupic   Castillo    As the speci  
form of the result is not particularly enlightening  we defer
its presentation to the supplementary material  The result
may also be derived using free probability theory  Muller 
  Burda et al    From either perspective it is possible to derive the the   transform  which reads 

RH     

  

         

 

See       for   comparison of our theoretical prediction for
the spectral density to numerical simulations 

     and the Wishart assumption
Unlike the situation for    to the best of our knowledge
the limiting spectral density of    cannot easily be
deduced from known results in the literature  In principle
it can be computed using diagrammatics and the method
of moments  Feinberg   Zee    Tao    but this
approach is complicated by serious technical dif culties
  for example  the matrix has both block structure and
Kronecker structure  and there are nonlinear functions
applied to the elements  Nevertheless  we may make
some progress by focusing our attention on the smallest
eigenvalues of    which we expect to be the most relevant
for computing the smallest eigenvalues of   

The empirical spectral density of    for an autoencoding network is shown in       At  rst glance  this
distribution does not closely resemble the MarchenkoPastur distribution  see       the       curve in      
owing to its heavy tail and small eigenvalue gap  On the
other hand  we are not concerned with the large eigenvalues  and even though the gap is small  its scaling with
  appears to be wellapproximated by   from eqn   

 As the derivation requires that the error signals are random  in
the simulations we manually overwrite the network   error signals
with random noise 

 Eigenvalue Probability density    over   runs  output   noise phi value    Eigenvalue Probability density    over   runs  output   noise phi value    Geometry of Neural Networks

Figure   The spectrum of    at initialization of   single layer
ReLU autoencoder with   hidden units and         downsampled  grayscaled  whitened CIFAR  images  There are  
null directions  and the corresponding zero eigenvalues have been
removed  The inset shows the values of the smallest   nonzero
eigenvalues  The positive value of the  rst datapoint re ects the
existence of   nonzero gap 

Figure   Evolution of the    spectral gap as   function of  
comparing the MarcenkoPastur  red  against empirical results for
   hidden unit singlelayer ReLU autoencoder at initialization
 black dots  each averaged over   independent runs  Dataset
was taken from   downsampled  grayscaled  whitened CIFAR 
  images    single    parameter  characterizing the variance of
the matrix elements in the Wishart ensemble  is used 

for appropriate   See       This observation suggests
that as    rst approximation  it is sensible to continue to
represent the limiting spectral distribution of    with the
MarchenkoPastur distribution 

  Improved approximation to the Hessian

The above analysis motivates us to propose an improved
approximation to RH    

RH      

 

       

 

  

         

 

where the  rst term is the   transform of the MarchenkoPastur distribution with the   parameter restored  As before  an algebraic equation de ning the Stieltjes transform
of    can be deduced from this expression 

     cid          cid      cid          cid   
   cid          cid   

        

 

 

   

Using the same techniques as in sec    we can use this
equation to obtain the function     We again  nd the
same   scaling      

 

 

 cid cid cid cid 

       

 cid cid cid cid        
 cid             cid 

  

Compared with eqn    the coef cient function  
differs from   and 

    

     

         
 

Despite the apparent pole at          actually vanishes
there 

      
 

                

 

Curiously  for       this is precisely the same behavior
we found for the behavior of    near   in the Wishart
plus Wigner approximation in sec    This observation 
combined with the fact that the   scaling in eqn    is
also what we found in sec    suggest that it is    rather
than    that is governing the behavior near    

  Empirical distribution of critical points

We conduct largescale experiments to examine the distribution of critical points and compare with our theoretical
predictions  Uniformly sampling critical points of varying
energy is   dif cult problem  Instead  we take more of  
brute force approach  for each possible value of the index 
we aim to collect many measurements of the energy and
compute the mean value  Because we cannot control the
index of the obtained critical point  we run   very large
number of experiments     in order to obtain suf cient
data for each value of   This procedure appears to be  
fairly robust method for inferring the   curve 

the following heuristic for  nding critical
We adopt
points  First we optimize the network with standard gradient descent until the loss reaches   random value between
  and the initial loss  From that point on  we switch
to minimizing   new objective  Jg       which 
unlike the primary objective  is attracted to saddle points 
Gradient descent on Jg only requires the computation of
Hessianvector products and can be executed ef ciently 

 Value of eigenvalue Histogram kth smallest nonzero eigenvaluek Value Gap in      spectrumGeometry of Neural Networks

    Index of critical points versus energy

    Energy of minimizers versus parameters data points

Figure   Empirical observations of the distribution of critical points in singlehidden layer tanh networks with varying ratios of parameters to data points        Each point represents the mean energy of critical points with index   averaged over   training runs  Solid
lines are best    curves for small             The good agreement  emphasized in the inset  which shows the behavior for small
  provides support for our theoretical prediction of the   scaling      The best    value of    from     versus     surprisingly good   
        The difference between the curves shows the bene   obtained
is obtained with       
from using   nonlinear activation function 

        Linear networks obey       

We discard any run for which the  nal Jg    
otherwise we record the  nal energy and index 

We consider relatively small networks and datasets in
order to run   large number of experiments  We train
singlehidden layer tanh networks of size       which
also equals the input and output dimensionality  For each
training run  the data and targets are randomly sampled
from standard normal distributions  which makes this
  kind of memorization task  The results are summarized in       We observe that for small   the scaling
            is   good approximation  especially for
smaller   This agreement with our theoretical predictions
provides support for our analytical framework and for the
validity of our assumptions 
As   byproduct of our experiments  we observe that the
energy of minimizers is well described by   simple func 
        Curiously    similar functional form
tion        
was derived for linear networks  Advani   Ganguli   
In both cases  the value at       and
      
      is understood simply  at       the network has
zero effective capacity and the variance of the target distribution determines the loss  at       the matrix of hidden
units is no longer rank constrained and can store the entire
inputoutput map  For intermediate values of   the fact
that the exponent of       is larger for tanh networks
than for linear networks is the mathematical manifestation
of the nonlinear network   better performance for the same
number of parameters  Inspired by these observations and
by the analysis of Zhang et al    we speculate that
this result may have   simple informationtheoretic explanation  but we leave   quantitative analysis to future work 

       

  Conclusions
We introduced   new analytical framework for studying
the Hessian matrix of neural networks based on free
probability and random matrix theory  By decomposing
the Hessian into two pieces             one can
systematically study the behavior of the spectrum and
associated quantities as   function of the energy   of  
critical point  The approximations invoked are on    and
   separately  which enables the analysis to move beyond
the simple representation of the Hessian as   member of
the Gaussian Orthogonal Ensemble of random matrices 

We derived explicit predictions for the spectrum and
the index under   set of simplifying assumptions  We
found empirical evidence in support of our prediction
that that small             raising the question of
how universal the   scaling may be  especially given the
results of  Bray   Dean    We also showed how
some of our assumptions can be relaxed at the expense of
reduced generality of network architecture and increased
technical calculations  An interesting result of our numerical simulations of   memorization task is that the energy of
minimizers appears to be wellapproximated by   simple
function of   We leave the explanation of this observation
as an open problem for future work 

Acknowledgements
We thank Dylan    Foster  Justin Gilmer  Daniel HoltmannRice  Ben Poole  Maithra Raghu  Samuel    Schoenholz 
Jascha SohlDickstein  and Felix    Yu for useful comments and discussions 

   Benefitofnonlinearity Geometry of Neural Networks

References
Advani  Madhu and Ganguli  Surya  Statistical mechanics
of optimal convex inference in high dimensions  Physical Review       

Aggarwal  Amol 

wigner matrices with few moments 
arXiv 

Bulk universality for generalized
arXiv preprint

Bray  Alan    and Dean  David    Statistics of critical
points of gaussian  elds on largedimensional spaces 
Phys  Rev  Lett    Apr   doi   
PhysRevLett  URL http link aps 
org doi PhysRevLett 

Burda     Jarosz     Livan     Nowak  MA  and Swiech 
   Eigenvalues and singular values of products of rectangular gaussian random matrices  Physical Review   
   

Chen  Jiahao  Van Voorhis  Troy  and Edelman  Alan 
arXiv preprint

Partial freeness of random matrices 
arXiv   

Choromanska  Anna  Henaff  Mikael  Mathieu  Micha el 
Arous    erard Ben  and LeCun  Yann  The loss surface
of multilayer networks  JMLR     

Dauphin  Yann    Pascanu  Razvan  Gulcehre  Caglar 
Cho  Kyunghyun  Ganguli  Surya  and Bengio  Yoshua 
Identifying and attacking the saddle point problem in
highdimensional nonconvex optimization  Advances
in Neural Information Processing Systems   pp   
   

Dupic  Thomas and Castillo  Isaac   erez  Spectral density
of products of wishart dilute random matrices  part    the
dense case  arXiv preprint arXiv   

Keskar  Nitish Shirish  Mudigere  Dheevatsa  Nocedal 
Jorge  Smelyanskiy  Mikhail  and Tang  Ping Tak Peter  On largebatch training for deep learning  GenICLR   URL
eralization gap and sharp minima 
http arxiv org abs 

Laisant  CA 

Int egration des fonctions inverses  Nouvelles annales de math ematiques  journal des candidats
aux  ecoles polytechnique et normale     

Mar cenko  Vladimir   and Pastur  Leonid Andreevich 
Distribution of eigenvalues for some sets of random matrices  Mathematics of the USSRSbornik   
 

Muller  Ralf    On the asymptotic eigenvalue distribution
IEEE
of concatenated vectorvalued fading channels 
Transactions on Information Theory   
 

Neyshabur  Behnam  Salakhutdinov  Ruslan  and Srebro 
Nathan  Pathsgd  Pathnormalized optimization in deep
neural networks  pp     

Safran  Itay and Shamir  Ohad  On the quality of the initial
basin in overspeci ed neural networks  JMLR     
URL http arxiv org abs 

Saxe  Andrew    McClelland  James    and Ganguli 
Surya  Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks  ICLR 

Speicher  Roland  Free probability theory  arXiv preprint

arXiv   

Tao     and Vu     Random covariance matrices  universality of local statistics of eigenvalues  Annals of Probability     

Tao  Terence  Topics in random matrix theory  volume  
American Mathematical Society Providence  RI   

Feinberg  Joshua and Zee  Anthony  Renormalizing rectangles and other topics in random matrix theory  Journal
of statistical physics     

Wigner  Eugene    Characteristic vectors of bordered matrices with in nite dimensions  Annals of Mathematics 
pp     

Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht  Benjamin  and Vinyals  Oriol  Understanding deep learning requires rethinking generalization  arXiv preprint
arXiv   

Freeman     Daniel and Bruna  Joan  Topology and geometry of halfrecti ed network optimization    URL
http arxiv org abs 

Goodfellow  Ian    Vinyals  Oriol  and Saxe  Andrew   
Qualitatively characterizing neural network optimization
problems  ICLR   URL http arxiv org 
abs 

Kawaguchi  Kenji  Deep learning without poor local minima  Advances in Neural Information Processing Systems   pp     

