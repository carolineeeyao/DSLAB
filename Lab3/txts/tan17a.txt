Partitioned Tensor Factorizations for Learning Mixed Membership Models

Zilong Tan   Sayan Mukherjee  

Abstract

We present an ef cient algorithm for learning
mixed membership models when the number of
variables   is much larger than the number of
hidden components    This algorithm reduces
the computational complexity of stateof theart
tensor methods  which require decomposing an

  cid   cid  tensor  to factorizing         subtensors
each of size   cid   cid  In addition  we address the

issue of negative entries in the empirical method
of moments based estimators  We provide suf 
 cient conditions under which our approach has
provable guarantees  Our approach obtains competitive empirical results on both simulated and
real data 

  Introduction
Mixed membership models  Woodbury et al   
Pritchard et al        Blei et al    Erosheva   
have been used extensively across applications ranging
from modeling population structure in genetics  Pritchard
et al        to topic modeling of documents  Woodbury et al    Blei et al    Erosheva    Mixed
membership models use Dirichlet latent variables to de ne
cluster membership where samples can partially belong to
each of   latent components  Parameter estimation for such
latent variables models  LVMs  using maximum likelihood
methods such as expectation maximization is computationally intensive for large data  for example  if number of samples   is large 
Parameter estimation using the method of moments for
LVMs is an attractive scalable alternative that has been
shown to have certain theoretical and computational advantages over maximum likelihood methods in the setting when   is large  For LVMs  method of moments approaches reduce to tensor methods the moments of the
model parameters are expressed as   function of statistics

 Duke University  Durham  NC  Correspondence to  Sayan

Mukherjee  sayan stat duke edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

of the observations in   tensor form  Inference in this setting becomes   problem of tensor factorization  Computational advantages of using tensor methods have been observed for many popular models  including latent Dirichlet
allocation  Anandkumar et al    spherical Gaussian
mixture models  Hsu   Kakade    hidden Markov
models  Anandkumar et al    independent component
analysis  Comon   Jutten    and multiview models
 Anandkumar et al    An appealing property of tensor
methods is the guarantee of   unique decomposition under
mild conditions  cf  Kruskal    Leurgans et al   
There are two complications to using standard tensor decomposition methods  Anandkumar et al      Gu
et al    Kuleshov et al    Nicol   Colombo and
Nikos Vlassis  Kim et al    Chi   Kolda    for
LVMs  The  rst problem is computation and space complexity  Given   variables in the LVM  parameter inference
requires factorizing typically   nonorthogonal estimator

tensor of size   cid   cid   Anandkumar et al    Kuleshov
nal and symmetric  this can be done in   cid    log   cid   Wang

et al    Nicol   Colombo and Nikos Vlassis  which
is prohibitive for large    When the estimator is orthogo 

et al    Online tensor decomposition  Huang et al 
  uses dimension reduction to instead factorize   reduced kby kby   tensor  However  the dimension reduction can be slower than decomposing the estimator directly
for large sample sizes  as well as suffer from high variance
 Wang et al    We introduce   simple factorization
with improved complexity for the general case where the
parameters are not required to be orthogonal 
The second problem arises from negative entries in the
empirical moments tensor  LVMs for count data are constrained to have nonnegative parameters  However  the empirical moments tensor computed from the data may contain negative elements due to sampling variation and noise 
Indeed  for small sample sizes or data with many small
or zero counts  there will be many negative entries in the
empirical moments tensor  General tensor decomposition
algorithms  Kuleshov et al    Nicol   Colombo and
Nikos Vlassis  including the tensor power method  TPM 
 Anandkumar et al    do not guarantee the nonnegativity of model parameters  Approaches such as positive nonnegative tensor factorization  Chi   Kolda   
Shashua   Hazan    Welling   Weber    also do

Partitioned Tensor Factorizations for Learning Mixed Membership Models

not address this situation as they require all the elements of
the tensor to be factorized to be nonnegative  With robust
tensor methods  Anandkumar et al    Gu et al   
sparse negative entries may potentially be treated as corrupted elements  however  these methods are not applicable
in this setting since there can be many negative elements 
In this paper  we introduce   novel parameter inference
algorithm called partitioned tensor parallel quadratic programming  PTPQP  that is ef cient in the setting where
the number of variables   is much larger than the number
of latent components    The algorithm is also robust to
negative entries in the empirical moments tensor  There
are two key innovations in the PTPQP algorithm  The  rst
innovation is   partitioning technique which recovers the
parameters through factorizing         much smaller sub 

tensors each of size   cid   cid  The second innovation is   par 

allel quadratic programming  Brand   Chen    based
algorithm to factor tensors with negative entries under the
constraint that the factors are all nonnegative  To the best
of our knowledge  this is the  rst algorithm designed to address the problem of negative entries in empirical estimator tensors  We show that the proposed factorization algorithm converges linearly with respect to each factor matrix 
We also provide suf cient conditions under which the partitioned factorization scheme is consistent  the parameter
estimates converge to the true parameters 

  Preliminaries
Notations  We use bold lowercase letters to represent vectors and bold capital letters for matrices  Tensors are denoted by calligraphic capital letters  The subscript notation
Aj refers to jth column of matrix    We denote the jth column of the identity matrix as ej and   is   vector
of ones  We further write diag     for   diagonal matrix
whose diagonal entries are    and diag     to mean   vector of the diagonal entries of   
Elementwise matrix operators include  cid  and  cid          cid 
  refers to
  means that   has nonnegative entries 
elementwise max       and  cid  respectively represent
elementwise multiplication and division  Moreover   
refers to the outer product and  cid  denotes the KhatriRao
product   cid cid   and  cid cid  represent the Frobenius norm and
spectral norm  respectively 
Tensor basics  This paper uses similar tensor notations
as  Kolda   Bader    In particular  we are primarily
concerned with Kruskal tensors in Rd      which can
be expressed in the form of

  cid 

   

Aj   Bj   Cj 

 

  

where       and   are respectively   byr    byr  and

  byr factor matrices  The rank of   is de ned as the
smallest   that admits such   decomposition  The decomposition is known as the CP  CANDECOMP PARAFAC 
decomposition  The jmode unfolding of     denoted by
     for           is   djby 
matrix whose
rows are serializations of the tensor  xing the index of the
jth dimension  The unfoldings have the following wellknown compact expressions 

 cid cid 

  cid   dt

 cid 

           cid    
           cid    

 cid 
 cid 

 

 

 cid 
           cid    

 

  Learning through Method of Moments
  Generalized Dirichlet latent variable models

  generalized Dirichlet latent variable model  GDLM  was
proposed in  Zhao et al    for the joint distribution of
  observations         yn  Each observation yi con 
 cid  GDLM
sists of   variables yi    yi  yi    yip 
assumes   generative process involving   hidden components  For each observation  sample   random Dirichlet
 cid        with concenvector xi    xi  xi    xik 
 cid  The elements
tration parameter              
of xi are the membership probabilities for yi to belong to
each of the   components  Speci cally 

yij     cid 

xihgj  jh   

  

where gj  jh  is the density of the jth variable speci  
to component   with parameter                 jk 
One advantage of GLDM is that yij can take categorical
values  Let dj denote the number of categories for the jth
variable  set dj     for scalar variables     becomes   djby   probability matrix where the cth row corresponds to
category    We aim to accurately recover    from independent copies of yi involving variables of mixed data types 
either categorical or noncategorical 

  Momentbased estimators

The moment estimators of latent variable models typically
take the form of   tensor  Anandkumar et al    Consider the estimators of GDLM  Zhao et al    for example  Let bij   eyij if variable   is categorical  bij   yij
otherwise  The secondand thirdorder parameter estimators for variable       and   are written
 cid 
Mjs      bij   bis         bij     bis 
Mjst      bij   bis   bit         bij       bis       bit 
            bij    bis   bit       bij      bis    bit 
    bij   bis      bit   

Partitioned Tensor Factorizations for Learning Mixed Membership Models

 cid 
 cid 

  

Alternatively  Mjs and Mjst have the following CP decomposition into parameters    
   jh    sh 

 uv   Rdi

Mjs  

 

 

Mjst  

   jh    sh    th 

 uv   Rdi 

  

Here               and    depend on only    cid 

      
For the special case of latent Dirichlet allocation  Mjs and
Mjst are scalar joint probabilities 
The parameters    are typically obtained by factorizing the
block tensor    whose       th element is the empirical

 cid Mjs and or    whose          th element is the empirical
 cid Mjst  Anandkumar et al      Zhao et al   

Note that    are generally nonorthogonal  and thus preprocessing steps are needed for orthogonal decomposition
methods  Wang et al    Song et al    Anandkumar et al    The preprocessing can be expensive and
often leads to suboptimal performance  Souloumiac   
Nicol   Colombo and Nikos Vlassis  Here  we highlight  
few relevant observations 

  Mjs alone does not yield unique parameters    due
to the wellknown rotation problem  This is true even
when enforcing nonnegativity constraints on parameters  Donoho   Stodden   

  Mjst is suf cient to uniquely recover the parameters
under certain mild conditions  Kruskal    for example  when any two of         and    have linearly
independent columns and the columns of the third are
pairwise linearly independent  Leurgans et al   

  The empirical estimator  cid Mjst generally contains neg 

ative entries due to variance and noise  The fraction of
negative entries can approach   as we shall see in
experiments  We address this issue in    

  While the decomposition   can be unique up to permutation and rescaling  the correspondence between
each column of the factor matrix and each hidden
component may not be consistent across multiple decompositions  Techniques for achieving consistency
are developed in    

  Computational complexity

  cid     

Tensor methods such as TPM typically decompose the

 cid  full estimator tensor that includes all vari 

ables  More ef cient algorithms have been developed for
the case that parameters are orthogonal  Wang et al   
Song et al    and when the sample size is small
 Huang et al    However  these methods do not

max

apply in the general case where the parameters are nonorthogonal and the sample size can be potentially large   
key insight underlying our approach is that it is suf cient to
recover the parameters by factorizing only         much

smaller subtensors each of size   cid   cid  This technique

can also be combined with the aforementioned methods to
further improve the complexity in certain cases 

  An ef cient algorithm
In this section  we develop partitioned tensor parallel
quadratic programming  PTPQP  an ef cient approximate
algorithm for learning mixed membership models  We  rst
introduce   novel partitioningand matching scheme that
reduces parameter estimation to factorizing   sequence of
subtensors  Then  we develop   nonnegative factorization algorithm that can handle negative entries in the subtensors 

  Partitioned factorization
Factorizing the full tensor formed by all Mjst is expensive
while   threevariable tensor Mjst in   alone may not be
suf cient to determine    when   is large  In this section 
we consider factorizing the subtensors corresponding to  
cover of the set of variables     such that each subtensor
admits an identi able CP decomposition        unique
up to permutation and rescaling of columns  This gives
the parameters for all variables  Suppose that       and
the maximum number of categories dmax is   constant  the
aggregated size of the subtensors can be much smaller      

  cid pk cid  than the size   cid   cid  of the full estimator 

Let         and    denote ordered subsets       with cardinality       pj        ps  and       pt  respectively 
Consider the pjby psby pt block tensor          whose
         th element is the tensor         
   
  
From   the tensor          is

uvw      

   

 

  cid 

  

  

   

 

 

  
  
  
  
 
  

pj  

  
  
  
  
 
  
ps

 

   

 

   

  
  
  
  
 
  
pt

 

from          by setting
   cid            

Clearly  the block tensor is identi able if it has an identi 
 able subtensor  Suppose that   subtensor         is
identi able  then one can construct an identi able tensor
    cid   cid   cid 
   cid            
We further remark that   subtensor can be identi able under mild conditions  for example  if the sum of the Kruskal
rank of the three factor matrices is at least than       
 Kruskal   

   cid            

 

Partitioned Tensor Factorizations for Learning Mixed Membership Models

Given an identi able subtensor         of anchor variables indexed by         and     the partitioning produces
  set of subtensors  partitions  constructed through  
that includes all variables  Thus          is   common
subtensor shared across all partitions  We choose anchor
variables whose parameter matrices are of full column rank
to obtain an identi able          Finally  one can divide
the rest of variables evenly and randomly into the partitions 

  Matching parameters with hidden components

Since the factorization of   partition   can only be identi able up to permutation and rescaling of the columns of
constituent     the correspondence between the columns of
   and hidden components can differ across partitions  To
enforce consistency  we associate   permutation operator
  are the parameters speci   to hidden component   across all variables   
Consider the following vector representation of  

   for each variable   such that cid    

 cid 

                        
                     

Observe that              within   factorization of
Mjst  and this also holds for the partitioned factorization   of          as well               
       
             
Consider the factorizations of          and         and

suppose that     cid             cid                  The

permutation operator for one factorization is determined
given the other by column matching the parameters of variable   in both factorizations  Thus  an inductive way to
achieve   consistent factorization is to start with one factorization  and let its permutation be the identity         
then perform the factorization over new sets of variables
with at least one variable in common with the initial factorization  Permutations for the sequential factorizations are
determined via column matching parameter matrices of the
common variables 
Given two factorized parameter matrices    and  cid 
  of variable    our goal is to  nd   consistent permutation    of   
with respect to  cid 
jh correspond
to the same hidden component for all         We now
present an algorithm with provable guarantees to compute
  consistent permutation 
Smallest angle matching   simple matching algorithm is
to match the two columns of the two parameter matrices
that have the smallest angle between them  Consider the
factorizations of Mjst and Mjuv which yield respectively
parameters    and  cid 
  for the common variable    Given the
permutation    for Mjst  the permutation    for Mjuv is

   such that      and  cid 

computed by 

 cid   cid cid 

       

 cid 

 

ts

 

  

    arg max

 

  represent respectively the normalized   

  with each column having unit Euclidean norm 

Here     and  cid 
and  cid 
There are cases that    computed via Equation   is not
consistent       contains duplicate entries and hence is
ineligible  and   since    and  cid 
  are the factorized parameter matrices which are generally perturbed from the
groundtruth  the resulting    may differ from the consistent permutation  To cope with these cases  we establish in
    the suf cient conditions for    to be consistent 
Orthogonal Procrustes matching One issue with the
smallest angle matching is that each column is paired independently  It is easy for multiple columns to be paired with
  common nearest neighbor  We describe   more robust algorithm based on the orthogonal Procrustes problem  and
show improved guarantees  Since   consistent permutation
is orthogonal    natural relaxation is to only require the operator to be orthogonal  This is an orthogonal Procrustes
problem  formulated in the same settings as    
      cid      

          

 cid cid   cid 

 cid cid 

 

 

 

min
 

Let  cid cid 
                cid  be the singular value decomposition  SVD  the solution   is given by the polar factor
 Sch onemann   

         cid 

 
Here    is orthogonal and does not immediately imply
the desired permutation     To compute     one can additionally restrict   to be   permutation matrix  and solve
for    using linear programming  Gower   Dijksterhuis 
Aside from ef ciency  one fundamental question is that under what assumptions the objective   yields the consistent
permutation 
Given the solution   to the Procrustes problem  we propose the following simple algorithm for computing    

  

    arg max

 

 
ts 

 

 cid cid    cid 

We  rst establish through Theorem   that if    obtained
using   is   valid permutation       no duplicate entries 
then it is optimal in terms of the objective  
Theorem   The    obtained using   satis es

 

         

         
for all permutations  
In section     we state suf cient conditions under which the
objective   yields   consistent permutation 

 

 cid cid 

 cid cid   cid 

 cid cid 

Partitioned Tensor Factorizations for Learning Mixed Membership Models

  Approximate nonnegative factorization

  subtensor  cid    cid 

In previous sections  we reduced the inference problem to
factorizing partitioned subtensors  We now present   factorization algorithm for the subtensors that contain negative entries  Our goal is to approximate   subtensor   by
  Aj   Bj   Cj where the factors   
   and   are nonnegative  The Frobenius norm is used to
quantify the approximation

 cid cid cid      cid   cid cid cid  

min

      cid 

 

 

Note that we do not assume that    cid    in   which
distinguishes our optimization problem from other approximate factorization algorithms  Welling   Weber    Chi
  Kolda    Kim et al    Shashua   Hazan   
In     we provide some details as to why negative entries
are problematic for standard approximate factorization algorithms  We can rewrite   using the  mode unfolding
as

 cid cid cid            cid    
 cid cid cid cid  

min

      cid 

 

 

 cid 

 cid 

  Muvw    cid Muvw

 cid 

 

Equivalent formulations with respect to the  mode and  
mode unfoldings can be readily obtained from  
We point out that another widelyused error measure   the
Idivergence  Finesso   Spreij    Chi   Kolda   
  may not be suitable for our learning problem  The optimization using Idivergence is given by

Muvw cid Muvw

min

Muvw log

     

      cid 
This optimization is useful for nonnegative   when each
entry follows   Poisson distribution  In this case  the objective is equivalent to the sum of KullbackLeibler divergence
across all entries of   

 cid 
Pois    Muvw cid cid  Pois

 cid 

    cid Muvw

 cid cid 

 

 cid 

DKL

     

However  the Poisson assumption does not generally hold
for the estimator tensor  

  Handling negative entries in empirical estimators

We  rst illustrate that factorizing   tensor with negative entries using either positive tensor factorization  Welling  
Weber    or nonnegative tensor factorization  Chi  
Kolda    Shashua   Hazan    will either result
in factors that violate the the nonnegativity constraint or
the result of the algorithm diverges  In addition  we show
that general tensor decompositions cannot enforce the factor nonnegativity even after rounding the negative entries
to zero 

We then present   simple method based on weighted nonnegative matrix factorization  WNMF   Zhang et al   
that enforce the factor nonnegativity constraint  We further
generalize this method using parallel quadratic programming  PQP   Brand   Chen    to obtain   method with
  provable convergence rate 
Issue of negative entries If the tensor is strictly nonnegative  the optimization speci ed in   can be reduced to
nonnegative matrix factorization  NMF  Solvers abound
for NMF including the celebrated LeeSeung   multiplicative updates  Lee   Seung    The reduction is done by
viewing   as  cid         cid 
        
and         cid    

  with     Mjst

 cid  and alternating

Wst   Wst

 

 

 cid     cid cid 

st

   HH cid st

over each unfolding and factor matrix     Obviously  the
updates may yield negative entries in   when the unfolding contains negative entries  In addition  convergence relies on the nonnegativity of the unfolding  cf  Lee   Seung    This issue extends to their tensor factorization
variants  Welling   Weber    Chi   Kolda    Kim
et al    known as the positive tensor factorization and
nonnegative tensor factorization  For these approaches   

naive resolution is to round negative entries of  cid Mjst to  

this however lacks theoretical guarantees 
It is important to note that the rounding does not help general tensor decompositions like TPM  The following example illustrates that the unique decomposition  up to permutation and rescaling  of   positive tensor can contain negative entries  Consider    by by  positive tensor  whose
 mode unfolding is given by

 cid 

 
 

 
 

 
 

 

 

 cid 
 cid 

 cid 

 
 

where the vertical bar separates two frontal slices  It has
the following decomposition  written in the form of  

 cid   
 cid 

       

     

 

 
Since all factors are of fullrank 
the decomposition
is unique up to permutation and rescaling of columns
 Kruskal    Thus    general tensor decomposition
yields     with negative entries regardless of rescaling 

 

 

  Factorization via WNMF
Since the groundtruth Mjst are nonnegative  we may  ig 

nore  the negative entries of  cid Mjst by treating them as

missing values  This idea leads to the following modi ed
objective 

 cid             cid 

 

min

     cid 

 

where           are chosen identically as   and de ne
the boolean  uv   Yuv     The optimization can be
carried out using WNMF  Here  we modify the original updates by introducing   positive constant   to ensure that the
updates are wellde ned 

Partitioned Tensor Factorizations for Learning Mixed Membership Models
Algorithm   Factorize          
       maxuvw  Muvw   
  Initialize with random nonnegative matrices 
    rand  dj         rand  ds         rand  dt    
  Create   set of alternating variable tuples 

     

 cid          cid cid 

Wuv   Wuv

uv    
            cid uv    

 

 

It can be easily shown that the presence of   does not affect the solution accuracy 
In addition  having   in both
the numerator and denominator of   guarantees that the
objective   is nonincreasing under the updates 

  Parallel quadratic programming

We now generalize the WNMF approach using parallel
quadratic programming to obtain   convergence rate  Let
   denote the set of symmetric positive de nite matrices 
we consider the following optimization problem

min

 

 
 

  cid Qx     cid                    

 

which can be solved by iterating multiplicative updates
 Brand   Chen    Sha et al    We use the parallel
quadratic programming  PQP  algorithm  Brand   Chen 
  Brand et al    to solve   partly because it has
  provable linear convergence rate  The PQP multiplicative
update for   takes the following simple form 

       cid         cid   cid cid         cid   

 

with

           diag    
            

           diag  
            

Here   and   are arguments to PQP  we will discuss these
arguments in section     The update maintains nonnegativity since all items are nonnegative  We make the following observation 

Theorem   The multiplicative updates for LeeSeung and
WNMF are special cases of PQP 

We can now solve the approximate nonnegative factorization problem stated in   using   Theorem   states the
multiplicative updates    more detailed discussion of   is
included in     We present pseudocode in Algorithm  
Theorem   For optimization   the following update
converges linearly to   local optimum

       cid       cid   cid cid AQ          cid 

 

   cid cid   cid   cid   cid   cid   cid   cid          cid    cid cid 
       cid cid   cid   cid   cid   cid   cid   cid          cid    cid cid 
      cid cid   cid   cid   cid   cid   cid   cid          cid    cid cid 
   cid diag  ZQ   cid diag    
       cid       cid   cid cid XQ          cid 

     
                 cid 

for each           in   do

 
min

repeat

 cid 

end for

until   ceased to change  or reached max  iterations
Normalize the columns of         to sum to  
return        

with

   cid   cid   cid   cid   cid   cid   

 cid cid 

            cid    

 cid 

 

 

   cid   
 

diag  ZQ   cid 

 min    

diag    

 cid       

where  min   is the smallest eigenvalue  Similar updates
for   and   are obtained using  

  Proposed approach

To summarize  the proposed approach  referred to as PTPQP  consists of three steps  Given the indexes of anchor
variables               the variables                  
are  rst evenly divided into   partitions  and the anchor
variables are added to each partition  The second step
consists of forming and factorizing the subtensor of each
partition using Algorithm   this step can be parallelized 

Third  normalize the anchor matrix cid   cid     cid     cid cid cid 

formed by the anchor variable parameters to have unit column Euclidean norm  and then use either   or   to
match over the anchor matrix 
Ef ciency  Most of the computational cost is in the
factorization  Consider one partition  and let         
be the corresponding subtensor 
the subtensor size is
   dh  The maximum number of cate 
        
gories for   variable is generally   constant for the GDLM 
Under smallest partitioning  this size is determined by the

 cid 
subtensor of anchor variables         cid   cid  which corre 

 cid 

sponds to       partitions  One bene   of PTPQP is that
the number of subtensor factorizations is linear in   due to
the partitioned factorization  this results in signi cant ef 
ciency gains when    cid     Furthermore  PTPQP is easy to

Partitioned Tensor Factorizations for Learning Mixed Membership Models

be parallelized across multiple CPUs and machines  since
the computation as well as data are not distributed across
partitions 

  Provable Guarantees
In this section  we state the main theoretical results of the
proposed partitioned factorization and tensor PQP factorization  The proofs can be found in the longer version of
this paper  Tan   Mukherjee   

  Suf cient conditions for guaranteed matching

Theorem   and Theorem   state that when the anchor parameter matrices from two factorizations are  close  the
proposed matching algorithms obtain   consistent permutation 
Theorem   Suppose that    is the groundtruth matrix
for variable    Solving Equation   results in   consistent

permutation if for all factors cid   of variable  
 cid cid cid jh  cid jh

 cid 

 cid 

 cid cid cid 

 cid 

 cid   cid 

 

  

 cid 

uv

 cid cid cid cid   

     

    max
   

 

 

 
 

 cid jh cid 

for all         where  jh    jh cid jh cid 
Theorem   states that one obtains   consistent permutation
by solving Equation   in the columns of the groundtruth
parameter matrix are distinct from each other in angles and
the factorized parameter matrix is near the groundtruth in
Frobenius norm  Thus    good anchor variable for the partitioned factorization   is one whose parameter matrix has
distant columns in angles 
The bound in Theorem   can be made sharp for certain    
and thus the smallest angle matching algorithm has general
guarantees only when the perturbation is small       the rel 

ative error ratio is less than    cid 

     

   

 

Theorem   Suppose that   and  cid  are two factorized parameter matrices for   variable  Solving   results in  
consistent permutation   if  cid   cid      

 cid 

 cid cid cid  and
     
 cid cid cid       

 

 

 

 cid cid cid 

 cid 

 cid   cid 

 

log

     
 

with

                   

      

where the error matrix is de ne as      
and      denotes the jth largest singular value 

 cid 

 cid     

The  rst condition in Theorem   requires that at least one
of   and  cid  must have full column rank  We may exchange

  and  cid  in Theorem   to  rst obtain the consistent permutation of  cid  with respect to     then follows immediately 
Theorem   states that solving   recovers   consistent
permutation whenever the error spectral norm is small as
compared to the smallest singular value of  cid  This is
especially useful for     Rd   with the number of rows
  much larger than the number of columns   
In particular  for   with independent and identically distributed
subgaussian entries    

 cid cid cid  is at least of the order

 cid 

 cid 

     

     

 Rudelson   Vershynin   

  Convergence

The following theorem states   suf cient condition for PQP
to achieve linear convergence rate  The theorem statement
and proof is an adaptation of results stated in  Brand  
Chen   the proof in  Brand   Chen    overlooks   required condition on   and the condition    cid 
diag  Qjj  in the original proof is unnecessary 
Theorem   The PQP algorithm given by   monotonically decreases the objective   and has linear convergence  if    cid        and
  cid    
 min    

diag          

 cid cid 

   cid   
 

 cid 

 

 

 

where  min   is the smallest eigenvalue 

  Results on real and simulated data
We compare the proposed algorithm ptpqp with stateof 
theart approaches including    the tensor power method
tpm  Anandkumar et al    and matrix simultaneous diagonalization  nojd  and nojd   Kuleshov et al 
 two general tensor decomposition methods   
nonnegative tensor factorization hals  Kim et al   
and   generalized method of moments meld  Zhao et al 
  We use the online code provided by the corresponding authors  The code to reproduce the experiments is
available at  https goo gl DBXIo 

  Learning GDLMs on simulated data

We adapt   simulation study from  Zhao et al    to
compare runtime and accuracy of parameter estimation 
We consider   GDLM where each variable takes categorical values         and the parameters of the Dirichlet mixing distribution are        
   We initially
consider   variables  The true parameters for each hidden component   are drawn from the Dirichlet distribution Dir         The resulting moment estimator is    by by  tensor  We vary the number
of components   and add noise by replacing   fraction  

Partitioned Tensor Factorizations for Learning Mixed Membership Models

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

     

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

     

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

     

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

Figure   RMSE between inferred and true parameters 

     

 

 

 

 

 

 

 

 

 

 

 

 

 

ptpqp
hals
meld
tpm
nojd 
nojd 

of the observations with draws from   discrete uniform
distribution  We also vary the number of samples    
        number of clusters            
and contamination           Across these settings
we found that the empirical thirdorder estimator typically
exhibits between   and   negative entries 
Accuracy of inference  Accuracy is measured by rootmean square error  RMSE  which we compare across algorithms as   function of the number of components for various sample sizes and levels of contamination  see Figure  
Both hals and ptpqp are consistently among the top estimators  and ptpqp outperforms hals as   grows  For small
sample sizes and many hidden components meld achieves
the smallest RMSE  The RMSE of tpm is relatively large 
probably due to the whitening technique used to approximately transform the nonorthogonal factorization into an
orthogonal one  see  Souloumiac    Nicol   Colombo
and Nikos Vlassis  The most relevant observation is that
ptpqp outperforms other methods for large  noisy data 
Computational cost  We examined how runtime scales as
  function of the number of partitions  For the same model
we set       variables and       samples  The
tensor is now  by by  We evaluated the
runtime of ptpqp  without parallelization  with the number of partitions set to           On   laptop
with Intel   HQ GHz CPU and  GB memory 
ptpqp with   partitions completes within   min    min 
and   min for           respectively  In addition  the
runtime monotonically decreases with the number of partitions  Further speedups can be obtained by parallelizing
the factorization of partitions across multiple CPUs or machines 

  Predicting crowdsourced labels

In  Zhang et al      combination of EM and tensor
decompositions was used to predict crowdsourcing annotations  The task is to predict the true label given incom 

plete and noisy observations from   set of workers  this is
  mixed membership problem  Dawid   Skene    In
 Zhang et al      thirdorder tensor estimator was proposed to obtain an initial estimate for the EM algorithm 
We compare the predictive performance on  ve data sets of
several tensor decomposition methods as well as the EM
algorithm initialized with majority voting by the workers
 MV EM  The fraction of incorrect predictions and the
size of each dataset are in the table below  Note that ptpqp matches or outperforms the other tensor methods on
all but one dataset  and even outperforms MV EM on two
datasets 

Table   Incorrectly predicted labels  

DATASET

BIRDS
 
PTPQP
 
HALS
 
TPM
 
NOJD 
NOJD 
 
MV EM  
 

SIZE

RTE
 
 
 
 
 
 
 

TREC DOGS WEB
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 

  Conclusions
We proposed an ef cient algorithm for learning mixed mixture models based on the idea of partitioned factorizations 
The key challenge is to consistently match the partitioned
parameters with the hidden components  We provided suf 
 cient conditions to ensure consistency 
In addition  we
have also developed   nonnegative approximation to handle
the negative entries in the empirical method of moments estimators    problem not addressed by several recent tensor
methods  Results on synthetic and real data corroborate
that the proposed approach achieves improved inference
accuracy as well as computational ef ciency than stateof 
theart methods 

Partitioned Tensor Factorizations for Learning Mixed Membership Models

Acknowledgements
     would like to thank Rong Ge for sharing helpful insights       would like to thank LekHeng Lim for insights       would like to acknowledge the support of
grants NSF CNS  NSF IIS  and NSF
CNS       would like to acknowledge the support of grants NSF IIS  NSF DMS 
NSF IIS  NSF DMS  and NSF DMS 
 

References
Anandkumar  Anima  Foster  Dean    Hsu  Daniel   
Kakade  Sham    and kai Liu  Yi    spectral algorithm for latent dirichlet allocation  In NIPS  pp   
   

Anandkumar  Anima  Jain  Prateek  Shi  Yang  and Niranjan        Tensor vs  matrix methods  Robust tensor
decomposition under block sparse perturbations  In AISTATS  pp     

Anandkumar  Animashree  Ge  Rong  Hsu  Daniel 
Kakade  Sham    and Telgarsky  Matus  Tensor decompositions for learning latent variable models  JMLR 
  January  

Blei  David    Ng  Andrew    and Jordan  Michael   

Latent Dirichlet allocation  JMLR     

Brand     and Chen     Parallel quadratic programming
for image processing  In IEEE International Conference
on Image Processing  ICIP  pp    September
 

Brand     Shilpiekandula     and Bortoff         parallel
quadratic programming algorithm for model predictive
control  In World Congress of the International Federation of Automatic Control  IFAC  volume   August
 

Chi  Eric    and Kolda  Tamara    On tensors  sparsity 
and nonnegative factorizations  SIAM Journal on Matrix
Analysis and Applications    December
 

Comon  Pierre and Jutten  Christian  eds  Handbook of
blind source separation   independent component analysis and applications  Communications engineering  Elsevier   

Dawid        and Skene        Maximum likelihood estimation of observer errorrates using the em algorithm 
Journal of the Royal Statistical Society  Series    Applied Statistics     

Donoho  David and Stodden  Victoria  When does nonnegative matrix factorization give   correct decomposition into parts  In NIPS  pp     

Erosheva  Elena    Comparing latent structures of the
grade of membership  rasch  and latent class models 
Psychometrika     

Finesso  Lorenzo and Spreij  Peter  Nonnegative matrix
factorization and Idivergence alternating minimization 
Linear Algebra and its Applications   
July  

Gower       and Dijksterhuis       Procrustes Problems 
ISBN

Oxford Statistical Science Series  OUP Oxford 
 

Gu  Quanquan  Gui  Huan  and Han  Jiawei  Robust tensor decomposition with gross corruption  In NIPS  pp 
   

Hsu  Daniel and Kakade  Sham    Learning mixtures of
spherical gaussians  Moment methods and spectral decompositions  In Innovations in Theoretical Computer
Science  ITCS  pp     

Huang  Furong  Niranjan        Hakeem  Mohammad Umar  Verma  Prateek  and Anandkumar  AnFast detection of overlapping communiimashree 
CoRR 
ties via online tensor methods on gpus 
abs    URL http arxiv org 
abs 

Kim  Jingu  He  Yunlong  and Park  Haesun  Algorithms
for nonnegative matrix and tensor factorizations    uni 
 ed view based on block coordinate descent framework 
Journal of Global Optimization     

Kolda  Tamara    and Bader  Brett    Tensor decompositions and applications  SIAM Rev    August  

Kruskal  Joseph    Threeway arrays  Rank and uniqueness of trilinear decompositions  with application to
arithmetic complexity and statistics  Linear Algebra and
Its Applications     

Kuleshov     Chaganty     and Liang     Tensor factoriza 

tion via matrix factorization  In AISTATS   

Lee  Daniel    and Seung     Sebastian  Algorithms for
nonnegative matrix factorization  In NIPS  pp   
 

Leurgans        Ross        and Abel          decomposition for threeway arrays  SIAM Journal on Matrix
Analysis and Applications     

Partitioned Tensor Factorizations for Learning Mixed Membership Models

Zhang  Sheng  Wang  Weihong  Ford  James  and Makedon  Fillia  Learning from incomplete ratings using nonnegative matrix factorization  In In Proceedings of the
 th SIAM Conference on Data Mining  SDM  pp   
   

Zhang  Yuchen  Chen  Xi  Zhou  Dengyong  and Jordan 
Michael    Spectral methods meet EM    provably optimal algorithm for crowdsourcing  In NIPS  pp   
   

Zhao  Shiwen  Engelhardt  Barbara    Mukherjee  Sayan 
and Dunson  David    Fast moment estimation for generalized latent dirichlet models  CoRR  abs 
 
URL http arxiv org abs 
 

Nicol   Colombo and Nikos Vlassis  title   Tensor Decomposition via Joint Matrix Schur Decomposition  booktitle   ICML pages     year    

Pritchard  Jonathan    Stephens  Matthew  and Donnelly 
Peter  Inference of population structure using multilocus
genotype data  Genetics       

Pritchard  Jonathan    Stephens  Matthew  Rosenberg 
Noah    and Donnelly  Peter  Association mapping in
structured populations  The American Journal of Human
Genetics       

Rudelson  Mark and Vershynin  Roman  Smallest singular
value of   random rectangular matrix  Comm  Pure Appl 
Math  pp     

Sch onemann  Peter    generalized solution of the orthogonal procrustes problem  Psychometrika   
 

Sha  Fei  Saul  Lawrence    and Lee  Daniel    Multiplicative updates for nonnegative quadratic programIn NIPS  pp   
ming in support vector machines 
   

Shashua  Amnon and Hazan  Tamir  Nonnegative tensor
factorization with applications to statistics and computer
vision  In ICML  pp     

Song  Zhao  Woodruff  David    and Zhang  Huan  Sublinear time orthogonal tensor decomposition  In NIPS 
pp     

Souloumiac     Joint diagonalization  is nonorthogonal
In  rd International
always preferable to orthogonal 
Workshop on Computational Advances in MultiSensor
Adaptive Processing  pp     

Tan  Zilong and Mukherjee  Sayan  Ef cient learning of
graded membership models  CoRR  abs 
 
URL http arxiv org abs 
 

Wang  Yining  Tung  HsiaoYu  Smola  Alexander    and
Anandkumar  Anima  Fast and guaranteed tensor decomposition via sketching  In NIPS  pp     

Welling  Max and Weber  Markus  Positive tensor factorization  Pattern Recognition Letters   
 

Woodbury  Max    Clive  Jonathan  and Garson  Arthur 
Jr  Mathematical typology    grade of membership technique for obtaining disease de nition  Computers and
Biomedical Research     

