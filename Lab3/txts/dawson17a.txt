An In nite Hidden Markov Model With SimilarityBiased Transitions

Colin Reimer Dawson   Chaofan Huang   Clayton    Morrison  

Abstract

We describe   generalization of the Hierarchical
Dirichlet Process Hidden Markov Model  HDPHMM  which is able to encode prior information that state transitions are more likely between  nearby  states  This is accomplished
by de ning   similarity function on the state
space and scaling transition probabilities by pairwise similarities  thereby inducing correlations
among the transition distributions  We present
an augmented data representation of the model
as   Markov Jump Process in which    some
jump attempts fail  and   the probability of success is proportional to the similarity between the
source and destination states  This augmentation restores conditional conjugacy and admits  
simple Gibbs sampler  We evaluate the model
and inference method on   speaker diarization
task and    harmonic parsing  task using fourpart chorale data  as well as on several synthetic
datasets  achieving favorable comparisons to existing models 

  Introduction and Background
The hierarchical Dirichlet process hidden Markov model
 HDPHMM   Beal et al    Teh et al    is  
Bayesian model for time series data that generalizes the
conventional hidden Markov Model to allow   countably
in nite state space  The hierarchical structure ensures that 
despite the in nite state space    common set of destination
states will be reachable with positive probability from each
source state  The HDPHMM can be characterized by the
following generative process 
Each state 
indexed by    has parameters      drawn
from   base measure       toplevel sequence of state
weights                  is drawn by iteratively break 

  Oberlin College  Oberlin  OH  USA  The University of Arizona  Tucson  AZ  USA  Correspondence to  Colin Reimer Dawson  cdawson oberlin edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ing    stick  off of the remaining weight according to  
Beta     distribution  The parameter       is known
as the concentration parameter and governs how quickly
the weights tend to decay  with large   corresponding to
slow decay  and hence more weights needed before   given
cumulative weight is reached  This stickbreaking process
is denoted by GEM  Ewens    Sethuraman    for
Grif ths  Engen and McCloskey  We thus have   discrete
probability measure     with weights    at locations    
                de ned by
        

    GEM 

 

  

   drawn in this way is   Dirichlet Process  DP  random
measure with concentration   and base measure   
The actual transition distribution      from state    is drawn
from another DP with concentration   and base measure
  

       DP   

  

 
where   represents the initial distribution  The hidden
state sequence              zT is then generated according to
         Cat  and

               

zt   zt   zt    Cat zt   

                 

 

Finally  the emission distribution for state   is   function of
    so that observation yt is drawn according to

yt   zt   zt      zt 

 

  shortcoming of the HDP prior on the transition matrix is
that it does not use the fact that the source and destination
states are the same set  that is  each    has   special element which corresponds to   selftransition  In the HDPHMM  however  selftransitions are no more likely   priori
than transitions to any other state  The Sticky HDPHMM
 Fox et al    addresses this issue by adding an extra
mass   at location   to the base measure of the DP that
generates     That is    is replaced by
     DP          

 

An alternative approach that treats selftransitions as special
is the HDP Hidden SemiMarkov Model  HDPHSMM  Johnson   Willsky   wherein state duration distributions are modeled separately  and ordinary selftransitions are ruled out  However  while both of these

An In nite HMM With SimilarityBiased Transitions

models have the ability to privilege selftransitions  they
contain no notion of similarity for pairs of states that are
not identical  in both cases  when the transition matrix is
integrated out  the prior probability of transitioning to state
  cid  depends only on the toplevel stick weight associated
with state   cid  and not on the identity or parameters of the
previous state   
The two main contributions of this paper are     generalization of the HDPHMM  which we call the HDPHMM
with local transitions  HDPHMM LT  that allows for   geometric structure to be de ned on the latent state space  so
that  nearby  states are   priori more likely to have transitions between them  and     simple Gibbs sampling algorithm for this model  The  LT  property is introduced by
elementwise rescaling and then renormalizing of the HDP
transition matrix  Two versions of the similarity structure
are illustrated  in one case  two states are similar to the extent that their emission distributions are similar  In another 
the similarity structure is inferred separately  In both cases 
we give augmented data representations that restore conditional conjugacy and thus allow   simple Gibbs sampling
algorithm to be used for inference 
  rescaling and renormalization approach similar to the
one used in the HDPHMM LT is used by Paisley et al 
  to de ne their Discrete In nite Logistic Normal
 DILN  model  an instance of   correlated random measure
 Ranganath   Blei    in the setting of topic modeling 
There  however  the contexts and the mixture components
 topics  are distinct sets  and there is no notion of temporal dependence  Zhu et al    developed an HMM
based directly on the DILN model  Both Paisley et al  and
Zhu et al  employ variational approximations  whereas we
present   Gibbs sampler  which converges asymptotically
to the true posterior  We discuss additional differences between our model and the DILNHMM in Sec   
One class of application in which it is useful to incorporate
  notion of locality occurs when the latent state sequence
consists of several parallel chains  so that the global state
changes incrementally  but where these increments are not
independent across chains  Factorial HMMs  Ghahramani
et al    are commonly used in this setting  but this ignores dependence among chains  and hence may do poorly
when some combinations of states are much more probable
than suggested by the chainwise dynamics 
Another setting where the LT property is useful is when
there is   notion of state geometry that licenses syllogisms 
     if   frequently leads to   and   and   frequently leads
to   and    then it may be sensible to infer that   and  
may lead to   and   as well  This property is arguably

 We thank an anonymous ICML reviewer for bringing this pa 

per to our attention 

present in musical harmony  where consecutive chords are
often  near neighbors in the  circle of  fths  and small
steps along the circle are more common than large ones 
The paper is structured as follows  In section   we de 
 ne the model 
In section   we develop   Gibbs sampling algorithm based on an augmented data representation  which we call the Markov Jump Process with Failed
Transitions  MJPFT  In section   we test two versions
of the model  one on   speaker diarization task in which
the speakers are interdependent  and another on   fourpart chorale corpus  demonstrating performance improvements over stateof theart models when  local transitions 
are more common in the data  Using sythetic data from
an HDPHMM  we show that the LT variant can learn
not to use its similarity bias when the data does not support it  Finally  in section   we conclude and discuss
the relationships between the HDPHMM LT and existing
HMM variants  Code and additional details are available at
http colindawson net hdphmm lt 

  An HDPHMM With Local Transitions
We wish to add to the transition model the concept of  
transition to    nearby  state  where transitions between
states   and   cid  are more likely   priori to the extent that
they are  nearby  in some similarity space 
In order to
accomplish this  we  rst consider an alternative construction of the transition distributions  based on the Normalized Gamma Process representation of the DP  Ishwaran  
Zarepour    Ferguson   

    Normalized Gamma Process representation of

the HDPHMM

The Dirichlet Process is an instance of   normalized completely random measure  Kingman    Ferguson   

that can be de ned as    cid 

         where

ind  Gamma         

  

  

otherwise  and subject to the constraint that cid 
distributed as   DP with base measure     cid 

  is   measure assigning   to sets if they contain   and  
         
and           It has been shown  Ferguson    Paisley et al    Favaro et al    that the normalization
constant   is positive and  nite almost surely  and that   is
        
If we draw                 from the GEM  stickbreaking process  draw an        sequence of    from   base
measure    and then draw an        sequence of random
measures   Gj                  from the above process  this
de nes   Hierarchical Dirichlet Process  HDP  If each Gj
is associated with the hidden states of an HMM    is the
in nite matrix where entry  jj cid  is the   cid th mass associated

 cid 

  

    

 

 

  
 

An In nite HMM With SimilarityBiased Transitions

and nj     cid 

with the jth random measure  and Tj is the sum of row   
then we obtain the prior for the HDPHMM  where

  zt   zt       zt zt    jj cid Tj

 

  Promoting  Local  Transitions

In the HDP prior  the rows of the transition matrix are conditionally independent  We wish to relax this assumption 
to incorporate possible prior knowledge that certain pairs of
states are  nearby  in some sense and thus more likely than
others to produce large transition weights between them  in
both directions  that is  transitions are likely to be  local  We accomplish this by associating each latent state
  with   location  cid   in some space   introducing    similarity function                  and scaling each
element  jj cid  by  jj cid     cid     cid   cid  For example  we might
wish to de ne    possibly asymmetric  divergence function
              and set  cid     cid      exp   cid     cid   cid 
so that transitions are less likely the farther apart two states
are  By setting       we obtain the standard HDPHMM 
The DILNHMM  Zhu et al    employs   similar
rescaling of transition probabilities via an exponentiated
Gaussian Process  following  Paisley et al    but the
scaling function must be positive semide nite  and in particular symmetric  whereas in the HDPHMM LT    need
only take values in     Moreover  the DILNHMM does
not allow the scales to be tied to other state parameters  and
hence encode an independent notion of similarity 
Letting  cid     cid   cid          we can replace   for       by

 jj cid       cid    Gamma   cid    Tj  

 jj cid jj cid 
  zt   zt     cid     zt zt 
Since the  jj cid  are positive and bounded above by  

 jj cid     jj cid jj cid Tj 

  cid 

            Tj  cid 

 jj cid     

 

 

  cid 

almost surely  where the last inequality carries over from
the original HDP  The prior means of the unnormalized
transition distributions     are then proportional  for each
   to    where                     
The distribution of the latent state sequence   given   and
 cid  is now

 cid 

  cid 
 cid 

  

  

 cid 

  cid 

   

 

        cid   

 zt zt zt ztT

 nzt 
zt 

 

njj cid 
jj cid   

njj cid 
jj cid 

 

 

where njj cid     cid  

     zt       zt     cid  is the number of transitions from state   to state   cid  in the sequence  

  cid  njj cid  is the total number of visits to state
   Since Tj is   sum over products of  jj cid  and  jj cid  terms 
the posterior for   is no longer   DP  However  conditional
conjugacy can be restored by   dataaugmentation process
with   natural interpretation  which is described next 

  The HDPHMM LT as the Marginalization of  
Markov Jump Process with  Failed  Transitions

 ut   Exp cid 

In this section  we de ne   stochastic process that we
call the Markov Jump Process with Failed Transitions
 MJPFT  from which we obtain the HDPHMM LT by
marginalizing over some of the variables  By reinstating
these auxiliary variables  we obtain   simple Gibbs sampling algorithm over the full MJPFT  which can be used
to sample from the marginal posterior of the variables used
by the HDPHMM LT 
Let      cid  and Tj                be de ned as in the
last section  Consider   continuoustime Markov Process
over the states                 and suppose that if the process makes   jump to state zt at time     the next jump 
which is to state zt  occurs at time       ut  where
  cid   jj cid  and   zt      cid    zt         jj cid 
independent of  ut  Note that in this formulation  unlike in
standard formulations of Markov Jump Processes  we are
assuming that selfjumps are possible 
If we only observe the jump sequence   and not the holding times  ut  this is an ordinary Markov chain with transition matrix rowproportional to   If we do not observe
the jumps directly  but instead an observation is generated
once per jump from   distribution that depends on the state
being jumped to  then we have an ordinary HMM whose
transition matrix is obtained by normalizing   that is  we
have the HDPHMM 
We modify this process as follows  Suppose each jump attempt from state   to state   cid  has probability      jj cid  of
failing  in which case no transition occurs and no observation is generated  Assuming independent failures  the rates
of successful and failed jumps from   to   cid  are  jj cid jj cid  and
 jj cid     jj cid  respectively  The probability that the  rst
successful jump is to state   cid   that is  that zt      cid  is
proportional to the rate of successful jump attempts to   cid 
which is  jj cid jj cid  Conditioned on zt  the holding time   ut 
is independent of zt  and is distributed as Exp Tzt  We
  zt    ut 

denote the total time spent in state   by uj  cid 

where  as the sum of        Exponentials 

uj         

ind  Gamma nj  Tj 

 
During this period there will be qjj cid  failed attempts to jump
to state   cid  where qjj cid    Poisson uj jj cid     jj cid  are independent  This data augmentation bears some conceptual
similarity to the Geometrically distributed   auxiliary variables introduced to the HDPHSMM  Johnson   Willsky 

An In nite HMM With SimilarityBiased Transitions

  to restore conditional conjugacy  However  there are
key differences   rst    measure how many steps the chain
would have remained in state   under Markovian dynamics  whereas our   represents putative continuous holding
times between each transition  and second   allows for the
restoration of   zeroed out entry in each row  whereas   allows us to work with unnormalized   entries  avoiding the
need to restore zeroed out entries in the HSMMLT
Incorporating      uj  and      qjj cid  as augmented
data simpli es the likelihood for   yielding

                                      

 

where dependence on  cid  has been omitted for conciseness 
After grouping terms and omitting terms that do not depend
on   this proportional  as   function of   to

 cid 

 cid 

 

  cid 

njj cid   qjj cid 
jj cid 

 

jj cid       jj cid qjj cid    jj cid  uj
njj cid 

 

 

Conveniently  the Tj have canceled  and the exponential
terms involving  jj cid  and  jj cid  in the Gamma and Poisson
distributions of uj and qjj cid  combine to cause  jj cid  to vanish 

  Sticky and SemiMarkov Generalizations

We note that the local transition property of the HDPHMM LT can be combined with the Sticky property of
the Sticky HDPHMM  Fox et al    or the nongeometric duration distributions of the HDPHSMM  Johnson   Willsky    to add additional prior weight on
selftransitions  In the former case  no changes to inference
are needed  one can simply add the the extra mass   to the
shape parameter of the Gamma prior on the  jj  and employ the same auxiliary variable method used by Fox et al 
to distinguish  Sticky  from  regular  selftransitions  For
the semiMarkov case  we can    the diagonal elements
of   to zero  and allow Dt observations to be emitted       
according to   statespeci   duration distribution  and sample the latent state sequence using   suitable semiMarkov
message passing algorithm  Johnson   Willsky    Inference for the   matrix is not affected  since the diagonal elements are assumed to be   Unlike in the original representation of the HDPHSMM  no further dataaugmentation is needed  as the  continuous  durations  
already account for the normalization of the  

  Obtaining the Factorial HMM as   Limiting Case

One setting in which   local transition property is desirable
is the case where the latent states encode multiple hidden
features at time   as   vector of categories  Such problems
are often modeled using factorial HMMs  Ghahramani
et al    In fact  the HDPHMM LT yields the factorial
HMM in the limit as          xing each row of   to be

uniform with probability   so the dynamics are controlled
entirely by   If      is the transition matrix for chain   
then setting  cid     cid   cid    exp   cid     cid   cid  with asymmetric
  yields the

 divergences    cid     cid   cid     cid 

  log     

 cid jd cid   cid   

factorial transition model 

  An In nite Factorial HDPHMM LT

Nonparametric extensions of the factorial HMM  such as
the in nite factorial hidden Markov Model  Gael et al 
  and the in nite factorial dynamic model  Valera
et al    have been developed in recent years by making use of the Indian Buffet Process  Ghahramani   Grif 
 ths    as   state prior 
It would be conceptually
straightforward to combine the IBP state prior with the similarity bias of the LT model  provided the chosen similarity
function is uniformly bounded above on the space of in 
 nite length binary vectors  for example  take        to
be the exponentiated negative Hamming distance between
  and    Since the number of differences between two
draws from the IBP is  nite with probability   this yields
  reasonable similarity metric 

  Inference
We develop   Gibbs sampling algorithm based on the MJPFT representation described in Sec    augmenting the
data with the duration variables    the failed jump attempt
count matrix     as well as additional auxiliary variables
which we will de ne below  In this representation the transition matrix is not represented directly  but is   deterministic function of the unscaled transition  rate  matrix    and
the similarity matrix    The full set of variables is partitioned into blocks                         cid  and
  where   represents   set of auxiliary variables that will
be introduced below    represents the emission parameters
 which may be further blocked depending on the speci  
choice of model  and   represents additional parameters
such as any free parameters of the similarity function   
and any hyperparameters of the emission distribution 

  Sampling Transition Parameters and

Hyperparameters

The joint posterior over       and   given the augmented
data                 will factor as

          

                              

 

We describe these four factors in reverse order 

Sampling   Having used data augmentation to simplify the likelihood for   to the factored conjugate form
in   the individual  jj cid  are   posteriori independent

Gamma   cid    njj cid    qjj cid      uj  distributed 

  Sampling   and the auxiliary variables

An In nite HMM With SimilarityBiased Transitions

Sampling   To enable joint sampling of    we employ
  weak limit approximation to the HDP  Johnson   Willsky    approximating the stickbreaking process for  
using    nite Dirichlet distribution with     components 
where   is larger than we expect to need  Due to the
productof Gammas form  we can integrate out   analytically to obtain the marginal likelihood 
        

    

 

 

   

 cid 
    uj cid 

 

 

 

  cid 

 

              cid 

  

   cid    njj cid    qjj cid 

   cid 

where we have used the fact that the    sum to   to pull
out terms of the form     uj   cid  from the inner product in the likelihood  Following Teh et al    we can
introduce auxiliary variables      mjj cid  with
ind  snjj cid   qjj cid   mjj cid   mjj cid   

  mjj cid       cid     

mjj cid 
  cid 

 

    cid     cid 

for integer mjj cid  ranging between   and njj cid    qjj cid  where
sn   is an unsigned Stirling number of the  rst kind  The
normalizing constant in this distribution cancels the ratio of Gamma functions in the   likelihood  so  letting
  cid      cid  the posterior for
 the truncated    is   Dirichlet whose jth mass parameter
is  

  mjj cid  and       cid 

        

Sampling Concentration Parameters
into    we can integrate out   to obtain
                  cid 
  cid cid  log uj cid cid   

Incorporating  

 

      

   

        
   
   

 

Assuming that   and   have Gamma priors with shape and
rate parameters       and       then
     Gamma             

log    uj 

 

 cid 
 cid 

 

 

To simplify the likelihood for   we can introduce    
nal set of auxiliary variables                  rJ   rj cid   
              cid  and         with the following distributions 

  rj cid            cid            cid    
                    

 

 

 

 cid   

 cid  

The normalizing constants are ratios of Gamma functions 
which cancel those in   so that

             Gamma              log   

 

We sample the hidden state sequence     jointly with the
auxiliary variables  which consist of            and   
The joint conditional distribution of these variables is de 
 ned directly by the generative model 
                                                  

Since we are conditioning on the transition matrix  we
can sample the entire sequence   jointly with the forwardbackward algorithm  as in an ordinary HMM  Since we are
sampling the labels jointly  this step requires         computation per iteration  which is the bottleneck of the inference algorithm for reasonably large   or    other updates
are constant in   or in    Having done this  we can sample
           and   from their forward distributions  It is also
possible to employ   variant on beam sampling  Van Gael
et al    to speed up each iteration  at the cost of slower
mixing  but we did not use this variant here 

  Sampling state and emission parameters

Depending on the application  the locations  cid  may or may
not depend on the emission parameters    If not  sampling
  conditional on   is unchanged from the HDPHMM 
There is no generalpurpose method for sampling  cid  or for
sampling   in the dependent case  due to the dependence
on the form of   and on the emission model  but speci  
instances are illustrated in the experiments below 

  Experiments
The parameter space for the hidden states  the associated
prior   on   and the similarity function   is applicationspeci    we consider here two cases  The  rst is   speakerdiarization task  where each state consists of    nite Ddimensional binary vector whose entries indicate which
speakers are currently speaking 
In this experiment  the
state vectors both determine the pairwise similarities and
partially determine the emission distributions via   linearGaussian model  In the second experiment  the data consists of Bach chorales  and the latent states can be thought
of as harmonic contexts  There  the components of the
states that govern similarities are modeled as independent
of the emission distributions  which are categorical distributions over fourvoice chords 

  Cocktail Party
The Data The data was constructed using audio signals
collected from the PASCAL  st Speech Separation Challenge  The underlying signal consisted of       speaker
channels recorded at each of       time steps  with the

 http laslab org SpeechSeparationChallenge 

An In nite HMM With SimilarityBiased Transitions

with only cid 

resulting       signal matrix  denoted by  
  mapped to
      microphone channels via   weight matrix     The
  speakers were grouped into   conversational groups of
  where speakers within   conversation took turns speaking  see Fig    In such   task  there are naively    possible states  here    However  due to the conversational grouping  if at most one speaker in   conversation is
speaking at any given time  the state space is constrained 
  sc     states possible  where sc is the number of speakers in conversation    in this case sc     for  
total of   possible states 
Each  turn  within   conversation consisted of   single
sentence  average duration       and turn orders within
  conversation were randomly generated  with random
pauses distributed as           inserted between
sentences  Every time   speaker has   turn  the sentence
is drawn randomly from the   sentences uttered by that
speaker in the data  The conversations continued for    
and the signal was downsampled to length   The  on 
portions of each speaker   signal were normalized to have
amplitudes with mean   and standard deviation   An additional column of    was added to the speaker signal ma 
  representing background noise  The resulting sigtrix   
  was thus       and the weight
nal matrix  denoted  
matrix     was       Following Gael et al    and
Valera et al    the weights were drawn independently
from   Unif    distribution  and independent      
noise was added to each entry of the observation matrix 

The Model The latent states      are the Ddimensional
binary vectors whose dth entry indicates whether or not
speaker   is speaking  The locations  cid   are identi ed with
the binary vectors   cid         We use   Laplacian similarity function on Hamming distance     so that  jj cid   
exp   cid     cid   cid        The emission model is linearGaussian as in the data  with            weight matrix
   and            signal matrix  
  whose tth row is
        zt   so that yt          WT 
 
      For the experiments discussed here  we assume that   is independent
of    but this assumption is easily relaxed if appropriate 
For  nitelength binary vector states  the set of possible
states is  nite  and so it may seem that   nonparametric
model is unnecessary  However  if   is reasonably large 
likely most of the    possible states are vanishingly unlikely  and the number of observations may well be less
than     and so we would like to encourage the selection
of   sparse set of states  Moreover  there could be more
than one state with the same emission parameters  but with
different transition dynamics  Next we describe the additional inference steps needed for this version of the model 

Sampling      cid  Since    and  cid   are identi ed  in uencing
both the transition matrix and the emission distributions 

both the state sequence   and the observation matrix   are
used in the update  We put independent BetaBernoulli priors on each coordinate of   and Gibbs sample each coordinate  jd conditioned on all the others and the coordinatewise prior means      which we sample in turn conditioned on   Details are in the supplement 

Sampling   The   parameter of the similarity function
governs the connection between  cid  and   Substituting the
de nition of   into   yields

         cid     cid 

 cid 

  djj cid  njj cid        djj cid   qjj cid 

 

 

  cid 

We put an Exp    prior on   which yields   posterior
density

 cid 
          cid        cid 
 cid 
 cid 
  cid  djj cid  njj cid   
      djj cid   qjj cid 

 

 

 

  cid 

This density is logconcave  and so we use Adaptive Rejection Sampling  Gilks   Wild    to sample from it 

    and
Sampling   and   Conditioned on   and  
  can be sampled as in Bayesian linear regression 
If
each column of   has   multivariate Normal prior  then
the columns are   posteriori independent multivariate Normals  For the experiments reported here  we      to its
  can be compared directly with
ground truth value so that  
the ground truth signal matrix  and we constrain   to be
diagonal  with Inverse Gamma priors on the variances  resulting in conjugate updates 

Results We attempted to infer the binary speaker matrices using  ve models      binarystate Factorial HMM
 Ghahramani et al    where individual binary speaker
sequences are modeled as independent    an ordinary
HDPHMM without local transitions  Teh et al   
where the latent states are binary vectors      Sticky HDPHMM  Fox et al      our HDPHMM LT model 
and     model that combines the Sticky and LT properties  For all models  all concentration and noise precision parameters are given Gamma    priors  For the
  is given   Unif    prior 
Sticky models  the ratio
We evaluated the models at each iteration using both the
Hamming distance between inferred and ground truth state
matrices and    score  We also plot the inferred decay rate
  and the number of states used by the LT and StickyLT
models  The results for the  ve models are in Fig    In

 

 We attempted to add   comparison to the DILNHMM  Zhu
et al    as well  but code could not be obtained  and the paper
did not provide enough detail to reproduce their inference algorithm 

An In nite HMM With SimilarityBiased Transitions

Figure   Top     score for inferred relative to ground truth binary speaker matrices on cocktail party data  evaluated every  th
Gibbs iteration after the  rst   aggregating across   runs of
each model  Middle  Inferred   for the LT and StickyLT models by Gibbs iteration  averaged over   runs  Bottom  Number of
states used     by each model in the training set  Error bands are
  con dence interval of the mean per iteration 

Fig    we plot the ground truth state matrix against the average state matrix    averaged over runs and postburn in
iterations 
The LT and StickyLT models outperform the others  while
the regular Sticky model exhibits only   small advantage
over the vanilla HDPHMM  Both converge on   nonnegligible   value of about    see Fig    suggesting that
the local transition structure explains the data well  The
LT models also use more states than the nonLT models 
perhaps owing to the fact that the weaker transition prior
of the nonLT model is more likely to explain nearby similar observations as   single persisting state  whereas the LT
model places   higher probability on transitioning to   new
state with   similar latent vector 

  Synthetic Data Without Local Transitions

We generated data directly from the ordinary HDPHMM
used in the cocktail experiment as   sanity check  to examine the performance of the LT model in the absence of  
similarity bias  The results are in Fig    When the   parameter is large  the LT model has worse performance than
the nonLT model on this data  however  the   parameter
settles near zero as the model learns that local transitions
are not more probable  When       the HDPHMM LT is
an ordinary HDPHMM  The LT model does not make entirely the same inferences as the nonLT model  however  in
particular  the   concentration parameter is larger  To some
extent    and   trade off  sparsity of the transition matrix
can be achieved either by beginning with   sparse rate matrix prior to rescaling   small  or by beginning with   less
sparse rate matrix which becomes sparser through rescaling

Figure   Binary speaker matrices for the cocktail data  with time
on the horizontal axis and speaker on the vertical axis  White
is   black is   The ground truth matrix is at the top  followed by the inferred speaker matrix for the Sticky HDPHMM 
LT  HDPHMM LT  binary factorial  StickyHDP HMM  and
 vanilla  HDPHMM  All inferred matrices are averaged over  
runs of   Gibbs iterations each  with the  rst   iterations
discarded as burnin 

 larger   and nonzero  

  Bach Chorales

To test   version of the HDPHMM LT model in which the
components of the latent state governing similarity are unrelated to the emission distributions  we used our model
to do unsupervised  grammar  learning from   corpus of
Bach chorales  The data was   corpus of   fourvoice
major key chorales by      Bach from music    of
which were randomly selected as   training set  with the
other   used as   test set to evaluate surprisal  marginal
log likelihood per observation  by the trained models  All
chorales were transposed to Cmajor  and each distinct
fourvoice chord  with voices ordered  was encoded as  
single integer  In total there were   distinct chord types
and   chord tokens in the   chorales  with  
types and   tokens in the   training chorales  and
  chord types that were unique to the test set 

Modi cations to Model and Inference Since the chords
were encoded as integers  the emission distribution for each
state is Cat    We use   symmetric Dirichlet prior for
each     resulting in conjugate updates to   conditioned on
the latent state sequence    
In this experiment  the locations   cid    are independent of the
    with        priors  We use   Gaussian similarity function   jj   exp   cid     cid   cid  where    is Euclidean
distance  Since the latent states are continuous  we use

 http web mit edu music 

   scoredensitymodelBFactLTnoLTStickyStickyLTllllllllllllllllllllllll IterationslambdamodellLTStickyLT   dotdensitymodelLTnoLTStickyStickyLTGroundtruthStickyLTLTBFactStickynoLTAn In nite HMM With SimilarityBiased Transitions

Figure   Top     score for inferred relative to ground truth binary speaker matrices on synthetic data generated from the vanilla
HDPHMM model  Middle  Learned similarity parameter    for
the LT model by Gibbs iteration  averaged over   runs  Bottom 
Number of states used     by each model in the training set  Error
bands are   con dence interval of the mean per iteration  The
 rst   iterations are omitted 

  Hamiltonian Monte Carlo  HMC  update  Duane et al 
  Neal et al    to update the  cid   simultaneously 
conditioned on   and    see the supplement for details 

Results We ran   Gibbs chains for   iterations each
using the HDPHMM LT  StickyHDP HMMLT  HDPHMM and StickyHDP HMM models on the   training
chorales  which were modeled as conditionally independent of one another  We evaluated the marginal log likelihood on the   test chorales  integrating out    at every
 th iteration  The training and test log likelihoods are in
Fig    Although the LT model does not achieve as close
     to the training data  its generalization performance is
better  suggesting that the vanilla HDPHMM is over tting  This is perhaps counterintuitive  since the LT model
is more  exible  and might be expected to be more prone
to over tting  However  the similarity bias induces greater
information sharing across parameters  as in   hierarchical
model  instead of each entry of the transition matrix being informed mainly by transitions directly involving the
corresponding states  it is informed to some extent by all
transitions  as they all inform the similarity structure 

  Discussion
We have de ned   new probabilistic model  the Hierarchical Dirichlet Process Hidden Markov Model with Local Transitions  HDPHMM LT  which generalizes the
HDPHMM by allowing state space geometry to be represented via   similarity kernel  making transitions between
 nearby  pairs of states  local  transitions  more likely  
priori  By introducing an augmented data representation 

Figure   Training set and test set log marginal likelihoods for
Bach chorale data on the four HDPbased models  HDPHMM 
LT  HDPHMM  Sticky HMM  and Sticky HDPHMM LT 

which we call the Markov Jump Process with Failed Transitions  MJPFT  we obtain   Gibbs sampling algorithm
that simpli es inference in both the LT and ordinary HDPHMM  When multiple latent chains are interdependent  as
in speaker diarization  the HDPHMM LT model combines
the HDPHMM   capacity to discover   small set of joint
states with the Factorial HMM   ability to encode the property that most transitions involve   small number of chains 
The HDPHMM LT outperforms both  as well as outperforming the StickyHDP HMM  on   speaker diarization
task in which speakers form conversational groups  Despite
the addition of the similarity kernel  the HDPHMM LT is
able to suppress its local transition prior when the data does
not support it  achieving identical performance to the HDPHMM on data generated directly from the latter 
The local transition property is particularly clear when
transitions occur at different times for different latent features  as with binary vectorvalued states in the cocktail
party setting  but the model can be used with any state
space equipped with   suitable similarity kernel  Similarities need not be de ned in terms of emission parameters 
state  locations  can be represented and inferred separately 
which we demonstrate using Bach chorale data  There 
the LT model achieves better predictive performance on  
heldout test set  while the ordinary HDPHMM over ts
the training set  the LT property here acts to encourage  
concise harmonic representation where chord contexts are
arranged in bidirectional functional relationships 
We focused on  xeddimension binary vectors for the cocktail party and synthetic data experiments  but it would be
straightforward to add the LT property to   model with nonparametric latent states  such as the iFHMM  Gael et al 
  and the in nite factorial dynamic model  Valera
et al    both of which use the Indian Buffet Process
 IBP   Ghahramani   Grif ths    as   state prior  The
similarity function used here could be employed without
changes  since only  nitely many coordinates are nonzero
in the IBP  the distance between any two states is  nite 

llllllllllllllllllll IterationsF scoremodellBFactLTnoLTllllllllllllllllllll IterationslambdamodellLT   dotdensitymodelLTnoLT train log likelihooddensitymodelLTnoLTStickyStickyLT test log likelihooddensitymodelLTnoLTStickyStickyLTAn In nite HMM With SimilarityBiased Transitions

Johnson  Matthew   and Willsky  Alan    Bayesian nonparametric hidden semiMarkov models  The Journal of
Machine Learning Research     

Kingman  John  Completely random measures  Paci  

Journal of Mathematics     

Neal  Radford   et al  MCMC using Hamiltonian dynamics  Handbook of Markov Chain Monte Carlo   
   

Paisley  John  Wang  Chong  and Blei  David    The discrete in nite logistic normal distribution  Bayesian Analysis     

Ranganath  Rajesh and Blei  David    Correlated random
measures  Journal of the American Statistical Association   

Sethuraman  Jayaram    constructive de nition of Dirich 

let processes  Statistica Sinica     

Teh  Yee Whye  Jordan  Michael    Beal  Matthew    and
Blei  David    Hierarchical Dirichlet processes  Journal
of the American Statistical Association     

Valera  Isabel  Ruiz  Francisco  Svensson  Lennart  and
PerezCruz  Fernando 
In nite factorial dynamical
model  In Advances in Neural Information Processing
Systems  pp     

Van Gael  Jurgen  Saatci  Yunus  Teh  Yee Whye  and
Ghahramani  Zoubin  Beam sampling for the in nite
In Proceedings of the  th Inhidden Markov model 
ternational Conference on Machine Learning  pp   
  ACM   

Zhu  Hao  Hu  Jinsong  and Leung  Henry  Hidden Markov
models with discrete in nite logistic normal distribution
priors  In Information Fusion  FUSION     th International Conference on  pp    IEEE   

ACKNOWLEDGMENTS

This work was funded in part by DARPA grant   NF 
  under the Big Mechanism Program and DARPA
grant   NF  under the Communicating with
Computers Program 

References
Beal  Matthew    Ghahramani  Zoubin  and Rasmussen 
Carl    The in nite hidden Markov model  In Advances
in neural information processing systems  pp   
 

Duane  Simon  Kennedy  Anthony    Pendleton  Brian   
and Roweth  Duncan  Hybrid monte carlo  Physics letters       

Ewens  Warren John  Population genetics theory   the past
and the future  In Mathematical and Statistical Developments of Evolutionary Theory  pp    Springer 
 

Favaro  Stefano  Teh  Yee Whye  et al  MCMC for normalized random measure mixture models  Statistical Science     

Ferguson  Thomas      Bayesian analysis of some nonparametric problems  The annals of statistics  pp   
   

Fox  Emily    Sudderth  Erik    Jordan  Michael    and
Willsky  Alan    An HDPHMM for systems with state
In Proceedings of the  th international
persistence 
conference on Machine learning  pp    ACM 
 

Gael  Jurgen    Teh  Yee    and Ghahramani  Zoubin  The
in nite factorial hidden Markov model  In Advances in
Neural Information Processing Systems  pp   
 

Ghahramani  Zoubin and Grif ths  Thomas    In nite latent feature models and the Indian buffet process 
In
Advances in neural information processing systems  pp 
   

Ghahramani  Zoubin  Jordan  Michael    and Smyth 
Padhraic  Factorial hidden Markov models  Machine
learning     

Gilks  Walter   and Wild  Pascal  Adaptive rejection sampling for Gibbs sampling  Applied Statistics  pp   
   

Ishwaran  Hemant and Zarepour  Mahmoud  Exact and approximate sum representations for the Dirichlet process 
Canadian Journal of Statistics     

