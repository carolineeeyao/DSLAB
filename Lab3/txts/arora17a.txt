Generalization and Equilibrium in Generative Adversarial Nets  GANs 

Sanjeev Arora   Rong Ge   Yingyu Liang   Tengyu Ma   Yi Zhang  

Abstract

It is shown that training of generative adversarial network  GAN  may not have good generalization properties       training may appear
successful but the trained distribution may be
far from target distribution in standard metrics 
However  generalization does occur for   weaker
metric called neural net distance  It is also shown
that an approximate pure equilibrium exists in the
discriminator generator game for   natural training objective  Wasserstein  when generator capacity and training set sizes are moderate  This
existence of equilibrium inspires MIX GAN protocol  which can be combined with any existing
GAN training  and empirically shown to improve
some of them 

  Introduction
Generative Adversarial Networks  GANs   Goodfellow
et al    have become one of the dominant methods for
 tting generative models to complicated reallife data  and
even found unusual uses such as designing good cryptographic primitives  Abadi   Andersen    See   survey
by  Goodfellow    Various novel architectures and
training objectives were introduced to address perceived
shortcomings of the original idea  leading to more stable
training and more realistic generative models in practice
 see
 Odena et al    Huang et al    Radford
et al    Tolstikhin et al    Salimans et al   
Jiwoong Im et al    Durugkar et al    and the reference therein 
The goal is to train   generator deep net whose input is
  standard Gaussian  and whose output is   sample from
some distribution   on Rd  which has to be close to some
target distribution Dreal  which could be  say  reallife im 
 Princeton UniverAuthors listed in alphabetical order 
sity  Princeton NJ  Duke University  Durham NC  Correspondence to  Rong Ge  rongge cs duke edu  Yi Zhang
   zhang cs princeton edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Figure   Probability density Dreal with many peaks and valleys
ages represented using raw pixels  The training uses samples from Dreal and together with the generator net also
trains   discriminator deep net trying to maximise its ability
to distinguish between samples from Dreal and    So long
as the discriminator is successful at this task with nonzero
probability  its success can be used to generate   feedback
 using backpropagation  to the generator  thus improving
its distribution    Training is continued until the generator
wins  meaning that the discriminator can do no better than
random guessing when deciding whether or not   particular
sample came from   or Dreal  This basic iterative framework has been tried with many training objectives  see Section   But it has been unclear what to conclude when the
generator wins this game  is   close to Dreal in some metric  One seems to need some extension of generalization
theory that would imply such   conclusion  The hurdle is
that distribution Dreal could be complicated and may have
many peaks and valleys  see Figure   The number of peaks
 modes  may even be exponential in     Recall the curse of
dimensionality  in   dimensions there are exp    directions
whose pairwise angle exceeds say   and each could be
the site of   peak  Whereas the number of samples from
Dreal  and from   for that matter  used in the training is
  lot fewer  and thus may not re ect most of the peaks and
valleys of Dreal 
  standard analysis due to  Goodfellow et al    shows
that when the discriminator capacity   number of parameters  and number of samples is  large enough  then   win
by the generator implies that   is very close to Dreal  see
Section   But the discussion in the previous paragraph
raises the possibility that  suf ciently large  in this analysis may need to be exp   
Another open theoretical issue is whether an equilibrium
always exists in this game between generator and discrim 

Generalization and Equilibrium in Generative Adversarial Nets  GANs 

inator  Just as   zero gradient is   necessary condition for
standard optimization to halt  the corresponding necessary
condition in   twoplayer game is an equilibrium  Conceivably some of the instability often observed while training GANs could just arise because of lack of equilibrium 
 Recently Arjovsky et al    suggest that using their
Wasserstein objective in practice reduces instability  but
we still lack proof of existence of an equilibrium  Standard game theory is of no help here because we need   socalled pure equilibrium  and simple counterexamples such
as rock paper scissors show that it doesn   exist in general 

  Our Contributions
We formally de ne generalization for GANs in Section  
and show that for previously studied notions of distance
between distributions  generalization is not guaranteed
 Lemma  
In fact we show that the generator can win
even when   and Dreal are arbitrarily far in any standard
metric 
However  we can guarantee some weaker notion of generalization by introducing   new metric on distributions  the
neural net distance  We show that generalization does happen with moderate number of training examples       when
the generator wins  the two distributions must be close in
neural net distance  However  this weaker metric comes at
  cost  it can be nearzero even when the trained and target
distributions are very far  Section  
To explore the existence of equilibria we turn in Section  
to in nite mixtures of generator deep nets  These are
clearly vastly more expressive than   single generator net 
       standard result in bayesian nonparametrics says that
every probability density is closely approximable by an in 
 nite mixture of Gaussians  Ghosh et al    Thus unsurprisingly  an in nite mixture should win the game  We
then prove rigorously that even    nite mixture of fairly
reasonable size can closely approximate the performance
of the in nite mixture  Theorem  
This insight also allows us to show for   natural GAN setting with Wasserstein objective there exists an approximate
equilibrium that is pure   Roughly speaking  an approximate equilibrium is one in which neither of the players can
gain much by deviating from their strategies 
This existence proof for an approximate equilibrium unfortunately involves   quadratic blowup in the  size  of the
generator  which is still better than the naive exponential
blowup one might expect  Improving this is left for future
theoretical work  But we propose   heuristic approximation to the mixture idea to introduce   new framework for
 Such counterexamples are easily turned into toy GAN scenarios with generator and discriminator having  nite capacity  and
the game lacks   pure equilibrium  See supplementary material 

training that we call MIX GAN  It can be added on top of
any existing GAN training procedure  including those that
use divergence objectives  Experiments in Section   show
that for several previous techniques  MIX GAN stabilizes
the training  and in some cases improves the performance 

  Preliminaries
Notations  Throughout the paper we use   for the dimension of samples  and   for the number of parameters in the
generator discriminator  In Section   we use   for number
of samples 

Generators and discriminators  Let  Gu       
    Rp  denote the class of generators  where Gu is  
function   which is often   neural network in practice  
from      Rd indexed by   that denotes the parameters
of the generators  Here   denotes the possible ranges
of the parameters and without
loss of generality we
assume   is   subset of the unit ball  The generator Gu
de nes   distribution DGu as follows  generate   from
 dimensional spherical Gaussian distribution and then
apply Gu on   and generate   sample     Gu    of the
distribution DGu  We drop the subscript   in DGu when
it   clear from context 
Let  Dv        denote the class of discriminators  where
Dv is function from Rd to     and   is the parameters of
Dv  The value Dv    is usually interpreted as the probability that the sample   comes from the real distribution Dreal
 as opposed to the generated distribution DG 
We assume Gu and Dv are LLipschitz with respect to their
parameters  That is  for all          and any input    we
have kGu      Gu       Lku        similar for   
Notice  this is distinct from the assumption  which we
will also sometimes make  that functions Gu  Dv are Lipschitz  that focuses on the change in function value when
we change    while keeping       xed 
Objective functions  The standard GAN training  Goodfellow et al    consists of training parameters      so
as to optimize an objective function 

  DGu

 

  Dreal

max
   

 log Dv       

 log    Dv   
min
   
 
Intuitively  this says that the discriminator Dv should give
high values Dv    to the real samples and low values
Dv    to the generated examples  The log function was
suggested because of its interpretation as the likelihood 

 Otherwise we can scale the parameter properly by changing

the parameterization 

 Both Lipschitz parameters can be exponential in the number
of layers in the neural net  however our Theorems only depend on
the log of the Lipschitz parameters

Generalization and Equilibrium in Generative Adversarial Nets  GANs 

max
   

 

  Dreal

  DGu

 

mPm

 Dv     

and it also has   nice informationtheoretic interpretation
described below  However  in practice it can cause problems since log       as       The objective still
makes intuitive sense if we replace log by any monotone
function              which yields the objective 
 Dv     
min
   
We call function   the measuring function  It should be
concave so that when Dreal and DG are the same distribution  the best strategy for the discriminator is just to output
  and the optimal value is   In later proofs  we
will require   to be bounded and Lipschitz  Indeed  in practice training often uses       log             which
takes values in  log     and is  Lipschitz  and the recently proposed Wasserstein GAN  Arjovsky et al   
objective uses         
Training with  nite samples 
The objective function   assumes we have in nite number of samples
from Dreal
to estimate the value Ex Dreal Dv   
With  nite training examples            xm    real 
one uses
  Dv xi  to estimate the quantity
Ex Dreal Dv    We call the distribution that gives
   probability to each of the xi   the empirical version
of the real distribution  Similarly  one can use   empirical
version to estimate Ex DGu     Dv   
Standard interpretation via distance between distributions  Towards analyzing GANs  researchers have assumed
access to in nite number of examples and that the discriminator is chosen optimally within some large class of functions that contain all possible neural nets  This often allows computing analytically the optimal discriminator and
therefore removing the maximum operation from the objective   which leads to some interpretation of how and
in what sense the resulting distribution DG is close to the
true distribution Dreal 
Using the original objective function   then the optimal
choice among all the possible functions from Rd      
is       
Preal   PG    as shown in  Goodfellow et al 
  Here Preal    is the density of   in the real distribution  and PG    is the density of   in the distribution generated by generator    Using this discriminator  
though it   computationally infeasible to obtain it   one
can show that the minimization problem over the generator correspond to minimizing the JensenShannon  JS  divergence between the true distribution Dreal and the generative distribution DG  Recall that for two distributions
  and   the JS divergence is de ned by dJS     
   KL    
 
Other measuring functions   and choice of discriminator class leads to different distance function between
distribution other than JS divergence  Notably 
 Ar 

      KL    

Preal   

   

the discriminator 

jovsky et al    shows that when          and
the discriminator is chosen among all  Lipschitz functions  maxing out
the generator is
attempting to minimize the Wasserstein distance between Dreal and Du    Recall that Wasserstein distance between   and   is de ned as dW      
supD is  Lipschitz  Ex        Ex       
  Generalization Theory for GANs
The above interpretation of GANs in terms of minimizing
distance  such as JS divergence and Wasserstein distance 
between the real distribution and the generated distribution
relies on two crucial assumptions      very expressive class
of discriminators such as the set of all bounded discriminator or the set of all  Lipschitz discriminators  and  ii  very
large number of examples to compute estimate the objective   or   Neither assumption holds in practice  and
we will show next that this greatly affects the generalization ability    notion we introduce in Section  

  De nition of Generalization
Our de nition is motivated from supervised classi cation 
where training is said to generalize if the training and test
error closely track each other   Since the purpose of GANs
training is to learn   distribution  one could also consider
  stronger de nition of successful training  as discussed in
Section  
Let            xm be the training examples  and let  Dreal
denote the uniform distribution over            xm  Similarly  let Gu            Gu hr  be   set of   examples from
In the training of GANs 
the generated distribution DG 
one implicitly uses Ex   Dreal
 Dv    to approximate
the quantity Ex Dreal Dv    Inspired by the observation that the training objective of GANs and its variants is
to minimize some distance  or divergence     between
Dreal and DG using  nite samples  we de ne the generalization of GANs as follows 
De nition     divergence or distance    between distributions generalizes with   training examples and error
  if for the learnt distribution DG  the following holds with
high probability 

   Dreal DG        Dreal   DG     

where  Dreal is an empirical version of the true distribution
 with   samples  and  DG is an empirical version of the
generated distribution with polynomial number of samples 

 

In words  generalization in GANs means that the population distance between the true and generated distribution is
close to the empirical distance between the empirical distributions  Our target is to make the former distance small 

Generalization and Equilibrium in Generative Adversarial Nets  GANs 

whereas the latter one is what we can access and minimize
in practice  The de nition allows only polynomial number of samples from the generated distribution because the
training algorithm should run in polynomial time 

  JS Divergence and Wasserstein don   Generalize
As   warmup  we show that JS divergence and Wasserstein
distance don   generalize with any polynomial number of
examples because the population distance  divergence  is
not re ected by the empirical distance 
Lemma   Let   be uniform Gaussian distributions
     and   be an empirical versions of   with   exN    
amples  Then we have dJS      log   dW        
There are two consequences of Lemma   First  consider
the situation where Dreal   DG     Then we have that
dW  Dreal DG      but dW    Dreal   DG      as long as
we have polynomial number of examples  This violates the
generalization de nition equation  
Second  consider the case Dreal     and DG    Dreal  
  that is  DG memorizes all of the training examples in
 Dreal  In this case  since DG is   discrete distribution with
 nite supports  with enough  polynomial  examples  in  DG 
effectively we also have that  DG       Therefore  we
have that dW    Dreal   DG      whereas dW  Dreal DG   
  In other words  with any polynomial number of examples  it   possible to over   to the training examples using
Wasserstein distance  The same argument also applies to
JS divergence  See supplementary material for the formal
proof 
Notice 
this result does not contradict the experiments
of  Arjovsky et al    since they actually use not
Wasserstein distance but   surrogate distance that does generalize  as we show next 

  Generalization Bounds for Neural Net Distance
Which distance measure between Dreal and DG is the
GAN objective actually minimizing and can we analyze
its generalization performance  Towards answering these
questions in full generality  given multiple GANs objectives  we consider the following general distance measure
that uni es JS divergence  Wasserstein distance  and the
neural net distance that we de ne later in this section 
De nition    Fdistance  Let   be   class of functions
from Rd to     and   be   concave measuring function 
Then the Fdivergence with respect to   between two distributions   and   supported on Rd is de ned as

dF      sup

        Ex 

     Ex 

   

        

When          we have that dF  is   distance function  
  and with slightly abuse of notation we write it simply as

dF       sup

        Ex 

     Ex 

       

Example   When       log    and    
 all functions from Rd to     we have that dF  is the
same as JS divergence  When         and    
 all  Lipschitz functions from Rd to     then dF  is
the Wasserstein distance 
Example   Suppose   is   set of neural networks and
      log    then original GAN objective function is
equivalent to minG dF   Dreal   DG   
Suppose   is the set of neural networks  and          then
the objective function used empirically in  Arjovsky et al 
  is equivalent to minG dF    Dreal   DG   
GANs training uses   to be   class of neural nets with  
bound   on the number of parameters  We then informally
refer to dF as the neural net distance  The next theorem
establishes generalization in the sense of equation   does
hold for it  with   uniform convergence    We assume that
the measuring function takes values in     and that it
is   Lipschitz  Further       Dv        is the class
of discriminators that is LLipschitz with respect to the parameters    As usual  we use   to denote the number of
parameters in   
Theorem   In the setting of previous paragraph  let    
be two distributions and     be empirical versions with at
least   samples each  There is   universal constant   such
that when     cp  log LL   
  we have with probability
at least     exp    over the randomness of   and  

 

 dF      dF     

See supplementary material for the proof  The intuition is
that there aren   too many distinct discriminators  and thus
given enough samples the expectation over the empirical
distribution converges to the expectation over the true distribution for all discriminators 
Theorem   shows that the neural network divergence  and
neural network distance  has   much better generalization
properties than JensenShannon divergence or Wasserstein
distance 
If the GAN successfully minimized the neural
network divergence between the empirical distributions 
that is      Dreal   DG  then we know the neural network
divergence   Dreal DG  between the distributions Dreal
and DG is also small  It is possible to change the proof to
also show that this generalization continues to hold at every
iteration of the training  See supplementary material 

 Technically it is   pseudometric  This is also known as inte 

gral probability metrics   uller   

Generalization and Equilibrium in Generative Adversarial Nets  GANs 

  Generalization vs Diversity
Since the  nal goal of GANs training is to learn   distribution  it is worth understanding that though weak generalization in the sense of Section   is guaranteed  it comes
with   cost  albeit   necessary one  For JS divergence and
Wasserstein distance  when the distance between two distributions     is small  it is safe to conclude that the distributions   and   are almost the same  However  the neural
net distance dN       can be small even if     are not
very close  As   simple Corollary of Lemma   we obtain 
Corollary    Lowcapacity discriminators cannot detect
lack of diversity  Let   be the empirical version of distribution   with   samples  There is   some universal constant   such that when     cp  log LL   
  we have that
with probability at least     exp    dF       
That is  the neural network distance for nets with   parameters cannot distinguish between   distribution   and   distribution with support       In fact the proof still works
if the disriminator is allowed to take many more samples
from   the reason they don   help is that its capacity is
limited to   

 

  Expressive Power and Equilibrium
Section   clari ed the notion of generalization for GANs 
namely  neuralnet divergence between the generated distribution   and Dreal on the empirical samples closely
tracks the divergence on the full distribution       unseen
samples  But this doesn   explain why in practice the generator usually  wins  so that the discriminator is unable to
do much better than random guessing at the end  In other
words  was it sheer luck that so many reallife distributions
Dreal turned out to be close in neuralnet distance to   distribution produced by   fairly compact neural net  This
section suggests no luck may be needed 
The explanation starts with   thought experiment  Imagine allowing   much more powerful generator  namely  an
in nite mixture of deep nets  each of size    So long as
the deep net class is capable of generating simple gaussians  such mixtures are quite powerful  since   classical
result says that an in nite mixtures of simple gaussians can
closely approximate Dreal  Thus an in nite mixture of
deep net generators will  win  the GAN game  not only
against   discriminator that is   small deep net but also
against more powerful discriminators       any Lipschitz
function 
The next stage in the thought experiment is to imagine  
much less powerful generator  which is   mix of only   few
deep nets  not in nitely many  Simple counterexamples
show that now the distribution   will not closely approxi 

mate arbitrary Dreal with respect to natural metrics like    
Nevertheless  could the generator still win the GAN game
against   deep net of bounded capacity       the deep net is
unable to distinguish   and Dreal  We show it can 
INFORMAL THEOREM  If the discriminator is   deep net
with   parameters  then   mixture of      log    generator nets can produce   distribution   that the discriminator will be unable to distinguish from Dreal with probability more than    Here     notation hides some nuisance factors 
This informal theorem is also   component of our result below about the existence of an approximate pure
equilibrium  With current technique this existence result
seems sensitive to the measuring function   and works for
              Wasserstein GAN  For other   we only
show existence of mixed equilibria with small mixtures 

  General   Mixed Equilibrium
For general measuring function   we can only show the
existence of   mixed equilibrium  where we allow the discriminator and generator to be  nite mixtures of deep nets 
For   class of generators  Gu        and   class of discriminators  Dv          we can de ne the payoff         
of the game between generator and discriminator

            

  Dreal

 Dv       

  DG

    Dv   

 

Of course as we discussed in the previous section  in practice these expectations should be with respect to the empirical distributions  Our discussions in this section does
not depend on the distributions Dreal and Dh  so we de ne
         this way for simplicity 
The wellknown minmax theorem     Neumann    in
game theory shows if both players are allowed to play
mixed strategies then the game has   minmax solution   
mixed strategy for the generator is just   distribution Du
supported on    and one for discriminator is   distribution
Dv supported on   
Theorem  
value
   
    
    Eu Su                    Ev Sv               
Note that this equilibrium involves both parties announcing their strategies Su Sv at the start  such that neither will
have any incentive to change their strategy after studying
the opponent   strategy  The payoff is generated by the generator  rst sample                 and then generate an
example     Gu    Therefore  the mixed generator is just
  linear mixture of generators  The discriminator will  rst
sample         and then output Dv    Note that in general this is very different from   discriminator   that outputs Ev Sv  Dv    because the measuring function   is in

 vonNeumann  There
and   pair of mixed strategies

exists
 Su Sv 

Generalization and Equilibrium in Generative Adversarial Nets  GANs 

general nonlinear  In particular  the correct payoff function
for   mixture of discriminator is 

         

 
  Sv
   
  Dreal
  Sv

 Dv       
  Dh
  Sv

    Dv Gu   

Of course  this equilibrium involving an in nite mixture
makes little sense in practice  We show that  as is folklore in game theory  Lipton   Young    that we can
approximate this minmax solution with mixture of  nitely
many generators and discriminators  More precisely we
de ne  approximate equilibrium 
De nition     pair of mixed strategies  Su Sv  is an  
approximate equilibrium  if for some value          
Eu Su                          Ev Sv            
      If the strategies Su Sv are pure strategies  then this
pair is called an  approximate pure equilibrium 
Suppose   is   Lipschitz and bounded in     the
generator and discriminators are LLipschitz with respect
to the parameters and   Lipschitz with respect to inputs  in
this setting we can formalize the above Informal Theorem
as follows 
Theorem   In the settings above  there is   universal constant       such that for any  
there exists
        log LL     
generators Gu          GuT and  
discriminators Dv          DvT   let Su be   uniform distribution on ui and Sv be   uniform distribution on vi  then
 Su Sv  is an  approximate equilibrium  Furthermore  in
this equilibrium the generator  wins  meaning discriminators cannot do better than random guessing 

 

The proof uses   standard probabilistic argument and epsilon net argument to show that if we sample   generators
and discriminators from in nite mixture  they form an approximate equilibrium with high probability  For the second part  we use the fact that every distribution can be approximated by in nite mixture of Gaussians  so the generator must be able to approximate the real distribution Dreal
and win  Therefore indeed   mixture of       generators
can achieve an  approximate equilibrium  See supplementary material for details 
In the special case of          Wasserstein GAN  we
show that   mixture of generator discriminator is equivalent to   specially designed  larger generator discriminator 
therefore an approximate pure equilibrium exists  See supplementary material for more details 
Theorem   Suppose the generator and discriminator
are both klayer neural networks        with   parameters  and the last layer uses ReLU activation function  In
the setting of Theorem   when         there exists

     layer neural networks of generators   and discrim 

inator   with        log LL     

that there exists an  approximate pure equilibrium  Furthermore  if the generator is capable of generating   Gaussian then the value      

  parameters  such

 

  MIX GANs
Theorem   show that using   mixture of  not too many 
generators and discriminators guarantees existence of approximate equilibrium  This suggests that using   mixture
may lead to more stable training 
Of course  it is impractical to use very large mixtures  so
we propose MIX GAN  use   mixture of   components 
where   is as large as allowed by size of GPU memory
 usually       Namely  train   mixture of   generators  Gui           and   discriminators  Dvi          
which share the same network architecture but have their
own trainable parameters  Maintaining   mixture means
of course maintaining   weight wui for the generator Gui
which corresponds to the probability of selecting the output
of Gui  These weights are also updated via backpropagation  This heuristic can be combined with existing methods like DCGAN  Radford et al    WASSERSTEINGAN  Arjovsky et al    etc  giving us new training
methods MIX DCGAN  MIX WASSERSTEINGAN etc 
We use exponentiated gradient  Kivinen   Warmuth 
  store the logprobabilities  ui           and then
obtain the weights by applying softmax function on them 
wui  

  uiPT
     uk  

        

Note that our algorithm is maintaining weights on different
generators and discriminators  This is very different from
the idea of boosting where weights are maintained on samples  ADAGAN  Tolstikhin et al    uses ideas similar
to boosting and maintains weights on training examples 
Given payoff function     training MIX GAN boils down
to optimizing 

max

 

   ui  vj 

min

 ui ui 
  min

 ui ui 

       

 vj vj  
max

 vj vj   Xi      

wuiwvj    ui  vj 

Here the payoff function is the same as Equation   We
use both measuring functions       log    for original GAN  and          for WASSERSTEINGAN  In
our experiments we alternatively update generators  and
discriminators  parameters as well as their corresponding
logprobabilities using ADAM  Kingma   Ba    with
learning rate lr    
Empirically  it is observed that some components of the
mixture tend to collapse and their weights diminish during

Generalization and Equilibrium in Generative Adversarial Nets  GANs 

  PT

the training  To encourage full use of the mixture capacity  we add to the training objective an entropy regularizer
that discourages the weights being far away from uniform 
  log wui    log wvi 
Rent wui wvi       
  Experiments
In this section  we  rst explore the qualitative bene 
 ts of our method on image generation tasks  MNIST
dataset  LeCun et al    of handwritten digits and the
CelebA  Liu et al    dataset of human faces  Then
for more quantitative evaluation we use the CIFAR 
dataset  Krizhevsky   Hinton    and use the Inception
Score introduced in  Salimans et al    MNIST contains   labeled  sized images of handwritten
digits  CelebA contains over     sized images
of human faces  we crop the center   pixels for our
experiments  and CIFAR  has   labeled  
sized RGB natural images which fall into   categories 
To reinforce the point that this technique works out of the
box  no extensive hyperparameter search or tuning is necessary  Please refer to our code for experimental setup   

  Qualitative Results
The DCGAN architecture  Radford et al    uses deep
convolutional nets as generators and discriminators  We
trained MIX DCGAN on MNIST and CelebA using the
authors  code as   black box  and compared visual qualities
of generated images to those by DCGAN 
Results on MNIST is shown in Figure   In this experiment  the baseline DCGAN consists of   pair of   generator and   discriminator  which are  layer deconvoluitonal
neural networks  and are conditioned on image labels  Our
MIX DCGAN model consists of   mixture of such DCGANs so that it has   generators and   discriminators  We
observe that our method produces somewhat cleaner digits
than the baseline  note the fuzziness in the latter 
Results on CelebA dataset are also in Figure   using the
same architecture as for MNIST  except the models are not
conditioned on image labels anymore  Again  our method
generates more faithful and more diverse samples than the
baseline  Note that one may need to zoom in to fully perceive the difference  since both the two datasets are rather
easy for DCGAN 

  Quantitative Results
Now we turn to quantitative measurement using Inception
Score  Our method is applied to DCGAN and WASSERSTEINGAN  Arjovsky et al    and throughout  mix 

 Related code is public online at https github com 

PrincetonML MIXplus GANs git

Figure   MNIST and CelebA Samples  Digits and Faces generated from     MIX DCGAN      DCGAN 
tures of   generators and   discriminators are used  At  rst
sight the comparison DCGAN      MIX DCGAN seems
unfair because the latter uses   times the capacity of the former  with corresponding penalty in running time per epoch 
To address this  we also compare our method with larger
versions of DCGAN with roughly the same number of parameters  and we found the former is consistently better
than the later  as detailed below 
To construct MIX DCGAN  we build on top of the DCGAN trained with losses proposed by Huang et al   
which is the best variant so far without improved training techniques  The same hyperparameters are used for
fair comparison  See  Huang et al    for more details  Similarly  for the MIX WASSERSTEINGAN  the
base GAN is identical to that proposed by Arjovsky et al 
  using their hyperparameter scheme 
For   quantitative comparison  inception score is calculated
for each model  using   freshly generated samples
that are not used in training  To sample   single image from
our MIX  models  we  rst select   generator from the mixture according to their assigned weights  wui  and then
draw   sample from the selected generator 
Table   shows the results on the CIFAR  dataset  We  nd
that  simply by applying our method to the baseline models  our MIX  models achieve          on DCGAN 
and          on WASSERSTEINGAN  To con rm that
the superiority of MIX  models is not solely due to more
parameters  we also tested   DCGAN model with   times
many parameters  roughly the same number of parameters

Generalization and Equilibrium in Generative Adversarial Nets  GANs 

Table   Inception Scores on CIFAR  Mixture of DCGANs
achieves higher score than any singlecomponent DCGAN does 
All models except for WASSERSTEINGAN variants are trained
with labels 

Method
SteinGAN  Wang   Liu   
Improved GAN  Salimans et al   
ACGAN  Odena et al   
SGAN  best variant in  Huang et al   
DCGAN  as reported in  Wang   Liu   
DCGAN  best variant in  Huang et al   
DCGAN    size 
MIX DCGAN  with   components 
WASSERSTEINGAN
MIX WASSERSTEINGAN  with   components 
Real data

Score
 
 
     
   
 
 
 
 
 
 
 

Figure   Training Curve of MIX DCGAN      DCGAN  Inception Score  MIX DCGAN is consistently higher than DCGAN 
as    component MIX DCGAN  which is tuned using  
grid search over   sets of hyperparameters  learning rates 
dropout rates  and regularization weights  It gets only  
 labeled as    size  in Table   which is lower than that
of   MIX DCGAN  It is unclear how to apply MIX  to
SGANs  We tried mixtures of the upper and bottom generators separately  resulting in worse scores somehow  We
leave that for future exploration 
Figure   shows how Inception Scores evolve during training  MIX DCGAN outperforms DCGAN throughout the
entire training process  showing that it makes effective use
of the capacity 
Arjovsky et al    shows that  approximated  Wasserstein loss  which is the neural network divergence by
our de nition  is meaningful because it correlates well
with visual quality of generated samples 
Figure  
shows the training dynamics of neural network divergence
of MIX WASSERSTEINGAN      WASSERSTEINGAN 
which clearly indicates our method is capable of achieving   much lower divergence as well as of improving the
visual quality of generated samples 

 Wasserstein
towards

of MIX WASSERSTEINGAN
Objective 
the end but

Figure   Training Curve
     WASSERSTEINGAN
MIX WASSERSTEINGAN is better
loss drops less smoothly  which needs further investigation 
  Conclusions
The notion of generalization for GANs has been clari ed
by introducing   new notion of distance between distributions  the neural net distance   Whereas popular distances
such as Wasserstein and JS may not generalize  Assuming the visual cortex also is   deep net  or some network of
moderate capacity  generalization with respect to this metric is in principle suf cient to make the  nal samples look
realistic to humans  even if the GAN doesn   actually learn
the true distribution 
One issue raised by our analysis is that the current GANs
objectives cannot even enforce that the synthetic distribution has high diversity  Section   Furthermore this cannot be  xed by simply providing the discriminator with
more training examples  Possibly some other change to the
GANs setup are needed 
The paper also made progress another unexplained issue
about GANs  by showing that   pure approximate equilibrium exists for   certain natural training objective  Wasserstein  and in which the generator wins the game  No assumption about the target distribution Dreal is needed 
Suspecting that   pure equilibrium may not exist for all objectives  we recommend in practice our MIX GAN protocol using   small mixture of discriminators and generators 
Our experiments show it improves the quality of several
existing GAN training methods 
Finally  existence of an equilibrium does not imply that  
simple algorithm  in this case  backpropagation  would  nd
it easily  Understanding convergence remains wide open 

Acknowledgements
This paper was done in part while the authors were hosted
by Simons Institute  We thank Moritz Hardt  Kunal Talwar 
Luca Trevisan  and the referees for useful comments  This
research was supported by NSF  Of ce of Naval Research 
and the Simons Foundation 

Generalization and Equilibrium in Generative Adversarial Nets  GANs 

  uller  Alfred  Integral probability metrics and their generating classes of functions  Advances in Applied Probability     

Odena  Augustus  Olah  Christopher  and Shlens  Jonathon 
Conditional image synthesis with auxiliary classi er
gans  arXiv preprint arXiv   

Radford  Alec  Metz  Luke  and Chintala  Soumith  Unsupervised representation learning with deep convolutional
generative adversarial networks  In International Conference on Learning Representations   

Salimans  Tim  Goodfellow  Ian  Zaremba  Wojciech  Cheung  Vicki  Radford  Alec  and Chen  Xi  Improved techniques for training gans  In Advances in Neural Information Processing Systems   

Tolstikhin  Ilya  Gelly  Sylvain  Bousquet  Olivier  SimonGabriel  CarlJohann  and Sch olkopf  Bernhard  Adagan  Boosting generative models 
arXiv preprint
arXiv   

   Neumann     Zur theorie der gesellschaftsspiele  Mathe 

matische annalen     

Wang  Dilin and Liu  Qiang  Learning to draw samples 
With application to amortized mle for generative adversarial learning  Technical report   

References
Abadi  Mart   and Andersen  David    Learning to protect communications with adversarial neural cryptography  arXiv preprint arXiv   

Arjovsky  Martin  Chintala  Soumith  and Bottou    eon 
arXiv preprint arXiv 

Wasserstein gan 
 

Durugkar     Gemp     and Mahadevan     Generative
MultiAdversarial Networks  ArXiv eprints  November
 

Ghosh  Jayanta    Ghosh  RVJK  and Ramamoorthi  RV 

Bayesian nonparametrics  Technical report   

Goodfellow  Ian  Nips   tutorial  Generative adversar 

ial networks  arXiv preprint arXiv   

Goodfellow  Ian  PougetAbadie  Jean  Mirza  Mehdi  Xu 
Bing  WardeFarley  David  Ozair  Sherjil  Courville 
Aaron  and Bengio  Yoshua  Generative adversarial nets 
In Advances in neural information processing systems 
pp     

Huang  Xun  Li  Yixuan  Poursaeed  Omid  Hopcroft  John 
and Belongie  Serge  Stacked generative adversarial
networks  In Computer Vision and Patter Recognition 
 

Jiwoong Im     Ma     Dongjoo Kim     and Taylor    
Generative Adversarial Parallelization  ArXiv eprints 
December  

Kingma  Diederik and Ba  Jimmy  Adam    method for
stochastic optimization  In International Conference on
Learning Representations   

Kivinen  Jyrki and Warmuth  Manfred    Exponentiated
gradient versus gradient descent for linear predictors  Information and Computation     

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple
layers of features from tiny images  Technical report 
 

LeCun  Yann  Cortes  Corinna  and Burges  Christopher JC  The mnist database of handwritten digits   

Lipton  Richard   and Young  Neal    Simple strategies for
large zerosum games with applications to complexity
theory  In Proceedings of the twentysixth annual ACM
symposium on Theory of computing  pp    ACM 
 

Liu  Ziwei  Luo  Ping  Wang  Xiaogang  and Tang  Xiaoou 
Deep learning face attributes in the wild  In Proceedings
of the IEEE International Conference on Computer Vision  pp     

