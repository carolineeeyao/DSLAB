DiameterBased Active Learning

Christopher Tosh   Sanjoy Dasgupta  

Abstract

To date  the tightest upper and lowerbounds for
the active learning of general concept classes
have been in terms of   parameter of the learning problem called the splitting index  We provide  for the  rst time  an ef cient algorithm that
is able to realize this upper bound  and we empirically demonstrate its good performance 

  Introduction
In many situations where   classi er is to be learned  it is
easy to collect unlabeled data but costly to obtain labels 
This has motivated the poolbased active learning model 
in which   learner has access to   collection of unlabeled
data points and is allowed to ask for individual labels in an
adaptive manner  The hope is that choosing these queries
intelligently will rapidly yield   lowerror classi er  much
more quickly than with random querying    central focus
of active learning is developing ef cient querying strategies
and understanding their label complexity 
Over the past decade or two 
there has been substantial progress in developing such rigorouslyjusti ed active
learning schemes for general concept classes  For the most
part  these schemes can be described as mellow  rather than
focusing upon maximally informative points  they query
any point whose label cannot reasonably be inferred from
the information received so far  It is of interest to develop
more aggressive strategies with better label complexity 
An exception to this general trend is the aggressive strategy
of  Dasgupta    whose label complexity is known to
be optimal in its dependence on   key parameter called the
splitting index  However  this strategy has been primarily
of theoretical interest because it is dif cult to implement
algorithmically  In this paper  we introduce   variant of the
methodology that yields ef cient algorithms  We show that

 Department of Computer Science and Engineering  UC
San Diego  La Jolla  CA  USA  Correspondence to  Christopher Tosh  ctosh cs ucsd edu  Sanjoy Dasgupta  dasgupta cs ucsd edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

it admits roughly the same label complexity bounds as well
as having promising experimental performance 
As with the original splitting index result  we operate in
the realizable setting  where data can be perfectly classi 
 ed by some function    in the hypothesis class    At any
given time during the active learning process  the remaining candidates that is  the elements of   consistent with
the data so far are called the version space  The goal of
aggressive active learners is typically to pick queries that
are likely to shrink this version space rapidly  But what is
the right notion of size  Dasgupta   pointed out that
the diameter of the version space is what matters  where the
distance between two classi ers is taken to be the fraction
of points on which they make different predictions  Unfortunately  the diameter is   dif cult measure to work with
because it cannot  in general  be decreased at   steady rate 
Thus the earlier work used   procedure that has quanti able
label complexity but is not conducive to implementation 
We take   fresh perspective on this earlier result  We start
by suggesting an alternative  but closely related  notion of
the size of   version space  the average pairwise distance
between hypotheses in the version space  with respect to
some underlying probability distribution   on    This distribution   can be arbitrary that is  there is no requirement that the target    is chosen from it but should be
chosen so that it is easy to sample from  When   consists
of linear separators  for instance    good choice would be  
logconcave density  such as   Gaussian 
At any given time  the next query   is chosen roughly as
follows 

  Sample   collection of classi ers               hm from

  restricted to the current version space    

  Compute the distances between them  this can be done

using just the unlabeled points 

  Any candidate query   partitions the classi ers  hi 
into two groups  those that assign it     label  call
these    
    and those that assign it     label  call these
       Estimate the averagediameter after labeling
  by the sum of the distances between classi ers hi
    or those within        whichever is larger 
within    
  Out of the pool of unlabeled data  pick the   for which

DiameterBased Active Learning

this diameterestimate is smallest 

This is repeated until the version space has small enough
average diameter that   random sample from it is very
likely to have error less than   userspeci ed threshold  
We show how all these steps can be achieved ef ciently  as
long as there is   sampler for  
Dasgupta   pointed out that the label complexity of
active learning depends on the underlying distribution  the
amount of unlabeled data  since more data means greater
potential for highlyinformative points  and also the target
classi er    That paper identi es   parameter called the
splitting index   that captures the relevant geometry  and
gives upper bounds on label complexity that are proportional to   as well as showing that this dependence is
inevitable  For our modi ed notion of diameter    different
averaged splitting index is needed  However  we show that
it can be bounded by the original splitting index  with an
extra multiplicative factor of log  thus all previouslyobtained label complexity results translate immediately for
our new algorithm 

  Related Work
The theory of active learning has developed along several
fronts 
One of these is nonparametric active learning  where the
learner starts with   pool of unlabeled points  adaptively
queries   few of them  and then  lls in the remaining labels  The goal is to do this with as few errors as possible   In particular  the learner does not return   classi er
from some prede ned parametrized class  One scheme begins by building   neighborhood graph on the unlabeled
data  and propagating queried labels along the edges of
this graph  Zhu et al    CesaBianchi et al   
Dasarathy et al    Another starts with   hierarchical
clustering of the data and moves down the tree  sampling at
random until it  nds clusters that are relatively pure in their
labels  Dasgupta   Hsu    The label complexity of
such methods have typically be given in terms of smoothness properties of the underlying data distribution  Castro
  Nowak    Kpotufe et al   
Another line of work has focused on active learning of linear separators  by querying points close to the current guess
at the decision boundary  Balcan et al    Dasgupta
et al    Balcan   Long    Such algorithms are
close in spirit to those used in practice  but their analysis
to date has required fairly strong assumptions to the effect
that the underlying distribution on the unlabeled points is
logconcave  Interestingly  regret guarantees for online algorithms of this sort can be shown under far weaker conditions  CesaBianchi et al   

The third category of results  to which the present paper belongs  considers active learning strategies for general concept classes    Some of these schemes  Cohn et al   
Dasgupta et al    Beygelzimer et al    Balcan
et al    Zhang   Chaudhuri    are fairly mellow in
the sense described earlier  using generalization bounds to
gauge which labels can be inferred from those obtained so
far  The label complexity of these methods can be bounded
in terms of   quantity known as the disagreement coef 
cient  Hanneke    In the realizable case  the canonical
such algorithm is that of  Cohn et al    henceforth referred to as CAL  Other methods use   prior distribution  
over the hypothesis class  sometimes assuming that the target classi er is   random draw from this prior  These methods typically aim to shrink the mass of the version space
under   either greedily and explicitly  Dasgupta   
Guillory   Bilmes    Golovin et al    or implicitly  Freund et al    Perhaps the most widelyused of
these methods is the latter  queryby committee  henceforth
QBC  As mentioned earlier  shrinking  mass is not an optimal strategy if low misclassi cation error is the ultimate
goal  In particular  what matters is not the prior mass of the
remaining version space  but rather how different these candidate classi ers are from each other  This motivates using
the diameter of the version space as   yardstick  which was
 rst proposed in  Dasgupta    and is taken up again
here 

  Preliminaries
Consider   binary hypothesis class      data space     and
  distribution   over     For mathematical convenience 
we will restrict ourselves to  nite hypothesis classes   We
can do this without loss of generality when   has  nite VC
dimension  since we only use the predictions of hypotheses
on   pool of unlabeled points  however  we do not spell out
the details of this reduction here  The hypothesis distance
induced by   over   is the pseudometric

            rx              
Given   point      and   subset        denote

   
                     

and               
    Given   sequence of data points
           xn and   target hypothesis    the induced version
space is the set of hypotheses that are consistent with the
target hypotheses on the sequence      

          xi      xi  for all                 

  Diameter and the Splitting Index
The diameter of   set of hypotheses      is the maximal
distance between any two hypotheses in         

diam       max
     

       

DiameterBased Active Learning

Without any prior information  any hypothesis in the version space could be the target  Thus the worst case error of
any hypothesis in the version space is the diameter of the
version space  The splitting index roughly characterizes the
number of queries required for an active learning algorithm
to reduce the diameter of the version space below  
While reducing the diameter of   version space       
we will sometimes identify pairs of hypotheses          
that are far apart and therefore need to be separated  We
will refer to        as an edge  Given   set of edges    
              hn           we say   data point     
splits   if querying   separates at least     fraction of the
pairs  that is  if

diameter of   subset      as the expected distance between two hypotheses in   randomly drawn from       

       Eh             

where    is the conditional distribution induced by restricting   to     that is                  for        
Intuitively    version space with very small average diameter ought to put high weight on hypotheses that are close to
the true hypothesis  Indeed  given   version space   with
         the following lemma shows that if      is small
enough  then   low error hypothesis can be found by two
popular heuristics  random sampling and MAP estimation 

where   
tempting to get accuracy     we need to only eliminate
edge of length greater than   De ne

                  
    and similarly for       When at 

max   
         
                           

 

The splitting index of   set      is   tuple         such
that for all  nite edgesets     
 

  rx       splits       

The following theorem  due to Dasgupta   bounds
the sample complexity of active learning in terms of the
splitting index  The    notation hides polylogarithmic factors in        log   and the failure probability  
Theorem    Dasgupta   Suppose   is   hypothesis
class with splitting index         Then to learn   hypothesis with error  

    any active learning algorithm with     unlabeled

samples must request at least   labels  and

    if   has VCdimension    there is an active learning
algorithm that draws         log  unlabeled
data points and requests       log  labels 

Unfortunately 
the only known algorithm satisfying    
above is intractable for all but the simplest hypothesis
classes  it constructs an  covering of the hypothesis space
and queries points which whittle away at the diameter of
this covering  To overcome this intractability  we consider
  slightly more benign setting in which we have   samplable prior distribution   over our hypothesis space   
  An Average Notion of Diameter
With   prior distribution  it makes sense to shift away from
the worstcase to the averagecase  We de ne the average

Lemma   Suppose      contains    Pick    
     Random sampling  If                 then

Eh               

     MAP estimation  Write pmap   maxh         

Pick       map  If

          min       pmap      

then             for any   with          pmap    

Proof  Part     follows from
       Eh                      Eh           
For     take     min       pmap     and de ne
                         Note that    contains
   as well as any       with          pmap    
We claim diam      is at most   Suppose not  Then there
exist            satisfying             implying

       Eh             

                                   

But this contradicts our assumption on      Since both
           we have    
  An Average Notion of Splitting
We now turn to de ning an average notion of splitting   
data point     average splits   if

   

       

         
   

                       
max      
And we say   set      has average splitting index
        if for any subset       such that         

  rx        average splits        

DiameterBased Active Learning

Intuitively  average splitting refers to the ability to signi 
cantly decrease the potential function

Remark  It is tempting to de ne average splitting in terms
of the average diameter as

          Eh                        

max    

                    

with   single query 
While this potential function may seem strange at  rst
glance  it is closely related to the original splitting index 
The following lemma  whose proof is deferred to Section  
shows the splitting index bounds the average splitting index
for any hypothesis class 
Lemma   Let   be   probability measure over   hypothesis class    If   has splitting index         then it has
average splitting index  

       

 

 dlog  

Dasgupta   derived the splitting indices for several
hypothesis classes  including intervals and homogeneous
linear separators  Lemma   implies average splitting indices within   log  factor in these settings 
Moreover  given access to samples from      we can
easily estimate the quantities appearing in the de nition of average splitting  For an edge sequence    
              hn       de ne
nXi 

  hi      

      

When hi      are        draws from    for all                 
which we denote            the random variables
             and     
    are unbiased estimators of the
quantities appearing in the de nition of average splitting 
Lemma   Given            we have
     
    
for any        Similarly for     and       
Proof  From de nitions and linearity of expectations  it is
easy to observe                 By the independence
of hi       we additionally have

           and     

    
         
   

    

   

 

 

 

 

 

 

 

    

nE    hi       

     
    
nE    hi      
 hi      
   
    hi            
      
      
      

    

   

   

   

 

 

  hi      
      hi      
          
    hi       hi           
   

However  this de nition does not satisfy   nice relationship with the splitting index  Indeed  there exist hypothesis
classes   for which there are many points which  split

  for any     

  but for which every      satis es
max    

                

This observation is formally proven in the appendix 

  An Average Splitting Index Algorithm
Suppose we are given   version space   with average splitting index         If we draw       points from the data
distribution then  with high probability  one of these will  
average split     Querying that point will result in   version
space     with signi cantly smaller potential        
If we knew the value     priori  then Lemma   combined
with standard concentration bounds  Hoeffding    Angluin   Valiant    would give us   relatively straightforward procedure to  nd   good query point 

      

  For suitable   and    it will be the case that with

  Draw            and compute the empirical estimateb        
  Draw           for   depending on   andb 
                      

Querying that point will decrease the potential 

max     

high probability  for some   

 
 

However  we typically would not know the average splitting index ahead of time  Moreover  it is possible that the
average splitting index may change from one version space
to the next  In the next section  we describe   query selection procedure that adapts to the splittability of the current
version space 

  Finding   Good Query Point
Algorithm   which we term SELECT  is our query selection procedure  It takes as input   sequence of data points
           xm  at least one of which  average splits the current version space  and with high probability  nds   data
point that  average splits the version space 
SELECT proceeds by positing an optimistic estimate of  

which we denoteb    and successively halving it until we are

DiameterBased Active Learning

Algorithm   DBAL

Algorithm   SELECT

Input  Hypothesis class    prior distribution  
Initialize      
while  
          

  for           do
Draw   data points                 xm 
Query point xi   SELECT       and set   to be consistent with the result

end while
return Current version space   in the form of the
queried points                   xK    xK 

Input  Version space     prior   data                 xm 

for               do

Setb     
Draw          mt and computeb      
Draw         nt
max     
If   xi     
then halt and return xi
Otherwise  letb         

xi      xi                

end for

    

 
nt

mt

the version space  In order for this algorithm to succeed  we
need to choose nt and mt such that with high probability

con dent that we have found   point thatb taverage splits
     is an accurate estimate of      and   our halting
condition will be true ifb   is within   constant factor of  

and false otherwise  The following lemma  whose proof is
in the appendix  provides such choices for nt and mt 
Lemma   Let           be given  Suppose that version space   satis es          In SELECT       round
  and data point      that exactly  average splits  
 that is  max       
                      
         If

      

 

 

log

 
 

 
 

 
   

  

mt  

and nt   max   
tb  
  

   log
  
then with probability                       and
    if         then
                          
                          

    If           then

max     

max     

 
nt

 
nt

Given the above lemma  we can establish   bound on the
number of rounds and the total number of hypotheses SELECT needs to  nd   data point that  average splits the
version space 
Theorem   Suppose that SELECT is called with   version space   with          and   collection of points
           xm such that at least one of xi  average splits    
If           log  then with probability at
least       SELECT returns   point xi that  average
splits      nishing in less than dlog       rounds and
sampling     
  hypotheses in total 

       log  

    log 

Remark   It is possible to modify SELECT to  nd   point
xi that    average splits   for any constant       while
only having to draw    more hypotheses in total  First

note that by halvingb   at each step  we immediately give

up   factor of two in our approximation  This can be made
smaller by taking narrower steps  Additionally  with   constant factor increase in mt and nt  the approximation ratios
in Lemma   can be set to any constant 
Remark   At  rst glance  it appears that SELECT requires
us to know   in order to calculate   However    crude
lower bound on   suf ces  Such   bound can always be
found in terms of   This is because any version space
is    splittable  Dasgupta    Lemma   By
Lemma   so long as   is less than   we can substitute

 

for   in when we compute  

 dlog  
Proof of Theorem   Let     dlog       By
Lemma   we know that for rounds                   we don  

  of hypotheses drawn  

  with probability       Moreover  in the    th round 
with probability       we will select   point which does
worse than  average split    
Note that we draw mt   nt hypotheses at each round  By

return any point which does worse thanb   average splits
it will be the case that             and therefore 
no worse thanb    average split     which in turn does no
Lemma   for each roundb              Thus
TXt 
   log
TXt    
  
  
tb  
  
     log
TXt   
   
Givenb        and         log   we have
   
 log  
TXt 

      TXt 

TXt 

mt   nt

 
   

 
 

 

 

 

 

   

 

 

 

 

 

 

 
 

 

 

 

  

DiameterBased Active Learning

Plugging in    
statement 

   log  we recover the theorem

 

  Active Learning Strategy
Using the SELECT procedure as   subroutine  Algorithm  
henceforth DBAL for Diameterbased Active Learning  is
our active learning strategy  Given   hypothesis class
with average splitting index       DBAL queries data
points provided by SELECT until it is con dent         
Denote by Vt the version space in the tth round of DBAL 
The following lemma  which is proven in the appendix 
demonstrates that the halting condition  that is        
    where   consists of   pairs sampled from     
guarantees that with high probability DBAL stops when
 Vt  is small 
Lemma   The following holds for DBAL 

    Suppose that for all                   that  Vt     
Then the probability that the termination condition is
ever true for any of those rounds is bounded above by

  exp    
 
    Suppose that for some                   that  Vt   
  Then the probability that the termination condition is not true in that round is bounded above by
  exp    
 

Given the guarantees on the SELECT procedure in Theorem   and on the termination condition provided by
Lemma   we get the following theorem 
Theorem   Suppose that   has average splitting index
      Then DBAL returns   version space   satisfying          with probability at least       while using
the following resources 
    rounds  with one label

 log  

         

      log

 

  log   

  unlabeled data points sampled per

per round 
         
round  and
            

    log 

 

    log log  

potheses sampled per round 

 log mK

  hy 

  log   

Proof  From de nition of the average splitting index  if
we draw      
  unlabeled points per round  then
with probability       each of the  rst   rounds will
have at least one data point that  average splits the current version space  In each such round  if the version space
has average diameter at least   then with probability
      SELECT will return   data point that  average
splits the current version space while sampling no more

 

 

  log  

  log  

if the termination check uses     

  log mK log  

     
per round by Theorem  
By Lemma  

  hypotheses
than         
  hypotheses per round  then with probability
    
      in the  rst   rounds the termination condition
will never be true when the current version space has average diameter greater than   and will certainly be true if
the current version space has diameter less than  
Thus it suf ces to bound the number of rounds in which we
can  average split the version space before encountering   version space with  
Since the version space is always consistent with the true
hypothesis    we will always have  Vt        After
    rounds of  average splitK    
ting  we have

      log

 

 log  
   VK     VK VK 
  

   

 

   

     

 

 

Thus in the  rst   rounds  we must terminate with   version space with average diameter less than  

  Proof of Lemma  
In this section  we give the proof of the following relationship between the original splitting index and our average
splitting index 
Lemma   Let   be   probability measure over   hypothesis class    If   has splitting index         then it has
average splitting index  

       

 

 dlog  

The  rst step in proving Lemma   is to relate the splitting
index to our estimator    Intuitively  splittability says
that for any set of large edges there are many data points
which remove   signi cant fraction of them  One may suspect this should imply that if   set of edges is large on average  then there should be many data points which remove
  signi cant fraction of their weight  The following lemma
con rms this suspicion 
Lemma   Suppose that      has splitting index
        and say                   hn       is   sequence of hypothesis pairs from   satisfying  
          
Then if        we have with probability at least  
max     

                

 dlog        

 

DiameterBased Active Learning

 

 
 

 

 

 
 

 

 

 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 
 

 

 

 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

Queries

 

 

 

 

Queries

 

 

 

 

       
Queries

Strategy

CAL

DBAL

QBC

Random

Figure   Simulation results on homogeneous linear separators  Left        Middle        Right       

Proof  Consider partitioning   as

                            and
Ek                               

for                 with     dlog  
    Then            EK are
all disjoint and their union is    De ne         
  Ek 
We  rst claim that               This follows from
the observation that because            and each edge in
   has length less than   we must have

                                      

Next  observe that because each edge        Ek with
      satis es                   we have
KXk 

KXk        Ek

         

   Ek 

        

Since there are only   summands on the right  at least one
of these must be larger than          Let   denote that
index and let   be   point which  splits Ek  Then we have

      

                 Ek    Ek 
   
              Ek 
          
   

 

    

Since               we have
              
   
        

 

 

          

Symmetric arguments show the same holds for      
Finally  by the de nition of splitting  the probability of
drawing   point   which  splits Ek is at least   giving
us the lemma 

With Lemma   in hand  we are now ready to prove
Lemma  

Proof of Lemma   Let      such that          Suppose that we draw   edges          from    and draw  
data point        Then Hoeffding   inequality  Hoeffding    combined with Lemma   tells us that there exist sequences           such that with probability at least
        the following hold simultaneously 
                   
   
          and
         

               
          
   
      
                   
   

                  

       

         
   
 
 

For    small enough  we have that               Combining the above with Lemma   we have with probability
at least        
max      
   

                 
           
max     
 dlog        
 
 dlog              
By taking       we have           giving us the
lemma 

   
   

 

 

 

  Simulations
We compared DBAL against the baseline passive learner as
well as two other generic active learning strategies  CAL

DiameterBased Active Learning

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

 

 

 
 
Queries

 

 

 

 

 
 
Queries

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

Queries

 

 

 

 

 

Queries

 

 

Strategy

CAL

DBAL

QBC

Random

Figure   Simulation results on ksparse monotone disjunctions  In all cases       Top left              Top right       
      Bottom left              Bottom right             

and QBC  CAL proceeds by randomly sampling   data
point and querying it if its label cannot be inferred from
previously queried data points  QBC uses   prior distribution   and maintains   version space     Given   randomly sampled data point    QBC samples two hypotheses
           and queries   if            
We tested on two hypothesis classes  homogeneous  or
throughthe origin  linear separators and ksparse monotone disjunctions  In each of our simulations  we drew our
target    from the prior distribution  After each query  we
estimated the average diameter of the version space  We
repeated each simulation several times and plotted the average performance of each algorithm 

Homogeneous
linear separators The class of ddimensional homogeneous linear separators can be identi ed with elements of the ddimensional unit sphere  That
is    hypothesis         acts on   data point     Rd via
the sign of their inner product 

       sign hh  xi 

In our simulations  both the prior distribution and the data
distribution are uniform over the unit sphere  Although
there is no known method to exactly sample uniformly
from the version space  GiladBachrach et al   
demonstrated that using samples generated by the hitand 
run Markov chain works well in practice  We adopted this
approach for our sampling tasks 
Figure   shows the results of our simulations on homogeneous linear separators 

Sparse monotone disjunctions   ksparse monotone
disjunction is   disjunction of   positive literals  Given
  Boolean vector             monotone disjunction  
classi es   as positive if and only if xi     for some positive literal   in   
In our simulations  each data point is   vector whose coordinates are        Bernoulli random variables with parameter    The prior distribution is uniform over all ksparse
monotone disjunctions  When   is constant  it is possible
to sample from the prior restricted to the version space in
expected polynomial time using rejection sampling 
The results of our simulations on ksparse monotone disjunctions are in Figure  

Acknowledgments
The authors are grateful to the reviewers for their feedback
and to the NSF for support under grants IIS  and
DGE  Part of this work was done at the Simons
Institute for Theoretical Computer Science  Berkeley  as
part of   program on the foundations of machine learning 
CT additionally thanks Daniel Hsu and Stefanos Poulis for
helpful discussions 

References
Angluin  Dana and Valiant  Leslie    Fast probabilistic algorithms for hamiltonian circuits and matchings  In Proceedings of the ninth annual ACM symposium on Theory
of computing  pp    ACM   

Balcan  MariaFlorina and Long  Phil  Active and passive

DiameterBased Active Learning

Dasgupta  Sanjoy  Kalai  Adam Tauman  and Monteleoni 
Claire  Analysis of perceptronbased active learning 
Journal of Machine Learning Research   Feb 
   

Freund  Yoav  Seung    Sebastian  Shamir  Eli  and Tishby 
Naftali  Selective sampling using the query by committee algorithm  Machine learning   
 

GiladBachrach  Ran  Navot  Amir  and Tishby  Naftali 
Query by committee made real  In Proceedings of the
 th International Conference on Neural Information
Processing Systems  pp    MIT Press   

Golovin  Daniel  Krause  Andreas  and Ray  Debajyoti 
Nearoptimal bayesian active learning with noisy observations  In Advances in Neural Information Processing
Systems  pp     

Guillory  Andrew and Bilmes  Jeff  Averagecase active
learning with costs 
In International Conference on
Algorithmic Learning Theory  pp    Springer 
 

Hanneke  Steve    bound on the label complexity of agnostic active learning  In Proceedings of the  th international conference on Machine learning  pp   
ACM   

Hoeffding  Wassily  Probability inequalities for sums of
bounded random variables  Journal of the American statistical association     

Kpotufe  Samory  Urner  Ruth  and BenDavid  Shai  Hierarchical label queries with datadependent partitions  In
Proceedings of The  th Conference on Learning Theory  pp     

Zhang  Chicheng and Chaudhuri  Kamalika  Beyond
disagreementbased agnostic active learning 
In Advances in Neural Information Processing Systems  pp 
   

Zhu  Xiaojin  Ghahramani  Zoubin  and Lafferty  John 
Semisupervised learning using gaussian  elds and harmonic functions 
In Proceedings of the  th International Conference on Machine Learning   

learning of linear separators under logconcave distributions  In Proceedings of the  th Conference on Learning Theory  pp     

Balcan  MariaFlorina  Broder  Andrei  and Zhang  Tong 
In International ConMargin based active learning 
ference on Computational Learning Theory  pp   
Springer   

Balcan  MariaFlorina  Beygelzimer  Alina  and Langford 
John  Agnostic active learning  Journal of Computer
and System Sciences     

Beygelzimer  Alina  Dasgupta  Sanjoy  and Langford 
John  Importance weighted active learning  In Proceedings of the  th Annual International Conference on Machine Learning  pp     

Castro  Rui   and Nowak  Robert    Minimax bounds
for active learning  IEEE Transactions on Information
Theory     

CesaBianchi  Nicolo  Gentile  Claudio  and Zaniboni 
Luca  Worstcase analysis of selective sampling for
linear classi cation  Journal of Machine Learning Research     

CesaBianchi  Nicolo  Gentile  Claudio  and Vitale  Fabio 
Learning unknown graphs  In International Conference
on Algorithmic Learning Theory  pp    Springer 
 

Cohn  David  Atlas  Les  and Ladner  Richard  Improving
generalization with active learning  Machine learning 
   

Dasarathy  Gautam  Nowak  Robert  and Zhu  Xiaojin    
An ef cient graph based active learning algorithm with
application to nonparametric classi cation  In Proceedings of The  th Conference on Learning Theory  pp 
   

Dasgupta  Sanjoy  Analysis of   greedy active learning
strategy  In Advances in neural information processing
systems  pp     

Dasgupta  Sanjoy  Coarse sample complexity bounds for
active learning  In Advances in neural information processing systems  pp     

Dasgupta  Sanjoy and Hsu  Daniel  Hierarchical sampling
for active learning 
In Proceedings of the  th international conference on Machine learning  pp   
ACM   

Dasgupta  Sanjoy  Monteleoni  Claire  and Hsu  Daniel     
general agnostic active learning algorithm  In Advances
in neural information processing systems  pp   
 

