Robust Probabilistic Modeling with Bayesian Data Reweighting

Yixin Wang   Alp Kucukelbir   David    Blei  

Abstract

Probabilistic models analyze data by relying on
  set of assumptions  Data that exhibit deviations from these assumptions can undermine inference and prediction quality  Robust models offer
protection against mismatch between   model  
assumptions and reality  We propose   way to
systematically detect and mitigate mismatch of  
large class of probabilistic models  The idea is
to raise the likelihood of each observation to  
weight and then to infer both the latent variables
and the weights from data  Inferring the weights
allows   model to identify observations that match
its assumptions and downweight others  This enables robust inference and improves predictive
accuracy  We study four different forms of mismatch with reality  ranging from missing latent
groups to structure misspeci cation    Poisson
factorization analysis of the Movielens    dataset
shows the bene ts of this approach in   practical
scenario 

  Introduction
Probabilistic modeling is   powerful approach to discovering hidden patterns in data  We begin by expressing assumptions about the class of patterns we expect to discover 
this is how we design   probability model  We follow by
inferring the posterior of the model  this is how we discover
the speci   patterns manifest in an observed data set  Advances in automated inference  Hoffman   Gelman   
Mansinghka et al    Kucukelbir et al    enable
easy development of new models for machine learning and
arti cial intelligence  Ghahramani   
In this paper  we present   recipe to robustify probabilistic
models  What do we mean by  robustify  Departure from
  model   assumptions can undermine its inference and
prediction performance  This can arise due to corrupted
 Columbia University  New York City  USA  Correspondence

to  Yixin Wang  yixin wang columbia edu 

Proceedings of the  th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

observations  or in general  measurements that do not belong
to the process we are modeling  Robust models should
perform well in spite of such mismatch with reality 
Consider   movie recommendation system  We gather data
of people watching movies via the account they use to log in 
Imagine   situation where   few observations are corrupted
For example    child logs in to her account and regularly
watches popular animated  lms  One day  her parents use
the same account to watch   horror movie  Recommendation models  like Poisson factorization  PF  struggle with
this kind of corrupted data  see Section   it begins to
recommend horror movies 
What can be done to detect and mitigate this effect  One
strategy is to design new models that are less sensitive to
corrupted data  such as by replacing   Gaussian likelihood
with   heaviertailed   distribution  Huber    Insua  
Ruggeri    Most probabilistic models we use have
more sophisticated structures  these template solutions for
speci   distributions are not readily applicable  Other classical robust techniques act mostly on distances between
observations  Huber    these approaches struggle with
highdimensional data  How can we still make use of our favorite probabilistic models while making them less sensitive
to the messy nature of reality 
Main idea  We propose reweighted probabilistic models
 RPM  The idea is simple  First  posit   probabilistic model 
Then adjust the contribution of each observation by raising
each likelihood term to its own  latent  weight  Finally  infer
these weights along with the latent variables of the original
probability model  The posterior of this adjusted model
identi es observations that match its assumptions  it downweights observations that disagree with its assumptions 

 
 
 
 
 
 
 

Uncorrupted
Corrupted
Original Model
Reweighted Model

 

 

 

Figure   Fitting   unimodal distribution to   dataset with corrupted
measurements  The RPM downweights the corrupted observations 

Figure   depicts this tradeoff  The dataset includes cor 

Robust Probabilistic Modeling with Bayesian Data Reweighting

rupted measurements that undermine the original model 
Bayesian data reweighting automatically trades off the low
likelihood of the corrupted data near   to focus on the
uncorrupted data near zero  The RPM  green curve  detects
this mismatch and mitigates its effect compared to the poor
   of the original model  red curve 
Formally  consider   dataset of   independent observations
                yN   The likelihood factorizes as   product
nD   yn     where   is   set of latent variables  Posit  
prior distribution     
Bayesian data reweighting follows three steps 

QN
  De ne   probabilistic model     QN

nD   yn    
  Raise each likelihood to   positive latent weight wn 
Then choose   prior on the weights pw     where    
            wN   This gives   reweighted probabilistic
model  RPM 

           

 
 

    pw    

 yn    wn  

NYnD 

where   is the normalizing factor 

  Infer the posterior of both the latent variables   and

the weights             

The latent weights   allow an RPM to automatically explore
which observations match its assumptions and which do not 
Writing out the logarithm of the RPM gives some intuition 
it is equal  up to an additive constant  to

log        log pw     CXn

wn log  yn    

 

Posterior inference  loosely speaking  seeks to maximize
the above with respect to   and    The prior on the weights
pw     plays   critical role 
it trades off extremely low
likelihood terms  caused by corrupted measurements  while
encouraging the weights to be close to one  We study three
options for this prior in Section  
How does Bayesian data reweighting induce robustness 
First  consider how the weights   affect Equation   The
logarithm of our priors are dominated by the log wn term 
this is the price of moving wn from one towards zero  By
shrinking wn  we gain an increase in wn log  yn     while
paying   price in   log wn  The gain outweighs the price we
pay if log  yn     is very negative  Our priors are set to
prefer wn to stay close to one  an RPM only shrinks wn for
very unlikely       corrupted  measurements 
Now consider how the latent variables   affect Equation  
As the weights of unlikely measurements shrink  the likelihood term can afford to assign low mass to those corrupted measurements and focus on the rest of the dataset 

Jointly  the weights and latent variables work together to
automatically identify unlikely measurements and focus on
observations that match the original model   assumptions 
Section   presents these intuitions in full detail  along with
theoretical corroboration  In Section   we study four models under various forms of mismatch with reality  including
missing modeling assumptions  misspeci ed nonlinearities 
and skewed data  RPMs provide better parameter inference
and improved predictive accuracy across these models  Section   presents   recommendation system example  where
we improve on predictive performance and identify atypical
 lm enthusiasts in the Movielens    dataset 
Related work  Jerzy Neyman elegantly motivates the main
idea behind robust probabilistic modeling     eld that has
attracted much research attention in the past century 

Every attempt to use mathematics to study
some real phenomena must begin with building  
mathematical model of these phenomena  Of necessity  the model simpli es matters to   greater
or lesser extent and   number of details are ignored 
  The solution of the mathematical
problem may be correct and yet it may be in violent con ict with realities simply because the
original assumptions of the mathematical model
diverge essentially from the conditions of the practical problem considered   Neyman      

Our work draws on three themes around robust modeling 
The  rst is   body of work on robust statistics and machine
learning  Provost   Fawcett    Song et al    Yu
et al    McWilliams et al    Feng et al   
Sha eezadehAbadeh et al    These developments
focus on making speci   models more robust to imprecise
measurements 
One strategy is popular  localization  To localize   probabilistic model  allow each likelihood to depend on its own
 copy  of the latent variable     This transforms the model
into

NYnD 

             

 yn               

 

where   toplevel latent variable   ties together all the   
variables  de Finetti    Wang   Blei    Localization decreases the effect of imprecise measurements  RPMs
present   broader approach to mitigating mismatch  with
improved performance over localization  Sections   and  
The second theme is robust Bayesian analysis  which studies sensitivity with respect to the prior  Berger et al   
  Localization also relates to JamesStein shrinkage  Efron

  connects these dots 

Robust Probabilistic Modeling with Bayesian Data Reweighting

Recent advances directly focus on sensitivity of the posterior  Minsker et al    Miller   Dunson    or the
posterior predictive distribution  Kucukelbir   Blei   
We draw connections to these ideas throughout this paper 
The third theme is data reweighting  This involves designing individual reweighting schemes for speci   tasks and
models  Consider robust methods that toss away  outliers 
This strategy involves manually assigning binary weights
to datapoints  Huber    Another example is covariate shift adaptation importance sampling where reweighting
transforms data to match another target distribution  Veach
  Guibas    Sugiyama et al    Shimodaira   
Wen et al    In contrast  RPMs treat weights as latent
variables  The weights are automatically inferred  no custom design is required  RPMs also connect to ideas around
ensemble learning and boosting  Schapire   Freund   
Boosting procedures reweight datapoints to build an ensemble of predictors for supervised learning  whereas RPMs
apply to Bayesian models in general 

  Reweighted Probabilistic Models
Reweighted probabilistic models  RPM  offer   new approach to robust modeling  The idea is to automatically
identify observations that match the assumptions of the
model and to base posterior inference on these observations 

  De nitions
An RPM scaffolds
nD   yn    

    QN

probabilistic model 
Raise each likelihood to  
latent weight and posit   prior on the weights  This gives
the reweighted joint density

over

 

 
 

 

    pw    

 yn    wn  

           

where   DR     pw    QN

NYnD 
nD   yn    wn dy    dw is
the normalizing factor 
The reweighted density integrates to one when the normalizing factor   is  nite  This is always true when the likelihood
     is an exponential family distribution with Lesbegue
base measure  Bernardo   Smith    this is the class of
models we study in this paper 
RPMs apply to likelihoods that factorize over the observations   We discuss nonexchangeable models in Section  
Figure   depicts an RPM as   graphical model  Speci  
models may have additional structure  such as   separation
of local and global latent variables  Hoffman et al   
or  xed parameters  we omit these in this  gure 

 Heavytailed likelihoods and Bayesian nonparametric priors

may violate this condition  we leave these for future analysis 

  

 

yn

 

    Original probabilistic model

  

 

yn

pw

wn

 

    Reweighted probabilistic model  RPM 

  

 

  

yn

    Localized probabilistic model

 

Figure   RPMs begin with   probabilistic model     and introduce
  set of weights   as latent variables  This gives   model     that
explores which data observations match its assumptions  Localization     instead  builds   hierarchical model   Appendix   shows
when   localized model is also an RPM 

The reweighted model introduces   set of weights  these
are latent variables  each with support wn      To gain
intuition  consider how these weights affect the posterior 
which is proportional to the product of the likelihood of every measurement    weight wn that is close to zero  attens
out its corresponding likelihood  yn    wn    weight that
is larger than one makes its likelihood more peaked  This  in
turn  enables the posterior to focus on some measurements
more than others  The prior pw     ensures that not too
many likelihood terms get  attened  in this sense  it plays
an important regularization role 
We study three options for this prior on weights    bank
of Beta distributions    scaled Dirichlet distribution  and  
bank of Gamma distributions 
Bank of Beta priors  This option constrains each weight as
wn       We posit an independent prior for each weight

pw      

NYnD 

Beta wn        

 

and use the same parameters   and   for all weights  This
is the most conservative option for the RPM  it ensures that
none of the likelihoods ever becomes more peaked than it
was in the original model 

Robust Probabilistic Modeling with Bayesian Data Reweighting

The parameters      offer an expressive language to describe
different attitudes towards the weights  For example  setting
both parameters less than one makes the Beta act like    two
spikes and   slab  prior  encouraging weights to be close to
zero or one  but not in between  As another example  setting
  greater than   encourages weights to lean towards one 
Scaled Dirichlet prior  This option ensures the sum of the
weights equals     We posit   symmetric Dirichlet prior on
all the weights

       

pv      Dirichlet   

 

where   is   scalar parameter and   is          vector of
ones  In the original model  where all the weights are one 
then the sum of the weights is     The Dirichlet option maintains this balance  while certain likelihoods may become
more peaked  others will  atten to compensate 
The concentration parameter   gives an intuitive way to
con gure the Dirichlet  Small values for   allow the model
to easily upor downweight many data observations  larger
values for   prefer   smoother distribution of weights  The
Dirichlet option connects to the bootstrap approaches in
Rubin et al    Kucukelbir   Blei   which also
preserves the sum of weights as    
Bank of Gamma priors  Here we posit an independent
Gamma prior for each weight

NYnD 

pw      

Gamma wn        

 

and use the same parameters   and   for all weights  We
do not recommend this option  because observations can
be arbitrarily upor downweighted  In this paper  we only
consider Equation   for our theoretical analysis in Section  
The bank of Beta and Dirichlet options perform similarly 
We prefer the Beta option as it is more conservative  yet
 nd the Dirichlet to be less sensitive to its parameters  We
explore these options in the empirical study  Section  

  Theory and intuition
How can theory justify Bayesian data reweighting  Here we
investigate its robustness properties  These analyses intend
to con rm our intuition from Section   Appendices   and  
present proofs in full technical detail 
Intuition  Recall the logarithm of the RPM joint density
from Equation  Now compute the maximuma posterior
 MAP  estimate of the weights    The partial derivative is

  log          

 wn

  log pw  wn 

dwn

 

  log  yn    

 

for all                   Plug the Gamma prior from Equation   into the partial derivative in Equation   and set it
equal to zero  This gives the MAP estimate of wn 

 

 

     

    log  yn    

bwn  

the contribution of observations that are unlikely under the
log likelihood  in turn  this encourages the MAP estimate

The MAP estimatebwn is an increasing function of the log
likelihood of yn when       This reveals thatbwn shrinks
forb  to describe the majority of the observations  This is

how an RPM makes   probabilistic model more robust 
  similar argument holds for other exponential family priors
on   with log wn as   suf cient statistic  We formalize this
intuition and generalize it in the following theorem  which
establishes suf cient conditions where   RPM improves the
inference of its latent variables  
Theorem   Denote the true value of   as   Let the posterior mean of   under the weighted and unweighted model
be     and     respectively  Assume mild conditions on
pw    and the corruption level  and that   yn          
 yn        holds    with high probability  Then 
there exists an     such that for         we have
                          where   denotes second order
stochastic dominance   Details in Appendix   

The likelihood bounding assumption is common in robust
statistics theory  it is satis ed for both likely and unlikely
 corrupted  measurements  How much of an improvement
does it give  We can quantify this through the in uence
function  IF  of     
Consider   distribution   and   statistic       to be   function of data that comes iid from    Take    xed distribution 
     the population distribution      Then  IF zI        measures how much an additional observation at   affects the
statistic        De ne
IF zI          lim
   

                             

for   where this limit exists  Roughly  the IF measures the
asymptotic bias on        caused by   speci   observation
  that does not come from     We consider   statistic   to
be robust if its IF is   bounded function of         if outliers
can only exert   limited in uence  Huber   
Here  we study the IF of the posterior mean         under
the true data generating distribution          Say  
value   has likelihood        that is nearly zero  we think
of this   as corrupted  Now consider the weight function
induced by the prior pw     Rewrite it as   function of the
log likelihood  like   log      as in Equation  
Theorem   If lima           and lima     
         then IF zI                as         

 

 

Robust Probabilistic Modeling with Bayesian Data Reweighting

This result shows that an RPM is robust in that its IF goes
to zero for unlikely measurements  This is true for all three
priors   Details in Appendix   

  Inference and computation
We now turn to inferring the posterior of an RPM 
          The posterior lacks an analytic closedform
expression for all but the simplest of models  even if the
original model admits such   posterior for   the reweighted
posterior may take   different form 
To approximate the posterior  we appeal to probabilistic programming    probabilistic programming system enables  
user to write   probability model as   computer program and
then compile that program into an inference executable  Automated inference is the backbone of such systems  it takes
in   probability model  expressed as   program  and outputs
an ef cient algorithm for inference  We use automated inference in Stan    probabilistic programming system  Carpenter
et al   
In the empirical study that follows  we highlight how RPMs
detect and mitigate various forms of model mismatch  As
  common metric  we compare the predictive accuracy on
held out data for the original  localized  and reweighted
model 
The posterior predictive likelihood of   new datapoint

   is poriginal                               Lo 
                             where   is the lo 

calization couples each observation with its own copy
of the latent variable 
this gives plocalized          
calized latent variable for the new datapoint  The prior
       has the same form as    in Equation  
Bayesian data reweighting gives the following posterior
predictive likelihood

pRPM                       pRPM           dw    

where pRPM       is the marginal posterior  integrating out
the inferred weights of the training dataset  and the prior
     has the same form as pw in Equation  

  Empirical Study
We study RPMs under four types of mismatch with reality 
This section involves simulations of realistic scenarios  the
next section presents   recommendation system example
using real data  We default to NoU Turn sampler  NUTS 
 Hoffman   Gelman    for inference in all experiments 
except for Sections   and   where we leverage variational
inference  Kucukelbir et al    The additional computational cost of inferring the weights is unnoticeable relative
to inference in the original model 

  Outliers    network waittime example
  router receives packets over   network and measures
the time it waits for each packet  Suppose we typically
observe waittimes that follow   Poisson distribution with
rate       We model each measurement using   Poisson
likelihood  yn       Poisson  and posit   Gamma prior
on the rate        Gam            
Imagine that     percent of the time  the network fails 
During these failures  the waittimes come from   Poisson
with much higher rate       Thus  the data actually
contains   mixture of two Poisson distributions  yet  our
model only assumes one   Details in Appendix   

truth
prior
original
RPM Beta
RPM Dirichlet
localized

 

 

 

 

 
    Posteriors for       failure rate 

original
RPM Beta
RPM Dirichlet
localized

 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

    Posterior   credible intervals 

Figure   Outliers simulation study  We compare Beta   
and Dir  as priors for the reweighted probabilistic model     
Posterior distributions on   show   marked difference in detecting
the correct waittime rate of           Posterior   con dence
intervals across failure rates   show consistent behavior for both
Beta and Dirichlet priors         with   replications 
How do we expect an RPM to behave in this situation 
Suppose the network failed   of the time  Figure   
shows the posterior distribution on the rate   The original
posterior is centered at   this is troubling  not only because
the rate is wrong but also because of how con dent the
posterior    is  Localization introduces greater uncertainty 
yet still estimates   rate around   The RPM correctly
identi es that the majority of the observations come from
      Observations from when the network failed are
downweighted  It gives   con dent posterior centered at
 ve 
Figure    shows posterior   credible intervals of   under
failure rates up to       The RPM is robust to corrupted measurements  instead it focuses on data that it can

Robust Probabilistic Modeling with Bayesian Data Reweighting

explain within its assumptions  When there is no corruption 
the RPM performs just as well as the original model 
Visualizing the weights elucidates this point  Figure   shows
the posterior mean estimates of   for       The
weights are sorted into two groups  for ease of viewing 
The weights of the corrupted observations are essentially
zero  this downweighting is what allows the RPM to shift
its posterior on   towards  ve 

 
 
 
 
 
 

 

 

 

 

 

 

 

Data Index

Figure   Posterior means of the weights   under the Dirichlet
prior  For visualization purposes  we sorted the data into two
groups  the  rst   contain observations from the normal network 
the remaining   are the observations when the network fails 

Despite this downweighting  the RPM posteriors on   are
not overdispersed  as in the localized case  This is due to the
interplay we described in the introduction  Downweighting
observations should lead to   smaller effective sample size 
which would increase posterior uncertainty  But the downweighted datapoints are corrupted observations  including
them also increases posterior uncertainty 
The RPM is insensitive to the prior on the weights  both
Beta and Dirichlet options perform similarly  From here on 
we focus on the Beta option  We let the shape parameter
  scale with the data size   such that         this
encodes   mild attitude towards unit weights  We now move
on to other forms of mismatch with reality 

  Missing latent groups  predicting color blindness
Color blindness is unevenly hereditary  it is much higher for
men than for women  Boron   Boulpaep    Suppose
we are not aware of this fact  We have   dataset of both genders with each individual   color blindness status and his her
relevant family history  No gender information is available 
Consider analyzing this data using logistic regression  It can
only capture one hereditary group  Thus  logistic regression
misrepresents both groups  even though men exhibit strong
heredity  In contrast  an RPM can detect and mitigate the
missing group effect by focusing on the dominant hereditary
trait  Here we consider men as the dominant group 
We simulate this scenario by drawing binary indicators of
color blindness yn   Bernoulli    exp pn  where
the pn   come from two latent groups  men exhibit  
stronger dependency on family history  pn    xn  than
women  pn    xn  We simulate family history as

xn   Unif    Consider   Bayesian logistic regression model without intercept  Posit   prior on the slope as
             and assume   Beta    prior on
the weights   Details in Appendix   

 

 

 

 men
original
RPM
localized

 

 

 

 

 

 

Figure   Missing latent groups study  Posterior   credible
intervals for the RPM always include the dominant  men    
as we vary the percentage of females in the data  Dataset size
      with   replications 
Figure   shows the posterior   credible intervals of  
as we vary the percentage of females from       to
    horizontal line indicates the correct slope for the
dominant group   men     As the size of the missing
latent group  women  increases  the original model quickly
shifts its credible interval away from   The reweighted
and localized posteriors both contain  men     for all
percentages  but the localized model exhibits much higher
variance in its estimates 
This analysis shows how RPMs can mitigate the effect of
missing latent groups  While the original logistic regression
model would perform equally poorly on both groups  an
RPM is able to automatically focus on the dominant group 

Corrupted       
Clean       

 
 
 
 
 
 
 

 

 

Ep wjy   

Figure   Kernel density estimate of the distribution of weights
across all measurements in the missing latent groups study  The
percentage of females is denoted by       hypothetical clean
dataset receives weights that concentrate around one  the actual
corrupted dataset exhibits   twohump distribution of weights 

An RPM also functions as   diagnostic tool to detect mismatch with reality  The distribution of the inferred weights
indicates the presence of datapoints that defy the assumptions of the original model  Figure   shows   kernel density
estimate of the inferred posterior weights    hypothetical
dataset with no corrupted measurements receives weights
close to one  In contrast  the actual dataset with measurements from   missing latent group exhibit   bimodal distribution of weights  Testing for bimodality of the inferred
weights is one way in which an RPM can be used to diagnose
mismatch with reality 

Robust Probabilistic Modeling with Bayesian Data Reweighting

True structure

Model structure

                     
                   
             

 

             
             
       

Original
mean std 
 
 
 

RPM

mean std 
 
 
 

Localization
mean std 
 
 
 

Table   RPMs improve absolute deviations of posterior mean   estimates    replications 

  Covariate dependence misspeci cation    lung

cancer risk study

Consider   study of lung cancer risk  While tobacco usage
exhibits   clear connection  other factors may also contribute 
For instance  obesity and tobacco usage appear to interact 
with evidence towards   quadratic dependence on obesity
 Odegaard et al   
Denote tobacco usage as    and obesity as    We study
three models of lung cancer risk dependency on these covariates  We are primarily interested in understanding the
effect of tobacco usage  thus we focus on   the regression
coef cient for tobacco  In each model  some form of covariance misspeci cation discriminates the true structure from
the assumed structure 
For each model  we simulate   dataset of size       with
random covariates            and           
and regression coef cients     Unif   
Consider   Bayesian linear regression model with prior
              Details in Appendix   
Table   summarizes the misspeci cation and shows absolute
differences on the estimated   regression coef cient  The
RPM yields better estimates of   in the  rst two models 
These highlight how the RPM leverages datapoints useful for
estimating   The third model is particularly challenging
because obesity is ignored in the misspeci ed model  Here 
the RPM gives similar results to the original model  this
highlights that RPMs can only use available information 
Since the original model lacks dependence on    the RPM
cannot compensate for this 

  Predictive likelihood results
Table   shows how RPMs also improve predictive accuracy 
In all the above examples  we simulate test data with and
without their respective types of corruption  RPMs improve
prediction for both clean and corrupted data  as they focus
on data that match the assumptions of the original model 

  Skewed data  cluster selection in   mixture model
Finally  we show how RPMs handle skewed data  The
Dirichlet process mixture model  DPMM  is   versatile
model for density estimation and clustering  Bishop   

Murphy    While real data may indeed come from
   nite mixture of clusters  there is no reason to assume
each cluster is distributed as   Gaussian  Inspired by the
experiments in Miller   Dunson   we show how  
reweighted DPMM reliably recovers the correct number of
components in   mixture of skewnormals dataset 

    Original model

    RPM

Figure      nite approximation DPMM to skewnormal distributed
data that come from three groups  The shade of each cluster
indicates the inferred mixture proportions       
  standard Gaussian mixture model  GMM  with large  
and   sparse Dirichlet prior on the mixture proportions is
an approximation to   DPMM  Ishwaran   James   
We simulate three clusters from twodimensional skewnormal distributions and      GMM with maximum      
Here we use automatic differentiation variational inference
 ADVI  as NUTS struggles with inference of mixture models  Kucukelbir et al     Details in Appendix   
Figure   shows posterior mean estimates from the original
GMM  it incorrectly  nds six clusters  In contrast  the RPM
identi es the correct three clusters  Datapoints in the tails
of each cluster get downweighted  these are datapoints that
do not match the Gaussianity assumption of the model 

  Case Study  Poisson factorization for

recommendation

We now turn to   study of real data    recommendation
system  Consider   video streaming service  data comes as  
binary matrix of users and the movies they choose to watch 
How can we identify patterns from such data  Poisson
factorization  PF  offers    exible solution  Cemgil   
Gopalan et al    The idea is to infer   Kdimensional

Robust Probabilistic Modeling with Bayesian Data Reweighting

Outliers

Clean
 
 
 

Corrupted
 
 
 

Missing latent groups Misspeci ed structure
Corrupted
Clean
 
 
 
 
 
 

Corrupted
 
 
 

Clean
 
 
 

Original model
Localized model
RPM

Table   Posterior predictive likelihoods of clean and corrupted test data  Outliers and missing latent groups have       The
misspeci ed structure is missing the interaction term  Results are similar for other levels and types of mismatch with reality 

Average log likelihood

Corrupted users

 

 

 
Original model      
RPM      

Table   Heldout predictive accuracy under varying amounts of
corruption  Heldout users chosen randomly   of total users 

latent space of user preferences   and movie attributes  
The inner product   determines the rate of   Poisson
likelihood for each binary measurement  Gamma priors
on   and   promote sparse patterns  As   result  PF  nds
interpretable groupings of movies  often clustered according
to popularity or genre   Full model in Appendix   
How does classical PF compare to its reweighted counterpart  As input  we use the MovieLens    dataset  which
contains one million movie ratings from     users on
    movies  We place iid Gamma    priors on the
preferences and attributes  Here  we have the option of
reweighting users or items  We focus on users and place  
Beta    prior on their weights  For this model  we use
MAP estimation   Localization is computationally challenging for PF  it requires   separate  copy  of   for each movie 
along with   separate   for each user  This dramatically
increases computational cost 

 

 

 
 
 
 
 
 

 

 

 

clean

    Original dataset

   

 

Ratio of corruption    
    Corrupted users

Figure   Inferred weights for clean and corrupted data      Most
users receive weights very close to one      Corrupted users receive
weights much smaller than one  Larger ratios of corruption  
imply lower weights 

We begin by analyzing the original
 clean  dataset 
Reweighting improves the average heldout log likelihood
from   of the original model to   of the corre 

sponding RPM  The boxplot in Figure    shows the inferred
weights  The majority of users receive weight one  but   few
users are downweighted  These are  lm enthusiasts who
appear to indiscriminately watch many movies from many
genres   Appendix   shows an example  These users do
not contribute towards identifying movies that go together 
this explains why the RPM downweights them 
Recall the example from our introduction    child typically
watches popular animated  lms  but her parents occasionally
use her account to watch horror  lms  We simulate this by
corrupting   small percentage of users  We replace   ratio
          of these users  movies with randomly
selected movies 
The boxplot in Figure    shows the weights we infer for
these corrupted users  based on how many of their movies
we randomly replace  The weights decrease as we corrupt
more movies  Table   shows how this leads to higher heldout predictive accuracy  downweighting these corrupted
users leads to better prediction 

  Discussion
Reweighted probabilistic models  RPM  offer   systematic
approach to mitigating various forms of mismatch with
reality  The idea is to raise each data likelihood to   weight
and to infer the weights along with the hidden patterns 
We demonstrate how this strategy introduces robustness and
improves prediction accuracy across four types of mismatch 
RPMs also offer   way to detect mismatch with reality  The
distribution of the inferred weights sheds light onto datapoints that fail to match the original model   assumptions 
RPMs can thus lead to new model development and deeper
insights about our data 
RPMs can also work with nonexchangeable data  such
as time series  Some time series models admit exchangeable likelihood approximations  Guinness   Stein   
For other models    nonoverlapping windowing approach
would also work  The idea of reweighting could also extend
to structured likelihoods  such as Hawkes process models 

Robust Probabilistic Modeling with Bayesian Data Reweighting

Acknowledgements
We thank Adji Dieng  Yuanjun Gao  Inchi Hu  Christian
Naesseth  Rajesh Ranganath  Francisco Ruiz  and Dustin
Tran for their insightful comments  This work is supported
by NSF IIS  ONR    DARPA
PPAML FA  DARPA SIMPLEX   
    and the Alfred    Sloan Foundation 

References
Berger  James    Moreno  El as  Pericchi  Luis Raul  Bayarri    Jes    Bernardo  Jos     Cano  Juan    De la
Horra  Juli    Mart    Jacinto    osIns    David  Betr 
Bruno  et al  An overview of robust Bayesian analysis 
Test     

Bernardo  Jos    and Smith  Adrian FM  Bayesian Theory 

John Wiley   Sons   

Bishop  Christopher    Pattern Recognition and Machine

Learning  Springer New York   

Boron  Walter   and Boulpaep  Emile    Medical Physiol 

ogy  Elsevier   

Carpenter  Bob  Gelman  Andrew  Hoffman  Matt  Lee 
Daniel  Goodrich  Ben  Betancourt  Michael  Brubaker 
Marcus    Guo  Jiqiang  Li  Peter  and Riddell  Allen 
Stan    probabilistic programming language  Journal of
Statistical Software   

Cemgil  Ali Taylan  Bayesian inference for nonnegative
matrix factorisation models  Computational Intelligence
and Neuroscience   

de Finetti  Bruno  The Bayesian approach to the rejection
of outliers  In Proceedings of the Fourth Berkeley Symposium on Probability and Statistics   

Efron  Bradley  LargeScale Inference  Cambridge Univer 

sity Press   

Feng  Jiashi  Xu  Huan  Mannor  Shie  and Yan  Shuicheng 
Robust logistic regression and classi cation  In NIPS 
 

Ghahramani  Zoubin  Probabilistic machine learning and
arti cial intelligence  Nature     

Gopalan  Prem  Hofman  Jake    and Blei  David    Scalable recommendation with hierarchical Poisson factorization  UAI   

Guinness  Joseph and Stein  Michael    Transformation to
approximate independence for locally stationary Gaussian processes  Journal of Time Series Analysis   
   

Hoffman  Matthew   and Gelman  Andrew  The NoU Turn
sampler  Journal of Machine Learning Research   
   

Hoffman  Matthew    Blei  David    Wang  Chong  and
Paisley  John  Stochastic variational inference  The Journal of Machine Learning Research   
 

Huber  Peter    Robust regression  asymptotics  conjectures
and Monte Carlo  The Annals of Statistics  pp   
 

Huber  Peter    Robust Statistics  Springer   

Insua  David   os and Ruggeri  Fabrizio  Robust Bayesian

Analysis  Springer Science   Business Media   

Ishwaran  Hemant and James  Lancelot    Approximate
Dirichlet process computing in  nite normal mixtures 
Journal of Computational and Graphical Statistics   

Kucukelbir  Alp and Blei  David    Population empirical

Bayes  In UAI   

Kucukelbir  Alp  Tran  Dustin  Ranganath  Rajesh  Gelman 
Andrew  and Blei  David    Automatic differentiation
variational inference  Journal of Machine Learning Research     

Mansinghka  Vikash  Selsam  Daniel  and Perov  Yura  Venture    higherorder probabilistic programming platform
with programmable inference  arXiv   

McWilliams  Brian  Krummenacher  Gabriel  Lucic  Mario 
and Buhmann  Joachim    Fast and robust least squares
estimation in corrupted linear models  In NIPS   

Miller  Jeffrey   and Dunson  David    Robust Bayesian inference via coarsening  arXiv preprint arXiv 
 

Minsker  Stanislav  Srivastava  Sanvesh  Lin  Lizhen  and
Dunson  David    Robust and scalable Bayes via  
median of subset posterior measures  arXiv preprint
arXiv   

Murphy  Kevin    Machine Learning    Probabilistic Per 

spective  MIT Press   

Neyman  Jerzy  On the problem of estimating the number of
schools of  sh  In    Neyman     Loeve and Yerushalmy 
    eds  University of California Publications in Statistics  volume   chapter   pp    University of California Press   

Odegaard  Andrew    Pereira  Mark    Koh  WoonPuay 
Gross  Myron    Duval  Sue  Mimi    Yu  and Yuan 
JianMin  BMI  allcause and causespeci   mortality in

Robust Probabilistic Modeling with Bayesian Data Reweighting

Chinese Singaporean men and women  PLoS One   
 

Provost  Foster and Fawcett  Tom  Robust classi cation
for imprecise environments  Machine Learning   
   

Rubin  Donald   et al  The Bayesian bootstrap  The annals

of statistics     

Schapire       and Freund     Boosting  Foundations and

Algorithms  MIT Press   

Sha eezadehAbadeh  Soroosh  Esfahani  Peyman Mohajerin  and Kuhn  Daniel  Distributionally robust logistic
regression  In NIPS   

Shimodaira  Hidetoshi  Improving predictive inference under covariate shift by weighting the loglikelihood function  Journal of Statistical Planning and Inference   
   

Song  Qing  Hu  Wenjie  and Xie  Wenfang  Robust support vector machine with bullet hole image classi cation 
Systems  Man  and Cybernetics  Part    Applications and
Reviews  IEEE Transactions on     

Sugiyama  Masashi  Krauledat  Matthias  and   ller 
KlausRobert  Covariate shift adaptation by importance
weighted cross validation  Journal of Machine Learning
Research     

Veach  Eric and Guibas  Leonidas    Optimally combining sampling techniques for Monte Carlo rendering  In
Proceedings of the  nd annual conference on Computer
graphics and interactive techniques  pp    ACM 
 

Wang  Chong and Blei  David      general method
arXiv preprint

robust Bayesian modeling 

for
arXiv   

Wen  Junfeng  Yu  Chunnam  and Greiner  Russell  Robust learning under uncertain test distributions  Relating
covariate shift to model misspeci cation  In ICML   

Yu  Yaoliang  Aslan   zlem  and Schuurmans  Dale   
In NIPS 

polynomialtime form of robust regression 
 

