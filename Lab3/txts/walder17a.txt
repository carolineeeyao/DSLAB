Fast Bayesian Intensity Estimation for the Permanental Process

Christian    Walder     Adrian    Bishop      

Abstract

The Cox process is   stochastic process which
generalises the Poisson process by letting the underlying intensity function itself be   stochastic
process  In this paper we present   fast Bayesian
inference scheme for the permanental process 
  Cox process under which the square root of
the intensity is   Gaussian process  In particular we exploit connections with reproducing kernel Hilbert spaces  to derive ef cient approximate Bayesian inference algorithms based on the
Laplace approximation to the predictive distribution and marginal likelihood  We obtain   simple
algorithm which we apply to toy and realworld
problems  obtaining orders of magnitude speed
improvements over previous work 

  Introduction
The Poisson process is an important model for point data
in which samples of the process are locally  nite subsets
of some domain such as time or space  The process is
parametrised by an intensity function  the integral of which
gives the expected number of points in the domain of integration   for   gentle introduction we recommend  Baddeley    In the typical case of unknown intensity function we may place   nonparametric prior over it via      the
Gaussian Process  GP  and perform Bayesian inference 
Inference under such models is challenging due to both the
GP prior and the non factorial nature of the Poisson process
likelihood   which includes an integral of the intensity
function  One may resort to discretising the domain  Rathbun   Cressie      ller et al    Rue et al   
or performing Monte Carlo approximations  Adams et al 
  Diggle et al    Fast Laplace approximates were
studied in  Cunningham et al    Illian et al   
Flaxman et al    and variational methods were applied

 Data  CSIRO  Australia  The Australian National University  University of Technology Sydney  Correspondence to 
Christian  christian walder anu edu au 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

in  Lloyd et al    Kom Samo   Roberts   
To satisfy nonnegativity of the intensity function one
transforms the GP prior  The logGaussian Cox Process 
with GP distributed log intensity  has been the subject of
much study  see     
 Rathbun   Cressie      ller
et al    Illian et al    Diggle et al    Alternative formulations for introducing   GP prior exist      
 Adams et al    More recent research has highlighted
the analytical and computational advantages  Lloyd et al 
    Flaxman et al      ller et al    of the
permanental process  which has GP distributed square root
intensity  Shirai   Takahashi    McCullagh     ller 
    we discuss the relationship between these methods and the present work in more detail in subsection  
In section   we introduce the Poisson and permanental processes  and place our work in the context of existing literature  Section   reviews Flaxman et al    slightly
recasting it as regularised maximum likelihood for the permanental process  Our Bayesian scheme is then derived in
section   In section   we discuss the choice of covariance
function for the GP prior  before presenting some numerical experiments in section   and concluding in section  

  The Model
  The Poisson Process

We view the inhomogeneous Poisson process on   as  
distribution over locally  nite subsets of   The number        of elements in some       is assumed to
be distributed as Poisson       where        
            gives the mean of the Poisson distribution  It turns out that this implies the likelihood function

 cid 

  cid 

   xi  

        

 xi  exp    

 

  

  Latent Gaussian Process Intensities

To model unknown     we employ   nonparametric
prior over functions  namely the Gaussian process  GP 
To ensure that   is nonnegative valued we include   deterministic  link  function            so that we have
the prior over   de ned by           and     GP   
where   is the covariance function for    The most com 

Fast Bayesian Permanental Processes

 cid  

   xi  for some     The function    may be expressed in terms of the Mercer expansion  Mercer   

     

  cid 

  

         

           

 

mon choice for   is the exponential function exp  leading to the logGaussian Cox process  LGCP     ller et al 
  Recently Adams et al    employed the transformation            exp      which permits ef 
 cient sampling via thinning  Lewis   Shedler    due
to the bound            

  PERMANENTAL PROCESSES  SQUARED LINK

FUNCTION

     known
In this paper we focus on the choice         
as the permanental process  Shirai   Takahashi    McCullagh     ller    Two recent papers have demonstrated the analytical and computational advantages of this
link function 

  Flaxman et al    derived   nonprobabilistic regularisation based algorithm which we review in section   and which exploited properties of reproducing kernel Hilbert spaces  The present work generalises their result  providing probabilistic predictions
and Bayesian model selection  Our derivation is by
necessity entirely different to Flaxman et al    as
their representer theorem  Sch olkopf et al    argument is insuf cient for our probabilistic setting  see
     subsubsection  

   Lloyd et al    derived   variational approximation to   Bayesian model with the squared link function  based on an inducing variable scheme similar to
 Titsias    and exploiting the tractability of certain required integrals  The present work has the advantage of   not requiring the inducing point approximation    being free of nonclosed form expressions
such as their    and   being simpler to implement
and orders of magnitude faster in practice while  as
we demonstrate  exhibiting comparable predictive accuracy 

  Regularised Maximum Likelihood
Flaxman et al    combined   with the regularisation
term  cid   cid      leading to the regularised maximum likeli 
 cid 
hood estimator for    namely     
 cid 

 cid cid   cid 
 cid 

      cid   cid     
 cid   cid     

   xi     
 

  cid 

 cid cid 

argmax

log

 
 

  

 

 

 

where we have implicitly de ned the new RKHS
                Now  provided we can compute
the associated new reproducing kernel     then we may
appeal to the representer theorem  Kimeldorf   Wahba 
  in order to compute the     which takes the form

wi         

where    are orthonormal in      To satisfy for arbii wi   the reproducing property  Aronszajn 

trary      cid 
 cid 
 cid 
  Hence   cid cid 
  we have  cid cid 

    

 

 

wi  

 cid 

 

    

         

 cid 
  wi   cid         cid 
  wi   cid       cid 
  cid 

 

     

 

  

we let    be orthogonal in      obtaining  cid       cid   
 ij 
      and from
    
       

  so

    

 

          

         

 

For approximate Bayesian inference however  we cannot
simply appeal to the representer theorem 

  Approximate Bayesian Inference
In subsection    of the supplementary material  we review the standard Laplace approximation to the GP with
nonGaussian likelihood  This   useful setup for what follows  but is not directly generalisable to our case due to the
integral in   Instead  in subsection   we now take  
different approach based on the Mercer expansion 

  Laplace Approximation
It is tempting to na vely substitute    into subsection    of
the supplementary material  and to neglect the integral part
of the likelihood  Indeed  this gives the correct approximate
predictive distribution  The marginal likelihood does not
work in this way however  due to the log determinant in
  We now perform   more direct analysis 

  MERCER EXPANSION SETUP

Mercer   theorem allows us to write   where for nondegenerate kernels        Assume   linear model in
             so that 

          cid   

 
and let           where        ii is   diagonal covariance matrix  This is equivalent to     GP    because

cov                  cid               

 We use   sloppy notation where      is the ith element of  

while  xi   is   vector with ith element xi  etc 

Fast Bayesian Permanental Processes

Figure   Test function   of subsection   The vertical grey lines are the input data points  sampled from the ground truth intensity
depicted by the heavy grey line  We plot two samples from the approximate predictive distribution  black lines  along with the median
 solid red  and     interval  lled red  The GP prior is that of subsection   with hyperparameters       number of cosine
frequencies       and the remaining      chosen to maximise the marginal likelihood 

log              

Recall that the Poisson process on   with intensity      
        has likelihood for      xi  
 
 cid 
 cid 

         

   xi 

   
 

 cid cid 

  

  cid 
 cid 

  

 cid 

log

 
 

 cid cid 

 cid 

  

  cid  

 log       

The joint in      is
log           
  log         
 

  cid       
 

log    
 

log  

  LAPLACE APPROXIMATION

We make   Laplace approximation to the posterior  which
is the normal distribution
log               log             
     
        cid              
 
 

log        
 

log  

 

  log            
where    is chosen as the mode of the true posterior  and  
is the inverse Hessian of the true posterior  evaluated at    

  PREDICTIVE MEAN

The mode    is

     argmax

 

log            

  argmax

 

log           
 

  cid        

 

Crucially     must satisfy the stationarity condition
               log              

 

where

   log                

  cid 

  

 xi 
 xi cid    

 

The approximate predictive mean is therefore

                        
     cid    
 

  cid 
  cid 

  

 

 

 xi cid    

   xi    

  

   xi cid        

  

This reveals the same    as   From   we have

           xi 

Putting     and   into   we obtain

  cid 

    argmin

 

  

log  

   

 cid     

 
 

 

 

where         xi  xj ij  This is equivalent to Flaxman
et al    though slightly simpli ed by   Interestingly  unlike Flaxman et al     or the analogous section   we did not appeal to the representer theorem 

 inputdomain Poissonintensity datalocationsxitrueintensity   predictivemedian pred intervalpredictivesamplesFast Bayesian Permanental Processes

  PREDICTIVE VARIANCE

We now compute the   in   The Hessian term giving the
inverse covariance becomes

 cid cid cid cid      
 cid cid cid cid      

      

 

     cid  log             

           

     

 

     cid  log       
  cid 

 xi xi cid 
 xi cid     

   

  

    DV  cid 

Similarly to   we get approximate marginal likelihood
log           log                log              
 cid 
  log         
 cid  
   log  
 

  log  log    cid 

   cid         

 cid         

   
 

 cid cid 

 cid cid 

 cid 

 cid 

 cid 

 cid 

 

We now use the determinant identity        DV  cid   
        cid          with the same      and   as
subsubsection   to derive

  log     log         log     log        DV  cid 

    log cid cid      cid cid    log cid cid         cid     cid cid     
 cid cid cid          cid   cid      
 cid cid cid      
  cid 
 cid 

  log

 cid cid 

      

 cid 

log

  

 

 

     

 
where       log           is the crucial ingredient 
not accounted for by na vely putting    into subsection   

  Covariance Functions
To apply our inference scheme we need to compute 

  The function    from equation   studied recently
by Flaxman et al    and earlier by Sollich  
Williams   as the equivalent kernel 

  The associated term          from equation  

required for the marginal likelihood 

This is often challenging for compact domains such as the
unit hypercube  Such domains are crucial however  if we
are to avoid the wellknown edgeeffects which arise from
neglecting the fact that our data are sampled from  say   
two dimensional rectangle  In subsection   we provide  
simple constructive approach to the case           The
following subsection   presents the general approximation scheme due to Flaxman et al    for the case when
we have   but not its Mercer expansion 

  ThinPlate Seminorms on the HyperCube

Consider the input domain          with Lebesgue measure     classical function regularisation term is the so
called mth order thinplate spline seminorm 

 cid 

  cid 

 cid 

     

  

  
   cid     mg cid        

 mf
   

 mg
        

 

 cid      cid         

where             xi  and      
      Rm    The
approximate predictive variance can now be rewritten as an
mdimensional matrix expression using the identity     
  DV  cid               cid            cid    with
and           along with   little algebra  to derive 

      Var              
     cid      

            cid          cid   cid   cid cid   cid          cid 
plication  and    cid          cid   cid       cid 

where  cid  is the Hadamard product  or elementwise multi 

  PREDICTIVE DISTRIBUTION

the

predictive

approximate
Given
distribution
                                     and the
relation      
      it is straightforward to derive the
corresponding              Gamma    where
the shape      
 

  and the scale      

 

  MARGINAL LIKELIHOOD
Letting              be
expansion of
log            about the mode        and evaluating at    gives  as linear and quadratic terms vanish 

the Taylor

log                log             

  log           
 

   cid         
 

log    
 

log  

 Where               is an       matrix of evaluations of    
 Gamma      has       

        exp   

 

Fast Bayesian Permanental Processes

    Decomposition of the marginal likelihood

    Predictive mean intensity 

Figure   Modeling the Redwood dataset      the log marginal likelihood log         along with its component terms from  
and   as   function of the hyperparameter   from subsection       the mean intensity corresponding to the maximum marginal
likelihood parameters  with isolines at         and   See subsection   for the details 

order      cid 

Here   is   multiindex running over all indices of total
          and the boundary conditions
  come from formal integration  see      Wahba  
section   We neglect    for reasons explained shortly 
and include the zeroth derivative to de ne

 cid      cid         cid               cid   

We may select the free parameters             and
       using the maximum marginal likelihood criterion 
In general  it is challenging to obtain the expressions we require in closed form for arbitrary      and    The analytical limit in the literature appears to be the case       with
dimension       along with socalled Neumann boundary conditions  which impose   vanishing gradient on the
boundary  Sommerfeld   Straus    That    has been
derived in closed form as the reproducing kernel of an associated Sobolev space by ThomasAgnan  
We now present   simple but powerful scheme which sidesteps these challenges via   well chosen series expansion 
Consider the basis function

         

    

cos jxj 

  

where   is   multiindex with nonnegative  integral  values  and   denotes the indicator function  which is one if
the condition is satis ed and zero otherwise  The   form
  convenient basis for our purposes  They are orthonormal 

 cid   cid           

and also eigenfunctions of our regularisation operator with

  cid 

 cid 

           

 
 

   

 

 

 cid  

 cid 

 cid 

  cid    cid 

  

Now if we restrict the function space to
      
      cid   cid       

   

 cid 

 cid 

 

     cid 

 

  

 cid 

 

then it is easily veri ed that the boundary conditions   in
  vanish  This is   common approach to solving partial
differential equations with Neumann boundary conditions
 see      Sommerfeld   Straus   By restricting in
this way  we merely impose zero partial derivatives at the
boundary  while otherwise enjoying the usual Fourier series approximation properties  Hence we can combine the
reproducing property   with   and   to derive

 cid 

 

 cid  

    
 

    

         

where        cid cid  

     

 

The above covariance function is not required for our inference algorithm  Rather  the point is that since the basis
is also orthonormal  we may substitute   and   into  
and   to obtain    and      as required 

Series truncation  We have discovered closed form expressions for    only for       and       In practice
we may truncate the series at any order and still obtain  
valid model due to the equivalence with the linear model
  Hence    large approximation error  in terms of    
due to truncation may be irrelevant from   machine learning perspective  merely implying   different GP prior over
functions  Indeed  the maximum marginal likelihood criterion based on subsubsection   may guide the selection of an appropriate truncation order  although some care
needs to be taken in this case 

 parametersaandb logp     andcomponentterms cid       log cid cid cid       cid cid   cid cid cid     mlog logh     logp     maximum   Arbitrary Covariances and Domains

Fast Bayesian Permanental Processes

Flaxman et al    suggested the following approximation for     for the case when   is known but the associated
Mercer expansion is not  The approximation is remarkably general and elegant  and may even be applied to nonvectorial data by employing  say    kernel function de ned
on strings  Lodhi et al    The idea is to note that the
       pairs are eigenfunctions of the integral operator  see
Rasmussen   Williams   section  

Tk              
   cid  Tkf  

 cid 

  

              dx 

where   is related to   of the previous subsection by
 cid 
           dx  The Nystr om approximation  Nystr om 
  to Tk draws   samples   from   and de nes
             Then the eigenfunctions
     
 
and eigenvectors of Tk may be approximated via the eigenand eigenvalues  mat 
vectors   mat 

of           as

     
 

 

 

 

  mat 

 

       mat 

 

   
 
   
 

 
   mat 

 

   

These approximations may be used for     as in  Flaxman
et al    as well as our         

measure   cid 

  Experiments
  Setup
Evaluation We use two metrics  The  cid  Error is the
squared difference to the ground truth        the Lebesgue
        true    dx  The test log likelihood is the logarithm of   at an independent
test
sample  one sample being   set of points      
  sample from the process  which we summarise by averaging over    nite number of test sets  for real data where
the ground truth intensity is unknown  and otherwise  if
we have the ground truth  by the analytical expression
EX PP 

log pX PP   

 cid 

 

 cid 

    log          

dx 

 cid 
 cid 

 cid 

  

where       is the process with intensity    see the supplementary subsection    This evaluation metric is
novel in this context  yet more accurate and computationally cheaper than the sampling of       Adams et al   

Decision Theory The above metrics are functions of  
single estimated intensity  In all cases we use the predictive
mean intensity for evaluation  We demonstrate in subsection    of the supplementary material that this is optimal

     cid  error vs  marginal likelihood 

    Expected logloss vs  marginal likelihood 

Figure   The relationship between the log marginal likelihood
and the  cid  error  both scaled and shifted to     on the benchmark problems of subsection     see  gure   for the details 

for the expected test log likelihood evaluation  the  cid  error
cases is similar as is trivial to show 

Algorithms We compare our new Laplace Bayesian
Point Process  LBPP  with two covariances 
the cosine
kernel of subsection   with  xed       and hyperparameters   and    LBPPCos  and the Gaussian kernel
            exp          with the method of
subsection    LBPPG  We compared with the Variational Bayesian Point Process  VBPP   Lloyd et al   
using the same Gaussian kernel  LBPPG and VBPP use
  regular grid for    of subsection   and the inducing
points  respectively  To compare timing we vary the number of basis functions       the number of grid points for
LBPPG and VBPP  and cosine terms for LBPPCos  We
include the baseline kernel smoothing with edge correction
 KS EC  method  Diggle    Lloyd et al    All

 logp     normalised cid Error normalised logp     normalised ExpectedTestLogLikelihood normalised Fast Bayesian Permanental Processes

Figure   Mean   one standard error performance on the toy problems of subsection   as   function of the number of basis functions
 with KS EC results replicated along the horizontal axis for comparison  The vertical axes of each  gure are normalised scores  see
subsubsection   for the details 

inference is performed with maximum marginal likelihood 
except for KS EC where we maximise the leave one out
metric described in  Lloyd et al   

     Toy Examples

We drew  ve toy intensities                as  
      where  
was sampled from the GP of Gaussian covariance  de ned
above  with       and       Figure   depicts     see
the caption for   description  The remaining test functions
are shown in  gure   of the supplementary material 

  MODEL SELECTION
As the marginal likelihood log         is   key advantage of our method over the nonprobabilistic approach
of Flaxman et al    we investigated its ef cacy for
model selection  Figure   plots log         against our
two error metrics  both rescaled to     for effective visualisation  based on   single training sample per test function  We observe   strong relationship  with larger values of
log         generally corresponding to lower error  This
demonstrates the practical utility of both the marginal likelihood itself  and our Laplace approximation to it 

  EVALUATION

We sampled   training sets from each of our  ve toy
functions  Figure   shows our evaluation metrics along
with the  tting time as   function of the number of basis functions  For visualisation all metrics  including   
time  are scaled to     by dividing by the maximum for
the given test function  over data replicates and algorithms 
LBBPG and and VBPP achieve the best performance  but
our LBPPG is two orders of magnitude faster  Our KS EC
implementation follows the methodology of Lloyd et al 
  we    the kernel density bandwidth using average leave one out log likelihood  This involves   quadratic
number of log        of the truncated normal calculations 

and logsum exp calculations  both of which involve large
time constants  but are asymptotically superior to the other
methods we considered  LBBPCos is slightly inferior in
terms of expected test log likelihood  which is expected due
to the toy functions having been sampled according to the
same Gaussian kernel of LBPPG and VBPP  as well as the
density estimator of KS EC 

  Real Data

We compared the methods on three real world datasets 

  coal    points in one temporal dimension  indicating the time of fatal coal mining accidents in the
United Kingdom  from   to    Collins   
  redwood    California redwood tree locations from

  square sampling region  Ripley   

  cav    caveolae locations from   square sampling

region of muscle  ber  Davison   Hinkley   

  Computational Speed

Similarly to subsubsection   we evaluate the  tting
speed and statistical performance vs  number of basis functions   see  gure   We omit the  cid  error as the ground
truth is unknown  Instead we generate   test problems
by each time randomly assigning each original datum to either the training or the testing set with equal probability 
Again we observe similar predictive performance of LBPP
and VBPP  but with much faster    times for our LBPP  Interestingly LBPPCos slightly outperform LBPPG 

     California Redwood Dataset

We conclude by further investigating the redwood dataset 
Once again we employed the MLII procedure to determine
  and     xing       for the covariance function of subsection   using the lowest   cosine frequencies in each

 NormalisedMetricExpectedTestLogLikelihood Numberofbasisfunctions inducingpoints cid Error FitTimeKS ECLBPPCosLBPP GVBPPFast Bayesian Permanental Processes

    cas dataset

    coal dataset

Figure   Mean   one standard error performance of the different methods on real data  as   function of the number of basis functions
 with KS EC results replicated along the horizontal axis for comparison  See subsection   for the details 

    redwood dataset

dimension for   total of       basis functions in the expansion   For ease of visualisation we also  xed       
Figure   plots the results  including   decomposition of the
log marginal likelihood log         and   visualisation
of the predictive mean  The mean function strongly resembles the result presented by Adams et al    where
computationally expensive MCMC was employed 
The decomposition of the marginal likelihood on the left
of  gure   provides insight into the role of the individual
terms in   and   which make up log         In
particular  the term          from   acts as   regulariser  guarding against over tting  and balancing against
the data term   of  

  Conclusion
We have discussed the permanental process  which places
  Gaussian Process prior over the square root of the inten 

sity function of the Poisson process  and derived the equations required for empirical Bayes under   Laplace posterior approximation  Our analysis provides   an alternative derivation and probabilistic generalization of  Flaxman
et al    and   an ef cient and easier to implement
alternative which does not rely on inducing inputs  but
rather reproducing kernel Hilbert space theory  to the related Bayesian approach of Lloyd et al    This further
demonstrates  in   new way  the mathematical convenience
and practical utility of the permanental process formulation
 in comparison with say log Gaussian Cox processes 

Acknowledgements
Thanks to Young Lee  Kar Wai Lim and Cheng Soon Ong
for useful discussions  Adrian is supported by the Australian Research Council  ARC  via   Discovery Early Career Researcher Award  DE 

 NormalisedMetric ratio TestLogLikelihood FitTimeKS ECLBPPCosLBPP GVBPP NormalisedMetric ratio Numberofbasisfunctions inducingpoints NormalisedMetric ratio Numberofbasisfunctions inducingpoints Fast Bayesian Permanental Processes

References
Adams  Ryan    Murray  Iain  and MacKay  David      
Tractable nonparametric Bayesian inference in Poisson
processes with gaussian process intensities 
In Bottou    eon and Littman  Michael  eds  ICML  pp   
Montreal  June  

Aronszajn     Theory of reproducing kernels  Transactions

of the American Mathematical Society     

Baddeley  Adrian  Spatial point processes and their applications  Lecture Notes in Mathematics  Stochastic Geometry     

Collins  Michael 

The InsideOutside Algorithm 

Columbia University Lecture Notes   

Cont  Rama and Tankov  Peter  Financial modelling with
jump processes  Financial mathematics series  Chapman
and Hall CRC  London    ISBN  

Cunningham       Shenoy       and Sahani     Fast
Gaussian Process Methods for Point Process Intensity
In Proceedings of the  th International
Estimation 
Conference on Machine Learning  ICML  pp   
Helsinki  Finland   

Davison        and Hinkley       

Bootstrap Methods and Their Application 
Cambridge University
Press  New York  NY  USA    ISBN  
 

Diggle  Peter    kernel method for smoothing point process

data  Applied Statistics     

Diggle       Moraga     Rowlingson     and Taylor 
     Spatial and spatiotemporal logGaussian Cox processes  Extending the geostatistical paradigm  Statistical
Science     

Flaxman     Wilson       Neill       Nickisch     and
Smola       Fast Kronecker Inference in Gaussian Processes with nonGaussian Likelihoods  In Proceedings
of the  nd International Conference on Machine Learning  ICML  pp    Lille  France   

Flaxman     Teh       and Sejdinovic     Poisson Intensity Estimation with Reproducing Kernels  In International Conference on Arti cial Intelligence and Statistics
 AISTATS   

Illian         rbye       and Rue       toolbox for  tting
complex spatial point process models using integrated
nested Laplace approximation  INLA  The Annals of
Applied Statistics     

Kimeldorf     and Wahba     Some results on Tchebychef 
 an spline functions  Journal of Mathematical Analysis
and Appl     

Kom Samo       and Roberts     Scalable nonparametric
bayesian inference on point processes with gaussian processes  In Proceedings of the  nd International Conference on Machine Learning  ICML  pp   
Lille  France   

Lewis           and Shedler        Simulation of nonhomogeneous Poisson processes by thinning  Naval Res 
Logistics Quart     

Lloyd  Chris    Gunter  Tom  Osborne  Michael   
Roberts  Stephen    and Nickson  Tom  Latent point process allocation  In Proceedings of the  th International
Conference on Arti cial Intelligence and Statistics  AISTATS   Cadiz  Spain  May     pp   
 

Lloyd       Gunter     Osborne       and Roberts 
     Variational inference for gaussian process modulated poisson processes  In Proceedings of the  nd International Conference on Machine Learning  ICML 
pp    Lille  France   

Lodhi  Huma  Saunders  Craig  ShaweTaylor  John  Cristianini  Nello  and Watkins  Chris  Text Classi cation
using String Kernels  Journal of Machine Learning Research    February  

McCullagh  Peter and   ller  Jesper  The permanental process  Advances in Applied Probability   
 

Mercer     Functions of positive and negative type  and their
connection with the theory of integral equations  Philosophical Transactions of the Royal Society of London   
Mathematical  Physical and Engineering Sciences   
    ISSN  

  ller     Syversveen     and Waagepetersen     Log gaussian cox processes  Scandanavian Journal of Statistics 
   

Nystr om  Evert Johannes 

 Uber die praktische au osung
von linearen integralgleichungen mit anwendungen auf
randwertaufgaben der potentialtheorie  Commentationes
physicomathematicae     

Rasmussen        and Williams          Gaussian Processes for Machine Learning  Adaptive Computation
and Machine Learning  The MIT Press  Cambridge 
Massachusetts   

Rathbun  Stephen    and Cressie  Noel  Asymptotic properties of estimators for the parameters of spatial inhomogeneous poisson point processes  Advances in Applied
Probability     

Fast Bayesian Permanental Processes

Ripley        Modeling Spatial Patterns  Journal of the
Royal Statistical Society Series   Statistical Methodolology     

Rue    avard  Martino  Sara  and Chopin  Nicolas  Approximate bayesian inference for latent gaussian models by
using integrated nested laplace approximations  Journal of the Royal Statistical Society  Series    Statistical
Methodology     

Sch olkopf  Bernhard  Herbrich  Ralf  and Smola  Alex   
  generalized representer theorem  In Proc  of the  th
Annual Conf  on Computational Learning Theory  pp 
  London  UK    SpringerVerlag  ISBN  
 

Shirai  Tomoyuki and Takahashi  Yoichiro  Random point
 elds associated with certain fredholm determinants ii 
Fermion shifts and their ergodic and gibbs properties 
Ann  Probab        doi   
aop 

Sollich  Peter and Williams  Christopher       Understanding Gaussian Process Regression Using the Equivalent Kernel  pp    Springer Berlin Heidelberg 
Berlin  Heidelberg   

Sommerfeld  Arnold and Straus  Ernst Gabor  Partial differential equations in physics  Pure and applied mathematics  Academic Press  New York   

ThomasAgnan  Christine  Computing   family of reproducing kernels for statistical applications  Numerical Algorithms     

Titsias  Michalis    Variational learning of inducing variables in sparse gaussian processes  In In Arti cial Intelligence and Statistics   pp     

Wahba     Spline Models for Observational Data  Series

in Applied Math  Vol    SIAM  Philadelphia   

