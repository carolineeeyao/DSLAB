Dueling Bandits with Weak Regret

Bangrui Chen   Peter    Frazier  

Abstract

We consider online content recommendation with
implicit feedback through pairwise comparisons 
formalized as the socalled dueling bandit problem  We study the dueling bandit problem in
the Condorcet winner setting  and consider two
notions of regret  the more wellstudied strong
regret  which is   only when both arms pulled are
the Condorcet winner  and the less wellstudied
weak regret  which is   if either arm pulled is the
Condorcet winner  We propose   new algorithm
for this problem  Winner Stays  WS  with variations for each kind of regret  WS for weak regret
 WSW  has expected cumulative weak regret that
is       and     log     if arms have   total
order  WS for strong regret  WSS  has expected
cumulative strong regret of           log    
and     log         log     if arms have   total order  WSW is the  rst dueling bandit algorithm with weak regret that is constant in time 
WS is simple to compute  even for problems with
many arms  and we demonstrate through numerical experiments on simulated and real data that
WS has signi cantly smaller regret than existing
algorithms in both the weakand strongregret
settings 

  Introduction
We consider bandit learning in personalized content recommendation with implicit pairwise comparisons  We offer
pairs of items to   user and record implicit feedback on
which offered item is preferred  seeking to learn the user  
preferences over items quickly  while also ensuring that the
fraction of time we fail to offer   highquality item is small 
Implicit pairwise comparisons avoid the inaccuracy of user
ratings  Joachims et al    and the dif culty of engaging
users in providing explicit feedback 

 Cornell University  Ithaca  NY  Correspondence to  Peter   

Frazier  pf cornell edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

We study   model for this setting called the dueling bandit
problem  Yue   Joachims    The items we may offer
to the user are called  arms  and we learn about these arms
through   sequence of  duels  In each duel  we  pull  two
arms and receive noisy feedback from the user telling us
which arm is preferred  When an arm is preferred within  
duel  we say that the arm has  won the duel 
We study this problem in the Condorcet winner setting  in
which we assume the existence of an arm  the Condorcet
winner  that wins with probability at least  
  when paired
with any of the other arms  In these settings  we consider
two notions of regret   weak regret  in which we avoid
regret by selecting the Condorcet winner as either arm in the
duel  and  strongregret  in which we can only avoid regret
by setting both arms in the duel to the Condorcet winner 
Weak regret was proposed by Yue et al    and arises
in content recommendation when arms correspond to items 
and the user incurs no regret whenever his most preferred
item is made available  Examples include inapp restaurant
recommendations provided by food delivery services like
Grubhub and UberEATS  in which implicit feedback may
be inferred from selections  and the user only incurs regret
if her most preferred restaurant is not recommended  Examples also include recommendation of online broadcasters on
platforms such as Twitch  in which implicit feedback may
again be inferred from selections  and the user is fully satis 
 ed as long as her favored broadcaster is listed  Despite its
applicability  Yue et al    is the only paper of which we
are aware that studies weak regret  and it does not provide
algorithms speci cally designed for this setting 
Strong regret has been more widely studied  as discussed
below  and has application to choosing ranking algorithms
for search  Hofmann et al    To perform   duel  query
results from two rankers are interleaved  Radlinski et al 
  and the ranking algorithm that provided the  rst
result chosen by the user is declared the winner of the duel 
Strong regret is appropriate in this setting because the user  
experience is enhanced by pulling the best arm twice  so
that all of that ranker   results are shown 
Our contribution is   new algorithm  Winner Stays  WS 
with variants designed for the weak  WSW  and strong
regret  WSS  settings  We prove that WSW has expected
cumulative weak regret that is constant in time  with de 

Submission and Formatting Instructions for ICML  

pendence on the number of arms   given by       If
the arms have   total order  we show   tighter bound of
    log     We then prove that WSS has expected cumulative strong regret that is           log     and prove
that   tighter bound of     log         log     holds if
arms have   total order  These regret bounds are optimal
in     and for weak regret are strictly better than those for
any previously proposed algorithm  although at the same
time both strong and weak regret bounds are sensitive to
the minimum gap in winning probability between arms  We
demonstrate through numerical experiments on simulated
and real data that WSW and WSS signi cantly outperform
existing algorithms on strong and weak regret 
The paper is structured as follows  Section   reviews related
work  Section   formulates our problem  Section   introduces the Winner Stays  WS  algorithm  Section   de nes
WSW for the weak regret setting  Section   proves that
WSW has cumulative expected regret that is constant in
time  Section   de nes WSS for the strong regret setting
and bounds its regret  Section   disusses   simple extension of our theoretical results to the utilitybased bandit
setting  which is used in our numerical experiments  Section   compares WS with three benchmark algorithms using
both simulated and real datasets   nding that WS outperforms these benchmarks on the problems considered 

  Related Work
Most work on dueling bandits focuses on strong regret  Yue
et al    shows that the worstcase expected cumulative
strong regret up to time   for any algorithm is    log    
Algorithms have been proposed that reach this lower bound
under the Condorcet winner assumption in the  nitehorizon
setting  Interleaved Filter  IF   Yue et al    and Beat
the Mean  BTM   Yue   Joachims    Relative Upper Con dence Bound  RUCB   Zoghi et al    also
reaches this lower bound in the horizonless setting  Relative Minimum Empirical Divergence  RMED   Komiyama
et al    is the  rst algorithm to have   regret bound that
matches this lower bound  Zoghi et al    proposed two
algorithms  Copeland Con dence Bound  CCB  and Scalable Copeland Bandits  SCB  which achieve an optimal
regret bound without assuming existence of   Condorcet
winner 
While weak regret was proposed in Yue et al    it has
not been widely studied to our knowledge  and despite its
applicability we are unaware of papers that provide algorithms designed for it speci cally  While one can apply
algorithms designed for the strong regret setting to weak
regret  and use the fact that strong dominates weak regret to
obtain weak regret bounds of     log     these are looser
than the constantin   bounds that we show 

Active learning using pairwise comparisons is also closely
related to our work  Jamieson   Nowak   considers
an active learning problem that is similar to our problem in
that the primary goal is to sort arms based on the user   preferences  using adaptive pairwise comparisons  It proposes
  novel algorithm  the Query Selection Algorithm  QSA 
that uses an expected number of operations of   log     to
sort   arms  where   is the dimension of the space in which
the arms are embedded  rather than   log     BusaFekete
et al    and BusaFekete et al    consider topk element selection using adaptive pairwise comparisons 
They propose   generalized racing algorithm focusing on
minimizing sample complexity   Pallone et al    studies adaptive preference learning across arms using pairwise
preferences  They show that   greedy algorithm is Bayesoptimal for an entropy objective  While similar in that they
use pairwise comparisons  these algorithms are different in
focus from the current work because they do not consider
cumulative regret 

  Problem Formulation
We consider   items  arms  At each time              
the system chooses two items and shows them to the user 
     the system performs   duel between two arms  The
user then provides binary feedback indicating her preferred
item  determining which arm wins the duel  This binary
feedback is random  and is conditionally independent of all
past interactions given the pair of arms shown  We let pi  
denote the probability that the user gives feedback indicating
  preference for arm    when shown arms   and    If the user
prefers arm   over arm    we assume pi       We also
assume symmetry  pi         pj   
We assume arm   is   Condorcet winner       that        
for            In some results  we also consider the
setting in which arms have   total order  by which we mean
that the arms are ordered so that pi       for all       
The total order assumption implies transitivity 
We let     minpi     pi       be   lower bound on the
probability that the user will choose her favourite arm 
We consider both weak and strong regret in its binary form 
The singleperiod weak regret incurred at this time is       
  if we do not pull the best arm and          otherwise 
The singleperiod strong regret is          if we do not
pull the best arm twice and          otherwise  We also
consider utilitybased extensions of weak and strong regret
in Section  
We use the same notation      to denote strong and weak
regret  and rely on context to distinguish the two cases  In
both cases  we de ne the cumulative regret up to time  
        We measure the quality of an

to be          cid  

algorithm by its expected cumulative regret 

Submission and Formatting Instructions for ICML  

  Winner Stays
We now propose an algorithm  called Winner Stays  WS 
with two variants  WSW designed for weak regret  and
WSS for strong regret  Section   introduces WSW
and illustrates its dynamics  Section   proves the expected cumulative weak regret of WSW is       under the Condorcet winner setting  and     log     under the total order setting  Section   introduces WSS
and proves that its expected cumulative strong regret is
          log     under the Condorcet winner setting 
and     log         log     under the total order setting 
both of which have optimal dependence on     Section  
extends our theoretical results to utilitybased bandits 

  Winner Stays with Weak Regret  WSW 

We now present WSW   rst de ning some notation  Let
 cid 
qi      be the number of times that arm   has defeated arm  
in   duel  up to and including time    Then  de ne          
  cid   qi        qj              is the difference between the
number of duels won and lost by arm    up to time    With
this notation  we de ne WSW in Algorithm  

Algorithm   WSW
Input  arms      
for         do

it

uniformly at

set it   jt 
choose
arg maxi           

Step   Pick it   arg maxi         breaking ties as
follows 
  If       and it    arg maxi         set it   it 
  Else if       and jt    arg maxi           
  Else
random from
Step   Pick jt   arg maxj cid it         breaking ties
as follows 
  If       and it    arg maxi cid it               it 
  Else if       and jt    arg maxi cid it        it 
  Else

random from

set jt   jt 
choose
arg maxj               it 

jt

set jt   it 

uniformly at

Step   Pull arms it and jt 
Step   Observe noisy binary feedback and update
     it  and      jt 

end

WSW   pulls can be organized into iterations  each of
which consists of   sequence of pulls of the same pair of
arms  and rounds  each of which consists of   sequence of
iterations in which arms that lose an iteration are not visited
again until the next round  We  rst describe iterations and
rounds informally with an example and in Figure   before
presenting our formal analysis 

Example  At time                 for all    and WSW
pulls two randomly chosen arms  Suppose it pulls arms
              and arm   wins  Then       is   for arm
    for arm   and   for the other arms  This  rst pull is
an iteration of length   arm   is the winner  and arm   is
the loser  This iteration is in the  rst round  We call       
the start of the  rst round  and        the start of the  rst
iteration in the  rst round 
At time                 is largest for arm   so WSW
chooses        Since            is   for arm   and  
for the other arms  WSW chooses    at random from arms
  through    suppose       Suppose it chooses arm
       This pair of arms   and   is different from the pair
pulled in the previous iteration   and   so        is the
start of the second iteration  in the  rst round 
WSW continues pulling arms   and   until         is  
for one of these arms and   for the other  WSW continues
to pull only arms   and   until one has             even
though this may involve times when         is   for both
arms   and   causing them to be tied with arms   and above 
because we break ties to prioritize pulling previously pulled
arms  The sequence of times when we pull arms   and   is
the second iteration  The arm that ends the iteration with
            is the winner of that iteration 
WSW continues this process  performing       iterations
on different pairs of arms  pitting the winner of each iteration
against   previously unplayed arm in the next iteration  This
sequence of iterations is the  rst round  The winner of
the  nal iteration in the  rst round  call it arm    has
                and all other arms    cid     have
         
The second round begins on the next pull after the end of
the  rst round  at time    WSW again performs      
iterations  playing    in the  rst iteration  Each iteration
has   winner that passes to the next iteration 
WSW repeats this process for an in nite number of rounds 
Each round is   sequence of     iterations  and an arm
that loses an iteration is not revisited until the next round 
Figure   illustrates these dynamics  and we formalize the
de nition of round and iteration in the next section 

  Analysis of WSW

In this section  we analyze the weak regret of WSW  After
presenting de nitions and preliminary results  we prove
WSW has expected cumulative weak regret bounded by
    log     when arms have   total order  Then  in the
more general Condorcet winner setting  we prove WSW
has expected cumulative weak regret bounded by      
We leave the proofs of all lemmas to the supplement 
We de ne   cid  the beginning of round  cid  and   cid      the

Submission and Formatting Instructions for ICML  

Figure   Our analysis of WSW decomposes its behavior into   sequence of rounds  In each round  pairs of arms play each other in  
sequence of iterations  The winner from an iteration passes on to play   new arm in the next iteration randomly selected from those that
have not yet played in the round  At the end of   round  the round   winner is considered  rst in the next round 

winner of round  cid  as the unique time and arm such     cid   
    cid              cid      and     cid            cid     
for all    cid    cid     
We de ne   cid    the beginning of iteration   in round  cid  as
the  rst time we pull the kth unique pair of arms in the  cid th
round  We let   cid   be the number of successive pulls of this
pair of arms 
We additionally de ne terminology to describe arms pulled
in an iteration  In   duel between arms   and   with pi    
  arm   is called the better arm and arm   is called the
worse arm  We say that an arm   is the incumbent in iteration
  iteration and round  cid  if     cid            unique such
arm exists except when  cid          When  cid         
the incumbent is the better of the two arms being played 
We call the arm being played that is not the incumbent the
challenger 
Using these de nitions  we present our  rst pair of results
toward bounding the expected cumulative weak regret of
WSW  They bound the number of pulls in an iteration 
Lemma   The conditional expected length of iteration  
in round  cid  given the arms being pulled  is bounded above
by    cid  
if the incumbent is worse than the challenger 
   
    if the incumbent is better than the challenger 
and by
 

Lemma   shows that iterations with   worse incumbent use
more pulls  We then bound the number of iterations with  
worse incumbent 
Lemma   Under the total order assumption  the conditional expected number of future iterations with an incumbent worse than the challenger  given history up to time   cid   
     log         for any     cid     
is bounded above by

   

Lemma   implies that the incumbent is worse than the challenger in  nitely many iterations with probability   We
now bound the tail distribution of the last such round 

Lemma   Let   denote the smallest  cid  such that no round
 cid cid     cid  contains an iteration in which the incumbent is worse

than the challenger  Then         cid   cid    

 cid cid 

 

 

To present our  nal set of preliminary lemmas  we de ne
several indicator functions  Let   cid     be   when the incumbent in iteration   of round  cid  is better than the challenger  Let   cid  be   if arm    the best arm  is the incumbent at the beginning of iteration   of round  cid  Denote
   cid             cid     and    cid          cid  Let    cid    
be   if   cid      and arm   loses in any iteration   through
      of round  cid 
We may only incur weak regret during round  cid  iteration  
if    cid      or if    cid    cid      for some   cid       We will
separately bound the regret incurred in these two different
scenarios  Moreover  our bound on the number of pulls 
and thus the regret incurred  in this iteration will depend on
whether   cid         or    cid         This leads us to state
four inequalities in the following pair of lemmas  which
we will in turn use to show Theorem   The  rst lemma
applies in both the total order and Condorcet settings  while
the second applies only in the total order setting  When
proving Theorem   we replace Lemma   by an alternate
pair of inequalities 
Lemma  

      cid   cid      cid       

      
     cid      cid      cid       

      

 

 cid       
 cid cid 
 cid cid 
 cid       

 

 

 

Lemma   Under the total order assumption 

 cid 

    cid cid   
 cid    
 cid cid 

  

 

   cid     cid      cid  

is bounded above by

    cid   
     log        

    cid cid   
 cid cid 
 cid    

 

      cid        cid      cid  
    cid   
     log        

 cid 

Submission and Formatting Instructions for ICML  

is bounded above by

This is bounded by

 cid 

   cid      cid kV  cid    

 

 cid 

  cid 

 cid 

  

   cid      cid      cid   

 cid 

 cid    log    

   

We now state our main result for the total order setting 
which shows that the expected cumulative weak regret is
 
Theorem   The expected cumulative weak regret of WSW
is bounded by
under
the total order assumption 

       log            

 cid     

   

 cid 

 

Proof  Iterations can be divided into two types  those in
which the incumbent is better than the challenger  and those
where the incumbent is worse 
We  rst bound expected total weak regret incurred in the
 rst type of iteration  and then below bound that incurred in
the second type  In this  rst bound  observe that we incur
weak regret during round  cid  if   cid      or if   cid      but
arm   loses to some other arm during this round  Under
the second scenario  we do not incur any regret until arm  
loses to another arm 
Thus  the expected weak regret incurred during iterations
with   better incumbent is bounded by

 cid   cid 

  cid 

 

 cid 

  cid 

  cid      cid      cid   

  cid      cid kV  cid    

 

 cid 

  

 cid 

  

The  rst term in the summation can be bounded by the  rst
inequality of Lemma   to obtain

 cid 

   cid      cid      cid 

    cid        
      

 log        

 cid       

 cid cid 

 

 

     
        log        

The second term in the summation can be bounded by the
 rst inequality of Lemma   to obtain

 

 cid   cid 
  cid 
 cid 

  

 cid 

 

   cid      cid kV  cid    

 cid 

 

    cid   
        log        

 cid 

        
          log        

 cid 

 cid       

 cid cid 

 

 cid   cid 

  cid 

 cid 

  

 

 

 cid   cid 
  cid 
 cid 

  

 cid 

 

 cid 

The  rst part of this summation can be bounded by the  rst
inequality in Lemma   to obtain

 cid 

 

 cid   cid 
  cid 
 cid       
 cid 

  

 cid 

 

 

 cid 

  cid      cid      cid 

 cid cid   

      

 

pN

        

The second part of this summation can be bounded by the
second inequality in Lemma   to obtain

 cid 

 

 cid   cid 
  cid 
 cid 

  

 cid 

 

 cid 

 

      

  cid      cid kV  cid    

 cid       

 cid cid 

 

        
        

 

Thus  the cumulative expected weak regret incurred during
iterations with   better incumbent is bounded by
     
Now we bound the expected weak regret incurred during
iterations where the incumbent is worse than the challenger 

 

   

Thus  the cumulative expected weak regret incurred during iterations with   worse incumbent is bounded by
       log        
Summing these two bounds  the cumulative expected weak
regret is bounded by
 

       log            

 cid     

   

 cid 

We prove the following result for the Condorcet winner
setting in   similar manner in the supplement 
Theorem   The expected cumulative weak regret of WSW is bounded by
    under the Condorcet
winner setting 

      pN  

 

  Winner Stays with Strong Regret  WSS 

In this section  we de ne   version of WS for strong regret 
WSS  which uses WSW as   subroutine  WSS is de ned
in Algorithm  
Each round of WSS consists of an exploration phase and an
exploitation phase  The length of the exploitation phase increases exponentially with the number of phases  Changing
the parameter   balances the lengths of these phases  and
thus balances between exploration and exploitation  Our
theoretical results below guide choosing  

Submission and Formatting Instructions for ICML  

    Simulated Dataset

    Yelp Academic Dataset

Figure   Comparison of the weak regret between WSW  RUCB and QSA using simulated data  and the Yelp academic dataset  In both
experiments  WSW outperforms RUCB and QSA  provided constant expected cumulative weak regret 

Algorithm   WSS
Input        arms      
for  cid        do

Exploration phase  Run the  cid th round of WSW 
Exploitation phase  Let   cid  be the index of the best
arm at the end of the  cid th round  For the next  cid cid cid 
time periods  pull arms   cid  and   cid  and ignore the
feedback 

end

 

 

 cid 

   

   

 cid     

If there is   total order among arms 
the expected cufor WSS is bounded by
  log      

We now bound the cumulative strong regret of this algorithm
under both the total order and Condorcet winner settings 
Theorem  
then for        
mulative strong regret
       log          
Proof  Suppose at time    we are in round  cid  Then      
 cid        Solving for  cid  we obtain  cid    log        
We bound the expected strong regret up to time     The
expected regret can be divided in two parts  the regret occuring during the exploration phase  and the regret occuring
during the exploitation phase 
First we focus on regret incurred during exploration  We
never pull the same arm twice during this phase  and so
regret is incurred in each time period  To bound regret
incurred during exploration  we bound the length of time
spent in this phase 
The length of time spent in exploration up to the end of
round  cid  with   better incumbent is bounded by    cid 
      The
length of time spent with   worse incumbent  based on the
proof of Theorem   is bounded by
       log        
Now we focus on regret incurred during exploitation  The

   

probability we have identi ed the wrong arm at the end of
the ith round is less than
  Thus  the expected regret
incurred during this phase up until the end of the  cid th round

 

 cid    
 cid  
 cid    
 cid           cid 

  

 

is bounded by cid cid 
 cid 
 cid 

   

 

Overall  this implies that the strong expected regret up to
time    recall that   is in round  cid  is bounded by
      cid 
      

          log            cid   

 cid 
  log        

 cid 

   

          log          

      

 

the expected strong regret up to time   is

Thus 
    log         log    

Theorem   Under the Condorcet winner setting and for
         
    the expected cumulative strong regret for
WSS is bounded by

 cid      
        log    
    log 

 cid 

 

Proof  The proof mirrors that of Theorem   with the only
difference being that we bound the length of exploration
with   worse incumbent using the proof of Theorem   rather
than Theorem   and the bound is       Due to its similarity  the proof is omitted 

These results provide guidance on the choice of   If   is
too close to   then we spend most of the time in the exploration phase  which is guaranteed to generate strong regret 
The last inequality in the proof of Theorem   suggests that
asymptotic regret will be smallest if we choose   as large
as possible without going beyond the         threshold 
Indeed  if   is too large  then WSS may incur large regret
in early exploitation stages when we have  nished only  
few rounds of exploration  In our numerical experiments

Submission and Formatting Instructions for ICML  

we set       which satis es the         constraint
assumed by our theory if               With  
properly chosen   the numerical experiments in section  
suggest WSS performs better than previously devised algorithms  At the same time  the best choice of   is dependent
on    Modifying WSS to eliminate parameters that must be
chosen with knowledge of   is left for future work 
Our regret bound grows as    which is the minimal gap
between two arms  shrinks  and   tends to decrease as the
number of arms   increases  Other dueling bandit algorithm for strong regret  such as RUCB and RMED  have
regret bounds with better dependence on the gaps between
arms  Modifying WSS to provide improved dependence on
these gaps is also left for future work 

  Extension to UtilityBased Regret

We now brie   discuss utilitybased extensions of weak and
strong regret for the total order setting  following utilitybased bandits studied in Ailon et al    Our regret
bounds also apply here  with   small modi cation 
Suppose that the user has   utility ui associated with each
arm    Without loss of generality  we assume          
    uN   and as in the total order setting  we require that
pi       when        Typically the pi   would come from
the utilities of arms   and   via   generative model  We give
an example in our numerical experiments 
Then  the singleperiod utilitybased weak regret is       
     max uit  ujt  which is the difference in utility between the best arm overall and the best arm that the user can
choose from those offered  The singleperiod utilitybased
strong regret is             uit  ujt
  To get zero regret
under strong regret  the best arm must be pulled twice 
Our results from Section   carry through to this more general regret setting  Let          uN be the maximum
singleperiod regret  Then  the expected cumulative utilitybased weak regret for WSW is  
  and the
expected cumulative utilitybased strong regret for WSS is
       log         log    

    log    
   

 cid 

 

 cid 

  Numerical Experiments
In this section  we evaluate WS under both the weak and
strong regret settings  considering both their original  binary  and utilitybased versions  In the weak regret setting 
we compare WSW with RUCB and QSA  In the strong
regret setting  we compare WSS with   benchmarks including RUCB and Relative Minimum Empirical Divergence
 RMED  by Komiyama et al    We also include an experiment violating the total order assumption in Section  
in the supplement  WS outperforms all benchmarks tested
in these numerical experiments 

  Weak Regret

We now compare WSW with QSA and RUCB using simulated data and the Yelp academic dataset  Yelp   

  SIMULATED DATA

In this example  we compare WSW with RUCB and QSA
on   problem with   arms and binary weak regret  Each
arm is    dimensional vector uniformly generated from
the unit circle  We assume pi       for all       
The results are summarized in Figure     RUCB has approximately linear regret over the time horizon pictured 
This is common in the dueling bandits literature  where
many algorithms require     comparisons before they
achieve log     cumulative regret for   arms  WSW  nds
the optimal arm after     comparisons and has   regret
that is consistent with our theoretically established constant
expected cumulative weak regret 

  YELP ACADEMIC DATASET

In this example  we compare WSW with RUCB and QSA
using the Yelp academic dataset  Yelp    and utilitybased weak regret 
We choose   restaurants from Las Vegas as our arms 
Associated with each arm  restaurant    is    dimensional
feature vector Ai  calculated using doc vec  Rehurek  
Sojka    from its reviews  We select   users who have
reviewed at least   of these   restaurants  For each user 
we model their utility for restaurant   as ui   Ai     where
  is    dimensional vector of preferences  We infer   for
each user using linear regression 
To model pi    we then use the probit model  We let  
be the estimated variance of the residuals from the linear
regression above  When presented with two restaurants  we
model the user as taking independent random draws from  
normal distribution with means ui and uj respectively and
variances   and choosing the restaurant with the larger
draw  This gives pij    ui   uj  where   is the cdf
for the normal distribution with mean   and variance  
We simulate performance for each user separately  and then
average the results  These results are summarized in Figure     WSW outperforms RUCB and QSA   nding the
optimal restaurant after     iterations 

  Strong Regret

In this section  we compare WSS using binary and utilitybased strong regret with   benchmarks from the literature 
We use the sushi and MSLR datasets  which were previously
used by Komiyama et al    and Zoghi et al   
respectively to evaluate dueling bandit algorithms 

Submission and Formatting Instructions for ICML  

    Sushi dataset with utilitybased strong regret

    Sushi dataset with binary strong regret

    MSLR dataset with utilitybased strong regret

    MSLR dataset with binary strong regret

Figure   Comparison of the strong regret between WSS and   benchmarks on the sushi and MSLR datasets  For utilitybased strong
regret  we start our plot from       since the performance of all algorithms are close to each other before       For the same reason 
we start our plot from       for the binary strong regret  WSS outperforms all benchmarks in all settings studied 

The sushi dataset  Komiyama et al    contains   arms
corresponding to types of sushi  with pairwise preferences
inferred from data on sushi preferences from   users in
Kamishima   The MSLR dataset has   arms  corresponding to ranking algorithms  with pairwise preferences
provided in Zoghi et al    We give preference matrices
 pi    for both datasets in the supplement  For utilitybased
regret  we de ne ui           
WSS has   userde ned parameter   In our experiments
we set       The corresponding minimum   for which
our theoretical bounds hold is           We
recommend       for problems of   arms or fewer  and
  closer to   for those problems with more arms that are
likely to have   closer to   We also conduct   sensitivity
analysis of   in the supplement 
Figure   shows the results of our comparisons  WSS outperforms all   benchmarks considered on both datasets using
both variants of strong regret 

  Conclusion
In this paper  we consider dueling bandits for online content
recommendation using both weak and strong regret 
We propose   new algorithm  WS  with variants designed for
the weak regret  WSW  and strong regret  WSS  settings 
We prove WS has constant weak regret and optimal strong
regret in     In numerical experiments  WS outperforms all
benchmarks considered on both simulated and real datasets 

Acknowledgements
The authors were partially supported by NSF CAREER
CMMI  NSF CMMI  NSF IIS 
NSF DMR  AFOSR FA  AFOSR
FA  and AFOSR FA 

Submission and Formatting Instructions for ICML  

Rehurek  Radim and Sojka  Petr  Software framework for
topic modelling with large corpora  In In Proceedings of
the LREC   Workshop on New Challenges for NLP
Frameworks  Citeseer   

Yelp  Inc  Yelp academic dataset    URL https 

 www yelp com dataset challenge 

Yue  Yisong and Joachims  Thorsten  Interactively optimizing information retrieval systems as   dueling bandits
problem  In Proceedings of the  th Annual International
Conference on Machine Learning  pp    ACM 
 

Yue  Yisong and Joachims  Thorsten  Beat the mean bandit 
In Proceedings of the  th International Conference on
Machine Learning  ICML  pp     

Yue  Yisong  Broder  Josef  Kleinberg  Robert  and
Joachims  Thorsten  The karmed dueling bandits problem  Journal of Computer and System Sciences   
   

Zoghi  Masrour  Whiteson  Shimon  Munos  Remi  Rijke 
Maarten de  et al  Relative upper con dence bound for the
karmed dueling bandit problem  In JMLR Workshop and
Conference Proceedings  number   pp    JMLR 
 

Zoghi  Masrour  Karnin  Zohar    Whiteson  Shimon  and
De Rijke  Maarten  Copeland dueling bandits  In Advances in Neural Information Processing Systems  pp 
   

References
Ailon  Nir  Karnin  Zohar Shay  and Joachims  Thorsten 
Reducing dueling bandits to cardinal bandits  In ICML 
volume   pp     

BusaFekete    bert  Sz   nyi  Bal zs  Cheng  Weiwei 
Weng  Paul  and   llermeier  Eyke  Topk selection
based on adaptive sampling of noisy preferences 
In
ICML   pp     

BusaFekete    bert    llermeier  Eyke  and Sz   nyi 
Bal zs  Preferencebased rank elicitation using statistical models  The case of mallows  In Proceedings of
the  st International Conference on Machine Learning
 ICML  pp     

Hofmann  Katja  Whiteson  Shimon  and Rijke  Maarten De 
Fidelity  soundness  and ef ciency of interleaved comparison methods  ACM Transactions on Information Systems
 TOIS     

Jamieson  Kevin   and Nowak  Robert  Active ranking
using pairwise comparisons  In Advances in Neural Information Processing Systems  pp     

Joachims  Thorsten  Granka  Laura  Pan  Bing  Hembrooke 
Helene  Radlinski  Filip  and Gay  Geri  Evaluating the
accuracy of implicit feedback from clicks and query reformulations in web search  ACM Transactions on Information Systems  TOIS     

Kamishima  Toshihiro  Nantonac collaborative  ltering 
recommendation based on order responses  In Proceedings of the ninth ACM SIGKDD international conference
on Knowledge discovery and data mining  pp   
ACM   

Komiyama  Junpei  Honda  Junya  Kashima  Hisashi  and
Nakagawa  Hiroshi  Regret lower bound and optimal
algorithm in dueling bandit problem  In COLT  pp   
   

Komiyama  Junpei  Honda  Junya  and Nakagawa  Hiroshi 
Copeland dueling bandit problem  Regret lower bound 
optimal algorithm  and computationally ef cient algorithm  arXiv preprint arXiv   

Pallone  Stephen    Frazier  Peter    and Henderson 
Shane   
Bayesoptimal entropy pursuit for active choicebased preference learning  arXiv preprint
arXiv   

Radlinski  Filip  Kurup  Madhu  and Joachims  Thorsten 
How does clickthrough data re ect retrieval quality  In
Proceedings of the  th ACM conference on Information
and knowledge management  pp    ACM   

