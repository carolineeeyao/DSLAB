Adaptive Sampling Probabilities for NonSmooth Optimization

Hongseok Namkoong   Aman Sinha   Steve Yadlowsky   John    Duchi    

Abstract

Standard forms of coordinate and stochastic gradient methods do not adapt to structure in data 
their good behavior under random sampling is
predicated on uniformity in data  When gradients in certain blocks of features  for coordinate
descent  or examples  for SGD  are larger than
others  there is   natural structure that can be exploited for quicker convergence  Yet adaptive
variants often suffer nontrivial computational
overhead  We present   framework that discovers and leverages such structural properties at  
low computational cost  We employ   bandit optimization procedure that  learns  probabilities
for sampling coordinates or examples in  nonsmooth  optimization problems  allowing us to
guarantee performance close to that of the optimal stationary sampling distribution  When such
structures exist  our algorithms achieve tighter
convergence guarantees than their nonadaptive
counterparts  and we complement our analysis
with experiments on several datasets 

  Introduction
Identifying and adapting to structural aspects of problem
data can often improve performance of optimization algorithms  In this paper  we study two forms of such structure 
variance in the relative importance of different features and
observations  as well as blocks thereof  As   motivating
concrete example  consider the    regression problem

         kAx   bkp

   

nXi 

      bi     
 aT

 

minimize

 

where ai denote the rows of     Rn    When the columns
 features  of   have highly varying norms say because

 Management Science   Engineering  Stanford University  USA  Electrical Engineering  Stanford University  USA
 Statistics  Stanford University  USA  Correspondence to 
Hongseok Namkoong  hnamk stanford edu  Aman Sinha
 amans stanford edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

certain features are infrequent we wish to leverage this
during optimization  Likewise  when rows ai have disparate norms   heavy  rows of   in uence the objective
more than others  We develop optimization algorithms that
automatically adapt to such irregularities for general nonsmooth convex optimization problems 
Standard  stochastic  subgradient methods  Nemirovski
et al    as well as more recent accelerated variants for
smooth  strongly convex incremental optimization problems       Johnson and Zhang    Defazio et al   
follow deterministic or random procedures that choose data
to use to compute updates in ways that are oblivious to conditioning and structure  As our experiments demonstrate 
choosing blocks of features or observations for instance 
all examples belonging to   particular class in classi cation problems can be advantageous  Adapting to such
structure can lead to substantial gains  and we propose
  method that adaptively updates the sampling probabilities from which it draws blocks of features observations
 columns rows in problem   as it performs subgradient
updates  Our method applies to both coordinate descent
 feature column sampling  and mirror descent  observation row sampling  Heuristically  our algorithm learns to
sample informative features observations using their gradient values and requires overhead only logarithmic in the
number of blocks over which it samples  We show that
our method optimizes   particular bound on convergence 
roughly sampling from the optimal stationary probability
distribution in hindsight  and leading to substantial improvements when the data has pronounced irregularity 
When the objective     is smooth and the desired solution accuracy is reasonably low   block  coordinate descent
methods are attractive because of their tractability  Nesterov    Necoara et al    Beck and Tetruashvili 
  Lee and Sidford    Richt arik and Tak       
Lu and Xiao    In this paper  we consider potentially
nonsmooth functions and present an adaptive block coordinate descent method  which iterates over   blocks of
coordinates  reminiscent of AdaGrad  Duchi et al   
Choosing   good sampling distribution for coordinates
in coordinate descent procedures is nontrivial  Lee and
Sidford    Necoara et al    ShalevShwartz and
Zhang    Richt arik and Tak        Csiba et al   
AllenZhu and Yuan    Most work focuses on choos 

Adaptive Sampling Probabilities for NonSmooth Optimization

ing   good stationary distribution using problemspeci  
knowledge  which may not be feasible  this motivates automatically adapting to individual problem instances  For example  Csiba et al    provide an updating scheme for
the probabilities in stochastic dual ascent  However  the update requires      time per iteration  making it impractical
for largescale problems  Similarly  Nutini et al    observe that the GaussSouthwell rule  choosing the coordinate with maximum gradient value  achieves better performance  but this also requires      time per iteration  Our
method roughly emulates this behavior via careful adaptive
sampling and bandit optimization  and we are able to provide   number of   posteriori optimality guarantees 
In addition to coordinate descent methods  we also consider
the  nitesum minimization problem

minimize

   

 
 

nXi 

fi   

where the fi are convex and may be nonsmooth  Variancereduction techniques for  nitesum problems often yield
substantial gains  Johnson and Zhang    Defazio et al 
  but
they generally require smoothness  More
broadly  importance sampling estimates  Strohmer and Vershynin    Needell et al    Zhao and Zhang   
  Csiba and Richt arik    can yield improved convergence  but the only work that allows online  problemspeci   adaptation of sampling probabilities of which we
are aware is Gopal   However  these updates require
     computation and do not have optimality guarantees 
We develop these ideas in the coming sections  focusing
 rst in Section   on adaptive procedures for  nonsmooth 
coordinate descent methods and developing the necessary
bandit optimization and adaptivity machinery  In Section  
we translate our development into convergence results for
 nitesum convex optimization problems  Complementing
our theoretical results  we provide   number of experiments
in Section   that show the importance of our algorithmic
development and the advantages of exploiting block structures in problem data 

  Adaptive sampling for coordinate descent
We begin with the convex optimization problem

minimize

   

     

 

where               Rd is   Cartesian product
of closed convex sets Xj   Rdj with Pj dj      and
  is convex and Lipschitz  When there is   natural block
structure in the problem  some blocks have larger gradient norms than others  and we wish to sample these blocks
more often in the coordinate descent algorithm  To that

end  we develop an adaptive procedure that exploits variability in block  importance  online  In the coming sections  we show that we obtain certain nearoptimal guarantees  and that the computational overhead over   simple
In
random choice of block         is at most   log   
addition  under some natural structural assumptions on the
blocks and problem data  we show how our adaptive sampling scheme provides convergence guarantees polynomially better in the dimension than those of naive uniform
sampling or gradient descent 

Notation for coordinate descent Without loss of genrality we assume that the  rst    coordinates of     Rd
correspond to    the second    to    and so on  We let
Uj        dj be the matrix identifying the jth block  so
that Id         Ud  We de ne the projected subgradient
vectors for each block   by

Gj      UjU          Rd 

 
      
   

where               is    xed element of the subdifferential        De ne                Rdj and         
    Gj                 Rdj   Let    denote   differentiable  strongly convex function on Xj with respect to the
norm   kj  meaning for all     Rdj we have
                              
and let   kj 
be the dual norm of   kj  Let Bj        
                             be the Bregman divergence associated with     and de ne the tensorized divergence          Pb
   Bj           Throughout the paper  we assume the following 
Assumption   For all           we have             
and       

  Coordinate descent for nonsmooth functions
The starting point of our analysis is the simple observation
that if   coordinate         is chosen according to   probability vector       then the importance sampling estimator
GJ    pJ satis es Ep GJ    pJ                  
Thus the randomized coordinate subgradient method
of Algorithm   is essentially   stochastic mirror descent method  Nemirovski and Yudin    Beck and
Teboulle    Nemirovski et al    and as long
as supx     kp 
      it converges at rate
  pT   With this insight    variant of standard stochastic mirror descent analysis yields the following convergence guarantee for Algorithm   with nonstationary probabilities  cf  Dang and Lan   who do not quite as
carefully track dependence on the sampling distribution

         for                 

  GJ      

Adaptive Sampling Probabilities for NonSmooth Optimization

Algorithm   Nonsmooth Coordinate Descent

Algorithm   Stepsize Doubling Coordinate Descent

Input  Stepsize        Probabilities            pT  
Initialize        
for                
Sample Jt   pt
Update   
 Jt    argminx XJt     Jt xt 
xt 

        

  

pt

Jt

BJt    xt

 Jt 

return  xT    

   xt

  PT

 

 

pt
 

  

   Throughout  we de ne the expected suboptimality gap

  pmin

  
  

  
 xT

TXt 

     xT    

bXj     xt 

Proposition   Under Assumption   Algorithm   achieves

of an algorithm outputing an estimate bx by     bx   
     bx    inf           See Section    for the proof 
     

  
    then       xT     RLq  

where       xT          xT     inf          
As an immediate consequence  if pt   pmin     and     
Lq  pmin
  To make this
more concrete  we consider sampling from the uniform distribution pt    
  so that pmin       and assume homogeneous block sizes dj       for simplicity  Algorithm  
solves problem   to  accuracy within   bR    iterations  where each iteration approximately costs       
plus the cost of projecting into Xj  In contrast  mirror descent with the same constraints and divergence   achieves
the same accuracy within        iterations  taking
     time plus the cost of projecting to   per iteration  As
the projection costs are linear in the number   of blocks  the
two algorithms are comparable 
In practice  coordinate descent procedures can signi cantly
outperform full gradient updates through ef cient memory
usage  For huge problems  coordinate descent methods can
leverage data locality by choosing appropriate block sizes
so that each gradient block  ts in local memory 

 

  Optimal stepsizes by doubling
In the the upper bound   we wish to choose the optimal
stepsize    that minimizes this bound  However  the term

kG   xt   
  

pt
 

  

     Pb
PT

  is unknown   priori  We cir 

cumvent this issue by using the doubling trick       ShalevShwartz    Section   to achieve the best possible
rate in hindsight  To simplify our analysis  we assume that
there is some pmin     such that

pt           Rb

Maintaining the running sum Pt

               pmin   
  Jl   Jl xl 

     

Jl 

Jl              do

Run inner loop of Algorithm   with

Initialize                       
while       do
whilePt
  pl
                 
         
         
return  xT    

Jl   Jl xl 
min   

   xt

bp 

 

  PT

requires incremental time   dJt  at each iteration    choosing the stepsizes adaptively via Algorithm   only requires
  constant factor of extra computation over using    xed
step size  The below result shows that the doubling trick in
Algorithm   acheives  up to log factors  the performance
of the optimal stepsize that minimizes the regret bound  
Proposition   Under Assumption   Algorithm   achieves

      xT      

 
 

 

   
TXt 
    

 

  

pt
 

  

  
bXj     xt 
log   bT   
pmin  

RL

pminT log  

where       xT          xT     inf          
  Adaptive probabilities
We now present an adaptive updating scheme for pt  the
sampling probabilities  From Proposition   the stationary
distribution achieving the smallest regret upper bound minimizes the criterion

TXt 

  

bXj     xt 

pj

  

   

TXt 

    Jt xt 

  
Jt

Jt 

   

where the equality follows from the tower property  Since
xt depends on the pt  we view this as an online convex
optimization problem and choose            pT to minimize
the regret

  

 

pj   

     
   
pt

 

max
   

TXt 

bXj     xt 
we only compute     xt 
we only observe the loss     xt 

Note that due to the block coordinate nature of Algorithm  
for the sampled     Jt at
each iteration  Hence  we treat this as   multiarmed bandit
problem where the arms are the blocks                 and
Jt  associated

with the arm Jt pulled at time   

 pt

  

  

Adaptive Sampling Probabilities for NonSmooth Optimization

scaling   ensures that we penalize blocks with low signal  as opposed to rewarding those with high signal  which
enforces diversity in the sampled coordinates as well  In
Section    we will see how this scaling plays   key role
in proving optimality of Algorithm   Here  the signal is
measured by the relative size of the gradient in the block
against the probability of sampling the block  This means
that blocks with large  surprises those with higher gradient norms relative to their sampling probability will get
sampled more frequently in the subsequent iterations  Algorithm   guarantees low regret for the online convex optimization problem   which in turn yields the following
guarantee for Algorithm  
Theorem   Under Assumption   the adaptive updates in
Algorithm   with        

achieve

min

Algorithm   Coordinate Descent with Adaptive Sampling

Input  Stepsize        Threshold pmin     with

         Rb

               pmin 

Initialize                
for                
Sample Jt   pt
Choose      according to Algorithm  
Update   
 Jt    argminx XJt     Jt xt 
        
xt 
Update    forb        de ned in  
wt    pt exp pb   Jt xt pt
pt    argminq   Dkl   wt 
return  xT    

   xt

pt

Jt

Jt eJt 

  PT

     xt

 Jt 

    

 

dx          we have

By using   bandit algorithm another coordinate descent
method  to update    we show that our updates achieve
performance comparable to the best stationary probability
in    in hindsight  To this end  we  rst bound the regret  
by the regret of   linear bandit problem  By convexity of
       and  
  
     
TXt 
bXj     xt 
   
pt
  
       xt 
TXt 
 
  

  pt     
 

pj 
  ob

Now  let us view the vector   as the loss vector for   constrained linear bandit problem with feasibility region    
We wish to apply EXP   due to Auer et al    or equivalently     sparse mirror descent to   with            log  
 see  for example  Section   of Bubeck and CesaBianchi
  for the connections  However  EXP  requires the
loss values be positive in order to be in the region where
   is strongly convex  so we scale our problem using the
fact that   and pt   are probability vectors  Namely 

  
 

 pt

 

  

 

 pt

 

  

        xt 
TXt 
EhDb   xt  pt   pEi  
TXt 
                  

  

 pt

where

  ob

  

  pt     

  

 

  
bp 

min

 

 

Using scaled loss values  we perform EXP   Algorithm
 
Intuitively  we penalize the probability of the sampled block by the strength of the signal on the block  The

 

        log  
  
bXj 
TXt 
  
 RL
pbT pmin

 

   best in hindsight

pj

kG   xt   
  

 
 
log   bT   
pmin    

 

      xT    

  

   

  vuuut min
 
     
  pmin    log  
  
 
 

   regret for bandit problem

 LR

 

 

where       xT          xT     inf          
See Section    for the proof  Note that there is   tradeoff
in the regret bound   in terms of pmin  for small pmin 
the  rst term is small  as the the set    is large  but second  regret  term is large  and vice versa  To interpret the
bound   take pmin      for some         The  rst
term dominates the remainder as long as       log   
we require      bR    to guarantee convergence of
coordinate descent in Proposition   so that we roughly expect the  rst term in the bound   to dominate  Thus  Algorithm   attains the best convergence guarantee for the
optimal stationary sampling distribution in hindsight 

  Ef cient updates for  
The updates for   in Algorithm   can be done in   log   
time by using   balanced binary tree  Let Dkl        
Pd
   pi log pi
denote the KullbackLeibler divergence beqi
tween   and   
Ignoring the subscript on   so that    
wt      pt and     Jt  the new probability vector   is
given by the minimizer of

Dkl                       pmin 

 

where   is the previous probability vector   modi ed only
at the index    We store   in   binary tree  keeping values up to their normalization factor  At each node  we
also store the sum of elements in the left right subtree for

Adaptive Sampling Probabilities for NonSmooth Optimization

Algorithm   KL Projection

  Input     pJ  wJ  mass  Pi wi
  wcand   pJ   mass 
  if wcand mass wJ   wcand    pmin then
wcand   pmin
 
 pmin
  Update wcand    

 mass wJ  

Algorithm

ACD

UCD
GD

     
log   
    log

   
 
   
   

 

   

   
 
   
 

       
  
    log

   
 
   
   

  log  
  log  

 

   

ef cient sampling  for completeness  the pseudocode for
sampling from the binary tree in   log    time is given in
Section    The total mass of the tree can be accessed by
inspecting the root of the tree alone 
The following proposition shows that it suf ces to touch at
most one element in the tree to do the update  See Section  
for the proof 
Proposition   The solution to   is given by

Table   Runtime comparison  computations needed to guarantee  optimality gap  under heavytailed block structures 
ACD adaptive coordinate descent  UCD uniform coordinate descent  GD gradient descent

tails    zU              where   is   uniform random
variable over             
Take Cj       for                  and        First  we
show that although for the uniform distribution       

wj

if wJ   pmin pJ  
 pmin
otherwise

 

dXj 

  kGj xt   
 

  

 

   

dXj 

  

        log   

qj    
qJ  

 

 pJ  wJ
 pmin
wj
 pJ
 

 pJ  wJ
pmin

  if wJ   pmin pJ  
 pmin

otherwise 

As seen in Algorithm   we need to modify at most one
element in the binary tree  Here  the update function modi es the value at index   and propagates the value up the
tree so that the sum of left right subtrees are appropriately
updated  We provide the full pseudocode in Section   

  Example
The optimality guarantee given in Theorem   is not directly
interpretable since the term     in the upper bound  
is only optimal given the iterates            xT despite the
fact that xt   themselves depend on the sampling probabilities  Hence  we now study   setting where we can further
bound   to get   explicit regret bound for Algorithm   that
is provably better than nonadaptive counterparts  Indeed 
under certain structural assumptions on the problem similar
to those of McMahan and Streeter   and Duchi et al 
  our adaptive sampling algorithm provably achieves
regret polynomially better in the dimension than either using   uniform sampling distribution or gradient descent 
Consider the SVM objective

       

 
 

nXi    yiz     

where   is small and   is large  Here          
 

       yiz          zi  Assume that for some
nPn
 xed       and Lj       we have  jf      
nPn
   In particular  this is the case if we
have sparse features zU          with power law

    zi        

 

the term     in   can be orders of magnitude smaller 
Proposition   Let        pmin      for some        
and Cj      
        for some
      then
dXj 

If kGj     
  Gj xt 

      
   log   

if      
if        

      pmin

    

min

pj

 

 

We defer the proof of the proposition to Section    Using
this bound  we can show explicit regret bounds for Algorithm   From Theorem   and Proposition   we have that
Algorithm   attains

      xT         log dpT

   

RpT

 
      

if      
if        
    Rd     log      

Setting above to be less than   and inverting with respect to
    we obtain the iteration complexity in Table  
To see the runtime bounds for uniformly sampled coordinate descent and gradient descent  recall the regret
bound   given in Proposition   Plugging pt
       in
the bound  we obtain

      xT       Rplog dp dT  
for                 where      Pd
larly  gradient descent with               attains
      xT       Rplog dp    

Since each gradient descent update takes      we obtain
the same runtime bound 

   Simi 

     

Adaptive Sampling Probabilities for NonSmooth Optimization

While nonadaptive algorithms such as uniformlysampled
coordinate descent or gradient descent have the same runtime for all   our adaptive sampling method automatically
tunes to the value of   Note that for       the  rst
term in the runtime bound for our adaptive method given in
Table   is strictly better than that of uniform coordinate descent or gradient descent  In particular  for       the
best stationary sampling distribution in hindsight yields an
improvement that is at most      better in the dimension 
However  due to the remainder terms for the bandit problem  this improvement only matters      rst term is larger
than second  when

   

  Rd   
  Rd  

 plog   
if      
    log     if        

In Section   we show that these remainder terms can be
made smaller than what their upper bounds indicate  Empirically  our adaptive method outperforms the uniformlysampled counterpart for larger values of   than above 

  Adaptive probabilities for stochastic

gradient descent

Consider the empirical risk minimization problem

       

 

minimize

fi           

nXi 

where    Rd is   closed convex set and fi  are convex functions  Let            Cb be   partition of the   samples so that each example belongs to some Cj    set of size
nj    Cj   note that the index   now refers to blocks of examples instead of coordinates  These block structures naturally arise  for example  when Cj   are the examples with
the same label in   multiclass classi cation problem  In
this stochastic optimization setting  we now sample   block
Jt   pt at each iteration    and perform gradient updates
using   gradient estimate on the block CJt  We show how
  similar adaptive updating scheme for pt   again achieves
the optimality guarantees given in Section  

  Mirror descent with nonstationary probabilities
Following the approach of  Nemirovski et al    we
run mirror descent for the updates on    At iteration
     block Jt is drawn from   bdimensional probability vector pt  We assume that we have access to unbiased stochastic gradients Gj    for each block  That is 
 fi    In particular  the estimate
  Gj       
GJt xt     fIt    where It is drawn uniformly in CJt
gives the usual unbiased stochastic gradient of minibatch
size   The other extreme is obtained by using   minibatch
size of nj where GJt xt     
 fi    Then 

njPi Cj

nJt Pi CJt

GJt xt  is an un 

the importance sampling estimator nJt
npt
Jt
biased estimate for the subgradient of the objective 
Let   be   differentiable  strongly convex function on
  with respect to the norm     as before and denote by
the dual norm of      Let                        
    
          be the Bregman divergence associated with
   In this section  we assume the below  standard  bound 
Assumption   For all           we have             
and kGj     
We use these stochastic gradients to perform mirror updates  replacing the update in Algorithm   with the update

      for                 

npt

      nJt

Jt  GJt  xt      

xt    argmin
From   standard argument        Nemirovski et al   
we obtain the following convergence guarantee  The proof
follows an argument similar to that of Proposition  
Proposition   Under Assumption   the updates   attain

     xt   

 
  

 

      xT    

  
 xT

 

  
  

TXt 

  kGj xt   
  
 

  pt
 

bXj 

  

     

 

  

    

  
Jl
    

where       xT          xT     inf          
Again  we wish to choose the optimal step size    that
minimizes the regret bound   To this end  modify
the doubling trick given in Algorithm   as follows  use
for the second while condition 

  Jl  GJl xl 
   
TXt 

Pt
and stepsizes                   maxj   
min    
  
bXj 
  Gj xt 
     
log       
bXj 

similar to Proposition   we have

      xT      

  

pminT log  

  RL

    Then 

  
 
  pt

maxj nj

pmin

  Adaptive probabilities
Now  we consider an adaptive updating scheme for pt  
similar to Section   Using the scaled gradient estimate

  
 

 

 

 

 

 
 

 

npt

 

    

   maxj   
 

  kGj     

to run EXP  we obtain Algorithm   Again  the additive

             nj
scaling   maxj nj npmin  is to ensure thatb      As in

Section   the updates for   in Algorithm   can be done in
  log    time  We can also show similar optimality guarantees for Algorithm   as before  The proof is essentially
the same to that given in Section   

min

Adaptive Sampling Probabilities for NonSmooth Optimization

Algorithm   Mirror Descent with Adaptive Sampling

Input  Stepsize       
Initialize                
for                
Sample Jt   pt
Choose      according to  modi ed  Algorithm  
Update   
Jt   argminx     
Jt  GJt  xt        
xt 
Update   
wt    pt exp pb   Jt xt pt
pt    argminq   Dkl   wt 
return  xT    

     xt

Jt eJt 

   xt

    

pt

Jt 

  PT

Theorem   Let       maxj nj
Algorithm   with       

pminn   Under Assumption  

bT

  
 

achieves

       log  
  
 
TXt 
bXj 
  pj GJt xt 
  
log       
  RW
bXj 
  log  

pmin

  
 

      xT    

  
 

min
   

         log   

 

   

where       xT          xT     inf          
With equal block sizes nj       and pmin      for
some         the  rst term in the boudn of Theorem   is        which dominates the second term if
  log    Since we usually have        for
   
SGD  as long as       log    we have

      xT         

 

Tvuuut min

   

TXt 

  

bXj     xt 

pj

  

 

 CA  

That is  Algorithm   attains the best regret bound achieved
by the optimal stationary distribution in hindsight had the
xt   had remained the same  Further  under similar structural assumptions kGj     
       as in Section   we
can prove that the regret bound for our algorithm is better
than that of the uniform distribution 

  Experiments
We compare performance of our adaptive approach with
stationary sampling distributions on real and synthetic
datasets  To minimize parameter tuning  we       at the
value suggested by theory in Theorems   and   However 
we make   heuristic modi cation to our adaptive algorithm
since rescaling the bandit gradient   and   dwarfs the
signals in gradient values if   is too large  We present
performance of our algorithm with respect to multiple estimates of the Lipschitz constant          for       where

  is the actual upper bound  We tune the stepsize    for
both methods  using the form  pt and tuning  
For all our experiments  we compare our method against
the uniform distribution and blockwise Lipschitz sampling
distribution pj   Lj where Lj is the Lipschitz constant
of the jth block  Zhao and Zhang    We observe
that the latter method often performs very well with respect to iteration count  However  since computing the
blockwise Lipschitz sampling distribution takes   nd  the
method is not competitive in largescale settings  Our algorithm  on the other hand  adaptively learns the latent structure and often outperforms stationary counterparts with respect to runtime  While all of our plots are for   single
run with   random seed  we can reject the null hypothesis
adaptive  at   con dence for all inf  xT
stances where our theory guarantees it  We take           
throughout this section 

uniform       xT

 

  Adaptive sampling for coordinate descent
Synthetic Data We begin with coordinate descent   rst
verifying the intuition of Section   on   synthetic dataset 
  kAx   bk 
We consider the problem minimizekxk 
and we endow     Rn   with the following block structure  the columns are drawn as aj            Thus 
the gradients of the columns decay in   heavytailed manner as in Section   so that   
       We set        
      the effects of changing ratios     and     manifest themselves via relative norms of the gradients in the
columns  which we control via   instead  We run all experiments with pmin      and multiple values of   
Results are shown in Figure   where we show the optimality gap vs  runtime in     and the learned sampling
distribution in     Increasing    stronger block structure 
improves our relative performance with respect to uniform
sampling and our ability to accurately learn the underlying
block structure  Experiments over more   and   in Section
  further elucidate the phase transition from uniformlike
behavior to regimes learning exploiting structure 
We also compare our method with  nonpreconditioned 
SGD using leverage scores pj   kajk  given by  Yang
et al    The leverage scores       sampling distribution proportional to blockwise Lipschitz constants  roughly
correpond to using pj      which is the stationary
distribution that minimizes the bound   in this synthetic
setting  this sampling probability coincides with the actual
block structure  Although this is expensive to compute  taking   nd  time  it exploits the latent block structure very
well as expected  Our method quickly learns the structure
and performs comparably with this  optimal  distribution 

 We guarantee   positive loss by taking max           

 

 

 

 

 

 

 

 

 

 

Adaptive Sampling Probabilities for NonSmooth Optimization

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    Optimality gap

    Optimality gap

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    Learned sampling distribution

Figure   Adaptive coordinate descent  left to right         

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    Learned sampling distribution

Figure   Adaptive SGD  left to right         

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    Optimality gap

    Learned distribution

 

 

 

 

 

 

 

 

 

 

 

 

    CUB 

    ALOI

Figure   Model selection for nucleotide sequences

Figure   Optimality gap for CUB  and ALOI

Model selection Our algorithm   ability to learn underlying block structure can be useful in its own right as an online feature selection mechanism  We present one example
of this task  studying an aptamer selection problem  Cho
et al    which consists of       nucleotide sequences  aptamers  that are onehot encoded with all kgrams of the sequence  where           so that    
    We train an   regularized SVM on the binary
labels  which denote  thresholded  binding af nity of the
aptamer  We set the blocksize as   features       
and pmin       Results are shown in Figure   where
we see that adaptive feature selection certainly improves
training time in     The learned sampling distribution depicted in     for the best case        places larger weight
on features known as Gcomplexes  these features are wellknown to affect binding af nities  Cho et al   

  Adaptive sampling for SGD
Synthetic data We use the same setup as in Section  
but now endow block structure on the rows of   rather than
the columns  In Figure   we see that when there is little
block structure       all sampling schemes perform
similarly  When the block structure is apparent      
our adaptive method again learns the underlying structure

and outperforms uniform sampling  We provide more experiments in Section   to illustrate behaviors over more
  and   We note that our method is able to handle online data streams unlike stationary methods such as leverage scores 

CUB ALOI We apply our method to two
multiclass object detection datasets  CaltechUCSD
Birds   Wah et al    and ALOI  Geusebroek
et al    Labels are used to form blocks so that      
for CUB and       for ALOI  We use softmax loss for
CUB  and   binary SVM loss for ALOI  where
in the latter we do binary classi cation between shells and
nonshell objects  We set pmin      to enforce enough
exploration  For the features  outputs of the last fullyconnected layer of ResNet   He et al    are used
for CUB so that we have  dimensional features  Since
our classi er   is        dimensional  this is   fairly large
scale problem  For ALOI  we use default histogram features        In each case  we have       and    
    respectively  We use          Rm   kxk      
where       for CUB and       for ALOI  We observe
in Figure   that our adaptive sampling method outperforms
stationary counterparts 

Adaptive Sampling Probabilities for NonSmooth Optimization

Acknowledgements
HN was supported by the Samsung Scholarship  AS and
SY were supported by Stanford Graduate Fellowships and
AS was also supported by   Fannie   John Hertz Foundation Fellowship  JCD was supported by NSFCAREER 
 

References
   AllenZhu and    Yuan  Even faster accelerated coordinate descent using nonuniform sampling  arXiv preprint
arXiv   

   Auer     CesaBianchi  and    Fischer  Finitetime analysis of the multiarmed bandit problem  Machine Learning     

      Duchi     Hazan  and    Singer  Adaptive subgradient
methods for online learning and stochastic optimization 
Journal of Machine Learning Research   
 

     Geusebroek        Burghouts  and       Smeulders 
The amsterdam library of object images  International
Journal of Computer Vision     

   Gopal  Adaptive sampling for sgd by exploiting side
information  In Proceedings of The  rd International
Conference on Machine Learning  pages    

   He     Zhang     Ren  and    Sun  Deep residual learning for image recognition  In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pages    

   Beck and    Teboulle  Mirror descent and nonlinear
projected subgradient methods for convex optimization 
Operations Research Letters     

   Johnson and    Zhang  Accelerating stochastic gradient
descent using predictive variance reduction  In Advances
in Neural Information Processing Systems    

   Beck and    Tetruashvili  On the convergence of block
coordinate descent type methods  SIAM Journal on Optimization     

   Bubeck and    CesaBianchi 

Regret analysis of
stochastic and nonstochastic multiarmed bandit problems  Foundations and Trends in Machine Learning   
   

   CesaBianchi and    Lugosi  Prediction  learning  and

games  Cambridge University Press   

   Cho        Oh     Nie     Stewart     Eisenstein 
   Chambers        Marth     Walker        Thomson 
and       Soh  Quantitative selection and parallel characterization of aptamers  Proceedings of the National
Academy of Sciences     

   Csiba and    Richt arik  Importance sampling for mini 

batches  arXiv preprint arXiv   

   Csiba     Qu  and    Richt arik  Stochastic dual coordinate ascent with adaptive probabilities  arXiv preprint
arXiv   

      Dang and    Lan 

Stochastic block mirror descent methods for nonsmooth and stochastic optimization  SIAM Journal on Optimization   
 

   Defazio     Bach  and    LacosteJulien  SAGA   
fast incremental gradient method with support for nonstrongly convex composite objectives 
In Advances in
Neural Information Processing Systems    

      Lee and    Sidford  Ef cient accelerated coordinate
descent methods and faster algorithms for solving linear
systems  In  th Annual Symposium on Foundations of
Computer Science  pages   IEEE   

   Lu and    Xiao  On the complexity analysis of randomized blockcoordinate descent methods  Mathematical
Programming     

   McMahan and    Streeter  Adaptive bound optimization for online convex optimization  In Proceedings of
the Twenty Third Annual Conference on Computational
Learning Theory   

   Necoara     Nesterov  and    Glineur    random coordinate descent method on large optimization problems with linear constraints  University Politehnica
Bucharest  Tech  Rep   

   Needell     Ward  and    Srebro  Stochastic gradient
descent  weighted sampling  and the randomized Kaczmarz algorithm  In Advances in Neural Information Processing Systems   pages    

   Nemirovski and    Yudin  Problem Complexity and

Method Ef ciency in Optimization  Wiley   

   Nemirovski     Juditsky     Lan  and    Shapiro  Robust stochastic approximation approach to stochastic
programming  SIAM Journal on Optimization   
   

   Nesterov  Ef ciency of coordinate descent methods on
hugescale optimization problems  SIAM Journal on Optimization     

Adaptive Sampling Probabilities for NonSmooth Optimization

   Nutini     Schmidt        Laradji     Friedlander  and
   Koepke  Coordinate descent converges faster with
the gausssouthwell rule than random selection  arXiv
preprint arXiv   

   Richt arik and    Tak      Iteration complexity of randomized blockcoordinate descent methods for minimizing  
composite function  Mathematical Programming   
   

   Richt arik and    Tak     

Parallel coordinate deMath 
scent methods for big data optimization 
ematical Programming 
 
URL http link springer com article 
   

page Online  rst 

   ShalevShwartz  Online learning and online convex optimization  Foundations and Trends in Machine Learning 
   

   ShalevShwartz and    Zhang 

Proximal stochastic
dual coordinate ascent  arXiv preprint arXiv 
 

   Strohmer and    Vershynin    randomized Kaczmarz algorithm with exponential convergence  Journal
of Fourier Analysis and Applications   
 

   Wah     Branson     Welinder     Perona  and    Belongie  The CaltechUCSD Birds  Dataset 
Technical Report CNSTR  California Institute of Technology   

   Yang       Chow          and       Mahoney  Weighted
sgd for   regression with randomized preconditioning  In Proceedings of the TwentySeventh Annual ACMSIAM Symposium on Discrete Algorithms  pages  
  Society for Industrial and Applied Mathematics 
 

   Zhao and    Zhang 

Accelerating minibatch
stochastic gradient descent using strati ed sampling 
arXiv   stat ML   

   Zhao and    Zhang  Stochastic optimization with importance sampling  In Proceedings of the  nd International
Conference on Machine Learning   

