The Shattered Gradients Problem 

If resnets are the answer  then what is the question 

David Balduzzi   Marcus Frean   Lennox Leary   JP Lewis     Kurt WanDuo Ma   Brian McWilliams  

Abstract

  longstanding obstacle to progress in deep
learning is the problem of vanishing and exploding gradients  Although  the problem has
largely been overcome via carefully constructed
initializations and batch normalization  architectures incorporating skipconnections such as
highway and resnets perform much better than
standard feedforward architectures despite wellchosen initialization and batch normalization  In
this paper  we identify the shattered gradients
problem  Speci cally  we show that the correlation between gradients in standard feedforward networks decays exponentially with depth
resulting in gradients that resemble white noise
whereas  in contrast  the gradients in architectures with skipconnections are far more resistant to shattering  decaying sublinearly  Detailed
empirical evidence is presented in support of the
analysis  on both fullyconnected networks and
convnets  Finally  we present   new  looks linear   LL  initialization that prevents shattering 
with preliminary experiments showing the new
initialization allows to train very deep networks
without the addition of skipconnections 

  Introduction
Deep neural networks have achieved outstanding performance  Krizhevsky et al    Szegedy et al    He
et al      Reducing the tendency of gradients to vanish or explode with depth  Hochreiter    Bengio et al 
  has been essential to this progress 
Combining careful initialization  Glorot   Bengio   

 Equal

contribution

ton  New Zealand  SEED  Electronic Arts
search    urich  Switzerland 
Balduzzi  dbalduzzi gmail com 
 brian disneyresearch com 

 Victoria University of Welling 
 Disney ReCorrespondence to  David
Brian McWilliams

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

He et al    with batch normalization  Ioffe   Szegedy 
  bakes two solutions to the vanishing exploding gradient problem into   single architecture  The He initialization ensures variance is preserved across recti er layers  and batch normalization ensures that backpropagation
through layers is unaffected by the scale of the weights
 Ioffe   Szegedy   
It is perhaps surprising then that residual networks  resnets 
still perform so much better than standard architectures
when networks are suf ciently deep  He et al       
This raises the question  If resnets are the solution  then
what is the problem  We identify the shattered gradient
problem    previously unnoticed dif culty with gradients
in deep recti er networks that is orthogonal to vanishing
and exploding gradients  The shattering gradients problem
is that  as depth increases  gradients in standard feedforward networks increasingly resemble white noise  Resnets
dramatically reduce the tendency of gradients to shatter 
Our analysis applies at initialization  Shattering should decrease during training  Understanding how shattering affects training is an important open problem 
Terminology  We refer to networks without skip connections as feedforward nets in contrast to residual nets
 resnets  and highway nets  We distinguish between the
realvalued output of   recti er and its binary activation 
the activation is   if the output is positive and   otherwise 

  The Shattered Gradients Problem
The  rst step is to simply look at the gradients of neural networks  Gradients are averaged over minibatches  depend
on both the loss and the random sample from the data  and
are extremely highdimensional  which introduces multiple
confounding factors and makes visualization dif cult  but
see section   We therefore construct   minimal model
designed to eliminate these confounding factors  The minimal model is   neural network fW         taking scalars
to scalars  each hidden layer contains       recti er
neurons  The model is not intended to be applied to real
data  Rather  it is   laboratory where gradients can be isolated and investigated 
We are interested in how the gradient varies  at initializa 

The Shattered Gradients Problem

 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

     layer feedforward       layer feedforward 

     layer resnet 

    Brown noise 

    White noise 

Figure   Comparison between noise and gradients of recti er nets with   neurons per hidden layer  Columns
     brown and white noise  Columns      Gradients of neural nets plotted for inputs taken from   uniform grid  The
 layer net uses meancentering  The  layer net uses batch normalization with       see Eq   

tion  as   function of the input 

dfW
dx

      where            is in  

 dim grid of        data points 

 

 nj
 wij

   fW
 nj

Updates during training depend on derivatives with respect
to weights  not inputs  Our results are relevant because  by
the chain rule   fW
  Weight updates thus de 
 wij
pend on  fW
      how the output of the network varies
 nj
with the output of neurons in one layer  which are just inputs to the next layer 
The top row of  gure   plots dfW
dx       for each point     
in the  dim grid  The bottom row shows the  absolute
value  of the covariance matrix 
               
 
where   is the  vector of gradients     the mean  and  
 
the variance 
If all the neurons were linear then the gradient would be
  horizontal line       the gradient would be constant as  
function of    Recti ers are not smooth  so the gradients
are discontinuous 

    Set                

Gradients of shallow networks resemble brown noise 
Suppose the network has   single hidden layer  fw       
             Following Glorot   Bengio  
weights   and biases   are sampled from       with
     
Figure    shows the gradient of the network for inputs
        and its covariance matrix  Figure    shows
  discrete approximation to brownian motion  BN      
Pt
    The plots are strikingly
   Ws where Ws       
similar  both clearly exhibit spatial covariance structure 
The resemblance is not coincidental  section    applies

Donsker   theorem to show the gradient converges to brownian motion as      
Gradients of deep networks resemble white noise  Figure    shows the gradient of    layer fullyconnected recti er network  Figure    shows white noise given by samples Wk        Again  the plots are strikingly similar 
Since the inputs lie on    dim grid  it makes sense to
compute the autocorrelation function  ACF  of the gradient  Figures    and    compare this function for feedforward networks of different depth with white and brown
noise  The ACF for shallow networks resembles the ACF
of brown noise  As the network gets deeper  the ACF
quickly comes to resemble that of white noise 
Theorem   explains this phenomenon  We show that correlations between gradients decrease exponentially  
   with
depth in feedforward recti er networks 

Training is dif cult when gradients behave like white
noise  The shattered gradient problem is that the spatial
structure of gradients is progressively obliterated as neural
nets deepen  The problem is clearly visible when inputs
are taken from   onedimensional grid  but is dif cult to
observe when inputs are randomly sampled from   highdimensional dataset 
Shattered gradients undermine the effectiveness of algorithms that assume gradients at nearby points are similar such as momentumbased and accelerated methods
 Sutskever et al    Balduzzi et al    If dfW
bednj
haves like white noise  then   neuron   effect on the output
of the network  whether increasing weights causes the network to output more or less  becomes extremely unstable

The Shattered Gradients Problem

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

Depth
 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

Brown noise
White noise

 

 

Lag

 

 

 

Lag

 

 

 

Lag

 

 

 

Lag

 

    Feedforward nets 

    Resnets      

    Resnets      

    White and brown noise 

Figure   Autocorrelation Function  ACF  Comparison of the ACF between white and brown noise  and feedforward
and resnets of different depths  Average over   runs 

making learning dif cult 

Gradients of deep resnets lie in between brown and
white noise 
Introducing skipconnections allows much
deeper networks to be trained  Srivastava et al    He
et al        Greff et al    Skipconnections significantly change the correlation structure of gradients  Figure    shows the concrete example of    layer resnet
which has markedly more structure than the equivalent
feedforward net  gure     Figure    shows the ACF of
resnets of different depths  Although the gradients become
progressively less structured  they do not whiten to the extent of the gradients in standard feedforward networks 
there are still correlations in the  layer resnet whereas in
the equivalent feedforward net  the gradients are indistinguishable from white noise  Figure    shows the dramatic
effect of recently proposed  rescaling  Szegedy et al 
 
the ACF of even the   layer network resemble
brownnoise 
Theorem   shows that correlations between gradients decay
sublinearly with depth  pL for resnets with batch normalization  We also show  corollary   that modi ed highway
networks  where the gates are scalars  can achieve   depth
independent correlation structure on gradients  The analysis explains why skipconnections  combined with suitable
rescaling  preserve the structure of gradients 

  Outline
Section   shows that batch normalization increases neural
ef ciency  We explore how batch normalization behaves
differently in feedforward and resnets  and draw out facts
that are relevant to the main results 
The main results are in section   They explain why gradients shatter and how skipconnections reduce shattering  The proofs are for   mathematically amenable model 
fullyconnected recti er networks with the same number of
hidden neurons in each layer  Section   presents empirical
results which show gradients similarly shatter in convnets
for real data  It also shows that shattering causes average

gradients over minibatches to decrease with depth  relative
to the average variance of gradients 
Finally  section   proposes the LLinit  looks linear initialization  which eliminates shattering  Preliminary experiments show the LLinit allows training of extremely deep
networks   layers  without skipconnections 
  Related work
Carefully initializing neural networks has led to   series
of performance breakthroughs dating back  at least  to the
unsupervised pretraining in Hinton et al    Bengio
et al    The insight of Glorot   Bengio   is
that controlling the variance of the distributions from which
weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and
error signals  More recently  He et al    re ned the
approach to take recti ers into account  Recti ers effectively halve the variance since  at initialization and on average  they are active for half their inputs  Orthogonalizing
weight matrices can yield further improvements albeit at  
computational cost  Saxe et al    Mishkin   Matas 
  The observation that the norms of weights form  
random walk was used by Sussillo   Abbott   to tune
the gains of neurons 
In short  it has proven useful to treat weights and gradients
as random variables  and carefully examine their effect on
the variance of the signals propagated through the network 
This paper presents   more detailed analysis that considers
correlations between gradients at different datapoints 
The closest work to ours is Veit et al    which shows
resnets behave like ensembles of shallow networks  We
provide   more detailed analysis of the effect of skipconnections on gradients    recent paper showed resnets
have universal  nitesample expressivity and may lack spurious local optima  Hardt   Ma    but does not explain
why deep feedforward nets are harder to train than resnets 
An interesting hypothesis is that skipconnections improve
performance by breaking symmetries  Orhan   

The Shattered Gradients Problem

The increased ef ciency comes at   price  The raster plot
for feedforward networks resembles static television noise 
the spatial structure is obliterated  Resnets  Figure     exhibit   compromise where neurons are utilized ef ciently
but the spatial structure is also somewhat preserved  The
preservation of spatial structure is quanti ed via the contiguity histograms which counts long runs of consistent activation  Resnets maintain   broad distribution of contiguity
even with deep networks whereas batch normalization on
feedforward nets shatters these into small sections 

  Analysis
This section analyzes the correlation structure of gradients
in neural nets at initialization  The main ideas and results
are presented  the details provided in section   
Perhaps the simplest way to probe the structure of   random process is to measure the  rst few moments  the mean 
variance and covariance  We investigate how the correlation between typical datapoints  de ned below  changes
with network structure and depth  Weaker correlations
correspond to whiter gradients  The analysis is for fullyconnected networks  Extending to convnets involves  signi cant  additional bookkeeping 

Proof strategy  The covariance de nes an inner product
on the vector space of realvalued random variables with
mean zero and  nite second moment  It was shown in Balduzzi et al    Balduzzi   that the gradients in
neural nets are sums of pathweights over active paths  see
section    The  rst step is to observe that pathweights are
orthogonal with respect to the variance inner product  To
express gradients as linear combinations of pathweights is
thus to express them over an orthogonal basis 
Working in the pathweight basis reduces computing the
covariance between gradients at different datapoints to
counting the number of coactive paths through the network  The second step is to count coactive paths and adjust
for rescaling factors       due to batch normalization 
The following assumption is crucial to the analysis 
Assumption    typical datapoints  We say      and     
are typical datapoints if half of neurons per layer are active
for each and   quarter per layer are coactive for both  We
assume all pairs of datapoints are typical 

The assumption will not hold for every pair of datapoints 
Figure   shows the assumption holds  on average  under
batch normalization for both activations and coactivations 
The initialization in He et al    assumes datapoints
activate half the neurons per layer  The assumption on
coactivations is implied by  and so weaker than  the assumption in Choromanska et al    that activations are
Bernoulli random variables independent of the inputs 

Figure   Activations and coactivations in feedforward
networks  Plots are averaged over   fully connected recti er networks with   hidden units per layer  Without
BN  solid  With BN  dotted  Activations  green  Proportion of inputs for which neurons in   given layer are active 
on average  Coactivations  blue  Proportion of distinct
pairs of inputs for which neurons are active  on average 

  Observations on batch normalization
Batch normalization was introduced to reduce covariate
shift  Ioffe   Szegedy    However  it has other effects that are less wellknown   and directly impact the
correlation structure of gradients  We investigate the effect
of batch normalization on neuronal activity at initialization
      when it meancenters and rescales to unit variance 
We  rst investigate batch normalization   effect on neural
activations  Neurons are active for half their inputs on average   gure   with or without batch normalization  Figure  
also shows how often neurons are coactive for two inputs 
  of
With batch normalization  neurons are coactive for  
distinct pairs of inputs  which is what would happen if activations were decided by unbiased coin  ips  Without batch
normalization  the coactive proportion climbs with depth 
suggesting neuronal responses are increasingly redundant 
Resnets with batch normalization behave the same as feedforward nets  not shown 
Figure   takes   closer look  It turns out that computing the
proportion of inputs causing neurons to be active on average is misleading  The distribution becomes increasingly
bimodal with depth  In particular  neurons are either always
active or always inactive for layer   in the feedforward net
without batch normalization  blue histogram in  gure    
Batch normalization causes most neurons to be active for
half the inputs  blue histograms in  gures      
Neurons that are always active may as well be linear  Neurons that are always inactive may as well not exist  It follows that batch normalization increases the ef ciency with
which recti er nonlinearities are utilized 

The Shattered Gradients Problem

 

 
 
 
 
 

 
 

 
 
 
 
 

 
 

 
 
 
 
 

    Feedforward net without batch norm 

    Feedforward with batch normalization 

    Resnet with batch normalization 

Figure   Activation of recti ers in deep networks 
Rasterplots  Activations of hidden units  yaxis  for inputs
indexed by the xaxis  Left histogram  activation per unit  distribution of average activation levels per neuron  Right
histogram  contiguity  distribution of  contiguity   length of contiguous sequences of   or   along rows in the raster plot 

Correlations between gradients  Weight updates in  
neural network are proportional to

 wjk  

 mbXi 

PXp 

 
 fp

 fp
 nk

 nk

 wjk     

where fp is the pth coordinate of the output of the network
and nk is the output of the kth neuron  The derivatives  
 fp
and  nk
do not depend on the network   internal struc 
 wjk
ture  We are interested in the middle term  fp
  which
 nk
does  It is mathematically convenient to work with the sum
   fp over output coordinates of the network  Section  
shows that our results hold for convnets on realdata with
the crossentropy loss  See also remark   

PP
De nition   Let ri   PP
 fp
         be the derivative with respect to neuron   given input           For
each input      the derivative ri is   realvalued random
variable  It has mean zero since weights are sampled from
distributions with mean zero  Denote the covariance and
correlation of gradients by

  

            ri rj  and          

  ri rj 
qE   

          
   

 

where the expectations are       the distribution on weights 

  Feedforward networks
Without loss of generality  pick   neuron   separated from
the output by   layers  The  rst major result is
Theorem    covariance of gradients in feedforward nets 
Suppose weights are initialized with variance      
  following He et al    Then

   The variance of the gradient at      is Cfnn       
   The covariance is Cfnn          
    
Part     recovers the observation in He et al    that
setting      
  preserves the variance across layers in recti er networks  Part     is new  It explains the empirical
observation   gure     that gradients in feedforward nets
whiten with depth 
Intuitively  gradients whiten because
the number of paths through the network grows exponentially faster with depth than the fraction of coactive paths 
see section    for details 

  Residual networks
The residual modules introduced in He et al      are

xl   xl    Wl BN Vl BN  xl 

where  BN        BN     and       max     is the
recti er  We analyse the strippeddown variant

 

xl      xl        Wl BN  xl 

where   and   are rescaling factors  Dropping Vl BN
makes no essential difference to the analysis  The  
rescaling was introduced in Szegedy et al    where
it was observed setting         reduces instability 
We include   for reasons of symmetry 
Theorem    covariance of gradients in resnets  Consider
  resnet with batch normalization disabled and        
  Suppose      

  as above  Then

   The variance of the gradient at      is Cres         

The Shattered Gradients Problem

   The covariance is Cres          
   
The correlation is Rres          
   

The theorem implies there are two problems in resnets
without batch normalization      the variance of gradients
grows and  ii  their correlation decays exponentially with
depth  Both problems are visible empirically 

  Rescaling in Resnets
  solution to the exploding variance of resnets is to rescale
layers by         which yields

Cres
          and Rres

            
  

 BN                
 BN          pL  and

and so controls the variance but the correlation between
gradients still decays exponentially with depth  Both theoretical predictions hold empirically 
In practice   rescaling is not used  Instead  activations are
rescaled by batch normalization  Ioffe   Szegedy   
and  more recently  setting         per Szegedy et al 
  The effect is dramatic 
Theorem    covariance of gradients in resnets with BN and
rescaling  Under the assumptions above  for resnets with
batch normalization and  rescaling 
   the variance is Cres
   the covariance  is Cres
the correlation is Rres
The theorem explains the empirical observation   gure    
that gradients in resnets whiten much more slowly with
depth than feedforward nets  It also explains why setting
  near zero further reduces whitening 
Batch normalization changes the decay of the correlations
from  
   to  pL  Intuitively  the reason is that the variance
of the outputs of layers grows linearly  so batch normalization rescales them by different amounts  Rescaling by
  introduces   constant factor  Concretely  the model predicts using batch normalization with       on    
layer resnet gives typical correlation Rres
 BN          
Setting       gives Rres
 BN           By contrast   
 layer feedforward net has correlation indistinguishable
from zero 

 BN          
 pL

 

  Highway networks
Highway networks can be thought of as   generalization of
resnets  that were in fact introduced slightly earlier  Srivas 

 See section    for exact computations 

tava et al    Greff et al    The standard highway
network has layers of the form

xl        xl    xl       xl      xl 
where     and    are learned gates and features respectively  Consider the following modi cation where   and
  are scalars satisfying  

     

     

xl       xl        Wl xl 

The module can be recovered by judiciously choosing  
and   in equation   However  it is worth studying in its
own right 
Corollary    covariance of gradients in highway networks  Under the assumptions above  for modi ed highway networks with  rescaling 

   the variance of gradients is CHN
          
   the correlation is RHN
In particular  if           
  RHN

          

lim

 
pe

 

          and

   

     

   
  and        

correlation between gradients does not decay with depth

  then the

The tradeoff is that the contributions of the layers becomes
increasingly trivial       close to the identity  as      
  Gradients shatter in convnets
In this section we provide empirical evidence that the main
results also hold for deep convnets using the CIFAR 
dataset  We instantiate feedforward and resnets with  
      and   layers of equivalent size  Using   slight
modi cation of the  bottleneck  architecture in He et al 
    we introduce one skipconnection for every two
convolutional layers and both network architectures use
batch normalization 
Figures    and   compare the covariance of gradients in
the  rst layer of feedforward and resnets       with  
minibatch of   random samples from CIFAR  for networks of depth   and   To highlight the spatial structure
of the gradients  the indices of the minibatches were reordered according to   kmeans clustering        applied
to the gradients of the twolayer networks  The same permutation is used for all networks within   row  The spatial
structure is visible in both twolayer networks  although it
is more apparent in the resnet  In the feedforward network
the structure quickly disappears with depth  In the resnet 
the structure remains apparent at   layers 

The Shattered Gradients Problem

 

 

 

 

 
 
 
 
 
 
 
 
 
 

 
 

 

 
 
 
 
 

 

 
 
 

 

Feedforward net
 
 
 

 

 
 
 
 

 
 

 

 
 
 
 
 
 

 

 

 

 

 

Depth

 

 

 

    Depth    

    Depth    

    Relative effective rank 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

Depth

 

 

    Gradient norm 

Figure   Results on CIFAR  Figures     show the covariance matrices for   single minibatch for feedforwardand
resnets  Figures     show the relative effective rank and average norms of the gradients averaged over   minibatches 

To quantify this effect we consider the  whiteness  of the
gradient using relative effective rank  Let   be the matrix whose columns are the gradients with respect to the
input  for each datapoint      in   minibatch  The effective
  and measures the intrinrank is      tr     
sic dimension of   matrix  Vershynin    It is bounded
above by the rank of    matrix with highly correlated
columns and therefore more structure will have   lower effective rank  We are interested in the effective rank of the
covariance matrix of the gradients relative to    white  matrix   of the same dimensions with        Gaussian entries 
The relative effective rank        measures the similarity between the second moments of   and   
Figure    shows that the relative effective rank  averaged
over   minibatches  grows much faster as   function of
depth for networks without skipconnections  For resnets 
the parameter   slows down the rate of growth of the effective rank as predicted by theorem  
Figure    shows the average  norm of the gradient in
each coordinate  normalized by the standard deviation
computed per minibatch  We observe that this quantity
decays much more rapidly as   function of depth for feedforward networks  This is due to the effect of averaging
increasingly whitening gradients within each minibatch 
In other words  the noise within minibatches overwhelms
the signal  The phenomenon is much less pronounced in
resnets 
Taken together these results con rm the results in section  
for networks with convolutional layers and show that the
gradients in resnets are indeed more structured than those
in feedforward nets and therefore do not vanish when averaged within   minibatch  This phenomena allows for the
training of very deep resnets 

  The  looks linear  initialization
Shattering gradients are not   problem for linear networks 
see remark after equation   Unfortunately  linear networks are not useful since they lack expressivity 
The LLinit combines the best of linear and recti er nets by
initializing recti ers to look linear  Several implementations are possible  see Zagoruyko   Komodakis   for
related architectures yielding good empirical results  We
use concatenated recti ers or CReLUs  Shang et al   

       
   

The key observation is that initializing weights with   mirrored block structure yields linear outputs

            

                    Wx 

The output will cease to be linear as soon as weight updates
cause the two blocks to diverge 
An alternative architecture is based on the PReLU introduced in He et al   

PReLU          

if      

ax else 

Setting       at initialization obtains   different kind of
LLinit  Preliminary experiments  not shown  suggest that
the LLinit is more effective on the CReLUbased architecture than PReLU  The reason is unclear 

Orthogonal convolutions    detailed analysis of learning in linear neural networks by Saxe et al    showed 
theoretically and experimentally  that arbitrarily deep linear
networks can be trained when initialized with orthogonal
weights  Motivated by these results  we use the LLinit in
conjunction with orthogonal weights 

The Shattered Gradients Problem

 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

CReLU    LL
Resnet
CReLU     LL
ReLU
Linear

 

 

 

 

 

 
Depth

Figure   CIFAR  test accuracy  Comparison of test accuracy between networks of different depths with and without LL initialization 

We brie   describe how we orthogonally initialize   kernel
  of size               where        First  set all the
entries of   to zero  Second  sample   random matrix  
of size         with orthonormal columns  Finally  set
              The kernel is used in conjunction with
strides of one and zeropadding 

  Experiments
We investigated the performance of the LLinit on very
deep networks  evaluated on CIFAR  The aim was not
to match the stateof theart  but rather to test the hypothesis that shattered gradients adversely affect training in very
deep recti er nets  We therefore designed an experiment
where  concatenated  recti er nets are and are not shattered
at initialization  We  nd that the LLinit allows to train signi cantly deeper nets  which con rms the hypothesis 
We compared   CReLU architecture with an orthogonal
LLinit against an equivalent CReLU network  resnet  and  
standard feedforward ReLU network  The other networks
were initialized according to He et al    The architectures are thin with the number of  lters per layer in the
ReLU networks ranging from   at the input layer to   see
section    Doubling with each spatial extent reduction 
The thinness of the architecture makes it particularly dif 
cult for gradients to propagate at high depth  The reduction
is performed by convolutional layers with strides of   and
following the last reduction the representation is passed to
  fully connected layer with   neurons for classi cation 
The numbers of  lters per layer of the CReLU models were
adjusted by   factor of     to achieve parameter parity
with the ReLU models  The Resnet version of the model is
the same as the basic ReLU model with skipconnections
after every two modules following He et al     

Updates were performed with Adam  Kingma   Ba   
Training schedules were automatically determined by an
autoscheduler that measures how quickly the loss on the
training set has been decreasing over the last ten epochs 
and drops the learning rate if   threshold remains crossed
for  ve measurements in   row  Standard data augmentation was performed  translating up to   pixels in any direction and  ipping horizontally with      
Results are shown in  gure   Each point is the mean of
  trained models  The ReLU and CReLU nets performed
steadily worse with depth  the ReLU net performing worse
than the linear baseline of   at the maximum depth of
  The feedforward net with LLinit performs comparably to   resnet  suggesting that shattered gradients are  
large part of the problem in training very deep networks 

  Conclusion
The representational power of recti er networks depends
on the number of linear regions into which it splits the input space  It was shown in Montufar et al    that the
number of linear regions can grow exponentially with depth
 but only polynomially with width  Hence deep neural
networks are capable of far richer mappings than shallow
ones  Telgarsky    An underappreciated consequence
of the exponential growth in linear regions is the proliferation of discontinuities in the gradients of recti er nets 
This paper has identi ed and analyzed   previously unnoticed problem with gradients in deep networks 
in  
randomly initialized network  the gradients of deeper layers are increasingly uncorrelated  Shattered gradients play
havoc with the optimization methods currently in use  and
may explain the dif culty in training deep feedforward
networks even when effective initialization and batch normalization are employed  Averaging gradients over minibatches becomes analogous to integrating over white noise
  there is no clear trend that can be summarized in   single
average direction  Shattered gradients can also introduce
numerical instabilities  since small differences in the input
can lead to large differences in gradients 
Skipconnections in combination with suitable rescaling
reduce shattering  Speci cally  we show that the rate at
which correlations between gradients decays changes from
exponential for feedforward architectures to sublinear for
resnets  The analysis uncovers   surprising and  to us at
least  unexpected sideeffect of batch normalization  An
alternate solution to the shattering gradient problem is to
design initializations that do not shatter such as the LLinit  An interesting future direction is to investigate hybrid
architectures combining the LLinit with skip connections 

 Note that even the choice of   step size in SGD typically re 
 ects an assumption about the correlation scale of the gradients 

The Shattered Gradients Problem

Mishkin    and Matas     All you need is   good init  In ICLR 

 

Montufar  Guido    Pascanu  Razvan  Cho  Kyunghyun  and Bengio  Yoshua  On the number of linear regions of deep neural
networks  In Advances in neural information processing systems  pp     

Orhan    Emin 

Skip Connections as Effective Symmetry 

Breaking  In arXiv   

Saxe  Andrew    McClelland  James    and Ganguli  Surya  Exact solutions to the nonlinear dynamics of learning in deep linear neural networks  In ICLR   

Shang  Wenling  Sohn  Kihyuk  Almeida  Diogo  and Lee 
Honglak  Understanding and Improving Convolutional Neural
Networks via Concatenated Recti ed Linear Units  In ICML 
 

Srivastava  Rupesh Kumar  Greff  Klaus  and Schmidhuber  Juer 

gen  Highway Networks  In arXiv   

Sussillo  David and Abbott       Random Walk Initialization for

Training Very Deep Feedforward Networks  In ICLR   

Sutskever  Ilya  Martens  James  Dahl  George  and Hinton  Geoffrey  On the importance of initialization and momentum in
deep learning  In Proceedings of the  th International Conference on Machine Learning  ICML  pp     

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet  Pierre 
Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew  Going Deeper With
Convolutions  In CVPR   

Szegedy  Christian 

Ioffe  Sergey  and Vanhoucke  Vincent 
Inceptionv  InceptionResNet and the Impact of Residual
Connections on Learning  In arXiv   

Telgarsky  Matus  Bene ts of depth in neural networks  In COLT 

 

Veit  Andreas  Wilber  Michael    and Belongie  Serge  Residual
Networks Behave Like Ensembles of Relatively Shallow Networks  In NIPS   

Vershynin  Roman  Introduction to the nonasymptotic analysis of
random matrices  In Compressed sensing  pp    Cambridge Univ Press   

Zagoruyko  Sergey and Komodakis  Nikos  DiracNets  Training Very Deep Neural Networks Without SkipConnections  In
arXiv   

References
Balduzzi  David  Deep Online Convex Optimization with Gated

Games  In arXiv   

Balduzzi  David  Vanchinathan  Hastagiri 

and Buhmann 
Joachim  Kickback cuts Backprop   redtape  Biologically
plausible credit assignment in neural networks  In AAAI Conference on Arti cial Intelligence  AAAI   

Balduzzi  David  McWilliams  Brian  and ButlerYeoman  Tony 
Neural Taylor Approximations  Convergence and Exploration
in Recti er Networks  In ICML   

Bengio     Lamblin     Popovici     and Larochelle     Greedy

LayerWise Training of Deep Networks  In NIPS   

Bengio  Yoshua  Simard     and Frasconi     Learning longterm
IEEE Trans 

dependencies with gradient descent is dif cult 
Neur  Net     

Choromanska     Henaff     Mathieu     Arous       and LeCun     The loss surface of multilayer networks  In Journal of
Machine Learning Research  Workshop and Conference Proceeedings  volume    AISTATS   

Glorot  Xavier and Bengio  Yoshua  Understanding the dif culty
In AISTATS 

of training deep feedforward neural networks 
 

Greff  Klaus  Srivastava  Rupesh Kumar  and Schmidhuber  Juergen  Highway and Residual Networks learn Unrolled Iterative
Estimation  In ICLR   

Hardt  Moritz and Ma  Tengyu  Identity Matters in Deep Learn 

ing  In ICLR   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian 
Delving Deep into Recti ers  Surpassing HumanLevel Performance on ImageNet Classi cation  In ICCV   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian 
In ECCV 

Identity Mappings in Deep Residual Networks 
   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian 
In CVPR 

Deep Residual Learning for Image Recognition 
   

Hinton  GE  Osindero     and Teh         Fast Learning Algorithm for Deep Belief Nets  Neural Computation   
   

Hochreiter  Sepp  Untersuchungen zu dynamischen neuronalen
Netzen  Master   thesis  Technische Universit at   unchen 
 

Ioffe  Sergey and Szegedy  Christian  Batch normalization  Accelerating deep network training by reducing internal covariate
shift  In ICML   

Kingma  Diederik   and Ba  Jimmy Lei  Adam    method for

stochastic optimization  In ICLR   

Krizhevsky     Sutskever     and Hinton       Imagenet classi 
cation with deep convolutional neural networks  In Advances
in Neural Information Processing Systems  NIPS   

