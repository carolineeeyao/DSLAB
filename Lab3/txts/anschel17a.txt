AveragedDQN  Variance Reduction and Stabilization for

Deep Reinforcement Learning

Oron Anschel   Nir Baram   Nahum Shimkin  

Abstract

Instability and variability of Deep Reinforcement
Learning  DRL  algorithms tend to adversely affect their performance  AveragedDQN is   simple extension to the DQN algorithm  based on
averaging previously learned Qvalues estimates 
which leads to   more stable training procedure
and improved performance by reducing approximation error variance in the target values  To understand the effect of the algorithm  we examine
the source of value function estimation errors and
provide an analytical comparison within   simpli ed model  We further present experiments
on the Arcade Learning Environment benchmark
that demonstrate signi cantly improved stability
and performance due to the proposed extension 

  Introduction
In Reinforcement Learning  RL  an agent seeks an optimal policy for   sequential decision making problem  Sutton   Barto    It does so by learning which action
is optimal for each environment state  Over the course
of time  many algorithms have been introduced for solving RL problems including Qlearning  Watkins   Dayan 
  SARSA  Rummery   Niranjan    Sutton  
Barto    and policy gradient methods  Sutton et al 
  These methods are often analyzed in the setup of
linear function approximation  where convergence is guaranteed under mild assumptions  Tsitsiklis    Jaakkola
et al    Tsitsiklis   Van Roy    EvenDar   Mansour    In practice  realworld problems usually involve highdimensional inputs forcing linear function approximation methods to rely upon hand engineered features

 Department

of
Electrical
Correspondence

Israel 

 
 oronanschel campus technion ac il 
 nirb campus technion ac il 
 shimkin ee technion ac il 

Engineering 
to 

Haifa
Oron Anschel
Baram
Nir
Shimkin

Nahum

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

for problemspeci   state representation  These problemspeci   features diminish the agent  exibility  and so the
need of an expressive and  exible nonlinear function approximation emerges  Except for few successful attempts
      TDgammon  Tesauro   the combination of
nonlinear function approximation and RL was considered
unstable and was shown to diverge even in simple domains
 Boyan   Moore   
The recent Deep QNetwork  DQN  algorithm  Mnih et al 
  was the  rst to successfully combine   powerful nonlinear function approximation technique known
as Deep Neural Network  DNN   LeCun et al   
Krizhevsky et al    together with the Qlearning algorithm  DQN presented   remarkably  exible and stable
algorithm  showing success in the majority of games within
the Arcade Learning Environment  ALE   Bellemare et al 
  DQN increased the training stability by breaking
the RL problem into sequential supervised learning tasks 
To do so  DQN introduces the concept of   target network
and uses an Experience Replay buffer  ER   Lin   
Following the DQN work  additional modi cations and extensions to the basic algorithm further increased training
stability  Schaul et al    suggested sophisticated ER
sampling strategy  Several works extended standard RL
exploration techniques to deal with highdimensional input
 Bellemare et al    Tang et al    Osband et al 
  Mnih et al    showed that sampling from ER
could be replaced with asynchronous updates from parallel
environments  which enables the use of onpolicy methods  Wang et al    suggested   network architecture
base on the advantage function decomposition  Baird III 
 
In this work we address issues that arise from the combination of Qlearning and function approximation  Thrun  
Schwartz   were  rst to investigate one of these issues
which they have termed as the overestimation phenomena 
The max operator in Qlearning can lead to overestimation
of stateaction values in the presence of noise  Van Hasselt
et al    suggest the DoubleDQN that uses the Double
Qlearning estimator  Van Hasselt    method as   solution to the problem  Additionally  Van Hasselt et al   
showed that Qlearning overestimation do occur in practice

AveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

 at least in the ALE 
This work suggests   different solution to the overestimation phenomena  named AveragedDQN  Section   based
on averaging previously learned Qvalues estimates  The
averaging reduces the target approximation error variance
 Sections   and   which leads to stability and improved
results  Additionally  we provide experimental results on
selected games of the Arcade Learning Environment 
We summarize the main contributions of this paper as follows 

    novel extension to the DQN algorithm which stabilizes training  and improves the attained performance 
by averaging over previously learned Qvalues 

  Variance analysis that explains some of the DQN
problems  and how the proposed extension addresses
them 

  Experiments with several ALE games demonstrating

the favorable effect of the proposed scheme 

  Background
In this section we elaborate on relevant RL background 
and speci cally on the Qlearning algorithm 

  Reinforcement Learning
We consider the usual RL learning framework  Sutton  
Barto    An agent is faced with   sequential decision making problem  where interaction with the environment takes place at discrete time steps                At
time   the agent observes state st      selects an action
at      which results in   scalar reward rt      and  
transition to   next state st       We consider in nite
horizon problems with   discounted cumulative reward objective Rt           trt  where         is the discount factor  The goal of the agent is to  nd an optimal
policy           that maximize its expected discounted
cumulative reward 
Valuebased methods for solving RL problems encode policies through the use of value functions  which denote the
expected discounted cumulative reward from   given state
   following   policy   Speci cally we are interested in
stateaction value functions 

                

 trt                   

The optimal value function is denoted as          
max          and an optimal policy   can be easily derived by       argmaxaQ      

  Qlearning
One of the most popular RL algorithms is the Qlearning
algorithm  Watkins   Dayan    This algorithm is
based on   simple value iteration update  Bellman   
directly estimating the optimal value function    Tabular
Qlearning assumes   table that contains old actionvalue
function estimates and preform updates using the following update rule 

  

                 
                           max
 
where    is the resulting state after applying action   in the
state      is the immediate reward observed for action   at
state      is the discount factor  and   is   learning rate 
When the number of states is large  maintaining   lookup table with all possible stateaction pairs values in memory is impractical    common solution to this issue is to
use function approximation parametrized by   such that
                   
  Deep   Networks  DQN 
We present in Algorithm     slightly different formulation
of the DQN algorithm  Mnih et al    In iteration  
the DQN algorithm solves   supervised learning problem
to approximate the actionvalue function            line  
This is an extension of implementing   in its function approximation form  Riedmiller   

Algorithm   DQN
  Initialize           with random weights  
  Initialize Experience Replay  ER  buffer  
  Initialize exploration procedure Explore 
  for                   do
 
               
 
 
  end for
output QDQN           

      EB        maxa                   
yi
     argmin  EB yi
Explore  update  

The target values yi
     line   are constructed using   designated targetnetwork              using the previous iteration parameters     where the expectation  EB  is taken
       the sample distribution of experience transitions in the
ER buffer                   The DQN loss  line   is minimized using   Stochastic Gradient Descent  SGD  variant 
sampling minibatches from the ER buffer  Additionally 
DQN requires an exploration procedure  which we denote
as Explore  to interact with the environment       an  
greedy exploration procedure  The number of new experience transitions              added by exploration to the ER

AveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

buffer in each iteration is small  relatively to the size of the
ER buffer  Thereby      can be used as   good initialization for   in iteration   
Note that in the original implementation  Mnih et al   
  transitions are added to the ER buffer simultaneously with the minimization of the DQN loss  line   Using the hyperparameters employed by Mnih et al   
   detailed for completeness in Appendix      of the
experience transitions in ER buffer are replaced between
target network parameter updates  and   are sampled for
minimization 

  Averaged DQN
The AveragedDQN algorithm  Algorithm   is an extension of the DQN algorithm  AveragedDQN uses the  
previously learned Qvalues estimates to produce the current actionvalue estimate  line   The AveragedDQN algorithm stabilizes the training process  see Figure   by
reducing the variance of target approximation error as we
elaborate in Section   The computational effort compared to DQN is  Kfold more forward passes through  
Qnetwork while minimizing the DQN loss  line   The
number of backpropagation updates remains the same as
in DQN  Computational cost experiments are provided in
Appedix    The output of the algorithm is the average over
the last   previously learned Qnetworks 

Figure   DQN and AveragedDQN performance in the Atari
game of BREAKOUT  The bold lines are averages over seven independent learning trials  Every    frames    performance test
using  greedy policy with       for   frames was conducted  The shaded area presents one standard deviation  For both
DQN and AveragedDQN the hyperparameters used were taken
from Mnih et al   

In Figures   and   we can see the performance of Averaged 

Algorithm   Averaged DQN
  Initialize           with random weights  
  Initialize Experience Replay  ER  buffer  
  Initialize exploration procedure Explore 
  for                   do
  QA
             
 
               
 
 
  end for
output QA

   
      EB       maxa  QA
     argmin  EB yi
Explore  update  
    

           
yi

                

                

            

DQN compared to DQN  and DoubleDQN  further experimental results are given in Section  
We note that recentlylearned stateaction value estimates
are likely to be better than older ones  therefore we have
also considered   recencyweighted average  In practice 
  weighted average scheme did not improve performance
and therefore is not presented here 

  Overestimation and Approximation Errors
Next  we discuss the various types of errors that arise due to
the combination of Qlearning and function approximation
in the DQN algorithm  and their effect on training stability 
We refer to DQN   performance in the BREAKOUT game
in Figure   The source of the learning curve variance in
DQN   performance is an occasional sudden drop in the
average score that is usually recovered in the next evaluation phase  for another illustration of the variance source
see Appendix    Another phenomenon can be observed in
Figure   where DQN initially reaches   steady state  after
  million frames  followed by   gradual deterioration in
performance 
For the rest of this section  we list the above mentioned errors  and discuss our hypothesis as to the relations between
each error and the instability phenomena depicted in Figures   and  
We follow terminology from Thrun   Schwartz  
and de ne some additional relevant quantities  Letting
            be the value function of DQN at iteration    we
denote                            and decompose it as
follows 

                          
  yi
 

                yi
 

Target Approximation

 

 

Error

   

       yi
 

Error

Overestimation

   

 

   yi

             
 

Optimality
Difference

 

 

 

 frames  millions Averaged score per episodeBreakoutDQNAveraged DQN    AveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

Figure   DQN  DoubleDQN  and AveragedDQN performance  left  and average value estimates  right  in the Atari game of ASTERIX 
The bold lines are averages over seven independent learning trials  The shaded area presents one standard deviation  Every    frames 
  performance test using  greedy policy with       for   frames was conducted  The hyperparameters used were taken from
Mnih et al   

Here yi

    is the DQN target  and  yi

    is the true target 

yi

 yi

      EB       max
      EB       max

  

  

                   
              

 yi 

Let us denote by    
by Ri

    the target approximation error  and

    the overestimation error  namely
                    yi
   
Ri
      yi

    

    

       yi

The optimality difference can be seen as the error of   standard tabular Qlearning  here we address the other errors 
We next discuss each error in turn 

  Target Approximation Error  TAE 
     is the error in the learned             relaThe TAE     
     which is determined after minimizing the DQN
tive to yi
loss  Algorithm   line   Algorithm   line   The TAE is
  result of several factors  Firstly  the suboptimality of   
due to inexact minimization  Secondly  the limited representation power of   neural net  model error  Lastly  the
generalization error for unseen stateaction pairs due to the
 nite size of the ER buffer 
The TAE can cause   deviations from   policy to   worse
one  For example  such deviation to   suboptimal policy
occurs in case yi

              and 

       yi

argmaxa               argmaxa                  

    

  argmaxa yi

    

We hypothesize that the variability in DQN   performance
in Figure   that was discussed at the start of this section  is
related to deviating from   steadystate policy induced by
the TAE 

  Overestimation Error
The Qlearning overestimation phenomena were  rst investigated by Thrun   Schwartz   In their work  Thrun
and Schwartz considered the TAE    
    as   random variable uniformly distributed in the interval     Due to the
     the expected overmax operator in the DQN target yi
     are upper bounded by     
estimation errors Ez Ri
  
 where   is the number of applicable actions in state   
The intuition for this upper bound is that in the worst case 
all   values are equal  and we get equality to the upper
bound 

Ez Ri

        Ez max
  

     

          

     
     

 

The overestimation error is different in its nature from the
TAE since it presents   positive bias that can cause asymptotically suboptimal policies  as was shown by Thrun  
Schwartz   and later by Van Hasselt et al   
in the ALE environment  Note that   uniform bias in the
actionvalue function will not cause   change in the induced
policy  Unfortunately  the overestimation bias is uneven
and is bigger in states where the Qvalues are similar for
the different actions  or in states which are the start of  
long trajectory  as we discuss in Section   on accumulation
of TAE variance 
Following from the above mentioned overestimation upper
bound  the magnitude of the bias is controlled by the variance of the TAE 

 frames  millions Averaged score per episodeAsterixDQNDouble   DQNAveraged DQN    frames  millions Value estimates  log scale AsterixDQNDouble   DQNAveraged DQN    AveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

The Double Qlearning and its DQN implementation
 DoubleDQN   Van Hasselt et al    Van Hasselt 
  is one possible approach to tackle the overestimation
problem  which replaces the positive bias with   negative
one  Another possible remedy to the adverse effects of this
error is to directly reduce the variance of the TAE  as in our
proposed scheme  Section  
In Figure   we repeated the experiment presented in
Van Hasselt et al     along with the application of
AveragedDQN  This experiment is discussed in Van Hasselt et al    as an example of overestimation that leads
to asymptotically suboptimal policies  Since AveragedDQN reduces the TAE variance  this experiment supports
an hypothesis that the main cause for overestimation in
DQN is the TAE variance 

         Var    

  TAE Variance Reduction
To analyse the TAE variance we  rst must assume   statistical model on the TAE  and we do so in   similar way to
Thrun   Schwartz   Suppose that the TAE    
    is
  random process such that      
  
        
and for        Cov    
           Furthermore  to focus only on the TAE we eliminate the overestimation error
by considering    xed policy for updating the target values 
Also  we can conveniently consider   zero reward      
everywhere since it has no effect on variance calculations 
Denote by Qi               the vector of value estimates
in iteration    where the  xed action   is suppressed  and
by Zi the vector of corresponding TAEs  For AveragedDQN we get 

        

Qi   Zi     

 
 

    

Qi   

 

where     RS  
is the transition probabilities matrix
for the given policy  The covariance of the above Vector Autoregressive  VAR  model is given by the discretetime Lyapunov equation  and can be solved directly or by
specialized numerical algorithms  Arthur   Bryson   
However  to obtain an explicit comparison  we further specialize the model to an Mstate unidirectional MDP as in
Figure  

 

 

 

 

  

  

 

sM 

sM 

Figure     states unidirectional MDP  The process starts at state
   then in each time step moves to the right  until the terminal
state sM  is reached    zero reward is obtained in any state 

  DQN Variance
We assume the statistical model mentioned at the start of
this section  Consider   unidirectional Markov Decision
Process  MDP  as in Figure   where the agent starts at
state    state sM  is   terminal state  and the reward in
any state is equal to zero 
Employing DQN on this MDP model  we get that for    
  

   

QDQN                
     
     
     

      yi
                  
           
      yi 
                   
           
sM    

          

where in the last equality we have used the fact yj
for all    terminal state  Therefore 

       

Var QDQN            

   

sm 

    

The above example gives intuition about the behavior of
the TAE variance in DQN  The TAE is accumulated over
the past DQN iterations on the updates trajectory  Accumulation of TAE errors results in bigger variance with its
associated adverse effect  as was discussed in Section  

Algorithm   Ensemble DQN
  Initialize   Qnetworks             with random

weights   

  for                 

             

  

  Initialize Experience Replay  ER  buffer  
  Initialize exploration procedure Explore 
  for                   do
           
  QE
             
yi
 
for                   do
 
               
 
 
 
  end for
output QE

   
      EB       maxa  QE
    argmin  EB yi
end for
Explore  update  
   

             
   

            

  

  Ensemble DQN Variance
We consider two approaches for TAE variance reduction 
The  rst one is the AveragedDQN and the second we term
EnsembleDQN  We start with EnsembleDQN which is  
straightforward way to obtain      variance reduction 

AveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

with   computational effort of Kfold learning problems 
compared to DQN  EnsembleDQN  Algorithm   solves
  DQN losses in parallel  then averages over the resulted
Qvalues estimates 
For EnsembleDQN on the unidirectional MDP in Figure
  we get for       

QE

          

Var QE

          

 

    
    

 
 

    
 

    

       
sm    

 
 

   
sm

Var QDQN          

     are two uncorrelated
where for             
TAEs  The calculations of QE       are detailed in Appendix   

    and      

  Averaged DQN Variance
We continue with AveragedDQN  and calculate the variance in state    for the unidirectional MDP in Figure   We
get that for     KM 

Var QA

          

DK     

sm  

    
    

    Un      with    
where DK      
 Un   
   denoting   Discrete Fourier Transform  DFT  of
  rectangle pulse  and  Un        The calculations of
QA       and DK   are more involved and are detailed in
Appendix   
Furthermore  for             we have that DK    
    Appendix    and therefore the following holds

Var QA

           Var QE

        

 

 
 

Var QDQN          

meaning that AveragedDQN is theoretically more ef cient
in TAE variance reduction than EnsembleDQN  and at
least   times better than DQN  The intuition here is that
AveragedDQN averages over TAEs averages  which are
the value estimates of the next states 

  Experiments
The experiments were designed to address the following
questions 

  How does the number   of averaged target networks
affect the error in value estimates  and in particular the
overestimation error 

  How does the averaging affect the learned polices

quality 

To that end  we ran AveragedDQN and DQN on the
ALE benchmark  Additionally  we ran AveragedDQN 
EnsembleDQN  and DQN on   Gridworld toy problem
where the optimal value function can be computed exactly 

  Arcade Learning Environment  ALE 
To evaluate AveragedDQN  we adopt
the typical RL
methodology where agent performance is measured at the
end of training  We refer the reader to Liang et al   
for further discussion about DQN evaluation methods on
the ALE benchmark  The hyperparameters used were taken
from Mnih et al    and are presented for completeness in Appendix    DQN code was taken from McGill
University RLLAB  and is available online   together with
AveragedDQN implementation 
We have evaluated the AveragedDQN algorithm on three
Atari games from the Arcade Learning Environment
 Bellemare et al    The game of BREAKOUT was
selected due to its popularity and the relative ease of the
DQN to reach   steady state policy  In contrast  the game
of SEAQUEST was selected due to its relative complexity 
and the signi cant improvement in performance obtained
by other DQN variants       Schaul et al    Wang
et al    Finally  the game of ASTERIX was presented
in Van Hasselt et al    as an example to overestimation
in DQN that leads to divergence 
As can be seen in Figure   and in Table   for all three
games  increasing the number of averaged networks in
AveragedDQN results in lower average values estimates 
betterpreforming policies  and less variability between the
runs of independent learning trials  For the game of ASTERIX  we see similarly to Van Hasselt et al    that
the divergence of DQN can be prevented by averaging 
Overall  the results suggest that in practice AveragedDQN
reduces the TAE variance  which leads to smaller overestimation  stabilized learning curves and signi cantly improved performance 

  Gridworld
The Gridworld problem  Figure   is   common RL benchmark       Boyan   Moore   As opposed to the
ALE  Gridworld has   smaller state space that allows the
ER buffer to contain all possible stateaction pairs  Additionally  it allows the optimal value function    to be ac 

 McGill University RLLAB DQN Atari code  https 

 bitbucket org rllabmcgill atari release 
AveragedDQN
oronanschel atari release averaged dqn

https bitbucket org 

code

AveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

Figure   The top row shows AveragedDQN performance for the different number   of averaged networks on three Atari games  For
      AveragedDQN is reduced to DQN  The bold lines are averaged over seven independent learning trials  Every    frames 
  performance test using  greedy policy with       for   frames was conducted  The shaded area presents one standard
deviation  The bottom row shows the average value estimates for the three games  It can be seen that as the number of averaged networks
is increased  overestimation of the values is reduced  performance improves  and less variability is observed  The hyperparameters used
were taken from Mnih et al   

curately computed 
For the experiments  we have used AveragedDQN  and
EnsembleDQN with ER buffer containing all possible
stateaction pairs  The network architecture that was used
composed of   small fully connected neural network with
one hidden layer of   neurons  For minimization of the
DQN loss  the ADAM optimizer  Kingma   Ba    was
used on   minibatches of   samples per target network
parameters update in the  rst experiment  and   minibatches in the second 

  ENVIRONMENT SETUP

In this experiment on the problem of Gridworld  Figure
  the state space contains pairs of points from      discrete grid                 The algorithm interacts with the environment through raw pixel features with  
onehot feature map  st     st             
There are four actions corresponding to steps in each compass direction    reward of       in state st      
and       otherwise  We consider the discounted return
problem with   discount factor of      

Figure   Gridworld problem  The agent starts at the leftbottom
of the grid  In the upperright corner    reward of   is obtained 

  OVERESTIMATION

In Figure   it can be seen that increasing the number   of
averaged target networks leads to reduced overestimation
eventually  Also  more averaged target networks seem to
reduces the overshoot of the values  and leads to smoother
and less inconsistent convergence 

  AVERAGED VERSUS ENSEMBLE DQN

In Figure   it can be seen that as was predicted by the
analysis in Section   EnsembleDQN is also inferior to
AveragedDQN regarding variance reduction  and as   con 

 frames  millions Averaged score per episodeBreakoutDQN    Averaged DQN    Averaged DQN    Averaged DQN    frames  millions Averaged score per episodeSeaquestDQN    Averaged DQN    Averaged DQN    Averaged DQN    frames  millions Averaged score per episodeAsterixDQN    Averaged DQN    Averaged DQN    Averaged DQN    frames  millions Value estimatesBreakoutDQN    Averaged DQN    Averaged DQN    Averaged DQN    frames  millions Value estimatesSeaquestDQN    Averaged DQN    Averaged DQN    Averaged DQN    frames  millions Value estimates  log scale AsterixDQN    Averaged DQN    Averaged DQN    Averaged DQN    startGridworldAveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

Table   The columns present the average performance of DQN and AveragedDQN after    frames  using  greedy policy with
      for   frames  The standard variation represents the variability over seven independent trials  Average performance
improved with the number of averaged networks  Human and random performance were taken from Mnih et al   

GAME

DQN

AVERAGEDDQN

AVERAGEDDQN

AVERAGEDDQN

HUMAN

RANDOM

AVG   STD  DEV 

   

   

BREAKOUT

 

 

 

 

 

 

   

   

 

SEAQUEST

 

 

 

   

 

 

 

 

 

 

 

ASTERIX

 

 

 

 

 

 

 

 

 

 

Figure   AveragedDQN average predicted value in Gridworld 
Increasing the number   of averaged target networks leads to  
faster convergence with less overestimation  positivebias  The
bold lines are averages over   independent learning trials  and
the shaded area presents one standard deviation 
In the  gure 
        present DQN  and AveragedDQN for    average overestimation 

sequence far more overestimates the values  We note that
EnsembleDQN was not implemented for the ALE experiments due to its demanding computational effort  and the
empirical evidence that was already obtained in this simple
Gridworld domain 

  Discussion and Future Directions
In this work  we have presented the AveragedDQN algorithm  an extension to DQN that stabilizes training  and improves performance by ef cient TAE variance reduction 
We have shown both in theory and in practice that the proposed scheme is superior in TAE variance reduction  compared to   straightforward but computationally demanding
approach such as EnsembleDQN  Algorithm   We have
demonstrated in several games of Atari that increasing the
number   of averaged target networks leads to better poli 

Figure   AveragedDQN and EnsembleDQN predicted value in
Gridworld  Averaging of past learned value is more bene cial
than learning in parallel  The bold lines are averages over  
independent learning trials  where the shaded area presents one
standard deviation 

cies while reducing overestimation  AveragedDQN is  
simple extension that can be easily integrated with other
DQN variants such as Schaul et al    Van Hasselt
et al    Wang et al    Bellemare et al   
He et al    Indeed  it would be of interest to study
the added value of averaging when combined with these
variants  Also  since AveragedDQN has variance reduction effect on the learning curve    more systematic comparison between the different variants can be facilitated as
discussed in  Liang et al   
In future work  we may dynamically learn when and how
many networks to average for best results  One simple suggestion may be to correlate the number of networks with
the state TDerror  similarly to Schaul et al    Finally 
incorporating averaging techniques similar to AveragedDQN within onpolicy methods such as SARSA and ActorCritic methods  Mnih et al    can further stabilize
these algorithms 

 Iterations AveragepredictedvalueABCDGridworldEs maxaQ     DQN   AveragedDQN   AveragedDQN   AveragedDQN   Iterations AveragepredictedvalueGridworldEs maxaQ     DQN   EnsembleDQN   AveragedDQN   AveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

References
Arthur   Bryson  Yu Chi Ho  Applied Optimal Control 
Optimization Estimation and Control  Hemisphere Publishing   

Baird III  Leemon    Advantage updating  Technical re 

port  DTIC Document   

Bellemare        Naddaf     Veness     and Bowling    
The arcade learning environment  An evaluation platform for general agents  Journal of Arti cial Intelligence
Research     

Bellemare  Marc    Srinivasan  Sriram  Ostrovski  Georg 
Schaul  Tom  Saxton  David  and Munos  Remi  Unifying countbased exploration and intrinsic motivation 
arXiv preprint arXiv   

Bellman  Richard    Markovian decision process  Indiana

Univ  Math        

Boyan  Justin and Moore  Andrew    Generalization in
reinforcement learning  Safely approximating the value
function  Advances in neural information processing
systems  pp     

EvenDar  Eyal and Mansour  Yishay  Learning rates for
qlearning  Journal of Machine Learning Research   
 Dec   

He  Frank    Yang Liu  Alexander    Schwing  and Peng 
Jian  Learning to play in   day  Faster deep reinforcement learning by optimality tightening  arXiv preprint
arXiv   

Jaakkola  Tommi  Jordan  Michael    and Singh  Satinder   
On the convergence of stochastic iterative dynamic programming algorithms  Neural Computation   
   

Kingma  Diederik    and Ba  Jimmy  Adam    method
arXiv preprint arXiv 

for stochastic optimization 
   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In Advances in NIPS  pp     

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  Proceedings of the IEEE   
   

Liang  Yitao  Machado  Marlos    Talvitie  Erik  and Bowling  Michael  State of the art control of Atari games
using shallow reinforcement learning 
In Proceedings
of the   International Conference on Autonomous
Agents   Multiagent Systems  pp     

Lin  LongJi  Reinforcement learning for robots using neural networks  Technical report  DTIC Document   

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David 
Graves  Alex  Antonoglou  Ioannis  Wierstra  Daan  and
Riedmiller  Martin  Playing Atari with deep reinforcement learning  arXiv preprint arXiv   

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David 
Rusu  Andrei    Veness  Joel  Bellemare  Marc   
Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas   
Ostrovski  Georg  et al  Humanlevel control through
deep reinforcement learning  Nature   
   

Mnih  Volodymyr  Badia  Adria Puigdomenech  Mirza 
Mehdi  Graves  Alex  Lillicrap  Timothy    Harley  Tim 
Silver  David  and Kavukcuoglu  Koray  Asynchronous
methods for deep reinforcement learning  arXiv preprint
arXiv   

Osband  Ian  Blundell  Charles  Pritzel  Alexander  and
Van Roy  Benjamin  Deep exploration via bootstrapped
DQN  arXiv preprint arXiv   

Riedmiller  Martin  Neural  tted   iteration rst experiences with   data ef cient neural reinforcement learning
method  In European Conference on Machine Learning 
pp    Springer   

Rummery  Gavin   and Niranjan  Mahesan  Online Qlearning using connectionist systems  University of
Cambridge  Department of Engineering   

Schaul  Tom  Quan  John  Antonoglou  Ioannis  and Silver  David  Prioritized experience replay  arXiv preprint
arXiv   

Sutton  Richard   and Barto  Andrew    Reinforcement
Learning  An Introduction  MIT Press Cambridge   

Sutton  Richard    McAllester  David    Singh  Satinder   
and Mansour  Yishay  Policy gradient methods for reinforcement learning with function approximation 
In
NIPS  volume   pp     

Tang  Haoran  Rein Houthooft  Davis Foote  Adam Stooke 
Xi Chen  Yan Duan  John Schulman  and Filip De Turck 
Pieter Abbeel   exploration    study of countbased exploration for deep reinforcement learning  arXiv preprint
arXiv   

Tesauro  Gerald  Temporal difference learning and tdgammon  Communications of the ACM   
 

Thrun  Sebastian and Schwartz  Anton 

function approximation for reinforcement learning 

Issues in using
In

AveragedDQN  Variance Reduction and Stabilization for Deep Reinforcement Learning

Proceedings of the   Connectionist Models Summer
School Hillsdale  NJ  Lawrence Erlbaum   

Tsitsiklis  John    Asynchronous stochastic approximation and qlearning  Machine Learning   
 

Tsitsiklis  John   and Van Roy  Benjamin  An analysis
of temporaldifference learning with function approximation  IEEE transactions on automatic control   
   

Van Hasselt  Hado  Double Qlearning  In Lafferty       
Williams           ShaweTaylor     Zemel        and
Culotta      eds  Advances in Neural Information Processing Systems   pp     

Van Hasselt  Hado  Guez  Arthur  and Silver  David  Deep
reinforcement learning with double Qlearning  arXiv
preprint arXiv     

Wang  Ziyu  de Freitas  Nando  and Lanctot  Marc  Dueling
network architectures for deep reinforcement learning 
arXiv preprint arXiv     

Watkins  Christopher JCH and Dayan  Peter  Qlearning 

Machine Learning     

