Sketched Ridge Regression  Optimization Perspective 

Statistical Perspective  and Model Averaging

Shusen Wang   Alex Gittens   Michael    Mahoney  

Abstract

We address the statistical and optimization impacts of using classical sketch versus Hessian
sketch to solve approximately the Matrix Ridge
Regression  MRR  problem  Prior research has
considered the effects of classical sketch on least
squares regression  LSR    strictly simpler problem  We establish that classical sketch has  
similar effect upon the optimization properties
of MRR as it does on those of LSR namely 
it recovers nearly optimal solutions  In contrast 
Hessian sketch does not have this guarantee  instead  the approximation error is governed by  
subtle interplay between the  mass  in the responses and the optimal objective value  For
both types of approximations  the regularization
in the sketched MRR problem gives it significantly different statistical properties from the
sketched LSR problem 
In particular  there is
  biasvariance tradeoff in sketched MRR that
is not present in sketched LSR  We provide upper and lower bounds on the biases and variances of sketched MRR  these establish that the
variance is signi cantly increased when classical
sketches are used  while the bias is signi cantly
increased when using Hessian sketches  Empirically  sketched MRR solutions can have risks that
are higher by an orderof magnitude than those
of the optimal MRR solutions  We establish theoretically and empirically that model averaging
greatly decreases this gap  Thus  in the distributed setting  sketching combined with model
averaging is   powerful technique that quickly
obtains nearoptimal solutions to the MRR prob 

 International Computer Science Institute and Department of
Statistics  University of California at Berkeley  USA  Department
of Computer Science  Rensselaer Polytechnic Institute  USA 
Correspondence to  Shusen Wang  shusen berkeley edu 
Alex Gittens  gittea rpi edu  Michael    Mahoney  mmahoney stat berkeley edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

lem while greatly mitigating the statistical risks
incurred by sketching 

 

            xT

  Introduction
Regression is one of the most fundamental problems in machine learning  The simplest and most thoroughly studied
regression model is least squares regression  LSR  Given
      Rn   and responses    
features      xT
            yn     Rn  the LSR problem minw  cid Xw     cid 
can be solved in   nd  time using the QR decomposition
or in   ndt  time using accelerated gradient descent algorithms  Here    is the number of iterations  which depends
on the initialization  the condition number of    and the
stopping criterion 
This paper considers the    cid    problem  where there
is much redundancy in    Matrix sketching  as used
within Randomized Linear Algebra  RLA   Mahoney 
  Woodruff    works by reducing the size of  
without losing too much information  this operation can
be modeled as taking actual rows or linear combinations
of the rows of   with   sketching matrix   to form the
sketch ST    Here     Rn   satis es        cid    so that
ST   generically has the same rank but much fewer rows
as    Sketching has been used to speed up LSR  Drineas
et al      Clarkson   Woodruff    Meng  
Mahoney    Nelson   Nguy en    by solving the
sketched LSR problem minw  cid ST Xw   ST   cid 
  instead
of the original LSR problem  Solving sketched LSR costs
either   sd    Ts  time using the QR decomposition or
  sdt   Ts  time using accelerated gradient descent algorithms  where   is as de ned previously  and Ts is the
time cost of sketching  For example  Ts     nd log   
when   is the subsampled randomized Hadamard transform  Drineas et al    and Ts     nd  when   is
  CountSketch matrix  Clarkson   Woodruff   
There has been much work in RLA on analyzing the
quality of sketched LSR with different sketching methods
and different objectives  see the reviews  Mahoney   

 The condition number of XT SST   is very close to that of

XT    and thus the number of iterations   is almost unchanged 

Sketched Ridge Regression

Table   The time cost of the solutions to MRR  Here Ts    and
Ts    denote the time cost of forming the sketches ST    
Rs   and ST     Rs   

De nition

Optimal
Classical
Hessian

 
 
 

Time

  nd    nmd 

  sd    smd    Ts      Ts   

  sd    nmd    Ts   

Woodruff    and the references therein 
The concept of sketched LSR originated in the theoretical computer science literature        Drineas et al   
  where the behavior of sketched LSR was studied
from an optimization perspective  Let   cid  be the optimal
LSR solution and    be the solution to sketched LSR  This
line of work established that if            poly   
  is at most  
  These works also bounded
  in terms of the difference in the objective func 

then the objective function value  cid          cid cid 
times worse than  cid Xw cid      cid cid 

 cid        cid cid 
tion values and the condition number of XT   
  more recent line of work has studied sketched LSR from
  statistical perspective   Ma et al    Raskutti   Mahoney    Pilanci   Wainwright    Wang et al 
    considered statistical properties of sketched LSR
like the bias and variance  In particular  Pilanci   Wainwright   showed that sketched LSR has much higher
variance than the optimal solution 
Both of these perspectives are important and of practical
interest  The optimization perspective is relevant when the
data can be taken as deterministic values  The statistical
perspective is relevant in machine learning and statistics
applications where the data are random  and the regression
coef cients are therefore themselves random variables 
In practice  regularized regression       ridge regression
and LASSO  exhibit more attractive biasvariance tradeoffs and generalization errors than vanilla LSR  Furthermore  the matrix generalization of LSR  where multiple
responses are to be predicted  is often more useful than
LSR  However  the properties of sketched regularized matrix regression are largely unknown  Hence  the question 
How  if at all  does our understanding of the optimization and statistical properties of sketched LSR generalize
to sketched regularized regression  We answer this question for sketched matrix ridge regression  MRR 
Recall that   is        Let     Rn   denote the matrix
of corresponding responses  We study the MRR problem

 cid 

 

   cid   cid 

 

 

 cid 

min
 

       cid   

 

 cid cid XW     cid cid 

 

LSR is   special case of MRR  with       and       The
optimal solution   cid  can be obtained in   nd    nmd 
time using   QR decomposition of    Sketching can be
applied to MRR in two ways 

Wc    XT SST       Id 
Wh    XT SST       Id 

 
 

 XT SST   
XT   

 
 

Following the convention of Pilanci   Wainwright  
Wang et al      we call Wc classical sketch and Wh
Hessian sketch  which approximate the optimal solution
  cid  Table   lists the time costs of the three solutions to
MRR 

  Main Results and Contributions
We  rst study classical and Hessian sketches from the optimization perspective  Theorems   and   show that

  Classical sketch achieves relative error in the objective
value  With sketch size           the objective
satis es    Wc              cid 

  Hessian sketch does not achieve relative error in the
  is much
objective value 
larger than      cid  then    Wh  can be far larger than
     cid 

In particular  if  

  cid   cid 

  For both classical and Hessian sketch  the relative
quality of approximation improves as the regularization parameter   increases 

We then study classical and Hessian sketches from the statistical perspective  by modeling     XW      as the
sum of   true linear model and random noise  decomposing the risk          cid XW   XW cid 
  into bias and
variance terms  and bounding these terms  We draw the
following conclusions  see Theorems      

 

  The bias of the classical sketch can be nearly as
small as that of the optimal solution  The variance

 cid  times that of the optimal solution  this bound

is  cid   

is optimal  Therefore overregularization       large
  should be used to supress the variance 
 As  
increases  the bias increases  and the variance decreases 

  Since   is not sketched with Hessian sketch  the variance of Hessian sketch can be close to the optimal solution  However  Hessian sketch has high bias  especially when    is small compared to  cid   cid 
  This indicates that overregularization is necessary for Hessian
sketch to have low bias 

which has optimal solution

  cid     XT       Id 

 
Here    denotes the MoorePenrose inversion operation 

XT   

 

Our empirical evaluations bear out these theoretical results 
In particular  in Section   we show in Figure   that even
when the regularization parameter   is  netuned  the risks
of classical sketch and Hessian sketch are worse than that

Sketched Ridge Regression

of the optimal solution by an order of magnitude  This is
an empirical demonstration of the fact that the nearoptimal
properties of sketching from the optimization perspective
are much less relevant in   statistical setting than its suboptimal statistical properties 
We propose to use model averaging  which averages the
solutions of   sketched MRR problems  to attain lower optimization and statistical errors  Without ambiguity  we denote classical and Hessian sketches with model averaging
by Wc and Wh  respectively  Theorems         give
the following results 

  Classical Sketch  Assume the sketch size          
   
    then the bound on    Wc         cid  is
and      
    and    
proportional to  
    the bias does not increase  the variance bound is
 
   
proportional to  

    Assume that          

  Hessian Sketch  Assume that          

     Assume that          

    and      
    
then the bound on    Wh         cid  is proportional
to  
    the variance does
not increase  if  additionally       
  and    is much
smaller than the squared spectral norm of    then the
bias bound is proportional to  
   

Note that classical sketch with uniform sampling and
model averaging is very well known as bagging  Breiman 
   or pasting  Breiman    or bootstrap aggregating  Different from bagging  our model averaging approach is not limited to uniform sampling 
Classical sketch with model averaging has three immediate
applications  In the singlemachine setting 

  Classical sketch with model averaging offers   way to
improve the statistical performance in the presence of
heavy noise  Assume the sketch size is        
nd 
As   grows larger than  
    the variance of the averaged
solution can be even lower than the optimal solution 
See Remark   for further discussion  This observation
is in accordance with the observation that bagging reduces variance 

 

In the distributed setting 
the featureresponse pairs
          xn  yn    Rd   Rm are divided among  
machines  Assuming that the data have been shuf ed randomly  each machine contains   sketch constructed by uniformly sampled rows from the dataset without replacement 
In this setting  the model averaging procedure will communicate the   local models only once to return the  nal
estimate  this process has very low communication complexity and latency  and it suggests two further applications
of classical sketch with model averaging 

  Model Averaging for Machine Learning 

If   lowprecision solution is acceptable  the averaged solution
can be used in lieu of distributed numerical optimization algorithms requiring multiple rounds of commug is big enough compared to   and the row
nication  If  
coherence of   is small  then  oneshot  model averaging has bias and variance comparable to the optimal
solution 

  Model Averaging for Optimization 

If   highprecision solution to MRR is required  then an iterative numerical optimization algorithm must be used 
The cost of such numerical optimization algorithms
heavily depends on the quality of the initialization 
  good initialization saves lots of iterations  The averaged model is provably close to the optimal solution 
so model averaging provides   highquality initialization for more expensive algorithms 

  Prior Work

The body of work on sketched LSR mentioned earlier
 Drineas et al      Clarkson   Woodruff   
Meng   Mahoney    Nelson   Nguy en    shares
many similarities with our results  However  the theories of
sketched LSR developed from the optimization perspective
do not obviously extend to MRR  and the statistical analysis of LSR and MRR differ  among other differences  LSR
is unbiased while MRR has   nontrivial bias and therefore
has   biasvariance tradeoff that must be considered 
Lu et al    has considered   different application of
sketching to ridge regression  they assume    cid     reduce
the number of features in   using sketching  and conduct
statistical analysis  Our setting differs in that we consider
   cid     reduce the number of samples by sketching  and
allow for multiple responses 
The model averaging analyzed in this paper is similar in
spirit to the AVGM algorithm of  Zhang et al    When
classical sketch is used with uniform row sampling without
replacement  our model averaging procedure is   special
case of AVGM  However  our results do not follow from
those of  Zhang et al     rst  we make no assumption
on the data  whereas they assumed      xn are       
from an unknown distribution  second  our results apply
to many other sketching ensembles than uniform sampling
without replacement  and third  we provide both optimization and statistical perspectives  whereas they provide only
  statistical perspective  Our results clearly indicate that the

 For example 
    

 

 

the conjugate gradient method satis es
  the stochastic block coordinate descent  Tu

 cid       cid cid 
 cid     cid cid 
et al    satis es Ef           cid 
  Here      is the
          cid 
output of the tth iteration            depend on the condition number of XT       Id and some other factors 

    

Sketched Ridge Regression

performance critically depends on the row coherence of   
this dependence is not captured in  Zhang et al    For
similar reasons  our work is different from the divideand 
conquer kernel ridge regression algorithm of  Zhang et al 
 
Iterative Hessian sketch has been studied by Pilanci  
Wainwright   Wang et al      By way of comparison  all the algorithms in this paper are  oneshot 
rather than iterative  Upon completion of this paper  we noticed the contemporary works  Avron et al    Thanei
et al    Avron et al    studied classical sketch
from the optimization perspective  and Thanei et al   
studied LSR with model averaging 

  Paper Organization

Section   de nes our notation and introduces the sketching
schemes we consider  Section   presents our theoretical
results  Section   conducts experiments to verify our theories and demonstrates the usefulness of model averaging 
Proofs of our claims and more empirical evaluations can be
found in the technical report version  Wang et al   

  Preliminaries
Throughout  we take In to be the      identity matrix and
  to be   vector or matrix of all zeroes of the appropriate
size  Given   matrix      aij  the ith row is denoted
by ai  and     denotes the jth column  The Frobenius
and spectral norms of   are written as  respectively   cid   cid  
and  cid   cid  The set          is written     Let     
and   be the standard asymptotic notation  Let    conceal
logarithm factors 
Throughout  we        Rn   as our matrix of features 
We set     rank    and write the SVD of   as    
  VT   where        are respectively            
and       matrices  We let               be the
singular values of    The MoorePenrose inverse of  
 UT   The row leverage scores
is de ned by        
  for         The row coherence of
of   are li    cid     cid 
  Throughout  we let   be
  is        
shorthand for    
Matrix sketching turns big matrices into smaller ones without losing too much information useful in tasks like linear
regression  We denote the process of sketching   matrix
    Rn   by   cid    ST    Here      Rn   is called  
sketching matrix and   cid    Rs   is called   sketch of   
In practice  except for Gaussian projection  where the entries of   are        sampled from         the sketching
matrix   is not formed explicitly  Matrix sketching can be
accomplished by random sampling or random projection 
Random sampling corresponds to sampling rows of  

  maxi  cid     cid 

       with replacement according to given row sampling
probabilities      pm       The corresponding  random  sketching matrix     Rn   has exactly one nonzero
entry per column  whose position indicates the index of the
selected row  in practice  this   is not explicitly formed 
Uniform sampling  xes          pn    
   Leverage
score sampling sets pi proportional to the  exact or approximate  Drineas et al    row leverage scores li of    In
practice shrinked leverage score sampling can be   better
choice than leverage score sampling  Ma et al    The
sampling probabilities of shrinked leverage score sampling
are de ned by pi    
 

 cid 

li cid  

 cid 

   
 

   lj

Gaussian projection is also wellknown as the prototypical JohnsonLindenstrauss transform  Johnson   Lindenstrauss    Let     Rm   be   standard Gaussian matrix       each entry is sampled independently from
      The matrix      
    is   Gaussian projection
matrix  It takes   nds  time to apply     Rn   to any
      dense matrix  which makes Gaussian projection inef cient relative to other forms of sketching 
Subsampled randomized Hadamard transform  SRHT 
 Drineas et al    Lu et al    Tropp    is
  more ef cient alternative to Gaussian projection  Let
Hn   Rn   be the WalshHadamard matrix with   and
  entries      Rn   be   diagonal matrix with diagonal
entries sampled uniformly from   and     Rn  
be the uniform row sampling matrix de ned above  The
  DHnP   Rn   is an SRHT matrix  and
matrix      
can be applied to any       matrix in   nd log    time 
In practice  the subsampled randomized Fourier transform
 SRFT   Woolfe et al    is often used in lieu of the
SRHT  because the SRFT exists for all values of    whereas
Hn exists only for some values of    Their performance
and theoretical analyses are very similar 
CountSketch can be applied to any     Rn   in   nd 
time  Charikar et al    Clarkson   Woodruff   
Meng   Mahoney    Nelson   Nguy en    Pham
  Pagh    Weinberger et al    Though more ef 
 cient to apply  CountSketch requires   bigger sketch size
than Gaussian projections  SRHT  and leverage score sampling to attain the same theoretical guarantees  The readers
can refer to  Woodruff    for   detailed description of
CountSketch 

  Main Results
Sections   and   analyze sketched MRR from  respectively  optimization and statistical perspectives  Sec 

 In fact  pi can be any convex combination of

 Ma et al    We use the weight  
sions extend in   straightforward manner to other weightings 

li cid  

and  
 
  for simplicity  our conclu 

   lj

Sketched Ridge Regression

tions   and   capture the impacts of model averaging
on  respectively  the optimization and statistical properties
of sketched MRR 
We described six sketching methods in Section   For simplicity  in this section  we refer to leverage score sampling 
shrinked leverage score sampling  Gaussian projection  and
SRHT as the four sketching methods  and we will mention explicitly uniform sampling and CountSketch  The notation de ned in Table   are used throughout 

Table   The commonly used notation 

Notation
    Rn  
    Rn  
  VT

 
 

 

    Rn  

De nition
each row is   data sample  feature vector 
each row contains the corresponding responses
the SVD of  
the row coherence of  
the regularization parameter
   
   
  sketching matrix

 cid   cid 

 cid   cid 

   

 

  Sketched MRR  Optimization Perspective
Theorem   shows that    Wc  the objective value of classical sketch  is very close to the optimal objective value
     cid  The approximation quality improves as   increases 
Theorem    Classical Sketch  For the four sketching

 cid  and CountSketch with       cid     

 cid  uniform sampling with    
 cid  the in 

methods with        cid    
  cid     log  

 

 

 
equality

   Wc         cid           cid 

holds with probability at least  

  cid   cid 

  cid   cid 

The corresponding guarantee for the performance of Hessian sketch is given in Theorem   It is weaker than the
guarantee for classical sketch  especially when  
  is
far larger than      cid  If   is nearly noiseless   is wellexplained by   linear combination of the columns of   
and   is small  then      cid  is close to zero  and consequently      cid  can be far smaller than  
    Therefore  in this case which is ideal for MRR     Wh  is not
close to      cid  and our theory suggests Hessian sketch
does not perform as well as classical sketch  This is veri 
 ed by our experiments  which show that unless   is big or
  large portion of   is outside the column space of    the
ratio    Wh 
Theorem    Hessian Sketch  For the four sketching meth 

 cid  uniform sampling with    

ods with        cid    
  cid     log  

 cid  and CountSketch with           
 cid 

 cid   cid   cid 
         cid 

   Wh         cid     

     cid  can be large 

 
equality

  the in 

 

 

 

 

holds with probability at least  

These two results imply that    Wc  and    Wh  can be
close to      cid  When this is the case  curvature of the
objective function ensures that the sketched solutions Wc
and Wh are close to the optimal solution   cid  Lemma  
studies the Mahalanobis distance  cid         cid cid 
    Here
  is any nonsingular matrix  in particular  it can be the
identity matrix or  XT   
Lemma   Let   be the objective function of MRR de ned
in       Rd   be arbitrary  and   cid  be the optimal
solution de ned in   For any nonsingular matrix    the
Mahalanobis distance satis es
 
 

 cid XT SST       Id   cid   
  cid XW  
  in terms of the difference in the objective values 

By choosing      XT    we can bound  
XW cid cid 

 cid cid         cid cid cid 

             cid 

 

 

min

 

 cid cid XW   XW cid cid cid 
  cid XWc   XW cid cid 

 

   cid              cid cid 
  cid XWh   XW cid cid 
   

  or  

With Lemma   we can directly apply Theorems   or   to
bound  

 
 

  Sketched MRR  Statistical Perspective
We consider the following  xed design model  Let    
Rn   be the observed feature matrix       Rd   be the
true and unknown model      Rn   contain unknown
random noise  and

 
be the observed responses  We make the following standard
weak assumptions on the noise 

    XW     

      

and          In 
We observe   and   and seek to estimate   
We can evaluate the quality of the estimate by the risk 

  cid cid XW   XW 

 cid cid 

 

 

        
 

 
where the expectation is taken        the noise   We study
the risk functions     cid    Wc  and   Wh  in the following 
Theorem    BiasVariance Decomposition  We consider
the data model described in this subsection  Let   be   cid 
Wc  or Wh  as de ned in       respectively  then
the risk function can be decomposed as

       bias      var   

Recall the SVD of          VT   The bias and variance
terms can be written as

bias cid   cid cid     
var cid   cid cid   
bias cid Wc cid     
var cid Wc cid   

 
 

 VT   

 cid cid cid        
 cid cid cid cid        
 cid cid cid cid 
 cid cid cid cid UT SST          
 cid cid cid cid UT SST       
 cid 

 cid cid cid  
 cid 
UT SST cid cid cid 

 

 

 

 
 
 
 

 cid cid cid  

 

 VT   

 
 

 

 

Sketched Ridge Regression

bias cid Wh cid     
var cid Wh cid   

 

 
 

    UT SST     

 cid cid cid cid 
 cid 
 cid 
 cid UT SST       
 cid cid cid cid UT SST       
 cid cid cid cid 

  

 
 

 

 

 VT   

 cid cid cid  

 

Theorem   provides upper and lower bounds on the bias
and variance of the classical sketch  In particular  we see
that that bias Wc  is within   factor of   of bias   cid 
However  var Wc  is    
Theorem    Classical Sketch  For Gaussian projection
and SRHT sketching with          
    uniform sampling
with          log  
    the
inequalities

  or CountSketch with          

    times worse than var   cid 

 

        bias Wc 
bias   cid 
  var Wc 
var   cid 

 
 

     

       

       

 
 

hold with probability at least  
For shrinked leverage score sampling with          log  
 
these inequalities  except for the lower bound on the variance  hold with probability at least  

 

Theorem   establishes similar upper and lower bounds on
the bias and variance of Hessian sketch  The situation is the
reverse of that with classical sketch  the variance of Wh is
close to that of   cid  if   is large enough  but as the regularization parameter   goes to zero  bias Wh  becomes much
larger than bias   cid 
Theorem    Hessian Sketch  For the four sketching
methods with          
    uniform sampling with    
     log  
    the in 
 
equalities

  and CountSketch with          
 cid 

bias Wh 
bias   cid 

 cid 
        var Wh 
var   cid 

       

   

 

 cid   cid 
  
       

 

hold with probability at least   Further assume that
      
 

    Then

bias Wh 
bias   cid 

   
     

 cid   

 
  

 cid 

   

holds with probability at least  

The lower bound on the bias shows that Hessian sketch
can suffer from   much higher bias than the optimal solution  The gap between bias Wh  and bias   cid  can be

 For shrinked leverage score sampling   cid   cid 

  does not enjoy
nontrivial lower bound  This is why we do not have   lower bound
on the variance 

lessened by increasing the regularization parameter   but
such overregularization increases the baseline bias   cid 
itself 
It is also worth mentioning that unlike bias   cid 
and bias Wc  bias Wh  is not monotonically increasing
with   as is empirically veri ed in Figure  
In sum  our theories show that classical and Hessian
sketches are not statistically comparable to the optimal solutions  classical sketch has too high   variance  and Hessian sketch has too high   bias for reasonable amounts of
regularization  In practice  the regularization parameter  
should be tuned to optimize the prediction accuracy  Our
experiments in Figure   show that even with  netuned  
the risks of classical and Hessian sketches can be higher
than the risk of the optimal solution by an order of magnitude  Formally speaking  min    Wc   cid  min      cid 
and min    Wh   cid  min      cid  hold in practice 
Our empirical study in Figure   suggests classical and Hessian sketches both require overregularization       setting
  larger than what is best for the optimal solution   cid  Formally speaking  argmin    Wc    argmin      cid  and
argmin    Wh    argmin      cid  Although this is the
case for both types of sketches  the underlying explanations
are different  Classical sketch has   high variance  so  
large   is required to supress the variance  its variance is
nonincreasing with   Hessian sketch has very high bias
when   is small  so   reasonably large   is necessary to
lower its bias 

  Model Averaging  Optimization Perspective

 

  

   Wc

 cid  

 cid  

  or Wh    

We consider model averaging as an approach to increasing the accuracy of sketched MRR solutions  The model
averaging procedure is straightforward  one independently
draws   sketching matrices      Sg   Rn    uses these
to form   sketched MRR solutions  denoted by  Wc
   
or  Wh
   
   and averages these solutions to obtain the  
   Wh
nal estimate Wc    
   
 
Practical applications of model averaging are enumerated
in Section  
Theorems   and   present guarantees on the optimization
accuracy of using model averaging to combine the classical
or Hessian sketch solutions  We can contrast these with the
guarantees provided for sketched MRR in Theorems   and
  For classical sketch with model averaging  we see that
    the bound on    Wh      cid  is proportional
when      
to     From Lemma   we can see that the distances from
Wc to   cid  also decreases accordingly 
Theorem    Classical Sketch with Model Averaging  For

 cid  for uniform sampling  let
the four methods  let        cid    
 cid  Then the inequality
      cid     log  
     cid 
   Wc         cid     cid   

     cid 

 

 

Sketched Ridge Regression

Figure   Empirical study of classical sketch and Hessian sketch from optimization perspective  The xaxis is the regularization parameter    logscale  the yaxis is the objective function values  logscale 

holds with probability at least  

     
For Hessian sketch with model averaging  if  
     then
the bound on    Wh         cid  is proportional to  
    
Theorem    Hessian Sketch with Model Averaging  For

the four methods let        cid    
pling let       cid     log  
   Wh         cid     cid   

 cid  and for uniform sam 
 cid  then the inequality
 cid cid   cid   cid 
         cid cid 

      

 

 

 

 

holds with probability at least  

  Model Averaging  Statistical Perspective

Model averaging also has the salutatory property of reducing the risks of the classical and Hessian sketch solutions 
Our  rst result conducts   biasvariance decomposition for
the averaged solution of sketched MRR 
Theorem    BiasVariance Decomposition  We consider
the  xed design model   The risk function de ned in  
can be decomposed as

       bias      var   

bias cid Wh cid     
var cid Wh cid   

 
 

   

 

 
 

 cid cid cid   
  cid 
 cid 
 cid UT SiST
 cid cid cid   
  cid 
 cid UT SiST

  

 

  

        
UT SiST
  

 cid 
 cid cid cid cid 

 

 

        

        

 VT   

 cid cid cid  

 

 cid 

Theorems   and   provide upper bounds on the bias
and variance of modelaveraged sketched MRR for  respectively  classical sketch and Hessian sketch  We can contrast
them with Theorems   and   to see the statistical bene ts
of model averaging 
Theorem    Classical Sketch with Model Averaging 
For shrinked leverage score sampling  Gaussian projec 

 cid  or uniform sampling with

tion  SRHT with        cid   
 cid  the inequalities
      cid     log  
 cid cid    

       

 cid 

 

 

bias Wc 
bias   cid 
var Wc 
var   cid 

   
 

     

hold with probability at least  
Remark   From this result  we see that if      
the variance is proportional to  

    If   and   are at least

    then

      cid   

 cid 

 

       cid 

nd cid 

and

The bias and variance terms are

bias cid Wc cid     
var cid Wc cid   

 
 

 
 

 cid cid cid   
  cid 
 cid UT SiST
 cid cid cid   
  cid 
 cid UT SiST

  

 

 

  

           

 VT   

        

UT SiST
 

 

 cid 

 cid 

 cid cid cid  

 

then the risk   Wc  is close to     cid  If   and   are
larger  then the variance var Wc  can even be even lower
than var   cid 

Theorem   shows that model averaging decreases the bias
of Hessian sketch without increasing the variance  For Hessian sketch without model averaging  recall that bias Wh 

 cid cid cid 

 

 Wc Objective Function Objective Function Objective Function Wh Objective Function Objective Function Objective Function       Uniform SamplingLeverage SamplingShrinkage Lev  SamplingGaussian ProjectionSRFTCount SketchOptimal SolutionSketched Ridge Regression

Figure   Empirical study of classical sketch and Hessian sketch from statistical perspective  The xaxis is the regularization parameter
   logscale  the yaxes are respectively bias  variance  and risk  logscale  We annotate the minimum risks and optimal   in the plots 
is larger than bias   cid  by   factor of   cid   cid 
   
Theorem   shows that model averaging reduces this ratio by   factor of  
Theorem    Hessian Sketch with Model Averaging  For

  when      
   

the four methods with        cid   
 cid  the inequalities
with       cid     log  
         cid   

 cid  or uniform sampling
   cid cid   cid 

 

 

 

 

 

  

bias Wh 
bias   cid 
var Wh 
var   cid 

       

We conducted experiments on synthetic data to verify Theorems   and   and to show the effects of classical and Hessian sketching on the bias and variance  We set the noise
intensity to be       In Figure   we plot the analytical
expressions for the squared bias  variance  and risk stated in
Theorem   against the regularization parameter   The results of this experiment match our theory  classical sketch
magni ed the variance  and Hessian sketch increased the
bias  Even if   is  netuned  the risks of classical and Hessian sketches can be much higher than those of the optimal
solution  Our experiments also indicate that classical and
Hessian sketches require setting   larger than the best regularization parameter for the optimal solution   cid 

  Conclusions
We studied sketched matrix ridge regression  MRR  from
optimization and statistical perspectives  Using classical
sketch  by taking   large enough sketch  one can obtain
an  accurate approximate solution  Counterintuitively and
in contrast to classical sketch  the relative error of Hessian sketch increases as the responses   are better approximated by linear combinations of the columns of    Both
classical and Hessian sketches can have statistical risks that
are worse than the risk of the optimal solution by an order
of magnitude  We proposed the use of model averaging
to attain better optimization and statistical properties  We
have shown that model averaging leads to substantial improvements in the theoretical error bounds  suggesting applications in distributed optimization and machine learning 

 In the experiment yielding Figure   Hessian sketch had
lower risk than classical sketch  This is not generally true  if we
used   smaller   so that the variance is dominated by bias  then
classical sketch results in lower risks than Hessian sketch 

hold with probability at least  

  Sketched Ridge Regression Experiments
Following  Ma et al    Yang et al    we
constructed     Rn   to have condition number
 XT        and high row coherence   xed     
            and set     Xw        Rn  where
the entries of     Rn were        sampled from      
The details of this data model are given in the technical report version  Wang et al    Let     Rn   be any of
the six sketching methods considered in this paper  We   
            and         Because the analytical expressions involve the random sketching matrix    we
randomly generate    repeat this procedure   times  and
report the averaged results 
In Figure   we plot the objective function value        
  cid Xw     cid 
  against   under different settings
 
of noise intensity   The results verify our theory  classical
sketch wc is always close to optimal  Hessian sketch wh
is much worse than the optimal when   is small and   is
mostly in the column space of   

     cid   cid 

Bias VarRisk Bias VarWc Bias Variance Risk the min risk of the classical sketchassicthe min risk of the optimal solutioneoeopt optimal   for the optimal solutionoptimal   for the classical sketch tionsicala Wh Bias Variance Risk the min risk of the Hessian sketchesssiathe min risk of the optimal solutioneoeop optimal   for the optimal solutionoptimal   for the Hessian sketch tionssian       Uniform SamplingLeverage SamplingShrinkage Lev  SamplingGaussian ProjectionSRFTCount SketchOptimal SolutionSketched Ridge Regression

Acknowledgements
We thank the anonymous reviewers for their helpful suggestions  We thank the Army Research Of ce and the Defense Advanced Research Projects Agency for partial support of this work 

References
Avron  Haim  Clarkson  Kenneth    and Woodruff  David   
Sharper bounds for regression and lowrank approximation
with regularization  arXiv preprint arXiv   

Breiman  Leo  Bagging predictors  Machine Learning   

   

Breiman  Leo  Pasting small votes for classi cation in large
databases and online  Machine Learning   
 

Charikar  Moses  Chen  Kevin  and FarachColton  Martin  Finding frequent items in data streams  Theoretical Computer Science     

Clarkson  Kenneth    and Woodruff  David    Low rank approximation and regression in input sparsity time  In Annual ACM
Symposium on theory of computing  STOC   

Drineas  Petros  Mahoney  Michael    and Muthukrishnan    
Sampling algorithms for  cid  regression and applications  In Annual ACMSIAM Symposium on Discrete Algorithm  SODA 
 

Drineas  Petros  Mahoney  Michael    Muthukrishnan     and
Sarl os  Tam as  Faster least squares approximation  Numerische
Mathematik     

Drineas  Petros  MagdonIsmail  Malik  Mahoney  Michael   
and Woodruff  David    Fast approximation of matrix coherence and statistical leverage  Journal of Machine Learning Research     

Johnson  William    and Lindenstrauss  Joram  Extensions of
Lipschitz mappings into   Hilbert space  Contemporary mathematics     

Lu  Yichao  Dhillon  Paramveer  Foster  Dean    and Ungar 
Lyle  Faster ridge regression via the subsampled randomized
Hadamard transform  In Advances in Neural Information Processing Systems  NIPS   

Ma  Ping  Mahoney  Michael    and Yu  Bin    statistical perspective on algorithmic leveraging  Journal of Machine Learning Research     

Mahoney  Michael    Randomized algorithms for matrices and
data  Foundations and Trends in Machine Learning   
   

Pham  Ninh and Pagh  Rasmus  Fast and scalable polynomial kernels via explicit feature maps  In ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining  KDD 
 

Pilanci  Mert and Wainwright  Martin    Iterative Hessian sketch 
Fast and accurate solution approximation for constrained leastsquares  Journal of Machine Learning Research  pp   
 

Raskutti  Garvesh and Mahoney  Michael      statistical perspective on randomized sketching for ordinary leastsquares 
Journal of Machine Learning Research     

Thanei  GianAndrea  Heinze  Christina  and Meinshausen  NicoarXiv

lai  Random projections for largescale regression 
preprint arXiv   

Tropp  Joel    Improved analysis of the subsampled randomized
Hadamard transform  Advances in Adaptive Data Analysis   
     

Tu  Stephen  Roelofs  Rebecca  Venkataraman  Shivaram  and
Recht  Benjamin  Large scale kernel learning using block coordinate descent  arXiv preprint arXiv   

Wang  Jialei  Lee  Jason    Mahdavi  Mehrdad  Kolar  Mladen 
and Srebro  Nathan  Sketching meets random projection in
the dual    provable recovery algorithm for big and highdimensional data  arXiv preprint arXiv     

Wang  Shusen  Gittens  Alex  and Mahoney  Michael    Sketched
ridge regression  Optimization perspective  statistical perspective  and model averaging  arXiv preprint arXiv 
 

Wang  Yining  Yu  Adams Wei  and Singh  Aarti 

Computationally feasible nearoptimal subset selection for linear
arXiv preprint
regression under measurement constraints 
arXiv     

Weinberger  Kilian  Dasgupta  Anirban  Langford  John  Smola 
Alex  and Attenberg  Josh  Feature hashing for large scale multitask learning  In International Conference on Machine Learning  ICML   

Woodruff  David    Sketching as   tool for numerical linear algebra  Foundations and Trends   cid  in Theoretical Computer Science     

Woolfe  Franco  Liberty  Edo  Rokhlin  Vladimir  and Tygert 
Mark    fast randomized algorithm for the approximation of
matrices  Applied and Computational Harmonic Analysis   
   

Yang  Jiyan  Meng  Xiangrui  and Mahoney  Michael   

Implementing randomized matrix algorithms in parallel and distributed environments  Proceedings of the IEEE   
   

Meng  Xiangrui and Mahoney  Michael    Lowdistortion subspace embeddings in inputsparsity time and applications to robust linear regression  In Annual ACM Symposium on Theory
of Computing  STOC   

Zhang  Yuchen  Duchi  John    and Wainwright  Martin   
Communicationef cient algorithms for statistical optimization  Journal of Machine Learning Research   
 

Nelson  John and Nguy en  Huy    Osnap  Faster numerical linear algebra algorithms via sparser subspace embeddings 
In
IEEE Annual Symposium on Foundations of Computer Science
 FOCS   

Zhang  Yuchen  Duchi  John  and Wainwright  Martin  Divide
and conquer kernel ridge regression    distributed algorithm
with minimax optimal rates  Journal of Machine Learning Research     

