Relative Fisher Information and Natural Gradient for Learning Large

Modular Models

Ke Sun   Frank Nielsen    

Abstract

Fisher information and natural gradient provided
deep insights and powerful tools to arti cial neural networks  However related analysis becomes
more and more dif cult as the learner   structure
turns large and complex  This paper makes   preliminary step towards   new direction  We extract
  local component from   large neural system 
and de ne its relative Fisher information metric
that describes accurately this small component 
and is invariant to the other parts of the system 
This concept is important because the geometry
structure is much simpli ed and it can be easily
applied to guide the learning of neural networks 
We provide an analysis on   list of commonly
used components  and demonstrate how to use
this concept to further improve optimization 

  Fisher Information Metric
The Fisher Information Metric  FIM        Iij  of  
statistical parametric model        of order   is de ned
by         positive semide nite  psd  matrix      cid   
with coef cients Iij   Ep
  where    denotes
the logdensity function log        Under light regular 
 cid 
ity conditions  FIM can be rewritten equivalently as

 cid 
 cid    
 cid   cid       

 cid       

  
  

 cid 

  

Iij    Ep

  

    

   

  

  

dx 

As its empirical counterpart  the observed FIM  Efron  
Hinkley    with respect to  wrt    sample set Xn  
 xk  
   is     Xn        Xn  which is often evaluated at the maximum likelihood estimate    
 Xn  By the law of large numbers      converges to
the  expected  FIM    as      

 King Abdullah University of Science and Technology  KAUST  Saudi Arabia    Ecole Polytechnique  France
 Sony Computer Science Laboratories Inc 
Correspondence to  Ke Sun  sunk ieee org  Frank Nielsen
 Frank Nielsen acm org 

Japan 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

 cid 

  

The FIM is not invariant and depends on the parameterization  We can optionally write    as    to emphasize the coordinate system  By de nition      
 cid     where      Jij  Jij     
is the JacoJ
bian matrix  For example  the FIM of regular natural ext           logponential families  NEFs        
linear models with suf cient statistics      is     
      cid    the Hessian of the lognormalizer function
    Although exponential families can approximate arbitrarily any smooth density  Cobb et al    the lognormalizer function may not be available in closedform
nor computationally tractable  Montanari   
The FIM is an important concept for statistical machine
learning  It gives   Riemannian metric  Hotelling   
Rao    of the learning parameter space which is
unique    Cencov    Dowty    Hence any learning is in   space that is intrinsically curved based on the
FIM  regardless of the choice of the coordinate system  It
also gives   bound  Fr echet    Cram er    Nielsen 
  of learning ef ciency saying that the variance of any
unbiased learning of   is at least      where   is
the        sample size  The FIM is applied to neural network
optimization  Amari    metric learning  Lebanon 
  reinforcement learning  Thomas    and manifold learning  Sun   MarchandMaillet   
However computing the FIM is expensive  Besides the fact
that learning machines have often singularities  Watanabe 
          not full rank  characterized by plateaux
in gradient learning  computing estimating the FIM of  
large neuron system       one with millions of parameters 
Szegedy  Christian et al    is very challenging due to
the  niteness of data  and the huge number     
of matrix coef cients to evaluate  Furthermore  gradient descent
techniques require inverting this large matrix and tuning the
learning rate 
To tackle this problem  past works mainly focus on how
to approximate the FIM with   block diagonal form  Kurita    Le Roux et al    Martens    Pascanu  
Bengio    Martens   Grosse    or quasidiagonal
form  Ollivier    MarceauCaron   Ollivier   
This global approach faces increasing approximation error
and increasing computational cost as the system scales up

 

Relative Fisher Information and Natural Gradient

and as complex and dynamic structures  Looks et al   
emerge 
This work aims at   different local approach  The idea is
to accurately describe the information geometry  IG  in  
subsystem of the large learning system  which is invariant
to the scaling up and structural change of the global system 
so that the local machinery  including optimization  can be
discussed regardless of the other parts 
For this purpose    novel concept  the Relative Fisher Information Metric  RFIM  is de ned  Unlike the traditional
geometric view of   highdimensional parameter manifold 
RFIMs de nes multiple projected lowdimensional geometries of subsystems  This geometry is correlated to the parameters beyond the subsystem and is therefore considered
dynamic  It can be used to characterize the ef ciency of
  local learning process  Taking this stance has potential
in deep learning because   deep neural network can be decomposed into many local components such as neurons or
layers  The RFIM is well suited to the compositional block
structures of neural networks  The RFIM can be used for
outof core learning 
The paper is organized as follows  Sec    reviews natural gradient within the context of MultiLayer Perceptrons
 MLPs  Sec    formally de nes the RFIM  and gives   table of RFIMs of several commonly used subsystems  Sec   
discusses the advantages of using the RFIM as compared
to the FIM  Sec    gives an algorithmic framework and
proofof concept experiments on neural network optimization  Sec    presents related works on parameter diagonalization  Sec    concludes this work and further hints at
perspectives 

  Natural Gradient  Review and Insights
Consider   MLP          hL 
cal model is the following conditional distribution
            

                  hL     

       whose statisti 

 cid 

    hL 

The often intractable sum over      hL 
can
   
rid off by deteriorating            
be get
  hL    hL      to Dirac   deltas   and letting
merely the last layer       hL      be stochastic  Other
models such as restricted Boltzmann machines  Nair  
Hinton    Montavon     uller    deep belief networks  Hinton et al    dropout  Wager et al   
and variational autoencoders  Kingma   Welling    do
consider the hi   to be stochastic 
The tensor metric of the neuromanifold  Amari   
   consisting of all MLPs with the same architecture but different parameter values 
is locally de ned
by the FIM  Because   MLP corresponds to   con 

  cid 

  

 

  

 cid  

 cid   lk

   Ep     xk 

 cid cid   

ditional distribution 
its FIM is   function of the input    By taking an empirical average over the input samples  xk  
the FIM of   MLP can be expressed as       
where lk    log       xk    denotes the conditional
loglikelihood function wrt xk 
To understand the meaning of the Riemannian metric
   it measures the intrinsic difference between two
nearby neural networks around          learning step
can be regarded as   tiny displacement   on    According to the FIM  the in nitesimal square distance

 lk

 

 cid cid 

 cid cid 

 cid   cid     

 
 

Ep     xk   

 

 cid   lk
 

      cid  

 
measures how much    with   radius constraint  is sta 
  or equivalently how much   affects
tistically along   
intrinsically the conditional distribution           
function
Consider
the
   log   yk   xk    wrt
the observed
pairs  xk  yk  
   we try to minimize the loss while
maintaining   small learning step size  cid   cid    on
   At         the target is to minimize wrt   the
Lagrange function

loglikelihood

negative

         
          

 
 
 cid   cid        

 cid   cid     
 
 

 

 cid     

where       is   learning rate  The optimal solution of the
above quadratic optimization gives   learning step

        

       cid      

In this update procedure          
     cid    
replaces the role of the usual gradient     and is
called the natural gradient  Amari   
Although the FIM depends on the chosen parameterization 
the natural gradient is invariant to reparameterization  Let
  be another coordinate system and   be the Jacobian matrix of the mapping       Then we have
   

  
     cid         

 cid   cid    

 cid      

      

     cid    

showing that     and     are the same dynamic
up to coordinate transformation  As the learning rate   is
not in nitesimal in practice  natural gradient descent actually depends on the coordinate system  see      Martens
  Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaux
of the error surface  and attaining Fisher ef ciency in online learning  see Sec    Amari  

   cid 

 cid 

Relative Fisher Information and Natural Gradient

  

  

           

           

          

Model 

Manifold 

          
  

Computational graph 

 

 

 

 

Metric 

    

      

        

 

  

 

  

  

 
 

  

 

 

  

  

 

 

  

  gh   

 

gh   

 

gy 

  

  

  

        
  

 

Figure    left  The traditional global geometry of   MLP   right  information geometry of subsystems  The gray and blue meshes show
that the subsystem geometry is dynamic when the reference variable makes   tiny move  The square under the  sub system means the
   FIM is computed by     computing the FIM in the traditional way wrt all free parameters that affect the system output   ii  choosing
  subblock that contains only the internal parameters of the  sub system and regarding the remaining variables as the reference 

For the sake of simplicity  we do not discuss singular FIMs
with   subset of parameters having zero metric  This set
of parameters forms an analytic variety  Watanabe   
and technically the MLP as   statistical model is said to
be nonregular  and the parameter   is not identi able 
The natural gradient has been extended  Thomas   
to cope with singular FIMs having positive semide nite
matrices by taking the MoorePenrose pseudoinverse  that
coincides with the inverse matrix for full rank matrices 
In the family of  ndorder optimization methods    fuzzy
line can be drawn from the natural gradient and alternative
methods such as the Hessianfree optimization  Martens 
  By de nition  the FIM is   property of the parameter space which is independent or weakly dependent on the
input samples  For example  the FIM of   MLP is independent of  yi  In contrast  the Hessian  or related concepts
such as the GaussNewton matrix  Martens   is   property of the learning cost function wrt the input samples 
Bonnabel  Bonnabel    proposed to use the Riemannian exponential map to de ne   gradient descent step  thus
ensuring to stay on the manifold for any chosen learning
rate  Convergence is proven for Hadamard manifolds  of
negative curvatures  However  it is not mathematically
tractable to express the exponential map of hierarchical
model manifolds like the neuromanifold 

  RFIM  De nition and Expressions
In general  for large parametric systems  it is impossible
to diagonalize or decorrelate all the parameters  so that we
split instead all random variables into three parts        and
   We examine their intuitive meanings before giving the
formal de nition  The reference       consists of the majority of the random variables that are considered  xed  therefore allowing us to simplify the analysis  This is in analogy to the notion of   reference frame in physics    is the

subsystem parameters  resembling the longterm memory
adapting slowly to the observations       neural network
weights  The response   is   random variable that reacts
to the variations of   Usually    is the output of the subsystem that is connected to neighbour subsystems       hidden
layer outputs  Formally    subsystem which factorizes the
learning machine is characterized by the conditional distribution             where   can be estimated based on  
and      We make the following de nition 
De nition    RFIM  Given      the RFIM   of   wrt   is
gh         
def  Ep           

 cid 
 cid  log            

log            

 cid   

 

 

 

or simply gh   corresponding to the estimation of  
based on observations of   given     

For example  consider   MLP  If we choose    to be the
input features    choose   to be the  nal output    and
choose   to be all the network weights   then the RFIM
becomes the FIM       gy    
More generally  we can choose the response   to be other
than the observables to compute the Fisher information
of subsystems  especially dynamically during the learning of the global machine  To see the meaning of the
RFIM  similar to eq    the in nitesimal square distance
 cid   cid gh    Ep           
measures how much   impacts intrinsically the stochastic
mapping       which features the subsystem  We have
the following proposition following de nition  
Proposition    Relative Geometry Consistency  If   consists of   subset of   so that         then    
   with the metric gh      has exactly the same Rie 
 We use the same term  relative FIM   Zegers    with  

  log            cid cid 

 cid cid 

 cid   

different de nition 

Relative Fisher Information and Natural Gradient

mannian metric with the submanifold       
  is  xed  induced by the ambient metric gh  

 

When the response   is chosen  then different splits of
       are consistent with the same ambient geometry 
Figure   shows the traditional global geometry of   learning system  where the curvature is de ned by the learner  
parameter sensitivity to the external environment    and
   as compared to the information geometry of subsystems  where the curvature is de ned by the parameter sensitivity wrt hidden interface variables    The twocolored
meshes show that the geometry structure is dynamic and
varies with the reference variable     
One should not confuse the RFIM with the diagonal blocks
of the FIM  Kurita    Both their meanings and expressions are different  The RFIM is computed by integrating out the hidden response variables    The FIM is always computed by integrating out the observables   and
   Hence the RFIM is   more general concept and includes the FIM as   special case  This highlights   main
difference with the backpropagated metric  Ollivier   
which essentially considers parameter sensitivity wrt the  
nal output  Despite the fact that the FIMs of small parametric structures such as single neurons was studied  Amari 
  we are not looking at   small singlecomponent system but   component embedded in   large system  targeting
at improving the large system 
In the following we provide   short table of commonly used
RFIMs for future reference  the RFIMs listed are mostly
straightforward from de nition   with detailed derivations
given in the supplementary material  This is meaningful
since the RFIM is   new concept  We also want to demonstrate these simple closed form expressions without any approximations 

  RFIMs of One Neuron

 cid 

 cid 

We start from the RFIM of single neuron models  Consider
  stochastic neuron with input   and weights    After  
nonlinear activation function    the output   is randomized
 cid 
surrounding the mean     
    with   variance  Through 
 cid  denotes the augmented vector
out this paper        
   
of    homogeneous coordinates  so that  
   contains  
bias term  and   general linear transformation can be written simply as      
Using   as the reference  the RFIM of   with respect
to   has   common form gy                         
 cid 
 
where           is   positive coef cient with large values
in the linear region  or the effective learning zone of the
neuron  This agrees with early studies on single neuron
FIMs  Amari    Kurita   
If         tanh    is the hyperbolic tangent func 

 cid 

 

 cid 

   cid 

   cid    sigm   

    where sech     
tion  then             sech  
exp   exp    is the hyperbolic secant function  Similarly  if         sigm    is the sigmoid function  then
            sigm   
If   is de ned by Parametric Recti ed Linear Unit
 PReLU   He et al    which includes Recti ed Linear Unit  ReLU   Nair   Hinton    as   special case 
so that                                              then
under certain approximations  see supplementary material 

 cid 

 cid       

 

 cid cid 

 cid 

 

  

 

           

         sigm

 cid 

 cid   

where       is   hyperparameter            
For the exponential linear unit  ELU   Clevert et al   
                            exp               where
      is   hyperparameter  We get

           

  exp   

 cid 
 cid 

      
      

if  
if  

 cid 

   

  RFIM of One Layer

 cid 

     cid      wDy

 cid  and stochastic output   can be

Let   denote the dimensionality of the corresponding variable    linear layer with input    connection weights
represented by        
        where   is the identity matrix  and   is the scale of the observation noise 
and      is   multivariate Gaussian distribution with
mean   and covariance matrix   We vectorize   by
stacking its columns  wi  Then gy        is   tensor
of size  Dx    Dy    Dx    Dy  given by gy         
  where diag  means the  block 
diag        
diagonal matrix constructed by the given matrix entries 
  nonlinear layer increments   linear layer by adding an
    and
elementwise activation function applied on  
then randomized wrt the choice of the neuron  By de 
nition   its RFIM is given by

         
 cid 

 cid 

 cid 

gy        
 cid 
  diag                  

          wm          

 cid 

   

 

where     wi     is given in Subsec   
  softmax layer  which often appears as the last layer of
  MLP  is given by                  where            
 cid  
   exp wi     Its RFIM is   dense matrix given by
        
 cid 
        
 cid 

exp wy    

gy      

     
 cid 
       
       
 cid 
 

         

 cid 

 
 
 
 

   

 

 
      
         

 cid 

        
 cid  resembles

Notice that its   th diagonal block       
the RFIM of   single sigm neuron 

Relative Fisher Information and Natural Gradient

  RFIM of Two Layers

By eq    the onelayer RFIM is   product metric  Jost 
  and does not consider the interneuron correlations 
which must be obtained by looking at   larger subsystem  Consider   twolayer model with stochastic output
  around the mean vector     
   
For simplicity  we ignore interlayer correlations between
the  rst layer and the second layer and focus on the interneuron correlations within the  rst layer  To do this  both  
and   are considered as references to compute the RFIM
of     By de nition   gy              Gij Dh Dh
and
each block has the form

 cid      where         

 cid 

Dy cid 

Gij  

 cid 
cilcjl    cl       wi       wj          

 

  

Now that we have the onelayer and twolayer RFIMs 
we can either split   given feedforward neural network
into onelayer subsystems or into twolayer subsystems 
  tradeoff is that using   larger subsystem entails greater
analytical and computational dif culty  although it could
more accurately model the global system dynamics  In the
extreme case  the FIM is obtained if the whole system is
considered as one single subsystem 

  cid 

  cid 

  RFIM  Key Advantages
This section discusses the theoretical advantages of the
RFIM over the FIM  Consider wlog   MLP with Bernoulli
outputs           whose mean   is   deterministic
function depending on the input   and the network parameters   By Sec    the FIM of the MLP can be computed
as  see supplementary for proof 

  

  

 

 
 

 

   xi 

    

   xi       xi 

   xi 
 cid   
 
Therefore rank      nm  The rank of   diagonal
block of    corresponding to one layer is even smaller 
In   deep neural network       Szegedy  Christian et al 
  if the sample size     dim    then   
is doomed to be singular  All methods trying to approximate the FIM suffer from this problem and therefore rely
on proper regularizations  If the network is decomposed
into layers  the RFIM of each subsystem  layer  is given
by eq    Each sample can contribute maximally   to
the rank of the neuronRFIM and can contribute maximally Dy to the rank of the layerRFIM  It only requires
maxi dim wi   the maximum layer width  observations
to have   full rank RFIM  where wi is the weight vector
of the   th neuron  The RFIM is expected to have   much
higher rank than the FIM  Higher rank means less singularity and more information is captured  Models that can

be distinguished by the RFIM may be identical in the sense
of the FIM  Essentially  the RFIM integrates the internal
randomness  Bengio    of the neural system by considering the output of each layer as   random variable  In
theory  the FIM should also consider stochastic neurons 
However it requires marginalizing the joint distribution of
             This makes the already infeasible computation even more challenging 
The RFIM is not an approximation of the FIM but is an accurate metric  de ning the geometry of   wrt to its direct
response   in the system  or adjacent nodes in   graphical
model  By the example in       gy    of the last layer is
exactly the corresponding block in    they both characterize how    affects the mapping hL       They
start to diverge from the second to last layer  To compute
the geometry of     the RFIM looks at how     affects the local mapping hL    hL  which can be measured reliably regardless of the rest of the system  think
of    debugging  process to separate and measure   single
component  In contrast  the FIM examines how     affects the nonlocal mapping hL       This is   dif cult
task because it must consider the correlation between different layers  As an approximation  the block diagonalized
version of the FIM ignores such correlations and therefore
faces the loss of accuracy 
The RFIM makes it possible to maintain global system stability so that the intrinsic variations of different subsystems
are balanced during learning  Consider   set of interconnected subsystems with internal parameters     and the
corresponding response variables  hl  The RFIM ghl    
measures how much the likelihood surface of hl is curved
wrt   small learning step     By constraining the squared
 cid 
Riemannian distance  
  ghl      having similar scales 
different subsystems will present similar variations during
learning  Within one subsystem  the learning along sensitive parameter directions is penalized  Among different
subsystems  the learning of sensitive subsystems is penalized  Globally  the intersubsystem stochastic connections
have similar variance  maintaining   stable reference system and achieving ef cient learning  This is similar to the
idea of batch normalization  BN   Ioffe   Szegedy   
but has   deeper theoretical foundation 
Formally  we have the following theorem 
Theorem   Consider   learning system represented by  
joint distribution         of    observables  and    hidden variables which connect subsystems  The joint FIM
      Ep
has   block diagonal form  Each block is Ep gh  where   is the parameters within   subsystem and   is its response variables to
neighour subsystems 

 cid  log          

log          

 cid 

 cid 

 

The global correspondence of the local RFIM is the joint

Relative Fisher Information and Natural Gradient

Ep cid 

    

FIM  By theorem   the square distance   

 cid        
 cid 
  ghl         measures the system variance  including both the observables   and the hidden variables
   An intrinsic tradeoff between the RFIM and the FIM
is learning system stability versus ef ciency  Normalizing the FIM is more ef cient because it helps to achieve
Fisher ef ciency  Amari    Normalizing the RFIM is
more stable since the hidden variations are bounded  which
only guarantees subsystem Fisher ef ciency characterized
by the Cram erRao lower bound of local parameters 

  Relative Natural Gradient Descent
The traditional nonparametric way of applying natural
gradient requires recalculating the FIM and solving   large
linear system in each learning step  Besides the huge computational cost  it has   large approximation error  For example during online learning    minibatch of samples cannot faithfully re ect the  true  geometry  which has to integrate the risk of sample variations  That is  the FIM of  
minibatch is likely to be singular or poorly conditioned 
  recent series of efforts  Montavon     uller    Raiko
et al    Desjardins et al    are gearing towards
  parametric approach to applying natural gradient  which
memorizes and learns   geometry  For example  natural
neural networks  Desjardins et al    augment each
layer with   redundant linear layer  and let these linear layers parametrize the geometry of the neural manifold 
By dividing the learning system into subsystems  the RFIM
potentially gives   systematical implementation of parametric natural gradient descent  The memory complexity
of storing the Riemannian metric has been reduced from
    where Di   dim wi  is the size
of the   th neuron  Consider there are   neurons in total 
then the memory cost is reduced by   factor of    The
computational complexity has been reduced from     
    Optimization
based on RFIM is called Relative Natural Gradient Descent
 RNGD 
The good performance of batch normalization  Ioffe  
Szegedy    provides an empirical support for the
RFIM  Basically  BN uses an intersample normalization
layer to transform the layer input   to   with zero mean and
unit variance and thus reduces  internal covariate shift  In
  typical case  above this normalization layer is   linear
layer given by      
    If each dimension of   is normalized  then the diagonal blocks of the linear layer RFIM
 cid 
  become   covariance magy       diag       
trix with identity diagonal entries  after taking an empirical
average  This gives the coordinate system     well conditioned RFIM for ef cient learning 

     to   cid 
      Williams   to   cid 

         
 cid 

    

    

 cid 

  RNGD with   relu MLP
This subsection builds   proofof concept experiment on
MLP optimization  We partition the MLP into layers  one
layer consists of   linear layer plus an elementwise nonlinear activation function  as the subsystems  By eq   
the RFIM of layer               with input hl       
   and weights  wl    wlml  is

    wl  hl hl    

 cid 
 cid 
           wlml   hl   hl    
  

 cid ml

one

The

during

subsystem stability
 cid 
       wli  hl  
li

learning step    can be measured geometrically by
Using this
term as the geometric cost  the Lagrange term  in the trust
region approach in Sec    we get the following RNGD
method 
In   stochastic gradient descent scenario  each
neuron   in layer   is updated by

 hl 

 cid 

diag

 cid  

  

 cid 

 

li   wold
wnew

li     

li

  
 wli

 

where   is the cost function and Gli is   learned metric 
The consideration is that   minibatch of samples do not
contain enough information to compute the RFIM  which
should be averaged over all training samples  Therefore 
for the   th neuron in layer    Gli is initialized to identity 
and is updated based on
li        Gold
Gnew

li       wli  hl hl 

 cid 
  
        

where       is   hyperparameter to avoid singularity
caused by small sample size  and the average is taken over
all samples in   minibatch  and   is   learning rate 
In
theory    should be gradually reduced to zero to guarantee
the convergence of this geometry learning  To avoid solving   linear system in each iteration  every   iterations we
recompute and store   
li based on the most updated Gli 
In the next   iterations  this   
li will be used as an approximation of the inverse RFIM  For the input layer which
scales with the number of input features  and the  nal softmax layer  we apply instead the RFIM of the corresponding
linear layer to improve the computational ef ciency 
We compare different optimizers on classifying MNIST
digits  The network has shape   with relu
activation units     nal softmax layer  and uses the persample average crossentropy with   regularization as the
learning cost function  We experiment on two different architectures  one is   plain MLP  PLAIN  the other has  
batch normalization layer after each hidden layer  BNA 
where   rescaling parameter is applied to ensure enough
 exibility of the parametric structure  Ioffe   Szegedy 
  For simplicity  the architecture  minibatch size
  and    regularization strength   are  xed to be
the same for all compared methods  The observations are
consistent when these con gurations vary 

Relative Fisher Information and Natural Gradient

formative samples for each output neuron are centered and
decorrelated 
In the above experiment  RNGD   computational time per
each epoch is roughly       times more than SGD and
ADAM on   modern graphic card  Therefore in terms of
wall clock time RNGD does not show advantages  This
can be improved by more ef cient implementations with
low rank approximation techniques and early stopping  Our
RNGD prototype hints at   promising direction to develop
scalable  ndorder deep learning optimizers based on the
RFIM 

      plain MLP with    regularization

      MLP with batch normalization and   
regularization

Figure   Learning curves of different optimizers on   MLP with
two different architectures  with and without BN  The best learning rate for each method is selected based on the validation accuracy  Using this learning rate  the learning curves wrt   different
random initializations are shown  The mean validation curve is
shown for   clear visualization 

Figure   shows the learning curves of different methods 
SGD is stochastic gradient descent  ADAM is the Adam
optimizer  Kingma   Ba    with            
and       Our RNGD is implemented by modifying
TensorFlow    Abadi  Mart   et al    SGD optimizer 
We set empirically             and      
RNGD presents   sharper learning curve and better generalization  especially when it is combined with BN  In this
case  the  nal tranining error of RNGD is slightly larger
than ADAM because by validation it favors   larger learning rate  which is applied on the neural network weights
 based on RNGD  and BN parameters  based on SGD  For
the ReLU activation      wi     is approximately binary 
 cid 
emphasizing such informative samples with  
        
which are the ones contributing to the learning of wi with
nonzero gradient values  Each output neuron has   different subset of informative samples  RNGD normalizes
  differently wrt different output neurons  so that the in 

 cid    

  

 cid 

 cid 

   

 

  Related Works on FIM Diagonalization
One may ponder whether we can always  nd   suitable parameterization that yields   diagonal FIM that is straightforward to invert  This fundamental problem of parameter
orthogonalization was  rst investigated by Jeffreys  
for decorrelating the parameters of interest from the nuisance parameters  Fisher diagonalization yields parameter
orthogonalization  Cox   Reid    and is proved useful when estimating   using   maximum likelihood estimator  MLE  that is asymptotically normally distributed 
            and ef cient since the variance
of the estimator matches the Cram erRao lower bound  Using the chain rule  this amounts to  nd   suitable parameterization       satisfying

   

    cid    

  
  

 

 

    

  

Thus in general  we end up with  cid  
 cid       that is when       When       the sin 
 cid  

 nonlinear  partial differential equations to satisfy  Huzurbazar 
  Therefore  in general there is no solution when

 cid        

       

 
gle differential equation is usually solvable and tractable 
and the solution may not be unique  For example  Huzurbazar   reports two orthogonalization schemes for the
locationscale families    
    that include the Gaussian family and the Cauchy family  Sometimes  the structure of the differential equation system yields   solution 
For example  Jeffreys   reported   parameter orthogonalization for Pearson   distributions of type   which is of
order       Cox and Reid   further investigated this
topic with application to conditional inference  and provide
examples  including the Weibull distribution 
From the viewpoint of geometry  the FIM induces   Riemannian manifold with metric tensor        
When the FIM may be degenerate  this yields   pseudoRiemannian manifold  Thomas   
In differential
geometry  orthogonalization amounts to transforming the
square length in nitesimal element gijd     of   Riemannian geometry into an orthogonal system   with match 

 accuracy epochs errorPLAIN SGD  train PLAIN SGD  valid PLAIN ADAM  train PLAIN ADAM  valid PLAIN RNGD  train PLAIN RNGD  valid accuracy epochs errorBNA SGD  train BNA SGD  valid BNA ADAM  train BNA ADAM  valid BNA RNGD  train BNA RNGD  valid Relative Fisher Information and Natural Gradient

    However 
ing square length in nitesimal element  iid 
such   global orthogonal metric does not exist  Huzurbazar 
  when       for an arbitrary metric tensor  although
interesting Riemannian parameterization structures may be
derived in Riemannian    geometry  Grant   Vickers 
 
the FIM can be made blockdiagonal easFor NEFs 
ily by using the mixed coordinate system  Amari   
    Hk    where     Ep             is the
moment parameter  for any               where vb  
 cid  of    The geometry of
denotes the subvector  vb    ve 
NEFs is   dually  at structure  Amari    induced by the
convex mgf  the potential function  It de nes   dual af ne
and ej         
coordinate systems ei         
  
 Hi
that are orthogonal   cid ei  ej cid      
      iff      
   where   
and   
      otherwise  Hence the FIM has two diagonal
blocks  Those dual af ne coordinate systems are de ned
up to an af ne invertible transformation             
             
In particular  for any order  NEF
       we can always obtain two mixed parameterizations      or      
The RFIM contributes another line of thought in parameter
diagonalization  We investigate the Fisher information of
hidden variables  or internal interfaces in the learning machine  This is novel since the majority of previous works
concentrate on the FIM of the observables  or the external
interface of the machine  From   causality perspective  we
factor out the main cause  parameters within the subsystem  of the response variable with   direct actionreaction
relationship  and regard the remaining parameters as   reference that can be easily estimated by the empirical distribution  This simpli cation may lead to broader applications of Fisher information in machine learning 
The particular case of   mixed coordinate system  that is
not an af ne coordinate system  induces in information geometry  Amari      dual pair of orthogonal eand morthogonal foliations  Our splits in RFIMs consider general
nonorthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds  that
are the leaves of the foliation  see section   of Amari  
Nagaoka  

  Conclusion and Discussions
We investigate local structures of large learning systems using the new concept of Relative Fisher Information Metric 
The key advantage of this approach is that the local learning
dynamics can be analyzed in an accurate way without approximation  We present   core list of such local structures
in neural networks  and give their corresponding RFIMs 
This list of recipes can be used to provide guiding principles to design new optimizers for deep learning 

Our work applies to mirror descent as well since natural
gradient is related to mirror descent  Raskutti   Mukherjee    as follows  In mirror descent to minimize   cost
function    given   strictly convex distance function
   in the  rst argument  playing the role of the proximity function  we express the gradient descent step as 

 cid 

      arg min
 

 cid 

 cid       

 
 

      

 

When     cid  is chosen as   Bregman divergence
BF    cid             cid         cid cid    cid  wrt
to   convex function     it has been proved that the mirror
descent on the  parameterization is equivalent  Raskutti
  Mukherjee    to the natural gradient optimization
on the induced Riemannian manifold with metric tensor
     parameterized by the dual coordinate system
        
In general  to perform   Riemannian gradient descent for
minimizing   realvalued function     on the manifold 
one needs to choose   proper metric tensor given in matrix form    Thomas   constructed   toy example showing that the natural gradient may diverge while
the ordinary gradient  for        converges  Recently 
Thomas et al    proposed   new kind of descent
method based on what they called the Energetic Natural
Gradient that generalizes the natural gradient  The energy distance DE           dp         
dp      cid    dp       cid  where      cid      
and       cid       where dp  is   distance metric over the support  Using   Taylor   expansion on their
energy distance  they get the Energy Information Matrix
 in   way similar to recovering the FIM from   Taylor  
expansion of any fdivergence like the KullbackLeibler
divergence  Their idea is to incorporate prior knowledge
on the structure of the support  observation space  to de ne
energy distance  Twisting the geometry of the support  say 
Wasserstein   optimal transport  with the geometry of the
parametric distributions  FisherRao geodesic distances  is
indeed important  Chizat et al    In information geometry  invariance on the support is provided by   Markov
morphism that is   probabilistic mapping of the support to
itself    Cencov    There is no neighbourhood structure
on the support in IG  Markov morphism includes deterministic transformation of   random variable by   statistic  It is
wellknown that IT    cid  IX   with equality iff     
      is   suf cient statistic of    Thus to get the same invariance for the energy distance  Thomas et al    one
shall further require dp                dp       
We believe that RFIMs will provide   sound methodology
to build further ef cient systems for deep learning  The
full source codes to reproduce the experimental results are
available at https www lix polytechnique 
fr nielsen RFIM 

Relative Fisher Information and Natural Gradient

Acknowledgements
The authors would like to thank the anonymous reviewers
and Yann Ollivier for the helpful comments  This work was
mainly conducted when the  rst author was   postdoctoral
researcher at  Ecole Polytechnique 

References
Abadi  Mart   et al  TensorFlow  Largescale machine
Software

learning on heterogeneous systems   
available from tensorflow org 

Amari  Shun ichi  Information geometry of the EM and em
algorithms for neural networks  Neural Networks   
   

Amari  Shun ichi  Neural learning in structured parameter
In NIPS   pp 

spaces   natural Riemannian gradient 
  MIT Press   

Amari  Shun ichi  Natural gradient works ef ciently in

learning  Neural Comput     

Amari  Shun ichi 

Information Geometry and its Applications  volume   of Applied Mathematical Sciences 
Springer Japan   

Amari  Shun ichi and Nagaoka  Hiroshi  Methods of Information Geometry  volume   of Translations of Mathematical Monographs  AMS and OUP     Published
in Japanese in  

Bengio  Yoshua 

Estimating or propagating gradients
through stochastic neurons  CoRR  abs   

Bonnabel  Silv ere  Stochastic gradient descent on RiemanIEEE Trans  Automat  Contr   

nian manifolds 
   

 Cencov  Nikola  Nikolaevich  Statistical decision rules and
optimal inference  volume   of Translations of Mathematical Monographs  American Mathematical Society 
  Translation from the Russian  published in  
edited by Lev    Leifman 

Chizat  Lenaic  Schmitzer  Bernhard  Peyr    Gabriel  and
Vialard  Franc oisXavier  An Interpolating Distance between Optimal Transport and FisherRao  arXiv eprints 
     math AP 

Clevert  DjorkArn    Unterthiner  Thomas  and Hochreiter 
Sepp  Fast and accurate deep network learning by exponential linear units  ELUs  CoRR  abs 
 

Cobb  Loren  Koppstein  Peter  and Chen  Neng Hsin  Estimation and moment recursion relations for multimodal
distributions of the exponential family  JASA   
   

Cox        and Reid     Parameter orthogonality and approximate conditional inference  Journal of the Royal
Statistical Society  Series    Methodological   
   

Cram er  Harald  Mathematical Methods of Statistics  volume   of Princeton Mathematical Series  Princeton University Press   

Desjardins  Guillaume  Simonyan  Karen  Pascanu  Razvan  and Kavukcuoglu  Koray  Natural neural networks 
In NIPS   pp    Curran Associates  Inc 
 

Dowty  James    Chentsov   theorem for exponential fam 

ilies  arXiv preprints       math ST 

Efron  Bradley and Hinkley  David    Assessing the accuracy of the maximum likelihood estimator  Observed
versus expected Fisher information  Biometrika   
   

Fr echet  Maurice  Sur   extension de certaines evaluations statistiques au cas de petits echantillons  Revue de
  Institut International de Statistique   Review of the International Statistical Institute     

Grant  James DE and Vickers  JA  Block diagonalization of
fourdimensional metrics  Classical and Quantum Gravity     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Delving deep into recti ers  Surpassing humanlevel performance on ImageNet classi cation  In ICCV 
 

Hinton  Geoffrey    Osindero  Simon  and Teh  YeeWhye    fast learning algorithm for deep belief nets 
Neural Comput     

Hotelling  Harold  Spaces of statistical parameters  American Mathematical Society Meeting     unpublished 
Presented orally by    Ore during the meeting 

Huzurbazar  Vasant Shankar  Probability distributions and
orthogonal parameters  Mathematical Proceedings of
the Cambridge Philosophical Society   
 

Ioffe  Sergey and Szegedy  Christian  Batch normalization 
Accelerating deep network training by reducing internal
In ICML  JMLR    CP   pp   
covariate shift 
   

Jeffreys  Harold  Theory of Probability  Oxford Classic
Texts in the Physical Sciences  OUP   rd edition   
First published in  

Relative Fisher Information and Natural Gradient

Pascanu  Razvan and Bengio  Yoshua  Revisiting natIn ICLR   

ural gradient for deep networks 
arXiv   cs LG 

Raiko  Tapani  Valpola  Harri  and LeCun  Yann  Deep
learning made easier by linear transformations in perceptrons  In AISTATS  JMLR   CP   pp   
 

Rao  Calyampudi Radhakrishna 

Information and accuracy attainable in the estimation of statistical parameters 
Bull  Cal  Math  Soc     

Raskutti  Garvesh and Mukherjee  Sayan  The information
In Geometric Science of
geometry of mirror descent 
Information  GSI  volume   of LNCS  pp   
Springer   

Sun  Ke and MarchandMaillet  St ephane  An information geometry of statistical manifold learning  In ICML 
JMLR   CP   pp     

Szegedy  Christian et al  Going deeper with convolutions 

In CVPR   

Thomas  Philip  GeNGA    generalization of natural gradient ascent with positive and negative convergence results  In ICML  JMLR   CP   pp     

Thomas  Philip  da Silva        Dann     and Brunskill 

   Energetic natural gradient descent  In ICML   

Wager  Stefan  Wang  Sida  and Liang  Percy    Dropout
training as adaptive regularization  In NIPS   pp   
  Curran Associates  Inc   

Watanabe  Sumio  Algebraic Geometry and Statistical
Learning Theory  volume   of Cambridge Monographs
on Applied and Computational Mathematics  CUP 
 

Williams  Virginia Vassilevska  Multiplying matrices faster
In Annual ACM Sympothan CoppersmithWinograd 
sium on Theory of Computing  STOC  pp   
 

Zegers  Pablo  Fisher information properties  Entropy   

   

Jost    urgen  Riemannian Geometry and Geometric Analy 

sis  Springer   th edition   

Kingma  Diederik    and Ba  Jimmy  Adam    method for

stochastic optimization  CoRR  abs   

Kingma  Diederik   and Welling  Max  Autoencoding
arXiv 

In ICLR   

variational Bayes 
 stat ML 

Kurita  Takio  Iterative weighted least squares algorithms
for neural networks classi ers  New Generation Computing     

Le Roux  Nicolas  Manzagol  PierreAntoine  and Bengio  Yoshua  Topmoumoute online natural gradient algorithm  In NIPS   pp    Curran Associates 
Inc   

Lebanon  Guy  Riemannian geometry and statistical ma 

chine learning  PhD thesis  CMU   

Looks  Moshe  Herreshoff  Marcello  Hutchins  DeLesley 
and Norvig  Peter  Deep learning with dynamic computation graphs 
arXiv 
 cs NE 

In ICLR   

MarceauCaron  Ga etan and Ollivier  Yann  Practical Riemannian neural networks  CoRR  abs   

Martens  James  Deep learning via Hessianfree optimiza 

tion  In ICML  pp     

Martens  James  New perspectives on the natural gradient

method  CoRR  abs   

Martens  James and Grosse  Roger  Optimizing neural networks with Kroneckerfactored approximate curvature 
In ICML  JMLR    CP   pp     

Montanari  Andrea  Computational implications of reducing data to suf cient statistics  Electron     Statist   
   

Montavon  Gr egoire and   uller  KlausRobert  Deep
Boltzmann machines and the centering trick  In Neural
Networks  Tricks of the Trade  pp    Springer
Berlin Heidelberg   nd edition   

Nair  Vinod and Hinton  Geoffrey    Recti ed linear units
improve restricted Boltzmann machines  In ICML  pp 
   

Nielsen  Frank  Cram erRao lower bound and information

geometry  CoRR  abs   

Ollivier  Yann  Riemannian metrics for neural networks 

CoRR  abs   

