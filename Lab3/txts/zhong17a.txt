Recovery Guarantees for Onehidden layer Neural Networks 

Kai Zhong   Zhao Song   Prateek Jain   Peter    Bartlett   Inderjit    Dhillon  

Abstract

In this paper  we consider regression problems
with onehidden layer neural networks  NNs 
We distill some properties of activation functions that lead to local strong convexity in the
neighborhood of the groundtruth parameters for
the  NN squaredloss objective and most popular nonlinear activation functions satisfy the distilled properties  including recti ed linear units
 ReLUs  leaky ReLUs  squared ReLUs and sigmoids  For activation functions that are also
smooth  we show local linear convergence guarantees of gradient descent under   resampling
rule  For homogeneous activations  we show tensor methods are able to initialize the parameters
to fall into the local strong convexity region  As
  result  tensor initialization followed by gradient
descent is guaranteed to recover the ground truth
with sample complexity     log    poly     
and computational complexity         poly     
for smooth homogeneous activations with high
probability  where   is the dimension of the input            is the number of hidden nodes 
  is   conditioning property of the groundtruth
parameter matrix between the input layer and the
hidden layer    is the targeted precision and   is
the number of samples  To the best of our knowledge  this is the  rst work that provides recovery guarantees for  NNs with both sample complexity and computational complexity linear in
the input dimension and logarithmic in the precision 

 The University of Texas at Austin  zhongkai ices utexas edu

 The University of Texas at Austin  zhaos utexas edu
 Microsoft Research  India  prajain microsoft com
 University of California  Berkeley  bartlett cs berkeley edu
 The University of Texas at Austin  inderjit cs utexas edu
 Full
pdf 
 zhongkai ices utexas edu 

at https arxiv org 
Kai Zhong

available

Correspondence

to 

version

is

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Introduction
Neural Networks  NNs  have achieved great practical success recently  Many theoretical contributions have been
made very recently to understand the extraordinary performance of NNs  The remarkable results of NNs on complex tasks in computer vision and natural language processing inspired works on the expressive power of NNs  Cohen et al    Cohen   Shashua    Raghu et al 
  Daniely et al    Poole et al    Montufar
et al    Telgarsky    Indeed  several works found
NNs are very powerful and the deeper the more powerful 
However  due to the high nonconvexity of NNs  knowing
the expressivity of NNs doesn   guarantee that the targeted
functions will be learned  Therefore  several other works
focused on the achievability of global optima  Many of
them considered the overparameterized setting  where the
global optima or local minima close to the global optima
will be achieved when the number of parameters is large
enough  including  Freeman   Bruna    Haeffele  
Vidal    Livni et al    Dauphin et al    Safran
  Shamir    Hardt   Ma    This  however  leads
to over tting easily and can   provide any generalization
guarantees  which are actually the essential goal in most
tasks 
  few works have considered generalization performance 
For example 
 Xie et al    provide generalization bound under the Rademacher generalization analysis
framework  Recently  Zhang et al      describe some
experiments showing that NNs are complex enough that
they actually memorize the training data but still generalize well  As they claim  this cannot be explained by applying generalization analysis techniques  like VC dimension and Rademacher complexity  to classi cation loss  although it does not rule out   margins analysis see  for example   Bartlett    their experiments involve the unbounded crossentropy loss 
In this paper  we don   develop   new generalization analysis  Instead we focus on parameter recovery setting  where
we assume there are underlying groundtruth parameters
and we provide recovery guarantees for the groundtruth
parameters up to equivalent permutations  Since the parameters are exactly recovered  the generalization performance
will also be guaranteed 

Recovery Guarantees for Onehidden layer Neural Network

Several other techniques are also provided to recover the
parameters or to guarantee generalization performance 
such as tensor methods  Janzamin et al    and kernel
methods  Arora et al    These methods require sam 

ple complexity      or computational complexity  cid     

which can be intractable in practice 
Recently  Shamir    show that neither speci   assumptions on the niceness of the input distribution or niceness of
the target function alone is suf cient to guarantee learnability using gradientbased methods  In this paper  we assume
data points are sampled from Gaussian distribution and the
parameters of hidden neurons are linearly independent 
Our main contributions are as follows 
  We distill some properties for activation functions 
which are satis ed by   wide range of activations  including
ReLU  squared ReLU  sigmoid and tanh  With these properties we show positive de niteness  PD  of the Hessian
in the neighborhood of the groundtruth parameters given
enough samples  Theorem   Further  for activations that
are also smooth  we show local linear convergence is guaranteed using gradient descent 
  We propose   tensor method to initialize the parameters
such that the initialized parameters fall into the local positive de niteness area  Our contribution is that we reduce
the sample computational complexity from cubic dependency on dimension to linear dependency  Theorem  
  Combining the above two results  we provide   globally
converging algorithm  Algorithm   for smooth homogeneous activations satisfying the distilled properties  The
whole procedure requires sample computational complexity linear in dimension and logarithmic in precision  Theorem  

  Related Work
The recent empirical success of NNs has boosted their theoretical analyses  Feng et al    Balduzzi    Balduzzi et al    Sagun et al    Andoni et al   
Arora et al    Goel et al   
In this paper  we
classify them into three main directions 

  Expressive Power

Expressive power is studied to understand the remarkable
performance of neural networks on complex tasks  Although onehidden layer neural networks with suf ciently
many hidden nodes can approximate any continuous function  Hornik    shallow networks can   achieve the
same performance in practice as deep networks  Theoretically  several recent works show the depth of NNs plays an
essential role in the expressive power of neural networks
 Daniely et al    As shown in  Cohen et al    Co 

hen   Shashua    Telgarsky    functions that can
be implemented by   deep network of polynomial size require exponential size in order to be implemented by   shallow network   Raghu et al    Poole et al    Montufar et al    Arora et al    design some measures
of expressivity that display an exponential dependence on
the depth of the network  However  the increasing of the
expressivity of NNs or its depth also increases the dif culty
of the learning process to achieve   good enough model  In
this paper  we focus on  NNs and provide recovery guarantees using    nite number of samples 

  Achievability of Global Optima

The global convergence is in general not guaranteed for
NNs due to their nonconvexity  It is widely believed that
training deep models using gradientbased methods works
so well because the error surface either has no local minima  or if they exist they need to be close in value to the
global minima   Swirszcz et al    present examples
showing that for this to be true additional assumptions on
the data  initialization schemes and or the model classes
have to be made  Indeed the achievability of global optima
has been shown under many different types of assumptions 
In particular   Choromanska et al    analyze the loss
surface of   special random neural network through spinglass theory and show that it has exponentially many local
optima  whose loss is small and close to that of   global
optimum  Later on   Kawaguchi    eliminate some assumptions made by  Choromanska et al    but still
require the independence of activations as  Choromanska
et al    which is unrealistic   Safran   Shamir   
study the geometric structure of the neural network objective function  They have shown that with high probability random initialization will fall into   basin with   small
objective value when the network is overparameterized 
 Livni et al    consider polynomial networks where
the activations are square functions  which are typically not
used in practice   Haeffele   Vidal    show that when
  local minimum has zero parameters related to   hidden
node    global optimum is achieved   Freeman   Bruna 
  study the landscape of  NN in terms of topology and
geometry  and show that the level set becomes connected
as the network is increasingly overparameterized   Hardt
  Ma    show that products of matrices don   have
spurious local minima and that deep residual networks can
represent any function on   sample  as long as the number of parameters is larger than the sample size   Soudry
  Carmon    consider overspeci ed NNs  where the
number of samples is smaller than the number of weights 
 Dauphin et al    propose   new approach to secondorder optimization that identi es and attacks the saddle
point problem in highdimensional nonconvex optimization  They apply the approach to recurrent neural networks

Recovery Guarantees for Onehidden layer Neural Network

and show practical performance   Arora et al    use
results from tropical geometry to show global optimality of
an algorithm  but it requires      poly    computational
complexity 
Almost all of these results require the number of parameters is larger than the number of points  which probably
over ts the model and no generalization performance will
be guaranteed  In this paper  we propose an ef cient and
provable algorithm for  NNs that can achieve the underlying groundtruth parameters 

  Generalization Bound   Recovery Guarantees

The achievability of global optima of the objective from
the training data doesn   guarantee the learned model to be
able to generalize well on unseen testing data  In the literature  we  nd three main approaches to generalization
guarantees 
  Use generalization analysis frameworks  including VC
dimension Rademacher complexity  to bound the generalization error    few works have studied the generalization
performance for NNs 
 Xie et al    follow  Soudry
  Carmon    but additionally provide generalization
bounds using Rademacher complexity  They assume the
obtained parameters are in   regularization set so that the
generalization performance is guaranteed  but this assumption can   be justi ed theoretically   Hardt et al    apply stability analysis to the generalization analysis of SGD
for convex and nonconvex problems  arguing early stopping is important for generalization performance 
  Assume an underlying model and try to recover this
model  This direction is popular for many nonconvex
problems including matrix completion sensing  Jain et al 
  Hardt    Sun   Luo    Balcan et al   
mixed linear regression  Zhong et al    subspace recovery  Elhamifar   Vidal    and other latent models
 Anandkumar et al   
Without making any assumptions  those nonconvex problems are intractable  Arora et al      Gillis   Vavasis 
  Song et al      Gillis   Glineur    Razenshteyn et al    Sontag   Roy    Hardt   Moitra 
  Arora et al      Yi et al    Recovery guarantees for NNs also need assumptions  Several different approaches under different assumptions are provided to have
recovery guarantees on different NN settings 
Tensor methods  Anandkumar et al    Wang et al 
  Wang   Anandkumar    Song et al    are
  general tool for recovering models with latent factors
by assuming the data distribution is known  Some existing recovery guarantees for NNs are provided by tensor
methods  Sedghi   Anandkumar    Janzamin et al 
  However   Sedghi   Anandkumar    only pro 

vide guarantees to recover the subspace spanned by the
weight matrix and no sample complexity is given  while
 Janzamin et al    require      sample complexity  In this paper  we use tensor methods as an initialization
step so that we don   need very accurate estimation of the
moments  which enables us to reduce the total sample complexity from   to log 
 Arora et al    provide polynomial sample complexity and computational complexity bounds for learning deep
representations in unsupervised setting  and they need to
assume the weights are sparse and randomly distributed in
   
 Tian    analyze  NN by assuming Gaussian inputs in
  supervised setting  in particular  regression and classi cation with   teacher  This paper also considers this setting 
However  there are some key differences      Tian   
require the secondlayer parameters are all ones  while we
can learn these parameters     In  Tian    the groundtruth  rstlayer weight vectors are required to be orthogonal  while we only require linear independence      Tian 
  require   good initialization but doesn   provide initialization methods  while we show the parameters can be
ef ciently initialized by tensor methods     In  Tian   
only the population case  in nite sample size  is considered  so there is no sample complexity analysis  while we
show  nite sample complexity 
Recovery guarantees for convolution neural network with
Gaussian inputs are provided in  Brutzkus   Globerson 
  where they show   globally converging guarantee of
gradient descent on   onehidden layer nooverlap convolution neural network  However  they consider population
case  so no sample complexity is provided  Also their analysis depends on ReLU activations and the nooverlap case
is very unlikely to be used in practice  In this paper  we
consider   large range of activation functions  but for onehidden layer fullyconnected NNs 
  Improper Learning  In the improper learning setting for
NNs  the learning algorithm is not restricted to output  
NN  but only should output   prediction function whose error is not much larger than the error of the best NN among
all the NNs considered 
 Zhang et al        propose
kernel methods to learn the prediction function which is
guaranteed to have generalization performance close to that
of the NN  However  the sample complexity and computational complexity are exponential 
 Aslan et al   
transform NNs to convex semide nite programming  The
works by  Bach    and  Bengio et al    are also
in this direction  However  these methods are actually
not learning the original NNs  Another work by  Zhang
et al      uses random initializations to achieve arbitrary small excess risk  However  their algorithm has exponential running time in  

Recovery Guarantees for Onehidden layer Neural Network

Roadmap  The paper is organized as follows 
In Section   we present our problem setting and show three key
properties of activations required for our guarantees 
In
Section   we introduce the formal theorem of local strong
convexity and show local linear convergence for smooth
activations  Section   presents   tensor method to initialize
the parameters so that they fall into the basin of the local
strong convexity region 

  Notation

For any positive integer    we use     to denote the set
         For random variable    let      denote the
expectation of    if this quantity exists  For any vector
    Rn  we use  cid   cid  to denote its  cid  norm  We provide
several de nitions related to matrix    Let det    denote
the determinant of   square matrix    Let   cid  denote the
transpose of    Let    denote the MoorePenrose pseudoinverse of    Let    denote the inverse of   full rank
square matrix  Let  cid   cid   denote the Frobenius norm of matrix    Let  cid   cid  denote the spectral norm of matrix    Let
      to denote the ith largest singular value of    For any
to    notation  for two functions       we use the shorthand    cid     resp   cid  to indicate that     Cg  resp   
for an absolute constant    We use   to denote outer product and   to denote dot product  Given two column vectors
       Rn  then         Rn   and              ui   vj 
   uivi      Given three column vectors           Rn  then             Rn     and
                   ui   vj   wk  We use       Rnr to
denote the vector     outer product with itself       times 

function    we de ne  cid       to be    logO     In addition

and   cid      cid  

  Problem Formulation
We consider the following regression problem  Given   set
of   samples

                   xn  yn    Rd     

let   denote   underlying distribution over Rd     with
parameters
    

     Rd  and    

      

    

   

    

      

such that each sample            is sampled        from this
distribution  with

  cid 

   

              

       cid 
  

    

 

  

where     is the activation function    is the number of
nodes in the hidden layer  The main question we want to
answer is  How many samples are suf cient to recover the
underlying parameters 

It is wellknown that  training one hidden layer neural network is NPcomplete  Blum   Rivest    Thus  without making any assumptions  learning deep neural network
is intractable  Throughout the paper  we assume   follows
  standard normal distribution  the data is noiseless  the
dimension of input data is at least the number of hidden
nodes  and activation function     satis es some reasonable properties 
Actually our results can be easily extended to multivariate
Gaussian distribution with positive de nite covariance and
zero mean since we can estimate the covariance  rst and
then transform the input to   standard normal distribution
but with some loss of accuracy  Although this paper focuses on the regression problem  we can transform classi 
 cation problems to regression problems if   good teacher
is provided as described in  Tian    Our analysis requires   to be no greater than    since the  rstlayer parameters will be linearly dependent otherwise 
For activation function     we assume it is continuous
and if it is nonsmooth let its  rst derivative be left derivative  Furthermore  we assume it satis es Property    
and   These properties are critical for the later analyses 
We also observe that most activation functions actually satisfy these three properties 
Property   The  rst derivative  cid    is nonnegative and
homogeneously bounded            cid            for
some constants        and      
Property   Let       Ez    cid      zq    
      and       Ez    cid      zq    
    Let   denote min     
   
       
 
 
The  rst derivative  cid    satis es that  for all       we
have      
Property   The second derivative  cid cid    is either    
globally bounded  cid cid         for some constant        
    is   smooth  or      cid cid        except for      is  
 nite constant  points 
Remark   The  rst two properties are related to the
 rst derivative  cid    and the last one is about the second
derivative  cid cid    At high level  Property   requires  
to be nondecreasing with homogeneously bounded derivative  Property   requires   to be highly nonlinear  Property   requires   to be either smooth or piecewise linear 
Theorem   ReLU       max     
leaky
ReLU       max        squared ReLU      
max      and any nonlinear nondecreasing smooth
functions with bounded symmetric  cid    like the sigmoid
function             the tanh function and the erf
dt  satisfy Property  
The linear function           doesn   satisfy Property  
and the quadratic function           doesn   satisfy
Property   and  

function        cid   

           

     

      

Recovery Guarantees for Onehidden layer Neural Network

The proof can be found in the full version  Zhong et al 
 

  Positive De niteness of Hessian
In this section  we study the Hessian of empirical risk near
the ground truth  We consider the case when    is already
known  Note that for homogeneous activations  we can asi       since                pz  where
sume   
  is the degree of homogeneity  As   
  only takes discrete
values for homogeneous activations  in the next section  we
show we can exactly recover    using tensor methods with
 nite samples 
For   set of samples    we de ne the Empirical Risk 

 cid    cid 
 cid 
 cid    cid 

 cid 
 cid   
Let   calculate the gradient and the Hessian of  cid fS     and

For   distribution    we de ne the Expected Risk 

fD     For each         the partial gradient of fD    
with respect to wj can be represented as

 cid fS      

        

        

  
     cid 

  
     cid 

fD      

 
   

      

      

 

 

 
 

  

  

 

 

 cid 

 fD    

 wj

   

      

  
     cid 

        

  
   cid   cid 

     

For each            and    cid     the second partial derivative
of fD     for the       th offdiagonal block is 

 fD    
 wj wl

   

      

    

   cid   cid 

    cid   cid 

and for each         the second partial derivative of
fD     for the jth diagonal block is

    xx cid cid   

 fD    

   
 

  cid 
  
     cid 
    xx cid 
   cid   cid 

  

   
       
     

 cid cid    cid 

  

 cid   

           

   cid cid   cid 

    xx cid 

        by the average over the samples    cid 

If     is nonsmooth  we use the Dirac function and its
derivatives to represent  cid cid    Replacing the expectation
       

we obtain the Hessian of the empirical risk 
Considering the case when           Rd    for all       
    we have 
 fD    
 wj wl

    xx cid cid   

    cid   cid 

   cid   cid 

 cid   

   

      

    

If Property     is satis ed   cid cid        almost surely  So
in this case the diagonal blocks of the empirical Hessian
can be written as 

 cid fS    

   
 

 

 
   

 cid 

      

   
   cid   cid 

    xx cid 

       

 cid  

   Let vmax denote maxi       

Now we show the Hessian of the objective near the global
optimum is positive de nite 
De nition   Given the ground truth matrix      
Rd   
let        denote the ith singular value of
    often abbreviated as     Let            
    and vmin
denote mini       
      Let     vmax vmin  Let   denote
    Let         min   
Theorem   For any     Rd   with  cid        cid   
       cid    cid  let   denote   set of
poly          
       samples from distribution    de ned in   and let
the activation function satisfy Property   Then
for any       if           poly log                  
   
we have with probability at least         

min      cid   cid fS      cid    kv 

max  

   

     

 cid 

 

Remark   As we can see from Theorem       from
Property   plays an important role for positive de nite
 PD  property  Interestingly  many popular activations  like
ReLU  sigmoid and tanh  have         while some simple functions like linear          and square         
functions have         and their Hessians are rankde cient  Another important numbers are   and   two
different condition numbers of the weight matrix  which diIf     is rank
rectly in uences the positive de niteness 
de cient              and we don   have PD property 
In the best case when     is orthogonal            In the
worse case    can be exponential in    Also   should be
close enough to     In the next section  we provide tensor
methods to initialize   
  such that they satisfy the
conditions in Theorem  

  and   

For the PD property to hold  we need the samples to be independent of the current parameters  Therefore  we need
to do resampling at each iteration to guarantee the convergence in iterative algorithms like gradient descent  The following theorem provides the linear convergence guarantee
of gradient descent for smooth activations 
Theorem    Linear convergence of gradient descent 
Let   be the current iterate satisfying  cid        cid   
   cid    cid  Let   denote   set of
poly          
samples from distribution    de ned in   with
      
          poly log                  
    and let the activation function satisfy Property   and     De ne
max  
   
        

min    and       kv 

If we perform gradient descent with step size     on

 cid fS     and obtain the next iterate 
 cid          

 cid fS    

  

then with probability at least         

 cid cid        cid 

          
  

 cid        cid 
   

Due to the space limitation  we provide the proofs in the
full version 

  Tensor Methods for Initialization
In this section  we show that Tensor methods can recover
the parameters     to some precision and exactly recover
   for homogeneous activations 
It is known that most tensor problems are NPhard    astad 
  Hillar   Lim    or even hard to approximate
 Song et al      However  by making some assumptions  tensor decomposition method becomes ef cient
 Anandkumar et al    Wang et al    Wang  
Anandkumar    Song et al    Here we utilize
the noiseless assumption and Gaussian inputs assumption
to show   provable and ef cient tensor methods 

  Preliminary

Let   de ne   special outer product  cid  for simpli cation of
matrix  then   cid      cid  
the notation  If     Rd is   vector and   is the identity
      ej   ej   ej       ej  
as    cid  
ej   ej      If   is   symmetric rankr matrix factorized
   siviv cid 
  cid 
  cid    

  and   is the identity matrix  then

  cid 

 cid 

Al     

si

  

  

  

where         vi  vi  ej   ej          vi  ej   vi  ej 
        ej   vi   vi   ej          vi   ej   ej   vi 
        ej   vi   ej   vi and         ej   ej   vi   vi 
Denote       cid   cid  Now let   calculate some moments 
De nition
  We
and
                   as follows  
                   
                            

                         cid   
                              cid       cid   

de ne            

      Ez         zj                
       cid   
       cid   
       cid   
       cid   

   cid 
   cid 
   cid     cid   
   cid     cid   
   cid 
   cid     cid   
   cid     cid   

   cid 

Recovery Guarantees for Onehidden layer Neural Network

According to De nition   we have the following results 

Claim   For each       Mj  cid  

     

  mj iw

  
 

 

Note that some mj     will be zero for speci   activations 
For example  for activations with symmetric  rst derivatives        cid       cid    like sigmoid and erf  we
have           being   constant and        since
      Another example is ReLU  ReLU functions have vanishing                as      
To make tensor methods work  we make the following assumption 
Assumption   Assume the activation function    
satis es the following conditions 
  If Mj  cid    then mj    cid    for all        
  At least one of    and    is nonzero 
  If             then     is an even function      
         
  If             then     is an odd function      
         

If     is an odd function then           and
    cid           cid    Hence we can always assume
If     is an even function  then     cid     
     
    cid    So if   recovers    then    also recovers
   Note that ReLU  leaky ReLU and squared ReLU satisfy
Assumption   We further de ne the following nonzero
moments 
De nition   Let     Rd denote   randomly picked
vector  We de ne    and    as follows      
Mj                 where      min      Mj  cid   
and      Mj                where      min    
 Mj  cid   
According to De nition   and   we have 

Claim         cid  
    cid  

  mj   cid   

     

     

  mj   cid   
       

 

 

       

 

and

  

is equal

In other words for the above de nition     is equal
to the  rst nonzero matrix in the ordered sequence
                         
to
the  rst nonzero tensor
in the ordered sequence
                 Since   is randomly picked up 
     cid    and we view this number as   constant throughw cid 
both    and    are rankk  Also  let  cid      Rd   and
out this paper  So by construction and Assumption  
 cid      Rd     denote the corresponding empirical mo 

ments of      Rd   and      Rd     respectively 

  Algorithm

Now we brie   introduce how to use   set of samples
with size linear in dimension to recover the ground truth
parameters to some precision  As shown in the previous section  we have   rankk  rdorder moment    that

Recovery Guarantees for Onehidden layer Neural Network

 cid  Theorem  

           PARTITION     

 cid      ES     
    POWERMETHOD cid      
 cid      ES            
 cid ui        KCL cid   

Algorithm   Initialization via Tensor Method
  procedure INITIALIZATION   
 
 
 
 
 
 
 
 
  end procedure

   
Return    

       
    

    

 

          RECMAGSIGN   cid ui        

   

max  
   

    log    poly          
     kv 
        Sq   PARTITION         
         INITIALIZATION   
Set   
for                   do

Algorithm   Globally Converging Algorithm
  procedure LEARNING NN             cid  Theorem  
 
 
 
 
 
 
 
 
 
  end procedure

      
                 cid fSq      

in Eq    for all  cid fSq              

end for
Return       

       
    

 

 

algorithm  Algorithm   that takes         cid      time and

if               poly         log    then there exists an
outputs   matrix       Rd   and   vector      Rk
such that  with probability at least         
 cid          cid         poly     cid    cid     and   

      
   

  Global Convergence
Combining the positive de niteness of the Hessian near
the global optimal in Section   and the tensor initialization
methods in Section   we come up with the overall globally
converging algorithm Algorithm   and its guarantee Theorem  
Theorem    Global convergence guarantees  Let   denote   set of        samples from distribution    de ned
in   and let the activation function be homogeneous satisfying Property         and Assumption   Then
for any       and any       if         log   
poly log                log    poly          
   
and          kv 
    then there is an Algorithm  procedure LEARNING NN in Algorithm   taking
          poly log         time and outputting   matrix
         Rd   and   vector      Rk satisfying

max  

 cid             cid      cid    cid     and   

      
   

with probability at least         

This follows by combining Theorem   and Theorem  

  Numerical Experiments
In this section we use synthetic data to verify our theoretical results  We generate data points  xi  yi       from
Distribution   de ned in Eq    We set             cid 
where     Rd   and     Rk   are orthogonal matrices generated from QR decomposition of Gaussian matrices    is   diagonal matrix whose diagonal elements

      
has tensor decomposition formed by    
  
Therefore  we can use the nonorthogonal decomposition
sponding estimated tensor  cid    and obtain an approximation
method  Kuleshov et al    to decompose the corre 

    

    

      

of the parameters  The precision of the obtained parameters depends on the estimation error of    which requires
    samples to achieve   error  Also  the time complexity for tensor decomposition on             tensor is
   
In this paper  we reduce the cubic dependency of sample computational complexity in dimension  Janzamin
et al    to linear dependency  Our idea follows the
techniques used in  Zhong et al    where they  rst
used    ndorder moment    to approximate the subspace spanned by    
   denoted as     then
use   to reduce   higherdimensional thirdorder tensor
     Rd     to   lowerdimensional tensor   
 
              Rk      Since the tensor decomposition and the tensor estimation are conducted on   lowerdimensional Rk     space  the sample complexity and
computational complexity are reduced 
The detailed algorithm is shown in Algorithm   First  we
randomly partition the dataset into three subsets each with

size  cid      Then apply the power method on  cid    which is
et al    on  cid    outputs  cid ui which estimates siV  cid   
can be estimated by siV cid ui  Finally we estimate the magni 

the estimation of    from    to estimate     After that 
the nonorthogonal tensor decomposition  KCL Kuleshov
for         with unknown sign si       Hence   
tude of   
  in the RECMAGSIGN function for homogeneous activations  We discuss the details
of each procedure and provide POWERMETHOD and RECMAGSIGN algorithms in the full version 

  and the signs si    

 

 

  Theoretical Analysis

We formally present our theorem for Algorithm   and provide the proof in the full version 
Theorem   Let the activation function be homogeneous
satisfying Assumption   For any           and      

Recovery Guarantees for Onehidden layer Neural Network

    Sample complexity for recovery

    Tensor initialization error

    Objective      iterations

Figure  Numerical Experiments

 

        

          

are        
In this experiment 
we set       and       We set   
to be randomly
picked from     with equal chance  We use squared
ReLU       max      which is   smooth homogeneous function  For nonorthogonal tensor methods  we directly use the code provided by  Kuleshov et al    with
the number of random projections  xed as       We
pick the stepsize       for gradient descent  In the experiments  we don   do the resampling since the algorithm
still works well without resampling 
First we show the number of samples required to recover the parameters for different dimensions  We   
      change   for             and   for
            For each pair of   and   
we run   trials  We say   trial successfully recovers the
parameters if there exists   permutation               such
that the returned parameters   and   satisfy

 cid   

        cid cid   

  cid      and          
   

max
    

We record the recovery rates and represent them as grey
scale in Fig      As we can see from Fig      the least
number of samples required to have   recovery rate is
about proportional to the dimension 
Next we test the tensor initialization  We show the error between the output of the tensor method and the ground truth
parameters against the number of samples under different
dimensions in Fig     The pure dark blocks indicate  in
    which
means   
is not correctly initialized  Let     denote the
set of all possible permutations               The grey
scale represents the averaged error 
      

at least one of the   trials cid  

 cid cid  

   cid cid   

     

     

  cid 

min
   

max
    

 cid   

 

 

over   trials  As we can see  with    xed dimension  the
more samples we have the better initialization we obtain 
We can also see that to achieve the same initialization error 
the sample complexity required is about proportional to the
dimension 

We also compare different initialization methods for gradient descent in Fig      We                     
and compare three different initialization approaches     
Let both   and   be initialized from tensor methods  and
then do gradient descent for   while   is  xed   II  Let
both   and   be initialized from random Gaussian  and
then do gradient descent for both   and     III  Let       
and   be initialized from random Gaussian  and then do
gradient descent for   while   is  xed  As we can see
from Fig     Approach     is the fastest and Approach  II 
doesn   converge even if more iterations are allowed  Both
Approach     and  III  have linear convergence rate when
the objective value is small enough  which veri es our local linear convergence claim 

  Conclusion
As shown in Theorem   the tensor initialization followed
by gradient descent will provide   globally converging algorithm with linear time sample complexity in dimension 
logarithmic in precision and polynomial in other factors for
smooth homogeneous activation functions  Our distilled
properties for activation functions include   wide range of
nonlinear functions and hopefully provide an intuition to
understand the role of nonlinear activations played in optimization  Deeper neural networks and convergence for
SGD will be considered in the future 
Acknowledgments
      Bartlett would like to gratefully acknowledge the support of Australian Research Council through an Australian
Laureate Fellowship  FL  and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers  ACEMS  and the support
of the NSF through grant IIS        Dhillon would
like to gratefully acknowledge the support of NSF grants
CCF  IIS  and CCF  Part of the
work was done while    Zhong was interning in Microsoft
Research  India  The authors would like to thank Surbhi
Goel  Adam Klivans  Qi Lei  Eric Price  David    Woodruff 
Peilin Zhong  Hongyang Zhang and Jiong Zhang for useful
discussions 

     Recovery Rate     Tensor initialization error iteration log obj Initialize     with TensorRandomly initialize both   WFix      randomly initialize  Recovery Guarantees for Onehidden layer Neural Network

References
Anandkumar  Animashree  Ge  Rong  Hsu  Daniel  Kakade 
Sham    and Telgarsky  Matus  Tensor decompositions for
learning latent variable models  JMLR     
Andoni  Alexandr  Panigrahy  Rina  Valiant  Gregory  and Zhang 
Li  Learning polynomials with neural networks  In Proceedings of the  st International Conference on Machine Learning
 ICML  pp     

Arora  Sanjeev  Ge  Rong  Kannan  Ravindran  and Moitra 
Ankur 
Computing   nonnegative matrix factorization 
provably  In Proceedings of the fortyfourth annual ACM symposium on Theory of computing  STOC  pp    ACM 
   

Arora  Sanjeev  Ge  Rong  and Moitra  Ankur  Learning topic
models going beyond svd  In Foundations of Computer Science  FOCS    IEEE  rd Annual Symposium on  pp 
  IEEE     

Arora  Sanjeev  Bhaskara  Aditya  Ge  Rong  and Ma  Tengyu 
Provable bounds for learning some deep representations 
In
Proceedings of the  st International Conference on Machine
Learning  ICML  pp    https arxiv org 
pdf pdf   

Arora  Sanjeev  Ge  Rong  Ma  Tengyu  and Risteski  Andrej 
Provable learning of noisyor networks  In Proceedings of the
 th Annual Symposium on the Theory of Computing  STOC 
https arxiv org pdf pdf   
Aslan   Ozlem  Zhang  Xinhua  and Schuurmans  Dale  Convex
deep learning via normalized kernels  In Advances in Neural
Information Processing Systems  NIPS  pp     
Bach  Francis  Breaking the curse of dimensionality with convex

neural networks  arXiv preprint arXiv   

Balcan  MariaFlorina  Liang  Yingyu  Woodruff  David    and
Zhang  Hongyang  Optimal sample complexity for matrix
completion and related problems via  cid regularization  arXiv
preprint arXiv   

Balduzzi  David  Deep online convex optimization with gated

games  arXiv preprint arXiv   

Balduzzi  David  McWilliams  Brian  and ButlerYeoman  Tony 
Neural taylor approximations  Convergence and exploration in
recti er networks  arXiv preprint arXiv   

Bartlett  Peter    The sample complexity of pattern classi cation
with neural networks  the size of the weights is more important
than the size of the network  IEEE Transactions on Information
Theory     

Bengio  Yoshua  Roux  Nicolas    Vincent  Pascal  Delalleau 
Olivier  and Marcotte  Patrice  Convex neural networks 
In
Advances in Neural Information Processing Systems  NIPS 
pp     

Blum  Avrim and Rivest  Ronald    Training    node neural
network is npcomplete  In Proceedings of the  st International
Conference on Neural Information Processing Systems  NIPS 
pp    MIT Press   

Brutzkus  Alon and Globerson  Amir  Globally optimal gradient descent for   convnet with gaussian inputs  arXiv preprint
arXiv   

Choromanska  Anna  Henaff  MIkael  Mathieu  Michael 
Ben Arous  Gerard  and LeCun  Yann  The loss surfaces of
multilayer networks  In Proceedings of the Eighteenth International Conference on Arti cial Intelligence and Statistics  AISTATS  pp     

Cohen  Nadav and Shashua  Amnon  Convolutional recti er networks as generalized tensor decompositions  In International
Conference on Machine Learning  ICML   

Cohen  Nadav  Sharir  Or  and Shashua  Amnon  On the expressive power of deep learning    tensor analysis  In  th Annual
Conference on Learning Theory  COLT  pp     
Daniely  Amit  Frostig  Roy  and Singer  Yoram  Toward deeper
understanding of neural networks  The power of initialization
and   dual view on expressivity  In Advances in neural information processing systems  NIPS  pp     

Dauphin  Yann    Pascanu  Razvan  Gulcehre  Caglar  Cho 
Kyunghyun  Ganguli  Surya  and Bengio  Yoshua 
Identifying and attacking the saddle point problem in highdimensional
nonconvex optimization  In Advances in neural information
processing systems  NIPS  pp     

Elhamifar  Ehsan and Vidal  Ren    Sparse subspace clustering  In

CVPR  pp     

Feng  Jiashi  Zahavy  Tom  Kang  Bingyi  Xu  Huan  and Mannor 
Shie  Ensemble robustness of deep learning algorithms  arXiv
preprint arXiv   

Freeman    Daniel and Bruna  Joan  Topology and geomeIn arXiv preprint 

try of halfrecti ed network optimization 
https arxiv org pdf pdf   

Gillis  Nicolas and Glineur  Franc ois  Lowrank matrix approximation with weights or missing data is nphard  SIAM Journal
on Matrix Analysis and Applications     
Gillis  Nicolas and Vavasis  Stephen    On the complexity of
robust pca and  cid norm lowrank matrix approximation  arXiv
preprint arXiv   

Goel  Surbhi  Kanade  Varun  Klivans  Adam  and Thaler  Justin 
Reliably learning the relu in polynomial time  In  th Annual
Conference on Learning Theory  COLT  https arxiv 
org pdf pdf   

Haeffele  Benjamin   and Vidal  Ren    Global optimality in tensor factorization  deep learning  and beyond  arXiv preprint
arXiv   

Hardt  Moritz  Understanding alternating minimization for matrix completion  In Foundations of Computer Science  FOCS 
  IEEE  th Annual Symposium on  pp    IEEE 
 

Hardt  Moritz and Ma  Tengyu  Identity matters in deep learning 

ICLR   

Hardt  Moritz and Moitra  Ankur  Algorithms and hardness for
robust subspace recovery  In COLT  volume   pp   
 

Hardt  Moritz  Recht  Ben  and Singer  Yoram  Train faster  generalize better  Stability of stochastic gradient descent  In ICML 
pp     

  astad  Johan  Tensor rank is npcomplete  Journal of Algo 

rithms     

Hillar  Christopher   and Lim  LekHeng  Most tensor problems
In Journal of the ACM  JACM  volume  
are nphard 
pp    https arxiv org pdf pdf 
 

Hornik  Kurt  Approximation capabilities of multilayer feedfor 

ward networks  Neural networks     

Jain  Prateek  Netrapalli  Praneeth  and Sanghavi  Sujay  Lowrank matrix completion using alternating minimization  In Proceedings of the forty fth annual ACM symposium on Theory
of computing  STOC   

Recovery Guarantees for Onehidden layer Neural Network

tions of Computer Science  FOCS  pp    IEEE   
Swirszcz  Grzegorz  Czarnecki  Wojciech Marian  and Pascanu 
Razvan  Local minima in training of deep networks  arXiv
preprint arXiv   

Telgarsky  Matus  Bene ts of depth in neural networks 

In
 th Annual Conference on Learning Theory  COLT  pp 
   

Tian  Yuandong  Symmetrybreaking convergence analysis of
certain twolayered neural networks with ReLU nonlinearity 
In Workshop at International Conference on Learning Representation   

Wang  Yining and Anandkumar  Anima 
differentiallyprivate tensor decomposition 
in Neural
   

Information Processing Systems

Online and
In Advances
 NIPS  pp 

Wang  Yining  Tung  HsiaoYu  Smola  Alexander    and Anandkumar  Anima  Fast and guaranteed tensor decomposition via
sketching  In Advances in Neural Information Processing Systems  NIPS  pp     

Xie  Bo  Liang  Yingyu  and Song  Le  Diversity leads to generalization in neural networks  In International Conference on
Arti cial Intelligence and Statistics  AISTATS   

Yi  Xinyang  Caramanis  Constantine  and Sanghavi  Sujay  Alternating minimization for mixed linear regression  In ICML 
pp     

Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht  Benjamin 
and Vinyals  Oriol  Understanding deep learning requires rethinking generalization  In ICLR     

Zhang  Yuchen  Lee  Jason    and Jordan  Michael      
regularized neural networks are improperly learnable in polynomial time  In Proceedings of The  rd International Conference on Machine Learning  ICML  pp       

Zhang  Yuchen  Liang  Percy  and Wainwright  Martin   
arXiv preprint

Convexi ed convolutional neural networks 
arXiv     

Zhang  Yuchen  Lee  Jason    Wainwright  Martin    and Jordan 
Michael    On the learnability of fullyconnected neural networks  In International Conference on Arti cial Intelligence
and Statistics     

Zhong  Kai  Jain  Prateek  and Dhillon  Inderjit    Mixed linear
regression with multiple components  In Advances in neural
information processing systems  NIPS  pp     
Zhong  Kai  Song  Zhao  Jain  Prateek  Bartlett  Peter    and
Dhillon  Inderjit    Recovery guarantees for onehidden layer
In ICML  https arxiv org pdf 
neural networks 
 pdf   

Janzamin  Majid  Sedghi  Hanie  and Anandkumar  Anima 
Beating the perils of nonconvexity  Guaranteed training
arXiv preprint
of neural networks using tensor methods 
arXiv   

Kawaguchi  Kenji  Deep learning without poor local minima 

arXiv preprint arXiv   

Kuleshov  Volodymyr  Chaganty  Arun  and Liang  Percy  Tensor
In Proceedings of the
factorization via matrix factorization 
Eighteenth International Conference on Arti cial Intelligence
and Statistics  AISTATS  pp     

Livni  Roi  ShalevShwartz  Shai  and Shamir  Ohad  On the computational ef ciency of training neural networks  In Advances
in neural information processing systems  NIPS  pp   
 

Montufar  Guido    Pascanu  Razvan  Cho  Kyunghyun  and Bengio  Yoshua  On the number of linear regions of deep neural
networks  In Advances in neural information processing systems  NIPS  pp     

Poole  Ben  Lahiri  Subhaneil  Raghu  Maithreyi  SohlDickstein 
Jascha  and Ganguli  Surya  Exponential expressivity in deep
neural networks through transient chaos  In Advances In Neural Information Processing Systems  NIPS  pp   
 

Raghu  Maithra  Poole  Ben  Kleinberg  Jon  Ganguli  Surya  and
SohlDickstein  Jascha  On the expressive power of deep neural
networks  arXiv preprint arXiv   

Razenshteyn 

Ilya    Song  Zhao  and Woodruff  David   
Weighted low rank approximations with provable guarantees 
In Proceedings of the  th Annual Symposium on the Theory
of Computing  STOC  pp     

Safran  Itay and Shamir  Ohad  On the quality of the initial basin
in overspeci ed neural networks  In International Conference
on Machine Learning  ICML   

Sagun  Levent  Bottou    eon  and LeCun  Yann  Singularity of
the Hessian in deep learning  arXiv preprint arXiv 
 

Sedghi  Hanie and Anandkumar  Anima  Provable methods for
training neural networks with sparse connectivity  In International Conference on Learning Representation  ICLR   
Shamir  Ohad  Distributionspeci   hardness of learning neural

networks  arXiv preprint arXiv   

Song  Zhao  Woodruff  David    and Zhang  Huan  Sublinear
time orthogonal tensor decomposition  In Advances in Neural
Information Processing Systems NIPS  pp     

Song  Zhao  Woodruff  David    and Zhong  Peilin 

Low
In Prorank approximation with entrywise  cid norm error 
ceedings of the  th Annual Symposium on the Theory of
Computing  STOC  ACM  https arxiv org pdf 
 pdf     

Song  Zhao  Woodruff  David    and Zhong  Peilin  Relative error
In arXiv preprint  https 

tensor low rank approximation 
 arxiv org pdf pdf     

Sontag  David and Roy  Dan  Complexity of inference in latent
In Advances in neural information pro 

dirichlet allocation 
cessing systems  pp     

Soudry  Daniel and Carmon  Yair  No bad local minima  Data independent training error guarantees for multilayer neural networks  arXiv preprint arXiv   

Sun  Ruoyu and Luo  ZhiQuan  Guaranteed matrix completion
via nonconvex factorization  In IEEE Symposium on Founda 

