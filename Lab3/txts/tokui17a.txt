Evaluating the Variance of LikelihoodRatio Gradient Estimators

Seiya Tokui     Issei Sato    

Abstract

The likelihoodratio method is often used to estimate gradients of stochastic computations  for
which baselines are required to reduce the estimation variance  Many types of baselines have
been proposed  although their degree of optimality is not well understood  In this study  we establish   novel framework of gradient estimation that includes most of the common gradient estimators as special cases  The framework
gives   natural derivation of the optimal estimator that can be interpreted as   special case of
the likelihoodratio method so that we can evaluate the optimal degree of practical techniques
with it 
It bridges the likelihoodratio method
and the reparameterization trick while still supporting discrete variables  It is derived from the
exchange property of the differentiation and integration  To be more speci    it is derived by
the reparameterization trick and local marginalization analogous to the local expectation gradient  We evaluate various baselines and the optimal estimator for variational learning and show
that the performance of the modern estimators is
close to the optimal estimator 

  Introduction
The success of deep learning owes much to ef cient gradient computation using backpropagation  Rumelhart et al 
  When the model of interest
includes internal
stochasticities  the objective function is often written as  
stochastic computational graph  Schulman et al    In
this case  the exact gradient computation is intractable in
general  and an approximate estimation is required  The
variance introduced by the approximation often degrades
the optimization performance for deep models  and there 

 Preferred Networks  Tokyo  Japan  The University of
Tokyo  Tokyo  Japan  RIKEN  Tokyo  Japan  Correspondence
to  Seiya Tokui  tokui preferred jp  Issei Sato  sato   utokyo ac jp 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

fore variance reduction is crucial for practical learning 
However  few things are known about its theoretical aspects  and we often struggle with modelspeci   heuristics
whose degree of optimality is dif cult to know 
The likelihoodratio method  Glynn    Williams   
and the reparameterization trick  Williams    Kingma
  Welling    Rezende et al    Titsias   Lzarogredilla    are widely used for the gradient estimation  The likelihoodratio method only requires the computation of density functions and their derivatives  and therefore it is applicable to   wide range of models including
those with discrete variables  It requires variance reduction
techniques in practice  The most common technique is the
use of   baseline value  Paisley et al    Bengio et al 
  Ranganath et al    Mnih   Gregor    Gu
et al      which is subtracted from   sampled objective
value  The optimal baseline is dif cult to compute in general  and we often use alternatives that are ef ciently computed  some of which are based on modelspeci   heuristics  The reparameterization trick  on the other hand  has  
small estimation variance in practice and is only applicable
to models with certain continuous variables  Various models with continuous variables have been proposed using it 
whereas less progress on the research of deep discrete variable models has been made because of the inapplicability
of this method 
In this paper  we give   novel framework to formulate gradient estimators 
It is derived by the reparameterization
and the local marginalization analogous to the local expectation gradient  Titsias     azaroGredilla    The
likelihoodratio method and the reparameterization trick
can be formalized under this framework  and therefore it
bridges these two families of estimators  We can derive the
optimal estimator  which gives   lower bound of the variance of all estimators covered by the framework  Since the
estimator is derived by applying local marginalization to
the reparameterized gradient  we named it the reparameterization and marginalization  RAM  estimator  This estimator is an instance of the likelihoodratio estimator with
the optimal baseline  and therefore it can be used to evaluate the variance of existing baseline techniques 
When the variable of interest follows   Bernoulli distribution  we can derive   tighter connection of   wider range of

Evaluating the Variance of LikelihoodRatio Gradient Estimators

estimators to the framework  For example  the local expectation gradient  Titsias     azaroGredilla    becomes
covered by our framework  and the straightthrough estimator  Hinton    Bengio et al    Raiko et al   
approximates the optimal estimator where the  nite difference of the objective function is replaced by the in nitesimal  rstorder approximation  Furthermore  the optimal
estimator is reduced to   likelihoodratio estimator with
an inputdependent baseline  which implies that   practical
baseline technique might achieve   nearoptimal variance 
The rest of this paper is organized as follows  We overview
the related work in Sec  and formulate the gradient estimation problem in Sec  We introduce our framework in
Sec  and derive important estimators with it in Sec  We
also introduce   wider range of estimators for Bernoulli
variables in Sec  We then show experimental results in
Sec  and give   conclusion in Sec 

  Related Work
The gradient estimation problem was being studied in the
 eld of simulation around   which is well summarized in   Ecuyer   The likelihoodratio method
 Glynn    is   general approach for solving the problem  in which the parametric density      is replaced by
    
          where         is  xed against   on differentiation  The ratio     
     is called the likelihood ratio  hence the
name of this method  It can be seen as an importance sampling method that uses   proposal     Jie   Abbeel   
with which there is   study on reducing the variance by using   proposal better than     Ruiz et al      Another
approach is the  nitedifference method  in which the use
of common random numbers       using the same random
numbers to run two perturbed simulations  is effective in
reducing the variance  The common random numbers naturally appear in the formulation of the optimal estimator of
our framework 
The likelihoodratio method has been combined with baselines and was introduced to the policy gradient methods for
reinforcement learning  which is called the REINFORCE
algorithm  Williams    The baseline technique is used
for reducing the variance    simple estimation of the average reward is commonly used as the baseline  and the
optimal constant baseline that minimizes the variance is
also derived  Weaver   Tao    The likelihoodratio
estimator has also been used for blackbox variational inference  Ranganath et al    The likelihoodratio estimator is used to derive the gradient estimation without
depending on the speci   form of the distributions  From
  statistical point of view  the baseline can be seen as  
special form of control variates  for which the optimal one
can be derived again  The baseline technique has been

further made sophisticated by involving the variablewise
baselines and those depending on the varaible of interest
 Mnih   Gregor    Gu et al      Some of them are
also exported to policygradient methods  Gu et al     
Taking the local expectation of the likelihoodratio estimator  Titsias     azaroGredilla    is another approach
of variance reduction 
For variational inference of models with continuous variables  the reparameterization trick  Kingma   Welling 
  Rezende et al    Titsias   Lzarogredilla   
is widely used  It is easy to implement with modern frameworks of automatic differentiation  It also has low variance
in practice  although the superiority to the likelihoodratio
estimator is not guaranteed in general  Gal    This
method is also applied to the continuous relaxation of discrete variables  Jang et al    Maddison et al   
On the one hand  the connection between the likelihoodratio method and the reparameterization trick is studied
in some literature  especially on continuous variables for
which   tractable reparameterization is not available  Ruiz
et al      On the other hand  there are fewer studies for
discrete variables  This paper provides   bridge between
these estimators for discrete variables 

        cid  

  Problem Formulation
Our task is to optimize an expectation over   parameterized distribution  The objective function is given as
         Eq              where   is   feasible function 
       zi pai  is   directed graphical model
of   variables                 zM   conditioned on an input
to the system    pai are the parent nodes of zi  and   are
the model parameters  Each conditional     zi pai  is continuously differentiable           and is typically   simple
distribution such as   Bernoulli or Gaussian whose parameters are computed by   neural network with weights    
For simplicity  we will assume that    and    cid  for    cid    cid 
do not share any parameters  however  this assumption can
be easily removed  We want to optimize   by stochastic
gradient methods  which require an unbiased estimation of
its gradient     
  motivating example is variational learning of   generative model         with an approximate posterior       
In this case  the objective function is the expectation of
           log           log        which gives   lower
bound of the log likelihood log      On the one hand 
the gradient        the generative parameter   is easily estimated by   Monte Carlo simulation  On the other hand 
estimating the gradient          is not trivial  which falls
into the above general setting  Note that we omit the gradient incurred from the dependency of the second term
  log        on   from our discussions since this gradi 

Evaluating the Variance of LikelihoodRatio Gradient Estimators

ent term is easy to estimate with low variance 

  Proposed Framework
Here we give   general formulation of our framework of
gradient estimation  The framework is based on the reparameterization of variables  which we also review 
Suppose that each sample drawn from   conditional is reparameterized as follows 

zi       zi pai    zi       pai               

Here    is   noise variable  We will give concrete examples
of reparameterization later  and here we only emphasize
that     might be   noncontinuous function  We write the
whole reparameterization as           
Using this
reparameterization  we derive the general
    
                              We partially exchange the
differentiation and integration as follows 

form of gradient

estimation 

Let

 iF          

              

       

  if           

 

Unlike the reparameterization trick  this equation holds
even if the function    is not continuous because the local expectation   if            is differentiable  The
technique of separating variables in leaveone out man 
  of Titsias     azaroGredilla
ner is similar to Eq 
  whereas it is applied to reparameterized  mutuallyindependent noise variables in our case 
Equation   gives our framework of gradient estimation 
Given   way to estimate the local gradient
  
                 we can estimate  iF      by
sampling    and estimating the local gradient  Many existing estimators are derived by specifying   method of local
gradient estimation  which we review in the next section 

Examples of Reparameterization  We introduce typical ways of reparameterizing popular distributions  When
    zi pai       zi     
    is   Gaussian distribution  zi
can be reparameterized as zi                            
 Kingma   Welling   
In this case  the changeof 
variable formula     pai           pai             pai     
is differentiable            We can derive   reparameterization for Bernoulli variables as well  Suppose zi  
    is   binary variable following   Bernoulli distribu 
 cid 
tion     zi pai     zi
         zi  It can be reparameterized using   uniform noise            as zi         
is the Heaviside step funcwhere       

        
        

tion 

In this case  the changeof variable formula zi  

    pai            is not continuous in general  For categorical variables  we can use the GumbelMax trick  Gumbel    Jang et al    Maddison et al    for the
reparameterization in   similar way 

  Derivation of Gradient Estimators
We derive existing estimators on the basis of our general
framework   We also derive the estimator that is optimal
in terms of the estimation variance 

  LikelihoodRatio Estimator

  if           

The likelihoodratio estimator is derived by using the logderivative trick for the local gradient estimation  Let
bi      be   baseline for zi  and           We use
           and omit the dependency of   on   The
likelihoodratio estimator with baseline bi is   Monte Carlo
estimate of the following expectation 
  
                 bi       log     zi pai    Ci       
 
Here Ci            ibi       log     zi pai  which
has to be analytically computed 
There are many baseline techniques for variance reduction 
We classify them into four categories as follows 

  Constant baseline is   constant of all variables      
It is   common choice for the baseline  In this case  it
holds that Ci     An exponential moving average of
the simulated function   is often used 

  Independent baseline is   baseline that is constant
against     It can depend on other variables         
In this case  it again holds that Ci     Two techniques proposed by Mnih   Gregor   can be
seen as examples of baselines in this class  One is the
inputdependent baseline  which is   neural network
that predicts the sampled objective value          from
  and pai  The other one is the use of local signals 
where the terms of   that are not descendants of zi
in the stochastic computational graphs are omitted  It
can be seen as   baseline that includes all these terms 
  Linear baseline is   baseline that is   linear function of
zi       bi     cid 
  ui          vi          where ui and
vi are arbitrary functions  In this case  we can write
Ci        cid ui        where      Eq    zi pai zi
is the mean of zi  The MuProp estimator  Gu et al 

 When zi is   binary or continuous scalar  the transposition is
not needed  When zi is   categorical variable  we represent it by  
onehot vector  for which   cid 
  ui is the innerproduct of two vectors 

Evaluating the Variance of LikelihoodRatio Gradient Estimators

    is an example of estimators with linear baselines  where the baseline is given as the  rstorder approximation of the mean eld network of   at    

  Fullyinformed baseline is   baseline that depends on
all of   and   possibly in   nonlinear way  This is the
most general class of baselines 

It is easily expected that the fullyinformed baseline can
achieve the lowest variance  We will show that the optimal
estimator under the framework   falls into this category 

  Reparameterization Trick Estimator

The reparameterization trick  Kingma   Welling   
Rezende et al    Titsias   Lzarogredilla    is  
common way to estimate the gradient for models with continuous variables  It is derived by exchanging the differentiation and integration of the local gradient as follows 

  

  if                  if           

 

Note that this equation holds only if the function       
is differentiable  and therefore the reparameterization trick
is only applicable to continuous variables 
The reparameterization trick often gives   better estimation
of the gradient compared with the likelihoodratio estimator  although there is no theoretical guarantee  Indeed  we
can construct an example for which the likelihoodratio estimator gives   better estimation  Gal   

  Optimal Estimator

  if            Let    

The optimal estimator is obtained by analytically computing the local gradient   
 
            zi  zi          zM  When we       and modify the value of zi  the descendant variables of zi might be
changed because they are functions of zi and noise variables  We denote the resulting values of     by      
       zi      The function        zi      represents
the ancestral sampling procedure of     with given    and
clamped zi  Using the reparameterization again  we obtain
  if              Eq    zi pai          The local gradient is then computed as follows 

 cid 
  
 

zi

  if           

        iq   zi pai 

 cid cid cid        

 

 

   zi   

If zi is continuous  the summation is replaced by an integral  which is approximated numerically  The resulting
algorithm is shown in Alg    which we name the reparameterization and marginalization  RAM  estimator 
It
requires   times evaluations of    and therefore it scales

Algorithm   Algorithm for RAM estimator   for discrete
zi    If zi is continuous  the loop over all the con gurations
of zi is replaced by   loop over integration points 
Require    set of parameters   and an input variable   
  Sample       
  for                 do
 
 
 
 

for all con gurations of zi do
             zi     
fzi           iq   zi pai 

      cid 

end for

  end for
  return                as an estimation of        

fzi 

zi

worse than other estimators   However  these evaluations
are easily parallelized  and it runs fast enough for models
of moderate size 
The optimality of this estimator is stated in the following
theorem  Let  ij be the jth element of the vector of parameters     and let  ij    ij for notational simplicity 
Theorem   Suppose an unbiased estimator  ij of the local
derivative  ijE if                  ij is   random variable whose expectation matches the local derivative  Let
Vij be the variance of the estimator  ij and    cid 
ij be the variij   Vij 
ance of the RAM estimator  Then  it holds that    cid 

Proof  This
Blackwellization argument 

follows

from the

standard

Rao 

We brie   review the relationships between the RAM estimator and existing ones 

Relationship to the LikelihoodRatio Estimator  The
RAM estimator can be seen as an example of
the
likelihoodratio estimators with fullyinformed baselines 
Let bi                     Then  the logderivative term
of Eq    is canceled  and only the residual Ci         
  
                 remains  The analyticallycomputed
residual is equivalent to the RAM estimator  Since this estimator gives the minimum variance  our likelihoodratio
formulation   contains the optimal estimator 
While the fullyinformed baseline is too general in practice
to be ef ciently computed  much more restrictive independent baselines can achieve the optimal estimator when zi
follows   Bernoulli distribution  Let   LR
ij  bi  be the variance of the likelihoodratio estimator with baseline bi 
Theorem   Suppose that     zi pai  is   Bernoulli distribution  Then  there is one and only one baseline   cid 
  such
  The difference in computational cost against the local expectation gradient  Titsias     azaroGredilla    comes from the
inapplicability of pivot samples 

Evaluating the Variance of LikelihoodRatio Gradient Estimators

that   cid 

  is constant against    and    cid 

ij     LR

   
ij    cid 

The proof is given in Sec  This result implies that  for
Bernoulli variables  the optimal variance might be obtained
by   practical class of baseline techniques  Note that the
optimal baseline   cid 
  might depend on the noise variables
corresponding to the descendants of zi  which are not used
by existing baseline techniques 

Relationship to the Reparameterization Trick  Theorem   also states that the RAM estimator gives   variance
not larger than that of the reparameterization trick 
Indeed  the RAM estimator is based on an analytical computation of the integral   and therefore gives the lower
or equal variance  In practice  it is infeasible to compute
the local gradient analytically  and numerical approximation is required  We can approximate the integral with high
precision in practice  because zi is usually   scalar variable and therefore we only need to evaluate the function
             at   few integration points of zi 

Relationship to the Local Expectation Gradient  The
local expectation gradient  Titsias     azaroGredilla 
  is an application of local marginalization to the
likelihoodratio estimator  Let mbi be the Markov blanket
of zi  This estimator is then derived as follows 
 iF     
  Eq               log     zi pai 
  Eq       Eq zi mbi           log     zi pai 
  Eq       

        iq   zi pai   

 cid 

  zi mbi 
    zi pai 

zi

For the Monte Carlo simulation of        is  rst sampled
from        and then zi is discarded 
It corresponds
to sampling   and computing     by using it in the reparameterized notation 
If the latent variables            zM
are mutually independent given    the density ratio factor
  zi mbi     zi pai  equals   and therefore this estimator is equivalent to the RAM estimator   Otherwise  the
density ratio factor remains  and these estimators do not
match in general  The inference distribution   zi mbi 
can be computed as

  zi mbi   

 cid 
  zi  mbi   pai pai 

  zi  mbi   pai pai 

zi

 

Therefore  the density ratio is proportional to   zi  mbi  
pai pai  It tends to concentrate on zi used in the sampling
of mbi   pai  in which case the estimator degenerates to
the plain likelihoodratio estimator  Therefore  it cannot be
guaranteed to have   lower variance than the likelihoodratio estimator with baselines in general 

The RAM estimator can be seen as an application of the
same technique to the reparameterized expectation  
Thanks to the reparameterization  there is no need to solve
the inference problem   zi mbi  and therefore the problematic density ratio factor does not appear  The evaluation
of          with  xed     does not re ect the full in uence
of the choice of zi  whereas the reparameterized counterpart                

   zi    does re ect it 

  Analyzing Estimators for Binary Variables
  Bernoulli variable is the most fundamental example of  
discrete variable  and some estimators are dedicated for it 
It is bene cial to study the applications of any estimators
to Bernoulli variables because they facilitate understanding
and still contain most of the essential characteristics of discrete distributions  In some cases  an estimator has   connection to other estimators only when applied to Bernoulli
variables  Here we focus on Bernoulli variables and introduce how each estimator can be formalized and related to
others  The derivations are given in the supplementary material  
Suppose that     zi pai  is   Bernoulli distribution of the
mean parameter         pai      Let fk         zi  
                zi      for              fk is the reparameterized objective value for zi      All estimators we
introduce here can be written as an estimation of the gradient           multiplied by       and therefore we only
focus on the gradient           denoted by    

  LikelihoodRatio Estimator

The likelihoodratio estimator for   Bernoulli variable with
an independent baseline bi is written as follows 

 LR

   

      bi  
      bi                    

        

 

 cid 

It can be interpreted as an importance sampling estimation
of the sum of   bi and    bi  Indeed  the likelihoodratio estimator for the general class of distributions can be
seen as an importance sampling estimation of the expectation  When the distribution has low entropy          is close
to   or   the variance of  LR
becomes large  However 
it does not always mean that the variance of the gradient
          becomes huge  because in this case the sigmoid
activation that outputs    is in    at regime so that its small
derivative somehow alleviates the large variance  Neither
does it mean that the variance is always small enough to
optimize complex models 

 

 The supplementary material is attached to the arXiv version

of this paper 

Evaluating the Variance of LikelihoodRatio Gradient Estimators

  Optimal Estimator

It is further rewritten as follows 

The RAM estimator of the gradient           is simply written as the difference of the   value at zi     and zi    

 LEG

 

 

       

  
       

  

     if cid 
 

     cid 

  
  

 if 

        
            

           
 cid 

 

Thus  it can be seen as   likelihoodratio estimator with
baseline

Using this formulation  we can prove Theorem  

bLEG
 

 

Proof of Theorem   Let bi              if  Then  the
likelihoodratio estimator   with baseline bi is equivalent
to the RAM estimator   In this case  both cases of  
are equal to  cid 
    This baseline does not depend on     and
therefore we conclude the proof by letting   cid 

    bi 

Interestingly  the optimal baseline is an expectation of fk
with the mean of   being       instead of   This is different from the mean objective value       if             
which   constant mean baseline approximates  The difference becomes large when    is close to   or  
The optimal estimator still has   positive variance since
the noise variables    are not integrated out  In Eq   
   and    are evaluated with the same con gurations of
these noise variables  The likelihoodratio estimator can
also be seen as an estimator that separately samples   
and    at different iterations in which they use the separate samples of     When the number of variables is
large  the in uence of one variable zi on the objective
value          is small  and we can expect that    and   
have   positive covariance  In general  the estimation variance of the difference of two random variables      is reduced by estimating them with   positive covariance since
            VX   VY    Cov        and therefore
our estimator effectively reduces the variance by using the
same con guration of the noise variables  This technique
is known as common random numbers  which is also used
to reduce the variance of the gradient estimations with the
 nite difference method for stochastic systems    Ecuyer 
 

  Local Expectation Gradient

The local expectation gradient   has   special view as  
likelihoodratio estimator when zi is   Bernoulli variable 
Let        zi    mbi  Let   cid 
          zi           
       zi                     cid 
   is the objective value
with  xed     and  ipped zi  Note that   cid 
  can be different
from fk when some variables in     depend on zi  Then 
the local expectation gradient is written as follows 

 cid    

 LEG

 

 

       
  
      
  cid 
  

  cid 
          
               

  
  
  

      zi mbi 
        zi pai 

  cid 

ik

 

ik           zi pai fk       zi pai   cid 
    where     is replaced by   cid 
is given by multiplying   cid 

where   cid 
    The
unweighted value   cid 
ik has   similar form as the optimal
    The  nal
baseline   cid 
baseline bLEG
ik by the density
ratio  We have seen that it tends to be close to   in which
case the baseline is also close to   so that the estimator degenerates to the plain likelihoodratio estimator  Even if
the weight does not vanish  Theorem   shows that the local expectation gradient estimator for Bernoulli variables
has higher variance than the optimal one unless all latent
variables are mutually independent given   

  StraightThrough Estimator

We give one example of an estimator dedicated for
Bernoulli variables  the straightthrough estimator  Hinton 
  Bengio et al    Raiko et al    It is   biased estimator that leverages the gradient of   so that we
can obtain the highdimensional information of the direction towards which the objective would be decreasing  The
estimator is written as follows 

    

  
 zi

 zi

 cid cid cid zi 
 cid cid cid zi 

 ST

   

        
            

 

Observing that Eq    gives the  nite difference of   between zi     and zi     we can see that the straightthrough estimator is its in nitesimal counterpart at the sampled zi  Thus  this estimator is equivalent to our estimator  and is therefore unbiased  when   is   linear function
of zi  The difference between these estimators becomes
large when the nonlinearity of   increases  If we consider
  general class of the evaluation function    we can construct an adversarial function   such that the derivative at
zi       has the opposite sign against the  nite differi zi  this estimator is equivalent to the optimal one  However  if we modify
  zi  the straightthrough
estimator always gives   gradient opposite to the steepest
direction of the expectation  which does not change as  
result of the modi cation                     Ef       

ence   For example  when           cid 
it to            cid 

  zi  sin  cid 

  Experiments
We conduct experiments to empirically verify Theorem  
and to demonstrate   procedure to analyze the optimal de 

Evaluating the Variance of LikelihoodRatio Gradient Estimators

gree of   given estimator covered by our framework  All
the methods are implemented with Chainer  Tokui et al 
 

  Experimental Settings

The task is variational learning of sigmoid belief networks
 SBN   Neal    which is   directed graphical model
with layers of Bernoulli variables  Let          be  
binary vector of input data and   cid       cid            cid   cid   
     cid  be   binary vector of the latent variables at the  cid 
th layer  Denote the input layer as        for notational
simplicity  Let   be the number of latent layers  The generative model is speci ed by   conditional of each layer
    cid   cid  and the prior of the deepest layer   ZL 
In our experiments  the prior of each variable zL     ZL
is independently parameterized by its logit  The conditional     cid   cid  is modeled by an af ne transformation
of   cid  that outputs the logit of   cid  The parameters of the
af ne transformation are optimized through the learning 
We use two models with       and       respectively 
Each layer consists of   cid      Bernoulli variables 
We model the approximate posterior        by   reversedirectional SBN  In this case  the prior      is not modeled 
and each conditional     cid   cid  is speci ed by its logit
as an af ne transformation of   cid 
Both the generative parameter   and the variational parameter   are optimized simultaneously to maximize the following variational lower bound 

log        log Eq     
  Eq      log

       
      
       
      

    

The second line follows Jensen   inequality with the concavity of log  The gradient          is estimated by   Monte
Carlo simulation of      Eq      log         We
use gradient estimators for approximating the gradient
        
The plain likelihoodratio estimator is denoted by LR 
whereas the constant baseline using the moving average
of            log           log        and the inputdependent baseline of Mnih   Gregor   are expressed
by the post xes    and  IDB  respectively  We also run
experiments for MuProp and the Local Expectation Gradient  LEG  Algorithm   is used to obtain the results for the
optimal estimator 
We use MNIST  Lecun et al    and Omniglot  Lake
et al    for our experiments  These are sets of    
pixel grayscale images of handwritten digits and handwritten characters from various languages  We binarize
each pixel by sampling from   Bernoulli distribution with

the mean equal to the pixel intensity  Salakhutdinov  
Murray    The binarization is done in an online manner       we sample binarized vectors at each iteration  For
the MNIST dataset  we use the standard split of  
training images and   test images  The training images are further split into   images and   images  the latter of which are used for validation  For the
Omniglot dataset  we use the standard split of   training images and   test images used in the of cial implementation of Burda et al      The training images
are further split into   images and   images  the
latter of which are used for validation 
We used RMSprop  Tieleman   Hinton    with   minibatch size of   to optimize the variational lower bound 
We apply   weight decay of the coef cient   for all parameters  All the weights are initialized with the method of
Glorot   Bengio   The learning rate is chosen from
              We evaluate the model on
the validation set during training  and choose the learning
rate with which the best validation performance with earlystopping beats the others  After each evaluation  we also
measure the variance of the gradient estimations of variational parameters for the training set with the same minibatch size 
Each experiment is done on an Intel    Xeon    CPU   
     at   GHz and an NVIDIA GeForce Titan   
Thanks to the parallel computation using the GPU  the
computational time of the RAM estimator is only two times
larger than the plain likelihoodratio estimator 

  Results

The results for the twolayer SBN and fourlayer SBN are
shown in Fig    and Fig    respectively  The results for
these models have almost the same trends  As is predicted
by Theorem   the optimal estimator gives the lower bound
of the estimation variance  The plots imply that the modern
baseline techniques effectively reduce the estimation variance  which is approaching the optimal value  However  the
gap between these practical methods and the optimal one
is not negligible  and there is still room for improvements 
The local expectation gradient actually does not degenerate
to the plain likelihoodratio estimator  whereas the variance
reduction effect is limited so that its variance stays at   similar level to that of the likelihoodratio estimator with   constant baseline  The validation score almost agrees with the
variance level  although there are some exceptions caused
by the differences in selected learning rates 
Note that we do not align the computational cost by sampling multiple values in the experiments because the purpose of these experiments is evaluating the optimal degree

 https github com yburda iwae

Evaluating the Variance of LikelihoodRatio Gradient Estimators

Figure   Results of twolayer SBN  Left  results using MNIST dataset  Right  results using Omniglot dataset  The mean of the gradient
variances of the variational parameters are plotted in the top  gures  The validation performance is plotted in the bottom  gures 

Figure   Results of fourlayer SBN  Left  results using MNIST dataset  Right  results using Omniglot dataset 

of each method  We can infer the performance with aligned
computational budget by comparing the variance and the
computational cost 

  Conclusion
We introduced   novel framework of gradient estimation
for stochastic computations using reparameterization  The
framework serves as   bridge between the likelihoodratio
method and the reparameterization trick  The optimal estimator is naturally derived under the framework  It provides
the minimum variance attainable by the likelihoodratio estimators with the general class of baselines  and therefore
can be used to evaluate the optimal degree of each practical baseline technique  We actually evaluated the common
baseline techniques against the optimal estimator for variational learning of sigmoid belief networks and showed that
the modern techniques achieve   variance level close to the
lower bound 

Comparison between continuous variable models and discrete variable models is needed for the further development
of deep probabilistic modeling  which should consider the
adequacy of the use of these variables in each task and the
ef ciency of gradient estimators available for these models  While this study does not provide   way to compare
such models in general  it bridges the gradient estimators
of them through the optimal case  and therefore provides
some insights on their relationships  Observing the experimental results  the modern estimators for Bernoulli variables achieve variance close to the optimal one  and therefore we can expect that the modern estimators for Bernoulli
variables are maturing and could be applied to much larger
models capturing discrete phenomena 

ACKNOWLEDGMENTS

We thank members of Preferred Networks  especially
Daisuke Okanohara  for the helpful discussions 

 Gradient varianceLRLR CLR   IDBMuProp   IDBLEGoptimal Iteration    Variational lower boundMNIST  Gradient varianceLRLR CLR   IDBMuProp   IDBLEGoptimal Iteration    Variational lower boundOmniglot  Gradient varianceLRLR CLR   IDBMuProp   IDBLEGoptimal Iteration    Variational lower boundMNIST  Gradient varianceLRLR CLR   IDBMuProp   IDBLEGoptimal Iteration    Variational lower boundOmniglot  Evaluating the Variance of LikelihoodRatio Gradient Estimators

References
Bengio  Yoshua    eonard  Nicholas  and Courville 
Aaron    Estimating or propagating gradients through
stochastic neurons for conditional computation  ArXiv 
   

Burda  Yuri  Grosse  Roger  and Salakhutdinov  Ruslan 
In Proceedings of
Importance weighted autoencoders 
the  rd International Conference on Learning Representations  ICLR   

Gal  Yarin  Uncertainty in Deep Learning  PhD thesis 
Department of Engineering  University of Cambridge 
 

Glorot  Xavier and Bengio  Yoshua  Understanding the dif 
 culty of training deep feedforward neural networks  In
In Proceedings of the International Conference on Arti 
 cial Intelligence and Statistics  AISTATS   

Glynn        Optimization of stochastic systems via simulation  In Proceedings of the  st Conference on Winter Simulation  pp      doi   
 

Glynn  Peter    Likelihood ratio gradient estimation for
stochastic systems  Communication of the ACM   
    doi   

Gu  Shixiang  Levine  Sergey  Sutskever  Ilya  and Mnih 
Andriy  Muprop  Unbiased backpropagation for stochastic neural networks  In Proceedings of the  th International Conference on Learning Representations  ICLR 
   

Gu  Shixiang  Lillicrap  Timothy  Ghahramani  Zoubin 
Turner  Richard    and Levine  Sergey 
Qprop 
Sampleef cient policy gradient with an offpolicy critic 
In NIPS   Deep Reinforcement Learning Workshop 
   

Gumbel  Emil Julius  Statistical theory of extreme values
and some practical applications        Govt  Print  Of 
 ce   

Hinton  Geoffrey  Neural networks for machine learning 

Coursera  video lectures   

Jang     Gu     and Poole     Categorical Reparameterization with GumbelSoftmax  ArXiv eprints    To
appear in ICLR  

Jie  Tang and Abbeel  Pieter  On   connection between importance sampling and the likelihood ratio policy gradient  In Advances in Neural Information Processing Systems   pp     

Kingma  Diederik    and Welling  Max  Autoencoding
In Proceedings of the International
variational bayes 
Conference on Learning Representations  ICLR   

Lake  Brenden    Salakhutdinov  Ruslan  and Tenenbaum  Joshua    Humanlevel concept learning through
probabilistic program induction  Science   
    doi   science aab 

Lecun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  In Proceedings of the IEEE  pp   
 

  Ecuyer  Pierre  An overview of derivative estimation  In
Proceedings of the  rd Conference on Winter Simulation  pp      ISBN  

Maddison  Chris    Mnih  Andriy  and Teh  Yee Whye  The
concrete distribution    continuous relaxation of discrete random variables  CoRR  abs   
To appear in ICLR  

Mnih  Andriy and Gregor  Karol  Neural variational inference and learning in belief networks  In Proceedings of
the  st International Conference on Machine Learning
 ICML  pp     

Neal  Radford    Connectionist learning of belief net 

works  Arti cial Intelligence     

Paisley  John  Blei  David    and Jordan  Michael    Variational bayesian inference with stochastic search  In Proceedings of the   th International Conference on Machine Learning  ICML   

Raiko  Tapani  Berglund  Mathias  Alain  Guillaume  and
Dinh  Laurent  Techniques for learning binary stochastic
feedforward neural networks  In Proceedings of the  rd
International Conference on Learning Representations
 ICLR   

Ranganath  Rajesh  Gerrish  Sean  and Blei  David   
Black box variational inference  In Arti cial Intelligence
and Statistics  AISTATS  pp     

Rezende  Danilo Jimenez  Mohamed  Shakir  and Wierstra 
Daan  Stochastic backpropagation and approximate inIn Proceedings of
ference in deep generative models 
the  st International Conference on Machine Learning
 ICML  pp     

Ruiz           Titsias        and Blei        Overdispersed blackbox variational inference  In Uncertainty
in Arti cial Intelligence  UAI     

Ruiz           Titsias        and Blei        The generalized reparameterization gradient  In Advances in Neural
Information Processing Systems     

Evaluating the Variance of LikelihoodRatio Gradient Estimators

Rumelhart  David    Hinton  Geoffrey    and Williams 
Ronald    Neurocomputing  Foundations of research 
chapter Learning Representations by Backpropagating
Errors  pp    MIT Press   

Salakhutdinov  Ruslan and Murray  Iain  On the quantitative analysis of Deep Belief Networks  In Proceedings
of the  th Annual International Conference on Machine
Learning  ICML  pp     

Schulman  John  Heess  Nicolas  Weber  Theophane  and
Abbeel  Pieter  Gradient estimation using stochastic
In Proceedings of the  th Intercomputation graphs 
national Conference on Neural Information Processing
Systems  pp     

Tieleman  Tijmen and Hinton  Geoffrey  Lecture  
rmsprop  Divide the gradient by   running average of
its recent magnitude  In CORSERA  Neural Networks
for Machine Learning   

Titsias  Michalis and   azaroGredilla  Miguel  Local expectation gradients for black box variational inference 
In Advances in Neural Information Processing Systems
   NIPS  pp     

Titsias  Michalis and Lzarogredilla  Miguel  Doubly
stochastic variational bayes for nonconjugate inference 
In Proceedings of the  st International Conference on
Machine Learning  ICML  pp     

Justin 

Chainer 

Tokui  Seiya  Oono  Kenta  Hido  Shohei  and Claya nextgeneration open
ton 
In Prosource framework for deep learning 
ceedings of Workshop on Machine Learning Systems  LearningSys  in The Twentyninth Annual Conference on Neural
Information Processing Systems
 NIPS    URL http learningsys org 
papers LearningSys paper pdf 

Weaver  Lex and Tao  Nigel  The optimal reward baseline
for gradientbased reinforcement learning  In Proceedings of the Seventeenth Conference on Uncertainty in
Arti cial Intelligence  UAI  pp    San Francisco  CA  USA    Morgan Kaufmann Publishers
Inc 
ISBN   URL http dl acm 
org citation cfm id 

Williams  Ronald    Simple statistical gradientfollowing
learning 

algorithms for connectionist reinforcement
Machine Learning     

