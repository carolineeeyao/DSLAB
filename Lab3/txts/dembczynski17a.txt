Consistency Analysis for Binary Classi cation Revisited

Krzysztof Dembczy nski   Wojciech Kot owski   Oluwasanmi Koyejo   Nagarajan Natarajan  

Abstract

Statistical learning theory is at an in ection point
enabled by recent advances in understanding and
optimizing   wide range of metrics  Of particular interest are nondecomposable metrics such
as the Fmeasure and the Jaccard measure which
cannot be represented as   simple average over
examples  Nondecomposability is the primary
source of dif culty in theoretical analysis  and
interestingly has led to two distinct settings and
notions of consistency 
In this manuscript we
analyze both settings  from statistical and algorithmic points of view  to explore the connections and to highlight differences between them
for   wide range of metrics  The analysis complements previous results on this topic  clari es
common confusions around both settings  and
provides guidance to the theory and practice of
binary classi cation with complex metrics 

  Introduction
Realworld applications of binary classi cation to complex
decision problems have led to the design of   wide range of
evaluation metrics  Choi   Cha    Prominent examples include area under the ROC curve  AUC  for imbalanced labels  Menon et al    Fmeasure for information retrieval  Lewis    and precision at the top  Kar
et al      Jasinska et al    To this end  several algorithms have been proposed for optimizing many of
these metrics  primarily focusing on largescale learning 
without   conscious emphasis on statistical consequences
of choosing models and their asymptotic behavior  Kar
et al    Joachims    Wide use of such complex
metrics has also reinvigorated research into their theoretical properties  which can then serve as   guide to prac 

Authors listed in the alphabetical order  Institute of Computing
Science  Poznan University of Technology  Poland  Department
of Computer Science  University of
Illinois at UrbanaChampaign  USA  Microsoft Research  India  Correspondence
to  Wojciech Kot owski  wkotlowski cs put poznan pl 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tice  Koyejo et al      Narasimhan et al      Dembczy nski et al    Waegeman et al    Natarajan
et al   
Complex evaluation metrics for binary classi cation are
best described as set metrics  or nondecomposable metrics   as  in general  the evaluation for   set of predictions
cannot be decomposed into the average of individual instance evaluations  This is in contrast to decomposable
metrics such as accuracy which are de ned as the empirical average of the instance evaluations  This property is
the primary source of dif culty in theoretical analysis  and
interestingly has led to two distinct settings and notions of
consistency  On one hand  Population Utility  PU  focuses
on estimation   so   consistent PU classi er is one which
correctly estimates the population optimal utility as the size
of the training set  equiv  test set  increases  The PU approach has strongest roots in classical statistical analysis
which often deals with asymptotically optimal estimation 
On the other hand  Expected Test Utility  ETU  focuses on
generalization  Thus  the consistent ETU classi er is one
which optimizes the expected prediction error over test sets
of   prede ned size  The ETU approach has strongest
roots in statistical machine learning which prizes generalization as the primary goal  Importantly  these distinctions
are irrelevant when the metric is   linear function of the
confusion matrix       weighted  accuracy and other linear
metrics  To the best of our knowledge  this dichotomy was
 rst explicitly noted by Ye et al    in the context of
Fmeasure  Like in Ye et al    our goal is not to adjudicate the correctness of either approach  but instead to
explore deep connections  and highlight signi cant differences between both approaches for   wide range of metrics 

Contributions  We present   variety of results comparing and contrasting the PU and ETU approaches for consistent classi cation 

  We show that for   wide range of metrics  PU and
ETU are asymptotically equivalent with respect to the
size of the test set  subject to   certain pLipschitzness

 Note that Ye et al    termed the two approaches Empirical Utility Maximization  EUM  and Decision Theoretic Approach  DTA  respectively  We have instead chosen the more descriptive names Population Utility  PU  and Expected Test Utility
 ETU 

Consistency Analysis for Binary Classi cation Revisited

condition which is satis ed by many metrics of interest  This further implies asymptotic equivalence of the
Bayes optimal classi ers  Section   Similar results
were previously only known for Fmeasure 

  We provide lower bounds for the difference between
PU and ETU metrics for  nite test sets  and for certain metrics   thereby highlighting the difference between PU and ETU consistent classi ers with small
test sets  Section  

  We analyze approximate ETU classi cation using low
order Taylor approximations  showing that the approximation can be computed with effectively linear
complexity  yet achieves low error under standard assumptions  Section  

  We consider the effects of model misspeci cation
and  nd that ETU may be more sensitive than PU 
but this may be alleviated by properly calibrating the
estimated probabilities  Section  

In addition  we present experimental results using simulated and real data to evaluate our theoretical claims  Section  

  Preliminaries and Problem Setup
We consider the binary classi cation problem  where the
input is   feature vector        and the output is   label    
    We assume the examples        are generated       
according to           classi er is   mapping        
    We let    denote the indicator function      equal
to one if   is satis ed  and zero otherwise 
Given   distribution   and   binary classi er    de ne 
TP                    TN                   
FP                    FN                   

which are entries of the socalled confusion matrix  namely
true positives  true negatives  false positives and false negatives  In this paper  we are interested in optimizing performance metrics         we use explicit dependence on  
because we will also consider the empirical version of  
that are functions of the above four quantities  However 
since the entries of the confusion matrix are interdependent  it suf ces to only use their three independent combinations  Following Natarajan et al    we parametrize
                       by means of 

       TP   

               and            

As argued by Natarajan et al    any metric being  
function of the confusion matrix can be parameterized in
this way  Table   lists popular examples of such metrics

Metric
Accuracy

De nition

TP   TN

AM

  
Jaccard

GMean

AUC

TP 

TP FN   TN 

TN FP

 TP

 TP FN FP

 cid 

TP

TP FP FN

TP TN

 TP FN TN FP 

FP FN

 TP FN FP TN 

         
              
        

    
  
    
     
     
        

    

         

    

Table   Examples of performance metrics 

with explicit parameterization           Throughout the
paper we assume           is bounded from above and
from below 

  Formal De nitions of PU and ETU
De nition    Population Utility  PU  Given   distribution   and classi er    the PU of   for   performance metric   is de ned as               We let   
PU denote any
maximizer of the PU 

PU   argmax
  

 

               

In words  the PU is obtained by taking the value of metric
  evaluated at the expected confusion matrix of   over   
Thus  one can think of the PU as evaluating the classi er  
on    single test set of in nite size  drawn        from   
In contrast  ETU evaluates the expected utility for    xedsize test set  Formally  given   sample      xi  yi  
   of

size    generated        from    we let cid     cid     cid   denote
  cid 
  cid 
 cid       
 cid     cid     cid   

  xi yi   cid       

the corresponding empirical quantities 

  xi   cid    

of metric   is

  cid 

value

then

and

the

empirical

De nition    Expected Test Utility  ETU  Let    
            xn        be an arbitrary sequence of inputs 
Given   distribution   and   classi er    the ETU of   for
  performance metric   conditioned on   is de ned as 

 
 

 
 

  

 
 

yi 

  

  

 cid 

 cid cid     cid     cid   cid cid 

 

Ey  

                 

 In fact  for essentially all metrics used in practice it holds
 The conditional expectation     is de ned up to   zeromeasure set  over    but this does not create any problems as
we always consider   being sampled from the data distribution 

Consistency Analysis for Binary Classi cation Revisited

where the expectation over                 yn  is with respect to the conditional distribution               over the
examples  We let   
ETU    denote any maximizer of the
ETU 

 cid 

 cid cid     cid     cid   cid cid 

 

ETU      argmax
  

 

Ey  

One can think of ETU as evaluating the classi er   on  in 
 nitely many test sets of size    drawn        from    We
will see  in Section   that the optimal predictions  in both
PU and ETU approaches  can be accurately estimated using the conditional probabilities   yi xi  In practice  we
 rst obtain an estimator of the conditional probability and
then compute the optimal predictions on test data based on
their conditional probability estimates 
Remark   More generally  ETU optimizes the expected
utility Ey  
  However  clearly  it is suf 
 cient to analyze the predictions at any given    Natarajan
et al    as in De nition  

 cid cid     cid     cid   cid cid 
 cid 

  Wellbehaved Performance Metrics

The two frameworks treat the metrics as utility measures
      they are to be maximized  Further  it is reasonable to
expect that        is nondecreasing in true positive and
true negative rates  and indeed  virtually all performance
measures used in practice behave this way  As shown by
Natarajan et al    such monotonicity in true positive
and true negative rates implies another property  called TP
monotonicity  which is better suited to the parameterization
employed here 
De nition    TP monotonicity            is said to be
TP monotonic if for any      and         it holds that
                     

changes in their arguments  This property turns out to be
essential to show equivalence between ETU and PU approaches  On the other hand  if we simply used   standard
de nition of Lipschitz function  with global constants 
it would not be satis ed by many interesting measures 
Hence  we weaken the Lipschitz property by allowing the
constant to vary as   function of    One can also show that
general linearfractional performance metrics studied in
 Koyejo et al      Narasimhan et al    Kot owski
  Dembczy nski    satisfy pLipschitzness under mild
conditions  Appendix   
Proposition   All measures in Table   are pLipschitz 

Proof  We only give   proof for   measure here  See Appendix   for the rest  For ease  let us denote            by
  Let          cid           cid 
   and     cid    cid    cid  by    cid 
           cid  We have 

 cid 

         cid 

         

     cid      cid      cid       

         cid      cid 

     cid      cid      cid       cid   

       

         cid      cid 
   cid 
   cid     cid     

  cid 

     

       
    

   cid     cid   
Since   cid    min   cid    cid  we have
   cid   cid     
   cid   cid   
   cid 
  and thus we can choose Up   Vp   Pp    
    

  cid 

 cid 

 

As for an example of   metric which is not pLipschitz 
consider the precision de ned as              
    Indeed 
if   is close to zero  choosing   cid          cid      and   cid     
gives 

               cid    cid    cid   

 
  

 

It is easy to verify that all measures in Table   are TP monotonic 
  contribution in this work is to develop   notion of regularity for metrics  that helps establish statistical connections
between the two frameworks and their optimal classi ers 
We call it pLipschitzness  de ned next 
De nition    pLipschitzness            is said to be
pLipschitz if 
           cid    cid    cid    Up     cid Vp     cid Pp     cid 
for any feasible            cid    cid    cid  The Lipschitz constants
Up  Vp  Pp are allowed to depend on    in contrast to the
standard Lipschitz functions 

The rationale behind pLipschitzness is that we want to
control the change in value of the measure under small

which can be arbitrarily large for suf ciently small    while
the difference        cid      is small  As it turns out in Section   this pathological behavior of the precision metric
is responsible for   large deviation between PU and ETU 
which suggests that pLipschitzness is in some sense necessary to establish connections 

  Equivalence of PU and ETU
Most of
the existing literature on optimizing nondecomposable classi cation metrics focus on one of the
two approaches in isolation  In this section  we show that
the two approaches are in fact asymptotically equivalent 
for   range of wellbehaved metrics  Informally  given  
distribution   and   performance metric   our  rst result is
that for suf ciently large    the PU of the associated   
ETU
is arbitrarily close to that of   
PU  and likewise  the ETU of
  
PU is arbitrarily close to that of   
ETU  In contrast  we also

Consistency Analysis for Binary Classi cation Revisited

show that the PU and ETU optimal classi ers may suffer
differences for small samples 

  Asymptotic Equivalence

 

The intuition behind the equivalence lies in the observation
that the optimal classi ers under the two approaches exhibit   very simple  similar form  under mild assumptions
on the distribution  Koyejo et al      Narasimhan et al 
    Natarajan et al    Let                 denote the conditional probability of positive class as   function of    The following lemma shows that for any  xed
classi er   that thresholds     and suf ciently large sample size    its performance measured with respect to PU
and ETU are close  in particular  differ by   factor that decays as fast as    
   In fact  the result holds uniformly
over all such binary classi ers 
Lemma   Let                            be
the class of thresholded binary decision functions  Let
  be   performance metric which is pLipschitz  Then 
with probability at least       over   random sample
     xi  yi  
   of size   generated        from    it holds
uniformly over all       

 cid cid cid                 Ey  
 cid cid     cid     cid     cid cid cid cid 
where cid     cid     cid      are empirical quantities evaluated

on    and Lp   max Up  Vp  Pp 
Remark   Lemma   generalizes the result obtained by Ye
et al    for   measure to arbitrary pLipschitz metrics  Furthermore  using more careful bounding technique 
 
we are able to get   better dependence on the sample size
   essentially    
    neglecting logarithmic terms  In
fact  this dependence cannot be improved any further in
general  See Appendix   

log  
 
  

  log      

   Lp

Lp 
 

 

 cid 

 cid 

   Lp

 

 

The uniform convergence result in Lemma   enables the
 rst main result of this work  In particular  the convergence
holds when the optimal classi ers with respect to ETU
PU      and
and PU are of the thresholded form         
ETU        almost surely  with respect to random samh 
ple of inputs    where                           
is the class of threshold functions on function     Several recent results have shown that the optimal classi er for
many popular metrics  including all metrics in Table   indeed has the thresholded form  Narasimhan et al     
Lewis    under   mild condition related to continuity
of the distribution of      See the proof of Theorem   in
Appendix    for details 
Assumption   The random variable     has   density
 with respect to the Lebesgue measure  on    

We are now ready to state the result  Proofs omitted in the
main text are supplied in the Appendix 
Theorem   Let   be   performance metric that is TP
monotonic and pLipschitz  and   be   distribution satisfying Assumption   Consider the ETU optimal classi er
  
ETU  De nition   and the PU optimal classi er   
PU  Definition   Then  for any given   and   we can choose
  large enough  in De nition   of ETU  such that  with
probability at least       over the random choice of the
sample of inputs    we have 
ETU       

Similarly  for large enough    with probability      

 cid cid cid cid     
 cid cid cid Ey  

 cid 

PU      

ETU      cid 
   cid     
 cid cid     
ETU   cid     
 cid 
 cid cid     
PU cid     

PU    cid cid cid cid     
ETU   cid   cid cid 
PU cid   cid cid cid cid cid     

 Ey  

Remark   In essence  Theorem   suggests that  for large
sample sizes  the optimal in the sense of one approach
gives an accurate estimate  or   proxy  of the optimal in
the sense of the other approach  Our characterization of
pLipschitzness is key to showing the equivalence 

  Finite Sample Regime

The aforementioned result is asymptotic  to elucidate the
point  we now give an example where optimal classi ers
corresponding to PU and ETU differ 
It is important to
be aware of such extremities  especially when one applies
  learned model to test data of modest sample sizes  The
way we argue   lower bound is by specifying   metric and
  distribution  such that on   randomly obtained test set of
modest size  say    the gap in the empirical metric computed on the test data for the two optimal classi ers can be
large  As one is typically primarily interested in the empirical metric on   given test set  focusing on the empirical
metric ensures fairness and forbids favoring either de nition 

Example  For some constant       consider the  adjusted  empirical precision metric de ned as 

 Prec cid       cid              cid       
 cid           

Note that  Prec    
    Furthermore  it is pLipschitz 
with Lipschitz constant Vp    
   see De nition   Thus 
choosing very small values of   implies very high Lipschitz
constant  and in turn the metric becomes less  stable  To
establish the desired lower bound  we choose   small    

 

 

Consistency Analysis for Binary Classi cation Revisited

 

 

   cid    Let         denote   partition of the instance
  Xi     and Xi   Xj     for any pair
space           
       Consider the joint distribution   de ned as 
                             
 
            

                 

                 

               

for some    cid        and note that the distribution is de 
 ned to be dependent on our choice of   The last line in
the above set of equations suggests that the distribution has
  small region where labels are deterministically positive
or negative  but overwhelmingly positive elsewhere 
Theorem   Let                    xn  denote   set of
from the distribution    Let    
instances drawn       
               yn  denote their labels drawn from the same
distribution  With probability at least            

 

 Prec cid     
 Prec cid     

ETU   cid     
PU   cid     

ETU         
PU         

 

      

 

  Algorithms  Optimization and Conditional

Probability Estimation

Characterization of the optimal classi er as   thresholding
of the conditional probability yields simple and ef cient
PU consistent estimators  The idea is to  rst obtain an estimator for the conditional probability using training data 
and then search for an optimal threshold on   separate validation set  Narasimhan et al      Koyejo et al     
Threshold search can be ef ciently implemented in linear
time  assuming probabilities are presorted  In contrast 
although   similar thresholding characterization exists for
ETU  Natarajan et al    evaluation and prediction require the computation of an expensive expectation  De nition   For general metrics  there is an      procedure
to determine the optimal test set labeling  Jansche   
Chai    Natarajan et al    and the procedure can
be sped up to      in some special cases  Ye et al   
Natarajan et al    Here  we consider an approximation to ETU that requires only      computation  yet
achieves error      compared to exact optimization 

  Approximation Algorithms

Recall that ETU seeks to  nd the classi er of the form 

 cid cid     cid     cid   cid   

  
ETU      argmax

 

Ey  

Following  Lewis    Natarajan et al    we know
that when   is TP monotonic  it suf ces to sort observations in decreasing order according to     and assign positive labels to top   of them  for                  Unfortunately  for each    we need to calculate the expected utility

 

  Set        cid   

Algorithm   Approximate ETU Consistent Classi er
  Input    and sorted estimates of                      
  Init   

 cid  
   yi   cid       
              cid      
Set cid uk      cid uk  
 cid vk    
Set       cid uk cid vk cid     via Lemmas   or  

  for                   do
 
 
  end for
       arg maxk      
  return           

      for        

 

 

measure  which is time consuming   requiring      in
general  Our goal here is to approximate this term  so that
it can be computed in      time  then the whole procedure
can be implemented in amortized time     
Fix   binary classi er             and the input sample

quantities  as de ned in Section   Furthermore  we de 
 ne semiempirical quantities 

                xn  Let cid     cid     cid   denote the empirical
 cid       
 there is no need to de ne  cid      Note that  cid       
 cid cid     cid  and cid     Ey    cid   

and  cid    

  cid 

  cid 

  xi xi 

Ey  

 xi 

 
 

 
 

  

  

Zerothorder approximation  Our  rst approximation
is based on Taylorexpanding the measure up to the second
order 
Lemma   If   is twicedifferentiable in        and all its
secondorder derivatives are bounded by constant    then 

 cid cid     cid     cid   cid     cid     cid     cid   cid cid     

 cid cid Ey  

 

  

We note that the  rst order terms vanish in the Taylor approximation  proof in Appendix  This constitutes   simple  yet powerful method for approximating ETU utility 
Algorithm   outlines the resulting algorithm  As shown 
the classi er can be computed in      time overall  assuming the data is already sorted according to  xi   otherwise 
the procedure is dominated by sorting time     log   
We note that  Lewis    proposed   similar  rst order
approximation  albeit without any rigorous guarantee 

Second order approximation  Naturally  we can get  
better approximation by Taylorexpanding the measure up
to the third order 
Lemma   Assume   is three times differentiable in       
and assume all its thirdorder derivatives are bounded by
constant    Let  
pp denote the secondorder

uu 

up 

pp  We then have 

derivative terms evaluated at  cid   cid    and likewise de ne
up 
 
 cid cid Ey  
 cid cid     cid     cid   cid     appr   cid cid     
 appr       cid     cid     cid   
uu    

up su    

where 

 

ppsp 

   

 

 

 
 

and

  cid 
  cid 

  

  

sp  

su  

 
  

 
  

 xi     xi 

  xi xi     xi 

Theorem    Consistency  Given   instances    
               xn  sort them in decreasing order of  xi 
For            let      denote the vector with positions
corresponding to top   of the sorted instances set to  
and   otherwise 
    Suppose  rst order derivatives are
bounded by    let 

    Suppose second order derivatives are bounded by   
let 

  
    arg max
    

 appr     

where  appr    is de ned in Lemma   We have 

   

ETU     appr   

        
   

 

As before  the approximation can be computed in      total time  We could also expand the function up to orders
higher than the third order  and get better approximations
 still with      computation if the order of the expansion
is independent of    at the cost of an even more complicated approximation formula  In experiments  we  nd that
on real datasets with test data sets of size   or more  even
the zeroth order approximation is highly accurate 

  Conditional Probability Estimation and Model

Misspeci cation

So far  we assumed that we have access to the true class
conditional density                 and the resulting

  
    arg max
    

ETU     cid     

   

 cid       cid       cid   
  cid       
  cid     

  

 

We have 

Consistency Analysis for Binary Classi cation Revisited

the optimal number of test set observations   to be clas 

classi er is   threshold function on     In practice  one
employs some probability estimation procedure and gets

it were   true conditional probability     to obtain PU or
ETU classi ers  Note that since     is unknown  and we

 cid    which we call   model  Then  one uses  cid    as if
only have access to  cid    the best we can hope for is to
choose the optimal threshold on  cid     for PU  or choose
si ed as positive after sorting them according to cid     for
treat cid    as given and  xed  make no other assumptions
on how it was obtained  Let  cid               cid         
 cid   
from  cid        

ETU  Next  we investigate these  nite sample effects in
practical PU and ETU procedures  For this analysis  we

    denote the class of binary threshold functions on

Consider PU  rst  and let    be the PUoptimal classi er

     argmax
  cid  

             

In practice  however  one does not have access to    and
Instead  given
thus             cannot be computed 
   to

 cid    one uses   validation sample      xi  yi  
choose   threshold on cid     and thus    classi er from  cid   

by directly optimizing the empirical version of the metric
on   

 cid     argmax
  cid  

 cid     cid     cid   

We would like to assess how close is cid   to    By following
 cid 
 cid cid   cid      cid                      cid cid     

the proof of Lemma    which never assumes the class  
is based on thresholding     it is easy to show that with
high probability 

 cid   

 

 

 

not require to know the true distribution in order to select

Thus  if we have   suf ciently large validation sample at
our disposal  we can set the threshold which maximizes
the empirical version of the metric  and our performance
   close to the performance of

is guaranteed to be  cid   
the  optimal classi er from  cid    In other words  PU does
the best classi er in  cid    only   suf ciently large validation
 cid    as   replacement for      which we do not know 
to decide upon label assignments  Let                 xn 
distribution of     and  cid    are continuous on     so
be the input sample of size    Assume for simplicity the
that for any    cid      xi   cid   xj  with probability one  and
 For instance cid    could be obtained from logistic regression

sample is required 
In contrast  ETU procedure is inherently based on using

or neural network with softmax function on the  nal layer 

Consistency Analysis for Binary Classi cation Revisited

 cid     argmax
  cid  

Ey cid   

     argmax
  cid  

Ey   

similarly for cid  Then  given   and cid  the ETU procedure

chooses the classi er of the form 

     by de nition  the optimal classi er in the restricted
class   involves the expectation with respect to the true
  Let us denote  ETU   Ey   
that    maximizes  ETU  In the supplementary material 
we show that under some mild assumptions on  

 cid cid     cid     cid   cid   
Likewise  the optimal ETU classi er in  cid   is given by 
 cid cid     cid     cid   cid   
 cid cid     cid     cid   cid  so
 cid 
 cid   
 cid cid cid ETU cid       ETU   cid cid cid      
  Pp       cid 
Up          cid   
  cid  
where   cid      cid cid   cid  and   cid        cid     cid   cid  are the
estimate cid  Thus  while for the PU procedure  the difference between cid   and    diminishes as   grows  it is not
the case of ETU  as there are two bias terms        cid  and
       cid    which do not depend on    These terms correspond to using incorrect conditional probability cid  while
of ETU procedure to have cid  calibrated with respect to the

quantities corresponding to   and      which were calculated by replacing the conditional probability   with its

selecting the classi er  and are present even if the sample
size tends to in nity  Thus  it seems crucial for the success

essentially solves   quadratic problem known as the Pool
of Adjacent Violators  At test time  we use the learnt  
and the logistic model to estimate     for test instances 

  Experiments
We empirically evaluate the effectiveness and accuracy of
ETU approximations introduced in Section   on synthetic as well as real datasets  We also show on several
benchmark datasets that  by carefully calibrating the conditional probabilities in ETU  we can improve the classi 
cation performance 

  Convergence of Approximations

We consider    and Jaccard metrics from Table   We
sample conditional probabilities    for   instances from the
uniform distribution  The optimal predictions  see De nition   are obtained using Algorithm   of  Natarajan et al 
   which is equivalent to searching over    possible
label vectors  Then we compute the approximate optimal
predictions using the  rst and the second order approximations discussed in Section   For each metric  we measure
the deviation between the true and the approximate optimal
values with increasing sample size in Figure   We observe linear convergence for the  rst order approximation
and quadratic convergence for the second order approximation  This suggests that the bounds in Theorem   indeed
can be improved for some metrics  if not in general 

      

    Jaccard

Figure   Convergence of approximations demonstrated on synthetic data 

  Approximations on Real Data

We report results on seven multiclass and multilabel benchmark datasets    LETTERS    train    test instances    SCENE    train    test   YEAST   
train    test   WEBPAGE    train    test  
IMAGE    train    test   BREAST CANCER   
train    test instances    SPAMBASE    train   
test instances  In case of multiclass datasets  we report results  using onevs all classi ers  averaged over classes  as

 See  Koyejo et al      Ye et al    for details 

Ex

 

  sup

true distribution 
  popular choice for class probability estimation is to use
logistic regression  However  if the model is misspeci ed 
which happens often in practice  the aforementioned discussion suggests that the desired ETU solution may not be
achieved  Therefore  we need to learn the class probability
function more carefully  Here  we consider two variants 
The  rst is to use the Isotron algorithm 
In case of the
generalized linear model                       for some
unknown link function   and model    Kalai   Sastry
  proposed   simple and elegant algorithm  see Appendix    that alternatively learns    and the link function
   approximated by   piecewise linear function  It provably learns the model under certain assumptions on    The
model    and link function   are learned using training
data  and at prediction time  the link function and the scores
of training data       xT
     are used to calibrate the class
probabilities     of test instances 
We also consider using   recalibrated logistic model      
we  rst estimate the class probabilities via standard logistic regression  and recalibrate the probabilities by running
one update of the   function in Isotron algorithm  which

    app      Second order approximationFirst order approximationn Jacc app    Jacc Second order approximationFirst order approximationConsistency Analysis for Binary Classi cation Revisited

DATASET

LETTERS  
SCENE  
YEAST  
WEB PAGE
SPAMBASE
IMAGE
BREAST CANCER

Exact Approx  rst  Approx  second 
  
 
 
 
 
 
 
 

  
 
 
 
 
 
 
 

  
 
 
 
 
 
 
 

Exact Approx  rst  Approx  second 
Jaccard
 
 
 
 
 
 
 

Jaccard
 
 
 
 
 
 
 

Jaccard
 
 
 
 
 
 
 

Table   Comparison of ETU approximation methods     and Jaccard metrics de ned in Table   Reported values correspond to
performance evaluated on heldout data  higher values are better  For multiclass datasets  number of classes indicated in parenthesis 
average performance over classes is reported  Evidently  the ETU approximations are accurate to at least   decimal digits in several
cases 

DATASET

LETTERS  
SCENE  
YEAST  
WEB PAGE
SPAMBASE
IMAGE
BREAST CANCER

Logistic
  
 
 
 
 
 
 
 

Isotron Recalibrated
Logistic    
 
 
 
 
 
 
 

  
 
 
 
 
 
 
 

PU
  
 
 
 
 
 
 
 

Logistic
Jaccard
 
 
 
 
 
 
 

Isotron
Jaccard
 
 
 
 
 
 
 

Recalibrated
Logistic  Jaccard
 
 
 
 
 
 
 

PU
Jaccard
 
 
 
 
 
 
 

Table   Modeling conditional probabilities in ETU  Logistic model vs calibrated model using Isotron     and Jaccard metrics de ned
in Table   Reported values correspond to performance evaluated on heldout data  higher values are better  For multiclass datasets
 number of classes indicated in parenthesis  average performance over classes is reported 

in Natarajan et al   
We compare the exact ETU optimal  computed using the
algorithm of Natarajan et al    with the approximations  The results for    and Jaccard metrics are presented
in Table   The results convincingly show that the approximations are highly accurate  and almost always indistinguishable from optimizing true metrics  on real datasets 
Note that even the  rstorder approximation  in fact  this is
zerothorder  as the  rst order term is zero  see Section  
achieves high accuracy  as the test set sizes are relatively
large 

  Model Misspeci cation

We now study how class probability estimation  CPE  and
model misspeci cation affects the performances of PU
and ETU approaches  on the seven benchmark datasets 
We compare four methods      ETU with logistic regression based CPE      ETU with Isotron based CPE  discussed in Section       ETU with recalibrated logistic regression based CPE  discussed in Section   and
    PU using logistic regression based CPE followed by
threshold tuning on validation set  Koyejo et al     
Additional comparisons to structured SVM  Joachims 
  and other classi ers are available in previously published work by others  Koyejo et al      Natarajan
et al    and are omitted here 

The results are presented in Table   We observe that the
logistic model  column   is insuf cient for many of the
datasets  The results improve in several cases using the
estimated generalized linear model with Isotron  column
  However  there is   confounding factor that the two algorithms are very different  and noticed improvement may
not necessarily be due to better CPE  To isolate this  recalibrated logistic model results are presented in column
  The results are in general much better than the standard logistic model  which suggests that it is indeed the
case of model misspeci cation in these datasets  Finally 
we present the results with PU algorithm in column   We
 nd that the results closely match that of the recalibrated
logistic model  except in the case of SCENE dataset  thus 
correcting for model misspeci cation helps demonstrate
the theorized asymptotic equivalence of PU and ETU approaches in practice 

  Conclusions and Future Work
We have presented new results which elucidate the relationship between the two notions of consistency for complex binary classi cation metrics  Next  we plan to explore surrogates to further improve training ef ciency nondecomposable metrics  We will also extend to more complex prediction problems such as multilabel classi cation 
where   similar dichotomy exists 

Consistency Analysis for Binary Classi cation Revisited

Acknowledgments
   Kot owski has been supported by the Polish National
Science Centre under Grant No     ST 

References
Chai  Kian Ming Adam 

Expectation of Fmeasures 
tractable exact computation and some empirical observations of its properties  In Proceedings of the  th Annual
Intl  ACM SIGIR Conf  on Research and Development in
Information Retrieval  pp    ACM   

Choi  SeungSeok and Cha  SungHyuk    survey of binary similarity and distance measures  Journal of Systemics  Cybernetics and Informatics  pp     

Dembczy nski  Krzysztof  Waegeman  Willem  Cheng 
Weiwei  and   ullermeier  Eyke  On label dependence
and loss minimization in multilabel classi cation  Machine Learning     

Jansche  Martin    maximum expected utility framework
for binary sequence labeling  In Annual Meeting of the
Association of Computational Linguistics  volume  
pp     

Jasinska  Kalina  Dembczynski  Krzysztof  BusaFekete 
  obert  Pfannschmidt  Karlson  Klerx  Timo  and
  ullermeier  Eyke  Extreme fmeasure maximization
In Proceedings of
using sparse probability estimates 
the  nd International Conference on Machine Learning  ICML   New York City  NY  USA  June  
  pp     

Joachims  Thorsten    support vector method for multivariate performance measures  In Proceedings of the  nd
Intl  Conf  on Machine Learning  pp    ACM 
 

Kalai  Adam and Sastry  Ravi  The Isotron algorithm 
Highdimensional isotonic regression  In Conference on
Learning Theory  COLT   

Kar  Purushottam  Narasimhan  Harikrishna  and Jain  Prateek  Online and stochastic gradient methods for nondecomposable loss functions  In Advances in Neural Information Processing Systems  pp     

Kar  Purushottam  Narasimhan  Harikrishna  and Jain  Prateek  Surrogate functions for maximizing precision at
the top  In Proceedings of the  nd International Conference on Machine Learning  ICML  pp   
 

Kot owski  Wojciech and Dembczy nski  Krzysztof  Surrogate regret bounds for generalized classi cation performance metrics  Machine Learning Journal  DOI
     

Koyejo  Oluwasanmi  Natarajan  Nagarajan  Ravikumar 
Pradeep    and Dhillon  Inderjit    Consistent binary
classi cation with generalized performance metrics  In
Neural Information Processing Systems  NIPS     

Koyejo  Oluwasanmi    Natarajan  Nagarajan  Ravikumar 
Pradeep    and Dhillon  Inderjit    Consistent binary
classi cation with generalized performance metrics  In
Advances in Neural Information Processing Systems  pp 
     

Lewis  David    Evaluating and optimizing autonomous
In Proceedings of the  th
text classi cation systems 
Intl  ACM SIGIR Conf  on Research and Development in
Information Retrieval  pp    ACM   

Menon  Aditya  Narasimhan  Harikrishna  Agarwal  Shivani  and Chawla  Sanjay  On the statistical consistency
of algorithms for binary classi cation under class imbalance  In Proceedings of the  th Intl  Conf  on Machine
Learning  pp     

Mohri  Mehryar  Rostamizadeh  Afshin  and Talwalkar 
Ameet  Foundations of Machine Learning  The MIT
Press   

Narasimhan  Harikrishna  Vaish  Rohit  and Agarwal  Shivani  On the statistical consistency of plugin classi ers
for nondecomposable performance measures  In Neural
Information Processing Systems  NIPS     

Narasimhan  Harikrishna  Vaish  Rohit  and Agarwal  Shivani  On the statistical consistency of plugin classi ers
In Adfor nondecomposable performance measures 
vances in Neural Information Processing Systems  pp 
     

Narasimhan  Harikrishna  Ramaswamy  Harish  Saha 
Aadirupa  and Agarwal  Shivani  Consistent multiclass
algorithms for complex performance measures  In Proceedings of the  nd Intl  Conf  on Machine Learning 
pp     

Natarajan  Nagarajan  Koyejo  Oluwasanmi  Ravikumar 
Pradeep  and Dhillon  Inderjit  Optimal classi cation
with multivariate losses  In Proceedings of The  rd International Conference on Machine Learning  pp   
   

Waegeman  Willem  Dembczynski  Krzysztof  Jachnik 
Arkadiusz  Cheng  Weiwei  and   ullermeier  Eyke  On
the bayesoptimality of Fmeasure maximizers  Journal
of Machine Learning Research     

Ye  Nan  Chai  Kian Ming    Lee  Wee Sun  and Chieu 
Hai Leong  Optimizing Fmeasures    tale of two approaches  In Proceedings of the Intl  Conf  on Machine
Learning   

