Algorithmic Stability and Hypothesis Complexity

Tongliang Liu     abor Lugosi       Gergely Neu   Dacheng Tao  

Abstract

We introduce   notion of algorithmic stability of
learning algorithms that we term argument stability that captures stability of the hypothesis
output by the learning algorithm in the normed
space of functions from which hypotheses are selected  The main result of the paper bounds the
generalization error of any learning algorithm in
terms of its argument stability  The bounds are
based on martingale inequalities in the Banach
space to which the hypotheses belong  We apply
the general bounds to bound the performance of
some learning algorithms based on empirical risk
minimization and stochastic gradient descent 

  Introduction
Many efforts have been made to analyze various notions
of algorithmic stability and prove that   broad spectrum of
learning algorithms are stable in some sense  Intuitively 
  learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the
output of the algorithm  and these changes vanish as the
data set grows bigger and bigger  Bonnans   Shapiro 
  For example  Devroye   Wagner   Lugosi
  Pawlak   and Zhang   showed that several
nonparametric learning algorithms are stable  Bousquet  
Elisseeff   proved that  cid  regularized learning algorithms are uniformly stable  Wibisono et al    generalized Bousquet and Elisseeff   results and proved that regularized learning algorithms with strongly convex penalty
functions on bounded domains        cid   regularized learning algorithms for           are also uniformly stable 
 UBTech Sydney AI Institute  School of IT  FEIT  The University of Sydney  Australia  Department of Economics and
Business  Pompeu Fabra University  Barcelona  Spain  ICREA 
Pg  Llus Companys     Barcelona  Spain  Barcelona
Graduate School of Economics  AI group  DTIC  Universitat Pompeu Fabra  Barcelona  Spain 
Correspondence to 
Tongliang Liu  tliang liu gmail com    abor Lugosi  gabor lugosi upf edu  Gergely Neu  gergely neu gmail com 
Dacheng Tao  dacheng tao sydney edu au 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia    JMLR    CP  Copyright
  by the author   

Hardt et al    showed that parametric models trained
by stochastic gradient descent algorithms are uniformly
stable  and Liu et al    proved that tasks in multitask
learning can act as regularizers and that multitask learning
in   very general setting will therefore be uniformly stable
under mild assumptions 
The notion of algorithmic stability has been an important
tool in deriving theoretical guarantees of the generalization
abilities of learning algorithms  Various notions of stability have been introduced and have been exploited to derive generalization bounds  For some examples  Mukherjee
et al    proved that   statistical form of leaveone out
stability is   suf cient and necessary condition for the generalization and learnability of empirical risk minimization
learning algorithms  ShalevShwartz et al    de ned
  weaker notion  the socalled  onaverage replaceone 
example stability  and showed that this condition is both
suf cient and necessary for the generalization and learnability of   general learning setting 
In this paper we study learning algorithms that select   hypothesis         function used for prediction  from   certain
 xed class of functions belonging to   separable Banach
space  We introduce   notion of argument stability which
measures the impact of changing   single training example on the hypothesis selected by the learning algorithm 
This notion of stability is stronger than uniform algorithmic stability of Bousquet   Elisseeff   that is only
concerned about the change in the loss but not the hypothesis itself  However  as we will show  the new notion is still
quite natural and holds for   variety of learning algorithms 
On the other hand  it allows one to exploit martingale inequalities  Boucheron et al    in the Banach space of
the hypotheses  Indeed  the performance bounds we derive
for stable algorithms depend on characteristics related to
the martingale type of the Banach space 
Generalization bounds typically depend on the complexity
of   class of hypotheses that can be chosen by the learning
algorithm  Exploiting the local estimates of the complexity of the prede ned hypothesis class is   promising way
to obtain sharp bounds  Building on martingale inequalities in the Banach space of the hypotheses  we de ne  
subset of the prede ned hypothesis class  whose elements
will  or will have   high probability to  be output by  

Algorithmic Stability and Hypothesis Complexity

learning algorithm  as the algorithmic hypothesis class  and
study the complexity of the algorithmic hypothesis class of
argumentstable learning algorithms  We show that  if the
hypotheses belong to   Hilbert space  the upper bound of
the Rademacher complexity of the algorithmic hypothesis
class will converge at   fast rate of order      where  
is the sample size 
The rest of the paper is organized as follows  Section  
introduces the mathematical framework and the proposed
notion of algorithmic stability  Section   presents the main
results of this study  namely the generalization bounds in
terms of argument stability  Section   specializes the results to some learning algorithms  including empirical risk
minimization and stochastic gradient descent  Section  
concludes the paper 

  Algorithmic Stability and Hypothesis Class
We consider the classical statistical
learning problem 
where the value of   real random variable   is to be predicted based on the observation of an another random variable    Let   be   training sample of          pairs of random variables                     Zn    Xn  Yn  drawn
from    xed distribution   on   set            where
  is the socalled feature space    learning algorithm
             cid  hS     is   mapping from     to  
hypothesis class   that we assume to be   subset of   separable Banach space    cid cid  We focus on linear prediction
problems  that is  when      is   linear functional of    We
write         cid      cid  In other words  we assume that the
feature space   is the algebraic dual of the Banach space
   We denote the norm in   by  cid     cid  The output hS of
the learning algorithm is   hypothesis used for predicting
the value for    
An important special case is when   is   Hilbert space  In
that case we may assume that       and that  cid      cid  is the
inner product in   
The quality of the predictions made by any hypothesis will
be measured by   loss function  cid                where
   denotes the set of positive reals  Speci cally   cid      
measures the loss of predicting an example   using   hypothesis   
The risk of       is de ned by

         cid        

while the empirical risk is

RS     

 
 

  cid 

  

 cid    Zi   

For the output hS of   learning algorithm    the general 

ization error is de ned as

  hS    RS hS   

 

The notion of algorithmic stability was proposed to measure the changes of outputs of   learning algorithm when
the input is changed  Various ways have been introduced to measure algorithmic stability  Here we rethe notion of uniform stability de ned by Bouscall
quet   Elisseeff   for comparison purposes  This
notion of stability relies on the altered sample Si  
            Zi    cid 
   Zi          Zn  the sample   with the
ith example being replaced by an independent copy of Zi 
De nition    Uniform Stability    learning algorithm  
is    uniformly stable with respect to the loss function  cid 
if for all                 

 cid hS        cid hSi            

with probability one  where           

We propose the following  similar  notion that  acts  on the
hypotheses directly  as opposed to the losses 
De nition    Uniform Argument Stability    learning algorithm   is    uniformly argument stable if for all
                

 cid hS   hSi cid         

with probability one  where           

The two notions of stability are closely related  Intuitively 
if the loss  cid       is   suf ciently smooth function of   
then uniform argument stability should imply uniform stability  To make this intuition precise  we de ne the notion
of Lipschitzcontinuous loss functions below 
De nition    LLipschitz Loss Function  The loss function  cid               is LLipschitz for an       if

 cid          cid   cid         cid      cid     cid   cid    cid 

holds for all       and      cid      
Additionally assuming that  cid   cid  is bounded by some
      with probability one  it is easy to see that an    
uniformly argument stable learning algorithm is uniformly
stable with       LB    since

 cid hS   hSi cid   

sup

     cid   cid 

 cid hS    cid     cid hSi    cid   

However  the reverse implication need not necessarily hold
and hence uniform argument stability is   stronger notion 
In the rest of the paper  we will focus on LLipschitz loss
functions and assume that  cid   cid      holds almost surely 

Algorithmic Stability and Hypothesis Complexity

so that

hS   EhS  

  cid 

  

Dt  

We have

 cid 

  

 

 cid Dt cid 
  cid 
  cid 
    cid 

  

  

 

 cid   hS            Zt      hS            Zt cid 

 cid   hS   hSt            Zt cid 

   cid hS   hSt cid            Zt 

Proof  Introduce the martingale differences

Dt     hS            Zt      hS            Zt 

         sup
   

 
 

   cid    Xi cid   

These assumptions are arguably stronger than those made
by Bousquet   Elisseeff   who only require that the
loss function be bounded  In contrast  our results will require that the loss  cid       be Lipschitz in the linear form
 cid      cid  which is only slightly more general than assuming generalized linear loss functions  Nevertheless  these
stronger assumptions will enable us to prove stronger generalization bounds 
The relationship between argument stability and generalization performance hinges on   property of the Banach
space   that is closely related to the martingale type of
the space see Pisier   for   comprehensive account 
For concreteness we assume that the Banach space   is
    smooth  or of martingale type   for some      
This means that for all      cid      

 cid       cid cid     cid       cid cid     cid   cid       cid   cid cid   

 

 

Dt

sup
  

 cid 

 cid 

    exp

 cid cid cid cid cid    cid 

  

 cid cid cid cid cid      
 cid 

Note that Hilbert spaces are    smooth  The property
we need is described in the following result of  Pinelis 
 
Proposition   Let            Dn be   martingale difference
sequence taking values in   separable     smooth Banach space    Then for any      

 cid 
where   is   constant satisfying that cid 

   
   
    cid Dt cid      
 and  cid Dt cid  is the essential supremum of the random variable  cid Dt cid 
Our arguments extend  in   straightforward manner  to
more general Banach spaces whenever exponential tail
inequalities for bounded martingale sequences similar to
Proposition   are available  We stay with the assumption
of     smoothness for convenience and because it applies to the perhaps most important special case when   is
  Hilbert space  We refer to Rakhlin   Sridharan   for
more information of martingale inequalities of this kind 
  key property of stable algorithms  implied by the martingale inequality  is that the hypothesis hS output by the algorithm is concentrated in the Banach space   around
its expectation EhS  This is established in the next simple
lemma 
Lemma   Let the Banach space   be     smooth  If
  learning algorithm   is    uniformly argument stable 
then  for any      

  cid cid hS   EhS cid        cid   log 

 cid           

  

        

Thus  by Proposition   we have

  cid cid hS   EShS cid         cid   log 

 cid     

 cid   

   

 cid 

 

for       exp

  Algorithmic Rademacher Complexity and

Generalization Bound

The concentration result of Lemma   justi es the following de nition of the  algorithmic hypothesis class  since
with high probability hS concentrates around its expectation EhS  what matters in the generalization performance
of the algorithm is the complexity of the ball centered at
EhS and not that of the entire hypothesis class    This observation may lead to signi cantly improved performance
guarantees 
De nition    Algorithmic Hypothesis Class  For   sample
size   and con dence parameter       let             

    cid   log  and de ne the algorithmic hypothe 

sis class of   stable learning algorithm by

Br          cid     EhS cid            

Note that  by Lemma   hS   Br with probability at least
     
We bound the generalization error   in terms of the
Rademacher complexity  Bartlett   Mendelson    of
the algorithmic hypothesis class  The Rademacher complexity of   hypothesis class   on the feature space   is
de ned as

  cid 

  

Algorithmic Stability and Hypothesis Complexity

where              are        Rademacher variables that are
uniformly distributed in    
The next theorem shows how the Rademacher complexity
of the algorithmic hypothesis class can be bounded  The
bound depends on the type of the feature space     Recall
that the Banach space     cid     cid  is of type       if there
exists   constant Cp such that for all            xn      

 cid cid cid cid cid    cid 

  

 

 cid cid cid cid cid 

 cid    cid 

  

 ixi

  Cp

 cid xi cid   

 

 cid  

In the important special case when   is   Hilbert space  the
space is of type   with constant       
Theorem   Assume that   is       smooth Banach
space and that its dual   is of type   
Suppose that
the marginal distribution of the Xi is such that  cid Xi cid   
  with probability one  for some      
If   learning algorithm is    uniformly argument stable  then the
Rademacher complexity of the algorithmic hypothesis class
Br on the feature space satis es

  Br    DCpB cid  log        

In particular  when   is   Hilbert space  the bound simpli 
 es to

  Br      cid  log     

Proof  We have

  Br 

    sup
  Br

 
 

   cid    Xi cid 

The theorem above may be easily used to bound the performance of an    uniformly argument stable learning
algorithm  For simplicity  we state the result for Hilbert
spaces only  The extension to     smooth Banach
spaces with   typep dual is straightforward 
Corollary   Assume that   is   separable Hilbert space 
Suppose that the marginal distribution of the Xi is such
that  cid Xi cid      with probability one  for some      
and that the loss function is bounded and Lipschitz  that is 
 cid           with probability one for some       and
 cid          cid   cid         cid      cid     cid   cid    cid  for all      
and      cid       If   learning algorithm is    uniformly
argument stable  then its generalization error is bounded
as follows  With probability at least      

  hS    RS hS 

   LB cid  log       

 cid 

log 

  

 

Proof  Note  rst that  by Lemma   with probability at least
     

  hS    RS hS    sup
  Br

        RS     

On the other hand  by the boundedness of the loss function 
and the bounded differences inequality  with probability at
least      

        RS   

sup
  Br

    sup
  Br

        RS       

     cid    Br     

log 

  

 

 cid 

 cid 

log 

  

  

  cid 
  cid 
  cid 
  cid 

  

 
 

   cid    Xi cid 

    sup
  Br
 iE cid hS  Xi cid     iE cid hS  Xi cid 

  

    sup
  Br

 
 

   cid    Xi cid      cid hS  Xi cid 

  

 
 

 
 

 iXi

 cid     EhS cid 

    cid     EhS  Xi cid 

    sup
  Br
    sup
  Br

 cid cid cid cid cid    cid 
 cid cid cid cid cid    cid 
     cid   log Cp

 cid cid cid cid cid 
 cid    cid 
  DCpB cid  log        

 cid cid cid cid cid 

   
 

   
 

 iXi

  

  

  

 

 cid Xi cid   

concluding the proof 

where  cid      denotes the set of compositions of functions  cid 
and        By the Lipschitz property of the loss function
and   standard contraction argument       Talagrand Contraction Lemma  Ledoux   Talagrand    we have 

  cid    Br          Br 

  LB cid  log     

 cid  

Note that the order of magnitude of     of many stable
algorithms is of order      For the notion of uniform
stability  such bounds appear in Lugosi   Pawlak  
Bousquet   Elisseeff   Wibisono et al    Hardt
et al    Liu et al    As we will show in the
examples below  many of these learning algorithms even
have uniform argument stability of order      In such
cases the bound of Corollary   is essentially equivalent of

 cid  log 

  

the earlier results cited above  The bound is dominated by
the term  
present by using the bounded differences inequality  Fluctuations of the order of     
are often inevitable  especially when   hS  is not typically
small  When small risk is reasonable to expect  one may
use more advanced concentration inequalities with secondmoment information  at the price of replacing the generalization error by the socalled  deformed  generalization
error   hS   
   RS hS  where       The next theorem
derives such   bound  relying on techniques developed by
Bartlett et al    This result improves essentially on
earlier stabilitybased bounds 
Theorem   Assume that   is   separable Hilbert space 
Suppose that the marginal distribution of the Xi is such
that  cid Xi cid      with probability one  for some      
and that the loss function is bounded and Lipschitz  that is 
 cid           with probability one for some       and
 cid          cid   cid         cid      cid     cid   cid    cid  for all      
and      cid       Let       If   learning algorithm is    
uniformly argument stable  then  with probability at least
     
  hS     
     

RS hS 

   LB cid  log     

        log 

  

 

The proof of Theorem   relies on techniques developed by
Bartlett et al   
In particular  we make use of the
following result 
Proposition    Bartlett et al    Theorem   Let
  be   class of functions that map   into       Assume that there is some       such that for every        
var           Then  with probability at least       we
have

 cid 
 cid 

sup
   

 

Ef        
 

 cid 

        

 cid 

   Xi 

  cid 

  

  log 

 

 

  
 

log 

 

 cid 

 

 cid 

To prove the theorem  we also need to introduce the following auxiliary lemma 
De ne
Gr     
It is evident that Gr    cid          Br          The
following lemma is proven in  Bartlett et al   
Lemma   De ne

max      cid        cid          Br

 cid 

 

 

 cid 

Vr   sup
  Gr

Eg       
 

  Zi 

 

 cid 

  cid 

  

Algorithmic Stability and Hypothesis Complexity

For any       and       if Vr       then every     Br
satis es

  cid          
     

 
 

 cid    Zi    Vr 

  cid 

  

Now  we are ready to prove Theorem  

Proof of Theorem   First  we introduce an inequality to
build the connection between algorithmic stability and hypothesis complexity  According to Lemma   for any      
and       with probability at least       we have
  hS     
     

         
     

RS hS    sup
  Br

RS     

 

Second  we are going to upper bound the term
suph Br          
It
is easy to check that for any     Gr  Eg        and
             Then

   RS    with high probability 

var                 MEg          

Applying Proposition  

 cid 

Vr      Gr   

     log 

 

 

  
 

log 

 

 

 cid 

Let

   Gr   

     log 

 

 

  
 

log 

 

 

 
 

 

We have
          log 

 

   aR Gr   

 
 

 aM log 

 

 

which means that there exists an            log 
 
 aR Gr     
such that Vr        holds  According to Lemma   for any     Br  with probability at
least       we have

 aM log 

 

 

 

  cid 

  

  cid          
     

 
 

  cid 
  cid 

  

   
     

 
 

 
 

   
     
     Gr   

  

 cid    Zi    Vr 

  
 

     log 

 cid    Zi   

 cid    Zi   

 

 

 
 

   log 

 

Algorithmic Stability and Hypothesis Complexity

By exploiting their results  we show that stable RERM algorithms have strong generalization properties 
Theorem   Assume that   is   separable Hilbert space 
Suppose that the marginal distribution of the Xi is such
that  cid Xi cid      with probability one  for some       and
that the loss function is convex in    bounded by   and
LLipschitz  Suppose that for some constants   and      
the penalty function       satis es

 cid  hS   hSi

 cid 

 
 

   log 

 

 

   hS       hSi      
    cid hS   hSi cid 

 

 

It is easy to verify that Gr    cid        Br           
convBr 
By elementary properties of the Rademacher complexity
 see       Bartlett   Mendelson     cid      implies
    cid         Then  with probability at least       we
have

  cid 

 cid 

 cid 

sup
  Br
       log 

  cid          
     
     cid    Br   

 
 

  

 

 cid    Xi 

The proof of Theorem   is complete by combining the
above inequality with inequality   the Talagrand Contraction Lemma  and Theorem  

In the next section  we specialize the above results to some
learning algorithms by proving their uniform argument stability 

  Applications
Various learning algorithms have been proved to possess
some kind of stability  We refer the reader to  Devroye
  Wagner    Lugosi   Pawlak    Bousquet  
Elisseeff    Zhang    Wibisono et al    Hardt
et al    Liu et al    for such examples  including
stochastic gradient descent methods  empirical risk minimization  and nonparametric learning algorithms such as
knearest neighbor rules and kernel regression 

  Empirical Risk Minimization

Regularized empirical risk minimization has been known to
be uniformly stable  Bousquet   Elisseeff    Here we
consider regularized empirical risk minimization  RERM 
algorithms of the following form  The empirical risk  or
the objective function  of RERM is formulated as

RS     

 
 

 cid    Xi          

where            cid             is   convex function  Its
corresponding expected counterpart is de ned as

  cid 

  

Then  for any       and       if hS is the output of
RERM  with probability at least       we have

  hS     
     

   LB

RS hS 

 cid   
 cid  log 

 cid  LB

   

        log 

  

 

 

 cid   

 cid   

Speci cally  when          cid   cid    holds with       and
     
 

   

 

Proof  The proof of Theorem   relies on the following result implied by Wibisono et al   
Proposition   Assume the conditions of Theorem   Then
the RERM learning algorithm is    uniformly stable
with

     

 

 cid      
 cid  kL

   

 cid   
 cid   

 

 

 

and is    uniformly argument stable with

     

   
  and           the
Speci cally  when          cid   cid  
condition   on the penalty function holds with       and
   hr   and   is
     
the index for the dimensionality 

         cid   

   cid 

    where  cid   cid  

 cid    

 

         cid               

Theorem   follows by combining Theorem   and Proposition  

Bousquet   Elisseeff   proved that  cid regularized
learning algorithms are    uniformly stable  Wibisono
et al    extended the result and studied   suf cient
condition of the penalty term       to ensure uniform
   stability  As we now show  both of their proof methods are applicable to the analysis of uniform argument stability 

  Stochastic Gradient Descent

Stochastic gradient descent  SGD  is one of the most
widely used optimization methods in machine learning 
Hardt et al    showed that parametric models trained
by SGD methods are uniformly stable  Their results apply to both convex and nonconvex learning problems and

Algorithmic Stability and Hypothesis Complexity

provide insights for why SGD performs well in practice  in
particular  for deep learning algorithms 
Their results are based on the assumptions that the loss
function employed is both Lipschitz and smooth 
In order to avoid technicalities of de ning derivatives in general
Hilbert spaces  in this section we assume that        
Rd  the ddimensional Euclidean space 
De nition    Smooth    differentiable loss function
 cid    is ssmooth if for all      cid       we have
 cid   cid         cid cid   cid cid      cid       cid cid 

where  xf     denotes the derivative of       with respect
to   and      
De nition    Strongly Convex    differentiable loss function  cid    is  strongly convex with respect to  cid     cid  if for
all      cid       we have

   cid         cid cid   cid          cid     cid       cid cid 

where      

Theorem   is applicable to the results of SGD when the
general loss function  cid       is LLipschitz  ssmooth  and
  is linear with respect to    Note that our de nition of LLipschitzness requires the loss function to be Lipschitz in
the linear form  cid      cid 
Theorem   Let the stochastic gradient update rule be
given by ht    ht        cid ht  Xit  where        is
the learning rate and it is the index for choosing one example for the tth update  Let hT and hi
  denote the outputs
of SGD run on sample   and Si  respectively  Assume that
 cid   cid      with probability one  Suppose that the loss
function is LLipschitz  ssmooth  and upper bounded by
   Let SGD is run with   monotonically nonincreasing
step size           where   is   universal constant  for  
steps  Then  for any       and       with probability at
least       we have

RS hT  

  hT      
     
   BL
     sc
     

 cBL 

 

sc   

sc cid  log 

sc

 

        log 

  

 

When the loss function  cid  is convex  Ladmissible  ssmooth 
and upper bounded by    suppose that SGD is run with
step sizes         for   steps  Then  for any       and
      with probability at least      

  hT      
     
       

 

  cid 

  

RS hT  

 cid  log 

  

 

        log 

  

 

Moreover  when the loss function  cid  is  strongly convex  ssmooth  and upper bounded by    let the stochastic gradient update be given by ht     ht        cid ht  Xit 
where   is   compact  convex set over which we wish to
optimize and   is   projection such that       
arg minh    cid       cid 
If the loss function is further LLipschitz over the set   and the projected SGD is run with
  constant step size        for   steps  Then  for any
      and       with probability at least       the
projected SGD satis es that

  hT      
     
   DB   

RS hT  

 cid  log   

  

        log 

  

 

Note that any  cid  regularized convex loss function is
strongly convex  Bousquet   Elisseeff   studied the
stability of batch methods  When the loss function is
strongly convex  the stability of SGD is consistent with the
result in  Bousquet   Elisseeff   
While the above result only applies to LLipschitz loss
functions as de ned in De nition   it does explain some
generalization properties of layerwise training of neural
networks by stochastic gradient descent 
In this oncecommon training scheme  see       Bengio et al   
one freezes the parameters of the network before after  
certain layer and performs SGD for this single layer  It is
easy to see that  as long as the activation function and the
loss function  connected with the network  are Lipschitzcontinuous in their inputs  the overall loss can easily satisfy the continuous conditions of Theorem   This implies
that the parameters in each layer may generalize well in  
certain sense if SGD is employed with an early stop 
The proof of Theorem   follows immediately from Theorem   combined with the following result implied by Hardt
et al     which is   collection of the results of Theorems     and   therein 
Proposition   Let the stochastic gradient update be given
by ht    ht     cid ht  Zit  where        is the learning rate and it is the index for choosing one example for
the tth update  Let hT and hi
  denote the outputs of SGD
running on sample   and Si respectively  When the loss
function is LLipschitz and ssmooth  suppose that SGD is
run with monotonically nonincreasing step size          
where   is   universal constant  for   steps  Then 

 cid hT   hi

  cid         sc
     

 cBL 

 

sc   

sc

sc   

When the loss function  cid  is convex  LLipschitz  and ssmooth  suppose that SGD is run with step sizes        

Algorithmic Stability and Hypothesis Complexity

for   steps  Then 

 cid hT   hi

  cid     BL
 

  cid 

  

   

Moreover  when the loss function  cid  is  strongly convex and
ssmooth  let the stochastic gradient update be given by
ht     ht        cid ht  Zit  where   is   compact 
convex set over which we wish to optimize and   is
  projection such that        arg minh    cid       cid  If
the loss function is LLipschitz over the set   and the projected SGD is run with constant step size        for  
steps  Then  the projected SGD satis es algorithmic argument stability with

 cid hT   hi

  cid     BL
  

 

  Conclusion
We introduced the concepts of uniform argument stability
and algorithmic hypothesis class  de ned as the class of
hypotheses that are likely to be output by the learning algorithm  We proposed   general probabilistic framework
to exploit local estimates for the complexity of hypothesis
class to obtain fast convergence rates for stable learning algorithms  Speci cally  we de ned the algorithmic hypothesis class by observing that the output of stable learning algorithms concentrates around EhS  The Rademacher complexity de ned on the algorithmic hypothesis class then
converges at the same rate as that of the uniform argument
stability in Hilbert space  which are of order      for
various learning algorithms  such as empirical risk minimization and stochastic gradient descent  We derived fast
convergence rates of order      for their deformed generalization errors  Unlike previously published guarantees
of similar  avor  our bounds hold with high probability 
rather than only in expectation 
Our study leaves some open problems and allows several
possible extensions  First  the algorithmic hypothesis class
de ned in this study depends mainly on the property of
learning algorithms but little on the data distribution 
It
would be interesting to investigate   way to de ne an algorithmic hypothesis class by considering both the algorithmic property and the data distribution  Second  it would be
interesting to explore if there are some algorithmic properties other than stability that could result in   small algorithmic hypothesis class 

Acknowledgments
Liu and Tao were partially supported by Australian Research Council Projects FT  DP 
LP  Lugosi was partially supported by the

Spanish Ministry of Economy and Competitiveness  Grant
MTM    and FEDER  EU  Neu was partially
supported by the UPFellows Fellowship  Marie Curie COFUND program  

References
Bartlett  Peter   and Mendelson  Shahar  Rademacher and
Gaussian complexities  Risk bounds and structural results  Journal of Machine Learning Research   
   

Bartlett  Peter    Bousquet  Olivier  and Mendelson  Shahar  Local Rademacher complexities  Annals of Statistics  pp     

Bengio  Yoshua  Lamblin  Pascal  Popovici  Dan  and
Larochelle  Hugo  Greedy layerwise training of deep
networks  In NIPS   

Bonnans    Fr ed eric and Shapiro  Alexander  Perturbation
analysis of optimization problems  Springer Science  
Business Media   

Boucheron  St ephane  Lugosi    abor  and Massart  Pascal 
Concentration inequalities    nonasymptotic theory of
independence  OUP Oxford   

Bousquet  Olivier and Elisseeff  Andr    Stability and generalization  Journal of Machine Learning Research   
   

Devroye  Luc and Wagner  Terry    Distributionfree inequalities for the deleted and holdout error estimates 
IEEE Transactions on Information Theory   
   

Hardt  Moritz  Recht  Benjamin  and Singer  Yoram  Train
faster  generalize better  Stability of stochastic gradient
descent  arXiv preprint arXiv   

Ledoux  Michel and Talagrand  Michel  Probability in Banach spaces  Isoperimetry and processes  Springer Science   Business Media   

Liu  Tongliang  Tao  Dacheng  Song  Mingli  and Maybank  Stephen    Algorithmdependent generalization
IEEE Transactions on
bounds for multitask learning 
Pattern Analysis and Machine Intelligence   
  February  

Lugosi    abor and Pawlak  Miroslaw  On the posteriorprobability estimate of the error rate of nonparametric
classi cation rules  IEEE Transactions on Information
Theory     

Mukherjee  Sayan  Niyogi  Partha  Poggio  Tomaso  and
Rifkin  Ryan  Learning theory  stability is suf cient for

Algorithmic Stability and Hypothesis Complexity

generalization and necessary and suf cient for consistency of empirical risk minimization  Advances in Computational Mathematics     

Pinelis  Iosif  Optimum bounds for the distributions of martingales in Banach spaces  The Annals of Probability  pp 
   

Pisier  Gilles  Martingales in Banach spaces  in connection

with type and cotype  IHP course notes   

Rakhlin  Alexander and Sridharan  Karthik  On equivalence of martingale tail bounds and deterministic regret
inequalities  arXiv preprint arXiv   

ShalevShwartz  Shai  Shamir  Ohad  Srebro  Nathan  and
Sridharan  Karthik  Learnability  stability and uniform
convergence  Journal of Machine Learning Research 
   

Wibisono  Andre  Rosasco  Lorenzo  and Poggio  Tomaso 
Suf cient conditions for uniform stability of regularization algorithms  Techincal Report MITCSAIL TR 
   

Zhang  Tong  Leaveone out bounds for kernel methods 

Neural Computation     

