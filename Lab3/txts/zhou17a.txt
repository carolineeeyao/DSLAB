Stochastic Adaptive QuasiNewton Methods for Minimizing Expected Values

Chaoxu Zhou     Wenbo Gao     Donald Goldfarb  

Abstract

We propose   novel class of stochastic  adaptive
methods for minimizing selfconcordant functions which can be expressed as an expected
value  These methods generate an estimate of
the true objective function by taking the empirical mean over   sample drawn at each step 
making the problem tractable  The use of adaptive step sizes eliminates the need for the user
to supply   step size  Methods in this class include extensions of gradient descent  GD  and
BFGS  We show that  given   suitable amount of
sampling  the stochastic adaptive GD attains linear convergence in expectation  and with further
sampling  the stochastic adaptive BFGS attains
Rsuperlinear convergence  We present experiments showing that these methods compare favorably to SGD 

  Introduction
We are concerned with minimizing functions of the form

                 

min
  Rn

 

Many common problems in statistics and machine learning
can be put into this form  For instance  in the empirical
risk minimization framework    model is learned from   set
            ym  of training data by minimizing an empirical
loss function of the form

  cid 

  

min

 

      

 
 

      yi 

 

It is easy to see that this formulation is equivalent to taking
  to be the uniform distribution on the points             ym 
An objective function of the form   is often impractical 
as the distribution of   is generally unavailable  making it

 Equal contribution  Dept  of Industrial Engineering and Operations Research  Columbia University  Correspondence to 
Chaoxu Zhou  cz columbia edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

infeasible to analytically compute Ef       This can be
resolved by replacing the expectation Ef       by the estimate   The strong law of large numbers implies that
the sample mean      converges almost surely to       as
the number of samples   increases  provided that the samples yi are drawn independently from the distribution of  
However  even the concrete problem   is not   good target
for classical optimization algorithms  as the amount of data
  is frequently extremely large    better strategy when optimizing   is to consider subsamples of the data to reduce
the computational cost  This leads to stochastic algorithms
where the objective function changes at each iteration by
randomly selecting subsamples 
Many stochastic algorithms have been proposed which use
this approach for solving   notably stochastic gradient descent  SGD   Bottou    and variancereduced
extensions of SGD  such as SVRG  Johnson   Zhang 
  SAG  Schmidt et al    and SAGA  Defazio
et al    These methods are  rstorder methods  extending gradient descent to the stochastic setting  and the
latter three  variancereduced  methods can be shown to
converge linearly for strongly convex objectives  Linearly convergent stochastic Limited Memory BFGS algorithms  Byrd et al   Moritz et al   Gower et al 
  have also been proposed  It is then natural to consider stochastic extensions of quasiNewton and secondorder methods  One such method  the Newton Incremental Method  NIM   Rodomanov   Kropotov    combines cyclic updating of    xed collection of functions
                       ym  with Newton   method  and attains
local superlinear convergence 
One of the key obstacles to developing stochastic extensions of quasiNewton methods is the necessity of selecting appropriate step sizes  The analysis of the global convergence of the BFGS method  Powell    and other
members of Broyden   convex class  Byrd et al   
assumes that ArmijoWolfe inexact line search is used 
This is rather undesirable for   stochastic algorithm  as
line search is both computationally expensive and dif 
cult to analyze in   probabilistic setting  However  there
is   special class of functions  the selfconcordant functions  whose properties allow us to compute an adaptive
step size and thereby avoid performing line searches 
In
 Gao   Goldfarb    it is shown that the BFGS method

Stochastic Adaptive QuasiNewton Methods

 Broyden     Fletcher   Goldfarb   Shanno 
  with adaptive step sizes converges superlinearly
when applied to selfconcordant functions 
In this paper  our goal is to develop   stochastic quasiNewton algorithm for selfconcordant functions  We propose an iterative method of the following form  At the kth
iteration  we draw mk       samples            mk  De ne the
empirical objective function at the kth iteration to be

mk cid 

  

Fk     

 
mk

         

 

Let Hk be   positive de nite matrix  The next step direction
is given by

dk    Hk Fk xk 

and the step size by

tk  

  

        

where

    

    

 Fk xk   Hk Fk xk 

 cid Fk xk   Hk Fk xk Hk Fk xk 

 
 

 

The motivation for this step size is described in Section  
  key feature of these methods is that the step size tk can
be computed analytically  using only local information  and
adapts itself to the local curvature     xed step size   is
typically used in variancereduced  rstorder methods  and
this step size must be determined experimentally  The theoretical analysis that has been provided for these methods is
of little help in choosing   as often   is constrained to be
impractically small  and moreover  is related to unknown
constants such as the Lipschitz parameter of the gradient 
Furthermore     xed   which was effective in one regime
may become ineffective as the algorithm progresses  and
enters regions of varying curvature 
Our new methods are also capable of solving general problems of the form   This is in contrast to popular
incrementaltype methods such as SAG  SAGA  and NIM 
which  because of their stored updating scheme  can only
be applied to problems of the form   with    xed data
set             ym  This opens up new avenues for the solutions of problems where new data can be sampled as the
algorithm progresses  as opposed to having    xed training
set throughout  In this respect  our new method is akin to
Streaming SVRG  Frostig et al    which is also able
to solve   with similar conditions on the growth of the
samples mk 
By choosing the matrices Hk appropriately  we obtain
stochastic extensions of classical methods 
In particular 
two choices of Hk will be of interest 

  Taking Hk     yields the stochastic adaptive gradient

descent method  SAGD 

  Fixing    and then taking Hk  to be the BFGS
update of Hk  yields the stochastic adaptive BFGS
method  SABFGS 

Our methods can be proven to converge under   suitable
sampling regime  When the number of samples mk is  xed 
and large enough  both SAGD and SABFGS converge Rlinearly in expectation to an  optimal solution  To obtain
Rsuperlinear convergence  the number of samples must
be allowed to grow rapidly  This principle  of increasing
the number of samples to match the desired rate of convergence  was previously applied to Rlinear convergence in
 Byrd et al    The SABFGS algorithm can be shown
to converge Rsuperlinearly almost surely to the true optimal solution if the number of samples is increased so that
  
This paper is organized as follows  In Section   we introduce our notation  and the technical assumptions needed
for the analysis of our methods  In Section   we describe
the relevant results from stochastic analysis which motivate our algorithms  In Section   we brie   describe the
required theory of selfconcordant functions  In Section  
we formally de ne stochastic adaptive methods  and state
the convergence results  In Section   we present preliminary numerical experiments  and conclude with   discussion in Section   Full proofs of the convergence theorems
can be found in the supplementary materials 

converges Rsuperlinearly to  

 

  Assumptions and Notation
The number of variables is    We write              
and               and gk       Fk    and Gk     
 Fk    for the gradient and Hessian of the empirical obIn the context of   sequence of iterates
jective function 
 xk 
   generated by an algorithm  we also write gk with
no argument to denote gk xk  and Gk for Gk xk  In the
context of BFGS  Hk denotes the approximation of the inverse Hessian  and Bk     
    The step direction  generally  Hkgk  is denoted dk  and the step size by tk  so the
actual step taken is sk   tkdk 
The optimal solution of min
  Rn
optimal solution of the empirical problem min
  Rn
denoted   
Unless otherwise speci ed  the norm  cid     cid  is the  norm  or
the operator  norm  The Frobenius norm is indicated as
 cid     cid    
We make the following technical assumptions on       and
        We will explain the motivation for these assumptions at the relevant points in the discussion 

      is denoted    and the
Fk    is

  is   random variable 

   Note that   

Stochastic Adaptive QuasiNewton Methods

Assumptions 
  There exist constants      cid      such that for every
    Rn and every realization of   the Hessian of  
with respect to   satis es
 cid    cid   

xf        cid  LI

That is          is strongly convex for all   with the
eigenvalues of  
xf       bounded by  cid  and    The
condition number  denoted   is given by   cid 

  Fk    is standard selfconcordant for every possible

sampling            mk 

  There exist compact sets    and   with        and
        such that if    is chosen in    then for
all possible realizations of the samples            mk for
every    the sequence of iterates  xk 
   produced
by the algorithm is contained within    We use    
sup cid       cid              for the diameter of   
Furthermore  we assume that the objective values and
gradients are bounded 

    sup

 

    inf
 

    sup

 

           
sup
   
               
inf
sup
   

 cid        cid     

   For BFGS only  The Hessian      is Lipschitz con 

tinuous with constant LH 

Note that Assumption   is standard when analyzing
stochastic algorithms  The function         is commonly
  loss function  and either         is itself strongly convex 
or each         is weakly convex and the strong convexity is ensured by adding   quadratic regularization term to
     

  Stochastic Framework
Our analysis is based on   uniform convergence law   
standard technique in learning theory and empirical processes  The central idea is that Fk    is an empirical
mean estimating       and we can closely control the error  Fk            over all       by varying the sample
size mk  The relevant stochastic analysis can be found in
    van der Vaart   Wellner    and  Goldfarb et al 
  These powerful techniques are required to analyze
secondorder stochastic methods  since inverting the product of   sampled Hessian and sampled gradient generates
both dependence and nonlinearity 
In comparison   rstorder stochastic methods can be analyzed by evaluating the
expected value and variance of the sampled gradient 
It

is also useful to compare this approach with that used in
 Byrd et al    and  Friedlander   Goh    where
the variance of the gradient is controlled by pointwise estimates or pointwise tail bounds 
Theorem    Corollary    Goldfarb et al    Let
      and         min     

    Then
 Fk                  exp

 cid 

  sup
   

where       nn Dn    If mk     then

 cid 
  mk       
       
 cid  log mk
 cid  log mk
 cid cid 

mk

mk

 Fk               

  sup
   
  Fk   

              
 cid 

 cid 

where   is   constant  depending only on     given by
 

     nn Dn exp

  

     

log

     

     
 
  
 

Assumption   is required for the uniform law of Theorem   to hold  The set    is assumed to be   region
where  if    is chosen within    the path of the stochastic
algorithm will remain within the larger set    For practical
purposes  we may take   to be an arbitrarily large bounded
set in order to ensure convergence  though this worsens the
complexity bounds provided by Theorem   We note that
the constant   is very much    worstcase  bound  and for
almost every problem arising in practice  the expected difference will be much smaller than Theorem   suggests 
Rather  the crucial implication is that the expected difference diminishes at the rate
  allowing   sharp level
of control by adjusting mk 

 cid  log mk

mk

  SelfConcordant Functions and Adaptive

Methods

  convex function     Rn     is selfconcordant if there
exists   constant   such that for every     Rn and every
    Rn  we have

                            

If         is standard selfconcordant  Selfconcordant
functions were introduced by Nesterov and Nemirovski
in the context of interior point methods  Nesterov   Nemirovski   
Many common problems have selfconcordant formulations  At least one method  the DiSCO algorithm of Zhang
and Xiao   is tailored for distributed selfconcordant
optimization and has been applied to many regression problems  Convex quadratic objective functions have third

Stochastic Adaptive QuasiNewton Methods

derivatives equal to zero  and are therefore trivially standard selfconcordant 
In particular  least squares regression is selfconcordant  In  Zhang   Xiao    it is also
shown that regularized regression  with either logistic loss
or hinge loss  is selfconcordant 
For selfconcordant functions  the notion of   local norm
is especially useful  Given   convex function    the local
norm with respect to   at the point   is given by

 cid 

 cid   cid    

hT       

Consider an iterative method for minimizing selfconcordant functions with steps given as follows  On the kth step  the step direction dk is given by dk    Hk    xk 
for some positive de nite matrix Hk  and the step size is
  where       cid dk cid xk and     
given by tk     
gT
  Hkgk

    

 

 
 

Methods of this type have been analyzed in  TranDinh
et al    and  Gao   Goldfarb    In  Gao   Goldfarb    the above choice of    is shown to guarantees
  decrease in the function value 
Theorem    Lemma    Gao   Goldfarb    Let
         xk   Hk    xk  If    is chosen to be        
 
then

 
 

   xk   tkdk       xk       

where        
  

and the function           log      

We make Assumption   in order to apply Theorem   to
the empirical objective functions Fk      natural question
is whether Assumption   can be relaxed to the assumption
that         is selfconcordant for all   and       is standard selfconcordant  In the case where   is  nitely supported  it is possible to scale       so that we may use this
weaker assumption 
Lemma   Suppose that   is  nitely supported on
            am  with pi        ai  Suppose that       ai 
is selfconcordant with constant     Let      
  Then
 
the scaled function                   is standard selfconcordant  and every empirical objective function       
is standard selfconcordant 

max  
 
min pi

 cid  

 
 

since  cid  

Proof  Observe that        ai pi is selfconcordant with
       We deduce that EF      
constant   
          ai pi is standard selfconcordant  Furtheri  pi     we have min pi  
more 
   Thus   
         ai  is selfconcordant with constant
 
         which implies that        is standard
selfconcordant for every possible       

However  if we do not assume that each         is selfconcordant  or if   is not compactly supported  it is un 

Algorithm   SAGD

Input                 
for                 do

Sample            mk       from  
Compute 

 cid 

    

    

gk    Fk xk  dk    gk 

gT
  Gk xk gk 

tk  

  

        

 

gT
  gk
 
 

 

 cid mk

where Fk       
mk
Set xk    xk   tkgk 

            

end for

clear whether every possible Fk    will be standard selfconcordant  even when       is standard selfconcordant 
Thus  we impose Assumption   in our analysis  while observing that it is unnecessary in the practical case where  
is derived from    nite data set 

  Stochastic Adaptive Methods
Our basic approach is to sample an empirical objective
function Fk    at each step  and then compute the step direction and adaptive step size   using Fk  For particular
choices of the matrices Hk  we recover analogues of classical methods such as gradient descent  LBFGS  and BFGS 

  Stochastic Adaptive GD

When Hk     for every    the resulting method is stochastic adaptive gradient descent  SAGD  which is given as
Algorithm   The number of samples drawn at each iteration is left as an input to the algorithm  and  weak  bounds
on the required number mk can be inferred from the convergence analysis 
Theorem   Let       Suppose that at each iteration 

we draw         samples  We may choose      cid   log  
and      cid    log   so that the SAGD method conHere   cid    absorbs constants depending only on     namely

verges in expectation to an  optimal solution after   steps 

  and  

  more precise quantitative form of Theorem   and its
proof  appear as Theorem    in the supplementary materials  The argument can be sketched as follows  Theorem   implies that the adaptive step size tk produces
  linear decrease towards the optimal value of Fk  that is 
Fk xk Fk   
   Here   is   deterministic constant depending only on  cid     and   By iterating  we can bound the true gap    xk       in terms

       Fk xk Fk   

Stochastic Adaptive QuasiNewton Methods

Algorithm   SABFGS

Input                          
for                 do

Sample            mk       from  
Compute

gk    Fk xk  dk    Hkgk

dT
  Gk xk dk 

 cid 

    

    

tk  

  

        

 

gT
  Hkgk

 
 

 

 cid mk

            

where Fk       
Set gk     Fk xk   tkdk 
mk
Set yk   gk    gk
if gT

  dk then

  dk    gT
Set dk   gk
Recompute         tk
Set Hk    Hk
Set Hk         skyT

else

end if
Set xk    xk   tkdk 

end for

 Hk     yksT

 
sT
  yk

    sksT
 
sT
  yk

 
sT
  yk

product  Hkgk  For   very large  it is infeasible to store
Hk  and we can instead store the pairs  sk  yk  and compute
 Hkgk using   twoloop recursion  Nocedal    This
corresponds to stochastic adaptive LBFGS  SALBFGS 
if we limit the number of past pairs  sk  yk  to only the  
most recent  and to SABFGS if we store everything  The
amount of storage used by SALBFGS surpasses that of
SABFGS as   approaches   
Theorem   holds for SALBFGS  since Theorem   
holds for any method where  Hk  has uniformly bounded
eigenvalues  Thus  SALBFGS also converges in expecta 

tion to an  optimal solution after      cid   log   steps
given samples of size      cid    log   though now

the constants within the bigO are also dependent on   
As with SAGD  one can obtain an  optimal solution in
expectation from SABFGS with    xed amount of sampling 
Theorem   Let       Suppose that at each iteration 

we draw         samples  We may choose      cid   log  
and      cid   log   so that the SABFGS method
steps  Here   cid    absorbs constants depending only on    

converges in expectation to an  optimal solution after  

namely   and  

of rk            
 Fk             
for            The  rst term can be made smaller than   by
taking   to be   log   and the second can be bounded
with Theorem  

  and rj sup
   

  Stochastic Adaptive BFGS

By updating Hk using the BFGS formula  we obtain the
stochastic adaptive BFGS  SABFGS  method  which is
given in Algorithm  
In Algorithm   we use the standard BFGS update with
yk   gk xk    gk xk  Another option is to replace yk
with the action of the Hessian on sk  so yk   Gk xk sk 
In general  we must compute Gk xk dk when  nding the
adaptive step size  so we can reuse the result of that computation instead of computing an extra gradient gk xk 
For technical reasons  our SABFGS procedure tests
whether the Wolfe condition is satis ed for the adaptive
step size tk  If not  we revert to taking   SAGD step  This
is an artifact of our analysis  and under suitable conditions
on the growth of the samples mk  there will be some point
after which the Wolfe condition is necessarily satis ed on
every step  In practice  this test can be omitted 
There are also two possible ways to implement SABFGS 
For problems with   at most mediumsized  it is possible to
explicitly store the matrix Hk  and compute dk by   matrix

This theorem follows from Lemma    in the supplementary materials  Note that the complexity bound is in fact
weaker than that of SAGD  This occurs because the initial matrices            Hk may be poor approximations of
       in which case the BFGS directions may perform poorly  We have little theoretical control over Hk except in the limit  and this is re ected in the weaker iteration
bounds for  optimality 
For convergence to the true optimal solution    it is necessary for the number of samples mk to increase over time 
In fact  by increasing the number of samples appropriately 
the SABFGS algorithm can be shown to converge to   
Rsuperlinearly with probability  
Theorem   Suppose that we draw mk samples on the kth step  where   
converges Rsuperlinearly to   Then
the SABFGS algorithm converges Rsuperlinearly to the
optimal solution    almost surely 

 

The complete proof is left to Lemma    in the supplementary materials  We brie   sketch it here 
Taking mk to be an increasing sequence  we can guarantee
  converges to   From basic
that the sequence        
results on the adaptive step size  this implies that tk eventually satis es the ArmijoWolfe conditions  and therefore
the algorithm eventually uses the BFGS direction  and performs   BFGS update on Hk  at every step  Our test for
the Wolfe condition ensures that in every iteration  we either can control the progress by analyzing the BFGS up 

  

Stochastic Adaptive QuasiNewton Methods

date Hk  or by using our earlier results on SAGD steps 
Together  these facts imply that SABFGS converges Rlinearly almost surely  and consequently  the sum of dis 

    cid xk     cid  is  nite almost surely 

tances cid 

Following the classical argument  we analyze the evolution
of Hk  to show that the steps taken by SABFGS converge to the Newton step  This is precisely the wellknown
DennisMor   condition  Dennis Jr    Mor      for Qsuperlinear convergence  however  in the stochastic setting 
we have an additional error term resulting from the variance in the sampling of the gradients and Hessians  We
apply Theorem   to show that gk    and      rapidly
converge to the true gradient and Hessian  and thus SABFGS attains Rsuperlinear convergence 
Increasing mk at this rate is clearly infeasible in reality 
and it is nontrivial to determine   level of sampling which
produces  iterationwise  superlinear convergence in   reasonable amount of real time  We note  for comparison  that
the Streaming SVRG method  Frostig et al    requires
sample sizes for the gradient that grow at   geometric rate 
and that the dynamic batch method  Byrd et al    obtains Rlinear convergence in expectation by increasing the
sample sizes at   geometric rate    superlinear increase in
the number of samples may be unavoidable as   condition
for superlinear convergence  given the statistical error of
the sample 
In practice  we are often uninterested in  nding the true
minimizer  and instead aim to quickly  nd an approximate
solution  The theoretical Rsuperlinear rate of SABFGS
suggests that SABFGS or SALBFGS may offer practical
speedups over  rstorder stochastic methods  even without
using substantial sampling  One intuitive heuristic would
be to draw fewer samples in the early phase of the algorithm  when computing the gradient with high accuracy is
less valuable  and then increasing the number of samples as
the solutions approach optimality to reduce  uctuation 

  Numerical Experiments
We compared several implementations of SAGD and SABFGS against the original SGD method and the robust
SGD method  Nemirovski et al    on   penalized least
squares problem with random design  That is  the objective
function has the form

                     

min
  Rp

 cid   cid 

 

 
 

where     Rp and       are random variables with  nite
second moments  We base our model on   standard linear
regression problem with Gaussian errors  so              
for   deterministic vector     Rp and           is  
noise component    was drawn according to   multivariate
      where        Ip       here   is the

allones matrix  By varying   we control the condition
number of the expected Hessian  We tested problems of
size       and      
The following eight algorithms were tested 

SGD  SGD with  xed mk     and diminishing step sizes
   for problems with       and tk  

 

tk  
   for      
 

SAGD  SAGD with  xed mk     

SAGD    SAGD with increasing samples mk    

     

   

SABFGS  SABFGS with  xed mk      We do not
test for the Wolfe condition at each step  and instead
always take the adaptive BFGS step and perform  
BFGS update 

SABFGS    SABFGS with increasing samples mk  
          We do not test for the Wolfe condition at
 
each step  and instead always take the adaptive BFGS
step and perform   BFGS update 

SABFGS GD  SABFGS with increasing samples mk  
       We test for the Wolfe condition and switch
 
to taking   SAGD step when the adaptive step tk for
SABFGS fails the test 

RS GDC  Robust SGD with constant step size tk  
 
  where   is the predetermined number of steps 
 
At the kth iteration  output the average of the previous    solutions 

RS GDV  Robust SGD with diminishing step size tk  
  At the kth iteration  output the average of the

 
 
previous    solutions 

We also tested the Streaming SVRG algorithm with the
hyperparameters speci ed in Corollary    Frostig et al 
  However  this algorithm was dif cult to tune and
performed poorly  so we have omitted it from the  gures 
The algorithms pre xed by  SA  used the adaptive step
size  For SGD  we followed the standard practice of using   diminishing and nonsummable step size tk    
    
The values               in our test were chosen experimentally  The SABFGS algorithms were implemented using   twoloop recursion to compute Hkgk when
      This signi cantly improved their performance
compared to storing the matrix Hk explicitly 
Figure   shows the performance of each algorithm on   series of problems with varying problem size   and parameter   The yaxis measures the gap log                
of the solution      obtained by the algorithm after using  

Stochastic Adaptive QuasiNewton Methods

seconds of CPU time  The algorithms were implemented in
Matlab     and the system was an Intel     running Ubuntu 
The increasing sample sizes used in SAGD   do not appear
to reduce its variance  compared to SAGD with mk  xed 
which goes against our initial expectations 
In plots    
        and     SAGD   appears to exhibit the same  uctuations as SAGD when the points approach optimality 
In plot     SAGD   brie   surpasses SAGD before the
objective value jumps again  It is only in plot     that SAGD   appears to descend more consistently than SAGD 
This suggests that  for these problem instances  the rate of
sampling mk        could even be increased further 
and that the tradeoff between taking more steps  versus taking fewer steps with more accurate estimates  favors more
accurate steps 
Likewise  SABFGS   failed to consistently perform better than SABFGS with mk  xed  SABFGS   seemed to
perform relatively better when the dimension was larger 
whereas SABFGS was faster when       One notable
example was plot     where SABFGS initially descends
rapidly before abruptly stalling  and is quickly surpassed
by SABFGS    The data going beyond   seconds of CPU
time veri ed that SABFGS continued to stall  and never
managed to obtain   logerror of less than   Our interpretation is that for this problem  the  xed sample size is
too small to obtain an accurate solution 
It also appears
that SABFGS requires larger samples to obtain   stable
solution when the problem is particularly illconditioned 
In the plots     and particularly     where       SABFGS rapidly reaches the point where variance in the gradient estimates introduces too much noise to make further
progress 
What is also interesting is that SAGD often outperforms
SABFGS if both algorithms use the same  xed sample
size  While somewhat disappointing  there is   natural reason for this  BFGS optimizes   local quadratic model of the
objective  and is performant when its approximation Hk
resembles the true Hessian  The Hessian exhibits greater
variance than the gradient  simply by virtue of having   
components compared to    and we generally expect that
more sampling is needed to accurately estimate the Hessian  With the same amount of sampling  SABFGS is
therefore  noisier  than SAGD relative to the true function 
We also see this re ected in the performance of SABFGS 
GD  which was able to outperform SABFGS and SABFGS   when       despite the additional cost of checking the Wolfe condition  For       it is less clear
whether   better strategy is simply to perform SABFGS 
   One strategy that might be effective is    xed pattern of
 switching  between GD and BFGS steps  with   greater
number of GD steps in the early phase 

Predictably  robust SGD was the most stable method  given
the lack of variancereduction in the other methods  Both
RS GDC and RS GDV exhibited almost no  uctuations  while improving at   reasonable rate  The RS GDC
method with   constant step size outperformed RS GDV 
which we  nd counterintuitive given that RS GDC used  
smaller step size than RS GDV  This method of variancereduction by averaging the solutions seems to be effective
and can be applied to any iterative method 
Numerical results for ERM  empirical risk minimization 
problems can be found in section   in the supplementary
material 

  Discussion
For selfconcordant objective functions  adaptive methods
eliminate the need to choose   step size  which is   big
advantage  This is independent of   new dif culty which
arises for stochastic methods  which is the choice of the
sample size mk  From our experiments  we see that choosing mk appropriately is crucial  Unlike SGD and SVRG 
where it is often most effective to use minibatches of size
  SABFGS and SAGD work best with comparatively
larger samples 
Note that SAGD and SABFGS are not purely  rstorder
methods  as computing the adaptive step size requires calculating the Hessianvector product Gk xk dk  However 
it is often possible to calculate Hessianvector products ef 
ciently  and at far less cost than computing the full Hessian
 Fk    Consider  for example      logistic regression
problem where the empirical loss function is

  cid 

  

      

 
 

log      yiaT

    

         yiaT

         yiaT

dT        it suf ces to compute cid  

for sampled data  ai  yi    ai   Rn  yi      
Let   denote the       matrix with columns ai  The
gradient is given by AT   where   is the vector     
 yie yiaT
     and the Hessian is given
by ABAT   where   is the diagonal matrix with entries
Bii     yiaT
     To compute the product
     and
this requires approximately the same number of arithmetic
operations as computing       Thus  computing    for
the adaptive step size requires roughly the same amount of
work as one additional gradient calculation  Also  as mentioned in Section   we may replace the BFGS update in
SABFGS with   modi ed update using the Hessian action
yk   Gk xk dk to save effort 
This work represents only   preliminary step in the development of stochastic quasiNewton methods  There are
several key questions that remain open 

   Bii aT

Stochastic Adaptive QuasiNewton Methods

               

               

               

               

               

               

Figure   Experimental results for         and varying   The xaxis is the elapsed CPU time and the yaxis is log            

  Theorem   partially resolves   question posed by
Moritz et al 
 Moritz et al    by proving superlinear convergence of   stochastic algorithm under
rather restrictive conditions  It would be strengthened
greatly if we could prove superlinear convergence under weaker conditions on mk 

  The theory developed for adaptive step sizes only
applies to selfconcordant functions  but the adaptive step size itself can be interpreted for nonself 
concordant functions  as an adjustment based on the
local curvature  It would be of great interest to extend
adaptive methods to general convex functions  thereby
replacing both inexact line search and  xed step sizes
on   large class of problems 

  How can variancereduction be applied to SAGD and
SABFGS  The control variates used in SVRG are effective  though costly  and perhaps dif cult to compute for functions of the form Ef       Another possible technique is to output an average of the solutions 
as is done by robust SGD  though this introduces an
additional hyperparameter  the number of past solutions to average  Better variancereduction strategies
might compensate for noisy estimates  allowing us to
reduce the number of samples needed in practice 

  What heuristics can be developed for the sample sizes
mk to improve the stability and speed up performance
of SAGD and SABFGS 

CPU time   log   xk                  CPU time   log   xk                  CPU time   log   xk                  CPU time   log   xk                  CPU time   log   xk                  CPU time   log   xk                  SGDSA GDSAGD ISABFGSSA BFGSISA BFGSGDR SGD CRS GDVStochastic Adaptive QuasiNewton Methods

References
Bottou    eon  Largescale machine learning with stochasIn Proceedings of COMP 

tic gradient descent 
STAT  pp    PhysicaVerlag HD   

Broyden  Charles    QuasiNewton methods and their application to function minimisation  Mathematics of Computation     

Byrd  Richard    Nocedal  Jorge  and Yuan  YaXiang 
Global convergence of   class of quasiNewton methods
on convex problems  Siam     Numer  Anal   
   

Byrd  Richard    Chin  Gillian    Nocedal  Jorge  and Wu 
Yuchen  Sample size selection in optimization methods
for machine learning  Mathematical Programming   
   

Byrd  Richard    Hansen        Nocedal  Jorge  and
Singer  Yoram    stochastic quasinewton method for
largescale optimization  SIAM Journal on Optimization 
   

Defazio  Aaron  Bach  Francis  and LacosteJulien  Simon 
Saga    fast incremental gradient method with support
In Adfor nonstrongly convex composite objectives 
vances in Neural Information Processing Systems  pp 
   

Dennis Jr  John    and Mor    Jorge    Characterization of superlinear convergence and its application to
quasiNewton methods  Math  Comp   
 

Fletcher  Roger    new approach to variable metric algo 

rithms  The Computer Journal     

Friedlander  Michael   and Goh  Gabriel 

bounds for stochastic approximation 
arXiv   

Tail
arXiv preprint

Frostig  Roy  Ge  Rong  Kakade  Sham    and Sidford 
Aaron  Competing with the empirical risk minimizer in
  single pass  In COLT  pp     

Gao  Wenbo and Goldfarb  Donald  QuasiNewton methods  Superlinear convergence without line search for
selfconcordant functions  in review  arXiv 
 

Goldfarb  Donald    family of variablemetric methods
derived by variational means  Mathematics of Computation     

Goldfarb  Donald  Iyengar  Garud  and Zhou  Chaoxu  Linear convergence of stochastic FrankWolfe variants  In
Proceedings of the  th International Conference on Arti cial Intelligence and Statistics  pp     

Gower  Robert  Goldfarb  Donald  and Richt arik  Peter 
Stochastic block BFGS   Squeezing more curvature out
of data  In Proceedings of the  rd International Conference on Machine Learning  pp     

Johnson  Rie and Zhang  Tong  Accelerating stochastic
gradient descent using predictive variance reduction  In
Advances in Neural Information Processing Systems  pp 
   

Moritz  Philipp  Nishihara  Robert  and Jordan  Michael   
  linearlyconvergent stochastic LBFGS algorithm  In
Proceedings of the Nineteenth International Conference
on Arti cial Intelligence and Statistics  pp   
 

Nemirovski  Arkadi  Juditsky  Anatoli  Lan  Guanghui  and
Shapiro  Alexander  Robust stochastic approximation
approach to stochastic programming  SIAM Journal on
Optimization     

Nesterov  Yurii and Nemirovski  Arkadi 

Interiorpoint
polynomial algorithms in convex programming  Society
for Industrial and Applied Mathematics  Philadelphia 
 

Nocedal  Jorge  Updating quasiNewton matrices with lim 

ited storage  Math  Comp     

Powell  Michael       Some global convergence properties of   variable metric algorithm for minimization without exact line searches  In Cottle  Richard and Lemke 
      eds  Nonlinear Programming  volume IX  SIAMAMS Proceedings   

Rodomanov  Anton and Kropotov  Dmitry 

 
superlinearlyconvergent proximal newtontype method
for the optimization of  nite sums  In Proceedings of the
 rd International Conference on Machine Learning 
pp     

Schmidt  Mark  Le Roux  Nicolas  and Bach  Francis  Minimizing  nite sums with the stochastic average gradient 
Mathematical Programming  pp     

Shanno  David    Conditioning of quasiNewton methods
for function minimization  Mathematics of Computation     

TranDinh  Quoc  Kyrillidis  Anastasios  and Cevher 
Volkan  Composite selfconcordant minimization  Journal of Machine Learning Research   Mar 
 

   van der Vaart  Aad  and Wellner  Jon    Weak Convergence and Empirical Processes  Springer New York 
 

Stochastic Adaptive QuasiNewton Methods

Zhang  Yuchen and Xiao  Lin  Disco  Distributed optiIn JMLR 
mization for selfconcordant empirical loss 
Workshop and Conference Proceedings  volume   pp 
   

