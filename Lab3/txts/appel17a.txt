  Simple MultiClass Boosting Framework

with Theoretical Guarantees and Empirical Pro ciency

Ron Appel   Pietro Perona  

Abstract

There is   need for simple yet accurate whitebox
learning systems that train quickly and with little data  To this end  we showcase REBEL   
multiclass boosting method  and present   novel
family of weak learners called localized similarities  Our framework provably minimizes the
training error of any dataset at an exponential
rate  We carry out experiments on   variety of
synthetic and real datasets  demonstrating   consistent tendency to avoid over tting  We evaluate our method on MNIST and standard UCI
datasets against other stateof theart methods 
showing the empirical pro ciency of our method 

  Motivation

The past couple of years have seen vast improvements in
the performance of machine learning algorithms  Deep
Nets of varying architectures reach almost  if not better
than  human performance in many domains  LeCun et al 
    key strength of these systems is their ability to
transform the data using complex feature representations
to facilitate classi cation  However  there are several considerable drawbacks to employing such networks 

   rst drawback is that validating through many architectures  each of which may have millions of parameters  requires   lot of data and time  In many  elds       pathology
of notso common diseases  expert curation of esoteric subjects  etc  gathering large amounts of data is expensive
or even impossible  Yu et al    Autonomous robots
that need to learn on the    may not be able to afford the
large amount of processing power or time required to properly train more complex networks simply due to their hardware constraints  Moreover  most potential users       nonmachine learning scientists  small business owners  hobby 

 Caltech  Pasadena  USA  Correspondence to 

Ron
Pietro Perona  per 

Appel  appel vision caltech edu 
ona vision caltech edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

    Old  Decision Stumps

    New  Localized Similarities

Figure       The typical decision stumps commonly used in
boosting lead to classi cation boundaries that are axis aligned
and not representative of the data  Although these methods can
achieve perfect training accuracy  it is apparent that they heavily
over        Our method uses localized similarities    novel family
of simple weak learners  see Sec    Paired with   procedure
that provably guarantees exponential loss minimization  our classi ers focus on smooth  wellgeneralizing boundaries 

ists  etc  may not have the expertise or artistry required to
hypothesize   set of appropriate models 

  second drawback is that the complex representations
achieved by these networks are dif cult to interpret and to
analyze  For many riskier applications       selfdriving
cars  robotic surgeries  military drones  etc    machine
should only run autonomously if it is able to explain its every decision and action  Further  when used towards the scienti   analysis of phenomena       understanding animal
behavior  weather patterns   nancial market trends  etc 
the goal is to extract   causal interpretation of the system in
question  hence  to be useful    machine should be able to
provide   clear explanation of its internal logic 

For these reasons  it is desirable to have   simple whitebox
machine learning system that is able to train quickly and
with little data  With these constraints in mind  we showcase   multiclass boosting algorithm called REBEL and
  novel family of weak learners called similarity stumps 
leading to much better generalization than decision stumps 
as shown in Fig    Our proposed framework is simple  ef 
cient  and is able to perfectly train on any dataset       fully
minimize the training error in    nite number of iterations 

  Simple Multiclass Boosting Framework

The main contributions of our work are as follows 

    simple multiclass boosting framework using local 

ized similarities as weak learners  see Sec   

    proof that the training error is fully minimized within

   nite number of iterations  see Sec   

    procedure for selecting an adequate learner at each

iteration  see Sec   

  empirical demonstrations of stateof theart results on

  range of datasets  see Sec   

  Background

Boosting is   fairly mature method  originally formulated
for binary classi cation       AdaBoost and similar variants   Schapire    Freund    Freund   Schapire 
  Multiclass classi cation is more complex than its
binary counterpart  however  many advances have been
made in both performance and theory in the context of
boosting  Since weak learners come in two  avors  binary and multiclass  two corresponding families of boosting methods have been explored 

The clever combination of multiple binary weak learners can result in   multiclass prediction  AdaBoost MH
reduces the Kclass problem into   single binary problem with   Kfold augmented dataset  Schapire   Singer 
  AdaBoost MO and similar methods reduce the
Kclass problem into   oneversus all binary problems
using ErrorCorrecting Output Codes to select the  nal
hypothesized class  Allwein et al    Sun et al   
Li    More recently  CDMCBoost and CWBoost
return   Kdimensional vector of class scores  focusing
each iteration on    binary  problem of improving the
margin of one class at   time  Saberian   Vasconcelos 
  Shen   Hao    REBEL also returns   vector of
class scores  increasing the margin between dynamicallyselected binary groupings of the   classes at each iteration
 Appel et al   

When multiclass weak learners are acceptable  and available    reduction to binary problems is unnecessary  AdaBoost    is   straightforward extension of its binary
counterpart  Freund   Schapire    AdaBoost    and
AdaBoost MR make use of   Kfold augmented dataset
to estimate output label probabilities or rankings for  
given input  Freund   Schapire    Schapire   Singer 
  More recent methods such as SAMME  AOSOLogitBoost  and GDMCBoost are based on linear combinations of    xed set of codewords  outputting Kdimensional score vectors  Zhu et al    Sun et al 
  Saberian   Vasconcelos   

In the noteworthy paper    Theory of Multiclass Boosting 
 Mukherjee   Schapire    many of the existing boosting methods were shown to be inadequate at training  either

because they require their weak learners to be too strong 
or because their loss functions are unable to deal with
some training data con gurations   Mukherjee   Schapire 
  outline the appropriate Weak Learning Condition
that   boosting algorithm must require of its weak learners in order to guarantee training convergence  However 
no method is prescribed with which to  nd an adequate set
of weak learners 

The goal of our work is to propose   multiclass boosting
framework with   simple family of binary weak learners
that guarantee training convergence and are easily interpretable  Using REBEL  Appel et al    as the multiclass boosting method  our framework is meant to be as
straightforward as possible so that it is accessible and practical to more users  outlining it in Sec    below 

  Our Framework

In this section  we de ne our notation  introduce our boosting framework  and describe our training procedure 

Notation
scalars  regular  vectors  bold                 
constant vectors 
indicator vector 
logical indicator function 
inner product 
elementwise multiplication 
elementwise function 

               
     with     in the kth entry 
 LOGICAL EXPRESSION     
hx  vi
     
                   

In the multiclass classi cation setting    datapoint is represented as   feature vector   and is associated with   class
label    Each point is comprised of   features and belongs
to one of   classes          Rd               

  good classi er reduces the training error while generalizing well to potentiallyunseen data  We use REBEL
 Appel et al    due to its support for binary weak learners  its mathematical simplicity      
closedform solution to loss minimization  and its strong empirical performance  REBEL returns   vectorvalued output    the sum
of    weak learner    accumulation vector    pairs  where
ft         and at    
Xt 

      

ft    at

 
 

 

The hypothesized class is simply the index of the maximal
entry in   

       arg max

     hH     yi 

The average misclassi cation error   can be expressed as 

   

 
 

 

Xn 

   xn  yn 

 

  Simple Multiclass Boosting Framework

  Binarizing MultiClass Data

At each iteration  the  rst step in determining an adequate
weak learner is binarizing the data       assigning   temporary binary label to each data point by placing it into one of
two groups  The following manipulations result in   procedure for binarizing datapoints given their boosting weights 

Eq    can be upperbounded as follows 

 

  xn  wn  yn

Eq    can be written as   product of matrices by stacking
all of the un as column vectors of       matrix   and
de ning   as   row vector with elements   xn 

REBEL uses an exponential loss function to upperbound
the average training misclassi cation error 

 

 
  

       

Xn hexp yn    xn    
where  yn      yn       all    with     in the yth

 

  index 

Being   greedy  additive model  all previouslytrained parameters are  xed and each iteration amounts to jointly optimizing   new weak learner   and accumulation vector   
To this end  the loss at iteration    can be expressed as 

LI   

 
 

 

Xn hwn  exp   xn  yn      

where  wn  

 
 

exp yn  HI xn 

 

Given   weak learner    we de ne true and false       correct
and incorrect  multiclass weight sums  sT

  and sF

    as 

sT
   

 
 

   xn yn     wn 

 

Xn 

thus 

sT
   sF

   

 
 

wn 

 

Xn 

sF
   

 
 

sT
  sF

   

 

Xn 
Xn 

 
 

 

   xn yn     wn

Using these weight sums  the loss can be simpli ed to 

LI    Lf   hsT

    exp       hsF

    exp    

 

In this form  it is easily shown that with the optimal accumulation vector    the loss has an explicit expression 

    

 

 cid ln sT

      ln sF

   cid     

 

     hpsT

    sF

      

 

At each iteration  growing decision trees requires an exhaustive search through   pool of decision stumps  which
is tractable but timeconsuming  storing the binary learner
that best reduces the multiclass loss in Eq    In some situations  axisaligned trees are simply unable to reduce the
loss any further  thereby stalling the training procedure 

Our proposed framework circumvents such situations  At
each iteration  instead of exhaustively searching for an adequate learner  we  rst determine an appropriate  binarization  of the multiclass data         separation of the Kclass
data into two distinct groups  and then  nd   weak learner
with   guaranteed reduction in loss  foregoing the need for
an exhaustive search 

 
 

 
 
   
   sF
   sT
 sT
   sF
   

 
     

 

   sF

        

         hsT
     cid 
   cid   
      is expressed as   squared norm 

using     

   

sT

sT sF

 

 

  sF

By expanding sT

     hpsT
 
since px     
    sF
  xn  wn  yni 
Pn 
   
wni
Pn 
where  un  

        

 

 

 

 

 

Xn 

 

  xn  un cid cid cid 

 

       cid cid cid 
   
Pn 

 
  

wn  yn

wn

        

 

 

    

Note that the trace of  

  can be lowerbounded 

 

 

tr  

    

 

Xn kunk   

since by Jensen   inequality 

 

     

 wn 

Pn 
Nh  
wni
Pn 
Xn 
  
   

 

 
   

 

Xn hwn    

 

  cid   
Xn 

xn cid 

 

  has    notnecessarily unique  nonFurthermore   
negative eigenvalues  each associated with an independent
eigenvector  Let  vn be the eigenvector corresponding to the
nth largest eigenvalue     Hence    can be decomposed as 

    hf           

       hf        

 

Xn 

 

 

Xn hf  vni  vn
 nhf  vni     hf      

  Simple Multiclass Boosting Framework

Since the trace of   matrix is equal to the sum of its eigenU has at most   nonzero eigenvalues  

 

values and  
being the largest  hence 

   

 
 

since 

 
 

KN

tr  

 

       
Xn hwn         

 

 

Based on this formulation  binarization is achieved by setting the binarized class bn of each sample   as the sign of
its corresponding element in     bn   sign       ni 
Accordingly  if   is the vector with elements bn  then 

hb        hsign                      

 

 please refer to supplement for proof 

Finally  by combining Eq    Eq    and Eq    with perfect
binarized classi cation       when the binary weak learner
perfectly classi es the binarized data  the loss ratio at any
iteration is bounded by 

Lf  
        

 

 KN

In general  there is no guarantee that any weak learner can
achieve perfect binarized classi cation 
In the following
section  we show that with the ability to isolate any single
point in space       to classify an inner point as   and all
outer points as   the loss decreases exponentially 

  Isolating Points

Assume that we have   weak learner fi that can isolate  
single point xi in the input space    Accordingly  denote
fi           as   vector of    with     in the ith entry 
corresponding to classi cation using the isolating learner
fi xn  If       then for any unit vector      RN  

   hfi vi   
max

 
 

 

 please refer to supplement for proof 

Combining Eq    Eq    and Eq    the loss ratio at each
iteration is upperbounded as follows 

mini Lfi 

  

     

 

KN  

Before the  rst iteration  the initial loss         Each
iteration decreases the loss exponentially  Since the training error is discrete and is upper bounded by the loss  as

in Eq    our framework attains minimal training error on
any  training set after    nite number of iterations 

KN  cid   cid KN  
de ne       ln KN  
ln cid   
   cid 
KN  cid  

          

   cid cid 
ln cid KN
 
         

 

 

 

 

Although this bound is too weak to be of practical use  it
is   bound nonetheless  and can likely be improved  In
the following section  we specify   suitable family of weak
learners with the ability to isolate single points 

  One TwoPoint Localized Similarities

Classical decision stumps compare   single feature to  
threshold  outputting   or   Instead  our proposed family of weak learners  called localized similarities  compare
points in the input space using   similarity measure  Due
to its simplicity and effectiveness  we use negative squared

Euclidean distance  kxi xjk  as   measure of similarity
between points xi and xj     localized similarity has two
modes of operation 

  In onepoint mode  given an anchor xi and   threshold
    the input space is classi ed as positive if it is more
similar to xi than     and negative otherwise  ranging
between   and  

fi     

    kxi xk 
    kxi xk 

  In twopoint mode  given supports xi and xj   the input
space is classi ed as positive if it is more similar to
xi than to xj  and viceversa  with maximal absolute
activations around xi and xj   falling off radially away
from the midpoint   

fij     

where     

 
 

hd    mi

 kdk    kx mk 
 
 

and     

 xi xj 

 xi  xj 

Onepoint mode enables the isolation of any single datapoint  guaranteeing   baseline reduction in loss  However 
it essentially leads to pure memorization of the training
data  mimicking   nearestneighbor classi er  Twopoint
mode adds the capability to generalize better by providing marginstyle functionality  The combination of these

  There may be situations in which multiple samples belonging to different classes are coincident in the input space  These
cases can be dealt with  before or during training  either by assigning all such points as   special  mixed  class  to be dealt with
at   later stage  or by setting the class labels of all coincident
points to the single label that minimizes the error 

  Simple Multiclass Boosting Framework

two modes enables the  exibility to tackle   wide range of
classi cation problems  Furthermore  in either mode  the
functionality of   localized similarity is easily interpretable 
 which of these  xed training points is   given query point
more similar to 

  Finding Adequate Localized Similarities

Given   dataset with   samples  there are about     possible localized similarities  The following procedure ef 
ciently selects an adequate localized similarity 

  Using Eq    calculate the base loss    for the homogeneous stump          the onepoint stump with any xi
and       classifying all points as  
points based on their binarized class labels bn 

  Compute the eigenvector      as in Eq    label the

  Find the optimal isolating localized similarity fi      
with xi and appropriate     classifying point   as   and
all other points as  
  Using Eq    calculate the corresponding loss Li  Of the
two stumps    and fi  store the one with smaller loss as
bestso far 

  Find point xj most similar  to xi among points of the

opposite binarized class 

xj   arg min

bj  bi kxi xjk 

  Calculate the loss achieved using the twopoint localized similarity fij   If it outperforms the previous best 
store the newer learner and update the bestso far loss 
  Find all points that are similar enough to xj and remove
them from consideration for the remainder of the current iteration  In our implementation  we remove all xn
for which 

fij xn    fij xj  

If all points have been removed  return the bestso far
localized similarity  otherwise  loop back to step  

Upon completion of this procedure  the bestso far localized similarity is guaranteed to lead to an adequate reduction in loss  based on the derivation in Sec    above 

  Generalization Experiments

Our boosting method provably reduces the loss well after
the training error is minimized  In this section  we demonstrate that the continual reduction in loss serves only to improve the decision boundaries and not to over   the data 

We generated  dimensional synthetic datasets in order to
better visualize and get an intuition for what the classi ers

   most similar  need not be exact  approximate nearest 

neighbors also works with negligible differences 

Figure      point  dimensional synthetic dataset with  
    split of train data  left plot  to test data  right plot 
Background shading corresponds to the hypothesized class using
our framework 

 

 

 

 

 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 

 

 
 
 

 

 

 

 

 

 
 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

log  Loss  
Train Error  
Test Error   

 

 
 

 
 
Iteration  max    

 

Figure     plot of training loss  training error  and test error as
  classi er is trained for   iterations  Note that the test error
does not increase even after the training error drops to zero  The
lower inset is   zoomedin plot of the train and test error  the upper
inset is   plot of training loss using   logscaled yaxis  both inset
plots are congruous with the original xaxis 

are doing  The results shown in this chapter are based on
  dataset composed of   points belonging to one of three
classes in   spiral formation  with       train test
split  Fig    shows the hypothesized class using   classi er
trained for   iterations 

Our classi er achieves perfect
training  left  and test
classi cation  right  producing   visually simple wellgeneralizing contour around the points  Training curves
are given in Fig    tracking the loss and classi cation errors per training iteration  Note that the test error does not
increase even after the training error drops to zero 

The following experiments explore the functionality of our
framework       REBEL using localized similarities  in two
scenarios that commonly arise in practice    varying sparsity of training data  and    varying amounts of mislabeled
training data 

    Train Data

    Test Data

    Training on   of the data   points 

    Train Data

    Test Data

  Simple Multiclass Boosting Framework

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 

 

 

 

 

 
 

 
 
 
 
 

 

 

 
 
 
 
 
  

 

 
 
 
 
 
 
 

 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

log  Loss  
Train Error  
Test Error   

 

 

Iteration  max    

 

    Train Data

    Test Data

    Training on   of the data   points 

    Train Data

    Test Data

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 

 

 

 

 

 
 

log  Loss  
Train Error  
Test Error   

 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 

 

 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 
 
Iteration  max    

 

    Train Data

    Test Data

    Training on   of the data   points 

    Train Data

    Test Data

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 

 

 

 

 

 
 

 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

log  Loss  
Train Error     
Test Error      

 

 

 

Iteration  max    

    Train Data

    Test Data

    Training on   of the data   points 

    Train Data

    Test Data

 

 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 

 

 

 

 

 
 

 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 

 

 

 

 

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 
 
 
 

 

 

log  Loss     
Train Error     
Test Error   

 

 

Iteration  max    

 

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 

 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 

 

 

 

 
 

      mislabeled data   points 
 

 
 
 
 
 

 

 

 

 

 

 
 

 

 

 

 
 
 
 
 
 

 

 

 

 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

log  Loss  
Train Error  
Test Error   

 

 

 

Iteration  max    

      mislabeled data   points 
 

 
 
 
 
 

 

 

 

 

 

 
 

log  Loss  
Train Error  
Test Error   

 

 

 

 
 
 
 
 
 

 

 

 

 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 
 
Iteration  max    

 

      mislabeled data   points 
 

 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 

 

 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

log  Loss  
Train Error  
Test Error   

 

 

Iteration  max    

 

      mislabeled data   points 
 

 
 
 
 
 

 

 

 

 

 

 
 

 

 

 
 
 
 
 
 

 

 

 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

log  Loss     
Train Error     
Test Error   

 

 

 

 

Iteration  max    

Figure   Classi cation boundaries       and training curves    
when   classi er is trained on varying amounts of data  Stars are
correctlyclassi ed  circles are misclassi ed  In all cases  the test
error is fairly stable once reaching its minimum 

Figure   Classi cation boundaries       and training curves    
when   classi er is trained on varying fractions of mislabeled data 
In all cases  the test error is fairly stable once reaching its minimum  Even with   mislabeled data  the classi cation boundaries are reasonable given the training labels 

  Sparse Training Data

In this section of experiments  classi ers were trained using
varying amounts of data  from   to   of the total training set  Fig    shows the classi cation boundaries learned
by the classi er  ai bi  and the training curves  ci  In all
cases  the boundaries seem to aptly     and not over    the
training data       being satis ed with isolated patches without overzealously trying to connect points of the same class
together  This is more rigorously observed from the training curves  the test error does not increase after reaching its
minimum  for hundreds of iterations 

  Mislabeled Training Data

In this section of experiments  classi ers were trained with
varying fractions of mislabeled data  from   to   of
the training set  Fig    shows the classi cation boundaries
 ai bi  and the training curves  ci  All classi ers seem to
degenerate gracefully  isolating rogue points and otherwise
maintaining smooth boundaries  Even the classi er trained
on   mislabeled data  which we would consider to be
unreasonably noisy  is able to maintain smooth boundaries 

In all cases  the training curves still show that the test error
is fairly stable once reaching its minimum value  Moreover 
test errors approximately equal the fraction of mislabeled
data  further validating the generalization of our method 

  Real Data

that

Although the above observations are promising  they could
result from the fact
the synthetic datasets are  
dimensional 
In order to rule out this possibility  we
performed similar experiments on several UCI datasets
 Bache   Lichman    of varying input dimensionalities  from   to   From the training curves in Fig   
we observe that once the test errors saturate  they no longer
increase  even after hundreds of iterations 

In Fig    we plot the training losses on   logscaled yaxis 
The linear trend signi es an exponential decrease in loss
per iteration  Our proven bound predicts   much slower  exponential  rate than the actual trend observed during training  Note that within the initial     of the iterations  the
loss drops at an even faster rate  after which it settles down

  Simple Multiclass Boosting Framework

PENDIGIT  dimensional 

GLASS

PENDIGIT

                  

                  

 

 

 

 

 

 

 

 

 

 

Initial Iterations

 

 

 

 

 

 

Proven bound
Loss
Exponential Fit

 
 
 
 

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Proven bound
Loss
Exponential Fit

 

 

 

 

Initial Iterations

 

 

 

 

 

 

 

 

 

 
Iteration Number

 

 

 

 

 

 

 

 

 
Iteration Number

 

 

   

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 

 

 

 

 

 
 

GLASS  dimensional 

 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

log  Loss  
Train Error     
Test Error   

 

 

 

 

Iteration  max    

 

 

 

 

 

OPTDIGIT  dimensional 

 
 
 
 
 

 

 

 

 
 
 
 
 
 

 

 

 
 

 
 
 
 

 

 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 

 

 

 

log  Loss  
Train Error  
Test Error   

 

 

 

 

Iteration  max    

 
 

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 
 
 
 

 

 

 
 
 
 
 
 

 

 
 
 
 
 
 

 

 
 
 
 
 

 

 
 
 

 

 

 

 

 

 
 

 

 

 

 

 

 
 

 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 

 

 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

log  Loss  
Train Error  
Test Error   

 

 

Iteration  max    

 

ISOLET  dimensional 

 
 
 
 
 

 

 

 
 
 
 
 
 

 

 

 

 

 
 
 
 

 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 

 

 

 

 
 

log  Loss  
Train Error  
Test Error   

 

 

 

 

Iteration  max    

Figure   Training curves for classi ers trained on UCI datasets
with   range of dimensionalities 
In all cases  the test error is
stable once it reaches its minimum 

to   seeminglyconstant rate of exponential decay  We have
not yet determined the characteristics       the theoretically
justi ed rates  of these observed trends  and relegate this
endeavor to future work 

  Comparison with Other Methods

In Sec    we proved that our framework adheres to theoretical guarantees  and in Sec    above  we showed
that it has promising empirical properties 
In this section  we compete against several stateof theart boosting baselines  Speci cally  we compared  vsAll AdaBoost and AdaBoost MH  Schapire   Singer   
AdaBoost ECC  Dietterich   Bakiri    StructBoost
 Shen et al    CWBoost  Shen   Hao    AOSOLogitBoost  Sun et al    REBEL  Appel et al   
using shallow decision trees  REBEL using only  point
 isolating  similarities  and our full framework  REBEL using  point localized similarities 

Based on the same experimental setup as in  Shen et al 
  Appel et al    competing methods are trained
to   maximum of   decision stumps  For each dataset 
 ve random splits are generated  with   of the samples
for training    for validation       for setting hyperparameters where needed  and the remaining   for testing 

REBEL using localized similarities is the most accurate
method on  ve of the six datasets tested 
In the Vowel
dataset  it achieves almost half of the error as the next best
method  Note that although our framework uses REBEL as
its boosting method  the localized similarities add an extra
edge  beating REBEL with decision trees in all runs 

Further  when limited to only using  point       isolating 
localized similarities  the performance is extremely poor 
validating the need for  point localized similarities as prescribed in Sec    Overall  these results demonstrate the

OPTDIGIT

ISOLET

                  

                  

 

 

 

 

 

 

 

Initial Iterations

 
 
 
 

 

 

 
 
 
 
 
 

 

Proven bound
Loss
Exponential Fit

 
 
 
 

 

 

 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
Iteration Number

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

Proven bound
Loss
Exponential Fit

 

 

   

Initial Iterations

 

 

 

 

 

 

 

 

 

 

 
Iteration Number

Figure   Training losses for classi ers trained on UCI datasets 
The linear trend  visualized using   logscaled yaxis  signi es
an exponential decrease in loss  albeit at   much faster rate than
established by our proven bound 

 
 
 
 
 

 
 
 
 
 
 
 
 

 

 

 

 

 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 

 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

      Ada  vsAll
      Ada MH
      Ada ECC
      StructBoost
      CWBoost
          Logit
      RBLStump
      RBLIso Sim
      RBLLoc Sim

 
 
 

 
 
 

 
 
 

 
 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 
 

GLASS

VOWEL

LANDSAT

MNIST

PENDIGITS

SEGMENT

Figure   Test errors of various stateof theart and baseline
classi cation methods on MNIST and several UCI datasets 
REBEL using localized similarities  shown in yellow  is the bestperforming method on all but one of the datasets shown  When
constrained to use only  point  isolating  similarities  shown in
red  the resulting classi er is completely inadequate 

ability of our framework to produce easily interpretable
classi ers that are also empirically pro cient 

  Comparison with Neural Networks and SVMs

Complex neural networks are able to achieve remarkable
performance on large datasets  but they require an amount
of training data proportional to their complexity 
In the
regime of small to medium amounts of data  within which
UCI and MNIST datasets belong                 training samples  such networks cannot be too complex  Accordingly  in Fig    we compare our method against fullyconnected neural networks 

  Simple Multiclass Boosting Framework

 

 

 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 
 
 

 

 

 

 

 

 

 

 

 

 

   

Method
  NN 
  SVM 
  Ours

Dataset
GLASS
SHUTTLE
VOWEL
PENDIGIT
LETTER
LANDSAT
OPTDIGIT
ISOLET
MNIST
CUB 

VV
 

 

 
 

 
AA

 
 
 

 

 
 

 

PP

 
 

 

 

 
 

 

 

 

     

       
       
       
Number of Training Samples    

       
       

         

         

         

Figure   Comparison of our method versus Neural Networks and
Support Vector Machines on ten datasets of varying sizes and dif 
 culties  Our method is the most accurate on almost all datasets 

Four neural networks were implemented  each having one
of the following architectures                 
                    where   is the number
of input dimensions and   is the number of output classes 
Only the one with the best test error is shown in the plot   
multiclass SVM  Chang   Lin    was validated using
        parameter sweep for   and   Our method was run
until the training loss fell below      Overall  REBEL using localized similarities achieves the best results on eight
of the ten datasets  decisively marking it as the method of
choice for this range of data 

  Discussion

In Sec    we observed that our classi ers tend to smoothen
the decision boundaries in the iterations beyond zero training error  In Fig    we see that this is not the case with
the typicallyused axisaligned decision stumps  Why does
this happen with our framework 

Figure   The contrasted difference between overtraining using
    classical decision stumps and     localized similarities 
   
leads to massive over tting of the training data  whereas     leads
to smoothening of the decision boundaries 

Firstly  we note that the largestmargin boundary between
two points is the hyperplane that bisects them  Every twopoint localized similarity acts as such   bisector  Therefore  it is not surprising that with only   pool of localized
similarities    classi er should have what it needs to place
good boundaries  Further  not all pairs need to be separated
 since many neighboring points belong to the same class 

hence  only   small subset of the       possible learners

will ever need to be selected 

Secondly  we note that if some point  either an outlier or an
unfortunatelyplaced point  continues to increase in weight
until it can nolonger be ignored  it can simply be isolated
and individually dealt with using   onepoint localized similarity  there is no need to combine it with other  innocentbystander  points  This phenomenon is observed in the mislabeled training experiments in Sec   

Together  the two types of localized similarities complement each other  With the guarantee that every step reduces
the loss  each iteration focuses on either further smoothening out an existing boundary  or reducing the weight of  
single un   point 

  Conclusions

We have presented   novel framework for multiclass boosting that makes use of   simple family of weak learners
called localized similarities  Each of these learners has  
clearly understandable functionality    test of similarity between   query point and some prede ned samples 

We have proven that the framework adheres to theoretical
guarantees  the training loss is minimized at an exponential rate  and since the loss upperbounds the training error
 which can only assume discrete values  our framework is
therefore able to achieve maximal accuracy on any dataset 

We further explored some of the empirical properties of
our framework  noting that the combination of localized
similarities and guaranteed loss reduction tend to lead to  
nonover tting regime  in which the classi er focuses on
smoothingout its decision boundaries  Finally  we compare our method against several stateof theart methods 
outperforming all of the methods in most of the datasets 

Altogether  we believe that we have achieved our goal of
presenting   simple multiclass boosting framework with
theoretical guarantees and empirical pro ciency 

Acknowledgements

The authors would like to thank anonymous reviewers for
their feedback and Google Inc  and the Of ce of Naval
Research MURI    for funding this work 

  Simple Multiclass Boosting Framework

References

Allwein        Schapire        and Singer     Reducing
multiclass to binary    unifying approach for margin classi ers  JMLR   

Appel     BurgosArtizzu        and Perona     Improved
multiclass costsensitive boosting via estimation of the
minimumrisk class  arXiv     

Bache    

and Lichman    

UCI machine
URL

 
learning repository  uc
http archive ics uci edu ml 

irvine 

Chang     and Lin     LIBSVM    library for support vector machines  Transactions on Intelligent Systems and
Technology   

Dietterich        and Bakiri     Solving multiclass learn 

ing problems via errorcorrecting output codes  arXiv 
   

Freund     Boosting   weak learning algorithm by majority 

Information and Computation   

Freund     and Schapire        Experiments with   new
boosting algorithm  In Machine Learning International
Workshop   

LeCun     Bengio     and Hinton        Deep learning 

Nature Research   

Li     Multiclass boosting with repartitioning  In ICML 

 

Mukherjee     and Schapire          theory of multiclass

boosting  In NIPS   

Saberian     and Vasconcelos     Multiclass boosting 

Theory and algorithms  In NIPS   

Schapire        The strength of weak learnability  Machine

Learning   

Schapire        and Singer    

Improved boosting algorithms using con dencerated predictions  In Conference
on Computational Learning Theory   

Shen     and Hao       direct formulation for totally 

corrective multiclass boosting  In CVPR   

Shen     Lin     and van den Hengel     Structboost 
Boosting methods for predicting structured output variables  PAMI   

Sun     Reid        and Zhou     Aosologitboost 
Adaptive onevs one logitboost for multiclass problem 
arXiv     

Sun     Todorovic     Li     and Wu     Unifying
the errorcorrecting and outputcode adaboost within the
margin framework  In ICML   

Yu     Zhang     Song     Seff     and Xiao     LSUN 
construction of   largescale image dataset using deep
learning with humans in the loop  arXiv   
 

Zhu     Zou     Rosset     and Hastie     Multiclass

adaboost  Statistics and its Interface   

