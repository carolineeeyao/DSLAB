Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear

Regression under RIP

Satyen Kale   Zohar Karnin   Tengyuan Liang     avid   al  

Abstract

Online sparse linear regression is an online problem where an algorithm repeatedly chooses  
subset of coordinates to observe in an adversarially chosen feature vector  makes   realvalued
prediction  receives the true label  and incurs the
squared loss  The goal is to design an online
learning algorithm with sublinear regret to the
best sparse linear predictor in hindsight  Without any assumptions  this problem is known to
be computationally intractable  In this paper  we
make the assumption that data matrix satis es restricted isometry property  and show that this assumption leads to computationally ef cient algorithms with sublinear regret for two variants of
the problem  In the  rst variant  the true label is
generated according to   sparse linear model with
additive Gaussian noise  In the second  the true
label is chosen adversarially 

  Introduction
In modern realworld sequential prediction problems  samples are typically high dimensional  and construction of
the features may itself be   computationally intensive task 
Therefore in sequential prediction  due to the computation
and resource constraints  it is preferable to design algorithms that compute only   limited number of features for
each new data example  One example of this situation 
from  CesaBianchi et al    is medical diagnosis of  
disease  in which each feature is the result of   medical test
on the patient  Since it is undesirable to subject   patient to

 Google Research  New York 

 Amazon  New York 
 University of Chicago  Booth School of Business  Chicago 
 Yahoo Research  New York  Work done while the authors were at Yahoo Research  New York 
CorresponSatyen Kale  satyenkale google com  Zodence to 
Liang
har Karnin  zkarnin gmail com 
  al
 Tengyuan Liang chicagobooth edu 
 dpal yahooinc com 

Tengyuan
  avid

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  battery of medical tests  we would like to adaptively design diagnostic procedures that rely on only   few  highly
informative tests 
Online sparse linear regression  OSLR  is   sequential prediction problem in which an algorithm is allowed to see
only   small subset of coordinates of each feature vector 
The problem is parameterized by   positive integers    
the dimension of the feature vectors     the sparsity of the
linear regressors we compare the algorithm   performance
to  and      budget on the number of features that can be
queried in each round by the algorithm  Generally we have
   cid    and        but not signi cantly larger  our algorithms need            
In the OSLR problem  the algorithm makes predictions
over   sequence of   rounds 
In each round    nature
chooses   feature vector xt   Rd  the algorithm chooses
  subset of                of size at most   cid  and observes
the corresponding coordinates of the feature vector  It then

makes   prediction cid yt     based on the observed features 
observes the true label yt  and suffers loss  yt  cid yt  The

goal of the learner is to make the cumulative loss comparable to that of the best ksparse linear predictor   in hindsight  The performance of the online learner is measured
by the regret  which is de ned as the difference between
the two losses 

  cid 

  

RegretT  

 yt  cid yt  min

    cid   cid  

  cid 

  

 yt    cid xt    cid   

The goal is to construct algorithms that enjoy regret that is
sublinear in     the total number of rounds    sublinear
regret implies that in the asymptotic sense  the average perround loss of the algorithm approaches the average perround loss of the best ksparse linear predictor 
Sparse regression is in general   computationally hard
problem 
In particular  given                  xT and
              yT as inputs  the of ine problem of  nding  
  yt    cid xt    cid 
does not admit   polynomial time algorithm under standard
complexity assumptions  Foster et al    This hard 
 In this paper  we use the     notation to suppress factors that

ksparse   that minimizes the error cid  

are polylogarithmic in the natural parameters of the problem 

Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear Regression under RIP

ness persists even under the assumption that there exists  
ksparse    such that yt    cid xt    cid  for all    Furthermore 
the computational hardness is present even when the solu 

tion is required to be only  cid     sparse solution and has to

minimize the error only approximately  see  Foster et al 
  for details  The hardness result was extended to online sparse regression by  Foster et al    They showed
that for all       there exists no polynomialtime algorithm
with regret       unless       BP    
Foster et al    posed the open question of what additional assumptions can be made on the data to make the
problem tractable  In this paper  we answer this open question by providing ef cient algorithms with sublinear regret
under the assumption that the matrix of feature vectors satis es the restricted isometry property  RIP   Candes   Tao 
  It has been shown that if RIP holds and there exists
  sparse linear predictor    such that yt    cid xt    cid      
where    is independent noise  the of ine sparse linear
regression problem admits computationally ef cient algorithms        Candes   Tao    RIP and related Restricted Eigenvalue Condition  Bickel et al    have
been widely used as   standard assumption for theoretical
analysis in the compressive sensing and sparse regression
literature  in the of ine case 
In the online setting  it is
natural to ask whether sparse regression avoids the computational dif culty under an appropriate form of the RIP
condition  In this paper  we answer this question in   positive way  both in the realizable setting and in the agnostic
setting  As   byproduct  we resolve the adaptive feature
selection problem as the ef cient algorithms we propose in
this paper adaptively choose   different  sparse  subset of
features to query at each round  This is closely related to
attributeef cient learning  see discussion in Section  
and online model selection 

  Summary of Results

for

two models

regression for

We design polynomialtime algorithms for online sparse
linear
the sequence
                       xT   yT   The  rst model is called
the realizable and the second is called agnostic  In both
models  we assume that  after proper normalization  for all
large enough    the matrix Xt formed from the  rst   feature
vectors               xt satis es the restricted isometry property  The two models differ in the assumptions on yt  The
realizable model assumes that yt    cid xt    cid       where
   is ksparse and    is an independent noise  In the agnostic model  yt can be arbitrary  and therefore  the regret
bounds we obtain are worse than in the realizable setting 
The models and corresponding algorithms are presented in
Sections   and   respectively  Interestingly enough  the algorithms and their corresponding analyses are completely
different in the realizable and agnostic case 

Our algorithms allow for somewhat more  exibility than
the problem de nition  they are designed to work with  
budget    on the number of features that can be queried that
may be larger than the sparsity parameter   of the comparator  The regret bounds we derive improve with increasing
values of    In the case when         the dependence on
  in the regret bounds is polynomial  as can be expected
in limited feedback settings  this is analogous to polynomial dependence on   in bandit settings  In the extreme
case when              we have access to all the features 
the dependence on the dimension   in the regret bounds we
prove is only logarithmic  The interpretation is that if we
have full access to the features  but the goal is to compete
with just   sparse linear regressors  then the number of data
points that need to be seen to achieve good predictive accuracy has only logarithmic dependence on    This is analogous to the  of ine  compressed sensing setting where the
sample complexity bounds  under RIP  only depend logarithmically on   
  major building block in the solution for the realizable setting  Section   consists of identifying the best ksparse linear predictor for the past data at any round in the prediction
problem  This is done by solving   sparse regression problem on the observed data  The solution of this problem cannot be obtained by   simple application of say  the Dantzig
selector  Candes   Tao    since we do not observe the
data matrix    but rather   subsample of its entries  Our
algorithm is   variant of the Dantzig selector that incorporates random sampling into the optimization  and computes
  nearoptimal solution by solving   linear program  The

resulting algorithm has   regret bound of  cid   log     This

bound has optimal dependence on     since even in the full
information setting where all features are observed there is
  lower bound of  log      Hazan   Kale   
The algorithm for the agnostic setting relies on the theory
of submodular optimization  The analysis in  Boutsidis
et al    shows that the RIP assumption implies that
the set function de ned as the minimum loss achievable
by   linear regressor restricted to the set in question satis 
 es   property called weak supermodularity  Weak supermodularity is   relaxation of standard supermodularity that
is still strong enough to show performance bounds for the
standard greedy feature selection algorithm for solving the
sparse regression problem  We then employ   technique
developed by Streeter   Golovin   to construct an
online learning algorithm that mimics the greedy feature
selection algorithm  The resulting algorithm has   regret

bound of  cid       It is unclear if this bound has the op 

 
timal dependence on     it is easy to prove   lower bound
    on the regret using standard arguments for the
of  
multiarmed bandit problem 

Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear Regression under RIP

  Related work

without RIP  Foster et al     

  related setting is attributeef cient
learning  CesaBianchi et al    Hazan   Koren    Kukliansky  
Shamir    This is   batch learning problem in which
the examples are generated        and the goal is to simply
output   linear regressor using only   limited number of
features per example with bounded excess risk compared
to the optimal linear regressor  when given full access to
the features at test time  Since the goal is not prediction
but simply computing the optimal linear regressor  ef cient
algorithms exist and have been developed by the aforementioned papers 
Without any assumptions  only inef cient algorithms for
the online sparse linear regression problem are known Zolghadr et al    Foster et al    Kale   posed
the open question of whether it is possible to design an ef 
 cient algorithm for the problem with   sublinear regret
bound  This question was answered in the negative by Foster et al    who showed that ef ciency can only be
obtained under additional assumptions on the data  This
paper shows that the RIP assumption yields tractability in
the online setting just as it does in the batch setting 
In the realizable setting  the linear program at the heart of
the algorithm is motivated from Dantzig selection  Candes
  Tao    and errorin variable regression  Rosenbaum
  Tsybakov    Belloni et al    The problem of
 nding the best sparse linear predictor when only   sample
of the entries in the data matrix is available is also discussed
by Belloni et al     see also the references therein  In
fact  these papers solve   more general problem where we
observe   matrix   rather than   that is an unbiased estimator of    While we can use their results in   blackbox
manner  they are tailored for the setting where the variance
of each Zij is constant and it is dif cult to obtain the exact
dependence on this variance in their bounds  In our setting 
this variance can be linear in the dimension of the feature
vectors  and hence we wish to control the dependence on
the variance in the bounds  Thus  we use an algorithm that
is similar to the one in  Belloni et al    and provide
an analysis for it  in the supplementary material  As an
added bonus  our algorithm results in solving   linear program rather than   conic or general convex program  hence
admits   solution that is more computationally ef cient 
In the agnostic setting  the computationally ef cient algorithm we propose is motivated from  online  supermodular optimization  Natarajan    Boutsidis et al   
Streeter   Golovin    The algorithm is computationally ef cient and enjoys sublinear regret under an RIPlike
condition  as we will show in Section   This result can
be contrasted with the known computationally prohibitive
algorithms for online sparse linear regression  Zolghadr
et al    Foster et al    and the hardness result

        

if       
if    cid    

 cid 
Let  cid      cid   cid 
by  cid   cid    For        cid   cid      cid 

    
 
              be the inner product of vectors

for                   

  Notation and Preliminaries
For        we denote by     the set                For  
vector in     Rd  denote by      its ith coordinate  For
  subset         we use the notation RS to indicate the
vector space spanned by the coordinate axes indexed by  
      the set of all vectors   supported on the set    For  
vector     Rd  denote by        Rd the projection of  
on RS  That is  the coordinates of      are

  and   
For       the  cid pnorm of   vector     Rd is denoted
   xi       cid   cid   
maxi  xi  and  cid   cid  is the number of nonzero coordinates
of   
The following de nition will play   key role 
De nition    Restricted Isometry Property  Candes   Tao 
  Let         and       We say that   matrix
    Rn   satis es restricted isometry property  RIP  with
parameters      if for any     Rd with  cid   cid      we
have

     cid   cid     

 

 cid Xw cid         cid   cid   

One can show that RIP holds with overwhelming probability if        log ed    and each row of the matrix
is sampled independently from an isotropic subGaussian
distribution  In the realizable setting  the subGaussian assumption can be relaxed to incorporate heavy tail distribution via the  small ball  analysis introduced in Mendelson   since we only require onesided lower isometry
property 

  Proper Online Sparse Linear Regression

We introduce   variant of online sparse regression  OSLR 
which we call proper online sparse linear regression
 POSLR  The adjective  proper  is to indicate that the algorithm is required to output   weight vector in each round
and its prediction is computed by taking an inner product
with the feature vector 
We assume that
there is an underlying sequence
                       xT   yT   of labeled examples in
Rd      In each round                     the algorithm behaves according to the following protocol 
  Choose   vector wt   Rd such that  cid wt cid      

Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear Regression under RIP

  Choose St       of size at most   
  Observe xt St  and yt  and incur loss  yt cid xt  wt cid 

Essentially 
 
 cid xt  wt cid  in round    The regret after   rounds of an algorithm with respect to     Rd is

the algorithm makes the prediction  cid yt
  cid 

 yt    cid xt  wt cid    cid 

 yt    cid xt    cid   

RegretT      

  

  

The regret after   rounds of an algorithm with respect to
the best ksparse linear regressor is de ned as

RegretT   max

    cid   cid  

RegretT      

Note that any algorithm for POSLR gives rise to an algorithm for OSLR  Namely  if an algorithm for POSLR
chooses wt and St  the corresponding algorithm for OSLR
queries the coordinates St     
  wt     cid    The algorithm for OSLR queries at most        coordinates and has
the same regret as the algorithm for POSLR 
Additionally  POSLR allows parameters settings which do
not have corresponding counterparts in OSLR  Namely  we
can consider the sparse  full information  setting where
       and    cid    
We denote by Xt the       matrix of  rst   unlabeled samples     
    Similarly  we
denote by Yt   Rt the vector of  rst   labels               yt 
We use the shorthand notation      for XT and YT respectively 
In order to get computationally ef cient algorithms  we assume that that for all        the matrix Xt satis es the
restricted isometry condition  The parameter    and RIP
parameters      will be speci ed later 

rows of Xt are xT

            xT

    xT

  Realizable Model
In this section we design an algorithm for POSLR for the
realizable model  In this setting we assume that there is  
vector      Rd such that  cid   cid      and the sequence
of labels               yT is generated according to the linear
model

yt    cid xt    cid        

 

where                are independent random variables
from       We assume that the standard deviation  
or an upper bound of it  is given to the algorithm as input 
We assume that  cid   cid      and  cid xt cid      for all   
For convenience  we use   to denote the vector
                 of noise variables 

  Algorithm

The algorithm maintains an unbiased estimate  cid Xt of the
matrix Xt  The rows of  cid Xt are vectors  cid xT
           cid xT
   cid xT

 
which are unbiased estimates of xT
    To construct the estimates  in each round    the set St       is
chosen uniformly at random from the collection of all subsets of     of size    The estimate is
  xt St 

            xT

    xT

 

To compute the predictions of the algorithm  we consider
the linear program

minimize cid   cid      

 
  

 cid xt  
 cid cid cid cid   

 

   

 cid    
 cid 

 

 cid 

 cid 
Yt    cid Xtw
 cid 

  log td 

tk 

 cid cid cid cid 

 cid Dtw
 cid 

 
  

 

 

 
 

   

The linear program   is called the Dantzig selector  We

 
Here        is   universal constant  and         is the

allowed failure probability   cid Dt  de ned in equation   is
   cid Xt 
  diagonal matrix that offsets the bias on the diag cid    
denote its optimal solution by  cid wt   We de ne  cid       
Based on  cid wt  we construct  cid wt   Rd  Let  cid wt     
 cid wt           cid wt id  be the coordinates sorted acto their index  Let  cid St                  ik  be the top   coordinates  We de ne  cid wt as cid wt    cid wt cid St 
The actual prediction wt is either zero if        or  cid ws for

cording to the their absolute value  breaking ties according

some       and it gets updated whenever   is   power of  
The algorithm queries at most        features each round 
and the linear program can be solved in polynomial time
using simplex method or interior point method  The algorithm solves the linear program only  cid log    cid  times by
using the same vector in the rounds                     This
lazy update improves both the computational aspects of the
algorithm and the regret bound 

 

  Main Result

The main result in this section provides   logarithmic regret
bound under the following assumptions  

  The feature vectors have the property that for any
       the matrix Xt satis es the RIP condition with
        with          log    log    
   

   more precise statement with the exact dependence on the
problem parameters can be found in the supplementary material 

Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear Regression under RIP

sampling bias on the diagonal elements of  cid    
   cid Xt
  diag cid    
 cid 
 cid 
 cid cid    
   cid Xt

and the estimated bias from the observed data is

 cid   
 cid 

 cid 
 cid 

 cid Dt  

  diag

   

Dt  

  Xt

  

 

      
 

 

Predict wt    

if        then

else if   is   power of   then

Let  cid wt be the solution of linear program  
Compute  cid wt according to  
Predict wt    cid wt

Algorithm   Dantzig Selector for POSLR
Require                
  for                   do
 
 
 
 
 
 
 
 
 
 
 
 
 
  end for

Construct estimate cid xt according to  
  to  cid Xt  to form  cid Xt   Rt  
Append cid xT

end if
Let St       be   random subset of size   
Observe xt St  and yt

Predict wt   wt 

else

  The underlying POSLR online prediction problem has

  sparsity budget of   and observation budget   

  The model is realizable as de ned in equation   with
      unbiased Gaussian noise with standard deviation
      

Theorem   For any       with probability at least    
Algorithm   satis es

RegretT     cid    log        log    cid   

The theorem asserts that an   log     regret bound is ef 
 ciently achievable in the realizable setting  Furthermore
when          the regret scales as log    meaning that
we do not necessarily require       to obtain   meaningful
result  We note that the complete expression for arbitrary
     is given in   in the supplementary material 
The algorithm can be easily understood via the errorin 
variable equation

yt    cid xt    cid        

 cid xt   xt      

with          cid xt   xt      where the expectation is
yt as well as the  noisy  feature vector  cid xt  and aims to
   cid Xt it is easy to verify

taken over random sampling introduced by the algorithm
when performing feature exploration  The learner observes
recover   
As mentioned above  we  implicitly  need an unbiased estimator of    
that the offdiagonal entries are indeed unbiased however
this is not the case for the diagonal  To this end we de ne
Dt   Rd   as the diagonal matrix compensating for the

  Xt  By taking  cid    

  cid 

  

Therefore  program   can be viewed as Dantzig selector
with plugin unbiased estimates for    
  Xt using
limited observed features 

  Yt and    

  Sketch of Proof

Lemma   It proves that the sequence of solutions  cid wt conterms  it shows that  cid cid wt     cid      
tion wt of  cid wt  Now  since  cid xt cid      we get that the

The main building block in proving Theorem   is stated in
verges to the optimal response    based on which the signal yt is created  More accurately  ignoring all second order
   In Lemma  
we show that the same applies for the sparse approximadifference between our response  cid xt  wt cid  and the  almost 
optimal response  cid xt    cid  is bounded by  
   Given this 
  careful calculation of the difference of losses leads to  
regret bound           Speci cally  an elementary analysis
of the loss expression leads to the equality

 

 

RegretT      

    cid xt       wt cid     cid xt       wt cid 

 
  bound on both summands can clearly be expressed in
terms of  cid xt       wt cid      
   The right summand
requires   martingale concentration bound and the left is
trivial  For both we obtain   bound of   log    
We are now left with two technicalities  The  rst is that
   is not necessarily the empirically optimal response  To
this end we provide  in Lemma   in the supplementary
material    constant  independent of     bound on the regret
of    compared to the empirical optimum  The second

technicality is the fact that we do not solve for  cid wt in every

round  but in exponential gaps  This translates to an added
factor of   to the bound  cid wt     cid  that affects only the
constants in the    terms 
Lemma    Estimation Rates  Assume that the matrix
Xt   Rt   satis es the RIP condition with       for some
gram   With probability at least      

      Let  cid wn    Rd be the optimal solution of pro 
 cid 
 cid 
 cid cid wt      cid       
 cid 
 cid 
 cid cid wt      cid       

    log   

 cid 
 cid 

   log   

 
  

 
  

   

   

 

 

 

 

 
  

 
  

Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear Regression under RIP

coordinates that are small in absolute value  However  we

Here       is some universal constant and   is the standard deviation of the noise 

Note the  cid wt may not be sparse  it can have many nonzero
take the top   coordinates of  cid wt in absolute value  Thanks
Lemma   Let  cid     Rd be an arbitrary vector and let
     Rd be   ksparse vector  Let  cid         be the top
  coordinates of  cid   in absolute value  Then 
 cid cid       cid   

 cid cid cid cid   cid        cid cid cid 

to the Lemma   below  we lose only   constant factor

 

 

 

 

  

  Agnostic Setting
In this section we focus on the agnostic setting  where
we don   impose any distributional assumption on the sequence  In this setting  there is no  true  sparse model  but
the learner   with limited access to features   is competing with the best ksparse model de ned using full information  xt  yt  
As before  we do assume that xt and yt are bounded  Without loss of generality   cid xt cid      and  yt      for all
   Once again  without any regularity condition on the design matrix  Foster et al    have shown that achieving
  sublinear regret       is in general computationally
hard  for any constant       unless NP   BPP 
We give an ef cient algorithm that achieves sublinear regret under the assumption that the design matrix of any
 suf ciently long  block of consecutive data points has
bounded restricted condition number  which we de ne below 
De nition    Restricted Condition Number  Let       be
  sparsity parameter  The restricted condition number for
sparsity   of   matrix     Rn   is de ned as

Note that in the random design setting where xt  for    
     are isotropic subGaussian vectors         log    
  log    suf ces to satisfy BBRCNP with high probability 
where the    notation hides   constant depending on  
We assume in this section that the sequence of feature vectors satis es BBRCNP with parameters      for some
        log     to be de ned in the course of the analysis 

  Algorithm

The algorithm in the agnostic setting is of distinct nature
from that in the stochastic setting  Our algorithm is motivated from literature on maximization of submodular
set function  Natarajan    Streeter   Golovin   
Boutsidis et al    Though the problem being NPhard  greedy algorithm on submodular maximization provides provable good approximation ratio  Speci cally 
 Streeter   Golovin    considered online optimization
of super submodular set functions using expert algorithm
as subroutine 
 Natarajan    Boutsidis et al   
cast the sparse linear regression as maximization of weakly
supermodular function  We will introduce an algorithm that
blends various ideas from referred literature  to attack the
online sparse regression with limited features 
First  let   introduce the notion of   weakly supermodular
function 
De nition   For parameters       and         set
function             is      weakly supermodular if for
any two sets             with          the following two
inequalities hold 

   monotonicity               and

   approximately decreasing marginal gain 

 cid 

     

 cid Xv cid 
 cid Xw cid   

                

sup

      cid   cid cid   cid 

 cid   cid cid   cid  

                 

It is easy to see that if   matrix   satis es RIP with parameters      then its restricted condition number for sparsity
  is at most  
  Thus  having bounded restricted condition
number is   weaker requirement than RIP 
We now de ne the Block Bounded Restricted Condition
Number Property  BBRCNP 
De nition    Block Bounded Restricted Condition Number Property  Let       and          sequence of feature
vectors               xT satis es BBRCNP with parameters
     if there is   constant    such that for any sequence
of consecutive time steps   with           the restricted
condition number for sparsity   of    the design matrix of
the feature vectors xt for         is at most  

The de nition is slightly stronger than that in  Boutsidis
et al    We will show that sparse linear regression can
be viewed as weakly supermodular minimization in De nition   once the design matrix has bounded restricted condition number 
Now we outline the algorithm  see Algorithm   We divide
the rounds               into minibatches of size   each  so
there are      such batches  The bth batch thus consists
of the examples  xt  yt  for     Tb                   
               bB  Within the bth batch  our algorithm
queries the same subset of features of size at most   
The algorithm consists of few key steps  First  one can
show that under BBRCNP  as long as   is large enough 

Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear Regression under RIP

the loss within batch   de nes   weakly supermodular set
function

gt     

 
 

inf
  RS

 yt    cid xt    cid 

 cid 

  Tb

Therefore  we can formulate the original online sparse regression problem into online weakly supermodular minimization problem  For the latter problem  we develop
an online greedy algorithm along the lines of  Streeter  
Golovin    We employ           budgeted experts
algorithms  Amin et al    denoted BEXP  with budget parameter    
  The precise characteristics of BEXP
  
are given in Theorem    adapted from Theorem   in  Amin
et al   
Theorem   For the problem of prediction from expert advice  let there be   experts  and let         be   budget
parameter  In each prediction round    the BEXP algorithm
chooses an expert jt and   set of experts Ut containing jt
of size at most    obtains as feedback the losses of all the
experts in Ut  suffers the loss of expert jt  and guarantees
  over   prediction
an expected regret bound of  
rounds 

 cid    log   

 

At the beginning of each minibatch    the BEXP algorithms are run  Each BEXP algorithm outputs   set of
coordinates of size   
as well as   special coordinate in
  
that set  The union of all of these sets is then used as the
set of features to query throughout the subsequent minibatch  Within the minibatch  the algorithm runs the standard VovkAzoury Warmuth algorithm for linear prediction with square loss restricted to set of special coordinates
output by all the BEXP algorithms 
At the end of the minibatch  every BEXP algorithm is provided carefully constructed losses for each coordinate that
was output as feedback  These losses ensure that the set of
special coordinates chosen by the BEXP algorithms mimic
the greedy algorithm for weakly supermodular minimization 

  Main Result

In this section  we will show that Algorithm   achieves sublinear regret under BBRCNP 
Theorem   Suppose the sequence of feature vectors satis es BBRCNP with parameters           for     
     log     and assume that   is large enough so that
 
          
 dk   Then if Algorithm   is run with parame 
 dk   and    as speci ed above  its expected
ters          
regret is at most      dk 
  

    

Proof  The proof relies on   number of lemmas whose

 We assume  for convenience  that    is divisible by   

Algorithm   Online Greedy Algorithm for POSLR
Require  Minibatch size    sparsity parameters    and

  
  Set up    budgeted prediction algorithms BEXP    for
        each using the coordinates in     as  experts 
with   perround budget of   
  

 

  for                      do
 

 

 

 

from BEXP    such that     

  and subset
   

For each         obtain   coordinate     
of coordinates      
     
De ne    
     cid 
Set up the VovkAzoury Warmuth  VAW  algorithm
for predicting using the features in      
for     Tb do

      and for each         de ne      

    cid      

   

 

 

 

Set St    cid 

          

 

  obtain xt St  and pass

 

  to VAW 

xt      
Set wt to be the weight vector output by VAW 
Obtain the true label yt and pass it to VAW 

end for
De ne the function

gb     

 
 

inf
  RS

 cid 

  Tb

 yt    cid xt    cid 

 

  compute gb      
For each          
pass it BEXP    as the loss for expert   

 

 

      and

 

 

 
 

 
 
 
 

 

  end for

proofs can be found in the supplementary material  We begin with the connection between sparse linear regression 
weakly supermodular function and RIP  formally stated in
Lemma   This lemma is   direct consequence of Lemma
  in  Boutsidis et al   
Lemma   Consider   sequence of examples  xt  yt   
Rd     for                    and let   be the design matrix
for the sequence  Consider the set function associated with
least squares optimization 

       inf
  RS

 
 

 yt    cid xt    cid 

  cid 

  

Suppose the restricted condition number of   for sparsity
  is bounded by   Then      is      weakly supermodular 

Even though minimization of weakly supermodular functions is NPhard  the greedy algorithm provides   good approximation  as shown in the next lemma 
Lemma   Consider        weakly supermodular set
function    Let      arg minj      Then  for any

Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear Regression under RIP

              cid 

subset   of size at most    we have
     
    

 cid 

           

The BEXP algorithms essentially implement the greedy algorithm in an online fashion  Using the properties of the
BEXP algorithm  we have the following regret guarantee 
Lemma   Suppose the sequence of feature vectors satis 
 es BBRCNP with parameters           Then for any set
  of coordinates of size at most    we have

 

gb      

 

    gb    

 cid   

     
    

 gb    gb         

 cid  dk  log    

   

 

Finally  within every minibatch  the VAW algorithm guarantees the following regret bound  an immediate consequence of Theorem   in CesaBianchi   Lugosi  
Lemma   Within every batch    the VAW algorithm generates weight vectors wt for     Tb such that

 yt    cid xt  wt cid    Bgb      

 

         log   

We can now prove Theorem   Combining the bounds of
lemma   and   we conclude that for any subset of coordinates   of size at most    we have

      cid 
 cid 
     cid 

  

  

 

 

 cid 

  Tb

 cid    cid 
     cid 
 cid 

  

  

 

 

 cid 

 yt    cid xt  wt cid 

 cid  dk  log   BT

  

 

 

 

Bgb             

        gb    gb    

   

  

Finally  note that

     cid 

  

Bgb       inf
  RV

 cid 

 

 
 

   log   

 

  cid 

  

 yt    cid xt    cid 

and

     cid 

  

      

        gb    gb           exp    

    

because gb      Using these bounds in   and plugging in the speci ed values of   and    we get the stated
regret bound 

  Conclusions and Future Work
In this paper  we gave computationally ef cient algorithms
for the online sparse linear regression problem under the
assumption that the design matrices of the feature vectors
satisfy RIPtype properties  Since the problem is hard without any assumptions  our work is the  rst one to show that
assumptions that are similar to the ones used to sparse recovery in the batch setting yield tractability in the online
setting as well 
Several open questions remain in this line of work and will
be the basis for future work  Is it possible to improve the
regret bound in the agnostic setting  Can we give matching lower bounds on the regret in various settings  Is it
possible to relax the RIP assumption on the design matrices and still have ef cient algorithms  Some obvious
weakenings of the RIP assumption we have made don  
yield tractability  For example  simply assuming that the
 nal matrix XT satis es RIP rather than every intermediate matrix Xt for large enough   is not suf cient    simple tweak to the lower bound construction of Foster et al 
  shows this  This tweak consists of simply padding
the construction with enough dummy examples which are
wellconditioned enough to overcome the illconditioning
of the original construction so that RIP is satis ed by XT  
We note however that in the realizable setting  our analysis can be easily adapted to work under weaker conditions
such as irrepresentability  Zhao   Yu    Javanmard  
Montanari   

References
Amin  Kareem  Kale  Satyen  Tesauro  Gerald  and Turaga 
Deepak    Budgeted prediction with expert advice  In
AAAI  pp     

Belloni  Alexandre  Rosenbaum  Mathieu  and Tsybakov 
Alexandre    Linear and conic programming estimators
in high dimensional errorsin variables models  Journal of the Royal Statistical Society  Series    Statistical
Methodology    ISSN  

Bickel  Peter    Ritov  Ya acov  and Tsybakov  Alexandre    Simultaneous analysis of Lasso and Dantzig selector  The Annals of Statistics  pp     

Boutsidis  Christos  Liberty  Edo  and Sviridenko  Maxim 
Greedy minimization of weakly supermodular set functions  arXiv preprint arXiv   

Candes  Emmanuel and Tao  Terence  The Dantzig selector  statistical estimation when   is much larger than   
The Annals of Statistics  pp     

Candes  Emmanuel   and Tao  Terence  Decoding by linear

Adaptive Feature Selection  Computationally Ef cient Online Sparse Linear Regression under RIP

programming  IEEE transactions on information theory 
   

CesaBianchi  Nicol   and Lugosi    abor 

Prediction 
learning  and games  Cambridge University Press   

CesaBianchi  Nicol    ShalevShwartz  Shai  and Shamir 
Ohad  Ef cient learning with partially observed atJournal of Machine Learning Research   
tributes 
 Oct   

Foster  Dean  Karloff  Howard  and Thaler  Justin  Variable

selection is hard  In COLT  pp     

Foster  Dean  Kale  Satyen  and Karloff  Howard  Online

sparse linear regression  In COLT   

Hazan  Elad and Kale  Satyen 

Beyond the regret
minimization barrier  optimal algorithms for stochastic stronglyconvex optimization  Journal of Machine
Learning Research     

Hazan  Elad and Koren  Tomer  Linear regression with lim 

ited observation  In ICML   

Javanmard  Adel and Montanari  Andrea  Model selection
for highdimensional regression under the generalized
In NIPS  pp   
irrepresentability condition 
 

Kale  Satyen  Open problem  Ef cient online sparse re 

gression  In COLT  pp     

Kukliansky  Doron and Shamir  Ohad  Attribute ef cient
linear regression with distributiondependent sampling 
In ICML  pp     

Mendelson  Shahar  Learning without concentration 

COLT  pp     

In

Natarajan  Balas Kausik  Sparse approximate solutions to
linear systems  SIAM journal on computing   
   

Rosenbaum  Mathieu and Tsybakov  Alexandre    Sparse
recovery under matrix uncertainty  The Annals of Statistics     

Streeter  Matthew    and Golovin  Daniel  An online algoIn NIPS 

rithm for maximizing submodular functions 
pp     

Zhao  Peng and Yu  Bin  On model selection consistency
of lasso  Journal of Machine learning research   Nov 
   

Zolghadr  Navid  Bart ok    abor  Greiner  Russell  Gy orgy 
Andr as  and Szepesv ari  Csaba  Online learning with
In NIPS  pp   
costly features and labels 
 

