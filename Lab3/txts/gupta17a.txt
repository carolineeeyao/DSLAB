ProtoNN  Compressed and Accurate kNN for Resourcescarce Devices

Chirag Gupta   Arun Sai Suggala     Ankit Goyal     Harsha Vardhan Simhadri  

Bhargavi Paranjape   Ashish Kumar   Saurabh Goyal   Raghavendra Udupa   Manik Varma  

Prateek Jain  

Abstract

Several realworld applications require realtime
prediction on resourcescarce devices such as an
Internet of Things  IoT  sensor  Such applications demand prediction models with small storage and computational complexity that do not
compromise signi cantly on accuracy 
In this
work  we propose ProtoNN    novel algorithm
that addresses the problem of realtime and accurate prediction on resourcescarce devices  ProtoNN is inspired by kNearest Neighbor  KNN 
but has several orders lower storage and prediction complexity  ProtoNN models can be deployed even on devices with puny storage and
computational power      
an Arduino UNO
with  kB RAM  to get excellent prediction accuracy  ProtoNN derives its strength from three key
ideas     learning   small number of prototypes
to represent the entire training set     sparse low
dimensional projection of data     joint discriminative learning of the projection and prototypes
with explicit model size constraint  We conduct
systematic empirical evaluation of ProtoNN on
  variety of supervised learning tasks  binary 
multiclass  multilabel classi cation  and show
that it gives nearly stateof theart prediction accuracy on resourcescarce devices while consuming several orders lower storage  and using minimal working memory 

  Introduction
Realtime and accurate prediction on resourceconstrained
devices is critical for several Machine Learning  ML  do 

India

India 

 Microsoft Research 

 Carnegie Mellon University  Pittsburgh  University of Michigan  Ann Arbor
 IIT Delhi 
Correspondence to  Arun Sai Suggala  asuggala andrew cmu edu 
Jain  prajain microsoft com 

Prateek

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

mains  Internetof things  IoT  is one such rapidly growing domain  IoT devices have the potential to provide realtime  local  sensorbased solutions for   variety of areas
like housing  factories  farming  even everyday utilities like
toothbrushes and spoons  The ability to use machine learning on data collected from IoT sensors opens up   myriad
of possibilities  For example  smart factories measure temperature  noise and various other parameters of their machines  ML based anomaly detection models can then be
applied on this sensor data to preemptively schedule maintenance of   machine and avoid failure 
However  machine learning in IoT scenarios is so far limited to cloudbased predictions where large deep learning
models are deployed to provide accurate predictions  The
sensors embedded devices have limited compute storage
abilities and are tasked only with sensing and transmitting
data to the cloud  Such   solution does not take into account
several practical concerns like privacy  bandwidth  latency
and battery issues  For example  consider the energy costs
of communication if each IoT device on each machine in
  smart factory has to continuously send data and receive
predictions from the cloud 
Consider   typical IoT device that has    kB RAM and
   MHz processor  Most existing ML models cannot be
deployed on such tiny devices  Recently  several methods
 Han et al    Nan et al    Kusner et al    have
been proposed to produce models that are compressed compared to large DNN kernelSVM decisiontree based classi ers  However  none of these methods work well at the
scale of IoT devices  Moreover  they do not offer natural
extensions to supervised learning problems other than the
ones they were initially designed for 
In this paper  we propose   novel kNN based algorithm
 ProtoNN  that can be deployed on the tiniest of devices 
can handle general supervised learning problems  and can
produce stateof theart accuracies with just  kB of
model size on many benchmark datasets    key reason for
selecting kNN as the algorithm of choice is due to its generality  ease of implementation on tiny devices  and small
number of parameters to avoid over tting  However  kNN
suffers from three issues which limit its applicability in

ProtoNN  kNN for Resourcescarce Devices

practice  especially in the small devices setting     Poor
accuracy  kNN is an illspeci ed algorithm as it is not  
priori clear which distance metric one should use to compare   given set of points  Standard metrics like Euclidean
distance   cid  distance etc  are not taskspeci   and lead to
poor accuracies     Model size  kNN requires the entire
training data for prediction  so its model size is too large
for the IoT setting     Prediction time  kNN requires computing the distance of   given test point        each training
point  making it prohibitive for prediction in realtime 
Several methods have been proposed to address some of
these concerns  For example  metric learning  Weinberger
  Saul    learns   taskspeci   metric that provides
better accuracies but ends up increasing modelsize and
prediction time  KDtrees  Bentley    can decrease the
prediction time  but they increase the model size and lead
to loss in accuracy  Finally  recent methods like Stochastic
Neighborhood Compression  SNC   Kusner et al   
can decrease model size and prediction time by learning  
small number of prototypes to represent the entire training
dataset  However  as our experiments show  their predictions are relatively inaccurate  especially in the tiny modelsize regime  Moreover  their formulations limit applicability to binary and multiclass classi cation problems  see
Section   for   detailed comparison to SNC 
In contrast  ProtoNN is able to address the abovementioned concerns by using three key ideas 
   Sparse lowd projection  we project the entire data
in lowd using   sparse projection matrix that is jointly
learned to provide good accuracy in the projected space 
   Prototypes  we learn prototypes to represent the entire
training dataset  Moreover  we learn labels for each prototype to further boost accuracy  This provides additional
 exibility  and allows us to seamlessly generalize ProtoNN
for multilabel or ranking problems 
   Joint optimization  we learn the projection matrix jointly
with the prototypes and their labels  Explicit sparsity constraints are imposed on our parameters during the optimization itself so that we can obtain an optimal model within the
given model size defacto  instead of postfacto pruning to
force the model to    in memory 
Unfortunately  our optimization problem is nonconvex
with hard  cid  constraints  Yet  we show that simple stochastic gradient descent  SGD  with iterative hardthresholding
 IHT  works well for optimization  ProtoNN can be implemented ef ciently  can handle datasets with millions of
points  and obtains stateof theart accuracies 
We analyze ProtoNN in   simple binary classi cation setting where the data is sampled from   mixture of two wellseparated Gaussians  each Gaussian representing one class 

We show that if we    the projection matrix and prototype labels  the prototypes themselves can be learned optimally in polynomial time with at least   constant probability  Moreover  assuming   strong initialization condition
we observe that our SGD IHT method when supplied  
small number of samples  proportional to the sparsity of
means  converges to the global optima  Although the data
model is simple  it nicely captures the main idea behind our
problem formulation  Further  our analysis is the  rst such
analysis for any method in this regime that tries to learn  
compressed nonlinear model for binary classi cation 
Finally  we conduct extensive experiments to benchmark
ProtoNN against existing stateof theart methods for various learning tasks  First  we show that on several binary  multiclass  problems  ProtoNN with    kB  kB 
memory budget signi cantly outperforms all the existing
methods in this regime  Moreover  in the binary classi 
 cation case  we show that ProtoNN with just    kB
of modelsize  provides nearly the same accuracy as most
popular methods like GBDT  RBFSVM   hidden layer
NN  etc  which might require up to  GB of RAM on the
same datasets  Similarly  on multilabel datasets  ProtoNN
can give   compression with     loss in accuracy 
Finally  we demonstrate that ProtoNN can be deployed on
  tiny Arduino Uno device  and leads to better accuracies
than existing methods while incurring signi cantly lesser
energy and prediction time costs  We have implemented
ProtoNN as part of an open source embedded device ML
library and it can be downloaded online 

  Related Works
kNN is   popular ML algorithm owing to its simplicity 
generality  and interpretability  Cover   Hart   
In
particular  kNN can learn complex decision boundaries and
has only one hyperparameter    However  vanilla kNN suffers from several issues as mentioned in the previous section    number of methods  which try to address these issues  exist in the literature  Broadly  these methods can be
divided into three subcategories 
Several existing methods reduce prediction time of kNN
using fast nearest neighbor retrieval  For example Bentley   Beygelzimer et al    use tree data structures and Gionis et al    Weiss et al    Kulis
  Darrell   Norouzi et al    Liu et al   
learn binary embeddings for fast nearest neighbor operations  These methods  although helpful in reducing the prediction time  lead to loss in accuracy and require the entire
training data to be in memory leading to large model sizes
that cannot be deployed on tiny IoT devices 

 https www arduino cc en Main ArduinoBoardUno
 https github com Microsoft ELL

ProtoNN  kNN for Resourcescarce Devices

Another class of methods improve accuracy of kNN by
learning   better metric to compare  given   pair of points
 Goldberger et al    Davis et al    For example 
 Weinberger   Saul    proposed   Large Margin Nearest Neighbor  LMNN  classi er which transforms the input
space such that in the transformed space points from same
class are closer compared to points from disparate classes 
LMNN   transformation matrix can map data into lower dimensions and reduce overall model size compared to kNN 
but it is still too large for most resourcescarce devices 
Finally  another class of methods constructs   set of prototypes to represent the entire training data  In some approaches  Angiulli    Devi   Murty    the prototypes are chosen from the original training data  while
some other approaches  Mollineda et al    construct
arti cial points for prototypes  Of these approaches  SNC 
Deep SNC  DSNC   Wang et al    Binary Neighbor
Compression  BNC   Zhong et al    are the current
stateof theart 
SNC learns arti cial prototypes such that the likelihood of
  particular class probability model is maximized  Thus 
SNC applies only to multiclass problems and its extension
to multilabel ranking problems is nontrivial  In contrast 
we have   more direct discriminative formulation that can
be applied to arbitrary supervised learning problems  To
decrease the model size  SNC introduces   preprocessing
step of lowd projection of the data via LMNN based projection matrix and then learns prototypes in the projected
space  The SNC parameters  projection matrix  prototypes 
might have to be hardthresholded postfacto to    within
the memory budget 
In contrast  ProtoNN   parameters
are defacto learnt jointly with model size constraints imposed during optimization  This leads to signi cant improvements over SNC and other stateof theart methods in
the small modelsize regime  see Figure    
DSNC is   nonlinear extension of SNC in that it learns
  nonlinear lowd transformation jointly with the prototypes  It has similar drawbacks as SNC     it only applies
to multiclass problems and    model size of DSNC can
be signi cantly larger than SNC as it uses   feedforward
network to learn the nonlinear transformation 
BNC is   binary embedding technique  which jointly learns
  binary embedding and   set of arti cial binary prototypes 
Although BNC learns binary embeddings  its dimensionality can be signi cantly higher  so it need not result in signi cant model compression  Moreover  the optimization in
BNC is dif cult because of the discrete optimization space 

  Problem Formulation
Given   data points                  xn   and the corresponding target output                  yn     where xi  

Rd  yi      our goal is to learn   model that accurately predicts the desired output of   given test point  In addition 
we also want our model to have small size  For both multilabel multiclass problems with   labels  yi        but
in multiclass  cid yi cid      Similarly  for ranking problems 
the output yi is   permutation 
Let   consider   smooth version of kNN prediction function
for the above given general supervised learning problem

 cid    cid 

 cid 

            

 yi      xi 

 

 

  

 cid  
where    is the predicted output for   given input        
    yi      xi  is the score vector for           
RL maps   given output into   score vector and     RL  
  maps the score function back to the output space  For
example  in the multiclass classi cation    is the identity
function while     Top  where  Top         if sj is
the largest element and   otherwise      Rd   Rd    
is the similarity function         xi  xj  computes similarity between xi and xj  For example  standard kNN uses
     xi      xi   Nk    where Nk    is the set of  
nearest neighbors of   in   
Note that kNN requires entire   to be stored in memory for prediction  so its model size and prediction time
are prohibitive for resource constrained devices  So  to
bring down model and prediction complexity of kNN  we
propose using prototypes that represent the entire training data  That is  we learn prototypes                 bm 
and the corresponding score vectors                 zm   
RL    so that the decision function is given by      
 

 cid cid  

 cid 

   zjK    bj 

 

Existing prototype based approaches like SNC  DSNC have
  speci   probabilistic model for multiclass problems with
the prototypes as the model parameters 
In contrast  we
take   more direct discriminative learning approach that allows us to obtain better accuracies in several settings along
with generalization to any supervised learning problem 
     multilabel classi cation  regression  ranking  etc 
However    is    xed similarity function like RBF kernel
which is not tuned for the task at hand and can lead to inaccurate results  We propose to solve this issue by learning
  lowdimensional matrix            that further brings
down model prediction complexity as well as transforms
data into   space where prediction is more accurate That is 
our proposed algorithm ProtoNN uses the following prediction function that is based on three sets of learned parameters                             bm            and    
            zm    RL          
To further reduce the model prediction complexity  we
learn sparse set of           Selecting the correct simi 

 cid cid  

   zjK      bj 

 cid 

 

ProtoNN  kNN for Resourcescarce Devices

larity function   is crucial to the performance of the algorithm  In this work we choose   to be the Gaussian kernel            exp cid       cid 
  which is   popular
choice in many nonparametric methods  including regression  classi cation  density estimation 
Note that if        and     Id    then our prediction
function reduces to the standard RBF kernelSVM   decision function for binary classi cation  That is  our function class is universal  we can learn any arbitrary function
given enough data and model complexity  We observe  
similar trend in our experiments  where even with reasonably small amount of model complexity  ProotNN nearly
matches RBFSVM   prediction error 
Training Objective  We now provide the formal optimization problem to learn parameters           Let         be
the loss  or  risk of predicting score vector    for   point
with label vector    For example  the loss function can be
standard hingeloss for binary classi cation  or NDCG loss
function for ranking problems 
Now  de ne the empirical risk associated with         as

   

  cid 

  

 

  cid 

  

 yi 
 cid 

Remp            

 
 

zjK bj    xi 

  cid 

yi cid  

In the sequel  to simplify the notation  we denote the risk
at ith data point by Li                Li            
  To jointly learn          
we minimize the empirical risk with explicit sparsity constraints 

   zjK bj    xi 

min

Remp            
  cid   cid sZ    cid   cid sB     cid   cid sW
where  cid   cid  is equal to the number of nonzero entries in
   For all our expeirments  multiclass multilabel  we
used the squared  cid  loss function as it helps us write down
the gradients easily and allows our algorithm to converge
faster and in   robust manner  That is  Remp            
  Note that the
 
 
sparsity constraints in the above objective gives us explicit
control over the model size  Furthermore  as we show in
our experiments  jointly optimizing all the three parameters            leads to better accuracies than optimizing
only   subset of parameters 

    cid yi    cid  
 cid  

   zjK bj    xi cid 

  Algorithm
We now present our algorithm for optimization of   Note
that the objective in   is nonconvex and is dif cult to
optimize  However  we present   simple alternating minimization technique for its optimization  In this technique 
we alternately minimize         while  xing the other two
parameters  Note that the resulting optimization problem

Algorithm   ProtoNN  Train Algorithm

Input  data         sparsities  sZ  sB  sW   kernel parameter   projection dimension     no  of prototypes   
iterations     SGD epochs   
Initialize        
for       to   do  alternating minimization 

repeat  minimization of   

randomly sample               
    HTsZ
until   epochs
repeat  minimization of   

randomly sample               
    HTsB
until   epochs
repeat  minimization of   

     ZLi          cid 
 cid       
 cid 
     BLi          cid 
 cid       
 cid 
 cid       
     WLi          cid 
 cid 

randomly sample               
    HTsW
until   epochs

end for
Output         

 cid      cid 

     ZLi          cid  where

in each of the alternating steps is still nonconvex  To optimize these subproblems we use projected Stochastic Gradient Descent  SGD  for large datasets and projected Gradient Descent  GD  for small datasets 
Suppose we want to minimize the objective         by
 xing        Then in each iteration of SGD we randomly sample   minibatch                and update  
as      HTsZ
HTsZ     is the hard thresholding operator that thresholds
the smallest         sZ entries  in magnitude  of   and
 ZLi           denotes the partial derivative of Li      
   Note that GD procedure is just SGD with batch size
         Algorithm   presents pseudocode for our entire
training procedure 
Stepsize  Setting correct stepsize is critical to convergence of SGD methods  especially for nonconvex optimization problems  For our algorithm  we select the initial step size using Armijo rule  Subsequent step sizes are
selected as         where   is the initial stepsize 
Initialization  Since our objective function   is nonconvex  good initialization for         is critical in converging ef ciently to   good local optima  We used  
randomly sampled Gaussian matrix to initialize   for binary and small multiclass benchmarks  However  for large
multi class datasets  aloi  we use LMNN based initialization of    Similarly  for multilabel datasets we use SLEEC
 Bhatia et al    for initialization of     SLEEC is an
embedding technique for large multilabel problems 
For initialization of prototypes  we experimented with two
different approaches  In one  we randomly sample training

ProtoNN  kNN for Resourcescarce Devices

data points in the transformed space and assign them as the
prototypes  this is   useful technique for multilabel problems 
In the other approach  we run kmeans clustering
in the transformed space on data points belonging to each
class and pick the cluster centers as our prototypes  We use
this approach for binary and multiclass problems 
Convergence  Although Algorithm   optimizes an  cid  constrained optimization problem  we can still show that it
converges to   local minimum due to smoothness of objective function  Blumensath   Davies    Moreover  if
the objective function satis es strong convexity in   small
ball around optima  then appropriate initialization leads to
convergence to that optima  Jain et al    In fact  our
next section presents such   strong convexity result  wrt   
if the data is generated from   mixture of wellseparated
Gaussians  Finally  our empirical results  Section   indicate that the objective function indeed converges at   fast
rate to   good local optimum leading to accurate models 

  Analysis
In this section  we present an analysis of our approach
for when data is generated from the following generative
model  let each point xi be sampled from   mixture of two
                       Rd
Gaussians       xi
and the corresponding label yi be the indicator of the Gaussian from which xi is sampled  Now  it is easy to see that
if the Gaussians are wellseparated then one can design  
     such that the error of our method with
prototypes   
      and  xed            will lead to nearly Bayes 
optimal classi er  ei is the ith canonical basis vector 
The goal of this section is to show that our method that
optimizes the squared  cid  loss objective          prototypes
   converges at   linear rate to   solution that is in   small
ball around the global optima  and hence leads to nearly
optimal classi cation accuracy 
We would like to stress that the goal of our analysis is to
justify our proposed approach in   simple and easy to study
setting  We do not claim new bounds for the mixture of
Gaussians problem  it is   wellstudied problem with several solid solutions  Our goal is to show that our method
in this simple setting indeed converges to   nearly optimal
solution at linear rate  thus providing some intuition for its
success in practice  Also  our current analysis only studies
optimization       
the prototypes   while  xing projection matrix   and prototype label vectors    Studying the
problem        all the three parameters is signi cantly more
challenging  and is beyond the scope of this paper 
Despite the simplicity of our model  ours is one of the  rst
rigorous studies of   classi cation method that is designed
for resource constrained problems  Typically  the proposed
methods in this regime are only validated using empirical

results as theoretical study is quite challenging owing to the
obtained nonconvex optimization surface and complicated
modeling assumptions 
For our  rst result  we ignore sparsity of         sB       
 
We consider the RBFkernel for   with      
Theorem   Let                 xn  and                 yn 
be generated from the above mentioned generative model 
Set                   and let       be the prototypes 
Let                 Also  let           
   cid cid  for some
           and let  
 xed constant       and          cid cid  for some
constant       Then  the following holds for the gradient
            where       Remp 
descent step   
 cid cid 
and       is appropriately chosen 
 cid   

 cid cid     cid   cid 

         

      exp

   cid cid 

 cid 

 cid 

 

 

 cid 
 cid   cid cid 

 

 cid 

 

if  cid cid     cid cid  exp

See Appendix   for   detailed proof of this as well as the
below given theorem  The above theorem shows that if the
Gaussians are wellseparated and the starting    is closer
to   than   then the gradient descent step decreases the
distance between    and   geometrically until    converges to   small ball around   the radius of the ball is
exponentially small in  cid     cid  Note that our initialization method indeed satis es the above mentioned assumption with at least   constant probability 
It is easy to see that in this setting  the loss function decomposes over independent terms from    and    and hence
an identical result can be obtained for    For simplicity 
we present the result for        hence  expected value 
Extension to  nite samples should be fairly straightforward
using standard tail bounds  The tail bounds will also lead to
  similar result for SGD but with an added variance term 
Next  we show that if the    is even closer to   then the
objective function becomes strongly convex in      
Theorem   Let             be as given in Theo 
   cid cid  for some small
rem   Also  let  
constant        cid cid   
 ln     and  cid cid      Then 
  with       and            is   strongly convex function of   with condition number bounded by  

         

 

Note that the initialization assumptions are much more
strict here  but strong convexity with bounded condition
number provides signi cantly faster convergence to optima  Moreover  this theorem also justi es our IHT based
method  Using standard tail bounds  it is easy to show
that if   grows linearly with sB rather than    the condition number bound still holds over sparse set of vectors 
     for sparse     and sparse       Using this restricted strong convexity with  Jain et al    guarantees

ProtoNN  kNN for Resourcescarce Devices

is de ned as      cid bj     xi cid         
ProtoNN vs  Uncompressed Baselines  In this experiment we compare the performance of ProtoNN with uncompressed baselines and demonstrate that even with compression  ProtoNN achieves near stateof theart accuracies  We restrict the model size of ProtoNN to  kB for
binary datasets and to  kB for multiclass datasets and
don   place any constraints on the model sizes of baselines 
We compare ProtoNN with  GBDT  RBFSVM   Hidden
Layer Neural Network  hidden NN  kNN  BNC and
SNC  For baselines the optimal hyperparameters are selected through crossvalidation  For SNC  BNC we set projection dimensions to     respectively and compression ratios to     For ProtoNN  hyperparameters
are set based on the following heuristics which ensure that
the model size constraints are satis ed  Binary        
sZ   sB           if sW     gives model larger
than  kB  Else  sW     and   is increased to reach
  kB model  Multiclass         sZ   sB    
     class if sW     gives model larger than  kb 
Else    is increased to reach   kB model  CUReT which
has   classes  requires smaller sZ to satisfy model size
constraints 
We use the above parameter settings for all binary  multiclass datasets except for binary versions of usps  character and eye which require  fold cross validation  Table  
presents the results on binary datasets and Table   presents
the results on multiclass datasets  For most of the datasets 
ProtoNN gets to within   accuracy of the best uncompressed baseline with       orders of magnitude reduction
in model size  For example on character recognition  ProtoNN is   more accurate than the best method  RBFSVM  while getting     compression in model size 
Similarly  on letter  our method is within   accuracy
of RBFSVM while getting     compression  Also note
that ProtoNN with  kB models is still able to outperform
BNC  SNC on most of the datasets 
ProtoNN vs  Compressed Baselines  In this experiment
we compare the performance of ProtoNN with other stateof theart compressed methods in the  kB model size
regime  BudgetRF  Nan et al    Decision Jungle
 Shotton et al    LDKL  Jose et al    Tree Pruning  Dekel et al    GBDT  Friedman    Budget Prune  Nan et al    SNC and NeuralNet Pruning  Han et al    All baselines plots are obtained
via crossvalidation  Figure   presents the memory vs 
accuracy plots  Hyperparameters of ProtoNN are set as
follows  Binary 
sB   sZ     For        
kB               sW     are set using the same
heuristic mentioned in the previous paragraph  Multiclass 
sB     For         kB              
sW   sZ    are set as de ned in the previous paragraph 
ProtoNN values obtained with the above hyperparameters

Figure   Model size  kB  Xaxis  vs Accuracy   Yaxis  comparison of ProtoNN against existing compression algorithms on
various datasets  The left two columns show the plots for binary
datasets and the right most column shows the plots for multiclass
datasets  For small model size  ProtoNN is signi cantly more accurate than baselines 

that with just   sB log    samples  our method will converge to   small ball around sparse   in polynomial time 
We skip these standard details as they are orthogonal to the
main point of this analysis section 

  Experiments
In this section we present the performance of ProtoNN
on various benchmark binary  multiclass and multilabel
datasets with   goal to demonstrate the following aspects 
   In severely resource constrained settings where we require model sizes to be less than  kB  which occur routinely for IoT devices like Arduino Uno  we outperform
all stateof the art compressed methods 
   For model sizes in the range       kB  we achieve
comparable accuracies to the best uncompressed methods 
   In multiclass and multilabel problems we achieve near
stateof theart accuracies with an order of magnitude reduction in model size  thus showing our approach is  exible
and general enough to handle   wide variety of problems 
Experimental Settings  Datasets 
Table   in Appendix   lists the binary  multiclass and multilabel
datasets used in our experiments  For binary and multiclass datasets  we standardize each feature in the data to
zeromean and unitvariance  For multilabel datasets  we
normalize the feature vector of each data point by projecting it onto   unit norm ball which preserves data sparsity 
Hyperparameters  In all our experiments  we    the no 
of alternating minimization iterations    to   Each such
iteration does emany epochs each over the   parameters 
       and    For small binary and multiclass datasets we
do GD with   set to   For multilabel and large multiclass  aloi  datasets  we do SGD with   set to   batch size
to   Kernel parameter   is computed after initializing
     as
median      where   is the set of distances between
prototypes and training points in the transformed space and

 

ProtoNN  kNN for Resourcescarce Devices

Table   Comparison of ProtoNN with uncompressed baselines on binary datasets  Model size is computed as  parameters    bytes 
sparse matrices taking an extra   bytes for each nonzero entry  for the index  For BNC it is computed as  parameters  bytes  GBDT
model size is computed using the  le size on disk 

BNC
 
 
 
 
 
 
 
 
 
 
 
 

GBDT

 hidden NN RBFSVM

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
 
 
 
 
 
 
 
 

Dataset

ProtoNN

character recognition model size  kB 
model size  kB 

accuracy

eye

accuracy

mnist

usps

ward

cifar

model size  kB 

accuracy

model size  kB 

accuracy

model size  kB 

accuracy

model size  kB 

accuracy

 
 
 
 
 
 
 
 
 
 
 
 

kNN
 
 
 
 
 

 
 
 

 
 
 
 

SNC
 
 
 
 
 
 
 
 
 
 
 
 

Table   Comparison of ProtoNN with uncompressed baselines on
multiclass datasets  First number in each cell refers to the model
size in kB and the second number denotes accuracy  Refer to
Table   for details about calculation of model size 

Dataset

letter 

mnist 

usps 

curet 

ProtoNN
 kB 
 
 
 
 
 
 
 
 

kNN
 
 

 

 
 
 
 
 

SNC
 
 
 
 
 
 
 
 

BNC
 
 
 
 
 
 
 
 

GBDT
 
 
 

 
 
 
 
 

 hidden

NN

 
 
 
 
 
 
 
 

RBF
SVM
 
 
 

 
 
 
 
 

are reported for all datasets  except usps and character
recognition which require  fold cross validation  ProtoNN
performs signi cantly better than the baselines on all the
datasets  This is especially true in the  kB regime  where
ProtoNN is     more accurate on most of the datasets 
ProtoNN on Multilabel and Large Multiclass Datasets 
We now present the performance of ProtoNN on larger
datasets  Here  we experimented with the following
datasets  aloi dataset which is   relatively large multiclass dataset   mediamill  delicious  eurlex which are smallmedium sized multilabel datasets 
  
We set the hyperparameters of ProtoNN as follows 
is set to   for all datasets  except for eurlex for which
we set
to   Other parameters are set as follows  sW     sB     sZ      for aloi and
sZ    avg  number of labels per training point   for
multilabel datasets            for multilabel datasets 
For aloi  we compare ProtoNN with the following baselines   vsA    Logistic Regression  vsALogi  RBFSVM  FastXML    largescale multilabel method  Prabhu
  Varma    Recall Tree    scalable method for large
multiclass problems  Daume III et al    For  vsALogi  Recall Tree we perform cross validation to pick the
best tuning parameter  For FastXML we use the default parameters  For RBFSVM we set   to the default value   

it

and do   limited tuning of the regularization parameter 
Left table of Figure   shows that ProtoNN  with    
  gets to within   of the accuracy of RBFSVM with
just  th of its model size and   times fewer  oating
point computations per prediction  For   better comparison of ProtoNN with FastXML  we set the number of prototypes        such that computations prediction of
both the methods are almost the same  We can see that ProtoNN gets similar accuracy as FastXML but with   model
size   orders of magnitude smaller than FastXML  Finally 
our method has almost same prediction cost as RecallTree
but with   higher accuracy and   smaller model size 
Right table of Figure   presents preliminary results on multilabel datasets  Here  we compare ProtoNN with SLEEC 
FastXML and DiSMEC  Babbar   Sh olkopf    which
learns    vsA linearSVM in   distributed fashion  ProtoNN almost matches the performance of all baselines with
huge reduction in model size 
These results show that ProtoNN is very  exible and can
handle   wide variety of problems very ef ciently  SNC
doesn   have such  exibility  For example  it can   be naturally extended to handle multilabel classi cation problems 
ProtoNN vs  BNC  SNC  In this experiment  we do   thorough performance comparison of ProtoNN with BNC and
SNC  To show that ProtoNN learns better prototypes than
BNC  SNC  we    the projection dimension    of all the
methods and vary the number of prototypes    To show
that ProtoNN learns   better embedding  we      and vary
    For BNC  which learns   binary embedding     is chosen such that the  parameters in its transformation matrix is   times the  parameters in transformation matrices of ProtoNN  SNC    is chosen similarly  Figure  
presents the results from this experiment on mnist binary
dataset  We use the following hyper parameters for ProtoNN  sW     sZ   sB     For SNC  we hard
threshold the input transformation matrix so that it has sparsity   Note that for small    our method is as much as

ProtoNN  kNN for Resourcescarce Devices

Figure   Left Table  ProtoNN vs baselines on aloi dataset  For Recall Tree we couldn   compute the avg  number of computations
needed per prediction  instead we report the prediction time        vsALogi  Right Table  ProtoNN vs baselines on multilabel datasets 
For SLEEC and FastXML we use the default parameters from the respective papers  Both the tables show that our method achieves
similar accuracies as the baselines  but often with       orders of magnitude compression in model size  On aloi our method is at most
  slower than  vsall while RBFSVM is   slower 

FastXML DiSMEC SLEEC ProtoNN

Method
 vsALogi
RBFSVM
FastXML
Recall Tree
ProtoNN
      
ProtoNN
      

Accuracy Model Size MB 

computations prediction

       vsALogi

 
 
 
 

 

 

 
 
 
 
 

 

 

 
 
 

 

 

Figure   Comparison of ProtoNN with BNC  SNC on mnist binary dataset with varying projection dimension    or number of
prototypes   

  more accurate than SNC    more accurate than BNC
and reaches nearly optimal accuracy for small    or   
Remark   Before we conclude the section we provide
some practical guidelines for hyperparameter selection in
ProtoNN  Consider the following two cases 
   Small       cid      In this case  parameters    and
sW govern the model size  Given   model size constraint 
 xing one parameter  xes the other  so that we effectively
have one hyperparameter to crossvalidate  Choosing  
such that             typically gives good accuracies 
   Large       cid      In this case  sZ also governs the
model size  sZ  sW and    can be selected through crossvalidation  If the model size allows it  increasing    typically
helps  Fixing     to   reasonable value such as   for
medium      for large   typically gives good accuracies 

  Experiments on tiny IoT devices
In the previous section  we showed that ProtoNN gets better
accuracies than other compressed baselines at low model
size regimes  For small devices  it is also critical to study
other aspects like energy consumption  which severely impact the effectiveness of   method in practice  In this section  we study the energy consumption and prediction time
of ProtoNN model of size  kB when deployed on an Arduino Uno The Arduino Uno has an   bit    MHz Atmega   microcontroller  with  kB of SRAM and  kB

Dataset
mediamill
     

     
     
delicious
     

     
     
eurlex

     
     
     

model size

  
  
  

model size

  
  
  

model size

  
  
  

  
 
 
 
  
 
 
 
  
 
 
 

  
 
 
 
  
 
 
 
  
 
 
 

  
 
 
 
  
 
 
 
  
 
 
 

  
 
 
 
  
 
 
 
  
 
 
 

Figure   Prediction time and energy consumed by ProtoNN
 kB  and its optimized version against baselines  The accuracy
of each model is on top of its prediction time bar 

of readonly  ash  We compare ProtoNN with   baselines
 LDKLL  NeuralNet Pruning     Logistic  on   binary
datasets  Figure   presents the results from this experiment 
ProtoNN shows almost the same characteristics as   simple
linear model    logistic  in most cases while providing
signi cantly more accurate predictions 
Further optimization  The Atmega   microcontroller
supports native integer arithmetic at    operation 
softwarebased  oating point arithmetic at    operation 
exponentials are   further order slower  It is thus desirable
to perform prediction only using integers  We implemented
an integer version of ProtoNN to leverage this  We factor
out   common  oat value from the parameters and round
the residuals by  byte integers  To avoid computing the
exponentials  we store   precomputed table of approximate exponential values  As can be seen in Figure   this
optimized version of ProtoNN loses only   little accuracy 
but obtains     reduction in energy and prediction cost 

Model Size Accuracy     Accuracy      ProtoNNSNCBNCProtoNN  kNN for Resourcescarce Devices

References
Angiulli  Fabrizio  Fast condensed nearest neighbor rule 

In ICML   

Babbar  Rohit and Sh olkopf  Bernhard  Dismecdistributed
sparse machines for extreme multilabel classi cation 
In arXiv preprint arXiv  Accepted for Web
Search and Data Mining Conference  WSDM   
 

Bentley  Jon Louis  Multidimensional binary search trees
used for associative searching  Commun  ACM   
   

Beygelzimer  Alina  Kakade  Sham  and Langford  John 

Cover trees for nearest neighbor  In ICML   

Bhatia  Kush  Jain  Himanshu  Kar  Purushottam  Varma 
Manik  and Jain  Prateek  Sparse local embeddings for
extreme multilabel classi cation  In NIPS  pp   
 

Blumensath  Thomas and Davies  Mike    Iterative thresholding for sparse approximations  Journal of Fourier
Analysis and Applications     

Cover     and Hart     Nearest neighbor pattern classi ca 

tion  IEEE Trans  Inf  Theor     

Daume III  Hal  Karampatziakis  Nikos  Langford  John 
and Mineiro  Paul  Logarithmic time oneagainst some 
arXiv preprint arXiv   

Davis  Jason    Kulis  Brian  Jain  Prateek  Sra  Suvrit  and
Dhillon  Inderjit    Informationtheoretic metric learning  In ICML   

Dekel     Jacobbs     and Xiao     Pruning decision

forests  In Personal Communications   

Devi    Susheela and Murty    Narasimha  An incremental prototype set building technique  Pattern Recognition     

Friedman  Jerome    Stochastic gradient boosting  Computational Statistics and Data Analysis     

Gionis  Aristides  Indyk  Piotr  Motwani  Rajeev  et al 
In

Similarity search in high dimensions via hashing 
VLDB  volume   pp     

Goldberger  Jacob  Roweis  Sam    Hinton  Geoffrey   
and Salakhutdinov  Ruslan  Neighbourhood components
analysis  In NIPS   

Jain  Prateek  Tewari  Ambuj  and Kar  Purushottam  On iterative hard thresholding methods for highdimensional
mestimation  In NIPS  pp     

Jose  Cijo  Goyal  Prasoon  Aggrwal  Parv  and Varma 
Manik  Local deep kernel learning for ef cient nonlinear SVM prediction  In Proceedings of the  th International Conference on Machine Learning  ICML  
Atlanta  GA  USA    June   pp     

Kulis  Brian and Darrell  Trevor  Learning to hash with
binary reconstructive embeddings  In Advances in neural
information processing systems  pp     

Kusner  Matt    Tyree  Stephen  Weinberger  Kilian  and
Agrawal  Kunal  Stochastic neighbor compression  In
ICML   

Liu  Wei  Wang  Jun  Ji  Rongrong  Jiang  YuGang  and
Chang  ShihFu  Supervised hashing with kernels 
In
Computer Vision and Pattern Recognition  CVPR   
IEEE Conference on  pp    IEEE   

Mollineda  Ram on Alberto  Ferri  Francesc    and Vidal 
Enrique  An ef cient prototype merging strategy for
the condensed  nn rule through classconditional hierarchical clustering  Pattern Recognition   
 

Nan     Wang     and Saligrama     Featurebudgeted ran 

dom forest  In ICML   

Nan     Wang     and Saligrama     Pruning random

forests for prediction on   budget   

Norouzi  Mohammad  Fleet  David    and Salakhutdinov 
In AdRuslan    Hamming distance metric learning 
vances in neural information processing systems  pp 
   

Prabhu  Yashoteja and Varma  Manik  Fastxml    fast 
accurate and stable treeclassi er for extreme multilabel
learning  In KDD   

Shotton     Sharp     Kohli     Nowozin     Winn    
and Criminisi     Decision jungles  Compact and rich
models for classi cation  In NIPS   

Wang  Wenlin  Chen  Changyou  Chen  Wenlin  Rai 
Piyush  and Carin  Lawrence  Deep metric learning
In Joint European Conferwith data summarization 
ence on Machine Learning and Knowledge Discovery in
Databases  pp    Springer   

Han     Mao     and Dally        Deep compression 
Compressing deep neural networks with pruning  trained
quantization and huffman coding  In ICLR   

Weinberger  Kilian    and Saul  Lawrence    Distance
metric learning for large margin nearest neighbor classi 
 cation     Mach  Learn  Res     

ProtoNN  kNN for Resourcescarce Devices

Weiss  Yair  Torralba  Antonio  and Fergus  Robert  SpecIn NIPS  pp    Curran Asso 

tral hashing 
ciates  Inc   

Zhong  Kai  Guo  Ruiqi  Kumar  Sanjiv  Yan  Bowei 
Simcha  David  and Dhillon  Inderjit  Fast Classi 
cation with Binary Prototypes 
In Singh  Aarti and
Zhu  Jerry  eds  Proceedings of the  th International
Conference on Arti cial Intelligence and Statistics  volume   of Proceedings of Machine Learning Research 
pp    Fort Lauderdale  FL  USA    Apr
  PMLR  URL http proceedings mlr 
press   zhong   html 

