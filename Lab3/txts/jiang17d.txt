Ef cient Nonmyopic Active Search

Shali Jiang   Gustavo Malkomes   Geoff Converse   Alyssa Shofner   Benjamin Moseley   Roman Garnett  

Abstract

Active search is an active learning setting with the
goal of identifying as many members of   given
class as possible under   labeling budget  In this
work  we  rst establish   theoretical hardness of
active search  proving that no polynomialtime
policy can achieve   constant factor approximation ratio with respect to the expected utility of the
optimal policy  We also propose   novel  computationally ef cient active search policy achieving
exceptional performance on several realworld
tasks  Our policy is nonmyopic  always considering the entire remaining search budget  It also automatically and dynamically balances exploration
and exploitation consistent with the remaining
budget  without relying on   parameter to control
this tradeoff  We conduct experiments on diverse
datasets from several domains  drug discovery 
materials science  and   citation network  Our
ef cient nonmyopic policy recovers signi cantly
more valuable points with the same budget than
several alternatives from the literature  including
myopic approximations to the optimal policy 

  Introduction
In many realworld applications  the process of analyzing
data incurs some cost  for example  obtaining   label may
require   laborious experiment    human action  or depletion of some other expensive resource  In these scenarios 
carefully selecting data to label can often help us achieve
our goals more ef ciently than       random sampling  This
is the motivation behind active learning 
Naturally  which data examples are the most useful to label might change according to our objective  Traditionally 
much of the active learning literature has focused on train 

 Washington University in St  Louis  St  Louis  MO  USA
 Simpson College  Indianola  IA  USA  University of South
Carolina  Columbia  SC  USA  Correspondence to  Shali Jiang
 jiang   wustl edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

ing   model to have high generalization performance with
few training examples  Here  we consider   special and
atypical realization of active learning  the active search
problem  Garnett et al    In active search  we seek
to sequentially inspect data so as to discover members of  
rare  desired class  The labels are not known   priori but can
be revealed by querying   costly labeling oracle  The goal
is to design an policy to sequentially query points to  nd
as many valuable points as possible under   labeling budget  Several realworld problems can be naturally posed in
terms of active search  drug discovery  fraud detection  and
product recommendation are   few examples    successful
active search policy faces the fundamental dilemma between
exploration and exploitation       whether to search for new
regions of valuable points  exploration  or take advantage
of the currently mostpromising regions  exploitation 
Previous work developed policies for active search by appealing to Bayesian decision theory  Garnett et al   
  Garnett et al    derived the optimal policy in
this framework with   natural utility function  Not surprisingly  realizing this policy in the general case requires
exponential computation  To overcome this intractability 
the authors of that work proposed using myopic lookahead
policies in practice  which compute the optimal policy only
up to   limited number of steps into the future  This de nes
  family of policies ranging in complexity from completely
greedy onestep lookahead to the optimal policy  which
looks ahead to the depletion of the entire budget  The authors demonstrated improved performance on active search
over the greedy policy even when looking just two steps into
the future  including in   drugdiscovery setting  Garnett
et al    The main limitation of these strategies is that
they completely ignore what can happen beyond the chosen
horizon  which for typical problems is necessarily limited
to  cid      even with aggressive pruning 
The contributions of this paper are twofold  First  we prove
that no polynomial time policy for active search can have
nontrivial approximation ratio with respect to the optimal
policy in terms of expected utility  This extends the result
by Garnett et al    that myopic approximations to the
optimal policy cannot approximate the optimal policy  The
proof of this theorem is constructive  creating   family of
explicitly dif cult active search instances and showing that
no polynomial time algorithm can perform well compared

Ef cient Nonmyopic Active Search

to the optimal  exponential cost  policy on these 
Second  we introduce   novel nonmyopic policy for active
search that considers not only the potential immediate contribution of each unlabeled point but also its potential impact
on the remaining points that could be chosen afterwards 
Our policy automatically balances exploitation against exploration consistent with the labeling budget without requiring any parameters controlling this tradeoff  We also
develop an effective strategy for pruning unlabeled points
by bounding their potential impact on the search problem 
We compare our method with several baselines by conducting experiments on numerous real datasets spanning several
domains including citation networks  materials science  and
drug discovery  Our results thoroughly demonstrate that
our policy typically signi cantly outperforms previously
proposed active search approaches 

  Active Search and the Optimal Policy
Suppose we are given    nite domain of elements    cid   xi 
We know that there is   rare subset         the members
of which are considered valuable  but their identities are
unknown   priori  We will call the elements of   targets
or positive items  Assume that there is an oracle that can
determine whether   speci ed element       is   target 
producing the binary output    cid          The oracle  however  is assumed to be expensive and may only be
queried   times  We seek to design   policy to sequentially
query elements to maximize the number of targets found 
We will express our preference over different sets of obser 

vations    cid cid xi  yi cid  through   simple utility 

      cid cid 

yi 

 

yi  

which simply counts the number of targets in    Then 
the problem is to sequentially construct   set of   observed
points   with the goal of maximizing      Throughout
this work  we use   subscript to specify   set of observed

data after       queries  de ning Di  cid cid xj  yj cid  

  

  The Bayesian Optimal Policy

Following previous work  we consider the active search
problem in the standard Bayesian framework  Assume we
have   probabilistic classi cation model that provides the
posterior probability of   point   belonging to    given
observed data    Pr             
Recall that we are allowed to perform   labeling queries 
and suppose we are at some iteration   for        having
already observed       examples  Di  We wish to submit
the ith item to the oracle  Bayesian decision theory compels
us to select the item that if we evaluate next maximizes the

expected utility of the  nal observed dataset 

  cid   Dt    xi Di 

 cid 

 

      arg max
xi   Di 

In other words  we choose   point     maximizing the expected number of targets found at termination  Unfortu 

nately  as we shall see later  computing   cid   Dt    xi Di 

is computationally impractical 
To better understand the optimal policy  consider the case
       so we already have      observations Dt  and there
is only one more query left  The expected utility is

 cid 

  cid   Dt    xt Dt 

 cid   cid 

yt

  Dt  Pr yt   xt Dt 
    Dt    Pr yt       xt Dt 
 
Note   Dt  is   constant  since Dt  was already observed  Thus  when there is one query remaining  the optimal decision is to greedily choose the remaining point with
maximum probability of being   target 
When two or more queries are left  the optimal policy is not
as trivial  The challenge is that after the  rst choice  the
probability model changes  affecting all future decisions 
Below  we show the expected utility for          

  cid   Dt    xt Dt 
 cid      Dt   
Pr yt       xt Dt cid 
 cid max

Pr yt        xt Dt   

Eyt 

 

xt

This expression has an intuitive interpretation  First  we
have the reward for the data already observed    Dt  The
second term is the expected reward contribution from the
point xt  under consideration  Pr yt        xt Dt 
Finally  the last term is the expected future reward  which is
the expected reward to be gathered on the next step  from
our previous analysis  we know that this will be maximized
by   greedy selection   These latter two terms can be
interpreted as encouraging exploitation and exploration  respectively  with the optimal secondto last query 
In general  we can compute expected utility   at time      
recursively as  Garnett et al   

 cid      Di   
  cid   Dt    xi Di 
 cid cid 
 cid 
 cid 
Pr yi       xi Di 
 cid 
maxx cid    cid   Dt   Di      cid Di
 cid cid 
 cid 

exploitation     

Eyi

 

 cid cid 
 cid 

Eq    is   cid   cid cid  where  cid              is the lookahead

It is easy to show that the time complexity for computing

exploration       

 

 

and   is the total number of unlabeled points 

Ef cient Nonmyopic Active Search

This exponential running time complexity makes the
Bayeisan optimal policy infeasible to compute  even for
smallscale applications    typical workaround is to pretend
there are only   few steps left in the search problem at each
iteration  and sequentially apply   myopic policy        
or   We will refer to these policies as the onestep and
twostep myopic policies  respectively  and more generally
to the  cid step myopic policy  with  cid             
Since these myopic approaches cannot plan more than  cid 
steps ahead  they can underestimate the potential bene  
of exploration  In particular  the potential magnitude of
the exploration term in   depends linearly on the budget 
whereas in an  cid step myopic policy  the magnitude of the
equivalent term can go no higher than    xed upper bound
of  cid  In fact  Garnett et al    showed via an explicit
construction that the expected performance of the  cid step
policy can be arbitrarily worse than any mstep policy with
 cid       exploiting this inability to  see past  the horizon 
When following this suggestion  we must thus trade off the
potential bene ts of nonmyopia and the rapidly increasing
computational burden of lookahead when choosing   policy 

  Hardness of Approximation

We extend the above hardness result to show that no
polynomialtime active search policy can be    constant
factor  approximation algorithm with respect to the optimal
policy  in terms of expected utility  In particular  under the
assumption that algorithms only have access to   unit cost
conditional marginal probability Pr              for any
  and    where     is less than the budget  then 

Theorem   There is no polynomialtime active search policy with   constant factor approximation ratio for optimizing
the expected utility 

We prove this theorem in the appendix  The main idea is
to construct   class of instances where   small  secret  set
of elements encodes the locations of   large  treasure  of
targets  The probability of revealing the treasure is vanishingly small without discovering the secret set  however  it
is extremely unlikely to observe any information about this
secret set with polynomialtime effort 
Despite the negative result of Theorem   we may still
search for policies that are empirically effective on real problems  In the next section  we propose   novel alternative
approximation to the optimal policy   that is nonmyopic 
computationally ef cient  and shows impressive empirical
performance 

 The optimal policy operates under these restrictions 

  Ef cient Nonmyopic Active Search
We have seen above how to myopically approximate the
Bayesian optimal policy using an  cid steplookahead approximate policy   Such an approximation  however  effectively assumes that the search procedure will terminate after
the next  cid  evaluations  which does not reward exploratory
behavior that improves performance beyond that horizon 
We propose to continue to exactly compute the expected
utility to some  xed horizon  but to approximate the remainder of the search differently  We will approximate the
expected utility from any remaining portion of the search by
assuming that any remaining points   xi  xi          xt 
in our budget will be selected simultaneously in one big
batch  One rationale is if we assume that after observing Di 
the labels of all remaining unlabeled points are conditionally
independent  then this approximation recovers the Bayesian
optimal policy exactly  By exploiting linearity of expectation  it is easy to work out the optimal policy for selecting
such   simultaneous batch observation  we simply select the
points with the highest probability of being valuable  The
resulting approximation is

we only sum the largest   values 
Our proposed policy selects points by maximizing the approximate  nal expected utility using 

 cid   cid cid     Pr           Di 
where the summationwith prime symbol cid cid   indicates that
 cid      Di   
  cid   Dt    xi Di 
 cid cid cid     Pr cid           Di
Pr yi       xi Di   
 cid cid 
 cid 

  cid   Dt Di      cid Di

max

  cid 

Eyi

 

 cid cid 
 cid 

 

 

exploration       

We will call this policy ef cient nonmyopic search  ENS 
As in the optimal policy  we can interpret   naturally as
rewarding both exploitation and exploration  where the exploration bene   is judged by   point   capability to increase
the top probabilities among currently unlabeled points  We
note further that in   the reward for exploration naturally
decreases over time as the budget is depleted  exactly as in
the optimal policy  In particular  the very last point xt is
chosen greedily by maximizing probability  agreeing with
the true optimal policy  The secondto last point is also
guaranteed to match the optimal policy 
Note that we may also use the approximation in   as part of
   nitehorizon lookahead with  cid      producing   family of
increasingly expensive but higher delity approximations to
the optimal policy  all retaining the same budget consciousness  The approximation in   is equivalent to   onestep
maximization of   We will see in our experiments that this
is often enough to show massive gains in performance  and

Ef cient Nonmyopic Active Search

that even this policy shows clear awareness of the remaining
budget throughout the search process  automatically and
dynamically trading off exploration and exploitation 

  Nonmyopic Behavior

To illustrate the nonmyopic behavior of our policy  we have
adapted the toy example presented by Garnett et al   
Let    cid      be the unit square  We repeated the following experiment   times  We selected   points       
uniformly at random from   to form the input space     We
create an active search problem by de ning the set of targets
      to be all points within Euclidean distance   from
either the center or any corner of    We took the closest
point to the center  always   target  as an initial training
set  We then applied ENS and the twostep lookahead  
policies to sequentially select   further points for labeling 
Figure   shows   kernel density estimate of the distribution
of locations selected by both methods during two time intervals  Figures       correspond to our method  Figures
      to twostep lookahead  Figures        consider the
distribution of the  rst   selected locations  Figures    
   consider the last   The qualitative difference between
these strategies is clear  The myopic policy focused on collecting all targets around the center  Figure     whereas
our policy explores the boundaries of the center clump with
considerable intensity  as well as some of the corners  Figure     As   result  our policy is capable of  nding some
of targets in the corners  whereas twostep lookahead hardly
ever can  Figure     We can also see that the highest probability mass in Figure     is the center  which shows that
our policy typically saves many highprobability points until
the end  On average  the ENS policy found about   more
targets at termination than the twostep lookahead policy 

  Implementation and Time Complexity

The complexity of our policy   is   cid   cid     log   cid cid   

     log    for         because we need to compute the
approximate expected utility for all   points  evaluate an expectation over its label  conditioning the model and sorting
the posterior probabilities in the expectation  However  for
some classi cation models Pr              observing one
point will only affect the probabilities on   small portion
of the other points       in   knn model  We can exploit
such structure to reduce the complexity of our method by
avoiding unnecessary computation 
Speci cally  suppose that after observing   point we only
need to update the probabilities of atmost   other points 
We can avoid repeatedly sorting the probabilities of every
unlabeled point when computing the score of each candidate point  Once the current probabilities are sorted
     log    we only need to update   probabilities and
sort these as well      log    now we can merge both

   

   

   

   

Figure   Kernel density estimates of the distribution of
points chosen by ENS  top  and  step lookahead  bottom 
during two different time intervals  The  gures on the left
show the kernel density estimates for the  rst   locations 
the  gures on the right  the last   chosen locations 

lists to get the top       posterior probabilities in time
         where   is the index of current iteration  In summary  these tricks can reduce the computational complex 

ity to   cid   log       log       cid  We can see the comO cid   log       cid  when using the same tricks 

plexity is about the same as twostep lookahead  which is

  Pruning the Search Space

To further reduce the computational complexity  we can use
  similar strategy as suggested by Garnett et al    to
bound the score function   and prune points that cannot
possibly maximize our score  We consider the same two
assumptions proposed by these authors  First  observing  
new negative point will not raise the probability of any other
point being   target  Second  we are able to bound the maximum probability of the unlabeled points after conditioning
on   given number of additional targets  that is  we assume
there is   function        such that
         max
     

Pr                 cid cid 

  cid   cid   cid      

That is  the probability of any unlabeled point can become
at most        after further conditioning on   or fewer
additional target points 
Consider an unlabeled point   at time    and de ne      
Pr           Di  for the remainder of this discussion  The

Ef cient Nonmyopic Active Search

score   denoted       here for simplicity  can be upper
bounded by

Note this upper bound is only   function of the current
probability   Let    be the point with maximum probability  Then       is certainly   lower bound of maxx      

           cid             Di cid   
       cid cid cid     Pr   cid          cid Di cid   cid     
Hence  those points satisfying   cid   cid          can be
                cid cid     Pr   cid          cid   

 
Then  all points with current probability lower than the
RHS of   can be removed from consideration  We will
show empirically that   large fraction of points can often be
pruned on massive datasets 

safely removed from consideration  Solving this inequality 
we have

       cid cid     Pr   cid          cid   

     

 

  Related Work
Our method falls into the broader framework of active learning  The particular setting of  nding elements of   valuable
class is rather unusual in active learning  which typically
considers the goal of training   high delity model  Lewis
  Gale    For an exhaustive introduction to active
learning  we refer the reader to Settles  
The multiarmed bandit  MAB  problem shares some similarities with active search  where selecting an item can
understood as  pulling an arm  However  in active search
the items are correlated  and  critically  they can never be
played twice  Despite the difference  we note that our ENS
policy is somewhat similar to the knowledge gradient policy
introduced by Frazier et al   
Active search can be seen as   special case of Bayesian
optimization  Brochu et al    Snoek et al    with
binary observations and cumulative reward  Several nonmyopic policies have been proposed for Bayesian optimization in the regression setting       Ling et al    and
our method is spiritually similar to the recently propopsed
GLASSES algorithm  Gonz lez et al   
Vanchinathan et al    proposed   method called GPSELECT to solve   class of problems the authors call  adaptive valuable item discovery  which generalizes active
search to the regression setting  GPSELECT employs  
Gaussian process regression model in   manner inspired
by the Gaussian process upper con dence bound  GPUCB 
algorithm  Srinivas et al      parameter must be speci ed to balance exploration and exploitation  whereas our
method automatically and dynamically trades off these quantities  The method is also critically tied to Gaussian process

regression as the underlying model  which is inappropriate
for classi cation  Our decisiontheoretic approach does not
make any assumptions about the classi cation model 
Active search can also be seen as   special case of  partially
observable  Markov decision processes  PO MDPs  for
which there are known hardness results  Sabbadin et al 
  for example  de ned the class of socalled  purely
epistemic  MDPs  EMDPs  where the state does not evolve
over time  The authors showed that the optimal policy
for these problems cannot admit polynomialtime constant
approximations  Unfortunately  these hardness results  for
the very rich class of EMDPs are not trivially transferred to
the morespeci   active search problem 
Our proposed approximation is similar in nature to the active
search policy proposed by Wang et al    which only
considered the effect of raising probabilities after observing
  positive label  and did not consider the budget  Rather  the
proposed score always encourages maximal exploration  in
opposition to the optimal policy 
There has been some attention to active search in the graph
setting where the input domain   is the nodes of   graph
 Garnett et al    Wang et al    Pfeiffer III et al 
  Ma et al      Our method does not restrict the
input space  Further  the classi cation models used in these
settings are often dif cult to scale to large datasets      
requiring the pseudoinverse of the graph Laplacian 
Finally  variations on the active search problem have also
been considered  Ma et al    proposed the active area
search problem  wherein   continuous function is sampled
to discover regions with large mean value  and Ma et al 
    extended this idea to de ne the moregeneral active
pointillistic pattern search problem  These settings do not
allow querying for labels directly and offer no insight to the
core active search problem 

  Experiments
We implemented our approximation to the Bayesian optimal
policy with the MATLAB active learning toolbox  and have
compared the performance of our proposed ENS policy with
several baselines  First we compare with the myopic onestep  greedy  and twostep approximations to the Bayesian
optimal policy  presented in   Note that Garnett et al 
  and Garnett et al    thoroughly compared the
oneand twostep policies  with the  nding that the lessmyopic twostep algorithm usually performs better in terms
of targets found  as one would expect  In our experiments we
will mainly focus on comparing our algorithm with myopic
twostep approximate policy 
We also consider   simple baseline which we call RANDOM 

 https github com rmgarnett active learning

Ef cient Nonmyopic Active Search

GREEDY  RG  Here we randomly select points to query
 exploration  during the  rst half of the budget  and select
the remainder using greedy selection  exploitation  Although na ve  this policy adapts to the budget 
We further compare with the score function proposed by
Wang et al    which we refer to as IMS 

IMS      Pr             cid      IM   cid 

 

 Auer    score function               cid     

where IM    measures the  expected impact  the sum of
the raised probabilities   results in if it is positive  Note
that it is dif cult to determine the tradeoff parameter  
without  expensive  cross validation  The empirical results
in  Wang et al    indicate that       performs well
on average  we will    this value in our experiments 
Finally  we have also considered the following UCBstyle
where     Pr              and   is   tradeoff parameter  The UCB score function is very popular and is the
essence of the methods in  Vanchinathan et al    Srinivas et al    developed for Gaussian processes  including
GPSELECT  We considered various   values and our experiments show that it is no better than twostep lookahead  so
we present these results in the appendix due to space 
The probability model Pr              we will adopt is the
knearest neighbor  kNN  classi er as described in Section
  of  Garnett et al    This model  while being rather
simple  shows reasonable generalization error  is nonparameteric  and can be rapidly updated given new training data 
an important property in the active setting we consider here 
We will also adopt the probability bound   for this model
described in that work  Note IMS was proposed together  but
orthogonally  with   graph model for the probability  which
is computationally infeasible       for our datasets  So
we also use kNN model for IMS 

  CiteSeerx Data

For our  rst real data experiment  we consider   subset of
the CiteSeerx citation network   rst described in  Garnett
et al    This dataset comprises     computer science papers published in the top  mostpopular computer
science venues  We form an undirected citation network
from these papers  The target class is papers published in
the NIPS proceedings  there are     such papers    of
the whole dataset  Note that distinguishing NIPS papers in
the citation network is not an easy task  because many other
highly related venues such as ICML  AAAI  IJCAI  etc  are
also among the mostpopular venues    feature vector for
each paper is computed by performing graph principal component analysis  Fouss et al    on the citation network
and retaining the  rst   principal components 
We select   single target         NIPS paper  uniformly at

Figure   The learning curve of our policy and other baselines on the CiteSeerx dataset 

random to form an initial training set  The budget is set to
      and we use       in the kNN model  These parameters match the choices in  Garnett et al    We use
each policy to sequentially select   papers for labeling  The
experiment was repeated   times  varying the initial seed
target  Figure   shows the average number of targets found
for each method as   function of the number of queries 
We  rst observe that the ranking of the performance is ENS 
twostep  IMS  onestep  and RG  and our policy outperforms
the twostep policy in this task by   large margin  The mean
difference in number of targets found at termination vs  twostep is     vs    an improvement on average of
    twosided paired ttest testing the hypothesis that
the average difference of targets found is zero returns   pvalue of       and     con dence interval on the
increase in number of targets found of    
Another interesting observation is that during the initial  
queries  ENS actually performs worse on average than all
baseline policies except RG  after which it quickly outperforms them  This feature perfectly illustrates an automatic
exploration exploitation transition made by our policy  As
we are always cognizant of our budget  we spend the initial
stage thoroughly exploring the domain  without immediate
reward  Once complete  we exploit what we learned for
the remainder of the budget  This tradeoff happens automatically and without any need for an explicit twostage
approach or arbitrary tuning parameters 
Varying the Budget    distinguishing feature of our
method is that it always takes the remaining budget into consideration when selecting   point  so we would expect different behavior with different budgets  We repeated the above
experiment for budgets               and
report in Table   the average number of targets found at
these time points for each method  We have the following
observations from the table  First  ENS performs better than

 numberofqueriesnumberoftargetsfoundENStwostepIMSone stepRGEf cient Nonmyopic Active Search

Table   CiteSeerx  left  and BMG  right  data  Average number of targets found by the oneand twostep myopic policies
and ENS with different  ve budgets  varying from   to   at speci   time steps  The performance of the best method at
each time waypoint is in bold 

CiteSeerx data

query number

policy

RG

IMS
onestep
twostep
ENS 
ENS 
ENS 
ENS 
ENS 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 

BMG data

query number

policy

RG

IMS
onestep
twostep
ENS 
ENS 
ENS 
ENS 
ENS 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 
 
 
 
 
 
 

all other baseline policies for every budget  Second  ENS is
able to adapt to the speci ed budget  For example  when
comparing performance after   queries  ENS  has located many more targets than the ENS methods with greater
budgets  which at that time are still strongly rewarding exploration    similar pattern holds when comparing other
pairs of ENS variations  with one minor exception 

  Finding Bulk Metallic Glasses

Our next dataset considers an application from materials science  discovering novel alloys forming bulk metallic glasses
 BMGs  BMGs have numerous desirable properties  including high toughness and good wear resistance compared
to crystalline alloys  We compiled   database of    
known alloys from the materials literature        Kawazoe
et al    all  an extension of the dataset from  Ward
et al    Of these        are known to exhibit
glassforming ability  which we de ne to be targets  We
conduct the same experiments described for the CiteSeerx
data above and show the results in Table   We can see the
results again demonstrate our policy   superior performance
over all other methods  and its ability of adapting to the
remaining budget 

  Virtual Drug Screening Data

We further conduct experiments on   massive database of
chemoinformatic data  The basic setting is to screen   large
database of compounds searching for those that show binding activity against some biological target  This is   basic
component of drugdiscovery pipelines  The dataset comprises   activity classes of human biological importance
selected from the Binding DB  Liu et al    database  For
each activity class  there are   small number of compounds
with signi cant binding activity  the number of targets varies

from   to     across the activity classes  From these we
de ne   different active search problems  There are also
    presumed inactive compounds selected at random
from the ZINC database  Sterling   Irwin    these are
used as   shared negative class for each of these problems 
For each compound  we consider two different feature representations  also known as chemoinformatic  ngerprints 
called ECFP  and GpiDAPH  These  ngerprints are binary
vectors encoding the relevant chemical characteristics of the
compounds  see  Garnett et al    for more details  So
in total we have   active search problems  each with more
than     points  and with targets less than  
As is standard in this setting  we compute  ngerprint similarities via the Jaccard index  Jasial et al    which are
used to de ne the weight matrix of the kNN model from
above  setting       for all the experiments  For active
search policies  we again randomly select one positive as the
initial training set  and sequentially query       further
points  We also report the performance of   baseline where
we randomly sample   strati ed sample of size   of the
database     points  more than   times the budget
of the active search policies  From this sample  we train
the same kNN model  compute the active probability of
the remaining points  and query the   points with the
highest posterior activity probability  All experiments were
repeated   times  varying the initial training point  Note we
did not test IMS on these data due to computational expense 
Our policy nominally has higher time complexity  but our
pruning strategy can reduce the computation signi cantly
in practice  as we show in Section  
Table   summarizes the results  First we notice that all
 We did not conduct experiments on the MACCS  ngerprint  It
was inferior in the  ndings of Garnett et al      reviewer
of  Jasial et al    noted that it is no longer used  due to clear
underperformance compared to       ECFP  and GpiDAPH 

Ef cient Nonmyopic Active Search

Table   Number of active compounds found by various active search policies at termination for each  ngerprint  averaged
over   active classes and   experiments  Also shown is the difference of performance between ENS and twostep
lookahead and the results of the corresponding paired ttest 

 ngerprint
ECFP 
GpiDAPH 

 NN

 
 

RG
 
 

policy
onestep

 
 

twostep

 
 

ENS
 
 

difference

 
 

ttest results
pvalue

     
     

  CI

 
 

 
 

  Effect of Pruning

To investigate how pruning can improve the ef ciency of
computing the policy  we computed the average number
of pruned points across all                  
iterations of active search  for each  ngerprint  On average
about   of the unlabeled points are pruned  dramatically
improving the computational ef ciency by approximately  
corresponding linear factor  The time for each experiment
was effectively reduced from on the order of one day to that
of one hour  See the appendix for detailed results 

  Conclusion
In this paper we proved the theoretical hardness of active
search and proposed an wellmotivated and empirically
betterperforming policy for solving this problem  In particular  we proved that no polynomialtime algorithm can
approximate the expected utility of the optimal policy within
  constant approximation ratio  We then proposed   novel
method  ef cient nonmyopic search  ENS  for the active
search problem  Our method approximates the Bayesian
optimal policy by computing  conditioned on the location of
the next point  how many targets are expected at termination 
if the remaining budget is spent simultaneously  By taking
the remaining budget into consideration in each step  we
are able to automatically balance exploration and exploitation  Despite being nonmyopic  ENS is ef cient to compute
because future steps are  attened into   single batch  in
contrast to the recursive simulation required when computing the true expected utility  We also derived an effective
pruning strategy that can reduce the number of candidate
points we must consider at each step  which can further improve the ef ciency dramatically in practice  We conducted
  massive empirical evaluation that clearly demonstrated
superior overall performance on various domains  as well as
our automatic balance between exploration and exploitation 
Given the hardness result we proved  in general there is
little point to require more of an algorithm than superior
empirical performance  However  one exciting future direction is to understand  under what conditions       some
assumption about the structure of problem instances  we
can  nd ef cient algorithms with guarantees 

Figure   The average difference in cumulative targets found
between ENS and the twostep policy  averaged over   activity classes and   experiments on the ECFP   ngerprint 

active search policies perform much better than the recall of
  simple classi cation algorithm  even though they observe
less than onetenth the data  Interestingly  even the na ve
randomgreedy  RG  policy performs much better than this
baseline  albeit much worse than other active search policies 
The twostep policy is again better than the greedy policy
for both  ngerprints  which is consistent with the results
reported in  Garnett et al    The ENS policy performs
signi cantly better than twostep lookahead    twosided
paired ttest overwhelmingly rejects the hypothesis that the
performance at termination is equal in both cases 
Figure   shows the mean difference in cumulative targets found between ENS and the twostep policy for the
ECFP   ngerprint  Again  we very clearly observe the automatic tradeoff between exploration and exploitation by
our method  In the initial stage of the search  we explore the
space without much initial reward  but around query  
our algorithm switches automatically to exploitation  outperforming the myopic policy signi cantly at termination  The
mean difference curves for the other  ngerprint is similar 
and can be found in the appendix  along with the individual
learning curves of the  rst six activity classes of ECFP 

 numberofqueriesdifferenceinutilitymeandifference CIEf cient Nonmyopic Active Search

Acknowledgments
We would like to thank Brendan Juba for insightful discussion  SJ  GM  and RG were supported by the National Science Foundation  NSF  under award number IIA 
GM was also supported by the Brazilian Federal Agency
for Support and Evaluation of Graduate Education  CAPES 
GC and AS were supported by NSF under award number
CNS  BM was supported by   Google Research
Award    Yahoo Research Award  and by NSF under award
number CCF 

References
ASM Alloy Center Database 

asminternational org ac 

URL http mio 

Auer  Peter  Using Con dence Bounds for Exploitation 
Exploration Tradeoffs  Journal of Machine Learning
Research     

Brochu  Eric  Cora  Vlad    and de Freitas  Nando   
Tutorial on Bayesian Optimization of Expensive Cost
Functions  with Application to Active User Modeling
and Hierarchical Reinforcement Learning    arXiv
preprint arXiv   cs LG 

Fouss  Francois  Pirotte  Alain  Renders  JeanMichel  and
Saerens  Marco  RandomWalk Computation of Similarities between Nodes of   Graph with Application to
Collaborative Recommendation  IEEE Transactions on
Knowledge and Data Engineering     

Frazier  Peter    Poweel  Warren    and Dayanik  Savas   
KnowledgeGradient Policy for Sequential Information
Collection  SIAM Journal on Control and Optimization 
   

Garnett  Roman  Krishnamurthy  Yamuna  Wang  Donghan 
Schneider  Jeff  and Mann  Richard  Bayesian Optimal
Active Search on Graphs  In  th Workshop on Mining
and Learning with Graphs   

Garnett  Roman  Krishnamurthy  Yamuna  Xiong  Xuehan 
Schneider  Jeff    and Mann  Richard    Bayesian Optimal Active Search and Surveying  In Proceedings of
the  th International Conference on Machine Learning 
 

Garnett  Roman    rtner  Thomas  Vogt  Martin  and Bajorath    rgen  Introducing the  active search  method for
iterative virtual screening  Journal of ComputerAided
Molecular Design     

Gonz lez  Javier  Osborne  Michael  and Lawrence  Neil   
GLASSES  Relieving The Myopia Of Bayesian Optimisation  In Proceedings of the  th International Conference

on Arti cial Intelligence and Statistics  number   in Proceedings of Machine Learning Research  pp   
 

Jasial  Swarit  Hu  Ye  Vogt  Martin  and Bajorath    rgen 
Activityrelevant similarity values for  ngerprints and
implications for similarity searching    Research 
 Chem Inf Sci   

Kawazoe  Yoshiyuki  Yu  JingZhi  Tsai  AnPang  and
Masumoto  Tsuyoshi  eds  Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys  volume    of
Condensed Matter  Springer Verlag   

Lewis  David    and Gale  William      Sequential Algorithm for Training Text Classi ers  In Proceedings of
the  th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval  pp 
   

Ling  Chun Kai  Low  Kian Hsiang  and Jaillet  Patrick 
Gaussian process planning with Lipschitz continuous reward functions  Towards unifying Bayesian optimization  active learning  and beyond  In Proceedings of the
 th AAAI Conference on Arti cial Intelligence  pp   
   

Liu  Tiqing  Lin  Yuhmei  Wen  Xin  Jorissen  Robert   
and Gilson  Michael    BindingDB    webaccessible
database of experimentally determined protein ligand
binding af nities  Nucleic Acids Research   suppl  
      

Ma  Yifei  Garnett  Roman  and Schneider  Jeff  Active Area
Search via Bayesian Quadrature  In Proceedings of the
 th International Conference on Arti cial Intelligence
and Statistics  number   in Proceedings of Machine
Learning Research  pp     

Ma  Yifei  Huang  TzuKuo  and Schneider  Jeff  Active
Search and Bandits on Graphs using SigmaOptimality 
In Proceedings of the  st Conference on Uncertainty in
Arti cial Intelligence  pp       

Ma  Yifei  Sutherland  Dougal    Garnett  Roman  and
Schneider  Jeff  Active Pointillistic Pattern Search  In
Proceedings of the  th International Conference on Arti cial Intelligence and Statistics  number   in Proceedings of Machine Learning Research  pp       

Pfeiffer III  Joseph    Neville  Jennifer  and Bennett  Paul   
Active Exploration in Networks  Using Probabilistic Relationships for Learning and Inference  In Proceedings of
the  rd ACM International Conference on Information
and Knowledge Management  pp     

Ef cient Nonmyopic Active Search

Sabbadin    gis  Lang      me  and Ravoanjanahary  Nasolo  Purely Epistemic Markov Decision Processes  In
Proceedings of the  nd AAAI Conference on Arti cial
Intelligence  pp     

Settles  Burr  Active Learning Literature Survey  Technical
report  University of Wisconsin Madison    Computer Sciences Technical Report  

Snoek  Jasper  Larochelle  Hugo  and Adams  Ryan    Practical Bayesian Optimization of Machine Learning Algorithms  In Advances in Neural Information Processing
Systems   pp     

Srinivas  Niranjan  Krause  Andreas  Kakade  Sham  and
Seeger  Matthias    Gaussian Process Optimization in
the Bandit Setting  No Regret and Experimental Design 
In Proceedings of the  th International Conference on
Machine Learning  pp     

Sterling  Teague and Irwin  John    ZINC     Ligand Discovery for Everyone  Journal of Chemical Information
and Modeling     

Vanchinathan  Hastagiri    Marfurt  Andreas  Robelin 
CharlesAntoine  Kossmann  Donald  and Krause  Andreas  Discovering Valuable Items from Massive Data 
In Proceedings of the  st ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining 
pp     

Wang  Xuezhi  Garnett  Roman  and Schneider  Jeff  Active Search on Graphs  In Proceedings of the  th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  pp     

Ward  Logan  Agrawal  Ankit  Choudhary  Alok  and
Wolverton  Christopher    GeneralPurpose Machine
Learning Framework for Predicting Properties of Inorganic Materials    arXiv preprint arXiv 
 condmat mtrlsci 

