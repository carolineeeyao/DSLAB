Oracle Complexity of SecondOrder Methods for FiniteSum Problems

Yossi Arjevani   Ohad Shamir  

Abstract

Finitesum optimization problems are ubiquitous
in machine learning  and are commonly solved
using  rstorder methods which rely on gradient computations  Recently  there has been
growing interest in secondorder methods  which
rely on both gradients and Hessians 
In principle  secondorder methods can require much
fewer iterations than  rstorder methods  and
hold the promise for more ef cient algorithms 
Although computing and manipulating Hessians
is prohibitive for highdimensional problems in
general  the Hessians of individual functions in
 nitesum problems can often be ef ciently computed      
because they possess   lowrank
structure  Can secondorder information indeed
be used to solve such problems more ef ciently 
In this paper  we provide evidence that the answer   perhaps surprisingly   is negative  at
least in terms of worstcase guarantees  We also
discuss what additional assumptions and algorithmic approaches might potentially circumvent
this negative result 

min
           

 
 

fi   

 

  cid 

  

  Introduction
We consider  nitesum problems of the form

where   is   closed convex subset of some Euclidean or
Hilbert space  each fi is convex and  smooth  and   is
 strongly convex  Such problems are ubiquitous in machine learning  for example in order to perform empirical
risk minimization using convex losses 

 Department of Computer Science and Applied Mathematics  Weizmann Institute of Science  Rehovot  Israel  Correspondence to  Yossi Arjevani  yossi arjevani weizmann ac il 
Ohad Shamir  ohad shamir weizmann ac il 

To study the complexity of this and other optimization
problems 
it is common to consider an oracle model 
where the optimization algorithm has no apriori information about the objective function  and obtains information
from an oracle which provides values and derivatives of the
function at various domain points  Nemirovsky and Yudin 
  The complexity of the algorithm is measured in
terms of the number of oracle calls required to optimize
the function to within some prescribed accuracy 
Existing lower bounds for  nitesum problems show that
using    rstorder oracle  which given   point   and index
                returns fi    and  fi    the number of
oracle queries required to  nd an  optimal solution is at
least of order

 cid 

 cid    

 

   

log

 

 cid   

 cid cid 

 

 

either under algorithmic assumptions or assuming the dimension is suf ciently large   Agarwal and Bottou   
Lan    Woodworth and Srebro    Arjevani and
Shamir      This is matched  up to log factors  by existing approaches  and cannot be improved in general 
An alternative to  rstorder methods are secondorder
methods  which also utilize Hessian information    prototypical example is the Newton method  which given  
 single  function     performs iterations of the form

wt    wt     

 
where             are the gradient and the Hessian
of   at    and    is   step size parameter  Secondorder
methods can have extremely fast convergence  better than
those of  rstorder methods       quadratic instead of linear  Moreover  they can be invariant to af ne transformations of the objective function  and provably independent of
its strong convexity and smoothness parameters  assuming
    
selfconcordance   Boyd and Vandenberghe   
  drawback of these methods  however  is that they can
be computationally prohibitive  In the context of machine

 cid      cid        

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   
strongly convex if     cid          cid     for all       

 For   twicedifferentiable function    it is  smooth and  

 Depending on how  optimality is de ned precisely  and
where the algorithm is assumed to start  these bounds may have
additional factors inside the log  For simplicity  we present the existing bounds assuming   is suf ciently small  so that   log 
term dominates 

Oracle Complexity of SecondOrder Methods for FiniteSum Problems

learning  we are often interested in highdimensional problems  where the dimension   is very large  and the Hessians are       matrices which in general may not even   
into computer memory  However  for optimization problems as in Eq    the Hessians of individual fi often have
  special structure  For example    very common special
case of  nitesum problems in machine learning is empirical risk minimization for linear predictors  where

fi       cid   cid    xi cid 

   cid    xi cid xix cid 

where xi is   training instance and  cid   is some loss function  In that case  assuming  cid   is twicedifferentiable  the
Hessian has the rank  form  cid cid cid 
    Therefore 
the memory and computational effort involved with storing and manipulating the Hessian of this function is merely
linear  rather than quadratic  in    Thus  it is tractable even
for highdimensional problems 
Building on this  several recent papers proposed and
analyzed secondorder methods for  nitesum problems 
which utilize Hessians of the individual functions fi  see
for instance Erdogdu and Montanari   Agarwal et al 
  Pilanci and Wainwright   RoostaKhorasani
and Mahoney       Bollapragada et al    Xu
 cid  
et al    and references therein  These can all be
viewed as approximate Newton methods  which replace the
    fi    in Eq    by
actual Hessian           
some approximation  based for instance on the Hessians of
  few individual functions fi sampled at random  One may
hope that such methods can inherit the favorable properties
of secondorder methods  and improve on the performance
of commonly used  rstorder methods 
In this paper  we consider the opposite direction  and study
lower bounds on the number of iterations required by algorithms using secondorder  or possibly even higherorder 
information  focusing on  nitesum problems which are
stronglyconvex and smooth  We make the following contributions 

 

  First  as   more minor contribution  we prove that
in the standard setting of optimizing   single smooth
and strongly convex function  secondorder information cannot improve the oracle complexity compared
to  rstorder methods  at least in high dimensions 
Although this may seem unexpected at  rst  the reason
is that the smoothness constraint must be extended to
higherorder derivatives  in order for higherorder information to be useful  We note that this observation
in itself is not new  and is brie   mentioned  without proof  in Nemirovsky and Yudin   Section
  Our contribution here is in providing   clean 
explicit statement and proof of this result 

  We then turn to present our main results  which state

 perhaps surprisingly  that under some mild algorithmic assumptions  and if the dimension is suf ciently
large  the oracle complexity of secondorder methods
for  nitesum problems is no better than  rstorder
methods  even if the  nitesum problem is composed
of quadratics  which are trivially smooth to any order 

  Despite this pessimistic conclusion  our

results
also indicate what assumptions and algorithmic approaches might be helpful in circumventing it 
In
particular  it appears that better  dimensiondependent
performance may be possible 
if the dimension is
moderate and the   individual functions in Eq   
are accessed adaptively  in   manner depending on
the functions rather than  xed in advance       sampling them from   nonuniform distribution depending on their Hessians  as opposed to sampling them
uniformly at random  This provides evidence to
the necessity of adaptive sampling schemes  and  
dimensiondependent analysis  which indeed accords
with some recently proposed algorithms and derivations        Agarwal et al    Xu et al    We
note that the limitations arising from oblivious optimization schemes  in   somewhat stronger sense  was
also explored in  Arjevani and Shamir       

The paper is structured as follows  We begin in Sec    with
  lower bound for algorithms utilizing secondorder information  in the simpler setting where there is   single function   to be optimized  rather than    nitesum problem 
We then turn to provide our main lower bounds in Sec   
and discuss their applicability to some existing approaches
in Sec    We conclude in Sec    where we also discuss
possible approaches to circumvent our lower bounds  The
formal proofs of our results appear in Appendix   

  Strongly Convex and Smooth Optimization

with   SecondOrder Oracle

Before presenting our main results for  nitesum optimization problems  we consider the simpler problem of minimizing   single stronglyconvex and smooth function    or
equivalently  Eq    when       and prove   result which
may be of independent interest 
To formalize the setting  we follow   standard oracle
model  and assume that the algorithm does not have apriori information on the objective function     except the
strongconvexity parameter   and smoothness parameter
  Instead  it has access to an oracle  which given   point
       returns values and derivatives of   at    either
       for    rstorder oracle  or             for  
secondorder oracle  The algorithm sequentially queries
the oracle using               wT  and returns the point
wT   Our goal is to lower bound the number of oracle calls

Oracle Complexity of SecondOrder Methods for FiniteSum Problems

    required to ensure that wT is an  suboptimal solution 
Given    rstorder oracle and   strongly convex and smooth
objective in suf ciently high dimensions  it is wellknown
that the worstcase oracle complexity is

 cid    log 

 Nemirovsky and Yudin    What if we replace this by
  secondorder oracle  which returns both        on top
of           
Perhaps unexpectedly  it turns out that this additional information does not substantially improve the worstcase oracle complexity bound  as evidenced by the following theorem 
Theorem   For any     such that           any
        and any deterministic algorithm  there exists    smooth    stronglyconvex function   on Rd  for

       cid  hiding factors logarithmic in      

such that the number of calls   to   secondorder oracle  required to ensure that    wT     minw Rd        
 cid 
           minw Rd       must be at least

 cid   

 cid cid   

 cid 

   

  log

 

 

  cid 

 

 

where      cid  are positive universal constants 

bound is  cid cid   

For suf ciently large  

    log cid   

 cid cid  which matches existing lower

  and small   this complexity lower

and upper bounds for optimizing stronglyconvex and
smooth functions using  rstorder methods  As mentioned
earlier  the observation that such  rstorder oracle bounds
can be extended to higherorder oracles is also brie   mentioned  without proof  in Nemirovsky and Yudin  
Section   Also  the theorem considers deterministic
algorithms  which includes standard secondorder methods  such as the Newton method  but otherwise makes no
assumption on the algorithm  Generalizing this result to
randomized algorithms should be quite doable  based on
the techniques developed in Woodworth and Srebro  
We leave   formal derivation to future work 
Although this result may seem surprising at  rst  it has
  simple explanation  In order for Hessian information 
which is local in nature  to be useful  there should be some
regularity constraint on the Hessian  which ensures that it
cannot change arbitrarily quickly as we move around the
domain    typical choice for   constraint of this kind is
Lipschitz continuity which dictates that

 cid               cid cid      cid       cid cid 

for some constant   
Indeed  the construction relies on
  function which does not have Lipschitz Hessians  It is

based on   standard lower bound construction for  rstorder oracles  but the function is locally  attened  in certain directions around points which are to be queried by
the algorithm  This is done in such   way  that the Hessian
observed by the algorithm does not provide more information than the gradient  and cannot be used to improve the
algorithm   performance 

  SecondOrder Oracle Complexity Bounds

for FiniteSum Problems

We now turn to study  nitesum problems of the form
given in Eq    and provide lower bounds on the number of oracle calls required to solve them  assuming  
secondorder oracle  To adapt the setting to    nitesum
problem  we assume that the secondorder oracle is given
both   point   and an index                  and returns  fi   fi   fi    The algorithm iteratively produces and queries the oracle with pointindex
pairs  wt  it  
   with the goal of making the suboptimality  or expected suboptimality  if the algorithm is randomized  smaller than   using   minimal number of oracle
calls    
In fact  the lower bound construction we use is such that
each function fi is quadratic  Unlike the construction of
the previous section  such functions have   constant  and
hence trivially Lipschitz  Hessian  Moreover  since any porder derivative of   quadratic for       is zero  this means
that our lower bounds automatically hold even if the oracle provides pth order derivatives at any    for arbitrarily
large   
However  in order to provide   lower bound using quadratic
functions  it is necessary to pose additional assumptions
on the structure of the algorithm  unlike Thm    which is
purely informationbased  To see why  note that with 
 cid  
out computational constraints  the algorithm can simply
 cid  
query the Hessians and gradients of each fi    at      
    fi  and
take the average to get         
    fi  and return the exact op 
        
timum  which for quadratics equals        
Therefore  with secondorder information  the best possible
informationbased lower bound for quadratics is no better
than     This is not   satisfying bound  since in order
to attain it we need to invert the  possibly highrank       
matrix      Therefore  if we are interested in bounds
for computationallyef cient algorithms  we need to forbid
such operations 
Speci cally  we will consider two algorithmic assumptions 
which are stated below  their applicability to existing algorithms is discussed in the next section  The  rst assumption constrains the algorithm to query and return points
  which are computable using linearalgebraic manipula 

 

 

Oracle Complexity of SecondOrder Methods for FiniteSum Problems

tions of previous points  gradients and Hessians  Moreover 
these manipulations can only depend on  at most  the last
 cid   cid  Hessians returned by the oracle  As discussed previously  this assumption is necessary to prevent the algorithm
from computing and inverting the full Hessian of     which
is computationally prohibitive  Formally  the assumption is
the following 
Assumption    LinearAlgebraic Computations  wt belongs to the set Wt   Rd  de ned recursively as follows 
       and Wt  is the closure of the set of vectors
derived from Wt    fit wt  by    nite number of operations of the following form 

       cid          cid   cid  where    cid  are arbitrary

scalars 

      Hw  where   is any      matrix which has the

same blockdiagonal structure as

  cid 

 fi       

 

   max   cid   cid 

for some arbitrary  

The  rst bullet allows to take arbitrary linear combinations
of previous points and gradients  and already covers standard  rstorder methods and their variants  As to the second bullet  by  same blockdiagonal structure  we mean
that if the matrix in Eq    can be decomposed to   diagonal blocks of size            dr in order  then   can also
be decomposed into   blocks of size            dr in order
 note that this does not exclude the possibility that each
such block is composed of additional subblocks  To give
  few examples  if we let Ht be the matrix in Eq    then
we may have 

      Ht 
        
       Ht     where   is some arbitrary diagonal

if Ht is invertible  or its pseudoinverse 

 

matrix  possibly acting as   regularizer 

    is   truncated SVD decomposition of Ht  or again 
Ht     or  Ht      for some arbitrary diagonal
matrix    or its pseudoinverse 

Moreover  for quadratic functions  it is easily veri ed that
the assumption also allows prox operations       returning
 cid       cid cid  for some     and previarg minw fi       
ously computed point   cid  Also  note that the assumption
places no limits on the number of such operations allowed

between oracle calls  However  crucially  all these operations can be performed starting from   linear combination of at most  cid   cid  recent Hessians  As mentioned earlier  this is necessary  since if we could compute the average of all Hessians  then we could implement the Newton method  The assumption that the algorithm only  remembers  the last  cid   cid  Hessians is also realistic  as existing computationallyef cient methods seek to use much
fewer than   individual Hessians at   time  We note that the
choice of  cid   cid  is rather arbitrary  and can be replaced by
   for any constant         Also  the way the assumption is formulated  the algorithm is assumed to be initialized at the origin   However  this is merely for simplicity 
and can be replaced by any other  xed vector  the lower
bound will hold by shifting the constructed  hard  function
appropriately 
The second  optional  assumption we will consider constrains the indices chosen by the algorithm to be oblivious 
in the following sense 
Assumption    Index Obliviousness  The
indices
            chosen by the algorithm are independent of
           fn 

To put this assumption differently  the indices may just as
well be chosen before the algorithm begins querying the
oracle  This can include  for instance  sampling functions
fi uniformly at random from            fn  and performing
deterministic passes over            fn in order  As we will
see later on  this assumption is not strictly necessary  and
can be removed at the cost of   somewhat weaker result 
Nevertheless  the assumption covers all optimal  rstorder
algorithms  as well as most secondorder methods we are
aware of  see Sec    for more details 
With these assumptions stated  we can  nally turn to
present the main result of this section 
Theorem   For any       any           any
          for some universal constant       and any
 possibly randomized  algorithm satisfying Assumptions  
and   there exists  smooth   strongly convex quadratic

functions            fn   Rd      for          cid   
 cid 
 cid 

hiding factors logarithmic in          such that the number of calls   to   secondorder oracle  so that

 cid 

      min
  Rd

     

 

 

   wT     min
  Rd

     

must be at least

 cid 

 

   

 cid    

 

  log

     

 cid 
 cid   

 

 cid cid 

 

 

Comparing this with the  tight   rstorder oracle complexity bounds discussed in the introduction  we see that the

Oracle Complexity of SecondOrder Methods for FiniteSum Problems

   cid 

  

lower bound is the same up to logfactors  despite the availability of secondorder information  In particular  the lower
bound exhibits none of the favorable properties associated
with full secondorder methods  which can compute and invert Hessians of     Whereas the full Newton method can
attain   log log  rates  and be independent of     if
  satis es   selfconcordance property  Boyd and Vandenberghe    here we only get   linear   log  rate 
and there is   strong dependence on     even though the
function is quadratic and hence selfconcordant 
The proof of the theorem is based on   randomized construction  which can be sketched as follows  We choose
indices            jd                 independently and uniformly at random  and de ne

fi            

 jl   wl   wl 

 cid   cid 

 
 

         

             
 cid  

where    is the indicator function of the event    and
              are parameters chosen based on        The average function          
 

   fi    equals

           cid 

  

            
 

  
 

 wl wl     

      

 cid   cid 

 
 

By setting the parameters appropriately  it can be shown
that   is  strongly convex and each fi is  smooth  Moreover  the optimum of   has the form                   qd  for

 
 

     
     

 

   

where

 

     

 

   

   

 

least cid  

is the socalled condition number of     The proof is based
on arguing that after   oracle calls  the points computable
by any algorithm satisfying Assumptions   and   must
have   values at all coordinates larger than some lT   hence
the squared distance of wT from the optimum must be at
  lT        which leads to our lower bound  Thus 
the proof revolves around upper bounding lT   We note
that   similar construction of   was used in some previous  rstorder lower bounds under algorithmic assumptions       Nesterov   Lan   as well as Arjevani
and Shamir   in   somewhat different context  The
main difference is in how we construct the individual functions fi  and in analyzing the effect of secondorder rather
than just  rstorder information 
To upper bound lT   we let lt  where                   be
the largest nonzero coordinate in wt  and track how lt

increases with    The key insight is that if            wt 
are zero beyond some coordinate    then any linear combinations of them  as well as multiplying them by matrices based on secondorder information  as speci ed in Assumption   will still result in vectors with zeros beyond coordinate    The only way to  advance  and increase the set
of nonzero coordinates is by happening to query the function fjl  However  since the indices of the queried functions are chosen obliviously  whereas each jl is chosen uniformly at random  the probability of this happening is quite
small  of order     Moreover  we show that even if this
event occurs  we are unlikely to  advance  by more than
   coordinates at   time  Thus  the algorithm essentially
needs to make     oracle calls in expectation  in order to
increase the number of nonzero coordinates by    It
can be shown that the number of coordinates needed to get

an  optimal solution is  cid   log   hiding some
about   times larger  namely  cid      log  To

logfactors  Therefore  the total number of oracle calls is

complete the theorem  we also provide   simple and separate     lower bound  which holds since each oracle call
gives us information on just one of the   individual functions            fn  and we need some information on most of
them in order to get   closeto optimal solution 
When considering nonoblivious       adaptive  algorithms  the construction used in Thm    fails as soon as the
algorithm obtains the Hessians of all the individual functions  potentially  after   oracle queries  Indeed  knowing
the Hessians of fi  one can devise an indexschedule which
gains at least one coordinate at every iteration  by querying the function which holds the desired       block  as
opposed to      on average in the oblivious case  Nevertheless  as mentioned before  we can still provide   result
similar to Thm    even if the indices are chosen adaptively 
at the cost of   much larger dimension 
Theorem   Thm    still holds if one omits Assumption  
and with probability   rather than in expectation  at the
cost of requiring an exponentially larger dimensionality of

   cid 

 

     

 

  

 cid 

 

The proof is rather straightforward  Making the dependence on the random indices            jd  explicit 
the
quadratic construction used in the previous theorem can be
written as

    jd     

    jd 
 

   

  

  cid Aj jd 

 

       cid      cid   

 cid   cid 

 
 

some       matrix Aj jd 

for
dependent on
           jd  and    xed parameter     Now  we create  
huge blockdiagonal matrices            An  where each Ai

 

  cid 

 
 

  cid 

  

 

 
 

Oracle Complexity of SecondOrder Methods for FiniteSum Problems

 cid 

 cid    wt cid      wt 
 cid cid 
  cid 

 fi wt 

  Comparison to Existing Approaches
As discussed in the introduction  there has been   recent
burst of activity involving secondorder methods for solving  nitesum problems  relying on Hessians of individual functions fi  In this section  we review the main algorithmic approaches and compare them to our results  The
bottom line is that most existing approaches satisfy the assumptions stated in Sec    and therefore our lower bounds
will apply  at least in   worstcase sense    possible exception to this is the Newton sketch algorithm  Pilanci and
Wainwright    which relies on random projections 
but on the  ip side is computationally expensive 
Turning to the details  existing approaches are based on taking the standard Newton iteration for such problems 
wt    wt     

 cid 
    fi   cid   and sometimes the vector term
 cid   
 cid  
 cid  
term
    fi    as well  by some approximation which
 
 
is computationally cheaper to compute  One standard and
wellknown approach is to use only gradient information
to construct such an approximation  leading to the family
of quasiNewton methods  Nocedal and Wright   
However  as they rely on  rstorder rather than secondorder information  they are orthogonal to the topic of our
work  and are already covered by existing complexity
lower bounds for  rstorder oracles 
Turning to consider Hessian approximation techniques using secondorder information  perhaps the simplest and
most intuitive approach is sampling  Since the Hessian
equals the average of many individual Hessians 

  wt     

 fi wt 

  cid 

replacing

Hessian

inverse

and

 

the

 
 

  

 
 

  

 

for each of the nd  possible choices
contains Aj jd 
of            jd  along its diagonal  in some canonical order  and one huge vector

 

         ed              nd   

We then let

       

 

  cid 
  cid 

  

  

 
 

 
 

fi   

  cid Aiw      cid      cid   

 cid   cid 

 
 

This function essentially combines all nd  problems
    jd  simultaneously  where each     jd  is embedded in   disjoint set of coordinates  Due to the blockdiagonal structure of each Ai  this function inherits the
strongconvexity and smoothness properties of the original
construction  Moreover  to optimize this function  the algorithm needs to  solve  all nd  problems simultaneously 
using the same choice of indices             Using   combinatorial argument which parallels the probabilistic argument in the proof of Thm    we can show that no matter
how these indices are chosen  the average number of nonzero coordinates of the iterates cannot grow too rapidly  and
lead to the same bound as in Thm    Since the construction is deterministic  and applies no matter how the indices
are chosen  the lower bound holds deterministically  rather
than in expectation as in Thm   
Lastly  it is useful to consider how the bounds stated in
Thm    and Thm    differ when the dimension   is  xed
and  nite  Inspecting the proofs of both theorems reveals
that in both cases the suboptimality  as   function of the
iteration number     has   linear convegence rate bounded
from below by

 cid     wT          cid 

           cid 

 cid 

 

 cid     

   

 cid 

 

     
     

   

 where   is as de ned in Eq    and   hides dependencies on the problem parameters  but is independent of    
However  whereas the bound established in Thm    is valid
for      number of iterations  Thm    applies to   much restricted range of roughly log    log    iterations  This indicate that adaptive optimization algorithms might be able
to gain   superlinear convergence rate after   signi cantly
smaller number of iterations in comparison to oblivious algorithms  see  Arjevani and Shamir      for   similar
discussion regarding  rstorder methods  That being said 
trading obliviousness for adaptivity may increase the periteration cost and reduce numerical stability 

  cid 

  

 cid 

   

        

 
 

 

 fi   

we can approximate it by taking   sample   of indices in
             uniformly at random  compute the Hessians of
the corresponding individual functions  and use the approximation

          
   

 fi   

If     is large enough  then by concentration of measure
arguments  this sample average should be close to the actual Hessian        On the other hand  if     is not too
large  then the resulting matrix is easier to invert       because it has   rank of only      if each individual Hessian has rank    as in the case of linear predictors 
Thus  one can hope that the right sample size will lead

Oracle Complexity of SecondOrder Methods for FiniteSum Problems

to computational savings  There have been several rigorous studies of such  subsampled Newton  methods  such
as Erdogdu and Montanari   RoostaKhorasani and
Mahoney       Bollapragada et al    and references therein  However  our lower bound in Thm    holds
for such an approach  since it satis es both Assumption  
and   As expected  the existing worstcase complexity upper bounds are no better than our lower bound 
 Xu et al    recently proposed   subsampled Newton method  together with nonuniform sampling  which
assigns more weight to individual functions which are
deemed more  important  This is measured via properties of the Hessians of the functions  such as their norms or
via leverage scores  This approach breaks Assumption  
as the sampled indices are now chosen in   way dependent
on the individual functions  However  our lower bound in
Thm    which does not require this assumption  still applies to such   method 
  variant of the subsampled Newton approach  studied in
Erdogdu and Montanari   uses   lowrank approximation of the sample Hessian  attained by truncated SVD 
in lieu of the sample Hessian itself  However  this still falls
in the framework of Assumption   and our lower bound
still applies 
  different approach to approximate the full Hessian is via
randomized sketching techniques  which replace the Hessian        by   lowrank approximation of the form

      SS cid      

where     Rd       cid    is   random sketching matrix 
and        is the matrix square root of       
This approach forms the basis of the Newton sketch algorithm proposed in Pilanci and Wainwright   This
approach currently escapes our lower bound  since it violates Assumption   That being said  this approach is inherently expensive in terms of computational resources  as
it requires us to compute the square root of the full Hessian
matrix  Even under favorable conditions  this requires us
to perform   full pass over all functions            fn at every iteration  Moreover  existing iteration complexity upper bounds have   strong dependence on both   as well
as the dimension    and are considerably worse than the
lower bound of Thm    Therefore  we conjecture that this
approach cannot lead to better worstcase results 
Agarwal et al    develop another line of stochastic
secondorder methods  which are based on the observation
that the Newton step             is the solution
of the system of linear equations

                

Thus  one can reduce the optimization problem to solving
this system as ef ciently as possible  The basic variant of

their algorithm  denoted as LiSSA  relies on operations of
the form

   cid        fi    

 for   sampled uniformly at random  as well as linear combinations of such vectors  which satisfy our assumptions   
second variant  LiSSAQuad  rephrases this linear system
as the  nitesum optimization problem
  cid                cid  

min

 

  cid 

  

 

 
 

  cid fi        fi   cid   

and uses some  rstorder method for  nitesum problems
in order to solve it  Since individual gradients of this objective are of the form  fi        fi    and most stateof theart  rstorder methods pick indices   obliviously 
this approach also satis es our assumptions  and our lower
bounds apply  Yet another proposed algorithm  LiSSASample  is based on replacing the optimization problem
above by

  cid                  cid   

 

min

 

where   is some invertible matrix  solving it  with the
to              and
optimum being equal
multiplying the solution by    to recover the solution
            to the original problem 
In order
to get computational savings    is chosen to be   linear combination of     log    sampled individual hessians  fi    where it is assumed that   log     cid    
and the sampling and weighting is carefully chosen  based
on the Hessians  so that Eq    has strong convexity and
smoothness parameters within   constant of each other  As
  result  Eq    can be solved fast using standard gradient descent  taking steps along the gradient  which equals
                   at any point    This gradient is
again computable under    using      oracle calls  since
  is   linear combination of   log     cid    sampled individual Hessians  Thus  our lower bound  in the form of
Thm    still applies to such methods 
That being said  it is important to note that the complexity
upper bound attained in Agarwal et al    for LiSSASample is on the order of

      cid      polylog 

 at least asymptotically as       which can be better than
our lower bound if    cid     There is no contradiction  since
the lower bound in Thm    only applies for   dimension  
much larger than    Interestingly  our results also indicate
that an adaptive index sampling scheme is necessary to get
this kind of improved performance when    cid     Otherwise  it could violate Thm    which establishes   lower

Oracle Complexity of SecondOrder Methods for FiniteSum Problems

bound of       cid    even if the dimension is quite
moderate           cid    which is  cid    under the

mild assumption that    cid    
The observation that an adaptive scheme  breaking assumption   can help performance when    cid    is also seen
in the lower bound construction used to prove Thm    If
      are such that the required dimension   is  cid     then
it means that only the functions fj          fjd  which are
  small fraction of all   individual functions  are informative and help us reduce the objective value  Thus  sampling
these functions in an adaptive manner is imperative to get
better complexity than the bound in Thm    Based on the
fact that only at most    out of   functions are relevant in
the construction  we conjecture that the possible improvement in the worstcase oracle complexity of such schemes
may amount to replacing dependencies on   with dependencies on    which is indeed the type of improvement attained  for small enough   in Agarwal et al   
Finally  we note that Agarwal et al    proposes another
algorithm tailored to selfconcordant functions  with runtime independent of the smoothness and strong convexity
parameters of the problem  However  it requires performing     full Newton steps  so the runtime is prohibitive for
largescale problems  indeed  for quadratics as used in our
lower bounds  even   single Newton step suf ces to compute an exact solution 

particular 

  Our lower bound for algorithms employing adaptive
index sampling schemes  Thm    only hold when the
dimension   is very large  This leaves open the possibility of better  non indexoblivious  algorithms when
  is moderate  as was recently demonstrated in the
context of the LiSSASample algorithm of Agarwal
et al     at least for small enough   As discussed
in the previous section  we conjecture that the possible improvement in the worstcase oracle complexity
of such schemes may amount to replacing dependencies on   with dependencies on   

  It might be possible to construct algorithms breaking
Assumption        by using operations which are not
linearalgebraic  That being said  we currently conjecture that this assumptions can be signi cantly relaxed  and similar results would hold for any algorithm which has  signi cantly  cheaper iterations  in
terms of runtime  compared to the Newton method 

                 cid    rows in expectation 

  Our lower bounds are worstcase over smooth and
stronglyconvex individual functions fi  It could be
that by assuming more structure  better bounds can
be obtained  For example  as discussed in the introduction  an important special case is when fi     
 cid     cid 
     for some scalar function  cid   and vector xi 
Our construction in Thm    does not quite    this structure  although it is easy to show that we still get functions of the form fi       cid     cid 
     where Xi has
which is     under   broad parameter regime  We
believe that the difference between     rows and  
row is not signi cant in terms of the attainable oracle
complexity  but we may be wrong  Another possibility is to provide results depending on more delicate
spectral properties of the function  beyond its strong
convexity and smoothness  which may lead to better
results and algorithms under favorable assumptions 

  Our lower bounds in Sec    which establish   linear convergence rate  logarithmic dependence on
log  are nontrivial only if the optimization error
  is suf ciently small  This does not preclude the possibility of attaining better initial performance when  
is relatively large 

In any case  we believe that our work lays the foundation
for   more comprehensive study of the complexity of ef 
cient secondorder methods  for  nitesum and related optimization and learning problems 

  Summary and Discussion
In this paper  we studied the oracle complexity for optimization problems  assuming availability of   secondorder
oracle  This is in contrast to most existing oracle complexity results  which focus on    rstorder oracle  First  we
formally proved that in the standard setting of stronglyconvex and smooth optimization problems  secondorder
information does not signi cantly improve the oracle complexity  and further assumptions       Lipschitzness of the
Hessians  are in fact necessary  We then presented our
main lower bounds  which show that for  nitesum problems with   secondorder oracle  under some reasonable
algorithmic assumptions  the resulting oracle complexity
is   again   not signi cantly better than what can be obtained using    rstorder oracle  Moreover  this is shown
using quadratic functions  which have   derivatives of order larger than   Hence  our lower bounds apply even if
we have access to an oracle returning derivatives of order  
for all       and the function is smooth to any order  In
Sec    we studied how our framework and lower bounds
are applicable to most existing approaches 
Although this conclusion may appear very pessimistic  they
are actually useful in pinpointing potential assumptions and
approaches which may circumvent these lower bounds  In

Oracle Complexity of SecondOrder Methods for FiniteSum Problems

Farbod RoostaKhorasani and Michael   Mahoney  Subsampled newton methods    globally convergent algorithms  arXiv preprint arXiv     

Farbod RoostaKhorasani and Michael   Mahoney  Subsampled newton methods ii  Local convergence rates 
arXiv preprint arXiv     

Blake Woodworth and Nathan Srebro  Tight complexity bounds for optimizing composite objectives  arXiv
preprint arXiv   

Peng Xu  Jiyan Yang  Farbod RoostaKhorasani  Christopher      and Michael   Mahoney  Subsampled newton methods with nonuniform sampling  arXiv preprint
arXiv   

ACKNOWLEDGMENTS

This research is supported in part by an FP  Marie Curie
CIG grant and an Israel Science Foundation grant  

References
Alekh Agarwal and Leon Bottou 

for the optimization of  nite sums 
arXiv   

  lower bound
arXiv preprint

Naman Agarwal  Brian Bullins  and Elad Hazan  Second order stochastic optimization in linear time  arXiv
preprint arXiv   

Yossi Arjevani and Ohad Shamir  Communication complexity of distributed convex learning and optimization 
In Advances in Neural Information Processing Systems 
pages    

Yossi Arjevani and Ohad Shamir  Dimensionfree iteration
complexity of  nite sum optimization problems  In Advances in Neural Information Processing Systems  pages
     

Yossi Arjevani and Ohad Shamir  On the iteration complexity of oblivious  rstorder optimization algorithms 
In Proceedings of the  nd International Conference on
Machine Learning  pages      

Raghu Bollapragada  Richard Byrd  and Jorge Nocedal 
Exact and inexact subsampled newton methods for optimization  arXiv preprint arXiv   

Stephen Boyd and Lieven Vandenberghe  Convex optimiza 

tion  Cambridge university press   

Murat   Erdogdu and Andrea Montanari  Convergence
rates of subsampled newton methods  In Advances in
Neural Information Processing Systems  pages  
   

Guanghui Lan  An optimal randomized incremental gradi 

ent method  arXiv preprint arXiv   

   Nemirovsky and    Yudin  Problem Complexity and
Method Ef ciency in Optimization  WileyInterscience 
 

Yurii Nesterov  Introductory lectures on convex optimization    basic course  volume   Springer Science  
Business Media   

Jorge Nocedal and Stephen Wright  Numerical optimiza 

tion  Springer Science   Business Media   

Mert Pilanci and Martin   Wainwright  Newton sketch   
lineartime optimization algorithm with linearquadratic
convergence  arXiv preprint arXiv   

