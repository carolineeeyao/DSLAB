Stochastic DCA for the Largesum of Nonconvex Functions Problem and its

Application to Group Variable Selection in Classi cation

Hoai An Le Thi   Hoai Minh Le   Duy Nhat Phan   Bach Tran  

Abstract

In this paper  we present   stochastic version
of DCA  Difference of Convex functions Algorithm  to solve   class of optimization problems
whose objective function is   large sum of nonconvex functions and   regularization term  We
consider the  cid  regularization to deal with the
group variables selection  By exploiting the special structure of the problem  we propose an ef 
 cient DC decomposition for which the corresponding stochastic DCA scheme is very inexpensive  it only requires the projection of points
onto balls that is explicitly computed  As an application  we applied our algorithm for the group
variables selection in multiclass logistic regression  Numerical experiments on several benchmark datasets and synthetic datasets illustrate the
ef ciency of our algorithm and its superiority
over wellknown methods  with respect to classi cation accuracy  sparsity of solution as well
as running time 

  Introduction
We consider the following optimization problem

 cid 

min

       

fi           

 

 

 cid 

  cid 

  

 
 

whose objective function   is   large sum of nonconvex
functions fi    and   regularization term      where fi   
corresponds to   criteria to optimize and       is   tradeoff parameter between the two terms  This model covers
  very vast class of problems arising from several  elds
such as machine learning  signal processing  etc  For instance  leastsquares regression  logistic regression problem  etc can be expressed in the form of  

 Laboratory of Theoretical and Applied Computer Science 
University of Lorraine  France  Correspondence to  Hoai Minh
Le  minh le univlorraine fr 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Nowadays  the growth of technologies leads to exponential augmentation of largescale data where the number of
both variables and samples are huge  Thus  optimization
methods for solving the problem   are faced with   great
challenge that is the number of samples   can be extremely
large  Among existing methods for this problem  stochastic
programming has been proved to be suitable thanks to its
ability to exploit the advantage of the sum structure of the
problem  In  Schmidt et al    the authors considered
  special case of the largesum problem   where fi are
convex and smooth functions and   corresponds to the  cid 
regularization  Stochastic Average Gradient was developed
to solve the resulting problem  Reddi et al   Reddi et al 
  developed Proximal Stochastic Gradient method for
the case where fi are smooth  can be nonconvex  and  
is convex  nonsmooth function  Motivated by its success 
we will study stochastic programming for solving   in order to deal with data having an extremely large number of
samples 
On the other hand  in realworld applications such as image processing  microarray analysis  etc  datasets contain
  very large number of variables  In such of cases  we are
often to face with the problem of redundant and irrelevant
variables  Redundant variables contain information already
presented by other variables while irrelevant variables do
not contain useful information  Variables selection methods that consist of selecting important variables for   considered task  are   popular and ef cient way to deal with
redundant and irrelevant variables  In this direction    natural idea is to formulate the variables selection problem as
  minimization of the  cid norm  or  cid cid  The sparse optimization has been extensively studied on both theoretical
and practical aspects  The readers can refer to Le Thi et al 
 Le Thi et al    for an extensive overview of existing
approaches for the minimization of  cid norm 
Nevertheless  when the data possesses certain group structures  we are naturally interested in selecting important
groups of variables rather than individual ones 
For
instance 
in multifactor analysis of variance    factor
with several levels may be expressed through   group of
dummy variables  In genomic data analysis  the correlations between genes sharing the biological pathway can

Stochastic DCA for the Largesum of Nonconvex Functions Problem

be high  Hence these genes should be considered as  
group  Recently  the mixednorm regularization has been
developed for the group variable selection 
It consists
in using the  cid  regularization term  Assume that    
      xm    Rm is partitioned into   nonoverlapping
groups           then the  cid norm of   is de ned by

 cid   cid     cid               cid     cid   cid   cid  Clearly   cid 

norm is nonconvex that makes the optimization problem
involving  cid  challenging  Several works have been developed to solve the problem of mixednorm regularization
 cid  The  rst approach  named the group Lasso  cid 
norm   Yuan   Lin    is closely connected to the
Lasso  cid norm    an approximation of the  cid norm  Tibshirani    This approach was widely used for selecting groups of variables in multitask learning  Obozinski
et al    multiclass support vector machine  Blondel
et al    principal component analysis  Khan et al 
  linear discriminant analysis  Gu et al    and
compressed sensing  Sun et al    etc  The second approach consists in replacing the  cid norm by   DC  Difference of Convex functions  approximation  In  Wang et al 
  the authors used the smoothly clipped absolute deviation  SCAD  approximation and developed   group coordinate descent based algorithm for the sparse linear regression  Later  Huang et al   Huang et al    used the
minimax concave penalty  MCP  for the same problem  In
 Lee et al    the authors considered both above approximations and developed DC programming and DCA
 DC algorithm  based method for the resulting problems 
Recently  Phan et al 
 Phan et al    proposed DCA
based algorithms for bilevel variable selection using the
combination of the  cid norm and  cid   norm 
Paper   contribution  In this paper  we aim at developing
ef cient methods to solve the problem   where   is extremely large and      corresponds to  cid  regularization
 in order to deal with the group variables selection  The
largesum optimization   becomes

 cid 

fi       cid   cid 

 

 

 cid 

min

       

  cid 

  

 
 

We assume that fi    is differentiable with LLipschitz
gradient  This assumption is broad enough to cover several applications  Various important problems in machine
learning such as Multitask feature selection  Sparse logistic regression  Minimizing an expected loss in stochastic
programming  etc  can be expressed in the form of  
As we have mentioned above  the  cid norm can be approximated by   convex        cid norm  or nonconvex
function  Using   nonconvex approximation will lead to  
 harder  optimization problem but it has been proved that
nonconvex approximations perform better than convex approximations in terms of sparsity  Le Thi et al    The
resulting problem is then reformulated as   DC program

and DCA based algorithm will be developed to solve it 
We exploit the special structure of the problem to propose
an ef cient DC decomposition for which the corresponding
DCA scheme is very inexpensive  it only requires the projection of points onto balls that is explicitly computed  On
the other hand  in order to deal with data having   large
number of samples  we present stochastic version DCA 
The convergence properties of the proposed algorithm is
rigorously studied to show that the convergence is guaranteed with probability one 
As an application of our algorithm  we consider the group
variables selection in multiclass logistic regression  We
perform an empirical comparison of stochastic DCA with
DCA and standard methods on very large synthetic and
realworld datasets  and show that the stochastic DCA is
ef cient in group variable selection ability and classi cation accuracy as well as running time 
The remainder of the paper is organized as follows  Solution method based on Stochastic DCA for solving   is
developed in Section   In Section   we apply the proposed
algorithm to the group variables selection in multiclass logistic regression  Finally Section   concludes the paper 

  Solution method via stochastic DCA
  Outline of DC programming and DCA

DC programming and DCA constitute the backbone of
smooth nonsmooth nonconvex programming and global
optimization  Pham Dinh   Le Thi      Le Thi  
Pham Dinh    They address the problem of minimizing   DC function on the whole space Rn or on   closed
convex set     Rn  Generally speaking    standard DC
program takes the form 

    inf                         Rn 

 Pdc 

where      are lower semicontinuous proper convex
functions on Rn  Such   function   is called   DC function  and     is   DC decomposition of   while   and  
are the DC components of       DC program with convex
constraint       can be equivalently expressed as  Pdc  by
adding the indicator function           if       and
  otherwise  to the  rst DC component   
The modulus of strong convexity of   on   denoted by
    or   if     Rn  is given by

      sup             cid cid  is convex on  

One says that   is strongly convex on   if        
For   convex function   the subdifferential of   at     
dom         Rn           denoted by     is

Stochastic DCA for the Largesum of Nonconvex Functions Problem

de ned by

           Rn                cid          cid 

     Rn 

The subdifferential     generalizes the derivative in the
sense that   is differentiable at    if and only if      
     
  point    is called   critical point of        or  
generalized KarushKuhn Tucker point  KKT  of  Pdc  if
               cid   
The main idea of DCA is simple  each iteration   of DCA
approximates the concave part    by its af ne majorization  that corresponds to taking vl      xl  and then
computes xl  by solving the resulting convex problem 

         cid vl    cid 

min
  Rn

The sequence  xl  generated by DCA enjoys the following
properties  Pham Dinh   Le Thi      Le Thi  
Pham Dinh   
    The sequence     xl  is decreasing 
 ii  If    xl       xl  then xl is   critical point of  Pdc 

and DCA terminates at lth iteration 

 iii  If               then the series  cid xk    xk cid 

converges 

 iv  If the optimal value   of  Pdc  is  nite and the in nite
sequence  xl  is bounded then every limit point of the
sequence  xl  is   critical point of       

  Stochastic DCA for solving the problem  

In this section  we introduce   stochastic version of DCA
for solving   that exploits the structure of objective function    We consider   family of DC approximations      
of  cid norm  de ned by

  cid 

  

       

 cid     cid 

where   is   nonconvex penalty function which includes
SCAD  MCP  Capped cid  exponential function   cid    with
           cid    with        see  Le Thi et al    for
more details        can be expressed as                
      where

  cid 

  cid 

 cid 

  cid 

 cid 

  

 
 

  

  

Hence  the approximate problem of   can be written as

min
  Rm

       

fi                   

 

 

 cid cid 

         

 cid     cid  and          

 cid     cid         

xl 
     

 cid 

 cid   cid   cid   

 
 

Each function fi    can be rewritten as

 cid   cid    fi   

 

fi     

 

 cid   cid    fi   cid  is strongly convex with        Hence 
 cid   

Since fi    is differentiable with LLipschitz gradient 

fi    is   DC function  Consequently        is   DC function with the following DC decomposition

                   

 

where      and      are convex functions de ned by

  cid 
 cid   cid         

        
        
 

  

hi    hi       

 cid   cid    fi           

DCA for solving   amounts to computing two sequences
 xl  and  vl  such that vl      xl  and xl  is an optimal
solution of the following convex problem

min cid         cid vl    cid cid   

 

The computation of subgradients of   requires the one of
all components hi  This can be expensive when   is very
large  Hence we propose   stochastic version of DCA in
which we only compute the subgradients of   small subset
of components hi  Precisely  at each iteration    we comfor    cid  sl 
pute vl
where sl          is   randomly chosen set of index 
     hi xl  can be given as vl
The computation of vl
   
 xl  fi xl    yl  where yl      xl  for all     sl  The
convex problem   take the form

     hi xl  for     sl and keep vl

    vl 

 

  cid 

  

 
 cid 

  

  cid 

     cid 
vl

   
        cid cid 

 

 

 

min

 cid     cid   

 
 

 cid   cid     cid   
 

We observe that the objective of   is separable in groups
of    then the solution to this problem can be computed by
solving   independent subproblems of the same form 

where vl
tion of   can be explicitly computed by

       

  vl

     for            The solu 

  

min

 cid     cid   

 cid     cid     cid vl

 
 

 cid  
 cid cid vl

 cid 
   cid     

vl
   
 cid vl
   cid 

 

 

 

Thus  the stochastic DCA  SDCA  for solving the problem
  is described in Algorithm  
Now we will prove that the convergence properties of
SDCA are guaranteed with probability one 

Stochastic DCA for the Largesum of Nonconvex Functions Problem

   Since yl 

 

     xl 
           xl 

 

  we have
     cid     xl 

 

 

 

 
Since fi    is   differentiable function with LLipschitz
gradient  we have
fi      fi xl 
Thus  we get that

 cid   xl 

 fi xl 

 cid   xl 

 cid 

 cid 

 
 

 

 

 

 

  yl 

 cid 

fi                  
From   and   we have

 

     

    xl        xl         

  

     

 

 cid 

  sl

 cid     xl 

 

 cid 

 

 cid xl   xl 

 

 cid 

 

 cid xl xl 

 

 cid 

By taking the expectation of the inequality   conditioned
on Fl  we have

By the supermartingale convergence theorem  we conclude
that

  cid 

  

   

 cid        xl         
  cid     xl Fl
  cid 
 cid 
 cid 

 cid xl   xl 

  

  

 cid xl   xl cid     

 

 cid     

is almost surely satis ed  In particular  we have

 

 

  

almost surely and hence liml   cid xl   xl cid      almost
surely 
   Assume that there exists   subsequence  xlk  of  xl 
such that xlk      almost surely  From   and  
we have  cid xlk    xlk
   cid      almost surely  Without
loss of generality  we can suppose that the subsequences
       almost surely  We note that ylk
ylk
    and
by the closed property of the subdifferential mapping    
we have            with probability one  It follows
from xlk    arg min   lk     that   lk  xlk      lk    
Taking       we get that
             
is almost surely satis ed for all     Rm  Thus  we have

 cid     cid cid          
 

       xlk

  cid 

 
 

  

 fi   cid 

with probability one  Therefore 

 

    

   
 

fi           

 fi           

 

 cid 

       

 

  cid 

  

 cid 

      
 

  cid 

  

Algorithm   SDCA for solving the problem  

Initialization  Choose      Rm        and     
            
Repeat
vl 

  Compute vl
for    cid  sl 
  Compute xl  by using  
  Set        and randomly choose   small subset

     hi xl  for     sl and keep vl

   

 

sl         
Until Stopping criterion 

Theorem   If     inf           and  sl      for all
      then SDCA generates the sequence  xl  such that

    cid 

       xl  is the almost sure convergent sequence 

    cid xl   xl cid  is almost surely  nite and

liml   cid xl   xl cid      almost surely 

   Every limit point of  xl  is   critical point of   with

probability one 

    xl and yl

    yl  for    cid  sl  We denote    

    yl for     sl  xl

    xl 
  the function given

 

Proof     Let xl
and yl
by

   
               

 cid     xl

  cid     cid     xl

     fi xl
  cid 

   yl

 
 

  fi xl

 cid  
        xl
  

 cid 

 
 

and           
 
  it follows that xl    arg min        Hence  we have

      From the step   in Algorithm

      

    xl        xl       xl   

 fi xl 

 

  sl
     xl        

 

 xl 

Let Fl denote the  algebra generated by the entire history of SDCA up to the iteration                  and
Fl         xl       sl  for all       By taking
the expectation of the inequality   conditioned on Fl  we
have

  cid     xl Fl

 cid        xl     

 cid     xl       xl cid   

 

By the supermartingale convergence theorem  we can conclude that the sequence      xl    converges almost
surely  Moreover 

 cid 

 cid     xl       xl cid     

  

almost surely and hence     xl  converges almost surely 

with probability one  This implies that    is   critical point
of   with probability one and the proof is complete 

Stochastic DCA for the Largesum of Nonconvex Functions Problem

  Application to Group Variables Selection in

Multiclass Logistic Regression

Logistic regression  introduced by    Cox in    Cox 
  is   popular method in supervised learning  Logistic
regression has been successfully applied in various reallife
problems such as cancer detection  Kim et al    medical  Bagley et al    Subasi   Erc elebi    social
science  King   Zeng    etc  Especially  logistic regression combined with feature selection has been proved
to be suitable for high dimensional problems  for instance 
document classi cation  Genkin et al    and microarray classi cation  Liao   Chin    Kim et al   
We describe the multiclass logistic regression problem as
follows  Let   be         matrix  where   and   are the
number of features and number of classes  respectively  We
denote the ith column of   by     and           bQ   
RQ  In the multiclass logistic classi cation problem    new
instance    is classi ed to class    by using the rule     
arg maxk                where                is
the conditional probability de ned by

                

exp by      

 yx 

 

 

exp bk      

 kx 

Given   training set containing   instances xi and their corresponding labels yi          we aim to  nd        for
which the total probability of the training instances xi belonging to its correct classes yi is maximized  To estimate
       we maximize the loglikelihood function de ned as

             
 

 cid xi  yi       

 

where  cid xi  yi            log       yi     xi  As
mentioned above  to deal with irrelevant and or redundant
variables in highdimensional data  we use variables selection method  Note that   variable   is to be removed if and
only if all components in the row   of   are zero  Therefore  we can consider each row of   as   group  Denote
by Wj  the jth row of the matrix     The  cid norm of    
     the number of nonzero rows of     is de ned by
 cid   cid                   cid Wj cid   cid   

Hence  the  cid  regularized multiclass logistic regression
problem is formulated as

  cid 

  

  cid 

  

 cid 

  cid 

  

 
 

min
   

 cid 

 cid xi  yi           cid   cid 

 

 

Observe that the problem   takes the form of   where
the function fi          cid xi  yi       
In this application  we use   nonconvex approximation of the  cid norm

based on the piecewise exponential penalty function  This
approximation function has shown its ef ciency in several problems  for instance  variables selection in SVM
 Bradley   Mangasarian    Le Thi et al    semisupervised support vector machines  Le et al    sparse
multiclass support vector machines  Le Thi   Nguyen 
  sparse signal recovery  Le Thi et al    sparse
linear discriminant analysis  Le Thi   Phan       
variables selection in SVM with uncertain data  Le Thi
et al    etc  Using the piecewise exponential penalty
function  the corresponding approximate problem of  
takes the form 

 cid 

fi               

 

 

 cid 
  cid 
where           cid  

min
   

 
 

  

    cid Wj cid  with          
exp    The function        can be expressed as   DC
  cid 
function 

 cid Wj cid          

          

where         cid  

  

  Wj cid   exp cid Wj cid 
According to the SDCA scheme in Algorithm   at each
           bl   
iteration    we have to compute  vl
   zl
 fi      bl     yl    for     sl  where

  xi     kyi
  xi     kyi

 cid  xi

  pl

 bk fi      bl 
 cid  

     fi      bl   cid pl
 

 cid    
  cid 

     
     

exp bl
   bl

  cid 

   
  

 cid    

yl
    

     xi 
     xi 

  xi   

with pl
and   otherwise  The computation of yl is given by
  cid     

and  kyi     if     yi

 

 

if  cid    
otherwise

 

SDCA for solving   is described in Algorithm  

  Numerical Experiment

  DATASETS

To illustrate the performances of algorithms  we performed
numerical tests on real datasets  aloi  covertype  madelon
and sensorless  and simulated datasets  sim   sim   and
sim   Dataset Aloi is   library of object images   while
covertype  madelon  sensorless  are taken from the wellknown UCI data repository 
We used the same way as proposed in  Witten   Tibshirani    to generate simulated datasets  In sim   features are independent with different means in each class 

 http aloi science uva nl 

Stochastic DCA for the Largesum of Nonconvex Functions Problem

Algorithm   SDCA for solving the problem  

 

 

   zl
  for    cid  sl 

Initialization  Choose       Rd         RQ        
                 
Repeat
 yl    for     sl using   and keep  vl
 vl 

  Compute  vl
  zl 
  Compute       bl  by

           bl     fi      bl   
    
 cid  
   cid cid vl
  cid     cid 
 cid  

bl 
    

  

vl
  
  cid 
 cid vl

   
  

   zl
 

   zl

 

  Set        and randomly choose   small subset

     for           

 

 

  vl

where vl
      
  
sl         
Until Stopping criterion 

In sim   features also have different means in each class 
however they are dependent  The dataset sim   has different onedimensional means in each class with independent
features  The procedure for generating simulated datasets
is described as follows 
For sim   this dataset consists of four classes  The class
  is sampled from the multivariate normal distribution
         where the mean vector         is given by
 kj     if                     and   otherwise 
We generate     samples for each class 
For sim   we generate   dataset with three classes sampled
from the multivariate normal distributions            
      where         is de ned by  kj          if
      and   otherwise  We use the block diagonal matrix
  with  ve blocks of dimension       whose element
      cid  is      cid      instances are generated 
For sim   we generate   dataset including four classes as
follows  xi   Ck then xij              if      
            and xij         otherwise  We generate
    instances with equal probabilities for each class 
For preprocessing data  we use standardization to scale the
data 

  COMPARATIVE ALGORITHMS

 cid 

We compare our algorithm with two algorithms  msgl and
liblinear  msgl  Vincent   Hansen    is   coordinate
gradient descent algorithm for solving the multiclass
logistic regression using  cid  regularization term       the
 
convex problem min
   
LibLinear  Fan et al   
is   wellknown
package for
solving largescale problems by using
the coordinate descent algorithm  We use the  cid 

 cid xi  yi           cid   cid 

  cid 

 cid 

  

 
 

 cid    cid 

min

regularized logistic regression solver of LibLinear
to
problem

binary

solve

the

logistic
log      yiwT xi     

 cid 

  cid 

regression
 wj 

 

and

then

  

 
the onevs therest strategy is used for the multiclass case 

  

  EXPERIMENT SETTING

The comparison of algorithms are performed in terms of
three criteria  classi cation accuracy on test set  sparsity of
solution and running time  Sparsity is computed as the percentage of selected features  where   feature                 
is considered to be removed if all absolute values of components of row Wj  are smaller than   threshold      
The crossvalidation procedure is used for experiments  We
randomly take   of the whole dataset as   training set
and the rest is used as test set   This process is repeated   times and we report the mean and standard deviation of each criterion 
We use the earlystopping condition for SDCA  This is  
wellknow technique in machine learning  especially in
stochastic learning which permits to avoid the over tting
problem  After each epoch  we compute the accuracy based
on the validation set  then we stop SDCA if the accuracy is
not improved after npatience     epochs  For comparative
algorithms  we use their default stopping parameters  We
also stop algorithms if they exceed   hours of running time
in the training process 
 
For SDCA  we set
tightness of zeronorm approximation            
For both LibLinear and msgl  the tradeoff parameter
is chosen in interval            
All experiments are performed on   PC Intel     Xeon    
        GHz of  GB RAM 

 cid     cid  and the parameter for controlling the

the tradeoff parameter  

  EXPERIMENT     COMPARISON OF SDCA AND

DCA

Firstly  we will study the impact of batch size on the quality
of solution and the running time of SDCA  The batch size
refers to the size of set of index sl       the number of components hi that are used to compute the subgradients of   
at each iteration      Algorithm   In DCA  or fullbatch
DCA  all components hi are used       sl               
The Table   reports the accuracy and the running time of
SDCA as the batch size varies on an arbitrary chosen dataset
 sensorless 
We observe that the running time is smallest     with
batch size equals to   while giving the second best classi cation accuracy   only   smaller than the
best one  Hence  we choose the batch size as   through 

Stochastic DCA for the Largesum of Nonconvex Functions Problem

Table   Performance of SDCA as batch size varies

Batch Size
Time    
Accuracy  

 
 
 
 
               

 
 

 
 

 
 

 
 

 
 

 
 

Table   Comparative results on both simulated and real datasets 
Bold values correspond to best results for each dataset  NA means that the algorithm fails to furnish   result       and   is the number
of instances  the number of dimensions and the number of classes respectively 

Dataset

aloi
           
     

covertype
           
     

madelon
           
     

sensorless
           
     

sim  
           
     

sim  
           
     

sim  
           
     

Algorithm

DCA  Fullbatch 
SDCA
MSGL
LibLinear

DCA  Fullbatch 
SDCA
MSGL
LibLinear

DCA  Fullbatch 
SDCA
MSGL
LibLinear

DCA  Fullbatch 
SDCA
MSGL
LibLinear

DCA  Fullbatch 
SDCA
MSGL
LibLinear

DCA  Fullbatch 
SDCA
MSGL
LibLinear

DCA  Fullbatch 
SDCA
MSGL
LibLinear

Accuracy  
STD
 
 
NA
 

Mean
 
 
NA
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

Time    

Mean
 
 
NA
 

STD
 
 
NA
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

Sparsity  
STD
 
 
NA
 

Mean
 
 
NA
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

out our experiments 
To illustrate the potential gain of SDCA  we compare it with
  DCA for solving the problem   From the Table   we
see that the gain of running time of SDCA ranges from  
times  aloi  to   times  covertype 
Concerning the classi cation accuracy  SDCA and DCA are

comparable  SDCA gives slightly better accuracy than DCA
on covertype  sim   sim   with   gain ranges from  
to   The gain of SDCA is higher on   datasets  madelon  sim     and   DCA furnishes   better result on aloi and sensorless  especially the gain is up to  
on sensorless  The results prove that SDCA can greatly improve the running time of DCA while archiving   similar

Stochastic DCA for the Largesum of Nonconvex Functions Problem

accuracy 

  EXPERIMENT     SIMULATED DATASET

For synthetic datasets  sim   sim   and sim   we know
in advance the informative features that were used to generate the datasets  Hence  the purpose of this experiment
is to study the ability of algorithms to select these informative features in order to furnish   good classi cation accuracy  The comparison is performed with   algorithms 
SDCA  msgl and LibLinear  We report the results in
Table   and observe that 
For sim   dataset  LibLinear gives   slightly better classi cation accuracy   comparing to SDCA  
and msgl   However  SDCA is by far the fastest
algorithm  SDCA is    resp    times faster than
LibLinear  resp  msgl  Furthermore  SDCA and
LibLinear successfully suppress the   uninformative
features  which it also matches with our procedure of generating this synthetic dataset  msgl fails on this purpose
by selecting   of features 
For sim   dataset  SDCA is the best algorithm on both criteria  classi cation accuracy and running time  Similarly to
sim   dataset  only SDCA and LibLinear can correctly
select the informative features  
For sim   dataset  SDCA exceeds LibLinear and
GLASSO on all three comparison criteria  classi cation accuracy  sparsity and speed  LibLinear almost selects all
the features   selected  but gives   accuracy
lower than SDCA   and it is also   times slower
than SDCA  Among the three algorithm  only SDCA successfully selects the informative features 
three synthetic datasets  SDCA
To summarize  for all
successfully selects
features 
LibLinear selects the exact features on   out of  
datasets while GLASSO fails on all three datasets 

informative

the

exact

  EXPERIMENT     REALWORLD DATASETS

In this experiment  we perform the comparative study
between SDCA  msgl and LibLinear on realworld
datasets   We observe from Table   that 
For aloi dataset  SDCA only selects   of features
for   classi cation accuracy of   while LibLinear
has   worse accuracy with   of features used  Moreover  SDCA is the   times faster than LibLinear while
msgl fails to furnish   result after   hours of running time 
For covertype dataset  SDCA furnishes better classi cation
accuracy than LibLinear and msgl  Moreover  SDCA is
by far faster than the two others  SDCA is SDCA is   times
faster than LibLinear and   times faster than msgl 
Concerning the sparsity of solution  msgl is the best while

sensorless dataset  SDCA is better

LibLinear fails to suppress features 
The dataset madelon is known to be nonlinear  Hence  all
three algorithms furnish quite low classi cation accuracy
  for SDCA    for LibLinear and  
for msgl  As for the sparsity  SDCA suppresses more features than LibLinear and msgl 
For
than both
LibLinear and msgl on all three aspects  classi cation accuracy  sparsity and running time  In terms of classi cation accuracy  the gain of SDCA versus msgl  resp 
LibLinear  is    resp    Regarding the running time  SDCA is   times faster than msgl and  
times faster than LibLinear  As for the sparsity  SDCA
selects   less features than msgl while LibLinear
fails to suppress features 
Overall  SDCA gives the best among the three in term of
classi cation accuracy on all   datasets  As for running
time  SDCA is by far the fastest algorithm  Concerning the
sparsity of solution  SDCA suppresses more features than
the two others on   out of   datasets 

  Conclusions
We have rigorously studied the largesum optimization
problem involving  cid  regularization  The  cid norm is approximated by   DC function  namely the piecewise exponential function  The resulting problem is then reformulated as   DC program and we developed stochastic DCA
to solve it  Exploiting the fact that each component fi   
is differentiable with LLipschitz gradient  we propose   
stochastic version of DCA that is very inexpensive  At
each iteration  the algorithm only requires the computing
the subgradients of   small subset of functions and the projection of points onto balls that is explicitly computed  We
have also proved that the convergence is guaranteed with
probability one  As an application  we applied our algorithm to the group variables selection in multiclass logistic
regression problem  Numerical experiments were carefully
conducted on both synthetic and realworld datasets  The
numerical results show that SDCA greatly improves the
running time of DCA while giving similar accuracy  Moreover  our algorithm SDCA outperforms standard algorithms
 Liblinear and msgl  on all   criteria  classi cation
accuracy  sparsity of solution and running time  Especially 
the gain in running time is huge  SDCA is up to   times
faster than msgl and   times faster than LibLinear 
We are convinced that stochastic DCA is   promising approach for handling very largescale datasets in machine
learning 

Stochastic DCA for the Largesum of Nonconvex Functions Problem

References
Bagley        White     and Golomb        Logistic regression in the medical literature  Standards for use and
reporting  with particular attention to one medical domain  Journal of Clinical Epidemiology   
   

Blondel  Mathieu  Seki  Kazuhiro  and Uehara  Kuniaki  Block coordinate descent algorithms for largescale
sparse multiclass classi cation  Machine Learning   
   

Bradley  Paul    and Mangasarian        Feature Selection via Concave Minimization and Support Vector MaIn Proceedings of the Fifteenth International
chines 
Conference on Machine Learning  ICML   pp   
  San Francisco  CA  USA    Morgan Kaufmann
Publishers Inc 

Cox  David  The regression analysis of binary sequences
 with discussion    Roy Stat Soc       

Fan       Chang       Hsieh       Wang       and
Lin       Liblinear    library for large linear classi cation  Journal of Machine Learning Research   
    URL https cran rproject 
org web packages LiblineaR index html 

Genkin  Alexander  Lewis  David    and Madigan  David 
Largescale Bayesian logistic regression for text categorization  Technometrics     

Gu  Quanquan  Li  Zhenhuif  and Han  Jiawei  Linear discriminant dimensionality reduction  In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases  pp    Springer   

Huang  Jian  Wei  Fengrong  and Ma  Shuangge  Semiparametric Regression Pursuit  Statistica Sinica   
   

Khan     Shafait     and Mian     Joint Group Sparse PCA
for Compressed Hyperspectral Imaging  IEEE Transactions on Image Processing     

Kim  Jinseog  Kim  Yuwon  and Kim  Yongdai   
GradientBased Optimization Algorithm for LASSO 
Journal of Computational and Graphical Statistics   
   

King  Gary and Zeng  Langche  Logistic Regression in
Rare Events Data  Political Analysis     

Le        Le Thi        and Nguyen        Sparse semisupervised support vector machines by DC programming and DCA  Neurocomputing     

Le Thi        and Nguyen        DCA based algorithms
for feature selection in multiclass support vector machine  Annals of Operations Research   
 

Le Thi        Nguyen           and Le        Sparse Signal Recovery by Difference of Convex Functions Algorithms  In Intelligent Information and Database Systems 
pp    Springer  Berlin  Heidelberg   

Le Thi        Vo        and Pham Dinh     Feature selection for linear SVMs under uncertain data  Robust
optimization based on difference of convex functions algorithms  Neural Networks     

Le Thi        Pham Dinh     Le        and Vo       
DC approximation approaches for sparse optimization 
European Journal of Operational Research   
   

Le Thi  Hoai An and Pham Dinh  Tao  The DC  Difference
of Convex Functions  Programming and DCA Revisited
with DC Models of Real World Nonconvex Optimization
Problems  Annals of Operations Research   
   

Le Thi  Hoai An and Phan  Duy Nhat  DC Programming
and DCA for Sparse Optimal Scoring Problem  Neurocomput         

Le Thi  Hoai An and Phan  Duy Nhat  DC programming
and DCA for sparse Fisher linear discriminant analysis 
Neural Computing and Applications  pp       

Le Thi  Hoai An  Le  Hoai Minh  Nguyen  Van Vinh  and
Pham  Dinh Tao    DC programming approach for feature selection in support vector machines learning  Advances in Data Analysis and Classi cation   
   

Lee  Sangin  Oh  Miae  and Kim  Yongdai  Sparse optimization for nonconvex group penalized estimation 
Journal of Statistical Computation and Simulation   
   

Liao        and Chin  KhewVoon  Logistic regression for
disease classi cation using microarray data  Model selection in   large   and small   case  Bioinformatics   
   

Obozinski  Guillaume  Taskar  Ben  and Jordan  Michael 
Multitask feature selection  Statistics Department  UC
Berkeley  Tech  Rep     

Pham Dinh  Tao and Le Thi  Hoai An  Convex analysis
approach to dc programming  Theory  algorithms and
applications  Acta Mathematica Vietnamica   
   

Stochastic DCA for the Largesum of Nonconvex Functions Problem

Pham Dinh  Tao and Le Thi  Hoai An          Optimization Algorithm for Solving the TrustRegion Subproblem  SIAM Journal of Optimization   
 

Phan  Duy Nhat  Le Thi  Hoai An  and Pham Dinh  Tao 
Ef cient bilevel variable selection and application to estimation of multiple covariance matrices  In Advances in
Knowledge Discovery and Data Mining   st Paci cAsia Conference  PAKDD   Proceedings  Part   
volume   pp    Springer International Publishing   

Reddi  Sashank    Sra  Suvrit  Poczos  Barnabas  and
Smola  Alexander    Proximal stochastic methods for
Nonsmooth Nonconvex FiniteSum Optimization 
In
Advances in Neural Information Processing Systems  pp 
   

Schmidt  Mark  Le Roux  Nicolas  and Bach  Francis  Minimizing  nite sums with the stochastic average gradient 
Mathematical Programming     

Subasi  Abdulhamit and Erc elebi  Ergun  Classi cation of
EEG signals using neural network and logistic regression  Computer Methods and Programs in Biomedicine 
   

Sun  Liang  Liu  Jun  Chen  Jianhui  and Ye  Jieping  Ef 
cient Recovery of Jointly Sparse Vectors  In Bengio    
Schuurmans     Lafferty        Williams           and
Culotta      eds  Advances in Neural Information Processing Systems   pp    Curran Associates 
Inc   

Tibshirani  Robert  Regression Shrinkage and Selection
Via the Lasso  Journal of the Royal Statistical Society 
Series       

Vincent  Martin and Hansen  Niels Richard  Sparse group
lasso and high dimensional multinomial classi cation 
Comput  Stat  Data Anal     

Wang  Lifeng  Chen  Guang  and Li  Hongzhe  Group
SCAD regression analysis for microarray time course
gene expression data  Bioinformatics   
   

Witten  Daniela    and Tibshirani  Robert 

Penalized
classi cation using Fisher   linear discriminant  Journal of the Royal Statistical Society  Series    Statistical
Methodology     

Yuan  Ming and Lin  Yi  Model selection and estimation in
regression with grouped variables  Journal of the Royal
Statistical Society  Series       

