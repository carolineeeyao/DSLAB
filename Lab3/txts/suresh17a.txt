Distributed Mean Estimation with Limited Communication

Ananda Theertha Suresh   Felix    Yu   Sanjiv Kumar      Brendan McMahan  

Abstract

Motivated by the need for distributed learning
and optimization algorithms with low communication cost  we study communication ef cient
algorithms for distributed mean estimation  Unlike previous works  we make no probabilistic assumptions on the data  We  rst show that for  
dimensional data with   clients    naive stochastic rounding approach yields   mean squared error  MSE  of       and uses   constant number of bits per dimension per client  We then extend this naive algorithm in two ways  we show
that applying   structured random rotation before
quantization reduces the error to   log     
and   better coding strategy further reduces the
error to      We also show that the latter
coding strategy is optimal up to   constant in the
minimax sense      it achieves the best MSE for
  given communication cost  We  nally demonstrate the practicality of our algorithms by applying them to distributed Lloyd   algorithm for kmeans and power iteration for PCA 

  Introduction
  Background
Given   vectors     def                Xn   Rd that reside
on   clients  the goal of distributed mean estimation is to
estimate the mean of the vectors 

   def 

 
 

nXi 

Xi 

 

This basic estimation problem is used as   subroutine in
several learning and optimization tasks where data is distributed across several clients  For example  in Lloyd   algorithm  Lloyd    for kmeans clustering  if data is distributed across several clients  the server needs to compute

 Google Research  New York  NY  USA  Google Research 
Seattle  WA  USA  Correspondence to  Ananda Theertha Suresh
 theertha google com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

the means of all clusters in each update step  Similarly  for
PCA  if data samples are distributed across several clients 
then for the poweriteration method  the server needs to average the output of all clients in each step 
Recently  algorithms involving distributed mean estimation
have been used extensively in training largescale neural
networks and other statistical models  McDonald et al 
  Povey et al    Dean et al    McMahan et al 
  Alistarh et al    In   typical scenario of synchronized distributed learning  each client obtains   copy
of   global model  The clients then update the model independently based on their local data  The updates  usually
in the form of gradients  are then sent to   server  where
they are averaged and used to update the global model   
critical step in all of the above algorithms is to estimate the
mean of   set of vectors as in Eq   
One of the main bottlenecks in distributed algorithms is the
communication cost  This has spurred   line of work focusing on communication cost in learning  Tsitsiklis  
Luo    Balcan et al    Zhang et al    Arjevani   Shamir    Chen et al    The communication cost can be prohibitive for modern applications  where
each client can be   lowpower and lowbandwidth device
such as   mobile phone  Kone cn   et al    Given such
  wide set of applications  we study the basic problem of
achieving the optimal minimax rate in distributed mean estimation with limited communication 
We note that our model and results differ from previous
works on mean estimation  Zhang et al    Garg et al 
  Braverman et al    in two ways  previous works
assume that the data is generated        according to some
distribution  we do not make any distribution assumptions
on data  Secondly  the objective in prior works is to estimate the mean of the underlying statistical model  our goal
is to estimate the empirical mean of the data 

  Model
Our proposed communication algorithms are simultaneous
and independent       the clients independently send data
to the server and they can transmit at the same time  In any
independent communication protocol  each client transmits
  function of Xi  say    Xi  and   central server estimates
the mean by some function of                        Xn 

 

and   sb                           each client sends
one bit per dimension  We further show that this bound is
tight  In many practical scenarios    is much larger than  
and the above error is prohibitive  Kone cn   et al   
  natural way to decease the error is to increase the number
of levels of quantization  If we use   levels of quantization 
in Theorem   we show that the error deceases as

nXi 

         

   
 Xi 

  sk           
However 
the communication cost would increase to
  sk              ddlog  ke       bits  which can be
expensive  if we would like the MSE to be       
In order to reduce the communication cost  we propose two
approaches 
Stochastic rotated quantization  We show that preprocessing the data by   random rotation reduces the mean
squared error  Speci cally  in Theorem   we show that
this new scheme  denoted by  srk  achieves an MSE of

 

 
 

 
 

nXi 

     
  srk            log  
 Xi 
         
and has   communication cost of   srk            
 ddlog  ke       Note that the new scheme achieves
much smaller MSE than naive stochastic quantization for
the same communication cost 
Variable length coding  Our second approach uses the
same quantization as  sk but encodes levels via variable
length coding  Instead of using dlog  ke bits per dimension  we show that using variable length encoding such as
arithmetic coding to compress the data reduces the communication cost signi cantly  In particular  in Theorem   we
show that there is   scheme  denoted by  svk  such that
  svk           nd    log                  
and   svk           sk       Hence  setting     pd

in Eqs    and   yields

  svk             

   

   
 Xi 

 
 

nXi 

and with  nd  bits of communication      constant number of bits per dimension per client  Of the three protocols 
 svk has the best MSE for   given communication cost 
Note that  svk uses   quantization levels but still uses   
bits per dimension per client for all     pd 

Theoretically  while variable length coding has better guarantees  stochastic rotated quantization has several practical

                     

 

   

We allow the use of both private and public randomness 
Private randomness refers to random values that are generated by each machine separately  and public randomness refers to   sequence of random values that are shared
among all parties 
The proposed algorithms work for any      To measure
the minimax performance  without loss of generality  we
restrict ourselves to the scenario where each Xi   Sd  the
ball of radius   in Rd           Sd iff

       

where     denotes the   norm of the vector    For  
protocol   the worst case error for all       Sd is

        def 

max

    Xi Sd  iE      

Let     denote the set of all protocols with communication cost at most    The minimax MSE is

     Sd  def  min

          

  Results and discussion
  ALGORITHMS
We  rst analyze the MSE         for three algorithms 
when           nd       each client sends   constant
number of bits per dimension 
Stochastic uniform quantization 
In Section   as  
warmup we  rst show that   naive stochastic binary quantization algorithm  denoted by  sb  achieves an MSE of

  sb            

   

   
 Xi 

 
 

nXi 

Distributed Mean Estimation with Limited Communication

Let   be any such protocol and let Ci  Xi  be the expected number of transmitted bits by the ith client during
protocol   where throughout the paper  expectation is over
the randomness in protocol  
The total number of bits transmitted by all clients with the
protocol   is

        def 

nXi 

Ci  Xi 

Let the estimated mean be     For   protocol   the MSE
of the estimate is

 In the absence of public randomness  the server can communicate   random seed that can be used by clients to emulate public
randomness 

 We use     to denote   log dn 
 All logarithms are to base    unless stated 

Distributed Mean Estimation with Limited Communication

advantages  it uses  xed length coding and hence can be
combined with encryption schemes for privacy preserving
secure aggregation  Bonawitz et al    It can also provide lower quantization error in some scenarios due to better constants  see Section   for details 
Concurrent to this work  Alistarh et al    showed that
stochastic quantization and Elias coding can be used to obtain communicationoptimal SGD  Recently  Kone cn    
Richt arik   showed that  sb can be improved further
by optimizing the choice of stochastic quantization boundaries  However  their results depend on the number of bits
necessary to represent    oat  whereas ours do not 

  MINIMAX MSE
In the above protocols  all of the clients transmit the data 
We augment these protocols with   sampling procedure 
where only   random fraction of clients transmit data  We
show that   combination of klevel quantization  variable
length coding  and sampling can be used to achieve information theoretically optimal MSE for   given communication cost  In particular  combining Corollary   and Theorem   yields our minimax result 
Theorem   There exists   universal constant       such
that for communication cost     ndt and        
    

     Sd     min 

 

This result shows that the product of communication cost
and MSE scales linearly in the number of dimensions 
The rest of the paper is organized as follows  We  rst analyze the stochastic uniform quantization technique in Section   In Section   we propose the stochastic rotated quantization technique  and in Section   we analyze arithmetic
coding  In Section   we combine the above algorithm with
  sampling technique and state the upper bound on the minimax risk  and in Section   we state the matching minimax lower bounds  Finally  in Section   we discuss some
practical considerations and apply these algorithms on distributed power iteration and Lloyd   algorithm 

  Stochastic uniform quantization
  Warmup  Stochastic binary quantization
For   vector Xi  let   max
  max     Xi    and simii   min     Xi    In the stochastic binary
larly let   min
quantization protocol  sb  for each client    the quantized
value for each coordinate   is generated independently with
private randomness as

 

Yi       max

  min

 

 

     Xi     min
     min
  max
otherwise 

 

 

 

Observe EYi      Xi    The server estimates    by

   sb  

 
 

nXi 

Yi 

We  rst bound the communication cost of the this protocol 
Lemma   There exists an implementation of stochastic
binary quantization that uses         bits per client and
hence   sb                   

 

 

 

and   min

Proof  Instead of sending vectors Yi  clients transmit two
real values   max
 to   desired error  and   bit
vector      such that              if Yi     max
and   otherwise  Hence each client transmits        bits  where  
is the number of bits to transmit the real value to   desired
error 
Let   be the maximum norm of the underlying vectors 
To bound    observe that using   bits  one can represent  
number between    and   to an error of      Thus
using   log dn      bits one can represent the minimum
and maximum to an additive error of   nd  This error in transmitting minimum and maximum of the vector
does not affect our calculations and we ignore it for simplicity  We note that in practice  each dimension of Xi is
often stored as     bit or   bit  oat  and   should be set
as either   or   In this case  using an even larger   does
not further reduce the error 

We now compute the estimation error of this protocol 
Lemma   For any set of vectors     

  sb        

 
  

nXi 

dXj 

Proof 

   max

   Xi   Xi     min

 

 

 

 

 

  sb                    
nXi 

 
  

 

 

     

nXi 

 

 

 

 Yi   Xi 

 

  Yi   Xi 
   

where the last equality follows by observing that Yi   Xi 
    are independent zero mean random variables  The proof
follows by observing that for every   

  Yi   Xi 

   

 

dXj 
dXj 

  Yi      Xi   

   max

    Xi   Xi        min

 

 

Distributed Mean Estimation with Limited Communication

Lemma   implies the following upper bound 
Lemma   For any set of vectors     

  sb        

 
    

 
 

nXi 

 Xi 
   

Proof  The proof follows by Lemma   observing that   
 
      min
   max

    Xi   Xi        min

   max

   

 

 

 

 

and

   max

      min

 

     Xi 
   

 

We also show that the above bound is tight 
Lemma   There exists   set of vectors     such that

  sb        

     
    

 
 

nXi 

 Xi 
   

Proof  For every    let Xi be de ned as follows  Xi   

    Xi        and for all       Xi       
For every      max
          Substituting
these bounds in the conclusion of Lemma    which is an
equality  yields the theorem 

      and   min

 

Therefore  the simple algorithm proposed in this section
gives MSE       Such an error is too large for realworld use  For example  in the application of neural networks  Kone cn   et al      can be on the order of millions  yet   can be much smaller than that  In such cases 
the MSE is even larger than the norm of the vector 

 

  max     Xi    and   min

  Stochastic klevel quantization
  natural generalization of binary quantization is klevel
quantization  Let   be   positive integer larger than  
We propose   klevel stochastic quantization scheme  sk
to quantize each coordinate  Recall that for   vector Xi 
    min     Xi   
  max
For every integer   in the range      let
rsi
     
    natural choice
where si satis es   min
    si     max
  The algorithm quanfor si would be   max
tizes each coordinate into one of Bi     stochastically  In
 sk  for the ith client and jth coordinate  if Xi     
 Bi    Bi      

Bi    def    min

    min

   

 

 

 

 

Yi     Bi            Xi   Bi   

otherwise 

Bi   Bi   

Bi   

 We will show in Section   however    higher value of si and

variable length coding has better guarantees 

The server estimates    by

   sk  

 
 

nXi 

Yi 

As before  the communication complexity of this protocol
is bounded  The proof is similar to that of Lemma   and
hence omitted 
Lemma   There exists an implementation of stochastic klevel quantization that uses ddlog          bits per client
and hence   sk            ddlog  ke      

The mean squared loss can be bounded as follows 
Theorem   If   max
for any      the  sk protocol satis es 

    si     Xi      then

      min

  sk        

Proof 

  sk                    

  Yi   Xi 

   

 
  

 

nXi 

 

 

 

 
  

 

          

 Xi 
   

 
 

nXi 
 
     
nXi 

 

 

 

 

 Yi   Xi 

nXi 
        

  
 

 

 

where the last equality follows by observing Yi     
Xi    is an independent zero mean random variable with
      si     Xi  completes
  Yi      Xi        
the proof 

 

We conclude this section by noting that si     max
     min
 
satis es the conditions for the above theorem by Eq   

 

 

     max

  Stochastic rotated quantization
We show that the algorithm of the previous section can
be signi cantly improved by   new protocol  The motivation comes from the fact that the MSE of stochastic
binary quantization and stochastic klevel quantization is
   the proof of Lemma   and TheO   
    min
  Therefore the MSE
orem   with si     max
 
is smaller when   max
are close  For example 
when Xi is generated uniformly on the unit sphere  with
high probability    max
  Gupta    In such case    sk       is    log  
   
stead of     
In this section  we show that even without any assumptions on the distribution of the data  we can  reduce 
  with   structured random rotation  yielding
  max
      min

     Dasgupta

is     log  

    min
 
and   max

      min

    in 

 

 

 

Distributed Mean Estimation with Limited Communication

    error  We call the method stochastic rotated

an    log  
quantization and denote it by  srk 
Using public randomness  all clients and the central server
generate   random rotation matrix  random orthogonal matrix      Rd   according to some known distribution  Let
Zi   RXi and            In the stochastic rotated quantization protocol  srk    clients quantize the vectors Zi
instead of Xi and transmit them similar to  srk  The server
estimates    by

   srk         

    

 
 

nXi 

Yi 

The communication cost is same as  sk and is given by
Lemma   We now bound the MSE 
Lemma   For any        srk         is at most
   Zmin

ERh Zmax

        

    

nXi 

 

 

 

where Zi   RXi and for every    let si   Zmax

    Zmin

 

 

Proof 

 

  srk                    
                  
             
  ERE          
 
 Zn
nXi 
    Zmin

        

ER Zmax

     

 

 

   

 

 

 

 

where the last inequality follows Eq    and the value of
si      follows from the fact that rotation does not change
the norm of the vector  and     follows from the tower law
of expectation  The lemma follows from observing that

 Zmax

    Zmin

 

     Zmax

 

     Zmin

 

 

 

 

  and  Zmin

To obtain strong bounds  we need to  nd an orthogonal matrix   that achieves low  Zmax
  In addition 
due to the fact that   can be huge in practice  we need  
type of orthogonal matrix that permits fast matrixvector
products  Naive orthogonal matrices that support fast multiplication such as blockdiagonal matrices often result in
high values of  Zmax
  Motivated by recent
works of structured matrices  Ailon   Chazelle    Yu
et al    we propose to use   special type of orthogonal matrix     HD  where   is   random diagonal matrix
with        Rademacher entries   with probability    
is   WalshHadamard matrix  Horadam    The WalshHadamard matrix of dimension    for      is given by

  and  Zmin

 

 

the recursive formula 

    

                     
            

 

 

Both applying the rotation and inverse rotation take
    log    time and    additional space  with an in 
  and
place algorithm  The next lemma bounds    Zmax
  for this choice of    The lemma is similar to
  Zmin

that of Ailon   Chazelle   and we give the proof in
Appendix   for completeness 
Lemma   Let     HD  where   is   diagonal matrix
with independent Radamacher random variables  For every   and every sequence     

 

  Zmin

 

      Zmax

 

     Xi 

 

    log      

 

Combining the above two lemmas yields the main result 
Theorem   For any       srk HD  protocol satis es 

  srk HD        

  log      

         

 
 

nXi 

 Xi 
   

  Variable length coding
Instead of preprocessing the data via   rotation matrix as
in  srk  in this section we propose to use   variable length
coding strategy to minimize the number of bits 
Consider the stochastic klevel quantization technique   
natural way of transmitting Yi is sending the bin number
for each coordinate  thus the total number of bits the algorithm sends per transmitted coordinate would be ddlog  ke 
This naive implementation is suboptimal  Instead  we propose to further encode the transmitted values using universal compression schemes  Krichevsky   Tro mov   
Falahatgar et al    We  rst encode hr  the number
of times each quantized value   has appeared  and then use
arithmetic or Huffman coding corresponding to the distribution pr   hr
    We denote this scheme by  svk  Since we
quantize vectors the same way in  sk and  svk  the MSE
of  svk is also given by Theorem   We now bound the
communication cost 

 

 

  

      log 

Theorem   Let si     Xi  There exists an implementation of  svk such that   svk       is at most
       
       log        
Proof  As in Lemma       bits are used to transmit the
si   and   min
  Recall that hr is the number of coordinates
that are quantized into bin    and   takes   possible values  Furthermore Pr hr      Thus the number of bits

        

 

 

Distributed Mean Estimation with Limited Communication

necessary to represent the hr   is

 log          

            log 

        

 

 

Once we have compressed the hr    we use arithmetic coding corresponding to the distribution pr   hr   to compress and transmit bin values for each coordinate  The total
number of bits arithmetic coding uses is  MacKay   

 

  Xr 

hr
 

log 

 
hr

   

      si  and    

Let pr   hr                min
 
Pk 
        br      Note that
Xr

pr log 

pr log 

 
pr

     br     

pr

 Xr
 Xr
 Xr
  log Xr

pr log     br     

pr log     br     

pr     br        log   

    yields        
pr     br      

where the  rst inequality follows from the positivity of KLdivergence  Choosing       
  and hence 

 

   

pr log 

      log   

pr   log Xr

Xr
Note that if Yi    belongs to bin         br        
      Recall that hr is the number of coordinates
    
quantized into bin    HencePr hr     br  is the scaled
Xr

hr     br          

normsquare of Yi      

dXj 

   
     

 

dXj 

 Xi                

where the       Yi      Xi    Taking expectations on
both sides and using the fact that the     are independent
zero mean random variables over   range of si       we
get
EXr

hr     br   

  Xi              
        

   

 

dXj 
  Xi 

Using Jensen   inequality yields the result 
Thus if     pd     the communication complexity is
  nd  and the MSE is     

  Communication MSE tradeoff
In the above protocols  all the clients transmit and hence
the communication cost scales linearly with    Instead  we
show that any of the above protocols can be combined by
client sampling to obtain tradeoffs between the MSE and
the communication cost  Note that similar analysis also
holds for sampling the coordinates 
Let   be   protocol where the mean estimate is of the form 

         
 

nXi 

Yi 

 

All three protocols we have discussed are of this form  Let
   be the protocol where each client participates independently with probability    The server estimates    by

           

 

npXi  

Yi 

where Yis are de ned in the previous section and   is the
set of clients that transmitted 
Lemma   For any set of vectors     and protocol   of the
form Equation   its sampled version    satis es

           

     
np

 
             

nXi 
                       

and

 Xi 
   

 

 

 

 

 

 

Proof  The proof of communication cost follows from
Lemma   and the fact that in expectation  np clients transmit  We now bound the MSE  Let   be the set of clients
that transmit  The error           is
      
 
 
  Yi      
 
           
npXi  
   
   
 
Xi      
 
   Yi   Xi 
 Xi  
    
 
npXi  
where the last equality follows by observing that   Yi  
Xi are independent zero mean random variables and hence
for any        Yi Xi    Pi   Xi          The  rst
 
  
      Xi 
nXi 
 
          Xi 
 Xi 

term can be bounded as
 

  

      

 
  

 

Xi

 
 

 

 

 

 

 

 

 

 

 

 

 

Xi      
npXi  
nXi  
nXi 

 
  
     
np  

 
 

  

 

 Xi 
   

Distributed Mean Estimation with Limited Communication

Furthermore  the second term can be bounded as

 

 

 

 

 

   
 

    
 
 Xi  
 
   Yi   Xi 
    
Eh   Yi   Xi 
  
    Xi  
Eh   Yi   Xi 
  Si
nXi 
Eh   Yi   Xi 
  
nXi 
   
  pE 
 
   Yi   Xi 
 
nXi 

 
   

    

 

 

 

 

 

 

 
pE      

where the last equality follows from the assumption that
   mean estimate is of the form       follows from the
fact that   Yi   Xi are independent zero mean random
variables 

Combining the above lemma with Theorem   and choosing     pd     results in the following 
Corollary   For every     nd log  there exists
  protocol   such that             and
            min 
 

    

  Lower bounds
The lower bound relies on the lower bounds on distributed
statistical estimation due to Zhang et al   
Lemma    Zhang et al    Proposition   There exists
such
that if any centralized server wishes to estimate the mean
of the underlying unknown distribution  then for any independent protocol  

  set of distributions Pd supported onh   pd

   pdid

max
pd Pd

  pd     

 

      min 

 

    

where    is the communication cost of the protocol 
 pd  is the mean of pd  and   is   positive constant 
Theorem   Let   be the constant in Lemma   For every
    ndt  and        

     Sd   

 
 

min 

 

    

Proof  Given   samples from the underlying distribution
where each sample belongs to Sd  it is easy to see that

  pd     pd 

 

   

 
 

 

 

 

 

 

 

 

 
 
 
 
 
 

 

uniform
rotation
variable

 

 
 

 

 
 
Bits per dimension

 

 

 

Figure   Distributed mean
estimation on data generated
from   Gaussian distribution 

where  pd  is the empirical mean of the observed samples 
Let Pd be the set of distributions in Lemma   Hence for
any protocol   there exists   distribution pd such that

 

 

 

 

 

 

 

 

   

   

   

 
 

 
 

 

 
 

 
 

  pd     
   pd     
min 
    

      pd     pd 
min 
    
    follows from the fact that                    
            follows from Lemma   and     follows from
the fact that           ndt  and        
Corollary   and Theorem   yield Theorem   We note that
the above lower bound holds only for communication cost
      nd  Extending the results for larger values of  
remains an open problem 
At    rst glance it may appear that combining structured
random matrix and variable length encoding may improve
the result asymptotically  and therefore violates the lower
bound  However  this is not true 
Observe that variable length coding  svk and stochastic rotated quantization  srk use different aspects of the data 
the variable length coding uses the fact that bins with
large values of index   are less frequent  Hence  we can
use fewer bits to encode frequent bins and thus improve
In this scheme binwidth  si      
communication 
is   Xi       Rotated quantization uses the fact
that rotation makes the min and max closer to each other
and hence we can make bins with smaller width  In such
  case  all the bins become more or less equally likely
and hence variable length coding does not help 
In this
scheme binwidth  si    is  Zmax
     
 Xi log   kd  which is much smaller than binwidth
for variable length coding  Hence variable length coding
and random rotation cannot be used simultaneously 

   Zmin

 

  Practical considerations and applications
Based on the theoretical analysis  the variablelength coding method provides the lowest quantization error asymptotically when using   constant number of bits  However in
practice  stochastic rotated quantization may be preferred
due to  hidden  constant factors and the fact that it uses  
 xed amount of bits per dimension  For example  considering quantizing the vector         stochastic rotated

Distributed Mean Estimation with Limited Communication

 
 
 
 
 
 
 
 

 

 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

uniform
rotation
variable

 

 
 
Total bits per dimension

 

 

 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

uniform
rotation
variable

 

 
 
Total bits per dimension

 

 

    MNIST      

    MNIST      

 

 

uniform
rotation
variable

 

 
 
Total bits per dimension

 

 

 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

uniform
rotation
variable

 

 
 
Total bits per dimension

 

 

 
 
 
 

 

 

 

 

 

 

 

 

 

 

 
 
 
 

 

 

 

 

 

 

 

 

uniform
rotation
variable

 
 
Total bits per dimension

 

 

    MNIST      

uniform
rotation
variable

 
 
Total bits per dimension

 

 

 

 

 
 
 
 

 

 

 

 

 

 

 

 

 

 

 
 
 
 

 

 

 

 

 

 

 

 

uniform
rotation
variable

 
 
Total bits per dimension

 

 

    MNIST      

uniform
rotation
variable

 

 

 
 
Total bits per dimension

 

 

    CIFAR      

    CIFAR      

    CIFAR      

    CIFAR      

Figure   Lloyd   algorithm with different types of quantizations 
Here we test two settings    quantization levels and   quantization levels  The xaxis is the averaged number of bits sent for each
data dimension  this scales linearly to the number of iterations 
and the yaxis is the global objective of Lloyd   algorithm 

quantization can use   bit per dimension and gives zero error  whereas the other two protocols do not  To see this 
observe that the naive quantization will quantize   to either
  or   and variable length coding cannot achieve   error
with   bit per dimension due to its constant factors 
We further note that the rotated quantization is preferred
when applied on  unbalanced  data  due to the fact that
the rotation can correct the unbalancedness  We demonstrate this by generating   dataset where the value of the
last feature dimension entry is much larger than others  We
generate   datapoints each with   dimensions  The
 rst   dimensions are generated        from       and
the last dimension is generated from       As shown
in Figure   the rotated stochastic quantization has the best
performance  The improvement is especially signi cant for
low bit rate cases 
We demonstrate two applications in the rest of this section 
The experiments are performed on the MNIST       
and CIFAR        datasets 
Distributed Lloyd   algorithm  In the distributed Lloyd  
 kmeans  algorithm  each client has access to   subset of
data points  In each iteration  the server broadcasts the cluster centers to all the clients  Each client updates the centers
based on its local data  and sends the centers back to the
server  The server then updates the centers by computing
the weighted average of the centers sent from all clients  In

Figure   Power iteration with different types of quantizations 
Here we test two settings    quantization levels and   quantization levels  The xaxis is the averaged number of bits sent for
each data dimension  this scales linearly to the number of iterations  and the yaxis is the   distance between the computed
eigenvector and the groundtruth eigenvector 

the quantized setting  the client compresses the new centers
before sending to the server  This saves the uplink communication cost  which is often the bottleneck of distributed
learning  We set both the number of centers and number
of clients to   Figure   shows the result 
Distributed power iteration  Power iteration is   widely
used method to compute the top eigenvector of   matrix 
In the distributed setting  each client has access to   subset
of data 
In each iteration  the server broadcasts the current estimate of the eigenvector to all clients  Each client
then updates the eigenvector based on one power iteration
on its local data  and sends the updated eigenvector back
to the server  The server updates the eigenvector by computing the weighted average of the eigenvectors sent by all
clients  Similar to the above distributed Lloyd   algorithm 
in the quantized setting  the client compresses the estimated
eigenvector before sending to the server  Figure   shows
the result  The dataset is distributed over   clients 
For both of these applications  variablelength coding
achieves the lowest quantization error in most of the settings  Furthermore  for lowbit rate  stochastic rotated
quantization is competitive with variablelength coding 

 In this setting  the downlink is   broadcast  and therefore its
cost can be reduced by   factor of      log    without quantization  where   is the number of clients 

Distributed Mean Estimation with Limited Communication

Acknowledgments
We thank Jayadev Acharya  Keith Bonawitz  Dan
HoltmannRice  Jakub Konecny  Tengyu Ma  and Xiang
Wu for helpful comments and discussions 

References
Ailon  Nir and Chazelle  Bernard  Approximate nearest
neighbors and the fast JohnsonLindenstrauss transform 
In STOC   

Alistarh  Dan  Li 

Jerry  Tomioka  Ryota  and Vojnovic  Milan 
QSGD  Randomized quantization
for communicationoptimal stochastic gradient descent 
arXiv   

Arjevani  Yossi and Shamir  Ohad  Communication complexity of distributed convex learning and optimization 
In NIPS   

Balcan  MariaFlorina  Blum  Avrim  Fine  Shai  and Mansour  Yishay  Distributed learning  communication complexity and privacy  In COLT   

Bonawitz  Keith  Ivanov  Vladimir  Kreuter  Ben  Marcedone  Antonio  McMahan    Brendan  Patel  Sarvar  Ramage  Daniel  Segal  Aaron  and Seth  Karn  Practical
secure aggregation for federated learning on userheld
data  arXiv   

Braverman  Mark  Garg  Ankit  Ma  Tengyu  Nguyen 
Huy    and Woodruff  David    Communication lower
bounds for statistical estimation problems via   distributed data processing inequality  In STOC   

Chen  Jiecao  Sun  He  Woodruff  David  and Zhang  Qin 
Communicationoptimal distributed clustering  In NIPS 
 

Dasgupta  Sanjoy and Gupta  Anupam  An elementary
proof of   theorem of johnson and lindenstrauss  Random Structures   Algorithms     

Dean  Jeffrey  Corrado  Greg  Monga  Rajat  Chen  Kai 
Devin  Matthieu  Mao  Mark  Senior  Andrew  Tucker 
Paul  Yang  Ke  Le  Quoc    et al  Large scale distributed
deep networks  In NIPS   

Efron  Bradley and Stein  Charles  The jackknife estimate
of variance  The Annals of Statistics  pp     

Garg  Ankit  Ma  Tengyu  and Nguyen  Huy    On communication cost of distributed statistical estimation and
dimensionality  In NIPS   

Horadam  Kathy    Hadamard matrices and their applica 

tions  Princeton university press   

Kone cn    Jakub and Richt arik  Peter  Randomized distributed mean estimation  Accuracy vs communication 
arXiv   

Kone cn    Jakub  McMahan    Brendan  Yu  Felix   
Richt arik  Peter  Suresh  Ananda Theertha  and Bacon 
Dave  Federated learning  Strategies for improving communication ef ciency  arXiv   

Krichevsky    and Tro mov     The performance of universal encoding  IEEE Transactions on Information Theory 
   

Lloyd  Stuart  Least squares quantization in PCM  IEEE
Transactions on Information Theory   
 

MacKay  David JC 

Information theory  inference and
learning algorithms  Cambridge university press   

McDonald  Ryan  Hall  Keith  and Mann  Gideon  Distributed training strategies for the structured perceptron 
In HLT   

McMahan     Brendan  Moore  Eider  Ramage  Daniel 
and   Arcas  Blaise Aguera  Federated learning of deep
networks using model averaging 
arXiv 
 

Povey  Daniel  Zhang  Xiaohui  and Khudanpur  Sanjeev 
Parallel training of deep neural networks with natural
gradient and parameter averaging  arXiv preprint   

Tsitsiklis  John   and Luo  ZhiQuan  Communication
complexity of convex optimization  Journal of Complexity     

Yu  Felix    Suresh  Ananda Theertha  Choromanski 
Krzysztof  HoltmannRice  Daniel  and Kumar  Sanjiv 
Orthogonal random features  In NIPS   

Zhang  Yuchen  Duchi  John  Jordan  Michael    and Wainwright  Martin   
Informationtheoretic lower bounds
for distributed statistical estimation with communication
constraints  In NIPS   

Falahatgar  Moein 

and

Jafarpour  Ashkan  Orlitsky 
Alon 
Suresh 
Ananda Theertha  Universal compression of powerlaw
distributions  In ISIT   

Pichapati  Venkatadheeraj 

