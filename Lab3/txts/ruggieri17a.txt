Enumerating Distinct Decision Trees

Salvatore Ruggieri  

Abstract

The search space for the feature selection problem in decision tree learning is the lattice of subsets of the available features  We provide an
exact enumeration procedure of the subsets that
lead to all and only the distinct decision trees 
The procedure can be adopted to prune the search
space of complete and heuristics search methods
in wrapper models for feature selection  Based
on this  we design   computational optimization
of the sequential backward elimination heuristics
with   performance improvement of up to  

  Introduction
Feature selection in machine learning classi cation is an
extremely relevant and widely studied topic  Wrapper
models for feature selection have shown superior performance in many contexts  Doak    They explore the
lattice of feature subsets  For   given subset    classi er
is built over the features in the subset and an optimality
condition is tested  However  complete search of the lattice of feature subsets is know to be NP hard  Amaldi  
Kann    For this reason  heuristics searches are typically adopted in practice  Nevertheless  complete strategies
have not to be exhaustive in order to  nd an optimal subset 
In particular  feature subsets that lead to duplicate decision
trees can be pruned from the search space  Such   pruning
would be useful not only for complete searches  but also
in the case of heuristics searches    na ve approach that
stores all distinct trees found during the search is  however 
unfeasible  since there may be an exponential number of
such trees  Our contribution is   nontrivial enumeration
algorithm of all distinct decision trees built using subsets
of the available features  The procedure requires the storage of   linear number of decision trees in the worst case 
The starting point is   recursive procedure for the visit of
the lattice of all subsets of features  The key idea is that

 University of Pisa and ISTICNR  Pisa  Italy  Correspon 

dence to  Salvatore Ruggieri  ruggieri di unipi it 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  subset of features is denoted by the union       of two
sets  where elements in   must necessarily be used as split
attributes  and elements in   may be used or not  Pruning
of the search space is driven by the observation that if   feature       is not used as split attribute by   decision tree
built on        then the feature subset             leads
to the same decision tree  Duplicate decision trees that still
pass such    necessary but not suf cient  pruning condition can be identi ed through   test on whether they use
all attributes in    Coupled with the tremendous computational optimization of decision tree induction algorithms 
our approach makes it possible to increase the limit of practical applicability of theoretically hard complete searches 
It also allows to optimize the sequential backward elimination  SBE  search heuristics when speci cally designed for
decision tree learning  with   speedup of up to   compared to   blackbox approach  This paper is organized as
follows  First  we recall related work in Section   The
visit of the lattice of feature subsets is based on   generalization of binary counting enumeration devised in Sect   
Next  Sect    introduces   procedure for the enumeration
of distinct decision trees as   pruning of the feature subset
lattice    whitebox optimization of SBE is described in
Sect    Experimental results are shown in Sect    Finally 
we summarize the contribution of the paper 

  Related Work
 Blum   Langley    Dash   Liu    Guyon   Elisseeff    Liu   Yu    Bol onCanedo et al   
provide   categorization of approaches of feature subset
selection along the orthogonal axes of the evaluation criteria  the search strategies  and the machine learning tasks 
Common evaluation criteria include  lter models  embedded and wrappers approaches  Filters are preprocessing
algorithms that select   subset of features by looking at the
data distribution  independently from the induction algorithm  Cover    Embedded approaches perform feature selection in the process of training and are speci   to
the learning algorithm  Wrappers approaches optimize induction algorithm performances as part of feature selection
 Kohavi   John    In particular  training data is split
into   building set and   search set  and the space of feature
subsets is explored  For each feature subset considered 
the building set is used to train   classi er  which is then

Enumerating Distinct Decision Trees

evaluated on the search set  For   dataset with   features 
the search space consists of    possible subsets  Search
space exploration strategies include  see  Doak   
hillclimbing search  forward selection  backward elimination  bidirectional selection  beam search  genetic search 
random search  random start hillclimbing  simulated annealing  Las Vegas  and complete search  The aim of complete search is to  nd an optimal feature subset according
to an evaluation metric  Typical objectives include minimizing the size of the feature subset provided that the classi er built from it has   minimal accuracy  dimensionality reduction  or minimizing the misclassi cation error of
the classi er  performance maximization  Finally  feature
subset selection has been considered both for classi cation
and clustering tasks  Machine learning models and algorithms can be either treated as blackboxes or  instead  feature selection methods can be speci   of the model and or
algorithm at hand  whitebox  Whitebox approaches are
less general  but can exploit assumptions on the model or
algorithm to direct and speed up the feature subset search 
This paper falls in the category of complete search using  
whitebox wrapper model  tailored to decision tree classi 
 ers  for performance maximization    feature subset is
optimal if it leads to   decision tree with minimal error
on the search set  Only complete space exploration can
provide the guarantee of  nding optimal subsets  whilst
heuristics approaches can lead to results arbitrarily worse
than the optimal  Murthy    Complete search is know
to be NP hard  Amaldi   Kann    However  complete strategies do not need to be exhaustive in order to  nd
an optimal subset  For instance   lter models can rely on
monotonic evaluation metrics to support Branch and Bound
search  Liu et al    Regarding wrapper approaches 
evaluation metrics such as misclassi cation error  lack of
the monotonicity property that would allow for pruning the
search space in   complete search  Approximate Monotonicity with Branch and Bound  AMB     Foroutan  
Sklansky    tries and tackles this limitation  but it provides no formal guarantee that an optimal feature subset
is found  Another form of search space pruning in wrapper approaches for decision trees has been pointed out by
 Caruana   Freitag    which examines  ve hillclimbing procedures  They adopt   caching approach to prevent
rebuilding duplicate decision trees  The basic property
they observe is reported in   generalized form in this paper as Remark   While caching improves ef ciency of
  limited search  in the case of   complete search  it requires an exponential number of decision trees to be stored
in cache  while our approach requires   linear number of
them  We will also observe that Remark   may still leave
duplicate trees in the search space       it is   necessary but
not suf cient condition for enumerating distinct decision
trees  while we will provide an exact enumeration 

 

  Enumerating Subsets
Let                 an  be   set of   elements  with      
The powerset of   is the set of its subsets  Pow      
   cid      cid       There are    subsets of    and  for

 cid  subsets of size    Fig     top 

           there are cid  

shows the lattice         set inclusion  of subsets for      
The order of visit of the lattice  or  equivalently  the order
of enumeration of elements in Pow     can be of primary
importance for problems that explore the lattice as search
space  Wellknown algorithms for subset generation produce lexicographic ordering  Grey code ordering  and binary counting ordering  Skiena    Binary counting
maps each subset into   binary number with   bits by setting the ith bit to   iff ai belongs to the subset  and generating subsets by counting from   to        Subsets for
      are generated as                     
                        In this section  we introduce   recursive algorithm for   generalization of reverse
binary counting  namely  counting from        down to  
that will be the building block for solving the problems of
generating distinct decision trees  Let us start by introducing the notation                     to denote sets
obtained by the union of   with elements of     In particular 

    Pow          cid         cid 

consists of the subsets of       which necessarily include
   This generalization of powersets will be crucial later
on when we have to distinguish predictive attributes that
must be used in   decision tree from those that may be
used    key observation of binary counting is that subsets
can be partitioned between those including the value    and
those not including it  For example  Pow            
      Pow              Pow        We can
iterate the observation for the leftmost occurrence of    and
obtain 

Pow                      Pow      
      Pow           Pow       
By iterating again for the leftmost occurrence of    we
conclude 

Pow                         Pow    

         Pow           Pow      
    Pow       

Since     Pow         the leftmost set in the above
union is          
In general  the following recurrence relation holds 
Lemma   Let                 an  We have 
    Pow                

                 ai    Pow  ai          an 

 cid 

    

Enumerating Distinct Decision Trees

      

   

         

      

   

 

      ow         

      

   

           ow 

        ow   

      ow      

        ow 

        ow 

      ow   

      ow 

Figure   Lattice of subsets and reverse binary counting 

This result can be readily translated into   procedure
subset       for the enumeration of elements in    
  ow    In particular  since       ow        ow   
subset     generates all subsets of    The procedure
is shown as Alg    The search space of the procedure is
the tree of the recursive calls of the procedure  The search
space for       is reported in Fig     bottom  According
to line   of Alg    the subset outputted at   node labelled
as       ow    is        Hence  the output is the reverse counting ordering                         
                     Two key properties of the
recursive procedure Alg    will be relevant for the rest of
the paper 
Remark     set   cid cid  generated at   nonroot node of the
search tree of Alg    is obtained by removing an element
from the set   cid  generated at the father node         cid cid   
  cid        for some       cid 
The invariant    cid      cid            readily holds for the
loop at lines   of Alg    Before the recursive call at line
  an element is removed from   cid  hence the set   cid      cid 
outputted at   child node has one element less than the set
      outputted at its father node 
Remark   The selection order of ai     at line   of
Alg    is irrelevant 

The procedure does not rely on any speci   order of selecting members of    which is   form of don   care nondeterminism in the visit of the lattice  Any choice generates
all elements in       ow   
In case of an apriori positional order of attributes  namely line   is  for ai    
order by   desc do  Alg    produces precisely the reversed binary counting order  However  if the selection
order varies from one recursive call to another  then the
output is still an enumeration of subsets 

Algorithm   subset       enumerates       ow   
  output      
    cid         
    cid     
  for ai     do
    cid      cid     ai 
subset   cid    cid 
 
  cid      cid     ai 
 
  end for

  Generating All Distinct Decision Trees
We build on the subset generation procedure to devise an
algorithm for the enumeration of all distinct decision trees
built on subsets of the predictive features 

  On TopDown Decision Tree Induction

Let us  rst introduce some notation and assumptions  Let
                an  be the set of predictive features  We
write     DT     to denote the decision tree built from
predictive features   on    xed training set  Throughout
the paper  we make the following assumption on the node
split criterion in topdown decision tree induction with univariate split conditions 

Assumption   Let     DT       split attribute at  
decision node of   is chosen as argmax   Sf        where
    is   quality measure and   are the cases of the training
set reaching the node 

Our results will hold for any quality measure     as far as
the split attribute is chosen as the one that maximizes    
Examples of quality measures used in this way include Information Gain  IG  Gain Ratio   GR  and the Gini index  used in     Quinlan    and CART algorithms
 Breiman et al      second assumption regards the
stopping criterion in topdown decision tree construction 
Let stop       be the boolean result of the stopping criterion at   node with cases   and predictive features   
Assumption   If stop         true then stop   cid      
true for every   cid      

The assumption states that either    the stopping criterion

 Gain Ratio normalizes Information Gain over the Split Information  SI  of an attribute       GR   IG SI  This de nition
does not work well for attributes which are  almost  constants
over the cases         when SI      Quinlan    proposed
the heuristics of restricting the evaluation of GR only to attributes
with above average IG  The heuristics is implemented in the   
system  Quinlan    It clearly breaks Assumption   making the selection of the split attribute dependent on the set    An
heuristics that satis es Assumption   consists of restricting the
evaluation of GR only for attributes with IG higher than   minimum threshold 

Enumerating Distinct Decision Trees

output  

Algorithm   DTdistinct       enumerates distinct decision trees necessarily using   and possibly using   as split
features
  build tree     DT        
      unused features in  
  if           then
 
  end if
    cid                
    cid         
  for ai         order by frontier     ai  do
    cid      cid     ai 
DTdistinct   cid    cid 
 
  cid      cid     ai 
 
  end for

does not depend on    or  if it does  then   stopping is
monotonic with regard to the set of predictive features   
is   fairly general assumption  since typical stopping criteria are based on the size of cases   at   node and or on
the purity of the class attribute in      applies to criteria
which require minimum quality of features for splitting  
node       the    criterion of stopping if IG of all features is below   minimum threshold satis es the assumption  The following remark  which is part of the decision
tree folklore  see       Caruana   Freitag    states  
useful consequence of Assumptions   and  

Remark   Let     DT     and    be the set of split
features used in     For every   cid  such that       cid        we
have DT    cid       

If the decision tree   built from   uses only features from
    then argmax   Sf          argmax      Sf        at any
decision node of     Hence  any unused attribute in       
will not change the result of maximizing the quality measure and then  by Assumption   the split attribute at  
decision node  Moreover  by Assumption     leaf node
in   will remain   leaf node for any   cid      

  Enumerating Distinct Decision Trees
Let                  ak  be the set of features used in split
nodes of the decision tree     DT     built from   
and           ak          an  the set of features never
selected as split features  By Remark   the decision
tree   is equal to the one built starting from features
           ak plus any subset of ak          an  In symbols  all
the decision trees for attribute subsets in             ak   
  ow ak          an  do coincide with     We will use
this observation to remove from the recurrence relation of
Lemma   those sets in     Pow     which lead to duplicate decision trees  Formally  when searching for feature
subsets that lead to distinct decision trees  the recurrence

DT          

DT          DT    

DT       

DT       

DT    

DT       DT  

DT    

DT  

DT          

DT       

DT          DT    

DT  

DT       

DT    

Figure   Search spaces of Alg    for different selection orders 

relation can be modi ed as 
    Pow                

 cid 

                 ai    Pow  ai          an 

since the missing union 

    

 cid 

                 ai    Pow  ai          an 

      

contains sets of features leading to the same decision tree as
DT       The simpli ed recurrence relation prunes from
the the search space features subsets that lead to duplicated
decision trees  However  we will show in Ex    that such  
pruning alone is not suf cient to generate distinct decision
trees only       duplicate trees may still exists 
Alg    provides an enumeration of all and only the distinct
decision trees  It builds on the generalized subset generation procedure  Line   constructs   tree   from features
       Feature in the set   of unused features in   are not
iterated over in the loop at lines   since those iterations
would yield the same tree as     This is formally justi ed
by the modi ed recurrence relation above  The tree   is
outputted at line   only if           namely features
required to be used          are actually used in decision
splits  This prevents from outputting more than once   decision tree that can be obtained from multiple paths of the
search tree 
Example   Let               Assume that    has
no discriminatory power unless data has been split by   
More formally  DT    cid    DT    cid        if     cid    cid 
The visit of feature subsets of Fig     bottom  gives rise
to the trees built by DTdistinct     as shown in Fig   
 top  For instance  the subset        visited at the node

Enumerating Distinct Decision Trees

labelled            in Fig     bottom  produces the decision tree DT        By assumption  such   tree is
equal to DT     which is   duplicate tree produced in
another node   underlined in Fig     top    corresponding
to the feature set visited at the node labelled        
Another example regarding DT       DT   is shown
in Fig     top  together with its underlined duplicate tree 
Unique trees for two or more duplicates can be characterized by the fact that features appearing to the left of   must
necessarily be used as split features by the constructed decision tree  In the two previous example cases  the node
underlined will output their decision trees  while the other
duplicates will not pass the test at line   of Alg   

Remark   states that the selection order in the recursive
calls of subset  is not relevant  Alg    adopts   speci  
order that  while not affecting the result  any order would
produce the enumeration of distinct decision trees  impacts
on the effectiveness of pruning the search space  We de ne
the frontier frontier     ai  of an attribute ai in   decision
tree   as the sum of the number of cases of the training set
that reach   node in   where ai is the split attribute  The
smaller the frontier is the smaller is the impact of removing
subtrees of   rooted at nodes with ai as split attribute 

Example    Ctd  The order of selection of ai   in the
visit of Fig     top  is by descending      This order does
not take into account the fact that    has more discriminatory power than         its presence gives rise to more
distinct decision trees  As   consequence  it would be better to have    removed in the rightmost child of   node 
which has the largest search subspace  and hence the best
possibilities of pruning  The ordering based on ascending frontier estimates the discriminatory power of ai by the
amount of cases in the training set discriminated by splits
using ai  In our example  such an order would likely be   
   and    The search space of DTdistinct     is then reported in Fig     bottom  Notice that there is no duplicate
tree here  Also notice that the size of the search space is
smaller than in the previous example  In fact  the node labelled as DT          DT     corresponds to the
exploration of            The    attribute is unused
and hence is pruned at line   of Alg    The subspace to
be searched consists then of only the subset of     not all
subsets of       

The following nontrivial result holds 

Theorem   DTdistinct       outputs the distinct decision trees built on sets of features in       ow   

Proof  The search space of DTdistinct  is   pruning of
the search space of subset  Every tree built at   node
and outputted is then constructed from   subset in    
  ow    By Remark   the order of selection of ai  

      at line   is irrelevant  since any order will lead to the
same space       ow   
Let us  rst show that decision trees in output are all
distinct  The key observation here is that  by line  
all features in   are used as split features in the outputted decision tree  The proof proceed by induction
If         then there is at most
on the size of   
one decision tree in output  hence the conclusion  Assume now         and let                 an  By
Lemma   any two recursive calls at line   have parameters                  ai ai          an  and     
            aj aj          an  for some        By inductive hypothesis  ai is missing as   predictive attribute
in the trees in output from the  rst call  while it must be
  split attribute in the trees in output by the second call 
Hence  the trees in output from recursive calls are all distinct among them  Moreover  they are all different from    
if it is outputted at line   In fact    has             split
features  whilst recursive calls construct decision trees with
at most                 features 
Let us now show that trees pruned at line   or at line  
are already outputted elsewhere  which implies that every
distinct decision tree is outputted at least once  First  by
Remark   the trees of the pruned iterations       at line
  are the same of the tree of   at line   Second  if the tree  
is not outputted at line   because        cid    we have that
it is outputted at another node of the search tree  The proof
is by induction on     For         it is trivial  Let    
            an  with       and let   cid                ai  be
such that ai     and   cid       There is   sibling node in
the search tree corresponding to   call with parameters   cid 
and   cid     ai          an       By inductive hypothesis on
   cid        the distinct decision trees with features in   cid   
  ow   cid  are all outputted  including   because   has split
features in          ai  which belongs to   cid      ow   cid 

 

Let us now point out some properties of DTdistinct 
Property   linear space complexity  Alg    is computationally linear  per number of trees built  in space in the number
of predictive features  An exhaustive search would instead
keep in memory the distinct decision trees built in order to
check whether   new decision trees is   duplicate  Similarly will do approaches based on complete search with
some forms of caching of duplicates  Caruana   Freitag 
  Those approaches would require exponential space 
as shown in the next example 

Example   Let us consider
the wellknown Adult
dataset   Lichman    consisting of   cases   
predictive features  and   binary class attribute  Fig   

 See Sect    for the experimental settings 

Enumerating Distinct Decision Trees

Figure   Left  distribution of distinct decision trees  Center  ratio built distinct decision trees  Right  elapsed times 

 left  shows  for the IG split criterion  the distribution of
distinct decision trees        the size of attribute subset  The
distributions are plotted for various values of the stopping
parameter   which halts tree construction if the number of
cases of the training set reaching the current node is lower
than   minimum threshold    formally  stop       is true
iff         

Property   reduced overhead  Our procedure may construct duplicate decision trees at line   which  however 
are not outputted thanks to the test at line   We measure
such an overhead of Alg    as the ratio of all decision trees
constructed at line   over the number of distinct decision
trees  An ideal ratio of   means that no duplicate decision
tree is constructed at all  The overhead can be controlled
by the attribute selection ordering at line  

Example    Ctd  Fig     center  shows the overhead at
the variation of   for three possible orderings of selection
at line   of Alg    One is the the ordering stated by DTdistinct  based on ascending frontier  The second one is
the reversed order  namely descending frontier  The third
one is based on assigning    xed index   to features ai   
and then ordering over    The DTdistinct  ordering is impressively effective  with an overhead close to          the
search space is precisely the set of distinct decision trees 

Fig     center  also reports the ratio of the number of trees
in an exhaustive search    for   features  over the number
of distinct trees  Smaller     lead to   smaller ratio  Thus 
for small   values  pruning duplicate trees does not guarantee alone an enumeration more ef cient than exhaustive
search  The next property will help 
Property   featureincremental tree building  The construction of each single decision tree at line   of Alg    can
be speed up by Remark   The decision tree    cid  at   child
node of the search tree differs from the decision tree   built
at the father node by one missing attribute  The construction of    cid  can then bene   from this observation  We  rst
recursively clone   and then rebuild only subtrees rooted
at node where the split attribute is ai 

Example    Ctd  Fig     right  contrasts the elapsed
times of exhaustive search and DTdistinct  For smaller
values of    there is an exponential number of duplicated
decision trees  but the running time of DTdistinct  is still
much better than the exhaustive search due to the incremental building of decision trees 

  PSBE    WhiteBox Optimization of SBE
In wrapper models  the training set is divided into   building set and   search set    decision tree is built on the
building set and its the accuracy is evaluated on the search
set  Our enumeration procedure DTdistinct  has   direct
application  which consists of running   complete wrapper search looking for the feature subset that leads to the
most accurate decision tree on the search set  On the practical side  however  using DTdistinct  to look for the optimal feature subset is computationally feasible only when
the number of predictive features is moderate  Moreover 
optimality on the search set may be obtained at the cost
of over tting  Doak    Reunanen    and instability  Nogueira   Brown    The ideas underlying our
approach  however  can impact also on the ef ciency of
heuristics searches 
In particular  we consider here the widely used sequential
backward elimination  SBE  heuristics  SBE starts building   decision tree   using the set   of all features  We call
  the top tree  For every ai        decision tree Ti is built
using features in      ai  If no Ti   has   smaller error
on the search set than     the algorithm stops returning   as
the subset of selected features  Otherwise  the procedure is
repeated removing ak from    where Tk is the tree with the
smallest error  In summary  features are eliminated one at
  time in   greedy way 
SBE is   blackbox approach  The procedure applies to any
type of classi er  not only to decision trees    whitebox
optimization can be devised for decision tree classi ers that
satisfy the assumptions of Section   The optimization
relies on Remark   Let   be the set of features not used
in the current decision tree     For ai      it turns out that

                                                No of distinct treesSubset sizeAdult  IGbinomialm                                       Ratio built distinct treesmAdult  IGDTdistinctfixedreverseexhaustive                            Elapsed time  secs mAdult  IGexhaustiveDTdistinctEnumerating Distinct Decision Trees

Table   Experimental results  IG and   

dataset

elapsed time  secs 

name
Adult
Letter
Hypo Thyroid
Ionosphere
Soybean
Anneal
Sonar
Coil 
Clean 
Clean 
Madelon
Gisette
  Mutants

inst 
 
 
 
 
 
 
 
 
 
 
 
 
 

feat 
 
 
 
 
 
 
 
 
 
 
 
 
 

PSBE
 
 
 
 
 
 
 
 
 
 
 
 
 

SBE
 
 
 
 
 
 
 
 
 
 
  
  
  

ratio
 
 
 
 
 
 
 
 
 
 
 
 
 

crossvalidation error  
top
     
     
     
     
     
     
     
     
     
     
     
     
     

   SBE
     
     
     
     
     
     
     
     
     
     
     
     
     

optimal
     
     
     
     
     
     
     
 
 
 
 
 
 

Ti       Thus  only trees built from      ai  for ai  cid   
need to be considered for backward elimination  This saves
the construction of       decision trees at each step of the
procedure  We call this optimization PSBE  Pruned SBE 

  Experiments
  Datasets and Experimental Settings

Table   reports the number of instances and of features
for small and large standard benchmarks datasets publicly
available from  Lichman    Following  Reunanen 
  we adopt  repeated strati ed  fold cross validation in experimenting with wrapper models  For each holdout fold  feature selection is performed by splitting the  
fold training set into   building set and   search set
using strati ed random sampling  Information Gain  IG 
is used as quality measure in node splitting  No form of
tree simpli cation       errorbased pruning  is used  The
search error is the average misclassi cation error on the
search set  The crossvalidation error is the average misclassi cation error on the holdout folds for the tree built on
the training set using the selected feature subset  Misclassi cation errors are computed using the     distribution
imputation method  SaarTsechansky   Provost   
All procedures described in this paper were implemented
by extending the YaDT system  Ruggieri      Aldinucci et al    It is   stateof theart mainmemory
   implementation of    with many algorithmic and
data structure optimizations as well as with multicore tree
building  The extended YaDT version is publicly downloadable from  http pages di unipi it ruggieri  Test were
performed on   commodity PC with Intel   cores   
  GHz    Gb RAM  and Windows   OS 

  How Fast is DTdistinct 

Or  in other words  how much our pruning approach will
make   complete search feasible in practice  Fig    shows
the ratio of the number of built trees over the number of distinct trees  left  and the total elapsed time of DTdistinct 
 center  for low to medium dimensionality datasets   actually  those for which DTdistinct  terminates within   timeout of     The ratio ranges from   to   which show that
the selection order based on ascending frontier size  line  
of Alg    is effective in practice  The total elapsed time of
the enumeration procedure  shown in Fig     center  grows
exponentially with the inverse of    the stopping parameter  This is intuitive  since lower     lead to higher numbers of distinct decision trees  and  as shown in Fig     left 
such numbers approach      where   is the number of features  However  the total elapsed time of DTdistinct  remains within   practically admissible bound for datasets
with   moderate number of features  Consider for instance 
the Anneal dataset  An exhaustive enumeration would be
impossible  since it consists of building        trees 
DTdistinct  runs in less than   seconds for      
and less than   seconds for       This is the coupled result of three factors  the pruning approach of Alg   
the featureincremental tree building optimization  and the
tremendous ef ciency of the stateof theart tree induction
implementations 

  PSBE vs SBE

Table   reports elapsed times that allows for comparing the
ef ciency of PSBE vs SBE  The   parameter is set to the
small value   for all datasets  The ratio of elapsed times
shows   speedup of up to   of PSBE vs SBE  The improvement increases with the number of features  For high 

Enumerating Distinct Decision Trees

Figure   Left  ratio built distinct trees  Center  elapsed times of DTdistinct  Right  search errors 

Figure   Crossvalidation errors 

dimensional datasets  the blackbox SBE does not even
terminate within   timeout of     The whitebox PSBE 
instead  runs in about   seconds for the highly dimensional dataset   Mutants  This is   relevant result for machine learning practitioners  extending the applicability of
the SBE heuristics 

  Complete Search or Heuristics 

Fig     right  shows the average search error over the Adult
dataset of the decision trees constructed on    all features
 top    the features selected by SBE  same as PSBE 
and   the features selected by DTdistinct  namely those
with the lowest error on the search set  hence  the name
optimal  Obviously  SBE is better than top  and optimal
is better than SBE  Interestingly  SBE is close to the optimal search error  in particular for small   parameter  Does
this generalize to unknown cases  Fig    reports the crossvalidation errors over the Adult  Ionosphere and Anneal
datasets  For Adult  optimal is better than SBE  which in
turn is better than top  For Ionosphere  instead  the optimal
tree has the worst performance  the top tree is the best for
almost all      and SBE is the best for small      For Anneal  SBE is the worst  and the top tree is better than the
optimal for large      Table   reports the crossvalidation
errors for       for all datasets  PSBE  or equivalently
SBE as they select the same subset of features  wins over
top in most cases  But there is no clear evidence of the superiority of optimal over SBE  This is consistent with the

conclusions of  Doak    Reunanen    that simple
sequential elimination exhibits better generalization performances than more exhaustive searches when considering
error on an unseen set of instances 

  Conclusions
We have introduced an original pruning algorithm of the
search space of feature subsets which allows for enumerating all and only the distinct decision trees  On the theoretical side  this makes it possible to run   complete wrapper procedure for moderate dimensionality datasets  This
will allow  for instance  for   deeper investigation of old
and new search heuristics by comparing their performances
with those of   complete search  On the practical side  ideas
and results of the paper have been applied to improve the
computational ef ciency of the SBE heuristics 
As future work  we will investigate the extension of the
proposed approach in presence of decision tree simpli cation and for ensembles of decision trees  bagging  random
forests  Moreover  we will consider the related problem of
 nding an optimal subset of features  which in the present
paper is tackled by simply enumerating all distinct decision
trees  Actually  there is no need to explore   subspace of
 distinct  decision trees  if   lower bound for the accuracy
of any tree in the subspace can be computed and such  
lower bound is higher than the best error found so far  This
idea would build upon the enumeration procedure DTdistinct  as   further pruning condition of the search space 

                                    Ratio built distinct treesmIGLetterHypoIonosphereSoybeanAnnealSonar                                Elapsed time  secs mIGLetterHypoIonosphereSoybeanAnnealSonar                                      Error on search set  mAdult  IGtop treeoptimalSBE                                    Crossvalidation error  mAdult  IGtop treeoptimalSBE                                Crossvalidation error  mIonosphere  IGtop treeoptimalSBE                              Crossvalidation error  mAnneal  IGtop treeoptimalSBEEnumerating Distinct Decision Trees

Liu     Motoda     and Dash       monotonic measure
for optimal feature selection  In Proc  of the European
Conference on Machine Learning  ECML   volume
  of Lecture Notes in Computer Science  pp   
  Springer   

Murthy        Automatic construction of decision trees
from data    multidisciplinary survey  Data Mining
and Knowledge Discovery     

Nogueira     and Brown     Measuring the stability of feature selection  In Proc  of Machine Learning and Knowledge Discovery in Databases  ECMLPKDD   Part
II  volume   of LNCS  pp     

Quinlan        Induction of decision trees  Machine Learn 

ing     

Quinlan           Programs for Machine Learning 

Morgan Kaufmann  San Mateo  CA   

Reunanen     Over tting in making comparisons between
variable selection methods  Journal of Machine Learning Research     

Ruggieri     Ef cient    IEEE Transactions on Knowl 

edge and Data Engineering     

Ruggieri     YaDT  Yet another Decision tree Builder  In
Proc  of Int  Conf  on Tools with Arti cial Intelligence
 ICTAI   pp    IEEE   

SaarTsechansky     and Provost     Handling missing values when applying classi cation models  Journal of Machine Learning Research     

Skiena        The Algorithm Design Manual  Springer   

edition   

References
Aldinucci     Ruggieri     and Torquati     Decision tree
building on multicore using FastFlow  Concurrency and
Computation  Practice and Experience   
 

Amaldi     and Kann     On the approximation of minimizing non zero variables or unsatis ed relations in linear
systems  Theoretical Computer Science   
 

Blum     and Langley     Selection of relevant features
and examples in machine learning  Artif  Intell   
   

Bol onCanedo       anchezMaro no     and AlonsoBetanzos       review of feature selection methods on
synthetic data  Knowledge and Information Systems   
   

Breiman     Friedman     Olshen     and Stone     Classi cation and Regression Trees  Wadsworth Publishing
Company   

Caruana     and Freitag     Greedy attribute selection 
In Proc  of the Int  Conf  on Machine Learning  ICML
  pp    Morgan Kaufmann   

Cover       On the possible ordering on the measurement
selection problem  Trans  Systems  Man  and Cybernetics     

Dash     and Liu     Feature selection for classi cation 

Intell  Data Anal     

Doak     An evaluation of feature selection methodsand
their application to computer security  Technical Report
CSE  University of California Davis   

Foroutan     and Sklansky     Feature selection for automatic classi cation of nongaussian data  Trans  Systems  Man  and Cybernetics     

Guyon     and Elisseeff     An introduction to variable
and feature selection  Journal of Machine Learning Research     

Kohavi     and John        Wrappers for feature subset

selection  Artif  Intell     

Lichman     UCI machine learning repository   

http archive ics uci edu ml 

Liu     and Yu     Toward integrating feature selection
algorithms for classi cation and clustering  IEEE Trans 
Knowl  Data Eng     

