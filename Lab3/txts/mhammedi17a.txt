Ef cient Orthogonal Parametrisation of Recurrent Neural Networks

Using Householder Re ections

Zakaria Mhammedi     Andrew Hellicar   Ashfaqur Rahman   James Bailey  

Abstract

The problem of learning longterm dependencies
in sequences using Recurrent Neural Networks
 RNNs  is still   major challenge  Recent methods have been suggested to solve this problem
by constraining the transition matrix to be unitary during training which ensures that its norm
is equal to one and prevents exploding gradients 
These methods either have limited expressiveness or scale poorly with the size of the network
when compared with the simple RNN case  especially when using stochastic gradient descent
with   small minibatch size  Our contributions
are as follows  we  rst show that constraining the
transition matrix to be unitary is   special case of
an orthogonal constraint  Then we present   new
parametrisation of the transition matrix which allows ef cient training of an RNN while ensuring
that the matrix is always orthogonal  Our results
show that the orthogonal constraint on the transition matrix applied through our parametrisation
gives similar bene ts to the unitary constraint 
without the time complexity limitations 

  Introduction
Recurrent Neural Networks  RNNs  have been successfully
used in many applications involving time series  This is because RNNs are well suited for sequential data as they process inputs one element at   time and store relevant information in their hidden state  In practice  however  training
simple RNNs  sRNN  can be challenging due to the problem of exploding and vanishing gradients  Hochreiter et al 
  It has been shown that exploding gradients can occur when the transition matrix of an RNN has   spectral
norm larger than one  Glorot   Bengio    This results

 The University of Melbourne  Parkville  Australia  Data 
CSIRO  Australia  Correspondence to  Zakaria Mhammedi
 zak mhammedi data csiro au 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

in an error surface  associated with some objective function  having very steep walls  Pascanu et al    On the
other hand  when the spectral norm of the transition matrix
is less than one  the information at one time step tend to
vanish quickly after   few time steps  This makes it challenging to learn longterm dependencies in sequential data 
Different methods have been suggested to solve either the
vanishing or exploding gradient problem  The LSTM has
been speci cally designed to help with the vanishing gradient  Hochreiter   Schmidhuber    This is achieved
by using gate vectors which allow   linear  ow of information through the hidden state  However  the LSTM
does not directly address the exploding gradient problem 
One approach to solving this issue is to clip the gradients
 Mikolov    when their norm exceeds some threshold value  However  this adds an extra hyperparameter to
the model  Furthermore  if exploding gradients can occur
within some parameter search space  the associated error
surface will still have steep walls  This can make training
challenging even with gradient clipping 
Another way to approach this problem is to improve the
shape of the error surface directly by making it smoother 
which can be achieved by constraining the spectral norm of
the transition matrix to be less than or equal to one  However    value of exactly one is best for the vanishing gradient problem    good choice of the activation function
between hidden states is also crucial in this case  These
ideas have been investigated in recent works  In particular  the unitary RNN  Arjovsky et al    uses   special parametrisation to constrain the transition matrix to be
unitary  and hence  of norm one  This parametrisation and
other similar ones  Hyland     atsch    Wisdom et al 
  have some advantages and drawbacks which we will
discuss in more details in the next section 
The main contributions of this work are as follows 

  We  rst show that constraining the search space of the
transition matrix of an RNN to the set of unitary matrices      is equivalent to limiting the search space
to   subset of            is the set of         orthogonal matrices  of   new RNN with twice the hidden size  This suggests that it may not be necessary to

Ef cient Orthogonal Parametrisation of Recurrent Neural Networks

work with complex matrices 

  We present   simple way to parametrise orthogonal transition matrices of RNNs using Householder
matrices  and we derive the expressions of
the
backpropagated gradients with respect to the new
parametrisation  This new parametrisation can also be
used in other deep architectures 

  We develop an algorithm to compute the backpropagated gradients ef ciently  Using this algorithm 
we show that the worst case time complexity of one
gradient step is of the same order as that of the sRNN 

  Related Work
Throughout this work we will refer to elements of the following sRNN architecture 

                       
             

 
 
where       and   are the hiddento hidden  inputto 
hidden  and hiddento output weight matrices       and
     are the hidden vectors at time steps       and   respectively  Finally    is   nonlinear activation function  We
have omitted the bias terms for simplicity 
Recent research explored how the initialisation of the transition matrix   in uences training and the ability to learn
longterm dependencies 
In particular  initialisation with
the identity or an orthogonal matrix can greatly improve
performance  Le et al    In addition to these initialisation methods  one study also considered removing the
nonlinearity between the hiddento hidden connections
 Henaff et al         the term        in Equation  
is outside the activation function   This method showed
good results when compared to the LSTM on pathological
problems exhibiting longterm dependencies 
After training   model for   few iterations using gradient
descent  nothing guarantees that the initial structures of the
transition matrix will be held  In fact  its spectral norm can
deviate from one  and exploding and vanishing gradients
can be   problem again  It is possible to constrain the transition matrix to be orthogonal during training using special
parametrisations  Arjovsky et al    Hyland     atsch 
  which ensure that its spectral norm is always equal
to one  The unitary RNN  uRNN   Arjovsky et al   
is one example where the hidden matrix     Cn   is the
product of elementary matrices  consisting of re ection  diagonal  and Fourier transform matrices  When the size of
hidden layer is equal to    the transition matrix has   total of
only    parameters  Another advantage of this parametrisation is computational ef ciency   the matrixvector product      for some vector    can be calculated in time complexity     log    However  it has been shown that this

 cid cid   

parametrisation does not allow the transition matrix to span
the full unitary group  Wisdom et al    when the size
of the hidden layer is greater than   This may limit the
expressiveness of the model 
Another interesting parametrisation  Hyland     atsch 
  has been suggested which takes advantage of the algebraic properties of the unitary group      The idea is
 cid 
to use the corresponding matrix Lie algebra      of skew
hermitian matrices  In particular  the transition matrix can
be written as     exp
  where exp is the exi   iTi
ponential matrix map and  Ti   
   are prede ned      
matrices forming   bases of the Lie algebra      The
learning parameters are the weights     The fact that the
matrix Lie algebra      is closed and connected ensures
that the exponential mapping from      to      is surjective  Therefore  with this parametrisation the search space
of the transition matrix spans the whole unitary group  This
is one advantage over the original unitary parametrisation
 Arjovsky et al    However  the cost of computing
the matrix exponential to get   is      where   is the
dimension of the hidden state   
Another method  Wisdom et al    performs optimisation directly of the Stiefel manifold using the Cayley
transformation  The corresponding model was called fullcapacity unitary RNN  Using this approach  the transition
matrix can span the full set of unitary matrices  However 
this method involves   matrix inverse as well as matrixmatrix products which have time complexity      This
can be problematic for large neural networks when using
stochastic gradient descent with   small minibatch size 
  more recent study  Vorontsov et al    investigated
the effect of soft versus hard orthogonal constraints on the
performance of RNNs  The soft constraint was applied by
specifying an allowable range for the maximum singular
value of the transition matrix  To this end  the transition
matrix was factorised as       SV  cid  where   and   are
orthogonal matrices and   is   diagonal matrix containing
the singular values of       soft orthogonal constraint consists of specifying small allowable intervals around   for
the diagonal elements of    Similarly to  Wisdom et al 
  the matrices   and   were updated at each training
iteration using the Cayley transformation  which involves  
matrix inverse  to ensure that they remain orthogonal 
All the methods discussed above  except for the original
unitary RNN  involve   step that requires at least       
time complexity  All of them  except for one  require the
use of complex matrices  Table   summarises the time
complexities of various methods  including our approach 
for one stochastic gradient step 
In the next section  we
show that imposing   unitary constraint on   transition matrix     Cn   is equivalent to imposing   special orthog 

Ef cient Orthogonal Parametrisation of Recurrent Neural Networks

onal constraint on   new RNN with twice the hidden size 
Furthermore  since the norm of orthogonal matrices is also
always one  using the latter has the same theoretical bene 
 ts as using unitary matrices when it comes to the exploding gradient problem 

 cid cid cid     cid 
 cid cid     cid cid 

  Complex unitary versus orthogonal
We can show that when the transition matrix     Cn   of
an RNN is unitary  there exists an equivalent representation
of this RNN involving an orthogonal matrix            
In fact  consider   complex unitary transition matrix    
    iB   Cn    where   and   are now realvalued matrices in Rn    We also de ne the following new variables
           
 
where  cid  and  cid  denote the real and imaginary parts of  
complex number  Note that                         
and          nx  where nx is the dimension of the input
vector      in Equation  
Assuming that the activation function   applies to the real
and imaginary parts separately  it is easy to show that the
update equation of the complex hidden state      of the unitary RNN has the following real space representation

 cid cid      cid      
 cid 

 cid     

      

   

      

 cid 

                            

 

Even when the activation function   does not apply to the
real and imaginary parts separately  it is still possible to  nd
an equivalent representation in the real space  Consider the
activation function proposed by  Arjovsky et al   

 cid               

 

if            
otherwise

 

 modRelU     

where   is   bias vector  For   hidden state           the
equivalent activation function in the real space representation is given by

 cid 

   ki

 cid 

  
     
ki
  
     
ki
 

ai 

if

    ki

      
  
ki
otherwise

   

 cid     

 cid 

 

 

 cid 

 

where ki           mod     and  ki      mod    for all
                  The activation function   is no longer
applied to hidden units independently 
Now we will show that the matrix    is orthogonal  By
de nition of   unitary matrix  we have           where
the   represents the conjugate transpose  This implies that
AA cid    BB cid      and BA cid    AB cid      And since we have

 cid AA cid    BB cid  AB cid    BA cid 

BA cid    AB cid  AA cid    BB cid 

 cid 

       cid   

 

 

it follows that        cid       Also note that    has   special
structure   it is   blockmatrix 
The discussion above shows that using   complex  unitary
transition matrix in Cn   is equivalent to using an orthogonal matrix  belonging to   subset of      in   new RNN
with twice the hidden size  This is why in this work we
focus mainly on parametrising orthogonal matrices 

  Parametrisation of the transition matrix
Before discussing the details of our parametrisation  we
 rst introduce   few notations  For          and        
   let Hk   Rk   Rn   be de ned as
 

 cid 

 cid 

In  

Hk     

 

Ik     uu cid 
 cid   cid 

 

 

where Ik denotes the kdimensional identity matrix  For
    Rk  Hk    is the Householder Matrix in      representing the re ection about the hyperplane orthogonal to
       cid cid    Rn and passing through the orithe vector  cid 
gin  where      denotes the zero vector in Rn   
We also de ne the mapping          Rn   as

      

 

 

 cid In 

 

 cid 

 
 

Note that      is not necessarily   Householder re ection  However  when            is orthogonal 
Finally  for          and            we de ne

Mk   Rk       Rn   Rn  

 uk          un   cid  Hn un       Hk uk 

We propose to parametrise the transition matrix   of an
RNN using the mappings  Mk  When using   re ection
vectors  ui  the parametrisation can be expressed as

    Mn   un            un 

  Hn un       Hn   un   

 

where ui   Ri for                          
For the particular case where       in the above
parametrisation  we have the following result 
Theorem   The image of    includes the set of all     
orthogonal matrices                        Rn 
Note that Theorem   would not be valid if    was
In fact  in the twoa standard Householder re ection 

dimensional case  for instance  the matrix cid     

 cid  cannot

be expressed as the product of exactly two standard Householder matrices 
The parametrisation in   has the following advantages 

   

Ef cient Orthogonal Parametrisation of Recurrent Neural Networks

Methods

uRNN  Arjovsky et al   

Fullcapacity uRNN
 Wisdom et al   

Unitary RNN

 Hyland     atsch   

oRNN

 Our approach 

Constraint on the Time complexity of one
transition matrix

online gradient step

 cid   cid     
 cid   cid     
 cid   cid     

      log   
           
           

 cid   cid     

    nm 
where      

Search space of the
transition matrix
  subset of     

when      

The full      set

The full      set

The full      set

when      

Table   Table showing the time complexities associated with one stochastic gradient step  minibatch size   for different methods 
when the size of the hidden layer of an RNN is   and the input sequence has length    

  The parametrisation is smooth  which is convenient
for training with gradient descent  It is also  exible    
good tradeoff between expressiveness and speed can
be found by tuning the number of re ection vectors 

  The time and space complexities involved in one gradient calculation are  in the worst case  the same as
that of the sRNN with the same number of hidden
units  This is discussed in the following subsections 

  When        the matrix   is always orthogonal  as
long as the re ection vectors are nonzero  For       
the only additional requirement for   to be orthogonal is that         

  When        the transition matrix can span the whole
set of      orthogonal matrices  In this case  the total
number of parameters needed for   is        
This results in only   redundant parameters since the
orthogonal set      is           manifold 

  Backpropagation algorithm
Let ui   Ri  Let      un       un      Rn   be
the parameter matrix constructed from the re ection vectors  ui  In particular  the jth column of   can be expressed using the zero vector       Rj  as

     

  Rn 

          

 

 cid 

 cid     

un   

Let   be   scalar loss function and                where
  is constructed using the  ui  vectors following Equation   In order to backpropagate the gradients through
time  we need to compute the following partial derivatives

 cid cid 
 cid        
 cid        
 cid cid 

  

     

 

  
      
  
      

  
      
  

     

 

 

 

 

 

 except on   subset of zero Lebesgue measure 

    cid  

at each time step    Note that in Equation        is
taken as   constant with respect to    Furthermore  we
  
have   
         where   is the length of the input
sequence  The gradient  ow through the RNN at time step
  is shown in Figure  

  

we have    cid  

Figure   Gradient  ow through the RNN at time step    Note that

   Lt ot  yt  where  yt  are target outputs 

  

       and

Before describing the algorithm to compute the backpropagated gradients   
        we  rst derive their
expressions as   function of         and   
      using the
compact WY representation  Joffrain et al    of the
product of Householder re ections 
Proposition   Let                         Let ui  
Ri and      un       un    be the matrix de ned in
Equation   We have
    striu   cid      
Hn un       Hn   un                 cid 

 
where striu   cid     and diag   cid     represent the strictly upper part and the diagonal of the matrix   cid    respectively 

diag   cid    

 

 
 

placed by cid            cid cid      where    is de ned in

Equation   is the compact WY representation of the
product of Householder re ections  For the particular case
where        the RHS of Equation   should be re 
  and      un         
The following theorem gives the expressions of the gradi 
      when           and         
ents   
  

       and

Ef cient Orthogonal Parametrisation of Recurrent Neural Networks

  

  
        

 hk    

Algorithm   Local forward and backward propagations at
time step    For   matrix        denotes the kth column 
          un       un   
  Inputs         
  
                      
  Outputs 
  Require      Rn        Rn      Rn   
       cid un cid           cid un   cid 
             
        
  for       to   do  Local Forward Propagation 
 
                hkU  
  end for
  for       to   do  Local Backward Propagation 
 
 
         hkg    CkH   
  end for
            
         
  
 
          
  
 

 Ck    
  cid 
 kg
         CkU  

  cid 
 kH   

Nk

Nk

Theorem   Let                      Let     Rn   
    Rn  and                 cid    where   is de ned in
Equation   If   is   scalar loss function which depends
on    then we have
  
  
  
  

         cid      cid           cid           
  
  

   cid         cid   

       

 

  

 

where           cid             cid   cid    
striu       
and   the Hadamard product 

     and    
     with   being the       matrix of all ones

The proof of Equations   and   is provided in Appendix    Based on Theorem   Algorithm   performs the
onestep forwardpropagation  FP  and backpropagation
 BP  required to compute      
        See
Appendix   for more detail about how this algorithm is derived using Theorem  
In the next section we analyse the time and space complexities of this algorithm 

  
         and

  

  Time and Space complexity

At each time step    the  op count required by Algorithm
  is           nm for the onestep FP and        
for the onestep BP  Note that the vector   only needs to
be calculated at one time step  This reduces the  op count
at the remaining time steps to          The fact that
the matrix   has all zeros in its upper triangular part can be
used to further reduce the total  op count to          

Model

oRNN       

sRNN    

FP

       

Flop counts

                        

BP

       

Table   Summary of the  op counts due to the computations of
onestep FP and BP through the hiddento hidden connections 
The BP  op counts for the oRNN case assumes that the   matrices  see Algorithm   are not locally generated during the BP
steps  Otherwise  the  op count would be              

                for the onestep FP and          
   for the onestep BP  See Appendix   for more details 
Note that if the values of the matrices    de ned in Algorithm   are  rst stored during    global  FP       through
all time steps  then used in the BP steps  the time complexity  for   global FP and BP using one input sequence
of length   are  respectively         and          when
      and    cid    In contrast with the sRNN case with
  hidden units  the global FP and BP have time complexities        and          Hence  when        the FP
and BP steps using our parametrisation require only about
twice more  ops than the sRNN case with the same number
of hidden units 
Note  however  that storing the values of the matrices   at
all time steps requires the storage of mnT values for one
sequence of length     compared with nT when only the
hidden states         are stored  When    cid    this may
not be practical  One solution to this problem is to generate
the matrices   locally at each BP step using   and     
This results in   global BP complexity of          
 mT   Table   summarises the  op counts for the FP and
BP steps  Note that these  op counts are for the case when
         When        the complexity added due to the
multiplication by      is negligible 

  Extension to the Unitary case

Although we decided to focus on the set of realvalued orthogonal matrices  for the reasons given in Section   our
parametrisation can readily be modi ed to apply to the general unitary case 
Let  Hk   Ck   Cn               be de ned by Equation
  where the transpose sign  cid  is replaced by the conjugate
transpose   Furthermore  let       Rn   Cn   be de ned
as       diag ei          ei     With the new mappings
   Hk    

     we have the following corollary 

 We considered only the time complexity due to computations

through the hiddento hidden connections of the network 

Ef cient Orthogonal Parametrisation of Recurrent Neural Networks

Corollary   Let     be the mapping de ned as
      Rn            Cn   Cn  

             un   cid   Hn un                 

The image of     spans the full set of unitary matrices
     and any point on its image is   unitary matrix 

  Experiments
All RNN models were implemented using the python library theano  Theano Development Team    For ef 
 ciency  we implemented the onestep FP and BP algorithms described in Algorithm   using   code  We tested
the new parametrisation on  ve different datasets all having
longterm dependencies  We call our parametrised network
oRNN  for orthogonal RNN  We set its activation function
       To
to the leaky ReLU de ned as       max   
ensure that the transition matrix of the oRNN is always orthogonal  we set the scalar    to   if        and   otherwise after each gradient update  Note that the parameter
matrix   in Equation   has all zeros in its upper triangular
part  Therefore  after calculating the gradient of   loss with
respect to           
     the values in the upper triangular
part are set to zero 
For all experiments  we used the adam method for stochastic gradient descent  Kingma   Ba    We initialised
all the parameters using uniform distributions similar to
 Arjovsky et al    The biases of all models were set
to zero  except for the forget bias of the LSTM  which we
set to   to facilitate the learning of longterm dependencies
 Koutn   et al   

  Sequence generation

In this experiment  we followed   similar setting to
 Koutn   et al    where we trained RNNs to encode
song excerpts  We used the track Manyrista from album
Musica Deposita by Cuprum  We extracted  ve consecutive excerpts around the beginning of the song  each having   data points and corresponding to  ms with  
 Hz sampling frequency  We trained an sRNN  LSTM 
and oRNN for   epochs on each of the pieces with
 ve random seeds  For each run  the lowest Normalised
Mean Squared Error  NMSE  during the   epochs was
recorded  For each model  we tested three different hidden
sizes  The total number of parameters Np corresponding
to these hidden sizes was approximately equal to    
and   For the oRNN  we set the number of re ection
vectors to the hidden size for each case  so that the transition matrix is allowed to span the full set of orthogonal

 Our implementation can be found at https github 

com zmhammedi Orthogonal RNN 

Figure   Sequence generation task  The plots show the NMSE
distributions for the different models with respect to the total number of parameters for the sequence generation task  The horizontal
red lines represent the medians of the NMSE over   data points
       ve seeds for each of the  ve song excerpts  The solid
rectangles and the dashed bars represent the       and
      con dence intervals respectively 

Figure   Sequence generation task  The RNNgenerated sequences against the true data for one of the  ve excerpts used 
We only displayed the best performing models for Np  cid   

matrices  The results are shown in Figures   and   All the
learning rates were set to   The orthogonal parametrisation outperformed the sRNN and performed on average
better than the LSTM 

  Addition Task

In this experiment  we followed   similar setting to  Arjovsky et al    where the goal of the RNN is to output
the sum of two elements in the  rst dimension of   twodimensional sequence  The location of the two elements
to be summed are speci ed by the entries in the second
dimension of the input sequence  In particular  the  rst dimension of every input sequence consists of random numbers between   and   The second dimension has all zeros
except for two elements equal to   The  rst unit entry

  param NMSE LSTM  param oRNN  param sRNN True dataLSTM True dataoRNN True datasRNNEf cient Orthogonal Parametrisation of Recurrent Neural Networks

is located in the  rst half of the sequence  and the second
one in the second half  We tested two different sequence
lengths         All models were trained to minimise the Mean Squared Error  MSE  The baseline MSE
for this task is   for   model that always outputs one 

Figure   MNIST experiment  Validation accuracy of the oRNN
in Table   as   function of the number of epochs  mBS and LR in
the legend stand for minibatch size and learning rate respectively 

Figure   Addition task  For each lag     the red curves represent
two runs of the oRNN model with two different random initialisation seeds  LSTM and sRNN did not beat the baseline MSE 

We trained an oRNN with       hidden units and
      re ections  We trained an LSTM and sRNN with
hidden sizes   and   respectively  corresponding to   total number of parameters           same as the oRNN
model  We chose   batch size of   and after each iteration    new set of sequences was generated randomly  The
learning rate for the oRNN was set to   Figure   displays the results for both lags 
The oRNN was able to beat the baseline MSE in less than
  iterations for both lags and for two different random
initialisation seeds  This is in line with the results of the
unitary RNN  Arjovsky et al   

  Pixel MNIST

In this experiment  we used the MNIST image dataset 
We split the dataset into training   instances  validation   instances  and test sets   instances  We
trained oRNNs with         and          
where   and   are the number of hidden units and re ections vectors respectively  to minimise the crossentropy
error function  We experimented with  minibatch size 
learning rate           
Table   compares the test performance of our best model
against results available in the literature for unitary orthogonal RNNs  Despite having fewer total number of parameters  our model performed better than three out the four
models selected for comparison  all having      parameters  Figure   shows the validation accuracy as   function
of the number of epochs of our oRNN model in Table  

Figure   MNIST experiment  Effect of varying the number of re 
 ection vectors   on the validation accuracy and speed of convergence  mBS  resp  LR  stands for minibatch size  resp  learning
rate 

Figure   shows the effect of varying the number of re ection vectors   on the performance 

  Penn Tree Bank

In this experiment  we tested the oRNN on the task of character level prediction using the Penn Tree Bank Corpus 
The data was split into training    characters  validation    characters  and test sets    characters 
The total number of unique characters in the corpus was  
The vocabulary size was    and any other words were replaced by the special token  unk  The number of characters per instance       char line  in the training data ranged
between   and   with an average of   char line  We
trained an oRNN and LSTM with hidden units   and  
respectively  corresponding to   total of      parameters  for   epochs  We set the number of re ections to  

    iterations  MSE      LSTMsRNNoRNNx   iterations       LSTMsRNNoRNNepochs Accuracy MNIST convergence plotoRNN       mBS  LR epochs Accuracy MNIST convergence plots for oRNN     mBS LR     mBS LR     mBS LR     mBS LR     mBS LR Ef cient Orthogonal Parametrisation of Recurrent Neural Networks

Model

oRNN

RNN  Vorontsov et al   
uRNN  Arjovsky et al   

RC uRNN  Wisdom et al   
FC uRNN  Wisdom et al   

hidden size
  re ections 
     

 
 
 
 

Number of
parameters

  
  
  
  
  

validation
accuracy
   

 
 
 
 

test

accuracy
   
   
   
   
   

Table   Results summary for the MNIST digit classi cation experiment and comparison with the uRNN results available in the literature 
 FC  and  RC  stand for FullCapacity and Restricted Capacity respectively  The oRNN with       was trained using   minibatch
size of   and   learning rate of  

Model  nh    params

 
 
 
 
 

  oRNN  
LSTM  
  oRNN  
LSTM  

 
 
 
 
 

  
  
  
  

valid cost

test cost

 
 
 
 

 
 
 
 

Table   Results summary for the Penn Tree bank experiment 

for the oRNN  The learning rate was set to   for both
models with   minibatch size of  
Similarly to  Pascanu et al    we considered two tasks 
one where the model predicts one character ahead and the
other where it predicts   character  ve steps ahead 
It
was suggested that solving the later task would require the
learning of longer term correlations in the data rather than
the shorter ones  Table   summarises the test results  The
oRNN and LSTM performed similarly to each other on the
onestep head prediction task  Whereas on the  vestep
ahead prediction task  the LSTM was better  The performance of both models on this task was close to the state of
the art result for RNNs   bpc  Pascanu et al   
Nevertheless  our oRNN still outperformed the results of
 Vorontsov et al    which used both soft and hard orthogonality constraints on the transition matrix  Their RNN
was trained on   of the data  sentences with     characters  and had the same number of hidden units as the
oRNN in our experiment  The lowest test cost achieved
was  bpc  for the onestep ahead prediction task 

  Copying task

We tested our model on the copy task described in details in
 Gers et al    Arjovsky et al    Using an oRNN
with the leaky ReLU we were not able to reproduce the
same performance as the uRNN  Arjovsky et al   
Wisdom et al    However  we were able to achieve
  comparable performance when using the OPLU activation
function  Chernodub   Nowicki    which is   normpreserving activation function  In order to explore whether
the poor performance of the oRNN was only due to the activation function  we tested the same activation as the uRNN

      the real representation of modReLU de ned in Equation   on the oRNN  This did not improve the performance compared to the leaky ReLU case suggesting that
the block structure of the uRNN transition matrix  when expressed in the real space  see Section   may confer special
bene ts in some cases 

  Discussion
In this work  we presented   new parametrisation of
the transition matrix of   recurrent neural network using Householder re ections  This method allows an easy
and computationally ef cient way to enforce an orthogonal constraint on the transition matrix which then ensures
that exploding gradients do not occur during training  Our
method could also be applied to other deep neural architectures to enforce orthogonality between hidden layers  Note
that    soft  orthogonal constraint could also be applied
using our parametrisation by  for example  allowing    to
vary continuously between   and  
It is important to note that our method is particularly advantageous for stochastic gradient descent when the minibatch size is close to   In fact  if   is the minibatch size
and   is the average length of the input sequences  then
  network with   hidden units trained using other methods  Vorontsov et al    Wisdom et al    Hyland
    atsch    that enforce orthogonality  see Section  
would have time complexity   BT        Clearly when
BT  cid    this becomes   BT    which is the same time
complexity as that of the sRNN and oRNN  with       
In contrast with the case of fully connected deep forward
networks with no weight sharing between layers   layer
     the time complexity using our method is   BLnm 
whereas other methods discussed in this work  see Section
  would have time complexity   BLn    Ln  The latter methods are less ef cient in this case since    cid    is
less likely to be the case compared with BT  cid    when
using SGD 
From   performance point of view  further experiments
should be performed to better understand the difference between the unitary versus orthogonal constraint 

Ef cient Orthogonal Parametrisation of Recurrent Neural Networks

Koutn    Jan  Greff  Klaus  Gomez  Faustino    and
Schmidhuber    urgen    clockwork RNN  In Proceedings of the  th International Conference on Machine
Learning  ICML   Beijing  China    June  
pp      URL http jmlr org 
proceedings papers   koutnik html 

Le  Quoc    Jaitly  Navdeep  and Hinton  Geoffrey     
simple way to initialize recurrent networks of recti ed
linear units  CoRR  abs    URL http 
 arxiv org abs 

Mikolov  Tom      Statistical language models based on
neural networks  PhD thesis  Brno University of Technology   

Pascanu  Razvan  Mikolov  Tomas  and Bengio  Yoshua 
On the dif culty of training recurrent neural networks 
ICML      

Theano Development Team  Theano    Python framework
for fast computation of mathematical expressions  arXiv
eprints  abs  May   URL http 
arxiv org abs 

Vorontsov  Eugene  Trabelsi  Chiheb  Kadoury  Samuel 
and Pal  Chris  On orthogonality and learning recurrent
networks with long term dependencies  arXiv preprint
arXiv   

Wisdom  Scott  Powers  Thomas  Hershey  John    Roux 
Jonathan Le  and Atlas  Les    Fullcapacity unitary recurrent neural networks  In Advances in Neural Information Processing Systems   Annual Conference on
Neural Information Processing Systems   December
    Barcelona  Spain  pp     

Acknowledgment
The authors would like to acknowledge Department of
State Growth Tasmania for partially funding this work
through SenseT  We would also like to thank Christfried
Webers for his valuable feedback 

References
Arjovsky  Martin  Shah  Amar  and Bengio  Yoshua  UniIn Internatary evolution recurrent neural networks 
tional Conference on Machine Learning  ICML   

Chernodub  Artem and Nowicki  Dimitri  Normpreserving
orthogonal permutation linear unit activation functions
 oplu  arXiv preprint arXiv   

Gers        Eck     and Schmidhuber     Applying
LSTM to time series predictable through timewindow
In Dorffner  Gerg  ed  Arti cial Neural
approaches 
Networks   ICANN    Proceedings  pp   
Berlin    Springer 

Glorot  Xavier and Bengio  Yoshua  Understanding the dif 
 culty of training deep feedforward neural networks  In
Aistats  volume   pp     

Henaff  Mikael  Szlam  Arthur  and LeCun  Yann  Orthogonal rnns and longmemory tasks  arXiv preprint
arXiv   

Hochreiter  Sepp and Schmidhuber    urgen  Long shortterm memory  Neural computation   
 

Hochreiter  Sepp  Bengio  Yoshua  Frasconi  Paolo  and
Schmidhuber    urgen  Gradient  ow in recurrent nets 
the dif culty of learning longterm dependencies   

Hyland  Stephanie    and   atsch  Gunnar 
Learning
In Prounitary operators with help from     
ceedings of
the ThirtyFirst AAAI Conference on
Arti cial Intelligence  February     San Francisco  California  USA  pp      URL
http aaai org ocs index php AAAI 
AAAI paper view 

Joffrain  Thierry  Low  Tze Meng  QuintanaOrt  Enrique    Geijn  Robert van de  and Zee  Field   Van  Accumulating householder transformations  revisited  ACM
Transactions on Mathematical Software  TOMS   
   

Kingma  Diederik    and Ba  Jimmy  Adam    method
for stochastic optimization  CoRR  abs   
URL http arxiv org abs 

