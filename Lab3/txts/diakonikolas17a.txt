Being Robust  in High Dimensions  Can Be Practical

Ilias Diakonikolas     Gautam Kamath     Daniel    Kane     Jerry Li     Ankur Moitra     Alistair Stewart    

Abstract

Robust estimation is much more challenging in
high dimensions than it is in one dimension 
Most techniques either lead to intractable optimization problems or estimators that can tolerate only   tiny fraction of errors  Recent work
in theoretical computer science has shown that 
in appropriate distributional models  it is possible to robustly estimate the mean and covariance
with polynomial time algorithms that can tolerate
  constant fraction of corruptions  independent of
the dimension  However  the sample and time
complexity of these algorithms is prohibitively
large for highdimensional applications  In this
work  we address both of these issues by establishing sample complexity bounds that are optimal  up to logarithmic factors  as well as giving
various re nements that allow the algorithms to
tolerate   much larger fraction of corruptions  Finally  we show on both synthetic and real data
that our algorithms have stateof theart performance and suddenly make highdimensional robust estimation   realistic possibility 

  Introduction
Robust statistics was founded in the seminal works of
 Tukey    and  Huber    The overarching motto
is that any model  especially   parametric one  is only approximately valid  and that any estimator designed for  
particular distribution that is to be used in practice must
also be stable in the presence of model misspeci cation 
The standard setup is to assume that the samples we are

 Equal contribution

 University of Southern California 
Los Angeles  California  USA  Massachusetts Institute of
Technology  Cambridge  Massachusetts  USA  University
of California  San Diego  La Jolla  California  USA  Correspondence to 
Ilias Diakonikolas  diakonik usc edu 
Gautam Kamath    csail mit edu  Daniel    Kane
 dakane cs ucsd edu 
Li  jerryzli mit edu 
Ankur Moitra  moitra mit edu  Alistair Stewart  alistais usc edu 

Jerry

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

given come from   nice distribution  but that an adversary
has the power to arbitrarily corrupt   constant fraction of
the observed data  After several decades of work  the robust statistics community has discovered   myriad of estimators that are provably robust  An important feature of
this line of work is that it can tolerate   constant fraction of
corruptions independent of the dimension and that there are
estimators for both the location       the mean  and scale
      the covariance  See  Huber   Ronchetti    and
 Hampel et al    for further background 
It turns out that there are vast gaps in our understanding of
robustness  when computational considerations are taken
into account 
In one dimension  robustness and computational ef ciency are in perfect harmony  The empirical
mean and empirical variance are not robust  because   single corruption can arbitrarily bias these estimates  but alternatives such as the median and the interquartile range are
straightforward to compute and are provably robust 
But in high dimensions  there is   striking tension between
robustness and computational ef ciency  Let us consider
estimators for location  The Tukey median  Tukey   
is   natural generalization of the onedimensional median
to highdimensions  It is known that it behaves well      
it needs few samples  when estimating the mean for various symmetric distributions  Donoho   Gasko    Chen
et al    However  it is hard to compute in general  Johnson   Preparata    Amaldi   Kann   
and the many heuristics for computing it degrade badly
in the quality of their approximation as the dimension
scales  Clarkson et al    Chan    Miller   Sheehy 
  The same issues plague estimators for scale  The
minimum volume ellipsoid  Rousseeuw    is   natural
generalization of the onedimensional interquartile range
and is provably robust in highdimensions  but is also hard
to compute  And once again  heuristics for computing
it  Van Aelst   Rousseeuw    Rousseeuw   Struyf 
  work poorly in high dimensions 
The fact that robustness in high dimensions seems to come

Collectively 

the authors were supported by NSF CCF 
  CCF  CCF  CCF  CCF 
  CCF  ONR    three Sloan
Research Fellowships  two Google Faculty Research Awards  an
NSF fellowship  the MIT NEC Corporation  and   USC startup
grant 

Being Robust  in High Dimensions  Can Be Practical

at such   steep price has long been   point of consternation within robust statistics  In     retrospective on the
development of robust statistics  Huber laments   It is one
thing to design   theoretical algorithm whose purpose is to
prove  large fractions of corruptions can be tolerated  and
quite another thing to design   practical version that can
be used not merely on small  but also on medium sized regression problems  with     by   matrix or so  This
last requirement would seem to exclude all of the recently
proposed  techniques 
The goal of this paper is to answer Huber   call to action and design estimators for both the mean and covariance that are highly practical  provably robust  and work
in highdimensions  Such estimators make the promise of
robust statistics   estimators that work in highdimensions
and limit the error induced by outliers   much closer to  
reality 
First  we make some remarks to dispel some common misconceptions  There has been   considerable amount of recent work on robust principal component analysis  much
of it making use of semide nite programming  Some of
these works can tolerate   constant fraction of corruptions
 Cand es et al    however require that the locations of
the corruptions are evenly spread throughout the dataset so
that no individual sample is entirely corrupted  In contrast 
the usual models in robust statistics are quite rigid in what
they require and they do this for good reason    common
scenario that is used to motivate robust statistical methods
is if two studies are mixed together  and one subpopulation
does not    the model  Then one wants estimators that work
without assuming anything at all about these outliers 
There have also been semide nite programming methods
proposed for robust principal component analysis with outliers  Xu et al    These methods assume that the uncorrupted matrix is rank   and that the fraction of outliers is
at most     which again degrades badly as the rank of the
matrix increases  Moreover  any method that uses semidefinite programming will have dif culty scaling to the sizes
of the problems we consider here  For sake of comparison   even with stateof theart interior point methods   it
is not currently feasible to solve the types of semide nite
programs that have been proposed when the matrices have
dimension larger than   hundred 

  Robustness in   Generative Model

Recent works in theoretical computer science have sought
to circumvent the usual dif culties of designing ef cient
and robust algorithms by instead working in   generative
model  The starting point for our paper is the work of Diakonikolas et al      who gave an ef cient algorithm
for the problem of agnostically learning   Gaussian  Given
  polynomial number of samples from   highdimensional

Gaussian       where an adversary has arbitrarily cor 

rupted an  fraction   nd   set of parameters    cid cid cid  that
satisfy dT          cid     cid   

 

dT          cid     cid   

Total variation distance is the natural metric to use to measure closeness of the parameters  since      fraction of
the observed samples came from   Gaussian   Diakonikolas et al      gave an algorithm for the above problem  note that the guarantees are dimension independent 
whose running time and sample complexity are polynomial
in the dimension   and    Lai et al    independently
gave an algorithm for the unknown mean case that achieves
log    and in the unknown covariance case achieves guarantees in   weaker metric that is not
af ne invariant    crucial feature is that both algorithms
work even when the moments of the underlying distribution satisfy certain conditions  and thus are not necessarily brittle to the modeling assumption that the inliers come
from   Gaussian distribution 
  more conceptual way to view such work is as   proofof concept that the Tukey median and minimum volume
ellipsoid can be computed ef ciently in   natural family of
distributional models  This follows because not only would
these be good estimates for the mean and covariance in the
above model  but in fact any estimates that are good must
also be close to them  Thus  these works    into the emerging research direction of circumventing worstcase lower
bounds by going beyond worstcase analysis 
Since the dissemination of the aforementioned works  Diakonikolas et al      Lai et al    there has been  
 urry of research activity on computationally ef cient robust estimation in   variety of highdimensional settings 
including studying graphical models  Diakonikolas et al 
    understanding the computationrobustness tradeoff
for statistical query algorithms  Diakonikolas et al     
tolerating much more noise by allowing the algorithm to
output   list of candidate hypotheses  Charikar et al   
and developing robust algorithms under sparsity assumptions  Li    Du et al    and more  Diakonikolas
et al    Steinhardt et al   

  Our Results

Our goal in this work is to show that highdimensional robust estimation can be highly practical  However  there
are two major obstacles to achieving this  First  the sample complexity and running time of the algorithms in  Diakonikolas et al      is prohibitively large for highdimensional applications  We just would not be able to
store as many samples as we would need  in order to com 
 We use the notation     to hide factors which are polylogarithmic in the argument   in particular  we note that this bound
does not depend on the dimension 

Being Robust  in High Dimensions  Can Be Practical

pute accurate estimates  in highdimensional applications 
Our  rst main contribution is to show nearlytight bounds
on the sample complexity of the  lteringbased algorithm
of  Diakonikolas et al      Roughly speaking  we accomplish this with   new de nition of the good set which
straightforwardly plugs into the existing analysis  show 

ing that one can estimate the mean with  cid      samples
 cid      samples  Both of these bounds are information 

 when the covariance is known  and the covariance with

theoretically optimal  up to logarithmic factors 
Our second main contribution is to vastly improve the fraction of adversarial corruptions that can be tolerated in applications  The fraction of errors that the algorithms of  Diakonikolas et al      can tolerate is indeed   constant
that is independent of the dimension  but it is very small
both in theory and in practice     naive implementation of
the algorithm did not remove any outliers in many realistic
scenarios  We avoid this by giving new ways to empirically tune the threshold for where to remove points from
the sample set 
Finally  we show that the same bounds on the error guarantee continue to work even when the underlying distribution is subGaussian  This theoretically con rms that the
robustness guarantees of such algorithms are in fact not
overly brittle to the distributional assumptions  In fact  the
 ltering algorithm of  Diakonikolas et al      is easily
shown to be robust under much weaker distributional assumptions  while retaining nearoptimal sample and error
guarantees  As an example  we show that it yields   near
sampleoptimal ef cient estimator for robustly estimating
the mean of   distribution  under the assumption that its
covariance is bounded  Even in this regime  the  ltering
algorithm guarantees optimal error  up to   constant factor  Furthermore we empirically corroborate this  nding by
showing that the algorithm works well on real world data 
as we describe below 
Now we come to the task of testing out our algorithms  To
the best of our knowledge  there have been no experimental
evaluations of the performance of the myriad of approaches
to robust estimation  It remains mostly   mystery which
ones perform well in highdimensions  and which do not 
To test out our algorithms  we design   synthetic experiment where        fraction of the samples come from
  Gaussian and the rest are noise and sampled from another distribution  in many cases  Bernoulli  This gives us
  baseline to compare how well various algorithms recover
  and   and how their performance degrades based on
the dimension  Our plots show   predictable and yet striking phenomenon  All earlier approaches have error rates
that scale polynomially with the dimension and ours is  
constant that is almost indistinguishable from the error that
comes from sample noise alone  Moreover  our algorithms

are able to scale to hundreds of dimensions 
But are algorithms for agnostically learning   Gaussian unduly sensitive to the distributional assumptions they make 
We are able to give an intriguing visual demonstration of
our techniques on real data  The famous study of  Novembre et al    showed that performing principal component analysis on   matrix of genetic data recovers   map of
Europe  More precisely  the top two singular vectors de ne
  projection into the plane and when the groups of individuals are colorcoded with where they are from  we recover
familiar country boundaries that corresponds to the map of
Europe  The conclusion from their study was that genes
mirror geography  Given that one of the most important
applications of robust estimation ought to be in exploratory
data analysis  we ask  To what extent can we recover the
map of Europe in the presence of noise  We show that
when   small number of corrupted samples are added to
the dataset  the picture becomes entirely distorted  and this
continues to hold even for many other methods that have
been proposed  In contrast  when we run our algorithm 
we are able to once again recover the map of Europe  Thus 
even when some fraction of the data has been corrupted
      medical studies were pooled together even though the
subpopulations studied were different  it is still possible to
perform principal component analysis and recover qualitatively similar conclusions as if there were no noise at all 

  Formal Framework
Notation  For   vector    we will let  cid   cid  denote its Euclidean norm  If   is   matrix  we will let  cid   cid  denote its
spectral norm and  cid   cid   denote its Frobenius norm  We
will write        to denote that   is drawn from the
empirical distribution de ned by   

Robust Estimation  We consider the following powerful
model of robust estimation that generalizes many other existing models  including Huber   contamination model 
De nition   Given       and   distribution family   
the adversary operates as follows  The algorithm speci es
some number of samples    The adversary generates  
samples               Xm from some  unknown        
It then draws   cid  from an appropriate distribution  This
distribution is allowed to depend on               Xm  but
when marginalized over the   samples satis es   cid   
Bin     The adversary is allowed to inspect the samples  removes   cid  of them  and replaces them with arbitrary
points  The set of   points is then given to the algorithm 

In summary  the adversary is allowed to inspect the samples
before corrupting them  both by adding corrupted points
and deleting uncorrupted points 
In contrast  in Huber  
model the adversary is oblivious to the samples and is only
allowed to add corrupted points 

Being Robust  in High Dimensions  Can Be Practical

We remark that there are no computational restrictions on
the adversary  The goal is to return the parameters of   dis 

tribution  cid   in   that are close to the true parameters in an
use the Mahalanobis distance        cid cid      cid    

appropriate metric  For the case of the mean  our metric
will be the Euclidean distance  For the covariance  we will

This is   strong af ne invariant distance that implies corresponding bounds in total variation distance 
We will use the following terminology 
De nition   We say that   set of samples is  corrupted
if it is generated by the process described in De nition  

  Nearly SampleOptimal Ef cient Robust

Learning

In this section  we present near sampleoptimal ef cient robust estimators for the mean and the covariance of highdimensional distributions under various structural assumptions of varying strength  Our estimators rely on the  ltering technique introduced in  Diakonikolas et al     
This paper gave two algorithmic techniques  the  rst one
was   spectral technique to iteratively remove outliers from
the dataset  ltering  and the second one was   softoutlier
removal method relying on convex programming  The  ltering technique seemed amenable to practical implementation  as it only uses simple eigenvalue computations 
but the corresponding sample complexity bounds given in
 Diakonikolas et al      are polynomially worse than
the informationtheoretic minimum  On the other hand 
the convex programming technique of Diakonikolas et al 
    achieved better sample complexity bounds      
near sampleoptimal for robust mean estimation  but relied on the ellipsoid method  which seemed to preclude  
practically ef cient implementation 
In this work  we achieve the best of both worlds  we give  
better analysis of the  lter  giving sampleoptimal bounds
 up to logarithmic factors  for both the mean and the covariance  Moreover  we show that the  ltering technique
easily extends to much weaker distributional assumptions
      under bounded second moments  Roughly speaking 
the  ltering technique follows   general iterative recipe   
via spectral methods   nd some univariate test which is violated by the corrupted points     nd some concrete tail
bound violated by the corrupted points  and   discard all
points which violate this tail bound 
We start with subgaussian distributions  Recall that if  
is subgaussian on Rd with mean vector   and parameter       then for any unit vector     Rd we have that
PrX                      exp   
Theorem   Let   be   subgaussian distribution on Rd
with parameter       mean     covariance matrix

   and       Let   be an  corrupted set of samples
from   of size     poly log    There exists an
ef cient algorithm that  on input   and       returns  

mean vector  cid  so that with probability at least   we
have  cid cid       cid      cid log 

Diakonikolas et al      gave algorithms for robustly estimating the mean of   Gaussian distribution with known
covariance and for robustly estimating the mean of   binary product distribution  The main motivation for considering these speci   distribution families is that robustly estimating the mean within Euclidean distance immediately
implies total variation distance bounds for these families 
The above theorem establishes that these guarantees hold
in   more general setting with near sampleoptimal bounds 
Under   bounded second moment assumption  we show 
Theorem   Let   be   distribution on Rd with unknown
mean vector    and unknown covariance matrix     cid 
    Let   be an  corrupted set of samples from   of size
    log    There exists an ef cient algorithm that  on

input   and       with probability   outputs  cid  with
 cid cid       cid      

 

 

The sample size above is optimal  up to   logarithmic factor  and the error guarantee is easily seen to be the best possible up to   constant factor  The main difference between
the  ltering algorithm establishing the above theorem and
the  ltering algorithm for the subgaussian case is how we
choose the threshold for the  lter  Instead of looking for  
violation of   concentration inequality  here we will choose
  threshold at random 
In this case  randomly choosing
  threshold weighted towards higher thresholds suf ces to
throw out more corrupted samples than uncorrupted samples in expectation  Although it is possible to reject many
good samples this way  we show that the algorithm still
only rejects   total of    samples with high probability 
Finally  estimating the covariance of   Gaussian 
Theorem   Let           be   Gaussian in   dimensions  and let       Let   be an  corrupted set of samples
from   of size     poly log    There exists an
ef cient algorithm that  given   and   returns the param 

eters of   Gaussian distribution   cid       cid  so that with
probability at least   it holds  cid    cid cid    

   log 

We now provide   highlevel description of the main ingredient which yields these improved sample complexity
bounds  The initial analysis of Diakonikolas et al     
established sample complexity bounds which were suboptimal by polynomial factors because it insisted that the
set of good samples       before the corruption  satis ed
very tight tail bounds  To some degree such bounds are
necessary  as when we perform our  ltering procedure  we
need to ensure that not too many good samples are thrown

Being Robust  in High Dimensions  Can Be Practical

away  However  the old analysis required that fairly strong
tail bounds hold uniformly  The idea for the improvement
is as follows  If the errors are suf cient to cause the variance of some polynomial    linear in the unknown mean
case or quadratic in the unknown covariance case  to increase by more than   it must be the case that for some    
roughly an      fraction of samples are error points with
            As long as we can ensure that less than an
     fraction of our good sample points have            
this will suf ce for our  ltering procedure to work  For
small values of     these are much weaker tail bounds than
were needed previously and can be achieved with   smaller
number of samples  For large values of     these tail bounds
are comparable to those used in previous work  Diakonikolas et al        but in such cases we can take advantage
of the fact that           only with very small probability  again allowing us to reduce the sample complexity  The
details are deferred to the supplementary material 

  Filtering
We now describe the  ltering technique more rigorously  as
well as some additional practical heuristics 

  Robust Mean Estimation

We  rst consider mean estimation  The algorithms which
achieve Theorems   and   both follow the general
recipe in Procedure   We must specify three parameter
functions 
  Thres  is   threshold function we terminate if the
covariance has spectral norm bounded by Thres 
  Tail               is an univariate tail bound  which
would only be violated by     fraction of points if they
were uncorrupted  but is violated by many more of the current set of points 
       is   slack function  which we require for technical reasons 
Given these objects  our  lter is fairly easy to state   rst 
we compute the empirical covariance  Then  we check
if the spectral norm of the empirical covariance exceeds
Thres  If it does not  we output the empirical mean with
the current set of data points  Otherwise  we project onto
the top eigenvector of the empirical covariance  and throw
away all points which violate Tail               for some
choice of slack function  

        cid      

Subgaussian case To instantiate this algorithm for the
subgaussian case  we take Thres       log  
and Tail                
  exp        
    log   log        where   is the subgaussian parameter  See the supplementary material for details 

 

An

Procedure   Filterbased algorithm template for robust
mean estimation
  Input 
 corrupted
  Compute the sample mean    cid 

set
Thres  Tail                   

  EX uS cid    covariance   approximations for the largest absolute eigenvalue and eigenvector of        cid cid  and   

samples   

of

 

return    cid 

  if  cid cid    Thres  then
 
  end if
  Let      cid cid 
  Find       such that

 cid              cid 

Pr

  uS cid 

 cid 

  Tail              

         

  return        cid                  cid 

         

Second moment case To instantiate this algorithm for
the second moment case  we take Thres           
and we take Tail to be   random rescaling of the largest
deviation in the data set  in the direction    See the supplementary material for details 

  Robust Covariance Estimation

Our algorithm for robust covariance follows the exact
recipe outlined above  with one key difference we check
for deviations in the empirical fourth moment tensor  Intuitively  just as in the robust mean setting  we used degree 
information to detect outliers for the mean  the degree 
moment  here we use degree  information to detect outliers for the covariance  the degree  moment 
This corresponds to  nding   normalized degree  polynomial whose empirical variance is too large 
Filtering along this polynomial with an appropriate choice of
Thres       and Tail gives the desired bounds  See
the supplementary material for more details 

  Better Univariate Tests

In the algorithms described above for robust mean estimation  after projecting onto one dimension  we center the
points at the empirical mean along this direction  This is
theoretically suf cient  however  introduces additional constant factors since the empirical mean along this direction
may be corrupted  Instead  one can use   robust estimate
for the mean in one direction  Namely  it is well known that
the median is   provably robust estimator for the mean for
symmetric distributions  Huber   Ronchetti    Hampel et al    and under certain models it is in fact optimal in terms of its resilience to noise  Dvoretzky et al 
  Massart    Chen    Daskalakis   Kamath 
  Diakonikolas et al    By centering the points

Being Robust  in High Dimensions  Can Be Practical

 

 

 

 
 
 
 
 

 
 cid 

 
 
 
 
 
 

 

 

 

 
 
 
 
 

 
 cid 

 
 
 
 
 
 

 

 

 

 

 

 

dimension

 

 

dimension

 

Filtering
Sample mean    noise
RANSAC

LRVMean
Pruning
Geometric Median

Figure   Experiments with synthetic data for robust mean estimation  excess  cid  error is reported against dimension 

at the median instead of the mean  we are able to achieve
better error in practice 

  Adaptive Tail Bounding

In our empirical evaluation  we found that it was important to  nd an appropriate choice of Tail  to achieve
good error rates  especially for robust covariance estimation  Concretely  in this setting  our tail bound is given by
Tail                    exp         Tail              
for some function Tail  and constants       We found
that for reasonable settings  the term that dominated was
always the  rst term on the RHS  and that Tail  is less signi cant  Thus  we focused on optimizing the  rst term 
We found that depending on the setting  it was useful to
change the constant    In particular  in low dimensions 
we could be more stringent  and enforce   stronger tail
bound  which corresponds to   higher    but in higher
dimensions  we must be more lax with the tail bound  To
do this in   principled manner  we introduced   heuristic we
call adaptive tail bounding  Our goal is to  nd   choice of
   which throws away roughly an  fraction of points  The
heuristic is fairly simple  we start with some initial guess
for    We then run our  lter with this    If we throw
away too many data points  we increase our    and retry 
If we throw away too few  then we decrease our    and
retry  Since increasing    strictly decreases the number of
points thrown away  and vice versa  we binary search over
our choice of    until we reach something close to our target accuracy  In our current implementation  we stop when
the fraction of points we throw away is between   and
  or if we ve binary searched for too long  We found
that this heuristic drastically improves our accuracy  and
allows our algorithm to scale fairly smoothly from low to
high dimension 

  Experiments
We performed an empirical evaluation of the above algorithms on synthetic and real data sets with and without

Isotropic

Skewed

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 

 

 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 

 

 

 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 

 
dimension

 

 

 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 

 
dimension

 

 

 

 

 

 

 

 

 
dimension

 

 

 

Filtering
Sample covariance    noise
RANSAC

 

 
dimension
LRVCov
Pruning

 

 

Figure   Experiments with synthetic data for robust covariance
estimation  excess Mahalanobis error is reported against dimension 
synthetic noise  All experiments were done on   laptop
computer with     GHz Intel Core    CPU and   GB of
RAM  The focus of this evaluation was on statistical accuracy  not time ef ciency  In all synthetic trials  our algorithm consistently had the smallest error  sometimes orders of magnitude better than any other algorithms  In the
semisynthetic benchmark  our algorithm also  arguably 
performs the best  though this is subjective  While we did
not optimize our code for runtime  it is always comparable
to  and often better than  the effective alternatives 

  Synthetic Data

Experiments with synthetic data allow us to verify the error
guarantees and the sample complexity rates proven in Section   In both cases  the experiments validate the accuracy
and usefulness of our algorithm  almost exactly matching
the best rate without noise 

Unknown mean The results of our synthetic mean experiment are shown in Figure  
In the synthetic mean
experiment  we set       and for dimension    
              we generate       
  samples  where
       fraction come from        and an   fraction
come from   noise distribution  Our goal is to produce an
estimator which minimizes the  cid  error the estimator has
to the truth  As   baseline  we compute the error that is
achieved by only the uncorrupted sample points  This error
will be used as the gold standard for comparison  since in
the presence of error  this is roughly the best one could do

Being Robust  in High Dimensions  Can Be Practical

even if all the noise points were identi ed exactly 
On this data  we compared the performance of our Filter algorithm to that of   the empirical mean of all the points 
    trivial pruning procedure    the geometric median of
the data      RANSACbased mean estimation algorithm 
and     recently proposed robust estimator for the mean
due to  Lai et al    which we will call LRVMean  For
  we use the implementation available in their Github 
In Figure   the xaxis indicates the dimension of the experiment  and the yaxis measures the  cid  error of our estimated
mean minus the  cid  error of the empirical mean of the true
samples from the Gaussian       the excess error induced
over the sampling error 
We tried various noise distributions  and found that the
same qualitative pattern arose for all of them  In the reported experiment  our noise distribution was   mixture
of two binary product distributions  where one had   couple of large coordinates  see the supplementary material
for   detailed description  For all  nontrivial  error distributions we tried  we observed that indeed the empirical
mean  pruning  geometric median  and RANSAC all have
error which diverges as   grows  as the theory predicts 
On the other hand  both our algorithm and LRVMean have
markedly smaller error as   function of dimension  Indeed 
our algorithm   error is almost identical to that of the empirical mean of the uncorrupted sample points 

Unknown covariance See Figure   for the results of our
synthetic covariance experiment  Our setup is similar to
that for the synthetic mean  Since both our algorithm and
LRVCov require access to fourth moments  we ran into issues with limited memory on machines  This limitation
prevented us from performing experiments at the same dimensionality as the unknown mean setting  and we could
not use as many samples  We          For disam 
mension                   we generate   
ples  where        fraction come from       and
 
the rest come from   noise distribution  We measure distance in the natural af ne invariant way  namely  the Ma 

halanobis distance induced by   to the identity  err cid   
 cid cid      cid     As before  we use the empirical

error of only the uncorrupted data points as   benchmark 
On this corrupted data  we compared the performance of
our Filter algorithm to that of   the empirical covariance of all the points      trivial pruning procedure   
  RANSACbased minimal volume ellipsoid  MVE  algorithm  and     recently proposed robust estimator for the
covariance due to  Lai et al    which we will call

 We note that it is possible that an estimator may achieve

slightly better error than this baseline 

 https github com kal AgnosticMean 

AndCovarianceCode

LRVCov  For   we again obtained the implementation
from their Github repository 
We tried various choices of   and noise distribution  Figure   shows two choices of   and noise  Again  the xaxis
indicates the dimension of the experiment and the yaxis
indicates the estimator   excess Mahalanobis error over the
sampling error  In the left  gure  we set        and our
noise points are simply all located at the allzeros vector  In
the right  gure  we set          eT
    where    is the  rst
basis vector  and our noise distribution is   somewhat more
complicated distribution  which is similarly spiked  but in  
different  random  direction  We formally de ne this distribution in the supplementary material  For all choices of  
and noise we tried  the qualitative behavior of our algorithm
and LRVCov was unchanged  Namely  we seem to match
the empirical error without noise up to   very small slack 
for all dimensions  On the other hand  the performance of
empirical mean  pruning  and RANSAC varies widely with
the noise distribution  The performance of all these algorithms degrades substantially with dimension  and their error gets worse as we increase the skew of the underlying
data  The performance of LRVCov is the most similar to
ours  but again is worse by   large constant factor  In particular  our excess risk was on the order of   for large
   for both experiments  whereas the excess risk achieved
by LRVCov was in all cases   constant between   and  
These experiments demonstrate that our statistical guarantees are in fact quite strong  As our excess error is almost zero  and orders of magnitude smaller than other approaches  this suggests that our sample complexity is indeed nearoptimal  since we match the rate without noise 
and that the constants and logarithmic factors in the theoretical recovery guarantee are often small or nonexistent 

  Semisynthetic Data

To demonstrate the ef cacy of our method on real data 
we revisit the famous study of Novembre et al    In
this study  the authors investigated data collected as part of
the POPRES project  This dataset consists of the genotyping of thousands of individuals using the Affymetrix   
single nucleotide polymorphism  SNP  chip  The authors
pruned the dataset to obtain the genetic data of over  
European individuals  annotated by their country of origin 
Using principal components analysis  they produce   twodimensional summary of the genetic variation  which bears
  striking resemblance to the map of Europe 
Our experimental setup is as follows  While the original
dataset is very high dimensional  we use     dimensional
version of the dataset as found in the authors  GitHub  We
 https github com NovembreLab Novembre 

etal misc

Being Robust  in High Dimensions  Can Be Practical

The data projected onto the top two
directions of the original data set
without noise

The data projected onto the top two directions
of the noisy data set after pruning

The  ltered set of points projected onto the
top two directions returned by the  lter

The data projected onto the top two
directions returned by the  lter

 

Figure   Given genetic data from  Novembre et al    projected down to  dimensions  with added noise  Colors indicate the
individual   country of origin  and match the colors of the countries in the map of Europe  Black points are added noise  The top left
plot is the original plot from  Novembre et al    We recover Europe in the presence of noise whereas naive methods do not 
 rst randomly rotate the data  as then   dimensional data
was diagonalized  and the high dimensional data does not
follow such structure  We then add an additional
  fraction of points  so that they make up an  fraction of the  nal
points  These added points were discrete points  following
  simple product distribution  described in the supplementary materials  We used   number of methods to obtain
  covariance matrix for this dataset  and we projected the
data onto the top two singular vectors of this matrix 
In
Figure   we compare our techniques to pruning  In particular  our output was able to more or less reproduce the
map of Europe  whereas pruning fails to  In the supplementary material  we also compare our result with   number of
other techniques  including those we tested against in the
unknown covariance experiments  and other robust PCA
techniques  The only alternative algorithm which was able
to produce meaningful output was LRVCov  which produced output that was similar to ours  but which produced
  map which was somewhat more skewed  We believe that
our algorithm produces the best picture 

In Figure   we also display the actual points which were
output by our algorithm   Filter  While it manages to remove most of the noise points  it also seems to remove
some of the true data points  particularly those from Eastern
Europe and Turkey  We attribute this to   lack of samples
from these regions  and thus one could consider them as
outliers to   dataset consisting of Western European individuals  For instance  Turkey had   data points  so it seems
quite reasonable that any robust algorithm would naturally
consider these points outliers 
We view our experiments as   proof of concept demonstration that our techniques can be useful in real world
exploratory data analysis tasks  particularly those in highdimensions  Our experiments reveal that   minimal amount
of noise can completely disrupt   data analyst   ability to
notice an interesting phenomenon  thus limiting us to only
very wellcurated data sets  But with robust methods  this
noise does not interfere with scienti   discovery  and we
can still recover interesting patterns which otherwise would
have been obscured by noise 

 Filter Output Filter Projection Original Data Pruning ProjectionBeing Robust  in High Dimensions  Can Be Practical

References
Amaldi     and Kann     The complexity and approximability of  nding maximum feasible subsystems of linear
relations  Theoretical Computer Science   
 

Cand es        Li     Ma     and Wright     Robust princi 

pal component analysis     ACM     

Chan        An optimal randomized algorithm for maxIn Proceedings of the Fifteenth
imum tukey depth 
Annual ACMSIAM Symposium on Discrete Algorithms
 SODA  pp     

Charikar     Steinhardt     and Valiant     Learning from

untrusted data  In Proceedings of STOC   

Chen     Gao     and Ren       general decision theory
for huber    contamination model  Electronic Journal
of Statistics     

Chen       note on bias robustness of the median  Statistics

  probability letters     

Clarkson        Eppstein     Miller        Sturtivant    
and Teng       Approximating center points with iterated radon points  In Proceedings of the Ninth Annual
Symposium on Computational Geometry  SCG   pp 
  New York  NY  USA    ACM 

Daskalakis     and Kamath     Faster and sample nearoptimal algorithms for proper learning mixtures of gaussians  In Proceedings of The  th Conference on Learning Theory  COLT   pp     

Diakonikolas     Kamath     Kane        Li     Moitra 
   and Stewart     Robust estimators in high dimenIn Prosions without the computational intractability 
ceedings of FOCS      Full version available at
https arxiv org pdf pdf 

Diakonikolas     Kane        and Stewart     Robust
learning of  xedstructure bayesian networks  CoRR 
abs     
URL https arxiv 
org abs 

Diakonikolas     Kane        and Stewart     Statistical query lower bounds for robust estimation of highdimensional gaussians and gaussian mixtures  CoRR 
abs      URL http arxiv org 
abs 

Diakonikolas     Kamath     Kane        Li     Moitra 
   and Stewart     Robustly learning   gaussian  Getting optimal error  ef ciently  CoRR  abs 
 

Donoho        and Gasko     Breakdown properties of location estimates based on halfspace depth and projected
outlyingness  Ann  Statist       

Du        Balakrishnan     and Singh     Computationally ef cient robust estimation of sparse functionals  In
Proceedings of COLT   

Dvoretzky     Kiefer     and Wolfowitz     Asymptotic
minimax character of the sample distribution function
and of the classical multinomial estimator  Ann  Mathematical Statistics     

Hampel        Ronchetti        Rousseeuw        and Stahel        Robust statistics  The approach based on in 
 uence functions  Wiley New York   

Huber        Robust estimation of   location parameter  The
Annals of Mathematical Statistics     

Huber        and Ronchetti        Robust statistics  Wiley

New York   

Johnson        and Preparata        The densest hemisphere problem  Theoretical Computer Science   
   

Lai        Rao        and Vempala     Agnostic esIn Proceedings of

timation of mean and covariance 
FOCS   

Li     Robust sparse estimation tasks in high dimensions 

In Proceedings of COLT   

Massart     The tight constant in the DvoretzkyKiefer 
Annals of Probability   

Wolfowitz inequality 
   

Miller       and Sheehy     Approximate centerpoints with

proofs  Comput  Geom     

Novembre     Johnson     Bryc     Kutalik     Boyko 
      Auton     Indap     King        Bergmann    
Nelson        et al  Genes mirror geography within
europe  Nature     

Rousseeuw     Multivariate estimation with high breakdown point  Mathematical Statistics and Applications 
pp     

Rousseeuw        and Struyf     Computing location depth
and regression depth in higher dimensions  Statistics and
Computing     

Steinhardt     Charikar     and Valiant     Resilience   
criterion for learning in the presence of arbitrary outliers 
CoRR  abs   

Being Robust  in High Dimensions  Can Be Practical

Tukey         survey of sampling from contaminated distributions  Contributions to probability and statistics   
   

Van Aelst     and Rousseeuw     Minimum volume ellipsoid  Wiley Interdisciplinary Reviews  Computational
Statistics     

Xu     Caramanis     and Sanghavi     Robust pca via
outlier pursuit  In Advances in Neural Information Processing Systems  pp     

