Variational Boosting  Iteratively Re ning Posterior Approximations

Andrew    Miller   Nicholas    Foti   Ryan    Adams    

Abstract

We propose   blackbox variational inference
method to approximate intractable distributions
with an increasingly rich approximating class 
Our method  variational boosting  iteratively re 
 nes an existing variational approximation by
solving   sequence of optimization problems  allowing   tradeoff between computation time and
accuracy  We expand the variational approximating class by incorporating additional covariance
structure and by introducing new components to
form   mixture  We apply variational boosting
to synthetic and real statistical models  and show
that the resulting posterior inferences compare
favorably to existing variational algorithms 

  Introduction
Variational inference  VI  is   family of methods to approximate an intractable target distribution  typically known
only up to   constant  with   tractable surrogate distribution  Blei et al    Jordan et al    Wainwright
  Jordan    VI procedures typically minimize the
KullbackLeibler  KL  divergence between the approximation and target distributions by maximizing   tractable lower
bound on the marginal likelihood  The approximating family is often  xed  and typically excludes the neighborhood
surrounding the target distribution  which prevents the approximation from becoming arbitrarily close to the true
posterior  In the context of Bayesian inference  this mismatch between the variational family and the true posterior
often manifests as underestimating the posterior variances of
the model parameters and the inability to capture posterior
correlations  Wainwright   Jordan   
An alternative approach to posterior inference uses Markov

 Harvard University  Cambridge  MA  USA  University
Seattle  WA  USA  Google Brain 
of Washington 
Cambridge  MA  USA  Correspondence
Andrew
   Miller  acm seas harvard edu  Nicholas    Foti
 nfoti uw edu 

to 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

chain Monte Carlo  MCMC  methods that approximate  
target distribution with samples drawn from   Markov chain
constructed to admit the target distribution as the stationary
distribution  MCMC enables   tradeoff between computation and accuracy  drawing more samples makes the approximation closer to the target distribution  However  MCMC
algorithms typically must be run iteratively and it can be dif 
 cult to assess convergence to the true target  Furthermore 
correctly specifying MCMC moves can be more algorithmically restrictive than optimizationbased approaches 
To alleviate the mismatch between tractable variational approximations and complicated posterior distributions  we
propose   variational inference method that iteratively allows the approximating family of distributions to become
more complex  Under certain conditions  the proposed approximations are eventually expressive enough to represent
the true target arbitrarily well  though we do not prove our algorithm attains such   universal approximation here  Thus 
the practitioner can trade time  tting   posterior approximation for increased accuracy of posterior estimates  Our
algorithm grows the complexity of the approximating class
in two ways    incorporating rich covariance structure  and
  sequentially adding new components to the approximating distribution  Our method builds on blackbox variational
inference methods using the reparameterization trick by
adapting it to be used with mixture distributions  This allows
our method to be applied to   variety of target distributions
including those arising from nonconjugate model speci 
cations  Kingma   Welling    Ranganath et al   
Salimans   Knowles    We demonstrate empirically
that our algorithm improves posterior estimates over other
variational methods for several practical Bayesian models 

  Variational Inference
Given   target distribution with density      for   continuous random variable         RD  variational inference approximates     with   tractable distribution 
       from which we can ef ciently draw samples and
form samplebased estimates of functions of    Variational
methods minimize the KLdivergence  KL    between
     and the true   as   function of variational parameter    Bishop    Although direct minimization of
 We assume     is known up to   constant            

Variational Boosting  Iteratively Re ning Posterior Approximations

KL    is often intractable  we can derive   tractable objective based on properties of the KLdivergence  This
objective is known as the evidence lower bound  ELBO 

 cid 
     Eq   ln       ln          lnC
  lnC   KL      lnC   ln

   dx

which  due to the positivity of KL    is   lower bound
on     log          the marginal likelihood 
Variational methods typically      family of distributions
                  parameterized by   and maximize
the ELBO with respect to       Often there exists some
 possibly nonunique        for which KL    is minimized  However  when the family   does not include  
then KL        which will result in biased estimates
of functions       Ex            cid  Ex      
The primary alternative to variational methods for approximate inference is Markov chain Monte Carlo  MCMC 
which constructs   Markov chain such that the target distribution remains invariant  Expectations with respect to
the target distribution can be calculated as an average with
respect to these correlated samples  MCMC typically enjoys nice asymptotic properties  as the number of samples
grows  MCMC samplers represent the true target distribution with increasing  delity  However  rules for constructing
correct Markov steps are restrictive  With   few exceptions 
most MCMC algorithms require evaluating   loglikelihood
that touches all data at each step in the chain  Maclaurin  
Adams    Welling   Teh    This becomes problematic during statistical analyses of large amounts of data  
MCMC is often considered unusable because of this computational bottleneck  Notably  variational methods can avoid
this bottleneck by subsampling the data  Ranganath et al 
    as unbiased estimates of the loglikelihood can often
be straightforwardly used with optimization methods 
In the next section  we propose an algorithm that iteratively
grows the approximating class   and reframes the VI procedure as   series of optimization problems  resulting in  
practical inference method that can both represent arbitrarily
complex distributions and scale to large data sets 

  Variational Boosting
We de ne our class of approximating distributions to be
mixtures of   simpler component distributions 

 cid 

 cid 

qc       can be any distribution over     RD from which
we can ef ciently draw samples using   continuous mapping
parameterized by          multivariate normal  Jaakkola  
Jordan    or   composition of invertible maps  Rezende
  Mohamed   
When posterior expectations and variances are of interest 
mixture distributions provide tractable summaries  Expectations are easily expressed in terms of component expectations 

 
             

           dx  

 cEqc       

 

In the case of multivariate normal components  the mean
and covariance of   mixture are easy to compute  as are
marginal distributions along any set of dimensions 
Variational boosting  VBoost  begins with   single mixture component                  with       We
         and use existing blackbox variational inference methods to    the  rst component parameter    At
the next iteration       we      and introduce   new
component into the mixture         We de ne   new
ELBO objective as   function of new component parameters    and   new mixture weight    We then optimize
this objective with respect to   and   until convergence 
At each subsequent round     we introduce new component
parameters and   mixing weight          which are then
optimized according to   new ELBO objective  The name
variational boosting is inspired by methods that iteratively
construct strong learners from ensembles of weak learners 
We apply VBoost to target distributions via blackbox variational inference with the reparameterization trick to   
each component and mixture weights  Kingma   Welling 
  Ranganath et al    Salimans   Knowles   
However  using mixtures as the variational approximation
complicates the use of the reparameterization trick 

  The reparameterization trick and mixtures

The reparameterization trick is used to compute an unbiased
estimate of the gradient of an objective that is expressed
as an intractable expectation with respect to   continuousvalued random variable  This situation arises in variational
inference when the ELBO cannot be evaluated analytically 
We form an unbiased estimate as 

     Eq  ln       ln       

  cid 

 cid 

   
 

 cid 

ln    cid    ln     cid   

 cid 

 

 

  cid 

 cid 

            

 cqc                     

      

  

 

where we denote the full mixture as      mixing
proportions                   and component distributions qc      parameterized by                   The

where   cid           To obtain   Monte Carlo estimate
of the gradient of    using the reparameterization
trick  we  rst separate the randomness needed to generate   cid  from the parameters   by de ning   deterministic map   cid   cid  fq    such that        implies

Variational Boosting  Iteratively Re ning Posterior Approximations

  cid           Note that    does not depend on   We
then differentiate Eq    with respect to   through the map
fq to obtain an estimate of    
When      is   mixture  applying the reparameterization
trick is not straightforward  The typical sampling procedure
for   mixture model includes   discrete random variable that
indicates   mixture component  which complicates differentiation  We circumvent this by rewriting the variational
objective as   weighted combination of expectations with
respect to individual mixture components 

 cid 

 cqc       

 ln       ln        dx

qc         ln       ln        dx

 cid   cid    cid 
 cid 
  cid 
  cid 

  

  

  

  

      

 

 

 cEqc  ln       ln       

  cid 

  

which is   weighted sum of componentspeci   ELBOs  If
the qc are continuous and there exists some function fc   
such that     fc    and     qc    when        then
we can apply the reparameterization trick to each component to obtain gradients of the ELBO  

 cL        
  cid 

 

 cEx       ln       ln       

 cid   ln  fc     
     ln   fc       cid   

 cE   

  

VBoost leverages the above formulation of  cL   
to use the reparameterization trick in   componentby 
component manner  allowing us to improve the variational
approximation as we incorporate new components 

  Incorporating New Components

Next  we describe how to incrementally add components
during the VBoost procedure 
The  rst component VBoost starts by  tting   approximation to     consisting of   single component     We do
this by maximizing the  rst ELBO objective

     Eq   ln       ln       

 
    arg max

    

 

 
 

Depending on the forms of   and    optimizing    can
be accomplished by various methods   an obvious choice
being blackbox VI with the reparameterization trick  After
convergence we      to be  
 

  cid 

Figure   Onedimensional illustration of the VBoost procedure 
Top  Initial singlecomponent approximation  solid blue  Middle 
  new component  dotted red  is initialized  Bottom  New component parameters and mixing weights are optimized using Monte
Carlo gradients of the ELBO  Note that the mass of the existing
components can rise and fall  but not shift in space 
Component       After iteration    our current approximation to     is   mixture distribution with   components 

            

 cqc       

 

  

Adding   component to Eq    introduces   new component parameter      and   new mixing weight      In
this section  the mixing parameter           mixes between the new component  qC      and the existing
approximation       The new approximate distribution is

          
                     qC         
The new ELBO  as   function of     and     is 
          
   
          
     EqC 

ln       ln               

ln       ln               

ln       ln               

      

 cid 

 cid 

 cid 

 cid 

 cid 

    

 cid 

 

Crucially  we have separated out two expectations  one
with respect to the existing approximation        which is
 xed  and the other with respect to the new component
distribution  qC  Because we have  xed      we only
need to optimize the new component parameters     
and     allowing us to use the reparameterization trick
to obtain gradients of      Note that evaluating the
gradient requires sampling from the existing components
which may result in larger variance than typical blackbox
variational methods  To mitigate the extra variance we use
many samples to estimate the gradient and leave variance
reduction to future work 

existing approx targetexisting approx initial new comp targetexisting approx optimized new comp targetVariational Boosting  Iteratively Re ning Posterior Approximations

Figure   illustrates the algorithm on   simple onedimensional example   the initialization of   new component and the resulting mixture after optimizing the second objective       Figure   depicts the result
of VBoost on   twodimensional  multimodal target distribution  In both cases  the component distributions are
Gaussians with diagonal covariance 

  Structured Multivariate Normal Components

Though our method can use any component distribution
that can be sampled using   continuous mapping    sensible
choice of component distribution is   multivariate normal

        cid 

 

                  

    exp cid   

 cid 
        

           

where the variational parameter   is transformed into   mean
vector   and covariance matrix  
Specifying the structure of the covariance matrix is   choice
that largely depends on the dimensionality of     RD
and the correlation structure of the target distribution 
  common choice of covariance is   diagonal matrix 
   which implies that   is indepen 
    diag 
dent across dimensions  When the approximation only consists of one component  this structure is commonly referred
to as the mean  eld family  While computationally ef cient 
mean  eld approximations cannot model posterior correlations  which often leads to underestimation of marginal
variances  Additionally  when diagonal covariances are used
as the component distributions in Eq    the resulting mixture may require   large number of components to represent
the strong correlations  see Fig    Furthermore  independence constraints can actually introduce local optima in the
variational objective  Wainwright   Jordan   
On the other end of the spectrum  we can parameterize
the entire covariance matrix using the Cholesky decompo 
 cid 
sition     such that LL
    This allows   to be any
positive semide nite matrix  enabling   to have the full
 exibility of   Ddimensional multivariate normal distribution  However  this introduces         parameters 
which can be computationally cumbersome when   is even
moderately large  Furthermore  only   few pairs of variables
may exhibit posterior correlations  particularly in multilevel
models or neural networks where different parameter types
may be nearly independent in the posterior 
As such  we would like to incorporate some capacity to
capture correlations between dimensions of   without overparameterizing the approximation  The next subsection discusses   covariance speci cation that provides this tradeoff 
while remaining computationally tractable 
Lowrank plus diagonal covariance Blackbox variational inference methods with the reparameterization trick

Figure   Sequence of increasingly complex approximate posteriors  with             isotropic Gaussian components  The
background  grey black  contours depict the target distribution 
and the foreground  red  contours depict the approximations 

require sampling from the variational distribution and ef 
 ciently computing  or approximating  the entropy of the
variational distribution  For multivariate normal distributions  the entropy is   function of the determinant of the
covariance matrix    while computing the log likelihood
requires computing   When the dimensionality of the
target     is large  computing determinants and inverses
will have      time complexity and therefore may be
prohibitively expensive to compute at every iteration 
However  it may be unnecessary to represent all     
possible correlations in the target distribution  particularly
if certain dimensions are close to independent  One way to
increase the capacity of        is to model the covariance
as   lowrank plus diagonal  LR    matrix

 cid 

       

  diag exp   

 
where     RD   is   matrix of off diagonal factors 
and     RD is the logdiagonal component  This is effectively approximating the target via   factor analysis model 
The choice of the rank   presents   tradeoff  with   larger
rank  the variational approximation can be more  exible 
with   lower rank  the computations necessary for  tting the
variational approximation are more ef cient  As   concrete
example  in Section   we present         dimensional
posterior resulting from   nonconjugate hierarchical model 
and we show that    rank       plus diagonal  covariance
does an excellent job capturing all             pairwise correlations and   marginal variances  Incorporating
more components using the VBoost framework further improves the approximation of the distribution 
To use the reparameterization trick with this low rank covariance  we can simulate from   in two steps

  lo        Ir 

  hi        ID 

        lo              hi 

Variational Boosting  Iteratively Re ning Posterior Approximations

where   lo  generates the randomness due to the lowrank
structure  and   hi  generates the randomness due to the diagonal structure  We use the operator        diag exp   
for notational brevity  This generative process can be differentiated  yielding Monte Carlo estimates of the gradient
with respect to   and   suitable for stochastic optimization 
In order to use LR   covariance structure within VBoost 
we will need to ef ciently compute the determinant and
inverse of   The matrix determinant lemma expresses the
determinant of   as the product of two determinants

 cid       
 Ir    

 cid       

  exp

vd

 cid 

    

              Ir    

 cid cid 

 cid 

 

where the left term is simply the product of the diagonal
component  and the right term is the determinant of an      
matrix  computable in      time  Harville   
To compute   the Woodbury matrix identity states that

 cid 

      

    
                Ir    

 cid         

 cid     

which involves the inversion of   smaller        matrix
and can be done in      time  Golub   Van Loan   
Importantly  for    cid    the above operations are ef ciently
differentiable and amenable for use in the BBVI framework 
Fitting the rank To specify the ELBO objective  we need
to choose   rank   for the component covariance  There are
many ways to decide on the rank of the variational approximation  some more appropriate for certain settings given
dimensionality and computational constraints  For instance 
we can greedily incorporate new rank components  Alternatively  we can      sequence of ranks                 rmax 
and choose the best result  in terms of KL  In the Bayesian
neural network model  we report results for    xed schedule
of ranks  In the hierarchical Poisson model  we monitor the
change in marginal variances to decide the appropriate rank 
See Section   of the supplement for further discussion 
Initializing new component parameters When we add
  new component  we must  rst initialize the component
parameters  We  nd that the VBoost optimization procedure can be sensitive to initialization  so we devise   cheap
importance samplingbased algorithm to generate   good
starting point  This initialization procedure is detailed in
Section   and Algorithm   of the supplement 

  Related Work

Mixtures of mean  eld approximations were introduced
in Jaakkola   Jordan   where mean  eldlike updates
were developed using   bound on the entropy term and

modelspeci   parameter updates  Nonparametric variational inference  introduced in Gershman et al    is  
blackbox variational inference algorithm that approximates
  target distribution with   mixture of equallyweighted
isotropic normals  The authors use   lowerbound on the entropy term in the ELBO to make the optimization procedure
tractable  Similarly  Salimans   Knowles   present  
method for  tting mixture distributions as an approximation 
However  their method is restricted to mixture component
distributions within the exponential family  and   joint optimization procedure  Mixture distributions are   type of
hierarchical variational model  Ranganath et al     
where the component identity can be thought of as latent
variables in the variational distribution  While in Ranganath
et al      the authors optimize   lower bound on the
ELBO to    general hierarchical variational models  our approach integrates out the discrete latent variables  allowing
us to directly optimize the ELBO 
Sequential maximumlikelihood estimation of mixture models has been studied previously where the error between the
sequentially learned model and the optimal model where all
components and weights are jointly learned is bounded by
     where   is the number of mixture components  Li
  Barron    Li    Rakhlin et al      similar
bound was proven in Zhang   using arguments from
convex analysis  More recently  sequentially constructing
  mixture of deep generative models has been shown to
achieve the same      error bound when trained using
an adversarial approach  Tolstikhin et al    Though
these ideas show promise for deriving error bounds for variational boosting  there are dif culties in applying them 
In concurrent work  Guo et al    developed   boosting
procedure to construct  exible approximations to posterior
distributions  In particular  they use gradientboosting to
determine candidate component distributions and then optimize the mixture weight for the new component  Friedman 
  However  Guo et al    assume that the gradientboosting procedure is able to  nd the optimal new component so that the arguments in Zhang   apply  which is
not true in general  We note that if we make the similar assumption that at each step of VBoost the component parameters  
  are found exactly  then the optimization of    is
convex and can be optimized exactly  We can then appeal to
the same arguments in Zhang   and obtain an     
error bound  The work in Guo et al    provides important  rst steps in the theoretical development of boosting
methods applied to variational inference  however  we note
that developing   comprehensive theory that deals with the
dif culties of multimodality and the nonjoint convexity of
KL divergence in   and   is still needed  Recently  Moore
  began to address issues of multimodality from model
symmetry in variational inference  However  the question
remains whether the entire distribution is being explored 

Variational Boosting  Iteratively Re ning Posterior Approximations

    baseball marginals

    baseball covariances

Figure   Left  Comparison of bivariate  top  and univariate  bottom  marginals for the baseball model  Histograms scatterplots depict
  NUTS samples  The top left depicts  ln     marginal samples and   mean  eld approximation  MFVI  The Top Right shows
the same bivariate marginal  and the VBoost approximation with isotropic components  The bottom panels compare NUTS  MFVI  and
VBoost on univariate marginals   and ln   Right  Comparison of posterior covariances for the      dimensional baseball model 
Each plot compares covariance estimates of VBoost  xaxis  with increasing numbers of components and MCMC samples  yaxis  As
more components are added  the VBoost estimates more closely match the MCMC covariance estimates 

Seeger   explored the use of lowrank covariance Gaussians as variational approximations using   PCAlike algorithm  Additionally  concurrent work has proposed the use
  LR   matrices as the covariances of Gaussian posterior
approximations  Ong et al    We have also found that
though the LR   covariance approximation is useful for
capturing posterior correlations  combining the idea with
boosting new components to capture nonGaussian posteriors yields superior posterior inferences 

  Experiments and Analysis
To supplement the previous synthetic examples  we use
VBoost to approximate various challenging posterior distributions arising from real statistical models of interest 

Binomial Regression We  rst apply VBoost to   nonconjugate hierarchical binomial regression model  The
model describes the binomial rates of success  batting averages  of baseball players using   hierarchical model  Efron
  Morris    parameterizing the  skill  of each player 

     Beta               
yj   Binomial Kj     

player   prior
player   hits  

where yj is the number of successes  hits  player   has
attempted in Kj attempts  at bats  Each player has   latent success rate     which is governed by two global variables   and   We specify the priors     Unif    and
    Pareto    There are   players in this example 
creating   posterior distribution with       parameters 
For each round of VBoost  we estimate       using
 Code available at https github com andymiller vboost 
 Model and data from the mcstan case studies

  samples each for qC  and qC  We use   iterations of adam with default parameters to update     and
     Kingma   Ba   
In all experiments  we use autograd to obtain gradients
with respect to new component parameters  Maclaurin et al 
      To highlight the  delity of our method  we compare VBoost with rank  components to mean  eld VI
 MFVI  and the NoU Turn Sampler  NUTS   Hoffman
  Gelman    The empirical distribution resulting from
   NUTS samples is considered the  ground truth  posterior in this example  Figure    compares   selection of
univariate and bivariate posterior marginals  We see that
VBoost is able to closely match the NUTS posteriors  improving upon the MFVI approximation  Figure    compares
the VBoost covariance estimates to the  ground truth  estimates of MCMC at various stages of the algorithm  We
see that VBoost is able to capture pairwise covariances with
increasing accuracy as the number of components increases 

Multilevel Poisson GLM We use VBoost to approximate the posterior of   hierarchical Poisson GLM    common nonconjugate Bayesian model  Here  we focus on  
speci   model that was formulated to measure the relative
rates of stopand frisk events for different ethnicities and in
different precincts  Gelman et al    and has been used
as an illustrative example of multilevel modeling  Gelman
  Hill    The model uses   precinct and ethnicity effect
to describe the relative rate of stopand frisk events

          
 
          
 

ln  ep                 ln Nep

Yep     ep 

ethnicity effect
precinct effect
log rate
stopand frisk events

Variational Boosting  Iteratively Re ning Posterior Approximations

    Rank    MFVI 

    Rank    component

    Rank  

    Rank    component

    Rank  

    Rank    component

Figure   Left    sampling of bivariate marginals for   single Gaussian component approximation for the      dimensional frisk
model  Each row incorporates more covariance structure  Though there are   total of   covariances to be approximated  only   few
directions in the Ddimensional parameter space exhibit nontrivial correlations  Right  The same marginals with   mixture approximation
using rank  Gaussians at various stages of the VBoost algorithm  Introducing new mixture components allows the posterior to take  
nonGaussian shape  most exhibited in the third column 

dataset
wine
boston
concrete
powerplant
yacht
energyeff 

pbp
     
     
     
     
     
     

rank  
     
     
     
     
     
     

vboost  
     
     
     
     
     
     

vboost  
     
     
     
     
     
     

vboost  
     
     
     
     
     
     

Table   Test log probability for PBP and
VBoost with varying number of components  xed rank of   Each entry shows
the average predictive performance of
the model and the standard deviation
across the   trials   bold indicates the
best average  though not necessarily  statistical signi cance 

  ln  

         

where Yep are the number of stopand frisk events within
ethnicity group   and precinct   over some  xed period of time  Nep is the total number of arrests of ethnicity group   in precinct   over the same period of
time     and    are the ethnicity and precinct effects  The
prior over the mean offset and group variances is given
by   ln  
As before  we simulate    NUTS samples  and compare
various variational approximations  Because of the high
posterior correlations present in this example  VBoost with
diagonal covariance components is inef cient in its representation of this structure  As such  this example relies
on the lowrank approximation to shape the posterior  Figure   shows how posterior accuracy is affected by incorporating covariance structure  left  and adding more components  right  Figures   and   in the supplement compare
VBoost covariances to MCMC samples  showing that increased posterior rank capacity and number of components
yield more accurate marginal variance and covariance estimates  These results indicate that while incorporating
covariance structure increases the accuracy of estimating
marginal variances  the nonGaussianity afforded by the use

of mixture components allows for   better posterior approximation translating into more accurate moment estimates 

Bayesian Neural Network We apply our method to  
Bayesian neural network  BNN  regression model  which
admits   highdimensional  nonGaussian posterior  We
compare predictive performance of VBoost to Probabilistic Backpropagation  PBP   Hern ndezLobato   Adams 
  Mimicking the experimental setup of Hern ndezLobato   Adams   we use   single  unit hidden
layer  with ReLU activation functions  We place   normal
prior over each weight in the neural network  governed by
the same variance and an inverse Gamma prior over the
observation variance yielding the model 

wi        

                        

weights
output distribution

where      wi  is the set of weights  and        is
  multilayer perceptron that maps input   to output  
as   function of parameters    Both   and   are given
Gamma    priors  We denote the set of parameters as
   cid            We approximate the posterior           

Variational Boosting  Iteratively Re ning Posterior Approximations

where   is the training set of  xn  yn  
   inputoutput
pairs  We then use the posterior predictive distribution to
compute the distribution for   new input   

 cid 

  cid 

 cid 

   
 

          

              

        cid   

 cid        

 

 

and report average predictive log probabilities for held out
data               For   dataset with input dimension
    the posterior has dimension           between
      and       for the data sets considered 
We report heldout predictive performance for different approximate posteriors for six datasets  For each dataset  we
perform the following training procedure   times  First 
we create   random partition into     training set and
  testing set  We then apply VBoost  adding rank   components  We allow each additional component only  
iterations  To save time on initialization  we draw   samples from the existing approximation  and initialize the new
component with the sample with maximum weight  For
comparison  Probabilistic backpropagation is given  
passes over the training data   empirically  suf cient for
the algorithm to converge 
Table   in the supplement presents outof sample log probability for singlecomponent multivariate Gaussian approximations with varying rank structure  Table   presents outof sample log probability for additional rank   components
added using VBoost  We note that though we do not see
much improvement as rank structure is added  we do see
predictive improvement as components are added  Our results suggest that incorporating and adapting new mixture
components is   recipe for   more expressive posterior approximation  translating into better predictive results  In fact 
for all datasets we see that incorporating   new component
improves test log probability  and we see further improvement with additional components for most of the datasets 
Furthermore  in  ve of the datasets we see predictive performance surpass probabilistic backpropagation as new
components are added  This highlights VBoost   ability to
trade computation for improved accuracy  These empirical
results suggest that augmenting   Gaussian approximation
to include additional capacity can improve predictive performance in   BNN while retaining computational tractability 

  Comparison to NPVI

We also compare VBoost to nonparametric variational inference  NPVI   Gershman et al      similar mixture based blackbox variational method  NPVI derives  
tractable lower bound to the ELBO which is then approximately maximized  NPVI requires computing the Hessian

num comps
VBoost
NPVI

 
 
 

 
 
 

 
 
 

 
 
 

 
 
 

Table   ELBO values for VBoost and NPVI  higher is better 
Note that VBoost with   component is MFVI  All ELBO values are
computed using   Monte Carlo estimate with        samples
from the variational distribution  In NPVI  each component is  
spherical gaussian with   single   shared across all dimensions
  this limits the capacity of the approximation  requiring more
components  Note  VBoost greedily incorporates components 
while NPVI is rerun using   different number of components 

of the model for the ELBO approximation  so we limit our
comparison to the lower dimensional hierarchical models 
We also note that the NPVI ELBO approximation does not
fully integrate the ln     term against the variational approximation         when optimizing the mean parameters
of the approximation components  When we applied NPVI
to the baseball model  we discovered an instability in
the optimization of these mean parameters  which we veri 
 ed by  nding that map optimization diverges  Black box
VI  VBoost  and MCMC were not susceptible to this pathology  Consequently  we only compare NPVI to VBoost on
the frisk model  Because NPVI uses diagonal components  we restrict VBoost to use purely diagonal components
       In Table   we show marginal likelihood lower
bounds  comparing NPVI to VBoost with   varying number
of components  Even with   single component  the NPVI
objective tends to underperform  The NPVI component variance is spherical  limiting its capacity to represent posterior
correlations  Further  NPVI is approximately optimizing  
looser lower bound to the marginal likelihood  These two
factors explain why NPVI fails to match MFVI and VBoost 

  Discussion and Conclusion
We proposed VBoost    practical variational inference
method that constructs an increasingly expressive posterior
approximation and is applicable to   variety of Bayesian
models  We demonstrated the ability of VBoost to learn
rich representations of complex  highdimensional posteriors on   variety of real world statistical models  One avenue
for future work is incorporating  exible component distributions such as compositions of invertible maps  Rezende
  Mohamed    or auxiliary variable variational models  Maal   et al    We also plan to study approximation guarantees of the VBoost method and variance reduction techniques for our reparameterization gradient approach  Also  when optimizing parameters of   variational
family  recent work has shown that the natural gradient can
be more robust and lead to better optima  Hoffman et al 
  Johnson et al    Deriving and applying natural
gradient updates for mixture approximations could make
VBoost more ef cient 

Variational Boosting  Iteratively Re ning Posterior Approximations

Acknowledgements

The authors would like to thank Arjumand Masood  Mike
Hughes  and Finale DoshiVelez for helpful feedback  ACM
is supported by the Applied Mathematics Program within
the Of ce of Science Advanced Scienti   Computing Research of the      Department of Energy under contract No 
DEAC CH  NJF is supported by   Washington
Research Foundation Innovation Postdoctoral Fellowship in
Neuroengineering and Data Science  RPA is supported by
NSF IIS  and the Alfred    Sloan Foundation 

References
Bishop     Pattern Recognition and Machine Learning 

SpringerVerlag   

Blei        Kucukelbir     and McAuliffe        Variational inference    review for statisticians  arXiv preprint
arXiv   

Dempster        Laird        and Rubin        Maximum
likelihood from incomplete data via the EM algorithm 
Journal of the Royal Statistical Society  Series    methodological  pp     

Efron     and Morris     Data analysis using Stein   estimator and its generalizations  Journal of the American
Statistical Association     

Friedman        Greedy function approximation    gradient
boosting machine  Annals of Statistics   
 

Gelman     and Hill     Data Analysis Using Regression and
Multilevel Hierarchical Models  Cambridge University
Press   

Gelman     Fagan     and Kiss     An analysis of the
NYPD   stopand frisk policy in the context of claims of
racial bias  Journal of the American Statistical Association     

Gershman     Hoffman     and Blei        Nonparametric
In International Conference on

variational inference 
Machine Learning   

Golub        and Van Loan        Matrix Computations 

JHU Press   

Guo     Wang     Fan     Broderick     and Dunson       
Boosting variational inference  arXiv   

Harville        Matrix Algebra from   Statistician   Per 

spective  SpringerVerlag   

Hoffman        and Gelman     The NoU turn sampler 
adaptively setting path lengths in Hamiltonian monte
carlo  Journal of Machine Learning Research   
   

Hoffman        Blei        Wang     and Paisley       
Stochastic variational inference  Journal of Machine
Learning Research     

Jaakkola        and Jordan        Improving the mean  eld
approximation via the use of mixture distributions  In
Learning in Graphical Models  pp    Springer 
 

Johnson        Duvenaud        Wiltschko        and
Datta       and Adams        Composing graphical models with neural networks for structured representations
and fast inference  In Advances in Neural Information
Processing Systems   

Jordan        Ghahramani     Jaakkola        and Saul 
      An introduction to variational methods for graphical
models  Machine learning     

Kingma     and Ba     Adam    method for stochastic
optimization  In International Conference on Learning
Representations   

Kingma        and Welling     Autoencoding variational
Bayes  In International Conference on Learning Representations   

Li     Estimation of Mixture Models  PhD thesis  Yale

University  May  

Li        and Barron        Mixture density estimation 
In Advances in Neural Information Processing Systems 
 

Maal         nderby          nderby        and Winther 
   Auxiliary deep generative models  In International
Conference on Machine Learning   

Maclaurin     and Adams        Fire   Monte Carlo  Exact
MCMC with subsets of data  In Uncertainty in Arti cial
Intelligence   

Maclaurin     Duvenaud     and Adams        Autograd 
Reversemode differentiation of native python  ICML
workshop on Automatic Machine Learning     

Maclaurin     Duvenaud     Johnson     and Adams 
      Autograd  Reversemode differentiation of native
Python      URL http github com HIPS 
autograd 

Hern ndezLobato        and Adams        Probabilistic
backpropagation for scalable learning of Bayesian neural
networks   

Moore        Symmetrized variational inference  In NIPS
Workshop on Advances in Approximate Bayesian Inferece 
 

Variational Boosting  Iteratively Re ning Posterior Approximations

Ong          Nott        and Smith        Gaussian variational approximation with factor covariance structure 
arXiv preprint arXiv   

Rakhlin        Panchenko  and    Mukherjee  Risk bounds
for mixture density estimation  ESAIM  Probability and
Statistics     

Ranganath     Gerrish     and Blei        Black box
In International Conference on
variational inference 
Arti cial Intelligence and Statistics  pp     

Ranganath     Altosaar     Tran     and Blei        Operator variational inference  In Advances in Neural Information Processing Systems     

Ranganath     Tran     and Blei        Hierarchical variational models  In International Conference on Machine
Learning     

Rezende     and Mohamed     Variational inference with
normalizing  ows  In International Conference on Machine Learning  pp     

Salimans     and Knowles        Fixedform variational posterior approximation through stochastic linear regression 
Bayesian Analysis     

Seeger        Gaussian covariance and scalable variational inference  In International Conference on Machine
Learning   

Tolstikhin     Gelly     Bousquet     SimonGabriel      
and Schoelkopf     Adagan  Boosting generative models 
arXiv preprint arXiv   

Wainwright        and Jordan        Graphical models  exponential families  and variational inference  Foundations
and Trends in Machine Learning     

Welling     and Teh        Bayesian learning via stochastic
gradient Langevin dynamics  In International Conference
on Machine Learning   

Zhang     Sequential greedy approximation for certain
convex optimization problems  IEEE Transactions on
Information Theory     

