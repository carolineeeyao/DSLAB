Unifying Task Speci cation in Reinforcement Learning

Martha White  

Abstract

Reinforcement learning tasks are typically speci 
 ed as Markov decision processes  This formalism has been highly successful  though speci cations often couple the dynamics of the environment and the learning objective  This lack of modularity can complicate generalization of the task
speci cation  as well as obfuscate connections
between different task settings  such as episodic
and continuing  In this work  we introduce the
RL task formalism  that provides   uni cation
through simple constructs including   generalization to transitionbased discounting  Through  
series of examples  we demonstrate the generality
and utility of this formalism  Finally  we extend
standard learning constructs  including Bellman
operators  and extend some seminal theoretical
results  including approximation errors bounds 
Overall  we provide   wellunderstood and sound
formalism on which to build theoretical results
and simplify algorithm use and development 

  Introduction
Reinforcement learning is   formalism for trialand error
interaction between an agent and an unknown environment 
This interaction is typically speci ed by   Markov decision
process  MDP  which contains   transition model  reward
model  and potentially discount parameters   specifying  
discount on the sum of future values in the return  Domains
are typically separated into two cases  episodic problems
 nite horizon  and continuing problems  in nite horizon 
In episodic problems  the agent reaches some terminal state 
and is teleported back to   start state  In continuing problems  the agent interaction is continual  with   discount to
ensure    nite total reward       constant    
This formalism has   long and successful tradition  but is
limited in the problems that can be speci ed  Progressively
there have been additions to specify   broader range of ob 

 Department of Computer Science  Indiana University  Corre 

spondence to  Martha White  martha indiana edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

jectives  including options  Sutton et al    statebased
discounting  Sutton    Sutton et al    and interest functions  Reza and Sutton    Sutton et al   
These generalizations have particularly been driven by offpolicy learning and the introduction of general value functions for Horde  Sutton et al    White    where
predictive knowledge can be encoded as more complex prediction and control tasks  Generalizations to problem speci 
cations provide exciting learning opportunities  but can also
reduce clarity and complicate algorithm development and
theory  For example  options and general value functions
have signi cant overlap  but because of different terminology and formalization  the connections are not transparent 
Another example is the classic divide between episodic and
continuing problems  which typically require different convergence proofs  Bertsekas and Tsitsiklis    Tsitsiklis
and Van Roy    Sutton et al    and different algorithm speci cations 
In this work  we propose   formalism for reinforcement
learning task speci cation that uni es many of these generalizations  The focus of the formalism is to separate the speci cation of the dynamics of the environment and the speci 
 cation of the objective within that environment  Though
natural  this represents   signi cant change in the way tasks
are currently speci ed in reinforcement learning and has
important rami cations for simplifying implementation  algorithm development and theory  The paper consists of two
main contributions  First  we demonstrate the utility of this
formalism by showing uni cation of previous tasks speci 
 ed in reinforcement learning  including options  general
value functions and episodic and continuing  and further
providing case studies of utility  We demonstrate how to
specify episodic and continuing tasks with only modi cations to the discount function  without the addition of states
and modi cations to the underlying Markov decision process  This enables   uni cation that signi cantly simpli es
implementation and easily generalizes theory to cover both
settings  Second  we prove novel contraction bounds on the
Bellman operator for these generalized RL tasks  and show
that previous bounds for both episodic and continuing tasks
are subsumed by this more general result  Overall  our goal
is to provide an RL task formalism that requires minimal
modi cations to previous task speci cation  with signi cant
gains in simplicity and uni cation across common settings 

Unifying Task Speci cation in Reinforcement Learning

  Generalized problem formulation
We assume the agent interacts with an environment formalized by   Markov decision process  MDP        Pr 
where   is the set of states            is the set of actions 
and Pr              is the transition probability
function where Pr          is the probability of transitioning
from state   into state    when taking action      reinforcement learning task  RL task  is speci ed on top of these
transition dynamics  as the tuple           where
    is   set of policies             
  the reward function           

  speci es

reward received from          

                  is   transitionbased discount

function 

           is an interest function that speci es the

user de ned interest in   state 

Each task could have different reward functions within the
same environment  For example  in   navigation task within
an of ce  one agent could have the goal to navigate to the
kitchen and the other the conference room  For   reinforcement learning task  whether prediction or control    set or
class of policies is typically considered  For prediction  policy evaluation  we often select one policy and evaluate its
longterm discounted reward  For control  where   policy is
learned  the set of policies may consist of all policies parameterized by weights that specify the actionvalue from states 
with the goal to  nd the weights that yield the optimal policy  For either prediction or control in an RL task  we often
evaluate the return of   policy  the cumulative discounted
reward obtained from following that policy

Gt  

 
  Yj 

 st    at    st     Rt  

 Xi 
where   
    st    at    st        Note that this
subsumes the setting with   constant discount         
by using                for every          and so giving
Qi 
    st    at    st        
     
for       As another example  the end of the episode 
              making the product of these discounts
zero and so terminating the recursion  We further explain how transitionbased discount enables speci cation of
episodic tasks and discuss the utility of the generalization to
transitionbased discounting throughout this paper  Finally 
the interest function   speci es the degree of importance

  for       and  

 We describe   further probabilistic generalization in Appendix
   much of the treatment remains the same  but the notation becomes cumbersome and the utility obfuscated 

of each state for the task  For example  if an agent is only
interested in learning an optimal policy for   subset of the
environment  the interest function could be set to one for
those states and to zero otherwise 
We  rst explain the speci cation and use of such tasks  and
then de ne   generalized Bellman operator and resulting
algorithmic extensions and approximation bounds 

  Unifying episodic and continuing speci cation
The RL task speci cation enables episodic and continuing
problems to be easily encoded with only modi cation to the
transitionbased discount  Previous approaches  including
the absorbing state formulation  Sutton and Barto     
and statebased discounting  Sutton    Reza and Sutton 
  Sutton et al   van Hasselt    Section  
require special cases or modi cations to the set of states
and underlying MDP  coupling task speci cation and the
dynamics of the environment 
We demonstrate how transitionbased discounting seamlessly enables episodic or continuing tasks to be speci ed in
an MDP via   simple chain world  Consider the chain world
with three states       and    in Figure   The start state is
   and the two actions are right and left  The reward is   per
step  with termination occurring when taking action right
from state    which causes   transition back to state   
The discount is   for each step  unless speci ed otherwise 
The interest is set to   in all states  which is the typical case 
meaning performance from each state is equally important 
Figure    depicts the classical approach to specifying
episodic problems using an absorbing state  drawn as  
square  The agent reaches the goal transitioning right from
state   then forever stays in the absorbing state  receiving
  reward of zero  This encapsulates the de nition of the
return  but does not allow the agent to start another episode 
In practice  when this absorbing state is reached  the agent
is  teleported  to   start state to start another episode  This
episodic interaction can instead be represented the same way
as   continuing problem  by specifying   transitionbased
discount     right         This de nes the same return 
but now the agent simply transitions normally to   start state 
and no hypothetical states are added 
To further understand the equivalence  consider the updates
made by TD  see equation   Assume linear function
approximation with feature function        Rd  with
weights     Rd  When the agent takes action right from
   the agent transitions from    to    with probability one
and so           right         This correctly gives
     rt                       rt         
and correctly clears the eligibility trace for the next step

et         et              

Unifying Task Speci cation in Reinforcement Learning

The stationary distribution is also clearly equal to the original episodic task  since the absorbing state is not used in the
computation of the stationary distribution 
Another strategy is to still introduce hypothetical states  but
use statebased   as discussed in Figure     Unlike absorbing states  the agent does not stay inde nitely in the
hypothetical state  When the agent goes right from    it
transitions to hypothetical state    and then transition deterministically to the start state    with           As before 
we get the correct update  because                 Because the stationary distribution has some nonzero probability in the hypothetical state    we must set            
 or          Otherwise  the value of the hypothetical
state will be learned  wasting function approximation resources and potentially modifying the approximation quality
of the value in other states  We could have tried statebased
discounting without adding an additional state    However  this leads to incorrect value estimates  as depicted in
Figure     the relationship between transitionbased and
statebased is further discussed in Appendix    Overall 
to keep the speci cation of the RL task and the MDP separate  transitionbased discounting is necessary to enable the
uni ed speci cation of episodic and continuing tasks 

  Options as RL tasks
The options framework  Sutton et al    generically
covers   wide range of settings  with discussion about macroactions  option models  interrupting options and intraoption
value learning  These concepts at the time merited their
own language  but with recent generalizations can be more
conveniently cast as RL subtasks 
Proposition   An option  de ned as the tuple  Sutton et al 
  Section          with policy             
termination function            and an initiation set
    from which the option can be run  can be equivalently
cast as an RL task 

This proof is mainly de nitional  but we state it as an
explicit proposition for clarity  The discount function
                    for all          speci es termination  The interest function           if      and         
otherwise  focuses learning resources on the states of interest  If   value function for the policy is queried  it would
only make sense to query it from these states of interest 
If the policy for this option is optimized for this interest
function  the policy should only be run starting from       
as elsewhere will be poorly learned  The rewards for the RL
task correspond to the rewards associated with the MDP 
RL tasks generalize options  by generalizing termination
conditions to transitionbased discounting and by providing
degrees of interest rather than binary interest  Further  the
policies associated with RL subtasks can be used as macro 

  

  

  

  

 

    Absorbing state formulation 

  

  

  

    right        

    Transitionbased termination      right        

         

  

  

  

  

 

    Statebased termination with          

  

  

  

          or          

    Incorrect statebased termination 

Figure   Three different ways to represent episodic problems as continuing problems  For     the statebased discount cannot represent the episodic chain problem without adding states  To see why  consider the two cases
for representing termination            or          
For simplicity  assume that     right      for all
states              and transitions are deterministic  If
          then the value for taking action right from    is
     right                     and the value for taking action right from    is      right              
  which are both incorrect  If           then the value
of taking action right from    is                  
which is correct  However  the value of taking action left
from    is                   which is incorrect 

actions  to specify   semiMarkov decision process  Sutton
et al    Theorem  

  General value functions
In   similar spirit of abstraction as options  general value
functions were introduced for single predictive or goaloriented questions about the world  Sutton et al   
The idea is to encode predictive knowledge in the form
of value function predictions  with   collection or horde
of prediction demons  this constitutes knowledge  Sutton
et al    Modayil et al    White    The work
on Horde  Sutton et al    and nexting  Modayil et al 
  provide numerous examples of the utility of the types
of questions that can be speci ed by general value functions 
and so by RL tasks  because general value functions can

Unifying Task Speci cation in Reinforcement Learning

naturally can be speci ed as an RL task 
The generalization to RL tasks provide additional bene 
 ts for predictive knowledge  The separation into underlying MDP dynamics and task speci cation is particularly
useful in offpolicy learning  with the Horde formalism 
where many demons  value functions  are learned offpolicy 
These demons share the underlying dynamics  and even feature representation  but have separate prediction and control
tasks  keeping these separate from the MDP is key for avoiding complications  see Appendix    Transitionbased
discounts  over statebased discounts  additionally enable
the prediction of   change  caused by transitioning between
states  Consider the taxi domain  described more fully in
Section   where the agent   goal is to pick up and drop
off passengers in   grid world with walls  The taxi agent
may wish to predict the probability of hitting   wall  when
following   given policy  This can be encoded by setting
              if   movement action causes the agent to
remain in the same state  which occurs when trying to move
through   wall  In addition to episodic problems and hard termination  transitionbased questions also enable soft termination for transitions  Hard termination uses              
and soft termination               for some small positive
value   Soft terminations can be useful for incorporating
some of the value of   policy right after the soft termination 
If two policies are equivalent up to   transition  but have
very different returns after the transition    soft termination
will re ect that difference  We empirically demonstrate the
utility of soft termination in the next section 

  Demonstration in the taxi domain
To better ground this generalized formalism and provide
some intuition  we provide   demonstration of RL task speci cation  We explore different transitionbased discounts in
the taxi domain  Dietterich    Diuk et al    The
goal of the agent is to take   passenger from   source platform to   destination platform  depicted in Figure   The
agent receives   reward of   on each step  except for successful pickup and dropoff  giving reward   We modify
the domain to include the orientation of the taxi  with additional cost for not continuing in the current orientation 
This encodes that turning right  left or going backwards are
more costly than going forwards  with additional negative
rewards of     and   respectively  This additional
cost is further multiplied by   factor of   when there is  
passenger in the vehicle  For grid size   and the number
of pickup dropoff locations    the full state information is
   tuple     position of taxi                  position of
taxi                location of passenger                  
location of destination                orientation of car
                The location of the passenger can be in
one of the pickup dropoff locations  or in the taxi  Optimal

policies and value functions are computed iteratively  with
an extensive number of iterations 
Figure   illustrates three policies for one part of the taxi domain  obtained with three different discount functions  The
optimal policy is learned using   softtermination  which
takes into consideration the importance of approaching the
passenger location with the right orientation to minimize
turns after picking up the passenger    suboptimal policy is
in fact learned with hard termination  as the policy prefers to
greedily minimize turns to get to the passenger  For further
details  refer to the caption in Figure  
We also compare to   constant   which corresponds to
an average reward goal  as demonstrated in Equation  
The table in Figure     summarizes the results  Though in
theory it should in fact recognize the relative values of orientation before and after picking up   passenger  and obtain the
same solution as the softtermination policy  in practice we
 nd that numerical imprecision actually causes   suboptimal
policy to be learned  Because most of the rewards are negative per step  small differences in orientation can be more
dif cult to distinguish amongst for an in nite discounted
sum  This result actually suggests that having multiple subgoals  as one might have with RL subtasks  could enable
better chaining of decisions and local evaluation of the optimal action  The utility of learning with   smaller    has
been previously described  Jiang et al    however  here
we further advocate that enabling   that provides subtasks
is another strategy to improve learning 

  Objectives and algorithms
With an intuition for the speci cation of problems as RL
tasks  we now turn to generalizing some key algorithmic
concepts to enable learning for RL tasks  We  rst generalize
the de nition of the Bellman operator for the value function 
For   policy              de ne         Rn  
and         Rn  indexed by states           

         Xa  
           Xa  
      Xa  
      Xs  
              Xs  

      Pr         

      Pr                 

Pr                   

             

where      is the expected return  starting from   state
       To compute   value function that satis es this recursion  we de ne   Bellman operator  The Bellman operator
has been generalized to include statebased discounting and
  statebased trace parameter   Sutton et al    Eq   
   generalization to statebased trace parameters has been
considered  Sutton    Sutton and Barto      Reza and

Unifying Task Speci cation in Reinforcement Learning

   

   

 

 

 

 

 

 

 

   

Car

 

 

 

 

 

   

 

 

   

PICKUP

TOTAL
AND DROPOFF
TRANSSOFT      
TRANSHARD      
STATEBASED      
            
            
            
            
            
            
            
            

ADDED COST
FOR TURNS
     
     
     
     
     
     
     
     
     
     
     

 

 

 

 
 

 

 

 

 

 

 

 

 
 

 
 

 

 

 

 

Figure       The taxi domain  where the pickup dropoff platforms are at       and   The Passenger   is at the source
platform   outlined in black  The Car starts in   with orientation   as indicated the arrow  needs to bring the passenger to
destination   platform at   outlined in blue  In           there are simulated trajectories for policies learned using hard and soft
termination 
    The optimal strategy  with  Car in source  Pickup    in Car      and   discount   elsewhere  The sequence of taxi locations are
                with Pickup action                      Successful pickup and dropoff with total reward  
    For  Car in source  Pickup    in Car      the agent does not learn the optimal strategy  The agent minimizes orientation cost to
the subgoal  not accounting for orientation after picking up the passenger  Consequently  it takes more left turns after pickup  resulting
in more total negative reward  The sequence of locations are                 with Pickup action                 
    Successful pickup and dropoff with total reward  
    For statebased  Car in source and   in Car      the agent remains around the source and does not complete   successful dropoff 
The sequence of locations are                 with Pickup action              The agent enters the source and
pickups up the passenger  When it leaves to location   its value function indicates better value going to   because the negative
return will again be cutoff by  Car in source and   in Car      even without actually performing   pickup  Since the cost to get to the
destination is higher than the   return received from going back to     the agent stays around     inde nitely 
    Number of successful passenger pickup and dropoff  as well as additional cost incurred from turns  over   steps  with   runs 
reported for   range of constant    and the policies in Figure   Due to numerical imprecision  several constant discounts do not get close
enough to the passenger to pickup or dropoff  The statebased approach  that does not add additional states for termination  oscillates
after picking up the passenger  and so constantly gets negative reward 

We further generalize the de nition to the transitionbased
    insetting  The trace parameter           
 uences the  xed point and provides   modi ed  biased 
return  called the  return  this parameter is typically motivated as   biasvariance tradeoff parameter  Kearns and
Singh    Because the focus of this work is on generalizing the discount  we opt for   simple constant    in
the main body of the text  we provide generalizations to
transitionbased trace parameters in the appendix 
The generalized Bellman operator      Rn   Rn is

        

      

   

     Rn

          cP              
where   
          cP      
  

 
 

To see why this is the de nition of the Bellman operator 
we de ne the expected  return       Rn for   given
approximate value function  given by   vector     Rn 
           Xs  

                 cv   
                             cP         

Sutton    Sutton et al    Yu   

Continuing the recursion  we obtain 

      Xi 

 cP                        

        cP                            
The  xed point for this formula satis es         for the
Bellman operator de ned in Equation  
To see how this generalized Bellman operator modi es the
algorithms  we consider the extension to temporal difference algorithms  Many algorithms can be easily generalized by replacing    or    st  with transitionbased
 st  at  st  For example  the TD algorithm is generalized by setting the discount on each step to      
 st  at  st 

wt    wt      tet

  for some stepsize   
 

     rt         st       st  
et      cet      st 
 For   matrix   with maximum eigenvalue less than  

     Mi           We show in Lemma   that    satis 

 es this condition  implying  cP  satis es this condition and so
this in nite sum is wellde ned 

Unifying Task Speci cation in Reinforcement Learning

The generalized TD  xedpoint  under linear function approximation  can be expressed as   linear system Aw    

             cP            
             cP     

where each row in     Rn   corresponds to features for
  state  and     Rn   is   diagonal weighting matrix 
Typically      diag    where      Rn is the stationary
distribution for the behavior policy             
generating the stream of interaction  In onpolicy learning 
        With the addition of the interest function  this
weighting changes to     diag         where   denotes
elementwise product  Hadamard product  More recently 
  new algorithm  emphatic TD  ETD   Mahmood et al 
  Sutton et al    speci ed yet another weighting 
      where     diag    with           
     
Importantly  even for offpolicy sampling  with this
  
weighting    is guaranteed to be positive de nite  We show
in the next section that the generalized Bellman operator for
both the onpolicy and emphasis weighting is   contraction 
and further in the appendix that the emphasis weighting
with   transitionbased trace function is also   contraction 

  Generalized theoretical properties
In this section  we provide   general approach to incorporating transitionbased discounting into approximation bounds 
Most previous bounds have assumed   constant discount 
For example  ETD was introduced with statebased     however   Hallak et al    analyzed approximation error
bounds of ETD using   constant discount     By using matrix norms on      we generalize previous approximation
bounds to both the episodic and continuing case 
De ne the set of bounded vectors for the general space
of value functions          Rn   kvkD      Let
Fv    be   subspace of possible solutions       Fv  
 Xw     Rd kwk     
   The action space   and state space   are  nite 
   For polices                there exist unique
invariant distributions       such that          
and           This assumption is typically satis ed
by assuming an ergodic Markov chain for the policy 

   There exist transition          such that              
and                       This assumptions states
that the policy reaches some part of the space where
the discount is less than  

   Assume for any         if          for all     
where          then          for all                 
  For linear function approximation  this requires
    span                       

For weighted norm kvkD   pv Dv  if we can take the
square root of    the induced matrix norm is kP 
 kD  
   sp  where the spectral norm   ksp is the
     
largest singular value of the matrix  For simplicity of notation below  de ne sD   kP 
 kD  For any diagonalizable 
nonnegative matrix    the projection            onto
Fv exists and is de ned  Dz   argminv Fv kz   vkD 
  Approximation bound
We  rst prove that the generalized Bellman operator in
Equation   is   contraction  We extend the bound from
 Tsitsiklis and Van Roy    Hallak et al    for constant discount and constant trace parameter  to the general
transitionbased setting  The normed difference to the true
value function could be de ned by multiple weightings   
wellknown result is that for        the Bellman operator is   contraction for constant    and     Tsitsiklis and
Van Roy    recently  this has been generalized for  
variant of ETD to    still with constant parameters  Hallak
et al    We extend this result for transitionbased   for
both    and the transitionbased emphasis matrix   
Lemma   For        or       
 kD    

sD   kP 

Proof  For        let     Rn be the vector of row sums
for   

    

kP 

  

      Then for any        with      
     Xs  
          
   Xs  
 vk 
       Xs  
 Xs  
      Xs  
 Xs  
    Xs  
  Xs  
     diag        
   

      
   

      
   

        

      

  

  

    

    

where the  rst inequality follows from Jensen   inequality 
because   
      is normalized  Now because   has entries
that are less than   because the row sums of   
  are less
than   as shown in Lemma   and because each of the values
in the above product are nonnegative 

   diag        
   
     diag     
   
     diag     
             
     diag               
     diag           diag          
  kvk 

 

Unifying Task Speci cation in Reinforcement Learning

The last inequality is   strict inequality because        has
at least one positive entry where   has   positive entry 
Otherwise  if          everywhere with          then
      which we assumed was not the case 
 vkM   kvkM for any       giving
Therefore  kP 
    This exact same
kP 
proof follows through verbatim for the generalization of   
 
to transitionbased trace  
For        Again  we use Jensen   inequality  but now
rely on the property           Because of Assumption
   for some       for any nonnegative   

 kM   maxv Rn    kP 

 vkM
kvkM

where the last inequality follows from Lemma   By the
Banach Fixed Point theorem  because the Bellman operator
 
is   contraction under    it has   unique  xed point 
Theorem   If   satis es sD     then there exists       
such that  DT        and the error to the true value
function is bounded as

kv     kD       sD   Dv      kD 

 

For constant discount          and constant trace parameter          this bound reduces to the original bound
 Tsitsiklis and Van Roy    Lemma  

    sD   

        
      

 

Proof  Let   be the unique  xed point of  DT  which
exists by Lemma  
kv     kD   kv    Dv kD     Dv      kD

           Dv kD     Dv      kD
  kT       kD     Dv      kD
  kT       kD     Dv      kD
       kD     Dv      kD
  kP 
 kDkv     kD     Dv      kD
  kP 
  sDkv     kD     Dv      kD

where the second inequality is due to     vkD  
kT vkD  the second equality due to           and
the third equality due to                
       
because the    cancels  By rearranging terms  we get

    sD kv     kD             kD 

and since sD     we get the  nal result 
For constant        and     because         

sD   kP 

 kD

           

  kD    Xi 
 Xi 
 Xi 
 Xi 

           
         
        

           

 

                 
ckD Pi 

      

  
   

cPi

  
   

  
   

ckPi 

  kD

  
   
 

 

    Pr                     

    Pr             sd   

         Xs Xa
  sXs Xa
           Xk 

    

Therefore  because vectors       are also nonnegative 

       kP            
 Xk 

     kd      

         
                sd   

and so
kP 

  

      
   

    

        

      

      

      

      Xs  
    Xs  
 vk 
    Xs  
  Xs  
    Xs  
  Xs  
 cs Xs  
      
       
 cskvk 
 cs     since      

  

        

 
where      
Lemma   Under assumptions      the Bellman operator    in Equation   is   contraction under   norm
weighted by        or             for     

kT vkD   kvkD 

Further  because the projection    is   contraction 
 DT  is also   contraction and has   unique  xed point
 DT       for        
Proof  Because any vector   can be written            
kT vkD   kT        kD   kP 
        kD

 kDkvkD

  kP 
  kvkD

Unifying Task Speci cation in Reinforcement Learning

We provide generalizations to transitionbased trace parameters in the appendix  for the emphasis weighting  and also
discuss issues with generalizing to statebased termination
for   standard weighting with    We show that for any
transitionbased discounting function                
the above contraction results hold under emphasis weighting  We then provide   general form for an upper bound
 kD  for transitionbased discounting  based on the
on kP 
contraction properties of two matrices within   
  We further provide an example where the Bellman operator is
not   contraction even under the simpler generalization to
statebased discounting  and discuss the requirements for
the transitionbased generalizations to ensure   contraction
with weighting    This further motivates the emphasis
weighting as   more  exible scheme for convergence under general setting both offpolicy and transitionbased
generalization 

  Properties of TD algorithms
Using this characterization of   
  we can reexamine previous results for temporal difference algorithms that either
used statebased or constant discounts 

Convergence of Emphatic TD for RL tasks  We can extend previous convergence results for ETD  for learning
value functions and actionvalue functions  for the RL task
formalism  For policy evaluation  ETD and ELSTD  the
leastsquares version of ETD that uses the above de ned
  and   with        have both been shown to converge
with probability one  Yu    As an important component of this proof is convergence in expectation  which relies
on   being positive de nite  In particular  for appropriate
stepsizes     see  Yu    if   is positive de nite  the
iterative update is convergent wt    wt          Awt 
For the generalization to transitionbased discounting  convergence in expectation extends for the emphatic algorithms 
We provide these details in the appendix for completeness 
with theorem statement and proof in Appendix   and pseudocode in Appendix   

Convergence rate of LSTD  Tagorti and Scherrer
  recently provided convergence rates for LSTD 
for continuing tasks  for some        These results can be
extended to the episodic setting with the generic treatment
of   
  For example  in  Tagorti and Scherrer    Lemma
  which describes the sensitivity of LSTD  the proof extends by replacing the matrix      cP       cP 
 which they call   in their proof  with the generalization
  resulting instead in the constant
in the bound
  
rather than     
  Further  this generalizes convergence
  
rate results to emphatic LSTD  since   satis es the required convergence properties  with rates dictated by sM
rather than sD  for standard LSTD 

 sD

 

Insights into sD  Though the generalized form enables
uni ed episodic and continuing results  the resulting bound
parameter sD is more dif cult to interpret than for constant
       With    increasing to one  the constant     
in
  
the upper bound decreased to one  For    decreasing to zero 
the bound also decreases to one  These trends are intuitive 
as the problem should be simpler when    is small  and bias
should be less when    is close to one  More generally 
however  the discount can be small or large for different
transitions  making it more dif cult to intuit the trend 
To gain some intuition for sD  consider   random policy in
the taxi domain  with sD summarized in Table   As   
goes to one  sD goes to zero and so     sD  goes to one 
Some outcomes of note are that   hard or soft termination
for the pickup results in the exact same sD    for   constant
gamma of        the episodic discount had   slightly
smaller sD  and   increasing    has   much stronger effect
than including more terminations  Whereas  when we added
random terminations  so that from   and   of the states 
termination occurred on at least one path within   steps or
even more aggressively on every path within   steps  the
values of sD were similar 

 

 

 

  

 

      

EPISODIC TAXI

 
         
         
         
  SINGLE PATH
  SINGLE PATH          
         
  ALL PATHS
         
  ALL PATHS

Table   The sD values for increasing     with discount
settings described in the text 

  Discussion and conclusion
The goal of this paper is to provide intuition and examples
of how to use the RL task formalism  Consequently  to avoid
jarring the explanation  technical contributions were not emphasized  and in some cases included only in the appendix 
For this reason  we would like to highlight and summarize the technical contributions  which include   the introduction of the RL task formalism  and of transitionbased
discounts    an explicit characterization of the relationship between statebased and transitionbased discounting 
and   generalized approximation bounds  applying to both
episodic and continuing tasks  and   insights into and
issues with extending contraction results for both statebased and transitionbased discounting  Through intuition
from simple examples and fundamental theoretical extensions  this work provides   relatively complete characterization of the RL task formalism  as   foundation for use in
practice and theory 

Unifying Task Speci cation in Reinforcement Learning

Richard   Sutton  Hamid Maei Reza  Doina Precup  and
Shalab Bhatnagar  Fast gradientdescent methods for
temporaldifference learning with linear function approximation  International Conference on Machine Learning 
 

Richard   Sutton  Joseph Modayil  Michael Delp  Thomas
Degris  Patrick   Pilarski  Adam White  and Doina Precup  Horde    scalable realtime architecture for learning
knowledge from unsupervised sensorimotor interaction 
In International Conference on Autonomous Agents and
Multiagent Systems   

Richard   Sutton  Ashique Rupam Mahmood  Doina Precup 
and Hado van Hasselt    new   lambda  with interim forward view and Monte Carlo equivalence  In International
Conference on Machine Learning   

Richard   Sutton  Ashique Rupam Mahmood  and Martha
White  An emphatic approach to the problem of offpolicy
temporaldifference learning  The Journal of Machine
Learning Research   

Manel Tagorti and Bruno Scherrer  On the Rate of Convergence and Error Bounds for LSTD  In International
Conference on Machine Learning   

Johnathan   Tsitsiklis and Benjamin Van Roy  An analysis
of temporaldifference learning with function approximation  IEEE Transactions on Automatic Control   

Hado Philip van Hasselt  Insights in Reinforcement Learn 

ing  PhD thesis  Hado van Hasselt   

Harm van Seijen and Rich Sutton  True online TD lambda 
In International Conference on Machine Learning   

Adam White  Developing   predictive approach to knowl 

edge  PhD thesis  University of Alberta   

Huizhen Yu  Least Squares Temporal Difference Methods 
An Analysis under General Conditions  SIAM Journal on
Control and Optimization   

Huizhen Yu  On convergence of emphatic temporaldifference learning  In Annual Conference on Learning
Theory   

Acknowledgements
Thanks to Hado van Hasselt for helpful discussions about
transitionbased discounting  and probabilistic discounts 

References
Dimitri   Bertsekas and John   Tsitsiklis  Neurodynamic

programming  Athena Scienti   Press   

Thomas   Dietterich  Hierarchical Reinforcement Learning
with the MAXQ Value Function Decomposition  Journal
of Arti cial Intelligence Research   

Carlos Diuk  Andre Cohen  and Michael   Littman  An
objectoriented representation for ef cient reinforcement
learning  In International Conference on Machine Learning   

Assaf Hallak  Aviv Tamar    mi Munos  and Shie Mannor 
Generalized Emphatic Temporal Difference Learning 
BiasVariance Analysis  CoRR abs   

Nan Jiang  Alex Kulesza  Satinder   Singh  and Richard  
Lewis  The Dependence of Effective Planning Horizon
on Model Accuracy  International Conference on Autonomous Agents and Multiagent Systems   

Michael   Kearns and Satinder   Singh  BiasVariance
error bounds for temporal difference updates  In Annual
Conference on Learning Theory   

Ashique Rupam Mahmood  Huizhen Yu  Martha White  and
Richard   Sutton  Emphatic temporaldifference learning 
In European Workshop on Reinforcement Learning   

Joseph Modayil  Adam White  and Richard   Sutton  Multitimescale nexting in   reinforcement learning robot 
Adaptive Behavior   Animals  Animats  Software Agents 
Robots  Adaptive Systems   

Hamid Maei Reza and Richard   Sutton  GQ     general gradient algorithm for temporaldifference prediction
learning with eligibility traces  In AGI   

Richard   Sutton  TD models  Modeling the world at  
mixture of time scales  In International Conference on
Machine Learning   

Richard   Sutton and     Barto  Introduction to reinforce 

ment learning  MIT Press     

Richard   Sutton and     Barto  Reinforcement Learning 

An Introduction  MIT press     

Richard   Sutton  Doina Precup  and Satinder Singh  Between MDPs and semiMDPs    framework for temporal
abstraction in reinforcement learning  Arti cial intelligence   

