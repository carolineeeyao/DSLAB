Sharp Minima Can Generalize For Deep Nets

Laurent Dinh   Razvan Pascanu   Samy Bengio   Yoshua Bengio    

Abstract

Despite their overwhelming capacity to over   
deep learning architectures tend to generalize relatively well to unseen data  allowing them to be
deployed in practice  However  explaining why
this is the case is still an open area of research 
One standing hypothesis that is gaining popularity 
     Hochreiter   Schmidhuber   Keskar
et al    is that the  atness of minima of the
loss function found by stochastic gradient based
methods results in good generalization  This paper argues that most notions of  atness are problematic for deep models and can not be directly
applied to explain generalization  Speci cally 
when focusing on deep networks with recti er
units  we can exploit the particular geometry of parameter space induced by the inherent symmetries
that these architectures exhibit to build equivalent
models corresponding to arbitrarily sharper minima  Furthermore  if we allow to reparametrize
  function  the geometry of its parameters can
change drastically without affecting its generalization properties 

Introduction

 
Deep learning techniques have been very successful in
several domains  like object recognition in images     
Krizhevsky et al    Simonyan   Zisserman   
Szegedy et al    He et al    machine translation       Cho et al    Sutskever et al    Bahdanau
et al    Wu et al    Gehring et al    and speech
recognition       Graves et al    Hannun et al   
Chorowski et al    Chan et al    Collobert et al 
  Several arguments have been brought forward to justify these empirical results  From   representational point of
view  it has been argued that deep networks can ef ciently

 Universit   of Montr eal  Montr eal  Canada  DeepMind  London  United Kingdom  Google Brain  Mountain View  United
States  CIFAR Senior Fellow  Correspondence to  Laurent Dinh
 laurent dinh umontreal ca 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

approximate certain functions       Montufar et al   
Raghu et al    Other works      Dauphin et al   
Sagun et al    Choromanska et al    have looked
at the structure of the error surface to analyze how trainable
these models are  Finally  another point of discussion is
how well these models can generalize  Nesterov   Vial 
  Keskar et al    Zhang et al    These correspond  respectively  to low approximation  optimization and
estimation error as described by Bottou  
Our work focuses on the analysis of the estimation error  In
particular  different approaches had been used to look at the
question of why stochastic gradient descent results in solutions that generalize well  Bottou   LeCun    Bottou  
Bousquet    For example  Duchi et al    Nesterov
  Vial   Hardt et al    Bottou et al    Gonen   ShalevShwartz   rely on the concept of stochastic approximation or uniform stability  Bousquet   Elisseeff 
  Another conjecture that was recently  Keskar et al 
  explored  but that could be traced back to Hochreiter
  Schmidhuber   relies on the geometry of the loss
function around   given solution  It argues that  at minima 
for some de nition of  atness  lead to better generalization 
Our work focuses on this particular conjecture  arguing that
there are critical issues when applying the concept of  at
minima to deep neural networks  which require rethinking
what  atness actually means 
While the concept of  at minima is not well de ned  having
slightly different meanings in different works  the intuition
is relatively simple  If one imagines the error as   onedimensional curve    minimum is  at if there is   wide
region around it with roughly the same error  otherwise
the minimum is sharp  When moving to higher dimensional spaces  de ning  atness becomes more complicated 
In Hochreiter   Schmidhuber   it is de ned as the
size of the connected region around the minimum where the
training loss is relatively similar  Chaudhari et al   
relies  in contrast  on the curvature of the second order structure around the minimum  while Keskar et al    looks
at the maximum loss in   bounded neighbourhood of the
minimum  All these works rely on the fact that  atness
results in robustness to low precision arithmetic or noise in
the parameter space  which  using an minimum description
lengthbased argument  suggests   low expected over tting 

Sharp Minima Can Generalize For Deep Nets

However  several common architectures and parametrizations in deep learning are already at odds with this conjecture  requiring at least some degree of re nement in the
statements made  In particular  we show how the geometry of the associated parameter space can alter the ranking
between prediction functions when considering several measures of  atness sharpness  We believe the reason for this
contradiction stems from the Bayesian arguments about KLdivergence made to justify the generalization ability of  at
minima  Hinton   Van Camp    Indeed  KullbackLiebler divergence is invariant to change of parameters
whereas the notion of  atness  is not  The demonstrations
of Hochreiter   Schmidhuber   are approximately
based on   Gibbs formalism and rely on strong assumptions
and approximations that can compromise the applicability
of the argument  including the assumption of   discrete
function space 

  De nitions of  atness sharpness

Figure   An illustration of the notion of  atness  The
loss   as   function of   is plotted in black  If the height
of the red area is   the width will represent the volume
 atness  If the width is   the height will then represent
the  sharpness  Best seen with colors 

For conciseness  we will restrict ourselves to supervised
scalar output problems  but several conclusions in this paper can apply to other problems as well  We will consider
  function   that takes as input an element   from an input space   and outputs   scalar    We will denote by   
the prediction function  This prediction function will be
parametrized by   parameter vector   in   parameter space
  Often  this prediction function will be overparametrized
and two parameters    cid      that yield the same prediction function everywhere                    cid    are
called observationally equivalent  The model is trained to
minimize   continuous loss function   which takes as argument the prediction function    We will often think of the
loss   as   function of   and adopt the notation   
The notion of  atness sharpness of   minimum is relative 
therefore we will discuss metrics that can be used to compare the relative  atness between two minima  In this section we will formalize three used de nitions of  atness in

the literature 
Hochreiter   Schmidhuber   de nes    at minimum
as    large connected region in weight space where the
error remains approximately constant  We interpret this
formulation as follows 
De nition   Given         minimum   and   loss   
we de ne          as the largest  using inclusion as the
partial order over the subsets of   connected set containing
  such that  cid               cid           The  
 atness will be de ned as the volume of          We will
call this measure the volume  atness 

In Figure            will be the purple line at the top of
the red area if the height is   and its volume will simply be
the length of the purple line 
Flatness can also be de ned using the local curvature of the
loss function around the minimum if it is   critical point  
Chaudhari et al    Keskar et al    suggest that
this information is encoded in the eigenvalues of the Hessian 
However  in order to compare how  at one minimum versus
another  the eigenvalues need to be reduced to   single
number  Here we consider the spectral norm and trace of
the Hessian  two typical measurements of the eigenvalues
of   matrix 
Additionally Keskar et al    de nes the notion of  
sharpness  In order to make proofs more readable  we will
slightly modify their de nition  However  because of norm
equivalence in  nite dimensional space  our results will
transfer to the original de nition in full space as well  Our
modi ed de nition is the following 
De nition   Let      be an Euclidean ball centered
on   minimum   with radius   Then  for   nonnegative
valued loss function    the  sharpness will be de ned as
proportional to

In Figure   if the width of the red area is   then the height
of the red area is max cid   
 sharpness can be related to the spectral norm of the Hessian  Indeed    secondorder Taylor expansion of   around
  critical point minimum is written

  cid        

 cid         cid      

 
 

    cid cid     cid 
 

In this second order approximation  the  sharpness at  

 In this paper  we will often assume that is the case when
dealing with Hessianbased measures in order to have them wellde ned 

max cid   

      

 cid   cid      cid 
 cid   cid      cid 

 

Sharp Minima Can Generalize For Deep Nets

would be

 cid cid cid cid cid cid   cid cid cid cid cid cid 
 cid      cid 

 

  Properties of Deep Recti ed

Networks

Before moving forward to our results  in this section we  rst
introduce the notation used in the rest of paper  Most of our
results  for clarity  will be on the deep recti ed feedforward
networks with   linear output layer that we describe below 
though they can easily be extended to other architectures
      convolutional  etc 
De nition   Given   weight matrices        with nk  
   nk  the output   of   deep
recti ed feedforward networks with   linear output layer is 

dim cid vec   cid  and      cid  

 cid   rect      cid       

 cid       

 cid 

 rect

     rect

where

    is the input to the model    highdimensional vector
   rect is the recti ed elementwise activation function  Jarrett et al    Nair   Hinton   
Glorot et al    which is the positive part
 zi  

 cid   max zi     

  vec reshapes   matrix into   vector 

Note that in our de nition we excluded the bias terms  usually found in any neural architecture  This is done mainly
for convenience  to simplify the rendition of our arguments 
However  the arguments can be extended to the case that
includes biases  see Appendix    Another choice is that of
the linear output layer  Having an output activation function does not affect our argument either  since the loss is  
function of the output activation  it can be rephrased as  
function of linear preactivation 
Deep recti er models have certain properties that allows
us in section   to arbitrary manipulate the  atness of  
minimum 
An important topic for optimization of neural networks is
understanding the nonEuclidean geometry of the parameter space as imposed by the neural architecture  see  for
example Amari    In principle  when we take   step
in parameter space what we expect to control is the change
in the behavior of the model       the mapping of the input
  to the output    In principle we are not interested in
the parameters per se  but rather only in the mapping they
represent 

Figure   An illustration of the effects of nonnegative homogeneity  The graph depicts level curves of the behavior
of the loss   embedded into the two dimensional parameter space with the axis given by   and   Speci cally 
each line of   given color corresponds to the parameter assignments     that result observationally in the same
prediction function    Best seen with colors 

If one de nes   measure for the change in the behavior of
the model  which can be done under some assumptions 
then  it can be used to de ne  at any point in the parameter
space    metric that says what is the equivalent change in
the parameters for   unit of change in the behavior of the
model  As it turns out  for neural networks  this metric is
not constant over   Intuitively  the metric is related to
the curvature  and since neural networks can be highly nonlinear  the curvature will not be constant  See Amari  
Pascanu   Bengio   for more details  Coming back
to the concept of  atness or sharpness of   minimum  this
metric should de ne the  atness 
However  the geometry of the parameter space is more complicated  Regardless of the measure chosen to compare two
instantiations of   neural network  because of the structure
of the model  it also exhibits   large number of symmetric con gurations that result in exactly the same behavior 
Because the recti er activation has the nonnegative homogeneity property  as we will see shortly  one can construct  
continuum of points that lead to the same behavior  hence
the metric is singular  Which means that one can exploit
these directions in which the model stays unchanged to
shape the neighbourhood around   minimum in such   way
that  by most de nitions of  atness  this property can be
controlled  See Figure   for   visual depiction  where the
 atness  given here as the distance between the different
level curves  can be changed by moving along the curve 

Sharp Minima Can Generalize For Deep Nets

Let us rede ne  for convenience  the nonnegative homogeneity property  Neyshabur et al    Lafond et al   
below  Note that beside this property  the reason for studying the recti ed linear activation is for its widespread adoption  Krizhevsky et al    Simonyan   Zisserman   
Szegedy et al    He et al   
De nition     given   function   is nonnegative homogeneous if

                        

  Deep Recti ed networks and  at

minima

In this section we exploit
the resulting strong nonidenti ability to showcase   few shortcomings of some
de nitions of  atness  Although  scale transformation
does not affect the function represented  it allows us to signi cantly decrease several measures of  atness  For another
de nition of  atness   scale transformation show that all
minima are equally  at 

 
Theorem   The recti ed function  rect      max     
is nonnegative homogeneous 

  Volume  atness
Theorem   For   onehidden layer recti ed neural network
of the form

Proof  Follows trivially from the constraint that      
given that                iff      

For   deep recti ed neural network it means that 

 cid      cid         rect          

 rect

meaning that for this one  hidden  layer neural network 
the parameters     is observationally equivalent to
    This observational equivalence similarly holds
for convolutional layers 
Given this nonnegative homogeneity  if      cid     

then  cid         cid  is an in nite set of obser 

vationally equivalent parameters  inducing   strong nonidenti ability in this learning scenario  Other models like
deep linear networks  Saxe et al    leaky recti ers  He
et al    or maxout networks  Goodfellow et al   
also have this nonnegative homogeneity property 
In what follows we will rely on such transformations  in
particular we will rely on the following de nition 
De nition   For   single hidden layer recti er feedforward
network we de ne the family of transformations

          cid     

which we refer to as    scale transformation 

Note that    scale transformation will not affect the generalization  as the behavior of the function is identical  Also
while the transformation is only de ned for   single layer
recti ed feedforward network  it can trivially be extended
to any architecture having   single recti ed network as  
submodule         deep recti ed feedforward network  For
simplicity and readability we will rely on this de nition 

     rect          

and   minimum         such that    cid    and    cid   
               has an in nite volume 
We will not consider the solution   where any of the weight
matrices     is zero        or       as it results
in   constant function which we will assume to give poor
training performance  For       the  scale transformation
          cid      has Jacobian determinant

      where once again      dim cid vec cid  and     
dim cid vec cid  Note that the Jacobian determinant of this

linear transformation is the change in the volume induced
by    and              We show below that there is
  connected region containing   with in nite volume and
where the error remains approximately constant 

Proof  We will  rst introduce   small region with approximately constant error around   with nonzero volume  Given
      and if we consider the loss function continuous with
respect to the parameter           is an open set containing
  Since we also have    cid    and    cid    let       such
that the    ball        is in          and has empty
intersection with  cid   cid 
      Let                 the
volume of       
Since the Jacobian determinant of    is the multiplicative
change of induced by    the volume of   
       If     cid     we can arbitrarily grow the volume
of   
by having   tends to   if         or to   otherwise 
If          cid        cid 

 cid       cid  is
 cid       cid  with error within an  interval of   

has volume    Let
    cid  is   connected region
where the error remains approximately constant       within
an  interval of   

  cid     cid 

      

      

 cid 

 cid 

 cid 

 cid 

 cid    cid 

Let      

 cid cid  
 cid cid     Since
                        

Sharp Minima Can Generalize For Deep Nets

curvature       Desjardins et al    Salimans   Kingma 
  In this section we look at two widely used measures
of the Hessian  the spectral radius and trace  showing that
either of these values can be manipulated without actually
changing the behavior of the function  If the  atness of  
minimum is de ned by any of these quantities  then it could
also be easily manipulated 
Theorem   The gradient and Hessian of the loss   with
respect to   can be modi ed by   

 cid     cid   cid  and     cid    have the same volume 
 cid     cid   cid        will there 

Figure   An illustration of how we build different disjoint volumes using   
In this twodimensional example    
    cid      
fore be   sequence of disjoint constant volumes    cid  will
therefore have an in nite volume  Best seen with colors 

 cid     cid   cid     

 

  

where   is the Cartesian set product  we have

 cid       cid                   
 cid       cid                see Figure  
 cid       cid       
 cid       cid     
 cid     cid   cid      cid  The vol 

Therefore    
Similarly           
are disjoint and have volume    We have also
   
ume of   cid  is then lower bounded by                  
 
and is therefore in nite           has then in nite volume
too  making the volume  atness of   in nite 

 cid     cid   cid       

 

This theorem can generalize to recti ed neural networks in
general with   similar proof  Given that every minimum
has an in nitely large region  volumewise  in which the
error remains approximately constant  that means that every
minimum would be in nitely  at according to the volume
 atness  Since all minima are equally  at  it is not possible
to use volume  atness to gauge the generalization property
of   minimum 

  Hessianbased measures
The nonEuclidean geometry of the parameter space  coupled with the manifolds of observationally equal behavior of
the model  allows one to move from one region of the parameter space to another  changing the curvature of the model
without actually changing the function  This approach has
been used with success to improve optimization  by moving
from   region of high curvature to   region of well behaved

Proof 

           

we have then by differentiation

             

               

 cid   In 
 cid   In 

 

 

 

 In 
 
 In 

and
     
 
 In 

 cid   In 

 

 

 cid 

     

 cid   In 

 

 
 In 

 cid 
 cid 

 cid 

 

Sharpest direction Through these transformations we
can easily  nd  for any critical point which is   minimum
with nonzero Hessian  an observationally equivalent parameter whose Hessian has an arbitrarily large spectral norm 
Theorem   For   onehidden layer recti ed neural network
of the form

     rect          

and critical point         being   minimum
 cid            
for    such that    

 cid cid cid cid cid cid   cid   cid cid cid cid cid cid cid      where cid cid cid cid cid cid   cid   cid cid cid cid cid cid cid  is
the spectral norm of    cid   cid 

Proof  The trace of   symmetric matrix is the sum of its
eigenvalues and   real symmetric matrix can be diagonalized
in    therefore if the Hessian is nonzero  there is one nonzero positive diagonal element  Without loss of generality 
we will assume that this nonzero element of value      
corresponds to an element in   Therefore the Frobenius

 cid   In 

 

 cid 

 

 
 In 

norm cid cid cid cid cid cid   cid   cid cid cid cid cid cid cid   of
 cid   In 

     
 
 In 

 cid 

 

 

     

Sharp Minima Can Generalize For Deep Nets

is lower bounded by  
Since all norms are equivalent in  nite dimension  there
exists   constant       such that             for all
    we are

symmetric matrices    So by picking      cid    
guaranteed that cid cid cid cid cid cid   cid   cid cid cid cid cid cid cid      

Any minimum with nonzero Hessian will be observationally equivalent to   minimum whose Hessian has an arbitrarily large spectral norm  Therefore for any minimum
in the loss function  if there exists another minimum that
generalizes better then there exists another minimum that
generalizes better and is also sharper according the spectral
norm of the Hessian  The spectral norm of critical points 
Hessian becomes as   result less relevant as   measure of
potential generalization error  Moreover  since the spectral
norm lower bounds the trace for   positive semide nite
symmetric matrix  the same conclusion can be drawn for
the trace 
Further properties of the Hessian are analyzed in Appendix 

   sharpness

Figure   An illustration of how we exploit nonidenti ability and its particular geometry to obtain sharper
minima  although   is far from the       line  the observationally equivalent parameter  cid  is closer  The green and red
circle centered on each of these points have the same radius 
Best seen with colors 

We have rede ned for       the  sharpness of Keskar et al 
  as follow

 cid   cid      cid 

max cid   

      

where      is the Euclidean ball of radius   centered
on   This modi cation will demonstrate more clearly the
issues of that metric as   measure of probable generalization  If we use       and     corresponding to  
nonconstant function          cid    and    cid    then we can
  We will now consider the observationde ne      cid cid 
   
ally equivalent parameter           cid cid 
Given that  cid cid     cid cid  we have that      
  
hood at least as high as the best constantvalued function 

 cid    cid  making the maximum loss in this neighbor 

incurring relatively high sharpness  Figure   provides  
visualization of the proof 
For recti ed neural network every minimum is observationally equivalent to   minimum that generalizes as well but
with high  sharpness  This also applies when using the
fullspace  sharpness used by Keskar et al    We
can prove this similarly using the equivalence of norms
in  nite dimensional vector spaces and the fact that for
                        see Keskar et al    We
have not been able to show   similar problem with random
subspace  sharpness used by Keskar et al        
  restriction of the maximization to   random subspace 
which could relate to the notion of wide valleys described
by Chaudhari et al   

By exploiting the nonEuclidean geometry and nonidenti ability of recti ed neural networks  we were able to
demonstrate some of the limits of using typical de nitions of
minimum    atness as core explanation for generalization 

  Allowing reparametrizations
In the previous section   we explored the case of    xed
parametrization  that of deep recti er models  In this section
we demonstrate   simple observation  If we are allowed to
change the parametrization of some function    we can
obtain arbitrarily different geometries without affecting how
the function evaluates on unseen data  The same holds for
reparametrization of the input space  The implication is that
the correlation between the geometry of the parameter space
 and hence the error surface  and the behavior of   given
function is meaningless if not preconditioned on the speci  
parametrization of the model 

  Model reparametrization
One thing that needs to be considered when relating  atness
of minima to their probable generalization is that the choice
of parametrization and its associated geometry are arbitrary 
Since we are interested in  nding   prediction function in  
given family of functions  no reparametrization of this family should in uence generalization of any of these functions 
Given   bijection   onto   we can de ne new transformed
parameter        Since   and   represent in different
space the same prediction function  they should generalize
as well 
Let   call            the loss function with respect to the
new parameter   We generalize the derivation of Subsec 

Sharp Minima Can Generalize For Deep Nets

critical point becomes

              cid   cid   

This means that by reparametrizing the problem we can
modify to   large extent the geometry of the loss function so
as to have sharp minima of   in   correspond to  at minima
of    in        and conversely  Figure   illustrates that
point in one dimension  Several practical  Dinh et al   
Rezende   Mohamed    Kingma et al    Dinh
et al    and theoretical works  Hyv arinen   Pajunen 
  show how powerful bijections can be  We can also
note that the formula for the transformed Hessian at   critical
point also applies if   is not invertible    would just need
to be surjective over   in order to cover exactly the same
family of prediction functions

             fg        

We show in Appendix  bijections that allow us to perturb
the relative  atness between    nite number of minima 
Instances of commonly used reparametrization are batch
normalization  Ioffe   Szegedy    or the virtual batch
normalization variant  Salimans et al    and weight
normalization  Badrinarayanan et al    Salimans  
Kingma    Arpit et al    Im et al    have
plotted how the loss function landscape was affected by
batch normalization  However  we will focus on weight normalization reparametrization as the analysis will be simpler 
but the intuition with batch normalization will be similar 
Weight normalization reparametrizes   nonzero weight  
as         cid   cid 
with the new parameter being the scale   and
the unnormalized weight    cid   
Since we can observe that   is invariant to scaling of   
reasoning similar to Section   can be applied with the sim 
       cid     for    cid    Moreover 
pler transformations    cid 
since this transformation is   simpler isotropic scaling  the
conclusion that we can draw can be actually more powerful
with respect to   

  every minimum has in nite volume  sharpness 
  every minimum is observationally equivalent to an
in nitely sharp minimum and to an in nitely  at minimum when considering nonzero eigenvalues of the
Hessian 

  every minimum is observationally equivalent to   minimum with arbitrarily low fullspace and random subspace  sharpness and   minimum with high fullspace
 sharpness 

This further weakens the link between the  atness of  
minimum and the generalization property of the associated
prediction function when   speci   parameter space has not
been speci ed and explained beforehand 

    Loss function with default parametrization

    Loss function with reparametrization

    Loss function with another reparametrization

Figure     onedimensional example on how much the
geometry of the loss function depends on the parameter
space chosen  The xaxis is the parameter value and the
yaxis is the loss  The points correspond to   regular grid in
the default parametrization  In the default parametrization 
all minima have roughly the same curvature but with  
careful choice of reparametrization  it is possible to turn
  minimum signi cantly  atter or sharper than the others 
Reparametrizations in this  gure are of the form        
             where              
  and   is shown with
the red vertical line 

tion  

       cid   cid 

           cid   cid   
                cid   cid   
   cid   cid      therefore the transformed Hessian at  

     cid   cid   

At   differentiable critical point  we have by de nition

Sharp Minima Can Generalize For Deep Nets

Input representation

 
As we conclude that the notion of  atness for   minimum in
the loss function by itself is not suf cient to determine its
generalization ability in the general case  we can choose to
focus instead on properties of the prediction function instead 
Motivated by some work in adversarial examples  Szegedy
et al    Goodfellow et al    for deep neural networks  one could decide on its generalization property by
analyzing the gradient of the prediction function on examples  Intuitively  if the gradient is small on typical points
from the distribution or has   small Lipschitz constant  then
  small change in the input should not incur   large change
in the prediction 
But this in nitesimal reasoning is once again very dependent
of the local geometry of the input space  For an invertible
preprocessing        feature standardization  whitening
or gaussianization  Chen   Gopinath    we will call
           the prediction function on the preprocessed input
        We can reproduce the derivation in Section  
to obtain

 cid   cid   

   
 uT

 cid   cid   

 uT    

  
 xT

As we can alter signi cantly the relative magnitude of the
gradient at each point  analyzing the amplitude of the gradient of the prediction function might prove problematic if the
choice of the input space have not been explained beforehand  This remark applies in applications involving images 
sound or other signals with invariances  Larsen et al   
For example  Theis et al    show for images how  
small drift of one to four pixels can incur   large difference
in terms of    norm 

  Discussion
It has been observed empirically that minima found by standard deep learning algorithms that generalize well tend
to be  atter than found minima that did not generalize
well  Chaudhari et al    Keskar et al    However  when following several de nitions of  atness  we have
shown that the conclusion that  at minima should generalize
better than sharp ones cannot be applied as is without further context  Previously used de nitions fail to account for
the complex geometry of some commonly used deep architectures  In particular  the nonidenti ability of the model
induced by symmetries  allows one to alter the  atness of  
minimum without affecting the function it represents  Additionally the whole geometry of the error surface with respect
to the parameters can be changed arbitrarily under different
parametrizations  In the spirit of  Swirszcz et al    our
work indicates that more care is needed to de ne  atness
to avoid degeneracies of the geometry of the model under
study  Also such   concept can not be divorced from the

particular parametrization of the model or input space 

Acknowledgements
The authors would like to thank Grzegorz  Swirszcz for an
insightful discussion on the paper  Harm De Vries  Yann
Dauphin  Jascha SohlDickstein and   esar Laurent for useful discussions about optimization  Danilo Rezende for explaining universal approximation using normalizing  ows
and Kyle Kastner  Adriana Romero  Junyoung Chung  Nicolas Ballas  Aaron Courville  George Dahl  Yaroslav Ganin 
Prajit Ramachandran       glar   ulc ehre  Ahmed Touati and
the ICML reviewers for useful feedback 

References
Amari  ShunIchi  Natural gradient works ef ciently in learning 

Neural Comput     

Arpit  Devansh  Zhou  Yingbo  Kota  Bhargava    and Govindaraju  Venu  Normalization propagation    parametric technique for removing internal covariate shift in deep networks 
arXiv preprint arXiv   

Bach  Francis    and Blei  David     eds  Proceedings of the
 nd International Conference on Machine Learning  ICML
  Lille  France    July   volume   of JMLR Workshop and Conference Proceedings    JMLR org  URL
http jmlr org proceedings papers   

Badrinarayanan  Vijay  Mishra  Bamdev  and Cipolla  Roberto 
Understanding symmetries in deep networks  arXiv preprint
arXiv   

Bahdanau  Dzmitry  Cho  Kyunghyun  and Bengio  Yoshua  Neural
machine translation by jointly learning to align and translate  In
ICLR  arXiv   

Bottou    eon  Largescale machine learning with stochastic gradient descent  In Proceedings of COMPSTAT  pp   
Springer   

Bottou    eon and Bousquet  Olivier  The tradeoffs of large
scale learning 
In Platt       Koller     Singer     and
Roweis      eds  Advances in Neural Information Processing Systems  volume   pp    NIPS Foundation
 http books nips cc    URL http leon bottou 
org papers bottoubousquet 

Bottou    eon and LeCun  Yann  Online learning for very large
datasets  Applied Stochastic Models in Business and Industry 
    URL http leon bottou org 
papers bottoulecun   

Bottou    eon  Curtis  Frank    and Nocedal  Jorge  Optimization methods for largescale machine learning  arXiv preprint
arXiv   

Bousquet  Olivier and Elisseeff  Andr    Stability and generalization  Journal of Machine Learning Research   Mar 
 

Sharp Minima Can Generalize For Deep Nets

Chan  William  Jaitly  Navdeep  Le  Quoc    and Vinyals  Oriol 
Listen  attend and spell    neural network for large vocabIn   IEEE Inulary conversational speech recognition 
ternational Conference on Acoustics  Speech and Signal Processing  ICASSP   Shanghai  China  March    
pp    IEEE    ISBN   doi 
 ICASSP  URL http dx doi 
org ICASSP 

Chaudhari  Pratik  Choromanska  Anna  Soatto  Stefano  LeCun  Yann  Baldassi  Carlo  Borgs  Christian  Chayes  Jennifer  Sagun  Levent  and Zecchina  Riccardo  Entropysgd 
In ICLR 
Biasing gradient descent into wide valleys 
arXiv   

Chen  Scott Saobing and Gopinath  Ramesh    Gaussianization  In
Leen        Dietterich        and Tresp      eds  Advances in
Neural Information Processing Systems   pp    MIT
Press    URL http papers nips cc paper 
 gaussianization pdf 

Cho  Kyunghyun  van Merrienboer  Bart    ulc ehre     aglar  Bahdanau  Dzmitry  Bougares  Fethi  Schwenk  Holger  and Bengio  Yoshua  Learning phrase representations using RNN
encoderdecoder for statistical machine translation  In Moschitti  Alessandro  Pang  Bo  and Daelemans  Walter  eds 
Proceedings of the   Conference on Empirical Methods in
Natural Language Processing  EMNLP   October  
  Doha  Qatar    meeting of SIGDAT    Special Interest
Group of the ACL  pp    ACL    ISBN  
  URL http aclweb org anthology 
      pdf 

Choromanska  Anna  Henaff  Mikael  Mathieu  Micha el  Arous 
  erard Ben  and LeCun  Yann  The loss surfaces of multilayer
networks  In AISTATS   

Chorowski  Jan    Bahdanau  Dzmitry  Serdyuk  Dmitriy  Cho 
Kyunghyun  and Bengio  Yoshua  Attentionbased models for
speech recognition  In Advances in Neural Information Processing Systems  pp     

Collobert  Ronan  Puhrsch  Christian  and Synnaeve  Gabriel 
Wav letter  an endto end convnetbased speech recognition
system  arXiv preprint arXiv   

Gehring  Jonas  Auli  Michael  Grangier  David  and Dauphin 
Yann      convolutional encoder model for neural machine
translation  arXiv preprint arXiv   

Glorot  Xavier  Bordes  Antoine  and Bengio  Yoshua  Deep sparse
recti er neural networks  In Aistats  volume   pp     

Gonen  Alon and ShalevShwartz  Shai  Fast rates for empirical
risk minimization of strict saddle problems  arXiv preprint
arXiv   

Goodfellow  Ian    WardeFarley  David  Mirza  Mehdi  Courville 
Aaron    and Bengio  Yoshua  Maxout networks  ICML    
   

Goodfellow  Ian    Shlens  Jonathon  and Szegedy  Christian  Explaining and harnessing adversarial examples  In ICLR 
arXiv   

Graves  Alex  Mohamed  Abdelrahman  and Hinton  Geoffrey 
Speech recognition with deep recurrent neural networks  In
Acoustics  speech and signal processing  icassp    ieee
international conference on  pp    IEEE   

Hannun  Awni    Case  Carl  Casper  Jared  Catanzaro  Bryan 
Diamos  Greg  Elsen  Erich  Prenger  Ryan  Satheesh  Sanjeev  Sengupta  Shubho  Coates  Adam  and Ng  Andrew   
Deep speech  Scaling up endto end speech recognition  CoRR 
abs    URL http arxiv org abs 
 

Hardt  Moritz  Recht  Ben  and Singer  Yoram  Train faster  generalize better  Stability of stochastic gradient descent  In Balcan 
MariaFlorina and Weinberger  Kilian     eds  Proceedings
of the  nd International Conference on Machine Learning 
ICML   New York City  NY  USA  June     volume   of JMLR Workshop and Conference Proceedings  pp 
  JMLR org    URL http jmlr org 
proceedings papers   hardt html 

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian 
Delving deep into recti ers  Surpassing humanlevel performance on imagenet classi cation  In Proceedings of the IEEE
international conference on computer vision  pp   
 

Dauphin  Yann    Pascanu  Razvan    ulc ehre     aglar  Cho 
KyungHyun  Ganguli  Surya  and Bengio  Yoshua  Identifying
and attacking the saddle point problem in highdimensional
nonconvex optimization  NIPS   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian 
Deep residual learning for image recognition  In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition  pp     

Desjardins  Guillaume  Simonyan  Karen  Pascanu  Razvan  and

Kavukcuoglu  Koray  Natural neural networks  NIPS   

Dinh  Laurent  Krueger  David  and Bengio  Yoshua  Nice  Nonlinear independent components estimation  arXiv preprint
arXiv   

Dinh  Laurent  SohlDickstein  Jascha  and Bengio  Samy  Density
estimation using real nvp  In ICLR  arXiv 
 

Duchi  John  Hazan  Elad  and Singer  Yoram  Adaptive subgradient methods for online learning and stochastic optimization 
Journal of Machine Learning Research   Jul 
 

Hinton  Geoffrey   and Van Camp  Drew  Keeping the neural
networks simple by minimizing the description length of the
In Proceedings of the sixth annual conference on
weights 
Computational learning theory  pp    ACM   

Hochreiter  Sepp and Schmidhuber    urgen  Flat minima  Neural

Computation     

Hyv arinen  Aapo and Pajunen  Petteri  Nonlinear independent
component analysis  Existence and uniqueness results  Neural
Networks     

Im  Daniel Jiwoong  Tao  Michael  and Branson  Kristin  An
empirical analysis of deep network loss surfaces  arXiv preprint
arXiv   

Sharp Minima Can Generalize For Deep Nets

Ioffe  Sergey and Szegedy  Christian 

Batch normalization  Accelerating deep network training by reducing internal covariate shift 
In Bach   Blei   pp   
  URL http jmlr org proceedings papers 
  ioffe html 

Jarrett  Kevin  Kavukcuoglu  Koray  LeCun  Yann  et al  What
is the best multistage architecture for object recognition  In
Computer Vision    IEEE  th International Conference on 
pp    IEEE   

Keskar  Nitish Shirish  Mudigere  Dheevatsa  Nocedal  Jorge 
Smelyanskiy  Mikhail  and Tang  Ping Tak Peter  On largebatch training for deep learning  Generalization gap and sharp
minima  In ICLR  arXiv   

Kingma  Diederik    Salimans  Tim  Jozefowicz  Rafal  Chen  Xi 
Sutskever  Ilya  and Welling  Max  Improved variational inference with inverse autoregressive  ow  In Lee        Sugiyama 
   Luxburg        Guyon     and Garnett      eds  Advances
in Neural Information Processing Systems   pp   
Curran Associates  Inc   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey    Imagenet classi cation with deep convolutional neural networks  In
Advances in neural information processing systems  pp   
   

Lafond  Jean  Vasilache  Nicolas  and Bottou    eon  About diagonal rescaling applied to neural nets  ICML Workshop on Optimization Methods for the Next Generation of Machine Learning 
 

Larsen  Anders Boesen Lindbo    nderby    ren Kaae  and
Winther  Ole  Autoencoding beyond pixels using   learned
similarity metric  CoRR  abs    URL http 
 arxiv org abs 

Montufar  Guido    Pascanu  Razvan  Cho  Kyunghyun  and Bengio  Yoshua  On the number of linear regions of deep neural
networks  In Advances in neural information processing systems  pp     

Nair  Vinod and Hinton  Geoffrey    Recti ed linear units improve
In Proceedings of the  th
restricted boltzmann machines 
international conference on machine learning  ICML  pp 
   

Nesterov  Yurii and Vial  JeanPhilippe  Con dence level solutions
for stochastic programming  Automatica   
 

Sagun  Levent    uney    Ugur  Arous  Gerard Ben  and LeCun 
Yann  Explorations on high dimensional landscapes  arXiv
preprint arXiv   

Salimans  Tim and Kingma  Diederik    Weight normalization   
simple reparameterization to accelerate training of deep neural networks  In Advances in Neural Information Processing
Systems  pp     

Salimans  Tim  Goodfellow  Ian  Zaremba  Wojciech  Cheung 
Vicki  Radford  Alec  and Chen  Xi  Improved techniques for
training gans  In Advances in Neural Information Processing
Systems  pp     

Saxe  Andrew    McClelland  James    and Ganguli  Surya 
Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks  CoRR  abs    URL
http arxiv org abs 

Simonyan  Karen and Zisserman  Andrew  Very deep convolutional
In ICLR 

networks for largescale image recognition 
arXiv   

Sutskever  Ilya  Vinyals  Oriol  and Le  Quoc    Sequence to
sequence learning with neural networks  In Advances in neural
information processing systems  pp     

Swirszcz  Grzegorz  Czarnecki  Wojciech Marian  and Pascanu 
Razvan  Local minima in training of deep networks  CoRR 
abs   

Szegedy  Christian  Zaremba  Wojciech  Sutskever  Ilya  Bruna 
Joan  Erhan  Dumitru  Goodfellow  Ian  and Fergus  Rob 
In ICLR 
Intriguing properties of neural networks 
arXiv   

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet  Pierre 
Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke 
Vincent  and Rabinovich  Andrew  Going deeper with convolutions  In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pp     

Theis  Lucas  Oord    aron van den  and Bethge  Matthias   
note on the evaluation of generative models  In ICLR 
arXiv   

Wu  Yonghui  Schuster  Mike  Chen  Zhifeng  Le  Quoc    Norouzi 
Mohammad  Macherey  Wolfgang  Krikun  Maxim  Cao  Yuan 
Gao  Qin  Macherey  Klaus  et al  Google   neural machine
translation system  Bridging the gap between human and machine translation  arXiv preprint arXiv   

Neyshabur  Behnam  Salakhutdinov  Ruslan    and Srebro  Nati 
Pathsgd  Pathnormalized optimization in deep neural networks  In Advances in Neural Information Processing Systems 
pp     

Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht  Benjamin 
and Vinyals  Oriol  Understanding deep learning requires reIn ICLR  arXiv 
thinking generalization 
 

Pascanu  Razvan and Bengio  Yoshua  Revisiting natural gradient

for deep networks  ICLR   

Raghu  Maithra  Poole  Ben  Kleinberg  Jon  Ganguli  Surya  and
SohlDickstein  Jascha  On the expressive power of deep neural
networks  arXiv preprint arXiv   

Rezende  Danilo Jimenez and Mohamed  Shakir  Variational inference with normalizing  ows  In Bach   Blei   pp 
  URL http jmlr org proceedings 
papers   rezende html 

