Learning In nite Layer Networks Without the Kernel Trick

Roi Livni   Daniel Carmon   Amir Globerson  

Abstract

In nite Layer Networks  ILN  have been proposed as an architecture that mimics neural networks while enjoying some of the advantages
of kernel methods 
ILN are networks that integrate over in nitely many nodes within   single hidden layer 
It has been demonstrated by
several authors that the problem of learning ILN
can be reduced to the kernel trick  implying that
whenever   certain integral can be computed analytically they are ef ciently learnable 
In this
work we give an online algorithm for ILN  which
avoids the kernel trick assumption  More generally and of independent interest  we show that
kernel methods in general can be exploited even
when the kernel cannot be ef ciently computed
but can only be estimated via sampling  We provide   regret analysis for our algorithm  showing
that it matches the sample complexity of methods
which have access to kernel values  Thus  our
method is the  rst to demonstrate that the kernel
trick is not necessary  as such  and random features suf ce to obtain comparable performance 

  Introduction
With the increasing success of highly nonconvex and complex learning architectures such as neural networks  there
is an increasing effort to further understand and explain the
limits of training such hierarchical structures 
Recently there have been attempts to draw mathematical
insight from kernel methods in order to better understand
deep learning  as well as come up with new computationally learnable architectures  One such line of work consists
of learning classi ers that are linear functions of   very
large or in nite collection of nonlinear functions  Bach 

 University

of Princeton 

Princeton  New Jersey 
CorresponRoi Livni  rlivni cs princeton edu  Daniel
Globerson

USA  TelAviv University  TelAviv 
dence to 
Carmon  carmonda mail tau ac il 
 gamir mail tau ac il 

Israel 

Amir

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

  Daniely et al    Cho   Saul    Heinemann
et al    Williams    Such models can be interpreted as   neural network with in nitely many nodes in  
hidden layer  and we thus refer to them as  In nite Layer
Networks   ILN  They are of course also related to kernel
based classi ers  as will be discussed later 
  target function in an ILN class will be of the form 

               

 

 cid 

   

Here   is some function of the input   and parameters   
and      is   prior over the parameter space  For example         can be   single sigmoidal neuron or   complete convolutional network  The integral can be thought
of as an in nite sum over all such possible networks  and
      can be thought of as an in nite output weight vector
to be trained 
  Standard  hidden layer network with    nite set of units
can be obtained from the above formalism as follows  First 
choose                  where   is an activation function       sigmoid or relu  Next  set      to be   discrete
measure over    nite set            wd  In this case  the integral results in   network with   hidden units  and the function   is the linear weights of the output layer  Namely 

  cid 

  

     
 

   wi         wi 

The main challenge when training  hidden layer networks
is of course to  nd the            wd on which we wish to
support our distribution  It is known  Livni et al   
that due to hardness of learning intersection of halfspaces
 Klivans   Sherstov    Daniely et al     hidden
layer neural networks are computationally hard for   wide
class of activation functions  Therefore  as the last example illustrates  the choice of   is indeed crucial for performance 
For    xed prior   the class of ILN functions is highly
expressive  since   can be chosen to approximate any  
hidden layer architecture to arbitrary precision  by setting
  to delta functions around the weights of the network  as

 cid  
        wi dw

 In   function notation         
 

Learning In nite Layer Networks Without the Kernel Trick

we did above for   However  this expressiveness comes
at   cost  As argued in Heinemann et al    ILN will
generalize well when there is   large probability mass of  
parameters that attain   small loss 
The key observation that makes certain ILN tractable to
learn is that Eq    is   linear functional in    In that sense it
is   linear classi er and enjoys the rich theory and algorithmic toolbox for such classi ers  In particular  one can use
the fact that linear classi ers can be learned via the kernel
trick in   batch  Cortes   Vapnik    as well as online
settings  Kivinen et al    In other words  we can reduce learning ILN to the problem of computing the kernel
function between two examples  Speci cally the problem
reduces to computing integrals of the following form 
                   
                   

         

 cid 

 

 

   
   

In this work we extend this result to the case where no
closed form kernel is available  and thus the kernel trick
is not directly applicable  We thus turn our attention to the
setting where features         vectors  can be randomly
sampled 
In this setting  our main result shows that for
the squared loss  we can ef ciently learn the above class 
Moreover  we can surprisingly do this with   computational
cost comparable to that of methods that have access to the
closed form kernel        
The observation we begin with is that sampling random
features         above  leads to an unbiased estimate of
the kernel in Eq    Thus  if for example  we ignore complexity issues and can sample in nitely many      it is not
surprising that we can avoid the need for exact computation of the kernel  However  our results provide   much
 
stronger and practical result  Given   training samples 
     see
the lower bound on achievable accuracy is   
Shamir    We show that we can in fact achieve this
rate  using        calls  to the random feature generator 
For comparison  note that       is the size of the kernel
matrix  and is thus likely to be the cost of any algorithm
that uses an explicit kernel matrix  where one is available 
As we discuss later  our approach improves on previous
random features based learning  Dai et al    Rahimi  
Recht    in terms of sample computational complexity 
and expressiveness 

  Problem Setup
We consider algorithms that learn   mapping from input
instances       to labels        We focus on the regression case where   is the interval     Our starting
point is   class of feature functions                 

 We use    notation to suppress logarithmic factors

parametrized by vectors       The functions       
may contain highly complex non linearities  such as multilayer networks consisting of convolution and pooling layers  Our only assumption on        is that for all      
and       it holds that           
Given   distribution   on   we denote by      the
class of square integrable functions over  

 cid 

 cid 

 cid 

      

   

             

 

We will use functions          as mixture weights
over the class   where each   naturally de nes   new regression function from   to   as follows 

               

 

 cid 

   

Our key algorithmic assumption is that the learner can ef 
 ciently sample random   according to the distribution  
Denote the time to generate one such sample by  
In what follows it will be simpler to express the integrals
as scalar products  De ne the following scalar product on
functions         
 cid      cid   

We denote the corresponding  cid  norm by  cid   cid   cid cid      cid 

             

 cid 

Also  given features   denote by     the function in
     given by                The regression functions we are considering are then of the form
     cid       cid 
  subclass of norm bounded elements in      induces
  natural subclass of regression functions  Namely  we consider the following class 

 

HB
          cid       cid     cid   cid        

Our ultimate goal is to output   predictor          that
is competitive  in terms of prediction  with the best target
function in the class HB
   
We will consider an online setting  and use it to derive generalization bounds via standard online to batch conversion 
In our setting  at each round   learner chooses   target function ft        and an adversary then reveals   sample
xt and label yt  The learner then incurs   loss of

 cid   ft   

 
 

 cid ft   xt cid    yt   

 

The use of squared loss might seem restrictive if one is interested in classi cation  However     loss is common by
now in classi cation with support vector machines and kernel methods since  Suykens   Vandewalle    Suykens

et al    More recently Zhang et al    showed that
when using   large number of features regression achieves
performance comparable to the corresponding linear classi ers  see Section   therein 
The objective of the learner is to minimize her   round
regret       norm bounded elements in      Namely 

  cid 

  

  cid 

  

 cid   ft    min
  HB

 

 cid     

 

In the statistical setting we assume that the sequence    
 xi  yi  
   is generated IID according to some unknown
distribution    We then de ne the expected loss of   predictor as

         

      

 cid       cid      

 

 

 cid   

 

 cid 

  Main Results
Theorem   states our result for the online model  The corresponding result for the statistical setting is given in Corollary   We will elaborate on the structure of the Algorithm
later  but  rst provide the main result 

Algorithm   The SHRINKING GRADIENT algorithm 
Data              
Result  Weights                  RT   Functions

ft        de ned as

ft  cid  

   xi 

      
Initialize         RT  
for                 do
Observe xt  yt 
Set Et  
EST SCALAR PROD         xt    
if  Et       then
         
   

   yt   Et 

 

else

       

     

Theorem   Run Algorithm   with parameters          
      
 

and       cid     log  BT  cid  Then 
 cid    cid 

served by the algorithm we have for            fT  
 

  For every sequence of squared losses  cid           cid   ob 

  cid 

 cid 

 cid     

     

   

 

  

 cid   ft    min
  HB

 

  

 Ignoring logarithmic factors in   and    

Learning In nite Layer Networks Without the Kernel Trick

Algorithm   EST SCALAR PROD

Data             
Result  Estimated scalar product  
if       then
Set      

else

for           do

   cid     

Sample   from the distribution       
 
Sample parameter    from   Set
       sgn   xi           

 cid  

Set    

 cid cid 
 

       

  The runtime of the algorithm is    cid      cid 

  For each               and   new test example    we can
with probability         estimate  cid ft     cid  within
accuracy   by running Algorithm   with parameters
     xi  
log   The resulting running time for   test point is then     

       and           
 
 

We next turn to the statistical setting  where we provide
bounds on the expected performance  Following standard
online to batch conversion and Theorem   we can obtain
the following Corollary       see ShalevShwartz   
Corollary    Statistical Setting  The following holds for
any       Run Algorithm   as in Theorem   with    
    Let      xt  yt  
     
   be an IID sample drawn
from some unknown distribution    Let fS    
Then the expected loss satis es 

 cid  ft 

 

 
       fS    inf
  HB

 

        

The runtime of the algorithm  as well as estimation time on
  test example are as de ned in Theorem  

Proofs of the results are provided in Section   and the
appendix 

  Related Work
Learning with random features can be traced to the early
days of learning  Minsky   Papert    and in nite
networks have also been introduced more than   years
ago  Williams    Hornik    More recent works
have considered learning neural nets  also multilayer  with
in nite hidden units using the kernel trick  Cho   Saul 
  Deng et al    Hazan   Jaakkola    Heinemann et al    These works take   similar approach

Learning In nite Layer Networks Without the Kernel Trick

to ours but focus on computing the kernel for certain feature classes in order to invoke the kernel trick  Our work
in contrast avoids using the kernel trick and applies to any
feature class that can be randomly generated  All the above
works are part of   broader effort of trying to circumvent
hardness in deep learning by mimicking deep nets through
kernels  Mairal et al    Bouvrie et al    Bo et al 
    and developing general duality between neural networks and kernels  Daniely et al   
From   different perspective the relation between random
features and kernels has been noted by Rahimi   Recht
  who show how to represent translation invariant kernels in terms of random features  This idea has been further
studied  Bach    Kar   Karnick    for other kernels as well  The focus of these works is mainly to allow
scaling down of the feature space and representation of the
 nal output classi er 
Dai et al    focus on tractability of large scale kernel
methods  and their proposed doubly stochastic algorithm
can also be used for learning with random features as we
have here  In Dai et al    the objective considered is
 cid   cid          with   correspondof the regularized form   
ing sample complexity of    samples needed to
achieve   approximation with respect to the risk of the optimum of the regularized objective 
To relate the above results to ours  we begin by emphasizing that the bound in  Dai et al    holds for  xed   and
refers to optimization of the regularized objective  Our objective is to minimize the risk       which is the expected
squared loss  for which we need to choose         
     in
order to attain accuracy    Sridharan et al    Plugging
this   into the generalization bound in Dai et al    we
obtain that the algorithm in Dai et al    needs      
   
samples to compete with the optimal target function in the
Bball  Our algorithm needs      
    examples which is
considerably better  We note that their method does extend to   larger class of losses  whereas our is restricted to
the quadratic loss 
In Rahimi   Recht  
the authors consider embedding the domain into the feature space    
                wm     where wi are IID random variables sampled according to some prior    
They
show that with       log  
  random features estimated on
      log  
HB
  max

  samples they can compete with the class 
   

                            

 cid 

 cid 

 

 

 

 cid 

Our algorithm relates to the mean square error cost function which does not meet the condition in Rahimi   Recht
  and is hence formally incomparable  Yet we can
invoke our algorithm to compete against   larger class

  max

  HB

of target functions  Our main result shows that Algo 
    estimated features and using      
rithm   using       
   
samples will  in expectation  output   predictor that is  
    Note that            implies
close to the best in HB
Ew            Hence HB
    Note however  that the number of estimated features  as   function of
   is worse in our case 
Our approach to the problem is to consider learning with
  noisy estimate of the kernel    related setting was studied in CesaBianchi et al      where the authors considered learning with kernels when the data is corrupted 
Noise in the data and noise in the scalar product estimation are not equivalent when there is nonlinearity in the
kernel space embedding  There is also extensive research
on linear regression with actively chosen attributes  CesaBianchi et al      Hazan   Koren    The convergence rates and complexity of the algorithms are dimension
dependent  It would be interesting to see if their method
can be extended from  nite set of attributes to   continuum
set of attributes 

  Algorithm
We next turn to present Algorithm   from which our main
result is derived  The algorithm is similar in spirit to Online
Gradient Descent  OGD   Zinkevich    but with some
important modi cations that are necessary for our analysis 
We  rst introduce the problem in the terminology of online
convex optimization  as in Zinkevich   At iteration  
our algorithm outputs   hypothesis ft  It then receives as
feedback  xt  yt  and suffers   loss  cid   ft  as in Eq    The
objective of the algorithm is to minimize the regret against
  benchmark of Bbounded functions  as in Eq   
  classic approach to the problem is to exploit the OGD
algorithm  Its simplest version would be to update ft   
ft      where   is   step size  and    is the gradient of
the loss          at ft  In our case     is given by 

      cid ft   xt cid    yt   xt 

 

 cid  

Applying this update would also result in   function ft  
      xt  as we have in Algorithm    but with different    from ours  However  in our setting this update is
not applicable since the scalar product  cid ft   xt cid  is not
available  One alternative is to use   stochastic unbiased
estimate of the gradient that we denote by     This induces an update step ft    ft         One can show that
OGD with such an estimated gradient enjoys the following
 cid   cid            see ShalevShwartz   

upper bound on the regret    cid   cid   ft     cid      for every

  cid 

  cid cid   cid cid     

  cid 

   cid    

 cid   

  

  

  
 

   

 

Et  

 cid   cid 

 

  cid 

  

where    cid    

 cid      cid cid          cid cid  We can bound the

      

 cid  

 rst two terms using standard techniques applicable for the
squared loss       see Zhang    Srebro et al   
The third term depends on our choice of gradient estimate 
There are various choices for such an estimate  and we use
  version which facilitates our analysis  as explained below 
Assume that at iteration    our function ft is given by ft  
   xt  We now want to use sampling to obtain
an unbiased estimate of  cid ft   xt cid  This will be done via
  two step sampling procedure  as described in Algorithm
  First  sample an index                  by sampling according to the distribution           
  Next  for the chosen    sample    according to   and use        xi     
to construct an estimate of  cid xi   xt cid  The resulting
unbiased estimate of  cid xi   xt cid  is denoted by Et and
given by 

 

sgn   

   xi     xt     

 

The corresponding unbiased gradient estimate is 

      Et   yt  xt

    regret 

 
The variance of   affects the convergence rate and depends
on both  cid cid  and the number of estimations    We wish to
 
maintain           estimations per round  while achieving   
To effectively regularize  cid cid  we modify the OGD algorithm so that whenever Et is larger then     we do not
perform the usual update  Instead  we perform   shrinking
step that divides      and hence ft  by   Treating   as
constant  this guarantees that  cid cid          and in turn
Var               
    we have that
          estimations are suf cient 
The rationale for the shrinkage is that whenever Et is large 
it indicates that ft is  far away  from the Bball  and  
shrinkage step  similar to projection  brings ft closer to the
optimal element in the Bball  However  due to stochasticity  the shrinkage step does add   further term to the regret
bound that we would need to take care of 

 
    Setting       

  Analysis

In what follows we analyze the regret for Algorithm   and
provide   high level proof of Theorem   The appendix
provides the necessary lemmas and   more detailed proof 
We begin by modifying the regret bound for OGD in Eq   
to accommodate for steps that differ from the standard gradient update  such as shrinkage  We use the following notation for the regret at iteration   

 cid 

Lemma   Let  cid           cid   be an arbitrary sequence of convex loss functions  and let            fT be random vectors 
produced by an online algorithm  Assume  cid fi cid    BT for
all         For each   let    be an unbiased estimator of
 cid   ft  Denote  ft   ft          and let

Pt        cid cid ft     cid     cid   ft     cid cid 
  cid 
  cid cid   cid cid     
   cid    

  cid 

   

 

For every  cid   cid      it holds that  

Rt        
  cid 
 

  

  

 BT     

   Pt   

  

 

 

 cid   

 

See Appendix    for proof of the lemma  As discussed
earlier  the  rst three terms on the RHS are the standard
bound for OGD from Eq    Note that in the standard
OGD it holds that ft    ft  and therefore Pt        and
the last term disappears 
The third term will be bounded by controlling  cid cid  The
last term Pt    is   penalty that results from updates that
stir ft away from the standard update step  ft  This will
indeed happen for the shrinkage step  The next lemma
bounds this term  See Appendix    for proof 
Lemma   Run Algorithm   with parameters          
and       Let    be the unbiased estimator of  cid   ft 
of the form       Et   yt xt  Denote  ft   ft       
and de ne Pt    as in Eq    Then 

 cid 

 cid 

Pt        exp

   
   

The following lemma  see Appendix    for proof  bounds
the second and third terms of Eq   
Lemma   Consider the setting as in Lemma   Then

and   cid cid   cid cid        cid   ft 

 cid         

   cid    

 

Proof of Theorem   Combining Lemmas     and   and
rearranging we get 

        Rt        
 

   

 cid         

 

          

 

 

  
 BT     

 

  

Pt   

  cid 
  cid 

To bound the second term in Eq    we note that 

  cid 

 cid          cid 

  

  

Learning In nite Layer Networks Without the Kernel Trick

 cid    cid 

  

Rt       

 cid   ft     cid     

 

min

 cid   cid  

 cid        

 

Learning In nite Layer Networks Without the Kernel Trick

 

 cid 

   

 cid          

We next set   and   as in the statement of the theorem 
 
  and            log   where
Namely       
 
 
  This choice of   implies
    max
that               and hence the third term in
Eq    is upper bounded by    
Next we have that         log   for every    and by the
bound on BT we have that        BT  
  Taken together
with Lemma   we have that 

 

  cid 

 BT     

Pt         

 

 

  

The above bounds imply that 

        Rt        
 

              

Finally by choice of   and dividing both sides by      
we obtain the desired result 

  Experiments
In this section we provide   toy experiment to compare
our Shrinking Gradient algorithm to other random feature based methods 
In particular  we consider the following three algorithms  FixedRandom  Sample   set of
  features            wr and evaluate these on all the train
and test points  Namely  all   points will be evaluated
on the same features  This is the standard random features approach proposed in Rahimi   Recht    
Doubly Stochastic Gradient Descent  Dai et al   
Here each training point   samples   features            wk 
These features will from that point on be used for evaluating dot products with    Thus  different   points will use
different features  Shrinking Gradient  This is the approach proposed here in Section   Namely  each training
point   samples   features in order to calculate the dot
product with the current regression function 
In comparing the algorithms we choose         so that the
same overall number of features is calculated  For all methods we explored different initial step sizes and schedules
for changing the step size 
The key question in comparing the three algorithms is how
well they use   given budget of random features  To explore this we perform an experiments to simulate the high
dimensional feature case  We consider vectors     RD 
where   random feature   corresponds to   uniform choice
of coordinate   in    We work in the regime where   is
large in the sense that         where   is the size of the
training data  Thus random sampling of   features will
not reveal all coordinates of    The training set is generated as follows  First    training set            xT   RD is

sampled from   standard Gaussian  We furthermore clip
negative values to zero  in order to make the data sparser
and more challenging for feature sampling  Next   weight
vector     RD is chosen as   random sparse linear combination of the training points  This is done in order for the
true function to be in the corresponding RKHS  Finally  the
training set is labeled using yi       xi 
During training we do not assume that the algorithms have
access to    Rather they can uniformly sample coordinates from it  which mimics our setting of random features 
For the experiment we take                   and
      All algorithms perform one pass over the data 
to emulate the online regret setting  The results shown in
Figure   show that our method indeed achieves   lower loss
while working with the same feature budget 

Figure   Comparison of three random feature methods  See Section   for details 

  Discussion
We presented   new online algorithm that employs kernels
implicitly but avoids the kernel trick assumption  Namely 
the algorithm can be invoked even when one has access to
only estimations of the scalar product  The problem was
motivated by kernels resulting from neural nets  but it can
of course be applied to any scalar product of the form we
described  As an example of an interesting extension  consider   setting where   learner can observe an unbiased estimate of   coordinate in   kernel matrix  or alternatively the
scalar product between any two observations  Our results
imply that in this setting the above rates are applicable  and
at least for the square loss  having no access to the true
values in the kernel matrix is not necessarily prohibitive
during training 
The results show that with sample size   we can achieve
error of      
  As demonstrated in Shamir   these

 

 input dimension lossFixedRandomDoubly StochShrinkGradLearning In nite Layer Networks Without the Kernel Trick

rates are optimal  even when the scalar product is computable  To achieve this rate our algorithm needs to perform          scalar product estimations  When the
scalar product can be computed  existing kernelized algorithms need to observe    xed proportion of the kernel matrix  hence they observe order of      scalar products 
In CesaBianchi et al    it was shown that when the
scalar product can be computed exactly  one would need
access to at least      entries to the kernel matrix  It is
still an open problem whether one has to access      entries when the kernel can be computed exactly  However 
as we show here  for  xed   even if the kernel can only be
estimated        estimations are enough  It would be interesting to further investigate and improve the performance
of our algorithm in terms of the norm bound   
To summarize  we have shown that the kernel trick is not
strictly necessary in terms of sample complexity  Instead 
simply sampling random features via our proposed algorithm results in   similar sample complexity  Recent empirical results by Zhang et al    show that using   large
number of random features and regression comes close to
the performance of the  rst successful multilayer CNNs
 Krizhevsky et al    on CIFAR  Although deep
learning architectures still substantially outperform random
features  it is conceivable that with the right choice of
random features  and scalable learning algorithms like we
present here  considerable improvement in performance is
possible 

   Estimation Concentration Bounds
In this section we provide concentration bounds for the estimation procedure in Algorithm  
Lemma   Run Algorithm   with   and   xi  

   Let      cid     xi  Assume that            for

      and

all   and    Let   be the output of Algorithm   Then  
is an unbiased estimator for  cid       cid  and 

        cid       cid        exp

 

 cid 

 cid 

    
 cid cid 

 

Proof  Consider the random variables  cid cid       where
is as de ned in Algorithm   and note that
    

they are IID  One can show that   cid cid cid     cid   
 cid   iE  xi             cid       cid  By the bound on
       we have that cid cid cid cid     cid cid     cid cid  with probability
 cid       the result follows directly from

  Since      
 
Hoeffding   inequality 

Next  we bound the     coeffcients and obtain   concentration bound for the estimated dot product Et 
Lemma   The     obtained in Algorithm   satis es 

 cid   cid            

As   corollary of this and Lemma   we have that the function ft satis es 

 cid 

 cid 

   Et    cid ft   xt cid        exp

 

  

        

 

Proof  We prove the statement by induction  We separate
into two cases  depending on whether the shrinkage step
was performed or not 
If  Et       the algorithm sets        

      and 
 cid   cid               

 cid   cid   

 
 

If  Et       the gradient update is performed  Since
 yt      we have that  Et   yt           and 
 cid   cid     cid   cid     Et   yi               

   Proofs of Lemmas
   Proof of Lemma  

First  by convexity we have that

 cid   ft     cid         cid    ft     cid   

 
Next we upper bound  cid    ft     cid  Denote by   the event
 cid ft      cid     cid   ft      cid  Note that 

  cid cid ft      cid cid      cid cid   ft      cid cid 
  cid cid ft      cid cid cid   cid    Pt   
    cid cid   ft      cid cid 
Eq    and   cid cid     cid cid      cid cid   cid cid       cid    

Plugging in  ft    ft         summing over   and using

 cid  we obtain

       BT  Pt   

 

the desired result 

   Proof for Lemma  

To prove the bound in the lemma  we  rst bound the event
Pt          to two possible events 
Lemma   Consider the setting as in Lemma   Run Algorithm   and for each   consider the following two events 

     
     

     Et       and  Et     
     Et       and  cid ft cid       

 cid ft cid 

For every  cid   cid      we have that Pt            

       
 

Learning In nite Layer Networks Without the Kernel Trick

Proof  Denote the event  Et       by    
does not happen  then ft    ft  Hence trivially

Pt        cid cid ft     cid     cid   ft     cid       

 cid 

 

  Note that if    

 

We will assume that     Et           Et     
 cid ft cid 
   cid ft cid        We then show  cid ft   cid     cid   ft   cid 
In other words  we will show that if    
  happens and  cid ft 
  or    
  cid     cid   ft      cid  then either    
  happened  This
will conclude the proof 
Fix    note that since            we have that  cid   cid   
  We then have 

 cid   ft cid     cid ft    Et     xt cid 
   cid ft cid     Et         
 

 cid ft cid     

 

where the last inequality is due to assumption   above 
We therefore have the following for every  cid   cid      

 cid   ft      cid     
 

 cid ft cid         

On the other hand  if ft   cid   ft  then by construction of
the algorithm ft     

  ft 

 cid ft      cid     cid ft cid     cid   cid     cid ft cid 

 

    

Next note that        and assumption   states  cid ft cid   
    Therefore   

 cid ft cid                 and 

 cid   ft      cid     
 
 
 
   
 

 

 cid   

 cid ft cid         
 cid ft cid   
 cid ft cid          
 cid ft cid         cid ft      cid 

 

 cid 

   

Next we upper bound       
perscript   is dropped 
  bound for            

       

  Assume that

  In what follows the su 

 cid   cid  and we get that    did not

Which implies Et    
happen  We conclude that if    and not    then 
     

 Et    cid ft   xt cid     

 
 

Since  
leading to 

          we have that   Et    cid ft   xt cid       
           

       Et    cid ft   xt cid         

 

  bound for       If  Et       and  cid ft cid       then
by normalization of  xt  we have that  cid ft   xt cid      
and trivially we have that  Et    cid ft   xt cid        and
therefore 

           Et    cid ft   xt cid         

Taking Eq    and Eq    we have that

                 Et    cid ft   xt cid         

 

 

By Lemma   we have that 

   Et    cid ft   xt cid         
   
   

             exp

exp      

 cid 

 cid 

Taking the above upper bounds together with Lemma   we
can prove Lemma  

   Proof of Lemma  
Begin by noting that since  cid   cid      it follows from the

de nitions of     that    cid    

 cid      cid cid          cid cid  and

therefore

   cid    

 cid      cid 

 Et    cid ft   xt cid cid 
 cid cid   cid 

     Et 

 cid 

By construction  see Algorithm   we have that 

   Et   

 
 

 

 xi    xt    

where the index   is sampled as in Algorithm   and
 xi    xt     is bounded by   By Lemma   we have
that

This provides the required bound on    cid    

   Et            

 

 

 cid  Additionally 

we have that

 Et    cid ft   xt cid     

     

 
 

  We have
We assume   is suf ciently large and      
          Since we assume    did not happen we must
 
     cid   cid 
have  cid ft cid       and  Et    cid ft   xt cid       
and therefore 

Et    cid   cid     Et    cid ft   xt cid     

   cid   cid 

 
 

 cid   cid     cid ft   xt cid    yt cid xt cid     cid   ft 

and the result follows by taking expectation 
Acknowledgements The authors would like to thank
Tomer Koren for helpful discussions  Roi Livni was supported by funding from Eric and Wendy Schmidt Fund
for Strategic Innovation  This work was supported by
the Blavatnik Computer Science Research Fund  the Intel
Collaborative Research Institute for Computational Intelligence  ICRICI  and an ISF Centers of Excellence grant 

Learning In nite Layer Networks Without the Kernel Trick

References
Bach  Francis  Breaking the curse of dimensionality with convex

neural networks  arXiv preprint arXiv   

Bach  Francis  On the equivalence between kernel quadraarXiv preprint

ture rules and random feature expansions 
arXiv   

Bo  Liefeng  Ren  Xiaofeng  and Fox  Dieter  Kernel descriptors
for visual recognition  In Advances in neural information processing systems  pp     

Bo  Liefeng  Lai  Kevin  Ren  Xiaofeng  and Fox  Dieter  Object
recognition with hierarchical kernel descriptors  In Computer
Vision and Pattern Recognition  CVPR    IEEE Conference on  pp    IEEE   

Bouvrie  Jake  Rosasco  Lorenzo  and Poggio  Tomaso  On invariance in hierarchical models  In Advances in Neural Information Processing Systems  pp     

CesaBianchi  Nicolo  ShalevShwartz  Shai  and Shamir  Ohad 
Ef cient learning with partially observed attributes  The Journal of Machine Learning Research       

CesaBianchi  Nicolo  ShalevShwartz  Shai  and Shamir  Ohad 
Information Theory  IEEE

Online learning of noisy data 
Transactions on       

CesaBianchi  Nicol    Mansour  Yishay  and Shamir  Ohad  On
the complexity of learning with kernels  In Proceedings of The
 th Conference on Learning Theory  pp     

Cho  Youngmin and Saul  Lawrence    Kernel methods for deep
In Advances in neural information processing sys 

learning 
tems  pp     

Cortes  Corinna and Vapnik  Vladimir  Supportvector networks 

Machine learning     

Dai  Bo  Xie  Bo  He  Niao  Liang  Yingyu  Raj  Anant  Balcan 
MariaFlorina    and Song  Le  Scalable kernel methods via
doubly stochastic gradients  In Advances in Neural Information Processing Systems  pp     

Daniely  Amit  Linial  Nati  and ShalevShwartz  Shai  From average case complexity to improper learning complexity  In Proceedings of the  th Annual ACM Symposium on Theory of
Computing  pp    ACM   

Daniely  Amit  Frostig  Roy  and Singer  Yoram  Toward deeper
understanding of neural networks  The power of initialization
and   dual view on expressivity  In Lee        Sugiyama    
Luxburg        Guyon     and Garnett      eds  Advances
in Neural Information Processing Systems   pp   
Curran Associates  Inc   

Deng  Li  Tur  Gokhan  He  Xiaodong  and HakkaniTur  Dilek 
Use of kernel deep convex networks and endto end learning
for spoken language understanding  In Spoken Language Technology Workshop  SLT    IEEE  pp    IEEE   

Hazan  Tamir and Jaakkola  Tommi  Steps toward deep kerarXiv preprint

nel methods from in nite neural networks 
arXiv   

Heinemann  Uri  Livni  Roi  Eban  Elad  Elidan  Gal  and Globerson  Amir  Improper deep kernels  In Proceedings of the  th
International Conference on Arti cial Intelligence and Statistics  pp     

Hornik  Kurt  Some new results on neural network approxima 

tion  Neural Networks     

Kar  Purushottam and Karnick  Harish  Random feature maps for
dot product kernels  In International Conference on Arti cial
Intelligence and Statistics  pp     

Kivinen  Jyrki  Smola  Alexander    and Williamson  Robert   
Online learning with kernels  IEEE transactions on signal processing     

Klivans  Adam   and Sherstov  Alexander    Cryptographic
hardness for learning intersections of halfspaces  In Foundations of Computer Science    FOCS   th Annual IEEE
Symposium on  pp    IEEE   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey    Imagenet classi cation with deep convolutional neural networks 
In Advances in neural information processing systems  pp 
   

Livni  Roi  ShalevShwartz  Shai  and Shamir  Ohad  On the computational ef ciency of training neural networks  In Advances
in Neural Information Processing Systems  pp     

Mairal  Julien  Koniusz  Piotr  Harchaoui  Zaid  and Schmid 
Cordelia  Convolutional kernel networks  In Advances in Neural Information Processing Systems  pp     

Minsky  Marvin and Papert  Seymour  Perceptrons  an introduc 

tion to computational geometry  expanded edition   

Rahimi  Ali and Recht  Benjamin  Random features for largescale kernel machines  In Advances in neural information processing systems  pp     

Rahimi  Ali and Recht  Benjamin  Weighted sums of random
kitchen sinks  Replacing minimization with randomization in
In Advances in neural information processing syslearning 
tems  pp     

ShalevShwartz  Shai  Online learning and online convex optimization  Foundations and Trends in Machine Learning   
   

Shamir  Ohad  The sample complexity of learning linear predictors with the squared loss  Journal of Machine Learning Research   Dec   

Srebro  Nathan  Sridharan  Karthik  and Tewari  Ambuj  Smoothness  low noise and fast rates  In Advances in neural information processing systems  pp     

Sridharan  Karthik  ShalevShwartz  Shai  and Srebro  Nathan 
In Advances in Neural

Fast rates for regularized objectives 
Information Processing Systems  pp     

Hazan  Elad and Koren  Tomer  Linear regression with limited observation  In Proceedings of the  th International Conference
on Machine Learning  ICML  pp     

Suykens  Johan AK and Vandewalle  Joos  Least squares support vector machine classi ers  Neural processing letters   
   

Learning In nite Layer Networks Without the Kernel Trick

Suykens  Johan AK  Van Gestel  Tony  and De Brabanter  Jos 
Least squares support vector machines  World Scienti     

Williams  Christopher  Computing with in nite networks  Advances in neural information processing systems  pp   
 

Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht  Benjamin 
and Vinyals  Oriol  Understanding deep learning requires
rethinking generalization  arXiv preprint arXiv 
 

Zhang  Tong  Solving large scale linear prediction problems using
stochastic gradient descent algorithms  In Proceedings of the
twenty rst international conference on Machine learning  pp 
  ACM   

Zinkevich  Martin  Online convex programming and generalized
in nitesimal gradient ascent  In Machine Learning  Proceedings of the Twentieth International Conference  pp   
 

