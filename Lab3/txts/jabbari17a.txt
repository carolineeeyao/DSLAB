Fairness in Reinforcement Learning  

Shahin Jabbari Matthew Joseph Michael Kearns Jamie Morgenstern Aaron Roth  

Abstract

We initiate the study of fairness in reinforcement
learning  where the actions of   learning algorithm may affect its environment and future rewards  Our fairness constraint requires that an
algorithm never prefers one action over another
if the longterm  discounted  reward of choosing
the latter action is higher  Our  rst result is negative  despite the fact that fairness is consistent
with the optimal policy  any learning algorithm
satisfying fairness must take time exponential in
the number of states to achieve nontrivial approximation to the optimal policy  We then provide   provably fair polynomial time algorithm
under an approximate notion of fairness  thus establishing an exponential gap between exact and
approximate fairness 

  Introduction
The growing use of machine learning for automated
decisionmaking has raised concerns about the potential for
unfairness in learning algorithms and models  In settings
as diverse as policing   hiring   lending   and
criminal sentencing   mounting empirical evidence suggests these concerns are not merely hypothetical    
We initiate the study of fairness in reinforcement learning 
where an algorithm   choices may in uence the state of the
world and future rewards  In contrast  previous work on fair
machine learning has focused on myopic settings where
such in uence is absent      
in        or noregret models         The resulting fairness de nitions therefore
do not generalize well to   reinforcement learning setting 
as they do not reason about the effects of shortterm actions on longterm rewards  This is relevant for the set 

 University of Pennsylvania  Philadelphia  PA  USA  Correspondence to  Shahin Jabbari  Matthew Joseph  Michael Kearns 
Jamie Morgenstern  Aaron Roth  jabbari  majos  mkearns 
jamiemor  aaroth cis upenn edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

 The full

technical version of this paper is available at

https arxiv org pdf pdf 

tings where historical context can have   distinct in uence
on the future  For concreteness  we consider the speci  
example of hiring  though other settings such as college
admission or lending decisions can be embedded into this
framework  Consider    rm aiming to hire employees for
  number of positions  The  rm might consider   variety
of hiring practices  ranging from targeting and hiring applicants from wellunderstood parts of the applicant pool
 which might be   reasonable policy for shortterm productivity of its workforce  to exploring   broader class of applicants whose backgrounds might differ from the current
set of employees at the company  which might incur shortterm productivity and learning costs but eventually lead to
  richer and stronger overall applicant pool 
We focus on the standard model of reinforcement learning 
in which an algorithm seeks to maximize its discounted
sum of rewards in   Markovian decision process  MDP 
Throughout  the reader should interpret the actions available to   learning algorithm as corresponding to choices or
policies affecting individuals       which applicants to target and hire  The reward for each action should be viewed
as the shortterm payoff of making the corresponding decision       the shortterm in uence on the  rm   productivity after hiring any particular candidate  The actions taken
by the algorithm affect the underlying state of the system
      the company   demographics as well as the available
applicant pool  and therefore in turn will affect the actions
and rewards available to the algorithm in the future 
Informally  our de nition of fairness requires that  with
high probability  in state    an algorithm never chooses an
available action   with probability higher than another action    unless                        the longterm reward of   is greater than that of    This de nition  adapted
from Joseph et al    is weakly meritocratic  facing
some set of actions  an algorithm must pick   distribution
over actions with  weakly  heavier weight on the better actions  in terms of their discounted longterm reward  Correspondingly    hiring process satisfying our fairness definition cannot probabilistically target one population over
another if hiring from either population will have similar
longterm bene   to the  rm   productivity 
Unfortunately  our  rst result shows an exponential separation in expected performance between the best unfair algo 

Fairness in Reinforcement Learning

rithm and any algorithm satisfying fairness  This motivates
our study of   natural relaxation of  exact  fairness  for
which we provide   polynomial time learning algorithm 
thus establishing an exponential separation between exact
and approximately fair learning in MDPs 
Our Results Throughout  we use  exact  fairness to refer
to the adaptation of Joseph et al     de nition de ning an action   quality as its potential longterm discounted
reward  We also consider two natural relaxations  The  rst 
approximatechoice fairness  requires that an algorithm
never chooses   worse action with probability substantially
higher than better actions  The second  approximateaction
fairness  requires that an algorithm never favors an action
of substantially lower quality than that of   better action 
The contributions of this paper can be divided into two
parts  First  in Section   we give   lower bound on the time
required for   learning algorithm to achieve nearoptimality
subject to  exact  fairness or approximatechoice fairness 
Theorem  Informal statement of Theorems     and  
For constant   to achieve  optimality      any fair or
approximatechoice fair algorithm takes   number of
rounds exponential in the number of MDP states and  ii 
any approximateaction fair algorithm takes   number of
rounds exponential in       for discount factor  
Second  we present an approximateaction fair algorithm
 FairE  in Section   and prove   polynomial upper bound
on the time it requires to achieve nearoptimality 
Theorem  Informal statement of Theorem   For constant
  and any MDP satisfying standard assumptions  FairE  is an approximateaction fair algorithm achieving  
optimality in   number of rounds that is  necessarily  exponential in       and polynomial in other parameters 
The exponential dependence of FairE  on       is
tight  it matches our lower bound on the time complexity
of any approximateaction fair algorithm  Furthermore  our
results establish rigorous tradeoffs between fairness and
performance facing reinforcement learning algorithms 

  Related Work
The most relevant parts of the large body of literature on
reinforcement learning focus on constructing learning algorithms with provable performance guarantees      
was the  rst learning algorithm with   polynomial learning rate  and subsequent work improved this rate  see Szita
and Szepesv ari   and references within  The study of
robust MDPs       examines MDPs with high parameter uncertainty but generally uses  optimistic  learning strategies that ignore  and often con ict with  fairness
and so do not directly apply to this work 
Our work also belongs to   growing literature studying the

problem of fairness in machine learning  Early work in
data mining             considered the question
from   primarily empirical standpoint  often using statistical parity as   fairness goal  Dwork et al    explicated
several drawbacks of statistical parity and instead proposed
one of the  rst broad de nitions of algorithmic fairness 
formalizing the idea that  similar individuals should be
treated similarly  Recent papers have proven several impossibility results for satisfying different fairness requirements simultaneously     More recently  Hardt et al 
  proposed new notions of fairness and showed how
to achieve these notions via postprocessing of   blackbox
classi er  Woodworth et al    and Zafar et al   
further studied these notion theoretically and empirically 

  Strengths and Limitations of Our Models
In recognition of the duration and consequence of choices
made by   learning algorithm during its learning process
      
job applicants not hired   our work departs from
previous work and aims to guarantee the fairness of the
learning process itself  To this end  we adapt the fairness
de nition of Joseph et al    who studied fairness in
the bandit framework and de ned fairness with respect to
onestep rewards  To capture the desired interaction and
evolution of the reinforcement learning setting  we modify this myopic de nition and de ne fairness with respect
to longterm rewards    fair learning algorithm may only
choose action   over action    if   has true longterm reward at least as high as    Our contributions thus depart
from previous work in reinforcement learning by incorporating   fairness requirement  ruling out existing algorithms
which commonly make heavy use of  optimistic  strategies
that violates fairness  and depart from previous work in fair
learning by requiring  online  fairness in   previously unconsidered reinforcement learning context 
First note that our de nition is weakly meritocratic  an algorithm satisfying our fairness de nition can never probabilistically favor   worse option but is not required to favor
  better option  This confers both strengths and limitations 
Our fairness notion still permits   type of  conditional discrimination  in which   fair algorithm favors group   over
group   by selecting choices from   when they are superior and randomizing between   and   when choices from
  are superior  In this sense  our fairness requirement is
relatively minimal  encoding   necessary variant of fairness
rather than   suf cient one  This makes our lower bounds
and impossibility results  Section   relatively stronger and
upper bounds  Section   relatively weaker 
Next  our fairness requirement holds  with high probability  across all decisions that   fair algorithm makes  We
view this strong constraint as worthy of serious consideration  since  forgiving  unfairness during the learning

Fairness in Reinforcement Learning

may badly mistreat the training population  especially if the
learning process is lengthy or even continual  Additionally 
it is unclear how to relax this requirement  even for   small
fraction of the algorithm   decisions  without enabling discrimination against   correspondingly small population 
Instead  aiming to preserve the  minimal  spirit of our definition  we consider   relaxation that only prevents an algorithm from favoring   signi cantly worse option over
  better option  Section   Hence  approximateaction
fairness should be viewed as   weaker constraint  rather
than safeguarding against every violation of  fairness  it
instead restricts how egregious these violations can be  We
discuss further relaxations of our de nition in Section  

  Preliminaries
In this paper we study reinforcement learning in Markov
Decision Processes  MDPs  An MDP is   tuple    
 SM  AM   PM   RM        where SM is   set of   states 
AM is   set of   actions    is   horizon of    possibly in 
nite  number of rounds of activity in    and   is   discount
factor  PM   SM           and RM   SM       denote the transition probability distribution and reward distribution  respectively  We use  RM to denote the mean of
RM    policy   is   mapping from   history    the sequence of triples  state  action  reward  observed so far 
to   distribution over actions  The discounted state and
stateaction value functions are denoted by     and    and
          represents expected discounted reward of following   from   for   steps  The highest values functions are
achieved by the optimal policy   and are denoted by    
and      We use   to denote the stationary distribution of   Throughout we make the following assumption 
Assumption    Unichain Assumption  The stationary distribution of any policy in   is independent of its start state 

We denote the  mixing time of   by    
    Lemma   relates
the  mixing time of any policy   to the number of rounds
until the    
  values of the visited states by   are close to
the expected    
  values  under the stationary distribution
  We defer all the omitted proofs to the Appendix 
Lemma   Fix     For any state    following   for
       

  steps from   satis es

Es      

            

 

   st     

   

   

TXt 

where st is the state visited at time   when following   from
  and the expectation in the second term is over the transition function and the randomization of  

 Note that  RM     and Var RM       for all states  The
bounded reward assumption can be relaxed  see        Also
assuming rewards in     can be made          up to scaling 

 Lemma   can be stated for   weaker notion of mixing time

 

 

 

TXt 

      st   

  log         log  of an
The horizon time    
 
MDP captures the number of steps an approximately optimal policy must optimize over  The expected discounted
reward of any policy after    
  steps approaches the expected asymptotic discounted reward  Kearns and Singh
  Lemma     learning algorithm   is   nonstationary policy that at each round takes the entire history
and outputs   distribution over actions  We now de ne  
performance measure for learning algorithms 
De nition    optimality  Let     and        
  achieves  optimality in   steps if for any     
Es                  
 
     
with probability at least       for st the state   reaches at
time    where the expectation is taken over the transitions
and the randomization of    for any MDP   
We thus ask that   learning algorithm  after suf ciently
many steps  visits states whose values are arbitrarily close
to the values of the states visited by the optimal policy 
Note that this is stronger than the  handraising  notion
in Kearns and Singh   which only asked that the
learning algorithm stop in   state from which discounted
return is nearoptimal  permitting termination in   state
from which the optimal discounted return is poor  In Definition   if there are states with poor optimal discounted
reward that the optimal policy eventually leaves for better
states  so must our algorithms  We also note the following
connection between the average    
  values of states visited under the stationary distribution of    and in particular
an optimal policy  and the average undiscounted rewards
achieved under the stationary distribution of that policy 
Lemma    Singh   Let  RM be the vector of mean
rewards in states of   and   
  the vector of discounted
rewards in states under   Then    RM      
   
We design an algorithm which quickly achieves  
optimality and we bound the number of steps   before this
happens by   polynomial in the parameters of   

  Notions of Fairness
We now turn to formal notions of fairness  Translated to
our setting  Joseph et al    de ne action     quality as
the expected immediate reward for choosing   from state
  and then require that an algorithm not probabilistically
favor   over    if   has lower expected immediate reward 
However  this naive translation does not adequately capture the structural differences between bandit and MDP setcalled the  reward mixing time which is always linearly bounded
by the  mixing time but can be much smaller in certain cases
 see Kearns and Singh   for   discussion 

 We suspect unfair    also satis es this stronger notion 

Fairness in Reinforcement Learning

tings since present rewards may depend on past choices in
MDPs  In particular  de ning fairness in terms of immediate rewards would prohibit any policy sacri cing shortterm rewards in favor of longterm rewards  This is undesirable  since it is the longterm rewards that matter in
reinforcement learning  and optimizing for longterm rewards often necessitates shortterm sacri ces  Moreover 
the longterm impact of   decision should be considered
when arguing about its relative fairness  We will therefore
de ne fairness using the stateaction value function     
De nition    Fairness    is fair if for all input     all
   all rounds    all states   and all actions      
                                  ht            ht 
with probability at least       over histories ht   
Fairness requires that an algorithm never probabilistically
favors an action with lower longterm reward over an action
with higher longterm reward  In hiring  this means that
an algorithm cannot target one applicant population over
another unless the targeted population has   higher quality 
In Section   we show that fairness can be extremely restrictive  Intuitively    must play uniformly at random until
it has high con dence about the     values  in some cases
taking exponential time to achieve nearoptimality  This
motivates relaxing De nition   We  rst relax the probabilistic requirement and require only that an algorithm not
substantially favor   worse action over   better one 
De nition    Approximatechoice Fairness    is  choice
fair if for all inputs     and     for all    all rounds
   all states   and actions      
                                  ht            ht 
with probability of at least       over histories ht 
If   is  choice fair for any input     we call  
approximatechoice fair 

  slight modi cation of the lower bound for  exact  fairness shows that algorithms satisfying approximatechoice
fairness can also require exponential time to achieve nearoptimality  We therefore propose an alternative relaxation 
where we relax the quality requirement  As described in
Section   the resulting notion of approximateaction fairness is in some sense the most  tting relaxation of fairness  and is   particularly attractive one because it allows
us to give algorithms circumventing the exponential hardness proved for fairness and approximatechoice fairness 
De nition    Approximateaction Fairness    is  action
fair if for all inputs     and     for all    all rounds
   all states   and actions      
                                  ht           ht 

 

           denotes the probability   chooses   from   given history   

with probability of at least       over histories ht 
If   is  action fair for any input     we call  
approximateaction fair 

Approximatechoice fairness prevents equally good actions from being chosen at very different rates  while
approximateaction fairness prevents substantially worse
actions from being chosen over better ones  In hiring  an
approximatelyaction fair  rm can only  probabilistically 
target one population over another if the targeted population is not substantially worse  While this is   weaker
guarantee  it at least forces an approximatelyaction fair
algorithm to learn different populations to statistical con 
 dence  This is   step forward from current practices  in
which companies have much higher degrees of uncertainty
about the quality  and impact  of hiring individuals from
underrepresented populations  For this reason and the
computational bene ts mentioned above  our upper bounds
will primarily focus on approximateaction fairness 
We now state several useful observations regarding fairness  We defer all the formal statements and their proofs
to the Appendix  We note that there always exists    possibly randomized  optimal policy which is fair  Observation   moreover  any optimal policy  deterministic or randomized  is approximateaction fair  Observation   as is
the uniformly random policy  Observation  
Finally  we consider   restriction of the actions in an MDP
  to nearlyoptimal actions  as measured by     values 
De nition    Restricted MDP  The  restricted MDP of
   denoted by     is identical to   except that in each
state    the set of available actions are restricted to     
             maxa AM                         
    has the following two properties      any policy in    
is  action fair in    Observation   and  ii  the optimal
policy in     is also optimal in    Observation   Observations   and   aid our design of an approximateaction
fair algorithm  we construct     from estimates of the    
values  see Section   for more details 

  Lower Bounds
We now demonstrate   stark separation between the performance of learning algorithms with and without fairness 
First  we show that neither fair nor approximatechoice fair
algorithms achieve nearoptimality unless the number of
time steps   is at least  kn  exponential in the size of the
state space  We then show that any approximateaction fair
algorithm requires   number of time steps   that is at least
 
    to achieve nearoptimality  We start by proving  
  
lower bound for fair algorithms 
Theorem   If    

  no fair algorithm

     

  and    

Fairness in Reinforcement Learning

can be  optimal in       kn  steps 
Standard reinforcement learning algorithms  absent   fairness constraint  learn an  optimal policy in   number of
steps polynomial in   and  
    Theorem   therefore shows
  steep cost of imposing fairness  We outline the idea for
proof of Theorem   For intuition   rst consider the special
case when the number of actions       We introduce the
MDPs witnessing the claim in Theorem   for this case 
De nition    Lower Bound Example  For AM         
let          SM  AM  PM  RM          be an MDP with
  for all         PM  si          PM  si     sj     
  for            RM  si      and RM  sn      

where     min          and is   otherwise 

Figure   MDP    Circles represent states  labels denote the state
name and deterministic reward  Arrows represent actions 

Figure   illustrates the MDP from De nition   All the
transitions and rewards in   are deterministic  but the reward at state sn can be either   or  
  and so no algorithm
 fair or otherwise  can determine whether the     values
of all the states are the same or not until it reaches sn and
observes its reward  Until then  fairness requires that the
algorithm play all the actions uniformly at random  if the
reward at sn is  
  any fair algorithm must play uniformly
at random forever  Thus  any fair algorithm will take exponential time in the number of states to reach sn  This can
be easily modi ed for       from each state si        of
the actions from state si  deterministically  return to state
   and only one action  deterministically  reaches any other
state smin      It will take kn steps before any fair algorithm reaches sn and can stop playing uniformly at random
 which is necessary for nearoptimality  The same example  with   slightly modi ed analysis  also provides   lower
bound of            time steps for approximatechoice fair algorithms as stated in Theorem  
Theorem   If    
  and    
choice fair algorithm is  optimal for       
steps 

  no  
       

       

       

 

Fairness and approximatechoice fairness are both extremely costly  ruling out polynomial time learning rates 

 We have not optimized the constants upperbounding parameters in the statement of Theorems     and   The values presented here are only chosen for convenience 

 

     
    max        
  no  action fair algorithm is  
 
      steps 

Hence  we focus on approximateaction fairness  Before
moving to positive results  we mention that the time complexity of approximateaction fair algorithms will still suffer from an exponential dependence on  
   
Theorem   For    
      and    ec 
   
optimal for        
The MDP in Figure   also witnesses the claim of Theorem   when       log 
   The discount factor  
is generally taken as   constant  so in most interesting
cases
this lower bound is substantially less
stringent than the lower bounds proven for fairness and
approximatechoice fairness  Hence  from now on  we focus on designing algorithms satisfying approximateaction
fairness with learning rates polynomial in every parameter
but

    and with tight dependence on  
   

      

 

 

 

    Fair and Ef cient Learning Algorithm
We now present an approximateaction fair algorithm 
FairE  with the performance guarantees stated below 

  and
Theorem   Given                
        as inputs  FairE  is an  action fair algorithm
which achieves  optimality after
min         

       

       

   

 

 

steps where    hides polylogarithmic terms 

 

  is necessary 

The running time of FairE   which we have not attempted
to optimize  is polynomial in all the parameters of the MDP
except
    Theorem   implies that this exponential dependence on  
Several more recent algorithms       RMAX   have
improved upon the performance of    We adapted    primarily for its simplicity  While the machinery required
to properly balance fairness and performance is somewhat
involved  the basic ideas of our adaptation are intuitive 
We further note that subsequent algorithms improving on
   tend to heavily leverage the principle of  optimism in
face of uncertainty  such behavior often violates fairness 
which generally requires uniformity in the face of uncertainty  Thus  adapting these algorithms to satisfy fairness
is more dif cult  This in particular suggests    as an apt
starting point for designing   fair planning algorithm 
The remainder of this section will explain FairE  beginning with   highlevel description in Section   We
then de ne the  known  states FairE  uses to plan in Section   explain this planning process in Section   and

bring this all together to prove FairE   fairness and performance guarantees in Section  

        

Fairness in Reinforcement Learning

 

min   

   
 

  log   

   

  Informal Description of FairE 
FairE  relies on the notion of  known  states    state   is
de ned to be known after all actions have been chosen from
  enough times to con dently estimate relevant reward distributions  transition probabilities  and   
  values for each
action  At each time    FairE  then uses known states to
reason about the MDP as follows 
  If in an unknown state  take   uniformly random trajec 
  If in   known state  compute     an exploration policy
which escapes to an unknown state quickly and    the
probability that this policy reaches an unknown state
within      steps  and  ii  an exploitation policy which
is nearoptimal in the known states of   
  If   is large enough  follow the exploration policy  oth 

tory of length    
   

erwise  follow the exploitation policy 

FairE  thus relies on known states to balance exploration
and exploitation in   reliable way  While FairE  and   
share this general idea  fairness forces FairE  to more
delicately balance exploration and exploitation  For example  while both algorithms explore until states become
 known  the de nition of   known state must be much
stronger in FairE  than in    because FairE  additionally requires accurate estimates of actions    
  values in
order to make decisions without violating fairness  For this
reason  FairE  replaces the deterministic exploratory actions of    with random trajectories of actions from unknown states  These random trajectories are then used to
estimate the necessary   
In   similar vein  FairE  requires particular care in computing exploration and exploitation policies  and must restrict the set of such policies to fair exploration and fair
exploitation policies  Correctly formulating this restriction
process to balance fairness and performance relies heavily
on the observations about the relationship between fairness
and performance provided in Section  

  values 

  Known States in FairE 
We now formally de ne the notion of known states for
FairE  We say   state   becomes known when one can
compute good estimates of     RM     and PM        for all
   and  ii             for all   

De nition    Known State  Let

       kH  

     

 

       

log   

  and

  state   becomes known after taking

mQ       max      

 

lengthH  

  random trajectories from   

It remains to show that motivating conditions     and  ii 
indeed hold for our formal de nition of   known state  Informally     random trajectories suf ce to ensure that we
have accurate estimates of all            values  and   
random trajectories suf ce to ensure accurate estimates of
the transition probabilities and rewards 
To formalize condition     we rely on Theorem   connecting the number of random trajectories taken from   to the
accuracy of the empirical    
Theorem    Theorem   Kearns et al    For any
state   and     after

  estimates 

 

       

   

      kH  

  from    with probability
  such that

        simultaneously for all      

   
log 
random trajectories of length    
of at least       we can compute estimates     
            
    
Theorem   enables us to translate between the number of
trajectories taken from   state and the uncertainty about its
  values for all policies  including   and hence      
   
Since     kn  we substitute log       log     To
estimate            values using the          values we increase the number of necessary lengthH  
  random trajectories by   factor of   
For condition  ii  we adapt the analysis of      which
states that if each action in   state   is taken    times 
then the transition probabilities and reward in state   can
be estimated accurately  see Section  

  Planning in FairE 
We now formalize the planning steps in FairE  from
known states  For the remainder of our exposition  we
make Assumption   for convenience  and show how to remove this assumption in the Appendix 
Assumption       is known 

FairE  constructs two ancillary MDPs for planning    
is the exploitation MDP  in which the unknown states of  
are condensed into   single absorbing state    with no reward  In the known states   transitions are kept intact and
the rewards are deterministically set to their mean value 
   thus incentivizes exploitation by giving reward only

Fairness in Reinforcement Learning

probability that   walk of    steps from   following  
will terminate in    exceeds     

Figure   Left  An MDP   with two actions    and    and deterministic transition functions and rewards  Green denotes the
set of known states   Middle     Right      

for staying within known states  In contrast       is the
exploration MDP  identical to    except for the rewards 
The rewards in the known states   are set to   and the reward in    is set to        then incentivizes exploration
by giving reward only for escaping to unknown states  See
the middle  right  panel of Figure   for an illustration of
         and Appendix for formal de nitions 
FairE  uses these constructed MDPs to plan according to
the following natural idea  when in   known state  FairE 
constructs     and       based on the estimated transition and rewards observed so far  see the Appendix for formal de nitions  and then uses these to compute additional
restricted MDPs     
    for approximateaction
fairness  FairE  then uses these restricted MDPs to
choose between exploration and exploitation 
More formally  if the optimal policy in     
    escapes to
the absorbing state of    with high enough probability
within      steps  then FairE  explores by following that
policy  Otherwise  FairE  exploits by following the optimal policy in     
  for     steps  While following either of
these policies  whenever FairE  encounters an unknown
state  it stops following the policy and proceeds by taking
  lengthH  

  random trajectory 

  and     

  Analysis of FairE 
In this section we formally analyze FairE  and prove Theorem   We begin by proving that    
  is useful in the following sense     
  has at least one of an exploitation policy
achieving high reward or an exploration policy that quickly
reaches an unknown state in   
Lemma    Exploit or Explore Lemma  For any state    
          and any       at least one of the statements
below holds 
  there exists an exploitation policy   in    

  such that

 

max
 

TXt 

   

             

TXt 

   

              

where the random variables       and       denote the
states reached from   after following   and   for   steps 
respectively 

  there exists an exploration policy   in    

  such that the

Instead  we compute an escape policy in    

We can use this fact to reason about exploration as follows 
First  since Observation   tells us that the optimal policy in
  is approximateaction fair  if the optimal policy stays in
the set of     known states    then following the optimal
policy in    
  is both optimal and approximateaction fair 
However  if instead the optimal policy in   quickly escapes to an unknown state in    the optimal policy in    
 
may not be able to compete with the optimal policy in   
Ignoring fairness  one natural way of computing an escape
policy to  keep up  with the optimal policy is to compute
the optimal policy in      Unfortunately  following this
escape policy might violate approximateaction fairness  
highquality actions might be ignored in lieu of lowquality
exploratory actions that quickly reach the unknown states
of   
   
and show that if no nearoptimal exploitation policy exists
in    then the optimal policy in    
     which is fair by
construction  quickly escapes to the unknown states of   
Next  in order for FairE  to check whether the optimal
    quickly reaches the absorbing state of   
policy in    
with signi cant probability  FairE  simulates the execu 
    for      steps from the
tion of the optimal policy of    
known state   in    
  several times  counting the ratio of
the runs ending in    and applying   Chernoff bound  this
is where Assumption   is used 
it remains to show that
Having discussed exploration 
the exploitation policy described in Lemma   satis es  
optimality as de ned in De nition   By setting        
in Lemma   and applying Lemmas   and   we can prove
Corollary   regarding this exploitation policy 
Corollary   For any state       and         if there
exists an exploitation policy   in    

  then

 
   

TXt 

 

   

            Es           

 
     

 

  and     

    is similar to the behavior of   in    

Finally  we have so far elided the fact that FairE  only has
access to the empirically estimated MDPs     
   
 see the Appendix for formal de nitions  We remedy this
issue by showing that the behavior of any policy   in     
 
 and     
   and
    To do so  we prove   stronger claim  the behavior
   
of any   in      and       is similar to the behavior of
  in     and     
Lemma   Let   be the set of known states and     the
approximation to    Then for any state       any action
  and any policy   with probability at least      

Fairness in Reinforcement Learning

     

       min 
min 
    
            min 
  
            min 

   
   

         

      

  

   

        

We now have the necessary results to prove Theorem  

Proof of Theorem   We divide the analysis into separate
the performance guarantee of FairE  and its
parts 
approximateaction fairness  We defer the analysis of the
probability of failure of FairE  to the Appendix 
We start with the performance guarantee and show that
when FairE  follows the exploitation policy the average
     values of the visited states is close to Es         
However  when following an exploration policy or taking
random trajectories  visited states       values can be small 
To bound the performance of FairE  we bound the number of these exploratory steps by the MDP parameters so
they only have   small effect on overall performance 
Note that in each      step exploitation phase of FairE  the
expectation of the average      values of the visited states
is at least Es                by Lemmas    
and Observation   By   Chernoff bound  the probability
that the actual average      values of the visited states is
less than Es                      is less than  
if there are at least log 
 
We now bound the total number of exploratory steps of
FairE  by

exploitation phases 

 
   

       nmQH  

    nmQ

   
 

   
log   

where mQ is de ned in Equation   of De nition   The
two components of this term bound the number of rounds
in which FairE  plays nonexploitatively  the  rst bounds
the number of steps taken when FairE  follows random
trajectories  and the second bounds how many steps are
taken following explicit exploration policies  The former
bound follows from the facts that each random trajectory
has length    
    that in each state  mQ trajectories are suf 
 cient for the state to become known  and that random trajectories are taken only before all   states are known  The
latter bound follows from the fact that FairE 
follows
an exploration policy for      steps  and an exploration
policy needs to be followed only       
    times before reaching an unknown state  since any exploration policy will end up in an unknown state with probability of at
least
according to Lemma   and applying   Chernoff
bound  that an unknown state becomes known after it is
visited mQ times  and that exploration policies are only
followed before all states are known 
Finally  to make up for the potentially poor performance in

  log   

 
   

exploration  the number of      steps exploitation phases
needed is at least

              

   
Therefore  after             steps we have

 

Es           

 

 
 

TXt 

      st   

 
     

 

   

wrap

up

by

proving

  and     

the additional nT  
 

as claimed in Equation   The running time of FairE  is
   nT  
factor comes from of ine
computation of the optimal policies in     

   
FairE 
satis es
We
approximateaction fairness in every round 
The actions taken during random trajectories are fair  and hence
approximateaction fair  by Observation   Moreover 
FairE  computes policies in     
By
Lemma   with probability at least       any    or    
  or     
value estimated in     
    is within   of its
corresponding true value in    
    As   result 
  or    
    
        contain all the optimal policies and
 ii  only contain actions with    values within   of the
optimal actions  It follows that any policy followed in     
 
and     
    is  action fair  so both the exploration and
exploitation policies followed by FairE  satisfy  action
fairness  and FairE  is therefore  action fair 

  and     

  and     

   

  Discussion and Future Work
Our work leaves open several interesting questions  For example  we give an algorithm that has an undesirable exponential dependence on     but we show that this dependence is unavoidable for any approximateaction fair algorithm  Without fairness  nearoptimality in learning can
be achieved in time that is polynomial in all of the parameters of the underlying MDP  So  we can ask  does there
exist   meaningful fairness notion that enables reinforcement learning in time polynomial in all parameters 
Moreover  our fairness de nitions remain open to further
modulation  It remains unclear whether one can strengthen
our fairness guarantee to bind across time rather than simply across actions available at the moment without large
performance tradeoffs  Similarly  it is not obvious whether
one can gain performance by relaxing the everystep nature of our fairness guarantee in   way that still forbids discrimination  These and other considerations suggest many
questions for further study  we therefore position our work
as    rst cut for incorporating fairness into   reinforcement
learning setting 

Fairness in Reinforcement Learning

References
Julia Angwin  Jeff Larson  Surya Mattu  and Lauren Kirch 

ner  Machine bias  Propublica   

Anna BarryJester  Ben Casselman  and Dana Goldstein 
The new science of sentencing  The Marshall Project 
August     URL https www themarshallproject 
org thenew scienceof sentencing  Retrieved  

Ronen Brafman and Moshe Tennenholtz  RMAX    
general polynomial time algorithm for nearoptimal reinforcement learning  Journal of Machine Learning Research     

Nanette Byrnes  Arti cial intolerance  MIT Technology Review  March     URL https www 
technologyreview com   arti cialintolerance 
Retrieved  

Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard Zemel  Fairness through awareness  In
Proceedings of the  rd Innovations in Theoretical Computer Science  pages    

Michael Feldman  Sorelle Friedler  John Moeller  Carlos
Scheidegger  and Suresh Venkatasubramanian  Certifying and removing disparate impact  In Proceedings of the
 th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages    

Sorelle Friedler  Carlos Scheidegger  and Suresh Venkatasubramanian  On the  im possibility of fairness  CoRR 
abs   

Sara Hajian and Josep DomingoFerrer    methodology
for direct and indirect discrimination prevention in data
mining  IEEE Transactions on Knowledge and Data Engineering     

Moritz Hardt  Eric Price  and Nathan Srebro  Equality of
opportunity in supervised learning 
In Proceedings of
the  th Annual Conference on Neural Information Processing Systems  pages    

Matthew Joseph  Michael Kearns  Jamie Morgenstern  and
Aaron Roth  Fairness in learning  Classic and contextual
bandits  In Proceedings of the  th Annual Conference
on Neural Information Processing Systems  pages  
   

Faisal Kamiran  Asim Karim  and Xiangliang Zhang  Decision theory for discriminationaware classi cation  In
Proceedings of the  th IEEE International Conference
on Data Mining  pages    

Toshihiro Kamishima  Shotaro Akaho  Hideki Asoh  and
Jun Sakuma  Fairnessaware classi er with prejudice remover regularizer  In Proceedings of the European Conference on Machine Learning and Knowledge Discovery
in Databases  pages    

Michael Kearns and Satinder Singh  Nearoptimal reinforcement learning in polynomial time  Machine Learning     

Michael Kearns  Yishay Mansour  and Andrew Ng  Approximate planning in large POMDPs via reusable trajectories  In Proceedings of the  th Annual Conference
on Neural Information Processing Systems  pages  
   

Jon Kleinberg  Sendhil Mullainathan  and Manish Raghavan  Inherent tradeoffs in the fair determination of risk
scores  In Proceedings of the  th Conference on Innovations in Theoretical Computer Science   

Shiau Hong Lim  Huan Xu  and Shie Mannor  Reinforcement learning in robust markov decision processes  In
Proceedings of the  th Annual Conference on Neural
Information Processing Systems  pages    

Binh Thanh Luong  Salvatore Ruggieri  and Franco Turini 
kNN as an implementation of situation testing for discrimination discovery and prevention 
In Proceedings
of the  th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining  pages  
 

Shie Mannor      Mebel  and Huan Xu  Lightning does
not strike twice  Robust MDPs with coupled uncertainty 
In Proceedings of the  th International Conference on
Machine Learning   

Clair Miller  Can an algorithm hire better than   human 
The New York Times  June     URL
http www nytimes com upshot canan 
algorithmhire betterthan ahuman html  Retrieved
 

Jun Morimoto and Kenji Doya  Robust reinforcement

learning  Neural computation     

Dino Pedreshi  Salvatore Ruggieri  and Franco Turini 
Discriminationaware data mining  In Proceedings of the
 th ACM SIGKDD international conference on Knowledge discovery and data mining  pages   ACM 
 

Cynthia Rudin  Predictive policing using machine learning to detect patterns of crime  Wired Magazine  August
  URL http www wired com insights 
predictivepolicing usingmachine learningto detectpatterns ofcrime  Retrieved  

Fairness in Reinforcement Learning

Satinder Singh  Personal Communication  June  

Richard Sutton and Andrew Barto  Introduction to Reinforcement Learning  MIT Press  Cambridge  MA  USA 
 st edition   

Latanya Sweeney  Discrimination in online ad delivery 

Communications of the ACM     

Istv an Szita and Csaba Szepesv ari  Modelbased reinforcement learning with nearly tight exploration complexity bounds 
In Proceedings of the  th International Conference on Machine Learning  pages  
   

Blake Woodworth  Suriya Gunasekar  Mesrob Ohannessian  and Nathan Srebro  Learning nondiscriminatory
predictors 
In Proceedings of the  th Conference on
Learning Theory   

Muhammad Bilal Zafar  Isabel Valera  GomezRodriguez
Manuel  and Krishna    Gummadi  Fairness beyond disparate treatment and disparate impact  Learning classi 
 cation without disparate mistreatment  In Proceedings
of the  th International World Wide Web Conference 
 

Rich Zemel  Yu Wu  Kevin Swersky  Toni Pitassi  and Cynthia Dwork  Learning fair representations  In Proceedings of the  th International Conference on Machine
Learning  pages    

