Neural Taylor Approximations 

Convergence and Exploration in Recti er Networks

David Balduzzi   Brian McWilliams   Tony ButlerYeoman  

Abstract

Modern convolutional networks 
incorporating
recti ers and maxpooling  are neither smooth
nor convex  standard guarantees therefore do not
apply  Nevertheless  methods from convex optimization such as gradient descent and Adam are
widely used as building blocks for deep learning
algorithms  This paper provides the  rst convergence guarantee applicable to modern convnets 
which furthermore matches   lower bound for
convex nonsmooth functions  The key technical tool is the neural Taylor approximation    
straightforward application of Taylor expansions
to neural networks   and the associated Taylor
loss  Experiments on   range of optimizers  layers  and tasks provide evidence that the analysis
accurately captures the dynamics of neural optimization  The second half of the paper applies
the Taylor approximation to isolate the main dif 
 culty in training recti er nets   that gradients
are shattered   and investigates the hypothesis
that  by exploring the space of activation con 
 gurations more thoroughly  adaptive optimizers
such as RMSProp and Adam are able to converge
to better solutions 

  Introduction
Deep learning has achieved impressive performance on  
range of tasks  LeCun et al    The workhorse underlying deep learning is gradient descent or backprop  Gradient descent has convergence guarantees in settings that
are smooth  convex or both  However  modern convnets
are neither smooth nor convex  Every winner of the ImageNet classi cation challenge since   has used recti 
 ers which are not smooth  Krizhevsky et al    Zeiler

 Victoria University of Wellington  New Zealand  Disney Research    urich  Switzerland  Correspondence to  David Balduzzi
 dbalduzzi gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Fig    Shattered gradients in   PLfunction 

  Fergus    Simonyan   Zisserman    Szegedy
et al    He et al    Even in convex settings 
convergence for nonsmooth functions is lowerbounded by
 pN  Bubeck   
The paper   main contribution is the  rst convergence result for modern convnets  Theorem   The idea is simple  backprop constructs linear snapshots  gradients  of  
neural net   landscape  section   introduces neural Taylor
approximations which are used to construct Taylor losses
as convex snapshots closely related to backprop  We then
use the online convex optimization framework  Zinkevich 
  to show  pN convergence to the Taylor optimum 
matching the lower bound in  Bubeck    Section  
investigates the Taylor optimum and regret terms empirically  We observe that convergence to the Taylor optimum
occurs at  pN in practice  The theorem applies to any
neural net with   loss convex in the output of the net  for
example  the crossentropy loss is convex in the output but
not the parameters of   neural net 
The nonsmoothness of recti er nets is perhaps underappreciated  Balduzzi et al    Fig    shows   piecewiselinear  PL  function and its gradient  The gradient is discontinuous or shattered  Shattering is problematic for accelerated and Hessianbased methods which speed up convergence by exploiting the relationship between gradients
at nearby points  Sutskever et al    The success of
these methods on recti er networks  where the number of
kinks grows exponentially with depth  Pascanu et al   
Telgarsky    requires explanation since gradients at
nearby points can be very different  Balduzzi et al   
Section   addresses the success of adaptive optimizers

Neural Taylor Approximations

in recti er nets  Adaptive optimizers normalize gradients by their rootmean square       AdaGrad  RMSProp 
Adam and RadaGrad  Duchi et al    Hinton et al 
  Kingma   Ba    Krummenacher et al   
Dauphin et al    argue that RMSProp approximates

the equilibriation matrixpdiag     which approximates
the absolute Hessian      Dauphin et al    However  the argument is at best part of the story when gradients are shattered  In fact  curvaturebased explanations
for RMSnormalization schemes do not tell the whole story
even in smooth convex settings  Krummenacher et al 
  and Duchi et al    show that diagonal normalization schemes show no theoretical improvement over
vanilla SGD when the coordinates are not axisaligned or
extremely sparse respectively 
The only way an optimizer can estimate gradients of   shattered function is to compute them directly  Effective optimizers must therefore explore the space of smooth regions
  the bound in theorem   is only as good as the optimum
over the Taylor losses encountered during backprop  Observations   and   relate smooth regions in recti er nets and
the Taylor losses to con gurations of active neurons  We
hypothesize that rootmean square normalization increases
exploration through the set of smooth regions in   recti er
net   landscape  Experiments in section   provide partial
support for the hypothesis 

  Comparison with related work
Researchers have applied convex techniques to neural networks  Bengio et al    show that choosing the number
of hidden units converts neural optimization into   convex
problem  see also Bach     convex multilayer architectures are developed in Aslan et al    Zhang et al 
  However  these approaches have not achieved the
practical success of convnets  In this work  we analyze convnets as they are rather than proposing   more tractable  but
potentially less useful  model    Taylor decomposition for
neural networks was proposed in Montavon et al   
They treat inputs as variable instead of weights and study
interpretability instead of convergence  Taylor approximations to neural nets have also been used in Schraudolph
  Martens et al    to construct the generalized
GaussNewton matrix as an alternative to the Hessian 
Our results are closely related to Balduzzi   which
uses gametheoretic techniques to prove convergence in
recti er nets  The approach taken here is more direct and
holds in greater generality 

 For simplicity  we restrict to fully connected recti er  ReLU 
nets 
The results also apply to convolutions  maxpooling 
dropout  dropconnect  maxout  PReLUs and CReLUs  Srivastava
et al    Wan et al    Goodfellow et al    He et al 
  Shang et al   

  Convergence of Neural Networks
Before presenting the main result  we highlight some issues that arise when studying convergence in recti er nets 
Many optimization methods have guarantees that hold in
convex or smooth settings  However  none of the guarantees extend to recti er nets  For example  the literature provides no rigorous account of when or why Adam
or Adagrad converges faster on recti er nets than vanilla
gradient descent  Instead  we currently have only intuition 
empirics and an analogy with convex or smooth settings 
Gradientbased optimization on neural nets can converge
on local optima that are substantially worse than the global
optimum  Fortunately   bad  local optima are rare in practice    partial explanation for the prevalence of  good
enough  local optima is Choromanska et al    Nevertheless  it is important to acknowledge that neural nets
can and do converge to bad local optima 
It is therefore
impossible to prove  nonstochastic  bounds relative to the
global optimum  Such   result may be provable under further assumptions  However  since the result would contradict empirical evidence  the assumptions would necessarily
be unrealistic 

  What kind of guarantee is possible 
The landscape of   recti er net decomposes into smooth
regions separated by kinks  Pascanu et al    Telgarsky    see  gure   Gradients on different sides of
  kink are unrelated since the derivative is discontinuous 
Gradientbased optimizers cannot  peer around  the kinks
in recti er nets 
Gradientbased optimization on recti er nets thus decomposes into two components  The  rst is steepest descent
in   smooth region  the second moves between smooth regions  The  rst component is vanilla optimization whereas
the second involves an element of exploration  what the
optimizer encounters when it crosses   kink cannot be predicted 
The convergence guarantee in theorem   takes both components of the factorization into account in different ways 
It is formulated in the adversarial setting of online convex
optimization  Intuitively  the adversary is the nonsmooth
geometry of the landscape  which generates what mayas 
wellbe   new loss whenever backprop enters   different
smooth region 
Backprop searches   vast nonconvex landscape with   linear  ashlight  the Taylor losses are   more sharply focused
convex  ashlight  see    The adversary is the landscape  from backprop   perspective its geometry   especially across kinks   is an unpredictable external force 
The Taylor losses are   series of convex problems that back 

Neural Taylor Approximations

Tn 

smooth regions

 
 
 
 

 

Tn

Tn 

Wn  Wn Wn 

Fig    Neural Taylor approximation 

prop de facto optimizes   the gradients of the actual and
Taylor losses are identical  The Taylor optimum improves
when  stepping over   kink  backprop shines its light on  
new  better  region of the landscape       Regret quanti 
 es the gap between the Taylor optimal loss and the losses
incurred during training 

  Online Convex Optimization
In online convex optimization  Zinkevich      learner
is given convex loss functions            On the nth round 
the learner predicts Wn prior to observing     and then
incurs loss    Wn  Since the losses are not known in advance  the performance of the learner is evaluated post hoc
via the regret  the difference between the incurred losses
and the optimal loss in hindsight 

losses incurred

Regret      

NXn      Wn 
      
where      argminV HhPN

optimalin hindsighti
       
      
           An algorithm

has noregret if limN  Regret          for any sequence of convex losses with bounded gradients  For example  Kingma   Ba   prove 
Theorem    Adam has noregret 
Suppose the convex losses    have bounded gradients
krW             and krW             for all
     and suppose that the weights chosen by the algorithm satisfy kWm   Wnk      and kWm   Wnk   
  for all                     Then Adam satis es
Regret          pN  for all      

 

The regret of gradient descent  AdaGrad  Duchi et al 
  mirror descent and   variety of related algorithms
satisfy   albeit with different constant terms that are hidden in the bigO notation  Finally  the  pN rate is also

  lowerbound  It cannot be improved without additional
assumptions 

  Neural Taylor Approximation
Consider   network with       hidden layers and weight
matrices                 WL  Let    denote the input 
For hidden layers                     set al   Wl   xl 
and xl     al  where    is applied coordinatewise  The
last layer outputs xL   aL   WL   xL  Let pl denote the size of the lth layer     is the size of the input
and pL is the size of the output  Suppose the loss        
is smooth and convex in the  rst argument  The training
   The network is trained on stochasdata is  xd  yd  
tic samples from the training data on   series of rounds
                 For simplicity we assume minibatch size
  the results generalize without dif culty 
We recall backprop using notation from Martens et al 
  Let Ja
  denote the Jacobian matrix of the vector  
with respect to the vector    By the chain rule the gradient
decomposes as

 

 

rWl  fW         JEL   JL
 
  JEL   

rf       

  

 xl 

     Jl 
  
 
  JL
  
 
rWl fW   Gl

    xl 

 

 

is the backpropagated error computed rewhere      JEl
  The middle expression in
cursively via            Jl 
  is the standard representation of backpropagated gradients  The expression on the right factorizes the backpropagated error      JEL JL
  into the gradient of the loss JEL and
the Jacobian JL
  between layers  which describes gradient
 ow within the network 
The  rstorder Taylor approximation to   differentiable
function           near   is Ta                   
The neural Taylor approximation for   fully connected network is as follows 

De nition   The Jacobian tensor of layer    Gl   JL
   
xl  is the gradient of the output of the neural network
with respect to the weights of layer    It is the outer product of    pL   pl matrix with   pl vector  and so is  
 pL  pl  pl tensor 
Given Gl and  pl   pl matrix   
the expression
      xl  is the pLvector computed via
hGl  Vi   JL
matrixmatrix vector multiplication  The neural Taylor approximation to   in   neighborhood of Wn  given input xn
   
 Note  we suppress the dependence of the Jacobians on the

 

round   to simplify notation 

is the  rstorder Taylor expansion

Tn
  Vl     Tn

  Vl  yn  Then 

Neural Taylor Approximations

Tn      fWn xn

     

LXl Gl  Vl   Wn

     fV xn

   

Finally 
Tn       Tn    yn 

the Taylor loss of the network on round   is

The Taylor approximation to layer   is

Tn

   Vl    fWn xn

     Gl  Vl   Wn
  

We can also construct the Taylor approximation to neuron
  in layer    Let the pLvector JL
        denote the
Jacobian with respect to neuron   and let the  pL   pl 
matrix      JL
    xl  denote the Jacobian with respect
to the weights of neuron   The Taylor approximation to
neuron   is

    JL

Tn

      fWn xn

             Wn
 

The Taylor losses are the simplest nontrivial       nonaf ne  convex functions encoding the information generated by backprop  see section   
The following theorem provides convergence guarantees at
mutiple spatial scales  networkwise  layerwise and neuronal  See sections    for   proof of the theorem  It is not
currently clear which scale provides the tightest bound 
Theorem    noregret relative to Taylor optimum 
Suppose  as in Theorem   the Taylor losses have bounded
gradients and the weights of the neural network have
bounded diameter during training 
Suppose the neural net is optimized by an algorithm with

Regret         pN   such as gradient descent  Ada 

Grad  Adam or mirror descent 

  Network guarantee 

The running average of the training error of the neural
network satis es

 

Tn   
 

 

 

 
 

  

    yn 

  min

NXn 

   fWn xn

running average of training errors

     
NXn 
  
 
      
pN 
 
  
 
  Layerwise   Neuronwise guarantee 

Regret     

 

 

The Taylor

loss of

 layerl

Taylor optimum

  neuron 

is

   fWn xn

    yn 

running average of training errors

NXn 

 
 

 

  

Tn

  Vl 
 

layerwise neuronal Taylor optimum
 

 

 

  min

Vl   
NXn 
 
  
pN 
      
 
  
 

Regret     

 

The running average of errors during training  or cumulative loss  is the same quantity that arises in the analyses
of Adam and Adagrad  Kingma   Ba    Duchi et al 
 

Implications of the theorem  The global optima of neural nets are not computationally accessible  Theorem  
sidesteps the problem by providing   guarantee relative to
the Taylor optimum  The bound is pathdependent  it depends on the convex snapshots encountered by backprop
during training 
Pathdependency is   key feature of the theorem 
It is  
simple matter to construct   deep fully connected network
    layers  that fails to learn because gradients do not
propagate through the network  He et al      convergence theorem for neural nets must also be applicable
in such pathological cases  Theorem   still holds because
the failure of gradients to propagate through the network
results in Taylor losses with poor solutions 
Although the bound in theorem   is pathdependent  it is
nevertheless meaningful  The righthand side is given by
the Taylor optimum  which is the optimal solution to the
best convex approximations to the actual losses  best in the
sense that they have the same value and have the same gradient for the encountered weights  The theorem replaces  
seemingly intractable problem   neither smooth nor convex
  with   sequence of convex problems 
Empirically  see below  we  nd that the Taylor optimum is
  tough target on   range of datasets and settings  MNIST
and CIFAR  supervised and unsupervised learning  convolutional and fullyconnected architectures  under   variety of optimizers  Adam  SGD  RMSProp  and for individual neurons as well as entire layers 
Finally  the decomposition of learning over recti er networks into vanilla optimization and exploration components suggests investigating the exploratory behavior of
different optimizers   with the theorem providing concrete
tools to do so  see section  

 

 
 
 
 
 

 
 
 

 
 
 
 

 

 

 

 

 

 
 
 
 

 
 
 

 
 
 
 

 

 

 

Layer  
Layer  
Layer  
 

 

 

 
 
epochs

    All conv layers

Neural Taylor Approximations

Neuron loss
Taylor optimal
Regret
Network loss

 

 
 
epochs

 

 

    Layer    input layer 

 
 
 
 

 
 
 

 
 
 
 

 

 

 

Neuron loss
Taylor optimal
Regret
Network loss

 

 
 
epochs
    Layer  

 

 

Fig    Average normalized cumulative regret for RMSProp on CIFAR      Average regret incurred by neurons in
each layer over   neurons layer        Average regret incurred eachs neuron in layers   and   respectively  along with
average loss  Taylor optimum and cumulative network loss  Shaded areas represent one standard deviation 

  Empirical Analysis of Online Neural Optimization
This section empirically investigates the Taylor optimum
and regret terms in theorem   on two tasks 
Autoencoder trained on MNIST  Dense layers with architecture                           and
ReLU nonlinearities  Trained with MSE loss using minibatches of  
Convnet trained on CIFAR  Three convolutional layers with stack size   and   receptive  elds  ReLU nonlinearities and       maxpooling  Followed by     unit
fullyconnected layer with ReLU before   tendimensional
fullyconnected output layer  Trained with crossentropy
loss using minibatches of  
For both tasks we compare the optimization performance of
Adam  RMSProp and SGD  gure   in appendix  Learning rates were tuned for optimal performance  Additional
parameters for Adam and RMSProp were left at default 
For the convnet all three methods perform equally well 
achieving   small loss and an accuracy of     on the
training set  However  SGD exhibits slightly more variance  For the autoencoder  although it is an extremely simple model  SGD with the best  xed  learning rate performs
signi cantly worse than the adaptive optimizers 
The neuronal and layerwise regret are evaluated for each
model  At every iteration we record the training error   the
lefthand side of eq    To evaluate the Taylor loss  we
record the input to the neuron layer  its weights  the output
of the network and the gradient tensor Gl  After training 
we minimize the Taylor loss with respect to   to  nd the
Taylor optimum at each round  The regret is the difference
between the observed training loss and the optimal Taylor
loss 
The  gures show cumulative losses and regret  For illustrative purposes we normalize by  pN  quantities growing

at pN therefore  atten out  Figure     compares the average regret incurred by neurons in each convolutional layer
of the convnet  Shaded regions show one standard deviation  Dashed lines are the regret of individual neurons  
importantly the regret behaviour of neurons holds both on
average and individually  Figs     and     show the regret  cumulative loss incurred by the network  the average
loss incurred and the Taylor optimal loss for neurons in layers   and   respectively 
Fig    compares Adam  RMSProp and SGD  Figure    
shows the layerwise regret on the convnet  The regret of all
of the optimizers scales as pN for both models  matching
the bound in Theorem   The additional variance exhibited by SGD explains the difference in regret magnitude 
Similar behaviour was observed in the other layers of the
networks and also for convnets trained on MNIST 
Figure     shows the same plot for the autoencoder  The
regret of all methods scales as pN  this also holds for the
other layers in the network  The gap in performance can
be further explained by examining the difference between
the observed loss and Taylor optimal loss  Figure     compares these quantities for each method on the autoencoder 
The adaptive optimizers incur lower losses than SGD  Further  the gap between the actually incurred and optimal loss
is smaller for adaptive optimizers  This is possibly because
adaptive optimizers  nd better activation con gurations of
the network  see discussion in section  
Remarkably   gures   and   con rm that regret scales as
pN for   variety of optimizers  datasets  models  neurons
and layers   verifying the multiscale guarantee of Theorem     possible explanation for why optimizers match
the worstcase  pN  regret is that the adversary  that is 
the landscape  keeps revealing Taylor losses with better solutions  The optimizer struggles to keep up with the hindsight optimum on the constantly changing Taylor losses 

 

 

 

 

 

 
 
 
 
 

 
 
 

 
 
 
 

Adam
RMSProp
SGD
 

 

 

 
 
epochs

Neural Taylor Approximations

 

 
 
 
 
 

 
 
 

 
 
 
 

  
  
  
  
  
  

 
 
 
 

 
 
 

 
 
 
 

 

 

 

Adam
RMSProp
SGD
 

 

 

 

epochs

Taylor optimal
loss

 

 

 

 

epochs

    Convnet regret  layer  

    Autoencoder regret  layer  

    Cumul  losses for autoencoder

Fig    Comparison of regret for Adam  RMSProp and SGD  The yaxis in     is scaled by       reports cumulative loss and Taylor optimal loss on layer   for each method 

  Optimization and Exploration in Recti er

Neural Networks

Poor optima in recti er nets are related to shattered gradients  backprop cannot estimate gradients in nearby smooth
regions without directly computing them 
the  ashlight
does not shine across kinks  Two recent papers have shown
that noise improves the local optima found during training  Neelakantan et al    introduce noise into gradients whereas Gulcehre et al    use noisy activations to
extract gradient information from across kinks  Intuitively 
noise is   mechanism to  peer around kinks  in shattered
landscapes 
Introducing noise is not the only way to  nd better optima 
Not only do adaptive optimizers often converge faster than
vanilla gradient descent  they often also converge to better
local minima 
This section investigates how adaptive optimizers explore
shattered landscapes  Section   shows that smooth regions in recti er nets correspond to con gurations of active
neurons and that neural Taylor approximations clamp the
activation con guration       
the convex  ashlight does
not shine across kinks in the landscape  Section   observes that adaptive optimizers incorporate an exploration
bias and hypothesizes that the success of adaptive optimizers derives from exploring the set of smooth regions more
extensively than SGD  Section   evaluates the hypothesis
empirically 

  The Role of Activation Con gurations in

Optimization

We describe how con gurations of active neurons relate to
smooth regions of recti er networks and to neural Taylor
approximations  Recall that the loss of   neural net on its

training data is

     

 
 

DXd 

 fW xd  yd 

De nition   Enumerate the data as                   
and neurons as      The activation con guration     
is          binary matrix representing the active neurons
for each input  The set of all possible activation con gurations corresponds to the set of all        binary matrices 
Observation    activation con gurations correspond to
smooth regions in recti er networks 
  parameter value exhibits   kink in   iff an in nitesimal
change alters the of activation con guration         is not
smooth at   iff there is                          
for all    

The neural Taylor approximation to   recti er net admits  
natural description in terms of activation con gurations 
Observation    the Taylor approximation clamps activation con gurations in recti er networks 
Suppose datapoint   is sampled on round    Let     
  Wn    layer    be the pkvector given by entries of
row   of   Wn  corresponding to neurons in layer   of  
recti er network  The Taylor approximation Tn

  is

Tn

   Vl        Yk  

Wn

 

    diag    
 
  

clamped

   Vl   Wn
   
 
  
 

free

which clamps the activation con guration  and weights of
all layers excluding   

Observations   and   connect shattered gradients in recti 
 er nets to activation con gurations and the Taylor loss 
The main implication is to factorize neural optimization

Neural Taylor Approximations

into hard  nding  good  smooth regions  and easy  optimizing within   smooth region  subproblems that correspond  roughly  to  nding  good  Taylor losses and optimizing them respectively 

  RMSNormalization encourages Exploration
Adaptive optimizers based on rootmean square normalization exhibit an upto exponential improvement over nonadaptive methods when gradients are sparse  Duchi et al 
  or lowrank  Krummenacher et al    in convex
settings  We propose an alternate explanation for the performance of adaptive optimizers in nonconvex nonsmooth
settings 
        denote the average gradient over
Let       
  dataset  RProp replaces the average gradient with its coordinatewise sign  Riedmiller   Braun    An interesting characterization of the signedgradient is
Observation    signedgradient is   maximizer 
Suppose none of the coordinates in    are zero  The
signedgradient satis es

DPD

sign      argmax
  Bp

  nkxk               

where Bp

         Rp   max

    xi   

The signedgradient
therefore has two key properties 
Firstly  small weight updates using the signedgradient decrease the loss since hr  sign         Secondly  the
signedgradient is the update that  subject to an   constraint  has the largest impact on the most coordinates  To
adapt RProp to minibatches  Hinton and Tieleman suggested to approximate the signed gradient by normalizing
with the rootmean square  sign      PD
       
         
pPD
where        is the square taken coordinatewise  Viewing
the signedgradient as changing weights   or exploring  
maximally suggests the following hypothesis 
Hypothesis    RMSnormalization encourages exploration
over activation con gurations 
Gradient descent with RMSnormalized updates  or running average of RMS  performs   broader search through
the space of activation con gurations than vanilla gradient
descent 

  Empirical Analysis of Exploration by Adaptive

Optimizers

Motivated by hypothesis   we investigate how RMSProp
and SGD explore the space of activation con gurations on
the tasks from section   see    for details 
For  xed parameters    the activation con guration of  
neural net with   neurons and   datapoints is represented

 

totPN 

    An     

as            binary matrix  recall de nition   The set of
activation con gurations encountered by   network over  
rounds of training is represented by an            binary
tensor denoted   where An                Wn 
Figure   quanti es exploration in the space of activation
con gurations in three ways 
    the
    Hamming distance plots mink   kAn   Akk 
minimum Hamming distance between the current activation con guration and all previous con gurations  It indicates the novelty of the current activation con guration 
    Activationstate switches plots

 An     

    the total number of times each data point
 sorted  switches its activation state across all neurons and
epochs as   proportion of possible switches  It indicates the
variability of the network response 
    Logproduct of singular values  The matrix       
     speci es the rounds and datapoints that activate neuron
   The right column plots the logproduct of           
 rst   singular values for each neuron  sorted  It indicates the  log volume of con guration space covered by
each neuron  Note that values reaching the bottom of the
plot indicate singular values near  
We observe the following 
RMSProp explores the space of activation con gurations far more than SGD  The result holds on both tasks 
across all three measures  and for multiple learning rates
for SGD  including the optimally tuned rate  The  nding
provides evidence for hypothesis  
RMSProp converges to   signi cantly better local optimum on the autoencoder  see Fig    We observe no
difference on CIFAR  We hypothesize that RMSProp
 nds   better optimum through more extensive exploration
through the space of activation con gurations  CIFAR is
an easier problem and possibly requires less exploration 
Adam explores less than RMSProp  Adam achieves the
best performance on the autoencoder  Surprisingly  it explores substantially less than RMSProp according to the
Hamming distance and activationswitches  although still
more than SGD  The singular values provide   higherresolution analysis 
the   most exploratory neurons
match the behavior of RMSProp  with   sharp dropoff from
neuron   onwards    possible explanation is that momentum encourages targeted exploration by rapidly discarding avenues that are not promising  The results for Adam
are more ambiguous than for RMSProp compared to SGD 
 The timeaverage is subtracted from each column of       
     If the response of neuron   to datapoint   is constant over
all rounds  then column            maps to             and does
not contribute to the volume 

Neural Taylor Approximations

 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

    Minimum hamming distance 

    Activationstate switches 

    Logproduct of singular values 

Fig    Top  results for   CIFARtrained convnet  Bottom  MNISTtrained autoencoder 
    Minimum hamming
distance between the activation con guration at curent epoch and all previous epochs 
    Number of activationstate
switches undergone for all neurons over all epochs for each data point  sorted      Logproduct of the  rst   singular
values of each neuron activation con guration  sorted 

More generally  the role of momentum in nonsmooth nonconvex optimization requires more investigation 

  Discussion
Recti er convnets are the dominant technology in computer vision and   host of related applications  Our main
contribution is the  rst convergence result applicable to
convnets as they are used in practice  including recti er
nets  maxpooling  dropout and related methods  The key
analytical tool is the neural Taylor approximation  the  rstorder approximation to the output of   neural net  The Taylor loss   the loss on the neural Taylor approximation   is  
convex approximation to the loss of the network  Remarkably  the convergence rate matches known lower bounds on
convex nonsmooth functions  Bubeck    Experiments
in section   show the regret matches the theoretical analysis under   wide range of settings 
The bound in theorem   contains an easy term to optimize
 the regret  and   hard term  nding  good  Taylor losses 
Section   observes that the Taylor losses speak directly to
the fundamental dif culty of optimizing nonsmooth functions  that gradients are shattered   the gradient at   point
is not   reliable estimate of nearby gradients 
Smooth regions of recti er nets correspond to activation

con gurations  Gradients in one smooth region cannot be
used to estimate gradients in another  Exploring the set of
activation con gurations may therefore be crucial for optimizers to  nd better local minima in shattered landscapes 
Empirical results in section   suggest that the improved
performance of RMSProp over SGD can be explained in
part by   carefully tuned exploration bias 
Finally  the paper raises several questions 

  To what extent is exploration necessary for good per 

formance 

  Can exploration exploitation tradeoffs in nonsmooth

neural nets be quanti ed 

  There are exponentially more kinks in early layers
 near the input  compared to later layers  Should optimizers explore more aggressively in early layers 

  Can exploring activation con gurations help design

better optimizers 

The Taylor decomposition provides   useful tool for separating the convex and nonconvex aspects of neural optimization  and may also prove useful when tackling exploration in neural nets 

RMSPropAdamSGD lr SGD lr SGD lr epochs data  ordered neurons  ordered epochs data  ordered neurons  ordered Neural Taylor Approximations

Acknowledgements
We thank    Helminger and    Vogels for useful discussions and help with TensorFlow  Some experiments were
performed using   Tesla    kindly donated by Nvidia 

References
Aslan     Zhang     and Schuurmans  Dale  Convex Deep Learn 

ing via Normalized Kernels  In NIPS   

Bach  Francis  Breaking the Curse of Dimensionality with Con 

vex Neural Networks  In arXiv   

Balduzzi  David  Deep Online Convex Optimization with Gated

Games  In arXiv   

Balduzzi  David  Frean  Marcus  Leary  Lennox  Ma  Kurt WanDuo  and McWilliams  Brian  The Shattered Gradients Problem  If resnets are the answer  then what is the question  In
ICML   

Bengio  Yoshua  Roux  Nicolas Le  Vincent  Pascal  Delalleau 
Olivier  and Marcotte  Patrice  Convex Neural Networks  In
NIPS   

Bubeck    ebastien  Convex Optimization  Algorithms and Complexity  Foundations and Trends in Machine Learning   
   

Choromanska     Henaff     Mathieu     Arous       and LeCun     The loss surface of multilayer networks  In Journal of
Machine Learning Research  Workshop and Conference Proceeedings  volume    AISTATS   

Dauphin  Yann  Pascanu  Razvan  Gulcehre  Caglar  Cho 
IdentifyKyunghyun  Ganguli  Surya  and Bengio  Yoshua 
ing and attacking the saddle point problem in highdimensional
nonconvex optimization  In NIPS   

Dauphin  Yann  de Vries  Harm  and Bengio  Yoshua  Equilibrated adaptive learning rates for nonconvex optimization  In
NIPS   

Duchi  John  Hazan  Elad  and Singer  Yoram  Adaptive Subgradient Methods for Online Learning and Stochastic Optimization 
JMLR     

Duchi  John  Jordan  Michael    and McMahan  Brendan  Estimation  optimization  and parallelism when data is sparse  In Advances in Neural Information Processing Systems  pp   
   

Goodfellow  Ian    WardeFarley  David  Mirza  Mehdi  Courville 
In ICML 

Aaron  and Bengio  Yoshua  Maxout Networks 
 

Gulcehre  Caglar  Moczulski  Marcin  Denil  Misha  and Bengio 

Yoshua  Noisy Activation Functions  In ICML   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian 
Delving Deep into Recti ers  Surpassing HumanLevel Performance on ImageNet Classi cation  In ICCV   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian 
In CVPR 

Deep Residual Learning for Image Recognition 
 

Hinton     Srivastava  Nitish  and Swersky  Kevin  Lecture    

Overview of minibatch gradient descent   

Kingma  Diederik   and Ba  Jimmy Lei  Adam    method for

stochastic optimization  In ICLR   

Krizhevsky     Sutskever     and Hinton       Imagenet classi 
cation with deep convolutional neural networks  In Advances
in Neural Information Processing Systems  NIPS   

Krummenacher  Gabriel  McWilliams  Brian  Kilcher  Yannic 
Buhmann  Joachim    and Meinshausen  Nicolai  Scalable
adaptive stochastic optimization using random projections  In
NIPS   

LeCun  Yann  Bengio  Yoshua  and Hinton  Geoffrey  Deep learn 

ing  Nature     

Martens  James  Sutskever  Ilya  and Swersky  Kevin  Estimating

the Hessian by Backpropagating Curvature  In ICML   

Montavon     Bach     Binder     Samek     and   uller     Explaining NonLinear Classi cation Decisions with Deep Taylor
Decomposition  In arXiv   

Neelakantan  Arvind  Vilnis  Luke  Le  Quoc  Sutskever  Ilya 
Kaiser  Lukasz  Kurach  Karol  and Martens  James  Adding
Gradient Noise Improves Learning for Very Deep Networks 
In ICLR   

Pascanu  Razvan  Gulcehre  Caglar  Cho  Kyunghyun  and Bengio  Yoshua  On the number of inference regions of deep feed
forward networks with piecewise linear activations  In ICLR 
 

Riedmiller  Martin and Braun       direct adaptive method for
In

faster backpropagation learning  The RPROP algorithm 
IEEE Int Conf on Neural Networks  pp         

Schraudolph  Nicol    Fast Curvature MatrixVector Products
for SecondOrder Gradient Descent  Neural Comp   
   

Shang  Wenling  Sohn  Kihyuk  Almeida  Diogo  and Lee 
Honglak  Understanding and Improving Convolutional Neural
Networks via Concatenated Recti ed Linear Units  In ICML 
 

Simonyan  Karen and Zisserman  Andrew  Very Deep Convolutional Networks for LargeScale Image Recognition  In ICLR 
 

Srivastava  Nitish  Hinton  Geoffrey  Krizhevsky  Alex  Sutskever 
Ilya  and Salakhutdinov  Ruslan  Dropout    Simple Way to
Prevent Neural Networks from Over tting  JMLR   
   

Sutskever  Ilya  Martens  James  Dahl  George  and Hinton  Geoffrey  On the importance of initialization and momentum in
deep learning  In Proceedings of the  th International Conference on Machine Learning  ICML  pp     

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet  Pierre 
Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew  Going Deeper With
Convolutions  In CVPR   

Telgarsky  Matus  Bene ts of depth in neural networks  In COLT 

 

Neural Taylor Approximations

Wan  Li  Zeiler  Matthew  Zhang  Sixin  LeCun  Yann  and Fergus  Rob  Regularization of Neural Networks using DropConnect  In ICML   

Zeiler  Matthew and Fergus  Rob  Visualizing and Understanding

Convolutional Networks  In ECCV   

Zhang  Yuchen  Liang  Percy  and Wainwright  Martin    ConvexIn arXiv 

  ed Convolutional Neural Networks 
 

Zinkevich  Martin  Online Convex Programming and Generalized

In nitesimal Gradient Ascent  In ICML   

