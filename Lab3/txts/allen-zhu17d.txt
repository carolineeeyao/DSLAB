Follow the Compressed Leader 

Faster Online Learning of Eigenvectors and Faster MMWU

Zeyuan AllenZhu     Yuanzhi Li    

Abstract

 

is fundamental

The online problem of computing the top eigenvector
to machine learning 
The famous matrixmultiplicative weightupdate
 MMWU  framework solves this online problem and gives optimal regret  However  since
MMWU runs very slow due to the computation of matrix exponentials  researchers proposed
the followthe perturbedleader  FTPL  framed worse than
work which is faster  but   factor
the optimal regret for dimensiond matrices  We
propose   followthe compressedleader framework which  not only matches the optimal regret
of MMWU  up to polylog factors  but runs no
slower than FTPL  Our main idea is to  compress  the MMWU strategy to dimension   in
the adversarial setting  or dimension   in the
stochastic setting  This resolves an open question regarding how to obtain both  nearly  optimal and ef cient algorithms for the online eigenvector problem 

  Introduction
Finding leading eigenvectors of symmetric matrices is one
of the most primitive problems in machine learning  In this
paper  we study the online variant of this problem  which
is   learning game between   player and an adversary  Nie
et al    Kot owski   Warmuth    Dwork et al 
  Garber et al    Abernethy et al   

Online Eigenvector Problem  The player plays   unitnorm vectors            wT   Rd in   row  after playing wk 
the adversary picks   feedback matrix Ak   Rd   that is
Future version of this paper shall
be found at https arxiv org abs 
 Microsoft Research  Princeton University  Correspondence
to  Zeyuan AllenZhu  zeyuan csail mit edu  Yuanzhi Li
 yuanzhil cs princeton edu 

 Equal contribution  

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

symmetric and satis es    cid  Ak  cid     Both these assumptions are for the sake of simplicity and can be relaxed  The
player then receives   gain

  Akwk   Ak   wkw cid 
  cid 

         

The regret minimization problem asks us the player to design   strategy to minimize regret  that is  the difference
between the total gain obtained by the player and that by
the   posteriori best  xed strategy     Rd 

minimize max
  Rd

 cid  
  Ak    uu cid    wkw cid 
 cid          AT       cid 
   

   max

  cid 
  Akwk  

  

The name comes from the fact that the player chooses only
vectors in   row  but wants to compete against the leading
eigenvector in hindsight  To make this problem meaningful  the feedback matrix Ak  is not allowed to depend on
wk but can depend on            wk 
  Known Results

The most famous solution to the online eigenvector problem is the matrix multiplicativeweight update  MMWU 
method  which has also been used towards ef cient algorithms for SDP  balanced separators  Ramanujan sparsi 
 ers  and even in the proof of QIP   PSPACE 

MMWU  At iteration    de ne Wk   exp   
Tr exp   
where           Ak  and       is the learning
rate  Then  compute its eigendecomposition

Tr exp     cid  

Wk   exp   

   pj   yjy cid 

 

where vectors yj are normalized eigenvectors  Now  the
MMWU strategy instructs the player to choose wk   yj
 We denote by    cid    spectral dominance that is equivalent

to saying that       is positive semide nite  PSD 

 Firstly  all the results cited and stated in this paper  after scaling  generalize to the scenario when the eigenvalues of Ak are
in the range        for arbitrary           For notational simplicity  we have assumed       and       in this paper  Secondly 
if Ak is not symmetric or even rectangular  classical reductions
can turn such   problem into an equivalent online game with only
symmetric matrices  see Sec   of  Garber et al   

Follow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

 

 

 
each with probability pj  The best choice    
 
log   
yields   total expected regret   
  log     Orecchia 
  and this is optimal up to constant  Arora et al 
  It requires some additional  but standard  effort to
turn this into   highcon dence result 
Unfortunately  the periteration running time of MMWU is
at least      due to eigendecomposition  where    is the
complexity for multiplying two       matrices 
MMWUJL 
Some researchers also use the JohnsonLindenstrauss  JL  compression to reduce the dimension
of Wk from MMWU to make it more ef ciently computable  Peng   Tangwongsan    AllenZhu et al 
    Lee   Sun    Speci cally  they compute
    using   random     Rd   
  sketch matrix       
  is  cid    this compression incurs an average regret
and then use YY cid  to approximate Wk  If the dimension
Unfortunately  to maintain   total regret  cid   
nential to dimension  cid       and is only useful when       

    one must
let         Therefore  JL compresses the matrix expo 

loss of   We call this method MMWUJL for short 

 

iteration complexity of FTCL is no slower than FTPL  and
much faster than MMWU and MMWUJL  We shall make
this comparison more explicit in Section  
  Our Side Result  Stochastic Online Eigenvector
We also study the special case of the online eigenvector
problem where the adversary is stochastic  meaning that
           AT are chosen        from   common distribution
whose expectation equals some matrix    independent of
the player   actions  For this problem 
  Garber et al   Garber et al    showed   block power

method gives   total regret   cid   log dT   and runs

in   nnz     time per iteration   We denote nnz   
the time to multiply   to   vector 

  Shamir  Shamir    showed Oja   algorithm  has
 
 

dT log     which is   factor

 

  total regret   
worse than optimum 

 
In this paper  we show that Oja   algorithm in fact only has
 
  log    for this stochastic setting  which
  total regret   
is optimal up to  
log   factor  Most importantly  the kth
iteration of Oja   runs in only   nnz Ak  time 
Example  Since in lowrank or sparse cases it usually
satis es nnz          and nnz Ak         our result
can be faster than block power method by   factor     

Our proof relies on   compression view of Oja   algorithm
which compresses MMWU to dimension       Our
proof is onepaged  indicating that FTCL might be   better framework of designing online algorithms for matrices 
  Our Results in   More Re ned Language
   max        AT   we have      
Denoting by      
according to the normalization Ak  cid    
In general  the
smaller   is  the better   learning algorithm should behave 
In the previous subsections  we have followed the tradition and discussed our results and prior works assuming the
worst possibility of   This has indeed simpli ed notations 
If   is much smaller than   our complexity bounds can
be improved to quantities that depend on   We call this
the  re ned language  At   high level  for our FTCL  in
both the adversarial and stochastic settings  the total regret

improves from  cid   

 

    to  cid   

 

    

We have an informationtheoretic lower bound of  
    
for the total regret in this  re ned language  see full version  This lower bound even holds for the stochastic problem  even when the matrices Ak are of rank  
As for prior work  it has been recorded that  cf  Theorem
  of  AllenZhu et al    the MMWU and MMWUJL methods have total regret   
   log    The block

 

 

 Here is   simple description of Oja   algorithm  beginning
with   random Gaussian vector     Rd  at each iteration   
choose wk to be    Ak         after normalization 

FTPL  Researchers also study the followthe perturbedleader  FTPL  strategy  Kot owski   Warmuth   
Dwork et al    Garber et al    Abernethy et al 
  Most notably  Garber  Hazan and Ma  Garber
et al    proposed to compute an  approximate  leading eigenvector of the matrix       rr cid  at iteration   
where   is   random vector whose norm is around

Unfortunately  the total regret of FTPL is  cid   

dT   which
  worse than the optimum regret  and interestd loss can indeed be

is   factor
ing only when        This factor
realized in practice  see Figure  

dT  

 

 

 

 

  Our Main Results

We propose   followthe compressedleader  FTCL  strategy that  at   high level  compresses the MMWU strategy to

dimension       as opposed to dimension      cid     in
  FTCL has regret  cid   

MMWUJL  Our FTCL strategy has signi cant advantages
over previous results because 

    which is optimal up to poly 

 

 

log factors  as opposed to

  in FTPL 

  Each iteration of FTCL is dominated by solving   log 

arithmic number of linear systems 

Since solving linear systems is generally no slower than
computing eigenvectors or matrix exponentials  the per 

 In fact  it is known that eigendecomposition has complexity
     when all the eigenvalues are distinct  and could possibly
go up to      when some eigenvalues are equal  Pan   Chen 
 

 Through the paper  we use the  cid   notation to hide polyloga 

rithmic factors in      and   if applicable 

Follow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

    random

    diagonal

    diagonal rotation

Figure   We generate synthetic data to verify that the total regret of FTPL can indeed be poorer than MMWU or our FTCL  We explain

how matrices Ak are chosen in Appendix    We have       and the xaxis represents the number of iterations 

Paper

MMWU

MMWUJL       only 

FTPL        only 

this paper

block power method

this paper

 cid   
 cid   
 cid   
 cid   
 cid   
 cid   

 
   
 
   
 
dT  
 
   

 
   
 
   

Total Regret

Time Per Iteration

Minimum Total Time
for   Average Regreta

at least     

Mev  

Mexp  cid      
Theorem   Mlin  cid   
  cid nnz cid 
Theorem     cid nnz   cid 

  stochastic online eigenvector only  

 

 cid   cid    
 cid 
 cid   cid   
  nnz cid 
 cid   cid    
  nnz cid 
Theorem    cid   cid   
  nnz cid  and
 cid   cid   
 cid   cid   
  nnz cid 
Theorem    cid   cid   
  nnz   cid 
 cid nnz Ak cid    nnz 

  nnz 

  nnz cid 

 

     

 
  nnz   

Table   Comparison of known methods for the online eigenvector problem  We denote by nnz    the time needed to multiply   to  

vector  by              AT   and by nnz      maxk    

  Mexp is the time to compute     multiplied with   vector  where     Rd   satis es    cid     cid   cid           
  Mlin is the time to solve   linear system for matrix     Rd    where   is PSD and of condition number    cid      
  nnz    cid    Mlin    cid   cid  min min     

  Mev is the time to compute the leading eigenvector of matrix   to multiplicative accuracy              
  If using iterative methods  the worstcase values Mev  Mexp  Mlin are

 
   

 

   nnz    cid   

 

Mev    cid   cid  min  
 cid   cid  

 
  nnz 

 
  nnz   

   

  nnz    cid    Mexp    cid   cid  min  
    nnz cid 

 

where    is the time needed to multiply two       matrices 

If using stochastic iterative methods  Mlin is at most

aThe total time complexity of the  rst    rounds where    is the earliest round to achieve an   average regret 

 cid   

 

power method  for the stochastic setting  has total regret
     by modifying the proof in  Garber et al   
To the best of our knowledge  FTPL has not been analyzed
in the  re ned language  We compare with prior work in
Table   in the appendix for this  re ned language 

  Other Related Works

The multiplicative weight update  MWU  method is   simple but extremely powerful algorithmic tool that has been
repeatedly discovered in theory of computation  machine
learning  optimization  and game theory  see for instance
the survey  Arora et al    and the book  CesaBianchi
  Lugosi   
Its natural matrix extension  matrixmultiplicative weightupdate  MMWU   Orecchia   
has been used towards ef cient algorithms for solving
semide nite programs  Arora   Kale    AllenZhu

et al    Peng   Tangwongsan    balanced separators  Orecchia et al    Ramanujan sparsi ers  AllenZhu et al    Lee   Sun    and even in the proof of
QIP   PSPACE  Jain et al    Some authors also refer
to MMWU as the followthe regularizedleader strategy or
FTRL for short  because MMWU can be analyzed from  
mirrordescent view with the matrix entropy function as its
regularizer 
For the online eigenvector problem  if the feedback madT   total regret of

trices Ak are only of rank  the  cid   
FTPL can be improved to  cid         This is  rst shown

by Dwork et al   Dwork et al    and independently
by Kot owski and Warmuth  Kot owski   Warmuth   
Abernethy et al  showed FTPL strategies can be analyzed
using   FTRL framework  Abernethy et al   

 

 Total RegretFTPLthis paperMMWU Total RegretFTPLthis paperMMWU Total RegretFTPLthis paperMMWUFollow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

Researchers also put efforts to understand highrank variants of the online eigenvector problem  Nie et al  studied the highrank variant using MMWU  Nie et al   
but their periteration complexity is also high due to eigendecomposition  Some authors study   very different online model for computing the top   eigenvectors  Boutsidis
et al    Karnin   Liberty    they wish to output
      poly  vectors instead of   but with   good PCA
reconstruction error 
The stochastic online eigenvector problem is related but
different from streaming PCA  Hardt   Price    AllenZhu   Li      In streaming PCA  we are given       
random matrices with an expectation   and asked to  nd  
unit vector   with large   cid Bw in the end  without worrying about the periteration gain  The cited papers use different techniques from ours and do not imply our result on
stochastic online eigenvector 
For the most ef cient of ine eigenvectors algorithms  we
refer interested readers to our paper  AllenZhu   Li 
     for PCA   SVD  and  AllenZhu   Li     for
CCA and generalized eigendecomposition 

  Roadmap

We introduce notations in Section   and compare the
periteration complexity of FTCL to prior work in
Section   We discuss highlevel intuitions and techniques
in Section   We introduce   new trace inequality in
Section   and state our main FTCL result for an oblivious adversary in Section   We extend it to the adversarial
setting and discuss how to implement FTCL fast in the full
version of this paper  Finally  in Section   we provide our
FTCL result for   stochastic adversary 
Our results are stated directly in the  re ned language 
  Notations and Preliminaries

De ne     cid  

   Ai for every                     Since
each Ak is positive semide nite  PSD  we can  nd Pk  
Rd   such that Ak   PkP cid 
    we only use Pk for analysis purpose only  Given two matrices        Rd   
we write         Tr   cid    We write    cid    if
     are symmetric matrices and       is PSD  We write
       the       th entry of    We use  cid   cid  to denote
the spectral norm of   matrix    We use nnz    to denote time needed to multiply matrix     Rd   with an
arbitrary vector in Rd  In particular  nnz    is at most  
plus the number of nonzero elements in    We denote
nnz      maxk    
Suppose      xt     are drawn        from the standard
  has   chisquared
distribution of tdegree freedom    is called inversechi 
squared distribution of tdegree freedom  It is known that
      

 cid nnz Ak cid 
Gaussian       then      cid  

   for      

     

  Detailed Comparison to Prior Work
We compare the periteration complexity of our results
more closely to prior work 
In the stochastic setting  Oja   method runs in time
nnz Ak  for iteration    and therefore is clearly faster than
the block power method which runs in time nnz   
In the adversarial setting  it is clear that the periteration
complexities of FTPL and FTCL are no greater than
MMWU  because computing the leading eigenvector and
the matrix inversion are both faster than computing the full
eigendecomposition  In the rest of this section  we compare
MMWUJL  FTPL and FTCL more closely  They respectively have periteration complexities
and

 cid         Mexp      Mev 

 cid      Mlin

where
  In MMWUJL  we denote by Mexp the time needed for
computing exp    multiplied to   vector  Re 

call that      cid    

  In FTPL  following the tradition  we denote by Mev
 
the time needed for computing the top eigenvector of
      rr cid  where the norm of   is   

  In FTCL  we denote by Mlin the time needed for solving
  linear system with matrix     cI       where
   cid   

    and      cid    

dT  

is because one can compute the singular value decom 

For exact computations  one may generally derive that
Mexp   Mev   Mlin  However  for largescale applications 
one usually applies iterative methods for the three tasks  Iterative methods utilize matrix sparsity  and have running
times that depend on matrix properties 
Worstcase Complexity  We compute that 

The  rst is because if using Chebyshev approximation 
one can compute exp    applied to   vector in

  Mexp in the worst case is  cid   min    nnz       
time at most  cid   cid cid   cid 
 nnz   cid  The second
position of     in time  cid      and then compute the
 cid   min      nnz       
 cid   cid    

The  rst is so because  as proved in  Garber et al 
  it suf ces to compute the top eigenvector of
      rr cid  up to   multiplicative error        
   
If one applies Lanczos method 
this is in time
       The second is because the leading eigenvector of         matrix can be computed directly in time
    
   multiplicative error   means to  nd   such that   cid   

  nnz    cid   Recall that it only works when

matrix exp    directly 

  Mev

      

worst

     

case

is

in

the

 

rr cid          max      rr cid 

be

to

case

is

the

worst

  Mlin

  Mlin

improved

  nnz     

can
  nnz      

using stochastic iterative methods 

The  rst is because our matrix   has   condition num 

dient  Shewchuk    one can solve   linear system
second is because the inverse of         matrix can be
computed directly in time       Bunch   Hopcroft 
 

 cid   cid  min min    
      nnz       cid 
ber at most          cid       If using conjugate gra 
      nnz    cid  The
for   in time at most  cid   cid  min    
    nnz       cid cid  if
 cid   cid  min cid    
iteration cost of FTCL is only  cid   Mlin  this is no slower

In sum  if using iterative methods  the worst case values of
Mlin  Mev  Mexp are on the same magnitude  Since the perthan   Mev  of FTPL  and much faster than       Mexp 
of MMWUJL 
Practical Complexity 
There are many algorithms to
compute leading eigenvectors  including Lanczos method 
shiftand invert  and the  slower  power method  The performance may depend on other properties of the matrix 
including  how wellclustered the eigenvalues are 
There are also numerous ways to compute matrix inversions  including conjugate gradient  accelerated coordinate
descent  Chebyshev method  accelerated SVRG  and many
others  Some of them also run faster when the eigenvalues
form clusters  Shewchuk   
In particular  for   random Gaussian matrix      with dimension       using the default scienti   package
SciPy of Python  Mev is roughly   times of Mlin 
Total WorstCase Complexity 
Since FTPL requires  
times more iterations in order to achieve the same average
regret as FTCL or MMWU  in the last column of Table  
we also summarize the minimum total time complexity
needed to achieve an   average regret 
Examples 
total complexity needed to achieve an   average regret 

If nnz          and nnz           the

 cid            us 
 cid       MMWUJL 

 cid       FTPL 
 cid       MMWU 

In the  re ned setting  one can revise the complexity
bounds accordingly  We ignore the details in this short version and present them in Table   in the appendix 
  HighLevel Discussion of Our Techniques
Revisit MMWU  We  rst revisit the highlevel idea behind the proof of MMWU  Recall Wk   exp ckI  
    where ck is the unique constant such that
TrWk     The main proof step  see for instance Theorem   of  AllenZhu et al    is to use the equality

Follow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

in

TrWk   TrWk      to derive   relationship between
ck   ck  and the gain value Wk   Ak at this iteration 
More speci cally  using the GoldenThompson inequality
we have

Tr cid eckI   cid    Tr cid eckI      Ak cid 
  Tr cid Wke Ak cid    Tr cid eckI   cid     Wk   Ak  
Tr cid eck     cid    Tr cid eckI   cid    ck    ck  

One can also use convexity to show

Adding these two inequalities  and using the fact that
TrWk   TrWk      we immediately have ck  
ck   cid   Wk Ak  In other words  the gain value Wk Ak
at iteration    up to   factor   is lower bounded by the
decrement of ck  On the other hand  it is easy to see
     cT      max         log    from        log  
and the de nition of cT   Together  we can derive that

 cid  
   Wk   Ak  cid   max      

In the rest of this section  we perform   thought experiment
to  modify  the above MMWU analysis stepby step  In
the end  the intuition of our FTCL shall become clear to
the reader 

Thinking Step   We wish to choose   random Gaussian
vector     Rd and  compress  MMWU to dimension  
in the direction of    More speci cally  we de ne Wk  
exp ckI       but this time ck is the unique constant
such that Tr Wkuu cid      cid Wku     In such   case 
we wish to say that

Tr cid eckI   uu cid cid    Tr cid eckI   Ak uu cid cid 
 cid  Tr cid   ckI   uu cid   ckI     Ak cid 
  Tr cid   

    Ak cid 

  uu cid   

  Ak  

  Tr Wkuu cid       

  uu cid   

    

 
then we could deIf the above inequality were true 
    which is   unit vector  because
 ne wk
    Ak
Tr Wkuu cid      and the gain   cid 
would again be proportional to the change of this new po 

tential function Tr cid eckI   uu cid cid  This idea almost

  Akwk   wkw cid 

worked except that inequality  cid  is false due to the noncommutativity of matrices 
Perhaps the most  immediate  idea to    this issue is to
use the randomness of uu cid  Recall that   uu cid      if we
choose properly normalize    and therefore it  seems like 
we have   Tr Wkuu cid    Tr Wk  and the inequality
will go through  Unfortunately  this idea fails for   fundamental reason  the normalization constant ck depends on

   analogy for this effect can be found in the inequality
Tr eA    Tr eB  for every    cid     This inequality becomes
false when multiplied with uu cid  and in general eA  cid  eB is false 

Follow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

   so Wk is not independent from the randomness of   
Thinking Step   Since Gaussian vectors are rotationally
invariant  we switch wlog to the eigenbasis of     so Wk
is   diagonal matrix  We make an important observation 

ck depends only on            ud 

but not on the    possible signs of            ud 

case that   and   are simultaneously diagonalizable 

or if we denote by     ckI       we want to study the

For this reason  we can      diagonal matrix   and consider all random uu cid  which agree with   on its diagonal 
All of such vectors   give the same normalization constant
ck  and it satis es   uu cid         This implies that we
can now study the conditional expected potential change

  cid Tr cid eckI   uu cid cid    Tr cid eckI    uu cid cid cid cid   cid 
  Tr cid eckI     cid    Tr cid eckI     cid   
difference Tr cid eB Ak   cid    Tr cid eBD cid  only in the special
Thinking Step     usual way to bound Tr cid eB Ak   cid 
Tr cid eBD cid  is to de ne       Tr cid eB Ak   cid  and bound
The zeroorder derivative     is Tr cid eBD cid  The  rst 

    cid cid       
    by its Taylor series         cid     
order derivative   cid    Tr AkeBD    eB DeB Ak
behaves exactly in the way we hope  and this strongly relies
on the commutativity between   and    Unfortunately 
higherorder derivatives       bene   less and less from
the commutativity between   and   due to the existence
of terms such as AkeBDeBAkD  For this reason  we need
to   truncate the Taylor series and   use different analytic tools  This motivates us to use the following regime
that can be viewed as    lowdegree  version of MMWU 
  Quick Detour 
In   recent result  the authors of  AllenZhu et al    generalized MMWU to  cid   regularized strategies  For every       they de ne Xk  
 ckI        where ck is the unique constant such
that ckI        cid    and TrXk     This is  
generalization of MMWU because when     log    the
matrix Xk behaves nearly the same as Wk  in particular  it gives the same regret bound  The analysis behind

 In fact  ck can be made almost independent from   if we
replace uu cid  with QQ cid  where   is   random       matrix for
some very large    That was the main idea behind MMWUJL 

 This is because  Tr eckI    uu cid     cid  
eck   cid  where    is the ith eigenvalue of    

 cid ui   

 That is  all random uu cid  such that  cid ui cid 

        For simplicity we also denote this event as   

    Di   for each

 The name  cid   strategies  comes from the following fact 
Recall MMWU naturally arises as the followthe regularizedleader strategy  where the regularizer is the matrix entropy 
If
the entropy function is replaced with   negative  cid   norm  the
resulting strategy becomes Xk  We encourage interested readers
to see the introduction of  AllenZhu et al    for more background  but we shall make this present paper selfcontained 

  

Tr cid ckI     cid  as opposed to Tr cid eckI   cid 

this new strategy is to keep track of the potential change in

and then use the socalled LiebThirring inequality  see
Section   to replace the use of GoldenThompson   Note
that ck is choosen with respect to   but the potential is with
respect to      
Thinking Step  
Let us now replace MMWU strategies in our Thinking Steps   with  cid   regularized
strategies  Such strategies have two advantages    they
help us overcome the issue for higherorder terms in Thinking Step   and   solving linear systems is more ef cient
than computing matrices exponentials  We shall choose
     log dT   in the end 
Speci cally  we prepare   random vector   and de ne the
normalization constant ck to be the unique one satisfying

Tr cid ckI      quu cid cid    Tr Xkuu cid      At itera 

    which is  

tion    we let the player choose strategy   
unit vector 
If one goes through all the math carefully  using Woodbury
formula  this time we are entitled to upper bound the trace

difference of the form Tr cid         cid Tr cid Bq   cid 
Tr cid            cid  and bound this polynomial     us 

where   is simultaneously diagonalizable with   but not
   Similar to Thinking Step   we can de ne      

ing its Taylor expansion at point   Commutativity between
  and   helps us compute   cid          Tr Bq CD 
but again we cannot bound higherderivatives directly  Fortunately  this time     is   degree       polynomial so we
can use Markov brothers  inequality to give an upper bound
on its higherorder terms  This is the place we lose   few
extra polylogarithmic factors in the total regret 
Thinking Step   Somehow necessarily  even the secondorder derivative   cid cid  can depend on terms such as  Dii
where Dii    ui  is the ith diagonal entry of    This
quantity  over the Gaussian random choice of ui  does not
have   bounded mean  More generally  the inverse chisquared distribution with degree    recall Section   has  
bounded mean only when       For this reason  instead
of picking   single random vector     Rd  we need to pick
three random vectors            Rd and replace all the
occurrences of uu cid  with  
previous thinking steps  As   result  each Dii becomes  
chisquared distribution of degree   so the issue goes away 
This is why we claimed in the introduction that

 cid  in the

 cid     cid 

        cid 

        cid 

 

 

we can compress MMWU to dimension  

 
Thinking Step  
Putting together previous steps  we
  log dT  
obtain   FTCL strategy with total regret   
factor
which is worse
  log dT   We call this method FTCLobl and include its analysis in Section   However  FTCLobl only
works for an oblivious adversary       when            AT

than MMWU only by  

Follow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

are  xed   priori  and gives an expected regret  To turn
it into   robust strategy against adversarial            AT  
and to make the regret bound work with high con dence 
we need to resample          every iteration  We call
this method FTCLadv    careful but standard analysis with
Azuma inequality helps us reduce FTCLadv to FTCLobl  We
state this result in the full version 
Running Time  As long as   is an even integer  the computation of  ckI       applied to   vector   or
in other words  solving linear systems  becomes the bottleneck for each iteration of FTCLobl and FTCLadv  However  as long as      log dT   we show that the condition number of the matrix ckI       is at most     
     Conjugate gradient solves each such linear sys 

Compress to    in Stochastic Online Eigenvector 
If
the adversary is stochastic  we observe that Oja   algorithm

tem in worstcase time  cid   min          nnz   
corresponds to   potential function Tr cid      Ak      
   uu cid               Ak cid  Because the matrices
dimension   namely Tr cid     Ak uu cid cid  In fact  just

are drawn from   common distribution  this potential behaves similar to the matrix exponential but compressed to

using linearity of expectation carefully  one can both upper and lower bound this potential  We state this result in
Section    and it can be proved in one page 
    New Trace Inequality
Prior work on MMWU and its extensions rely heavily
on one of the following trace inequalities 
the GoldenThompson inequality

Tr eA      Tr cid eAe   cid 

and the LiebThirring inequality

Due to our compression framework in this paper  we need
inequalities of type

Tr cid       cid    Tr cid Ak     BA kAk cid   
  Tr eA BD    Tr cid   BeA DeA cid   
  Tr cid        kD cid 
  Tr cid        BA kAk DAk cid     

like  generalizations  of Goldenwhich look almost
Thompson and LiebThirring  by setting        Unfortunately  such generalizations do not hold for an arbitrary    For instance  if the  rst  generalization  holds
for every PSD matrix   then it would imply   eA    cid 
eA   BeA    which is   false inequality due to matrix
noncommutativity 
In this paper  we show that if   is commutative with   
then the  generalization    holds for the zeroth and  rst
order terms with respect to   As for higher order terms 
we can control it using Markov brothers  inequality   Proof
see full paper 

Lemma   For every symmetric matrices          
Rd    every integer       every       and every    
      if   and   are commutative  then

 cid     

 cid 

                Ak             Ak  

 cid cid cid      cid           Ak     cid cid cid 

 

 

 

max

 cid 

  Oblivious Online EV   Expected Regret
In this section we  rst focus on   simpler oblivious setting 
           AT are   PSD matrices chosen by the adversary
in advance  and they do not depend on the player   actions
in the   iterations  We are interested in upper bounding the
total expected regret

 max

   Ak

    cid 

  Akwk   

  

 cid cid  

 cid   cid  

where the expectation is over player   random choices
wk   Rd  recall  cid wk cid     
In the full version  we generalize this result to the full adversarial setting along with highcon dence regret 
Our algorithm FTCLobl is presented in Algorithm   It is
parameterized by an even integer       and   learning rate
      It initializes with   rank  Wishart random matrix

   For every            we denote by Xk    cid ckI  

 cid   where ck     is the unique constant     
and Tr cid XkU cid       

ckI        cid   

   

  UX 

        

  UX 
   

At iteration          the player plays   random unit vector wk  among the three eigenvectors of   
    It
satis es   wkw cid 
We prove the following theorem for the total regret of
FTCLobl         in the full version of this paper 
Theorem   If we have an oblivious adversary  there exists absolute constant       such that if       log dT  

 cid  then FTCLobl         satis es
 cid   cid         log dT  cid max        

  cid 
  Akwk

   

 

 

Corollary  

If       log dT   and    

  

 max    

 re ned language 

  cid 
  Akwk
   max        

 cid 
 cid cid max     log dT  
 cid 
 cid  we have
or choosing the same   but      cid  log dT  
 cid     max     
 cid 
 cid 
 cid  
 This ck is unique because Tr cid XkU cid  is   strictly decreasing

 general language 

  cid 
  Akwk

  log dT  

  cid 

  

 

 

 

 

function for ck    max   

and    cid 
  cid 
  cid 
 cid  log dT  
 cid  

 

  

 cid 
  cid 

Follow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

 

 

        cid 

  the learning rate 
        cid 

Algorithm   FTCLobl        
Input      number of iterations        an even integer 
  Choose            Rd where the    coordinates are        drawn from      
       
  for       to   do
 
 
 
 
 
  end for

 cid     cid 
 cid 
     cid   
Denote by Xk  cid ckI      
   cid 

   pj   yjy cid 
Compute   
Choose wk   yj with probability pj 
Play strategy wk and receive matrix Ak 

 cid   where ck is the constant     

 cid  theorypredicted choice      log dT  

 cid  theorypredicted choice     log dT  cid max    
and Tr cid XkU cid       

ckI        cid   
 cid  it satis es              and                 

  where          are orthogonal unit vectors in Rd 

  UX 

   Ai 

 

  cid    max    we
  log     log   cid   

 

 
  

 re ned language 
     we have with prob          

  Missing Theorems
In the full version of this paper  we state Theorem     similar result of Theorem   but   with   highcon dence regret bound and   for the adversarial setting  Ak may depend on the player   strategies            wk  In the full
version  we also provide Theorem   which addresses the
worstcase complexity for implementing the matrix inversion steps in FTCLobl 
  Stochastic Online Eigenvector
Consider the special case when the matrices            AT
are generated        from   common distribution whose expectation equals    This is known as the stochastic online
eigenvector problem  and we wish to minimize the regret

 cid  
     cid 

  Akwk        max     

We revisit Oja   algorithm  beginning with   random Gaussian vector     Rd  at each iteration    let wk be     
 Ak            after normalization  It is clear that
wk can be computed from wk  in time nnz   
In the full version  we prove the next theorem in one page 
Theorem   There exists       such that  for every
rithm  we have with probability at least       

        if    cid cid      max   cid  in Oja   algo 
 cid  
     cid 

  Akwk      max      log   log   

 

Corollary   Choosing    
have with prob          

 cid  
     cid 

  Akwk        max   

    cid cid      max   

 

 

 cid  
Choosing    
    cid 
     cid 
  
 

  Akwk        max   

  log     log   cid   

 general language 

 

The proof of Theorem   uses   potential function analysis
which is similar to the matrix exponential potential used in
MMWU  but compressed to dimension  
  Conclusions
We give   new learning algorithm FTCL for the online
eigenvector problem 
It matches the optimum regret obtained by MMWU  but runs much faster  It matches the fast
periteration running time of FTPL  but has   much smaller
regret 
In the stochastic setting  our side result on Oja  
algorithm also outperforms previous results  We believe
our novel idea of  follow the compressed leader  may  nd
other applications in the future 
Acknowledgement
We thank Yin Tat Lee for discussing the problem regarding
how to compress MMWU to constant dimension in  
We thank Elad Hazan for suggesting us the problem and
for several insightful discussions  We thank Dan Garber
and Tengyu Ma for clarifying some results of prior work
 Garber et al   
References
Abernethy  Jacob  Lee  Chansoo  Sinha  Abhinav  and
Tewari  Ambuj  Online linear optimization via smooth 

Follow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

ing  In COLT  pp     

Abernethy  Jacob  Lee  Chansoo  and Tewari  Ambuj 
Spectral smoothing via random matrix perturbations 
ArXiv eprints  abs   

AllenZhu  Zeyuan and Li  Yuanzhi  LazySVD  Even
Faster SVD Decomposition Yet Without Agonizing
Pain  In NIPS     

AllenZhu  Zeyuan and Li  Yuanzhi  First Ef cient Convergence for Streaming kPCA    Global  GapFree  and
NearOptimal Rate  ArXiv eprints  abs 
July    

AllenZhu  Zeyuan and Li  Yuanzhi  Doubly Accelerated
Methods for Faster CCA and Generalized Eigendecomposition  In Proceedings of the  th International Conference on Machine Learning  ICML    

AllenZhu  Zeyuan and Yuan  Yang 

Improved SVRG
for NonStrongly Convex or Sumof NonConvex Objectives  In ICML   

AllenZhu  Zeyuan  Liao  Zhenyu  and Orecchia  Lorenzo 
Spectral Sparsi cation and Regret Minimization BeIn Proceedings of the
yond Multiplicative Updates 
 th Annual ACM Symposium on Theory of Computing 
STOC    

AllenZhu  Zeyuan  Lee  Yin Tat  and Orecchia  Lorenzo 
Using optimization to obtain   widthindependent  parIn Proallel  simpler  and faster positive SDP solver 
ceedings of the  th ACMSIAM Symposium on Discrete
Algorithms  SODA    

Arora  Sanjeev and Kale  Satyen    combinatorial  primaldual approach to semide nite programs  In Proceedings
of the thirtyninth annual ACM symposium on Theory of
computing   STOC   pp    New York  New York 
USA    ACM Press 
ISBN   doi 
 

Arora  Sanjeev  Hazan  Elad  and Kale  Satyen  The Multiplicative Weights Update Method    MetaAlgorithm
and Applications  Theory of Computing   
  doi   toc     

Boutsidis  Christos  Garber  Dan  Karnin  Zohar  and Liberty  Edo  Online principal components analysis  In Proceedings of the TwentySixth Annual ACMSIAM Symposium on Discrete Algorithms  pp    SIAM   

Bunch  James   and Hopcroft  John    Triangular factorization and inversion by fast matrix multiplication 
Mathematics of Computation     

CesaBianchi  Nicolo and Lugosi  Gabor 

Prediction 
Learning  and Games  Cambridge University Press 
Cambridge    ISBN   doi   
CBO 

Dwork  Cynthia  Talwar  Kunal  Thakurta  Abhradeep  and
Zhang  Li  Analyze gauss  optimal bounds for privacypreserving principal component analysis  In STOC  pp 
  ACM   

Frostig  Roy  Ge  Rong  Kakade  Sham    and Sidford 
Aaron  Unregularizing  approximate proximal point
and faster stochastic algorithms for empirical risk minimization  In ICML  volume   pp      URL
http arxiv org abs 

Garber  Dan and Hazan  Elad  Fast and simple PCA via

convex optimization  ArXiv eprints  September  

Garber  Dan  Hazan  Elad  and Ma  Tengyu  Online learning of eigenvectors  In Proceedings of the  nd International Conference on Machine Learning  ICML  pp 
   

Hardt  Moritz and Price  Eric  The noisy power method 
  meta algorithm with applications  In NIPS  pp   
   

Jain  Rahul  Ji  Zhengfeng  Upadhyay  Sarvagya  and WaJournal of the ACM

trous  John  QIP   PSPACE 
 JACM     

Karnin  Zohar and Liberty  Edo  Online pca with spectral
bounds  In Proceedings of the  th Annual Conference
on Computational Learning Theory  COLT  pp   
   

Kot owski  Wojciech and Warmuth  Manfred    Pca with
gaussian perturbations  ArXiv eprints  abs 
 

Lee  Yin Tat and Sun  He  Constructing linearsized specIn FOCS  pp 

tral sparsi cation in almostlinear time 
  IEEE   

Lin  Hongzhou  Mairal  Julien  and Harchaoui  Zaid 
  Universal Catalyst
for FirstOrder Optimization 
In NIPS    URL http arxiv org pdf 
   pdf 

Nesterov  Yurii  Introductory Lectures on Convex Programming Volume    Basic course  volume    Kluwer Academic Publishers    ISBN  

Nie  Jiazhong  Kot owski  Wojciech  and Warmuth  ManIn Internafred    Online pca with optimal regrets 
tional Conference on Algorithmic Learning Theory  pp 
  Springer   

Follow the Compressed Leader  Faster Online Learning of Eigenvectors and Faster MMWU

Orecchia  Lorenzo  Fast Approximation Algorithms for
Graph Partitioning using Spectral and Semide niteProgramming Techniques  PhD thesis  EECS Department  University of California  Berkeley  May  

Orecchia  Lorenzo  Sachdeva  Sushant  and Vishnoi 
Nisheeth    Approximating the exponential  the lanczos

method and an  cid     time spectral algorithm for bal 

anced separator  In STOC   ACM Press  November
 

Pan  Victor   and Chen  Zhao    The complexity of the
matrix eigenproblem  In Proceedings of the thirty rst
annual ACM symposium on Theory of computing  pp 
  ACM   

Peng  Richard and Tangwongsan  Kanat  Faster and simpler widthindependent parallel algorithms for positive
semide nite programming  In Proceedinbgs of the  th
ACM symposium on Parallelism in algorithms and architectures   SPAA   pp    New York  New York 
USA  January  

ShalevShwartz  Shai  SDCA without Duality  Regulariza 

tion  and Individual Convexity  In ICML   

Shamir  Ohad  Convergence of stochastic gradient descent

for pca  In ICML   

Shewchuk  Jonathan Richard  An introduction to the conjugate gradient method without the agonizing pain   

Wendel        Note on the gamma function  The American

Mathematical Monthly     

