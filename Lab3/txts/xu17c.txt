Adaptive Consensus ADMM for Distributed Optimization

Zheng Xu   Gavin Taylor   Hao Li     ario       Figueiredo   Xiaoming Yuan   Tom Goldstein  

Abstract

The alternating direction method of multipliers  ADMM  is commonly used for distributed model  tting problems  but its performance and reliability depend strongly on userde ned penalty parameters  We study distributed
ADMM methods that boost performance by using different  netuned algorithm parameters on
each worker node  We present        convergence rate for adaptive ADMM methods with
nodespeci   parameters  and propose adaptive
consensus ADMM  ACADMM  which automatically tunes parameters without user oversight 

  Introduction
The alternating direction method of multipliers  ADMM 
is   popular tool for solving problems of the form 

min

            

subject to Au   Bv     

 

  Rn   Rm
where     Rn     and     Rm     are convex functions 
    Rp        Rp    and     Rp  ADMM was  rst
introduced in  Glowinski   Marroco    and  Gabay  
Mercier    and has found applications in many optimization problems in machine learning  distributed computing and many other areas  Boyd et al   
Consensus ADMM  Boyd et al    solves minimization problems involving   composite objective        
  fi    where worker   stores the data needed to compute fi  and so is well suited for distributed model  tting
problems  Boyd et al    Zhang   Kwok    Song
et al    Chang et al    Goldstein et al   
Taylor et al    To distribute this problem  consensus methods assign   separate copy of the unknowns  ui  to

 cid 

 University of Maryland  College Park   United States Naval
Academy  Annapolis   Instituto de Telecomunicac   oes  IST  ULisboa  Portugal   Hong Kong Baptist University  Hong Kong  Correspondence to  Zheng Xu  xuzhustc gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

each worker  and then apply ADMM to solve

  cid 

  

min

ui Rd   Rd

fi ui        

subject to ui     

 

where   is the  central  copy of the unknowns  and     
is   regularizer  The consensus problem   coincides with
  by de ning                 uN     RdN       IdN  
RdN dN   and      Id          Id    RdN    where Id
represents the       identity matrix 
ADMM methods rely on   penalty parameter  stepsize  that
is chosen by the user  In theory  ADMM converges for any
constant penalty parameter  Eckstein   Bertsekas   
He   Yuan    Ouyang et al    In practice  however  the ef ciency of ADMM is highly sensitive to this
parameter choice  Nishihara et al    Ghadimi et al 
  and can be improved via adaptive penalty selection methods  He et al    Song et al    Xu et al 
   
One such approach  residual balancing  RB   He et al 
  adapts the penalty parameter so that the residuals  derivatives of the Lagrangian with respect to primal
and dual variables  have similar magnitudes  When the
same penalty parameter is used across nodes  RB is known
to converge  although without   known rate guarantee 
  more recent approach  AADMM  Xu et al     
achieves impressive practical convergence speed on many
applications  including consensus problems  with adaptive
penalty parameters by estimating the local curvature of the
dual functions  However  the dimension of the unknown
variables in consensus problems grows with the number of
distributed nodes  causing the curvature estimation to be
inaccurate and unstable  AADMM uses the same convergence analysis as RB  Consensus residual balancing  CRB 
 Song et al    extends residual balancing to consensusbased ADMM for distributed optimization by balancing the
local primal and dual residuals on each node  However 
convergence guarantees for this method are fairly weak 
and adaptive penalties need to be reset after several iterations to guarantee convergence 
We study the use of adaptive ADMM in the distributed setting  where different workers use different local algorithm
parameters to accelerate convergence  We begin by studying the theory and provide convergence guarantees when

Adaptive Consensus ADMM for Distributed Optimization

nodespeci   penalty parameters are used  We demonstrate        convergence rate under mild conditions
that is applicable for many forms of adaptive ADMM including all the above methods  Our theory is more general than the convergence guarantee in  He et al    Xu
et al      that only shows convergence when the scalar
penalty parameter is adapted  Next  we propose an adaptive consensus ADMM  ACADMM  method to automate
local algorithm parameters selection  Instead of estimating
one global penalty parameter for all workers  different local
penalty parameters are estimated using the local curvature
of subproblems on each node 

  Related work
ADMM is known to have        convergence rate under
mild conditions for convex problems  He   Yuan   
  while        rate is possible when at least one
of the functions is strongly convex or smooth  Goldfarb
et al    Goldstein et al    Kadkhodaie et al   
Tian   Yuan    Linear convergence can be achieved
with strong convexity assumptions  Davis   Yin   
Nishihara et al    Giselsson   Boyd    All of
these results assume constant parameters  to the best of
our knowledge  no convergence rate has been proven for
ADMM with an adaptive penalty   He et al    Xu
et al      proves convergence without providing   rate 
and  Lin et al    Banert et al    Goldstein et al 
  prove convergence for some particular variants of
ADMM  linearized  or  preconditioned 
To improve practical convergence of ADMM   xed optimal
parameters are discussed in  Raghunathan   Di Cairano 
  Ghadimi et al    Nishihara et al    Franc  
  Bento    These methods make strong assumptions
about the objective and require information about the spectrum of   and or    Additionally  adaptive methods have
been proposed  the most closely related work to our own
is  Song et al    which extends the results of  He
et al    to consensus problems  where communication
is controlled by prede ned network structure and the regularizer      is absent  In contrast to these methods  the
proposed ACADMM extends the spectral penalty in  Xu
et al      to consensus problems and provides convergence theory that can be applied to   broad range of adaptive ADMM variants 

  Consensus ADMM
In the following  we use the subscript   to denote iterates computed on the ith node  superscript   is the iteration number    
  is the dual vector of Lagrange multipliers 
and    
    are iteration workerspeci   penalty parameters
 contrasted with the single constant penalty parameter   of

 vanilla  ADMM  Consensus methods apply ADMM to
  resulting in the steps

uk 
    arg min
ui

vk    arg min

 

   
      

       

 cid 

 cid vk   ui  

  
 
   
 
 cid     uk 

   

   
 
 

fi ui   

  cid 
   
 
      
 
   vk    uk 

  

 

 

 cid 

  
 
   
 

 

 

 

The primal and dual residuals  rk and dk  are used to monitor convergence 

    dk  

  rk

 
rk
 

 

   

  dk

 
dk
 

 

 cid 

rk  

    vk   uk
rk
dk
       

   vk    vk 

 

 

The primal residual rk approaches zero when the iterates
accurately satisfy the linear constraints in   and the dual
residual dk approaches zero as the iterates near   minimizer
of the objective  Iteration can be terminated when
 cid uk
   cid    cid vk cid 
   cid 
 cid  

 cid rk cid     tol max cid  
and  cid dk cid     tol cid  

 

  

  

where  tol is the stopping tolerance  The residuals in  
and stopping criterion in   are adopted from the general
problem  Boyd et al    to the consensus problem  The
observation that residuals rk  dk can be decomposed into
 local residuals  rk
  has been exploited to generalize the
residual balancing method  He et al    for distributed
consensus problems  Song et al   

    dk

  Convergence analysis
We now study the convergence of ADMM with nodespeci   adaptive penalty parameters  We provide conditions on penalty parameters that guarantee convergence 
and also   convergence rate  The issue of how to automatically tune penalty parameters effectively will be discussed
in Section  

  Diagonal penalty parameters for ADMM

  Id             

  Id  be   diagonal matrix conLet       diag   
taining nonnegative penalty parameters on iteration    De 
 ne the norm  cid   cid 
    uT      Using the notation de ned
above with                 uN     RdN   we can rewrite the
consensus ADMM steps   as

uk    arg min

         cid Au     cid 
   cid     Au   Bvk cid 

 

   

 

Adaptive Consensus ADMM for Distributed Optimization

vk    arg min

 

        cid Bv     cid 

   cid     Auk    Bv cid 
                   Auk    Bvk 

   

 

 

When using   diagonal penalty matrix  the generalized

residuals become cid 

rk       Auk   Buk
dk   AT   kB vk   vk 

 

The sequel contains   convergence proof for generalized
ADMM with adaptive penalty matrix      Our proof is inspired by the variational inequality  VI  approach in  He
et al    He   Yuan     

  Preliminaries
Notation  We use the following notation to simplify the
discussions  De ne the combined variables             
Rn   and                Rn      and denote iterates as yk    uk  vk  and zk    uk  vk      Let    and
   denote optimal primal dual solutions  Further de ne
   
   
   

      zk    zk and    

       
          zk  Set

       
      
    

     

 

                          

 
       

   
 

 AT  BT  
 In
Au   Bv    

 
Im

 
 
 
     kB Ip

     

 

 
  BT   kB  
 

 

     
is   monotone operator

Note that      
satisfying
      cid         cid                 cid      We introduce intermediate variable  zk     uk  vk      where
                   Auk    Bvk  We thus have

   

        zk    zk 

 

Variational inequality formulation  The optimal solution
   of problem   satis es the variational inequality  VI 
 

                                  

From the optimality conditions for the substeps     we
see that yk  satis es the variational inequalities
               uk         uk  

 AT     Auk    Bvk        AT        

             vk         vk  

 BT     Auk    Bvk         BT        

 

 

which can be combined as

       yk 

        zk   cid    zk          

 cid     

 

 

Lemmas  We present several lemmas to facilitate the
proof of our main convergence theory  which extend previous results regarding ADMM  He   Yuan     
to ADMM with   diagonal penalty matrix  Lemma   shows
the difference between iterates decreases as the iterates approach the true solution  while Lemma   implies   contraction in the VI sense  Full proofs are provided in supplementary material  Eq    and Eq    are supported using equations       and standard techniques  while
Eq    is proven from Eq    Lemma   is supported by
the relationship in Eq   
Lemma   The optimal solution               and
sequence zk    uk  vk      of generalized ADMM satisfy

     
   

      
        
   cid 
 cid   

     
     
       cid   
  cid 

       cid   

  cid 

     

 
 
 

Lemma   The sequence  zk    uk  vk      and zk  
 uk  vk       from generalized ADMM satisfy     
Hk  cid zk   cid 
 zk           

 cid zk   cid 

Hk    

     
 

  Convergence criteria

We provide   convergence analysis of ADMM with an
adaptive diagonal penalty matrix by showing     the norm
of the residuals converges to zero   ii  the method attains
  worstcase ergodic      convergence rate in the VI
sense  The key idea of the proof is to bound the adaptivity
of     so that ADMM is stable enough to converge  which
is presented as the following assumption 
Assumption   The adaptivity of the diagonal penalty matrix       diag   

    is bounded by

               

        where       max
      max   
  

        

      

  
     
   

    

 

 

   

 

 cid 

  

We can apply Assumption   to verify that

 

             
    

 

 

         

 

which is needed to prove Lemma  
Lemma   Suppose Assumption   holds 
         and   cid       cid    cid   cid  satisfy        cid 
             cid       cid cid 

 cid       cid cid 

Then    

    

 

Adaptive Consensus ADMM for Distributed Optimization

Now we are ready to prove the convergence of generalized
ADMM with adaptive penalty under Assumption   We
prove the following quantity  which is   norm of the residuals  converges to zero 

 cid  

Theorem   Suppose Assumption   holds  Consider the
sequence  zk    uk  vk      of generalized ADMM and dek   zk  Then sequence  zl satis es the con 
 ne  zl    
 
vergence bound

 

 

 

 

 cid   
   cid 

     cid     
   cid 

 cid AT     dk cid 

       cid 
   cid 
     
       cid rk cid 
     

 

where    denotes generalized inverse of   matrix    Note
that  cid   
    converges to zero only if  cid rk cid  and  cid dk cid 
   cid 
converge to zero  provided   and     are bounded 
Theorem   Suppose Assumption   holds  Then the iterates zk    uk  vk      of generalized ADMM satisfy

  cid   
   cid 

lim

       

Proof  Let     zk    cid       in Lemma   to achieve

    cid   

  cid 

       

    

 cid   
  cid 

             cid   
  cid 
Combine   with Lemma     to get
   cid 
 cid   
         cid   
  cid 
  cid 
  cid 
Accumulate   for       to   
  cid 

       cid   
   cid 

     

       cid   
 cid 

       cid   

    

  

  

  cid 
     

  cid 

Then we have

 cid   
   cid 

        cid 
When       Assumption   suggests  cid 
        which means cid 

       cid   
 cid 

    cid   
   cid 

   

  

  

limk   cid   
   cid 

       

    
        Hence

We further exploit Assumption   and Lemma   to prove
Lemma   and combine VI   Lemma   and Lemma  
to prove the      convergence rate in Theorem  
Lemma   Suppose Assumption   holds 
Then    
           Rm     and the iterates zk    uk  vk     
of generalized ADMM satisfy    

 cid     zk cid 

  cid 
   cid 

     

   

  

where    

       cid     zk cid 

        
       
       cid   
 cid 
        

   cid 

   cid       cid 
        

 

 cid       cid 
       yl          zl      zl       
   
   cid   
 cid 
     

   cid       cid 

         

     

     

   

   
 

Proof  We can verify with simple algebra that
       cid                  cid        cid 

 
Apply   with   cid     zk  and combine VI   and
Lemma   to get

       yk          zk        
       yk          zk      zk 
 zk              
   
 

       cid zk     cid 
Summing for       to       gives us

 cid zk      cid 

     

 

 cid  
 cid  

  

   
 

  

       yk          zk        
        cid     zk cid 
 cid     zk cid 

    

 
 
 

 

 

 

 

 

Since     is convex  the left hand side of   satis es 

LHS             cid 

 yk             cid 

 zk        

  

  

             yl              zl        

 

Applying Lemma   we see the right hand side satis es 

  cid 
  cid 

  

RHS  

 
 

   
 

 cid     zk cid 

       cid     zk cid 

    

        cid     zk cid 

     

  

 cid     zk cid 

       cid       cid 
   cid       cid 

 
 
 cid     zl cid 
   
     
 
 cid       cid 
         
   cid   
 cid 
     
   

     

   

     

     
 

   
       cid   
 cid 
   cid       cid 

   

   

Combining inequalities     and   and letting   cid   
 zk in   yields the      convergence rate in  

Adaptive Consensus ADMM for Distributed Optimization

  Adaptive Consensus ADMM  ACADMM 
To address the issue of how to automatically tune parameters on each node for optimal performance  we propose adaptive consensus ADMM  ACADMM  which sets
workerspeci   penalty parameters by exploiting curvature
information  We derive our method from the dual interpretation of ADMM   DouglasRachford splitting  DRS    using   diagonal penalty matrix  We then derive the spectral
stepsizes for consensus problems by assuming the curvatures of the objectives are diagonal matrices with diverse
parameters on different nodes  At last  we discuss the practical computation of the spectral stepsizes from consensus
ADMM iterates and apply our theory in Section   to guarantee convergence 

  Dual interpretation of generalized ADMM

The dual form of problem   can be written

 cid 

min
 Rp

  AT      cid    cid 

    BT  
 

 

 cid cid 

    

 cid 

 cid 

 cid cid 

   

 cid 

where   denotes the dual variable  while       denote the
Fenchel conjugate of       Rockafellar    It is known
that ADMM steps for the primal problem   are equivalent to performing DouglasRachford splitting  DRS  on
the dual problem    Eckstein   Bertsekas    Xu
et al      In particular  the generalized ADMM iterates satisfy the DRS update formulas
                                    
 
                                      

where   denotes the intermediate variable de ned in Section   We prove the equivalence of generalized ADMM
and DRS in the supplementary material 

  Generalized spectral stepsize rule

Xu et al       rst derived spectral penalty parameters
for ADMM using the DRS  Proposition   in  Xu et al 
 
    proved that the minimum residual of DRS can be
obtained by setting the scalar penalty to        
   
where we assume the subgradients are locally linear as

                 and

             

 

        represent scalar curvatures  and       Rp 
We now present generalized spectral stepsize rules that can
accomodate consensus problems 
Proposition    Generalized spectral DRS  Suppose the
generalized DRS steps     are used  and assume the
subgradients are locally linear 

           

      and

              

 

for matrices      diag Id             Id  and     
diag Id             Id  and some       Rp  Then the
minimal residual of                is obtained by setting    

                        

 
     

Proof  Substituting subgradients            into the
generalized DRS steps     and using our linear assumption   yields
         
         

                                    
                                     

     
     

Since            are diagonal matrices  we can split the
equations into independent blocks                   
      
       
       
      
 

                              
                               

Applying Proposition   in  Xu et al      to each block 
      minimizes the block residual represented
   
     
DR      cid              ai   bi cid  where ai  
by rk 
    bi       Hence the residual norm at step       which
is  cid                    cid   
DR    is
minimized by setting    

  rk 
                        

 cid cid  

 
     

  Stepsize estimation for consensus problems

Thanks to the equivalence of ADMM and DRS  Proposition   can also be used to guide the selection of the  optimal  penalty parameter  We now show that the generalized
spectral stepsizes can be estimated from the ADMM iterates for the primal consensus problem   without explicitly supplying the dual functions 
The subgradients of dual functions           can be computed from the ADMM iterates using the identities derived
from    
Auk                 and Bvk           

For the consensus problem we have     IdN      
 Id          Id  and       and so

 

          uk 
 uk 
 vk          vk 

 cid cid 

              
 cid 
         

 cid 

  duplicates of vk 

 
 

If we approximate the behavior of these subgradients
using the linear approximation   and break the subgradients into blocks  one for each worker node  we get
 omitting iteration index   for clarity 

ui     

     ai and               bi    

 

where    and    represent the curvature of local functions
 fi and  gi on the ith node 

Adaptive Consensus ADMM for Distributed Optimization

 
     

We select stepsizes with   two step procedure  which follows the spectral stepsize literature  First  we estimate the
local curvature parameters     and     by  nding leastsquares solutions to   Second  we plug these curvature
       This formula
estimates into the formula    
produces the optimal stepsize when    and    are well approximated by   linear function  as shown in Proposition  
For notational convenience  we work with the quantities
          
        which are estimated on each
  
node using the current iterates uk
  and also an
older iterate uk 
          De ning  uk
   
 
    uk 
    
and following the literature
uk
for BarzilaiBorwein spectral stepsize estimation  there are
two least squares estimators that can be obtained from  

  vk     
 
      

     
 
       

    vk    

      

 

 

  

SD    

 cid  
 cid uk

   cid 
      
   cid  and   
      

MG    

 cid uk
 cid uk

       cid 
   cid 
     uk

 

  on each node by  

       

          

  on each node by  

Locally update uk
Globally update vk on central server by  
Locally update dual variable   
if mod    Tf       then
       

Algorithm   Adaptive consensus ADMM  ACADMM 
Input  initialize     
  while not converge by   and     maxiter do
 
 
 
 
 
 
 
 
 
 
 
 
 
  end while

Locally update   
Locally compute spectral stepsizes   
Locally estimate correlations   
cor       
Locally update     
using  
      
       
    
end if
         

   vk    uk
   
      
 
cor  

       

else

 

 

where SD stands for steepest descent  and MG stands for
minimum gradient   Zhou et al    recommend using  
hybrid of these two estimators  and choosing

and Theorem   to guarantee convergence  The  nal safeguarded ACADMM rule is

 cid 

  

   

MG  

  
SD       
  

MG   

MG       

if     
otherwise 

SD  

 

    
 

 

 

      

  from  vk    vk   vk  and   

It was observed that this choice worked well for nondistributed ADMM in  Xu et al      We can similarly
       
estimate   
 
ACADMM estimates the curvatures in the original ddimensional feature space  and avoids estimating the curvature in the higher   ddimensional feature space  which
grows with the number of nodes   in AADMM  Xu et al 
    which is especially useful for heterogeneous data
with different distributions allocated to different nodes 
The overhead of our adaptive scheme is only   few inner
products  and the computation is naturally distributed on
different workers 

  
 

  
 

 

 cid 

  
 
  
 
   
 

cor      cor and   
cor      cor and   
cor      cor and   

if   
if   
if   
otherwise 

cor      cor
cor      cor
cor      cor

 

    
 

  max min    

 

     

Ccg
      

     

   
 

    Ccg   

 

The complete adaptive consensus ADMM is shown in Algorithm   We suggest updating the stepsize every Tf    
iterations   xing the safeguarding threshold  cor     and
choosing   large convergence constant Ccg    

  Experiments   Applications
We now study the performance of ACADMM on benchmark problems  and compare to other methods 

  Safeguarding and convergence

  Applications

Spectral stepsizes for gradient descent methods are
equipped with safeguarding strategies like backtracking
line search to handle inaccurate curvature estimation and to
guarantee convergence  To safeguard the proposed spectral
penalty parameters  we check whether our linear subgradient assumption is reasonable before updating the stepsizes 
We do this by testing that the correlations

  

cor    

 cid uk
 cid uk

   cid 
      
   cid  and   
   cid cid  

cor    

   cid 
 cid vk    
 cid vk cid cid  
   cid   

 

are bounded away from zero by    xed threshold  We also
bound changes in the penalty parameter by     Ccg    according to Assumption   which was shown in Theorem  

Our experiments use the following test problems that are
commonly solved using consensus methods 
Linear regression with elastic net regularizer  We consider consensus formulations of the elastic net  Zou  
Hastie    with fi and   de ned as 

 
 

 cid Diui   ci cid              

 cid   cid   
fi ui   
where Di   Rni   is the data matrix on node    and ci is
  vector of measurements 
Sparse logistic regression with  cid  regularizer can be
written in the consensus form for distributed computing 

 
 

Adaptive Consensus ADMM for Distributed Optimization

Table   Iterations  and runtime in seconds  cores are used  absence of convergence after   iterations is indicated as   
Proposed
ACADMM

Application

 Xu et al     

AADMM

CADMM

Dataset

 samples  
 features  
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

Synthetic 
Synthetic 
MNIST
CIFAR   
News 
RCV 
Realsim
Synthetic 
Synthetic 
MNIST
CIFAR 
News 
RCV 
Realsim
Synthetic 
Synthetic 
MNIST
CIFAR 
News 
RCV 
Realsim
Ham 

 

RBADMM
 He et al   
 Boyd et al   
   
   
   
       
   
   
   
           
           
   
   
 
 
 
 
   
 
 
 
 
 
 
   

   
   
 
 
 
 
   
 
 
 
 
 
 
   

 
 
   
 
 
 

 
 
   

 
   
 

   

 

 

 
 
       

 
 
 

CRBADMM
 Song et al   
   
   
   
   
   
   
   
 

 
 
   
 
 
 
 
 

   
   

 

 
   

       

 
 
   
 
   

 
   
 
 
 
 

   
 
 
 
 
 
 

   

 
 
 

Elastic net
regression

Sparse
logistic
regression

Support
Vector
Machine

SDP
   vertices    edges for SDP 

 We only use the  rst training batch of CIFAR 

ni cid 

  

ni cid 

  

fi ui   

log    exp ci jDT

  jui            

 

where Di     Rm is the jth sample  and ci         is
the corresponding label  The minimization substep   in
this case is solved by LBFGS  Liu   Nocedal   
Support Vector Machines  SVMs  minimize the distributed objective function  Goldstein et al   

fi ui     

max    ci jDT

  jui          

 cid   cid 

   

 
 

where Di     Rm is the jth sample on the ith node  and
ci         is its label  The minimization   is solved
by dual coordinate ascent  Chang   Lin   
Semide nite programming  SDP  can be distributed as 
fi Ui     Di Ui    ci          cid       cid        cid     
where     is   characteristic function that is   if condition
  is satis ed and in nity otherwise     cid  indicates that  
is positive semide nite        Di     Rn   are symmetric
matrices   cid       cid    trace         denotes the inner product
of   and     and Di       cid Di    cid         cid Di mi    cid 

  Experimental Setup

We test the problems in Section   with synthetic and real
datasets  The number of samples and features are speci 
 ed in Table   Synthetic  contains samples from   normal distribution  and Synthetic  contains samples from  

mixture of   random Gaussians  Synthetic  is heterogeneous because the data block on each individual node is
sampled from only   of the   Gaussians  We also acquire large empirical datasets from the LIBSVM webpage
 Liu et al    as well as MNIST digital images  LeCun
et al    and CIFAR  object images  Krizhevsky  
Hinton    For binary classi cation tasks  SVM and
logreg  we equally split the   category labels of MNIST
and CIFAR into  positive  and  negative  groups  We use
  graph from the Seventh DIMACS Implementation Challenge on Semide nite and Related Optimization Problems
following  Burer   Monteiro    for Semide nite Programming  SDP  The regularization parameter is  xed at
      in all experiments 
Consensus ADMM  CADMM   Boyd et al    residual balancing  RBADMM   He et al    adaptive
ADMM  AADMM   Xu et al      and consensus
residual balancing  CRBADMM   Song et al   
are implemented and reported for comparison  Hyperparameters of these methods are set as suggested by their
creators  The initial penalty is  xed at       for all methods unless otherwise speci ed 

  Convergence results

Table   reports the convergence speed in iterations and
wallclock time  secs  for various test cases  These experiments are performed with   cores on   Cray XC  supercomputer  CADMM with default penalty        Boyd
et al    is often slow to converge  ACADMM outperforms the other ADMM variants on all the realworld

Adaptive Consensus ADMM for Distributed Optimization

    Sensitivity of iteration count to initial
penalty   Synthetic problems of EN regression are studied with   cores 

    Sensitivity of iteration count to number
of cores  top  and number of samples  bottom 

    Sensitivity of iteration count  top  and
wall time  bottom  to number of cores 

Figure   ACADMM is robust to the initial penalty   number of cores    and number of training samples 

datasets  and is competitive with AADMM on two homogeneous synthetic datasets where the curvature may be
globally estimated with   scalar 
ACADMM is more reliable than AADMM since the curvature estimation becomes dif cult for high dimensional
variables  RB is relatively stable but sometimes has dif 
culty  nding the exact optimal penalty  as the adaptation
can stop because the difference of residuals are not significant enough to trigger changes  RB does not change the
initial penalty in several experiments such as logistic regression on RCV  CRB achieves comparable results with
RB  which suggests that the relative sizes of local residuals
may not always be very informative  ACADMM signi 
cantly boosts AADMM and the local curvature estimations
are helpful in practice 

  Robustness and sensitivity

Fig     shows that the practical convergence of ADMM is
sensitive to the choice of penalty parameter  ACADMM is
robust to the selection of the initial penalty parameter and
achieves promising results for both homogeneous and heterogeneous data  comparable to ADMM with    netuned
penalty parameter 
We study scalability of the method by varying the number of workers and training samples  Fig      ACADMM
is fairly robust to the scaling factor  AADMM occasion 

ally performs well when small numbers of nodes are used 
while ACADMM is much more stable  RB and CRB
are more stable than AADMM  but cannot compete with
ACADMM  Fig      bottom  presents the acceleration in
 wallclock secs  achieved by increasing the number of
workers 
Finally  ACADMM is insensitive to the safeguarding
hyperparameters  correlation threshold  cor and convergence constant Ccg  Though tuning these parameters may
further improve the performance  the  xed default values generally perform well in our experiments and enable
ACADMM to run without user oversight 
In further experiments in the supplementary material  we also show that
ACADMM is fairly insensitive to the regularization parameter   in our classi cation regression models 

  Conclusion
We propose ACADMM    fully automated algorithm for
distributed optimization  Numerical experiments on various applications and realworld datasets demonstrate the
ef ciency and robustness of ACADMM  We also prove  
     convergence rate for ADMM with adaptive penalties under mild conditions  By automating the selection of
algorithm parameters  adaptive methods make distributed
systems more reliable  and more accessible to users that
lack expertise in optimization 

 Initial penalty parameter IterationsENRegressionSynthetic CADMMRBADMMAADMMCRB ADMMACADMM Number of cores IterationsENRegressionSynthetic CADMMRBADMMAADMMCRB ADMMACADMM Number of cores IterationsSVMSynthetic CADMMRBADMMAADMMCRB ADMMACADMM Initial penalty parameter IterationsENRegressionSynthetic CADMMRBADMMAADMMCRB ADMMACADMM Number of samples IterationsENRegressionSynthetic CADMMRBADMMAADMMCRB ADMMACADMM Number of cores SecondsSVMSynthetic CADMMRBADMMAADMMCRB ADMMACADMMAdaptive Consensus ADMM for Distributed Optimization

Acknowledgements
ZX   GT  HL and TG were supported by the US Of ce of
Naval Research under grant    and by the
US National Science Foundation  NSF  under grant CCF 
  GT was partially supported by the DOD High
Performance Computing Modernization Program  MF was
partially supported by the Fundac ao para   Ci encia   Tecnologia  grant UID EEA  XY was supported
by the General Research Fund from Hong Kong Research
Grants Council under grant HKBU 

References
Banert  Sebastian  Bot  Radu Ioan 

Ern   Robert 
results on the admm algorithm 
arXiv   

and Csetnek 
Fixing and extending some recent
arXiv preprint

Boyd  Stephen  Parikh  Neal  Chu  Eric  Peleato  Borja  and
Eckstein  Jonathan  Distributed optimization and statistical learning via the alternating direction method of multipliers  Found  and Trends in Mach  Learning   
 

Burer  Samuel and Monteiro  Renato DC    nonlinear programming algorithm for solving semide nite programs
via lowrank factorization  Mathematical Programming 
   

Chang  ChihChung and Lin  ChihJen  LIBSVM    library
for support vector machines  ACM Transactions on Intelligent Systems and Technology  TIST     

Chang  TsungHui  Hong  Mingyi  Liao  WeiCheng  and
Wang  Xiangfeng  Asynchronous distributed alternating
direction method of multipliers  Algorithm and convergence analysis  In   IEEE International Conference
on Acoustics  Speech and Signal Processing  ICASSP 
pp    IEEE   

Davis  Damek and Yin  Wotao  Faster convergence rates of
relaxed peacemanrachford and admm under regularity
assumptions  arXiv preprint arXiv   

Eckstein  Jonathan and Bertsekas  Dimitri 

On the
DouglasRachford splitting method and the proximal
point algorithm for maximal monotone operators  Mathematical Programming     

Franc    Guilherme and Bento  Jos    An explicit rate bound
In Information Theory  ISIT 
for overrelaxed admm 
  IEEE International Symposium on  pp   
IEEE   

Gabay  Daniel and Mercier  Bertrand    dual algorithm for
the solution of nonlinear variational problems via  nite
element approximation  Computers   Mathematics with
Applications     

Ghadimi  Euhanna  Teixeira  Andr    Shames  Iman  and Johansson  Mikael  Optimal parameter selection for the
alternating direction method of multipliers  quadratic
IEEE Trans  Autom  Control   
problems 
 

Giselsson  Pontus and Boyd  Stephen  Linear convergence
and metric selection in douglasrachford splitting and
admm   

Glowinski  Roland and Marroco     Sur   approximation 
par  el ements  nis   ordre un  et
la   esolution  par
  enalisationdualit     une classe de probl emes de
Dirichlet non lin eaires  ESAIM  Modlisation Mathmatique et Analyse Numrique     

Goldfarb  Donald  Ma  Shiqian  and Scheinberg  Katya 
Fast alternating linearization methods for minimizing the
sum of two convex functions  Mathematical Programming     

Goldstein  Tom and Setzer  Simon  Highorder methods for

basis pursuit  UCLA CAM Report  pp     

Goldstein  Tom    Donoghue  Brendan  Setzer  Simon 
and Baraniuk  Richard  Fast alternating direction optimization methods  SIAM Journal on Imaging Sciences 
   

Goldstein  Tom  Li  Min  and Yuan  Xiaoming  Adaptive
primaldual splitting methods for statistical learning and
In Advances in Neural Information
image processing 
Processing Systems  pp     

Goldstein  Tom  Taylor  Gavin  Barabin  Kawika  and
Sayre  Kent  Unwrapping ADMM  ef cient distributed
computing via transpose reduction  In AISTATS   

He  Bingsheng and Yuan  Xiaoming  On the      convergence rate of the douglasrachford alternating direction
method  SIAM Journal on Numerical Analysis   
   

He  Bingsheng and Yuan  Xiaoming  On nonergodic convergence rate of DouglasRachford alternating direction
method of multipliers  Numerische Mathematik   
   

He  Bingsheng  Yang  Hai  and Wang  Shengli  Alternating
direction method with selfadaptive penalty parameters
for monotone variational inequalities  Jour  Optim  Theory and Appl     

Kadkhodaie  Mojtaba  Christakopoulou  Konstantina  Sanjabi  Maziar  and Banerjee  Arindam  Accelerated alternating direction method of multipliers  In Proceedings
of the  th ACM SIGKDD  pp     

Adaptive Consensus ADMM for Distributed Optimization

Krizhevsky  Alex and Hinton  Geoffrey  Learning multiple

layers of features from tiny images   

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document
recognition  Proceedings of the IEEE   
   

Lin  Zhouchen  Liu  Risheng  and Su  Zhixun  Linearized
alternating direction method with adaptive penalty for
lowrank representation  In NIPS  pp     

Liu  Dong   and Nocedal  Jorge  On the limited memory
bfgs method for large scale optimization  Mathematical
programming     

Liu  Jun  Chen  Jianhui  and Ye  Jieping  Largescale sparse
In ACM SIGKDD  pp   

logistic regression 
 

Nishihara     Lessard     Recht     Packard     and
Jordan       general analysis of the convergence of
ADMM  In ICML   

Ouyang  Hua  He  Niao  Tran  Long  and Gray  Alexander    Stochastic alternating direction method of multipliers  ICML      

Raghunathan  Arvind and Di Cairano  Stefano  Alternating direction method of multipliers for strictly convex
quadratic programs  Optimal parameter selection 
In
American Control Conf  pp     

Rockafellar     Convex Analysis  Princeton University

Press   

Song  Changkyu  Yoon  Sejong  and Pavlovic  Vladimir 
Fast ADMM algorithm for distributed optimization with
adaptive penalty  AAAI   

Studer  Christoph  Goldstein  Tom  Yin  Wotao  and Baraniuk  Richard    Democratic representations  arXiv
preprint arXiv   

Taylor  Gavin  Burmeister  Ryan  Xu  Zheng  Singh 
Bharat  Patel  Ankit  and Goldstein  Tom  Training neural networks without gradients    scalable ADMM approach  ICML   

Tian  Wenyi and Yuan  Xiaoming  Faster alternating direction method of multipliers with   worstcase      
convergence rate   

Xu  Zheng  Figueiredo  Mario AT  and Goldstein  Thomas 
Adaptive ADMM with spectral penalty parameter selection  AISTATS     

Xu  Zheng  Figueiredo  Mario AT  Yuan  Xiaoming  Studer 
Christoph  and Goldstein  Thomas  Adaptive relaxed
ADMM  Convergence theory and practical implementation  CVPR     

Zhang  Ruiliang and Kwok  James    Asynchronous distributed ADMM for consensus optimization  In ICML 
pp     

Zhou  Bin  Gao  Li  and Dai  YuHong  Gradient methods
with adaptive stepsizes  Computational Optimization
and Applications     

Zou  Hui and Hastie  Trevor  Regularization and variable
selection via the elastic net  Journal of the Royal Statistical Society  Series    Statistical Methodology   
   

