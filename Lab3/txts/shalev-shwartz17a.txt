Failures of GradientBased Deep Learning

Shai ShalevShwartz   Ohad Shamir   Shaked Shammah  

Abstract

In recent years  Deep Learning has become the
goto solution for   broad range of applications 
often outperforming stateof theart  However 
it is important  for both theoreticians and practitioners  to gain   deeper understanding of the dif 
 culties and limitations associated with common
approaches and algorithms  We describe four
types of simple problems  for which the gradientbased algorithms commonly used in deep learning either fail or suffer from signi cant dif culties  We illustrate the failures through practical experiments  and provide theoretical insights
explaining their source  and how they might be
remedied 

  Introduction
The success stories of deep learning form an ever lengthening list of practical breakthroughs and stateof theart performances  ranging the  elds of computer vision
 Krizhevsky et al    He et al    Schroff et al 
  Taigman et al    audio and natural language
processing and generation  Collobert   Weston   
Hinton et al    Graves et al    van den Oord et al 
  as well as robotics  Mnih et al    Schulman
et al    to name just   few  The list of success stories
can be matched and surpassed by   list of practical  tips
and tricks  from different optimization algorithms  parameter tuning methods  Sutskever et al    Kingma   Ba 
  initialization schemes  Glorot   Bengio    architecture designs  Szegedy et al    loss functions 
data augmentation  Krizhevsky et al    and so on 
The current theoretical understanding of deep learning is
far from being suf cient for   rigorous analysis of the dif 
culties faced by practitioners  Progress must be made from

 School of Computer Science and Engineering  The Hebrew University  Weizmann Institute of Science  Correspondence to  Shai ShalevShwartz  shais cs huji ac il  Ohad
Shamir  ohad shamir weizmann ac il  Shaked Shammah
 shaked shammah mail huji ac il 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

both parties  from   practitioner   perspective  emphasizing
the dif culties provides practical insights to the theoretician  which in turn  supplies theoretical insights and guarantees  further strengthening and sharpening practical intuitions and wisdom  In particular  understanding failures of
existing algorithms is as important as understanding where
they succeed 
Our goal in this paper is to present and discuss families
of simple problems for which commonly used methods do
not show as exceptional   performance as one might expect  We use empirical results and insights as   ground
on which to build   theoretical analysis  characterising the
sources of failure  Those understandings are aligned  and
sometimes lead to  different approaches  either for an architecture  loss function  or an optimization scheme  and
explain their superiority when applied to members of those
families  Interestingly  the sources for failure in our experiment do not seem to relate to stationary point issues such
as spurious local minima or   plethora of saddle points   
topic of much recent interest        Dauphin et al   
Choromanska et al    but rather to more subtle issues  having to do with informativeness of the gradients 
signalto noise ratios  conditioning etc  The code for running all our experiments is available online  In this version  due to the lack of space  we focus on two families of
failures  and brie   describe two others in Section   We
refer the reader to  ShalevShwartz et al    for an extended version of this paper 
We start off in Section   by discussing   class of simple
learning problems for which the gradient information  central to deep learning algorithms  provably carries negligible information on the target function which we attempt to
learn  This result is   property of the learning problems
themselves  and holds for any speci   network architecture one may choose for tackling the learning problem  implying that no gradientbased method is likely to succeed 
Our analysis relies on tools and insights from the Statistical Queries literature  and underscores one of the main
de ciencies of Deep Learning  its reliance on local properties of the loss function  with the objective being of   global
nature 

 

https github com shakedshammah 

failures of DL  See command lines in Appendix   

Failures of GradientBased Deep Learning

Next  in Section   we tackle the ongoing dispute between two common approaches to learning  Most  if not
all  learning and optimization problems can be viewed as
some structured set of subproblems  The  rst approach 
which we refer to as the  endto end  approach  will tend
to solve all of the subproblems together in one shot  by optimizing   single primary objective  The second approach 
which we refer to as the  decomposition  one  will tend
to handle these subproblems separately  solving each one
by de ning and optimizing additional objectives  and not
rely solely on the primary objective  The bene ts of the
endto end approach  both in terms of requiring   smaller
amount of labeling and prior knowledge  and perhaps enabling more expressive architectures  cannot be ignored 
On the other hand  intuitively and empirically  the extra supervision injected through decomposition is helpful in the
optimization process  We experiment with   simple problem in which application of the two approaches is possible 
and the distinction between them is clear and intuitive  We
observe that an endto end approach can be much slower
than   decomposition method  to the extent that  as the
scale of the problem grows  no progress is observed  We
analyze this gap by showing  theoretically and empirically 
that the gradients are much more noisy and less informative with the endto end approach  as opposed to the decomposition approach  explaining the disparity in practical
performance 

  Parities and LinearPeriodic Functions
Most existing deep learning algorithms are gradientbased
methods  namely  algorithms which optimize an objective
through access to its gradient        some weight vector   
or estimates of the gradient  We consider   setting where
the goal of this optimization process is to learn some underlying hypothesis class    of which one member         is
responsible for labelling the data  This yields an optimization problem of the form

min
 

Fh   

The underlying assumption is that the gradient of the objective           rFh    contains useful information regarding the target function    and will help us make progress 
Below  we discuss   family of problems for which with
high probability  at any  xed point  the gradient  rFh   
will be essentially the same regardless of the underlying
target function    Furthermore  we prove that this holds
independently of the choice of architecture or parametrization  and using   deeper wider network will not help  The
family we study is that of compositions of linear and periodic functions  and we experiment with the classical problem of learning parities  Our empirical and theoretical
study shows that indeed  if there   little information in the

 

 

 
 
 
 
 
 
 
 

  
  
  

 

 

 

 

 

Training Iterations

 
 

Figure   Parity Experiment  Accuracy as   function of the number of training iterations  for various input dimensions 

gradient  using it for learning cannot succeed 

  Experiment
We begin with the simple problem of learning random parities  After choosing some           uniformly at random  our goal is to train   predictor mapping         
to      hx      where   is uniformly distributed  In
words    indicates whether the number of    in   certain
subset of coordinates of    indicated by    is odd or even 
For our experiments  we use the hinge loss  and   simple
network architecture of one fully connected layer of width
  with ReLU activations  and   fully connected
       
output layer with linear activation and   single unit  Note
that this class realizes the parity function corresponding to
any     see Lemma   in the appendix 
Empirically  as the dimension   increases  so does the dif 
 culty of learning  which can be measured in the accuracy
we arrive at after    xed number of training iterations  to
the point where around       no advance beyond random
performance is observed after reasonable time  Figure   illustrates the results 

  Analysis
To formally explain the failure from   geometric perspective  consider the stochastic optimization problem associated with learning   target function   

min
 

Fh      Ex

 pw          

 

where   is   loss function    are the stochastic inputs  assumed to be vectors in Euclidean space  and pw is some
predictor parametrized by   parameter vector           neural network of   certain architecture  We will assume that
  is differentiable    key quantity we will be interested in
studying is the variance of the gradient of   with respect to
   when   is drawn uniformly at random from   collection

Failures of GradientBased Deep Learning

of candidate target functions   

Var             

  rFh       

   rFh   

 

 

Intuitively  this measures the expected amount of  signal 
about the underlying target function contained in the gradient  As we will see later  this variance correlates with the
dif culty of solving   using gradientbased methods 
The following theorem bounds this variance term 

Theorem   Suppose that

    consists of realvalued functions   satisfying
Ex          such that for any two distinct        
   Ex             
  pw    is differentiable           and satis es
   pw             for some scalar     
Ex    
  The loss function   in   is either the square loss
          or   classi cation loss of the
          
form                   for some  Lipschitz function
   and the target function   takes values in  

Then

Var           

    
   

 

The proof is given in Appendix    The theorem implies
that if we try to learn an unknown target function  possibly
coming from   large collection of uncorrelated functions 
then the sensitivity of the gradient to the target function at
any point decreases linearly with    
Before we make   more general statement  let us return to
the case of parities  and study it through the lens of this
framework  Suppose that our target function is some parity
function chosen uniformly at random         random element from the set of    functions           hx      
           These are binary functions  which are easily seen to be mutually orthogonal  Indeed  for any      
Exh hx vi hx   ii   Exh hx     ii
Eh xi vi        
dYi 
which is nonzero if and only if        Therefore  by
Theorem   we get that Var                    that
is  exponentially small in the dimension    By Chebyshev  
inequality  this implies that the gradient at any point   will

 vi        vi     

dYi 

 

 

 This should not be confused with the variance of gradient

estimates used by SGD  which we discuss in Section  

be extremely concentrated around    xed point independent of   
This phenomenon of exponentiallysmall variance can also
be observed for other distributions  and learning problems
other than parities  Indeed  in  Shamir    it was shown
that this also holds in   more general setup  when the output
  corresponds to   linear function composed with   periodic
one  and the input   is sampled from   smooth distribution 

Theorem    Shamir   Let   be    xed periodic function  and let                   kv        for some
      Suppose     Rd is sampled from an arbitrary
mixture of distributions with the following property  The
square root of the density function   has   Fourier transform   satisfying Rx kxk      dx
  exp    Then if
Rx    dx
  denotes the objective function with respect to the squared
loss 

Var               exp      exp     

The condition on the Fourier transform of the density is
generally satis ed for smooth distributions       arbitrary
Gaussians whose covariance matrices are positive de nite 
with all eigenvalues at least     Thus  the bound is
extremely small as long as the norm   and the dimension  
are moderately large  and indicates that the gradients contains little signal on the underlying target function 
Based on these bounds  one can also formally prove that  
gradientbased method  under   reasonable model  will fail
in returning   reasonable predictor  unless the number of
iterations is exponentially large in   and       This provides
strong evidence that gradientbased methods indeed cannot learn random parities and linearperiodic functions  We
emphasize that these results hold regardless of which class
of predictors we use       they can be arbitrarily complex
neural networks    the problem lies in using   gradientbased method to train them  Also  we note that the dif 
culty lies in the random choice of    and the problem is
not dif cult if    is known and  xed in advance  for example  for   full parity                  this problem is
known to be solvable with an appropriate LSTM network
 Hochreiter   Schmidhuber   
Finally  we remark that the connection between parities 
dif culty of learning and orthogonal functions is not new 
and has already been made in the context of statistical
query learning  Kearns    Blum et al    This
refers to algorithms which are constrained to interact with

 Formally  this requires an oraclebased model  where given  
point    the algorithm receives the gradient at   up to some arbitrary error much smaller than machine precision  See  Shamir 
  Theorem   for details 

Failures of GradientBased Deep Learning

data by receiving estimates of the expected value of some
query over the underlying distribution      
the expected
value of the  rst coordinate  and it is wellknown that parities cannot be learned with such algorithms  Recently 
 Feldman et al    have formally shown that gradientbased methods with an approximate gradient oracle can be
implemented as   statistical query algorithm  which implies
that gradientbased methods are indeed unlikely to solve
learning problems which are known to be hard in the statistical queries framework  in particular parities  In the discussion on random parities above  we have simply made
the connection between gradientbased methods and parities more explicit  by direct examination of gradients  variance        the target function 

  Decomposition vs  Endto end
Many practical learning problems  and more generally  algorithmic problems  can be viewed as   structured composition of subproblems  Applicable approaches for   solution can either be tackling the problem in an endto end
manner  or by decomposition  Whereas for   traditional
algorithmic solution  the  divideand conquer  strategy is
an obvious choice  the ability of deep learning to utilize
big data and expressive architectures has made  endto end
training  an attractive alternative  Prior results of endto 
end  Mnih et al    Graves et al    and decomposition and added feedback    ulc ehre   Bengio    Hinton   Salakhutdinov    Szegedy et al    Caruana 
  approaches show success in both directions  Here 
we try to address the following questions  What is the price
of the rather appealing endto end approach  Is letting  
network  learn by itself  such   bad idea  When is it necessary  or worth the effort  to  help  it 
There are various aspects which can be considered in this
context  For example   ShalevShwartz   Shashua   
analyzed the difference between the approaches from the
sample complexity point of view  Here  we focus on
the optimization aspect  showing that an endto end approach might suffer from noninformative or noisy gradients  which may signi cantly affect the training time 
Helping the SGD process by decomposing the problem
leads to much faster training  We present   simple experiment  motivated by questions every practitioner must
answer when facing   new  non trivial problem  What exactly is the required training data  what network architecture should be used  and what is the right distribution of development efforts  These are all correlated questions with
no clear answer  Our experiments and analysis show that
making the wrong choice can be expensive 

Figure   Section    experiment   examples of samples from
   The   values of the top and bottom rows are   and   respectively 

  Experiment
Our experiment compares the two approaches in   computer vision setting  where convolutional neural networks
 CNN  have become the most widely used and successful
algorithmic architectures  We de ne   family of problems 
parameterized by        and show   gap  rapidly growing
with    between the performances of the endto end and
decomposition approaches 
Let   denote the space of       binary images  with
  distribution   de ned by the following sampling procedure 

  Sample                                 

     

  The image          associated with the above sample
is set to   everywhere  except for   straight line of
length    centered at        and rotated at an angle  
Note that as the images space is discrete  we round the
values corresponding to the points on the lines to the
closest integer coordinate 

Let us de ne an  intermediate  labeling function        
  denoting whether the line in   given image slopes
upwards or downwards  formally 

            

 

if    
otherwise

 

Figure   shows   few examples  We can now de ne
the problem for each    Each input instance is   tuple xk
                xk  of   images sampled        as
above  The target output is the parity over the image labels                xk  namely    xk
Many architectures of DNN can be used for predicting
   xk

   Qj     xj 

  given xk

    natural  highlevel  choice can be 

Failures of GradientBased Deep Learning

  Feed each of the images  separately  to   single CNN
 of some standard speci   architecture  for example 
LeNetlike  denoted    
   and parameterized by its
weights vector    outputting   single scalar  which
can be regarded as    score 

  Concatenate the  scores  of   tuple   entries  transform them to the range     using   sigmoid function  and feed the resulting vector into another network     
     of   similar architecture to the one de ned
in Section   outputting   single  tuplescore  which
can then be thresholded for obtaining the binary prediction 

     

     

 

 

 

 

     

 

 

 

 

     

Let the whole architecture be denoted Nw  Assuming that
    is expressive enough to provide  at least    weak
learner for      reasonable assumption  and that     can
express the relevant parity function  see Lemma   in the
appendix  we obtain that this architecture has the potential
for good performance 
The  nal piece of the experimental setting is the choice of  
loss function  Clearly  the primary loss which we   like to
minimize is the expected zeroone loss over the prediction 
Nw xk

  namely 

  and the label     xk
         

xk

 Nw xk

 
       xk

   secondary  loss which can be used in the decomposition approach is the zeroone loss over the prediction of
   

  and the respective   xk

  value 

    xk

        

xk

 hN  

    xk

  
      xk

Let       be some differentiable surrogates for       
  classical endto end approach will be to minimize     and
only it  this is our  primary  objective  We have no explicit
desire for     to output any speci   value  and hence  
is    priori  irrelevant    decomposition approach would be
to minimize both losses  under the assumption that   can
 direct     towards an  area  in which we know that the
resulting outputs of     can be separated by     Note
that using   is only possible when the   values are known
to us 
Empirically  when comparing performances based on the
 primary  objective  we see that the endto end approach
is signi cantly inferior to the decomposition approach  see
Figure   Using decomposition  we quickly arrive at  
good solution  regardless of the tuple   length     as long
as   is in the range where perfect input to     is solvable
by SGD  as described in Section   However  using the
endto end approach works only for         and completely fails already when        or larger  This may

Figure   Performance comparison  Section    experiment 
The red and blue curves correspond to the endto end and decomposition approaches  respectively  The plots show the zeroone
accuracy with respect to the primary objective  over   held out
test set  as   function of training iterations  We have trained the
endto end network for   SGD iterations  and the decomposition networks for only   iterations 

be somewhat surprising  as the endto end approach optimizes exactly the primary objective  composed of two subproblems each of which is easily solved on its own  and
with no additional irrelevant objectives 

  Analysis
We study the experiment from two directions  Theoretically  by analyzing the gradient variance  as in Section  
for   somewhat idealized version of the experiment  and
empirically  by estimating   signalto noise ratio  SNR 
measure of the stochastic gradients used by the algorithm 
Both approaches point to   similar issue  With the endto 
end approach  the gradients do not seem to be suf ciently
informative for the optimization process to succeed 
Before continuing  we note that   conceptually similar experiment to ours has been reported in    ulc ehre   Bengio 
   also involving   composition of an image recognition task and   simple Boolean formula  and with qualitatively similar results  However  that experiment came
without   formal analysis  and the failure was attributed to
local minima  In contrast  our analysis indicates that the
problem is not due to localminima  or saddle points  but
from the gradients being noninformative and noisy 
We begin with   theoretical result  which considers our experimental setup under two simplifying assumptions  First 
the input is assumed to be standard Gaussian  and second 
we assume the labels are generated by   target function of
the form hu xk
   sign   xl  The  rst assumption is merely to simplify the analysis  similar results can

    Qk

Failures of GradientBased Deep Learning

be shown more generally  but the argument becomes more
involved  The second assumption is equivalent to assuming that the labels      of individual images can be realized
by   linear predictor  which is roughly the case for simple
image labelling task such as ours 

Theorem   Let xk
  denote   ktuple             xk  of input
instances  and assume that each xl is        standard Gaussian in Rd  De ne

hu xk

   

sign   xl 

kYl 
 pw xk

and the objective         some predictor pw parameterized
by   

         
xk

  hu xk

   

Where the loss function   is either the square loss         
          or   classi cation loss of the form         
 
         for some  Lipschitz function   
Fix some    and suppose that pw    is differentiable       
           Then if
  and satis es Exk
   pw xk
     hu       Rd kuk     then

    

Var                         log   
    

 

The proof is given in Appendix    The theorem shows
that the  signal  regarding hu  or  if applying to our experiment  the signal for learning     had   been drawn
uniformly at random from some set of functions over   
decreases exponentially with    This is similar to the parity result in Section   but with an important difference 
Whereas the base of the exponent there was   here it is
the much smaller quantity   log   pd       in our experiment  we have       and       This indicates that already for very small values of    the information contained
in the gradients about   can become extremely small  and
prevent gradientbased methods from succeeding  fully according with our experiment 
To complement this analysis  which applies to an idealized
version of our experiment  we consider   related  signalto noise   SNR  quantity  which can be empirically estimated in our actual experiment  To motivate it  note that  
key quantity used in the proof of Theorem   for estimating
the amount of signal carried by the gradient  is the squared
norm of the correlation between the gradient of the predictor pw    xk
  and the target function hu 
which we denote by Sigu 

   pw xk

     

Sigu   

xk

 hu xk

   xk

 

 

 

 

 

 

 

 

 

Figure   Section    experiment 
comparing the SNR for
the endto end approach  red  and the decomposition approach
 blue  as   function of    in loge scale 

We will consider the ratio between this quantity and  
 noise  term Noiu       the variance of this correlation over
the samples 

Noiu    
xk

   xk

     

xk

 hu xk

   xk

 hu xk

 

 

 

Since here the randomness is with respect to the data rather
than the target function  as in Theorem   we can estimate this SNR ratio in our experiment  It is wellknown
       Ghadimi   Lan    that the amount of noise in
the stochastic gradient estimates used by stochastic gradient descent crucially affects its convergence rate  Hence 
smaller SNR should be correlated with worse performance 
We empirically estimated this SNR measure  Sigy Noiy 
for the gradients        the weights of the last layer of    
 which potentially learns our intermediate labeling function    at the initialization point in parameter space  The
SNR estimate for various values of   are plotted in Figure   We indeed see that when       the SNR appears
to approach extremely small values  where the estimator  
noise  and the additional noise introduced by    nite  oating point representation  can completely mask the signal 
which can explain the failure in this case 
In Section   in the Appendix  we also present   second 
more synthetic  experiment  which demonstrates   case
where the decomposition approach directly decreases the
stochastic noise in the SGD optimization process  hence
bene ting the convergence rate 

  Additional Failure Families   Brief

Discussion

In an extended version of this paper   ShalevShwartz et al 
  we broadly discuss two additional families of failures  Here  due to lack of space  we present them brie   
First  we demonstrate the importance of both the network  

Failures of GradientBased Deep Learning

architecture and the optimization algorithm on the training
time  While the choice of architecture is usually studied
in the context of its expressive power  we show that even
when two architectures have the same expressive power for
  given task  there may be   tremendous difference in the
ability to optimize them  We analyze the required runtime
of gradient descent for the two architectures through the
lens of the condition number of the problem  We further
show that conditioning techniques can yield additional orders of magnitude speedups  The experimental setup for
this problem is around   seemingly simple problem   encoding   piecewise linear onedimensional curve    summary of experimental results  when training with different
architectures and conditioning techniques  is found in Figure   Despite the simplicity of this problem  we show
that following the common rule of  perhaps   should use
  deeper wider network  does not signi cantly help here 
Finally  we consider deep learning   reliance on  vanilla 
gradient information for the optimization process  We previously discussed the de ciency of using   local property
of the objective in directing global optimization  We turn
our focus to   simple case in which it is possible to solve
the optimization problem based on local information  but
not in the form of   gradient  We experiment with architectures that contain activation functions with  at regions 
which leads to the well known vanishing gradient problem 
Practitioners take great care when working with such activation functions  and many heuristic tricks are applied in
order to initialize the network   weights in non at areas
of its activations  Here  we show that by using   different update rule  we manage to solve the learning problem
ef ciently  Moreover  one can show convergence guarantees for   family of such functions  This provides   clean
example where nongradient based optimization schemes
can overcome the limitations of gradientbased learning 

  Summary
In this paper  we considered different families of problems 
where standard gradientbased deep learning approaches
appear to suffer from signi cant dif culties  Our analysis
indicates that these dif culties are not necessarily related
to stationary point issues such as spurious local minima or
  plethora of saddle points  but rather more subtle issues 
Insuf cient information in the gradients about the underlying target function  low SNR  bad conditioning  or  atness
in the activations  see Figure   for   graphical illustration 
We consider it as    rst step towards   better understanding of where standard deep learning methods might fail  as
well as what approaches might overcome these failures 

    Linear architecture 

    Convolutional architecture 

    Convolutional architecture with conditioning 

    Vanilla deep auto encoder 

Figure   Examples for decoded outputs of several experiments 
learning to encode PWL curves  In blue are the original curves 
In red are the decoded curves  The plot shows the outputs for
two curves  after     and   iterations  from left to
right  The convolutional architecture  with conditioning  clearly
outperforms others  both in terms of convergence rate and  nal
accuracy 

 See

fizzbuzz intensorflow  for
hind this quote 

http joelgrus com 
inspiration be 

the

Failures of GradientBased Deep Learning

 

 

 

 

 

   

 

 

 

    Extremely small variance in the loss surface   gradient         different target functions  each with   very
different optimum 

 

   

 

 

 

    Low SNR of gradient estimates  The dashed lines
represent losses        different samples  each implying
  very different estimate than the average gradient 

 

 

 

 

 

 

    Bad conditioning     dimensional example of   loss
function   quiver  Following the gradient is far from
being the best direction to go 

 

 

 

   

 

 

 

    Completely  at activation   no information in the
gradient 

Figure     graphical summary of limitations of gradientbased
learning 

References
Blum  Avrim  Furst  Merrick  Jackson  Jeffrey  Kearns 
Michael  Mansour  Yishay  and Rudich  Steven  Weakly
learning dnf and characterizing statistical query learning using fourier analysis  In Proceedings of the twentysixth annual ACM symposium on Theory of computing 
pp    ACM   

Caruana  Rich  Multitask learning  In Learning to learn 

pp    Springer   

Choromanska  Anna  Henaff  Mikael  Mathieu  Michael 
Arous    erard Ben  and LeCun  Yann  The loss surfaces
of multilayer networks  In AISTATS   

Collobert  Ronan and Weston  Jason    uni ed architecture
for natural language processing  Deep neural networks
with multitask learning  In Proceedings of the  th international conference on Machine learning  pp   
ACM   

Dauphin  Yann    Pascanu  Razvan  Gulcehre  Caglar 
Cho  Kyunghyun  Ganguli  Surya  and Bengio  Yoshua 
Identifying and attacking the saddle point problem in
highdimensional nonconvex optimization  In Advances
in neural information processing systems  pp   
   

Feldman  Vitaly  Guzman  Cristobal  and Vempala  Santosh  Statistical query algorithms for stochastic convex
optimization  arXiv preprint arXiv   

Ghadimi  Saeed and Lan  Guanghui  Stochastic  rstand
zerothorder methods for nonconvex stochastic programming  SIAM Journal on Optimization   
 

Glorot  Xavier and Bengio  Yoshua  Understanding the dif 
 culty of training deep feedforward neural networks  In
Aistats  volume   pp     

Graves  Alex  Mohamed  Abdelrahman  and Hinton  Geoffrey  Speech recognition with deep recurrent neural networks  In Acoustics  speech and signal processing  icassp    ieee international conference on  pp 
  IEEE   

  ulc ehre     alar and Bengio  Yoshua  Knowledge matters 
Importance of prior information for optimization  Journal of Machine Learning Research     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition  In
The IEEE Conference on Computer Vision and Pattern
Recognition  CVPR  June  

Failures of GradientBased Deep Learning

Sutskever  Ilya  Martens  James  Dahl  George    and Hinton  Geoffrey    On the importance of initialization and
momentum in deep learning  ICML    
 

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 
Going deeper with convolutions 
In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition  pp     

Szegedy  Christian  Ioffe  Sergey  Vanhoucke  Vincent  and
Alemi  Alex  Inceptionv  inceptionresnet and the impact of residual connections on learning  arXiv preprint
arXiv   

Taigman  Yaniv  Yang  Ming  Ranzato  Marc Aurelio  and
Wolf  Lior  Deepface  Closing the gap to humanlevel
performance in face veri cation 
In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition  pp     

van den Oord    aron  Dieleman  Sander  Zen  Heiga  Simonyan  Karen  Vinyals  Oriol  Graves  Alex  Kalchbrenner  Nal  Senior  Andrew  and Kavukcuoglu  Koray  Wavenet    generative model for raw audio  CoRR
abs   

Hinton  Geoffrey  Deng  Li  Yu  Dong  Dahl  George   
Mohamed  Abdelrahman  Jaitly  Navdeep  Senior  Andrew  Vanhoucke  Vincent  Nguyen  Patrick  Sainath 
Tara    et al  Deep neural networks for acoustic modeling in speech recognition  The shared views of four
research groups  IEEE Signal Processing Magazine   
   

Hinton  Geoffrey   and Salakhutdinov  Ruslan    Reducing the dimensionality of data with neural networks  science     

Hochreiter  Sepp and Schmidhuber    urgen  Long shortterm memory  Neural computation   
 

Kearns  Michael  Ef cient noisetolerant learning from statistical queries  Journal of the ACM  JACM   
   

Kingma  Diederik and Ba 

Jimmy 
method for stochastic optimization 
arXiv   

Adam 

 
arXiv preprint

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In Advances in neural information processing
systems  pp     

Mnih  Volodymyr  Kavukcuoglu  Koray  Silver  David 
Rusu  Andrei    Veness  Joel  Bellemare  Marc   
Graves  Alex  Riedmiller  Martin  Fidjeland  Andreas   
Ostrovski  Georg  et al  Humanlevel control through
deep reinforcement learning  Nature   
   

Schroff  Florian  Kalenichenko  Dmitry  and Philbin 
James  Facenet    uni ed embedding for face recognition and clustering 
In The IEEE Conference on
Computer Vision and Pattern Recognition  CVPR  June
 

Schulman  John  Levine  Sergey  Abbeel  Pieter  Jordan 
Michael    and Moritz  Philipp  Trust region policy optimization  In ICML  pp     

ShalevShwartz  Shai and Shashua  Amnon  On the sample
complexity of endto end training vs  semantic abstraction training  arXiv preprint arXiv   

ShalevShwartz  Shai  Shamir  Ohad  and Shammah 
arXiv preprint

Failures of deep learning 

Shaked 
arXiv   

Shamir  Ohad  Distributionspeci   hardness of learning neural networks  arXiv preprint arXiv 
 

