On the Expressive Power of Deep Neural Networks

Maithra Raghu     Ben Poole   Jon Kleinberg   Surya Ganguli   Jascha Sohl Dickstein  

Abstract

We propose   new approach to the problem of
neural network expressivity  which seeks to characterize how structural properties of   neural network family affect the functions it is able to compute  Our approach is based on an interrelated
set of measures of expressivity  uni ed by the
novel notion of trajectory length  which measures how the output of   network changes as
the input sweeps along   onedimensional path 
Our  ndings show that    The complexity of
the computed function grows exponentially with
depth   All weights are not equal  trained networks are more sensitive to their lower  initial 
layer weights   Trajectory regularization is  
simpler alternative to batch normalization  with
the same performance 

  Introduction
Deep neural networks have proved astoundingly effective
at   wide range of empirical tasks  from image classi cation  Krizhevsky et al    to playing Go  Silver et al 
  and even modeling human learning  Piech et al 
 
Despite these successes  understanding of how and why
neural network architectures achieve their empirical successes is still lacking  This includes even the fundamental question of neural network expressivity  how the architectural properties of   neural network  depth  width  layer
type  affect the resulting functions it can compute  and its
ensuing performance 
This is   foundational question  and there is   rich history
of prior work addressing expressivity in neural networks 
However  it has been challenging to derive conclusions that
provide both theoretical generality with respect to choices
of architecture as well as meaningful insights into practical

 Cornell University  Google Brain  Stanford University  Cor 

respondence to  Maithra Raghu  maithrar gmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

performance 
Indeed  the very  rst results on this question take   highly
theoretical approach  from using functional analysis to
show universal approximation results  Hornik et al   
Cybenko    to analysing expressivity via comparisons
to boolean circuits  Maass et al    and studying network VC dimension  Bartlett et al    While these
results provided theoretically general conclusions  the shallow networks they studied are very different from the deep
models that have proven so successful in recent years 
In response  several recent papers have focused on understanding the bene ts of depth for neural networks  Pascanu et al    Montufar et al    Eldan and Shamir 
  Telgarsky    Martens et al    Bianchini and
Scarselli    These results are compelling and take
modern architectural changes into account  but they only
show that   speci   choice of weights for   deeper network
results in inapproximability by   shallow  typically one or
two hidden layers  network 
In particular  the goal of this new line of work has been
to establish lower bounds   showing separations between
shallow and deep networks   and as such they are based
on handcoded constructions of speci   network weights 
Even if the weight values used in these constructions are
robust to small perturbations  as in  Pascanu et al   
Montufar et al    the functions that arise from these
constructions tend toward extremal properties by design 
and there is no evidence that   network trained on data ever
resembles such   function 
This has meant that   set of fundamental questions about
neural network expressivity has remained largely unanswered  First  we lack   good understanding of the  typical  case rather than the worst case in these bounds for
deep networks  and consequently have no way to evaluate whether the handcoded extremal constructions provide
  re ection of the complexity encountered in more standard settings  Second  we lack an understanding of upper
bounds to match the lower bounds produced by this prior
work  do the constructions used to date place us near the
limit of the expressive power of neural networks  or are
there still large gaps  Finally  if we had an understanding
of these two issues  we might begin to draw connections
between network expressivity and observed performance 

On the Expressive Power of Deep Neural Networks

Our contributions  Measures of Expressivity and their
Applications
In this paper  we address this set of challenges by de ning and analyzing an interrelated set of measures of expressivity for neural networks  our framework
applies to   wide range of standard architectures  independent of speci   weight choices  We begin our analysis at
the start of training  after random initialization  and later
derive insights connecting network expressivity and performance 
Our  rst measure of expressivity is based on the notion of
an activation pattern  in   network where the units compute
functions based on discrete thresholds  we can ask which
units are above or below their thresholds       which units
are  active  and which are not  For the range of standard
architectures that we consider  the network is essentially
computing   linear function once we    the activation pattern  thus  counting the number of possible activation patterns provides   concrete way of measuring the complexity
beyond linearity that the network provides  We give an upper bound on the number of possible activation patterns 
over any setting of the weights  This bound is tight as it
matches the handconstructed lower bounds of earlier work
 Pascanu et al    Montufar et al   
Key to our analysis is the notion of   transition  in which
changing an input   to   nearby input       changes the
activation pattern  We study the behavior of transitions as
we pass the input along   onedimensional parametrized
trajectory      Our central  nding is that the trajectory
length grows exponentially in the depth of the network 
Trajectory length serves as   unifying notion in our measures of expressivity  and it leads to insights into the behavior of trained networks  Speci cally  we  nd that the
exponential growth in trajectory length as   function of
depth implies that small adjustments in parameters lower
in the network induce larger changes than comparable adjustments higher in the network  We demonstrate this phenomenon through experiments on MNIST and CIFAR 
where the network displays much less robustness to noise
in the lower layers  and better performance when they are
trained well  We also explore the effects of regularization
methods on trajectory length as the network trains and propose   less computationally intensive method of regularization  trajectory regularization  that offers the same performance as batch normalization 
The contributions of this paper are thus 

  Measures of expressivity  We propose easily computable measures of neural network expressivity that
capture the expressive power inherent in different
neural network architectures  independent of speci  
weight settings 

  Exponential

trajectories  We  nd an exponen 

tial depth dependence displayed by these measures 
through   unifying analysis in which we study how the
network transforms its input by measuring trajectory
length

  All weights are not equal  the lower layers matter
more  We show how these results on trajectory length
suggest that optimizing weights in lower layers of the
network is particularly important 

  Trajectory Regularization Based on understanding the
effect of batch norm on trajectory length  we propose
  new method of regularization  trajectory regularization  that offers the same advantages as batch norm 
and is computationally more ef cient 

In prior work  Poole et al    we studied the propagation of Riemannian curvature through random networks
by developing   mean  eld theory approach  Here  we take
an approach grounded in computational geometry  presenting measures with   combinatorial  avor and explore the
consequences during and after training 

  Measures of Expressivity
Given   neural network of   certain architecture    some
depth  width  layer types  we have an associated function 
FA        where   is an input and   represents all the
parameters of the network  Our goal is to understand how
the behavior of FA        changes as   changes  for values
of   that we might encounter during training  and across
inputs   
The  rst major dif culty comes from the high dimensionality of the input  Precisely quantifying the properties of
FA        over the entire input space is intractable  As  
tractable alternative  we study simple one dimensional trajectories through input space  More formally 
De nition  Given two points          Rm  we say     
is   trajectory  between    and    if      is   curve
parametrized by   scalar         with         and
       
Simple examples of   trajectory would be   line        
tx             or   circular arc         cos       
sin      but in general      may be more complicated  and potentially not expressible in closed form 
Armed with this notion of trajectories  we can begin to de 
 ne measures of expressivity of   network FA        over
trajectories     

  Neuron Transitions and Activation Patterns

In  Montufar et al    the notion of    linear region 
is introduced  Given   neural network with piecewise lin 

On the Expressive Power of Deep Neural Networks

ear activations  such as ReLU or hard tanh  the function
it computes is also piecewise linear    consequence of the
fact that composing piecewise linear functions results in  
piecewise linear function  So one way to measure the  expressive power  of different architectures   is to count the
number of linear pieces  regions  which determines how
nonlinear the function is 
In fact    change in linear region is caused by   neuron
transition in the output layer  More precisely 
De nition For  xed     we say   neuron with piecewise
linear region transitions between inputs          if its activation function switches linear region between   and      
So   ReLU transition would be given by   neuron switching
from off to on  or vice versa  and for hard tanh by switching between saturation at   to its linear middle region to
saturation at   For any generic trajectory      we can thus
de ne    FA          to be the number of transitions undergone by output neurons      
the number of linear regions  as we sweep the input      Instead of just concentrating on the output neurons however  we can look at this
pattern over the entire network  We call this an activation
patten 
De nition We can de ne AP FA        to be the activation pattern     string of form    num neurons  for ReLUs 
and      num neurons  for hard tanh  of the network encoding the linear region of the activation function of every
neuron  for an input   and weights    
Overloading notation slightly  we can also de ne  similarly
to transitions    FA          as the number of distinct
activation patterns as we sweep   along      As each
distinct activation pattern corresponds to   different linear
function of the input  this combinatorial measure captures
how much more expressive   is over   simple linear mapping 
Returning to Montufar et al  they provide   construction
       speci   set of weights    that results in an exponential increase of linear regions with the depth of the architectures  They also appeal to Zaslavsky   theorem  Stanley    from the theory of hyperplane arrangements to
show that   shallow network       one hidden layer  with the
same number of parameters as   deep network  has   much
smaller number of linear regions than the number achieved
by their choice of weights    for the deep network 
More formally  letting    be   fully connected network
with one hidden layer  and Al   fully connected network
with the same number of parameters  but   hidden layers 
they show

 WT  FA               FA      

 

We derive   much more general result by considering the

 global  activation patterns over the entire input space  and
prove that for any fully connected network  with any number of hidden layers  we can upper bound the number of linear regions it can achieve  over all possible weight settings
    This upper bound is asymptotically tight  matched by
the construction given in  Montufar et al    Our result
can be written formally as 
Theorem    Tight  Upper Bound for Number of Activation Patterns Let        denote   fully connected network
with   hidden layers of width    and inputs in Rm  Then the
number of activation patterns   FAn    Rm      is upper
bounded by   kmn  for ReLU activations  and     mn 
for hard tanh 

From this we can derive   chain of inequalities  Firstly 
from the theorem above we  nd an upper bound of
  FAn    Rm      over all         

      FA     Rm                   

Next  suppose we have   neurons in total  Then we want to
compare  for wlog ReLUs  quantities like      cid      cid    
for different   cid 
But      cid      cid             cid mn cid 

 cid mx  for        is          we get   for

  and so  noting that

the maxima of cid   

 

          in comparison to  

               

        

 
 

            

 
     

                  

We prove this via an inductive proof on regions in   hyperplane arrangement  The proof can be found in the Appendix  As noted in the introduction  this result differs
from earlier lowerbound constructions in that it is an upper
bound that applies to all possible sets of weights  Via our
analysis  we also prove
Theorem   Regions in Input Space Given the corresponding function of   neural network FA Rm      with ReLU
or hard tanh activations  the input space is partitioned into
convex polytopes  with FA Rm      corresponding to   different linear function on each region 

This result is of independent interest for optimization    
linear function over   convex polytope results in   well behaved loss function and an easy optimization problem  Understanding the density of these regions during the training
process would likely shed light on properties of the loss
surface  and improved optimization methods    picture of
  network   regions is shown in Figure  

On the Expressive Power of Deep Neural Networks

Figure   Deep networks with piecewise linear activations subdivide input space into convex polytopes  We take   three hidden
layer ReLU network  with input        and four units in each
layer  The left pane shows activations for the  rst layer only  As
the input is in    neurons in the  rst hidden layer have an associated line in    depicting their activation boundary  The left pane
thus has four such lines  For the second hidden layer each neuron
again has   line in input space corresponding to on off  but this
line is different for each region described by the  rst layer activation pattern  So in the centre pane  which shows activation boundary lines corresponding to second hidden layer neurons in green
 and  rst hidden layer in black  we can see the green lines  bend 
at the boundaries   The reason for this bending becomes apparent through the proof of Theorem   Finally  the right pane adds
the on off boundaries for neurons in the third hidden layer  in purple  These lines can bend at both black and green boundaries  as
the image shows  This  nal set of convex polytopes corresponds
to all activation patterns for this network  with its current set of
weights  over the unit square  with each polytope representing  
different linear function 
  EMPIRICALLY COUNTING TRANSITIONS

We empirically tested the growth of the number of activations and transitions as we varied   along      to understand their behavior  We found that for bounded non linearities  especially tanh and hardtanh  not only do we observe
exponential growth with depth  as hinted at by the upper
bound  but that the scale of parameter initialization also affects the observations  Figure   We also experimented
with sweeping the weights   of   layer through   trajectory       and counting the different labellings output by
the network  This  dichotomies  measure is discussed further in the Appendix  and also exhibits the same growth
properties  Figure  

  Trajectory Length

In fact  there turns out to be   reason for the exponential
growth with depth  and the sensitivity to initialization scale 
Returning to our de nition of trajectory  we can de ne an
immediately related quantity  trajectory length
De nition  Given   trajectory       we de ne its length 
       to be the standard arc length 

 cid 

 

 cid cid cid cid cid cid cid cid  dx   

dt

 cid cid cid cid cid cid cid cid  dt

        

Intuitively  the arc length breaks      up into in nitesimal
intervals and sums together the Euclidean length of these

Figure   The number of transitions seen for fully connected networks of different widths  depths and initialization scales  with
  circular trajectory between MNIST datapoints  The number of
transitions grows exponentially with the depth of the architecture 
as seen in  left  The same rate of growth is not seen with increasing architecture width  plotted in  right  There is   surprising
dependence on the scale of initialization  explained in  

Figure   Picture showing   trajectory increasing with the depth of
  network  We start off with   circular trajectory  left most pane 
and feed it through   fully connected tanh network with width
  Pane second from left shows the image of the circular trajectory  projected down to two dimensions  after being transformed
by the  rst hidden layer  Subsequent panes show projections of
the latent image of the circular trajectory after being transformed
by more hidden layers  The  nal pane shows the the trajectory
after being transformed by all the hidden layers 

intervals 
If we let        denote  as before  fully connected networks
with   hidden layers each of width    and initializing with
weights        
      accounting for input scaling as
typical  and biases        
Theorem   Bound on Growth of Trajectory Length Let
FA   cid      be   ReLU or hard tanh random neural network
and        one dimensional trajectory with         having
  non trival perpendicular component to      for all     
      not   line  Then de ning                   to be
the image of the trajectory in layer   of the network  we
have

    we  nd that 

   

  cid 

 cid     

        

 cid  

 cid 

 
  
  
     

      

for ReLUs

   

  cid 

        

 cid     

 

 cid 

 

 

  

      cid 

     
 

     
 

  

      

     Layer    Layer    Layer  Network depth Transitions numberNumber of transitions with increasing depthw  scl    scl    scl    scl    scl    scl    scl Layer width Number of transitionsNumber of transitions with increasing widthlay  scl lay  scl lay  scl lay  scl lay  scl lay  scl lay  scl lay  scl lay  scl On the Expressive Power of Deep Neural Networks

for hard tanh

                   

Figure   We look at trajectory growth with different initialization scales as   trajectory is propagated through   convolutional
architecture for CIFAR  with ReLU activations  The analysis of Theorem   was for fully connected networks  but we see
that trajectory growth holds  albeit with slightly higher scales  for
convolutional architectures also  Note that the decrease in trajectory length  seen in layers   and   is expected  as those layers are
pooling layers 

That is         grows exponentially with the depth of the
network  but the width only appears as   base  of the exponent  This bound is in fact tight in the limits of large   
and   
  schematic image depicting this can be seen in Figure  
and the proof can be found in the Appendix    rough outline is as follows  we look at the expected growth of the
difference between   point        on the curve and   small
perturbation       dt  from layer   to layer    Denot 

ing this quantity cid cid cid cid       cid cid cid cid  we derive   recurrence relating cid cid cid cid       cid cid cid cid  and cid cid cid cid       cid cid cid cid  which can be composed

 cid cid cid cid cid cid     

   

to give the desired growth rate 
The analysis is complicated by the statistical dependence
on the image of the input        So we instead form
  recursion by looking at the component of the difference
perpendicular to the image of the input in that layer      

 cid cid cid cid cid cid  which results in the condition on      in the

statement 
In Figures     we see the growth of an input trajectory
for ReLU networks on CIFAR  and MNIST  The CIFAR 
  network is convolutional but we observe that these layers also result in similar rates of trajectory length increases
to the fully connected layers  We also see  as would be
expected  that pooling layers act to reduce the trajectory
length  We discuss upper bounds in the Appendix 
For the hard tanh case  and more generally any bounded
nonlinearity  we can formally prove the relation of trajectory length and transitions under an assumption  assume
that while we sweep      all neurons are saturated unless transitioning saturation endpoints  which happens very

Figure   The number of transitions is linear in trajectory length 
Here we compare the empirical number of transitions to the length
of the trajectory  for different depths of   hardtanh network  We
repeat this comparison for   variety of network architectures  with
different network width   and weight variance  
  

rapidly   This is the case for      large initialization scales 
Then we have 
Theorem   Transitions proportional to trajectory length
Let FAn   be   hard tanh network with   hidden layers each
of width    And let

   
  cid 

     
 
 
 

  

Then    FAn                                 for   initialized with weight and bias scales        

Note that the expression for                 is exactly the
expression given by Theorem   when    is very large and
dominates     We can also verify this experimentally in
settings where the simpilfying assumption does not hold 
as in Figure  

  Insights from Network Expressivity
Here we explore the insights gained from applying
our measurements of expressivity  particularly trajectory
length  to understand network performance  We examine
the connection of expressivity and stability  and inspired
by this  propose   new method of regularization  trajectory
regularization that offers the same advantages as the more
computationally intensive batch normalization 

  Expressivity and Network Stability

The analysis of network expressivity offers interesting
takeaways related to the parameter and functional stability of   network  From the proof of Theorem   we saw
that   perturbation to the input would grow exponentially
in the depth of the network  It is easy to see that this analysis is not limited to the input layer  but can be applied to
any layer  In this form  it would say

  perturbation at   layer grows exponentially in the

remaining depth after that layer 

This means that perturbations to weights in lower layers
should be more costly than perturbations in the upper lay 

 Network depth Trajectory lengthTrajectory length growth with increasing depthscl scl scl scl scl scl On the Expressive Power of Deep Neural Networks

tion  One regularization technique that has been extremely
successful for training neural networks is Batch Normalization  Ioffe and Szegedy   

Figure   We then pick   single layer of   conv net trained to high
accuracy on CIFAR  and add noise to the layer weights of increasing magnitudes  testing the network accuracy as we do so 
We  nd that the initial  lower  layers of the network are least robust to noise   as the  gure shows  adding noise of   magnitude to the  rst layer results in     drop in accuracy  while the
same amount of noise added to the  fth layer barely results in  
  drop in accuracy  This pattern is seen for many different initialization scales  even for    typical  scaling of  
      used in
the experiment 

ers  due to exponentially increasing magnitude of noise 
and result in   much larger drop of accuracy  Figure   in
which we train   conv network on CIFAR  and add noise
of varying magnitudes to exactly one layer  shows exactly
this 
We also  nd that the converse  in some sense  holds  after
initializing   network  we trained   single layer at different
depths in the network and found monotonically increasing
performance as layers lower in the network were trained 
This is shown in Figure   and Figure   in the Appendix 

Figure   Demonstration of expressive power of remaining depth
on MNIST  Here we plot train and test accuracy achieved by training exactly one layer of   fully connected neural net on MNIST 
The different lines are generated by varying the hidden layer chosen to train  All other layers are kept frozen after random initialization  We see that training lower hidden layers leads to better
performance  The networks had width       weight variance
      and hardtanh nonlinearities  Note that we only train
 
from the second hidden layer  weights     onwards  so that
the number of parameters trained remains  xed 

Figure   Training increases trajectory length even for typical
 
      initialization values of     Here we propagate   circular trajectory joining two CIFAR  datapoints through   conv
net without batch norm  and look at how trajectory length changes
through training  We see that training causes trajectory length
to increase exponentially with depth  exceptions only being the
pooling layers and the  nal fc layer  which halves the number of
neurons  Note that at Step   the network is not in the exponential growth regime  We observe  discussed in Figure   that even
networks that aren   initialized in the exponential growth regime
can be pushed there through training 

By taking measures of trajectories during training we  nd
that without batch norm  trajectory length tends to increase
during training  as shown in Figures   and Figure   in
In these experiments  two networks were
the Appendix 
      and trained to high test accuracy on
initialized with  
CIFAR  and MNIST  We see that in both cases  trajectory
length increases as training progresses 
      is not in the exponential
  surprising observation is  
growth increase regime at initialization for the CIFAR 
architecture  Figure   at Step   But note that even with  
smaller weight initialization  weight norms increase during
training  shown in Figure   pushing typically initialized
networks into the exponential growth regime 
While the initial growth of trajectory length enables greater
functional expressivity  large trajectory growth in the learnt
representation results in an unstable representation  witnessed in Figure  
In Figure   we train another conv
net on CIFAR  but this time with batch normalization 
We see that the batch norm layers reduce trajectory length 
helping stability 

  Trajectory Length and Regularization  The Effect

of Batch Normalization

Expressivity measures  especially trajectory length  can
also be used to better understand the effect of regulariza 

  Trajectory Regularization

Motivated by the fact that batch normalization decreases
trajectory length and hence helps stability and generalization  we consider directly regularizing on trajectory length 

 Noise magnitude AccuracyCIFAR   accuracy against noise in diff layerslay lay lay lay lay lay lay Epoch Number AccuracyTrain Accuracy Against Epoch Epoch NumberTest Accuracy Against Epochlay  lay  lay  lay  lay  lay  lay  lay  inc             fc fc Trajectory length Layer depthCIFAR  Trajectory growth  without Batch Norm On the Expressive Power of Deep Neural Networks

Figure   This  gure shows how the weight scaling of   CIFAR 
network evolves during training  The network was initialized with
      which increases across all layers during training 
 

Figure   Growth of circular trajectory between two datapoints
with batch norm layers for   conv net on CIFAR  The network
was initialized as typical  with  
      Note that the batch norm
layers in Step   are poorly behaved due to division by   close to  
variance  But after just   few hundred gradient steps and continuing onwards  we see the batch norm layers  dotted lines  reduce
trajectory length  stabilising the representation without sacri cing
expressivity 

we replace every batch norm layer used in the conv net
in Figure   with   trajectory regularization layer  This
layer adds to the loss  current length orig length  and
then scales the outgoing activations by   where   is   parameter to be learnt  In implementation  we typically scale
the additional loss above with   constant   to reduce
magnitude in comparison to classi cation loss  Our results 
Figure   show that both trajectory regularization and batch
norm perform comparably  and considerably better than not
using batch norm  One advantage of using Trajectory Regularization is that we don   require different computations
to be performed for train and test  enabling more ef cient
implementation 

  Discussion
Characterizing the expressiveness of neural networks  and
understanding how expressiveness varies with parameters
of the architecture  has been   challenging problem due to
the dif culty in identifying meaningful notions of expressivity and in linking their analysis to implications for these
networks in practice  In this paper we have presented an

Figure   We replace each batch norm layer of the CIFAR 
conv net with   trajectory regularization layer  described in Section   During training trajectory length is easily calculated as
  piecewise linear trajectory between adjacent datapoints in the
minibatch  We see that trajectory regularization achieves the same
performance as batch norm  albeit with slightly more train time 
However  as trajectory regularization behaves the same during
train and test time  it is simpler and more ef cient to implement 

interrelated set of expressivity measures  we have shown
tight exponential bounds on the growth of these measures
in the depth of the networks  and we have offered   unifying view of the analysis through the notion of trajectory
length  Our analysis of trajectories provides insights for the
performance of trained networks as well  suggesting that
networks in practice may be more sensitive to small perturbations in weights at lower layers  We also used this to
explore the empirical success of batch norm  and developed
  new regularization method   trajectory regularization 
This work raises many interesting directions for future
work  At   general level  continuing the theme of  principled deep understanding  it would be interesting to link
measures of expressivity to other properties of neural network performance  There is also   natural connection between adversarial examples   Goodfellow et al    and
trajectory length  adversarial perturbations are only   small
distance away in input space  but result in   large change in
classi cation  the output layer  Understanding how trajectories between the original input and an adversarial perturbation grow might provide insights into this phenomenon 
Another direction  partially explored in this paper  is regularizing based on trajectory length    very simple version
of this was presented  but further performance gains might
be achieved through more sophisticated use of this method 

Acknowledgements
We thank Samy Bengio  Ian Goodfellow  Laurent Dinh 
and Quoc Le for extremely helpful discussion 

 Train Step Scaled weight varianceCIFAR  scaled weight variances with traininglay  lay  lay  lay  lay  lay  lay  inc   bn         bn   fc bn fc bn Layer depth Trajectory lengthCIFAR  Trajectory growth with  Batch Norm Train step Test AccuracyCIFAR  Accuracy for  trajectory and batch norm reguarlizersbatch normtraj regno batch norm or traj regOn the Expressive Power of Deep Neural Networks

vances in neural information processing systems  pages
   

Richard Stanley  Hyperplane arrangements  Enumerative

Combinatorics   

Sergey Ioffe and Christian Szegedy  Batch normalization 
Accelerating deep network training by reducing interIn Proceedings of the  nd Internal covariate shift 
national Conference on Machine Learning  ICML  
Lille  France    July   pages    

Ian    Goodfellow  Jonathon Shlens  and Christian Szegedy 
Explaining and harnessing adversarial examples  CoRR 
abs   

   Kershaw  Some extensions of    gautschi   inequalities
for the gamma function  Mathematics of Computation 
   

Andrea Laforgia and Pierpaolo Natalini  On some inequalities for the gamma function  Advances in Dynamical
Systems and Applications     

Norbert Sauer  On the density of families of sets  Journal of
Combinatorial Theory  Series       

References
Alex Krizhevsky  Ilya Sutskever  and Geoffrey   Hinton 
Imagenet classi cation with deep convolutional neural
networks  In Advances in neural information processing
systems  pages    

David Silver  Aja Huang  Chris   Maddison  Arthur Guez 
Laurent Sifre  George Van Den Driessche  Julian Schrittwieser 
Ioannis Antonoglou  Veda Panneershelvam 
Marc Lanctot  et al  Mastering the game of go with deep
neural networks and tree search  Nature   
   

Chris Piech  Jonathan Bassen  Jonathan Huang  Surya Ganguli  Mehran Sahami  Leonidas   Guibas  and Jascha
SohlDickstein  Deep knowledge tracing  In Advances in
Neural Information Processing Systems  pages  
 

Kurt Hornik  Maxwell Stinchcombe  and Halbert White 
Multilayer feedforward networks are universal approximators  Neural networks     

George Cybenko  Approximation by superpositions of  
sigmoidal function  Mathematics of control  signals and
systems     

Wolfgang Maass  Georg Schnitger  and Eduardo   Sontag    comparison of the computational power of sigmoid and Boolean threshold circuits  Springer   

Peter   Bartlett  Vitaly Maiorov  and Ron Meir  Almost linear vcdimension bounds for piecewise polynomial networks  Neural computation     

Razvan Pascanu  Guido Montufar  and Yoshua Bengio  On
the number of response regions of deep feed forward networks with piecewise linear activations  arXiv preprint
arXiv   

Guido   Montufar  Razvan Pascanu  Kyunghyun Cho  and
Yoshua Bengio  On the number of linear regions of deep
neural networks  In Advances in neural information processing systems  pages    

Ronen Eldan and Ohad Shamir  The power of depth
arXiv preprint

feedforward neural networks 

for
arXiv   

Matus Telgarsky  Representation bene ts of deep feedforward networks  arXiv preprint arXiv   
James Martens  Arkadev Chattopadhya  Toni Pitassi  and
Richard Zemel  On the representational ef ciency of restricted boltzmann machines  In Advances in Neural Information Processing Systems  pages    
Monica Bianchini and Franco Scarselli  On the complexity of neural network classi ers    comparison between
shallow and deep architectures  Neural Networks and
Learning Systems  IEEE Transactions on   
   

Ben Poole  Subhaneil Lahiri  Maithra Raghu  Jascha SohlDickstein  and Surya Ganguli  Exponential expressivity
in deep neural networks through transient chaos  In Ad 

