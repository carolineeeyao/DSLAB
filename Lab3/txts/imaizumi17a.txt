Tensor Decomposition with Smoothness

Masaaki Imaizumi   Kohei Hayashi    

Abstract

Real data tensors are typically high dimensional 
however  their intrinsic information is preserved
in lowdimensional space  which motivates the
use of tensor decompositions such as Tucker
decomposition 
Frequently  real data tensors
smooth in addition to being low dimensional 
which implies that adjacent elements are similar
or continuously changing  These elements typically appear as spatial or temporal data  We propose smoothed Tucker decomposition  STD  to
incorporate the smoothness property  STD leverages smoothness using the sum of   few basis
functions  this reduces the number of parameters 
An objective function is formulated as   convex
problem  and an algorithm based on the alternating direction method of multipliers is derived to
solve the problem  We theoretically show that 
under the smoothness assumption  STD achieves
  better error bound  The theoretical result and
performances of STD are numerically veri ed 

  Introduction
  tensor         multiway array  is   data structure that is  
generalization of   matrix  and it can represent higherorder
relationships  Tensors appear in various applications such
as image analysis  Jia et al    data mining  Kolda  
Sun    and medical analysis  Zhou et al    For
instance  functional magnetic resonance imaging  fMRI 
records brain activities in each time period as voxels  which
are represented as  way tensors  Xaxis   Yaxis   Zaxis
  time  Frequently  data tensors in the real world contain several missing elements and or are corrupted by noise 
which leads to the tensor completion problem for predicting missing elements and the tensor recovery problem for
removing noise 

 Institute of Statistical Mathematics  National Institute of Advanced Industrial Science and Technology  RIKEN  Correspondence to  Masaaki Imaizumi  insou hotmail com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

To solve these problems  the lowrank assumption      
given tensor is generated from   small number of latent
factors  is widely used 
If the number of observed elements is suf ciently larger than the number of latent factors       rank  and noise level  we can estimate latent factors and reconstruct the entire structure  The methods of
estimating latent factors are collectively referred to as tensor decompositions  There are several formulations of tensor decompositions such as Tucker decomposition  Tucker 
  and the CANDECOMP PARAFAC CP  decomposition  Harshman    While these methods were originally formulated as nonconvex problems  several authors
have studied their convex relaxations in recent years  Liu
et al    Tomioka et al    Signoretto et al   
Gandy et al   
Another important  yet less explored  assumption is the
smoothness property 
Consider fMRI data as   tensor    As fMRI data are spatiotemporal  each element of   is expected to be similar to its adjacent elements with every way       xi       should be close to
xi        xi        xi        and xi        In statistics 
this kind of smoothness property has been studied through
functional data analysis  Ramsay    Hsing   Eubank 
  Studies show that the smoothness assumption increases sample ef ciency       estimation is more accurate with small sample size  Another advantage is that
the smoothness assumption makes interpolation possible 
     we can impute an unobserved value using its adjacent
observed values  This interpolation ability is particularly
useful for solving   speci   tensor completion problem referred to as the tensor interpolation problem  as known as
the  coldstart  problem  Gantner et al    Suppose
  case in which fMRI tensor   is completely missing at
       In this case  standard tensor decompositions cannot
predict missing elements because there is no information
to estimate the latent factor at        However  using the
smoothness property  we can estimate the missing elements
from the elements at            and           
  fundamental challenge of tensor completion and recovery methods is to analyze their performance  Tomioka et al 
  extensively studied the statistical performance of
lowrank tensor decompositions  On the contrary  the performance of tensor decompositions incorporating smoothness  Yokota et al        Amini et al    has never

Tensor Decomposition with Smoothness

been addressed  The most important barrier is that all methods are formulated as nonconvex problems  which hinders
the use of the tools developed in the convex tensor decompositions  Liu et al    Signoretto et al    Tomioka
et al   

Contributions
In this paper  we propose   simple tensor
decomposition model incorporating the smoothness property  which we refer to as Smoothed Tucker Decomposition
 STD  Following the notions of functional data analysis 
STD approximates an observed tensor by   small number
of basis functions  such as Fourier series  and decomposes
them through Tucker decomposition  Figure   STD is formulated as   convex optimization problem that regularizes
the ranks and degree of smoothness  To solve this problem  we derive an algorithm based on the alternating direction method of multipliers  ADMM  which always  nds
the global optimum 
Based on the convex formulation  we provide   few theoretical guarantees of STD  namely  we derive error bounds
for tensor recovery and interpolation problems  We show
that the error bounds for smooth tensors are improved and
better than those for other methods  In addition  to the best
of our knowledge  this is the  rst analysis that establishes
an error bound for tensor interpolation  These results are
empirically con rmed through experiments using synthetic
and real data 
To summarize  STD has the following advantages 

less sample size 

  Sample ef ciency  STD achieves the same error with
  Interplation ability  STD can solve the tensor interpo 
  Convex formulation  STD ensures that   global solu 

lation problem 

tion is obtained 

Related Works   few authors have investigated the
smoothness property for tensor decompositions  Amini
et al    proposed   kernel method  and Yokota et al 
      developed   smooth decomposition method for
matrices and tensors using basis functions  These studies demonstrated that the smoothness assumption signi 
cantly improves the performance of tensor decompositions
for actual applications such as noisy image reconstruction  Yokota et al      However  these performance
gains were con rmed only in an empirical manner 
Several authors have addressed the tensor interpolation
problem by extending tensor decompositions  however  instead of smoothness  these methods utilize additional information such as network structures  Hu et al   
or side information  Gantner et al    Narita et al 
  Moreover  the performance of the tensor interpolation problem has never been analyzed theoretically 

Figure   Comparison of Tucker decomposition and STD  In
STD  the modewise smoothness of the observed tensor is preserved via basis functions 

  Preliminaries
Given       natural numbers             IK      let   
RI IK be the space of   Kway tensor  and      be
the Kway tensor that belongs to     For practical use  we
de ne      Qk   Ik  Each way of   tensor is referred
to as mode  Ik is the dimensionality of the kth mode for
                 For vector     Rd        denotes its jth element  Similarly         jK denotes the                iK 
th element of    The inner product in   is de ned as
hX        PI   IK
    jK        jK        jK for
           This induces the Frobenius norm        
phX  Xi  For vectors     Rd  let kZk   pZT   denote
the norm  In addition  we introduce the    norm for func 
    RI      dt for function           with
tions as kfk 
some domain             denotes   set of an  times
differentiable function on   

  Tucker Decomposition
With   set of  nite positive integers             RK  the
Tucker decomposition of   is de ned as

   

  RKXr rK  

gr rK     

         

               
rK  

 

where gr rK     is   coef cient for each rk    denotes the tensor product  and urk   RIk denotes vector
for each rk                  which are orthogonal to each
other for rk             Rk  Here  we refer to             RK 
as the Tucker rank  and   is an             RK rank tensor 
In addition  we let tensor     RR RK with
     rK   gr rK be   core tensor and matrix       
     
rK     RIk Rk is   set of the vectors for all
                 Using this notation  Tucker decomposition

             

Tensor Decomposition with Smoothness

  can be written as

                               

 
where    denotes the kmode matrix product  see Kolda
  Bader   for more details 

  Application Problems for Tensors
Let                    jK     IK
    jK   be an index set and
        Let      be the ith element of   for                 
Then  we consider the following observation model 

yi                

 

where       is an unobserved true tensor   yi  
   is
the set of observed values  and    is noise  where the mean
is zero and the variance is   We de ne an observation
vector               yn     Rn  and   noise vector    
              Rn  Additionally  we de ne   rearranging
operator        Rn via                  Using this
notation  observation model   is written as

             

 

problem of estimating    is referred to as the tensor recovery problem  When   few elements of   are missing 

When all the elements are observed           Qk Ik  the
         Qk Ik  the problem is referred to as the tensor
completion problem  Speci cally  for any mode    if there
exists an index        Ik  that   does not contain  we refer
to the problem of estimating      
jK   as
the tensor interpolation problem 
Using observation model   we provide an estimator for
the unknown true tensor    The estimator of    is obtained by solving the following optimization problem 

    jk      IK

    

 nkY                

min

 

where      is   convex subset of     and       
is   regularization term  For the regularization     the
overlapped Schatten  norm is frequently used  Liu et al 
  Tomioka et al    Signoretto et al    Gandy
et al    is is de ned as

 
 

KXk 

 
 

KXk 

RkXrk 

 rk      

kX   ks  

      
where        RIk Qk   Ik denotes the unfolding matrix
obtained by concatenating the modek  bers of   as column vectors and  rk       denotes the rkth largest eigenvalue of      This penalty term regularizes the Tucker
rank of    Negahban   Wainwright    Tomioka et al 
 

To solve the problem   using the Schatten regularization  ADMM is frequently employed  Boyd et al   
Tomioka et al    ADMM generates   sequence of
variables and Lagrangian multipliers by iteratively minimizing the augmented Lagrangian function 
It is known
that ADMM can easily solve an optimization problem with
  nondifferentiable regularization term such as    
  STD  Smoothed Tucker Decomposition
  Smoothness on Tensors
Before explaining the proposed approach  we introduce the
notion of smoothness on tensors  We start with the idea that
  data tensor is obtained as   result of the discretization of
  multivariate function  For example  consider an observation model of the wind power on   land surface  Suppose
that the land surface is described by   plain           longitude and latitude  and the observation model is given by
  function              Assume that we have in 
nite memory space so that we can record the wind power
             for any points            In such an unrealistic case  it is possible to handle the entire information
about    However  only  nite memory space is available 
we resort to retain  nite observations     ai  bi  
   If the
points  ai  bi  are considered as   grid  the observations can
be considered as   matrix 
This idea is generalized to tensors as follows  Consider  
Kvariate function fX             and   set of points
            jK           Ik
  jK   as grid points in      
Then  each element of   is represented as

     jK   fX gj          gjK  

 

for jk             with each                 
As the smoothness of the function  we assume that fX is
differentiable with respect to all   arguments  which allows for the expansion of the basis function to   few useful basis functions  for example  Tsybakov   and the
decomposition of multivariate functions by the basis  see
Hackbusch   for detail  Let    
             
be   set of orthonormal basis functions  such as Fourier series or wavelet series  and  wm mK       mK be
  set of coef cients  Because of the differentiability  fX is
written as the weighted sum of the basis functions as

fX  

 Xm 

 

 XmK  

wm mK  

        
mK  

 

Combining   and   yields   formulation of the elements
of the smooth tensor as

     jK

 

 Xm 

 

 XmK  

 

wm mK  

  gj     

mK  gjK  

Tensor Decomposition with Smoothness

Hereafter  we say that   is smooth if it follows  

  Objective Function
Model   is not directly applicable because it requires an
in nite number of basis functions  To prevent this  we consider their truncation  Let           be the basis functions
for mode    which represents   degree of smoothness of  
in terms of mode    For example  when       is large  such
as         Ik for all    the basis function formulation can
represent any    which implies that it neglects the smooth
structure  Then  we consider   such that it satis es the
following relation 

     jK

 

   Xm 

 

     XmK  

 

wm mK  

  gj     

mK  gjK  

For practical use  let WX   RM        be   coef 
cient tensor that satis es

 WX   mK   wm mK  

with given   
Using the representation  we propose an objective function
of STD  Based on the same convex optimization approach
as   we de ne the objective function as

min

  are employed 

    
  
 nkY               WX        WX 
 
where            are regularization coef cients that depend on   and            as       Here  regularization
terms  WX   and  WX 
There are three primary advantages of the formulation
given by   Firstly    is written as   convex optimization problem  Thus  it is ensured that to obtain the global
solution of WX will be obtained  Secondly  regularization
term  WX   determines the Tucker rank of WX appropriately  Even though we must select     this is considerably
easier than selecting the values of    Thirdly  the regularization  WX 
  penalizes the smoothness of fX  Note that
the smoothness of fX is related to       and we introduce
 WX 
  Algorithm
To optimize   we  rst reformulate it through vectorization and matricization  We de ne     RQk Ik as the vectorized tensor of    and   is aQk Ik     matrix  which is the
 ne        Qk         and Zk   RMk       is the
modek unfolding matrix of WX  Let     RQk       be the

matricized version of the rearranging operator    We de 

  to select an appropriate degree of smoothness 

vectorized tensor of WX  Let   RQk           
RI IK be an operator that converts WX to   as given
by   using    gj      Let   be aQk Ik  Qk      

matrix that satis es          As     is   linear mapping by   the existence of   is ensured  Then    is
rewritten as follows 

    Zk  

min

 
 nkY   Qxk   

KXk 
            Pk      Zk  

  
 

  

kZkks    nkwk 
 

where Pk   RQk         RM          is   rearranging
operator from the vector to the unfolding matrix  Note that
       kwkF holds by the de nition of   
We use the ADMM approach to solve   Maximizing
the augmented Lagrangian function for   we obtain the
following iteration steps  Here      is   step size and

   and     RQk Ik are Lagrangian
     RQk       
     be an
multipliers  Let       Zk  
initial point  The ADMM step at the  th iteration is written
as follows 

     

      nI    KI       

    QT         QT              
                
   KXk    

      

     

 

             

  
    prox   Pk       
  
 
     
     
 
                 
where prox    denotes the shrinkage operation of
the singular values  which is de ned as prox     
  max                where       and   are obtained
through the singular value decomposition as       SV    
Note that the ADMM steps for Zk and    are required for
every                  For   Tomioka et al    suggest

setting      pVar yi  with some constant   As the

regularization terms are convex  the sequence of the variables of ADMM is ensured to converge to the optimal solution of    Gandy et al    Theorem  

  Practical Issues
STD has several hyperparameters                   
and        and    can be determined through cross
validation         is initialized as   large value and reduced during the algorithm depending on     As      
does not exceed Ik because of an identi cation reason  the
initial value of       is bounded  Practically        is considerably less than Ik  Thus  we can start the iteration with
small values 

Tensor Decomposition with Smoothness

One can criticize that   few data tensors are smooth with
one mode  but not with others  We emphasize that STD
can address such   situation by controlling       for each   
As STD can represent tensors without the smooth structure
when         Ik  setting         Ik for some mode  
and         Ik for other modes is suf cient to address the
situation 
In this study  selecting the form of the basis functions
     is not our primary interest because it does not
speci cally affect the theoretical result  However  there
are   few typical choices  For instance  when the data tensor is periodic  such as audio  the Fourier basis is appropriate  Even through other functions  such as wavelet or
spline functions  provide the theoretical guarantees of approximating   

KPK

  Theoretical Analysis
Let
We introduce   few notations for convenience 
   maxr         be   norm of   tensor 
        
which is necessary for evaluating the penalty parameters 
Let    be an adjoint operator of    namely  hX         
hz        holds for all            For theoretical requirement  we let the basis functions                be
uniformly bounded for all       All proofs of this section
are provided in the supplemental material 

  Error Bound with   
First  we impose the following assumption on   
Assumption    Restricted Strong Convexity  RSC  condition     nite constant CX     depending on  Ik   exists 
then the rearranging operator   satis es

 
 nkX        CX   
   

for all      
Intuitively  this assumption requires that   is suf ciently
sensitive to the perturbation of      similar type of condition has been used in previous studies on sparse regression 
such as LASSO  Bickel et al    Raskutti et al   
     the restricted isometry condition  The RSC condition is
weaker than the isometry condition because the RSC condition requires only the lower bound 
We provide the following lemma regarding the error bound
when true tensor    can be neither smooth nor lowrank 
Let  RW
Lemma   Consider        and the rearranging
operator   that satis es the RSC condition 
Suppose
there exist sequences         and    that satisfy     
              and             with   constant
    

    be the Tucker rank of WX 

            RW

      Then  with some constants              we have

             max    II  III   

where    II  and III are 

   

   

 

 

II      
III      

 
 

   

KXk qRW
   WX    
KXk  Xr RW
    mk            mk  

 

 

respectively      mk is the coef cient of   

 
 

 

Lemma   states that the estimation error of    is bounded
by three types of values  where     indicates the error resulting from estimating   tensor that is smooth and lowrank   II  indicates the error resulting from introducing
the lowrank  and  III  indicates the error resulting from
approximating by the smooth tensor 
From Lemma   we see that  II  and  III  disappear and
    remains when    is low rank and smooth  which we
show in the next proposition  Here  we de ne       as
the set of the tensors represented by   with         and
the coef cient tensor WX with its rank  RW
Proposition   Suppose the same conditions of Lemma  
hold and        is smooth and lowrank  Then  with some
constant Cf     we have

            RW

   

            

Cf   

 

KXk qRW

   

  Error Bound with fX 
One of the advantages of STD is that it can estimate
   and the smooth function fX de ned in   which
allows for the interpolation of    Here  we evaluate
the estimation error of STD with respect to the norm
       for the functional space  Let us de ne fX   
PM       
mK which is one of
the smooth function as the limit of    as Ik     for
all                  We de ne the estimator of fX  by the
following 

    mK  

        

  mK

      

        Xm mK  

 wm mK  

        
mK  

where  wm mK is an element of       Estimation error is
provided as follows 

Tensor Decomposition with Smoothness

RECOVERY
METHOD
  RX   
TOMIOKA ET AL   
  RX   
TOMIOKA   SUZUKI  
WIMALAWARNE ET AL      RX   
STD

  RW            pRW 

INTERPOLATION

   
   
   

ASSUMPTION
LOWRANK
LOWRANK
LOWRANK

LOWRANK   SMOOTH

Table   Comparison of error bounds under the lowrank and smoothness assumptions  For clarity  the special cases of the error bounds
are shown  where the shape of   is symmetric                    IK  RX   RX

   
        RW

        RX

   and RW   RW

Lemma   Suppose that the rearranging operator   that
satis es the RSC condition and the true tensor       
Then  with some constant CF     we have

sup

               fX   

CF   

 

KXk qRW

   

When        by the setting  Lemma   shows that     
estimated by STD uniformly converges to fX 

  Applications and Comparison
To discuss the result of Lemma   more precisely  we consider the following two practical settings  tensor recovery
and tensor interpolation  For each setting  we derive rigorous error bounds 

nKPK

  TENSOR RECOVERY
We consider that all the elements of   are observed  and
they are affected by noise       we set     QK
   Ik and
  is   vectorization operator  Then  by applying Lemma  
we obtain the following result 
Theorem   Suppose thatX      and the rearranging
operator   that satis es the RSC condition  and the noise is
       Gaussian  Let           be some  nite constants 
  pIk  pI    with high
By setting          
probability  we have
         
   
       
KXk qRW
   
   pIk  pI   
KPK
the second part    
KPK

KXk pIk  qI      

part
comes from the noise and

  qRW
   

Tucker rank of WX 

comes from the

Theorem  

Note

that 

 rst

the

in

 

 

 

 

  TENSOR INTERPOLATION
Lemma   shows that STD can estimate the value of fX 
for all in           and not only on the given grids
 gj          gjK         By tuning    in Lemma   we
obtain the error bound for the tensor interpolation problem 

Theorem   Suppose thatX      and the rearranging
operator   that satis es the RSC condition  and the noise
is        Gaussian  By setting           and     
  pIk pI    with high probability  we have
  pnK  PK
               fX   
     
pn    
KXk qRW
 

KXk pIk  qI      

  

sup

 

 

  Comparison to Related Studies
Several studies have derived an error bound for       
  
     in each situation  Tomioka et al    investigated the tensor decomposition problem with an overlapped Schattennorm and derived the error bound as
            
     

KXk pIk  qI      

KXk qRX

    

 

 

 

            RX

   is the Tucker of    The error bound
where  RX
  in
in Proposition   is obtained by replacing the part RX
    Tomioka   Suzuki   and Wimalawarne
  by RW
et al    introduced modi ed norms with the Schattennorm and derived other error bounds 
Table   compares the main coef cients for the convergence 
When the tensor is suf ciently smooth       RW
    RX
 
case  the bound of STD is tighter than those of the other
methods 

  Experiment
  Theoretical Validation
Firstly  we verify the theoretical bound derived in Section
  through experiments for the tensor recovery problem 
We generate data tensors by following data generating processes and investigate the relation between   mean squared
error  MSE  and other factors  We set       and prepare two different sizes                    and
      We set the Tucker rank as           and
select Rk from       for each           In addition 
we generate the core tensor and its elements are obtained

Tensor Decomposition with Smoothness

Figure   Result for tensor recovery problem     and    are  
 red circle     green triangle     blue square     purple
diamond  and    yellow star  Each dashed line is the linear
 tting to the errors 

Figure   Plotted MSEs with the tensor recovery problem against
the noise level  Each symbol shows Schatten  blue circle  STD
 red diamond  latent Schatten  purple left triangle  and matrix
completion  green triangle 

  

 pIk   

KPK

follows 

the STD rank as

using the standard normal distribution  Then  we generate vectors     
rk in the following manner and obtain   using   To make   smooth  we set     
rk as   discretized
smooth functions fk             
rk      fk   Ik   fk   
is de ned as follows                        and
           with random parameters       The
scale of noise is varied as        
To investigate the MSE          
 ne
  qRW
   
   
KPK

      we deSTD rank
 
  According
to
the theoretical result 
the upper bounds for the MSEs
for STD have   linear relation with the STD rank  see
Theorem  
Figure   shows   lot of the MSEs against the STD rank 
The results show that the MSE and STD rank have   linear
relationship for each panel and each value of the penalty
parameters  This result supports Theorem   the bound for
MSE in Theorem   varies linearly with the STD rank  In
addition  we can see that the increment in the MSEs against
the STD rank increases with the regularization parameter 
and it decreases as the size of the tensor increases  This
result is explained by the theoretical results  as the MSE is
scaled by the regularization parameter and divided by   

  Comparison with Other Convex Methods
We compare the performances of convex tensor decompositions with the tensor recovery problem  To investigate the
performance with smoothness  we generate two types of

tensors         smooth tensor and   nonsmooth tensor  The
smooth tensor  which is   discretized smooth function  is
generated using basis vectors  For the nonsmooth tensor 
we generate vectors     
rk using   multivariate normal distribution  and make   through   The scale of noise is
varied as                  
In the experiment  we compare the following four methods 
STD  Tucker decomposition with Schatten regularization 
Tucker decomposition with latent Schatten regularization 
and matrix decomposition with unfolding    where the last
three methods were proposed by Tomioka et al    For
each method  regularization parameters are selected such
that they minimize the generalization error           
with grid search in an interval    
The MSEs and their standard deviations for   replications are shown in Figure   For   small tensor size
          STD performs better when the tensor is
smooth  and the latent Schatten approach is better when the
tensor is not smooth  With the large tensor          
with the smooth structure  STD outperforms other methods  For   large tensor is nonsmooth  the advantages of
STD reduce  even though it exhibits good performance 
When the tensor is small  the optimization of STD is close
to that of Tucker decomposition  as       and Ik are similar  Thus  the performances of the methods are similar for
the small tensor  In contrast  when the tensor is large  STD
can provide   different estimator by letting         Ik 
and STD successfully reduces the MSE  This difference becomes evident when the tensor has the smooth structure 

Tensor Decomposition with Smoothness

  Analysis of Real Data
  AMINO ACID DATA
We conduct
tensor completion and interpolation using
amino acids data  Kiers    The dataset contains
amounts of tyrosine  dissolved in phosphate water  which
are measured using   spectro uorometer for each   nm interval  and the data are represented by       matrices 
We make   few elements of the dataset missing  and complete them using the Schatten method  Tomioka et al 
  and STD  We consider the following four missing
patterns      elementwise missing       elementwise missing       elementwise missing   and
    columnwise and rowwise missing   We employ
the trigonometric basis functions for STD  The hyperparameters are determined through the crossvalidation 
Figure   shows the result  For random missing cases    
    and     the Schatten method and STD can complete
the missing elements  In contrast  when rows and columns
are completely missing     only STD can interpolate missing values and achieve the data 

Observed

Schatten

STD

Figure   Completion of missing elements in amino acid data 

  HUMAN ACTIVITY VIDEO
We conduct an experiment with   human activity video
dataset  Schuldt et al    It contains the human running action  it has   resolution of       pixels and
an average length for   seconds length for average with  
frame rate of   fps  First  we downscale the resolution of
each frame to onefourth so that             tensor
is obtained  We make   few pixels   or   of the
video at         missing  and complete them from other
frames  The experiments are conducted using matrix completion  the Schatten method  Tomioka et al    and
STD  For STD  we set the basis functions as trigonometric
series and            
Figure   shows the results of the experiments  We observe
that matrix completion and the Schatten method work well
when the   missing case  However  they recover nothing
when   pixels are missing  On the contrary  STD successfully recovers the background  In addition  the man  

body and shadow are interpolated at the correct position
at    even though they are blurred  Note that the completion result for STD for   missing pixels contains blocknoise  This is possibly because of over tting by the basis
functions 

Observed frame

Target frame

Observed frame

      

   

      

Completed results   missing 

Matrix

Schatten

STD

Completed results   missing 

Matrix

Schatten

STD

Figure   Completion of missing pixels in the human activity
video 

  Discussion
The smoothness we focus on is closely related to the studies of matrix completion  When   tensor is expanded by
the basis functions that are close to independent with each
other  this implies the tensor satis es the incoherence property  which is frequently used as requirement for the matrix completion problem  Candes   Plan    Candes  
Recht    Using the similarity  we may apply the formulation of STD to the tensor completion problem and analyze some properties such as the sample complexity 
Note that the focus of this study is primary on the theoretical aspect  which provides scope for addressing more
practical requirements  First  the ADMM algorithm is not
scalable when the size of the tensor is large  The primary computational burden is caused by   matrix of size

Qk Ik  Qk       which is essential for the convex for 

mulation  We may use   reduction technique as proposed
by Cheng et al    this is an important challenge for
future work  Second  the assumed smoothness       differentiability  can be extremely general for   few actual
applications  For example  images possibly contain solid
edges  such as the boundaries of objects  which do not
   the smoothness assumption  Exploring more domainspeci   smoothness is an open problem 

Tensor Decomposition with Smoothness

References
Amini  Arash    Levina  Elizaveta  and Shedden  Kerby   
Structured functional
for highdimensional spatial spectroscopy data  arXiv preprint
arXiv   

regression models

Bickel  Peter    Ritov  Ya acov  and Tsybakov  Alexandre    Simultaneous analysis of lasso and dantzig selector  The Annals of Statistics  pp     

Boyd  Stephen  Parikh  Neal  Chu  Eric  Peleato  Borja  and
Eckstein  Jonathan  Distributed optimization and statistical learning via the alternating direction method of multipliers  Foundations and Trends    in Machine Learning     

Candes  Emmanuel and Recht  Benjamin  Exact matrix
completion via convex optimization  Communications
of the ACM     

Candes  Emmanuel   and Plan  Yaniv  Matrix completion
with noise  Proceedings of the IEEE   
 

Cheng  Hao  Yu  Yaoliang  Zhang  Xinhua  Xing  Eric  and
Schuurmans  Dale  Scalable and sound lowrank tensor learning 
In Proceedings of the  th International
Conference on Arti cial Intelligence and Statistics  pp 
   

Gandy  Silvia  Recht  Benjamin  and Yamada  Isao  Tensor
completion and lown rank tensor recovery via convex
optimization  Inverse Problems     

Gantner  Zeno  Drumond  Lucas  Freudenthaler  Christoph 
Rendle  Steffen  and SchmidtThieme  Lars  Learning
attributeto feature mappings for coldstart recommendations  In   IEEE International Conference on Data
Mining  pp    IEEE   

Hackbusch  Wolfgang  Tensor spaces and numerical tensor calculus  volume   Springer Science   Business
Media   

Harshman  Richard    Foundations of the parafac procedure  Models and conditions for an  explanatory  multimodal factor analysis  UCLA Working Papers in Phonetics     

Hsing  Tailen and Eubank  Randall  Theoretical foundations of functional data analysis  with an introduction to
linear operators  John Wiley   Sons   

Hu  Changwei  Rai  Piyush  and Carin  Lawrence  Zerotruncated poisson tensor factorization for massive binary
tensors  arXiv preprint arXiv   

Jia  Chengcheng  Zhong  Guoqiang  and Fu  Yun  Lowrank tensor learning with discriminant analysis for action classi cation and image recovery  In Proceedings
of the TwentyEighth AAAI Conference on Arti cial Intelligence  pp    AAAI Press   

Kiers  Henk AL    threestep algorithm for candecomp parafac analysis of large data sets with multicollinearity  Journal of Chemometrics   
 

Kolda  Tamara   and Bader  Brett    Tensor decompositions and applications  SIAM review   
 

Kolda  Tamara   and Sun  Jimeng  Scalable tensor decompositions for multiaspect data mining  In   Eighth
IEEE international conference on data mining  pp   
  IEEE   

Liu  Ji  Musialski  Przemyslaw  Wonka  Peter  and Ye 
Jieping  Tensor completion for estimating missing values in visual data  In ICCV   

Narita  Atsuhiro  Hayashi  Kohei  Tomioka  Ryota  and
Kashima  Hisashi  Tensor factorization using auxiliary
information  In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases  pp 
  Springer   

Negahban  Sahand and Wainwright  Martin    Estimation of  near  lowrank matrices with noise and highdimensional scaling  The Annals of Statistics   
   

Ramsay  James    Functional data analysis  Wiley Online

Library   

Raskutti  Garvesh  Wainwright  Martin    and Yu  Bin  Restricted eigenvalue properties for correlated gaussian designs  Journal of Machine Learning Research   Aug 
   

Schuldt  Christian  Laptev  Ivan  and Caputo  Barbara  Recognizing human actions    local svm approach  In Pattern Recognition    ICPR   Proceedings of the
 th International Conference on  volume   pp   
IEEE   

Signoretto     Van de Plas     De Moor     and Suykens 
         Tensor Versus Matrix Completion    Comparison With Application to Spectral Data  Signal Processing Letters  IEEE     

Signoretto  Marco  De Lathauwer  Lieven  and Suykens 
Johan AK  Nuclear norms for tensors and their use for
convex multilinear estimation  Submitted to Linear Algebra and Its Applications     

Tensor Decomposition with Smoothness

Tomioka  Ryota and Suzuki  Taiji  Convex tensor decomposition via structured schatten norm regularization  In
Advances in neural information processing systems  pp 
   

Tomioka  Ryota  Hayashi  Kohei  and Kashima  Hisashi 
Estimation of lowrank tensors via convex optimization 
arXiv preprint arXiv   

Tomioka  Ryota  Suzuki  Taiji  Hayashi  Kohei  and
Kashima  Hisashi  Statistical performance of convex tensor decomposition  In Advances in Neural Information
Processing Systems  pp     

Tsybakov  Alexandre   

ric Estimation 
corporated   st edition   
 

Springer Publishing Company 

Introduction to NonparametIn 
ISBN  

Tucker  Ledyard    Some mathematical notes on threemode factor analysis  Psychometrika   
 

Wimalawarne  Kishan  Sugiyama  Masashi  and Tomioka 
Ryota  Multitask learning meets tensor factorization 
task imputation via convex optimization  In Advances in
neural information processing systems  pp   
 

Yokota  Tatsuya  Zdunek  Rafal  Cichocki  Andrzej  and
Yamashita  Yukihiko  Smooth nonnegative matrix and
tensor factorizations for robust multiway data analysis 
Signal Processing       

Yokota  Tatsuya  Zhao  Qibin  and Cichocki  Andrzej 
Smooth parafac decomposition for tensor completion 
arXiv preprint arXiv     

Zhou  Hua  Li  Lexin  and Zhu  Hongtu  Tensor regression
with applications in neuroimaging data analysis  Journal
of the American Statistical Association   
   

