Dissipativity Theory for Nesterov   Accelerated Method

Bin Hu   Laurent Lessard  

Abstract

In this paper  we adapt the control theoretic concept of dissipativity theory to provide   natural
understanding of Nesterov   accelerated method 
Our theory ties rigorous convergence rate analysis to the physically intuitive notion of energy
dissipation  Moreover  dissipativity allows one to
ef ciently construct Lyapunov functions  either
numerically or analytically  by solving   small
semide nite program  Using novel supply rate
functions  we show how to recover known rate
bounds for Nesterov   method and we generalize
the approach to certify both linear and sublinear
rates in   variety of settings  Finally  we link the
continuoustime version of dissipativity to recent
works on algorithm analysis that use discretizations of ordinary differential equations 

  Introduction
Nesterov   accelerated method  Nesterov    has garnered attention and interest in the machine learning community because of its fast global convergence rate guarantees  The original convergence rate proofs of Nesterov  
accelerated method are derived using the method of estimate sequences  which has proven dif cult to interpret 
This observation motivated   sequence of recent works
on new analysis and interpretations of Nesterov   accelerated method  Bubeck et al    Lessard et al    Su
et al    Drusvyatskiy et al    Flammarion   Bach 
  Wibisono et al    Wilson et al   
Many of these recent papers rely on Lyapunovbased stability arguments  Lyapunov theory is an analogue to the
principle of minimum energy and brings   physical intuition to convergence behaviors  When applying such proof
techniques  one must construct   Lyapunov function  which
is   nonnegative function of the algorithm   state  an  internal energy  that decreases along all admissible trajec 

 University of Wisconsin Madison  Madison  WI  

USA  Correspondence to  Bin Hu  bhu wisc edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

tories  Once   Lyapunov function is found  one can relate
the rate of decrease of this internal energy to the rate of
convergence of the algorithm  The main challenge in applying Lyapunov   method is  nding   suitable Lyapunov
function 
There are two main approaches for Lyapunov function constructions  The  rst approach adopts the integral quadratic
constraint  IQC  framework  Megretski   Rantzer   
from control theory and formulates   linear matrix equality  LMI  whose feasibility implies the linear convergence
of the algorithm  Lessard et al    Despite the generality of the IQC approach and the small size of the associated LMI  one must typically resort to numerical simulations to solve the LMI  The second approach seeks an
ordinary differential equation  ODE  that can be appropriately discretized to yield the algorithm of interest  One can
then gain intuition about the trajectories of the algorithm
by examining trajectories of the continuoustime ODE  Su
et al    Wibisono et al    Wilson et al   
The work of Wilson et al    also establishes   general equivalence between Lyapunov functions and estimate
sequence proofs 
In this paper  we bridge the IQC approach  Lessard et al 
  and the discretization approach  Wilson et al   
by using dissipativity theory  Willems        The term
 dissipativity  is borrowed from the notion of energy dissipation in physics and the theory provides   general approach for the intuitive understanding and construction
of Lyapunov functions  Dissipativity for quadratic Lyapunov functions in particular  Willems      has seen
widespread use in controls  In the sequel  we tailor dissipativity theory to the automated construction of Lyapunov
functions  which are not necessarily quadratic  for the analysis of optimization algorithms  Our dissipation inequality
leads to an LMI condition that is simpler than the one in
Lessard et al    and hence more amenable to being
solved analytically  When specialized to Nesterov   accelerated method  our LMI recovers the Lyapunov function
proposed in Wilson et al    Finally  we extend our
LMIbased approach to the sublinear convergence analysis of Nesterov   accelerated method in both discrete and
continuous time domains  This complements the original
LMIbased approach in Lessard et al    which mainly
handles linear convergence rate analyses 

Dissipativity Theory for Nesterov   Accelerated Method

An LMIbased approach for sublinear rate analysis similar to ours was independently and simultaneously proposed
by Fazlyab et al    While this work and the present
work both draw connections to the continuoustime results
mentioned above  different algorithms and function classes
are emphasized  For example  Fazlyab et al    develops LMIs for gradient descent and proximal projectionbased variants with convex quasiconvex objective functions 
In contrast  the present work develops LMIs tailored to the analysis of discretetime accelerated methods
and Nesterov   method in particular 

  Preliminaries
  Notation
Let   and    denote the real and nonnegative real numbers  respectively  Let Ip and    denote the       identity
and zero matrices  respectively  The Kronecker product of
two matrices is denoted       and satis es the properties
         AT BT and              AC BD 
when the matrices have compatible dimensions  Matrix
inequalities hold in the semide nite sense unless otherwise indicated    differentiable function     Rp    
is mstrongly convex if                            
   cid       cid  for all        Rp and is Lsmooth if
      
 cid           cid      cid     cid  for all        Rp  Note that
  is convex if   is  strongly convex  We use   cid  to denote
  point satisfying       cid      When   is Lsmooth and
mstrongly convex    cid  is unique 

  Classical Dissipativity Theory

Consider   linear dynamical system governed by the statespace model

            Bwk 

 
Here       Rn  is the state  wk   Rnw is the input     
Rn    is the state transition matrix  and     Rn nw
is the input matrix  The input wk can be physically interpreted as   driving force  Classical dissipativity theory
describes how the internal energy stored in the state   
evolves with time   as one applies the input wk to drive
the system    key concept in dissipativity theory is the
supply rate  which characterizes the energy change in   
due to the driving force wk  The supply rate is   function
    Rn    Rnw     that maps any state input pair     
to   scalar measuring the amount of energy delivered from
  to state   Now we introduce the notion of dissipativity 

De nition   The dynamical system   is dissipative with
respect to the supply rate   if there exists   function    
Rn       such that         for all     Rn  and

                     wk 

 

for all    The function   is called   storage function  which
quanti es the energy stored in the state   In addition   
is called the dissipation inequality 

The dissipation inequality   states that the change of the
internal energy stored in    is equal to the difference between the supplied energy and the dissipated energy  Since
there will always be some energy dissipating from the system  the change in the stored energy  which is exactly
              is always bounded above by the energy supplied to the system  which is exactly      wk 
  variant of   known as the exponential dissipation inequality states that for some           we have
                      wk 

 
which states that at least   fraction       of the internal
energy will dissipate at every step 
The dissipation inequality   provides   direct way to construct   Lyapunov function based on the storage function 
It is often the case that we have prior knowledge about how
the driving force wk is related to the state     Thus  we may
know additional information about the supply rate function
     wk  For example  if      wk      for all   then  
directly implies that                and the storage
function   can serve as   Lyapunov function  The condition      wk      means that the driving force wk does
not inject any energy into the system and may even extract
energy out of the system  Then  the internal energy will
decrease no slower than the linear rate   and approach  
minimum value at equilibrium 
An advantage of dissipativity theory is that for any
quadratic supply rate  one can automatically construct the
dissipation inequality using semide nite programming  We
now state   standard result from the controls literature 

Theorem   Consider the following quadratic supply rate
with         nw   nw  and         

 
If there exists   matrix     Rn    with       such that

 

 

 

 

 cid 

 cid   

 cid  
 cid ATP        ATP  

       

 cid   
 cid 

BTP  

BTP  

       

 

then the dissipation inequality   holds for all trajectories
of   with        TP  

Proof  Based on the statespace model   we have

          

       

         Bwk TP        Bwk 

 cid   cid ATP   ATP  

 cid cid    

 cid    

BTP   BTP  

wk

 

wk

 cid 

 

Hence we can left and right multiply   by cid  
 cid  

 cid  and
 cid    and directly obtain the desired conclusion 

  wT
 

  wT
 

The lefthand side of   is linear in     so   is   linear
matrix inequality  LMI  for any  xed            The set
of   such that   holds is therefore   convex set and can be
ef ciently searched using interior point methods  for example  To apply the dissipativity theory for linear convergence
rate analysis  one typically follows two steps 

  Choose   proper quadratic supply rate function   satisfying certain desired properties            wk     
  Solve the LMI   to obtain   storage function    
which is then used to construct   Lyapunov function 
In step   the LMI obtained is typically very small        
or       so we can often solve the LMI analytically  For
illustrative purposes  we rephrase the existing LMI analysis
of the gradient descent method  Lessard et al     
using the notion of dissipativity 

  Example  Dissipativity for Gradient Descent

There is an intrinsic connection between dissipativity theory and the IQC approach  Megretski et al    Seiler 
  The IQC analysis of the gradient descent method
in Lessard et al    may be reframed using dissipativity theory  Then  the pointwise IQC  Lessard et al   
Lemma   amounts to using   quadratic supply rate   with
      Speci cally  assume   is Lsmooth and mstrongly
convex  and consider the gradient descent method

xk    xk       xk 

 
We have xk    cid    xk    cid      xk  where   cid  is the
unique point satisfying       cid      De ne      xk     cid 
and wk       xk  Then the gradient descent method
is modeled by   with     Ip and      Ip  Since
wk             cid  we can de ne the following quadratic
supply rate

 cid cid    

 cid 

 cid   cid 

 cid    

wk

     wk   

 mLIp

       Ip

       Ip

 Ip

wk

 
By cocoercivity  we have      wk      for all    This
just restates Lessard et al    Lemma   Then  we can
directly apply Theorem   to construct the dissipation inequality  We can parameterize         Ip and de ne the
storage function as           cid   cid      cid xk     cid cid  The
LMI   becomes

 cid cid         
 cid 

 cid mL      
 cid cid 

  

 

  

     

 

  Ip    

Dissipativity Theory for Nesterov   Accelerated Method

Hence for any           we have   cid xk      cid cid   
   cid xk     cid cid  if there exists       such that

 cid         
 cid 

 cid mL      
 cid 

  

 

  

     

 

   

 

 

       

         

The LMI   is simple and can be analytically solved to
recover the existing rate results for the gradient descent
For example  we can choose        to be
method 
         
       or  
          to immedi 
   
ately recover the standard rate results in Polyak  
Based on the example above  it is evident that choosing  
proper supply rate is critical for the construction of   Lyapunov function  The supply rate   turns out to be inadequate for the analysis of Nesterov   accelerated method 
For Nesterov   accelerated method  the dependence between the internal energy and the driving force is more
complicated due to the presence of momentum terms  We
will next develop   new supply rate that captures this complicated dependence  We will also make use of this new
supply rate to recover the standard linear rate results for
Nesterov   accelerated method 

  Dissipativity for Accelerated Linear Rates
  Dissipativity for Nesterov   Method

Suppose   is Lsmooth and mstrongly convex with    
  Let   cid  be the unique point satisfying       cid      Now
we consider Nesterov   accelerated method  which uses the
following iteration rule to  nd   cid 

xk    yk       yk 

yk        xk    xk 

   
   

We can rewrite   as

 cid 

 cid xk      cid 

xk     cid 

 cid 

 cid  xk     cid 

xk      cid 

   

  Bwk

 

where wk       yk            xk    xk  Also 
         Ip           Ip  and        are de ned by

 cid       
  with     cid xk     cid  

    

 

 

 

    

 cid 
 cid 
 cid 
 xk      cid   cid   

 

Hence  Nesterov   accelerated method   is in the form of

 

 

Nesterov   accelerated method can improve the convergence rate since the input wk depends on both xk and
xk  and drives the state in   speci   direction       along
 xk xk  This leads to   supply rate that extracts
energy out of the system signi cantly faster than with gradient descent  This is formally stated in the next lemma 

Dissipativity Theory for Nesterov   Accelerated Method

Lemma   Let   be Lsmooth and mstrongly convex with
      Let   cid  be the unique point satisfying       cid     
Consider Nesterov   method   or equivalently   The
following inequalities hold for all trajectories 

  xk     cid 
  xk     cid 

xk      cid 
    yk 

xk      cid 
    yk 

  

  

xk      cid 
    yk 

xk      cid 
    yk 

       xk       xk 
         cid       xk 

  xk     cid 
  
  xk     cid 
  
       
                     

     
 
 

      

 
 

 

  

 

 

   

      
     

      

 

where Xi    Xi   Ip for         and  Xi are de ned by

     

     

 
 

 
 

Given any           one can de ne the supply rate as
  with   particular choice of                  
Then this supply rate satis es the condition

     wk        xk         cid 

      xk         cid 

 

Proof  The proof is similar to the proof of  
in Bubeck   but Bubeck   Lemma   must be
modi ed to account for the strong convexity of    See the
supplementary material for   detailed proof 

The supply rate   captures how the driving force wk is
impacting the future state xk  The physical interpretation is that there is some amount of hidden energy in the
system that takes the form of    xk         cid  The supply
rate condition   describes how the driving force wk is
coupled with the hidden energy in the future  It says the
delivered energy is bounded by   weighted decrease of the
hidden energy  Based on this supply rate  one can search
Lyapunov function using the following theorem 

Theorem   Let   be Lsmooth and mstrongly convex
with       Let   cid  be the unique point satisfying
      cid      Consider Nesterov   accelerated method  
For any rate           set                where
    and     are de ned in   In addition  let       
be de ned by   If there exists   matrix         with
       such that

 cid   AT             

 BT      

 cid 

 AT      
 BT      

        

 

then set          Ip and de ne the Lyapunov function

 cid  xk     cid 

 cid  

 cid  xk     cid 

 cid 

 

Vk  

xk      cid 

xk      cid 

     xk         cid 
 
which satis es Vk     Vk for all    Moreover  we have
   xk         cid     kV  for Nesterov   method 

Proof  Take the Kronecker product of   and Ip  and
hence   holds with          Ip           Ip  and
       Ip  Let the supply rate   be de ned by   Then 
de ne the quadratic storage function           
      
and apply Theorem   to show                 
     wk  Based on the supply rate condition   we can
de ne the Lyapunov function Vk           xk      cid 
and show Vk     Vk  Finally  since       we have
   xk         cid     kV 

We can immediately recover the proposed Lyapunov function in Wilson et al    Theorem   by setting    to

 cid   
   cid   

 

 cid 

 

 

which is clearly negative semide nite  Hence we can
immediately construct   Lyapunov function using   to

Searching for analytic certi cates such as   can either
be carried out by directly analyzing the LMI  or by using
numerical solutions to guide the search  For example  numerically solving   for any  xed   and   directly yields
  which makes  nding the analytical expression easy 

  Dissipativity Theory for More General Methods

We demonstrate the generality of the dissipativity theory
on   more general variant of Nesterov   method  Consider
  modi ed accelerated method

xk         xk    xk        yk 

yk        xk    xk 

   
   

When       we recover Nesterov   accelerated method 
When       we recover the Heavyball method of Polyak

Clearly        Now de ne      
   
verify that the left side of the LMI   is equal to

   Given      
  
    it is straightforward to

 
 

 

 

    

 cid cid   
 
 cid   
 cid   
   cid   
    and        cid   
 
prove the linear rate        cid   

 
  
   

     
 

 

   

 
   
 
 

   

 
 
 

Dissipativity Theory for Nesterov   Accelerated Method

  We can rewrite   in statespace form   where
wk       yk            xk    xk          Ip 
         Ip  and        are de ned by

 cid       

 cid 

 

 

    

 

    

 cid 

 cid 

 

 

Lemma   Let   be Lsmooth and mstrongly convex with
      Let   cid  be the unique point satisfying       cid     
Consider the general accelerated method   De ne the
wk       yk            xk    xk  Then the
following inequalities hold for all trajectories 

 xk      cid   cid   and the input
 cid 
 cid 

state       cid xk     cid  
 cid    
 cid    
 cid    
 cid    

     xk       xk 

 cid  
 cid  

        

 

wk

wk

       cid       xk 

        

 

wk

wk

with Xi    Xi   Ip for           and  Xi are de ned by

  

  
   

      

      
      
      

     
         
                     

     
 
 

      

 

 
 

  

 
 

      
     

 
 

 

     

     

     

 
 

 
 

 
 

with           In addition  one can de ne the supply
rate as   with                        Then for
all trajectories     wk  of the general accelerated method
  this supply rate satis es the inequality

     wk        xk         cid 

      xk         cid 

 

Proof    detailed proof is presented in the supplementary
material  One mainly needs to modify the proof by taking
the difference between   and   into accounts 

rate        cid   

Based the supply rate   we can immediately modify
Theorem   to handle the more general algorithm  
Although we do not have general analytical formulas for
the convergence rate of   preliminary numerical results
suggest that there are   family of       leading to the
    and the required value of    is quite
different from   This indicates that our proposed LMI
approach could go beyond the Lyapunov function  
Remark   It is noted in Lessard et al      that
searching over combinations of multiple IQCs may yield

improved rate bounds  The same is true of supply rates 
For example  we could include         as decision variables and search for   dissipation inequality with supply
rate           where         is   and    is  

  Dissipativity for Sublinear Rates
The LMI approach in  Lessard et al    is tailored for
the analysis of linear convergence rates for algorithms that
are timeinvariant  the   and   matrices in   do not
change with    We now show that dissipativity theory can
be used to analyze the sublinear rates      and     
via slight modi cations of the dissipation inequality 

  Dissipativity for      rates

The      modi cation  which we present  rst  is very
similar to the linear rate result 

Theorem   Suppose   has    nite minimum   cid  Consider
the LTI system   with   supply rate satisfying

     wk        zk      cid 

 

for some sequence  zk 
If there exists   nonnegative
storage function   such that the dissipation inequality  
holds over all trajectories of     wk  then the following
inequality holds over all trajectories as well 

  cid 

    zk      cid       

 

 

  

In addition  we have the sublinear convergence rate

min
     

    zk      cid       
     

 

If    zk       zk  for all    then   implies that
   zk      cid       
Proof  By the supply rate condition   and the dissipation
inequality   we immediately get

   for all   

                   zk      cid     

Summing the above inequality from       to   and using
      yields the desired result 

To address the sublinear rate analysis  the critical step is
to choose an appropriate supply rate 
If   is Lsmooth
and convex  this is easily done  Consider the gradient
method   and de ne the quantities      xk     cid 
    Ip  and      Ip as in Section   Since   is
Lsmooth and convex  de ne the quadratic supply rate

 cid    

 cid   cid    

wk

   

  Ip

 cid cid    

 cid 

 

wk

   
  Ip
 
   Ip

     wk   

Dissipativity Theory for Nesterov   Accelerated Method

 

 cid 
 cid        

      

which satis es      wk      cid       xk  for all    cocoercivity  Then we can directly apply the LMI   with
      to construct the dissipation inequality  Setting
        Ip and de ning the storage function as        
  cid   cid      cid xk     cid cid  the LMI   becomes

 cid cid   

  
     
which is equivalent to

 cid 

 cid 

 

 
 

 

     

  

  Ip    

      

      
        

  

 

 

Due to the     entry being zero    holds if and only if

 cid cid 
 cid 
 cid       

   

 

     
      
  and the bound   becomes

     

 

 

We can choose      

    xk      cid      cid        cid cid 

 

      

min
   

Since gradient descent has monotonically nonincreasing iterates  that is    xk       xk  for all    we immediately
recover the standard      rate result 

  Dissipativity for      rates

Certifying        rate for the gradient method required
solving   single LMI   However  this is not the case
for the      rate analysis of Nesterov   accelerated
method  Nesterov   algorithm has parameters that depend
on   so the analysis is more involved  We will begin with
the general case and then specialize to Nesterov   algorithm  Consider the dynamical system

      Ak     Bkwk

 

The state matrix Ak and input matrix Bk change with the
time step    and hence   is referred to as    linear timevarying   LTV  system  The analysis of LTV systems typically requires   timedependent supply rate such as

 cid  

 cid    

wk

 cid    

 cid 

wk

Xk

 cid 

Sk    wk   

 

 cid 

If there exists   sequence  Pk  with Pk     such that

  Pk Ak   Pk AT
AT
BT
BT

  Pk Ak

  Pk Bk
  Pk Bk

  Xk    

 

for all    then we have Vk    Vk      Sk    wk 
with the timedependent storage function de ned as
  Pk    This is   standard approach for disVk        
sipation inequality constructions of LTV systems and can

be proved using the same proof technique in Theorem  
Note that we need   to simultaneously hold for all   
This leads to an in nite number of LMIs in general 
Now we consider Nesterov   accelerated method for   convex Lsmooth objective function    Nesterov   

xk    yk         yk 

yk          xk    kxk 

   
   

It is known that   achieves   rate of      when
        and    is de ned recursively as follows 

   cid       

       

 

 

     

     
 
The sequence     satis es    
   We now
present   dissipativity theory for the sublinear rate analysis
of Nesterov   accelerated method  Rewrite   as

    
            

  

 

 cid 

 cid xk      cid 

xk     cid 

 cid 

 cid  xk     cid 

xk      cid 

  Ak

  Bkwk

 

where wk       yk              xk    kxk 
Ak    Ak   Ip  Bk    Bk   Ip  and  Ak   Bk are given by

 cid         
 cid 
of   with       cid xk     cid  

 Ak  

 

 

 

 cid  

 cid 

 

 Bk  

 

 xk      cid   cid    The

Hence  Nesterov   accelerated method   is in the form

     rate analysis of Nesterov   method   requires
the following timedependent supply rate 

Lemma   Let   be Lsmooth and convex  Let   cid  be  
point satisfying       cid      In addition  set   cid         cid 
Consider Nesterov   method   or equivalently   The
following inequalities hold for all trajectories and for all   

where Mk    Mk   Ip  Nk    Nk   Ip  and  Mk   Nk are
de ned by

xk      cid 
    yk 

xk      cid 
    yk 

  xk     cid 
  
  xk     cid 
  
   
 

 Mk  

 Nk  

  xk     cid 
  xk     cid 

xk      cid 
    yk 

xk      cid 
    yk 

Mk

Nk

 
 
 
    

 
   
    
 
 

   

         

   
    
 
    
 
  
 
 
 
    

       xk       xk 
         cid       xk 
   

   

 

 

   

         

 
    
 
  

Dissipativity Theory for Nesterov   Accelerated Method

 cid 

 cid 

Given any nondecreasing sequence     one can de ne
the supply rate as   with the particular choice Xk  
 kMk            Nk for all    Then this supply rate
satis es the condition

     wk          xk      cid 

        xk      cid 

 

Proof  The proof is very similar to the proof of Lemma  
with an extra condition         detailed proof is presented in the supplementary material 

Theorem   Consider the LTV dynamical system   If
there exist matrices  Pk  with Pk     and   nondecreasing
sequence of nonnegative scalars     such that

  Pk Ak   Pk AT
AT
BT
BT

  Pk Ak

  Pk Bk
  Pk Bk

   kMk            Nk    

 
then we have Vk      Vk      Sk    wk  with the
storage function Vk        
  Pk   and the supply rate
  using Xk    kMk            Nk for all    In
addition  if this supply rate satis es   we have

   xk      cid               cid      
 cid  and cid  

and right multiply   by cid  

Proof  Based on the statespace model   we can left

 cid    and

 

  

directly obtain the dissipation inequality  Combining this
dissipation inequality with   we can show
Vk         fk      cid    Vk         fk     cid 
Summing the above inequality as in the proof of Theorem  
and using the fact that Pk     for all   yields the result 

  wT
 

  wT
 

 cid     

 cid cid   

       

       

 cid  Note that Pk    

We are now ready to show the      rate result for
Nesterov   accelerated method  Set          and
Pk    
 
and              It is straightforward to verify that this
choice of  Pk      makes the left side of   the zero matrix and hence   holds  Using the fact that         
 easily proved by induction  we have         and the
     rate for Nesterov   method follows 

Remark   Theorem   is quite general  The in nite family of LMIs   can also be applied for linear rate analysis and collapses down to the single LMI   in that case 
To apply   to linear rate analysis  one needs to slightly
modify   such that the strong convexity parameter

  is incorporated into the formulas of Mk  Nk  By setting
        and Pk    kP   then the LMI   is the
same for all   and we recover   This illustrates how the
in nite number of LMIs   can collapse to   single LMI
under special circumstances 

  Continuoustime Dissipation Inequality
Finally  we brie   discuss dissipativity theory for the
continuoustime ODEs used in optimization research  Note
that dissipativity theory was  rst introduced in Willems
      in the context of continuoustime systems  We
denote continuoustime variables in upper case  Consider  
continuoustime statespace model

                        

 

where     is the state        is the input  and     denotes
the time derivative of     In continuoustime  the supply
rate is   function     Rn    RnW          that assigns
  scalar to each possible state and input pair  Here  we
allow   to also depend on time        To simplify our
exposition  we will omit the explicit time dependence    
from our notation 

De nition   The dynamical system   is dissipative
with respect to the supply rate   if there exists   function     Rn            such that            for
all     Rn  and       and

                  

 

   denotes the Lie
for every trajectory of   Here 
derivative  or total derivative  it accounts for    dependence on    The function   is called   storage function 
and   is    continuoustime  dissipation inequality 

For any given quadratic supply rate  one can automatically
construct the continuoustime dissipation inequality using
semide nite programs  The following result is standard in
the controls literature 
Theorem   Suppose            nW    nW   and
             for all    Consider the quadratic supply rate

          

 

for all   

 

If there exists   family of matrices         Rn    with
          such that

 cid ATP               

 cid 

BTP

 

       

for all   

 

Then we have                    with the storage function de ned as           TP  

 cid  

 cid   

 

 cid   

 cid 

 

Proof  Based on the statespace model   we can apply
the product rule for total derivatives and obtain

 cid   

           TP      TP            

 cid   cid ATP               

 cid cid   
 cid 
Hence we can left and right multiply   by cid       cid 
and cid       cid   and obtain the desired conclusion 

BTP

 

 

 

 

 

The algebraic structure of the LMI   is simpler than that
of its discretetime counterpart   because for given     the
continuoustime LMI is linear in      rather than being
quadratic  This may explain why continuoustime ODEs
are sometimes more amenable to analytic approaches than
their discretized counterparts 
We demonstrate the utility of   on the continuoustime
limit of Nesterov   accelerated method in Su et al   

 
 

                

    

which we rewrite as   with      cid             xT

              cid  is   point satisfying       cid      and
     are de ned by

 cid   

 

 cid 

 cid   

 cid 

      

        

  Ip
Ip

  
  

 cid Ip

 cid 

  

 

Suppose   is convex and set   cid         cid  Su et al   
Theorem   constructs the Lyapunov function          
       cid cid  to show that        and
          cid cid      
then directly demonstrate        rate for the ODE  
To illustrate the power of the dissipation inequality  we use
the LMI   to recover this Lyapunov function  Denote
                       cid  Note that convexity implies
           cid                   cid  which we rewrite as

 

    

  
  

 

  
  
tIp

  
tIp
  

  

      cid 

 

 

Since                          cid                   we have

              cid   

 

    

  

      cid 

 

Now choose the supply rate   as   with      given by

  

 

      cid 

 
      
    

  
  
  Ip

   

  
  
tIp

  
  Ip
tIp
  

  

      cid 

 

 
   

        

  
  
tIp

  
  Ip
tIp
  

  
  
  Ip

Dissipativity Theory for Nesterov   Accelerated Method

         cid   

 cid   cid   

 cid  Substituting   and  

Clearly                       Now we can choose

Ip

Ip

  Ip

  Ip
into   the left side of   becomes identically zero 
                                with the
Therefore 
storage function           TP   By de ning the Lyapunov function                              we immediately obtain        and also recover the same Lyapunov
function used in Su et al   

Remark   As in the discretetime case  the in nite family of LMIs   can also be reduced to   single LMI for
the linear rate analysis of continuoustime ODEs  For further discussion on the topic of continuoustime exponential
dissipation inequalities  see Hu   Seiler  

  Conclusion and Future Work
In this paper  we developed new notions of dissipativity
theory for understanding of Nesterov   accelerated method 
Our approach enjoys advantages of both the IQC framework  Lessard et al    and the discretization approach
 Wilson et al    in the sense that our proposed LMI
condition is simple enough for analytical rate analysis of
Nesterov   method and can also be easily generalized to
more complicated algorithms  Our approach also gives an
intuitive interpretation of the convergence behavior of Nesterov   method using an energy dissipation perspective 
One potential application of our dissipativity theory is for
the design of accelerated methods that are robust to gradient noise  This is similar to the algorithm design work in
Lessard et al      However  compared with the IQC
approach in Lessard et al    our dissipativity theory
leads to smaller LMIs  This can be bene cial since smaller
LMIs are generally easier to solve analytically  In addition 
the IQC approach in Lessard et al    is only applicable
to stronglyconvex objective functions while our dissipativity theory may facilitate the design of robust algorithm
for weaklyconvex objective functions  The dissipativity
framework may also lead to the design of adaptive or timevarying algorithms 

Acknowledgements
Both authors would like to thank the anonymous reviewers
for helpful suggestions that improved the clarity and quality of the  nal manuscript 
This material is based upon work supported by the National Science Foundation under Grant No    Both
authors also acknowledge support from the Wisconsin Institute for Discovery  the College of Engineering  and the
Department of Electrical and Computer Engineering at the
University of Wisconsin Madison 

Dissipativity Theory for Nesterov   Accelerated Method

Wibisono     Wilson     and Jordan       variational perspective on accelerated methods in optimization  Proceedings of the National Academy of Sciences 
pp     

Willems       Dissipative dynamical systems Part    General theory  Archive for Rational Mech  and Analysis   
     

Willems       Dissipative dynamical systems Part II  Linear
systems with quadratic supply rates  Archive for Rational Mech  and Analysis       

Wilson     Recht     and Jordan       Lyapunov analysis
of momentum methods in optimization  arXiv preprint
arXiv   

References
Bubeck     Convex optimization  Algorithms and complexity  Foundations and Trends   cid  in Machine Learning 
   

Bubeck     Lee     and Singh       geometric alternative to Nesterov   accelerated gradient descent  arXiv
preprint arXiv   

Drusvyatskiy     Fazel     and Roy     An optimal
 rst order method based on optimal quadratic averaging 
arXiv preprint arXiv   

Fazlyab     Ribeiro     Morari     and Preciado 
      Analysis of optimization algorithms via integral quadratic constraints  Nonstrongly convex problems  arXiv preprint arXiv   

Flammarion     and Bach     From averaging to acceleration  there is only   stepsize  In COLT  pp   
 

Hu     and Seiler     Exponential decay rate conditions
for uncertain linear systems using integral quadratic conIEEE Transactions on Automatic Control   
straints 
   

Lessard     Recht     and Packard     Analysis and design
of optimization algorithms via integral quadratic constraints  SIAM Journal on Optimization   
 

Megretski     and Rantzer     System analysis via integral
quadratic constraints  IEEE Transactions on Automatic
Control     

Megretski       onsson     Kao        and Rantzer 
Integral

   Control Systems Handbook  chapter  
Quadratic Constraints  CRC Press   

Nesterov    

Introductory Lectures on Convex Optimization    Basic Course  Kluwer Academic Publishers 
 

Polyak        Introduction to optimization  Optimization

Software   

Seiler     Stability analysis with dissipation inequalities and
integral quadratic constraints  IEEE Transactions on Automatic Control     

Su     Boyd     and Cand es       differential equation for
modeling Nesterov   accelerated gradient method  Theory and insights  Journal of Machine Learning Research 
   

