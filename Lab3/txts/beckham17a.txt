Unimodal Probability Distributions for Deep Ordinal Classi cation

Christopher Beckham   Christopher Pal  

Abstract

Probability distributions produced by the crossentropy loss for ordinal classi cation problems
can possess undesired properties  We propose
  straightforward technique to constrain discrete
ordinal probability distributions to be unimodal
via the use of the Poisson and binomial probability distributions  We evaluate this approach in
the context of deep learning on two large ordinal
image datasets  obtaining promising results 

  Introduction
Ordinal classi cation  sometimes called ordinal regression 
is   prediction task in which the classes to be predicted are
discrete and ordered in some fashion  This is different from
discrete classi cation in which the classes are not ordered 
and different from regression in that we typically do not
know the distances between the classes  unlike regression 
in which we know the distances because the predictions lie
on the real number line  Some examples of ordinal classi cation tasks include predicting the stages of disease for
  cancer  Gentry et al    predicting what star rating
  user gave to   movie  Koren   Sill    or predicting
the age of   person  Eidinger et al   
Two of the easiest techniques used to deal with ordinal
problems include either treating the problem as   discrete
classi cation and minimising the crossentropy loss  or
treating the problem as   regression and using the squared
error loss  The former ignores the inherent ordering between the classes  while the latter takes into account the
distances between them  due to the square in the error term 
but assumes that the labels are actually realvalued   that
is  adjacent classes are equally distant  Furthermore  the
crossentropy loss   under   onehot target encoding   is
formulated such that it only  cares  about the ground truth
class  and that probability estimates corresponding to the

 Montr eal

Institute of Learning Algorithms  Qu ebec 
Canada  Correspondence to  Christopher Beckham  christopher beckham polymtl ca 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

other classes may not necessarily make sense in context 
We present an example of this in Figure   showing three
probability distributions        and    all conditioned on
some input image  Highlighted in orange is the ground
truth      
the image is of an adult  and all probability
distributions have identical crossentropy  this is because
the loss only takes into account the ground truth class 
  log          where     adult  and all three distributions have the same probability mass for the adult class 
Despite all distributions having the same crossentropy
loss  some distributions are  better  than others  For example  between   and      is preferred  since   puts an
unusually high mass on the baby class  However    and
  are both unusual  because the probability mass does not
gradually decrease to the left and right of the ground truth 
In other words  it seems unusual to place more con dence
on  schooler  than  teen   distribution    considering that
  teenager looks more like an adult than   schooler  and
it seems unusual to place more con dence on  baby  than
 teen  considering that again    teenager looks more like an
adult than   baby  Distribution   makes the most sense because the probability mass gradually decreases as we move
further away from the most con dent class  In this paper 
we propose   simple method to enforce this constraint  utilising the probability mass function of either the Poisson or
binomial distribution 
For the remainder of this paper  we will refer to distributions like   as  unimodal  distributions  that is  distributions where the probability mass gradually decreases on
both sides of the class that has the majority of the mass 

  Related work

Our work is inspired by the recent work of Hou et al 
  who shed light on the issues associated with different probability distributions having the same crossentropy
loss for ordinal problems  In their work  they alleviate this
issue by minimising the  Earth mover   distance  which is
de ned as the minimum cost needed to transform one probability distribution to another  Because this metric takes
into account the distances between classes   moving probability mass to   faraway class incurs   large cost   the
metric is appropriate to minimise for an ordinal problem  It
turns out that in the case of an ordinal problem  the Earth

Unimodal Probability Distributions for Deep Ordinal Classi cation

    An adult woman

    Three probability distributions exhibiting the same mass for the  adult  class and
therefore the same crossentropy error 

Figure   Three ordinal probability distributions conditioned on an image of an adult woman  Distributions   and   are unusual in the
sense that they are multimodal 

 cid   

 cid   

 

emd        

mover   distance reduces down to Mallow   distance 
   cmf      cmf     

 
where cmf  denotes the cumulative mass function for
some probability distribution    denotes the ground truth
 onehot encoded     the corresponding predicted probability distribution  and   the number of classes  The authors
evaluate the EMD loss on two age estimation and one aesthetic estimation dataset and obtain stateof theart results 
However  the authors do not show comparisons between the
probability distributions learned between EMD and crossentropy 
Unimodality has been explored for ordinal neural networks
in da Costa et al    They explored the use of the binomial and Poisson distributions and   nonparametric way
of enforcing unimodal probability distributions  which we
do not explore  One key difference between their work
and ours here is that we evaluate these unimodal distributions in the context of deep learning  where the datasets are
generally much larger and have more variability  however 
there are numerous other differences which we will highlight throughout this paper 
Beckham   Pal   explored   loss function with an
intermediate form between   crossentropy and regression
loss  In their work the squared error loss is still used  but  
probability distribution over classes is still learned  This is
done by adding   regression layer         oneunit layer  at
the top of what would normally be the classi cation layer 
       Instead of learning the weight vector   it is  xed
to                  and the squared error loss is minimised 
This can be interpreted as drawing the class label from  
Gaussian distribution                             This
technique was evaluated against the diabetic retinopathy
dataset and beat most of the baselines employed 
Interestingly  since        is   Gaussian  this is also unimodal 

though it is   somewhat odd formulation as it assumes   is
continuous when it is really discrete 
Cheng   proposed the use of binary crossentropy or
squared error on an ordinal encoding scheme rather than the
onehot encoding which is commonly used in discrete classi cation  For example  if we have   classes  then we have
labels of length       where the  rst class is            
second class is             third class is               and so
forth  With this formulation  we can think of the   th output
unit as computing the cumulative probability           
where                     Frank   Hall   also
proposed this scheme but in   more general sense by using multiple classi ers  not just neural networks  to model
each cumulative probability  and Niu et al    proposed
  similar scheme using CNNs for age estimation  This
technique however suffers from the issue that the cumulative probabilities                                      
are not guaranteed to be monotonically decreasing  which
means that if we compute the discrete probabilities      
                                these are not guaranteed
to be strictly positive  To address the monotonicity issue 
Schapire et al    proposed   heuristic solution 
techniques but which do not
There are other ordinal
impose unimodal constraints 
The proportional odds
model  POM  and its neural network extensions  POMNN 
CHNN  Guti errez et al    do not suffer from the
monotonicity issue due to the utilization of monotonically
increasing biases in the calculation of probabilities  The
stickbreaking approach by Khan et al    which is
  reformulation of the multinomial logit  softmax  could
also be used in the ordinal case as it technically imposes an
ordering on classes 

babykidschoolerteenadultseniorelderdistribution Ap     babykidschoolerteenadultseniorelderdistribution Bp     babykidschoolerteenadultseniorelderdistribution   Unimodal Probability Distributions for Deep Ordinal Classi cation

  Poisson distribution

The Poisson distribution is commonly used to model the
probability of the number of events            occurring
in   particular interval of time  The average frequency of
these events is denoted by        The probability mass
function is de ned as 

   exp 

 

  

        

 
where               While we are not actually using
this distribution to model the occurrence of events  we can
make use of its probability mass function  PMF  to enforce
discrete unimodal probability distributions  For   purely
technical reason  we instead deal with the log of the PMF 

 cid   kexp 

 cid 

log

  

  log kexp    log   
  log      log exp    log   
    log        log   

 

If we let       denote the scalar output of our deep net
 where           which can be enforced with the softplus
nonlinearity  then we denote       to be 

  log                 log   

 

where we have simply replaced the   in equation   with
      Then             is simply   softmax over     

            

 cid  
exp         
   exp         

 

 

which is required since the support of the Poisson is in 
 nite  We have also introduced   hyperparameter to the
softmax    to control the relative magnitudes of each value
of                  the variance of the distribution  Note
that as       the probability distribution becomes more
uniform  and as       the distribution becomes more
 onehot  like with respect to the class with the largest presoftmax value  We can illustrate this technique in terms of
the layers at the end of the deep network  which is shown
in Figure  
We note that the term in equation   can be rearranged
and simpli ed to

          log                 log   
             log         log   
           bj      

 

In this form  we can see that the probability of class   is determined by the scalar term       and   bias term that also
depends on       Another technique that uses biases to determine class probabilities is the proportional odds model

Figure   The  rst layer after       is    copy  layer  that is 
                            The second layer applies the
log Poisson PMF transform followed by the softmax layer 

 POM  also called the ordered logit  McCullagh   
where the cumulative probability of   class depends on  
learned bias 

               sigm         bj 

 
where          bK  Unlike our technique however  the
bias vector   is not   function of   nor       but    xed
vector that is learned during training  which is interesting 
Furthermore  probability distributions computed using this
technique are not guaranteed to be unimodal 
Figure   shows the resulting probability distributions for
values of             when       and      
We can see that all distributions are unimodal and that by
gradually increasing       we gradually change which class
has the most mass associated with itself  The   is also
an important parameter to tune as it alters the variance
of the distribution  For example  in Figure     we can
see that if we are con dent in predicting the second class 
      should be     though in this case the other classes
receive almost just as much probability mass 
If we set
      however  Figure     at           the second
class has relatively more mass  which is to say we are even
more con dent that this is the correct class  An unfortunate
side effect of using the Poisson distribution is that the variance is equivalent to the mean    This means that in the
case of   large number of classes probability mass will be
widely distributed  and this can be seen in the       case
in Figure   While careful selection of   can mitigate this 
we also use this problem to motivate the use of the binomial
distribution 
In the work of da Costa et al    they address the in 
 nite support problem by using    righttruncated  Poisson
distribution  In this formulation  they simply  nd the normalization constant such that the probabilities sum to one 
This is almost equivalent to what we do  since we use  
softmax  although the softmax exponentiates its inputs and
we also introduce the temperature parameter   to control
for the variance of the distribution 

                       log                log    log                log          deep	  net	  softmax Unimodal Probability Distributions for Deep Ordinal Classi cation

  Binomial distribution

The binomial distribution is used to model the probability
of   given number of  successes  out of   given number of
trials and some success probability  The probability mass
function for this distribution   for   successes  where    
          given       trials and success probability    
is 

 cid      
 cid 

 

               

pk         

 

In the context of applying this to   neural network    denotes the class we wish to predict        denotes the number of classes  minus one since we index from zero  and
                is the output of the network that we wish
to estimate  While no normalisation is theoretically needed
since the binomial distribution   support is  nite  we still
had to take the log of the PMF and normalise with   softmax to address numeric stability issues  This means the
resulting network is equivalent to that shown in Figure  
but with the log binomial PMF instead of Poisson  Just like
with the Poisson formulation  we can introduce the temperature term   into the resulting softmax to control for the
variance of the resulting distribution 
Figure   shows the resulting distributions achieved by varying   for when       and      

  Methods and Results
In this section we go into details of our experiments  including the datasets used and the precise architectures 

  Data

We make use of two ordinal datasets appropriate for deep
neural networks 

  Diabetic retinopathy  This is   dataset consisting
of extremely highresolution fundus image data  The
training set consists of   pairs of images  where
  pair consists of   left and right eye image corresponding to   patient 
In this dataset  we try and
predict from  ve levels of diabetic retinopathy  no
DR   images  mild DR   images  moderate DR   images  severe DR   images  or
proliferative DR   images    validation set is set
aside  consisting of   of the patients in the training
set  The images are preprocessed using the technique
proposed by competition winner Graham   and
subsequently resized to  px width and height 

  The Adience face dataset   Eidinger et al    This
dataset consists of   faces belonging to  

 https www kaggle com   diabeticretinopathy detection 
 http www openu ac il home hassner Adience data html

subjects  We use the form of the dataset where faces
have been precropped and aligned  We further preprocess the dataset so that the images are  px in
width and height  The training set consists of merging the  rst four crossvalidation folds together  the
last crossvalidation fold is the test set  which comprises   total of   images  From this    of the
images are held out as part of   validation set 

  Network

We make use of   modest ResNet  He et al    architecture to conduct our experiments  Table   describes the exact architecture  We use the ReLU nonlinearity and HeNormal initialization throughout the network 

Layer
Input      
Conv      
MaxPool      
    ResBlock      
    ResBlock      
    ResBlock      
    ResBlock      
    ResBlock      
    ResBlock      
    ResBlock      
AveragePool      

Output size
         
         
         
         
         
         
         
         
         
         
         

Table   Description of the ResNet architecture used in the experiments  For convolution  WxH FsS    lter size of dimension  
     with   feature maps  and   stride of    For average pooling 
WxHsS     pool size of dimension       with   stride of    This
architecture comprises   total of   learnable parameters 

We conduct the following experiments for both DR and
Adience datasets 

   Baseline  crossentropy loss  This simply corresponds to   softmax layer for   classes at the end of
the average pooling layer in Table   For Adience and
DR  this corresponds to   network with   and
  learnable parameters  respectively 

   Baseline  squarederror loss  Rather than regress
      against    we regress with       sigm      
since we have observed better results with this formulation in the past  For Adience and DR  this corresponds     and   learnable parameters  respectively 

  Crossentropy loss using the Poisson and binomial extensions at the end of the architecture  see Figure  
For Adience and DR  this corresponds to  
learnable parameters for both  Although da Costa

Unimodal Probability Distributions for Deep Ordinal Classi cation

et al    mention that crossentropy or squared error can be used  their equations assume   squared error between the  onehot encoded  ground truth and
       whereas we use crossentropy 

  EMD loss  equation   where  cid            Euclidean
norm  and the entire term is squared  to get rid of the
square root induced by the norm  using Poisson and
binomial extensions at the end of architecture  Again 
this corresponds to   learnable parameters for
both networks 

Amongst these experiments  we use       and also learn
  as   bias  When we learn   we instead learn sigm   
since we found this made training more stable  Note that
we can also go one step further and learn   as   function
of    though experiments did not show any signi cant gain
over simply learning it as   bias  However  one advantage
of this technique is that the network can quantify its uncertainty on   perexample basis  It is also worth noting that
the Poisson and binomial formulations are slightly underparameterised compared to their baselines  but experiments
we ran that addressed this  by matching model capacity 
did not yield signi cantly different results 
It is also important to note that in the case of ordinal
prediction  there are two ways to compute the  nal prediction  simply taking the argmax of         which is
what is simply done in discrete classi cation  or taking
   smoothed  prediction which is simply the expectation
of the integer labels        the probability distribution      
                       For the latter  we call this the  expectation trick    bene   of the latter is that it computes  
prediction that considers the probability mass of all classes 
One bene   of the former however is that we can use it to
easily rank our predictions  which can be important if we
are interested in computing topk accuracy  rather than top 
 
We also introduce an ordinal evaluation metric   the
quadratic weighted kappa  QWK   Cohen      which
has seen recent use on ordinal competitions on Kaggle  Intuitively  this is   number between   where   kappa
      denotes the model does no better than random
chance        denotes worst than random chance  and
      better than random chance  with       being the
best score  The  quadratic  part of the metric imposes  
quadratic penalty on misclassi cations  making it an appropriate metric to use for ordinal problems 
All experiments utilise an  cid  norm of   ADAM optimiser  Kingma   Ba    with initial learning rate  
and batch size      manual  learning rate schedule is

 The quadratic penalty is arbitrary but somewhat appropriate
for ordinal problems  One can plug in any cost matrix into the
kappa calculation 

employed where we manually divide the learning rate by  
when either the validation loss or valid set QWK plateaus
 whichever plateaus last  down to   minimum of   for
Adience and   for DR 

  Experiments

Figure   shows the experiments run for the Adience dataset 
for when        Figure     and when   is learned  Figure     We can see that for our methods  careful selection
of   is necessary for the accuracy on the validation set to be
on par with that of the crossentropy baseline  For      
accuracy is poor  but even less so when   is learned  To
some extent  using the smoothed prediction with the expectation trick alleviates this gap  However  because the
dataset is ordinal  accuracy can be very misleading  so we
should also consider the QWK  For both argmax and expectation  our methods either outperform or are quite competitive with the baselines  with the exception of the QWK
argmax plot for when       where only our binomial formulations were competitive with the crossentropy baseline  Overall  considering all plots in Figure   it appears
the binomial formulation produces better results than Poisson  There also appears to be some bene   gained from
using the EMD loss for Poisson  but not for binomial 
Figure   show the experiments run for diabetic retinopathy  We note that unlike Adience  the validation accuracy
does not appear to be so affected across all speci cations
of   One potential reason for this is due to Adience having
  larger number of classes compared to DR  As we mentioned earlier  the Poisson distribution is somewhat awkward as its variance is equivalent to its mean  Since most of
the probability mass sits at the mean  if the mean of the distribution is very high  which is the case for datasets with  
large   such as Adience  then the large variance can negatively impact the distribution by taking probability mass
away from the correct class  We can see this effect by comparing the distributions in Figure          and Figure  
       As with the Adience dataset  the use of the expectation trick brings the accuracy of our methods to be almost
onpar with the baselines  In terms of QWK  only our binomial formulations appear to be competitive  but only in the
argmax case do one of our methods  the binomial formulation  beat the crossentropy baseline  At least for accuracy 
there appears to be some gain in using the EMD loss for the
binomial formulation  Because DR is   much larger dataset
compared to Adience  it is possible that the deep net is able
to learn reasonable and  unimodallike  probability distributions without it being enforced in the model architecture 

 We also reran experiments using an automatic heuristic to
change the learning rate  and similar experimental results were
obtained 

Unimodal Probability Distributions for Deep Ordinal Classi cation

    Learning curves for Adience dataset  for       For both accuracy and QWK  both the argmax and expectation way of
computing   prediction are employed 

    Learning curves for Adience dataset  for when   is learned as   bias  For both accuracy and QWK  both the argmax and
expectation way of computing   prediction are employed 

Figure   Experiments for the Adience dataset  For       and     learned  we compare typical crossentropy loss  blue  crossentropy EMD with Poisson formulation  orange solid   dashed  respectively  crossentropy EMD with binomial formulation  green
solid   dashed  respectively  and regression  red  Learning curves have been smoothed with   LOESS regression for presentation
purposes 

    Learning curves for diabetic retinopathy dataset  for       For both accuracy and QWK  both the argmax and expectation
way of computing   prediction are employed 

    Learning curves for diabetic retinopathy dataset  where   is made   learnable bias  For both accuracy and QWK  both the
argmax and expectation way of computing   prediction are employed 

Figure   Experiments for the diabetic retinopathy  DR  dataset  For       and     learned  we compare typical crossentropy loss
 blue  crossentropy EMD with Poisson formulation  orange solid   dashed  respectively  crossentropy EMD with binomial formulation  green solid   dashed  respectively  and regression  red  Learning curves have been smoothed with   LOESS regression for
presentation purposes 

 epochvalid acc  argmax   ent  tau   ent   pois  tau emd    pois  tau   ent   binom  tau emd    binom  tau sq err epochvalid acc  exp   ent  tau   ent   pois  tau emd    pois  tau   ent   binom  tau emd    binom  tau sq err epochvalid QWK  argmax   ent  tau   ent   pois  tau emd    pois  tau   ent   binom  tau emd    binom  tau sq err epochvalid QWK  exp   ent  tau   ent   pois  tau emd    pois  tau   ent   binom  tau emd    binom  tau sq err epochvalid acc  argmax   ent  tau   ent   pois  tau learned emd    pois  tau learned   ent   binom  tau learned emd    binom  tau learned sq err epochvalid acc  exp   ent  tau   ent   pois  tau learned emd    pois  tau learned   ent   binom  tau learned emd    binom  tau learned sq err epochvalid QWK  argmax   ent  tau   ent   pois  tau learned emd    pois  tau learned   ent   binom  tau learned emd    binom  tau learned sq err epochvalid QWK  exp   ent  tau   ent   pois  tau learned emd    pois  tau learned   ent   binom  tau learned emd    binom  tau learned sq err epochvalid acc  argmax   ent  tau   ent   pois  tau emd    pois  tau   ent   binom  tau emd    binom  tau sq err epochvalid acc  exp   ent  tau   ent   pois  tau emd    pois  tau   ent   binom  tau emd    binom  tau sq err epochvalid QWK  argmax   ent  tau   ent   pois  tau emd    pois  tau   ent   binom  tau emd    binom  tau sq err epochvalid QWK  exp   ent  tau   ent   pois  tau emd    pois  tau   ent   binom  tau emd    binom  tau sq err epochvalid acc  argmax   ent  tau   ent   pois  tau learned emd    pois  tau learned   ent   binom  tau learned emd    binom  tau learned sq err epochvalid acc  exp   ent  tau   ent   pois  tau learned emd    pois  tau learned   ent   binom  tau learned emd    binom  tau learned sq err epochvalid QWK  argmax   ent  tau   ent   pois  tau learned emd    pois  tau learned   ent   binom  tau learned emd    binom  tau learned sq err epochvalid QWK  exp   ent  tau   ent   pois  tau learned emd    pois  tau learned   ent   binom  tau learned emd    binom  tau learned sq errUnimodal Probability Distributions for Deep Ordinal Classi cation

               

               

Figure   Illustration of the probability distributions that are obtained from varying             for when there are four classes
       and when        left  and        right  We can see that lowering   results in   lower variance distribution  Depending on
the number of classes  it may be necessary to tune   to ensure the right amount of probability mass hits the correct class 

               

               

Figure   Illustration of the probability distributions that are obtained from varying             for when there are eight classes
       and when        left  and        right  Because we have   greater number of classes        must take on   greater range
of values  unlike Figure   before most of the probability mass moves to the last class 

    Faces from Adience valid set

    Crossentropy  baseline 

    Crossentropy   Poisson      

Figure   Probability distributions over selected examples in the validation set for Adience  those selected have nonunimodal probability
distributions for the crossentropy baseline  Left  from crossentropy   Poisson model   learned  right  crossentropy  baseline  model

 Unimodal Probability Distributions for Deep Ordinal Classi cation

         

         

Figure   Illustration of the probability distributions that are obtained from varying         for the binomial classes when        left 
and        right 

we compute the topk accuracy on the test set of the Adience dataset  shown in Figure   We can see that even with
the worstperforming model   the Poisson formulation with
       orange    produces   better top  accuracy than the
crossentropy baseline  blue 

  Conclusion
In conclusion  we present   simple technique to enforce
unimodal ordinal probabilistic predictions through the use
of the binomial and Poisson distributions  This is an important property to consider in ordinal classi cation because
of the inherent ordering between classes  We evaluate our
technique on two ordinal image datasets and obtain results
competitive or superior to the crossentropy baseline for
both the quadratic weighted kappa  QWK  metric and topk accuracy for both crossentropy and EMD losses  especially under the binomial distribution  Lastly  the unimodal
constraint can makes the probability distributions behave
more sensibly in certain settings  However  there may be
ordinal problems where   multimodal distribution may be
more appropriate  We leave an exploration of this issue for
future work  Code will be made available here 

  Acknowledgements
We thank Samsung for funding this research  We would
like to thank the contributors of Theano  Theano Development Team    and Lasagne  Dieleman et al   
 which this project was developed in predominantly  as
well as Keras  Chollet et al    for extra useful code 
We thank the ICML reviewers for useful feedback  as well
as Eibe Frank 

 https github com christopherbeckham deepunimodal 

ordinal

Figure   Topk accuracies computed on the Adience test set 
where          

Overall  across both datasets the QWK for our methods are
generally at least competitive with the baselines  especially
if we learn   to control for the variance  In the empirical results of da Costa et al    they found that the binomial
formulation performed better than the Poisson  and when
we consider all of our results in Figure   and   we come
to the same conclusion  They justify this result by de ning the  exibility  of   discrete probability distribution and
show that the binomial distribution is more  exible  than
Poisson  From our results  we believe that these unimodal
methods act as   form of regularization which can be useful
in regimes where one is interested in topk accuracy  For
example  in the case of topk accuracy  we want to know
if the ground truth was in the top   predictions  and we
may be interested in such metrics if it is dif cult to achieve
good top  accuracy  Assume that our probability distribution        has most of its mass on the wrong class  but the
correct class is on either side of it  Under   unimodal constraint  it is guaranteed that the two classes on either side of
the majority class will receive the next greatest amount of
probability mass  and this can result in   correct prediction
if we consider top  or top  accuracy  To illustrate this 

                                                                                                                                                                                                                 entx ent pois tau   ent binom tau   ent binom tau  Unimodal Probability Distributions for Deep Ordinal Classi cation

References
Beckham  Christopher and Pal  Christopher    simple
squarederror reformulation for ordinal classi cation 
arXiv preprint arXiv   

Cheng  Jianlin    neural network approach to ordinal regression  CoRR  abs    URL http 
 arxiv org abs 

Chollet  Franc ois et al  Keras  https github com 

fchollet keras   

Cohen  Jacob  Weighted kappa  Nominal scale agreement
provision for scaled disagreement or partial credit  Psychological bulletin     

da Costa  Joaquim   Pinto  Alonso  Hugo  and Cardoso 
Jaime    The unimodal model for the classi cation of
ordinal data  Neural Networks     

Dieleman  Sander  Schlter  Jan  Raffel  Colin  Olson  Eben 
Snderby  Sren Kaae  Nouri  Daniel  and et al  Lasagne 
First release  August   URL http dx doi 
org zenodo 

Eidinger  Eran  Enbar  Roee  and Hassner  Tal  Age and
IEEE Transgender estimation of un ltered faces 
actions on Information Forensics and Security   
   

Frank  Eibe and Hall  Mark    simple approach to ordinal classi cation  In European Conference on Machine
Learning  pp    Springer   

Gentry  Amanda Elswick  JacksonCook  Colleen    Lyon 
Debra    and Archer  Kellie    Penalized ordinal regression methods for predicting stage of cancer in highdimensional covariate spaces  Cancer informatics   
 Suppl    

Hou  Le  Yu  ChenPing  and Samaras  Dimitris  Squared
earth mover   distancebased loss for training deep neural networks  CoRR  abs    URL
http arxiv org abs 

Khan  Mohammad    Mohamed  Shakir  Marlin  Benjamin    and Murphy  Kevin      stickbreaking likelihood for categorical data analysis with latent gaussian
models  In International conference on Arti cial Intelligence and Statistics  pp     

Kingma  Diederik    and Ba  Jimmy  Adam    method
for stochastic optimization  CoRR  abs   
URL http arxiv org abs 

Koren  Yehuda and Sill  Joe  Ordrec  an ordinal model for
predicting personalized item rating distributions  In Proceedings of the  fth ACM conference on Recommender
systems  pp    ACM   

McCullagh  Peter  Regression models for ordinal data 
Journal of the royal statistical society  Series    Methodological  pp     

Niu  Zhenxing  Zhou  Mo  Wang  Le  Gao  Xinbo  and
Hua  Gang  Ordinal regression with multiple output cnn
for age estimation  In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp 
   

Schapire  Robert    Stone  Peter  McAllester  David 
Littman  Michael    and Csirik    anos    Modeling auction price uncertainty using boostingbased conditional
density estimation  In ICML  pp     

Theano Development Team  Theano    Python framework
for fast computation of mathematical expressions  arXiv
eprints  abs  May   URL http 
arxiv org abs 

Graham  Ben 

Kaggle diabetic retinopathy detecURL https 

tion competition report   
 kaggle blob core windows net 
forummessage attachments 
competitionreport pdf 

Guti errez  Pedro Antonio  Ti no  Peter  and Herv asMart nez    esar  Ordinal regression neural networks
based on concentric hyperspheres  Neural Netw   
  November  
ISSN   doi   
  neunet  URL http dx doi org 
   neunet 

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition 
CoRR  abs    URL http arxiv 
org abs 

