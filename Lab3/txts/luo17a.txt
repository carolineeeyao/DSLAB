Learning Deep Architectures via Generalized Whitened Neural Networks

Ping Luo    

Abstract

Whitened Neural Network  WNN  is   recent
advanced deep architecture  which improves convergence and generalization of canonical neural
networks by whitening their internal hidden representation  However  the whitening transformation increases computation time  Unlike WNN
that reduced runtime by performing whitening
every thousand iterations  which degenerates
convergence due to the ill conditioning  we
present generalized WNN  GWNN  which has
three appealing properties 
First  GWNN is
able to learn compact representation to reduce
computations 
it enables whitening
transformation to be performed in   short period 
preserving good conditioning  Third  we propose
  dataindependent estimation of the covariance
matrix to further improve computational ef ciency  Extensive experiments on various datasets
demonstrate the bene ts of GWNN 

Second 

  Introduction
Deep neural networks  DNNs  have improved performances of many applications  as the nonlinearity of DNNs
provides expressive modeling capacity  but it also makes
DNNs dif cult to train and easy to over   the training data 
Whitened neural network  WNN   Desjardins et al   
  recent advanced deep architecture  is ideally to solve the
above dif culties  WNN extends batch normalization  BN 
 Ioffe   Szegedy    by normalizing the internal hidden
representation using whitening transformation instead of
standardization  Whitening helps regularize each diagonal
block of the Fisher Information Matrix  FIM  to be an

 Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology  Shenzhen Institutes of
Advanced Technology  Chinese Academy of Sciences  Shenzhen  China  Multimedia Laboratory  The Chinese University
of Hong Kong  Hong Kong  Correspondence to  Ping Luo
 pluo ie cuhk edu hk 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

approximation of the identity matrix  This is an appealing property  as training WNN using stochastic gradient
descent  SGD  mimics the fast convergence of natural
gradient descent  NGD   Amari   Nagaoka    The
whitening transformation also improves generalization  As
demonstrated in  Desjardins et al    WNN exhibited superiority when being applied to various network
architectures  such as autoencoder and convolutional neural
network  outperforming many previous works including
SGD  RMSprop  Tieleman   Hinton    and BN 
Although WNN is able to reduce the number of training
iterations and improve generalization  it comes with   price
of increasing training time  because eigendecomposition
occupies large computations  The runtime scales up when
the number of hidden layers that require whitening transformation increases  We revisit WNN by breaking down
its performance and show that its main runtime comes
from two aspects    computing full covariance matrix
for whitening and   solving singular value decomposition
 SVD  Previous work  Desjardins et al    suggests to
overcome these problems by    using   subset of training
data to estimate the full covariance matrix and    solving
the SVD every hundreds or thousands of training iterations 
Both of them rely on the assumption that the SVD holds in
this period  but it is generally not true  When this period
becomes large  WNN degenerates to canonical SGD due to
ill conditioning of FIM 
We propose generalized WNN  GWNN  which possesses
the bene cial properties of WNN  but signi cantly reduces
its runtime and improves its generalization  We introduce
two variants of GWNN  including prewhitening and postwhitening GWNNs  The former one whitens   hidden
layer   input values  whilst the latter one whitens the preactivation values  hidden features  GWNN has three appealing characteristics  First  compared to WNN  GWNN
is capable of learning more compact hidden representation  such that the SVD can be approximated by   few
top eigenvectors to reduce computation  This compact
representation also improves generalization  Second  it
enables the whitening transformation to be performed in
  short period  maintaining conditioning of FIM  Third 
by exploiting knowledge of the distribution of the hidden
features  we calculate the covariance matrix in an analytical
form to further improve computational ef ciency 

Generalized Whitened Neural Network

Figure   Comparisons of differnet architectures  An ordinary fullyconnected  fc  layer can be adapted into       whitened fc layer     
  preGWNN layer  and       postGWNN layer      and     learn more compact representation than     does 

  Notation and Background
We begin by de ning the basic notation for feedforward
neural network    neural network transforms an input
vector    to an output vector   cid  through   series of  cid 
hidden layers  oi cid 
   We assume each layer has identical
dimension for the simplicity of notation       oi   Rd 
In this case  all vectors and matrixes in the following
should have   rows unless otherwise stated  As shown
in Fig      each fullyconnected  fc  layer consists of
  weight matrix       and   set of hidden neurons  hi 
each of which receives as input   weighted sum of outputs
from the previous layer  We have hi     ioi 
In
this work  we take fullyconnected network as an example 
Note that the above computation can be also applied to  
convolutional network  where an image patch is vectorized
as   column vector and represented by oi  and each row
of     represents    lter 
As the recent deep architectures typically stack   batch
normalization  BN  layer before the preactivation values 
we do not explicitly include   bias term when computing
hi  because it
is normalized in BN  such that     
 
hi   hi 
  where the expectation and variance are computed
Var hi 
over   minibatch of samples  LeCun et al    showed
that such normalization speeds up convergence even when
the hidden features are not decorrelated  Furthermore 
output of each layer is calculated by   nonlinear activation
function    popular choice is the recti ed linear unit 
relu      max     The precise computation for an
output is oi   max  diag           where diag   
represents   matrix whose diagonal entries are       and   
are two vectors that scale and shift the normalized features 
in order to maintain the network   representation capacity 

  Whitened Neural Networks

This section revisits whitened neural networks  WNN 
Any neural architecture can be adapted to   WNN by
stacking   whitening transformation layer after the layer  
input  For example  Fig      adapts   fc layer as shown in

    into   whitened fc layer  Its information  ow becomes

 cid oi        oi         hi        cid oi 

 

  oi   max  diag          

    

 hi 
Var hi 

matrix         cid oi cid oi  

where     represents   centering variable       
  oi       is   whitening matrix whose rows are
obtained from eigendecomposition of     which is
the covariance matrix of the input          oi   
   oi          The input is decorrelated by     
in the sense that its covariance matrix becomes an identity
       To avoid ambiguity  we use
  to distinguish the variables in WNN and the canonical
fc layer whenever necessary  For instance       represents  
whitened weight matrix 
In Eqn  computation of the
BN layer has been simpli ed because we have   hi   
   iP     oi           
We de ne   to be   vector consisting of all
the
whitened weight matrixes concatenated together     
 vec         vec           vec      cid    where vec  is an
operator that vectorizes   matrix by stacking its columns 
Let     cid       denote   loss function of WNN  which
measures the disagreement between   prediction   cid  made
by the network  and   target    WNN is trained by
minimizing the loss function with respect to the parameter
vector   and two constraints

       cid oi cid oi  

 

    cid      

min
       hi     hi     hi       cid 

 

To satisfy the  rst constraint       is obtained by decomposing the covariance matrix            Si       
We choose         Si   
         where Si  is  
diagonal matrix whose diagonal elements are eigenvalues
and      is an orthogonal matrix of eigenvectors  The
 rst constraint holds under the construction of eigendecomposition 
The second constraint  hi     hi     hi  enforces that
the centered hidden features are the same  before and after

oi oiWioi Pi hioi Pioi Pi hi   fullyconnected layer  fc    whitened fc layer of WNN    prewhitening GWNN   postwhitening GWNNWiWiWirelubn ioioioioioioi           ioi oi oi oi hiweight matrixwhitening matrixhi hi hihid             Generalized Whitened Neural Network

     cid   ioi                  cid oi cid 

adapting   fc layer to WNN  as shown in Fig      and    
In other words  it ensures that their representation powers
are identical  By combing the computations in Fig      and
Eqn  the second constraint implies that  cid hi     hi   
 hi cid 
      which
has   closedform solution                  To see
this  we have  hi               oi         
    oi      hi   hi  The representation capacity
can be preserved by mapping the whitened weight matrix
from the ordinary weight matrix 
Conditioning of the FIM  Here we show that WNN
improves training ef ciency by conditioning the Fisher
information matrix  FIM   Amari   Nagaoka     
FIM  denoted as     consists of  cid     cid  blocks  Each block is
indexed by Fij  representing the covariance  coadaptation 
between the whitened weight matrixes of the ith and jth
layers  We have Fij     vec       vec          where
       indicates the gradient of the ith whitened weight
matrix  For example  the gradient of      is achieved by

 cid oi hi    as illustrated in Eqn  We have vec         
vec cid oi hi       hi    cid oi  where   denotes the
as   hi    cid oi hj    cid oj        hi hj    
 cid oi cid oj    By assuming    and cid   are independent as
imated by   hi hj        cid oi cid oj    As   result 
diagonal structure because we have   cid oi cid oi        as

demonstrated in  Raiko et al    Fij can be approx 

when        each diagonal block of     Fii  has   block

In this case  Fij can be rewritten

Kronecker product 

shown in Eqn  which improves conditioning of FIM and
thus speeds up training  In general  WNN regularizes the
diagonal blocks of FIM and achieves stronger conditioning
than those methods  LeCun et al    Tieleman  
Hinton    that regularized the diagonal entries 
Training WNN  Alg  summarizes training of WNN  At
the  st line  the whitened weight matrix     
  is initialized
by     of the ordinary fc layer  which can be pretrained
or sampled from   Gaussian distribution  The  th line
shows that     
  is updated in each iteration   using SGD 
The  rst and second constraints are achieved in the  th and
 th lines respectively  For example  the  th line ensures
that the hidden features are the same before and after
updating the whitening matrix  As the distribution of the
hidden representation changes after every update of the
whitened weight matrix  to maintain good conditioning of
FIM  the whitening matrix       needs to be reconstructed
frequently by performing eigendecomposition on    
which is estimated using   samples    is typically
  in experiments  However  this raw strategy increases
computation time  Desjardins et al    performed
whitening in every   iterations as shown in the  th line of
Alg  to reduce computations            
How good is the conditioning of the FIM by using Al 

Algorithm   Training WNN
  Init  initial network parameters           whitening matrix

          iteration           

               cid 

  repeat
 
 

for       to  cid  do

 
 
 

  using SGD 

  and parameters

update whitened weight matrix     
  
if mod            then

     
store old whitening matrix     
construct new matrix      by eigendecomposition
on     which is estimated using   samples 
     
transform weight matrix     

        

        

      

 

 
 
 
 
  until convergence

end if
end for
         

Figure   Visualizations of different covariance matrixes      
which have different Pearson   correlations  top  with respect to
an identity matrix  Larger Pearson   correlation indicates higher
orthogonality 
      are sampled from   uniform distribution
between   and  
      are generated by truncating   random
orthogonal matrix with different numbers of columns  The
colorbar  right  indicates the value of each entry in these matrixes 

   We measure the similarity of the covariance matrix 

  cid oi cid oi    with the identity matrix    This is called

the orthogonality  We employ Pearson   correlation  as the
similarity between two matrixes  Intuitively  this measure
has   value between   and   representing negative
and positive correlations  Larger values indicate higher
orthogonality  Fig  visualizes four randomly generated
covariance matrixes  where       are sampled from  
uniform distribution between   and   Fig        are
generated by truncating different numbers of columns of  
randomly generated orthogonal matrix  For instance       
have small similarity with respect to the identity matrix 
In contrast  when the correlation equals   as shown in
    all entries in the diagonal are larger than   and more
than   offdiagonal entries have values smaller than  
Furthermore  Pearson   correlation is insensitive to the size
of matrix  such that orthogonality of different layers can
be compared together  For example  although matrixes in

 Given an identity matrix     and   covariance matrix    the
Pearson   correlation between them is de ned as corr      
 
  where vec  is   normalized vecvec Tvec vec   Tvec   
tor by subtracting mean of all entries 

vec Tvec   

         Generalized Whitened Neural Network

as illustrated in Fig      When adapting   fc layer to
preGWNN  the whitening matrix is truncated by removing
those eigenvectors that have small eigenvalues  in order to
learn compact representation  This allows the input vector
to vary its length  so as to gradually adapt the learned
representation to informative patterns with high variations 
but not noises  Learning preGWNN is formulated analogously to learning WNN in Eqn  but with one additional
constraint truncated the rank of the whitening matrix 

Figure   Comparisons of conditioning when training   networkin network  Lin et al    on CIFAR   Krizhevsky   
by using     WNN and     preGWNN  Compared to    
the orthogonalities of three different layers in     have large
 uctuations due to the ill conditioning of whitening  which is
performed in   large period   As   result  when   increases 
WNN will degenerate to the canonical SGD 

    and     have different sizes  they have similar value
of orthogonality when they are sampled from the same
distribution 
As shown in Fig      we adopt networkin network
 NIN   Lin et al    that is trained on CIFAR 
 Krizhevsky    and plot the orthogonalities of three
different convolutional layers  which are whitened every
      iterations by using Alg  We see that orthogonality values during training have large  uctuations
except those of the  rst convolutional layer  abbreviated as
 conv  This is because the distributions of deeper layers 
inputs change after the whitened weight matrixes have
been updated  leading to illconditions of the whitening
matrixes  which are estimated in   large interval  In fact 
large   will degenerate WNN to canonical SGD  However 
 conv  uses image data as inputs  whose distribution is
typically stable during training  Its whitening matrix can
be estimated once at the beginning and  xed in the entire
training stage 
In the section below  we present generalized whitened
neural networks to improve conditioning of FIM while
reducing computation time 

  Generalized Whitened Neural Networks
We present two types of generalized WNN  GWNN  including prewhitening and postwhitening GWNNs  Both
models share bene cial properties of WNN  but have lower
computation time 

  Prewhitening GWNN

This section introduces prewhitening GWNN  abbreviated
as preGWNN  which performs whitening transformation
before applying the weight matrix       whiten the input 

 

min

    cid      

     rank          cid    cid oi 

  cid   cid oi  

  cid 
hi     hi     hi       cid 

 

      

      
  cid 

  cid     Si 

  cid        
  cid 

     cid oi 

  cid    Rd  where                
  cid 

lowdimensional space   cid oi 

Let   be the dimension of the original fc layer  By combin 
          Rd   
ing Eqn  we have         Si   
   
which is truncated by using     
  cid     
Rd cid    where Si 
is achieved by keeping rows and
  cid 
columns associated with the  rst   cid 
large eigenvalues 
whilst     
contains the corresponding   cid  eigenvectors 
  cid 
The value of   cid  can be tuned using   validation set 
For simplicity  we choose   cid     
    which works well
throughout our experiments  This is inspired by the  nding
in  Zhang et al    who disclosed that the  rst  
eigenvectors contribute over   energy in   deep model 
More speci cally  preGWNN  rst projects an input vector
 oi   
to     cid 
      Rd cid 
The whitened weight matrix then
produces   hidden feature vector of   dimensions  which
has the same length as the ordinary fc layer        hi  
    Rd   cid 
 
The computations of BN and the nonlinear activation are
identical to Eqn 
Training preGWNN is similar to Alg 
The main
modi cation is produced at the  th line in order to reduce
runtime  Although Alg  decreases number of iterations
when training converged  each iteration has additional
computation time for eigendecomposition  For example 
in WNN  the required computation of full singular value
decomposition  SVD  is typically        where  
represents the number of samples employed to estimate
the covariance matrix 
In particular  when we have  cid 
whitened layers and   is the number of iterations  all
whitening transformations occupy           cid 
  runtime in
the entire training stage  In contrast  preGWNN performs
the popular online estimation for the top   cid  eigenvectors
in     
such as online SVD  Shamir    Povey et al 
  cid 
instead of using full SVD as WNN did  This
 
difference reduces runtime to             cid    cid 
  where  cid 
represents the whitening interval in GWNN and   is the
number of samples used to estimate the top eigenvectors 
We have       as employed in previous works 

 

 cid 

 orthogonalityiterations    conv conv conv orthogonalityiterations    conv conv conv   WNN   preGWNNGeneralized Whitened Neural Network

For preGWNN  reducing runtime and improving conditioning is   tradeoff  since the former requires to increase
 cid  but the latter requires to decrease it  When      
and   cid     
    we compare the runtime complexity of preGWNN to that of WNN  and obtain   ratio of   cid 
    which
tells us that whitening can be performed in   short interval
without increasing runtime  For instance  as shown in
Fig      when  cid      orthogonalities are well preserved
and more stable than those in    
In this case  preGWNN reduces computations of WNN by at least  
when       which is   typical choice in recent deep
architectures  Krizhevsky et al    Lin et al   
where          

  Postwhitening GWNN

Another variant we propose is postwhitening GWNN 
abbreviated as postGWNN  Unlike WNN and preGWNN 
postGWNN performs whitening transformation after applying the weight matrix       whiten the feature  as
In general  postGWNN reduces
illustrated in Fig     
runtime to       cid      cid    cid 
  where   cid   cid    
Fig      shows how to adapt   fc layer to postGWNN 
Suppose oi 
in the previous
  cid 
layer  at the ith layer we have
  cid       
 hi        oi 
  cid 
  oi
  cid    max  diag  

has been whitened by     
  cid 

  cid   hi 
  cid  

  cid    hi

  cid       

  cid      

  
  cid   

  cid 

 cid 

hi

 

Var hi

  cid   

 

  cid     

  cid      oi 
  cid 

where    
In Eqn    feature vector
 hi   Rd  is  rst produced by applying   whitened weight
matrix on the input  in order to recover the original feature
length as the fc layer    whitening matrix then projects
  cid    Rd cid  We
 hi to   decorrelated feature vector hi
    Rd   cid 
have                
  where     
 
  cid 
  cid 
    Rd cid    and      and Si  contain
      
 Si 
  cid 
eigenvectors and eigenvalues of the hidden features at the
     th layer 
Conditioning 
Here we disclose that whitening hidden features also enforces good conditioning of FIM  At
this point  we have decorrelated the hidden features by
satisfying   hi
  cid  follows   standard
  cid       Then hi
  cid           As
multivariate Gaussian distribution  hi
  result  the layer   output follows   recti ed Gaussian
distribution  which is uncorrelated as presented in remark
  In postGWNN  whitening hidden features of the      
th layer improves conditioning for the ith layer  To see
this  by following the description in Sec  the diagonal
block of FIM associated with the ith layer can be written
as Fii     hi hi        oi 
  cid     
where the parameters have low correlations since Fii has  
block diagonal structure 

  cid       

  cid       

  cid   oi 

  cid hiT

Algorithm   Training postGWNN
  Init  initial           and       tw          tw

             

               cid 
for       to  cid  do
      

update     
   and   
if mod            then
        
  cid 

    

  repeat
 
 
 
 
 

  by SGD 

 

store old     
estimate mean and variance of  hi by   minibatch of
  cid  samples or following remark   when   cid     
update     
by online SVD 
  cid 
      
        
transform     
tw     and     tw
   

     
  cid 

 

 

 
 
 
 
 
 
 
  until convergence

end if
end for
         
if tw     then tw   tw     end if

Remark   Let            and     max  Ah     
Then   oj     oj ok     ok      if   is   diagonal
matrix  where      index any two entries of   and    cid    

  cid  and       

For remark   we have     diag  
  cid  It
tells us three things  First  by using whitening and BN 
covariance of any two different entries of oi
  cid  approaches
  cid  we
zero  Second  at the iteration when we construct    
can estimate the full covariance matrix of  hi using the
  cid   
mean and variance of oi 
  cid 
   
  cid   oi 
  cid         iT  The mean and variance can
be estimated with   minibatch of samples        cid   cid    
Third  to the extreme  when   cid      these statistics can
still be computed in analytical forms leveraging remark  

       iE oi 

  cid       

    hi hiT

Remark   Let   random variable           and    
max  ax      Then          
 
and        ab 
    erf    and erf    is the error function 

    
      

  
  where      

      

          

       

    

 

 

  

 

 

     

The above remark derives the mean and variance of  
recti ed output unit that has shift and scale parameters  It
generalizes  Arpit et al    that presented   special case
when       and       In that case  we have         
and Var                  
    which are consistent
with previous work 
Extensions  Remark   and   can be extended to other
nonlinear activation functions  such as leaky recti ed unit
de ned as leakyrelu      max      min     where
the slope of the negative part is controlled by the coef cient
   which is  xed in  Maas et al    and is learned in  He
et al   

 

Generalized Whitened Neural Network

  Training postGWNN

Similar to preGWNN  the learning problem can be formulated as

     cid              cid cid 

 

min
     rank          cid    hi

   Lfeat hi   hi     

  cid hiT

  cid            cid 

 

 cid hi     hi     hi cid 

Eqn  has two loss functions  Different from WNN
and preGWNN where the feature equality constraint can
be satis ed in   closed form 
this constraint is treated
as an auxiliary loss function in postGWNN  de ned as
Lfeat hi   hi     
  and minimized in
the training stage  It does not have an analytical solution
because there is   nonlinear activation function between
the weight matrix and the whitening matrix      
in the
previous layer  In Eqn    is   coef cient that balances
the contribution of two loss functions  and       is linearly
decayed as           tw
  where tw            At each
time after we update the whitening matrix  we start decay
by setting tw     and   indicates the iterations at which
we stop annealing 
Alg  summarizes the training procedure  It preforms online update of the top   cid  eigenvectors of the whitening matrix similar to preGWNN  In comparison  it decreases the
runtime of whitening transformation to       cid      cid    cid 
 
which is     
  cid   fold reduction with respect to preGWNN 
For example  when       and   cid      postGWNN
is capable of reducing computations of preGWNN and
WNN by   and  cid  respectively  while maintaining
better conditioning than these alternatives by choosing
small  cid 

 cid 

  Empirical Studies
We compare WNN  preGWNN  and postGWNN in the
following aspects  including    number of iterations when
training converged     computation times for training  and
   generalization capacities on various datasets  We also
conduct ablation studies with respect to   effect of the
number of samples   to estimate the covariance matrix for
preGWNN and   effect of   cid  for postGWNN  Finally 
we try to tune the value of   cid 
Datasets  We employ the following datasets 
   MNIST  Lecun et al    has           images
of   handwritten digits   for training and another
    test images      images from the training set
are randomly selected as   validation set 
   CIFAR   Krizhevsky    consists of        
  color images for training and     images for testing 
Each image is categorized into one of the   object labels 
For CIFAR      images are chosen for validation 
   CIFAR   Krizhevsky    has the same number of

images as CIFAR  but each image is classi ed into  
categories  For CIFAR  we select     images from
training set for validation 
   SVHN  Netzer et al    consists of color images of
house numbers collected by Google Street View  The task
is to predict the center digit   of each image  which is
of size   There are     images in the training set 
    images for test  and     additional examples 
We follow  Sermanet et al    to build   validation set
by selecting   samples per class from the training set and
  samples per class from the additional set  We didn  
train on validation  which is for tuning hyperparameters 
Experimental Settings 
We have two settings  an
unsupervised and   supervised learning settings  First 
following  Desjardins et al    we compare the above
three approaches on the task of minimizing reconstruction
error of an autoencoder on MNIST  The encoder consists
of   fc sigmoidal
layers  which have      
and   hidden neurons respectively 
The decoder is
symmetric and untied with respect to the encoder  Second 
for the task of image classi cation on CIFAR   
and SVHN  we employ the same networkin network
 NIN 
 Lin et al    architecture  which has  
convolutional layers and   pooling layers de ned as 
conv   conv   maxpool   conv   
conv   conv   avgpool   conv   
conv   conv   conv     avgpool   
where       for CIFAR  and SVHN and       for
CIFAR  For all models  we use SGD with momentum
of  

  Comparisons of Convergence and Computations

We record the number of epochs and computation time 
when training WNN  pre  and postGWNN on MNIST
and CIFAR  respectively  We employ the  rst setting
above for MNIST and the second setting for CIFAR 
  For both settings  hyperparamters are chosen by grid
search on the validation sets  The search speci cations of
minibatch size  learning rate  and whitening interval   are
            and        
respectively  In particular  for WNN and preGWNN  the
number of samples used to estimate the covariance matrix 
      For postGWNN 
   is picked up from    
  cid  is chosen to be the same as the minibatch size and the
decay period       For   fair comparison  we report
the best performance on validation set for each approach 
and didn   employ any data augmentation such as random
image cropping and  ipping 

 The  conv   maxpool  and  avgpool  represent convolution 
max pooling  and average pooling respectively  Each convolutional layer is de ned as conv number of  lters   lter size 
For each pooling layer  we have pool kernel size  stride  All
convolutions have stride  

Generalized Whitened Neural Network

Figure   Training of WNN  pre  postGWNN on CIFAR        and MNIST           and     plot the convergence and computation
time on the validation and test set of CIFAR  respectively      and     report corresponding results on MNIST 

The convergence and computation time are reported in
Fig   ad  We have several
important observations 
First  all three approaches converge much faster than the
canonical network trained by SGD  Second  preand postGWNN achieve better convergence than WNN on both
datasets as shown in     and     Moreover  postGWNN
outperforms preGWNN  Third  postGWNN signi cantly
reduces computation time compared to all the other methods  as illustrated in     and     We see that although
WNN reduces the number of epochs  it takes long time to
train because its whitening transformation occupies large
computations 

  Performances on various Datasets

We evaluate WNN  pre  and postGWNN on CIFAR 
  and SVHN datasets  and compare their classi cation
accuracies to existing stateof theart methods  For all
the datasets and approaches  we utilize the same network
structure as mentioned in the second setting above  For
two CIFAR datasets  we adopt minibatch size   and initial
learning rate   which is reduced by half after every  
epochs  We train for   epochs  As SVHN is   large
dataset  we train for   epochs with minibatch size  
and initial learning rate   which is reduced by half after
every   epochs  We train on CIFAR  and   using
both without and with data augmentation  which includes
random cropping and horizontal  ipping  For SVHN  we
didn   augment data following  Sermanet et al   
For all the methods  we shuf   samples at the beginning
of every epoch  We use      
for WNN and preGWNN and   cid      for postGWNN  For both pre 
 
and postGWNN  we have       and   cid     
    The
other experimental settings are similar to Sec  Table
  shows the results  We see that preand postGWNN
consistently achieve better results than those of WNN  and
also outperform previous stateof theart approaches 

Figure   Training of preand postGWNN on CIFAR     
visualizes the impact of different values of   for preGWNN 
showing that performance degrades when   is small      plots
the impact of   cid  for postGWNN  which is insensitive to small
values of   cid 

  Ablation Studies

The following experiments are conducted on CIFAR 
using preor postGWNN  The  rst
two experiments
follow the setting as mentioned in Sec  First  we
evaluate the effect of the number of samples     used to
estimate the covariance matrix in preGWNN  We compare
performances of using different values of   picked up
from                   Fig      plots the
results  We see that performance can drop because of ill
conditioning when   is small            When it is
too large            we observe slightly over tting 
Second  Fig      highlights the effect of   cid 
in postGWNN  We see that postGWNN can work reasonably
well when   cid  is small 
  as   constant in
Finally 
training  we study the effect of tuning its value on
the validation set using   simple heuristic strategy 
If
the validation error reduces more than   over   consecutive evaluations  we have   cid      cid    rate     cid 

instead of treating   cid     

 test	errorminutesSGDpost GWNNpre GWNNWNN validation	errorepochsSGDpost GWNNpre GWNNWNN test	errorhourSGDpost GWNNpre GWNNWNN validation	errorepochsSGDpost GWNNpre GWNNWNN         epochspost GWNN	  cid post GWNN	  cid post GWNN	  cid validation	errorepochspre GWNN	  cid pre GWNN	  cid pre GWNN	  cid pre GWNN	  cid pre GWNN	  cid     Generalized Whitened Neural Network

Table   Comparisons of test errors on various datasets  The top
two performances are highlighted for each dataset 

  NIN  Lin et al   

CIFAR 

Error 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Error 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Error 

 
 
 
 
 
 
 
 
 

  Conclusion
We presented generalized WNN  GWNN  to reduce runtime and improve generalization of WNN  Different from
WNN that reduces computation time by whitening with  
large period  leading to ill conditioning of FIM  GWNN
learns compact internal representation  such that SVD is
approximated by the top eigenvectors in an online manner 
making GWNN not only reduces computations but also
improves generalization  By exploiting the knowledge of
the hidden representation   distribution  we showed that
postGWNN is able to compute the covariance matrix in
  closed form  which can be also extended to the other
activation function  Extensive experiments demonstrated
the effectiveness of GWNN 

Acknowledgements
This work is partially supported by the National Natural Science Foundation of China    
   the National Key Research and Development Program of China  No YFC  the External Cooperation Program of BIC  Chinese Academy of
Sciences  No KYSB  and the Science
and Technology Planning Project of Guangdong Province
       

References
Agostinelli  Forest  Hoffman  Matthew  Sadowski  Peter 
Learning activation functions to

and Baldi  Pierre 
improve deep neural networks  In ICLR   

Amari  Shunichi and Nagaoka  Hiroshi  Methods of
In Tanslations of Mathematical

information geometry 
Monographs   

Arpit  Devansh  Zhou  Yingbo  Kota  Bhargava    and
Govindaraju  Venu  Normalization propagation   
parametric technique for removing internal covariate
shift in deep networks  In ICML   

Desjardins  Guillaume  Simonyan  Karen  Pascanu  Razvan  and Kavukcuoglu  Koray  Natural neural networks 
In NIPS   

Goodfellow  Ian    WardeFarley  David  Mirza  Mehdi 
Courville  Aaron  and Bengio  Yoshua  Maxout networks  In ICML   

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Delving deep into recti ers  Surpassing humanlevel performance on imagenet classi cation  In ICCV 
 

Ioffe  Sergey and Szegedy  Christian  Batch normalization 

  NIN  Lin et al   

CIFAR 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 

 

 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 

 

NIN ALP  Agostinelli et al   
Normalization Propagation  Arpit et al   
BatchNorm  Ioffe   Szegedy   
DSN  Lee et al   
Maxout  Goodfellow et al   
WNN  Desjardins et al   
preGWNN
postGWNN
NIN  Lin et al   
NIN ALP  Agostinelli et al   
Normalization Propagation  Arpit et al   
BatchNorm  Ioffe   Szegedy   
DSN  Lee et al   
Maxout  Goodfellow et al   
WNN  Desjardins et al   
preGWNN
postGWNN

NIN ALP  Agostinelli et al   
Normalization Propagation  Arpit et al   
BatchNorm  Ioffe   Szegedy   
DSN  Lee et al   
Maxout  Goodfellow et al   
WNN  Desjardins et al   
preGWNN
postGWNN
NIN  Lin et al   
NIN ALP  Agostinelli et al   
Normalization Propagation  Arpit et al   
BatchNorm  Ioffe   Szegedy   
DSN  Lee et al   
WNN  Desjardins et al   
preGWNN
postGWNN

SVHN
NIN  Lin et al   
NIN ALP  Agostinelli et al   
Normalization Propagation  Arpit et al   
BatchNorm  Ioffe   Szegedy   
DSN  Lee et al   
Maxout  Goodfellow et al   
WNN  Desjardins et al   
preGWNN
postGWNN

is

If
the error has no reduction over this period 
  cid 
increased by the
same rate as above  We
use postGWNN and follow experimental setting in
Sec  We take two different rates     as
examples  Fig  plots the
variations of dimensions
when       and shows
their test errors  We  nd
that keeping   cid  as   constant generally produces better
result than those obtained by the above strategy  but this
strategy yields less runtime because more dimensions are
pruned 

Figure   Effect of tuning   cid 

 dimensionsepochsrate cid 	 cid cid rate cid 	 cid cid Generalized Whitened Neural Network

Accelerating deep network training by reducing internal
covariate shift  In ICML   

Krizhevsky  Alex  Learning multiple layers of features

from tiny images  In Technical Report   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
Imagenet classi cation with deep convolutional neural
networks  In NIPS   

Lecun     Bottou     Bengio     and Haffner     GradientIn

based learning applied to document recognition 
Proceeding of IEEE   

LeCun  Yann  Bottou  Leon  Orr  Genevieve    and Mller 
Klaus Robert  Ef cient backprop  In Neural Networks 
Tricks of the Trade   

Lee  ChenYu  Xie  Saining  Gallagher  Patrick  Zhang 
Zhengyou  and Tu  Zhuowen  Deeplysupervised nets 
In AISTATS   

Lin  Min  Chen  Qiang  and Yan  Shuicheng  Network in

network  In ICLR   

Maas  Andrew    Hannun  Awni      and Ng  Andrew   
Recti er nonlinearities improve neural network acoustic
models  In ICML   

Netzer     Wang     Coates     Bissacco     Wu    
and Ng        Reading digits in natural images with
In NIPS Workshop on
unsupervised feature learning 
Deep Learning and Unsupervised Feature Learning 
 

Povey  Daniel  Zhang  Xiaohui  and Khudanpur  Sanjeev 
Parallel training of dnns with natural gradient and parameter averaging  In ICLR workshop   

Raiko  Tapani  Valpola  Harri  and LeCun  Yann  Deep
learning made easier by linear transformations in perceptrons  In AISTATS   

Sermanet  Pierre  Chintala  Soumith  and LeCun  Yann 
Convolutional neural networks applied to house numbers
digit classi cation  In arXiv   

Shamir  Ohad    stochastic pca and svd algorithm with an

exponential convergence rate  In ICML   

Tieleman  Tijmen and Hinton  Geoffrey  Rmsprop  Divide
the gradient by   running average of its recent magnitude  In Neural Networks for Machine Learning  Lecture
   

Zhang  Xiangyu  Zou  Jianhua  He  Kaiming  and Sun 
Jian  Accelerating very deep convolutional networks for
In IEEE Transactions on
classi cation and detection 
Pattern Analysis and Machine Intelligence   

