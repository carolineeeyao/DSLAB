PostInference Prior Swapping

Willie Neiswanger   Eric Xing  

Abstract

While Bayesian methods are praised for their
ability to incorporate useful prior knowledge  in
practice  convenient priors that allow for computationally cheap or tractable inference are commonly used  In this paper  we investigate the following question  for   given model  is it possible
to compute an inference result with any convenient false prior  and afterwards  given any target prior of interest  quickly transform this result into the target posterior    potential solution is to use importance sampling  IS  However  we demonstrate that IS will fail for many
choices of the target prior  depending on its parametric form and similarity to the false prior  Instead  we propose prior swapping    method that
leverages the preinferred false posterior to ef 
ciently generate accurate posterior samples under arbitrary target priors  Prior swapping lets us
apply lesscostly inference algorithms to certain
models  and incorporate new or updated prior information  postinference  We give theoretical
guarantees about our method  and demonstrate it
empirically on   number of models and priors 

  Introduction
There are many cases in Bayesian modeling where   certain choice of prior distribution allows for computationally
simple or tractable inference  For example 

  Conjugate priors yield posteriors with   known parametric form and therefore allow for noniterative  exact inference  Diaconis et al   

  Certain priors yield models with tractable conditional
or marginal distributions  which allows ef cient approximate inference algorithms to be applied      
Gibbs sampling  Smith   Roberts    sampling

 Carnegie Mellon University  Machine Learning Department 
Pittsburgh  USA  CMU School of Computer Science  Correspondence to  Willie Neiswanger  willie cs cmu edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

in collapsed models  Teh et al    or mean eld
variational methods  Wang   Blei   

  Simple parametric priors allow for computationally
cheap density queries  maximization  and sampling 
which can reduce costs in iterative inference algorithms       MetropolisHastings  Metropolis et al 
  gradientbased MCMC  Neal    or sequential Monte Carlo  Doucet et al   

For these reasons  one might hope to infer   result under  
convenientbut unrealistic prior  and afterwards  attempt to
correct the result  More generally  given an inference result
 under   convenient prior or otherwise  one might wish to
incorporate updated prior information  or see   result under different prior assumptions  without having to rerun  
costly inference algorithm 
This leads to the main question of this paper  for   given
model  is it possible to use any convenient false prior to
infer   false posterior  and afterwards  given any target prior
of interest  ef ciently and accurately infer the associated
target posterior 
One potential strategy involves sampling from the false
posterior and reweighting these samples via importance
sampling  IS  However  depending on the chosen target
prior both its parametric form and similarity to the false
prior the resulting inference can be inaccurate due to high
or in nite variance IS estimates  demonstrated in Sec   
We instead aim to devise   method that yields accurate inferences for arbitrary target priors  Furthermore  like IS  we
want to make use of the preinferred false posterior  without
simply running standard inference algorithms on the target
posterior  Note that most standard inference algorithms are
iterative and datadependent  parameter updates at each iteration involve data  and the computational cost or quality
of each update depends on the amount of data used  Hence 
running inference algorithms directly on the target posterior can be costly  especially given   large amount of data
or many target priors of interest  and defeats the purpose of
using   convenient false prior 
In this paper  we propose prior swapping  an iterative  dataindependent method for generating accurate posterior samples under arbitrary target priors  Prior swapping uses the
preinferred false posterior to perform ef cient updates that

PostInference Prior Swapping

do not depend on the data  and thus proceeds very quickly 
We therefore advocate breaking dif cult inference problems into two easier steps   rst  do inference using the most
computationally convenient prior for   given model  and
then  for all future priors of interest  use prior swapping 
In the following sections  we demonstrate the pitfalls of
using IS  describe the proposed prior swapping methods
for different types of false posterior inference results      
exact or approximate density functions  or samples  and
give theoretical guarantees for these methods  Finally  we
show empirical results on heavytailed and sparsity priors
in Bayesian generalized linear models  and relational priors
over components in mixture and topic models 
  Methodology
Suppose we have   dataset of   vectors xn               xn 
xi   Rp  and we have chosen   family of models with the
likelihood function   xn      xn  parameterized by
    Rd  Suppose we have   prior distribution over the
space of model parameters   with probability density function  PDF    The likelihood and prior de ne   joint
model with PDF    xn       xn  In Bayesian inference  we are interested in computing the posterior  conditional  distribution of this joint model  with PDF

  xn   

 cid     xn    

   xn 

 

 

Suppose we ve chosen   different prior distribution     
which we refer to as   false prior  while we refer to  
as the target prior  We can now de ne   new posterior

pf  xn   

 cid        xn    

      xn 

 

which we refer to as   false posterior 
We are interested in the following task  given   false posterior inference result       samples from pf  xn  or some
exact or approximate PDF  choose an arbitrary target prior
  and ef ciently sample from the associated target posterior   xn or  more generally  compute an expectation      Ep     for some test function    with respect to the target posterior 

  Importance Sampling and Prior Sensitivity
We begin by describing an initial strategy  and existing
work in   related task known as prior sensitivity analysis 
Suppose we have   false posterior samples     
    
pf  xn  In importance sampling  IS  samples from an
importance distribution are used to estimate the expectation of   test function with respect to   target distribution 
  straightforward idea is to use the false posterior as an

importance distribution  and compute the IS estimate

 IS
   

        

 

  cid 

  

 cid 

 cid 

Epf

 cid 

 cid 

     and

pf  xn     

where the weight function        xn 
the   weights are normalized to sum to one 
ISbased methods have been developed for the task of prior
sensitivity analysis  PSA  In PSA  the goal is to determine
how the posterior varies over   sequence of priors       over
  parameterized family of priors                    
Existing work has proposed inferring   single posterior under prior     and then using IS methods to infer further posteriors in the sequence  Besag et al    Hastings    Bornn et al   
This strategy is effective when subsequent priors are similar enough  but breaks down when two priors are suf 
ciently dissimilar  or are from illmatched parametric families  which we illustrate in an example below 
Note that  in general for IS  as        IS
       almost surely  However  IS estimates can still fail in practice
if  IS
  has high or in nite variance  If so  the variance of
the weights      will be large    problem often referred
to as weight degeneracy  which can lead to inaccurate estimates  In our case  the variance of  IS

  is only  nite if

  Ep

   

    
    

    
    

 
For   broad class of    this is satis ed if there exists      
           Geweke    Given some presuch that  
inferred pf  xn  with false prior      the accuracy of IS
thus depends on the target prior of interest  For example  if
  has heavier tails than      the variance of  IS
  will
be in nite for many    Intuitively  we expect the variance
to be higher for   that are more dissimilar to     
We show   concrete example of this in Fig    Consider  
normal model for data xn         with   standard normal false prior              This yields   closedform false posterior  due to the conjugate      which is also
normal  Suppose we   like to estimate the posterior expectation under   Laplace target prior  with mean   and variance   for test function              an estimate of the
target posterior mean  We draw   false posterior samples
     pf  xn  compute weights      and IS esti 
    
mate  IS
    slows signi cantly as  
We see in Fig    that       IS
increases  and maintains   high error even as   is made
very large  We can analyze this issue theoretically  Suph       Since we know pf  xn 
pose we want       IS
is normal  we can compute   lower bound on the number of false posterior samples   that would be needed for

    and compare it with the true expectation    

PostInference Prior Swapping

Figure   Importance sampling with false posterior samples  As the number of samples   grows  the difference between the IS estimate
 IS
  and the true value    decreases increasingly slowly  The difference remains large even when       See text for analysis 

the expected estimate to be within   of     Namely  if
pf  xn             in order for      Epf  IS
       
we   need

 cid   
               

 cid 

    exp

 

In the example in Fig    we have              and
       Hence  for      Epf  IS
        we   need
      samples  see appendix for full details of this
analysis  Note that this bound actually has nothing to do
with the parametric form of  it is based solely on the
normal false posterior  and its distance to the target posterior mean     However  even if this distance was small 
the importance estimate would still have in nite variance
due to the Laplace target prior  Further  note that the situation can signi cantly worsen in higher dimensions  or if
the false posterior has   lower variance 

  Prior Swapping

We   like   method that will work well even when false
and target priors      and   are signi cantly different 
or are from different parametric families  with performance
that does not worsen  in accuracy nor computational complexity  as the priors are made more dissimilar 
Redoing inference for each new target posterior can be
very costly  especially when the data size   is large  because the periteration cost of most standard inference algorithms scales with    and many iterations may be needed
for accurate inference  This includes both MCMC and sequential monte carlo  SMC  algorithms       repeatedIS 
methods that infer   sequence of distributions  In SMC 
the periteration cost still scales with    and the variance
estimates can still be in nite if subsequent distributions are
illmatched 
Instead  we aim to leverage the inferred false posterior to
moreef ciently compute any future target posterior  We
begin by de ning   prior swap density ps  Suppose for
now that   false posterior inference algorithm has returned
  density function  pf    we will give more details on  pf

later  assume for now that it is either equal to pf  xn  or
approximates it  We then de ne the prior swap density as

ps     pf  
    

 

 

Note that if  pf     pf  xn  then ps      xn 
However  depending on how we represent  pf   ps  can
have   much simpler analytic representation than   xn 
which is typically de ned via   likelihood function        
function of the data  and causes inference algorithms to
have costs that scale with the data size    Speci cally  we
will only use lowcomplexity  pf   that can be evaluated
in constant time with respect to the data size   
Our general strategy is to use ps  as   surrogate for
  xn  in standard MCMC or optimization procedures  to
yield dataindependent algorithms with constant cost per iteration  Intuitively  the likelihood information is captured
by the false posterior we make use of this instead of the
likelihood function  which is costly to evaluate 
More concretely  at each iteration in standard inference algorithms  we must evaluate   datadependent function associated with the posterior density  For example  we evaluate
  function proportional to   xn  in MetropolisHastings
 MH   Metropolis et al    and   log   xn  in
gradientbased MCMC methods  such as Langevin dynamics  LD   Rossky et al    and Hamiltonian Monte
Carlo  HMC   Neal    and in optimization procedures
that yield   MAP point estimate  In prior swapping  we instead evaluate ps  in MH  or   log ps  in LD  HMC 
or gradient optimization to   MAP estimate  see appendix
for algorithm pseudocode  Here  each iteration only requires evaluating   few simple analytic expressions  and
thus has    complexity with respect to data size 
We demonstrate prior swapping on our previous example
 using   normal false prior and Laplace target prior  in
Fig    where we have   closedform  normal PDF   pf  
To do prior swapping  we run   MetropolisHastings algorithm on the target density ps  Note that drawing each

 PDFFalse posterior samples  PostInference Prior Swapping

Figure   Using prior swapping to compute estimate  PS

  by drawing samples     

     ps 

sample in this Markov chain does not involve the data xn 
and can be done in constant time with respect to    which
we can see by viewing the wall time for different     In
     ps  compute  
Fig    we draw   samples     
sample estimate  PS
       and compare it with
the true value     We see that  PS
  converges to    after  
relatively small number of samples   

 cid  

     
 

  Prior Swapping with False Posterior Samples

The previous method is only applicable if our false posterior inference result is   PDF  pf    such as in closedform
inference or variational approximations  Here  we develop
prior swapping methods for the setting where we only have
access to samples    Tf
     pf  xn  We propose the
following procedure 
  Use    Tf
  Sample from ps       pf  

   to form an estimate  pf     pf  xn 
     with prior swapping 

as before 

Note that  in general  ps  only approximates   xn  As
   nal step  after sampling from ps  we can 
  Apply   correction to samples from ps 

We will describe two methods for applying   correction to
ps samples one involving importance sampling  and one
involving semiparametric density estimation  Additionally 
we will discuss forms for  pf   guarantees about these
forms  and how to optimize the choice of  pf   In particular  we will argue why  in constrast to the initial IS strategy 
these methods do not fail when   xn  and pf  xn  are
very dissimilar or have illmatching parametric forms 

Prior swap importance sampling  Our  rst proposal for
applying   correction to prior swap samples involves IS  aft    ps 
ter estimating some  pf   and sampling     
we can treat     
   as importance samples  and compute
the IS estimate

 PSis
   

        

 

  cid 

  

where the weight function is now

       xn 

  pf  xn 
and the weights are normalized so that cid  

 pf  

ps 

 

           

The key difference between this and the previous IS strategy is the weight function  Recall that  previously  an accurate estimate depended on the similarity between   and
     both the distance to and parametric form of  
could produce high or in nite variance estimates  This was
an issue because we wanted the procedure to work well for
any   Now  however  the performance depends on the
similarity between  pf   and pf  xn and by using the
false posterior samples  we can estimate    pf   that well
approximates pf  xn  Additionally  we can prove that
certain choices of  pf   guarantee    nite variance IS estimate  Note that the variance of  PSis

is only  nite if

 cid 
   pf  xn 

 cid 

 pf  

Epf

 

 cid 
   pf  xn 

 cid 

 pf  

  Ep

   

To bound this  it is suf cient to show that there exists
      such that pf  xn 
 pf       for all    assuming  
test function    with  nite variance   Geweke    To
satisfy this condition  we will propose   certain parametric
    Note that  to maintain   prior swapping profamily    
    that can be evalucedure with    cost  we want      
ated in constant time  In general       
    with fewer terms
will yield   faster procedure  With these in mind  we propose the following family of densities 
De nition  For   parameter                        Rp 
      let density    

    satisfy

          
   

       

 

  

where      denotes the model conditional PDF 
The number of terms in    
     and cost to evaluate  is
determined by the parameter    Note that this family is

  cid 

 PDFPrior swapping samples  PostInference Prior Swapping

inspired by the true form of the false posterior pf  xn 
    has constanttime evaluation  and we
However     
can estimate its parameter   using samples    Tf
    
pf  xn  Furthermore  we have the following guarantees 
Theorem   For any                     Rp and      
let    
    be de ned as in Eq    Then  there exists      
such that pf  xn 
         for all     Rd 
 cid 
Corollary   For     
       
         
pf    xn 
       pf    xn 
  and test function
   
     
that satis es Varp         the variance of IS estimate
 PSis

   cid  

            is  nite 

 cid cid  

   
    

   
     

  

   

 

Proofs for these theorems are given in the appendix 
Note that we do not know the normalization constant for
    This is not an issue for its use in prior swapping 
   
since we only need access to   function proportional to
         
       in most MCMC algorithms 
  
However  we still need to estimate   which is an issue because the unknown normalization constant is   function of
  Fortunately  we can use the method of score matching
 Hyv arinen    to estimate   given   density such as
    with unknown normalization constant 
   
Once we have found an optimal parameter   we draw
       compute
samples from   
weights for these samples  Eq    and compute the IS
estimate  PSis
    We give pseudocode for the full prior swap
importance sampling procedure in Alg   

         

Algorithm   Prior Swap Importance Sampling
Input  False posterior samples    Tf
Output  IS estimate  PSis
   
  Score matching  estimate   using    Tf
  
  Prior swapping  sample     
  Importance sampling  compute  PSis

   cid  
         

       

     pf  xn 

   

 
           

    

swapping 

Semiparametric prior
In the previous
method  we chose   parametric form for    
    in general 
even the optimal   will yield an inexact approximation to
pf  xn  Here  we aim to incorporate methods that return
an increasingly exact estimate  pf   when given more
false posterior samples    Tf
  
One idea is to use   nonparametric kernel density estimate
      
 pnp
    and plug this into pnp
However  nonparametric density estimates can yield inaccurate density tails and fare badly in high dimensions  To
help mitigate these problems  we turn to   semiparametric estimate  which begins with   parametric estimate  and

       pnp

adjusts it as samples are generated  In particular  we use
  density estimate that can be viewed as the product of  
parametric density estimate and   nonparametric correction function  Hjort   Glad    This density estimate
is consistent as the number of samples Tf     Instead
of  or in addition to  correcting prior swap samples with
importance sampling  we can correct them by updating the
nonparametric correction function as we continue to generate false posterior samples 
Given Tf samples    Tf
parametric false posterior estimate as

     pf  xn  we write the semi 

 cid cid   cid   cid 

 cid     

 cid 

 psp
     

 
Tf

 
bd  

 

   
     
   

 

 

where   denotes   probability density kernel  with bandwidth    where       as Tf      see  Wasserman   
for details on probability density kernels and bandwidth selection  The semiparametric prior swap density is then
       psp
psp

   
   

 cid 

 

 

   
    

        bd
   

 cid 

Tf cid 

  

Tf cid 
 cid cid   cid 

  

 

 cid cid   cid 
 cid 
   

 

     
   

 

   

Tf

 
Tf

Tf cid 

  

     

   

 

Hence  the prior swap density psp
    is proportional to the
product of two densities  the parametric prior swap density
    and   correction density  To estimate expectations
  
    we can follow Alg    as before  but
with respect to psp
replace the weight function in the  nal IS estimate with

 cid cid   cid 

 cid 

Tf cid 

  

 

     psp
   
  
   

   
Tf

 

     
   

 

 

One advantage of this strategy is that computing the
weights doesn   require the data it thus has constant cost
with respect to data size    though its cost does increase
with the number of false posterior samples Tf   Additionally  as in importance sampling  we can prove that this procedure yields an exact estimate of      asymptotically 
as Tf      and we can provide an explicit bound on the
    converges to   xn  We do this by
rate at which psp
    is consistent for   xn 
showing that psp
    
Theorem   Given false posterior samples    Tf
pf  xn  and    cid   
  is consis 
 cid cid 
 cid 
tent for   xn       its meansquared error satis es
 

  the estimator psp

   
 

        xn    
 psp

 

 

sup
  xn 

     
 

for some       and          
The proof for this theorem is given in the appendix 

PostInference Prior Swapping

  Empirical Results
We show empirical results on Bayesian generalized linear models  including linear and logistic regression  with
sparsity and heavy tailed priors  and on latent factor models  including mixture models and topic models  with relational priors over factors      
diversityencouraging 
agglomerateencouraging  etc  We aim to demonstrate
empirically that prior swapping ef ciently yields correct
samples and  in some cases  allows us to apply certain inference algorithms to morecomplex models than was previously possible  In the following experiments  we will refer
to the following procedures 

  Target posterior inference  some standard inference
algorithm       MCMC  run on   xn 
  False posterior inference  some standard inference
algorithm run on pf  xn 
  False posterior IS  IS using samples from pf  xn 
  Prior swap exact  prior swapping with closedform
 pf     pf  xn 
  Prior swap parametric  prior swapping with para 

metric    

    given by Eq   

  Prior swap IS  correcting samples from    

    with

IS 

  Prior swap semiparametric 

correcting samples
    with the semiparametric estimate IS pro 

from    
cedure 

To assess performance  we choose   test function    and
compute the Euclidean distance between      Ep   
and some estimate    returned by   procedure  We denote
this performance metric by posterior error    cid        cid 
Since    is typically not available analytically  we run  
single chain of MCMC on the target posterior for one million steps  and use these samples as ground truth to compute     For timing plots  to assess error of   method at  
given time point  we collect samples drawn before this time
point  remove the  rst quarter as burn in  and add the time
it takes to compute any of the corrections 

  Sparsity Inducing and Heavy Tailed Priors in

Bayesian Generalized Linear Models

Sparsityencouraging regularizers have gained   high level
of popularity over the past decade due to their ability to produce models with greater interpretability and parsimony 
For example  the    norm has been used to induce sparsity
with great effect  Tibshirani    and has been shown
to be equivalent to   meanzero independent Laplace prior
 Tibshirani    Seeger    In   Bayesian setting  inference given   sparsity prior can be dif cult  and often requires   computationally intensive method  such as MH or

 

  

 with PDF VerySparse     cid  

HMC  or posterior approximations       expectation propagation  Minka    that make factorization or parametric assumptions  Seeger    Gerwinn et al    We
propose   cheap yet accurate solution   rst get an inference
result with   moretractable prior  such as   normal prior 
and then use prior swapping to quickly convert the result to
the posterior given   sparsity prior 
Our  rst set of experiments are on Bayesian linear regression models  which we can write as yi   Xi   
                         
For   we
compute results on Laplace  Student      and VerySparse
  exp   
 Seeger    priors  Here    normal    is conjugate
and allows for exact false posterior inference  Our second set of experiments are on Bayesian logistic regression models  which we write as yi   Bern pi  pi  
logistic Xi                which we will pair
with both heavy tailed priors and   hierarchical target prior
                Gamma    For these experiments  we also use   normal      However  this false prior
is no longer conjugate  and so we use MCMC to sample
from pf  xn 
For linear regression  we use the YearPredictionMSD data
set               in which regression is used to
predict the year associated with     song  and for logistic regression we use the MiniBooNE particle identi cation data
set               in which binary classi cation
is used to distinguish particles 
In Fig    we compare prior swapping and IS methods  in
order to show that the prior swapping procedures yield accurate posterior estimates  and to compare their speeds of
convergence  We plot posterior error vs  wall time for each
method   estimate of the posterior mean Ep      Ep 
for two sparsity target priors  Laplace and VerySparse 
for both linear and logistic regression  In linear regression
 only  since the normal conjugate    allows us to compute
  closed form pf  xn  we can run the prior swap exact
method  where  pf     pf  xn  However  we can also
sample from pf  xn  to compute    
    and therefore
compare methods such as prior swap parametric and the
two correction methods  In logistic regression  we do not
have   closed form pf  xn  here  we only compare the
methods that make use of samples from pf  xn  In Fig   
we see that the prior swapping methods  particularly prior
swap IS  quickly converge to nearly zero posterior error 
Additionally  in linear regression  we see that prior swap
parametric  using  pf        
    yields similar posterior
error as prior swap exact  which uses  pf       xn 

 https archive ics uci edu ml datasets 

YearPredictionMSD

 https archive ics uci edu ml datasets 

MiniBooNE particle identification

PostInference Prior Swapping

Figure   Comparison of prior swapping and IS methods for Bayesian linear and logistic regression under Laplace and VerySparse target
priors  The prior swapping methods  particularly prior swap exact and prior swap IS  quickly converge to low posterior errors 

Figure   Prior swapping for fast inference in Bayesian linear models with sparsity and heavytailed priors   ab  Convergence plots
showing that prior swapping performs accurate inference faster than the comparison methods and is robust to changing       Inferred
   density marginals when prior sparsity is increased      Prior swapping results for   variety of different sparsity priors 

In Fig    we show how prior swapping can be used for
fast inference in Bayesian linear models with sparsity or
heavytailed priors  We plot the time needed to  rst compute the false posterior  via exact inference  and then run
prior swapping  via the MH procedure  on some target posterior  and compare this with the MH algorithm run directly
on the target posterior  In     and     we show convergence
plots and see that prior swapping performs faster inference
 by   few orders of magnitude  than direct MH  In plot
    we reduce the variance of the target prior  while this
hurts the accuracy of false posterior IS  prior swapping still
quickly converges to zero error  In     we show    density
marginals as we increase the prior sparsity  and in     we
show prior swapping results for various sparsity priors 
In the appendix  we also include results on logistic regression with the hierarchical target prior  as well as results for
synthetic data where we are able to compare timing and
posterior error as we tune   and   

  Priors over Factors in Latent Variable Models

Many latent variable models in machine learning such as
mixture models  topic models  probabilistic matrix factorization  and others involve   set of latent factors      
components or topics  Often  we   like to use priors
that encourage interesting behaviors among the factors 
For example  we might want dissimilar factors through
  diversitypromoting prior  Kwok   Adams    Xie
et al    or for the factors to show some sort of sparsity pattern  Mayrink et al    Knowles   Ghahramani 
  Inference in such models is often computationally
expensive or designed on   caseby case basis  Xie et al 
  Knowles   Ghahramani   
However  when conjugate priors are placed over the factor
parameters  collapsed Gibbs sampling can be applied  In
this method  the factor parameters are integrated out  leaving only   subset of variables  on these  the conditional

              False posterior IS               Prior swap exact               Prior swap parametric               Prior swap IS              Prior swap SPPosterior errorPosterior errorPosterior errorPosterior errorWall time    Wall time    Wall time    Wall time    Bayesian Linear RegressionBayesian Logistic Regression          Normal  Student      Laplace  Laplace  VerySparse  VerySparse PDFDimensions   and  Dimensions   and  PDFPDF       Laplace        Laplace         Laplace Dimension          False posterior inference  exact Target posterior inference  MCMC False posterior ISPrior swap  exact Wall time    Posterior errorWall time    Posterior error Application  fast inference in Bayesian linear regressionPostInference Prior Swapping

Figure   Latent factor models      Prior swapping results for relational target priors  de ned in     over components in   mixture
model      Prior swapping with   diversitypromoting target prior on an LDA topic model  Simple English Wikipedia corpus  to separate
redundant topic clusters  the top   words per topic are shown  In        we show wall times for the initial inference and prior swapping 

distributions can be computed analytically  which allows
for Gibbs sampling over these variables  Afterwards  samples of the collapsed factor parameters can be computed 
Hence  we propose the following strategy   rst  assign  
prior for the factor parameters that allows for collapsed
Gibbs sampling  afterwards  reconstruct the factor samples
and apply prior swapping for more complex relational priors over the factors  We can thus perform convenient inference in the collapsed model  yet apply moresophisticated
priors to variables in the uncollapsed model 
We  rst show results on   Gaussian mixture model  GMM 
    
written xi      zi   zi  zi   Dir      
          Using   normal    over     
   allows
for collapsed Gibbs sampling  We also show results on  
topic model  latent Dirichlet allocation  LDA   Blei et al 
  for text data  for the form of this model  see  Blei
et al    Wang   Blei    Here  using   Dirichlet
   over topics allows for collapsed Gibbs sampling  For
mixture models  we generate synthetic data from the above
model           and for topic models  we
use the Simple English Wikipedia  corpus     documents  vocab  words  and set    topics 

 https simple wikipedia org 

In Fig    we show results for mixture and topic models 
In     we show inferred posteriors over GMM components
for   number of relational target priors  which we de ne
in    
In     we apply the diversitypromoting target
prior to LDA  to separate redundant topics  Here  we show
two topic clusters  geography  and  family  in pf  xn 
which are separated into distinct  yet thematicallysimilar 
topics after prior swapping  In     and     we also show
wall times of the inference methods 
  Conclusion
Given some false posterior inference result  and an arbitrary target prior  we have studied methods to accurately
compute the associated target posterior  or expectations
with respect to it  and to do this ef ciently by leveraging the preinferred result  We have argued and shown empirically that this strategy is effective even when the false
and target posteriors are quite dissimilar  We believe that
this strategy shows promise to allow   wider range of  and
possibly lesscostly  inference alorithms to be applied to
certain models  and to allow updated or new prior information to be moreeasily incorporated into models without
reincurring the full costs of standard inference algorithms 

     Collapsed GibbsPriorSwappingWall time  seconds     southern  northern  region  western  eastern southTopic  Topic Model  False Posterior via Collapsed GibbsCluster GeographyCluster Family  west  south  coast  north  east westernTopic   north  asia  south  western  southern easternTopic   father  family  brother  born  son  childrenTopic   children  daughter  born  son  family  fatherTopic   born  died  father  years  family  livedTopic   born  parents  studied  moved  age  yearTopic   north  west  east  south  eastern westernTopic   Topic Model  Target Posterior via Prior Swapping  brother  sister  younger  older  youngest  sistersTopic   husband  marriage  wife  death  marry  childrenTopic   school  college  graduated  studies  university  fellowTopic  Topic   important  stayed  wrote  travelled  started  died  territory  region  regions  provinces  capital territoriesTopic  Territories  bay  south  coast  area  land seaTopic  Coast  america  europe  asia  world  countries africaTopic  Countries  side  east  bordered  west  middle borderTopic  BorderSiblingsMarriageBiographySchoolPrior swapping for diverse topics          Diverse   Relational target priors  over factors              Chain  SparseChain  Origin Diverse  SparseAgglom FalsePosterior  Origin  Agglom  Diverse  SparseOrigin Wall time  seconds Mixture Model  False Posterior via Collapsed Gibbs  Target Posterior via Prior SwappingPostInference Prior Swapping

References
Besag 

Julian  Green  Peter  Higdon  David 

and
Mengersen  Kerrie  Bayesian computation and stochastic systems  Statistical science  pp     

Blei  David    Ng  Andrew    and Jordan  Michael    Latent dirichlet allocation  The Journal of Machine Learning Research     

Bornn  Luke  Doucet  Arnaud  and Gottardo  Raphael 
An ef cient computational approach for prior sensitivity analysis and crossvalidation  Canadian Journal of
Statistics     

Diaconis  Persi  Ylvisaker  Donald  et al  Conjugate priors
for exponential families  The Annals of statistics   
   

Doucet  Arnaud  Godsill  Simon  and Andrieu  Christophe 
On sequential monte carlo sampling methods for
bayesian  ltering  Statistics and computing   
   

Gerwinn  Sebastian  Macke 

Jakob    and Bethge 
Matthias  Bayesian inference for generalized linear models for spiking neurons  Frontiers in Computational Neuroscience     

Geweke  John  Bayesian inference in econometric models
using monte carlo integration  Econometrica  Journal of
the Econometric Society  pp     

Hastings    Keith  Monte carlo sampling methods using
markov chains and their applications  Biometrika   
   

Hjort  Nils Lid and Glad  Ingrid    Nonparametric density
estimation with   parametric start  The Annals of Statistics  pp     

Hyv arinen  Aapo  Estimation of nonnormalized statistical
models by score matching  Journal of Machine Learning
Research   Apr   

Knowles  David and Ghahramani  Zoubin  Nonparametric
bayesian sparse factor models with application to gene
expression modeling  The Annals of Applied Statistics 
pp     

Kwok  James   and Adams  Ryan    Priors for diversity in
generative latent variable models  In Advances in Neural
Information Processing Systems  pp     

Mayrink  Vinicius Diniz  Lucas  Joseph Edward  et al 
Sparse latent factor models with interactions  Analysis
of gene expression data  The Annals of Applied Statistics     

Metropolis  Nicholas  Rosenbluth  Arianna    Rosenbluth 
Marshall    Teller  Augusta    and Teller  Edward 
Equation of state calculations by fast computing machines  The journal of chemical physics   
   

Minka  Thomas    Expectation propagation for approxIn Proceedings of the Sevimate bayesian inference 
enteenth conference on Uncertainty in arti cial intelligence  pp    Morgan Kaufmann Publishers Inc 
 

Neal     MCMC using hamiltonian dynamics  Handbook

of Markov Chain Monte Carlo  pp     

Rossky  PJ  Doll  JD  and Friedman  HL  Brownian dynamics as smart monte carlo simulation  The Journal of
Chemical Physics     

Seeger  Matthias    Bayesian inference and optimal design for the sparse linear model  The Journal of Machine
Learning Research     

Smith  Adrian FM and Roberts  Gareth    Bayesian computation via the gibbs sampler and related markov chain
monte carlo methods  Journal of the Royal Statistical
Society  Series    Methodological  pp     

Teh  Yee    Newman  David  and Welling  Max    collapsed variational bayesian inference algorithm for latent
dirichlet allocation  In Advances in neural information
processing systems  pp     

Tibshirani  Robert  Regression shrinkage and selection via
the lasso  Journal of the Royal Statistical Society  Series
   Methodological  pp     

Wang  Chong and Blei  David    Collaborative topic modeling for recommending scienti   articles  In Proceedings of the  th ACM SIGKDD international conference
on Knowledge discovery and data mining  pp   
ACM   

Wang  Chong and Blei  David    Variational inference in
nonconjugate models  The Journal of Machine Learning
Research     

Wasserman  Larry 

All of nonparametric statistics 

Springer Science   Business Media   

Xie  Pengtao  Zhu  Jun  and Xing  Eric  Diversitypromoting bayesian learning of latent variable models 
In Proceedings of the  st International Conference on
Machine Learning  ICML   

