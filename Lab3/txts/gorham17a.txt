Measuring Sample Quality with Kernels

Jackson Gorham   Lester Mackey  

Abstract

Approximate Markov
chain Monte Carlo
 MCMC  offers the promise of more rapid sampling at the cost of more biased inference  Since
standard MCMC diagnostics fail to detect these
biases  researchers have developed computable
Stein discrepancy measures that provably determine the convergence of   sample to its
target distribution  This approach was recently
combined with the theory of reproducing kernels
to de ne   closedform kernel Stein discrepancy
 KSD  computable by summing kernel evaluations across pairs of sample points  We develop
  theory of weak convergence for KSDs based
on Stein   method  demonstrate that commonly
used KSDs fail to detect nonconvergence even
for Gaussian targets  and show that kernels with
slowly decaying tails provably determine convergence for   large class of target distributions 
The resulting convergencedetermining KSDs
are suitable for comparing biased  exact  and
deterministic sample sequences and simpler to
compute and parallelize than alternative Stein
discrepancies  We use our tools to compare biased samplers  select sampler hyperparameters 
and improve upon existing KSD approaches
to onesample hypothesis testing and sample
quality improvement 

  Introduction
When Bayesian inference and maximum likelihood estimation  Geyer    demand the evaluation of intractable ex 

pectations EP                   dx under   target dis 

tribution     Markov chain Monte Carlo  MCMC  methods
 Brooks et al    are often employed to approximate
these integrals with asymptotically correct sample aver 

 Stanford University  Palo Alto  CA USA  Microsoft Research New England  Cambridge  MA USA  Correspondence
to  Jackson Gorham  jgorham stanford edu  Lester Mackey
 lmackey microsoft com 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

nPn

ages EQn         
     xi  However  many exact
MCMC methods are computationally expensive  and recent
years have seen the introduction of biased MCMC procedures  see       Welling   Teh    Ahn et al    Korattikara et al    that exchange asymptotic correctness
for increased sampling speed 
Since standard MCMC diagnostics  like mean and trace
plots  pooled and withinchain variance measures  effective
sample size  and asymptotic variance  Brooks et al   
do not account for asymptotic bias  Gorham   Mackey
  de ned   new family of sample quality measures
  the Stein discrepancies   that measure how well EQn
approximates EP while avoiding explicit integration under
    Gorham   Mackey   Mackey   Gorham  
Gorham et al    further showed that speci   members of this family   the graph Stein discrepancies   were
    ef ciently computable by solving   linear program and
    convergencedetermining for large classes of targets    
Building on the zero mean reproducing kernel theory of
Oates et al      Chwialkowski et al    and Liu
et al    later showed that other members of the Stein
discrepancy family had   closedform solution involving
the sum of kernel evaluations over pairs of sample points 
This closed form represents   signi cant practical advantage  as no linear program solvers are necessary  and the
computation of the discrepancy can be easily parallelized 
However  as we will see in Section   not all kernel Stein
discrepancies are suitable for our setting  In particular  in
dimension       the kernel Stein discrepancies previously
recommended in the literature fail to detect when   sample is not converging to the target  To address this shortcoming  we develop   theory of weak convergence for the
kernel Stein discrepancies analogous to that of  Gorham  
Mackey    Mackey   Gorham    Gorham et al 
  and design   class of kernel Stein discrepancies that
provably control weak convergence for   large class of target distributions 
After formally describing our goals for measuring sample quality in Section   we outline our strategy  based
on Stein   method  for constructing and analyzing practical
quality measures at the start of Section   In Section  
we de ne our family of closedform quality measures   the
kernel Stein discrepancies  KSDs    and establish several

Measuring Sample Quality with Kernels

 

 

 

appealing practical properties of these measures  We analyze the convergence properties of KSDs in Sections  
and   showing that previously proposed KSDs fail to detect nonconvergence and proposing practical convergencedetermining alternatives  Section   illustrates the value
of convergencedetermining kernel Stein discrepancies in
  variety of applications  including hyperparameter selection  sampler selection  onesample hypothesis testing  and
sample quality improvement  Finally  in Section   we conclude with   discussion of related and future work 
Notation We will use   to denote   generic probability measure and   to denote the weak convergence of  
sequence of probability measures  We will use   kr for
      to represent the    norm on Rd and occasionally refer to   generic norm     with associated dual norm
kak    supb Rd kbk  ha  bi for vectors     Rd  We
let ej be the jth standard basis vector  For any function
    Rd   Rd  we de ne        supx Rdkg     
       supx ykg            kx   yk  and rg as
the gradient with components  rg   jk   rxk gj    We
further let     Cm indicate that   is   times continuindicate that     Cm
ously differentiable and     Cm
and rlg is vanishing at in nity for all                 
We de ne         respectively        
and       
 
to be the set of functions     Rd   Rd     with
yk       continuous  respectively  contin 
         rl
uous and uniformly bounded  continuous and vanishing at
in nity  for all                 
  Quality measures for samples
Consider   target distribution   with continuously differentiable  Lebesgue  density   supported on all of Rd  We
assume that the score function       log   can be evaluated  but that  for most functions of interest  direct integration under   is infeasible  We will therefore approximate integration under   using   weighted sample Qn  
Pn
   qn xi xi with sample points            xn   Rd and
qn   probability mass function  We will make no assumptions about the origins of the sample points  they may be
the output of   Markov chain or even deterministically generated 
Each Qn offers an approximation EQn       
expectation
EP       and our aim is to effectively compare the
quality of the approximation offered by any two samples
targeting    
In particular  we wish to produce   quality
measure that     identi es when   sequence of samples is
converging to the target   ii  determines when   sequence
of samples is not converging to the target  and  iii  is
ef ciently computable  Since our interest is in approx 

   qn xi   xi 

Pn

xrl

for

each

intractable

 No knowledge of the normalizing constant is needed 

imating expectations  we will consider discrepancies
quantifying the maximum expectation error over   class of
test functions   

dH Qn        sup

    EP         EQn     

 

When   is large enough  for any sequence of probability
measures       dH            only if          In
this case  we call   an integral probability metric  IPM 
   uller    For example  when     BLk          
is
Rd                      the IPM dBLk   
called the bounded Lipschitz or Dudley metric and exactly
metrizes convergence in distribution  Alternatively  when
    Wk           Rd               is the set of
 Lipschitz functions  the IPM dWk  
in   is known as the
Wasserstein metric 
An apparent practical problem with using the IPM dH as  
sample quality measure is that EP       may not be computable for        However  if   were chosen such that
EP           for all        then no explicit integration under   would be necessary  To generate such   class
of test functions and to show that the resulting IPM still
satis es our desiderata  we follow the lead of Gorham  
Mackey   and consider Charles Stein   method for
characterizing distributional convergence 

  Stein   method with kernels
Stein   method  Stein    provides   threestep recipe
for assessing convergence in distribution 

  Identify   Stein operator   that maps functions    
Rd   Rd from   domain   to realvalued functions
    such that

EP             for all       

For any such Stein operator and Stein set    Gorham
  Mackey   de ned the Stein discrepancy as
          sup
                dT         
which  crucially  avoids explicit integration under    
  Lower bound the Stein discrepancy by an IPM dH
known to dominate weak convergence  This can be
done once for   broad class of target distributions
to ensure that        whenever            
  for   sequence of probability measures      
 Desideratum  ii 

  Provide an upper bound on the Stein discrepancy ensuring that               under suitable convergence of    to    Desideratum    

Measuring Sample Quality with Kernels

While Stein   method is principally used as   mathematical tool to prove convergence in distribution  we seek 
in the spirit of  Gorham   Mackey    Gorham et al 
  to harness the Stein discrepancy as   practical tool
for measuring sample quality  The subsections to follow
develop   speci    practical instantiation of the abstract
Stein   method recipe based on reproducing kernel Hilbert
spaces  An empirical analysis of the Stein discrepancies
recommended by our theory follows in Section  

  Selecting   Stein operator and   Stein set
  standard  widely applicable univariate Stein operator is
the density method operator  see Stein et al    Chatterjee   Shao    Chen et al    Ley et al   
 
dx                            

           

    

Inspired by the generator method of Barbour    
and   otze   Gorham   Mackey   generalized this operator to multiple dimensions  The resulting
Langevin Stein operator
 TP         
    hr              hg            hr       
for functions     Rd   Rd was independently developed  without connection to Stein   method  by Oates et al 
    for the design of Monte Carlo control functionals  Notably  the Langevin Stein operator depends on  
only through its score function       log   and hence is
computable even when the normalizing constant of   is not 
While our work is compatible with other practical Stein operators  like the family of diffusion Stein operators de ned
in  Gorham et al    we will focus on the Langevin
operator for the sake of brevity 
Hereafter  we will let     Rd   Rd     be the reproducing
kernel of   reproducing kernel Hilbert space  RKHS  Kk
of functions from Rd      That is  Kk is   Hilbert space
of functions such that  for all     Rd            and
        hf      iKk whenever         We let   kKk be
the norm induced from the inner product on Kk 
With this de nition  we de ne our kernel Stein set Gk    
as the set of vectorvalued functions                 gd  such
that each component function gj belongs to Kk and the vector of their norms kgjkKk
Gk                        gd     vk      for vj   kgjkKk 
The following result  proved in Section    establishes that
this is an acceptable domain for TP  
Proposition    Zero mean test functions  If       
and EP  kr log            then EP  TP          for
all            

belongs to the      unit ball 

 

 Our analyses and algorithms support each gj belonging to  

different RKHS Kkj   but we will not need that  exibility here 

The Langevin Stein operator and kernel Stein set together
de ne our quality measure of interest  the kernel Stein discrepancy  KSD    TP  Gk      When            this
de nition recovers the KSD proposed by Chwialkowski
et al    and Liu et al    Our next result shows
that  for any      the KSD admits   closedform solution 
Proposition    KSD closed form  Suppose       
and  for each                de ne the Stein kernel
        rxjryj                 

        
kj

 

 

   kj

     hkj

  bj   bj             bj   ryj        
  bj   rxj           rxjryj        
      

to evaluating each kj
     qn xi kj

       then   TP  Gk       
        with       iid   

IfPd
kwk where wj  qE kj
discrete measure Qn  Pn
qPn
Pd

The proof is found in Section    Notably  when   is the
   qn xi xi  the KSD reduces
  at pairs of support points as wj  
 xi  xi qn xi    computation which
is easily parallelized over sample pairs and coordinates   
Our Stein set choice was motivated by the work of Oates
et al      who used the sum of Stein kernels     
  to develop nonparametric control variates  Each
term wj in Proposition   can also be viewed as an instance
of the maximum mean discrepancy  MMD   Gretton et al 
  between   and   measured with respect to the Stein
kernel kj
  In standard uses of MMD  an arbitrary kernel
function is selected  and one must be able to compute expectations of the kernel function under     Here  this requirement is satis ed automatically  since our induced kernels are chosen to have mean zero under    
For clarity we will focus on the speci   kernel Stein set
choice Gk   Gk      for the remainder of the paper  but our
results extend directly to KSDs based on any      since all
KSDs are equivalent in   strong sense 
Proposition    Kernel Stein set equivalence  Under the assumptions of Proposition  
there are constants cd          depending only on   and    
such that cdS TP  Gk          TP  Gk     
   
  dS TP  Gk     
The short proof is found in Section   

  Lower bounding the kernel Stein discrepancy
We next aim to establish conditions under which the KSD
    TP  Gk      only if         Desideratum  ii 
Recently  Gorham et al    showed that the Langevin
graph Stein discrepancy dominates convergence in distribution whenever   belongs to the class   of distantly dissipative distributions with Lipschitz score function   

Measuring Sample Quality with Kernels

De nition    Distant dissipativity  Eberle    Gorham
et al      distribution   is distantly dissipative if
    lim inf            for

      inf hb         yi

kx yk 

 

  kx   yk      

 

 

Examples of distributions in   include  nite Gaussian
mixtures with common covariance and all distributions
strongly logconcave outside of   compact set  including
Bayesian linear  logistic  and Huber regression posteriors
with Gaussian priors  see Gorham et al    Section  
Moreover  when       membership in   is suf cient
to provide   lower bound on the KSD for most common
kernels including the Gaussian  Mat ern  and inverse multiquadric kernels 
Theorem    Univariate KSD detects nonconvergence 
Suppose that      and                  for       
If
with   nonvanishing generalized Fourier transform 
      then     TP  Gk      only if         
The proof in Section   provides   lower bound on the
KSD in terms of an IPM known to dominate weak convergence  However  our next theorem shows that in higher
dimensions   Qn TP  Gk  can converge to   without the
sequence  Qn    converging to any probability measure 
This de ciency occurs even when the target is Gaussian 
Theorem    KSD fails with light kernel tails  Suppose
      
and de ne the kernel decay rate
      sup max        krxk        

 hrx ryk           kx   yk      
If               Id  and            for        
   
    then   Qn TP  Gk      does not imply Qn      
Theorem   implies that KSDs based on the commonly used
Gaussian kernel  Mat ern kernel  and compactly supported
kernels of Wendland   Theorem   all fail to deIn addition  KSDs
tect nonconvergence when      
based on the inverse multiquadric kernel                 
kx   yk 
  for     fail to detect nonconvergence
for any           The proof in Section   shows
that the violating sample sequences  Qn    are simple to
construct  and we provide an empirical demonstration of
this failure to detect nonconvergence in Section  
The failure of the KSDs in Theorem   can be traced to
their inability to enforce uniform tightness    sequence
of probability measures       is uniformly tight if for
every     there is    nite number    such that
lim supm    kXk           Uniform tightness
implies that no mass in the sequence of probability measures escapes to in nity  When the kernel   decays more
rapidly than the score function grows  the KSD ignores excess mass in the tails and hence can be driven to zero by  

 

nontight sequence of increasingly diffuse probability measures  The following theorem demonstrates uniform tightness is the missing piece to ensure weak convergence 
Theorem    KSD detects tight nonconvergence  Suppose
that      and                for        with   nonIf       is
vanishing generalized Fourier transform 
uniformly tight  then     TP  Gk      only if         
Our proof in Section   explicitly lower bounds the KSD
  TP  Gk  in terms of the bounded Lipschitz metric
      which exactly metrizes weak convergence 
dBLk  
Ideally  when   sequence of probability measures is not uniformly tight  the KSD would re ect this divergence in its
reported value  To achieve this  we consider the inverse
multiquadric  IMQ  kernel                 kx   yk 
 
for some     and       While KSDs based on IMQ
kernels fail to determine convergence when      by
Theorem   our next theorem shows that they automatically enforce tightness and detect nonconvergence whenever        
Theorem    IMQ KSD detects nonconvergence  Suppose      and                 kx   yk 
  for      
and         If     TP  Gk      then         
The proof in Section   provides   lower bound on the KSD
in terms of the bounded Lipschitz metric dBLk  
     
The success of the IMQ kernel over other common characteristic kernels can be attributed to its slow decay rate 
When      and the IMQ exponent     the function class TPGk contains unbounded  coercive  functions 
These functions ensure that the IMQ KSD     TP  Gk 
goes to   only if       is uniformly tight 
  Upper bounding the kernel Stein discrepancy
The usual goal in upper bounding the Stein discrepancy
is to provide   rate of convergence to   for particular
approximating sequences       Because we aim to
directly compute the KSD for arbitrary samples Qn  our
chief purpose in this section is to ensure that the KSD
    TP  Gk  will converge to zero when    is converging to    Desideratum    
Proposition    KSD detects convergence  If       
and   log   is Lipschitz with EP  kr log       
     
then     TP  Gk      whenever the Wasserstein distance dWk   
Proposition   applies to common kernels like the Gaussian 
Mat ern  and IMQ kernels  and its proof in Section   provides an explicit upper bound on the KSD in terms of the
Wasserstein distance dWk   
    xi for
iid     Liu et al    Thm    further implies that
xi
  Qn TP  Gk      TP  Gk  at an      rate under
continuity and integrability assumptions on  

           

nPn

  When Qn    

 

Measuring Sample Quality with Kernels

  Experiments
We next conduct an empirical evaluation of the KSD quality measures recommended by our theory  recording all
timings on an Intel Xeon CPU          GHz 
Throughout  we will refer to the KSD with IMQ base kernel                 kx   yk 
  exponent        
 
and       as the IMQ KSD  Code reproducing all experiments can be found on the Julia  Bezanson et al 
  package site https jgorham github io 
SteinDiscrepancy jl 

 kx     

        

 kx     

  Comparing discrepancies
Our  rst  simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures 
the Wasserstein distance dWk   
  which can be computed
for simple univariate targets  Vallender    and the
spanner graph Stein discrepancy of Gorham   Mackey
  We adopt   bimodal Gaussian mixture with       
    
  and       as our target
  and generate    rst sample point sequence        from the
target and   second sequence        from one component of
the mixture        Id  As seen in the left panel of
Figure   where       the IMQ KSD decays at an   
rate when applied to the  rst   points in the target sample
and remains bounded away from zero when applied to the
to the single component sample  This desirable behavior is
closely mirrored by the Wasserstein distance and the graph
Stein discrepancy 
The middle panel of Figure   records the time consumed
by the graph and kernel Stein discrepancies applied to the
       sample points from     Each method is given access to
  cores when working in   dimensions  and we use the released code of Gorham   Mackey   with the default
Gurobi   linear program solver for the graph Stein discrepancy  We  nd that the two methods have nearly identical runtimes when       but that the KSD is   to  
times faster when       In addition  the KSD is straightforwardly parallelized and does not require access to   linear program solver  making it an appealing practical choice
for   quality measure 
Finally  the right panel displays the optimal Stein functions  gj     
  recovered by
the IMQ KSD when       and       The associated
are
the meanzero functions under   that best discriminate the
target   and the sample Qn  As might be expected  the
optimal test function for the single component sample features large magnitude values in the oversampled region far
from the missing mode 

test functions         TP        Pd

EQn bj          rxj       

   EQn kj
  Qn TP  Gk 

  Qn TP  Gk 

     

nPn

  The importance of kernel choice
Theorem   established that kernels with rapidly decaying tails yield KSDs that can be driven to zero by offtarget sample sequences  Our next experiment provides
an empirical demonstration of this issue for   multivariate Gaussian target         Id  and KSDs based on
the popular Gaussian              kx yk 
  and Mat ern
                 kx   yk     kx yk  radial kernels 
Following the proof Theorem   in Section    we construct
an offtarget sequence  Qn    that sends   Qn TP  Gk 
to   for these kernel choices whenever       Speci cally 
for each    we let Qn    
    xi where  for all   and   
kxik         log   and kxi   xjk      log    To select
these sample points  we independently sample candidate
points uniformly from the ball      kxk         log   
accept any points not within   log   Euclidean distance of
any previously accepted point  and terminate when   points
have been accepted 
For various dimensions  Figure   displays the result of
applying each KSD to the offtarget sequence  Qn   
and an  ontarget  sequence of points sampled        from
    For comparison  we also display the behavior of the
IMQ KSD which provably controls tightness and dominates weak convergence for this target by Theorem   As
predicted  the Gaussian and Mat ern KSDs decay to   under
the offtarget sequence and decay more rapidly as the dimension   increases  the IMQ KSD remains bounded away
from  

  Selecting sampler hyperparameters
The approximate slice sampler of DuBois et al   
is   biased MCMC procedure designed to accelerate inference when the target density takes the form       
   QL
    yl    for     prior distribution on Rd and
 yl    the likelihood of   datapoint yl    standard slice
sampler must evaluate the likelihood of all   datapoints to
draw each new sample point xi  To reduce this cost  the
approximate slice sampler introduces   tuning parameter  
which determines the number of datapoints that contribute
to an approximation of the slice sampling step  an appropriate setting of this parameter is imperative for accurate inference  When   is too small  relatively few sample points will
be generated in   given amount of sampling time  yielding sample expectations with high Monte Carlo variance 
When   is too large  the large approximation error will produce biased samples that no longer resemble the target 
To assess the suitability of the KSD for tolerance parameter selection  we take as our target   the bimodal Gaussian
mixture model posterior of  Welling   Teh    For an
array of   values  we generated   independent approximate slice sampling chains with batch size   each with  

Discrepancy

 

IMQ KSD
Graph Stein
discrepancy
Wasserstein

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

Measuring Sample Quality with Kernels

       from mixture

target  

       from single
mixture component

     

     

 

 

 

   

 

 

 
 
 
 

 

 

 
 
 

 

 
 
 
 
 

 

 

 

 

 

 

 

 

 

   

     

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 

 
 
 
 
 
 
 

 

 

 

 

 
 
 
 

 

 
 
 

 

 

 

 

 

 

 

 
 
 
 
 
 
 

       

       

Number of sample points   

       

       

Number of sample points   

 

 

 

 
 
 
 

       from mixture

target  

       from single
mixture component

 

 
 
 
 
 
 

 

 

 

 

 

 

 

 

Figure   Left  For       comparison of discrepancy measures for samples drawn        from either the bimodal Gaussian mixture target
  or   single mixture component  see Section   Middle  Ontarget discrepancy computation time using   cores in   dimensions 
Right  For       and       the Stein functions   and discriminating test functions     TP   which maximize the KSD 

 
 
 
 
 
 
 
 
 
 

 

 

 

 
 
 

 

 
 

 
 
 
 
 

 

 

 

 

 

 

 

 

       from target  

Off target sample

 

     

     

     

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

     

     

     

 

     

     

     

Dimension
       
     
     

 
 
 
 
 
 
 

 

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

 
 
Number of sample points   

 

 

 

Figure   Gaussian and Mat ern KSDs are driven to   by an offtarget sequence that does not converge to the target         Id 
 see Section   The IMQ KSD does not share this de ciency 

budget of   likelihood evaluations  and plotted the
median IMQ KSD and effective sample size  ESS    standard sample quality measure based on asymptotic variance
 Brooks et al    in Figure   ESS  which does not
detect Markov chain bias  is maximized at the largest hyperparameter evaluated       while the KSD is minimized at an intermediate value       The right panel
of Figure   shows representative samples produced by several settings of   The sample produced by the ESSselected
chain is signi cantly overdispersed  while the sample from
      has minimal coverage of the second mode due to

its small sample size  The sample produced by the KSDselected chain best resembles the posterior target  Using  
cores  the longest KSD computation with       sample
points took    

  Selecting samplers
Ahn et al    developed two biased MCMC samplers
for accelerated posterior inference  both called Stochastic Gradient Fisher Scoring  SGFS  In the full version of
SGFS  termed SGFSf          matrix must be inverted to
draw each new sample point  Since this can be costly for
large    the authors developed   second sampler  termed
SGFSd  in which only   diagonal matrix must be inverted
to draw each new sample point  Both samplers can be
viewed as discretetime approximations to   continuoustime Markov process that has the target   as its stationary distribution  however  because no MetropolisHastings
correction is employed  neither sampler has the target as
its stationary distribution  Hence we will use the KSD    
quality measure that accounts for asymptotic bias   to evaluate and choose between these samplers 
Speci cally  we evaluate the SGFSf and SGFSd samples
produced in  Ahn et al    Sec    The target   is
  Bayesian logistic regression with    at prior  conditioned
on   dataset of   MNIST handwritten digit images  From
each image  the authors extracted   random projections of
the raw pixel values as covariates and   label indicating
whether the image was     or     After discarding the  rst
half of sample points as burnin  we obtained regression
coef cient samples with       points and       dimensions  including the intercept term  Figure   displays
the IMQ KSD applied to the  rst   points in each sample 
As external validation  we follow the protocol of Ahn et al 
  to  nd the bivariate marginal means and   con 
dence ellipses of each sample that align best and worst with
those of   surrogate ground truth sample obtained from  

Measuring Sample Quality with Kernels

            

     

       

     

       

ESS  higher is better 

 

 

 

 

 

KSD  lower is better 

 
 

 
 
 
 
 
 
 

 
       
Tolerance parameter   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
  

 
 
 
 

 

 

 

 
 
 
 
 
 
 
 
 

 

 

 

 
 
 
 
 
 
 
 

 

Figure   Left  Median hyperparameter selection criteria across   independent approximate slice sampler sample sequences  see Section   IMQ KSD selects       effective sample size selects       Right  Representative approximate slice sampler
samples requiring   likelihood evaluations with posterior equidensity contours overlaid    is the associated sample size 

Hamiltonian Monte Carlo chain with   iterates  Both the
KSD and the surrogate ground truth suggest that the moderate speedup provided by SGFSd    per sample vs 
   for SGFSf  is outweighed by the signi cant loss
in inferential accuracy  However  the KSD assessment does
not require access to an external trustworthy ground truth
sample  The longest KSD computation took    using  
cores 

  Beyond sample quality comparison
While our investigation of the KSD was motivated by the
desire to develop practical  trustworthy tools for sample
quality comparison  the kernels recommended by our theory can serve as dropin replacements in other inferential
tasks that make use of kernel Stein discrepancies 

 

  ONESAMPLE HYPOTHESIS TESTING
Chwialkowski et al 
recently used the KSD
  Qn TP  Gk  to develop   hypothesis test of whether  
given sample from   Markov chain was drawn from   target distribution    see also Liu et al    However  the
authors noted that the KSD test with their default Gaussian
base kernel   experienced   considerable loss of power as
the dimension   increased  We recreate their experiment
and show that this loss of power can be avoided by using
  and       Folour default IMQ kernel with        
lowing  Chwialkowski et al    Section   we draw
iid  Unif    to generate   sample
iid     Id  and ui
zi
   with xi   zi   ui    for       and various di 
 xi  
mensions    Using the authors  code  modi ed to include
an IMQ kernel  we compare the power of the Gaussian
KSD test  the IMQ KSD test  and the standard normality test of Baringhaus   Henze         to discern
whether the sample  xi 
   came from the null distribution
        Id  The results  averaged over   simula 

tions  are shown in Table   Notably  the IMQ KSD experiences no power degradation over this range of dimensions 
thus improving on both the Gaussian KSD and the standard
    normality tests 

Table   Power of one sample tests for multivariate normality  averaged over   simulations  see Section  

   

Gaussian

IMQ

  
 
 
 

  
 
 
 

  
 
 
 

  
 
 
 

  
 
 
 

  
 
 
 

hkx yk 

  IMPROVING SAMPLE QUALITY
Liu   Lee   recently used the KSD   Qn TP  Gk 
as   means of improving the quality of   sample  Speci 
cally  given an initial sample Qn supported on            xn 
they minimize     Qn TP  Gk  over all measures  Qn supported on the same sample points to obtain   new sample
that better approximates   over the class of test functions
    TPGk  In all experiments  Liu   Lee   employ
  Gaussian kernel               
  with bandwidth
  selected to be the median of the squared Euclidean distance between pairs of sample points  Using the authors 
code  we recreate the experiment from  Liu   Lee   
Fig      and introduce   KSD objective with an IMQ kernel                
  with bandwidth selected in the same fashion  The starting sample is given by
nPn
    xi for       various dimensions    and
Qn    
each sample point drawn        from         Id  For
the initial sample and the optimized samples produced by
each KSD  Figure   displays the mean squared error  MSE 
  averaged across   independently
dkEP          Qn
 
generated initial samples  Out of the box  the IMQ kernel
produces better mean estimates than the standard Gaussian 

hkx   yk 

     

Measuring Sample Quality with Kernels

   

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 

 

 

 

 
 
 

 

 
 

 
 
 
 
 
 
 
 

 

     

 

       

Sampler
  SGFS  
SGFS  

           
Number of sample points   

 
 
 

 
 
 

 
 
 

 
 
 
 

SGFS  

 

 
 
 
 

 
 
 

 
 
 
 
 

SGFS  

 

           

 

 

 

 
  

SGFS  

 

 
 
 
 

 
 
 
 

 
 
 

SGFS  

  

 

 
 
 
 
 

 
 
 
 
 

 

 

 

 
  

         

  

Figure   Left  Quality comparison for Bayesian logistic regression with two SGFS samplers  see Section   Right  Scatter plots of
          SGFS sample points with overlaid bivariate marginal means and   con dence ellipses  dashed blue  that align best and
worst with surrogate ground truth sample  solid red 

 

 

 

 

 

Sample

 

Initial Qn
Gaussian KSD
IMQ KSD

 

 

 

 

 

 

 

 
 
 

 

 
 
 
 
 
 
 
 
 
 

 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

   

 

Dimension   

 

 

Figure   Average quality of mean estimates   standard errors 
under optimized samples  Qn for target         Id  MSE averaged over   independent initial samples  see Section  

  Related and future work
The score statistic of Fan et al    and the Gibbs sampler convergence criteria of Zellner   Min   detect
certain forms of nonconvergence but fail to detect others
due to the  nite number of test functions tested  For example  when           the score statistic  Fan et al 
  only monitors sample means and variances 
For an approximation   with continuously differentiable
density    Chwialkowski et al    Thm 
  and
Liu et al    Prop    established that if   is   
universal  Carmeli et al    Defn    or integrally
strictly positive de nite  ISPD  Stewart    Sec    and
            kr log     
   kj
 
then   TP  Gk      only if         However  this property is insuf cient to conclude that probability measures
with small KSD are close to   in any traditional sense  Indeed  Gaussian and Mat ern kernels are    universal and
ISPD  but  by Theorem   their KSDs can be driven to zero
by sequences not converging to     On compact domains 

      for     Pd

      

where tightness is no longer an issue  the combined results
of  Oates et al      Lem     Fukumizu et al   
Lem    and  SimonGabriel   Sch olkopf    Thm   
give conditions for   KSD to dominate weak convergence 
While assessing sample quality was our chief objective  our
results may hold bene ts for other applications that make
use of Stein discrepancies or Stein operators  In particular  our kernel recommendations could be incorporated into
the Monte Carlo control functionals framework of Oates
et al      Oates   Girolami   the variational
inference approaches of Liu   Wang   Liu   Feng
  Ranganath et al    and the Stein generative
adversarial network approach of Wang   Liu  
In the future  we aim to leverage stochastic  lowrank  and
sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of
sample and data points while still guaranteeing control over
weak convergence    reader may also wonder for which
distributions outside of   the KSD dominates weak convergence  The following theorem  proved in Section    shows
that no KSD with      kernel dominates weak convergence
when the target has   bounded score function 
Theorem    KSD fails for bounded scores  If   log   is
bounded and       
  then   Qn TP  Gk      does
not imply Qn      
However  Gorham et al    developed convergencedetermining graph Stein discrepancies for heavytailed
targets by replacing the Langevin Stein operator TP
with diffusion Stein operators of the form          
    hr                        An analogous construction should yield convergencedetermining diffusion KSDs
for   outside of    Our results also extend to targets  
supported on   convex subset   of Rd by choosing   to
satisfy              for all   on the boundary of    

 

 

Measuring Sample Quality with Kernels

References
Ahn     Korattikara     and Welling     Bayesian posterior sampling via stochastic gradient Fisher scoring  In
Proc   th ICML  ICML   

Bachman     and Narici     Functional Analysis  Academic Press textbooks in mathematics  Dover Publications    ISBN  

Baker     Integration of radial functions  Mathematics Mag 

azine     

Barbour        Stein   method and Poisson process convergence     Appl  Probab   Special Vol     
  ISSN     celebration of applied probability 

Barbour        Stein   method for diffusion approximations  Probab  Theory Related Fields   
  ISSN   doi   BF 

Baringhaus     and Henze       consistent test for multivariate normality based on the empirical characteristic
function  Metrika     

Bezanson     Edelman     Karpinski     and Shah      
Julia    fresh approach to numerical computing  arXiv
preprint arXiv   

Brooks     Gelman     Jones     and Meng       Hand 

book of Markov chain Monte Carlo  CRC press   

Carmeli     De Vito     Toigo     and Umanit       Vector
valued reproducing kernel hilbert spaces and universality  Analysis and Applications     

Chatterjee     and Shao     Nonnormal approximation by
Stein   method of exchangeable pairs with application to
the CurieWeiss model  Ann  Appl  Probab   
    ISSN   doi   AAP 

Chen     Goldstein     and Shao     Normal approximation by Stein   method  Probability and its Applications 
Springer  Heidelberg    ISBN  
doi   

Fan     Brooks        and Gelman     Output assessment
for Monte Carlo simulations via the score statistic    
Comp  Graph  Stat     

Fukumizu     Gretton     Sun     and Sch olkopf     Kernel measures of conditional dependence  In NIPS  volume   pp     

Geyer        Markov chain Monte Carlo maximum likelihood  Computer Science and Statistics  Proc   rd
Symp  Interface  pp     

Gorham     and Mackey     Measuring sample quality with
Stein   method 
In Cortes     Lawrence        Lee 
      Sugiyama     and Garnett      eds  Adv  NIPS
  pp    Curran Associates  Inc   

Gorham     Duncan     Vollmer     and Mackey 
Measuring sample quality with diffusions 

  
arXiv  Nov   

  otze     On the rate of convergence in the multivariate

CLT  Ann  Probab     

Gretton     Borgwardt     Rasch     Sch olkopf     and
Smola       kernel twosample test     Mach  Learn 
Res     

Herb     and Sally Jr       The Plancherel formula  the
Plancherel theorem  and the Fourier transform of orbital
integrals 
In Representation Theory and Mathematical
Physics  Conference in Honor of Gregg Zuckerman  
 th Birthday  October     Yale University 
volume   pp    American Mathematical Soc   

Korattikara     Chen     and Welling     Austerity in
MCMC land  Cutting the MetropolisHastings budget 
In Proc  of  st ICML  ICML   

Ley     Reinert     and Swan     Stein   method for comparison of univariate distributions  Probab  Surveys   
    doi   PS 

Liu     and Feng     Two methods for wild variational

inference  arXiv preprint arXiv   

Chwialkowski     Strathmann     and Gretton       kernel test of goodness of     In Proc   rd ICML  ICML 
 

Liu     and Lee     Blackbox importance sampling 
arXiv  October   To appear in AISTATS  

DuBois     Korattikara     Welling     and Smyth    
Approximate slice sampling for Bayesian posterior inference  In Proc   th AISTATS  pp     

Liu     and Wang     Stein Variational Gradient Descent    General Purpose Bayesian Inference Algorithm  arXiv  August  

Eberle     Re ection couplings and contraction rates for
diffusions  Probab  Theory Related Fields  pp   
  doi     

Liu     Lee     and Jordan       kernelized Stein discrepIn Proc  of  rd ICML 

ancy for goodnessof   tests 
volume   of ICML  pp     

Measuring Sample Quality with Kernels

Vallender     Calculation of the Wasserstein distance
between probability distributions on the line  Theory
Probab  Appl     

Wainwright    

nonasymptotic viewpoint 
 www stat berkeley edu wainwrig 
nachdiplom Chap Sep pdf 

Highdimensional
 

statistics 
 
URL http 

Wang     and Liu     Learning to Draw Samples  With Application to Amortized MLE for Generative Adversarial
Learning  arXiv  November  

Welling     and Teh     Bayesian learning via stochastic

gradient Langevin dynamics  In ICML   

Wendland     Scattered data approximation  volume  

Cambridge university press   

Zellner     and Min     Gibbs sampler convergence crite 

ria  JASA     

Mackey     and Gorham     Multivariate Stein factors
for   class of strongly logconcave distributions  Electron  Commun  Probab    pp    doi   
 ECP 

  uller     Integral probability metrics and their generating
classes of functions  Ann  Appl  Probab   pp   
   

Oates     and Girolami     Control functionals for Quasi 

Monte Carlo integration  arXiv   

Oates     Cockayne     Briol     and Girolami     Convergence rates for   class of estimators based on steins
method  arXiv preprint arXiv     

Oates        Girolami     and Chopin     Control functionals for Monte Carlo integration 
Journal of the
Royal Statistical Society  Series    Statistical Methodology  pp              
doi 
 rssb 

ISSN  

Ranganath     Tran     Altosaar     and Blei     Operator
variational inference  In Advances in Neural Information
Processing Systems  pp     

SimonGabriel     and Sch olkopf     Kernel distribution embeddings  Universal kernels  characteristic kernels and kernel metrics on distributions  arXiv preprint
arXiv   

Sriperumbudur     On the optimal estimation of probability
measures in weak and strong topologies  Bernoulli   
   

Sriperumbudur     Gretton     Fukumizu     Sch olkopf 
   and Lanckriet     Hilbert space embeddings and metrics on probability measures     Mach  Learn  Res   
 Apr   

Stein       bound for the error in the normal approximation
to the distribution of   sum of dependent random variables  In Proc   th Berkeley Symposium on Mathematical Statistics and Probability  Univ  California  Berkeley  Calif    Vol  II  Probability theory  pp 
  Univ  California Press  Berkeley  Calif   

Stein     Diaconis     Holmes     and Reinert     Use
of exchangeable pairs in the analysis of simulations  In
Stein   method  expository lectures and applications 
volume   of IMS Lecture Notes Monogr  Ser  pp   
Inst  Math  Statist  Beachwood  OH   

Steinwart     and Christmann     Support Vector Machines 

Springer Science   Business Media   

Stewart     Positive de nite functions and generalizations 
an historical survey  Rocky Mountain    Math   
      doi   RMJ 

