 Convex Until Proven Guilty  DimensionFree Acceleration

of Gradient Descent on NonConvex Functions

Yair Carmon John    Duchi Oliver Hinder Aaron Sidford  

Abstract

We develop and analyze   variant of Nesterov  
accelerated gradient descent  AGD  for minimization of smooth nonconvex functions  We
prove that one of two cases occurs 
either
our AGD variant converges quickly  as if the
function was convex  or we produce   certi cate that the function is  guilty  of being
nonconvex  This nonconvexity certi cate allows us to exploit negative curvature and obtain deterministic  dimensionfree acceleration
of convergence for nonconvex functions  For
  function   with Lipschitz continuous gradient and Hessian  we compute   point   with
krf          in    log  gradient and
function evaluations  Assuming additionally that
the third derivative is Lipschitz  we require only
   log  evaluations 

  Introduction
Nesterov   seminal   accelerated gradient method has
inspired substantial development of  rstorder methods
for largescale convex optimization  In recent years  machine learning and statistics have seen   shift toward large
scale nonconvex problems  including methods for matrix
completion  Koren et al    phase retrieval  Cand es
et al    Wang et al    dictionary learning  Mairal
et al    and neural network training  LeCun et al 
 
In practice  techniques from accelerated gradient
methods namely  momentum can have substantial bene ts for stochastic gradient methods  for example  in training neural networks  Rumelhart et al    Kingma and
Ba    Yet little of the rich theory of acceleration for
convex optimization is known to transfer into nonconvex
optimization 

 Stanford University  Stanford  California  USA  Correspondence to  Yair Carmon  yairc stanford edu  Oliver Hinder
 ohinder stanford edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

Optimization becomes more dif cult without convexity  as
gradients no longer provide global information about the
function  Even determining if   stationary point is   local minimum is  generally  NPhard  Murty and Kabadi 
  Nesterov   
It is  however  possible to leverage nonconvexity to improve objectives in smooth optimization  moving in directions of negative curvature can
guarantee function value reduction  We explore the interplay between negative curvature  smoothness  and acceleration techniques  showing how an understanding of the
three simultaneously yields   method that provably accelerates convergence of gradient descent for   broad class of
nonconvex functions 

  Problem setting
We consider the unconstrained minimization problem

minimize

 

     

 

where     Rd     is smooth but potentially nonconvex 
We assume throughout the paper that   is bounded from
below  twotimes differentiable  and has Lipschitz continuous gradient and Hessian  In Section   we strengthen our
results under the additional assumption that   has Lipschitz continuous third derivatives  Following the standard
 rstorder oracle model  Nemirovski and Yudin    we
consider optimization methods that access only values and
gradients of    and not higher order derivatives  and we
measure their complexity by the total number of gradient
and function evaluations 
Approximating the global minimum of   to  accuracy is
generally intractable  requiring time exponential in   log  
 
Instead  we seek  
 Nemirovski and Yudin     
point   that is  approximately stationary  that is 

krf         

 

Finding stationary points is   canonical problem in nonlinear optimization  Nocedal and Wright    and while
saddle points and local maxima are stationary  excepting
pathological cases  descent methods that converge to   stationary point converge to   local minimum  Lee et al 
  Nemirovski     

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

If we assume   is convex  gradient descent satis es the
bound   after    gradient evaluations  and AGD improves this rate to    log  
     Nesterov    Without convexity  gradient descent is signi cantly worse  having worstcase complexity    Cartis et al   
More sophisticated gradientbased methods  including nonlinear conjugate gradient  Hager and Zhang    and LBFGS  Liu and Nocedal    provide excellent practical
performance  but their global convergence guarantees are
no better than    Our work  Carmon et al    and 
independently  Agarwal et al    break this   
barrier  obtaining the rate    log  
    Before we discuss this line of work in Section   we overview our contributions 

  Our contributions
 Convex until proven guilty  Underpinning our results
is the observation that when we run Nesterov   accelerated
gradient descent  AGD  on any smooth function    one of
two outcomes must follow 

    AGD behaves as though   was  strongly convex  sat 

isfying inequality   in    log  

    iterations 

    There exist points      in the AGD trajectory that prove

  is  guilty  of not being  strongly convex 

                rf               

 
  ku   vk   

 

The intuition behind these observations is that if inequality   never holds during the iterations of AGD  then  
 looks  strongly convex  and the convergence     follows 
In Section   we make this observation precise  presenting an algorithm to monitor AGD and quickly  nd the
witness pair      satisfying   whenever AGD progresses
more slowly than it does on strongly convex functions 
We believe there is potential to apply this strategy beyond AGD  extending additional convex gradient methods
to nonconvex settings 

An accelerated nonconvex gradient method In Section   we propose   method that iteratively applies our
monitored AGD algorithm to   augmented by   proximal
regularizer  We show that both outcomes     and     above
imply progress minimizing    where in case     we make
explicit use of the negative curvature that AGD exposes 
These progress guarantees translate to an overall  rstorder
      strict improvement
oracle complexity of    log  
over the    rate of gradient descent 
In Section  
we report preliminary experimental results  showing   basic implementation of our method outperforms gradient descent but not nonlinear conjugate gradient 

Improved guarantees with thirdorder smoothness As
we show in Section   assuming Lipschitz continuous third
derivatives instead of Lipschitz continuous Hessian allows us to increase the step size we take when exploiting negative curvature  making more function progress 
Consequently  the complexity of our method improves to
    While the analysis of the thirdorder setO  log  
ting is more complex  the method remains essentially unchanged  In particular  we still use only  rstorder information  never computing higherorder derivatives 

  Related work
Nesterov and Polyak   show that cubic regularization of Newton   method  nds   point that satis es the stationarity condition   in    evaluations of the Hessian  Given suf ciently accurate arithmetic operations   
Lipschitz continuous Hessian is approximable to arbitrary
precision using  nite gradient differences  and obtaining
  full Hessian requires      gradient evaluations    direct implementation of the NesterovPolyak method with
   rstorder oracle therefore has gradient evaluation complexity      improving on gradient descent only if
      which may fail in highdimensions 
In two recent papers  we  Carmon et al    and  independently  Agarwal et al  obtain better rates for  rstorder
methods  Agarwal et al    propose   careful implementation of the NesterovPolyak method  using accelerated methods for fast approximate matrix inversion  In our
earlier work  we employ   combination of  regularized  accelerated gradient descent and the Lanczos method  Both
 nd   point that satis es the bound   with probability at

  gradient and Hessian 

least       using    log  

vector product evaluations 
The primary conceptual difference between our approach
and those of Carmon et al  and Agarwal et al 
is that
we perform no eigenvector search  we automatically  nd
directions of negative curvature whenever AGD proves  
 guilty  of nonconvexity  Qualitatively  this shows that
explicit second orders information is unnecessary to improve upon gradient descent for stationary point computation  Quantitatively  this leads to the following improvements 

    Our result is dimensionfree and deterministic  with
complexity independent of the ratio    compared to
  dependence of previous works  This is sigthe log  
ni cant  as log  
  may be comparable to   log  
   

 ii  Our method uses only gradient evaluations  and does
not require Hessianvector products 
In practice 
Hessianvector products may be dif cult to implement and more expensive to compute than gradients 

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

 iii  Under thirdorder smoothness assumptions we im 
    rate  It is
prove our method to achieve    log  
unclear how to extend previous approaches to obtain
similar guarantees 

In distinction from the methods of Carmon et al    and
Agarwal et al    our method provides no guarantees
on positive de niteness of         if initialized at   saddle
point it will terminate immediately  However  as we further
explain in Section    we may combine our method with  
fast eigenvector search to recover the approximate positive
de niteness guarantee                 even improving
it to              using thirdorder smoothness  but
at the cost of reintroducing randomization  Hessianvector
products and   log  

  complexity term 

  Preliminaries and notation
Here we introduce notation and brie   overview de nitions
and results we use throughout  We index sequences by subscripts  and use xj
  as shorthand for xi  xi    xj  We use
                       and   to denote points in Rd  Additionally    denotes step sizes      denote desired accuracy   
denotes   scalar and     denotes the Euclidean norm on
Rd  We denote the nth derivative of   function          
by      We let log      max  log   
  function     Rd     has LnLipschitz nth derivative if
it is   times differentiable and for every    and unit vector
  the onedimensional function            satis es

               Ln     

Ln

 
  

 

    

nXi 

We refer to this property as nthorder smoothness  or simply smoothness for       where it coincides with the Lipschitz continuity of rf  Throughout the paper  we make extensive use of the wellknown consequence of Taylor   theorem  that the Lipschitz constant of the nthorder derivative
controls the error in the nth order Taylor series expansion
of         for         we have

            

  ku   vk  for all        Rd 

            
 
  function   is  strongly convex if                
rf                 
  Algorithm components
We begin our development by presenting the two building
blocks of our result    monitored variation of AGD  Section   and   negative curvature descent step  Section  
that we use when the monitored version of AGD certi es
nonconvexity  In Section   we combine these components
to obtain an accelerated method for nonconvex functions 

  convexity violation
  wt   

  yt

  yt

if krf  yt       then return  xt

         FINDWITNESS PAIR    xt
return  xt

Algorithm   AGDUNTIL GUILTY             
     and        
  Set           
  for               do
yt   xt     
 
Lrf  xt 
 
xt   yt      yt   yt 
wt   CERTIFYPROGRESS       yt      
 
if wt   NULL then
 
  yt
 
       
 
  NULL 
 
  function CERTIFYPROGRESS       yt        
 
 
 
 
 
 
 
  function FINDWITNESS PAIR    xt
 
 
 
 
 

return   
Set zt   yt    
Set   zt               zt     
if krf  yt          zt        then
else return NULL

for                       do

 by Corollary   this line is never reached 

if eq    holds with     xj then

if    yt          then

for     yj  wt do

return     xj 

Lrf  yt 

return zt

  nonconvex behavior

  kzt       
  AGD has stalled

  yt

  wt   

  AGD as   convexity monitor
The main component in our approach is Alg    AGDUNTIL GUILTY  We take as input an Lsmooth function
   conjectured to be  strongly convex  and optimize it
with Nesterov   accelerated gradient descent method for
strongly convex functions  lines   and   At every iteration  the method invokes CERTIFYPROGRESS to test
whether the optimization is progressing as it should for
strongly convex functions  and in particular that the gradient norm is decreasing exponentially quickly  line   If the
test fails  FINDWITNESS PAIR produces points      proving that   violates  strong convexity  Otherwise  we proceed until we  nd   point   such that krf         
The ef cacy of our method is based on the following guarantee on the performance of AGD 

Proposition   Let   be Lsmooth  and let yt
  be the
sequence of iterates generated by AGDUNTIL GUILTY   
          for some     and          Fix     Rd 
If for                       we have

  and xt

           xs    rf  xs        xs   

 
 ku   xsk 

 

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

for both       and     ys  then

 

   yt             
  and                         

   

    

 

where      

  kw       
Proposition   is essentially   restatement of established results  Nesterov    Bubeck    where we take care
to phrase the requirements on   in terms of local inequalities  rather than   global strong convexity assumption  For
completeness  we provide   proof of Proposition   in Section    in the supplementary material 
With Proposition   in hand  we summarize the guarantees
of Alg    as follows 
Corollary   Let     Rd     be Lsmooth  let      Rd 
    and          Let  xt
          AGDUNTIL 
GUILTY              Then the number of iterations  
satis es

  yt

        max    

log      zt 
  kz        is as in line   of
where                    
CERTIFYPROGRESS  If        NULL  nonconvexity was
detected  then

   

 

 

 

                rf               

 
 ku   vk 

 

where     xj for some           and     yj or     wt
 de ned on line   of AGDUNTIL GUILTY  Moreover 

max                  yt             

 

Proof  The bound   is clear for       For       the
algorithm has not terminated at iteration       and so we
know that neither the condition in line   of AGDUNTIL 
GUILTY nor the condition in line   of CERTIFYPROGRESS
held at iteration       Thus

    krf  yt          zt       

which gives the bound   when rearranged 
     and   from
Now we consider the returned vectors xt
  yt
AGDUNTIL GUILTY  Note that        NULL only if
wt   NULL  Suppose that wt      then by line   of
CERTIFYPROGRESS we have 

   yt       wt         

 

   

  wt 

since   wt             Since this contradicts the
progress bound   we obtain the certi cate   by the contrapositive of Proposition   condition   must not hold for

 

 

  zt 

   

Lrf  yt  then by line   of

some            implying FINDWITNESS PAIR will return for some       
Similarly  if wt   zt   yt    
CERTIFYPROGRESS we must have
   krf  yt        zt           
Since   is Lsmooth we have the standard progress guarantee       Nesterov        yt       zt   
   krf  yt    again contradicting inequality  
To see that the bound   holds  note that    ys          for
                    since condition   of CERTIFYPROGRESS
did not hold 
If     yj for some           then
              holds trivially  Alternatively  if ut  
wt   zt then condition   did not hold at time   as well 
so we have    yt          and also            zt   
   krf  yt    as noted above  therefore    zt   
   yt     
     

 

Before continuing  we make two remarks about implementation of Alg   

  As stated  the algorithm requires evaluation of two
function gradients per iteration  at xt and yt  Corollary   holds essentially unchanged if we execute line  
of AGDUNTIL GUILTY and lines   of CERTIFYPROGRESS only once every   iterations  where   is
some  xed number  say   This reduces the number
of gradient evaluations to      

  per iteration 

  xt

  and rf  xt

  Direct implementation would require          memory to store the sequences yt
  for later
use by FINDWITNESS PAIR  Alternatively  FINDWITNESS PAIR can regenerate these sequences from
their recursive de nition while iterating over    reducing the memory requirement to      and increasing
the number of gradient and function evaluations by at
most   factor of  

In addition  while our emphasis is on applying AGDUNTIL GUILTY to nonconvex problems 
the algorithm
has implications for convex optimization  For example 
we rarely know the strong convexity parameter   of  
given function    to remedy this    Donoghue and Cand es
  propose adaptive restart schemes  Instead  one may
repeatedly apply AGDUNTIL GUILTY and use the witnesses to update  

  Using negative curvature
The second component of our approach is exploitation of
negative curvature to decrease function values  in Section  

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

Algorithm   EXPLOITNC PAIR           
             ku   vk
            
            
  return arg minz           

we use AGDUNTIL GUILTY to generate      such that

                rf               

 
  ku   vk   

 

  nontrivial violation of convexity  where     is   parameter we control using   proximal term  By taking an
appropriately sized step from   in the direction        
Alg    can substantially lower the function value near  
whenever the convexity violation   holds  The following basic lemma shows this essential progress guarantee 
Lemma   Let     Rd     have   Lipschitz Hessian 
 
Let     and let   and   satisfy   If ku   vk    
   
  EXPLOITNC PAIR             nds
then for every      
  point   such that

  

               

 
 

 

 

We give the proof of Lemma   in Section    and we outline it here  The proof is split into two parts  both using
the Lipschitz continuity of       In the  rst part  we show
using   that   has negative curvature of at least   in
the direction of   at the point    In the second part  we consider the Taylor series expansion of    The  rst order term
predicts  due to its antisymmetry  that either   step size of
  or   in the direction   reduces the objective  Adding
our knowledge of the negative curvature from the  rst part
yields the required progress 

  Accelerating nonconvex optimization
We now combine the accelerated convergence guarantee
of Corollary   and the nonconvex progress guarantee of
Lemma   to form GUARDEDNON CONVEXAGD  The
idea for the algorithm is as follows  Consider iterate
      denoted pk  We create   proximal function    by
adding the proximal term  kx   pk    to    Applying
AGDUNTIL GUILTY to    yields the sequences            xt 
           yt and possibly   nonconvexity witnessing pair
      line   If      are not available  we set pk   yt and
continue to the next iteration  Otherwise  by Corollary  
  and   certify that    is not   strongly convex  and therefore that   has negative curvature  EXPLOITNC PAIR then
leverages this negative curvature  obtaining   point   
The next iterate pk is the best out of            yt    and   
in terms of function value 

          

         

if        NULL then

Algorithm  
GUARDEDNON CONVEXAGD               
  for               do
 
 

Set                   kx   pk   
  yt
 xt
AGDUNTIL GUILTY      pk   
pk   yt
     FINDBEST ITERATE    yt
     EXPLOITNC PAIR           
pk   arg minz           
return pk

 
 
 
 
 
 
 
 
  function FINDBEST ITERATE    yt
return arg minz     yt       
 

if krf  pk       then

       

else

     effectively str  convex
  nonconvexity proof available

       

The following central lemma provides   progress guarantee
for each of the iterations of Alg   
Lemma   Let     Rd     be   smooth and have   
Lipschitz continuous Hessian  let         and      Rd 
Let            pK be the iterates GUARDEDNON CONVEXAGD               
  generates  Then for each    
  
               

   pk       pk    min   

 

 

 
   

   

 

We defer   detailed proof of Lemma   to Section    and
instead sketch the main arguments  Fix an iteration   of
GUARDEDNON CONVEXAGD that is not the  nal one
             Then  if    was effectively strongly convex we
must have kr     yt       and standard proximal point
arguments show that we reduce the objective by  
Otherwise    witness pair      is available for which  
holds by Corollary   and                              pk 
To apply Lemma   it remains to show that ku   vk  
    We note that  since    yi     kyi         
    yi          for every        if any iterate yi is far from
      yi  must be substantially lower than       and therefore    makes good progress  Formalizing the converse of
this claim gives Lemma   which we prove Section   
Lemma   Let   be   smooth  and       At any iteration of GUARDEDNON CONVEXAGD  if        NULL
and the best iterate    satis es                    
then for           

kyi           and kxi          

Consequently  ku   vk    
Lemma   explains the role of    produced by FINDBEST 
ITERATE  it is an  insurance policy  against ku   vk being

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

too large  To complete the proof of Lemma   we take    
 
   

  so that either

                             

 
   
 

 

   

   
 

Indeed 

by Lemma   and
by Lemma    with

or we have ku   vk        
therefore                  
       
Lemma   shows we can accelerate gradient descent in
  nonconvex setting 
ignoring all problemdependent constants  setting        in the bound  
shows that we make   progress at every iteration
of GUARDEDNON CONVEXAGD  and consequently the
number of iterations is bounded by    Arguing that
calls to AGDUNTIL GUILTY each require    log  
   
gradient computations yields the following complexity
guarantee  which we prove in Section   
Theorem   Let     Rd     be   smooth and have   
Lipschitz continuous Hessian  Let      Rd            
inf   Rd       and       min 
   
Set
     pL 
 

then GUARDEDNON CONVEXAGD               
  
 nds   point pK such that krf  pK       with at most

    

    

 

 

     
    
 

 

   

log

    

 

 

gradient evaluations 

The conditions on   simply guarantee that
the clean
bound   is nontrivial  as gradient descent yields better
convergence guarantees for larger values of  
While we state Theorem   in terms of gradient evaluation
count    similar bound holds for function evaluations as
well  Indeed  inspection of our method reveals that each iteration of Alg    evaluates the function and not the gradient
at at most the three points       and    both complexity
measures are therefore of the same order 

  Incorporating thirdorder smoothness
In this section  we show that when thirdorder derivatives
are Lipschitz continuous  we can improve the convergence
rate of Alg    by modifying two of its subroutines  In Section   we introduce   modi ed version of EXPLOITNC 
PAIR that can decrease function values further using thirdorder smoothness  In Section   we change FINDBEST 
ITERATE to provide   guarantee that       is never too large 
We combine these two results in Section   and present
our improved complexity bounds 

Algorithm   EXPLOITNC PAIR           
             ku   vk
          ku   vk    ku   vk
            
            
  return arg minz           

  Making better use of negative curvature
Our  rst observation is that thirdorder smoothness allows
us to take larger steps and make greater progress when exploiting negative curvature  as the next lemma formalizes 
Lemma   Let     Rd     have   Lipschitz thirdorder
If
derivatives      Rd  and     Rd be   unit vector 
      then  for every            
 Tr            

min                          

 
 

 

 

Proof  For        de ne            By assumption
   is   Lipschitz continuous  and therefore

 

 

  

 
 

 
 

    

    

 
 

  and

    

            

  That               gives the result 

 
                 
 
Set              and set      sign   
As                 we have
 
           

 
 
the last inequality using                    
     
Comparing Lemma   to the second part of the proof of
Lemma   we see that secondorder smoothness with optimal   guarantees    
  function decrease  while
thirdorder smoothness guarantees       decrease 
Recalling Theorem   where   scales as   power of   this
is evidently   signi cant improvement  Additionally  this
bene   is essentially free  there is no increase in computational cost and no access to higher order derivatives  Examining the proof  we see that the result is rooted in the
antisymmetry of the oddorder terms in the Taylor expansion  This rules out extending this idea to higher orders of
smoothness  as they contain symmetric fourth order terms 
Extending this insight to the setting of Lemma   is complicated by the fact that  at relevant scales of ku   vk  it is
no longer possible to guarantee that there is negative curvature at either   or    Nevertheless  we are able to show
that   small modi cation of EXPLOITNC PAIR achieves
the required progress 
Lemma   Let     Rd     have   Lipschitz thirdorder
derivatives  Let     and let   and   satisfy   and let

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

       

Algorithm   FINDBEST ITERATE    yt
  Let           be such that     xj
  cj    yj   yj  if       else   
if       else   
  qj    yi    yj 
  return arg minz   yt cj  qj          
        Then for every ku   vk     EXPLOIT 

NCPAIR             nds   point   such that

        maxnf      

 
 

         

 
 

    

 

We prove Lemma   in Section    it is essentially   more
technical version of the proof of Lemma   where we address the asymmetry of condition   by taking steps of
different sizes from   and   

  Bounding the function values of the iterates using

cubic interpolation

An important difference between Lemmas   and   is that
the former guarantees lower objective value than      
while the latter only improves max             We invoke these lemmas for     xj for some xj produced
by AGDUNTIL GUILTY  but Corollary   only bounds the
function value at yj and       xj  might be much larger
than       rendering the progress guaranteed by Lemma  
useless  Fortunately  we are able show that whenever this
happens  there must be   point on the line that connects
xj  yj and yj  for which the function value is much lower
than       We take advantage of this fact in Alg   
where we modify FINDBEST ITERATE to consider additional points  so that whenever the iterate it  nds is not
much better than    then    xj  is guaranteed to be close
to       We formalize this claim in the following lemma 
which we prove in Section   
Lemma   Let   be   smooth and have   Lipschitz con 

tinuous thirdorder derivatives  and let         
with               
Consider GUARDEDNON 
CONVEXAGD with FINDBEST ITERATE replaced by
FINDBEST ITERATE  At any iteration  if        NULL
and the best iterate    satis es                    
then 

                   

We now explain the idea behind the proof of Lemma   Let
          be such that     xj  such   always exists by
Corollary   If       then xj      and the result is trivial 
so we assume       Let fr         be the restriction
of   to the line containing yj  and yj  and also qj  cj and
xj  Suppose now that fr is   cubic polynomial  Then  it
is completely determined by its values at any   points  and
   xj          qj         yj         cj         yj 

for Cj     independent of    By substituting the bounds
   yj       yj          and    qj       cj           
            we obtain an upper bound on    xj  when fr
is cubic  To generalize this upper bound to fr with Lipschitz thirdorder derivative  we can simply add to it the approximation error of an appropriate thirdorder Taylor series expansion  which is bounded by   term proportional to
          
  An improved rate of convergence
With our algorithmic and analytic upgrades established 
we are ready to state the enhanced performance guarantees for GUARDEDNON CONVEXAGD  where from here
on we assume that EXPLOITNC PAIR  and FINDBEST 
ITERATE  subsume EXPLOITNC PAIR and FINDBEST 
ITERATE  respectively 
Lemma   Let     Rd     be   smooth and have   
Lipschitz continuous thirdorder derivatives  let        
  is the sequence of iterates produced by
and      Rd  If pK
GUARDEDNON CONVEXAGD                 
 
then for every           

  

   pk       pk    min   

 

 

 

     

 

The proof of Lemma   is essentially identical to the proof
of Lemma   where we replace Lemma   with Lemmas  

and   and set         For completeness  we give

  full proof in Section    The gradient evaluation complexity guarantee for thirdorder smoothness then follows
precisely as in our proof of Theorem   see Sec     for  
proof of the following
Theorem   Let     Rd     be   smooth and have
  Lipschitz continuous thirdorder derivatives  Let     
Rd               inf   Rd       and        
min 

      

    

 

 

  If we set
 

       

 

 

GUARDEDNON CONVEXAGD                 
 
 nds   point pK such that krf  pK       and requires
at most

  

     
    
 

 

   

log      

 

 

 

gradient evaluations 

We remark that Lemma   and Theorem   remain valid after
the modi cations described in this section  Thus  Alg   
transitions between smoothness regimes by simply varying
the scaling of   and   with  

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

Gradient descent
Algorithm       ExploitNC pair
Restarted AGD
Algorithm  
Nonlinear conjugate gradient     line search 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure   Performance on   nonconvex regression problem  Left  cumulative distribution of number of steps required to achieve gradient norm     Center  gradient norm trace for   representative instance  Right  function value trace for the same instance  For Alg    the dots correspond
to negative curvature detection and the diamonds correspond to negative curvature exploitation
      when               For RAGD  the squares indicate restarts due to nonmonotonicity 

Figure   Performance on neural
network training 

  Preliminary experiments
The primary purpose of this paper is to demonstrate the
feasibility of acceleration for nonconvex problems using
only  rstorder information  Given the long history of development of careful schemes for nonlinear optimization 
it is unrealistic to expect   simple implementation of the
momentumbased Algorithm   to outperform stateof theart methods such as nonlinear conjugate gradients and LBFGS  It is important  however  to understand the degree of
nonconvexity in problems we encounter in practice  and to
investigate the ef cacy of the negative curvature detectionand exploitation scheme we propose 
Toward this end  we present two experiments     tting  
nonlinear regression model and   training   small neural
network  In these experiments we compare   basic implementation of Alg    with   number baseline optimization
methods  gradient descent  GD  nonlinear conjugate gradients  NCG   Hager and Zhang    Accelerated Gradient Descent  AGD  with adaptive restart    Donoghue
and Cand es     RAGD  and   crippled version of
Alg    without negative curvature exploitation  CAlg   
We compare the algorithms on the number of gradient
steps  but note that the number of oracle queries per step
varies between methods  We provide implementation details in Section   
For our  rst experiment  we study robust linear regression
with the smooth biweight loss  Beaton and Tukey   
      bi  where          
         
For   independent experiments  we randomly generate
problem data to create   highly nonconvex problem  see
Section   
In Figure   we plot aggregate convergence
time statistics  as well as gradient norm and function value
trajectories for   single representative problem instance 
The  gure shows that gradient descent and CAlg     which
does not exploit curvature  converge more slowly than the

mPm

    aT

other methods  When CAlg    stalls it is detecting negative
curvature  which implies the stalling occurs around saddle
points  When negative curvature exploitation is enabled 
Alg    is faster than RAGD  but slower than NCG  In this
highly nonconvex problem  different methods often converge to local minima with  sometimes signi cantly  different function values  However  each method found the
 best  local minimum in   similar fraction of the generated
instances  so there does not appear to be   signi cant difference in the ability of the methods to  nd  good  local
minima in this problem ensemble 
For the second experiment we      neural network model 
comprising three fullyconnected hidden layers containing
    and   units  respectively  on the MNIST handwritten
digits dataset  LeCun et al     see Section    Figure   shows   substantial performance gap between gradient descent and the other methods  including Alg    However  this is not due to negative curvature exploitation  in
fact  Alg    never detects negative curvature in this problem  implying AGD never stalls  Moreover  RAGD never
restarts  This suggests that the loss function   is  effectively convex  in large portions of the training trajectory 
consistent with the empirical observations of Goodfellow
et al      phenomenon that may merit further investigation 
We conclude that our approach can augment AGD in the
presence of negative curvature  but that more work is necessary to make it competitive with established methods such
as nonlinear conjugate gradients  For example  adaptive
schemes for setting     and    must be developed  However  the success of our method may depend on whether
AGD stalls at all in real applications of nonconvex optimization 

 Our approach in its current form is inapplicable to training
neural networks of modern scale  as it requires computation of
exact gradients 

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

Acknowledgment
OH was supported by the PACCAR INC fellowship  YC
and JCD were partially supported by the SAILToyota Center for AI Research and NSFCAREER award  
YC was partially supported by the Stanford Graduate Fellowship and the Numerical Technologies Fellowship 

References
   Agarwal     AllenZhu     Bullins     Hazan  and
   Ma  Finding approximate local minima for nonconvex optimization in linear time 
arXiv preprint
arXiv   

   Koren     Bell  and    Volinsky  Matrix factorization
techniques for recommender systems  Computer   
 

   LeCun     Cortes  and       Burges  The MNIST

database of handwritten digits   

   LeCun     Bengio  and    Hinton  Deep learning  Na 

ture     

      Lee     Simchowitz        Jordan  and    Recht 
Gradient descent only converges to minimizers  In  th
Annual Conference on Learning Theory  COLT  pages
   

      Beaton and       Tukey  The  tting of power series 
meaning polynomials  illustrated on bandspectroscopic
data  Technometrics     

      Liu and    Nocedal  On the limited memory BFGS
method for large scale optimization  Mathematical Programming     

   Beck and    Teboulle  Gradientbased algorithms with
applications to signal recovery  Convex optimization
in signal processing and communications  pages  
 

   Bubeck  Convex optimization  Algorithms and com 

plexity  arXiv preprint arXiv   

      Cand es     Li  and    Soltanolkotabi  Phase retrieval
via Wirtinger  ow  Theory and algorithms  IEEE Transactions on Information Theory     

   Carmon        Duchi     Hinder  and    Sidford  Accelerated methods for nonconvex optimization  arXiv
preprint arXiv   

   Cartis        Gould  and       Toint  On the complexity of steepest descent  Newton   and regularized Newton   methods for nonconvex unconstrained optimization
problems  SIAM journal on optimization   
   

   Glorot and    Bengio  Understanding the dif culty of
In Aistats 

training deep feedforward neural networks 
volume   pages    

      Goodfellow     Vinyals  and       Saxe  Qualitatively
characterizing neural network optimization problems  In
International Conference on Learning Representations 
 

      Hager and    Zhang    survey of nonlinear conjugate gradient methods  Paci   Journal of Optimization 
   

   Kingma and    Ba  Adam    method for stochastic optimization  In International Conference on Learning Representations   

   Mairal     Bach     Ponce     Sapiro  and    Zisserman 
Supervised dictionary learning  In Advances in Neural
Information Processing Systems    

   Murty and    Kabadi  Some NPcomplete problems
in quadratic and nonlinear programming  Mathematical
Programming     

for nonlinear

Optimization II  Standard numerA  Nemirovski 
continuous optimizaical methods
tion 
Institute of Technology 
  URL http www isye gatech edu 
 nemirovs Lect OptII pdf 

Technion   Israel

   Nemirovski and    Yudin  Problem Complexity and

Method Ef ciency in Optimization  Wiley   

   Nesterov    method of solving   convex programming
problem with convergence rate      Soviet Mathematics Doklady     

   Nesterov  Squared functional systems and optimization problems 
In High Performance Optimization 
volume   of Applied Optimization  pages  
Springer   

   Nesterov 

Introductory Lectures on Convex Optimiza 

tion  Kluwer Academic Publishers   

   Nesterov  How to make the gradients small  Optima  

 

   Nesterov and       Polyak  Cubic regularization of Newton method and its global performance  Mathematical
Programming     

   Nocedal and       Wright  Numerical Optimization 

Springer   

 Convex Until Proven Guilty  DimensionFree Acceleration of Gradient Descent on NonConvex Functions

     Donoghue and    Cand es  Adaptive restart for accelerated gradient schemes  Foundations of Computational
Mathematics     

   Polak and    Ribi ere  Note sur la convergence de directions conjug ees  Rev  Fr  Inform  Rech  Oper     pages
   

      Rumelhart        Hinton  and       Williams  Learning internal representations by error propagation 
In
      Rumelhart and       McClelland  editors  Parallel
Distributed Processing   Explorations in the Microstructure of Cognition  chapter   pages   MIT Press 
 

   Wang        Giannakis  and       Eldar  Solving systems of random quadratic equations via truncated amplitude  ow  arXiv   stat ML   

