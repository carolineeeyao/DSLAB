Statistical Inference for Incomplete Ranking Data 

The Case of RankDependent Coarsening

Mohsen Ahmadi Fahandar   Eyke   ullermeier   In es Couso  

Abstract

We consider the problem of statistical inference
for ranking data  speci cally rank aggregation  under the assumption that samples are incomplete in
the sense of not comprising all choice alternatives 
In contrast to most existing methods  we explicitly
model the process of turning   full ranking into
an incomplete one  which we call the coarsening
process  To this end  we propose the concept of
rankdependent coarsening  which assumes that
incomplete rankings are produced by projecting
  full ranking to   random subset of ranks  For  
concrete instantiation of our model  in which full
rankings are drawn from   PlackettLuce distribution and observations take the form of pairwise
preferences  we study the performance of various
rank aggregation methods 
In addition to predictive accuracy in the  nite sample setting  we
address the theoretical question of consistency  by
which we mean the ability to recover   target ranking when the sample size goes to in nity  despite
  potential bias in the observations caused by the
 unknown  coarsening 

  Introduction
The analysis of rank data has   long tradition in statistics 
and corresponding methods have been used in various  elds
of application  such as psychology and the social sciences
 Marden    More recently  applications in information
retrieval and machine learning have caused   renewed interest in the analysis of rankings and topics such as  learningto rank   Liu    and preference learning    urnkranz  
  ullermeier   
In most applications  the rankings observed are incomplete
or partial in the sense of including only   subset of the
underlying choice alternatives  subsequently referred to as

 Paderborn University  Germany  University of Oviedo  Spain 

Correspondence to  Eyke   ullermeier  eyke upb de 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright   by
the author   

 items  whereas no preferences are revealed about the remaining ones pairwise comparisons can be seen as an
important special case  Somewhat surprisingly  most methods for learning from ranking data  including methods for
rank aggregation  simply ignore the process of turning   full
ranking into an incomplete one  Or  they implicitly assume
that the process is unimportant from   statistical point of
view  because the subset of items observed is independent
of the underlying ranking 
Obviously  this assumption is often not valid  as shown by
practically relevant examples such as topk observations 
Motivated by examples of this kind  we propose the concept
of rankdependent coarsening  which assumes that incomplete rankings are produced by projecting   full ranking to
  random subset of ranks  The notion of  coarsening  is
meant to indicate that an incomplete ranking can be associated with   set of complete rankings  namely the set of its
consistent extensions setvalued data of that kind is also
called  coarse data  in statistics  Heitjan   Rubin   
Gill et al    The idea of coarsening is similar to the interpretation of partial rankings as  censored data   Lebanon
  Mao    The assumption of rankdependent coarsening can be seen as orthogonal to standard marginalization 
which acts on items instead of ranks  Rajkumar   Agarwal 
  Sibony et al   
In addition to introducing   general statistical framework for
analyzing incomplete ranking data  Section   we outline
several problems and learning tasks to be addressed in this
framework  Learning of the entire model will normally not
be feasible  even under restrictive assumptions on the coarsening    speci cally interesting question  therefore  is to
what extent and in what sense successful learning is possible for methods that are agnostic of the coarsening process 
We investigate this question  both practically  Section  
and theoretically  Section   for several ranking methods
 Section   and   concrete instantiation of our framework  in
which full rankings are drawn from   PlackettLuce distribution and observations take the form of pairwise preferences
 Section   In particular  we are interested in the property
of consistency  by which we mean the ability to recover  
target ranking when the sample size goes to in nity  despite
  potential bias in the observations caused by the coarsening 

Statistical Inference for Incomplete Ranking Data

  Preliminaries and Notation
Let SK denote the collection of rankings  permutations 
over   set                 aK  of   items ak           
             We denote by                 complete
ranking    generic element of SK  where     denotes the
position of the kth item ak in the ranking  and by   the
ordering associated with   ranking           is the index
of the item on position    We write rankings in brackets and
orderings in parentheses  for example                and
              both denote the ranking     cid      cid 
    cid      cid    
For   possibly incomplete ranking  which includes only
some of the items  we use the symbol    instead of   If
the kth item does not occur in   ranking  then           by
de nition  otherwise        is the rank of the kth item  In
the corresponding ordering  the missing items do simply not
occur  For example  the ranking     cid      cid     would be
encoded as             and           respectively 
We let                             and denote the set of
all rankings  complete or incomplete  by SK 
An incomplete ranking   can be associated with its set of
linear extensions        SK  where          if   is
consistent with the order of items in                  
                  for all             An important
special case is an incomplete ranking                   in
the form of   pairwise comparison ai  cid  aj                
                    otherwise  which is associated with
the set of extensions

         ai  cid  aj        SK              

Modeling an incomplete observation   by the set of linear
extensions      re ects the idea that   has been produced
from an underlying complete ranking   by some  coarsening  or  imprecisiation  process  which essentially consists
of omitting some of the items from the ranking       then
corresponds to the set of all consistent extension   if nothing is known about the coarsening  except that it does not
change the relative order of any items 

  General Setting and Problems
The type of data we assume as observations is incomplete
rankings     SK  Statistical inference for this type of
data requires   probabilistic model of the underlying data
generating process  that is    probability distribution on SK 

    Stochastic Model for Incomplete Rankings

Recalling our idea of   coarsening process  it is natural to
consider the data generating process as   two step procedure 
in which   full ranking   is generated  rst and turned into an
incomplete ranking   afterward  We model this assumption
in terms of   distribution on SK   SK  which assigns  

degree of probability to each pair     More speci cally 
we assume   parameterized distribution of the following
form 

                  

 
Thus  while the generation of full rankings is determined by
the distribution

     SK        

 

the coarsening process is speci ed by   family of conditional
probability distributions

 cid            SK       cid   

 

where   collects all parameters of these distributions 
     is the probability of producing the data      
SK   SK  Note  however  that   is actually not observed 

  RANKDEPENDENT COARSENING

In its most general form  the coarsening process   is extremely rich  even if being restricted by the consistency
assumption            for    cid       In fact  since the
number of probabilities to be speci ed is of the order  KK 
inference about   will generally be dif cult  Therefore 
   certainly needs to be restricted by further assumptions 
Apart from practical reasons  such assumptions are also
indispensable for successful learning  Otherwise  observations could be arbitrarily biased in favor or disfavor of
items  so that an estimation of the underlying  full  preferences  as re ected by    becomes completely impossible 
For example  the coarsening process may leave   ranking
  unchanged whenever item    is on the last position  and
remove    from   otherwise  Obviously  this item will then
appear to have   very low preference 
As shown by this example  the estimation of preferences will
generally be impossible unless the coarsening is somehow
more  neutral  The assumption we make here is   property
we call rankdependent coarsening    coarsening procedure
is rankdependent if the incompletion is only acting on
ranks  positions  but not on items  That is  the procedure
randomly selects   subset of ranks and removes the items
on these ranks  independently of the items themselves  In
other words  an incomplete observation   is obtained by
projecting   complete ranking   on   random subset of
positions              the family   of distributions
     is speci ed by   single measure on     Or  stated
more formally 

  cid     cid      cid     cid 

for all       SK and         where     denotes the
projection of the ordering   to the positions in   
The assumption of rankdependent coarsening can be seen
as orthogonal to standard marginalization  while the latter projects   full ranking to   subset of items  the former

Statistical Inference for Incomplete Ranking Data

projects   ranking to   subset of positions  The practically
relevant case of topk observations is   special  degenerate 
case of rankdependent coarsening  in which
if                 
otherwise

 cid   

      

 

This model could be weakened in various ways  For example  instead of  xing the length of observed rankings to
  constant    could be   random variable  Or  one could
assume that positions are discarded with increasing probability  though independently of each other  thus  the probability
to observe   subset of items on ranks                  is
given by

       

         

 cid 

   

    cid 

  cid  

The coarsening is then de ned by the   parameters    
               

  Learning Tasks
Suppose   sample of  training  data                   to
be given  As for statistical inference about the process  
several problems could be tackled 

  The most obvious problem is to estimate the complete
distribution on SK       the parameters   and   As
already explained before  this will require speci   assumptions about the coarsening 

    somewhat weaker goal is to estimate the  precise
part       the parameter   Indeed  in many cases   
will be the relevant part of the model  as it speci es the
preferences on items  whereas the coarsening is rather
considered as   complication of the estimation  In this
case    is of interest only in so far as it helps to estimate
 
Ideally  it would even be possible to estimate  
without any inference about        by simply ignoring
the coarsening process 

  An even weaker goal is to estimate  not the parameter  
itself  but only an underlying  ground truth  ranking  
associated with   Indeed  in the context of learning to
rank  the ultimate goal is typically to predict   ranking 
not necessarily   complete distribution  For example 
the ranking   could be the mode of the distribution
   or any other sort of representative statistics  This
problem is especially relevant in practical applications
such as rank aggregation  in which   would play the
role of   consensus ranking 

Here  we are mainly interested in the third problem      
the estimation of   groundtruth ranking   Moreover  due
to reasons of ef ciency  we are aiming for an estimation
technique that circumvents direct inference about   while

being robust in the sense of producing reasonably good
results for   wide range of coarsening procedures 

  Speci   Setting and Problems
The development and analysis of methods is only possible
for concrete instantiations of the setting introduced in the
previous section  An instantiation of that kind will be proposed in this section  The  rst part of our data generating
process     will be modeled by the PlackettLuce model
 Plackett    Luce    To make the second part    
manageable  we restrict observations to the practically relevant case of pairwise comparisons       incomplete rankings
of length  

  The PlackettLuce Model

The PlackettLuce  PL  model is parameterized by   vector
                          RK
    Each    can be interpreted as the weight or  strength  of the option ai  The
probability assigned by the PL model to   ranking represented by   permutation     SK is given by

   

 

                       

  

  cid 

pl   

Obviously  the PL model is invariant toward multiplication
of   with   constant            pl    plc  for all
    SK and       Consequently    can be normalized
without loss of generality  and the number of degrees of
freedom is only       instead of    Note that the most
probable ranking       the mode of the PL distribution  is
simply obtained by sorting the items in decreasing order of
their weight 

    arg max
 SK

pl    arg sort
    

               

 

As   convenient property of PL  let us mention that it allows
for an easy computation of marginals  because the marginal
probability on   subset   cid     ai          aiJ      of      
items is again   PL model parametrized by              iJ  
Thus  for every     SK with          cid 

  cid 

pl     

   

                       

  

In particular  this yields pairwise probabilities

pi     pl       

  

       

 

 

where               represents the preference ai  cid  aj  This
is the wellknown BradleyTerry Luce model  Bradley  
Terry      model for the pairwise comparison of alternatives  Obviously  the larger    in comparison to     the

Statistical Inference for Incomplete Ranking Data

higher the probability that ai is chosen  The PL model can
be seen as an extension of this principle to more than two
items  the larger the parameter    in   in comparison to
the parameters        cid     the higher the probability that ai
appears on   top rank 

  Pairwise Preferences

If rankdependent coarsening is restricted to the generation of pairwise comparisons  the entire distribution    is
speci ed by the set of         probabilities

                              

        

 

 

 cid 

      

 cid 

 cid 

where      denotes the probability that the ranks   and   are
selected 
The problem of ranking based on pairwise comparisons
has been studied quite extensively in the literature  albeit
without taking coarsening into account       without asking
where the pairwise comparisons are coming from  or implicitly assuming they are generated as marginals  Yet  worth
mentioning is   recent study on rank breaking  Sou ani
et al    that is  of the estimation bias  for models such
as PL  caused by replacing full rankings in the training data
by the set of all pairwise comparisons our study of the
bias caused by coarsening is very much in the same spirit 

  The Data Generating Process

Combining the PL model   with the coarsening process
  we obtain   distribution   on SK such that

         qi    

pl       

 

 cid 

   ai cid aj  

tribution on incomplete rankings  cid 

for pairwise preferences               and          otherwise  Clearly  the pairwise probabilities   will normally
not agree with the pairwise marginals   Instead  they may
provide   biased view of the pairwise preferences between
items  Please note  however  that the marginals pi   are not
directly comparable with the qi    because the latter is   disi   qi       whereas
the former is   set of marginal distributions  pi     pj      
Instead  pi   should be compared to   cid 
      qi   qi     qj   
which is the probability that  in the coarsened model  ai is
observed as   winner  given it is paired with aj 
As an illustration  consider   concrete example with    
            and degenerate coarsening distribution
speci ed by        top  selection  One easily derives
the probabilities of pairwise marginals   and coarsened
 top  observations   as follows 

    

pi  

qi  
  cid 

   

   

 
 
 
 
 
 

   

 
 
 
 
 
 

   

 
 

 

 

 
 

   

 
 
 
 
 
 

   

 

 

 

 

 
 

   

 
 

 

 

 
 

While the pi   are completely coherent with   PL model
 namely pl  with           the qi   and   cid 
    no longer
are 
  special case where coarsening is guaranteed to not introduce any bias is the uniform distribution              
In this case  random projection to ranks effectively coincides
with random selection to items 

  Problems

Of course  in spite of the inconsistency of the pairwise
observations in the above example    PL model could still
be estimated  for example using the maximum likelihood
principle  The corresponding estimate   will also yield an
estimate   of the target ranking    which is simply obtained
by sorting the items ai in decreasing order of the estimated
scores     As already said  there is little hope that   could
be an unbiased estimate of   instead   will necessarily be
biased  There is hope  however  to recover the target ranking
  Indeed  the ranking will be predicted correctly provided
  is comonotonic with                            for all
           Roughly speaking    small bias in the estimate
can be tolerated  as long as the order of the parameters is
preserved 
Obviously  these considerations are not restricted to the
PL model  Instead  any method for aggregating pairwise
comparisons into an overall ranking can be used to predict
  This leads us to the following questions 

  Practical performance  What is the performance of  
rank aggregation method in the  nite sample setting 
     how close is the prediction   to the ground truth
  How is the performance in uenced by the coarsening process 

  Consistency  Is   method consistent in the sense that  
is recovered  with high probability  with an increasing
sample size       either under speci   assumptions
on the coarsening process  or perhaps even regardless
of the coarsening       only assuming the property of
rankdependence 

  Rank Aggregation Methods
In this section  we discuss different rank aggregation methods that operate on pairwise data  categorized according to
the some basic principles  To this end  let us de ne the comparison matrix   with entries ci    where ci   denotes the

Statistical Inference for Incomplete Ranking Data

number of wins of ai over aj       the number of times the
preference ai  cid  aj is observed  Correspondingly  we de ne
the probability matrix    with entries  pi     ci  
  which
can be seen as estimates of the winning probabilities pi  
 often also interpreted as weighted preferences  All rank
aggregation methods produce rankings   based on either
matrix   or     

ci    cj  

  Statistical Estimation

  BRADLEYTERRY LUCE MODEL  BTL 

The BradleyTerry Luce model is wellknown in the literature on discrete choice  Bradley   Terry    It starts
from the parametric model   of pairwise comparisons      
the marginals of the PL model  and estimates the parameters
by likelihood maximization 

 cid 

 cid    

 cid ci  

   cid    

       

    arg max
 RK

The predicted ranking is obtained by sorting items according
to their estimated strengths      arg sort 
As already explained  coarsening of rankings may cause  
bias in the number of observed pairwise preferences  In
particular  it may happen that some  pairs of  items are observed much more often than others  Therefore  in addition
to the BTL problem as formalized above  we also consider
the same problem with relative winning frequencies  pi  
instead of absolute frequencies ci    we call this approach
BTL   

  LEAST SQUARES HODGERANK  LS 

The HodgeRank algorithm  Jiang et al    is based on
  least squares approach  First  the probability matrix    is
mapped to   matrix   as follows 

 cid   pi  

 cid 

 pj  

 log

 

Xi    

if    cid    and  pj        

otherwise

Then      arg sort  where

 cid 

 cid 

    arg min
 RK

and    cid                      Xi    cid   cid 

      

           Xi  

 cid 

 

  Voting Methods

  BORDA COUNT  BORDA 

favor of each item 

where si  cid  

    arg sort            sK   
    pi   

  COPELAND  CP 

Copeland  Copeland    works in the same way as
Borda  except that scores are derived from binary instead of
weighted votes 

 cid 

  cid 

 

 cid 

 
 

si  

 pi    

 

  

  Spectral Methods

The idea of deriving   consensus ranking from the stationary
distribution of   suitably constructed Markov chain has been
thoroughly studied in the literature  Seeley    Vigna 
  Brin   Page    The corresponding Markov chain
with transition probabilities   is de ned by the pairwise
preferences  Then  if   is an irreducible  aperiodic Markov
chain  the stationary distribution   can be computed  and
the predicted ranking is given by     arg sort 

  RANK CENTRALITY  RC 

The rank centrality algorithm  Negahban et al    is
based on the following transition probabilities 
if    cid   

 
 pi  
 
     
 

 cid 

  cid  

Qi    

 pk  

if      

  MC  AND MC 

Dwork et al    introduce four spectral ranking algorithms  two of which we consider for our study  namely
MC  and MC  translated to the setting of pairwise preferences  For MC  the transition probabilities are given as
follows 

Qi    

    pi  

 pj  

if    cid   

if      

 

 

 

 cid  

 

 

 cid    cid 

  

The MC  algorithm is based on the transition probabilities

 pi  

deg ai 
   
 

deg ai 

 cid 

  cid  

if    cid   

 pk  

if      

Qi    

where

 cid 

 

  pj      

  cid 

  

Borda  Borda    is   scoring rule that sorts items according to the sum of weighted preferences or  votes  in

deg ai    max

  pi      

  Graphbased Methods

  FEEDBACK ARC SET  FAS 

Statistical Inference for Incomplete Ranking Data

  METHOD BY PRICE ET AL   PRICE 

If the ci   are interpreted as  weighted  preferences  the
degree of inconsistency of ranking ai before aj is naturally
quanti ed in terms of cj    Starting from   formalization
in terms of   graph whose nodes correspond to the items
and whose edges are labeled with the weighted preferences 
the weighted feedback arc set problem  Saab    Fomin
et al    is to  nd the ranking that causes the lowest sum
of penalties 

 cid 

    arg min

 SK

cj  

           

For the same reason as in the case of BTL  we also consider
the FAS problem with edge weights given by relative winning frequencies  pi   and binary preferences   pi      
instead of absolute frequencies ci    we call the former approach FAS    and the latter FAS   

  Pairwise Coupling

  common approach to multiclass classi cation is the allpairs decomposition  in which one binary classi er hi   is
trained for each pair of classes ai and aj    urnkranz   
At prediction time  each classi er produces   prediction 
which can be interpreted as   vote  or weighted vote  pi  
in case of   probabilistic classi er  in favor of item ai over
aj  The problem of combining these predictions into an
overall prediction for the multiclass problem is also called
pairwise coupling  Hastie   Tibshirani   
To the best of our knowledge  pairwise coupling has not
been used for rank aggregation so far  In fact  the original
purpose of this technique is not to rank items but merely
to identify   single winner  Nevertheless  since coupling
methods are eventually based on scoring items  they can be
generalized for the purpose of ranking in   straightforward
way  Indeed  they have been used for that purpose in the
context of label ranking    ullermeier et al   

  METHOD BY HASTIE AND TIBSHIRANI  HT 

Hastie   Tibshirani   tackle the problem in the following way  Given relative frequencies      they suggest to  nd
the probability vector                 pK  that minimizes the
 weighted  KullbackLeibler  KL  distance

 cid     

ni  

 pi   log

       pi    log

 pi  
    

     pi  
        

 cid 

 cid 

   

 cid 

Price et al    propose the following parameter estimation for each        

 cid cid 

    

 cid          

 

  cid  

 
 pi  

Then  the predicted ranking is given by     arg sort 

  METHOD BY TINGFAN WU ET AL   WU  WU 

Wu et al    propose two methods  The  rst one  WU 
is based on the Markov chain approach with transition matrix

 pi        
 cid 

  cid  

 pi        

Qi    

if    cid   
if      

 

Once the stationary distribution   of   is obtained  the predicted ranking is given by     arg sort  In their second
approach  WU  the following optimization problem is
proposed 

where

Qi    

       

 

min

 pi    pj  
 cid 

   
   

  cid  

if    cid   
if      

 

Once the optimization is solved  the predicted ranking is
    arg sort 

  Practical Performance
  Synthetic Data

To investigate the practical performance of the methods presented in the previous section  we  rst conducted controlled
experiments with synthetic data  for which the ground truth
  is known  To this end  data in the form of pairwise
observations was generated according to   with different
distributions    and    predictions   were produced and
compared with   in terms of the Kendall distance       the
number of pairwise inversions 

sign           cid  sign         

 

 cid 

 cid 

  cid 

      

For each setting  speci ed by parameters        we are
interested in the expected performance of   method as  
function of the sample size    which was approximated by
averaging over   simulation runs 

between  pi   and        pi
  where ni     ci     cj    To
this end  the problem is formulated as    xed point problem
and solved using an iterative algorithm  Once   is obtained 
the predicted ranking is determined by     arg sort   

pi pj

  PL DISTRIBUTION

In    rst series of experiments  synthetic data was produced
for                the PL model with ground truth

Statistical Inference for Incomplete Ranking Data

parameter     RK
    and coarsening rankings by projecting
to all possible pairs of ranks       using   degenerate distribution    with          for some                As  
baseline  we also produced the performance of each method
for the case of full pairwise information about   ranking      
adding all pairwise preferences to the data  instead of only
      cid       that can be extracted from   ranking  
Due to space restrictions  the results are only shown in the
supplementary material  The following observations can be
made 

  Comparing the results for learning from incomplete
data with the baseline  it is obvious that coarsening
comes with   loss of information and makes learning
more dif cult 

  The dif culty of the learning task increases with decreasing distance        between observed ranks and is
most challenging for the case of observing neighboring
ranks          

  All methods perform rather well  although FAS is visibly worse while BTL is   bit better than the others  On
the one side  this could be explained by the fact that
the BTL model is consistent with the PL model  as it
corresponds to the marginals of    On the other side 
like all other methods  BTL is agnostic of the coarsening    from this point of view  the strong performance
is indeed   bit surprising  As   side remark  BTL   
does not improve on BTL 

  While FAS is on   par with FAS    FAS    tends
to do slightly better  especially for   larger number
of items  However  as already said  FAS performs
generally worse than the others 

Figure   Results for the sushi data in terms of normalized Kendall
distance  one boxblot per method  experiment     on the left and
    on the right 

parameter               

    

 

  

exp cid         cid 

In another set of experiments  we examined the averaged
performance of methods over all coarsening positions  The
results are again shown in the supplementary material  It is
noticeable that  as the number of items increases  the performance of the FASbased approaches decreases  Moreover 
as pointed out earlier  the BTL performs moderately better
than other approaches 

  MALLOWS DISTRIBUTION

In   second series of experiments  we replaced the PL distribution with another wellknown distribution on rankings 
namely the Mallows distribution  Thus  data is now generated with    in   given by Mallows instead of PL  This
experiment serves as   kind of sensitivity analysis  especially for those methods that  explicitly or implicitly  rely
on the assumption of PL 
The Mallows model  Mallows    is parametrized by  
reference ranking    which is the mode  and   dispersion

where      is the Kendall distance and      normalization constant 
The results and observations for this series of experiments
are quite similar to those for the PL model  What is notable 
however  is   visible drop in performance for BTL  whereas
Copeland ranking now performs much better than all other
algorithms  Furthermore  FAS    is signi cantly better than
FAS and FAS    These results might be explained by the
ordinal nature of the Mallows model  which is arguably
better    by methods based on binary comparisons than by
scorebased approaches 

  Real Data

To compare the methods on   real world data set  we used the
Sushi data  Kamishima    that contains the preferences
 full rankings  of   people over   types of sushi 
For this data set  there is no obvious ground truth   There 

lllllRCLSBordaCPMC MC BTLBTL   FASFAS   FAS   PriceWU HTWU llllllllllllllRCLSBordaCPMC MC BTLBTL   FASFAS   FAS   PriceWU HTWU Statistical Inference for Incomplete Ranking Data

fore  we de ne   separate target for each data set individually  which is the ranking produced by that method on the
full set of pairwise comparisons that can be extracted from
the training data  The distance between this ranking and the
one predicted for coarsened data can essentially be seen as
  measure of the loss of information caused by coarsening 
We conduct this experiment for     degenerate coarsening
where          for some        and     random coarsening
where                  for all        Note that  while
the information content is the same in both cases  one pairwise comparison per customer  random coarsening does
not introduce any bias  as opposed to degenerate coarsening 
The results  distributions for   repetitions  are shown in
Figure   Again  all methods are more or less on   par 
However  as expected  random coarsening does indeed lead
to better estimates  again suggesting that  real  coarsening
indeed makes learning harder 

  Consistency
Recall the  speci    setting we introduced in Section   in
which full rankings are generated according to   PL model
with parameter                   and the coarsening corresponds to   projection to   random pair of ranks  for technical reasons  we subsequently assume     cid     for    cid    
Also recall that we denote by pi   the probability of   ranking   in which ai precedes aj  and by qi   the probability to
observe the preference ai  cid  aj after coarsening  According
to PL  we have pi                and the ground truth
ranking   is such that

                        pi      

Finally  we denote by  pi   the estimate of the pairwise preference  probability  pi    If not stated differently  we always
assume estimates to be given by relative frequencies      
 pi     wi   wi     wj    with wi   the observed number
of preferences ai  cid  aj 
De nition   Let    denote the ranking produced as  
prediction by   ranking method on the basis of   observed
 pairwise  preferences  The method is consistent if      
      for      
The proofs of the following results are given in the supplementary material 
Lemma   Let us consider   probability measure    over
   ai cid aj               cid 
  
 The model   with    not necessarily PL  If
            for all       ai  cid  aj  then qi     qj   
 cid     for    cid    
Lemma   Assume the model     
and        for all         The coarsening   is orderpreserving for PL in the sense that pi       if and only if
  cid 
        where   cid 

SK  Consider qi    cid 

      qi   qi     qj   

     

      cid 

The last result is indeed remarkable  Although coarsening
will bias the pairwise probabilities pi    the  binary  preferences will be preserved in the sense that sign pi        
        Indeed  the result heavily exploits propsign   cid 
erties of the PL distribution and does not hold in general  For example  consider   distribution   on    such
that                               
Then  with   coarsening   such that       we have
            but   cid 
Lemma   Assume the model       cid     for    cid     and
       for all         Let us take an arbitrarily small
      There exists        such that         if and only
if  pi       for all            with probability at least
      after having observed at least    preferences 
Theorem   Copeland ranking is consistent 
Theorem   FAS  FAS    and FAS    are consistent 
Our experimental results so far suggest that consistency does
not only hold for Copeland and FAS  but also for most other
methods  including BTL  and hence that rankdependent
coarsening is indeed somehow  goodnatured  Anyway 
for these cases  the proofs are still pending 

  Summary and Conclusion
In this paper  we addressed the problem of learning from
incomplete ranking data and advocated an explicit consideration of the process of turning   full ranking into an incomplete one   step that we referred to as  coarsening  To
this end  we proposed   suitable probabilistic model and introduced the property of rankdependent coarsening  which
can be seen as orthogonal to standard marginalization  while
the latter projects   ranking to   subset of items  the former
projects to   subset of ranks 
First experimental and theoretical results suggest that agnostic learning can be successful under rankdependent coarsening  even if ignorance of the coarsening may lead to
biased parameter estimates  the ranking task itself can still
be solved properly  This applies at least to the speci   setting that we considered  namely rank aggregation based on
pairwise preferences  with PlackettLuce  or Mallows  as
an underlying distribution 
Needless to say  this paper is only    rst step  Many questions are still open  for example regarding the consistency
of ranking methods  not only for the speci   setting considered here but even more so for generalizations thereof  In
addition to theoretical problems of that kind  we are also
interested in practical applications such as  crowdordering 
 Matsui et al    Chen et al    in which coarsening
could play an important role 

Statistical Inference for Incomplete Ranking Data

References
Borda         emoire sur les  elections au scrutin  Histoire

de   Acad emie Royale des Sciences   

Bradley       and Terry      The rank analysis of incomplete block designs    The method of paired comparisons 
Biometrika     

Brin     and Page     The anatomy of   largescale hyIn Proceedings of the
pertextual web search engine 
 th International Conference on World Wide Web  pp 
  Amsterdam  The Netherlands    Elsevier
Science Publishers      

Chen     Bennett       CollinsThompson     and Horvitz 
  Pairwise ranking aggregation in   crowdsourced setting  In Proceedings of the  th ACM International Conference on Web Search and Data Mining  pp   
Rome  Italy   

Copeland           reasonable  social welfare function  In
Seminar on Mathematics in Social Sciences  University
of Michigan   

Dwork     Kumar     Naor     and Sivakumar     Rank
In Proceedings of
aggregation methods for the web 
the  th International Conference on World Wide Web 
WWW   pp    New York  USA    ACM 

Fomin       Lokshtanov     Raman     and Saurabh    
Fast local search algorithm for weighted feedback arc
In Proceedings of the  th AAAI
set in tournaments 
Conference on Arti cial Intelligence  AAAI  pp   
  AAAI Press   

Jiang     Lim     Yao     and Ye     Statistical ranking and
combinatorial hodge theory  Mathematical Programming 
   

Kamishima     Nantonac collaborative  ltering  RecomIn Proceedings
mendation based on order responses 
of the  th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining  KDD   pp 
  New York  USA    ACM 

Lebanon     and Mao     Nonparametric modeling of partially ranked data  Journal of Machine Learning Research 
   

Liu       Learning to Rank for Information Retrieval 

SpringerVerlag   

Luce      

Individual Choice Behavior    Theoretical

Analysis  John Wiley and Sons  New York   

Mallows     Nonnull ranking models  Biometrika   

   

Marden       Analyzing and Modeling Rank Data  London 

New York   

Matsui     Baba     Kamishima     and Kashima    Crowdordering  In Proceedings of the  th Paci cAsia Conference on Advances in Knowledge Discovery and Data
Mining  pp    Tainan  Taiwan   

Negahban     Oh     and Shah     Iterative ranking from
In Advances in Neural Inforpair wise comparisons 
mation Processing Systems  NIPS  pp   
 

  urnkranz     Round robin classi cation  Journal of Ma 

chine Learning Research     

Plackett     The analysis of permutations  Applied Statistics 

   

  urnkranz     and   ullermeier     Preference Learning 

SpringerVerlag   

Gill        Laan        and Robins        Coarsening at random  Characterizations  conjectures  counterexamples 
In Proceedings of the  st Seattle Symposium in Biostatistics  Survival Analysis  pp    Springer US  New
York   

Hastie     and Tibshirani     Classi cation by pairwise
coupling  In Advances in Neural Information Processing
Systems  NIPS  The MIT Press   

Heitjan       and Rubin       Ignorability and coarse data 

The Annals of Statistics     

Price     Knerr     Personnaz     and Dreyfus     Pairwise neural network classi ers with probabilistic outputs 
In Proceedings of the  th International Conference on
Neural Information Processing Systems  NIPS  pp 
  Cambridge  MA  USA    MIT Press 

Rajkumar     and Agarwal      statistical convergence perspective of algorithms for rank aggregation from pairwise
data  In Proceedings of the  th International Conference on Machine Learning  pp    Beijing  China 
 

Saab       fast and effective algorithm for the feedback arc
set problem  Journal of Heuristics    May
 

  ullermeier       urnkranz     Cheng     and Brinker    
Label ranking by learning pairwise preferences  Arti cial
Intelligence     

Seeley        The net of reciprocal in uence    problem in
treating sociometric data  Canadian Journal of Psychology    Dec  

Statistical Inference for Incomplete Ranking Data

Sibony     Cl emenc on     and Jakubowicz    MRAbased
statistical learning from incomplete rankings  In Proceedings of the  th International Conference on Machine
Learning  pp    Lille  France   

Sou ani       Parkes       and Xia    Computing parametric ranking models via rankbreaking  In Proceedings
of the  th International Conference on Machine Learning  Beijing  China   

Vigna     Spectral ranking  CoRR  abs   

Wu       Lin       and Weng       Probability estimates for
multiclass classi cation by pairwise coupling  Journal
of Machine Learning Research    December
 

