Guarantees for Greedy Maximization of

Nonsubmodular Functions with Applications

Andrew An Bian   Joachim    Buhmann   Andreas Krause   Sebastian Tschiatschek  

Abstract

We investigate the performance of the standard
GREEDY algorithm for cardinality constrained
maximization of nonsubmodular nondecreasing
set functions  While there are strong theoretical
guarantees on the performance of GREEDY for
maximizing submodular functions  there are few
guarantees for nonsubmodular ones  However 
GREEDY enjoys strong empirical performance
for many important nonsubmodular functions 
     the Bayesian Aoptimality objective in experimental design  We prove theoretical guarantees supporting the empirical performance  Our
guarantees are characterized by   combination
of the  generalized  curvature   and the submodularity ratio   In particular  we prove that
GREEDY enjoys   tight approximation guarantee
of  
         for cardinality constrained maximization 
In addition  we bound the submodularity ratio and curvature for several important
realworld objectives  including the Bayesian Aoptimality objective  the determinantal function
of   square submatrix and certain linear programs
with combinatorial constraints  We experimentally validate our theoretical  ndings for both
synthetic and realworld applications 

  Introduction
Many important problems  such as experimental design
and sparse modeling  are naturally formulated as   subset
selection problem  where   set function       over   Kcardinality constraint is maximized      

max

       

     

   

 Department of Computer Science  ETH Zurich  Zurich 
Switzerland  Correspondence to  Joachim    Buhmann  jbuhmann inf ethz ch  Andreas Krause  krausea ethz ch 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

where                 vn  is the ground set  Speci cally 
in experimental design  the goal is to select   set of experiments to perform such that some statistical criterion is optimized  This problem arises naturally in domains where performing experiments is costly  In sparse modeling  the task
is to identify sparse representations of signals  enabling interpretability and robustness in highdimensional statistical
problems properties that are crucial in modern data analysis 
Frequently  the standard GREEDY algorithm  Alg    is
used to  approximately  solve     For the case that      

Algorithm   The GREEDY Algorithm
Input  Ground set    set function            budget  
      
for                 do

     arg maxv   St     St            St 
St   St      

Output  SK

is   monotone nondecreasing submodular set function 
the GREEDY algorithm enjoys the multiplicative approximation guarantee of          Nemhauser et al   
Vondr ak    Krause   Golovin    This constant
factor can be improved by re ning the characterization of
the objective using the curvature  Conforti   Cornu ejols 
  Vondr ak    Iyer et al    which informally
quanti es how close   submodular function is to being
modular             and        are submodular 
However  for many applications  including experimental
design and sparse Gaussian processes  Lawrence et al 
        is in general not submodular  Krause et al 
  and the above guarantee does not hold  In practice 
however  the standard GREEDY algorithm often achieves
very good performance on these applications       in subset selection with the     squared multiple correlation  ob 
     is monotone nondecreasing if                      
                is submodular iff it satis es the diminishing
returns property                                      
for all               Assume wlog  that     is normalized 
            

Guarantees for Greedy Maximization of Nonsubmodular Functions with Applications

jective  Das   Kempe    To explain the good empirical performance  Das   Kempe   proposed the submodularity ratio    quantity characterizing how close   set
function is to being submodular 
Another important class of nonsubmodular set functions
comes as the auxiliary function when optimizing   continuous function            combinatorial constraints      
minx   supp           where supp           xi    
is the support set of      is   convex set  and   is the
independent sets of the combinatorial structure  One of
the most popular ways to solve this problem is to use
the GREEDY algorithm to maximize the auxiliary function
        maxx   supp            This setting covers
various important applications  to name   few  feature selection  Guyon   Elisseeff    sparse approximation
 Das   Kempe    Krause   Cevher    sparse
recovery  Candes et al    sparse Mestimation  Jain
et al    linear programming  LP  with combinatorial constraints  and column subset selection  Altschuler
et al    Recently  Elenberg et al    proved that if
      has Lrestricted smoothness and mrestricted strong
convexity  then the submodularity ratio of       is lower
bounded by      This result signi cantly enlarges the domain where the GREEDY algorithm can be applied 
In this paper  we combine and generalize the ideas of curvature and submodularity ratio to derive improved constant
factor approximation guarantees of the GREEDY algorithm 
Our guarantees allow us to better characterize the empirical success of applying GREEDY on   signi cantly larger
class of nonsubmodular functions  Furthermore  we bound
these characteristics for important applications  rendering
the usage of GREEDY   principled choice rather than   mere
heuristic  Our main contributions are 

  We prove the  rst tight constantfactor approximation guarantees for GREEDY on maximizing nonsubmodular nondecreasing set functions        cardinality constraint  characterized by   novel combination of
the  generalized  notions of submodularity ratio   and
curvature  

  By theoretically bounding parameters     for several
important objectives  including Bayesian Aoptimality
in experimental design  the determinantal function of  
square submatrix and maximization of LPs with combinatorial constraints  our theory implies the  rst guarantees for them 

  Lastly  we experimentally validate our theory on several realworld applications 
It is worth noting that
for the Bayesian Aoptimality objective  GREEDY generates comparable solutions as the classically used
semide nite programming  SDP  based method  but is
usually two orders of magnitude faster 

Notation  We use boldface letters          to represent vectors  and capital boldface letters          to denote matrices  xi is the ith entry of the vector    We refer to
          vn  as the ground set  We use     to denote
  continuous function  and     to represent   set function 
supp              xi     is the support set of the
vector    and              for an integer       We
denote the marginal gain of   set      in context of   set
     as                        For        we use
the shorthand       for      
  Submodularity Ratio and Curvature
In this section we provide the submodularity ratio and
curvature for general  not necessarily submodular functions  they are natural extensions of the classical ones  Let
       St         jt            be the successive
sets chosen by GREEDY  For brevity  let       jt St 
be the marginal gain of GREEDY in step   
De nition    Submodularity ratio  Das   Kempe   
The submodularity ratio of   nonnegative set function    
is the largest scalar       

   

                  

The greedy submodularity ratio is the largest scalar        

 St       St                          

  St
It is easy to see that        The submodularity ratio
measures to what extent     has submodular properties 
We make the following observations 
Remark   For   nondecreasing function     it holds   
                   is submodular iff      
De nition    Generalized curvature  The curvature of  
nonnegative function     is the smallest scalar       

                             
                 

The greedy curvature is the smallest scalar            

 ji Si               ji Si 
   

           ji   SK 

 Curvature is commonly de ned for submodular functions 
Sviridenko et al    presented   notion of curvature for monotone nonsubmodular functions  We show in Appendix   the
details of these notions and the relations to ours  Additionally 
we prove in Remark   of Appendix    that our combination of
curvature and submodularity ratio is more expressive than that
of Sviridenko et al    in characterizing the maximization of
problem     using standard GREEDY 

Guarantees for Greedy Maximization of Nonsubmodular Functions with Applications

When       or   SK    it is natural to de ne
       It is easy to observe that        Note that the
       
classical total curvature is  total       mini  
 
Remark   For   nondecreasing function     it holds 
                     is supermodular iff          If
    is submodular  then           total 
So for   submodular function  our notion of curvature is
consistent with  total  Notably     usually characterizes the
problem better than  total  as will be validated in Section  

   

  Approximation Guarantee
We present approximation guarantee of GREEDY in Theorem   Note that both versions of the submodularity ratio
and curvature apply in the proof  For brevity  we use  
and   to refer to any of these versions in the sequel  In
Section   we prove tightness of the approximation guarantees  All omitted proofs are given in Appendix   
Theorem   Let     be   nonnegative nondecreasing set
function with submodularity ratio         and curvature
        The GREEDY algorithm enjoys the following
approximation guarantee for solving problem    

   SK   

 

 

 
 

         

         

         

 

where   is the optimal solution of     and SK the output
of the GREEDY algorithm 

 

  Interpreting Theorem  
Before proving the theorem  we want to give the reader an
intuition of the results and show how our results recover
and extend several classical guarantees for the GREEDY algorithm  For the case                 is supermodular 
the approximation guarantee is lim
            
 
which gives the  rst guarantee of greedily maximizing
  nondecreasing supermodular function with bounded  
When                 is submodular  we recover the
guarantee of       Conforti   Cornu ejols   
For the case       we have   guarantee of       
 Das   Kempe    For the case           we
recover the classical guarantee of          Nemhauser
et al    We plot the constantfactor approximation
guarantees for different values of   and   in Fig    One
interesting phenomenon is that   and   play different roles 
Looking at       the approximation factor is always   independent of the value   takes  In contrast  for       the

 For the setting that GREEDY is allowed to pick more than  
elements       pick        elements  our theory can be easily
extended to show that    SK                   

Figure   Approximation guarantee  
      The blue
cross marks the classical        guarantee of GREEDY 
The red line illustrates the in uence of the curvature on the
guarantees for submodular functions  and the black line illustrates the in uence of   on the guarantees for the worstcase curvature       The green line is the guarantees for
Kcardinality constrained supermodular maximization 

approximation guarantee is        This can be interpreted as the curvature boosting the guarantees 

  Proof of Theorem  
The highlevel proof framework is based on Conforti  
Cornu ejols    where they derive the approximation
guarantee for maximizing   nondecreasing submodular
function with bounded curvature  However  adapting
the proof to nonsubmodular functions requires several
changes detailed in Section  
Proof overview  Let us denote all problem instances
of maximizing   nonnegative nondecreasing function
         Kcardinality constraint  max           to be
PK  where     is parametrized by submodularity ratio   and curvature   Let   SK       denote those
problem instances with optimal solution   and greedy solution SK  We group all problem instances PK  according to the set     SK         jm       jm          ls  
jms  where jm          jms are consistent with the order of
greedy selection  Let us denote the problem instances with
    SK               ls  as the group PK              ls 
The main idea of the proof is to investigate the worstcase
approximation ratio of each group of the problem instances
PK              ls            ls  SK  We do this by
constructing LPs based on the properties of the problem instances  By studying the structures of these LPs  we will
prove that the worstcase approximation ratio of all problem instances occurs when     SK     Thus the desired
approximation guarantee corresponds to the worstcase approximation ratio of PK   
The proof  When       or           holds naturally 
In the following  let         and         First  we
present Lemma   which will be used to construct the LPs 

 approx  guarantee modularGuarantees for Greedy Maximization of Nonsubmodular Functions with Applications

Lemma   For any      with       and any    
                let wt    St     It holds that
  Xi ji St 

          wt         

     Xi ji St 

  

  

    De ne xi     

     Pi

We now specify the constructing of the LPs  For any problem instance   SK                   ls  we know
that    SK   PK
       telescoping sum  Hence  the approximation ratio is    SK  
    which we denote
as              ls   Pi
         
    Since   is nondecreasing  xi     Plugging    
into Lemma   and considering                     we have
in total   constraints over the variables xi  which constitute the constraints of the LP  So the worstcase approximation ratio of the group PK              ls  is 
             ls    minXK

xi       xi     and 

  

 
 

 

 
     
 
   
 
   
 
 
   
 
   

 

 

 

 

  

 
 

 

 

 

 

 

   
 

 

 

 

 

 
      

 

 

 

 
 

row  
row  

 

 
 

row        
row        
row      lr 

row  ls    

row       

 
 
 

 

 
 
 

 

 

 

 

 

The following Lemma presents the key structure of the constructed LPs  which will be used to deduce the relation between the LPs of different problem instance groups 
Lemma   Assume that the optimal solution of the constructed LP is      RK
  and that         SK    For
all           it holds that            where     lr 
Proof sketch of Lemma   Assume by virture of creating  
contradiction that            We can always create  
new feasible solution      RK
  by decreasing     by some
    while increasing all the      to     by some proper
values          has smaller LP objective value  Speci 
cally  we de ne    as  for                               
              for                                     where
 ks are de ned recursively as           

      and

                 

           

                   
Claim      The new solution           All of the constraints in   are still feasible for   

         

After that the change of the LP objective is 

 LP                              

 

One can prove that the LP objective decreases 
Claim   For all                      it holds that
 LP           Equality is achieved when      
and      
Therefore we reach the contradiction that    is an optimal
solution of the constructed LP 
Given Lemma   we prove in the following Lemma  which
states that the worstcase approximation ratio of all problem instances occurs when     SK    
Lemma   For all             ls 
SK 
             ls          
So the greedy solution has objective    SK   
           

      

     

           

      

it holds that

 

 

 

 

 

 

 

 

 

 
 

 
 

 

 
 
 

that

demonstrate

submodularity

  
  
 
xl 
xl 
xq 
 
xls
 
xK

  Tightness Result
We
the
approximation guarantee in
Theorem   is tight       for
every
ratio
  and every curvature  
there exist set functions that
achieve the bound exactly 
 
Assume the ground set   con 
     
tains the elements in    
            jK  and the elements in               
         and        dummy elements  The objective
function we are going to construct will not depend on these
dummy elements       the objective value of   set does not
change if dummy elements are removed from or added to
that set  Consequently  the dummy elements will not affect
the submodularity ratio and the curvature  For the constants
              we de ne the objective function as 
 

        

        

   

 

 

 

      Xi ji    
     

      Xi ji    
                   

     

where       
       
  
      Note that       is convex nondecreasing over
     and that                           
It
is clear that         and     is monotone nondecreasing  The following lemma shows that it is generally nonsubmodular and nonsupermodular 
Lemma   For the objective in      When       it is
supermodular     When       it is submodular           
has submodularity ratio   and curvature  

Guarantees for Greedy Maximization of Nonsubmodular Functions with Applications

Considering the problem of max            we claim
that the GREEDY algorithm may output    This can be
proved by induction  One can see that          
  so GREEDY can choose    in the  rst step  Assume
in step       GREEDY has chosen St                jt 
one can verify that
    
 jt St            St  However  the optimal solution is actually   with function value as        
    So the

the marginal gains coincide 

approximation ratio is      
matches our approximation guarantee in Theorem  

       

      

      which

  Applications
We consider several important realworld applications and
their corresponding objective functions  We show that the
submodularity ratio and the curvature of these functions
can be bounded and  hence  the approximation guarantees
from our theoretical results are applicable  All the omitted
proofs are provided in Appendix   

  Bayesian Aoptimality in Experimental Design
In Bayesian experimental design  Chaloner   Verdinelli 
  the goal is to select   set of experiments to perform
     some statistical criterion is optimized       the variance of certain parameter estimates is minimized  Krause
et al    investigated several criteria for this purpose 
amongst others the Bayesian Aoptimality criterion  This
criterion is used to maximally reduce the variance in the
posterior distribution over the parameters  In general  the
criterion is not submodular as shown in Krause et al   
Section  
Formally 
stimuli
            xn  each xi   Rd  which constitute the data
matrix     Rd    Let us arrange   set      of stimuli as
  matrix XS    xv          xvs    Rd    Let     Rd be
the parameter vector in the linear model yS             
where   is the Gaussian noise with zero mean and variance
                  and yS is the vector of dependent
variables  Suppose the prior takes the form of an isotropic
Gaussian                        Then 
                     XS      
 yS
     

assume there are   experimental

 XS

This implies that  yS        XSX     The Aoptimality objective is de ned as 

FA      tr    tr yS  

  tr    tr     XSX    

 

The following Proposition gives bounds on the submodularity ratio and curvature of  

 

 

Let

Proposition   Assume normalized stimuli       kxik  
the spectral norm of   be kXk 
       
Then     The objective in   is monotone nondecreasing     Its submodularity ratio   can be lower bounded
kXk kXk  and its curvature   can be upper
by
bounded by    
kXk kXk 
  The Determinantal Function
The determinantal function of   square submatrix is widely
used in many areas       in determinantal point processes
 Kulesza   Taskar    and active set selection for sparse
Gaussian processes  Monotone nondecreasing determinantal functions appear in the second problem  Assume   is
the covariance matrix parameterized by   positive de nite
kernel  In the Informative Vector Machine  Lawrence et al 
  the information gain of   subset of points      is
  log       where

 

        det        

 

where   is the noise variance in the Gaussian process
model     is the square submatrix with both its rows and
columns indexed by    Although log       is submodular        is in general not submodular  The approximation
guarantee of GREEDY for maximizing log       does not
translate to   guarantee for maximizing       The following Proposition characterizes  
Proposition            in   is supermodular  its curvature is      Let the eigenvalues of           be
           The greedy submodularity ratio of
      can be lower bounded by     
       
 QK
  LPs with Combinatorial Constraints
LPs with combinatorial constraints appear frequently in
practice  Consider the following example  Suppose that
  is the set of all products   company can produce  Given
budget constraints on the raw materials needed  companies
consider the LP maxx Phd  xi  where   is the vector of
pro ts for the individual products and where   is   polytope representing the continuous constraints  The above
LP can be used to assess the pro   maximizing production
plan  Usually the company needs to consider combinatorial constraints as well  For instance  the company has at
most   production lines  thus they have to select   subset
of   products to produce  Often this kind of problems can
be formalized as maxx   supp   Ihd  xi  where   is the
independent set of the combinatorial structure  Hence   
natural auxiliary set function is 

        maxsupp        Phd  xi         

 
 By Weyl   inequality    naive upper bound is kXk   pn 

Guarantees for Greedy Maximization of Nonsubmodular Functions with Applications

 

 
      Rm

Let          Rn               Ax          
      Rm  
In general       in  
Rn
is nonsubmodular as illustrated by two examples in Appendix    Upper bounding the curvature is equivalent to
lower bounding             
  which can be   in the
worst case  However  the submodularity ratio can be lower
bounded by   nonzero scalar 
Proposition            in   is   normalized nondecreasing set function     With regular nondegenerancy assumptions  details in Appendix    its submodularity ratio
can be lower bounded by      

            

  More Applications
Many realworld applications can bene   from the theory in
this work  for instance  subset selection using the    objective  sparse modeling and the budget allocation problem
with combinatorial constraints  Details on these applications are deferred to Appendix   

 

 
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 
 
 

 

 
 
 
 
 
 

 

 

 

 

 

 

Greedy
SDP

 

 

 

 
 

 

 

 

 cid total
 cid  
 cid  

 

 

 

 

 

 

    Objective values OPT

    Parameters

Figure   Results on the Boston Housing data 

  Experimental Results
We empirically validated approximation guarantees characterized by the submodularity ratio and the curvature for
several applications  Since it is too time consuming to calculate the full versions of   and   using exhaustive search 
we only calculated the greedy versions        All averaged results are from   repeated experiments  Source
code is available at https github com bianan 
nonsubmodular max  More results are put in Appendix   

  Bayesian Experimental Design
We considered the Bayesian Aoptimality objective for
both synthetic and realworld data  In all experiments  we
normalized the data points to have unit  norm 
Realworld results  We used the Boston Housing Data 
 All experiments were implemented using Matlab  We used

the SDP solver provided by CVX  Version  

 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

Greedy
SDP

 

 

 

 

 

 

 

 cid total
 cid  
 cid  

 

 

 

 

 

 

 

    Correlation   

 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

Greedy
SDP

 

 

 

 

 

 

 

 cid total
 cid  
 cid  

 

 

 

 

 

 

 

    Correlation   

Figure   Results for Aoptimality on synthetic data 

The dataset  has   features       crime rate  property tax
rates  etc  and   samples  To be able to quickly calculate
the parameters and optimal solution by exhaustive search 
the  rst       samples were used  As   baseline  we used
an SDPbased algorithm  abbreviated as SDP  details are
available in Appendix    Results are shown in Fig    for
varying values of    In Fig     we can observe that both
GREEDY and SDP compute nearoptimal solutions  From
Fig     we can see that the greedy submodularity ratio   
is close to   and that the greedy curvature    is less than
  while the classical curvature  total is always    the worstcase value  This implies that the classical total curvature
 total characterizes the considered maximization problems
less accurate than the greedy curvature 
Synthetic results  We generated random observations
from   multivariate Gaussian distribution with different
correlations  To be able to assess the ground truth  we used
      samples with       features  Fig    shows the
results with correlation    rst column  and    second
column  respectively  The  rst row shows the average objective values over the optimal value with error bars  and
the second row shows the parameters  One can observe that
GREEDY always obtains nearoptimal solutions and that
these solutions are roughly comparable with those obtained
by the SDP  The classical curvature  total is always close to
  while    take smaller values  and    takes values close
to   thus characterize the performance of GREEDY better 
Mediumscale synthetic experiments  To compare the
runtime of SDP and GREEDY  we considered mediumscale datasets  we cannot report results on larger datasets
because of the huge computational demands of the SDP 

 https archive ics uci edu ml datasets 

Housing

Guarantees for Greedy Maximization of Nonsubmodular Functions with Applications

 
 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 

 
 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 

 
 

 
 
 
 
 
 

SDP
Greedy

SDP
Greedy

 

 

 
 

 

 

 

 

 

 

 

                corr     

                corr     

Figure   Aoptimality on mediumscale problems

Fig    shows the objective value achieved by GREEDY and
SDP for different numbers of features   and numbers of
samples    as well as the correlations  We can observe
that GREEDY computes solutions that are on par or superior
to those of SDP  In Table   we summarize the runtime of
GREEDY and SDP for different values of   and    for correlation   Furthermore  we show the ratio of runtimes of
the two algorithms  We can observe that GREEDY is usually two orders of magnitude faster than SDP 

Table   Runtime in seconds of GREEDY and SDP  The
last row shows the ratio of runtimes of SDP and GREEDY 

    
    
 
 
 

    
    
 
 
 

    
    
 
 
 

    
    
 
 
 

    
    
 
 
 

GREEDY
SDP
SDP

GREEDY

  LPs with Combinatorial Constraints

 

 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

OPT
Greedy

 

 

 cid total

 

 

 cid  

 

 

 

 cid  

 

 
 
 
 
 
 
 
 
 
 

 

 
 
 
 
 
 
 

OPT
Greedy
 

 cid  

 

 

 

 cid total

 

 cid  

    We set           and set    as   The  rst
row of Fig    plots the optimal LP objective  calculated
using exhaustive search  and the LP objective returned by
GREEDY  The second row shows the curvature and submodularity ratio  The  rst column  Fig      presents the results for             while the second column  Fig     
presents that for             Note the greedy submodularity ratio takes values between     and   and that
the curvature is close to the worstcase value of   These
observations are consistent with the theory in Section  

  Determinantal Functions Maximization
We experimented with synthetic and realworld data  For
synthetic data  we generated random covariance matrices
    Rn   with uniformly distributed eigenvalues in    
We set           In Fig     left  we plot the optimal determinantal objective value and the value achieved
by GREEDY  Fig     right  traces the greedy submodularity ratio     Since the determinantal objective is supermodular  so the approximation guarantee equals to     We
can see that    can reasonably predict the performance of
GREEDY 

OPT
Greedy

 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 

 
 
 

 
 

 

 

 

 

 

 

 

 

 

 

 

 

 
 
 

 

 
 
 
 
 
 

 

 

 

 

 cid  

 

 

Figure   Synthetic result  Left  objective value  right    

For realworld data  we considered an active set selection
task on the CIFAR  dataset  The  rst       images in the test set were used to calculate the covariance
matrix with an squared exponential kernel    xi  xj   
exp kxi   xjk      was set to be   The results in
Fig    shows similar results as with the synthetic data 

 

 

 
 
 

 

 
 
 
 
 
 

 

 

 
 
 

 

 
 
 
 
 
 

 

 

 

 

 

 

 

 

 

               

 

 

 

 

 

               

Figure   Results for LPs with Kcardinality constraints 
We generated synthetic LPs as follows  Firstly  we generated the matrix     Rm  
  Aij       by drawing
all entries independently from   uniform distribution on

 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 

OPT
Greedy

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 
 
 
 
 
 
 
 
 

 cid  

 

 

 

 

 

 

 

Figure   CIFAR  result  Left  objective value  right    

 https www cs toronto edu kriz cifar 

html

Guarantees for Greedy Maximization of Nonsubmodular Functions with Applications

  Related Work
In this section we brie   discuss related work on various
notions of nonsubmodularity and the optimization of nonsubmodular functions  Further details in Appendix   
Relation to Conforti   Cornu ejols   in deriving
approximation guarantees 
In proving Theorem   we
use the similar proof framework       utilizing LP formulations to analyze the worstcase approximation ratios
of different groups of problem instances  as that in Conforti   Cornu ejols   where they derive guarantees
for maximizing submodular functions  However  since we
are proving guarantees for nonsubmodular functions  the
speci   techniques on how to manipulate these LPs are
different  Speci cally    The building block to construct
LPs  Lemma   is different    The technique to prove the
structure of the LPs  which corresponds to Lemma   is
signi cantly different for   submodular function and   nonsubmodular function  and Lemma   is the key to investigate
the worstcase approximation ratios of different groups of
problem instances    The speci   way to prove Lemma  
is also different since the constraints of the LPs are different for submodular and nonsubmodular functions 
Submodularity ratio and curvature  Curvature is typically de ned for submodular functions  Sviridenko et al 
  present   notion of curvature for monotone nonsubmodular functions  Appendix   provides details of that
notion and relates it to our de nition  Yoshida   prove
an improved approximation ratio for knapsackconstrained
maximization of submodular functions with bounded curvature  Submodularity ratio  Das   Kempe    is  
quantity characterizing how close   function is to being
submodular 
Approximate submodularity  Krause et al    de 
 ne approximately submodular functions with parameter
      as those functions   that satisfy an approximate
diminishing returns property                     it
holds that                   GREEDY yields   solution with objective    SK                   for
maximizing   monotone          Kcardinality constraint 
Du et al    study the greedy maximization of nonsubmodular potential functions with restricted submodularity and shifted submodularity  Restricted submodularity
refers to functions which are submodular only over some
collection of subsets of    and shifted submodularity can
be viewed as   special case of the approximate diminishing
returns as de ned above  Recently  Horel   Singer  
study  approximately submodular functions  which arised
from their research on  noisy  submodular functions   
function     is  approximately submodular if there exists   submodular function                           
                 

Weak submodularity  Borodin et al    study weakly
submodular functions       montone  nomalized functions
        
for any        it holds                       
                                    For   function    
we show in Remark   that the following two facts do not
imply each other         is weakly submodular  ii  The
submodularity ratio of     is strictly larger than   and its
curvature is strictly smaller than  
Other notions of nonsubmodularity  Feige   Izsak
  introduce the supermodular degree as   complexity
measure for set functions  They show that   greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree  Zhou   Spanos   use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm  Buchbinder et al    for maximizing   nonmonotone function 
Optimization of nonsubmodular functions 
The
submodularsupermodular procedure has been proposed
to minimize the difference of two submodular functions
 Narasimhan   Bilmes    Iyer   Bilmes   
Jegelka   Bilmes   present the problem of minimizing  cooperative cuts  which are nonsubmodular in
general  and propose ef cient algorithms for optimization  Kawahara et al    analyze unconstrained minimization of the sum of   submodular function and   treestructured supermodular function  Bai et al    investigate the minimization of the ratio of two submodular functions  which can be solved with bounded approximation
factor 

  Conclusion
We analyzed the guarantees for greedy maximization of
nonsubmodular nondecreasing set functions  By combining the  generalized  curvature   and submodularity ratio
  for generic set functions  we prove the  rst tight approximation bounds in terms of these de nitions for greedily maximizing nondecreasing set functions  These approximation bounds signi cantly enlarge the domain where
GREEDY has guarantees  Furthermore  we theoretically
bounded the parameters   and   for several nontrivial applications  and validate our theory in various experiments 

ACKNOWLEDGEMENTS
The authors would like to thank Adish Singla         Levy
and Aurelien Lucchi for valuable discussions  This research was partially supported by ERC StG   and
the Max Planck ETH Center for Learning Systems  This
work was done in part while Andreas Krause was visiting
the Simons Institute for the Theory of Computing 

Guarantees for Greedy Maximization of Nonsubmodular Functions with Applications

References
Altschuler  Jason  Bhaskara  Aditya  Fu  Gang  Mirrokni 
Vahab  Rostamizadeh  Afshin  and Zadimoghaddam 
Morteza  Greedy column subset selection  New bounds
and distributed algorithms 
In ICML  pp   
 

Bach  Francis 

Learning with submodular functions 
  convex optimization perspective  Foundations and
Trends    in Machine Learning     
Bai  Wenruo  Iyer  Rishabh  Wei  Kai  and Bilmes  Jeff 
Algorithms for optimizing the ratio of submodular functions  In ICML  pp     
Bertsimas  Dimitris and Tsitsiklis  John 

Introduction to
Linear Optimization  Athena Scienti     st edition 
 

Bian  Andrew An  Mirzasoleiman  Baharan  Buhmann 
Joachim    and Krause  Andreas  Guaranteed nonconvex optimization  Submodular maximization over
continuous domains  In AISTATS  pp     

Borodin  Allan  Le  Dai Tri Man  and Ye  Yuli  Weakly
submodular functions  arXiv preprint arXiv 
 

Boyd  Stephen and Vandenberghe  Lieven  Convex opti 

mization  Cambridge university press   

Buchbinder  Niv  Feldman  Moran  Naor  Joseph  and
Schwartz  Roy  Submodular maximization with cardinality constraints  In SODA  pp     

Candes  Emmanuel    Romberg  Justin    and Tao  Terence  Stable signal recovery from incomplete and inaccurate measurements  Communications on Pure and
Applied Mathematics     

Chaloner  Kathryn and Verdinelli  Isabella  Bayesian experimental design    review  Statistical Science   
   

Conforti  Michele and Cornu ejols    erard  Submodular
set functions  matroids and the greedy algorithm  tight
worstcase bounds and some generalizations of the radoedmonds theorem  Discrete Applied Mathematics   
   

Du  DingZhu  Graham  Ronald    Pardalos  Panos   
Wan  PengJun  Wu  Weili  and Zhao  Wenbo  Analysis
of greedy approximations with nonsubmodular potential
functions  In SODA  pp     

Elenberg  Ethan    Khanna  Rajiv  Dimakis  Alexandros    and Negahban  Sahand  Restricted strong convexity implies weak submodularity 
arXiv preprint
arXiv   

Feige  Uriel and Izsak  Rani  Welfare maximization and the
supermodular degree  In Proceedings of the Fourth Conference on Innovations in Theoretical Computer Science 
pp     

Guyon  Isabelle and Elisseeff  Andr    An introduction to
variable and feature selection  Journal of machine learning research   Mar   

Horel  Thibaut and Singer  Yaron  Maximization of approximately submodular functions  In NIPS  pp   
   

Iyer  Rishabh and Bilmes  Jeff  Algorithms for approximate minimization of the difference between submodular functions  with applications 
In Proceedings of the
TwentyEighth Conference on Uncertainty in Arti cial
Intelligence  pp     

Iyer  Rishabh    Jegelka  Stefanie  and Bilmes  Jeff    Curvature and optimal algorithms for learning and minimizing submodular functions  NIPS  pp     
Jain  Prateek  Tewari  Ambuj  and Kar  Purushottam  On iterative hard thresholding methods for highdimensional
mestimation  In NIPS  pp     

Jegelka  Stefanie and Bilmes  Jeff  Submodularity beyond submodular energies  coupling edges in graph cuts 
In IEEE Conference on Computer Vision and Pattern
Recognition  pp     

Kawahara  Yoshinobu  Iyer  Rishabh    and Bilmes  Jeff   
On approximate nonsubmodular minimization via treestructured supermodularity 
In AISTATS  pp   
 

Krause  Andreas and Cevher  Volkan  Submodular dictioIn ICML  pp 

nary selection for sparse representation 
   

Das  Abhimanyu and Kempe  David  Algorithms for subset
selection in linear regression  In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing 
pp     

Krause  Andreas and Golovin  Daniel 

Submodular
function maximization 
In Tractability  Practical
Approaches to Hard Problems  Cambridge University
Press  February  

Das  Abhimanyu and Kempe  David  Submodular meets
spectral  Greedy algorithms for subset selection  sparse
approximation and dictionary selection 
In ICML  pp 
   

Krause  Andreas  Singh  Ajit  and Guestrin  Carlos  Nearoptimal sensor placements in gaussian processes  Theory  ef cient algorithms and empirical studies  Journal
of Machine Learning Research   Feb   

Guarantees for Greedy Maximization of Nonsubmodular Functions with Applications

Kulesza  Alex and Taskar  Ben  Determinantal point processes for machine learning  Foundations and Trends   
in Machine Learning     

Lawrence  Neil  Seeger  Matthias  and Herbrich  Ralf  Fast
sparse gaussian process methods  The informative vector machine  NIPS  pp     

Narasimhan  Mukund and Bilmes  Jeff    submodularsupermodular procedure with applications to discriminative structure learning  In Proceedings of the TwentyFirst Conference on Uncertainty in Arti cial Intelligence  pp     

Nemhauser  George    Wolsey  Laurence    and Fisher 
Marshall    An analysis of approximations for maximizing submodular set functions    Mathematical Programming     

Soma  Tasuku  Kakimura  Naonori  Inaba  Kazuhiro  and
Kawarabayashi  Kenichi  Optimal budget allocation 
Theoretical guarantee and ef cient algorithm  In ICML 
pp     

Sviridenko  Maxim  Vondr ak  Jan  and Ward  Justin  Optimal approximation for submodular and supermodular
optimization with bounded curvature  arXiv preprint
arXiv   

Vondr ak  Jan  Optimal approximation for the submodular
welfare problem in the value oracle model  In Proceedings of the Fortieth Annual ACM Symposium on Theory
of Computing  pp     

Vondr ak  Jan  Submodularity and curvature  the optimal
algorithm  RIMS Kokyuroku Bessatsu     
 

Yoshida  Yuichi  Maximizing   monotone submodular
function with   bounded curvature under   knapsack
constraint  arXiv preprint arXiv   

Zhou  Yuxun and Spanos  Costas    Causal meets submodular  Subset selection with directed information  In
NIPS  pp     

