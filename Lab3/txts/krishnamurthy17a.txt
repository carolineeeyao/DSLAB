Active Learning for CostSensitive Classi cation

Akshay Krishnamurthy   Alekh Agarwal   TzuKuo Huang   Hal Daum   III   John Langford  

Abstract

We design an active learning algorithm for
costsensitive multiclass classi cation  problems
where different errors have different costs  Our
algorithm  COAL  makes predictions by regressing to each label   cost and predicting the smallest  On   new example  it uses   set of regressors
that perform well on past data to estimate possible costs for each label  It queries only the labels
that could be the best  ignoring the sure losers 
We prove COAL can be ef ciently implemented
for any regression family that admits squared loss
optimization 
it also enjoys strong guarantees
with respect to predictive performance and labeling effort  Our experiment with COAL show signi cant improvements in labeling effort and test
cost over passive and active baselines 

  Introduction
The  eld of active learning studies how to ef ciently elicit
relevant information so learning algorithms can make good
decisions  Almost all active learning algorithms are designed for binary classi cation problems  leading to the
natural question  How can active learning address more
complex prediction problems  Multiclass and importanceweighted classi cation require only minor modi cations
but we know of no active learning algorithms that enjoy
theoretical guarantees for more complex problems 
One such problem is costsensitive multiclass classi cation
 CSMC  In CSMC with   classes  passive learners receive
input examples   and cost vectors     RK  where     
is the cost of predicting label   on      natural design
for an active CSMC learner then is to adaptively query the

 University of Massachusetts  Amherst  MA  Microsoft Research  New York  NY  Uber Advanced Technology Center  Pittsburgh  PA  University of Maryland  College Park  MD  Correspondence to  Akshay Krishnamurthy  akshay cs umass edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

 Cost here refers to prediction cost and not labeling effort or

the cost of acquiring different labels 

costs of only    possibly empty  subset of labels on each
   Since measuring label complexity is more nuanced in
CSMC       is it more expensive to query three costs on  
single example or one cost on three examples  we track
both the number of examples for which at least one cost is
queried  along with the total number of cost queries issued 
The  rst corresponds to    xed human effort for inspecting
   The second captures the additional effort for judging the
cost of each prediction  which depends on the number of
labels queried   By querying   label  we mean querying the
cost of predicting that label given an example 
In this setup  we develop   new active learning algorithm for CSMC called Cost Overlapped Active Learning  COAL  COAL assumes access to   set of regression functions  and  when processing an example    it uses
the functions with good past performance to compute the
range of possible costs that each label might take  Naturally  COAL only queries labels with large cost range  but
furthermore  it only queries     that could possibly have the
smallest cost  avoiding the uncertain  but surely suboptimal
labels  The key algorithmic innovation is an ef cient way
to compute the cost range realized by good regressors  This
computation  and COAL as   whole  only requires that the
regression set admits ef cient squared loss optimization 
in contrast with prior algorithms that require   loss optimization  Beygelzimer et al    Hanneke   
Among our results  we prove that when processing    unlabeled  examples with   classes and   regressors 
  The algorithm needs to solve   Kn  log    regression
problems over the function class  Cor    which can be
done in polynomial time for convex regression sets 

  With no assumptions on the noise in the probthe algorithm achieves generalization error

lem 

   pK ln     

from      pK ln     examples

costs
 Thms   
and   where     are the disagreement coef cients
 Def    The worst case offers minimal improvement
over passive learning  akin to binary classi cation 

and

requests

     pK ln    

  With   favorable noise assumption  As   

the algorithm achieves generalization error      ln     
while requesting    Kc    ln     labels from
         ln     examples  Cor    Thm    where
      suppresses logarithmic dependence on   and   

Active Learning for CostSensitive Classi cation

butional assumptions  The online learning community has
also studied active learning of linear separators under adversarial assumptions  Cavallanti et al    Dekel et al 
  Orabona   CesaBianchi    Agarwal   
Our work falls into the framework of disagreementbased
active learning  which studies general hypothesis spaces
typically in an agnostic setup  see Hanneke   for an
excellent survey  Existing results study binary classi cation  while our work generalizes to CSMC  assuming that
we can accurately predict costs using regression functions
from our class  The other main differences are that our
query rule checks the range of predicted costs for   label 
and we use   square loss oracle to search the version space 
In contrast  prior work either explicitly enumerates the version space  Balcan et al    Zhang   Chaudhuri   
or uses     loss classi cation oracle for the search  Dasgupta et al    Beygelzimer et al      Huang
et al    In most instantiations  the oracle solves an
NPhard problem and so does not directly lead to an ef 
cient algorithm  although practical implementations using
heuristics are still quite effective  Our approach instead
uses   squaredloss regression oracle  which can often be
implemented ef ciently via convex optimization and leads
to   polynomial time algorithm 
Supervised learning oracles that solve NPhard optimization problems in the worst case have been used in other
problems including contextual bandits  Agarwal et al 
  Syrgkanis et al    and structured prediction  Daum   III et al    Thus we hope that our work
can inspire new algorithms for these settings as well 
Lastly  we mention that square loss regression has been
used to estimate costs for passive CSMC  Langford  
Beygelzimer    but  to our knowledge  using   square
loss oracle for active CSMC is new 

  Problem Setting and Notations
We study costsensitive multiclass classi cation problems
with   classes  where there is an instance space       label
space                  and   distribution   supported on
         If             we refer to   as the costvector 
where      is the cost of predicting           classi er
         has expected cost                and we
aim to  nd   classi er with minimal expected cost 
Let                  denote   set of base regressors
and let     GK denote   set of vector regressors where
the yth coordinate of      is written as        The set
of classi ers under consideration is      hf        
 In general  labels just serve as indices for the cost vector in
CSMC  and the data distribution is over        pairs instead of
       pairs as in binary and multiclass classi cation 

Figure   Empirical evaluation of COAL on Reuters text categorization dataset  Active learning achieves better test cost than passive  with   factor of   fewer queries  See Section   for details 

        is   safety parameter and   is   constant 

We also discuss some intuitive examples highlighting the
bene ts of using COAL 
CSMC provides   more expressive language for success
and failure than multiclass classi cation  which allows algorithms to better tradeoff errors and broadens potential
applications  For example  CSMC can naturally express
partial failure in hierarchical classi cation  Silla Jr    Freitas    Experimentally  we show that COAL substantially outperforms the passive learning baseline with orders
of magnitude savings in the labeling effort on   number of
hierarchical classi cation datasets  see Fig    for comparison between passive learning and COAL on Reuters text
categorization 
CSMC also forms the basis of learning to avoid cascading
failures in joint prediction tasks  Daum   III et al   
Ross   Bagnell    Chang et al    like structured
prediction and reinforcement learning  As our second application  we consider learning to search algorithms for
joint  or structured  prediction  which operate by   reduction to CSMC  In this reduction  evaluating the cost of  
class often involves   computationally expensive  rollout 
so using an active learning algorithm inside such    passive 
joint prediction method can lead to signi cant computational savings  We show that using COAL within the AGGRAVATE algorithm  Ross   Bagnell    Chang et al 
  to  
  reduces the number of rollouts by   factor of  
on several joint prediction tasks 

 

Related Work  Active learning is   thriving research area
with many theoretical and empirical studies  We recommend the survey of Settles   for an overview of more
empirical research  We focus here on theoretical results 
Castro   Nowak   study active learning with nonparametric decision sets  while Balcan et al    Balcan
  Long   focus on linear representations under distri 

Active Learning for CostSensitive Classi cation

where each   de nes   classi er hf         by

hf       argmin

        

 

 

Given   set of examples and queried costs  we often restrict
attention to regression functions that predict these costs
well  and assess the uncertainty in their predictions given
  new example    For   subset of regressors        we
measure uncertainty over possible cost values for   with

 

 

                
 maxg       

         
 ming       

 

  

 

 

  

 

For vector regressors        we de ne the cost range
for   label   given   as                  GF     where
GF                        is the set of base regressors
induced by   for   
When using   set of regression functions for   classi cation
task  it is natural to assume that the expected costs under  
can be predicted well by some function in the set  This
motivates the following realizability assumption 
Assumption    Realizability  De ne the Bayesoptimal
regressor     which has            Ec           
 with                  We assume that         
While     is always well de ned  note that the cost itself
may be noisy  In comparison with our assumption  the existence of   zerocost classi er in    which is often assumed
in active learning  is stronger  while the existence of hf   in
  is weaker but has not been leveraged in active learning 
In typical settings  the set   is extremely large  which introduces   computational challenge of managing this set 
To address this challenge  we leverage existing algorithmic research on supervised learning and assume access to
  regression oracle for    Given an importanceweighted
dataset      xi  ci  wi  
   the regression oracle computes

nXi 

ORACLE      argmin
   

wi   xi    ci 

 

In many cases this is   convex problem and can be solved
ef ciently  In the special case of linear functions  this is
just least squares and can be computed in closed form 
To measure the labeling effort  we track the number of examples for which even   single cost is queried as well as
the total number of queries  This bookkeeping captures
settings where the editorial effort for inspecting an example is high  but each cost requires minimal further effort 
as well as those where each cost requires substantial effort 
Formally  we de ne Qi    to be the indicator that the algorithm queries label   on the ith example and measure

    

nXi  

Qi    and     

nXi Xy

Qi   

 

  

end for

   
   

Algorithm   Cost Overlapped Active Learning  COAL 
  Input  Regressors    failure probability        
safety parameter        
  Set       pi             log       
      
           
  Set         
  for                   do
gi     arg ming   bRi        See Eq   
 
De ne fi    gi    
 
Gi              bRi         bRi gi           
 
Receive new example    Qi              
 
for every       do
 
 bRi    
cc      MAXCOST             
 
 bRi    
cc      MINCOST             
 
 
              cc      miny cc   
 
if          then
 
Qi        if         andcc     cc        
 
end if
 
Query costs of each   with Qi       
 
  end for
  Cost Overlapped Active Learning
The pseudocode for our algorithm  Cost Overlapped Active Learning  COAL  is given in Algorithm   Given an
example    COAL queries the costs of some of the labels
  for    These costs are chosen by   computing   set
of good regression functions based on the past data      
the version space    computing the range of predictions
achievable by these functions for each    and   querying
each   that could be the best label and has substantial uncertainty  We now detail each step 
To compute an approximate version space we  rst  nd the
regression function that minimizes the empirical risk for
each label    which at round   is 

bRi        

 
     

  Xj 

   xj    cj   Qj   

 

Recall that Qj    is the indicator that we query label   on
the jth example  Computing the minimizer requires one oracle call  We implicitly construct the version space Gi   
in Line   as the regressors with low square loss regret to
the empirical risk minimizer  The tolerance on this regret
is    at round    which depends on the safety parameter
        in the algorithm  When   is large  the tolerance
is also large and the algorithm issues many queries  Conversely when   is small the algorithm is more aggressive 
However  for any strictly positive   the de nition of   
ensures that                for all      
COAL then computes the maximum and minimum costs
predicted by the version space Gi    on the new example
   Since the true expected cost is          and         

Active Learning for CostSensitive Classi cation

Gi    these quantities serve as   con dence bound for this
value  The computation is done by the MAXCOST and
MINCOST subroutines which produce approximations to
    Gi    and     Gi     Eq    respectively 
Finally  using the predicted costs  COAL issues  possibly
zero  queries  The algorithm queries any nondominated
label that has   large cost range  where   label is nondominated if its estimated minimum cost is smaller than
the smallest maximum cost  among all labels  and the cost
range is the difference between the label   estimated maximum and minimum costs 
Intuitively  COAL queries the cost of every label which
cannot be ruled out as having the smallest cost on    but
only if there is suf cient ambiguity about the actual value
of the cost  The idea is that labels with little disagreement
do not provide much information for further reducing the
version space  since by construction all functions would
suffer similar loss  Moreover  only the labels that could
be the best need to be queried at all  since the costsensitive
performance of   hypothesis hf depends only on the label
that it predicts  Hence  labels that are dominated or have
small cost range need not be queried 
Similar query rules appear in prior works on binary and
multiclass classi cation  Orabona   CesaBianchi   
Dekel et al    Agarwal    but specialized to linear
representations  The key advantage of the linear case is that
the set Gi     formally    different set with similar properties  along with      and      have closed form expressions  so that the algorithms are easily implemented  However  with   general set   and   regression oracle  computing these con dence intervals is less straightforward  We
use the MAXCOST and MINCOST subroutines  and discuss
this aspect of our algorithm next 

  Ef cient Computation of Cost Range
In this section  we describe the MAXCOST subroutine
which uses the oracle to approximate the maximum cost
on label   realized by Gi     recall de nition in Eq   
Describing the algorithm requires some additional notation  Given the empirical risk functional bR       over  

set of examples  we suppress the subscript as the number
of examples is  xed here  we de ne   weighted risk functional incorporating   fresh unlabeled example   as

 

eR               bR                    

We also de ne   set of nearoptimal regressors

Finding argming eR             involves   single oracle call 
   bR              
       ng      bR         min

 MINCOST is similar and omitted due to space constraints 

Algorithm   MAXCOST

  Input             risk functional bR    
  gmin   argming   bR      
  while            do

                   

gc   argming   eR             see Eq   
If gc          see Eq    output gc       
 gl  gh    BSEARCH           bR    
If gh         output gh   
Else     max gl        gh          
   

 
 
 
 
 
  end while
  return   
Algorithm   BINARYSEARCH BSEARCH 

 

  Input                risk functional bR    
                      
  while  wt    wt      do
 
 
 
 
  end while

  gt   argming   eR    wt       
wt   wt wt  
If gt         wt    wt  wt     wt   
Else wt    wt  wt     wt 
         
  return      argming eR    wt        and gh  
argming eR    wt         
Thus at round    the set Gi    in COAL is equivalent to
        although we will use different radii here 
The algorithm for the maximum cost approximation  displayed in Algorithm   is based on   form of binary search 
When invoked with   radius parameter   the algorithm
maintains an interval      that contains          
and uses   binary search to re ne the interval  Using    xed
cost   and starting with some initial weight    at each iteration  the binary search computes argming eR             and
veri es if the resulting regressor belongs to       If it
does  it increases    and otherwise it shrinks    Once   termination criteria is reached  the BINARYSEARCH routine
outputs two regressors     gh  that provide new upper and
lower bounds on           The MAXCOST routine
terminates and outputs gh    if it has reasonable empirical
regret  Otherwise  it updates parameters for the next binary
search based on      gh   
Our main algorithmic result guarantees that this procedure
produces an adequate approximation to          
without requiring too many oracle calls 
Theorem   For any          and   the MAXCOST algorithm outputs    satisfying

                               

Further  the algorithm uses    log  oracle calls 
An immediate consequence of the theorem is   bound on
the oracle complexity of COAL 

Active Learning for CostSensitive Classi cation

Corollary   Over the course of   examples  COAL makes
  Kn  log    calls to the square loss oracle 

Thus COAL can be implemented in polynomial time for
any set   that admits ef cient square loss optimization 
However  in practice  the number of oracle calls and the
oracle itself are too computationally demanding to scale to
larger problems  Our implementation alleviates this with
an alternative heuristic approximation based on   sensitivity analysis of the oracle  which we detail in Section  

  Generalization Analysis
In this section  we derive generalization guarantees for
COAL  Our analysis assumes that the regressor set   is
large  but  nite  We study two different settings  one with
minimal assumptions and one lownoise setting 
Our lownoise assumption is related to the Massart noise
condition  Massart     ed elec    which in binary classi cation posits that the Bayes optimal predictor is bounded
away from   for all    Our condition generalizes this to
CSMC and posits that the expected cost of the best label is
separated from the expected cost of all other labels 
Assumption     distribution   supported over       
pairs satis es the Massart noise condition  if there exists
    such that for all    with         

             min
      
where        argminy         

            

The Massart noise condition describes favorable prediction
problems that lead to sharper generalization and label complexity bounds for COAL  COAL can also be analyzed
under   milder assumption inspired by the Tsybakov noise
condition  an analysis that we defer to an extended version 
Our results depend on the noise level in the problem  which
we de ne using the following quantity  given any    

     Pr
   

  min
      

                         

 

   describes the probability that the expected cost of the
best label  which is      is close to the expected cost of
the second best label  When    is small for large   the
labels are wellseparated so learning is easier  For instance 
under   Massart condition        for all      
We now state our generalization guarantee 
Theorem   For any       for all         with probability at least       we have
      
Ex     hfi        hf       min

     

    

 

ples  To compare  the standard generalization bound is

In the worst case  we bound    by   and optimize for

Line   of Algorithm   and hfi is de ned in Equation  

  fi is as de ned in
where            log        
  to obtain an    pK log      bound after   sam 
   plog       Littlestone   Warmuth    which
agrees with our bound since            in our case 
However  since the bound captures the dif culty of the
CSMC problem as measured by    we can obtain   sharper
result under Assumption   by setting      
Corollary   Under Assumption   for any       for all
        with probability at least       we have
    

Ex     hfi        hf      

 

  

Thus  Massarttype conditions lead to   faster       convergence rate  This agrees with the literature on active
learning for classi cation  Massart     ed elec    and
can be viewed as   generalization to CSMC  Importantly 
both generalization bounds recover the optimal rates and
are independent of the safety parameter  

  Label Complexity Analysis
Without distributional assumptions  the label complexity
of COAL can be      just as in the binary classi cation
case  since there may always be confusing labels that force
querying  In line with prior work  we introduce two disagreement coef cients that characterize favorable distributional properties  We  rst de ne   set of good classi ers 
the costsensitive regret ball 

Fcsr     nf           hf         hf       ro  

We may now de ne the disagreement coef cients 
De nition    Disagreement coef cients  De ne
                 Fcsr    and
DIS                       csr    hf           hf   
Then the disagreement coef cients are de ned as 

    sup
   
    sup
   

 
                            DIS      
  Xy
 
                     DIS        

Intuitively  the conditions in both coef cients correspond
to the checks on the domination and cost range of   label in Lines   and   of Algorithm   Speci cally  when

Active Learning for CostSensitive Classi cation

    DIS       there is confusion about whether   is the optimal label or not  and hence   is not dominated  The condition on          additionally captures the fact that   small
cost range provides little information  even when   is nondominated  Collectively  the coef cients capture the probability of an example   where the good classi ers disagree
substantially on   in both predicted costs and labels  Importantly  the notion of good classi ers is via the algorithmindependent set Fcsr    and is only   property of   and   
The de nition is   natural adaptation from binary classi 
cation  Hanneke    where   similar disagreement region to DIS       is used  Our de nition asks for confusion about the optimality of   speci   label    which provides more detailed information about the coststructure
than simply asking for any confusion among the good classi ers  The    scaling leads to bounded coef cients in
many examples  Hanneke    and we also scale by the
cost range parameter   so that the favorable settings for
active learning can be concisely expressed as having    
bounded  as opposed to   complex function of  
The next two results bound the labeling effort  Eq    in
the high noise and low noise cases respectively  The low
noise assumption enables   signi cantly sharper bound 
Theorem   With probability at least   the label complexity of the algorithm over   examples is bounded by 

 

         pK     log 
         pK       log   
where      log        
Theorem   Assume the Massart noise condition holds 
With probability at least       the label complexity of the
algorithm over   examples is at most 
         
          

     log        log 
    log               log   

 

   

   

In the highnoise case  the bounds scales with    for the
respective coef cients  This agrees with results in binary
classi cation  where at best constantfactor savings over
passive learning are possible  On the other hand  in the low
noise case  the label complexity scales as          
which is   polynomial improvement over passive learning 
However  the constant scales exponentially with   so  
should not be chosen to be too small 
Note that   can be much smaller than    as demonstrated through an example in the next section 
In such
cases  only   few labels are ever queried and the    bound

in the high noise case re ects this additional savings over
passive learning  Unfortunately  under Massartnoise  the
   bound depends directly on    so that we do not bene   when        This can be resolved by letting   
depend on the noise level   but we prefer to use the more
robust choice       pi which still allows COAL to partially adapt to low noise and achieve low label complexity 
Unfortunately  comparing our bound to binary classi cation reveals suboptimality here  Under Massart noise and
bounded coef cients  the label complexity for binary classi cation is typically log      which contrasts with our
     rate  This loss in rate arises from setting    
However  with         suboptimal regressor that left the
version space may reenter at   later round and cause us to
issue more queries  since we may not accumulate evidence
against this regressor unless it is in the version space  Binary classi cation methods address this issue by hallucinating labels for unqueried examples  Dasgupta et al   
but hallucinating costs does not seem applicable to CSMC
since the only safe choice that avoids eliminating    appears to be         which is unknown  Our solution uses
    to induce   shrinking radius so that bad regressors
cannot reenter the version space  However  to avoid eliminating     the initial radius   must be larger than standard
concentration arguments require  so the algorithm is conservative 
Informationtheoretically  by enumerating the
version space  the logarithmic rate is possible in CSMC 
but we do not know of ef cient algorithms for this 

  Two Examples
Our  rst example shows the bene ts of using the domination criterion in querying  in addition to the cost range
condition  Consider   problem under Assumption   where
the optimal cost is predicted perfectly  the second best cost
is   worse and all the other costs are substantially worse 
but with variability in the predictions  Since all classi ers
predict the right label  we get           so our label
complexity bound is    Intuitively  since every regressor is certain of the optimal label and its cost  we actually
make zero queries  On the other hand  all of the suboptimal
labels have large cost ranges  so querying based solely on  
cost range criteria leads to   large label complexity 
  related example demonstrates the improvement in our
query rule over more na ve approaches where we query either no label or all labels  which is the natural generalization of query rules from multiclass classi cation  Agarwal 
  In the above example  if the best and second best
labels are confused occasionally   may be large  but we
expect        since only the second best label can be
confused with the best  Thus  the    bound in Theorem  
is   factor of   smaller than with   na ve query rule since
COAL only queries the best and second best labels 

Active Learning for CostSensitive Classi cation

    feat
       
INet  
INet  
       
RCV           

    feat

 
POS          
NER          
Wiki
         

Table   Dataset statistics   is the average sequence length 
  Experiments
For computational ef ciency  we implemented an approximate version of COAL using online optimization  based on
online linear leastsquares regression  The algorithm processes the data in one pass  and the idea is to   replace
gi    the ERM  with an approximation go
    obtained by online updates  and   compute the minimum and maximum
costs via   sensitivity analysis of the online update  Specifically  we de ne the   sensitivity value         go
        
which is the derivative of the prediction on   as   function
of the importance weight    for the new example   and
cost       or        for    and    respectively  Then we
approximate    via go
     where wo is
the largest weight   satisfying

       wo          go

  go

          go

         ws      go

          

and    is the radius used at round    We similarly approximate the maximum cost  See Appendix   for details 

  Simulated Active Learning
We performed simulated active learning experiments with
three datasets  ImageNet   and   are subtrees of the ImageNet hierarchy covering   and   most frequent classes 
where each example has   single zerocost label and the
cost for incorrect labels is the treedistance to the correct one  The feature vectors are the top layer of the Inception neural network  Szegedy et al    The third
dataset  RCV     Lewis et al    is   multilabel textcategorization dataset  which has   topic labels  organized as   tree with similar treedistance cost structure as
the ImageNet data  Some dataset statistics are in Table  
We compare our online version of COAL to passive online learning  We use the costsensitive oneagainst all
 CSOAA  implementation in Vowpal Wabbit  which performs online linear regression for each label separately 
There are two tuning parameters in our implementation 
First  instead of     we set the radius of the version space
to         
            and the log factor     
  

  scales with    and instead tune the con 

stant   This alternate  mellowness  parameter controls
how aggressive the query strategy is  The second parameter is the learning rate used by online linear regression 

log        

 

 http hunch net vw
 We use the default online learning algorithm in Vowpal

For each parameter setting and each dataset  we make one
pass through the training set and check the test cost  which
is just the normalized expected cost  of the model every
doubling number of queries  We repeat this on   random
permutations of the training data and plot the results in Figures   and   For each mellowness  we show the results of
the best learning rate  which maximizes   notion of AUC
that re ects the tradeoff between test cost and number of
queries  see Eq    in Appendix   
The  gures show  for each dataset and mellowness  the
number of queries against the median test cost along with
bars extending from the  th to  th quantile  Overall 
COAL achieves   better tradeoff between performance
and queries  With proper mellowness parameter  active
learning achieves similar test cost as passive learning with  
factor of   to   less queries  On ImageNet   and RCV 
    recall Figure   active learning achieves better test cost
with   factor of   less queries  On RCV    COAL
queries like passive up to around    queries  since the
data is very sparse  and linear regression has the property
that the cost range is maximal when an example has   new
unseen feature  Once COAL sees all features   few times 
it queries much more ef ciently than passive  Note that
these plots correspond to the label complexity    with
similar results for    in Appendix   
While not always the best  we recommend   mellowness
setting of   as it achieves reasonable performance on all
three datasets  This is also con rmed by the learningto 
search experiments  which we discuss in the next section 
We also compare COAL with two active learning baselines  Both algorithms differ from COAL only in their
query rule  ALLORNONE queries either all labels or no
labels using both domination and costrange conditions
and is an adaptation of existing multiclass active learners  Agarwal    NODOM just uses the costrange condition  inspired by active regression  Castro et al   
The results for ImageNet   are displayed in Figure  
 See also Appendix    where we use the AUC strategy to choose the learning rate  We choose the mellowness by visual inspection for the baselines and use   for
COAL  COAL substantially outperforms both baselines 
which provide minimal improvement over passive learning 

  Learning to Search
We also experiment with COAL as the base leaner in
learningto search  Daum   III et al    Chang et al 
  which reduces joint prediction problems to CSMC 
Here    joint prediction example de nes   search space 
Wabbit  which is   scalefree  Ross et al    importance
weight invariant  Karampatziakis   Langford    form of
AdaGrad  Duchi et al   

 We use   for ALLORNONE and   for NODOM 

Active Learning for CostSensitive Classi cation

Figure   Experiments with COAL  Top row shows test cost vs  number of queries for simulated active learning experiments  Bottom
row shows accuracy vs  number of rollouts for active and passive learning as the CSMC algorithm in learningto search 

where   sequence of decisions are made to generate the
structured label  We focus here on sequence labeling tasks 
where the input is   sentence and the output is   sequence
of labels  speci cally  parts of speech or named entities 
Learningto search solves joint prediction problems by
generating the output one label at   time  conditioning the
input   on all past decisions  Since mistakes may lead to
compounding errors  it is natural to represent the decision
space as   CSMC problem  where the classes are the  actions  available  possible labels for   word  and the costs
re ect the long term loss of each choice  Intuitively  we
should be able to avoid expensive computation of long term
loss on decisions like  is  the    DETERMINER  once we
are quite sure of the answer  Similar ideas motivate adaptive sampling for structured prediction   Shi et al   
We speci cally use AGGRAVATE  Ross   Bagnell   
Chang et al    which runs   learned policy to produce
  backbone sequence of labels  For each position in the
input  it then considers all possible deviation actions and
executes an oracle for the rest of the sequence  The loss
on this complete output is used as the cost for the deviating
action  Run in this way  AGGRAVATE requires    rollouts
when the input sentence has   words and each word can
take one of   possible labels 
Since each rollout takes    time  this can be computationally prohibitive  so we use active learning to reduce the
number of rollouts  We use COAL and   passive learning baseline inside AGGRAVATE on three joint prediction
datasets  statistics are in Figure   upper right  As above 

we use several mellowness values and the same AUC criteria to select the best learning rate  The results are in Figure   and again our recommended mellowness is  
Overall  active learning reduces the number of rollouts required  but the improvements vary on the three datasets  On
the Wikipedia data  COAL performs   factor of   less rollouts to achieve similar performance to passive learning and
achieves substantially better test performance    similar 
but less dramatic  behavior arises on the NER task  On the
other hand  COAL offers minimal improvement over passive learning on the POStagging task  This agrees with our
theory and prior empirical results  Hsu    which show
that active learning may not always improve upon passive 

  Discussion
This paper presents   new active learning algorithm for
costsensitive multiclass classi cation  The algorithm enjoys strong theoretical guarantees and also outperforms
passive baselines both in CSMC and structured prediction 
We close with some intriguing questions 
  Can we use   square loss oracle in other partial infor 

mation problems like contextual bandits 

  Can we avoid the safety parameter to achieve the opti 

mal complexity in the low noise case 

  Can we analyze the online approximation to COAL 
We hope to answer these questions in future work 

Active Learning for CostSensitive Classi cation

References
Agarwal    

Selective sampling algorithms for costsensitive multiclass prediction  In International Conference on Machine Learning   

Agarwal     Hsu     Kale     Langford     Li     and
Schapire       Taming the monster    fast and simple
algorithm for contextual bandits  In International Conference on Machine Learning   

Balcan       and Long     Active and passive learning
of linear separators under logconcave distributions  In
Conference on Learning Theory   

Balcan       Beygelzimer     and Langford     Agnostic
active learning  In International Conference on Machine
Learning   

Balcan       Broder     and Zhang     Margin based active learning  In Conference on Learning Theory   

Beygelzimer     Dasgupta     and Langford    

Importance weighted active learning  In International Conference on Machine Learning   

Beygelzimer     Hsu     Langford     and Zhang     Agnostic active learning without constraints  In Advances
in Neural Information Processing Systems   

Beygelzimer     Langford     Li     Reyzin     and
Schapire       Contextual bandit algorithms with supervised learning guarantees  In Arti cial Intelligence and
Statistics   

Castro     Willett     and Nowak       Faster rates in
In Advances in Neural

regression via active learning 
Information Processing Systems   

Castro       and Nowak       Minimax bounds for active

learning  Transaction on Information Theory   

Cavallanti     CesaBianchi     and Gentile     Learning
noisy linear classi ers via adaptive and selective sampling  Machine Learning   

Chang       Krishnamurthy     Agarwal     Daum   III 
   and Langford     Learning to search better than your
teacher  In International Conference on Machine Learning   

Dasgupta     Hsu     and Monteleoni       general agnostic active learning algorithm  In Advances in Neural
Information Processing Systems   

Daum   III     Langford     and Marcu     Searchbased

structured prediction  Machine Learning   

Dekel     Gentile     and Sridharan     Robust selective
sampling from single and multiple teachers  In Conference on Learning Theory   

Duchi     Hazan     and Singer     Adaptive subgradient
methods for online learning and stochastic optimization 
In Conference on Learning Theory   

Hanneke     Theory of disagreementbased active learning 

Foundations and Trends in Machine Learning   

Hsu     Algorithms for Active Learning  PhD thesis  Uni 

versity of California at San Diego   

Huang       Agarwal     Hsu     Langford     and
Schapire       Ef cient and parsimonious agnostic active learning  In Advances in Neural Information Processing Systems   

Karampatziakis     and Langford     Online importance
weight aware updates  In Uncertainty in Arti cial Intelligence   

Langford     and Beygelzimer     Sensitive error correcting
output codes  In Conference on Learning Theory   

Lewis       Yang     Rose      

  new benchmark collection for

and Li    
text
Rcv 
of Machine
categorization
Learning Research 
at
available
http www jmlr org papers volume 
lewis   lyrl rcv   README htm 

research 

 

Journal
Data

Littlestone     and Warmuth       The weighted majority
algorithm  In Foundations of Computer Science   
Massart     and   ed elec      Risk bounds for statistical

learning  The Annals of Statistics   

Orabona     and CesaBianchi     Better algorithms for
selective sampling  In International Conference on Machine Learning   

Ross     and Bagnell      

itation learning via interactive noregret
arXiv   

Reinforcement and imlearning 

Ross     Mineiro     and Langford     Normalized online
learning  In Uncertainty in Arti cial Intelligence   

Settles     Active learning  Synthesis Lectures on Arti cial

Intelligence and Machine Learning   

Shi     Steinhardt     and Liang     Learning where to
sample in structured prediction  In Arti cial Intelligence
and Statistics   

Silla Jr       and Freitas         survey of hierarchical
classi cation across different application domains  Data
Mining and Knowledge Discovery   

Active Learning for CostSensitive Classi cation

Syrgkanis     Krishnamurthy     and Schapire       Ef 
 cient algorithms for adversarial contextual learning  In
International Conference on Machine Learning   

Szegedy     Liu     Jia     Sermanet     Reed      
Anguelov     Erhan     Vanhoucke     and Rabinovich     Going deeper with convolutions 
In Computer Vision and Pattern Recognition   

Zhang     and Chaudhuri     Beyond disagreementbased
In Advances in Neural Infor 

agnostic active learning 
mation Processing Systems   

