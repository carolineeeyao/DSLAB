Stochastic Gradient Monomial Gamma Sampler

Yizhe Zhang   Changyou Chen   Zhe Gan   Ricardo Henao   Lawrence Carin  

Abstract

Recent advances in stochastic gradient
techniques have made it possible to estimate posterior distributions from large datasets via Markov
Chain Monte Carlo  MCMC  However  when
the target posterior is multimodal  mixing performance is often poor  This results in inadequate exploration of the posterior distribution   
framework is proposed to improve the sampling
ef ciency of stochastic gradient MCMC  based
on Hamiltonian Monte Carlo    generalized kinetic function is leveraged  delivering superior
stationary mixing  especially for multimodal distributions  Techniques are also discussed to overcome the practical issues introduced by this generalization 
It is shown that the proposed approach is better at exploring complex multimodal
posterior distributions  as demonstrated on multiple applications and in comparison with other
stochastic gradient MCMC methods 

  Introduction
The development of increasingly sophisticated Bayesian
models in modern machine learning has accentuated the
need for ef cient generation of asymptotically exact samples from complex posterior distributions  Markov Chain
Monte Carlo  MCMC  is an important framework for drawing samples from   target density function  MCMC sampling typically aims to estimate   desired expectation in
terms of   collection of samples  avoiding the need to
compute intractable integrals  The Metropolis algorithm
 Metropolis et al    was originally proposed to tackle
this task  Despite great success  this method is based
on random walk exploration  which often leads to inef 
cient posterior sampling  with    nite number of samples 
Alternatively  exploration of   target distribution can be
guided using proposals inspired by Hamiltonian dynam 

 Duke University  Durham  NC    Correspondence to 

Yizhe Zhang  yizhe zhang duke edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

ics  leading to Hamiltonian Monte Carlo  HMC   Duane
et al    Aided by gradient information  HMC is able
to move ef ciently in parameter space  thus greatly improving exploration  However  the emergence of big datasets
poses   new challenge for HMC  as evaluation of gradients
on whole datasets becomes computationally demanding  if
not prohibitive  in many cases 
To scale HMC methods to big data  recent advances in
Stochastic Gradient MCMC  SGMCMC  have subsampled the dataset into minibatches in each iteration  to decrease computational burden  Welling   Teh    Chen
et al    Ding et al    Ma et al    Stochastic Gradient Langevin Dynamics  SGLD   Welling   Teh 
  was  rst proposed to generate approximate samples from   posterior distribution using minibatches  Since
then  research has focused on leveraging the minibatch idea
while also providing theoretical guarantees  For instance 
Teh et al    showed that by appropriately injecting
noise while using   stepsizedecay scheme  SGLD is able to
converge asymptotically to the desired posterior  Stochastic Gradient Hamiltonian Monte Carlo  SGHMC   Chen
et al    extended SGLD with auxiliary momentum
variables  akin to HMC  and introduced   friction term to
counteract the stochastic noise due to subsampling  However  exact estimation of such noise is needed to guarantee   correct SGHMC sampler  To alleviate this issue  the
Stochastic Gradient Nos eHoover Thermostat  SGNHT 
 Ding et al    algorithm introduced socalled thermostat variables to adaptively estimate stochastic noise via  
thermalequilibrium condition 
One standing challenge of SGMCMC methods is inef 
ciency when exploring complex multimodal distributions 
This limitation is commonly found in latent variable models with   multilayer structure 
Inef ciency is manifested because sampling algorithms have dif culties moving across modes  while traveling along the surface of the
distribution  As   result  it may take   very large number of
iterations  posterior samples  to cover more than one mode 
greatly limiting scalability 
We investigate strategies for improving mixing in SGMCMC  We propose the Stochastic Gradient Monomial
Gamma Thermostat  SGMGT  building upon the Monomial Gamma Sampler  MGS  proposed by Zhang et al 

Stochastic Gradient Monomial Gamma Sampler

  They showed that   generalized kinetic function typically improves the stationary mixing ef ciency of
HMC  especially when the target distribution has multiple modes  However  this advantage comes with numerical dif culties  and convergence problems due to poor initialization  By de ning   smooth version of this generalized kinetic function  we can leverage its mixing ef ciency 
while satisfying the required conditions for stationarity of
the corresponding stochastic process  as well as alleviating numerical dif culties arising from differentiability issues  To ameliorate the convergence issues  we further introduce      sampler with an underlying elliptic stochastic
differential equation system and ii    resampling scheme
for auxiliary variables  momentum and thermostats  with
theoretical guarantees  The result is an elegant framework
to improve stationary mixing performance on existing SGMCMC algorithms augmented with auxiliary variables 

 cid  

  Preliminaries
Hamiltonian Monte Carlo Suppose we are interested
in sampling from   posterior distribution represented as
               exp       where   denotes model parameters and                 xN  represents   data points  Assuming        data  the potential
energy function       denotes the negative log posterior density  up to   normalizing constant              
   log   xi    log    For simplicity  in the following we omit the conditioning of   in       and
write     In HMC  the posterior density is augmented
with an auxiliary momentum random variable      is independent of   and typically has   marginal Gaussian distribution with zeromean and covariance matrix    The
joint distribution is written as         exp        cid 
exp            where       is the total energy
  pT     is the standard
 or Hamiltonian  and         
 Gaussian  kinetic energy function  and   is the mass matrix  HMC leverages Hamiltonian dynamics  driven by the
following differential equations

       pdt  

dp       dt  

 

where   is the system   time index 
The total Hamiltonian is preserved under perfect simulation       by solving   exactly  However  closedform
solutions for   and   are often intractable  thus numerical
integrators such as the leapfrog method are utilized to generate approximate samples of    Neal    This leads to
the following update scheme 
pt    pt    
              pt   
pt    pt     

        

        

 

where   is the stepsize 

Monomial Gamma HMC In the Monomial Gamma
Hamiltonian Monte Carlo  MGHMC   Zhang et al   
algorithm  the following generalized kinetic function is
employed as   substitute for the Gaussian kinetics of standard HMC 

            

            
    

 

where      
   denotes the elementwise power operation   
is the monomial parameter  Note that when        
recovers the standard  Gaussian  kinetics  For general   
the update equations are identical to   except for

              pt   

 

Zhang et al    proved in the univariate case that
MGHMC can yield better mixing performance when the
sampler reaches its stationary distribution  under perfect
dynamic simulation       in nitesimal stepsize in the limit
and adequate  nite  simulation stepsize  Additionally  it
was shown that for multimodal distributions sampled via
MGMHC  the probability of getting trapped in   single
mode goes to zero  as      
However  these theoretical advantages are accompanied by
two practical issues     the numerical dif culties accentuate
dramatically as   increases  due to the lack of differentiability of      for       and ii  convergence is slow with
poor initialization  For example  in   and   if    is far
away from the mode    of the distribution         will be
large  causing the updated momentum pt  to blow up 
This renders the change of           pt  to be arbitrarily small for large    thus slowing convergence 

Stochastic Gradient MCMC SGMCMC is desirable
when the dataset     is too large to evaluate the potential
    using all   samples  The idea behind SGMCMC
is to replace     with an unbiased stochastic likelihood 
     evaluated from   subset of data  termed   minibatch 

          

   log          log     

 
where        cid  is   random subset of         
of size   cid   cid     SGMCMC algorithms are typically
driven by   continuoustime Markov stochastic process of
the form  Chen et al   

  cid cid   cid 

        dt     dW  

 

where   denotes the parameters of the augmented system         and       and    are referred as drift and
diffusion vectors  respectively  and   denotes   standard
Wiener process 
In SGHMC  Chen et al    the resulting stochastic dynamic process is governed by the following Stochastic Dif 

Stochastic Gradient Monomial Gamma Sampler

ferential Equations  SDEs   with       

 cid 

     pdt  
dp            Ap dt  

 AI      dW  

 

where              is   function of             and
   is   function of                is modeled as

               cid    where           and  

is the discretization stepsize      is an estimator of   
  is   userspeci ed diffusion factor and   is the identity
matrix  Chen et al    set         for simplicity  The
reasoning is that the injected noise      Ah  will dominate as          remains as   constant  whereas   
goes to zero  Unfortunately  the covariance function    
of the stochastic noise    is dif cult to estimate in practice 
Recently  SGNHT  Ding et al    considered incorporating additional auxiliary variables  thermostats  The resulting SDEs correspond to

 
dp               cid    dt  
     pdt            cid       dt  

 
 
where  cid  represents the Hadamard  elementwise  product
and   are thermostat variables  Note that the diffusion factor     is decoupled in   thus   can adaptively    to the
unknown noise from the stochastic gradient       

 AIdW  

  Stochastic Gradient Monomial Gamma

Sampler

We now consider      more ef cient  generalized  kinetic
function  ii  adapting the proposed kinetics to satisfy stationary requirements and alleviate numerical dif culties 
iii  incorporating an additional  rstorder stochastic process to   and iv  stochastic resampling of the momentum
and thermostats to lessen convergence issues 

Generalized kinetics The statistical physics literature
traditionally considers   quadratic form of the kinetics
function  and   Gaussian distribution for the thermostats
in   when analyzing the dynamic system of   canonical
ensemble  Tuckerman   
Inspired by this  one typical assumption in previous SGMCMC work is that the
marginal distribution for the momentum and thermostat is
Gaussian  Ding et al    Li et al    However  this
assumption  while convenient  does not necessarily guarantee an optimal sampler 
In recent work  Lu et al    extended the standard
 Newtonian  kinetics to   more general form inspired by
relativity theory  By bounding the momentum  their relativistic Monte Carlo can lessen the problem associated with
large potential gradients         thus resulting in   more
robust alternative to standard HMC  Further  Zhang et al 

  demonstrated that adopting nonGaussian kinetics
delivers better mixing and reduces sampling autocorrelation  especially for cases where the posterior distribution
has multiple modes 
These ideas motivate   more general framework to characterize SGMCMC  with potentially nonGaussian kinetics
and thermostats  As   relaxation of SGNHT  Ding et al 
  Ma et al    we consider   Hamiltonian system
de ned in   more general form

                      

 
where    and     are any valid potential functions  inherently implying that exp    and exp     de ne
valid probability density functions 
We  rst consider the SDEs of SGNHT with generalized kinetics      The system can be obtained by generalizing
       pT     with identity mass matrix   for simplicity  in   with arbitrary      thus

          dt  
dp               cid       dt  
            cid               dt  

 

 AIdW  

 

However  if we set      as in   with       the dynamics
governing the SDEs in   will often fail to converge  This
is because the suf cient condition to guarantee that the It  
process governed by the SDEs in   converge to   stationary distribution generally requires the FokkerPlanck
equation to hold  Risken    Further  the existence and
uniqueness of the solutions to the FokkerPlanck equation
require Lipschitz continuity of drift and diffusion vectors
in    Bris   Lions    Unfortunately  this is not the
case for the drift vectors in   when       as       is
nondifferentiable at the origin            

Softened kinetics The above limitation can be avoided
by using   softened kinetic function Kc    However  to
keep the performance bene ts from the original stiff kinetics  we must ensure that Kc    has the same tail behavior 
We propose that for         the softened kinetics are
 for clarity we consider    case  however higher dimensions still apply 

 cid          log    ecp       

Kc     

     

 

  ec     

 

       

 

where       is   softening parameter  Note that Kc    is
 in nitely  differentiable for any   and asymptotically approaches the stiff kinetics as         comparison between stiff kinetics       and softened kinetics Kc    is
shown in Figure   for different values of    Discussion and
formulation of the softened kinetics for arbitrary    and   
are provided in the Supplementary Material  SM 

Stochastic Gradient Monomial Gamma Sampler

rior samples from the invariant joint distribution     
exp    yielding the desired marginal distribution
         as      exp    
Theorem   The stochastic process governed by   converges to   stationary distribution      exp   
where    is as de ned in   and           
The reasoning behind increasing stochasticity in the SDEs
is twofold  First  the additional Langevin dynamics are
crucial to SGMCMC with generalized kinetics for large
   For instance  for       the update for   from  
is               pt    When       and  pt  is large 
       will be close to zero  thus      the
         
next sample  will be close to          the sampler moves
arbitrarily slow  As discussed by Zhang et al    this
can happen when   moves to   region where the gradient
     takes   large absolute value       near the lowdensity regions in   lighttailed distribution  Fortunately 
the additional Langevin dynamics in         dt  
 
 dW   compensate for the weak updating signal from
      by an immediate gradient signal        Additionally  when      becomes small        will become
large  As   result  these two updating signals       and
       compensate each other  thereby delivering   stable
updating scheme  Likewise  the immediate gradient     
in   can provide complementary updating signal for the
thermostat variables    to offset the weak deterministic update  Kc     cid   Kc       Kc    when   is large 
Second    has noise components on all parameters 
       making the corresponding SDEs elliptic  From  
theoretical perspective  ellipticity hypoellipticity are necessary conditions to guarantee existence of bounded solutions
for   particular partial differential equation related to the
diffusion   in nitesimal generator  which lies in the core of
most recent SGMCMC theory  Teh et al    Vollmer
et al    Chen et al    Ellipticity is characterized
by   noise process  Brownian motion  covering all components of the system  via the diffusion     in   This
means    is block diagonal  thus   positive de nite matrix  Mattingly et al    In   typical hypoelliptic case 
the noise process is imposed on   subset of   However 
hypoellipticity also requires the noise to be able to spread
through the system via the drift term      which may not
be true for general     For instance  in             
and    is block diagonal with entries  
 AI        
  and   are not explicitly in uenced by the noise process 
    thus hypoellipticity cannot be guaranteed 
To the authors  knowledge  for existing SGMCMC algorithms  only SGLD where            dt 
 dW   satis es the ellipticity property  while other algorithms such
as SGHMC and SGNHT assume hypoellipticity  thus their
corresponding    are not positive de nite 
One caveat of   is that if   and   are too large  the up 

 

 

Figure   Softened vs  stiff kinetics     Left        Right 
     

To generate samples of the momentum variable     from
the density with softened kinetics  which is proportional
to exp Kc    we use   coordinatewise rejection sampling       the proposed pd for the dth dimension is rejected with probability     exp   pd    Kc pd 
In practice  setting   to   relatively large value would still
make the gradient  Kc    illposed close to       thus
causing high integration error when simulating the Hamiltonian dynamics  Conversely  setting   to   small value will
cause   high approximation error        the original     
thus resulting in   less ef cient sampler  Consequently   
has to be determined empirically as   tradeoff between integration and approximation errors 

Additional First Order Dynamics
Inspired by Ma et al 
  we consider adding Brownian motion to   and  
in   with variances   and   respectively  while maintaining the stochastic process  asymptotically  converging
to the correct marginal distribution of   Speci cally  we
consider the following SDEs 

 
             dt    Kc   dt  
dp                cid   Kc   dt

      cid Kc     cid   Kc       Kc   cid  dt

        dt  cid pdW  
      dt  cid dW  

 dW  

 

 

The variances         control
the Brownian motion for        respectively  and       denotes  
rescaling factor for the friction term of momentum updates  The additional terms       dt  
 dW and

    dt  cid dW can be understood as  rstorder

Langevin dynamics  Welling   Teh    The variance
term    controls the contribution of        to the update
of           Kc    This is analogous to the hyperparameter balancing        and   in the SGDwith momentum
algorithm  Rumelhart et al    Derivation details for
 Kc    and  Kc    in   as well as other values of   
are provided in the SM 
The following theorem  proven in the SM  shows that under regularity conditions  the SDEs in   lead to poste 

 momentum   Kinetic fcn valueK     Kc      Kc      Kc      momentum   Kinetic fcn valueK     Kc      Kc      Kc      Stochastic Gradient Monomial Gamma Sampler

At the burnin stage  this momentumaccumulation energydrop cycle seen in Figure  bottom  via resampling momentum can happen several times  until equilibrium is
found  In practice  the resulting energy level is often much
lower than initially  thereby delivering   more ef cient and
accurate dynamic updating 
The frequency of resampling from the marginal of the stationary distribution can have   direct impact on the mixing
performance  Setting the frequency too high will result in
  randomwalk behavior  Conversely  with   low frequency
resampling  the randomwalk behavior is suppressed at  
cost of fewer jumps between trajectories associated with
different energy levels  It is advisable to increase the resampling frequency if the sampler is initialized on lowdensity       lighttailed  region 
The resampling step on   and   plays   role that is similar
to adding   Langevin component to   in the sense that both
improve convergence for       However  these two strategies  resampling and Langevin  are fundamentally different  We empirically observe that resampling is most helpful
during burnin  while the additional Langevinstyle updates
are more helpful with mixing during stationary sampling 

SGMGT The speci cations described above constitute
an SGMCMC method for the SDEs in   which we call
Stochastic Gradient Monomial Gamma Thermostat  SGMGT  We denote the SGMCMC method with additional
Brownian motion on   and   in   as SGMGTD  Diagonal       with       and       The complete update
scheme  with Euler integrator  for SGMGT is presented in
the SM  Note that with                        
SGMGTD recovers SGHMC as in Chen et al   
Moreover  when                   it becomes
SGNHT as in Ding et al   
We note that SGMGTD improves upon SGNHT in three
respects      we introduce generalized kinetics  which provably yield lower autocorrelations than standard HMC  especially in multimodal cases   ii  the additional stochastic
noise on thermostat variables yields more ef cient mixing 
 iii  we use stochastic resampling to allow for faster interchange between different energy levels  thus alleviating
sampling stickiness 
To the authors  knowledge  despite existing analysis for
Langevin Monte Carlo  Bubeck et al    Dalalyan 
  rigorous analysis and comparison of the mixing performance of general SGMCMC is very dif cult  thus not
yet established  Toward understanding the mixing performance of SGMGTD  we argue that as the minibatch size
increases  and the contribution of the diffusion in   decreases  the SGMGTD will approach MGHMC  in which
case    large   will result in high stationary mixing performance  especially when sampling multimodal distribution 

Figure   Momentum resampling  Top  stochastic process with
resampling helps sampler move quickly to   lower Hamiltonian
contour  Bottom  resampling decreases energy stepwise during
burnin stage  Resampling of   occurs every   iterations 

dates will be dominated by  rstorder dynamics  thus losing the convergence bene ts from secondorder dynamics
 Chen et al    In practice    and   are problemspeci    thus need to be tuned         by crossvalidation 

Stochastic resampling When generating samples from
the stochastic process in   we resample momentum
and thermostats from their marginal distribution with  
 xed frequency  instead of every iteration from their conditionals  Since the momentum and thermostats are drawn
from the independent marginals of stationary distribution
     exp    it can be shown that reconstructing
the stochastic process with the solution of the SDEs will
still leave the stochastic process invariant to the target stationary distribution  Brunick et al   
To simplify the discussion  consider   stochastic process of
  particle      as in   with  xed   As show in Figure   suppose the initial value of   is far from the maximum   posteriori value  The dynamics governed by  
will stochastically move along the Hamiltonian contour 
The total Hamiltonian energy level is affected by the joint
effect of the stochastic diffusion and momentum refraction
       pdt  which changes continuously over time 
From previous discussions  moving on   high Hamiltonian
contour when       is less ef cient because the absolute
value of the momentum      will get increasingly large 
slowing down the movement of   Resampling of momentum according to its marginal will enable the sampler to
immediately move to   lower Hamiltonian energy level 

   pHamiltonian contourVector field of dynamicHigher energyLower energyMarginal dist  of ptheta  Hamiltonian levelKinetic energy levelStochastic Gradient Monomial Gamma Sampler

as theoretically shown by Zhang et al    Although
our experiments support our intuition    more formal theoretical justi cation is needed  We leave this as interesting
future work 
We observe empirically that when increasing the value of
   SGMGTD may not always achieve superior mixing performance  One possible reason for this is   larger value
of   induces  stiffer  behavior of exp      at      
which typically requires   higher level of softening  thus
higher rejection rates during the rejection sampling step 
Also  when the dimensionality of   is higher  the rejection
rate of the rejection sampling step increases  proportional
to    In such cases  the ef ciency of the sampler decreases
with large    For these reasons  we limit our experiments
to        
We clearly have more hyperparameters than SGNHT  In
practice  we                   and set the resampling frequency Tp          which provides robust
performance  Thus  only two additional hyperparameters
are employed   and   compared to SGNHT  and these
parameters require further tuning  We use either validation
or   holdout set in our experiments 

More accurate numerical integrators Using    rstorder Euler integrator to approximate the solution of the
continuoustime SDEs in   leads to      errors in the
approximate samples  Chen et al    Alternatively 
we can use the symmetric splitting scheme of Chen et al 
  to reduce the order of the approximate error to
     Details of the splitting used in this work are provided in the SM 

verges to the true posterior average     cid cid       

Convergence properties The SGMGT framework  as an
instance of SGMCMC  enjoys the same convergence properties of general SGMCMC algorithms studied in Chen
et al   
It   worth to mention that on challenging
problems the posterior may not be densely sampled to yield
ideal posterior computation  and the asymptotic theory is
being used as   useful heuristic  Speci cally  it is of interest to quantify how fast the sample average       confor     cid   
       where   is number of iterations 
Here we make the same assumptions of Chen et al   
and further assume that    rstorder Euler integrator and  
 xed stepsize are used 
Proposition   For the proposed SGMGT and SGMGTD
algorithms  if    xed stepsize   is used  we have 

 cid  

 

 cid cid cid                  
 cid cid cid         
MSE    cid       
 cid 
    cid          cid   

Bias 

This proposition indicates that with larger number of itera 

Figure   Synthetic multimodal distribution  Left  empirical distributions for different methods  Right  traceplot for each method 

tions and smaller step sizes  smaller bias and MSE bounds
can be achieved  We note that these bounds have similar rates compared to other SGMCMC algorithms such as
SGLD  however  as we demonstrate below in the experiments  SGMGT and SGMGTD usually converge faster
than existing SGMCMC methods 
In addition  for stochastic resampling  we can extend
Proposition   to the following complementary results 
Lemma   Let    be the stationary distribution of
SGMGTD  The stationary distribution of SGMGTD with
momentum resampling is the same as    
Lemma   The optimal  nitetime bias and MSE bounds
for SGMGTD with momentum replacement remain the
same as SGMGTD 

Proofs of Lemma   and Lemma   are provided in the SM 
The proposed SGMGT framework has   strong connection
with secondorder stochastic optimization methods  leading to   sampling scheme with minibatches with similar
mixing performance as slice sampling  Neal    We
discuss the details of this in the SM 

  Experiments
  Multiplewell Synthetic Potential

We  rst evaluate the mixing ef ciency of SGMGT and
SGMGTD for   synthetic problem  to generate samples
from   complex multimodal distribution  The distribution
is shown in Figure  left  See SM for the de nition of
its potential energy  The modes are almost isolated with  
lowdensity region connecting each other  Consequently 
traversing between modes is dif cult 
In order to simulate the noise of the gradient estimates  we set         
               similar to Ding et al    where
     
We compare SGNHT with SGMGT and SGMGTD with
monomial parameter       and          For all three
algorithms  we try   number of hyperparameter settings 
     stepsize            and the soft parameter    and
present the best results in Figure   Standard SGNHT fails
to escape from one of the modes of the distribution  For

   SGNHTSGMGTSGMGTDOracle SGNHT SGMGT Iterations SGMGTDStochastic Gradient Monomial Gamma Sampler

Table   Average AUROC and median ESS  Dataset dimensionality is indicated in parenthesis after the name of each dataset 

AUROC    

SGNHT

SGMGT   
SGMGTD   
SGMGT   
SGMGTD   

ESS    
SGNHT

SGMGTD   
SGMGTD   

   
 
 
 
 
 
   
 
 
 

   
 
 
 
 
 
   
 
 
 

   
 
 
 
 
 
   
 
 
 

  
 
 
 
 
 
  
 
 
 

   
 
 
 
 
 
   
 
 
 

   
 
 
 
 
 
   
 
 
 

SGMGT with       the generated samples reached  
modes  For SGMGTD with       the sampler identi 
 ed all   modes  In Figure  right  SGMGTD adequately
moves across different modes and yields rapid mixing performance  unlike SGMGT which exhibits stickier behavior 

  Bayesian Logistic Regression

We evaluated the mixing ef ciency and accuracy of SGMGT and SGMGTD using Bayesian logistic regression
 BLR  on   realworld datasets from the UCI repository
 Bache   Lichman    German credit     Australian
credit     Pima Indian     Heart     Ripley     and Caravan     The data dimensionality ranges from   to   and
total observations vary between   to   Gaussian priors are imposed on the regression coef cients  We set the
minibatch size to   Other hyperparameters are provided
in the SM  For each experiment  we draw   iterations
with   burnin samples 
Results in terms of median Effective Sample Size  ESS 
and prediction accuracies as Area Under Receiver Operating Characteristic  AUROC  are summarized in Table  
All the results are averages over   independent runs with
random initialization  In general  SGMGTD performs better than SGMGT  For higherdimensional dataset Cavaran 
the performance of       decreases signi cantly  indicating numerical dif culties  The performance gap between
SGMGT and SGMGTD with       or       is usually
larger than the gap between SGNHT        Presumably when   is greater than   SGMGTD has better convergence 

  Latent Dirichlet Allocation

We also test our methods on Latent Dirichlet Allocation
 LDA   Blei et al    Details of LDA and our implementation are provided in the SM  We use the ICML
dataset  Chen et al    which contains   documents
corresponding to abstracts of ICML proceedings from  
to   After stopword removal  we obtain   vocabulary
size of   and about    words  We use   of the documents for training and the remaining   for testing  The
number of topics is set to   resulting in   parameters  We use   symmetric Dirichlet prior with concentration

Table   The test perplexity with varying stepsize 
 
 
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 

 
 
 
 
 
 
 

stepsize
SGLD
SGNHT

SGMGT   
SGMGTD   
SGMGT   
SGMGTD   

 
 
 
 
 
 
 

      All experiments are based on   MCMC samples with   burnin rounds  We set the minibatch size to
  Other hyperparameter settings are provided in the SM 
Table   shows the test perplexities for SGMGT and
SGMGTD for different stepsizes  For each method we
highlight the best perplexity  The SGMGTD with      
outperforms other methods  however SGMGT with      
fails to achieve   comparable result with SGMGT with
      probably because   good initialization is hard to
achieve for   highdimensional distribution 

  Discriminative RBM

We applied our SGMGT to the Discriminative Restricted
Boltzmann Machine  DRBM 
 Larochelle   Bengio 
  on MNIST data  We choose DRBM instead of RBM
because it provides explicit stochastic gradient formulas 
We evaluated our methods empirically and compare them
with SGNHT  We use one hidden layer with   units  For
each method we performed   iterations with   burnin samples  The minibatch size is set to   Details of
the hyperparameter settings for SGMGT and SGMGTD
are provided in the SM  As shown in Figure  rightmost
  panels  we observe that SGMGTD with       yields  
superior mixing performance  For SGMGTD with      
the posterior samples demonstrated both rapid local mixing  and longrange movement  In contrast  SGLD seems
trapped into   local mode after around   iterations 
Figure  left  shows that SGMGTD with       delivers the fastest convergence with the highest test accuracy 
  The SGMGTD improves over SGMGT  while performance of SGMGTD seems to increase with   large
value of    We observed that the stochastic resampling
played   crucial role for SGMGT  as removing the resampling step resulted in   large drop in testing performance
and mixing ef ciency 

  Recurrent Neural Network

We test our framework on Recurrent Neural Networks
 RNNs  for sequence modeling  Gan et al    We consider two tasks      polyphonic music prediction  and  ii 
wordlevel language modeling  detailed below  Additional
details of the experiment are provided in the SM 

Stochastic Gradient Monomial Gamma Sampler

Figure   Experimental results for DRBM  Left  testing accuracies for SGLD  SGNHT  SGMGT and SGMGTD  Middleleft through
right  traceplots for SGLD  SGNHT and SGMGTD with       respectively 

Polyphonic music prediction We use four datasets 
Pianomidi de  Piano  Nottingham  Nott  MuseData
 Muse  and JSB chorales  JSB   BoulangerLewandowski
et al    Each of these are represented as   collection
of  dimensional binary sequences  that span the whole
range of piano from    to   
We use   onelayer LSTM  Hochreiter   Schmidhuber 
  model  and set the number of hidden units to  
The total number of parameters is around     Each
model is trained for   epochs  We perform early stopping  while selecting the stepsize and other hyperparameters by monitoring the performance on validation sets  Updates are performed using minibatches from one sequence 

Language modeling The Penn Treebank  PTB  corpus  Marcus et al    is used for wordlevel language
modeling  We adopt the standard split    training
words     validation words  and    test words  The
vocabulary size is     We train   twolayer LSTM model
on this dataset  The total number of parameters is approximately     Each LSTM layer contains   units 

Table   Test negative loglikelihood results on polyphonic music
datasets and test perplexities on PTB using RNN 

Algorithms
SGLD
SGNHT
SGMGT    
SGMGT    
SGMGTD    
SGMGTD    
SGD
RMSprop

Piano Nott Muse
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

JSB
 
 
 
 
 
 
 
 

PTB
 
 
 
 
 
 
 
 

Results are shown in Table   The best loglikelihood results on the test set are achieved by using SGMGTD with
either       or        depending on the dataset  To compare with optimizationbased methods  we also include results for SGD  Bottou    and RMSprop  Tieleman  
Hinton      more comprehensive comparison is provided in the SM 
Learning curves for Nott and PTB datasets are shown in
Figure   We omit the SGLD results since they are not com 

parable with other methods  For both datasets  we observe
that SGMGTD delivers fastest convergence  The best negative loglikelihood is achieved by SGMGTD       The
difference between       and       is small  though
SGMGTD with       seems to decrease slightly faster
after   epochs for PTB data 

Figure   Learning curves of different SGMCMC methods on sequence modeling using RNN  Left  Nott  Right  Penn Treebank 

We also observe that the SGMGT with       seems suboptimal compared with SGMGT with       and SGNHT  We
hypothesize that numerical dif culties hinder the success
of SGMGT with       especially in higherdimensional
cases  and without the additional Langevin components of
SGMGTD 

  Conclusions
We improve upon existing SGMCMC methods with several generalizations  We employed   moregeneral kinetic
function  which we have shown to have better mixing ef 
 ciency  especially for multimodal distributions  Since
practical use of the generalized kinetics is limited by
convergence issues during burnin  we injected additional
Langevin dynamics and incorporated   stochastic resampling step to obtain generalized SDEs that alleviate the
convergence issues  Possible areas of future research include designing an algorithm in   slicesampling fashion 
which maintains the invariant distribution by leveraging
the connections between HMC and slice sampling  Zhang
et al    In addition  it is desirable to design an algorithm that can adaptively choose the monomial parameter    thereby achieving better mixing while automatically
avoiding numerical dif culties 

  of iterations Testing accuracySGLDSGNHTSGMGT    SGMGT    SGMGTD    SGMGTD     of iterations  of iterations  of Iterations Epochs Negative LoglikelihoodNottSGNHTSGMGT    SGMGT    SGMGTD    SGMGTD    Epochs Negative LoglikelihoodPenn TreebankStochastic Gradient Monomial Gamma Sampler

Acknowledgments
This research was supported by ARO  DARPA  DOE 
NGA  ONR and NSF 

References
Bache  Kevin and Lichman  Moshe  UCI machine learning

repository   

Betancourt  MJ 

incompatibility of
Hamiltonian Monte Carlo and data subsampling  ArXiv 
 

The fundamental

Blei  David    Ng  Andrew    and Jordan  Michael   

Latent Dirichlet allocation  JMLR     

Bottou    eon  Largescale machine learning with stochas 

tic gradient descent  In COMPSTAT   

BoulangerLewandowski  Nicolas  Bengio  Yoshua  and
Vincent  Pascal  Modeling temporal dependencies in
highdimensional sequences  Application to polyphonic
music generation and transcription  In ICML   

Bris    Le and Lions  PL  Existence and uniqueness of
solutions to fokker planck type equations with irregular coef cients  Communications in Partial Differential
Equations     

Brunick  Gerard  Shreve  Steven  et al  Mimicking an it  
process by   solution of   stochastic differential equation  The Annals of Applied Probability   
   

Bubeck  Sebastien  Eldan  Ronen  and Lehec  Joseph 
Finitetime analysis of projected langevin monte carlo 
In NIPS   

Chen  Changyou  Rao  Vinayak  Buntine  Wray  and
Whye Teh  Yee  Dependent normalized random measures  In ICML   

Chen  Changyou  Ding  Nan  and Carin  Lawrence  On
the convergence of stochastic gradient mcmc algorithms
In NIPS  pp   
with highorder integrators 
 

Chen  Changyou  Carlson  David  Gan  Zhe  Li  Chunyuan 
and Carin  Lawrence  Bridging the gap between stochastic gradient mcmc and stochastic optimization  In AISTATS   

Chen  Tianqi  Fox  Emily    and Guestrin  Carlos  Stochas 

tic gradient hamiltonian monte carlo  ArXiv   

Dalalyan  Arnak    Theoretical guarantees for approximate
sampling from smooth and logconcave densities  Journal of the Royal Statistical Society  Series    Statistical
Methodology   

Ding  Nan  Fang     Babbush     Chen     Skeel        and
Neven     Bayesian sampling using stochastic gradient
thermostats  In NIPS   

Duane  Simon  Kennedy  Anthony    Pendleton  Brian   
and Roweth  Duncan  Hybrid Monte Carlo  Physics letters       

DuBois  Christopher  Balan  Anoop Korattikara  Welling 
Max  and Smyth  Padhraic  Approximate slice sampling
for bayesian posterior inference  In AISTATS  pp   
   

Gan  Zhe  Li  Chunyuan  Chen  Changyou  Pu  Yunchen 
Su  Qinliang  and Carin  Lawrence  Scalable bayesian
learning of recurrent neural networks for language modeling  In ACL   

Geyer        Markov chain monte carlo lecture notes   

Hochreiter  Sepp and Schmidhuber    urgen  Long short 

term memory  Neural computation   

Hwang  ChiiRuey  HwangMa  ShuYin  Sheu  ShuennJyi  et al  Accelerating diffusions  The Annals of Applied
Probability   

Larochelle     and Bengio     Classi cation using discriminative restricted boltzmann machines  In ICML   

Li  Chunyuan  Chen  Changyou  Fan  Kai  and Carin 
Lawrence  Highorder stochastic gradient thermostats
for bayesian learning of deep models  In AAAI   

Lu  Xiaoyu  Perrone  Valerio  Hasenclever  Leonard  Teh 
Yee Whye  and Vollmer  Sebastian    Relativistic monte
carlo  arXiv   

Ma  YiAn  Chen  Tianqi  and Fox  Emily    complete
recipe for stochastic gradient mcmc  In NIPS  pp   
   

Marcus  Mitchell    Marcinkiewicz  Mary Ann  and Santorini  Beatrice  Building   large annotated corpus of
english  The penn treebank  Computational linguistics 
 

Mattingly        Stuart        and Tretyakov        Construction of numerical timeaverage and stationary measures via Poisson equations  SIAM    NUMER  ANAL 
   

Metropolis  Nicholas  Rosenbluth  Arianna    Rosenbluth 
Marshall    Teller  Augusta    and Teller  Edward 
Equation of state calculations by fast computing machines  The journal of chemical physics     

Neal  Radford    Slice sampling  Annals of statistics 

 

Stochastic Gradient Monomial Gamma Sampler

Neal  Radford    MCMC using Hamiltonian dynamics 

Handbook of Markov Chain Monte Carlo     

Risken  Hannes  Fokkerplanck equation  In The Fokker 

Planck Equation  pp    Springer   

Rumelhart  David    Hinton  Geoffrey    and Williams 
Ronald    Learning representations by backpropagating
errors  Cognitive modeling     

Teh  Yee Whye  Thi ery  Alexandre  and Vollmer  Sebastian  Consistency and  uctuations for stochastic gradient
langevin dynamics  ArXiv   

Tieleman  Tijmen and Hinton  Geoffrey  Lecture  
rmsprop  Divide the gradient by   running average of
its recent magnitude  COURSERA  Neural networks for
machine learning   

Tuckerman  Mark 

Statistical mechanics 

theory and

molecular simulation  Oxford University Press   

Vollmer  Sebastian    Zygalakis  Konstantinos    and Teh 
Yee Whye  Exploration of the  non  asymptotic bias
and variance of stochastic gradient langevin dynamics 
Journal of Machine Learning Research   
 

Welling  Max and Teh  Yee    Bayesian learning via
stochastic gradient langevin dynamics  In ICML   

Zhang  Yizhe  Wang  Xiangyu  Chen  Changyou  Henao 
Ricardo  Fan  Kai  and Carin  Lawrence  Towards unifying hamiltonian monte carlo and slice sampling 
In
NIPS   

