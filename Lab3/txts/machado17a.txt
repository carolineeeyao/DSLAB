  Laplacian Framework for Option Discovery in Reinforcement Learning

Marlos    Machado   Marc    Bellemare   Michael Bowling  

Abstract

Representation learning and option discovery are
two of the biggest challenges in reinforcement
learning  RL  Protovalue functions  PVFs  are
  wellknown approach for representation learning in MDPs  In this paper we address the option discovery problem by showing how PVFs
implicitly de ne options  We do it by introducing eigenpurposes  intrinsic reward functions derived from the learned representations  The options discovered from eigenpurposes traverse the
principal directions of the state space  They are
useful for multiple tasks because they are discovered without taking the environment   rewards
into consideration  Moreover  different options
act at different time scales  making them helpful for exploration  We demonstrate features of
eigenpurposes in traditional tabular domains as
well as in Atari   games 

  Introduction
Two important challenges in reinforcement learning  RL 
are the problems of representation learning and of automatic discovery of skills  Protovalue functions  PVFs 
are   wellknown solution for the problem of representation learning  Mahadevan    Mahadevan   Maggioni 
  while the problem of skill discovery is generally
posed under the options framework  Sutton et al   
Precup    which models skills as options 
In this paper  we tie together representation learning and
option discovery by showing how PVFs implicitly de ne
options  One of our main contributions is to introduce
the concepts of eigenpurpose and eigenbehavior  Eigenpurposes are intrinsic reward functions that incentivize the
agent to traverse the state space by following the principal
directions of the learned representation  Each intrinsic reward function leads to   different eigenbehavior  which is

 University of Alberta  Google DeepMind  Correspondence

to  Marlos    Machado  machado ualberta ca 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

the optimal policy for that reward function  In this paper we
introduce an algorithm for option discovery that leverages
these ideas  The options we discover are taskindependent
because  as PVFs  the eigenpurposes are obtained without
any information about the environment   reward structure 
We  rst present these ideas in the tabular case and then
show how they can be generalized to the function approximation case 
Exploration  while traditionally   separate problem from
option discovery  can also be addressed through the careful
construction of options  McGovern   Barto      ims ek
et al    Solway et al    Kulkarni et al   
In this paper  we provide evidence that not all options capable of accelerating planning are useful for exploration 
We show that options traditionally used in the literature to
speed up planning hinder the agents  performance if used
for random exploration during learning  Our options have
two important properties that allow them to improve exploration      they operate at different time scales  and  ii  they
can be easily sequenced  Having options that operate at
different time scales allows agents to make  nely timed actions while also decreasing the likelihood the agent will explore only   small portion of the state space  Moreover  because our options are de ned across the whole state space 
multiple options are available in every state  which allows
them to be easily sequenced 

  Background
We generally indicate random variables by capital letters
      Rt  vectors by bold letters         functions by lowercase letters          and sets by calligraphic font         

  Reinforcement Learning
In the RL framework  Sutton   Barto    an agent aims
to maximize cumulative reward by taking actions in an environment  These actions affect the agent   next state and
the rewards it experiences  We use the MDP formalism
throughout this paper  An MDP is    tuple hS              
At time   the agent is in state st     where it takes action
at     that leads to the next state st      according to
the transition probability kernel           which encodes
Pr St      St      At      The agent also observes
  reward Rt            The agent   goal is to learn  

  Laplacian Framework for Option Discovery in Reinforcement Learning

 

policy                 that maximizes the expected
  Ep       kRt   st  where
discounted return Gt
        is the discount factor 
It is common to use the policy improvement theorem  Bellman    when learning to maximize Gt  One technique
is to alternate between solving the Bellman equations for
the actionvalue function           

          

 

        Gt St      At     
 Xs  
                 Xa 

                 

and making the next policy      greedy            

   

 
  arg max

   

          

until converging to an optimal policy  
Sometimes it is not feasible to learn   value for each stateaction pair due to the size of the state space  Generally 
this is addressed by parameterizing         with   set of
weights     Rn such that                    
It is
common to approximate    through   linear function      
                   where        denotes   linear
feature representation of state   when taking action   

  The Options Framework
The options framework extends RL by introducing temporally extended actions called skills or options  An option  
is    tuple     hI       where       denotes the option   initiation set                 denotes the option  
policy  and       denotes the option   termination set  After the agent decides to follow option   from   state in   
actions are selected according to   until the agent reaches  
state in     Intuitively  options are higherlevel actions that
extend over several time steps  generalizing MDPs to semiMarkov decision processes  SMDPs   Puterman   
Traditionally  options capable of moving agents to bottleneck states are sought after  Bottleneck states are those
states that connect different densely connected regions of
the state space       doorways     ims ek   Barto   
Solway et al    They have been shown to be very
ef cient for planning as these states are the states most frequently visited when considering the shortest distance between any two states in an MDP  Solway et al   

  ProtoValue Functions
Protovalue functions  PVFs  are learned representations
that capture largescale temporal properties of an environment  Mahadevan    Mahadevan   Maggioni   
They are obtained by diagonalizing   diffusion model 
which is constructed from the MDP   transition matrix   
diffusion model captures information  ow on   graph  and

it is commonly de ned by the combinatorial graph Laplacian matrix            where   is the graph   adjacency matrix and   the diagonal matrix whose entries are
the row sums of    Notice that the adjacency matrix  
easily generalizes to   weight matrix     PVFs are de 
 ned to be the eigenvectors obtained after the eigendecomposition of    Different diffusion models can be used to
generate PVFs  such as the normalized graph Laplacian
        

    which we use in this paper 

             

  Option Discovery through the Laplacian
PVFs capture the largescale geometry of the environment 
such as symmetries and bottlenecks  They are task independent  in the sense that they do not use information related to reward functions  Moreover  they are de ned over
the whole state space since each eigenvector induces   realvalued mapping over each state  We can imagine that options with these properties should also be useful  In this
section we show how to use PVFs to discover options 
Let us start with an example  Consider the traditional  
room domain depicted in Figure     Gray squares represent walls and white squares represent accessible states 
Four actions are available  up  down  right  and left  The
transitions are deterministic and the agent is not allowed to
move into   wall  Ideally  we would like to discover options
that move the agent from room to room  Thus  we should
be able to automatically distinguish between the different
rooms in the environment  This is exactly what PVFs do 
as depicted in Figure    left  Instead of interpreting   PVF
as   basis function  we can interpret the PVF in our example as   desire to reach the highest point of the plot  corresponding to the centre of the room  Because the sign of an
eigenvector is arbitrary    PVF can also be interpreted as  
desire to reach the lowest point of the plot  corresponding
to the opposite room  In this paper we use the eigenvectors
in both directions       both signs 
An eigenpurpose formalizes the interpretation above by
de ning an intrinsic reward function  We can see it as
de ning   purpose for the agent  that is  to maximize the
discounted sum of these rewards 
De nition    Eigenpurpose  An eigenpurpose is the intrinsic reward function re
         of   protovalue function
         such that

                     
re

 

where     denotes the feature representation of state   

                      

Notice that an eigenpurpose  in the tabular case  can be
written as re
We can now de ne   new MDP to learn the option associated with the purpose  Me
           where

    hS     re

  Laplacian Framework for Option Discovery in Reinforcement Learning

      grid

    IMaze

     room domain

Figure   Domains used for evaluation 

  we de ne   new statevalue function ve

the reward function is de ned as in   and the action set is
augmented by the action terminate   which allows the
agent to leave Me
  without any cost  The state space and
the transition probability kernel remain unchanged from the
original problem  The discount rate can be chosen arbitrarily  although it impacts the timescale the option encodes 
With Me
    for
policy   as the expected value of the cumulative discounted intrinsic reward if the agent starts in state   and
follows policy   until termination  Similarly  we de ne  
new actionvalue function qe
       as the expected value
of the cumulative discounted intrinsic reward if the agent
starts in state    takes action    and then follows policy  
until termination  We can also describe the optimal value
function for any eigenpurpose obtained through   

ve
      max

 

ve
   

and

qe
         max

 

qe
      

These de nitions naturally lead us to eigenbehaviors 
De nition    Eigenbehavior  An eigenbehavior is   policy            that is optimal with respect to the eigenpurpose re

                 arg maxa   qe

      

Finding the optimal policy   
now becomes   traditional
 
RL problem  with   different reward function  Importantly 
this reward function tends to be dense  avoiding challenging situations due to exploration issues  In this paper we
use policy iteration to solve for an optimal policy 
If each eigenpurpose de nes an option  its corresponding
eigenbehavior is the option   policy  Thus  we need to de 
 ne the option   initiation and termination set  An option
should be available in every state where it is possible to
achieve its purpose  and to terminate when it is achieved 
When de ning the MDP to learn the option  we augmented
the agent   action set with the terminate action  allowing
the agent to interrupt the option anytime  We want options
to terminate when the agent achieves its purpose       when
it is unable to accumulate further positive intrinsic rewards 
With the de ned reward function  this happens when the
agent reaches the state with largest value in the eigenpurpose  or   local maximum when       Any subsequent
reward will be negative  We are able to formalize this con 

Figure   Second PVF  left  and its corresponding option  right 
in the  room domain  Action terminate is depicted in red  top
right corner  other actions are depicted as arrows 

 
    for all     When the terdition by de ning     
minate action is selected  control is returned to the higher
level policy  Dietterich    An option following   policy    terminates when qe
           for all        We
de ne the initiation set to be all states in which there exists
           Thus  the option  
an action       such that qe
policy is         arg maxa    qe
       We refer to
the options discovered with our approach as eigenoptions 
The eigenoption corresponding to the example at the beginning of this section is depicted in Figure    right 
For any eigenoption  there is always at least one state in
which it terminates  as we now show 
Theorem    Option   Termination  Consider an
eigenoption     hIo     Toi and       Then  in an
MDP with  nite state space  To is nonempty 
Proof  We can write the Bellman equation in the matrix
form                where   is    nite column vector with
one entry per state encoding its value function  From  
we have             with           where   denotes
the eigenpurpose of interest  Therefore 

                  

                       
                   

                           
                           
               
             

       

 

We can shift   by any  nite constant without changing the
reward                         because      
  sincePj Ti       Hence  we can assume       Let
     arg maxs ws  so that ws        Clearly vs   
  otherwise            vs    ws    vs    ws   
ws        arriving at   contradiction 

  Laplacian Framework for Option Discovery in Reinforcement Learning

Figure   Options obtained from the four smallest eigenvectors in the   grid  Action terminate is depicted in red 

Figure   Options obtained from the four smallest eigenvectors in the IMaze domain  Action terminate is depicted in red 

This result is applicable in both the tabular and linear function approximation case  An algorithm that does not rely
on knowing the underlying graph is provided in Section  

  Empirical Evaluation
We used three MDPs in our empirical study       Figure  
an open room  an IMaze  and the  room domain  Their
transitions are deterministic and gray squares denote walls 
Agents have access to four actions  up  down  right  and
left  When an action that would have taken the agent into
  wall is chosen  the agent   state does not change  We
demonstrate three aspects of our framework 

  How the eigenoptions present speci   purposes  Interestingly  options leading to bottlenecks are not the
 rst ones we discover 

  How eigenoptions improve exploration by reducing
the expected number of steps required to navigate between any two states 

  How eigenoptions help agents to accumulate reward
faster  We show how few options may hurt the agents 
performance while enough options speed up learning 

  Discovered Options
In the PVF theory  the  smoothest  eigenvectors  corresponding to the smallest eigenvalues  are preferred  Mahadevan   Maggioni    The same intuition applies to
eigenoptions  with the eigenpurposes corresponding to the
smallest eigenvalues being preferred  Figures     and  
depict the  rst eigenoptions discovered in the three domains used for evaluation 
Eigenoptions do not necessarily look for bottleneck states 

 Python code can be found at 
https github com mcmachado options

allowing us to apply our algorithm in many environments in
which there are no obvious  or meaningful  bottlenecks  We
discover meaningful options in these environments  such as
walking down   corridor  or going to the corners of an open
room  Interestingly  doorways are not the  rst options we
discover in the  room domain  the  fth eigenoption is the
 rst to terminate at the entrance of   doorway  In the next
sections we provide empirical evidence that eigenoptions
are useful  and often more so than bottleneck options 

  Exploration
  major challenge for agents to explore an environment
is to be decisive  avoiding the dithering commonly observed in random walks  Machado   Bowling    Osband et al    Options provide such decisiveness by
operating in   higher level of abstraction  Agents performing   random walk  when equipped with options  are expected to cover larger distances in the state space  navigating back and forth between subgoals instead of dithering
around the starting state  However  options need to satisfy
two conditions to improve exploration    they have to be
available in several parts of the state space  ensuring the
agent always has access to many different options  and  
they have to operate at different time scales  For instance 
in the  room domain  it is unlikely an agent randomly selects enough primitive actions leading it to   corner if all
options move the agent between doorways  An important
result in this section is to show that it is very unlikely for
an agent to explore the whole environment if it keeps going
back and forth between similar highlevel goals 
Eigenoptions satisfy both conditions  As demonstrated in
Section   eigenoptions are often de ned in the whole
state space  allowing sequencing  Moreover  PVFs can be
seen as    frequency  basis  with different PVFs being associated with different frequencies  Mahadevan   Maggioni    The corresponding eigenoptions also operate

  Laplacian Framework for Option Discovery in Reinforcement Learning

Figure   Options obtained from the four smallest eigenvectors in the  room domain  Action terminate is depicted in red 

Primitive actions

Options

Primitive actions

Primitive actions

Options

Options

Options

Primitive actions

      grid

    IMaze

     room domain

    Bottleneck options

Figure   Expected number of steps between any two states when following   random walk  Figure    shows the performance of options
that look for doorways in the  room domain 

at different frequencies  with the length of   trajectory until
termination varying  This behavior can be seen when comparing the second and fourth eigenoptions in the      
grid  Figure   The fourth eigenoption terminates  on expectation  twice as often as the second eigenoption 
In this section we show that eigenoptions improve exploration  We do so by introducing   new metric  which we
call diffusion time  Diffusion time encodes the expected
number of steps required to navigate between two states
randomly chosen in the MDP while following   random
walk    small expected number of steps implies that it is
more likely that the agent will reach all states with   random walk  We discuss how this metric can be computed in
the Appendix 
Figure   depicts  for our the three environments  the diffusion time with options and the diffusion time using only
primitive actions  We add options incrementally in order of
increasing eigenvalue when computing the diffusion time
for different sets of options 
The  rst options added hurt exploration  but when enough
options are added  exploration is greatly improved when
compared to   random walk using only primitive actions 
The fact that few options hurt exploration may be surprising at  rst  based on the fact that few useful options are generally sought after in the literature  However  this is   major difference between using options for planning and for
learning  In planning  options shortcut the agents  trajectories  pruning the search space  All other actions are still
taken into consideration  When exploring    uniformly random policy over options and primitive actions skews where

agents spend their time  Options that are much longer than
primitive actions reduce the likelihood that an agent will
deviate much from the options  trajectories  since sampling
an option may undo dozens of primitive actions  This biasing is often observed when fewer options are available 
The discussion above can be made clearer with an example  In the  room domain  if the only options available are
those leading the agent to doorways       Appendix  it is
less likely the agent will reach the outer corners  To do so
the agent would have to select enough consecutive primitive actions without sampling an option  Also  it is very
likely agents will be always moving between rooms  never
really exploring inside   room  These issues are mitigated
with eigenoptions  The  rst eigenoptions lead agents to individual rooms  but other eigenoptions operate in different
time scales  allowing agents to explore different parts of
rooms 
Figure    supports the intuition that options leading to bottleneck states are not suf cient  by themselves  for exploration  It shows how the diffusion time in the  room domain is increased when only bottleneck options are used 
As in the PVF literature  the ideal number of options to be
used by an agent can be seen as   model selection problem 

  Accumulating Rewards
We now illustrate the usefulness of our options when the
agent   goal is to accumulate reward  We also study the
impact of an increasing number of options in such   task 
In these experiments  the agent starts at the bottom left cor 

  Laplacian Framework for Option Discovery in Reinforcement Learning

  options

  options

  options

  options

  options

  options

  options

  options

  options

  options

Primitive
actions

  options

  options

      grid

Primitive
actions

  options

  options

  options

  options

  options

Primitive
actions

    IMaze

     room domain

Figure   The agents  performance accumulating reward as options are added to the action set in their behavior policy  These results use
the eigenpurposes directly obtained from the eigendecomposition as well as their negation 

ner and its goal is to reach the top right corner  The agent
observes   reward of   until the goal is reached  when it
observes   reward of   We used QLearning  Watkins  
Dayan                to learn   policy over
primitive actions  The behavior policy chooses uniformly
over primitive actions and options  following them until termination  Figure   depicts  after learning for   given number of episodes  the average over   trials of the agents 
 nal performance  Episodes were   time steps long  and
we learned for   episodes in the       grid and in the
IMaze  and for   episodes in the  room domain 
In most scenarios eigenoptions improve performance  As
in the previous section  exceptions occur when only   few
options are added to the agent   action set  The best results
were obtained using   options  Despite being an additional parameter  our results show that the agent   performance is fairly robust across different numbers of options 
Eigenoptions are taskindependent by construction  Additional results in the appendix show how the same set of
eigenoptions is able to speedup learning in different tasks 
In the appendix we also compare eigenoptions to random
options  that is  options that use   random state as subgoal 

  Approximate Option Discovery
So far we have assumed that agents have access to the adjacency matrix representing the underlying MDP  However 
in practical settings this is generally not true  In fact  the
number of states in these settings is often so large that
agents rarely visit the same state twice  These problems
are generally tackled with samplebased methods and some
sort of function approximation 
In this section we propose   samplebased approach for option discovery that asymptotically discovers eigenoptions 
We then extend this algorithm to linear function approximation  We provide anecdotal evidence in Atari  
games that this relatively na ve samplebased approach to
function approximation discovers purposeful options 

  Samplebased Option Discovery
In the online setting  agents must sample trajectories  Naturally  one can sample trajectories until one is able to perfectly construct the MDP   adjacency matrix  as suggested
by Mahadevan   Maggioni   However  this approach does not easily extend to linear function approximation  In this section we provide an approach that does
not build the adjacency matrix allowing us to extend the
concept of eigenpurposes to linear function approximation 
In our algorithm    sample transition is added to   matrix   if it was not previously encountered  The transition is added as the difference between the current and
In the tabular
previous observations                
case we de ne     to be the onehot encoding of state   
Once enough transitions have been sampled  we perform
  singular value decomposition on the matrix   such that
           We use the columns of     which correspond
to the righteigenvectors of     to generate the eigenpurposes  The intrinsic reward and the termination criterion
for an eigenbehavior are the same as before 
Matrix   is known as the incidence matrix  If all transitions
in the graph are sampled once  for tabular representations 
this algorithm discovers the same options we obtain with
the combinatorial Laplacian  The theorem below states the
equivalence between the obtained eigenpurposes 
Theorem   Consider the SVD of     UT           with
each row of   consisting of the difference between observations               In the tabular case  if all transitions in the MDP have been sampled once  the orthonormal
eigenvectors of   are the columns of       

Proof  Given the SVD decomposition of   matrix    
      
the columns of   are the eigenvectors of
     Strang    We know that            where
           Lemma        Appendix  Thus  the
columns of VT are the eigenvectors of        which can be
rewritten as          Therefore  the columns of VT are
also the eigenvectors of   

  Laplacian Framework for Option Discovery in Reinforcement Learning

Option  

Option  

Option  

Option  

Option  

Option  

Figure   Options in FREEWAY       text for details 

Figure   Options in MONTEZUMA   REV        text for details 

There is   tradeoff between reconstructing the adjacency
matrix and constructing the incidence matrix  In MDPs in
which states are sparsely connected  such as the IMaze  the
latter is preferred since it has fewer transitions than states 
However  what makes this result interesting is the fact that
our algorithm can be easily generalized to linear function
approximation 

  Function Approximation
An adjacency matrix is not very useful when the agent has
access only to features of the state  However  we can use
the intuition about the incidence matrix to propose an algorithm compatible with linear function approximation 
In fact  to apply the algorithm proposed in the previous section  we just need to de ne what constitutes   new transition  We de ne two vectors    and    to be identical if
and only if            We then use   set data structure to
avoid duplicates when storing         This is   na ve
approach  but it provides encouraging evidence eigenoptions generalize to linear function approximation  We expect more involved methods to perform even better 
We tested our method in the ALE  Bellemare et al   
The agent   representation consists of the emulator   RAM
state   bits  The  nal incidence matrix in which we
ran the SVD had   rows  which we sampled uniformly from the set of observed transitions  We provide
further details of the experimental setup in the appendix 
In the tabular case we start selecting eigenpurposes generated by the eigenvectors with smallest eigenvalue  because
these are the  smoothest  ones  However  it is not clear
such intuition holds here because we are in the function approximation setting and the matrix of transitions does not
contain all possible transitions  Therefore  we analyzed  for
each game  all   discovered options 
We approximate these options greedily      
The next
with the ALE emulator  
action   
selected as

lookahead 
an eigenpurpose   is

        

for

arg maxb ARs            re

Even with such   myopic action selection mechanism we

were able to obtain options that clearly demonstrate intent 
In FREEWAY    game in which   chicken is expected to
cross the road while avoiding cars  we observe options in
which the agent clearly wants to reach   speci   lane in the
street  Figure    left  depicts where the chicken tends to
be when the option is executed  On the right we see   histogram representing the chicken   height during an episode 
We can clearly see how the chicken   height varies for different options  and how   random walk over primitive actions  rand  does not explore the environment properly  Remarkably  option   scores   points at the end of the
episode  without ever explicitly taking the reward signal
into consideration  This performance is very close to those
obtained by stateof theart algorithms 
In MONTEZUMA   REVENGE    game in which the agent
needs to navigate through   room to pickup   key so it can
open   door  we also observe the agent having the clear
intent of reaching particular positions on the screen  such
as staircases  ropes and doors  Figure   Interestingly  the
options we discover are very similar to those handcrafted
by Kulkarni et al    when evaluating the usefulness of
options to tackle such   game    video of the highlighted
options can be found online 

  Related Work
Most algorithms for option discovery can be seen as topdown approaches  Agents use trajectories leading to informative rewards  as   starting point  decomposing and re 
 ning them into options  There are many approaches based
on this principle  such as methods that use the observed
rewards to generate intrinsic rewards leading to new value
functions       McGovern   Barto    Menache et al 
  Konidaris   Barto    methods that use the observed rewards to climb   gradient       Mankowitz et al 
  Vezhnevets et al    Bacon et al    or to do

 https youtu be BVicx CDWA
 We de ne an informative reward to be the signal that informs
the agent it has reached   goal  For example  when trying to escape from   maze  we consider   to be an informative reward if
the agent observes rewards of value   in every time step it is inside the maze    different example is   positive reward observed
by an agent that typically observes rewards of value  

  Laplacian Framework for Option Discovery in Reinforcement Learning

probabilistic inference  Daniel et al    However  such
approaches are not applicable in large state spaces with
sparse rewards  If informative rewards are unlikely to be
found by an agent using only primitive actions  requiring
long or speci   sequences of actions  options are equally
unlikely to be discovered 
Our algorithm can be seen as   bottomup approach  in
which options are constructed before the agent observes
any informative reward  These options are composed to
generate the desired policy  Options discovered this way
tend to be independent of an agent   intention  and are
potentially useful in many different tasks  Gregor et al 
  Such options can also be seen as being useful for
exploration by allowing agents to commit to   behavior for
an extended period of time  Machado   Bowling   
Among the approaches to discover options without using
extrinsic rewards are the use of global or local graph centrality measures    ims ek   Barto      ims ek et al 
    ims ek   Barto    and clustering of states  Mannor et al    Bacon    Lakshminarayanan et al 
 
Interestingly    ims ek et al    and Lakshminarayanan et al    also use the graph Laplacian in their
algorithm  but to identify bottleneck states 
Baranes   Oudeyer   and MoulinFrier   Oudeyer
  show how one can build policies to explicitly assist agents to explore the environment  The proposed algorithms selfgenerate subgoals in order to maximize learning
progress  The policies built can be seen as options  Recently  Solway et al    proved that  optimal hierarchy
minimizes the geometric mean number of trialand error
attempts necessary for the agent to discover the optimal
policy for any selected task   Our experiments con rm
this result  although we propose diffusion time as   different
metric to evaluate how options improve exploration 
The idea of discovering options by learning to control parts
of the environment is also related to our work  Eigenpurposes encode different rates of change in the agents representation of the world  while the corresponding options
aim at maximizing such change  Others have also proposed ways to discover options based on the idea of learning to control the environment  Hengst   for instance 
proposes an algorithm that explicitly models changes in
the variables that form the agent   representation  Recently  Gregor et al    proposed an algorithm in which
agents discover options by maximizing   notion of empowerment  Salge et al    where the agent aims at getting
to states with   maximal set of available intrinsic options 
Continual Curiosity driven Skill Acquisition  CCSA 
 Kompella et al  In Press  is the closest approach to ours 
CCSA also discovers skills that maximize an intrinsic reward obtained by some extracted representation  While we
use PVFs  CCSA uses Incremental Slow Feature Analysis

 SFA   Kompella et al    to de ne the intrinsic reward
function  Sprekeler   has shown that  given   speci   choice of adjacency function  PVFs are equivalent to
SFA  Wiskott   Sejnowski    SFA becomes an approximation of PVFs if the function space used in the SFA
does not allow arbitrary mappings from the observed data
to an embedding  Our method differs in how we de ne
the initiation and termination sets  as well as in the objective being maximized  CCSA acquires skills that produce
  large variation in the slowfeature outputs  leading to options that seek for bottlenecks  Our approach does not seek
for bottlenecks  focusing on traversing different directions
of the learned representation 

  Conclusion
Being able to properly abstract MDPs into SMDPs can reduce the overall expense of learning  Sutton et al   
Solway et al    mainly when the learned options are
reused in multiple tasks  On the other hand  the wrong hierarchy can hinder the agents  learning process  moving the
agent away from desired goal states  Current algorithms
for option discovery often depend on an initial informative
reward signal  which may not be readily available in large
MDPs  In this paper  we introduced an approach that is effective in different environments  for   multitude of tasks 
Our algorithm uses the graph Laplacian  being directly related to the concept of protovalue functions  The learned
representation informs the agent what are meaningful options to be sought after  The discovered options can be seen
as traversing each one of the dimensions in the learned representation  We believe successful algorithms in the future
will be able to simultaneously discover representations and
options  Agents will use their learned representation to discover options  which will be used to further explore the
environment  improving the agent   representation 
Interestingly  the options  rst discovered by our approach
do not necessarily  nd bottlenecks  which are commonly
sought after  In this paper we showed how bottleneck options can hinder exploration strategies if naively added to
the agent   action set  and how the options we discover can
help an agent to explore  Also  we have shown how the
discovered options can be used to accumulate reward in  
multitude of tasks  leveraging their exploratory properties 
There are several exciting avenues for future work  As
noted  SFA can be seen as an approximation to PVFs 
It would be interesting to compare such an approach to
eigenoptions  It would also be interesting to see if the options we discover can be generated incrementally and with
incomplete graphs  Finally  one can also imagine extensions to the proposed algorithm where   hierarchy of options is built 

  Laplacian Framework for Option Discovery in Reinforcement Learning

Acknowledgements
The authors would like to thank Will Dabney    emi Munos
and Csaba Szepesv ari for useful discussions  This work
was supported by grants from Alberta Innovates Technology Futures and the Alberta Machine Intelligence Institute
 Amii  Computing resources were provided by Compute
Canada through CalculQu ebec 

References
Bacon  PierreLuc  On the Bottleneck Concept for Options
Discovery  Theoretical Underpinnings and Extension in
Continuous State Spaces  Master   thesis  McGill University   

Bacon  PierreLuc  Harb  Jean  and Precup  Doina  The
optioncritic architecture  In Proceedings of the National
Conference on Arti cial Intelligence  AAAI   

Baranes  Adrien and Oudeyer  PierreYves  Active learning of inverse models with intrinsically motivated goal
exploration in robots  Robotics and Autonomous Systems     

Bellemare  Marc    Naddaf  Yavar  Veness  Joel  and
Bowling  Michael  The Arcade Learning Environment 
An Evaluation Platform for General Agents  Journal of
Arti cial Intelligence Research     

Bellman  Richard    Dynamic Programming  Princeton

University Press  Princeton  NJ   

  ims ek   Ozg ur and Barto  Andrew    Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning  In Proceedings of the International
Conference on Machine Learning  ICML   

  ims ek   Ozg ur and Barto  Andrew    Skill Characterization Based on Betweenness  In Proceedings of Advances
in Neural Information Processing Systems  NIPS   
  ims ek   Ozg ur  Wolfe  Alicia    and Barto  Andrew   
Identifying Useful Subgoals in Reinforcement Learning
by Local Graph Partitioning  In Proceedings of the International Conference on Machine Learning  ICML 
 

Daniel  Christian  van Hoof  Herke  Peters  Jan  and Neumann  Gerhard  Probabilistic Inference for Determining
Options in Reinforcement Learning  Machine Learning 
   

Dietterich  Thomas    Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition 
Journal of Arti cial Intelligence Research  JAIR   
   

Gregor  Karol  Rezende  Danilo  and Wierstra  Daan  Variational Intrinsic Control  CoRR  abs   

Hengst  Bernhard  Discovering Hierarchy in Reinforcement Learning with HEXQ 
In Proceedings of the International Conference on Machine Learning  ICML 
 

Kompella  Varun Raj  Luciw  Matthew    and Schmidhuber    urgen  Incremental Slow Feature Analysis  In Proceedings of the International Joint Conference on Arti 
cial Intelligence  IJCAI  pp     

Kompella  Varun Raj  Stollenga  Marijn  Luciw  Matthew 
and Schmidhuber  Juergen  Continual CuriosityDriven
Skill Acquisition from HighDimensional Video Inputs
for Humanoid Robots  Arti cial Intelligence  In Press 
ISSN   Available online   February  

Konidaris  George and Barto  Andrew  Skill Discovery
in Continuous Reinforcement Learning Domains using
Skill Chaining 
In Proceedings of Advances in Neural
Information Processing Systems  NIPS  pp   
 

Kulkarni  Tejas    Narasimhan  Karthik    Saeedi  Ardavan  and Tenenbaum  Joshua    Hierarchical Deep Reinforcement Learning  Integrating Temporal Abstraction
and Intrinsic Motivation  ArXiv eprints   

Lakshminarayanan  Aravind  Krishnamurthy  Ramnandan 
Kumar  Peeyush  and Ravindran  Balaraman  Option
Discovery in Hierarchical Reinforcement Learning using SpatioTemporal Clustering  CoRR  abs 
  Presented at the ICML  Workshop on Abstraction in Reinforcement Learning 

Machado  Marlos    and Bowling  Michael  Learning Purposeful Behaviour in the Absence of Rewards  CoRR 
abs    Presented at the ICML  Workshop on Abstraction in Reinforcement Learning 

Mahadevan  Sridhar  ProtoValue Functions  Developmental Reinforcement Learning  In Proceedings of the International Conference on Machine Learning  ICML  pp 
   

Mahadevan  Sridhar and Maggioni  Mauro  Protovalue
Functions    Laplacian Framework for Learning Representation and Control in Markov Decision Processes 
Journal of Machine Learning Research  JMLR   
   

Mankowitz  Daniel    Mann  Timothy Arthur  and Mannor  Shie  Adaptive Skills Adaptive Partitions  ASAP 
In Proceedings of Advances in Neural Information Processing Systems  NIPS  pp     

  Laplacian Framework for Option Discovery in Reinforcement Learning

Mannor  Shie  Menache  Ishai  Hoze  Amit  and Klein 
Uri  Dynamic Abstraction in Reinforcement Learning
via Clustering  In Proceedings of the International Conference on Machine Learning  ICML   

Solway  Alec  Diuk  Carlos    ordova  Natalia  Yee  Debbie  Barto  Andrew    Niv  Yael  and Botvinick 
Matthew    Optimal Behavioral Hierarchy  PLOS Computational Biology     

McGovern  Amy and Barto  Andrew    Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density  In Proceedings of the International Conference on Machine Learning  ICML   

Menache  Ishai  Mannor  Shie  and Shimkin  Nahum  QCut   Dynamic Discovery of Subgoals in Reinforcement
Learning  In Proceedings of the European Conference
on Machine Learning  ECML   

MoulinFrier  Cl ement and Oudeyer  PierreYves  Exploration Strategies in Developmental Robotics    Uni 
 ed Probabilistic Framework 
In Proceedings of the
Joint IEEE International Conference on Development
and Learning and Epigenetic Robotics  ICDLEpiRob 
pp     

Osband  Ian  Roy  Benjamin Van  and Wen  Zheng  Generalization and Exploration via Randomized Value Functions  In Proceedings of the International Conference on
Machine Learning  ICML  pp     

Precup  Doina  Temporal Abstraction in Reinforcement
PhD thesis  University of Massachusetts

Learning 
Amherst   

Puterman  Martin    Markov Decision Processes  Discrete
Stochastic Dynamic Programming  John Wiley   Sons 
Inc  New York  NY  USA   

Salge  Christoph  Glackin  Cornelius  and Polani  Daniel 
In Guided Self 

Empowerment   An Introduction 
Organization  Inception  pp    Springer   

Sprekeler  Henning  On the Relation of Slow Feature Analysis and Laplacian Eigenmaps  Neural Computation   
   

Strang  Gilbert 

Linear Algebra and Its Applications 

Brooks Cole   

Sutton  Richard    and Barto  Andrew    Reinforcement

Learning  An Introduction  MIT Press   

Sutton  Richard    Precup  Doina  and Singh  Satinder  Between MDPs and semiMDPs    Framework for Temporal Abstraction in Reinforcement Learning  Arti cial
Intelligence         

Vezhnevets  Alexander  Mnih  Volodymyr  Osindero  Simon  Graves  Alex  Vinyals  Oriol  Agapiou  John  and
Strategic Attentive Writer for
Kavukcuoglu  Koray 
Learning MacroActions 
In Proceedings of Advances
in Neural Information Processing Systems  NIPS  pp 
   

Watkins  Christopher          and Dayan  Peter  Technical Note  QLearning  Machine Learning    May
 

Wiskott  Laurenz and Sejnowski  Terrence    Slow Feature
Analysis  Unsupervised Learning of Invariances  Neural
Computation     

