Combined Group and Exclusive Sparsity for Deep Neural Networks

Jaehong Yoon   Sung Ju Hwang    

Abstract

The number of parameters in   deep neural network is usually very large  which helps with its
learning capacity but also hinders its scalability and practicality due to memory time inef 
ciency and over tting  To resolve this issue 
we propose   sparsity regularization method that
exploits both positive and negative correlations
among the features to enforce the network to be
sparse  and at the same time remove any redundancies among the features to fully utilize the
capacity of the network  Speci cally  we propose to use an exclusive sparsity regularization
based on    norm  which promotes competition for features between different weights  thus
enforcing them to    to disjoint sets of features 
We further combine the exclusive sparsity with
the group sparsity based on    norm  to promote both sharing and competition for features
in training of   deep neural network  We validate our method on multiple public datasets  and
the results show that our method can obtain more
compact and ef cient networks while also improving the performance over the base networks
with full weights  as opposed to existing sparsity
regularizations that often obtain ef ciency at the
expense of prediction accuracy 

  Introduction
Deep neural networks have shown tremendous success
in recent years  achieving nearhuman performances on
tasks such as visual recognition  Krizhevsky et al   
Szegedy et al    He et al    One of the key factors in this success of deep network is its expressive power 
which is made possible by multiple layers of nonlinear
transformations  However  this expressive power comes
at   cost  increased number of parameters  Due to large

 UNIST  Ulsan  South Korea  AITrics  Seoul  South Korea 

Correspondence to  Sung Ju Hwang  sjhwang unist ac kr 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

number of parameters  deep networks require large amount
of memory and computation power to train  Further  large
number of parameters also mean that the model is highly
susceptible to over tting as well  if trained with insuf 
cient data  To resolve such issues  researchers have sought
ways to make the model more compact and lightweight by
parameter reduction  via model compression  Ba   Caruana    Hinton et al    or removing unnecessary
weights either by pruning  Reed    Han et al   
and  cid regularization  Collins   Kohli    However 
one of the main problems of these methods is that they often achieve such ef ciency at the expense of accuracy 
How can we then obtain   compact deep network without
sacri cing the prediction accuracy  One way to achieve
this goal is better utilizing the capacity of the network  by
reducing redundancies in the model parameters  In the optimal case  the weights at each layer will be fully orthogonal to each other  and thus forming an orthogonal basis
set  However  since this is   dif cult constraint to satisfy  in practice  such constraint is given only at the initialization stage  Saxe et al    or enforced implicitly
through regularizations such as dropout  Srivastava et al 
  that prevents feature coadaption  Contrary to these
existing approaches  we propose to impose an explicit regularization to reduce redundancies  Our idea is to enforce
network weights at each layer to    to different sets of input
features as much as possible  This exclusive feature learning is implemented by the exclusive sparsity regularization
based on    norm  Zhou et al    Kong et al   
which basically promotes network weights at each layer to
compete for few meaningful features from the lower layer 
However  it is not practical nor desirable to restrict each
weight to be completely disjoint from others as some features still need to be shared  For example  if the lowerlayer feature is   wheel  and the upper layer weights are
features describing car and bicycle respectively  then the
two upper layer weights should share the common feature
that describes the wheel  Thus  we also allow for sharing
of some important features  by introducing an additional
group sparsity regularizer based on    norm and combine the two regularization terms  balancing their effect at
each layer of the network to adjust the degree of feature
sharing and competition 

Combined Group and Exclusive Sparsity for Deep Neural Networks

Our combined regularizer can be applied to all layers
of   generic deep neural network  including plain fullyconnected feedforward networks and convolutional networks  We validate our regularized network on four public
datasets with different base networks  on which it achieves
  compact  lighter model while achieving superior performance over networks trained with other sparsityinducing
regularizers  sometimes obtaining even better accuracy
than the full model  As an example  on CIFAR  dataset 
our network obtains   accuracy improvements while
using   less number of parameters and   less
 oating point operations  Further empirical analysis shows
that exclusive sparsity helps the network to converge faster
to   given error rate  and learn less redundant features 

  Related Work
Sparsity for deep neural networks Obtaining compact
deep networks by removing unnecessary weights  is   longstudied topic in deep learning research  The most simplest yet popular weight removal method is to prune out
weak weights by simple thresholding  Reed    Han
et al    Another way to induce sparsity on weights
is by  cid regularization  Tibshirani    Collins   Kohli
  applied the  cid regularization to convolutional neural networks  demonstrating that it can obtain   compact 
memoryef cient network at the expense of small reduction in the prediction accuracy  Few recent work applied
group sparsity  Yuan   Lin    regularization to deep
networks  as it has   number of nice properties  By removing an entire feature group  group sparsity can automatically decide the number of neurons  Alvarez   Salzmann 
  Further  if applied between the weights at different
layers  it can also be used to decide optimal number of layers to use for the given network  Wen et al    In terms
of ef ciency  structured sparsity using    norm exhibits
better data locality than the regular sparsity  and results in
larger speedups  Wen et al    Alvarez   Salzmann 
  We also employ the group sparsity in our combined regularizer  but we mainly group the features across
multiple  lters  to promote feature sharing among the  lters  While all the previously introduced models do help
reduce number of parameters and result in certain amount
of speedups  such memory and time ef ciency is mostly
obtained at the expense of reduced accuracy  Our combined group and exclusive sparsity regularization  on the
other hand  do not degenerate performance  since its aim in
learning sparse weights features is in removing redundancy
to better utilize the network capacity 

Exclusive feature learning There exists quite   number
of work on imposing exclusivity among the learned model
parameters features  One popular way is to enforce orthogonality  as this will minimize the dependency and redun 

dancy among the variables that are being regularized  Orthogonality at initialization stage has been much studied in
the deep learning context  Saxe et al    as in such
  nonconvex optimization setting this can lead to convergence to   better local optimum  Zhou et al    enforced orthogonality via explicit dot product regularization to make the parameters for parentlevel and childlevel classi ers in   hierarchical classi er to be orthogonal  However  the orthogonal regularizer is nonconvex
and does not scale well  since it scales quadratically to the
number of participating vectors  Another way to enforce
exclusivity is through    norm  which is basically the
 norm over  norm groups  that results in promoting sparsity across different vectors  The    norm is  rst proposed in  Zhou et al    where it is used to promote
competitions among the models jointly learned in   multitask learning framework    similar regularizer was used
in  Hwang et al    in   metric learning setting  with
an additional  cid regularization that helps learn discriminative features for each metric  Kong et al    generalized the    norm to be used with arbitrary objective and
handle overlapping groups  In deep learning context  Goo
et al    proposed   difference pooling technique that
has   similar motivation to exclusive lasso  which subtracts
the common superclass level feature map from the classspeci   feature maps to learn classexclusive features for
 negrained classi cation  In all existing models  exclusivity is applied only at the classlevel  and application of the
exclusivity regularization to weights at any layers of deep
networks through    norm  has not yet been explored 
Further  our regularizer is   combined term of both group
and exclusive lasso which allows sharing of important features while making each weight to be as different as possible  rather than purely exclusive feature learning that is impractical  The regularizer proposed in  Kim   Xing   
is similar to ours  which proposes   weighted    norm
that has   similar effect of varying the degree of competition and grouping  although our regularizer is more explicit
in its effect and optimization 

  Approach
Our main objective is to implement   sparse deep neural network with signi cantly less number of parameters
than what the original nonsparse network has  which at
the same time obtains comparable or even better performance to the original model  The training objective for  
generic  deep  neural network for classi cation  is given as
follows 

  cid 

  

             

min
      

      

 

 While our method is generic and can be also applied to re 

gression  we only consider the classi cation task for simplicity 

Combined Group and Exclusive Sparsity for Deep Neural Networks

Here       xi  yi  
   is   training dataset with   instances where xi   Rd is   ddimensional input feature
and yi                is its class label which is one of the
  classes         is the set of weights across all layers 
      is the loss parameterized by       is the total number of layers        is the weight matrix  or tensor  for
layer           is some regularization term on the network weights at layer    and   is the regularization parameter that balances the loss with the regularization 
The usual and the most often used regularization term is the
 norm            cid      cid 
  which is also called as the
 cid regularizer  The regularization has an effect of adding
  bias term to reduce variance of the model  which in turn
results in   lower generalization error 
However  since our goal is in obtaining   sparse model
where large portion of       is zeroed out  we want
       to be   sparsityinducing regularizer  The most
common regularizer for promoting sparsity is the  norm 

          cid      cid 

 

This  norm regularization results in obtaining   sparse
weight matrix  since it requires the solution to be found at
the corner of the  norm ball  thus eliminating unnecessary
elements  The elementwise sparsity can be helpful when
most of the features are irrelevant to the learning objective 
as in the datadriven approaches  However  as aforementioned  when applied to   deep network it usually results
in slight accuracy reduction  Further  elementwise sparsity  while achieving   memoryef cient model  usually do
not result in meaningful speedups in practical network architectures such as CNNs  since the bottleneck is in the
convolutional operations that do not reduce much when the
number of  lters stays the same  Wen et al   
Group sparsity  on the other hand  can help reduce the intrinsic complexity of the model by eliminating   neuron
or   convolutional  lter as   whole  and thus can help obtain practical speedups in deep neural networks  Wen et al 
  Alvarez   Salzmann    The group sparsity regularization is de ned as follows 

 cid 

 cid cid 

 cid 

        

 cid      

   cid   

 

    
   

 

 

 

 

 

where       is   weight group       
is the weight matrix
 or   vector  for group   that is de ned on       and wg  
is   weight at index    for group    Since  cid norm has the
grouping effect that results in similar weights for correlated
features  this will result in complete elimination of some
groups  thus removing some input neurons  See Figure  
    This has an effect of automatically deciding how many
neurons to use at each layer 

    Group Sparsity

    Exclusive Sparsity

Figure   Illustration of the regularizers      When grouping
weights from the the same input neuron into each group  the group
sparsity has an effect of completely removing some neurons that
are not shared across different weights  highlighted in red     
Exclusive sparsity  on the other hand  does not result in removal
of any input neurons  but rather it makes each upper layer unit to
select from   set of lowerlayer units  that is disjoint from the sets
used by other units 

Still  this group sparsity does not maximally utilize the capacity of the network since there still could be redundancy
among the features that are selected  Thus  we propose
to apply   sparsityinducing regularization that obtains  
sparse network weight matrix  while also minimizing the
redundancy among network weights for better utilization
of the network capacity 

  Exclusive Sparsity Regularization for Deep Neural

Networks

Exclusive sparsity  or exclusive lasso was  rst introduced
in  Zhou et al    in multitask learning context  The
main idea in the work is to enforce the model parameters
for different tasks to compete for features  instead of sharing features as suggested by previous work on multitask
learning that leverages group lasso  When the task is  
classi cation task  this makes sense since the objective is
to differentiate between classes which can be achieved by
identifying discriminative feature for each class 
The generic exclusive sparsity regularization is de ned as
follows 

 cid cid 

 cid 

 cid 
    
     

 

 

 

 cid 

 

        

 
 

 cid      
   cid 

   

 
 

where     
    is the ith instance of the submatrix  or the vector       
    This norm is often called as    norm  and
is basically the  norm over  norm groups  The sparsity is now enforced within each group  as opposed to
the group sparsity regularizer which promotes intergroup
sparsity  Applying  norm over these  norm groups will
result in even weights among the groups  that is  all groups
should have similar number of nonsparse weights  and
thus no group can have large number of nonsparse weight 
In  Zhou et al    Wg is de ned to be the model parameter for multiple tasks on the same feature  in which
case the  norm enforces each task predictor to    to

Combined Group and Exclusive Sparsity for Deep Neural Networks

    Group Sparsity   Filter

    Group Sparsity   Feature

    Exclusive Sparsity

Figure   Illustration of the effect of each regularizer on convoultional  lters      Group sparsity  when each group is de ned as  
 lter  can result in complete elimination of some  lters that are not shared among multiple highlevel  lters  Wen et al    Alvarez  
Salzmann        Group sparsity  when applied across  lters for the same feature  will remove certain spatial features as   whole     
Exclusive sparsity enforces each convolutional  lter to learn features that are as different as possible  by promoting competition among
the  lters for the same spatial feature 

few features that are most useful for it  Exclusive sparsity can be straightforwardly applied to fully connected layers of   deep network  by grouping network weights from
the same neuron at each layer into one group and applying    norm on these groups  See Figure     This
will enforce each output neuron to compete for input neurons  which will result in learning largely disparate network
weights at each layer 

Exclusive sparsity on convolutional  lters For convolutional layers of   convolutional neural network  exclusive sparsity can be applied in the same manner as in fully
connected layers  where we apply Eq    on the convolutional  lters  while de ning each group   as the same feature across multiple convolutional  lters  Figure     illustrates the feature groups and effect of exclusive sparsity
on the convolutional  lters  This will enforce the convolutional  lters to be as different as possible from each other 
removing any redundancies between them 

  Combined Group   Exclusive Sparsity

Regularization

As mentioned earlier  our main intuition is that there are
varying degree of sharing and exclusivity among different
features  Exclusivity alone cannot result in learning an optimal set of features  since some features need to be shared
across multiple higherlevel features  Thus we need to allow for some degree of sharing across the features  while
still making each weight to be suf ciently different in order for each feature to be meaningful  How can we then
come up with   regularizer that can achieve the two seemingly con icting goals 
In tree guided group lasso  Kim   Xing    each pair
of weights are given different degree of sharing and competition based on the similarity between the tasks given by
  taxonomy  which can be either semantically de ned or
obtained through clustering  through   regularization similar to an elasticnet formulation  While this model can be

applied at the  nal softmax layer  on the softmax weight
for each class  such taxonomy does not exist for the intermediate level network weights  and it is also not ef cient to
obtain them through clustering or other means 
Thus we propose to simply combine the group sparsity and
the exclusive sparsity together  which will result in   similar effect  where network weights exhibit certain degree of
sharing if they are correlated  but are learned to be different
on other parts that are not shared  Our combined group and
exclusive lasso regularizer is given as follows 

 cid 

 cid 

 cid 

 

        

       cid      

   cid   

 cid      
   cid 

 

  
 

 

where   is the parameter that decides the entire regularization effect      is the weight matrix for lth layer  and   
is the parameter for balancing the sharing and competition
term at each layer 
Then how should we set the balancing term    at each
layer  One simple solution is to set all    to be   single
constant  but   better way is to set them differently at each
layer  based on the degree of sharing and competition required at each layer  At lower layers  features will be quite
generic and might need to be shared across all highlevel
neurons for accurate expression of the input data  wheareas
at the top layer softmax weights  it would be better to have
the weights to select features as disjoint as possible for better discriminativity  Thus  we set                   
  
to re ect such intuition  where   is   total number of all
layers                is an index of each layer  and
          is the lowest parameter value for the exclusive
sparsity term  If       the regularizer reduces to    
norm regularizer with       at the lowest layer  while at
the topmost softmax layer  the regularizer is an    norm
regularizer      

  Numerical Optimization

Our regularized learning objective can be solved using
proximal gradient descent  which is often used for optimizing objectives formed as   combination of both smooth

Combined Group and Exclusive Sparsity for Deep Neural Networks

and nonsmooth terms  The proximal gradient algorithm
for regularized objective  rst obtains the intermediate soby taking   gradient step using the gradient
lution Wt   
computed on the loss only  and then optimize for the regularization term while performing Euclidean projection of it
to the solution space  as in the following formulation 

 

min
Wt 

 Wt   

 

  

 cid Wt    Wt   

 

 cid 

 

 

where Wt  is the variable to obtain after the current iteration    is the regularization parameter  and   is the step size 
When  Wt  is   group sparsity regularizer or an exclusive sparsity regularizer  the above problem has   closedform solution 
The solution  or the proximal operator for the group sparsity regularizer  proxGL     is given as follows 

 cid 

 cid 

proxGL     

     

 cid wg cid 

wg  

 

 

for all   and    where   is each group  and   is an element
of in each group  The proximal operator for the exclusive
sparsity regularizer  proxEL    is obtained as follows 

 cid 
     cid wg cid 
 wg   

 cid 

proxEL     

  sign wg   cid cid wg  

wg  

 cid cid     cid wg cid 

 

 

for all   and    The combined regularizer can be optimized
simply by applying the two proximal operators in   row at
each gradient step  after updating the variable with the lossbased gradient  Algorithm   describes the proximal gradient algorithm for optimizing our regularized objective 

Algorithm   Stochastic Proximal Gradient Algorithm for
Combined   and   regularization
Input         minibatch size    learning rate  

Initialize     
while Some prede ned stopping criterion is satis ed do

Randomly select   samples from             
for each layer    do
      

 cid 
   cid fp     
   

     st

    
 cid  Update the
    
 
parameter with the gradient of   nonregularized objective
    
   cid  Apply proxGL in Eq   
    
    
     proxEL      
    
end for
end while

   cid  Apply proxEL in Eq   

  proxGL     
    
 

   GL

   GL

 

  Experiment
We perform all experiments with convolutional neural network as the base network model  The regularization is ap 

plied at the network weights for all layers  excluding the
bias term  All models are implemented and experimented
using Tensor ow  Abadi et al    framework  

Baselines and our models We compare our regularized
networks against relevant baselines 
   cid  The network trained with  cid regularization 
   cid  The network trained with  cid regularization  which
has elementwise sparse network weights 
  Group SparsityFilter  The network regularized with
 cid norm on the weights  which groups each convolutional
 lter as   group at convolutional layers  This network is an
implementation of the model in  Wen et al   
  Group SparsityFeature  The network that uses the
same  cid regularization as in   but with each group de 
 ned as the same feature at different  lters 
  Exclusive Sparsity  This is the network whose weights
at each layer are regularized with  cid norm only 
  Combined Group and Exclusive Sparsity  The network regularized with our combined structured sparsity on
the weights  The combination weight that balances both
regularizations are dynamically set at each layer 

Datasets and base networks We validate our method on
four public datasets for classi cation  with four different
convolutional networks 
  MNIST  This dataset contains         grayscale
images of handwritten digits for training example images 
where there is     training instances and     test instances per class  As for the base network  we use   simple
convolutional neural network with two convolutional layers
and two fully connected layers 
  CIFAR  This dataset consists of     images
sized       from ten animal and vehicle classes  airplane  automobile  bird  cat  deer  dog  frog  horse  ship 
and truck  For each class  there are     images for training and     images for test  For the base network  we
use LeNet  Lecun et al    that has two convolutional
layers followed by three fully connected layers 
  CIFAR  This dataset also consists of     images of     pixels as in CIFAR  but has   generic
object classes instead of   For each class    images
are used for training and   images are used for test  For
the base network  we use   variant of Wide Residual Network  Zagoruyko   Komodakis    which has   layers with the widening factor of      
  ImageNet    This is the dataset for   ImageNet

 Codes available at https github com jaehongyoon CGES

Combined Group and Exclusive Sparsity for Deep Neural Networks

    MNIST  Accuracy     of Parameters

    CIFAR  Accuracy     of Parameters

    CIFAR  Accuracy     of Parameters

    MNIST  Accuracy   FLOP

    CIFAR  Accuracy   FLOP

    CIFAR  Accuracy   FLOP

Figure   Accuracyef ciency tradeoff  We report the accuracy over number of parameters  and accuracy over FLOP to see how each
sparsityinducing regularization at various sparsity range affect the model accuracy  The reported results are average model accuracy
over three runs  with random weight initialization  and the errorbars denote standard errors for   con dence interval     and    are
the networks trained with  cid regularization   cid regularization  GS lter and GSfeature are  lterwise and featurewise group sparsity
respectively  ES is our exclusive sparsity regularizer  and CGES is our proposed combined group and exclusive sparsity regularizer 

Large Scale Visual Recognition Challange  Deng et al 
  that consists of       images from    
generic object categories  For evaluation  we used the validation set that consists of     images  following the
standard procedure  For the base network  we used an implementation of AlexNet  Krizhevsky et al   
For MNIST and CIFAR  experiment  we train all networks from the scratch  for CIFAR  and ImageNet 
   experiment where we use larger networks  WRN and
AlexNet  we  netune the network from the  cid  regularized networks  since training them from scratch takes prohibitively long time 

  Quantitative analysis

We  rst validate whether our sparsityinducing regularizations result in better accuracyef ciency tradeoff compared to baseline methods  by measuring the prediction accuracy over number of parameters  and number of  oating
point operations  FLOP  for each method 
Figure   shows the prediction accuracy of the different
models over number of parameters FLOP  obtained by differentiating the sparsityinducing regularization parameter
for each method  As expected   cid regularization greatly reduces the number of parameters  while maintaining   similar performance to the original model  The group sparsity regularization in general performs worse than  cid  but

achieves better accuracy in certain sparsity ranges  The exclusive sparsity improves the performance over the base  cid 
regularization model in lowsparsity range which is especially well shown in CIFAR  result  but degenerates performance as the sparsity increases  We attribute this to the
fact that exclusive sparsity aims to make each weight lter
to    to completely disjoint sets of lowlevel features  which
is unrealistic as features may need to    to the same set of
lowlevel features for accurate representation 
Finally  our combined group and exclusive sparsity 
which allows for certain degree of sharing between the
weights features while enforcing exclusivity  achieves the
best accuracy parameter tradeoff  achieving similar or better performance gain to the exclusive sparsity while also
greatly reducing the number of parameters  Fig     shows
the results on the MNIST dataset  on which our CGES obtains no accuracy reduction  using   less number of
parameters and   less computation  On CIFAR 
dataset  CGES improves the classi cation accuracy over
the  cid  baselines by   using   less number of
parameters using   less FLOP  CGES obtains slight
accuracy reduction of   on CIFAR  dataset  using
only   of parameters and   less FLOP 
On ImageNet  Table   CGES obtains similar or slightly
worse performance to the full network while using    
  of its parameters  while  cid  shows noticeable performance degeneration at the same sparsity level 

 Percentage of Parameters used  Accuracy      GS filterGS featureESCGES Percentage of Parameters used  Accuracy      GS filterGS featureESCGES Percentage of Parameters used  Accuracy      GS filterGS featureESCGES Flop    Accuracy      GS filterGS featureESCGES Flop    Accuracy      GS filterGS featureESCGES Flop    Accuracy      GS filterGS featureESCGESCombined Group and Exclusive Sparsity for Deep Neural Networks

    Convergence speed

    Conv vs  FC layers

    Effect of   

Figure   Further Analysis of the exclusive sparsity on CIFAR  dataset      Convergence speed  Networks regularized with ES  Light
Blue  or CGES  Dark Blue  converge fastest to   given error rate  compared to  cid      Effect of exclusive sparsity at different types
of layers  The network regularized with exclusive sparsity at all layers performed better with higher sparsity  compared to models that
used ES only at convolutional  or fully connected layers      Effect of     ESincreasing is our combined regularizer  where exclusivity
increases with network layer    For ESconstant  we set        at all layers 

Table   Accuracyef ciency tradeoff on the Imagenet dataset 
Model Accuracy   Params  Accuracy   Params 

  
  
ES

CGES

 
 
 
 

 
 
 
 

 

 
 
 

 

 
 
 

Table   Performance of CGES coupled with iterative pruning 
The reported results are averages over   runs and standard errors
for   con dence interval 

Model

    Full Network 
Han et al   

CGES

MNIST
 

CIFAR 
 

           
           

Iterative pruning Iterative pruning  Han et al    is
another effective method for obtaining   sparse network
while maintaining high accuracy  As iterative pruning is
orthogonal to our method  we can couple the two methods
to obtain even better performance per number of parameters used  speci cally  we replace the usual weight decay
regularizer used in  Han et al    with our CGES regularizer  We report the accuracy of this combined model
on MNIST and CIFAR  dataset  when using   of the
parameters of the full network  Table   The results show
that CGES coupled with iterative pruning obtains similar or
even better results to the original model using only   fraction of the parameters  signi cantly outperforming the base
pruning model which suffers substantial accuracy loss 

Convergence speed We further analyze the empirical
convergence rate of our regularized network  since it will
be impractical if the regularized network requires much
longer iterations to reach the same accuracy 
Interestingly  we empirically found that our exclusive sparsity regularizer also helps network achieve the same error using
much fewer iterations  Figure     compared to base  cid 
regularization  This faster convergence agrees with the observations in  Saxe et al    where networks whose
weights are initialized as random orthgonal matrices con 

verged faster than networks with random Gaussian initialization 

Convolutional vs 
fully connected layers To see how
much effect our combined regularizer has on different types
of layers  we experiment applying the model only to the
fully connected layer  or convolutional layers  while applying usual  cid regularizer to other layers  Figure     shows
the result of this experiment  where we plot the accuracy
over percentage of parameters used  for models that applies
ES only to fully connected layers  only to convolutional
layers  and both  We observe improvements on all models  which shows the effectiveness of the exclusive sparsity regularizer to all types of network weights  Further 
ES results in larger improvements on convolutional layers 
which makes sense since lowerlayer features are more important as they are more generic across different classes 
than the features learned at fully connected layers  However  conv layers obtained the best accuracy at lowsparsity
range  since strict enforcement of exclusivity hurts the representational power of the features  whereas FC layers obtained improvements even on highsparsity range  this may
be because loss of expressiveness could be compensated by
better discriminativity of the features at high level 

Sharing vs  Competing for Features We further explore how varying the degree of sharing and competition
affect the accuracy and ef ciency of the model  by experimenting with different con gurations of    in Eq    at each
layer  We report the results in Figure     Speci cally 
we test two different approaches to balance the degree of
sharing and competition at each layer  The  rst model 
ESIncreasing  is the actual combination we have used in
our method which increases the effect of exclusive sparsity with increasing    This model re ects our intuition
that competition will help at high layers  while sharing will
help more at lower layers  The second model  ESConstant
combines the two terms with        throughout all layers  We observe that ESIncreasing works better than ES 

 Iteration    Error      ESCGES Percentage of Parameters used  Accuracy    ES convES fcES full Percentage of Parameters used  Accuracy    ES IncreasingES ConstantCombined Group and Exclusive Sparsity for Deep Neural Networks

     cid regularization

     cid regularization

    Group sparsity

    Exclusive sparsity

    CGES

Figure   Visualizations of the last fully connected layer weights on the CIFAR  dataset  These  gures show the weight of  rst  
weights out of   weights  The rows are output units for each class and the columns are features 

     cid  Sparsity     

     cid  Sparsity     

    GSFilter  Sparsity          CGES  Sparsity     

Figure   Visualizaiton of the  st convolution layer  lters from the network trained on CIFAR  dataset       cid regularization results
in smooth nonsparse  lters       cid regualrization results in  lters that are elementwise sparse      GSFilter results in complete removal
of some  lters      CGES obtains sharper  lters with some spatial features completely zeroed out  from competition among the  lters 

Constant across all sparsity ranges  which shows that our
scheme of increasing exclusivity at higher layers indeed
helps improve the model performance 

  Qualitative analysis

For further qualitative analysis  we visualize the weights
and convolutional  lters obtained using the baselines and
our methods 
Figure   visualizes the weights of the softmax layer for different regularization methods  from the network trained on
the CIFAR  dataset  Each row is the softmax parameter for each class   cid  and  cid  work as expected  resulting
in nonsparse and elementwise sparse weights  The group
sparsity regularizer results in the total elimination of certain
features that are not shared across multiple classes  The
exclusive sparsity regularizer  when used on its own  results in disjoint feature selection for each class  However 
when combined with the group lasso  it allows certain degree of feature reuse  while still obtaining parameters that
are largely disparate across classes 
To show that such effect is not con ned to the fully connected layer  we also visualize the convolutional  lters in
the  rst convolutional layer of the network trained on the
CIFAR  dataset  in Figure   We observe that the combined group and exclusive sparsity regularizer results in  lters that are much sharper than the ones that are obtained by
 cid  or group sparsity regularization  with some spatial features dropped altogether from the competition with other
 lters  Further  there is less redundancy among the  lters 

unlike the  lters learned by other regularization methods 
Note that we set the exclusivity factor       just for
visualization purpose  since our weighting scheme will set
  as   low value in the  rst convolutional layer 

  Conclusion
In this work  we proposed   novel regularizer for generic
deep neural networks that effectively utilizes the capacity
of the network  by exploiting the sharing and competing
relationships among different network weights  Speci 
cally  we propose to use an exclusive sparsity regularization
based on    norm on the network weights  along with
group sparsity regularization using    norm  such that
exclusive sparsity enforces the network weights to use input neurons that are as different as possible from the other
weights  while the group sparsity allows for some degree
of sharing among them  as it is impossible to make the network weights to    to completely disjoint set of features 
We validate our method on some public datasets for both
the accuracy and ef ciency against other sparsityinducing
regularizers  and the results show that our combined regularizer helps obtain even better performance than the original full network  while signi cantly reducing the memory
and computation requirements 

Acknowledgements This work was supported by Basic Science Research Program through the National
Research Foundation of Korea  NRF  funded by the
Ministry of Science 
ICT   Future Planning  NRF 
        and UAVtechnology development

Combined Group and Exclusive Sparsity for Deep Neural Networks

program through the National Research Foundation of Korea  NRF  funded by the Korea Aerospace Research Institute  NRF         

Lecun     Bottou     Bengio     and Haffner     Gradientbased Learning Applied to Document Recognition  Proceedings of the IEEE     

References
Abadi  Mart    Agarwal  Ashish  Barham  Paul  Brevdo 
Eugene  Chen  Zhifeng  Citro  Craig  Corrado  Greg   
Davis  Andy  Dean  Jeffrey  Devin  Matthieu  et al  Tensor ow  Largescale Machine Learning on Heterogeneous Distributed Systems  arXiv   

Alvarez  Jose   and Salzmann  Mathieu  Learning the

number of neurons in deep networks  In NIPS   

Ba  Jimmy and Caruana  Rich  Do deep nets really need to

be deep  In NIPS   

Collins  Maxwell   and Kohli  Pushmeet  Memory
bounded deep convolutional networks  arXiv preprint
arXiv   

Deng     Dong     Socher     Li       Li     and ei 
   FeiF  Imagenet    LargeScale Hierarchical Image
Database  In CVPR   

Reed     Pruning algorithmsa survey  IEEE Transactions
ISSN

on Neural Networks    Sep  
  doi   

Saxe  Andrew    McClelland  James    and Ganguli 
Surya  Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks  In ICLR   

Srivastava  Nitish  Hinton  Geoffrey  Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan  Dropout   
simple way to prevent neural networks from over tting 
Journal of Machine Learning Research   
 

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet 
Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  and Rabinovich  Andrew 
Going Deeper with Convolutions  In CVPR   

Tibshirani     Regression shrinkage and selection via the
lasso  Journal of the Royal Statistical Society  Series   
   

Goo  Wonjoon  Kim  Juyong  Kim  Gunhee  and Hwang 
Sung Ju  TaxonomyRegularized Semantic Deep Convolutional Neural Networks  In ECCV   

Wen  Wei  Wu  Chunpeng  Wang  Yandan  Chen  Yiran 
and Li  Hai  Learning structured sparsity in deep neural
networks  In NIPS  pp     

Han  Song  Pool  Jeff  Tran  John  and Dally  William 
Learning both weights and connections for ef cient neural network  In NIPS   

Yuan  Ming and Lin  Yi  Model selection and estimation in
regression with grouped variables  Journal of the Royal
Statistical Society  Series       

Zagoruyko  Sergey and Komodakis  Nikos  Wide residual

networks  In BMVC   

Zhou     Xiao     and Wu     Hierarchical Classi cation

via Orthogonal Transfer  In ICML   

Zhou  Yang  Jin  Rong  and Hoi  Steven       Exclusive
lasso for multitask feature selection  Journal of Machine Learning Research     

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun 
Jian  Deep residual learning for image recognition  In
CVPR   

Hinton  Geoffrey  Vinyals  Oriol  and Dean  Jeff  Distilling
the knowledge in   neural network  In NIPS   Deep
Learning Workshop   

Hwang  Sung Ju  Grauman  Kristen  and Sha  Fei  Learning
  tree of metrics with disjoint visual features  In NIPS 
 

Kim     and Xing        Treeguided group lasso for multiIn ICML  pp 

task regression with structured sparsity 
   

Kong  Deguang  Fujimaki  Ryohei  Liu  Ji  Nie  Feiping 
and Ding  Chris  Exclusive feature learning on arbitrary
structures via  cid   norm  In NIPS   

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey   
ImageNet Classi cation with Deep Convolutional Neural Networks  In NIPS   

