meProp  Sparsi ed Back Propagation for Accelerated Deep Learning

with Reduced Over tting

Xu Sun     Xuancheng Ren     Shuming Ma     Houfeng Wang    

Abstract

We propose   simple yet effective technique for
neural network learning  The forward propagation is computed as usual  In back propagation 
only   small subset of the full gradient is computed to update the model parameters  The gradient vectors are sparsi ed in such   way that
only the topk elements  in terms of magnitude 
are kept  As   result  only   rows or columns
 depending on the layout  of the weight matrix
are modi ed  leading to   linear reduction    divided by the vector dimension  in the computational cost  Surprisingly  experimental results demonstrate that we can update only   of the
weights at each back propagation pass  This does
not result in   larger number of training iterations  More interestingly  the accuracy of the resulting models is actually improved rather than
degraded  and   detailed analysis is given 

  Introduction
Neural network learning is typically slow  where back propagation usually dominates the computational cost during
the learning process  Back propagation entails   high computational cost because it needs to compute full gradients
and update all model parameters in each learning step  It
is not uncommon for   neural network to have   massive
number of model parameters 
In this study  we propose   minimal effort back propagation method  which we call meProp  for neural network
learning  The idea is that we compute only   very small
but critical portion of the gradient information  and update
only the corresponding minimal portion of the parameters
in each learning step  This leads to sparsi ed gradients 

 School of Electronics Engineering and Computer Science 
Peking University  China  MOE Key Laboratory of Computational Linguistics  Peking University  China  Correspondence to 
Xu Sun  xusun pku edu cn 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

such that only highly relevant parameters are updated and
other parameters stay untouched  The sparsi ed back propagation leads to   linear reduction in the computational
cost 
To realize our approach  we need to answer two questions 
The  rst question is how to  nd the highly relevant subset
of the parameters from the current sample in stochastic learning  We propose   topk search method to  nd the most
important parameters 
Interestingly  experimental results
demonstrate that we can update only   of the weights
at each back propagation pass  This does not result in  
larger number of training iterations  The proposed method
is generalpurpose and it is independent of speci   models
and speci   optimizers       Adam and AdaGrad 
The second question is whether or not this minimal effort
back propagation strategy will hurt the accuracy of the trained models  We show that our strategy does not degrade
the accuracy of the trained model  even when   very small
portion of the parameters is updated  More interestingly 
our experimental results reveal that our strategy actually
improves the model accuracy in most cases  Based on our
experiments  we  nd that it is probably because the minimal effort update does not modify weakly relevant parameters in each update  which makes over tting less likely 
similar to the dropout effect 
The contributions of this work are as follows 

  We propose   sparsi ed back propagation technique
for neural network learning  in which only   small
subset of the full gradient is computed to update the
model parameters  Experimental results demonstrate
that we can update only   of the weights at each
back propagation pass  This does not result in   larger
number of training iterations 

  Surprisingly  our experimental results reveal that the
accuracy of the resulting models is actually improved  rather than degraded  We demonstrate this effect
by conducting experiments on different deep learning
models  LSTM and MLP  various optimization methods  Adam and AdaGrad  and diverse tasks  natural
language processing and image recognition 

Sparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Figure   An illustration of meProp 

  Proposed Method
We propose   simple yet effective technique for neural network learning  The forward propagation is computed as
usual  During back propagation  only   small subset of the
full gradient is computed to update the model parameters 
The gradient vectors are  quantized  so that only the topk
components in terms of magnitude are kept  We  rst present the proposed method and then describe the implementation details 

  meProp

Forward propagation of neural network models  including
feedforward neural networks  RNN  LSTM  consists of linear transformations and nonlinear transformations  For
simplicity  we take   computation unit with one linear
transformation and one nonlinear transformation as an example 

       

 

       

 
where     Rn        Rm      Rn      Rn    is the
dimension of the input vector    is the dimension of the output vector  and   is   nonlinear function       relu  tanh 
and sigmoid  During back propagation  we need to compute the gradient of the parameter matrix   and the input
vector   

  
 Wij

 cid 
ixT
 

                     

   

                     

 cid 

 

 cid 
 

 

ij  

   

  
 xi
    Rn means  zi

 cid 

  
 Wij

 cid 

 

  
 xi

That

The proposed meProp uses approximate gradients by
keeping only topk elements based on the magnitude values 
is  only the topk elements with the largest absolute values are kept  For example  suppose  
vector      cid     cid  then top       cid     cid 
     topk values as
We denote the indices of vector  
         tk           and the approximate gradient
of the parameter matrix   and input vector   is 

 cid 

   

 cid 
ixT
 

             tk  else  

if

 

   

ij  

 cid 
 

if              tk  else    

As   result  only   rows or columns  depending on the layout  of the weight matrix are modi ed  leading to   linear
reduction    divided by the vector dimension  in the computational cost 
Figure   is an illustration of meProp for   single computation unit of neural models  The original back propagation
uses the full gradient of the output vectors to compute the
gradient of the parameters  The proposed method selects
the topk values of the gradient of the output vector  and
backpropagates the loss through the corresponding subset
of the total model parameters 
As for   complete neural network framework with   loss   
the original back propagation computes the gradient of the
parameter matrix   as 

 

 

  
  

 

  
  

    
  

while the gradient of the input vector   is 

  
  

 

  
  

    
  

 

 

  We can see that the computatiwhere  
onal cost of back propagation is directly proportional to the
dimension of output vector   

 yi

The proposed meProp selects topk elements of the gradient   
   to approximate the original gradient  and passes

Sparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Figure   An illustration of the computational  ow of meProp 

them through the gradient computation graph according to
the chain rule  Hence  the gradient of   goes to 

  
  

  topk 

  
  

      
  

while the gradient of the vector   is 
  topk 

  
  

    
  

  
  

 

 

 

Figure   shows an illustration of the computational  ow of
meProp  The forward propagation is the same as traditional forward propagation  which computes the output vector via   matrix multiplication operation between two input
tensors  The original back propagation computes the full
gradient for the input vector and the weight matrix  For meProp  back propagation computes an approximate gradient
by keeping topk values of the backward  owed gradient
and masking the remaining values to  
Figure   further shows the computational  ow of meProp
for the minibatch case 

  Implementation

We have coded two neural network models  including an
LSTM model for partof speech  POS  tagging  and   feedforward NN model  MLP  for transitionbased dependency

parsing and MNIST image recognition  We use the optimizers with automatically adaptive learning rates  including
Adam  Kingma   Ba    and AdaGrad  Duchi et al 
  In our implementation  we make no modi cation to
the optimizers  although there are many zero elements in
the gradients 
Most of the experiments on CPU are conducted on the framework coded in    on our own  This framework builds  
dynamic computation graph of the model for each sample 
making it suitable for data in variable lengths    typical
training procedure contains four parts  building the computation graph  forward propagation  back propagation  and
parameter update  We also have an implementation based
on the PyTorch framework for GPU based experiments 

  WHERE TO APPLY MEPROP

The proposed method aims to reduce the complexity of the
back propagation by reducing the elements in the computationally intensive operations  In our preliminary observations  matrixmatrix or matrixvector multiplication consumed more than   of the time of back propagation  In our
implementation  we apply meProp only to the back propagation from the output of the multiplication to its inputs 
For other elementwise operations       activation functions  the original back propagation procedure is kept  be 

Sparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Figure   An illustration of the computational  ow of meProp on   minibatch learning setting 

cause those operations are already fast enough compared
with matrixmatrix or matrixvector multiplication operations 
If there are multiple hidden layers  the topk sparsi cation
needs to be applied to every hidden layer  because the sparsi ed gradient will again be dense from one layer to another  That is  in meProp the gradients are sparsi ed with  
topk operation at the output of every hidden layer 
While we apply meProp to all hidden layers using the same
  of topk  usually the   for the output layer could be different from the   for the hidden layers  because the output
layer typically has   very different dimension compared
with the hidden layers  For example  there are   tags in
the MNIST task  so the dimension of the output layer is
  and we use an MLP with the hidden dimension of  
Thus  the best   for the output layer could be different from
that of the hidden layers 

  CHOICE OF TOPk ALGORITHMS

Instead of sorting the entire vector  we use the wellknown
minheap based topk selection method  which is slightly
changed to focus on memory reuse  The algorithm has  
time complexity of     log    and   space complexity of
    

  Related Work
Riedmiller and Braun   proposed   direct adaptive
method for fast learning  which performs   local adaptation of the weight update according to the behavior of
the error function  Tollenaere   also proposed an
adaptive acceleration strategy for back propagation  Dropout  Srivastava et al    is proposed to improve training speed and reduce the risk of over tting  Sparse coding is   class of unsupervised methods for learning sets of
overcomplete bases to represent data ef ciently  Olshausen   Field    Ranzato et al    proposed   sparse
autoencoder model for learning sparse overcomplete features  The proposed method is quite different compared
with those prior studies on back propagation  dropout  and
sparse coding 
The sampledoutput loss methods  Jean et al    are
limited to the softmax layer  output layer  and are only
based on random sampling  while our method does not
have those limitations  The sparselygated mixtureof 
experts  Shazeer et al    only sparsi es the mixtureof experts gated layer and it is limited to the speci   setting of mixtureof experts  while our method does not have
those limitations  There are also prior studies focusing
on reducing the communication cost in distributed systems  Seide et al    Dryden et al    by quanti 

Sparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Table   Results based on LSTM MLP models and AdaGrad Adam optimizers  Time means averaged time per iteration  Iter means the
number of iterations to reach the optimal score on development data  The model of this iteration is then used to obtain the test score 

POSTag  AdaGrad 
LSTM    
meProp    
Parsing  AdaGrad 
MLP    
meProp    
MNIST  AdaGrad 
MLP    
meProp    
POSTag  Adam 
LSTM    
meProp    
Parsing  Adam 
MLP    
meProp    
MNIST  Adam 
MLP    
meProp    

Iter Backprop time     Dev Acc  

 
 

 

     

 
   
Iter Backprop time     Dev UAS  
 
 

 
   
Iter Backprop time     Dev Acc  

     

 

 

 
 
 
   
Iter Backprop time     Dev Acc  

     

 
 

 

     

 
   
Iter Backprop time     Dev UAS  
 
 

 
   
Iter Backprop time     Dev Acc  
 
 

 
   

     

     

 

 

Test Acc  
 
   
Test UAS  
 
   
Test Acc  
 
   
Test Acc  
 
   
Test UAS  
 
   
Test Acc  
 
   

Table   Overall forward propagation time vs  overall back propagation time  Time means averaged time per iteration  FP means
forward propagation  BP means back propagation  Ov  time means overall training time  FP   BP 

     
  

POSTag  Adam  Ov  FP time Ov  BP time
LSTM    
meProp    
Parsing  Adam  Ov  FP time Ov  BP time
MLP    
meProp    
MNIST  Adam  Ov  FP time Ov  BP time
MLP    
meProp    

  
  

  

  
  

  

      

      

      

Ov  time
  
  
Ov  time
  
  
Ov  time
  
  

zing each value of the gradient from  bit  oat to only
 bit  Those settings are also different from ours 

  Experiments
To demonstrate that
the proposed method is generalpurpose  we perform experiments on different models
 LSTM MLP  various training methods  Adam AdaGrad 
and diverse tasks 
Partof Speech Tagging  POSTag  We use the standard
benchmark dataset in prior work  Collins    which is
derived from the Penn Treebank corpus  and use sections
  of the Wall Street Journal  WSJ  for training  
examples  and sections   for testing   examples 
The evaluation metric is perword accuracy    popular model for this task is the LSTM model  Hochreiter   Schmidhuber    which is used as our baseline 

 In this work  we use the bidirectional LSTM  BiLSTM  as

the implementation of LSTM 

Transitionbased Dependency Parsing  Parsing  Following prior work  we use English Penn TreeBank  PTB 
 Marcus et al    for evaluation  We follow the standard split of the corpus and use sections   as the training set   sentences    transition examples  section   as the development set   sentences 
  transition examples  and section   as the  nal test
set   sentences    transition examples  The
evaluation metric is unlabeled attachment score  UAS  We
implement   parser using MLP following Chen and Manning   which is used as our baseline 
MNIST Image Recognition  MNIST  We use the
MNIST handwritten digit dataset  LeCun et al    for
evaluation  MNIST consists of     pixel training images and additional   test examples  Each
image contains   single numerical digit   We select
the  rst   images of the training images as the development set and the rest as the training set  The evaluation
metric is perimage accuracy  We use the MLP model as
the baseline 

  Experimental Settings

We set the dimension of the hidden layers to   for all
the tasks  For POSTag  the input dimension is    word   
   dim per word       features       dim per feature   
  and the output dimension is   For Parsing  the input
dimension is    features       dim per feature     
and the output dimension is   For MNIST  the input dimension is    pixels per row       pixels per column   
   dim per pixel      and the output dimension is  
As discussed in Section   the optimal   of topk for the

   transition example consists of   parsing context and its op 

timal transition action 

Sparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Figure   Accuracy vs  meProp   backprop ratio  left  Results of topk meProp vs  random meProp  middle  Results of topk meProp
vs  baseline with the hidden dimension    right 

output layer could be different from the hidden layers  because their dimensions could be very different  For Parsing
and MNIST  we  nd using the same   for the output and the
hidden layers works well  and we simply do so  For another task  POSTag  we  nd the the output layer should use
  different   from the hidden layers  For simplicity  we do
not apply meProp to the output layer for POSTag  because
in this task we  nd the computational cost of the output
layer is almost negligible compared with other layers 
The hyperparameters are tuned based on the development
data  For the Adam optimization method  we  nd the
default hyperparameters work well on development sets 
which are as follows 
the learning rate       and
                  For the AdaGrad learner  the learning rate is set to           for POSTag  Parsing  and MNIST  respectively  and          
The experiments on CPU are conducted on   computer with
the INTEL    Xeon     GHz CPU  The experiments on
GPU are conducted on NVIDIA GeForce GTX  

  Experimental Results

In this experiment  the LSTM is based on one hidden layer
and the MLP is based on two hidden layers  experiments
on more hidden layers will be presented later  We conduct
experiments on different optimization methods  including
AdaGrad and Adam  Since meProp is applied to the linear transformations  which entail the major computational
cost  we report the linear transformation related backprop
time as Backprop Time  It does not include nonlinear activations  which usually have only less than   computational cost  The total time of back propagation  including nonlinear activations  is reported as Overall Backprop Time 
Based on the development set and prior work  we set the
minibatch size to    sentence     transition examples  and    images  for POSTag  Parsing  and MNIST 
respectively  Using   transition examples for Parsing
follows Chen and Manning  
Table   shows the results based on different models and

different optimization methods  In the table  meProp means applying meProp to the corresponding baseline model 
      means that the hidden layer dimension is   and
      means that meProp uses top  elements  among
  in total  for back propagation  Note that  for fair comparisons  all experiments are  rst conducted on the development data and the test data is not observable  Then 
the optimal number of iterations is decided based on the
optimal score on development data  and the model of this
iteration is used upon the test data to obtain the test scores 
As we can see  applying meProp can substantially speed
up the back propagation  It provides   linear reduction in
the computational cost  Surprisingly  results demonstrate
that we can update only   of the weights at each back
propagation pass  This does not result in   larger number
of training iterations  More surprisingly  the accuracy of
the resulting models is actually improved rather than decreased  The main reason could be that the minimal effort
update does not modify weakly relevant parameters  which
makes over tting less likely  similar to the dropout effect 
Table   shows the overall forward propagation time  the
overall back propagation time  and the training time by
summing up forward and backward propagation time  As
we can see  back propagation has the major computational
cost in training LSTM MLP 
The results are consistent among AdaGrad and Adam  The
results demonstrate that meProp is independent of speci 
   optimization methods  For simplicity  in the following
experiments the optimizer is based on Adam 

  Varying Backprop Ratio

In Figure    left  we vary the   of topk meProp to compare the test accuracy on different ratios of meProp backprop  For example  when    it means that the backprop
ratio is   The optimizer is Adam  As we can see 
meProp achieves consistently better accuracy than the baseline  The best test accuracy of meProp     

 Backprop Ratio  Accuracy  MNIST  Reduce Overfitting  mePropMLP Backprop Ratio  Accuracy  MNIST  Topk vs Random  Topk mePropRandom meProp Backprop Hidden Ratio  Accuracy  MNIST  Change      mePropMLPSparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Table   Results based on the same   and   
POSTag  Adam 
LSTM    
meProp    
Parsing  Adam 
MLP    
meProp    
MNIST  Adam 
MLP    
meProp    

Test Acc  
 
   
Test UAS  
 
   
Test Acc  
 
   

Iter
 
 
Iter
 
 
Iter
 
 

Table   Adding the dropout technique 

POSTag  Adam  Dropout
 
LSTM    
 
meProp    
Parsing  Adam 
Dropout
 
MLP    
meProp    
 
MNIST  Adam 
Dropout
 
MLP    
meProp    
 

Test Acc  
 
   
Test UAS  
 
   
Test Acc  
 
   

is actually better than the one reported in Table  

  Topk vs  Random

It will be interesting to check the role of topk elements 
Figure    middle  shows the results of topk meProp vs 
random meProp  The random meProp means that random
elements  instead of topk ones  are selected for back propagation  As we can see  the topk version works better
than the random version  It suggests that topk elements
contain the most important information of the gradients 

Table   Varying the number of hidden layers on the MNIST task 
The optimizer is Adam  Layers  the number of hidden layers 

Layers Method

MLP    
meProp    
MLP    
meProp    
MLP    
meProp    
MLP    
meProp    

Test Acc  
 
   
 
   
 
   
 
   

 

 

 

 

 

 

Table   Results of simple uni ed topk meProp based on   whole
minibatch       uni ed sparse patterns  The optimizer is Adam 
Minibatch Size is  

Layers Method

MLP    
meProp    
MLP    
meProp    

Test Acc  
 
   
 
   

  Adding Dropout

Since we have observed that meProp can reduce over tting
of deep learning    natural question is that if meProp is reducing the same type of over tting risk as dropout  Thus 
we use development data to  nd   proper value of the dropout rate on those tasks  and then further add meProp to
check if further improvement is possible 
Table   shows the results  As we can see  meProp can
achieve further improvement over dropout  In particular 
meProp has an improvement of   UAS on Parsing  The
results suggest that the type of over tting that meProp reduces is probably different from that of dropout  Thus   
model should be able to take advantage of both meProp
and dropout to reduce over tting 

  Varying Hidden Dimension

  Adding More Hidden Layers

We still have   question  does the topk meProp work well
simply because the original model does not require that big
dimension of the hidden layers  For example  the meProp
 topk  works simply because the LSTM works well with
the hidden dimension of   and there is no need to use the
hidden dimension of   To examine this  we perform
experiments on using the same hidden dimension as    and
the results are shown in Table   As we can see  however 
the results of the small hidden dimensions are much worse
than those of meProp 
In addition  Figure    right  shows more detailed curves
by varying the value of    In the  gure  different   gives
different backprop ratio for meProp and different hidden
dimension ratio for LSTM MLP  As we can see  the answer to that question is negative  meProp does not rely on
redundant hidden layer elements 

Another question is whether or not meProp relies on shallow models with only   few hidden layers  To answer this
question  we also perform experiments on more hidden layers  from   hidden layers to   hidden layers  We  nd setting
the dropout rate to   works well for most cases of different numbers of layers  For simplicity of comparison  we
set the same dropout rate to   in this experiment  Table  
shows that adding the number of hidden layers does not
hurt the performance of meProp 

  Speedup on GPU

For implementing meProp on GPU  the simplest solution
is to treat the entire minibatch as    big training example  where the topk operation is based on the averaged values of all examples in the minibatch  In this way  the big
sparse matrix of the minibatch will have consistent sparse

Sparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Table   Acceleration results on the matrix multiplication synthetic data using GPU  The batch size is  

Method
Baseline    
meProp    
meProp    
meProp    
meProp    
meProp    
meProp    
meProp    

Method
MLP    
meProp    
meProp    
meProp    
meProp    
meProp    
meProp    
meProp    

Backprop time  ms 

 

     
     
     
     
     
     
     

 
     
     
     
     
     
     
     

meProp also has substantial GPU speedup on MNIST with
the large hidden dimension  In this experiment  the speedup
is based on Overall Backprop Time  see the prior de nition  Those results demonstrate that meProp can achieve
good speedup on GPU when it is applied to heavy models 
Finally  there are potentially other implementation choices
of meProp on GPU  For example  another natural solution
is to use   big sparse matrix to represent the sparsi ed gradient of the output of   minibatch  Then  the sparse matrix
multiplication library can be used to accelerate the computation  This could be an interesting direction of future
work 

  Related Systems on the Tasks

The POS tagging task is   wellknown benchmark task 
with the accuracy reports from   to    Toutanova
et al    Sun    Shen et al    Tsuruoka et al 
  Collobert et al    Huang et al    Our method achieves    Table  
For the transitionbased dependency parsing task  existing
approaches typically can achieve the UAS score from  
to    Zhang   Clark    Nivre et al    Huang  
Sagae    As one of the most popular transitionbased
parsers  MaltParser  Nivre et al    has   UAS  Chen
and Manning   achieves   UAS using neural networks  Our method achieves   UAS  Table  
For MNIST  the MLP based approaches can achieve  
  accuracy  often around    LeCun et al    Simard et al    Ciresan et al    Our method achieves    Table   With the help from convolutional
layers and other techniques  the accuracy can be improved
to over    Jarrett et al    Ciresan et al    Our
method can also be improved with those additional techniques  which  however  are not the focus of this paper 

  Conclusions
The back propagation in deep learning tries to modify all
parameters in each stochastic update  which is inef cient
and may even lead to over tting due to unnecessary modi cation of many weakly relevant parameters  We propose   minimal effort back propagation method  meProp 
in which we compute only   very small but critical portion
of the gradient  and modify only the corresponding small
portion of the parameters in each update  This leads to very
sparsi ed gradients to modify only highly relevant parameters for the given training sample  The proposed meProp
is independent of the optimization method  Experiments
show that meProp can reduce the computational cost of
back propagation by one to two orders of magnitude via
updating only   parameters  and yet improve the model accuracy in most cases 

Table   Acceleration results on MNIST using GPU 

Overall backprop time  ms 

patterns among examples  and this consistent sparse matrix
can be transformed into   small dense matrix by removing
the zero values  We call this implementation as simple uni 
 ed topk  This experiment is based on PyTorch 
Despite its simplicity  Table   shows the good performance
of this implementation  which is based on the minibatch
size of   We also  nd the speedup on GPU is less signi cant when the hidden dimension is low  The reason
is that our GPU   computational power is not fully consumed by the baseline  with small hidden layers  so that the
normal back propagation is already fast enough  making it
hard for meProp to achieve substantial speedup  For example  supposing   GPU can  nish   operations in one
cycle  there could be no speed difference between   method with   and   method with   operations  Indeed  we
 nd MLP     and MLP     have almost the same
GPU speed even on forward propagation       without meProp  while theoretically there should be an    difference 
With GPU  the forward propagation time of MLP    
and MLP     is  ms and  ms  respectively  This
provides evidence for our hypothesis that our GPU is not
fully consumed with the small hidden dimensions 
Thus  the speedup test on GPU is more meaningful for the
heavy models  such that the baseline can at least fully consume the GPU   computational power  To check this  we
test the GPU speedup on synthetic data of matrix multiplication with   larger hidden dimension  Indeed  Table  
shows that meProp achieves much higher speed than the
traditional backprop with the large hidden dimension  Furthermore  we test the GPU speedup on MLP with the large
hidden dimension  Dryden et al    Table   shows that

Sparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Acknowledgements
The authors would like to thank the anonymous reviewers for insightful comments and suggestions on this paper  This work was supported in part by National Natural
Science Foundation of China  No    National
High Technology Research and Development Program of
China   Program  No   AA  and an Okawa
Research Grant  

References
Chen  Danqi and Manning  Christopher      fast and accurate dependency parser using neural networks  In Proceedings of EMNLP  pp     

Ciresan  Dan    Meier  Ueli  Gambardella  Luca Maria 
and Schmidhuber    urgen  Deep  big  simple neural nets
for handwritten digit recognition  Neural Computation 
   

Ciresan  Dan    Meier  Ueli  and Schmidhuber    urgen 
Multicolumn deep neural networks for image classi cation  In Proceedings of CVPR  pp     

Collins  Michael  Discriminative training methods for hidden markov models  Theory and experiments with perceptron algorithms  In Proceedings of EMNLP  pp 
   

Collobert  Ronan  Weston  Jason  Bottou    eon  Karlen 
Michael  Kavukcuoglu  Koray  and Kuksa  Pavel    Natural language processing  almost  from scratch  Journal
of Machine Learning Research     

Dryden  Nikoli  Moon  Tim  Jacobs  Sam Ade  and Essen  Brian Van  Communication quantization for dataparallel training of deep neural networks  In Proceedings
of the  nd Workshop on Machine Learning in HPC Environments  pp     

Duchi  John    Hazan  Elad  and Singer  Yoram  Adaptive
subgradient methods for online learning and stochastic
optimization  Journal of Machine Learning Research 
   

Hochreiter  Sepp and Schmidhuber    urgen  Long shortterm memory  Neural computation   
 

Huang  Liang and Sagae  Kenji  Dynamic programming
In Proceedings of

for lineartime incremental parsing 
ACL  pp     

Jarrett 

Kevin 

Kavukcuoglu 

Ranzato 
Marc Aurelio  and LeCun  Yann  What is the best
multistage architecture for object recognition 
In
Proceeding of ICCV  pp     

Koray 

Jean    ebastien  Cho  KyungHyun  Memisevic  Roland 
and Bengio  Yoshua  On using very large target vocabulary for neural machine translation  In Proceedings of
ACL IJCNLP  pp     

Kingma  Diederik    and Ba  Jimmy  Adam    method for

stochastic optimization  CoRR  abs   

LeCun  Yann  Bottou    eon  Bengio  Yoshua  and Haffner 
Patrick  Gradientbased learning applied to document recognition  Proceedings of the IEEE   
 

Marcus  Mitchell    Marcinkiewicz  Mary Ann  and Santorini  Beatrice  Building   large annotated corpus of
english  The penn treebank  Computational linguistics 
   

Nivre  Joakim  Hall  Johan  Nilsson  Jens  Chanev  Atanas 
Eryigit    ulsen    ubler  Sandra  Marinov  Svetoslav  and
Marsi  Erwin  Maltparser    languageindependent system for datadriven dependency parsing  Natural Language Engineering     

Olshausen  Bruno   and Field  David    Natural image
statistics and ef cient coding  Network  Computation in
Neural Systems     

Ranzato  Marc Aurelio  Poultney  Christopher    Chopra 
Sumit  and LeCun  Yann  Ef cient learning of sparse representations with an energybased model  In NIPS 
pp     

Riedmiller  Martin and Braun  Heinrich    direct adaptive
method for faster backpropagation learning  The rprop
algorithm  In Proceedings of IEEE International Conference on Neural Networks   pp     

Seide  Frank  Fu  Hao  Droppo  Jasha  Li  Gang  and Yu 
Dong   bit stochastic gradient descent and its application to dataparallel distributed training of speech dnns 
In Proceedings of INTERSPEECH  pp   
 

Shazeer  Noam  Mirhoseini  Azalia  Maziarz  Krzysztof  Davis  Andy  Le  Quoc    Hinton  Geoffrey   
and Dean  Jeff  Outrageously large neural networks 
The sparselygated mixtureof experts layer  CoRR 
abs   

Huang  Zhiheng  Xu  Wei  and Yu  Kai 

Bidirectional lstmcrf models for sequence tagging  CoRR 
abs   

Shen  Libin  Satta  Giorgio  and Joshi  Aravind    Guided learning for bidirectional sequence classi cation  In
Proceedings of ACL  pp     

Sparsi ed Back Propagation for Accelerated Deep Learning with Reduced Over tting

Simard  Patrice    Steinkraus  Dave  and Platt  John   
Best practices for convolutional neural networks applied
to visual document analysis  In Proceedings of ICDR 
pp     

Srivastava  Nitish  Hinton  Geoffrey    Krizhevsky  Alex 
Sutskever  Ilya  and Salakhutdinov  Ruslan  Dropout 
  simple way to prevent neural networks from over tJournal of Machine Learning Research   
ting 
   

Sun  Xu  Structure regularization for structured prediction 

In NIPS  pp     

Tollenaere  Tom  Supersab  fast adaptive back propagation with good scaling properties  Neural networks   
   

Toutanova  Kristina  Klein  Dan  Manning  Christopher   
and Singer  Yoram  Featurerich partof speech tagging
In Proceedings of
with   cyclic dependency network 
HLTNAACL  pp     

Tsuruoka  Yoshimasa  Miyao  Yusuke  and Kazama 
Jun ichi  Learning with lookahead  Can historybased
models rival globally optimized models  In Proceedings
of CoNLL  pp     

Zhang  Yue and Clark  Stephen    tale of two parsers 
Investigating and combining graphbased and
transitionbased dependency parsing  In Proceedings of
EMNLP  pp     

