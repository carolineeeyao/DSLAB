Highdimensional NonGaussian Single Index Models via Thresholded Score

Function Estimation

Zhuoran Yang   Krishnakumar Balasubramanian   Han Liu  

Abstract

We consider estimating the parametric component of single index models in high dimensions 
Compared with existing work  we do not require
the covariate to be normally distributed  Utilizing
Stein   Lemma  we propose estimators based on
the score function of the covariate  Moreover  to
handle score function and response variables that
are heavytailed  our estimators are constructed
via carefully thresholding their empirical counterparts  Under   bounded fourth moment condition  we establish optimal statistical rates of convergence for the proposed estimators  Extensive
numerical experiments are provided to back up
our theory 

  Introduction
Estimators for highdimensional parametric  linear  models have been developed and analyzed extensively in
two decades  see for example    uhlmann  
the last
van de Geer    Vershynin    for comprehensive
overviews  While being   useful testbed for illustrating
conceptual phenomenon  they often suffer from   lack of
 exibility in modeling realworld situations  On the other
hand  completely nonparametric models  although  exible  suffer from the curse of dimensionality unless restrictive additive sparsity or smoothness assumptions are imposed  Ravikumar et al    Yuan et al    An interesting compromise between the parametric and nonparametric models is provided by the socalled semiparametric
index models  Horowitz    Here  the response and the
covariate are linked through   lowdimensional nonparametric function that takes in as input   linear transformation of the covariate  The nonparametric component is also
called as the link function and the linear components are

  Department of Operations Research and Financial Engineering  Princeton University  Princeton  NJ   USA  Correspondence to  Han Liu  hanliu princeton edu 

Proceedings of the   th International Conference on Machine
Learning  Sydney  Australia  PMLR     Copyright  
by the author   

called as the indices 
In this work  we focus on the simplest family of such models  the single index models  SIMs  which assume that the
response   and the covariate   satisfy        cid     cid   
  where   is the true signal    is the meanzero random
noise  and   is   univariate link function   see   for the
precise de nition  They form the basis of more complicated models such as Multiple Index Models  MIMs   Diaconis   Shahshahani    and Deep Neural Networks
 DNNs   LeCun et al    which are cascades of MIMs 
Moreover  we focus on the task of estimating the parametric  linear  component   without the knowledge of
the nonparametric part   in the highdimensional setting 
where the number of samples is much smaller than the dimensionality of  
Estimating the parametric component without depending
on the speci   form of the nonparametric part appears naturally in several situations  For example  in onebit compressed sensing  Boufounos   Baraniuk    and sparse
generalized linear models  Loh   Wainwright    we
are interested in recovering the true signal vector based on
nonlinear measurements  Furthermore  in   DNN  the activation function is prespeci ed and the task is to estimate
the linear components  which are used for prediction in the
test stage  Performing nonlinear leastsquares in this setting  leads to nonconvex optimization problems that are invariably suboptimal without further assumptions  Hence 
developing estimators for the linear component that are
both statistically accurate and computationally ef cient for
  class of activation functions provide   compelling alternative  Understanding such estimators for SIMs is hence
crucial for understanding the more complicated DNNs 
Although SIMs appear to be   simple extension of the
standard linear models  most existing work in the highdimensional setting assume   follows   Gaussian distribution for estimating   without the knowledge of the nonparametric part 
It is not clear whether those estimation
methods are still valid and optimal when   is drawn from
  more general class of distributions  To relax the Gaussian
assumption  we study the setting where the distribution of
  is nonGaussian but known   priori 

NonGaussian Single Index Models via Thresholded Score Function Estimation

  Challenges of the Single Index Models

There are signi cant challenges that appear when we are
dealing with estimators for SIMs  They can be summarized as assumptions on either the link function or the data
distribution  for example  nonGaussian assumption 

  Knowledge of link function  Suppose the link function is known  for example             which corresponds to the phase retrieval model  see  Jaganathan
et al    for   survey and history of this model 
Then using an Mestimator to estimate   is   natural
procedure  Jaganathan et al    But computationally the problem becomes nonconvex and one need to
resort to either SDP based convex relaxations that are
computationally expensive or do nonconvex alternating minimization that require Gaussian assumptions
on the data for successful initialization in the highdimensional setting  Cai et al    Furthermore 
if the link function is changed  it might become challenging or impossible to obtain provably computable
estimators 

  Knowledge of data distribution  Now suppose we
want to be agnostic about the link function       we
want to estimate the linear component for   general
class of link functions  Then it becomes necessary to
make assumptions about the distribution from which
the covariates are sampled from 
In particular  assuming the covariate has Gaussian and symmetric elliptical distributions respectively   Plan   Vershynin 
  and  Goldstein et al    propose estimators
in the highdimensional setting for   large class of unknown link functions 

As mentioned previously  our estimators are based on
Stein   Lemma for nonGaussian distributions  which utilizes the score function  Estimating with the score function
is challenging due to their heavy tails  In order to illustrate
that  consider the univariate histograms provided in Figure 
  The dark shaded  more concentrated one corresponds to
the histogram of          samples from Gamma distribution with scale and shape parameters set to   and  
respectively  The transparent histogram corresponds to the
distribution of the score function of the same Gamma distribution  Note that even when the actual Gamma distribution is well concentrated  the distribution of the corresponding score function is wellspread and heavytailed  In the
high dimensional setting  in order to estimate with the score
functions  we require certain vectors or matrices based on
the score functions to be wellconcentrated in appropriate
norms  In order to achieve that  we construct robust estimators via careful truncation arguments to balance the bias
 due to thresholding variance  of the estimator  tradeoff
and achieve the required concentration 

Figure   Histogram of Score Function based on  
independent samples from the Gamma distribution with
shape   and scale   The dark histogram  we recommend
the reader to zoom in to notice it  concentrated around zero
corresponds to the Gamma distribution and the transparent
histogram corresponds to the distribution of the score of the
same Gamma distribution 

  Related Work

There is   signi cant body of work on SIMs in the lowdimensional setting  They are based on assumptions on
either the distribution of the covariate or the link functions  Assuming   monotonic link function   Han   
Sherman    propose the maximum rank correlation
estimator exploiting the relationship between monotonic
functions and rankcorrelations  Furthermore   Li   Duan 
  propose an estimator for   wide class of unknown
link functions under the assumption that the covariate follows   symmetric elliptical distribution  This assumption is
restrictive as often times the covariates are not from   symmetric distribution  For example  in several economic applications where the covariates are usually highly skewed
and heavytailed  Horowitz      line of work for
estimation in SIMs is proposed by KerChau Li which is
based on sliced inverse regression  Li    and principal Hessian directions  Li      These estimators are
based on similar symmetry assumptions and involve computing secondorder  conditional and unconditional  moments which are dif cult to estimate in highdimensions
without restrictive assumptions 
The success of Lasso and related linear estimators in highdimensions    uhlmann   van de Geer    also enabled the exploration of highdimensional SIMs  Although 
this is very much work in progress  As mentioned previously   Plan   Vershynin    show that the Lasso estimator works for the SIMs in high dimensions when the
data is Gaussian    more tighter albeit an asymptotic results under the same setting was proved in  Thrampoulidis
et al    Very recently  Goldstein et al    extend

 NonGaussian Single Index Models via Thresholded Score Function Estimation

the results of  Li   Duan    to the high dimensional
setting but it suffers from similar problems as mentioned
in the lowdimensional setting  For the case of monotone
nonparametric component   Yang et al    analyze  
nonconvex least squares approach under the assumption
that the data is subGaussian  However  the success of
their method hinges on the knowledge of the link function  Furthermore   Jiang   Liu    Lin et al   
Zhu et al    analyze the sliced inverse regression estimator in the highdimensional setting concentrating mainly
on support recovery and consistency properties  Similar
to the lowdimensional case  the assumptions made on the
covariate distribution restrict them from several realworld
applications involving nonGaussian or nonsymmetric covariate  for example highdimensional problems in economics  Fan et al    Furthermore  several results
are established on   caseby case basis for  xed link function  Speci cally  Boufounos   Baraniuk    Ai et al 
  and  Davenport et al    consider  bit compressed sensing and matrix completion respectively  where
the link is assumed to be the sign function  Also   Waldspurger et al    and  Cai et al    propose and analyze convex and nonconvex estimators for phase retrieval
respectively  in which the link is the square function  All
the above works  except  Ai et al    make Gaussian
assumptions on the data and are specialized for the speci  
link functions  The nonasymptotic result obtained in  Ai
et al    is under subGaussian assumptions  but the estimator is not consistent  Finally  there is   line of work
focussing on estimating both the parametric and the nonparametric component  Kalai   Sastry    Kakade et al 
  Alquier   Biau    Radchenko    We do not
focus on this situation in this paper as mentioned before 
To summarize  all the above works require restrictive assumption on either the data distribution or on the link function  We propose and analyze an estimator for   class of
 unknown  link functions for the case when the covariates
are drawn from   nonGaussian distribution   under the assumption that we know the distribution   priori  Note that
in several situations  one could    specialized distributions 
to realworld data that is often times skewed and heavytailed  so that it provides   good generative model of the
data  Also  mixture of Gaussian distribution  with the number of components selected appropriately  approximates the
set of all square integrable distributions to arbitrary accuracy  see for example  McLachlan   Peel    Furthermore  since this is   density estimation problem it is
unlabeled and there is no issue of label scarcity  Hence it is
possible to get accurate estimate of the distribution in most
situations of interest  Thus our work is complementary to
the existing literature and provides an estimator for   class
of models that is not addressed in the previous works  We
conclude this section with   summary of our main contri 

butions in this paper 

  We propose estimators for the parametric component
of   sparse SIM and lowrank SIM for   class of unknown link function under the assumption that the covariate distribution is nonGaussian but known   priori 

  We show that it is possible to recover   ssparse ddimensional vector and   rankr          dimensional
matrix with number of samples of the order of   log  
and           log         respectively under significantly mild moment assumptions in the SIM setting 
  We provide numerical simulation results that con rm

our theoretical predictions 

  Single Index Models
In this section  we introduce the notation and de ne the
single index models  Throughout this work  we use     to
denote the set              In addition  for   vector     Rd 
we denote by  cid   cid   the  cid pnorm of   for any       We
use      to denote the unit sphere in Rd  which is de ned
as             Rd    cid   cid      In addition  we de ne
the support of     Rd as supp               vj  cid   
Moreover  we denote the nuclear norm  operator norm  and
Frobenius norm of   matrix     Rd    by  cid cid cid   cid cid op  and
 cid     cid fro  respectively  We denote by vec    the vectorization of matrix    which is   vector in Rd    For two matrices        Rd    we de ne the trace inner product as
 cid      cid    Trace   cid    Note that it can be viewed as the
standard inner product between vec    and vec    In addition  for an univariate function            we denote by
       and        the output of applying   to each element
of   vector   and   matrix    respectively  Finally  for   random variable       with density    we use       Rd    
to denote the joint density of       Xd  which are  
identical copies of   
Now we are ready to de ne the statistical model  Let
          be an univariate function and   be the parameter of interest  which is   structured vector or   matrix 
The single index model in general is formulated as

 
where   is the covariate        is the response  and
  is the exogenous noise that is independent of    We
assume that   is centered and has bounded fourth moment 
     Ep      and        for an absolute constant
      Note in particular that this allows for heavytailed
In addition  we assume that the entries
noise as well 
of   are       
random variables with density    This
assumption could be further relaxed using more sophisticated concentration arguments  here we focus on the       
setting to clearly present the main message of this paper 

       cid     cid     

NonGaussian Single Index Models via Thresholded Score Function Estimation

Let  Yi  Xi  
   be          observations of the SIM  Our
goal is to consistently estimate   without the knowledge
of    In particular  we focus on the case when   is either
sparse or lowrank  which are de ned as follows 

       

Sparse single index model  In this setting  we assume
that      
  cid  is   sparse vector in Rd with   
nonzero entries  such that     cid     cid     Moreover  for the
model in   to be identi able  we further assume   lies
on the unit sphere      as the norm of   can always be
absorbed in the unknown link function   
Lowrank single index model  In this setting  we assume
that     Rd    has rank     cid  min      
In this
scenario      Rd    and the inner product in   is
 cid     cid    Trace   cid  For model identi ability  we
further assume that  cid cid       similar to the sparse case 

  Estimation via Score Functions
Our estimator is primarily motivated by an interesting phenomenon illustrated in  Plan   Vershynin    for the
Gaussian setting  Below  we  rst brie   summarize the
result from  Plan   Vershynin    and then provide
our alternative justi cation for the same result via Stein  
Lemma  We mainly leverage this alternative justi cation
and propose our estimators for the more general setting we
consider  Assuming for simplicity  we work in the onedimensional setting and are given          samples from the
SIM  Consider the leastsquares estimator

 cid LS   argmin

  

  cid 

  

 
 

 Yi   Xi   

Note that the above estimator is the standard leastsquares
estimator assuming   linear model       identity link function  The surprising observation from  Plan   Vershynin 
  is that  under the crucial assumption that   is stan 

dard Gaussian cid LS is   good estimator of    up to   scal 

ing  even when the data is generated from the nonlinear
SIM  The same holds true for the highdimensional setting
when the minimization is performed in an appropriately
constrained normball  for example  the  cid ball  Hence the
theory developed for the linear setting could be leveraged to
understand the performance in the SIM setting  Below  we
give an alternative justi cation for the above estimator as an
implication of Stein   Lemma in the Gaussian case  which
is summarized as follows 
Proposition    Gaussian Stein   Lemma  Stein   
Let           and           be   continuos function
such that     cid        Then we have           
    cid   
Note that in our context for SIMs  we have     cid       
and                      Now consider the following

estimator  which is based on performing leastsquares on
the sample version of the above proposition 

 cid SL   argmin

  

 
 

  cid 

 YiXi    

  

Note that  cid LS and  cid SL are the same estimators assuming

          as       This observation leads to an alternative interpretation of the estimator proposed by  Plan
  Vershynin    via Stein   Lemma for Gaussian random variables  Thus it provides an alternative justi cation
for why the linear leastsquares estimator should work in
the SIM setting  This observation naturally leads to leveraging nonGaussian versions of Stein   Lemma for dealing
with nonGaussian covariates 
We now describe our estimator for the nonGaussian setting
based on the above observation  We  rst de ne the score
function associate to   density  Let     Rd     be   probability density function de ned on Rd  The score function
Sp   Rd     associated to   is de ned as

Sp         log         xp       

Note that in the above de nition  the derivative is taken
with respect to    This is different from the more traditional
de nition of the score function where the density belongs
to   parametrized family and the derivative is taken with respect to the parameters  In the rest of the paper to simplify
the notation  we omit the subscript   from     We also
omit the subscript   from Sp when the underlying density
  is clear from the context 
We now describe   version of Stein   Lemma that is applicable for nonGaussian random variables  Note from
the motivating example for the Gaussian case that while
utilizing the Stein   Lemma for SIM estimation  assumptions on the function in Stein   Lemma translate directly
to those on the link function in SIM  We now introduce  
version of Stein   Lemma that applies to nonGaussian random variables and for continuously differentiable functions
from  Stein et al      more general version of the
Stein   Lemma that applies to   class of regular functions
is available in  Stein et al    We assume continuously
differentiable functions in the Stein   Lemma below as they
cover   wide range of practical SIM such as generalized linear models and singlelayer neural networks 
Lemma    NonGaussian Stein   Lemma  Stein et al 
  Let     Rd     be continuously differentiable
function and     Rd be   random vector with density
    Rd      which is also continuously differentiable  Under the assumption that the expectations              
and        are both wellde ned  we have the follow 

NonGaussian Single Index Models via Thresholded Score Function Estimation

ing generalized Stein   identity

                 

            dx

            dx         

 

 cid 

Rd

 cid 

 

Rd

 

Recall that in the two single index models introduced in
    in   has        entries with density    To unify
both the vector and matrix settings  in the lowrank SIM 
we identify   with vec      Rd where             In
this case    has density        
and the corresponding
score function     Rd   Rd is given by
         log                              
where the univariate function        cid 
    is applied to each
entry of    Thus      has        entries  In addition  by
Lemma   we have            by setting   to be  
constant function in   Moreover  in the context of SIMs
speci ed in   we have

               cid    cid     cid        cid 

      cid cid     cid     

 Rd

as long as the density and the link function satisfy the conditions stated in Lemma   This implies that optimization
problem

 cid cid   cid            cid       cid cid 

minimize

 
has solution         where         cid cid     cid  Hence
the above program could be used to obtain the unknown  
as long as    cid    Before we proceed to describe the sample version of the above program  we make the following
brief remark  The requirement    cid    rules out in particular the use of our approach for nonGaussian phase retrieval  where            as in that case we have      
when   is centered  But we emphasize that the same holds
true in the Gaussian and elliptical setting as well  as noted
in  Plan   Vershynin    and  Goldstein et al   
Their methods also fail to recover the true   when the SIM
model corresponds to phase retrieval  We refer the reader to
  for   discussion on overcoming this limitation 
Finally  we use   sample version of the above program as
an estimator for the unknown   In order to deal with the
highdimensional setting  we consider   regularized version
of the above formulation  More speci cally  we use the
 cid norm and nuclear norm regularization in the vector and
matrix settings respectively  However    major dif culty in
the sample setting for this procedure is that           
and its empirical counterpart may not be close enough due
to   lack of concentration  Recall our discussion from  
that even if the random variable   is lighttailed  its scorefunction      might be arbitrarily heavytailed  Furthermore  boundedfourth moment assumption on the noise   

too can be heavytailed  Thus the naive method of using the
sample version of   to estimate   leads to suboptimal
statistical rates of convergence 
To improve concentration and obtain optimal rates of convergence  we replace          with   transformed random
variable          which will be de ned precisely in   for
the sparse and lowrank cases  In particular           is  
carefully truncated version of          introduced and analyzed in  Catoni et al    Fan et al    for related
problems  that enables us to obtain wellconcentrated esti 

mators  Thus our  nal estimator cid  is de ned as the solution

to the following regularized optimization problem

minimize

 Rd

           

      cid   cid     
 

 

 cid    Yi  Xi   cid 

  cid 

  

where       is the regularization parameter which will be
speci ed later and    is the  cid norm in the vector case
and the nuclear norm in the matrix case 

  Theoretical Results
In this section  we state our main results in Theorem  
and Theorem  which establish the statistical rates of
convergence of the estimator de ned in   The proof
for both Theorems is presented in the supplementary material  Before doing so  we introduce our main moment
assumption for the single index model  This assumption is
made apart from the assumptions made on the noise and
the link function in   and   respectively  Recall that
each entry of the score function de ned in   is equal
to           cid 
        We  rst state the assumption
and make   few remarks about it 
Assumption   There exists an absolute constant    
  such that           and Ep   
          where
random variable       has density   
Consider the assumption            By CauchySchwarz inequality we have              
      cid      Note that we assum   to be centered  independent of   and has bounded fourth moment  see   If
the covariate   has bounded fourth moment along the direction of true parameter  since     is continuously differentiable     cid     cid  has bounded fourth moment as well
if     is de ned on   compact subset of    Hence the
condition           is relatively easy to satisfy and signi cantly milder than assuming that   is bounded or has
         is relatively
lighter tails  Furthermore  Ep   
mild and satis ed by   wide class of random variables 
Speci cally random variables that are nonsymmetric and
nonGaussian satisfy this property thereby allowing our approach to work with covariates not previously possible 

NonGaussian Single Index Models via Thresholded Score Function Estimation

We believe it is highly nontrivial to weaken this condition
without losing signi cantly in the rates of convergence that
we discuss below 

  Sparse Single Index Model

 

Under the above assumptions  we  rst state our theorem
on the sparse SIM  As discussed in            can by
heavytailed and hence we apply truncation to achieve concentration  Denote the jth entry of the score function  
in   as Sj   Rd              We de ne the truncated
response and score function as

 cid     sign                 
 cid Sj      sign Sj     cid Sj       cid 

where       is   predetermined threshold value  We de ne

 cid Yi similarly for all Yi          Then we de ne the estimator  cid  as the solution to the optimization problem in  
with    Yi  Xi     cid Yi    cid   Xi  and       cid cid  Here we

apply elementwise truncation in   to ensure the sample average of   converges to            in the  cid norm for an
appropriately chosen   Note that the  cid norm is the dual
norm of the  cid norm  Such   convergence requirement in
the dual norm is standard in the analysis of regularized Mestimators  Negahban et al    to achieve optimal rates 
The following theorem characterizes the convergence rates

of cid 
larization parameter   in   as   cid     log      where
least     the  cid regularized estimator cid  de ned in  

Theorem    Signal recovery for the sparse single index
model  For the sparse SIM de ned in   we assume that
    Rd has    nonzero entries  Under Assumption  
we let          log      in   and set the regu 

      is an absolute constant  Then with probability at

        cid cid     cid           
 cid cid     cid     
From this theorem  the  cid  and  cid convergence rates of  cid 
are  cid cid     cid        cid log      and  cid cid     cid   
  cid    log      respectively  These rates match the con 

satis es

vergence rates of sparse generalized linear models  Loh
  Wainwright    and sparse single index models with
Gaussian and symmetric elliptical covariates  Plan   Vershynin    Goldstein et al    which are known to
be minimaxoptimal for this problem via matching lower
bounds 

  Lowrank Single Index Model

We next state our theorem for the lowrank SIM  In this
case  we apply the nuclear norm regularization to promote
lowrankness  Note that by de nition    is matrixvalued 

Since the dual norm of the nuclear norm is the operator
norm  we need the sample average of   to converge to
          in the operator norm rapidly to achieve optimal
rates of convergence  To achieve such   goal  we leverage
the truncation argument from  Catoni et al    Minsker 
  to construct         
Let           be   nondecreasing function such that
  log              log             
Based on   we de ne   linear mapping     Rd     
Rd    as follows  For any     Rd    let

and let  cid  be the eigenvalue decomposition of  cid    In

addition  let          cid  where   is applied elementwisely on   Then we write   in block from as

 cid 

 cid     

  cid   

 cid    

 cid      

 cid 

     

   

and de ne          Finally  we de ne           

     cid            cid  where       will be speci ed later 
Therefore  our  nal estimator cid    Rd    is de ned as the

solution to the optimization problem in   with     
 cid cid cid  We note here the minimization in   is taken over
Rd    The following theorem quanti es the convergence
rates of the proposed estimator 
Theorem    Signal recovery for the lowrank single index model  For the lowrank single index model de ned
in   we assume that rank       Under Assumption
  we let

 cid     log        
 cid         

   

 

in          Moreover  the regularization parameter   in

      is an absolute constant  Then with probability at
least            the nuclear norm regularized estima 

  is set to   cid                log           where
tor cid  satis es
        cid cid     cid cid           
 cid cid     cid fro    
 cid cid     cid fro
  cid             log           and  cid cid     cid cid   
      cid           log           Note that the rate

theorem  we

have

obtained is minimaxoptimal up to   logarithmic factor 
Furthermore 
it matches the rates for lowrank single
index models with Gaussian and symmetric elliptical
distributions up to   logarithmic factor  Plan   Vershynin 
  Goldstein et al   

this

By

 

NonGaussian Single Index Models via Thresholded Score Function Estimation

  Numerical Experiments
We assess the  nite sample performance of the proposed
estimators on simulated data  Throughout this section  we
let           and set the link function in   as one of
       exp   
              sin    and       
which are plotted in Figure   We set    to be one of
    Gamma distribution with shape parameter   and scale
parameter    ii  Student   tdistribution with   degrees of
freedom  and  iii  Rayleigh distribution with scale parameter   To measure the estimation accuracy  we use the cosine distance

 

cos  cid           cid cid cid   cid cid   cid 

with nonGaussian data  Our current approach requires
that    cid    which is not applicable  The main reason this
happens is we use    rstorder version of Stein   Lemma 
Such   problem could overcome by secondorder Stein  
Lemma  Janzamin et al    Obtaining rateoptimal
estimators based on secondorder score functions require
addressing several challenges  Concentrating on phase retrieval  and sparse phase retrieval  we plan to report our
results for the above problem in the near future 

where   stands for the Euclidean norm in the vector case
and the Frobenius norm when   is   matrix  Here we re 

port the cosine distance rather than  cid cid     cid  to com 

pare the performances for   having different distributions 
where   may have different values 
For the vector case  we                 and vary
   The support of   is chosen uniformly random among
all subsets of              For each     supp  we set
 
         where each    is an        Rademacher
 
     
random variable  In addition  the regularization parameter

  is set to  cid log      We plot the cosine distance against
the signal strength cid    log     in Figure     and     for

   and    respectively  based on   independent trials for
each    As shown in this  gure  the estimation error grows
sublinearly as   function of the signal strength 
As for the matrix case  we                       and let
  vary  The signal parameter   is equal to   SV  cid  where
       Rd   are random orthogonal matrices and   is  
diagonal matrix with    nonzero entries  Moreover  we set
   which implies
the nonzero diagonal entries of   as  
 cid cid fro     We set the regularization parameter as    

 cid         log           Furthermore  we use the
against the signal strength cid           log         

proximal gradient descent algorithm  with the learning rate
 xed to   to solve the nuclear norm regularization problem in   To present the result  we plot the cosine distant

in Figure     based on   independent trials  As shown
in this  gure  the error is bounded by   linear function of
the signal strength  which corroborates Theorem  

 

  Conclusion
In this paper  we consider SIMs in the highdimensional
nonGaussian setting and proposed estimators based on
Stein   Lemma for   wider class of unknown link functions and covariate distributions  We consider both sparse
and lowrank models and propose minimax rateoptimal
estimators under fairly mild assumptions  An interesting
avenue of future work is the problem of phase retrieval

NonGaussian Single Index Models via Thresholded Score Function Estimation

Figure   Plot of the link functions               sin     left  and       
are nonlinear and not monotone 

 

       exp     right  Both functions

Figure   Cosine distances between the true parameter and the estimated parameter in the sparse SIM with the link function
in   set to     left  and     right  Here we set           and vary   

Figure   Cosine distances between the true parameter and the estimated parameter in the lowrank SIM with link function
in   set to     left  and     right  Here we set                    and vary   

        sin             exp   ps logd   cos   Gamma   Rayleigh ps logd   cos   Gamma   Rayleigh pr     log       cos   Gamma   Rayleigh pr     log       cos   Gamma   Rayleigh NonGaussian Single Index Models via Thresholded Score Function Estimation

References
Ai  Albert  Lapanowski  Alex  Plan  Yaniv  and Vershynin 
Roman  Onebit compressed sensing with nongaussian
measurements  Linear Algebra and its Applications 
   

Alquier  Pierre and Biau    erard  Sparse singleindex
model  The Journal of Machine Learning Research   
   

Boucheron  St ephane  Lugosi    abor  and Massart  Pascal 
Concentration inequalities    nonasymptotic theory of
independence  Oxford university press   

Boufounos  Petros   and Baraniuk  Richard     bit compressive sensing  In Information Sciences and Systems 
  CISS    nd Annual Conference on  pp   
  IEEE   

  uhlmann  Peter and van de Geer  Sara  Statistics for highdimensional data  methods  theory and applications 
Springer Science   Business Media   

Cai    Tony  Li  Xiaodong  and Ma  Zongming  Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger  ow  arXiv preprint
arXiv   

Catoni  Olivier et al  Challenging the empirical mean
and empirical variance    deviation study  Annales de
  Institut Henri Poincar    Probabilit es et Statistiques   
   

Davenport  Mark    Plan  Yaniv  van den Berg  Ewout  and
Wootters  Mary   bit matrix completion  Information
and Inference     

Diaconis     and Shahshahani     On nonlinear functions
of linear combinations  SIAM Journal on Scienti   and
Statistical Computing     

Fan     Lv     and Qi     Sparse highdimensional models
in economics  Annual review of economics   
   

Fan  Jianqing  Wang  Weichen  and Zhu  Ziwei  RoarXiv preprint

lowrank matrix recovery 

bust
arXiv   

Goldstein  Larry  Minsker  Stanislav  and Wei  Xiaohan 
Structured signal recovery from nonlinear and heavytailed measurements  arXiv preprint arXiv 
 

Horowitz  Joel   

Semiparametric and nonparametric

methods in econometrics  volume   Springer   

Jaganathan  Kishore  Eldar  Yonina    and Hassibi  Babak 
Phase retrieval  An overview of recent developments 
arXiv preprint arXiv   

Janzamin  Majid  Sedghi  Hanie  and Anandkumar  Anima  Score function features for discriminative learnarXiv preprint
ing  Matrix and tensor framework 
arXiv   

Jiang     and Liu        Variable selection for general index models via sliced inverse regression  The Annals of
Statistics     

Kakade  Sham    Kanade  Varun  Shamir  Ohad  and
Kalai  Adam  Ef cient learning of generalized linear
and single index models with isotonic regression  In Advances in Neural Information Processing Systems  pp 
   

Kalai  Adam Tauman and Sastry  Ravi  The isotron algorithm  Highdimensional isotonic regression  In Conference on Learning Theory   

LeCun  Yann  Bengio  Yoshua  and Hinton  Geoffrey  Deep

learning  Nature     

Li  KerChau  Sliced inverse regression for dimension reduction  Journal of the American Statistical Association 
   

Li  KerChau  On principal Hessian directions for data visualization and dimension reduction  Another application of Stein   lemma  Journal of the American Statistical Association     

Li  KerChau and Duan  Naihua  Regression analysis under link violation  The Annals of Statistics   
   

Lin     Zhao     and Liu        On consistency and sparsity
for sliced inverse regression in high dimensions  arXiv
preprint arXiv   

Loh  PoLing and Wainwright  Martin    Regularized mestimators with nonconvexity  Statistical and algorithmic theory for local optima  Journal of Machine Learning Research     

McLachlan  Geoffrey and Peel  David  Finite mixture mod 

els  John Wiley   Sons   

Han  Aaron    Nonparametric analysis of   generalized
regression model  the maximum rank correlation estimator  Journal of Econometrics     

Minsker  Stanislav  Subgaussian estimators of the mean
arXiv

of   random matrix with heavytailed entries 
preprint arXiv   

NonGaussian Single Index Models via Thresholded Score Function Estimation

Zhu  Lixing  Miao  Baiqi  and Peng  Heng  On sliced inverse regression with highdimensional covariates  Journal of the American Statistical Association   
   

Negahban  Sahand    Ravikumar  Pradeep  Wainwright 
Martin    and Yu  Bin    uni ed framework for highdimensional analysis of Mestimators with decomposable regularizers  Statistical Science     
 

Plan  Yaniv and Vershynin  Roman  The generalized lasso
with nonlinear observations  IEEE Transactions on information theory     

Radchenko  Peter  High dimensional single index models 

Journal of Multivariate Analysis     

Ravikumar  Pradeep  Lafferty  John  Liu  Han  and Wasserman  Larry  Sparse additive models  Journal of the Royal
Statistical Society  Series    Statistical Methodology 
   

Sherman  Robert    The limiting distribution of the maximum rank correlation estimator  Econometrica  Journal
of the Econometric Society     

Stein       bound for the error in the normal approximation to the distribution of   sum of dependent random
variables  In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability  Volume
  Probability Theory  The Regents of the University of
California   

Stein  Charles  Diaconis  Persi  Holmes  Susan  Reinert 
Gesine  et al  Use of exchangeable pairs in the analysis of
simulations  In Stein   Method  Institute of Mathematical
Statistics   

Thrampoulidis  Christos  Abbasi  Ehsan  and Hassibi 
Babak  Lasso with nonlinear measurements is equivalent to one with linear measurements  Advances in Neural Information Processing Systems   

Vershynin  Roman  Estimation in high dimensions    geometric perspective  In Sampling theory    renaissance 
pp    Springer   

Waldspurger  Ir ene  dAspremont  Alexandre  and Mallat 
St ephane  Phase recovery  maxcut and complex semidefinite programming  Mathematical Programming   
   

Yang  Zhuoran  Wang  Zhaoran  Liu  Han  Eldar  Yonina   
and Zhang  Tong  Sparse nonlinear regression  Parameter estimation and asymptotic inference  International
Conference on Machine Learning   

Yuan  Ming  Zhou  DingXuan  et al  Minimax optimal
rates of estimation in high dimensional additive models 
The Annals of Statistics     

