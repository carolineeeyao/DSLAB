{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 \n",
    "\n",
    "Ali Tejani, amt3639\n",
    "\n",
    "Caroline Yao, chy253\n",
    "\n",
    "Allen Hwang, ah45755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating beta with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 51\n",
    "p = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_b(reg,n,p,seed=None):\n",
    "    # seed rng so that we can compare different models\n",
    "    np.random.seed(seed=seed)\n",
    "    b_hat = []\n",
    "    for _ in range(50):\n",
    "        # create different distributions and fit the model\n",
    "        X = np.random.normal(0,1,(n,p))\n",
    "        e = np.random.normal(0,.25,(n,1))\n",
    "        b = np.ones((p,1))\n",
    "        y = np.dot(X,b) + e\n",
    "        reg.fit(X,y)\n",
    "        reg.coef_\n",
    "        b_hat.append(reg.coef_)\n",
    "    b_hat = np.array(b_hat)\n",
    "    print type(reg).__name__,':'\n",
    "    if isinstance(reg,Ridge):\n",
    "        print 'alpha',reg.get_params()['alpha']\n",
    "    print 'Average of B values: ',b_hat.mean(axis=0).mean()\n",
    "    print 'Variance of B values: ',b_hat.var(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression :\n",
      "Average of B values:  0.998922566105\n",
      "Variance of B values:  0.0856353673332\n"
     ]
    }
   ],
   "source": [
    "estimate_b(LinearRegression(fit_intercept=False),n,p,seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge :\n",
      "alpha 0.01\n",
      "Average of B values:  0.978387136606\n",
      "Variance of B values:  0.0575246517313\n",
      "Ridge :\n",
      "alpha 0.1\n",
      "Average of B values:  0.941339096029\n",
      "Variance of B values:  0.0434700881018\n",
      "Ridge :\n",
      "alpha 1\n",
      "Average of B values:  0.850710160296\n",
      "Variance of B values:  0.0699959890577\n",
      "Ridge :\n",
      "alpha 10\n",
      "Average of B values:  0.629478441272\n",
      "Variance of B values:  0.096124835666\n",
      "Ridge :\n",
      "alpha 100\n",
      "Average of B values:  0.264736366056\n",
      "Variance of B values:  0.0418580179253\n",
      "Ridge :\n",
      "alpha 1000\n",
      "Average of B values:  0.0452410331668\n",
      "Variance of B values:  0.00189075156562\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01,0.1,1,10,100,1000]\n",
    "for a in alphas:\n",
    "    estimate_b(Ridge(alpha=a),n,p,seed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression gives us an acurate value when beta is simple. Ridge regression gives us increasingly worse values for beta as we increase alpha. However, ridge regression also gives a lower variance in certain ranges for alpha, which means the values are more closer together and more simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 2: Chapter 6, Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.  Split the data set into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data, clean up, and split to train and test data, 80/20\n",
    "college = pd.read_csv('College.csv',index_col=0)\n",
    "college['Private'] = college['Private'].replace('Yes',1).replace('No',0)\n",
    "collegeX = college[[i for i in college.axes[1] if i != 'Apps']]\n",
    "collegeY = college['Apps']\n",
    "college_trainX,college_testX,college_trainY,college_testY = train_test_split(collegeX,collegeY)\n",
    "college_trainX = scale(college_trainX)\n",
    "college_testX = scale(college_testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Fit a linear model using least squares on the training set, and report the test error obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_score(reg):\n",
    "    reg.fit(college_trainX,college_trainY)\n",
    "    print 'R**2:',reg.score(college_testX,college_testY)\n",
    "    print 'MSE:',mean_squared_error(reg.predict(college_testX),college_testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: 0.868094345284\n",
      "MSE: 2884797.30007\n"
     ]
    }
   ],
   "source": [
    "reg_score(LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.  Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: 0.865695224679\n",
      "MSE: 2937266.44297\n",
      "alpha chosen: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV,LassoCV\n",
    "ridge_reg = RidgeCV(alphas=[0.001,0.005,0.01,0.05,0.1,0.5,1,10,100])\n",
    "reg_score(ridge_reg)\n",
    "print 'alpha chosen:',ridge_reg.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.  Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: 0.863810974856\n",
      "MSE: 2978475.28131\n",
      "alpha chosen: 10.0\n",
      "number of zeros: 3\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = LassoCV(alphas=[0.0001,0.001,0.005,0.01,0.05,0.1,0.5,1,10,100])\n",
    "reg_score(lasso_reg)\n",
    "print 'alpha chosen:',lasso_reg.alpha_\n",
    "print 'number of zeros:',np.count_nonzero(np.isclose(lasso_reg.coef_,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### e. Fit a PCR model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "pca = PCA()\n",
    "linreg = LinearRegression()\n",
    "# pipe pca to linear regression\n",
    "pipe = Pipeline(steps=[('pca',pca),('linear',linreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: 0.868094345284\n",
      "MSE: 2884797.30007\n",
      "number of components used: 17\n"
     ]
    }
   ],
   "source": [
    "# run cv on our model, using from 1 to 17 features\n",
    "gscv = GridSearchCV(pipe, dict(pca__n_components=np.arange(1,college_trainX.shape[1]+1)),cv=10)\n",
    "reg_score(gscv)\n",
    "print 'number of components used:',gscv.best_params_['pca__n_components']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: 0.86876889326\n",
      "MSE: 2870044.83035\n",
      "number of components used: 17\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale \n",
    "pls = PLSRegression()\n",
    "# run cv on our model, using from 1 to 17 features\n",
    "gscv = GridSearchCV(pls, dict(n_components=np.arange(1,college_trainX.shape[1]+1)),cv=10)\n",
    "reg_score(gscv)\n",
    "print 'number of components used:',gscv.cv_results_['rank_test_score'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best scores are received for linear regression and for PCR/PLS with all components. This means that the output fits closely with the features. Linear regression is the best model for this data, since it is the most accurate without needing the extra work of performing pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 3\n",
    "## A) Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677082</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             crim          zn       indus        chas         nox          rm  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              age         dis         rad         tax     ptratio       black  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            lstat        medv  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.read_csv(\"boston.csv\")\n",
    "X = boston.loc[:, \"zn\":\"medv\"]\n",
    "y = boston[\"crim\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: 0.514718296775\n",
      "MSE: 62.5329492975\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV(alphas = (np.arange(0.01, 10, 0.01)), cv=5)\n",
    "lasso.fit(X_train, y_train)\n",
    "print 'R**2:',lasso.score(X_train, y_train, sample_weight=None)\n",
    "print 'MSE:',mean_squared_error(y_test, lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: 0.515239153617\n",
      "MSE: 62.587376193\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeCV(alphas = (np.arange(0.01, 10, 0.01)), cv=5)\n",
    "ridge.fit(X_train, y_train)\n",
    "print 'R**2:',ridge.score(X_train, y_train, sample_weight=None)\n",
    "print 'MSE:',mean_squared_error(y_test, ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUfRCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe757b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca2 = PCA()\n",
    "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
    "n = len(X_reduced_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = KFold(n_splits = 10, shuffle=True)\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 13 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 14):\n",
    "    score = -1*cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "\n",
    "plt.plot(np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Boston Crime Data')\n",
    "plt.xlim(xmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows that the training MSE can be reduced by doing regression on 12 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: -5950.88524485\n",
      "MSE: 62.5364837106\n"
     ]
    }
   ],
   "source": [
    "pls = PLSRegression(n_components=8)\n",
    "pls.fit(scale(X_train), y_train)\n",
    "print 'R**2:',pls.score(X_train, y_train)\n",
    "print 'MSE:',mean_squared_error(y_test, pls.predict(scale(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2: -6943.3973141\n",
      "MSE: 62.7086707259\n"
     ]
    }
   ],
   "source": [
    "pls = PLSRegression(n_components=12)\n",
    "pls.fit(scale(X_train), y_train)\n",
    "print 'R**2:',pls.score(X_train, y_train)\n",
    "print 'MSE:',mean_squared_error(y_test, pls.predict(scale(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that gives the best MSE is the LassoCV.  This means that the error between each of the predictions is lower than the other models.  It should be noted that the R-Squared of the all the data, RidgeCV did slightly better.  This means that the RidgeCV model actually factors more the data given even though the average error of each of their prediction is much higher.  Finally, with PCA, it has both high MSE (meaning each prediction has a higher amount of error) and a negative score( which means that for this error, the model doesn't account for any of it), thus PCA isn't the best regarding this data.\n",
    "\n",
    "## B+ C) Model Selection and Relevant features\n",
    "Based on the data given, the model that seems to perform well on this data set is RidgeCV.  That is because even thought the MSE was higher than Lasso, it also has a higher R-Sqaured.  This means that the Ridge model more closely fits with the given data, and is also able to \"make up\" the difference of the MSE.  The model selected uses all features available to the data, as shown in the graph above that the lowest MSE of the data hovers aroudn 12-13 features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
